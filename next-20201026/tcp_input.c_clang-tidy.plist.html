<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"0": {"id": 0, "path": "/src/net/ipv4/tcp_input.c", "content": "// SPDX-License-Identifier: GPL-2.0\n/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tImplementation of the Transmission Control Protocol(TCP).\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tMark Evans, <evansmp@uhura.aston.ac.uk>\n *\t\tCorey Minyard <wf-rch!minyard@relay.EU.net>\n *\t\tFlorian La Roche, <flla@stud.uni-sb.de>\n *\t\tCharles Hedrick, <hedrick@klinzhai.rutgers.edu>\n *\t\tLinus Torvalds, <torvalds@cs.helsinki.fi>\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\t\tMatthew Dillon, <dillon@apollo.west.oic.com>\n *\t\tArnt Gulbrandsen, <agulbra@nvg.unit.no>\n *\t\tJorge Cwik, <jorge@laser.satlink.net>\n */\n\n/*\n * Changes:\n *\t\tPedro Roque\t:\tFast Retransmit/Recovery.\n *\t\t\t\t\tTwo receive queues.\n *\t\t\t\t\tRetransmit queue handled by TCP.\n *\t\t\t\t\tBetter retransmit timer handling.\n *\t\t\t\t\tNew congestion avoidance.\n *\t\t\t\t\tHeader prediction.\n *\t\t\t\t\tVariable renaming.\n *\n *\t\tEric\t\t:\tFast Retransmit.\n *\t\tRandy Scott\t:\tMSS option defines.\n *\t\tEric Schenk\t:\tFixes to slow start algorithm.\n *\t\tEric Schenk\t:\tYet another double ACK bug.\n *\t\tEric Schenk\t:\tDelayed ACK bug fixes.\n *\t\tEric Schenk\t:\tFloyd style fast retrans war avoidance.\n *\t\tDavid S. Miller\t:\tDon't allow zero congestion window.\n *\t\tEric Schenk\t:\tFix retransmitter so that it sends\n *\t\t\t\t\tnext packet on ack of previous packet.\n *\t\tAndi Kleen\t:\tMoved open_request checking here\n *\t\t\t\t\tand process RSTs for open_requests.\n *\t\tAndi Kleen\t:\tBetter prune_queue, and other fixes.\n *\t\tAndrey Savochkin:\tFix RTT measurements in the presence of\n *\t\t\t\t\ttimestamps.\n *\t\tAndrey Savochkin:\tCheck sequence numbers correctly when\n *\t\t\t\t\tremoving SACKs due to in sequence incoming\n *\t\t\t\t\tdata segments.\n *\t\tAndi Kleen:\t\tMake sure we never ack data there is not\n *\t\t\t\t\tenough room for. Also make this condition\n *\t\t\t\t\ta fatal error if it might still happen.\n *\t\tAndi Kleen:\t\tAdd tcp_measure_rcv_mss to make\n *\t\t\t\t\tconnections with MSS<min(MTU,ann. MSS)\n *\t\t\t\t\twork without delayed acks.\n *\t\tAndi Kleen:\t\tProcess packets with PSH set in the\n *\t\t\t\t\tfast path.\n *\t\tJ Hadi Salim:\t\tECN support\n *\t \tAndrei Gurtov,\n *\t\tPasi Sarolahti,\n *\t\tPanu Kuhlberg:\t\tExperimental audit of TCP (re)transmission\n *\t\t\t\t\tengine. Lots of bugs are found.\n *\t\tPasi Sarolahti:\t\tF-RTO for dealing with spurious RTOs\n */\n\n#define pr_fmt(fmt) \"TCP: \" fmt\n\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n#include <linux/sysctl.h>\n#include <linux/kernel.h>\n#include <linux/prefetch.h>\n#include <net/dst.h>\n#include <net/tcp.h>\n#include <net/inet_common.h>\n#include <linux/ipsec.h>\n#include <asm/unaligned.h>\n#include <linux/errqueue.h>\n#include <trace/events/tcp.h>\n#include <linux/jump_label_ratelimit.h>\n#include <net/busy_poll.h>\n#include <net/mptcp.h>\n\nint sysctl_tcp_max_orphans __read_mostly = NR_FILE;\n\n#define FLAG_DATA\t\t0x01 /* Incoming frame contained data.\t\t*/\n#define FLAG_WIN_UPDATE\t\t0x02 /* Incoming ACK was a window update.\t*/\n#define FLAG_DATA_ACKED\t\t0x04 /* This ACK acknowledged new data.\t\t*/\n#define FLAG_RETRANS_DATA_ACKED\t0x08 /* \"\" \"\" some of which was retransmitted.\t*/\n#define FLAG_SYN_ACKED\t\t0x10 /* This ACK acknowledged SYN.\t\t*/\n#define FLAG_DATA_SACKED\t0x20 /* New SACK.\t\t\t\t*/\n#define FLAG_ECE\t\t0x40 /* ECE in this ACK\t\t\t\t*/\n#define FLAG_LOST_RETRANS\t0x80 /* This ACK marks some retransmission lost */\n#define FLAG_SLOWPATH\t\t0x100 /* Do not skip RFC checks for window update.*/\n#define FLAG_ORIG_SACK_ACKED\t0x200 /* Never retransmitted data are (s)acked\t*/\n#define FLAG_SND_UNA_ADVANCED\t0x400 /* Snd_una was changed (!= FLAG_DATA_ACKED) */\n#define FLAG_DSACKING_ACK\t0x800 /* SACK blocks contained D-SACK info */\n#define FLAG_SET_XMIT_TIMER\t0x1000 /* Set TLP or RTO timer */\n#define FLAG_SACK_RENEGING\t0x2000 /* snd_una advanced to a sacked seq */\n#define FLAG_UPDATE_TS_RECENT\t0x4000 /* tcp_replace_ts_recent() */\n#define FLAG_NO_CHALLENGE_ACK\t0x8000 /* do not call tcp_send_challenge_ack()\t*/\n#define FLAG_ACK_MAYBE_DELAYED\t0x10000 /* Likely a delayed ACK */\n\n#define FLAG_ACKED\t\t(FLAG_DATA_ACKED|FLAG_SYN_ACKED)\n#define FLAG_NOT_DUP\t\t(FLAG_DATA|FLAG_WIN_UPDATE|FLAG_ACKED)\n#define FLAG_CA_ALERT\t\t(FLAG_DATA_SACKED|FLAG_ECE|FLAG_DSACKING_ACK)\n#define FLAG_FORWARD_PROGRESS\t(FLAG_ACKED|FLAG_DATA_SACKED)\n\n#define TCP_REMNANT (TCP_FLAG_FIN|TCP_FLAG_URG|TCP_FLAG_SYN|TCP_FLAG_PSH)\n#define TCP_HP_BITS (~(TCP_RESERVED_BITS|TCP_FLAG_PSH))\n\n#define REXMIT_NONE\t0 /* no loss recovery to do */\n#define REXMIT_LOST\t1 /* retransmit packets marked lost */\n#define REXMIT_NEW\t2 /* FRTO-style transmit of unsent/new packets */\n\n#if IS_ENABLED(CONFIG_TLS_DEVICE)\nstatic DEFINE_STATIC_KEY_DEFERRED_FALSE(clean_acked_data_enabled, HZ);\n\nvoid clean_acked_data_enable(struct inet_connection_sock *icsk,\n\t\t\t     void (*cad)(struct sock *sk, u32 ack_seq))\n{\n\ticsk->icsk_clean_acked = cad;\n\tstatic_branch_deferred_inc(&clean_acked_data_enabled);\n}\nEXPORT_SYMBOL_GPL(clean_acked_data_enable);\n\nvoid clean_acked_data_disable(struct inet_connection_sock *icsk)\n{\n\tstatic_branch_slow_dec_deferred(&clean_acked_data_enabled);\n\ticsk->icsk_clean_acked = NULL;\n}\nEXPORT_SYMBOL_GPL(clean_acked_data_disable);\n\nvoid clean_acked_data_flush(void)\n{\n\tstatic_key_deferred_flush(&clean_acked_data_enabled);\n}\nEXPORT_SYMBOL_GPL(clean_acked_data_flush);\n#endif\n\n#ifdef CONFIG_CGROUP_BPF\nstatic void bpf_skops_parse_hdr(struct sock *sk, struct sk_buff *skb)\n{\n\tbool unknown_opt = tcp_sk(sk)->rx_opt.saw_unknown &&\n\t\tBPF_SOCK_OPS_TEST_FLAG(tcp_sk(sk),\n\t\t\t\t       BPF_SOCK_OPS_PARSE_UNKNOWN_HDR_OPT_CB_FLAG);\n\tbool parse_all_opt = BPF_SOCK_OPS_TEST_FLAG(tcp_sk(sk),\n\t\t\t\t\t\t    BPF_SOCK_OPS_PARSE_ALL_HDR_OPT_CB_FLAG);\n\tstruct bpf_sock_ops_kern sock_ops;\n\n\tif (likely(!unknown_opt && !parse_all_opt))\n\t\treturn;\n\n\t/* The skb will be handled in the\n\t * bpf_skops_established() or\n\t * bpf_skops_write_hdr_opt().\n\t */\n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_RECV:\n\tcase TCP_SYN_SENT:\n\tcase TCP_LISTEN:\n\t\treturn;\n\t}\n\n\tsock_owned_by_me(sk);\n\n\tmemset(&sock_ops, 0, offsetof(struct bpf_sock_ops_kern, temp));\n\tsock_ops.op = BPF_SOCK_OPS_PARSE_HDR_OPT_CB;\n\tsock_ops.is_fullsock = 1;\n\tsock_ops.sk = sk;\n\tbpf_skops_init_skb(&sock_ops, skb, tcp_hdrlen(skb));\n\n\tBPF_CGROUP_RUN_PROG_SOCK_OPS(&sock_ops);\n}\n\nstatic void bpf_skops_established(struct sock *sk, int bpf_op,\n\t\t\t\t  struct sk_buff *skb)\n{\n\tstruct bpf_sock_ops_kern sock_ops;\n\n\tsock_owned_by_me(sk);\n\n\tmemset(&sock_ops, 0, offsetof(struct bpf_sock_ops_kern, temp));\n\tsock_ops.op = bpf_op;\n\tsock_ops.is_fullsock = 1;\n\tsock_ops.sk = sk;\n\t/* sk with TCP_REPAIR_ON does not have skb in tcp_finish_connect */\n\tif (skb)\n\t\tbpf_skops_init_skb(&sock_ops, skb, tcp_hdrlen(skb));\n\n\tBPF_CGROUP_RUN_PROG_SOCK_OPS(&sock_ops);\n}\n#else\nstatic void bpf_skops_parse_hdr(struct sock *sk, struct sk_buff *skb)\n{\n}\n\nstatic void bpf_skops_established(struct sock *sk, int bpf_op,\n\t\t\t\t  struct sk_buff *skb)\n{\n}\n#endif\n\nstatic void tcp_gro_dev_warn(struct sock *sk, const struct sk_buff *skb,\n\t\t\t     unsigned int len)\n{\n\tstatic bool __once __read_mostly;\n\n\tif (!__once) {\n\t\tstruct net_device *dev;\n\n\t\t__once = true;\n\n\t\trcu_read_lock();\n\t\tdev = dev_get_by_index_rcu(sock_net(sk), skb->skb_iif);\n\t\tif (!dev || len >= dev->mtu)\n\t\t\tpr_warn(\"%s: Driver has suspect GRO implementation, TCP performance may be compromised.\\n\",\n\t\t\t\tdev ? dev->name : \"Unknown driver\");\n\t\trcu_read_unlock();\n\t}\n}\n\n/* Adapt the MSS value used to make delayed ack decision to the\n * real world.\n */\nstatic void tcp_measure_rcv_mss(struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tconst unsigned int lss = icsk->icsk_ack.last_seg_size;\n\tunsigned int len;\n\n\ticsk->icsk_ack.last_seg_size = 0;\n\n\t/* skb->len may jitter because of SACKs, even if peer\n\t * sends good full-sized frames.\n\t */\n\tlen = skb_shinfo(skb)->gso_size ? : skb->len;\n\tif (len >= icsk->icsk_ack.rcv_mss) {\n\t\ticsk->icsk_ack.rcv_mss = min_t(unsigned int, len,\n\t\t\t\t\t       tcp_sk(sk)->advmss);\n\t\t/* Account for possibly-removed options */\n\t\tif (unlikely(len > icsk->icsk_ack.rcv_mss +\n\t\t\t\t   MAX_TCP_OPTION_SPACE))\n\t\t\ttcp_gro_dev_warn(sk, skb, len);\n\t} else {\n\t\t/* Otherwise, we make more careful check taking into account,\n\t\t * that SACKs block is variable.\n\t\t *\n\t\t * \"len\" is invariant segment length, including TCP header.\n\t\t */\n\t\tlen += skb->data - skb_transport_header(skb);\n\t\tif (len >= TCP_MSS_DEFAULT + sizeof(struct tcphdr) ||\n\t\t    /* If PSH is not set, packet should be\n\t\t     * full sized, provided peer TCP is not badly broken.\n\t\t     * This observation (if it is correct 8)) allows\n\t\t     * to handle super-low mtu links fairly.\n\t\t     */\n\t\t    (len >= TCP_MIN_MSS + sizeof(struct tcphdr) &&\n\t\t     !(tcp_flag_word(tcp_hdr(skb)) & TCP_REMNANT))) {\n\t\t\t/* Subtract also invariant (if peer is RFC compliant),\n\t\t\t * tcp header plus fixed timestamp option length.\n\t\t\t * Resulting \"len\" is MSS free of SACK jitter.\n\t\t\t */\n\t\t\tlen -= tcp_sk(sk)->tcp_header_len;\n\t\t\ticsk->icsk_ack.last_seg_size = len;\n\t\t\tif (len == lss) {\n\t\t\t\ticsk->icsk_ack.rcv_mss = len;\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t\tif (icsk->icsk_ack.pending & ICSK_ACK_PUSHED)\n\t\t\ticsk->icsk_ack.pending |= ICSK_ACK_PUSHED2;\n\t\ticsk->icsk_ack.pending |= ICSK_ACK_PUSHED;\n\t}\n}\n\nstatic void tcp_incr_quickack(struct sock *sk, unsigned int max_quickacks)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tunsigned int quickacks = tcp_sk(sk)->rcv_wnd / (2 * icsk->icsk_ack.rcv_mss);\n\n\tif (quickacks == 0)\n\t\tquickacks = 2;\n\tquickacks = min(quickacks, max_quickacks);\n\tif (quickacks > icsk->icsk_ack.quick)\n\t\ticsk->icsk_ack.quick = quickacks;\n}\n\nvoid tcp_enter_quickack_mode(struct sock *sk, unsigned int max_quickacks)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\ttcp_incr_quickack(sk, max_quickacks);\n\tinet_csk_exit_pingpong_mode(sk);\n\ticsk->icsk_ack.ato = TCP_ATO_MIN;\n}\nEXPORT_SYMBOL(tcp_enter_quickack_mode);\n\n/* Send ACKs quickly, if \"quick\" count is not exhausted\n * and the session is not interactive.\n */\n\nstatic bool tcp_in_quickack_mode(struct sock *sk)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\n\treturn (dst && dst_metric(dst, RTAX_QUICKACK)) ||\n\t\t(icsk->icsk_ack.quick && !inet_csk_in_pingpong_mode(sk));\n}\n\nstatic void tcp_ecn_queue_cwr(struct tcp_sock *tp)\n{\n\tif (tp->ecn_flags & TCP_ECN_OK)\n\t\ttp->ecn_flags |= TCP_ECN_QUEUE_CWR;\n}\n\nstatic void tcp_ecn_accept_cwr(struct sock *sk, const struct sk_buff *skb)\n{\n\tif (tcp_hdr(skb)->cwr) {\n\t\ttcp_sk(sk)->ecn_flags &= ~TCP_ECN_DEMAND_CWR;\n\n\t\t/* If the sender is telling us it has entered CWR, then its\n\t\t * cwnd may be very low (even just 1 packet), so we should ACK\n\t\t * immediately.\n\t\t */\n\t\tif (TCP_SKB_CB(skb)->seq != TCP_SKB_CB(skb)->end_seq)\n\t\t\tinet_csk(sk)->icsk_ack.pending |= ICSK_ACK_NOW;\n\t}\n}\n\nstatic void tcp_ecn_withdraw_cwr(struct tcp_sock *tp)\n{\n\ttp->ecn_flags &= ~TCP_ECN_QUEUE_CWR;\n}\n\nstatic void __tcp_ecn_check_ce(struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tswitch (TCP_SKB_CB(skb)->ip_dsfield & INET_ECN_MASK) {\n\tcase INET_ECN_NOT_ECT:\n\t\t/* Funny extension: if ECT is not set on a segment,\n\t\t * and we already seen ECT on a previous segment,\n\t\t * it is probably a retransmit.\n\t\t */\n\t\tif (tp->ecn_flags & TCP_ECN_SEEN)\n\t\t\ttcp_enter_quickack_mode(sk, 2);\n\t\tbreak;\n\tcase INET_ECN_CE:\n\t\tif (tcp_ca_needs_ecn(sk))\n\t\t\ttcp_ca_event(sk, CA_EVENT_ECN_IS_CE);\n\n\t\tif (!(tp->ecn_flags & TCP_ECN_DEMAND_CWR)) {\n\t\t\t/* Better not delay acks, sender can have a very low cwnd */\n\t\t\ttcp_enter_quickack_mode(sk, 2);\n\t\t\ttp->ecn_flags |= TCP_ECN_DEMAND_CWR;\n\t\t}\n\t\ttp->ecn_flags |= TCP_ECN_SEEN;\n\t\tbreak;\n\tdefault:\n\t\tif (tcp_ca_needs_ecn(sk))\n\t\t\ttcp_ca_event(sk, CA_EVENT_ECN_NO_CE);\n\t\ttp->ecn_flags |= TCP_ECN_SEEN;\n\t\tbreak;\n\t}\n}\n\nstatic void tcp_ecn_check_ce(struct sock *sk, const struct sk_buff *skb)\n{\n\tif (tcp_sk(sk)->ecn_flags & TCP_ECN_OK)\n\t\t__tcp_ecn_check_ce(sk, skb);\n}\n\nstatic void tcp_ecn_rcv_synack(struct tcp_sock *tp, const struct tcphdr *th)\n{\n\tif ((tp->ecn_flags & TCP_ECN_OK) && (!th->ece || th->cwr))\n\t\ttp->ecn_flags &= ~TCP_ECN_OK;\n}\n\nstatic void tcp_ecn_rcv_syn(struct tcp_sock *tp, const struct tcphdr *th)\n{\n\tif ((tp->ecn_flags & TCP_ECN_OK) && (!th->ece || !th->cwr))\n\t\ttp->ecn_flags &= ~TCP_ECN_OK;\n}\n\nstatic bool tcp_ecn_rcv_ecn_echo(const struct tcp_sock *tp, const struct tcphdr *th)\n{\n\tif (th->ece && !th->syn && (tp->ecn_flags & TCP_ECN_OK))\n\t\treturn true;\n\treturn false;\n}\n\n/* Buffer size and advertised window tuning.\n *\n * 1. Tuning sk->sk_sndbuf, when connection enters established state.\n */\n\nstatic void tcp_sndbuf_expand(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcp_congestion_ops *ca_ops = inet_csk(sk)->icsk_ca_ops;\n\tint sndmem, per_mss;\n\tu32 nr_segs;\n\n\t/* Worst case is non GSO/TSO : each frame consumes one skb\n\t * and skb->head is kmalloced using power of two area of memory\n\t */\n\tper_mss = max_t(u32, tp->rx_opt.mss_clamp, tp->mss_cache) +\n\t\t  MAX_TCP_HEADER +\n\t\t  SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tper_mss = roundup_pow_of_two(per_mss) +\n\t\t  SKB_DATA_ALIGN(sizeof(struct sk_buff));\n\n\tnr_segs = max_t(u32, TCP_INIT_CWND, tp->snd_cwnd);\n\tnr_segs = max_t(u32, nr_segs, tp->reordering + 1);\n\n\t/* Fast Recovery (RFC 5681 3.2) :\n\t * Cubic needs 1.7 factor, rounded to 2 to include\n\t * extra cushion (application might react slowly to EPOLLOUT)\n\t */\n\tsndmem = ca_ops->sndbuf_expand ? ca_ops->sndbuf_expand(sk) : 2;\n\tsndmem *= nr_segs * per_mss;\n\n\tif (sk->sk_sndbuf < sndmem)\n\t\tWRITE_ONCE(sk->sk_sndbuf,\n\t\t\t   min(sndmem, sock_net(sk)->ipv4.sysctl_tcp_wmem[2]));\n}\n\n/* 2. Tuning advertised window (window_clamp, rcv_ssthresh)\n *\n * All tcp_full_space() is split to two parts: \"network\" buffer, allocated\n * forward and advertised in receiver window (tp->rcv_wnd) and\n * \"application buffer\", required to isolate scheduling/application\n * latencies from network.\n * window_clamp is maximal advertised window. It can be less than\n * tcp_full_space(), in this case tcp_full_space() - window_clamp\n * is reserved for \"application\" buffer. The less window_clamp is\n * the smoother our behaviour from viewpoint of network, but the lower\n * throughput and the higher sensitivity of the connection to losses. 8)\n *\n * rcv_ssthresh is more strict window_clamp used at \"slow start\"\n * phase to predict further behaviour of this connection.\n * It is used for two goals:\n * - to enforce header prediction at sender, even when application\n *   requires some significant \"application buffer\". It is check #1.\n * - to prevent pruning of receive queue because of misprediction\n *   of receiver window. Check #2.\n *\n * The scheme does not work when sender sends good segments opening\n * window and then starts to feed us spaghetti. But it should work\n * in common situations. Otherwise, we have to rely on queue collapsing.\n */\n\n/* Slow part of check#2. */\nstatic int __tcp_grow_window(const struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\t/* Optimize this! */\n\tint truesize = tcp_win_from_space(sk, skb->truesize) >> 1;\n\tint window = tcp_win_from_space(sk, sock_net(sk)->ipv4.sysctl_tcp_rmem[2]) >> 1;\n\n\twhile (tp->rcv_ssthresh <= window) {\n\t\tif (truesize <= skb->len)\n\t\t\treturn 2 * inet_csk(sk)->icsk_ack.rcv_mss;\n\n\t\ttruesize >>= 1;\n\t\twindow >>= 1;\n\t}\n\treturn 0;\n}\n\nstatic void tcp_grow_window(struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint room;\n\n\troom = min_t(int, tp->window_clamp, tcp_space(sk)) - tp->rcv_ssthresh;\n\n\t/* Check #1 */\n\tif (room > 0 && !tcp_under_memory_pressure(sk)) {\n\t\tint incr;\n\n\t\t/* Check #2. Increase window, if skb with such overhead\n\t\t * will fit to rcvbuf in future.\n\t\t */\n\t\tif (tcp_win_from_space(sk, skb->truesize) <= skb->len)\n\t\t\tincr = 2 * tp->advmss;\n\t\telse\n\t\t\tincr = __tcp_grow_window(sk, skb);\n\n\t\tif (incr) {\n\t\t\tincr = max_t(int, incr, 2 * skb->len);\n\t\t\ttp->rcv_ssthresh += min(room, incr);\n\t\t\tinet_csk(sk)->icsk_ack.quick |= 1;\n\t\t}\n\t}\n}\n\n/* 3. Try to fixup all. It is made immediately after connection enters\n *    established state.\n */\nstatic void tcp_init_buffer_space(struct sock *sk)\n{\n\tint tcp_app_win = sock_net(sk)->ipv4.sysctl_tcp_app_win;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint maxwin;\n\n\tif (!(sk->sk_userlocks & SOCK_SNDBUF_LOCK))\n\t\ttcp_sndbuf_expand(sk);\n\n\ttp->rcvq_space.space = min_t(u32, tp->rcv_wnd, TCP_INIT_CWND * tp->advmss);\n\ttcp_mstamp_refresh(tp);\n\ttp->rcvq_space.time = tp->tcp_mstamp;\n\ttp->rcvq_space.seq = tp->copied_seq;\n\n\tmaxwin = tcp_full_space(sk);\n\n\tif (tp->window_clamp >= maxwin) {\n\t\ttp->window_clamp = maxwin;\n\n\t\tif (tcp_app_win && maxwin > 4 * tp->advmss)\n\t\t\ttp->window_clamp = max(maxwin -\n\t\t\t\t\t       (maxwin >> tcp_app_win),\n\t\t\t\t\t       4 * tp->advmss);\n\t}\n\n\t/* Force reservation of one segment. */\n\tif (tcp_app_win &&\n\t    tp->window_clamp > 2 * tp->advmss &&\n\t    tp->window_clamp + tp->advmss > maxwin)\n\t\ttp->window_clamp = max(2 * tp->advmss, maxwin - tp->advmss);\n\n\ttp->rcv_ssthresh = min(tp->rcv_ssthresh, tp->window_clamp);\n\ttp->snd_cwnd_stamp = tcp_jiffies32;\n}\n\n/* 4. Recalculate window clamp after socket hit its memory bounds. */\nstatic void tcp_clamp_window(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct net *net = sock_net(sk);\n\n\ticsk->icsk_ack.quick = 0;\n\n\tif (sk->sk_rcvbuf < net->ipv4.sysctl_tcp_rmem[2] &&\n\t    !(sk->sk_userlocks & SOCK_RCVBUF_LOCK) &&\n\t    !tcp_under_memory_pressure(sk) &&\n\t    sk_memory_allocated(sk) < sk_prot_mem_limits(sk, 0)) {\n\t\tWRITE_ONCE(sk->sk_rcvbuf,\n\t\t\t   min(atomic_read(&sk->sk_rmem_alloc),\n\t\t\t       net->ipv4.sysctl_tcp_rmem[2]));\n\t}\n\tif (atomic_read(&sk->sk_rmem_alloc) > sk->sk_rcvbuf)\n\t\ttp->rcv_ssthresh = min(tp->window_clamp, 2U * tp->advmss);\n}\n\n/* Initialize RCV_MSS value.\n * RCV_MSS is an our guess about MSS used by the peer.\n * We haven't any direct information about the MSS.\n * It's better to underestimate the RCV_MSS rather than overestimate.\n * Overestimations make us ACKing less frequently than needed.\n * Underestimations are more easy to detect and fix by tcp_measure_rcv_mss().\n */\nvoid tcp_initialize_rcv_mss(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int hint = min_t(unsigned int, tp->advmss, tp->mss_cache);\n\n\thint = min(hint, tp->rcv_wnd / 2);\n\thint = min(hint, TCP_MSS_DEFAULT);\n\thint = max(hint, TCP_MIN_MSS);\n\n\tinet_csk(sk)->icsk_ack.rcv_mss = hint;\n}\nEXPORT_SYMBOL(tcp_initialize_rcv_mss);\n\n/* Receiver \"autotuning\" code.\n *\n * The algorithm for RTT estimation w/o timestamps is based on\n * Dynamic Right-Sizing (DRS) by Wu Feng and Mike Fisk of LANL.\n * <https://public.lanl.gov/radiant/pubs.html#DRS>\n *\n * More detail on this code can be found at\n * <http://staff.psc.edu/jheffner/>,\n * though this reference is out of date.  A new paper\n * is pending.\n */\nstatic void tcp_rcv_rtt_update(struct tcp_sock *tp, u32 sample, int win_dep)\n{\n\tu32 new_sample = tp->rcv_rtt_est.rtt_us;\n\tlong m = sample;\n\n\tif (new_sample != 0) {\n\t\t/* If we sample in larger samples in the non-timestamp\n\t\t * case, we could grossly overestimate the RTT especially\n\t\t * with chatty applications or bulk transfer apps which\n\t\t * are stalled on filesystem I/O.\n\t\t *\n\t\t * Also, since we are only going for a minimum in the\n\t\t * non-timestamp case, we do not smooth things out\n\t\t * else with timestamps disabled convergence takes too\n\t\t * long.\n\t\t */\n\t\tif (!win_dep) {\n\t\t\tm -= (new_sample >> 3);\n\t\t\tnew_sample += m;\n\t\t} else {\n\t\t\tm <<= 3;\n\t\t\tif (m < new_sample)\n\t\t\t\tnew_sample = m;\n\t\t}\n\t} else {\n\t\t/* No previous measure. */\n\t\tnew_sample = m << 3;\n\t}\n\n\ttp->rcv_rtt_est.rtt_us = new_sample;\n}\n\nstatic inline void tcp_rcv_rtt_measure(struct tcp_sock *tp)\n{\n\tu32 delta_us;\n\n\tif (tp->rcv_rtt_est.time == 0)\n\t\tgoto new_measure;\n\tif (before(tp->rcv_nxt, tp->rcv_rtt_est.seq))\n\t\treturn;\n\tdelta_us = tcp_stamp_us_delta(tp->tcp_mstamp, tp->rcv_rtt_est.time);\n\tif (!delta_us)\n\t\tdelta_us = 1;\n\ttcp_rcv_rtt_update(tp, delta_us, 1);\n\nnew_measure:\n\ttp->rcv_rtt_est.seq = tp->rcv_nxt + tp->rcv_wnd;\n\ttp->rcv_rtt_est.time = tp->tcp_mstamp;\n}\n\nstatic inline void tcp_rcv_rtt_measure_ts(struct sock *sk,\n\t\t\t\t\t  const struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tp->rx_opt.rcv_tsecr == tp->rcv_rtt_last_tsecr)\n\t\treturn;\n\ttp->rcv_rtt_last_tsecr = tp->rx_opt.rcv_tsecr;\n\n\tif (TCP_SKB_CB(skb)->end_seq -\n\t    TCP_SKB_CB(skb)->seq >= inet_csk(sk)->icsk_ack.rcv_mss) {\n\t\tu32 delta = tcp_time_stamp(tp) - tp->rx_opt.rcv_tsecr;\n\t\tu32 delta_us;\n\n\t\tif (likely(delta < INT_MAX / (USEC_PER_SEC / TCP_TS_HZ))) {\n\t\t\tif (!delta)\n\t\t\t\tdelta = 1;\n\t\t\tdelta_us = delta * (USEC_PER_SEC / TCP_TS_HZ);\n\t\t\ttcp_rcv_rtt_update(tp, delta_us, 0);\n\t\t}\n\t}\n}\n\n/*\n * This function should be called every time data is copied to user space.\n * It calculates the appropriate TCP receive buffer space.\n */\nvoid tcp_rcv_space_adjust(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 copied;\n\tint time;\n\n\ttrace_tcp_rcv_space_adjust(sk);\n\n\ttcp_mstamp_refresh(tp);\n\ttime = tcp_stamp_us_delta(tp->tcp_mstamp, tp->rcvq_space.time);\n\tif (time < (tp->rcv_rtt_est.rtt_us >> 3) || tp->rcv_rtt_est.rtt_us == 0)\n\t\treturn;\n\n\t/* Number of bytes copied to user in last RTT */\n\tcopied = tp->copied_seq - tp->rcvq_space.seq;\n\tif (copied <= tp->rcvq_space.space)\n\t\tgoto new_measure;\n\n\t/* A bit of theory :\n\t * copied = bytes received in previous RTT, our base window\n\t * To cope with packet losses, we need a 2x factor\n\t * To cope with slow start, and sender growing its cwin by 100 %\n\t * every RTT, we need a 4x factor, because the ACK we are sending\n\t * now is for the next RTT, not the current one :\n\t * <prev RTT . ><current RTT .. ><next RTT .... >\n\t */\n\n\tif (sock_net(sk)->ipv4.sysctl_tcp_moderate_rcvbuf &&\n\t    !(sk->sk_userlocks & SOCK_RCVBUF_LOCK)) {\n\t\tint rcvmem, rcvbuf;\n\t\tu64 rcvwin, grow;\n\n\t\t/* minimal window to cope with packet losses, assuming\n\t\t * steady state. Add some cushion because of small variations.\n\t\t */\n\t\trcvwin = ((u64)copied << 1) + 16 * tp->advmss;\n\n\t\t/* Accommodate for sender rate increase (eg. slow start) */\n\t\tgrow = rcvwin * (copied - tp->rcvq_space.space);\n\t\tdo_div(grow, tp->rcvq_space.space);\n\t\trcvwin += (grow << 1);\n\n\t\trcvmem = SKB_TRUESIZE(tp->advmss + MAX_TCP_HEADER);\n\t\twhile (tcp_win_from_space(sk, rcvmem) < tp->advmss)\n\t\t\trcvmem += 128;\n\n\t\tdo_div(rcvwin, tp->advmss);\n\t\trcvbuf = min_t(u64, rcvwin * rcvmem,\n\t\t\t       sock_net(sk)->ipv4.sysctl_tcp_rmem[2]);\n\t\tif (rcvbuf > sk->sk_rcvbuf) {\n\t\t\tWRITE_ONCE(sk->sk_rcvbuf, rcvbuf);\n\n\t\t\t/* Make the window clamp follow along.  */\n\t\t\ttp->window_clamp = tcp_win_from_space(sk, rcvbuf);\n\t\t}\n\t}\n\ttp->rcvq_space.space = copied;\n\nnew_measure:\n\ttp->rcvq_space.seq = tp->copied_seq;\n\ttp->rcvq_space.time = tp->tcp_mstamp;\n}\n\n/* There is something which you must keep in mind when you analyze the\n * behavior of the tp->ato delayed ack timeout interval.  When a\n * connection starts up, we want to ack as quickly as possible.  The\n * problem is that \"good\" TCP's do slow start at the beginning of data\n * transmission.  The means that until we send the first few ACK's the\n * sender will sit on his end and only queue most of his data, because\n * he can only send snd_cwnd unacked packets at any given time.  For\n * each ACK we send, he increments snd_cwnd and transmits more of his\n * queue.  -DaveM\n */\nstatic void tcp_event_data_recv(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tu32 now;\n\n\tinet_csk_schedule_ack(sk);\n\n\ttcp_measure_rcv_mss(sk, skb);\n\n\ttcp_rcv_rtt_measure(tp);\n\n\tnow = tcp_jiffies32;\n\n\tif (!icsk->icsk_ack.ato) {\n\t\t/* The _first_ data packet received, initialize\n\t\t * delayed ACK engine.\n\t\t */\n\t\ttcp_incr_quickack(sk, TCP_MAX_QUICKACKS);\n\t\ticsk->icsk_ack.ato = TCP_ATO_MIN;\n\t} else {\n\t\tint m = now - icsk->icsk_ack.lrcvtime;\n\n\t\tif (m <= TCP_ATO_MIN / 2) {\n\t\t\t/* The fastest case is the first. */\n\t\t\ticsk->icsk_ack.ato = (icsk->icsk_ack.ato >> 1) + TCP_ATO_MIN / 2;\n\t\t} else if (m < icsk->icsk_ack.ato) {\n\t\t\ticsk->icsk_ack.ato = (icsk->icsk_ack.ato >> 1) + m;\n\t\t\tif (icsk->icsk_ack.ato > icsk->icsk_rto)\n\t\t\t\ticsk->icsk_ack.ato = icsk->icsk_rto;\n\t\t} else if (m > icsk->icsk_rto) {\n\t\t\t/* Too long gap. Apparently sender failed to\n\t\t\t * restart window, so that we send ACKs quickly.\n\t\t\t */\n\t\t\ttcp_incr_quickack(sk, TCP_MAX_QUICKACKS);\n\t\t\tsk_mem_reclaim(sk);\n\t\t}\n\t}\n\ticsk->icsk_ack.lrcvtime = now;\n\n\ttcp_ecn_check_ce(sk, skb);\n\n\tif (skb->len >= 128)\n\t\ttcp_grow_window(sk, skb);\n}\n\n/* Called to compute a smoothed rtt estimate. The data fed to this\n * routine either comes from timestamps, or from segments that were\n * known _not_ to have been retransmitted [see Karn/Partridge\n * Proceedings SIGCOMM 87]. The algorithm is from the SIGCOMM 88\n * piece by Van Jacobson.\n * NOTE: the next three routines used to be one big routine.\n * To save cycles in the RFC 1323 implementation it was better to break\n * it up into three procedures. -- erics\n */\nstatic void tcp_rtt_estimator(struct sock *sk, long mrtt_us)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tlong m = mrtt_us; /* RTT */\n\tu32 srtt = tp->srtt_us;\n\n\t/*\tThe following amusing code comes from Jacobson's\n\t *\tarticle in SIGCOMM '88.  Note that rtt and mdev\n\t *\tare scaled versions of rtt and mean deviation.\n\t *\tThis is designed to be as fast as possible\n\t *\tm stands for \"measurement\".\n\t *\n\t *\tOn a 1990 paper the rto value is changed to:\n\t *\tRTO = rtt + 4 * mdev\n\t *\n\t * Funny. This algorithm seems to be very broken.\n\t * These formulae increase RTO, when it should be decreased, increase\n\t * too slowly, when it should be increased quickly, decrease too quickly\n\t * etc. I guess in BSD RTO takes ONE value, so that it is absolutely\n\t * does not matter how to _calculate_ it. Seems, it was trap\n\t * that VJ failed to avoid. 8)\n\t */\n\tif (srtt != 0) {\n\t\tm -= (srtt >> 3);\t/* m is now error in rtt est */\n\t\tsrtt += m;\t\t/* rtt = 7/8 rtt + 1/8 new */\n\t\tif (m < 0) {\n\t\t\tm = -m;\t\t/* m is now abs(error) */\n\t\t\tm -= (tp->mdev_us >> 2);   /* similar update on mdev */\n\t\t\t/* This is similar to one of Eifel findings.\n\t\t\t * Eifel blocks mdev updates when rtt decreases.\n\t\t\t * This solution is a bit different: we use finer gain\n\t\t\t * for mdev in this case (alpha*beta).\n\t\t\t * Like Eifel it also prevents growth of rto,\n\t\t\t * but also it limits too fast rto decreases,\n\t\t\t * happening in pure Eifel.\n\t\t\t */\n\t\t\tif (m > 0)\n\t\t\t\tm >>= 3;\n\t\t} else {\n\t\t\tm -= (tp->mdev_us >> 2);   /* similar update on mdev */\n\t\t}\n\t\ttp->mdev_us += m;\t\t/* mdev = 3/4 mdev + 1/4 new */\n\t\tif (tp->mdev_us > tp->mdev_max_us) {\n\t\t\ttp->mdev_max_us = tp->mdev_us;\n\t\t\tif (tp->mdev_max_us > tp->rttvar_us)\n\t\t\t\ttp->rttvar_us = tp->mdev_max_us;\n\t\t}\n\t\tif (after(tp->snd_una, tp->rtt_seq)) {\n\t\t\tif (tp->mdev_max_us < tp->rttvar_us)\n\t\t\t\ttp->rttvar_us -= (tp->rttvar_us - tp->mdev_max_us) >> 2;\n\t\t\ttp->rtt_seq = tp->snd_nxt;\n\t\t\ttp->mdev_max_us = tcp_rto_min_us(sk);\n\n\t\t\ttcp_bpf_rtt(sk);\n\t\t}\n\t} else {\n\t\t/* no previous measure. */\n\t\tsrtt = m << 3;\t\t/* take the measured time to be rtt */\n\t\ttp->mdev_us = m << 1;\t/* make sure rto = 3*rtt */\n\t\ttp->rttvar_us = max(tp->mdev_us, tcp_rto_min_us(sk));\n\t\ttp->mdev_max_us = tp->rttvar_us;\n\t\ttp->rtt_seq = tp->snd_nxt;\n\n\t\ttcp_bpf_rtt(sk);\n\t}\n\ttp->srtt_us = max(1U, srtt);\n}\n\nstatic void tcp_update_pacing_rate(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tu64 rate;\n\n\t/* set sk_pacing_rate to 200 % of current rate (mss * cwnd / srtt) */\n\trate = (u64)tp->mss_cache * ((USEC_PER_SEC / 100) << 3);\n\n\t/* current rate is (cwnd * mss) / srtt\n\t * In Slow Start [1], set sk_pacing_rate to 200 % the current rate.\n\t * In Congestion Avoidance phase, set it to 120 % the current rate.\n\t *\n\t * [1] : Normal Slow Start condition is (tp->snd_cwnd < tp->snd_ssthresh)\n\t *\t If snd_cwnd >= (tp->snd_ssthresh / 2), we are approaching\n\t *\t end of slow start and should slow down.\n\t */\n\tif (tp->snd_cwnd < tp->snd_ssthresh / 2)\n\t\trate *= sock_net(sk)->ipv4.sysctl_tcp_pacing_ss_ratio;\n\telse\n\t\trate *= sock_net(sk)->ipv4.sysctl_tcp_pacing_ca_ratio;\n\n\trate *= max(tp->snd_cwnd, tp->packets_out);\n\n\tif (likely(tp->srtt_us))\n\t\tdo_div(rate, tp->srtt_us);\n\n\t/* WRITE_ONCE() is needed because sch_fq fetches sk_pacing_rate\n\t * without any lock. We want to make sure compiler wont store\n\t * intermediate values in this location.\n\t */\n\tWRITE_ONCE(sk->sk_pacing_rate, min_t(u64, rate,\n\t\t\t\t\t     sk->sk_max_pacing_rate));\n}\n\n/* Calculate rto without backoff.  This is the second half of Van Jacobson's\n * routine referred to above.\n */\nstatic void tcp_set_rto(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\t/* Old crap is replaced with new one. 8)\n\t *\n\t * More seriously:\n\t * 1. If rtt variance happened to be less 50msec, it is hallucination.\n\t *    It cannot be less due to utterly erratic ACK generation made\n\t *    at least by solaris and freebsd. \"Erratic ACKs\" has _nothing_\n\t *    to do with delayed acks, because at cwnd>2 true delack timeout\n\t *    is invisible. Actually, Linux-2.4 also generates erratic\n\t *    ACKs in some circumstances.\n\t */\n\tinet_csk(sk)->icsk_rto = __tcp_set_rto(tp);\n\n\t/* 2. Fixups made earlier cannot be right.\n\t *    If we do not estimate RTO correctly without them,\n\t *    all the algo is pure shit and should be replaced\n\t *    with correct one. It is exactly, which we pretend to do.\n\t */\n\n\t/* NOTE: clamping at TCP_RTO_MIN is not required, current algo\n\t * guarantees that rto is higher.\n\t */\n\ttcp_bound_rto(sk);\n}\n\n__u32 tcp_init_cwnd(const struct tcp_sock *tp, const struct dst_entry *dst)\n{\n\t__u32 cwnd = (dst ? dst_metric(dst, RTAX_INITCWND) : 0);\n\n\tif (!cwnd)\n\t\tcwnd = TCP_INIT_CWND;\n\treturn min_t(__u32, cwnd, tp->snd_cwnd_clamp);\n}\n\nstruct tcp_sacktag_state {\n\t/* Timestamps for earliest and latest never-retransmitted segment\n\t * that was SACKed. RTO needs the earliest RTT to stay conservative,\n\t * but congestion control should still get an accurate delay signal.\n\t */\n\tu64\tfirst_sackt;\n\tu64\tlast_sackt;\n\tu32\treord;\n\tu32\tsack_delivered;\n\tint\tflag;\n\tunsigned int mss_now;\n\tstruct rate_sample *rate;\n};\n\n/* Take a notice that peer is sending D-SACKs. Skip update of data delivery\n * and spurious retransmission information if this DSACK is unlikely caused by\n * sender's action:\n * - DSACKed sequence range is larger than maximum receiver's window.\n * - Total no. of DSACKed segments exceed the total no. of retransmitted segs.\n */\nstatic u32 tcp_dsack_seen(struct tcp_sock *tp, u32 start_seq,\n\t\t\t  u32 end_seq, struct tcp_sacktag_state *state)\n{\n\tu32 seq_len, dup_segs = 1;\n\n\tif (!before(start_seq, end_seq))\n\t\treturn 0;\n\n\tseq_len = end_seq - start_seq;\n\t/* Dubious DSACK: DSACKed range greater than maximum advertised rwnd */\n\tif (seq_len > tp->max_window)\n\t\treturn 0;\n\tif (seq_len > tp->mss_cache)\n\t\tdup_segs = DIV_ROUND_UP(seq_len, tp->mss_cache);\n\n\ttp->dsack_dups += dup_segs;\n\t/* Skip the DSACK if dup segs weren't retransmitted by sender */\n\tif (tp->dsack_dups > tp->total_retrans)\n\t\treturn 0;\n\n\ttp->rx_opt.sack_ok |= TCP_DSACK_SEEN;\n\ttp->rack.dsack_seen = 1;\n\n\tstate->flag |= FLAG_DSACKING_ACK;\n\t/* A spurious retransmission is delivered */\n\tstate->sack_delivered += dup_segs;\n\n\treturn dup_segs;\n}\n\n/* It's reordering when higher sequence was delivered (i.e. sacked) before\n * some lower never-retransmitted sequence (\"low_seq\"). The maximum reordering\n * distance is approximated in full-mss packet distance (\"reordering\").\n */\nstatic void tcp_check_sack_reordering(struct sock *sk, const u32 low_seq,\n\t\t\t\t      const int ts)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst u32 mss = tp->mss_cache;\n\tu32 fack, metric;\n\n\tfack = tcp_highest_sack_seq(tp);\n\tif (!before(low_seq, fack))\n\t\treturn;\n\n\tmetric = fack - low_seq;\n\tif ((metric > tp->reordering * mss) && mss) {\n#if FASTRETRANS_DEBUG > 1\n\t\tpr_debug(\"Disorder%d %d %u f%u s%u rr%d\\n\",\n\t\t\t tp->rx_opt.sack_ok, inet_csk(sk)->icsk_ca_state,\n\t\t\t tp->reordering,\n\t\t\t 0,\n\t\t\t tp->sacked_out,\n\t\t\t tp->undo_marker ? tp->undo_retrans : 0);\n#endif\n\t\ttp->reordering = min_t(u32, (metric + mss - 1) / mss,\n\t\t\t\t       sock_net(sk)->ipv4.sysctl_tcp_max_reordering);\n\t}\n\n\t/* This exciting event is worth to be remembered. 8) */\n\ttp->reord_seen++;\n\tNET_INC_STATS(sock_net(sk),\n\t\t      ts ? LINUX_MIB_TCPTSREORDER : LINUX_MIB_TCPSACKREORDER);\n}\n\n /* This must be called before lost_out or retrans_out are updated\n  * on a new loss, because we want to know if all skbs previously\n  * known to be lost have already been retransmitted, indicating\n  * that this newly lost skb is our next skb to retransmit.\n  */\nstatic void tcp_verify_retransmit_hint(struct tcp_sock *tp, struct sk_buff *skb)\n{\n\tif ((!tp->retransmit_skb_hint && tp->retrans_out >= tp->lost_out) ||\n\t    (tp->retransmit_skb_hint &&\n\t     before(TCP_SKB_CB(skb)->seq,\n\t\t    TCP_SKB_CB(tp->retransmit_skb_hint)->seq)))\n\t\ttp->retransmit_skb_hint = skb;\n}\n\n/* Sum the number of packets on the wire we have marked as lost, and\n * notify the congestion control module that the given skb was marked lost.\n */\nstatic void tcp_notify_skb_loss_event(struct tcp_sock *tp, const struct sk_buff *skb)\n{\n\ttp->lost += tcp_skb_pcount(skb);\n}\n\nvoid tcp_mark_skb_lost(struct sock *sk, struct sk_buff *skb)\n{\n\t__u8 sacked = TCP_SKB_CB(skb)->sacked;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (sacked & TCPCB_SACKED_ACKED)\n\t\treturn;\n\n\ttcp_verify_retransmit_hint(tp, skb);\n\tif (sacked & TCPCB_LOST) {\n\t\tif (sacked & TCPCB_SACKED_RETRANS) {\n\t\t\t/* Account for retransmits that are lost again */\n\t\t\tTCP_SKB_CB(skb)->sacked &= ~TCPCB_SACKED_RETRANS;\n\t\t\ttp->retrans_out -= tcp_skb_pcount(skb);\n\t\t\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPLOSTRETRANSMIT,\n\t\t\t\t      tcp_skb_pcount(skb));\n\t\t\ttcp_notify_skb_loss_event(tp, skb);\n\t\t}\n\t} else {\n\t\ttp->lost_out += tcp_skb_pcount(skb);\n\t\tTCP_SKB_CB(skb)->sacked |= TCPCB_LOST;\n\t\ttcp_notify_skb_loss_event(tp, skb);\n\t}\n}\n\n/* Updates the delivered and delivered_ce counts */\nstatic void tcp_count_delivered(struct tcp_sock *tp, u32 delivered,\n\t\t\t\tbool ece_ack)\n{\n\ttp->delivered += delivered;\n\tif (ece_ack)\n\t\ttp->delivered_ce += delivered;\n}\n\n/* This procedure tags the retransmission queue when SACKs arrive.\n *\n * We have three tag bits: SACKED(S), RETRANS(R) and LOST(L).\n * Packets in queue with these bits set are counted in variables\n * sacked_out, retrans_out and lost_out, correspondingly.\n *\n * Valid combinations are:\n * Tag  InFlight\tDescription\n * 0\t1\t\t- orig segment is in flight.\n * S\t0\t\t- nothing flies, orig reached receiver.\n * L\t0\t\t- nothing flies, orig lost by net.\n * R\t2\t\t- both orig and retransmit are in flight.\n * L|R\t1\t\t- orig is lost, retransmit is in flight.\n * S|R  1\t\t- orig reached receiver, retrans is still in flight.\n * (L|S|R is logically valid, it could occur when L|R is sacked,\n *  but it is equivalent to plain S and code short-curcuits it to S.\n *  L|S is logically invalid, it would mean -1 packet in flight 8))\n *\n * These 6 states form finite state machine, controlled by the following events:\n * 1. New ACK (+SACK) arrives. (tcp_sacktag_write_queue())\n * 2. Retransmission. (tcp_retransmit_skb(), tcp_xmit_retransmit_queue())\n * 3. Loss detection event of two flavors:\n *\tA. Scoreboard estimator decided the packet is lost.\n *\t   A'. Reno \"three dupacks\" marks head of queue lost.\n *\tB. SACK arrives sacking SND.NXT at the moment, when the\n *\t   segment was retransmitted.\n * 4. D-SACK added new rule: D-SACK changes any tag to S.\n *\n * It is pleasant to note, that state diagram turns out to be commutative,\n * so that we are allowed not to be bothered by order of our actions,\n * when multiple events arrive simultaneously. (see the function below).\n *\n * Reordering detection.\n * --------------------\n * Reordering metric is maximal distance, which a packet can be displaced\n * in packet stream. With SACKs we can estimate it:\n *\n * 1. SACK fills old hole and the corresponding segment was not\n *    ever retransmitted -> reordering. Alas, we cannot use it\n *    when segment was retransmitted.\n * 2. The last flaw is solved with D-SACK. D-SACK arrives\n *    for retransmitted and already SACKed segment -> reordering..\n * Both of these heuristics are not used in Loss state, when we cannot\n * account for retransmits accurately.\n *\n * SACK block validation.\n * ----------------------\n *\n * SACK block range validation checks that the received SACK block fits to\n * the expected sequence limits, i.e., it is between SND.UNA and SND.NXT.\n * Note that SND.UNA is not included to the range though being valid because\n * it means that the receiver is rather inconsistent with itself reporting\n * SACK reneging when it should advance SND.UNA. Such SACK block this is\n * perfectly valid, however, in light of RFC2018 which explicitly states\n * that \"SACK block MUST reflect the newest segment.  Even if the newest\n * segment is going to be discarded ...\", not that it looks very clever\n * in case of head skb. Due to potentional receiver driven attacks, we\n * choose to avoid immediate execution of a walk in write queue due to\n * reneging and defer head skb's loss recovery to standard loss recovery\n * procedure that will eventually trigger (nothing forbids us doing this).\n *\n * Implements also blockage to start_seq wrap-around. Problem lies in the\n * fact that though start_seq (s) is before end_seq (i.e., not reversed),\n * there's no guarantee that it will be before snd_nxt (n). The problem\n * happens when start_seq resides between end_seq wrap (e_w) and snd_nxt\n * wrap (s_w):\n *\n *         <- outs wnd ->                          <- wrapzone ->\n *         u     e      n                         u_w   e_w  s n_w\n *         |     |      |                          |     |   |  |\n * |<------------+------+----- TCP seqno space --------------+---------->|\n * ...-- <2^31 ->|                                           |<--------...\n * ...---- >2^31 ------>|                                    |<--------...\n *\n * Current code wouldn't be vulnerable but it's better still to discard such\n * crazy SACK blocks. Doing this check for start_seq alone closes somewhat\n * similar case (end_seq after snd_nxt wrap) as earlier reversed check in\n * snd_nxt wrap -> snd_una region will then become \"well defined\", i.e.,\n * equal to the ideal case (infinite seqno space without wrap caused issues).\n *\n * With D-SACK the lower bound is extended to cover sequence space below\n * SND.UNA down to undo_marker, which is the last point of interest. Yet\n * again, D-SACK block must not to go across snd_una (for the same reason as\n * for the normal SACK blocks, explained above). But there all simplicity\n * ends, TCP might receive valid D-SACKs below that. As long as they reside\n * fully below undo_marker they do not affect behavior in anyway and can\n * therefore be safely ignored. In rare cases (which are more or less\n * theoretical ones), the D-SACK will nicely cross that boundary due to skb\n * fragmentation and packet reordering past skb's retransmission. To consider\n * them correctly, the acceptable range must be extended even more though\n * the exact amount is rather hard to quantify. However, tp->max_window can\n * be used as an exaggerated estimate.\n */\nstatic bool tcp_is_sackblock_valid(struct tcp_sock *tp, bool is_dsack,\n\t\t\t\t   u32 start_seq, u32 end_seq)\n{\n\t/* Too far in future, or reversed (interpretation is ambiguous) */\n\tif (after(end_seq, tp->snd_nxt) || !before(start_seq, end_seq))\n\t\treturn false;\n\n\t/* Nasty start_seq wrap-around check (see comments above) */\n\tif (!before(start_seq, tp->snd_nxt))\n\t\treturn false;\n\n\t/* In outstanding window? ...This is valid exit for D-SACKs too.\n\t * start_seq == snd_una is non-sensical (see comments above)\n\t */\n\tif (after(start_seq, tp->snd_una))\n\t\treturn true;\n\n\tif (!is_dsack || !tp->undo_marker)\n\t\treturn false;\n\n\t/* ...Then it's D-SACK, and must reside below snd_una completely */\n\tif (after(end_seq, tp->snd_una))\n\t\treturn false;\n\n\tif (!before(start_seq, tp->undo_marker))\n\t\treturn true;\n\n\t/* Too old */\n\tif (!after(end_seq, tp->undo_marker))\n\t\treturn false;\n\n\t/* Undo_marker boundary crossing (overestimates a lot). Known already:\n\t *   start_seq < undo_marker and end_seq >= undo_marker.\n\t */\n\treturn !before(start_seq, end_seq - tp->max_window);\n}\n\nstatic bool tcp_check_dsack(struct sock *sk, const struct sk_buff *ack_skb,\n\t\t\t    struct tcp_sack_block_wire *sp, int num_sacks,\n\t\t\t    u32 prior_snd_una, struct tcp_sacktag_state *state)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 start_seq_0 = get_unaligned_be32(&sp[0].start_seq);\n\tu32 end_seq_0 = get_unaligned_be32(&sp[0].end_seq);\n\tu32 dup_segs;\n\n\tif (before(start_seq_0, TCP_SKB_CB(ack_skb)->ack_seq)) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDSACKRECV);\n\t} else if (num_sacks > 1) {\n\t\tu32 end_seq_1 = get_unaligned_be32(&sp[1].end_seq);\n\t\tu32 start_seq_1 = get_unaligned_be32(&sp[1].start_seq);\n\n\t\tif (after(end_seq_0, end_seq_1) || before(start_seq_0, start_seq_1))\n\t\t\treturn false;\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDSACKOFORECV);\n\t} else {\n\t\treturn false;\n\t}\n\n\tdup_segs = tcp_dsack_seen(tp, start_seq_0, end_seq_0, state);\n\tif (!dup_segs) {\t/* Skip dubious DSACK */\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDSACKIGNOREDDUBIOUS);\n\t\treturn false;\n\t}\n\n\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPDSACKRECVSEGS, dup_segs);\n\n\t/* D-SACK for already forgotten data... Do dumb counting. */\n\tif (tp->undo_marker && tp->undo_retrans > 0 &&\n\t    !after(end_seq_0, prior_snd_una) &&\n\t    after(end_seq_0, tp->undo_marker))\n\t\ttp->undo_retrans = max_t(int, 0, tp->undo_retrans - dup_segs);\n\n\treturn true;\n}\n\n/* Check if skb is fully within the SACK block. In presence of GSO skbs,\n * the incoming SACK may not exactly match but we can find smaller MSS\n * aligned portion of it that matches. Therefore we might need to fragment\n * which may fail and creates some hassle (caller must handle error case\n * returns).\n *\n * FIXME: this could be merged to shift decision code\n */\nstatic int tcp_match_skb_to_sack(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  u32 start_seq, u32 end_seq)\n{\n\tint err;\n\tbool in_sack;\n\tunsigned int pkt_len;\n\tunsigned int mss;\n\n\tin_sack = !after(start_seq, TCP_SKB_CB(skb)->seq) &&\n\t\t  !before(end_seq, TCP_SKB_CB(skb)->end_seq);\n\n\tif (tcp_skb_pcount(skb) > 1 && !in_sack &&\n\t    after(TCP_SKB_CB(skb)->end_seq, start_seq)) {\n\t\tmss = tcp_skb_mss(skb);\n\t\tin_sack = !after(start_seq, TCP_SKB_CB(skb)->seq);\n\n\t\tif (!in_sack) {\n\t\t\tpkt_len = start_seq - TCP_SKB_CB(skb)->seq;\n\t\t\tif (pkt_len < mss)\n\t\t\t\tpkt_len = mss;\n\t\t} else {\n\t\t\tpkt_len = end_seq - TCP_SKB_CB(skb)->seq;\n\t\t\tif (pkt_len < mss)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* Round if necessary so that SACKs cover only full MSSes\n\t\t * and/or the remaining small portion (if present)\n\t\t */\n\t\tif (pkt_len > mss) {\n\t\t\tunsigned int new_len = (pkt_len / mss) * mss;\n\t\t\tif (!in_sack && new_len < pkt_len)\n\t\t\t\tnew_len += mss;\n\t\t\tpkt_len = new_len;\n\t\t}\n\n\t\tif (pkt_len >= skb->len && !in_sack)\n\t\t\treturn 0;\n\n\t\terr = tcp_fragment(sk, TCP_FRAG_IN_RTX_QUEUE, skb,\n\t\t\t\t   pkt_len, mss, GFP_ATOMIC);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\treturn in_sack;\n}\n\n/* Mark the given newly-SACKed range as such, adjusting counters and hints. */\nstatic u8 tcp_sacktag_one(struct sock *sk,\n\t\t\t  struct tcp_sacktag_state *state, u8 sacked,\n\t\t\t  u32 start_seq, u32 end_seq,\n\t\t\t  int dup_sack, int pcount,\n\t\t\t  u64 xmit_time)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Account D-SACK for retransmitted packet. */\n\tif (dup_sack && (sacked & TCPCB_RETRANS)) {\n\t\tif (tp->undo_marker && tp->undo_retrans > 0 &&\n\t\t    after(end_seq, tp->undo_marker))\n\t\t\ttp->undo_retrans--;\n\t\tif ((sacked & TCPCB_SACKED_ACKED) &&\n\t\t    before(start_seq, state->reord))\n\t\t\t\tstate->reord = start_seq;\n\t}\n\n\t/* Nothing to do; acked frame is about to be dropped (was ACKed). */\n\tif (!after(end_seq, tp->snd_una))\n\t\treturn sacked;\n\n\tif (!(sacked & TCPCB_SACKED_ACKED)) {\n\t\ttcp_rack_advance(tp, sacked, end_seq, xmit_time);\n\n\t\tif (sacked & TCPCB_SACKED_RETRANS) {\n\t\t\t/* If the segment is not tagged as lost,\n\t\t\t * we do not clear RETRANS, believing\n\t\t\t * that retransmission is still in flight.\n\t\t\t */\n\t\t\tif (sacked & TCPCB_LOST) {\n\t\t\t\tsacked &= ~(TCPCB_LOST|TCPCB_SACKED_RETRANS);\n\t\t\t\ttp->lost_out -= pcount;\n\t\t\t\ttp->retrans_out -= pcount;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!(sacked & TCPCB_RETRANS)) {\n\t\t\t\t/* New sack for not retransmitted frame,\n\t\t\t\t * which was in hole. It is reordering.\n\t\t\t\t */\n\t\t\t\tif (before(start_seq,\n\t\t\t\t\t   tcp_highest_sack_seq(tp)) &&\n\t\t\t\t    before(start_seq, state->reord))\n\t\t\t\t\tstate->reord = start_seq;\n\n\t\t\t\tif (!after(end_seq, tp->high_seq))\n\t\t\t\t\tstate->flag |= FLAG_ORIG_SACK_ACKED;\n\t\t\t\tif (state->first_sackt == 0)\n\t\t\t\t\tstate->first_sackt = xmit_time;\n\t\t\t\tstate->last_sackt = xmit_time;\n\t\t\t}\n\n\t\t\tif (sacked & TCPCB_LOST) {\n\t\t\t\tsacked &= ~TCPCB_LOST;\n\t\t\t\ttp->lost_out -= pcount;\n\t\t\t}\n\t\t}\n\n\t\tsacked |= TCPCB_SACKED_ACKED;\n\t\tstate->flag |= FLAG_DATA_SACKED;\n\t\ttp->sacked_out += pcount;\n\t\t/* Out-of-order packets delivered */\n\t\tstate->sack_delivered += pcount;\n\n\t\t/* Lost marker hint past SACKed? Tweak RFC3517 cnt */\n\t\tif (tp->lost_skb_hint &&\n\t\t    before(start_seq, TCP_SKB_CB(tp->lost_skb_hint)->seq))\n\t\t\ttp->lost_cnt_hint += pcount;\n\t}\n\n\t/* D-SACK. We can detect redundant retransmission in S|R and plain R\n\t * frames and clear it. undo_retrans is decreased above, L|R frames\n\t * are accounted above as well.\n\t */\n\tif (dup_sack && (sacked & TCPCB_SACKED_RETRANS)) {\n\t\tsacked &= ~TCPCB_SACKED_RETRANS;\n\t\ttp->retrans_out -= pcount;\n\t}\n\n\treturn sacked;\n}\n\n/* Shift newly-SACKed bytes from this skb to the immediately previous\n * already-SACKed sk_buff. Mark the newly-SACKed bytes as such.\n */\nstatic bool tcp_shifted_skb(struct sock *sk, struct sk_buff *prev,\n\t\t\t    struct sk_buff *skb,\n\t\t\t    struct tcp_sacktag_state *state,\n\t\t\t    unsigned int pcount, int shifted, int mss,\n\t\t\t    bool dup_sack)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 start_seq = TCP_SKB_CB(skb)->seq;\t/* start of newly-SACKed */\n\tu32 end_seq = start_seq + shifted;\t/* end of newly-SACKed */\n\n\tBUG_ON(!pcount);\n\n\t/* Adjust counters and hints for the newly sacked sequence\n\t * range but discard the return value since prev is already\n\t * marked. We must tag the range first because the seq\n\t * advancement below implicitly advances\n\t * tcp_highest_sack_seq() when skb is highest_sack.\n\t */\n\ttcp_sacktag_one(sk, state, TCP_SKB_CB(skb)->sacked,\n\t\t\tstart_seq, end_seq, dup_sack, pcount,\n\t\t\ttcp_skb_timestamp_us(skb));\n\ttcp_rate_skb_delivered(sk, skb, state->rate);\n\n\tif (skb == tp->lost_skb_hint)\n\t\ttp->lost_cnt_hint += pcount;\n\n\tTCP_SKB_CB(prev)->end_seq += shifted;\n\tTCP_SKB_CB(skb)->seq += shifted;\n\n\ttcp_skb_pcount_add(prev, pcount);\n\tWARN_ON_ONCE(tcp_skb_pcount(skb) < pcount);\n\ttcp_skb_pcount_add(skb, -pcount);\n\n\t/* When we're adding to gso_segs == 1, gso_size will be zero,\n\t * in theory this shouldn't be necessary but as long as DSACK\n\t * code can come after this skb later on it's better to keep\n\t * setting gso_size to something.\n\t */\n\tif (!TCP_SKB_CB(prev)->tcp_gso_size)\n\t\tTCP_SKB_CB(prev)->tcp_gso_size = mss;\n\n\t/* CHECKME: To clear or not to clear? Mimics normal skb currently */\n\tif (tcp_skb_pcount(skb) <= 1)\n\t\tTCP_SKB_CB(skb)->tcp_gso_size = 0;\n\n\t/* Difference in this won't matter, both ACKed by the same cumul. ACK */\n\tTCP_SKB_CB(prev)->sacked |= (TCP_SKB_CB(skb)->sacked & TCPCB_EVER_RETRANS);\n\n\tif (skb->len > 0) {\n\t\tBUG_ON(!tcp_skb_pcount(skb));\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_SACKSHIFTED);\n\t\treturn false;\n\t}\n\n\t/* Whole SKB was eaten :-) */\n\n\tif (skb == tp->retransmit_skb_hint)\n\t\ttp->retransmit_skb_hint = prev;\n\tif (skb == tp->lost_skb_hint) {\n\t\ttp->lost_skb_hint = prev;\n\t\ttp->lost_cnt_hint -= tcp_skb_pcount(prev);\n\t}\n\n\tTCP_SKB_CB(prev)->tcp_flags |= TCP_SKB_CB(skb)->tcp_flags;\n\tTCP_SKB_CB(prev)->eor = TCP_SKB_CB(skb)->eor;\n\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)\n\t\tTCP_SKB_CB(prev)->end_seq++;\n\n\tif (skb == tcp_highest_sack(sk))\n\t\ttcp_advance_highest_sack(sk, skb);\n\n\ttcp_skb_collapse_tstamp(prev, skb);\n\tif (unlikely(TCP_SKB_CB(prev)->tx.delivered_mstamp))\n\t\tTCP_SKB_CB(prev)->tx.delivered_mstamp = 0;\n\n\ttcp_rtx_queue_unlink_and_free(skb, sk);\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_SACKMERGED);\n\n\treturn true;\n}\n\n/* I wish gso_size would have a bit more sane initialization than\n * something-or-zero which complicates things\n */\nstatic int tcp_skb_seglen(const struct sk_buff *skb)\n{\n\treturn tcp_skb_pcount(skb) == 1 ? skb->len : tcp_skb_mss(skb);\n}\n\n/* Shifting pages past head area doesn't work */\nstatic int skb_can_shift(const struct sk_buff *skb)\n{\n\treturn !skb_headlen(skb) && skb_is_nonlinear(skb);\n}\n\nint tcp_skb_shift(struct sk_buff *to, struct sk_buff *from,\n\t\t  int pcount, int shiftlen)\n{\n\t/* TCP min gso_size is 8 bytes (TCP_MIN_GSO_SIZE)\n\t * Since TCP_SKB_CB(skb)->tcp_gso_segs is 16 bits, we need\n\t * to make sure not storing more than 65535 * 8 bytes per skb,\n\t * even if current MSS is bigger.\n\t */\n\tif (unlikely(to->len + shiftlen >= 65535 * TCP_MIN_GSO_SIZE))\n\t\treturn 0;\n\tif (unlikely(tcp_skb_pcount(to) + pcount > 65535))\n\t\treturn 0;\n\treturn skb_shift(to, from, shiftlen);\n}\n\n/* Try collapsing SACK blocks spanning across multiple skbs to a single\n * skb.\n */\nstatic struct sk_buff *tcp_shift_skb_data(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t  struct tcp_sacktag_state *state,\n\t\t\t\t\t  u32 start_seq, u32 end_seq,\n\t\t\t\t\t  bool dup_sack)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *prev;\n\tint mss;\n\tint pcount = 0;\n\tint len;\n\tint in_sack;\n\n\t/* Normally R but no L won't result in plain S */\n\tif (!dup_sack &&\n\t    (TCP_SKB_CB(skb)->sacked & (TCPCB_LOST|TCPCB_SACKED_RETRANS)) == TCPCB_SACKED_RETRANS)\n\t\tgoto fallback;\n\tif (!skb_can_shift(skb))\n\t\tgoto fallback;\n\t/* This frame is about to be dropped (was ACKed). */\n\tif (!after(TCP_SKB_CB(skb)->end_seq, tp->snd_una))\n\t\tgoto fallback;\n\n\t/* Can only happen with delayed DSACK + discard craziness */\n\tprev = skb_rb_prev(skb);\n\tif (!prev)\n\t\tgoto fallback;\n\n\tif ((TCP_SKB_CB(prev)->sacked & TCPCB_TAGBITS) != TCPCB_SACKED_ACKED)\n\t\tgoto fallback;\n\n\tif (!tcp_skb_can_collapse(prev, skb))\n\t\tgoto fallback;\n\n\tin_sack = !after(start_seq, TCP_SKB_CB(skb)->seq) &&\n\t\t  !before(end_seq, TCP_SKB_CB(skb)->end_seq);\n\n\tif (in_sack) {\n\t\tlen = skb->len;\n\t\tpcount = tcp_skb_pcount(skb);\n\t\tmss = tcp_skb_seglen(skb);\n\n\t\t/* TODO: Fix DSACKs to not fragment already SACKed and we can\n\t\t * drop this restriction as unnecessary\n\t\t */\n\t\tif (mss != tcp_skb_seglen(prev))\n\t\t\tgoto fallback;\n\t} else {\n\t\tif (!after(TCP_SKB_CB(skb)->end_seq, start_seq))\n\t\t\tgoto noop;\n\t\t/* CHECKME: This is non-MSS split case only?, this will\n\t\t * cause skipped skbs due to advancing loop btw, original\n\t\t * has that feature too\n\t\t */\n\t\tif (tcp_skb_pcount(skb) <= 1)\n\t\t\tgoto noop;\n\n\t\tin_sack = !after(start_seq, TCP_SKB_CB(skb)->seq);\n\t\tif (!in_sack) {\n\t\t\t/* TODO: head merge to next could be attempted here\n\t\t\t * if (!after(TCP_SKB_CB(skb)->end_seq, end_seq)),\n\t\t\t * though it might not be worth of the additional hassle\n\t\t\t *\n\t\t\t * ...we can probably just fallback to what was done\n\t\t\t * previously. We could try merging non-SACKed ones\n\t\t\t * as well but it probably isn't going to buy off\n\t\t\t * because later SACKs might again split them, and\n\t\t\t * it would make skb timestamp tracking considerably\n\t\t\t * harder problem.\n\t\t\t */\n\t\t\tgoto fallback;\n\t\t}\n\n\t\tlen = end_seq - TCP_SKB_CB(skb)->seq;\n\t\tBUG_ON(len < 0);\n\t\tBUG_ON(len > skb->len);\n\n\t\t/* MSS boundaries should be honoured or else pcount will\n\t\t * severely break even though it makes things bit trickier.\n\t\t * Optimize common case to avoid most of the divides\n\t\t */\n\t\tmss = tcp_skb_mss(skb);\n\n\t\t/* TODO: Fix DSACKs to not fragment already SACKed and we can\n\t\t * drop this restriction as unnecessary\n\t\t */\n\t\tif (mss != tcp_skb_seglen(prev))\n\t\t\tgoto fallback;\n\n\t\tif (len == mss) {\n\t\t\tpcount = 1;\n\t\t} else if (len < mss) {\n\t\t\tgoto noop;\n\t\t} else {\n\t\t\tpcount = len / mss;\n\t\t\tlen = pcount * mss;\n\t\t}\n\t}\n\n\t/* tcp_sacktag_one() won't SACK-tag ranges below snd_una */\n\tif (!after(TCP_SKB_CB(skb)->seq + len, tp->snd_una))\n\t\tgoto fallback;\n\n\tif (!tcp_skb_shift(prev, skb, pcount, len))\n\t\tgoto fallback;\n\tif (!tcp_shifted_skb(sk, prev, skb, state, pcount, len, mss, dup_sack))\n\t\tgoto out;\n\n\t/* Hole filled allows collapsing with the next as well, this is very\n\t * useful when hole on every nth skb pattern happens\n\t */\n\tskb = skb_rb_next(prev);\n\tif (!skb)\n\t\tgoto out;\n\n\tif (!skb_can_shift(skb) ||\n\t    ((TCP_SKB_CB(skb)->sacked & TCPCB_TAGBITS) != TCPCB_SACKED_ACKED) ||\n\t    (mss != tcp_skb_seglen(skb)))\n\t\tgoto out;\n\n\tlen = skb->len;\n\tpcount = tcp_skb_pcount(skb);\n\tif (tcp_skb_shift(prev, skb, pcount, len))\n\t\ttcp_shifted_skb(sk, prev, skb, state, pcount,\n\t\t\t\tlen, mss, 0);\n\nout:\n\treturn prev;\n\nnoop:\n\treturn skb;\n\nfallback:\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_SACKSHIFTFALLBACK);\n\treturn NULL;\n}\n\nstatic struct sk_buff *tcp_sacktag_walk(struct sk_buff *skb, struct sock *sk,\n\t\t\t\t\tstruct tcp_sack_block *next_dup,\n\t\t\t\t\tstruct tcp_sacktag_state *state,\n\t\t\t\t\tu32 start_seq, u32 end_seq,\n\t\t\t\t\tbool dup_sack_in)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *tmp;\n\n\tskb_rbtree_walk_from(skb) {\n\t\tint in_sack = 0;\n\t\tbool dup_sack = dup_sack_in;\n\n\t\t/* queue is in-order => we can short-circuit the walk early */\n\t\tif (!before(TCP_SKB_CB(skb)->seq, end_seq))\n\t\t\tbreak;\n\n\t\tif (next_dup  &&\n\t\t    before(TCP_SKB_CB(skb)->seq, next_dup->end_seq)) {\n\t\t\tin_sack = tcp_match_skb_to_sack(sk, skb,\n\t\t\t\t\t\t\tnext_dup->start_seq,\n\t\t\t\t\t\t\tnext_dup->end_seq);\n\t\t\tif (in_sack > 0)\n\t\t\t\tdup_sack = true;\n\t\t}\n\n\t\t/* skb reference here is a bit tricky to get right, since\n\t\t * shifting can eat and free both this skb and the next,\n\t\t * so not even _safe variant of the loop is enough.\n\t\t */\n\t\tif (in_sack <= 0) {\n\t\t\ttmp = tcp_shift_skb_data(sk, skb, state,\n\t\t\t\t\t\t start_seq, end_seq, dup_sack);\n\t\t\tif (tmp) {\n\t\t\t\tif (tmp != skb) {\n\t\t\t\t\tskb = tmp;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tin_sack = 0;\n\t\t\t} else {\n\t\t\t\tin_sack = tcp_match_skb_to_sack(sk, skb,\n\t\t\t\t\t\t\t\tstart_seq,\n\t\t\t\t\t\t\t\tend_seq);\n\t\t\t}\n\t\t}\n\n\t\tif (unlikely(in_sack < 0))\n\t\t\tbreak;\n\n\t\tif (in_sack) {\n\t\t\tTCP_SKB_CB(skb)->sacked =\n\t\t\t\ttcp_sacktag_one(sk,\n\t\t\t\t\t\tstate,\n\t\t\t\t\t\tTCP_SKB_CB(skb)->sacked,\n\t\t\t\t\t\tTCP_SKB_CB(skb)->seq,\n\t\t\t\t\t\tTCP_SKB_CB(skb)->end_seq,\n\t\t\t\t\t\tdup_sack,\n\t\t\t\t\t\ttcp_skb_pcount(skb),\n\t\t\t\t\t\ttcp_skb_timestamp_us(skb));\n\t\t\ttcp_rate_skb_delivered(sk, skb, state->rate);\n\t\t\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED)\n\t\t\t\tlist_del_init(&skb->tcp_tsorted_anchor);\n\n\t\t\tif (!before(TCP_SKB_CB(skb)->seq,\n\t\t\t\t    tcp_highest_sack_seq(tp)))\n\t\t\t\ttcp_advance_highest_sack(sk, skb);\n\t\t}\n\t}\n\treturn skb;\n}\n\nstatic struct sk_buff *tcp_sacktag_bsearch(struct sock *sk, u32 seq)\n{\n\tstruct rb_node *parent, **p = &sk->tcp_rtx_queue.rb_node;\n\tstruct sk_buff *skb;\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tskb = rb_to_skb(parent);\n\t\tif (before(seq, TCP_SKB_CB(skb)->seq)) {\n\t\t\tp = &parent->rb_left;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!before(seq, TCP_SKB_CB(skb)->end_seq)) {\n\t\t\tp = &parent->rb_right;\n\t\t\tcontinue;\n\t\t}\n\t\treturn skb;\n\t}\n\treturn NULL;\n}\n\nstatic struct sk_buff *tcp_sacktag_skip(struct sk_buff *skb, struct sock *sk,\n\t\t\t\t\tu32 skip_to_seq)\n{\n\tif (skb && after(TCP_SKB_CB(skb)->seq, skip_to_seq))\n\t\treturn skb;\n\n\treturn tcp_sacktag_bsearch(sk, skip_to_seq);\n}\n\nstatic struct sk_buff *tcp_maybe_skipping_dsack(struct sk_buff *skb,\n\t\t\t\t\t\tstruct sock *sk,\n\t\t\t\t\t\tstruct tcp_sack_block *next_dup,\n\t\t\t\t\t\tstruct tcp_sacktag_state *state,\n\t\t\t\t\t\tu32 skip_to_seq)\n{\n\tif (!next_dup)\n\t\treturn skb;\n\n\tif (before(next_dup->start_seq, skip_to_seq)) {\n\t\tskb = tcp_sacktag_skip(skb, sk, next_dup->start_seq);\n\t\tskb = tcp_sacktag_walk(skb, sk, NULL, state,\n\t\t\t\t       next_dup->start_seq, next_dup->end_seq,\n\t\t\t\t       1);\n\t}\n\n\treturn skb;\n}\n\nstatic int tcp_sack_cache_ok(const struct tcp_sock *tp, const struct tcp_sack_block *cache)\n{\n\treturn cache < tp->recv_sack_cache + ARRAY_SIZE(tp->recv_sack_cache);\n}\n\nstatic int\ntcp_sacktag_write_queue(struct sock *sk, const struct sk_buff *ack_skb,\n\t\t\tu32 prior_snd_una, struct tcp_sacktag_state *state)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst unsigned char *ptr = (skb_transport_header(ack_skb) +\n\t\t\t\t    TCP_SKB_CB(ack_skb)->sacked);\n\tstruct tcp_sack_block_wire *sp_wire = (struct tcp_sack_block_wire *)(ptr+2);\n\tstruct tcp_sack_block sp[TCP_NUM_SACKS];\n\tstruct tcp_sack_block *cache;\n\tstruct sk_buff *skb;\n\tint num_sacks = min(TCP_NUM_SACKS, (ptr[1] - TCPOLEN_SACK_BASE) >> 3);\n\tint used_sacks;\n\tbool found_dup_sack = false;\n\tint i, j;\n\tint first_sack_index;\n\n\tstate->flag = 0;\n\tstate->reord = tp->snd_nxt;\n\n\tif (!tp->sacked_out)\n\t\ttcp_highest_sack_reset(sk);\n\n\tfound_dup_sack = tcp_check_dsack(sk, ack_skb, sp_wire,\n\t\t\t\t\t num_sacks, prior_snd_una, state);\n\n\t/* Eliminate too old ACKs, but take into\n\t * account more or less fresh ones, they can\n\t * contain valid SACK info.\n\t */\n\tif (before(TCP_SKB_CB(ack_skb)->ack_seq, prior_snd_una - tp->max_window))\n\t\treturn 0;\n\n\tif (!tp->packets_out)\n\t\tgoto out;\n\n\tused_sacks = 0;\n\tfirst_sack_index = 0;\n\tfor (i = 0; i < num_sacks; i++) {\n\t\tbool dup_sack = !i && found_dup_sack;\n\n\t\tsp[used_sacks].start_seq = get_unaligned_be32(&sp_wire[i].start_seq);\n\t\tsp[used_sacks].end_seq = get_unaligned_be32(&sp_wire[i].end_seq);\n\n\t\tif (!tcp_is_sackblock_valid(tp, dup_sack,\n\t\t\t\t\t    sp[used_sacks].start_seq,\n\t\t\t\t\t    sp[used_sacks].end_seq)) {\n\t\t\tint mib_idx;\n\n\t\t\tif (dup_sack) {\n\t\t\t\tif (!tp->undo_marker)\n\t\t\t\t\tmib_idx = LINUX_MIB_TCPDSACKIGNOREDNOUNDO;\n\t\t\t\telse\n\t\t\t\t\tmib_idx = LINUX_MIB_TCPDSACKIGNOREDOLD;\n\t\t\t} else {\n\t\t\t\t/* Don't count olds caused by ACK reordering */\n\t\t\t\tif ((TCP_SKB_CB(ack_skb)->ack_seq != tp->snd_una) &&\n\t\t\t\t    !after(sp[used_sacks].end_seq, tp->snd_una))\n\t\t\t\t\tcontinue;\n\t\t\t\tmib_idx = LINUX_MIB_TCPSACKDISCARD;\n\t\t\t}\n\n\t\t\tNET_INC_STATS(sock_net(sk), mib_idx);\n\t\t\tif (i == 0)\n\t\t\t\tfirst_sack_index = -1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Ignore very old stuff early */\n\t\tif (!after(sp[used_sacks].end_seq, prior_snd_una)) {\n\t\t\tif (i == 0)\n\t\t\t\tfirst_sack_index = -1;\n\t\t\tcontinue;\n\t\t}\n\n\t\tused_sacks++;\n\t}\n\n\t/* order SACK blocks to allow in order walk of the retrans queue */\n\tfor (i = used_sacks - 1; i > 0; i--) {\n\t\tfor (j = 0; j < i; j++) {\n\t\t\tif (after(sp[j].start_seq, sp[j + 1].start_seq)) {\n\t\t\t\tswap(sp[j], sp[j + 1]);\n\n\t\t\t\t/* Track where the first SACK block goes to */\n\t\t\t\tif (j == first_sack_index)\n\t\t\t\t\tfirst_sack_index = j + 1;\n\t\t\t}\n\t\t}\n\t}\n\n\tstate->mss_now = tcp_current_mss(sk);\n\tskb = NULL;\n\ti = 0;\n\n\tif (!tp->sacked_out) {\n\t\t/* It's already past, so skip checking against it */\n\t\tcache = tp->recv_sack_cache + ARRAY_SIZE(tp->recv_sack_cache);\n\t} else {\n\t\tcache = tp->recv_sack_cache;\n\t\t/* Skip empty blocks in at head of the cache */\n\t\twhile (tcp_sack_cache_ok(tp, cache) && !cache->start_seq &&\n\t\t       !cache->end_seq)\n\t\t\tcache++;\n\t}\n\n\twhile (i < used_sacks) {\n\t\tu32 start_seq = sp[i].start_seq;\n\t\tu32 end_seq = sp[i].end_seq;\n\t\tbool dup_sack = (found_dup_sack && (i == first_sack_index));\n\t\tstruct tcp_sack_block *next_dup = NULL;\n\n\t\tif (found_dup_sack && ((i + 1) == first_sack_index))\n\t\t\tnext_dup = &sp[i + 1];\n\n\t\t/* Skip too early cached blocks */\n\t\twhile (tcp_sack_cache_ok(tp, cache) &&\n\t\t       !before(start_seq, cache->end_seq))\n\t\t\tcache++;\n\n\t\t/* Can skip some work by looking recv_sack_cache? */\n\t\tif (tcp_sack_cache_ok(tp, cache) && !dup_sack &&\n\t\t    after(end_seq, cache->start_seq)) {\n\n\t\t\t/* Head todo? */\n\t\t\tif (before(start_seq, cache->start_seq)) {\n\t\t\t\tskb = tcp_sacktag_skip(skb, sk, start_seq);\n\t\t\t\tskb = tcp_sacktag_walk(skb, sk, next_dup,\n\t\t\t\t\t\t       state,\n\t\t\t\t\t\t       start_seq,\n\t\t\t\t\t\t       cache->start_seq,\n\t\t\t\t\t\t       dup_sack);\n\t\t\t}\n\n\t\t\t/* Rest of the block already fully processed? */\n\t\t\tif (!after(end_seq, cache->end_seq))\n\t\t\t\tgoto advance_sp;\n\n\t\t\tskb = tcp_maybe_skipping_dsack(skb, sk, next_dup,\n\t\t\t\t\t\t       state,\n\t\t\t\t\t\t       cache->end_seq);\n\n\t\t\t/* ...tail remains todo... */\n\t\t\tif (tcp_highest_sack_seq(tp) == cache->end_seq) {\n\t\t\t\t/* ...but better entrypoint exists! */\n\t\t\t\tskb = tcp_highest_sack(sk);\n\t\t\t\tif (!skb)\n\t\t\t\t\tbreak;\n\t\t\t\tcache++;\n\t\t\t\tgoto walk;\n\t\t\t}\n\n\t\t\tskb = tcp_sacktag_skip(skb, sk, cache->end_seq);\n\t\t\t/* Check overlap against next cached too (past this one already) */\n\t\t\tcache++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!before(start_seq, tcp_highest_sack_seq(tp))) {\n\t\t\tskb = tcp_highest_sack(sk);\n\t\t\tif (!skb)\n\t\t\t\tbreak;\n\t\t}\n\t\tskb = tcp_sacktag_skip(skb, sk, start_seq);\n\nwalk:\n\t\tskb = tcp_sacktag_walk(skb, sk, next_dup, state,\n\t\t\t\t       start_seq, end_seq, dup_sack);\n\nadvance_sp:\n\t\ti++;\n\t}\n\n\t/* Clear the head of the cache sack blocks so we can skip it next time */\n\tfor (i = 0; i < ARRAY_SIZE(tp->recv_sack_cache) - used_sacks; i++) {\n\t\ttp->recv_sack_cache[i].start_seq = 0;\n\t\ttp->recv_sack_cache[i].end_seq = 0;\n\t}\n\tfor (j = 0; j < used_sacks; j++)\n\t\ttp->recv_sack_cache[i++] = sp[j];\n\n\tif (inet_csk(sk)->icsk_ca_state != TCP_CA_Loss || tp->undo_marker)\n\t\ttcp_check_sack_reordering(sk, state->reord, 0);\n\n\ttcp_verify_left_out(tp);\nout:\n\n#if FASTRETRANS_DEBUG > 0\n\tWARN_ON((int)tp->sacked_out < 0);\n\tWARN_ON((int)tp->lost_out < 0);\n\tWARN_ON((int)tp->retrans_out < 0);\n\tWARN_ON((int)tcp_packets_in_flight(tp) < 0);\n#endif\n\treturn state->flag;\n}\n\n/* Limits sacked_out so that sum with lost_out isn't ever larger than\n * packets_out. Returns false if sacked_out adjustement wasn't necessary.\n */\nstatic bool tcp_limit_reno_sacked(struct tcp_sock *tp)\n{\n\tu32 holes;\n\n\tholes = max(tp->lost_out, 1U);\n\tholes = min(holes, tp->packets_out);\n\n\tif ((tp->sacked_out + holes) > tp->packets_out) {\n\t\ttp->sacked_out = tp->packets_out - holes;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/* If we receive more dupacks than we expected counting segments\n * in assumption of absent reordering, interpret this as reordering.\n * The only another reason could be bug in receiver TCP.\n */\nstatic void tcp_check_reno_reordering(struct sock *sk, const int addend)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (!tcp_limit_reno_sacked(tp))\n\t\treturn;\n\n\ttp->reordering = min_t(u32, tp->packets_out + addend,\n\t\t\t       sock_net(sk)->ipv4.sysctl_tcp_max_reordering);\n\ttp->reord_seen++;\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPRENOREORDER);\n}\n\n/* Emulate SACKs for SACKless connection: account for a new dupack. */\n\nstatic void tcp_add_reno_sack(struct sock *sk, int num_dupack, bool ece_ack)\n{\n\tif (num_dupack) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\t\tu32 prior_sacked = tp->sacked_out;\n\t\ts32 delivered;\n\n\t\ttp->sacked_out += num_dupack;\n\t\ttcp_check_reno_reordering(sk, 0);\n\t\tdelivered = tp->sacked_out - prior_sacked;\n\t\tif (delivered > 0)\n\t\t\ttcp_count_delivered(tp, delivered, ece_ack);\n\t\ttcp_verify_left_out(tp);\n\t}\n}\n\n/* Account for ACK, ACKing some data in Reno Recovery phase. */\n\nstatic void tcp_remove_reno_sacks(struct sock *sk, int acked, bool ece_ack)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (acked > 0) {\n\t\t/* One ACK acked hole. The rest eat duplicate ACKs. */\n\t\ttcp_count_delivered(tp, max_t(int, acked - tp->sacked_out, 1),\n\t\t\t\t    ece_ack);\n\t\tif (acked - 1 >= tp->sacked_out)\n\t\t\ttp->sacked_out = 0;\n\t\telse\n\t\t\ttp->sacked_out -= acked - 1;\n\t}\n\ttcp_check_reno_reordering(sk, acked);\n\ttcp_verify_left_out(tp);\n}\n\nstatic inline void tcp_reset_reno_sack(struct tcp_sock *tp)\n{\n\ttp->sacked_out = 0;\n}\n\nvoid tcp_clear_retrans(struct tcp_sock *tp)\n{\n\ttp->retrans_out = 0;\n\ttp->lost_out = 0;\n\ttp->undo_marker = 0;\n\ttp->undo_retrans = -1;\n\ttp->sacked_out = 0;\n}\n\nstatic inline void tcp_init_undo(struct tcp_sock *tp)\n{\n\ttp->undo_marker = tp->snd_una;\n\t/* Retransmission still in flight may cause DSACKs later. */\n\ttp->undo_retrans = tp->retrans_out ? : -1;\n}\n\nstatic bool tcp_is_rack(const struct sock *sk)\n{\n\treturn sock_net(sk)->ipv4.sysctl_tcp_recovery & TCP_RACK_LOSS_DETECTION;\n}\n\n/* If we detect SACK reneging, forget all SACK information\n * and reset tags completely, otherwise preserve SACKs. If receiver\n * dropped its ofo queue, we will know this due to reneging detection.\n */\nstatic void tcp_timeout_mark_lost(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb, *head;\n\tbool is_reneg;\t\t\t/* is receiver reneging on SACKs? */\n\n\thead = tcp_rtx_queue_head(sk);\n\tis_reneg = head && (TCP_SKB_CB(head)->sacked & TCPCB_SACKED_ACKED);\n\tif (is_reneg) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPSACKRENEGING);\n\t\ttp->sacked_out = 0;\n\t\t/* Mark SACK reneging until we recover from this loss event. */\n\t\ttp->is_sack_reneg = 1;\n\t} else if (tcp_is_reno(tp)) {\n\t\ttcp_reset_reno_sack(tp);\n\t}\n\n\tskb = head;\n\tskb_rbtree_walk_from(skb) {\n\t\tif (is_reneg)\n\t\t\tTCP_SKB_CB(skb)->sacked &= ~TCPCB_SACKED_ACKED;\n\t\telse if (tcp_is_rack(sk) && skb != head &&\n\t\t\t tcp_rack_skb_timeout(tp, skb, 0) > 0)\n\t\t\tcontinue; /* Don't mark recently sent ones lost yet */\n\t\ttcp_mark_skb_lost(sk, skb);\n\t}\n\ttcp_verify_left_out(tp);\n\ttcp_clear_all_retrans_hints(tp);\n}\n\n/* Enter Loss state. */\nvoid tcp_enter_loss(struct sock *sk)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tbool new_recovery = icsk->icsk_ca_state < TCP_CA_Recovery;\n\n\ttcp_timeout_mark_lost(sk);\n\n\t/* Reduce ssthresh if it has not yet been made inside this window. */\n\tif (icsk->icsk_ca_state <= TCP_CA_Disorder ||\n\t    !after(tp->high_seq, tp->snd_una) ||\n\t    (icsk->icsk_ca_state == TCP_CA_Loss && !icsk->icsk_retransmits)) {\n\t\ttp->prior_ssthresh = tcp_current_ssthresh(sk);\n\t\ttp->prior_cwnd = tp->snd_cwnd;\n\t\ttp->snd_ssthresh = icsk->icsk_ca_ops->ssthresh(sk);\n\t\ttcp_ca_event(sk, CA_EVENT_LOSS);\n\t\ttcp_init_undo(tp);\n\t}\n\ttp->snd_cwnd\t   = tcp_packets_in_flight(tp) + 1;\n\ttp->snd_cwnd_cnt   = 0;\n\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\n\t/* Timeout in disordered state after receiving substantial DUPACKs\n\t * suggests that the degree of reordering is over-estimated.\n\t */\n\tif (icsk->icsk_ca_state <= TCP_CA_Disorder &&\n\t    tp->sacked_out >= net->ipv4.sysctl_tcp_reordering)\n\t\ttp->reordering = min_t(unsigned int, tp->reordering,\n\t\t\t\t       net->ipv4.sysctl_tcp_reordering);\n\ttcp_set_ca_state(sk, TCP_CA_Loss);\n\ttp->high_seq = tp->snd_nxt;\n\ttcp_ecn_queue_cwr(tp);\n\n\t/* F-RTO RFC5682 sec 3.1 step 1: retransmit SND.UNA if no previous\n\t * loss recovery is underway except recurring timeout(s) on\n\t * the same SND.UNA (sec 3.2). Disable F-RTO on path MTU probing\n\t */\n\ttp->frto = net->ipv4.sysctl_tcp_frto &&\n\t\t   (new_recovery || icsk->icsk_retransmits) &&\n\t\t   !inet_csk(sk)->icsk_mtup.probe_size;\n}\n\n/* If ACK arrived pointing to a remembered SACK, it means that our\n * remembered SACKs do not reflect real state of receiver i.e.\n * receiver _host_ is heavily congested (or buggy).\n *\n * To avoid big spurious retransmission bursts due to transient SACK\n * scoreboard oddities that look like reneging, we give the receiver a\n * little time (max(RTT/2, 10ms)) to send us some more ACKs that will\n * restore sanity to the SACK scoreboard. If the apparent reneging\n * persists until this RTO then we'll clear the SACK scoreboard.\n */\nstatic bool tcp_check_sack_reneging(struct sock *sk, int flag)\n{\n\tif (flag & FLAG_SACK_RENEGING) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\t\tunsigned long delay = max(usecs_to_jiffies(tp->srtt_us >> 4),\n\t\t\t\t\t  msecs_to_jiffies(10));\n\n\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\n\t\t\t\t\t  delay, TCP_RTO_MAX);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/* Heurestics to calculate number of duplicate ACKs. There's no dupACKs\n * counter when SACK is enabled (without SACK, sacked_out is used for\n * that purpose).\n *\n * With reordering, holes may still be in flight, so RFC3517 recovery\n * uses pure sacked_out (total number of SACKed segments) even though\n * it violates the RFC that uses duplicate ACKs, often these are equal\n * but when e.g. out-of-window ACKs or packet duplication occurs,\n * they differ. Since neither occurs due to loss, TCP should really\n * ignore them.\n */\nstatic inline int tcp_dupack_heuristics(const struct tcp_sock *tp)\n{\n\treturn tp->sacked_out + 1;\n}\n\n/* Linux NewReno/SACK/ECN state machine.\n * --------------------------------------\n *\n * \"Open\"\tNormal state, no dubious events, fast path.\n * \"Disorder\"   In all the respects it is \"Open\",\n *\t\tbut requires a bit more attention. It is entered when\n *\t\twe see some SACKs or dupacks. It is split of \"Open\"\n *\t\tmainly to move some processing from fast path to slow one.\n * \"CWR\"\tCWND was reduced due to some Congestion Notification event.\n *\t\tIt can be ECN, ICMP source quench, local device congestion.\n * \"Recovery\"\tCWND was reduced, we are fast-retransmitting.\n * \"Loss\"\tCWND was reduced due to RTO timeout or SACK reneging.\n *\n * tcp_fastretrans_alert() is entered:\n * - each incoming ACK, if state is not \"Open\"\n * - when arrived ACK is unusual, namely:\n *\t* SACK\n *\t* Duplicate ACK.\n *\t* ECN ECE.\n *\n * Counting packets in flight is pretty simple.\n *\n *\tin_flight = packets_out - left_out + retrans_out\n *\n *\tpackets_out is SND.NXT-SND.UNA counted in packets.\n *\n *\tretrans_out is number of retransmitted segments.\n *\n *\tleft_out is number of segments left network, but not ACKed yet.\n *\n *\t\tleft_out = sacked_out + lost_out\n *\n *     sacked_out: Packets, which arrived to receiver out of order\n *\t\t   and hence not ACKed. With SACKs this number is simply\n *\t\t   amount of SACKed data. Even without SACKs\n *\t\t   it is easy to give pretty reliable estimate of this number,\n *\t\t   counting duplicate ACKs.\n *\n *       lost_out: Packets lost by network. TCP has no explicit\n *\t\t   \"loss notification\" feedback from network (for now).\n *\t\t   It means that this number can be only _guessed_.\n *\t\t   Actually, it is the heuristics to predict lossage that\n *\t\t   distinguishes different algorithms.\n *\n *\tF.e. after RTO, when all the queue is considered as lost,\n *\tlost_out = packets_out and in_flight = retrans_out.\n *\n *\t\tEssentially, we have now a few algorithms detecting\n *\t\tlost packets.\n *\n *\t\tIf the receiver supports SACK:\n *\n *\t\tRFC6675/3517: It is the conventional algorithm. A packet is\n *\t\tconsidered lost if the number of higher sequence packets\n *\t\tSACKed is greater than or equal the DUPACK thoreshold\n *\t\t(reordering). This is implemented in tcp_mark_head_lost and\n *\t\ttcp_update_scoreboard.\n *\n *\t\tRACK (draft-ietf-tcpm-rack-01): it is a newer algorithm\n *\t\t(2017-) that checks timing instead of counting DUPACKs.\n *\t\tEssentially a packet is considered lost if it's not S/ACKed\n *\t\tafter RTT + reordering_window, where both metrics are\n *\t\tdynamically measured and adjusted. This is implemented in\n *\t\ttcp_rack_mark_lost.\n *\n *\t\tIf the receiver does not support SACK:\n *\n *\t\tNewReno (RFC6582): in Recovery we assume that one segment\n *\t\tis lost (classic Reno). While we are in Recovery and\n *\t\ta partial ACK arrives, we assume that one more packet\n *\t\tis lost (NewReno). This heuristics are the same in NewReno\n *\t\tand SACK.\n *\n * Really tricky (and requiring careful tuning) part of algorithm\n * is hidden in functions tcp_time_to_recover() and tcp_xmit_retransmit_queue().\n * The first determines the moment _when_ we should reduce CWND and,\n * hence, slow down forward transmission. In fact, it determines the moment\n * when we decide that hole is caused by loss, rather than by a reorder.\n *\n * tcp_xmit_retransmit_queue() decides, _what_ we should retransmit to fill\n * holes, caused by lost packets.\n *\n * And the most logically complicated part of algorithm is undo\n * heuristics. We detect false retransmits due to both too early\n * fast retransmit (reordering) and underestimated RTO, analyzing\n * timestamps and D-SACKs. When we detect that some segments were\n * retransmitted by mistake and CWND reduction was wrong, we undo\n * window reduction and abort recovery phase. This logic is hidden\n * inside several functions named tcp_try_undo_<something>.\n */\n\n/* This function decides, when we should leave Disordered state\n * and enter Recovery phase, reducing congestion window.\n *\n * Main question: may we further continue forward transmission\n * with the same cwnd?\n */\nstatic bool tcp_time_to_recover(struct sock *sk, int flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Trick#1: The loss is proven. */\n\tif (tp->lost_out)\n\t\treturn true;\n\n\t/* Not-A-Trick#2 : Classic rule... */\n\tif (!tcp_is_rack(sk) && tcp_dupack_heuristics(tp) > tp->reordering)\n\t\treturn true;\n\n\treturn false;\n}\n\n/* Detect loss in event \"A\" above by marking head of queue up as lost.\n * For RFC3517 SACK, a segment is considered lost if it\n * has at least tp->reordering SACKed seqments above it; \"packets\" refers to\n * the maximum SACKed segments to pass before reaching this limit.\n */\nstatic void tcp_mark_head_lost(struct sock *sk, int packets, int mark_head)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\tint cnt;\n\t/* Use SACK to deduce losses of new sequences sent during recovery */\n\tconst u32 loss_high = tp->snd_nxt;\n\n\tWARN_ON(packets > tp->packets_out);\n\tskb = tp->lost_skb_hint;\n\tif (skb) {\n\t\t/* Head already handled? */\n\t\tif (mark_head && after(TCP_SKB_CB(skb)->seq, tp->snd_una))\n\t\t\treturn;\n\t\tcnt = tp->lost_cnt_hint;\n\t} else {\n\t\tskb = tcp_rtx_queue_head(sk);\n\t\tcnt = 0;\n\t}\n\n\tskb_rbtree_walk_from(skb) {\n\t\t/* TODO: do this better */\n\t\t/* this is not the most efficient way to do this... */\n\t\ttp->lost_skb_hint = skb;\n\t\ttp->lost_cnt_hint = cnt;\n\n\t\tif (after(TCP_SKB_CB(skb)->end_seq, loss_high))\n\t\t\tbreak;\n\n\t\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED)\n\t\t\tcnt += tcp_skb_pcount(skb);\n\n\t\tif (cnt > packets)\n\t\t\tbreak;\n\n\t\tif (!(TCP_SKB_CB(skb)->sacked & TCPCB_LOST))\n\t\t\ttcp_mark_skb_lost(sk, skb);\n\n\t\tif (mark_head)\n\t\t\tbreak;\n\t}\n\ttcp_verify_left_out(tp);\n}\n\n/* Account newly detected lost packet(s) */\n\nstatic void tcp_update_scoreboard(struct sock *sk, int fast_rexmit)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tcp_is_sack(tp)) {\n\t\tint sacked_upto = tp->sacked_out - tp->reordering;\n\t\tif (sacked_upto >= 0)\n\t\t\ttcp_mark_head_lost(sk, sacked_upto, 0);\n\t\telse if (fast_rexmit)\n\t\t\ttcp_mark_head_lost(sk, 1, 1);\n\t}\n}\n\nstatic bool tcp_tsopt_ecr_before(const struct tcp_sock *tp, u32 when)\n{\n\treturn tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr &&\n\t       before(tp->rx_opt.rcv_tsecr, when);\n}\n\n/* skb is spurious retransmitted if the returned timestamp echo\n * reply is prior to the skb transmission time\n */\nstatic bool tcp_skb_spurious_retrans(const struct tcp_sock *tp,\n\t\t\t\t     const struct sk_buff *skb)\n{\n\treturn (TCP_SKB_CB(skb)->sacked & TCPCB_RETRANS) &&\n\t       tcp_tsopt_ecr_before(tp, tcp_skb_timestamp(skb));\n}\n\n/* Nothing was retransmitted or returned timestamp is less\n * than timestamp of the first retransmission.\n */\nstatic inline bool tcp_packet_delayed(const struct tcp_sock *tp)\n{\n\treturn tp->retrans_stamp &&\n\t       tcp_tsopt_ecr_before(tp, tp->retrans_stamp);\n}\n\n/* Undo procedures. */\n\n/* We can clear retrans_stamp when there are no retransmissions in the\n * window. It would seem that it is trivially available for us in\n * tp->retrans_out, however, that kind of assumptions doesn't consider\n * what will happen if errors occur when sending retransmission for the\n * second time. ...It could the that such segment has only\n * TCPCB_EVER_RETRANS set at the present time. It seems that checking\n * the head skb is enough except for some reneging corner cases that\n * are not worth the effort.\n *\n * Main reason for all this complexity is the fact that connection dying\n * time now depends on the validity of the retrans_stamp, in particular,\n * that successive retransmissions of a segment must not advance\n * retrans_stamp under any conditions.\n */\nstatic bool tcp_any_retrans_done(const struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\n\tif (tp->retrans_out)\n\t\treturn true;\n\n\tskb = tcp_rtx_queue_head(sk);\n\tif (unlikely(skb && TCP_SKB_CB(skb)->sacked & TCPCB_EVER_RETRANS))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic void DBGUNDO(struct sock *sk, const char *msg)\n{\n#if FASTRETRANS_DEBUG > 1\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\tif (sk->sk_family == AF_INET) {\n\t\tpr_debug(\"Undo %s %pI4/%u c%u l%u ss%u/%u p%u\\n\",\n\t\t\t msg,\n\t\t\t &inet->inet_daddr, ntohs(inet->inet_dport),\n\t\t\t tp->snd_cwnd, tcp_left_out(tp),\n\t\t\t tp->snd_ssthresh, tp->prior_ssthresh,\n\t\t\t tp->packets_out);\n\t}\n#if IS_ENABLED(CONFIG_IPV6)\n\telse if (sk->sk_family == AF_INET6) {\n\t\tpr_debug(\"Undo %s %pI6/%u c%u l%u ss%u/%u p%u\\n\",\n\t\t\t msg,\n\t\t\t &sk->sk_v6_daddr, ntohs(inet->inet_dport),\n\t\t\t tp->snd_cwnd, tcp_left_out(tp),\n\t\t\t tp->snd_ssthresh, tp->prior_ssthresh,\n\t\t\t tp->packets_out);\n\t}\n#endif\n#endif\n}\n\nstatic void tcp_undo_cwnd_reduction(struct sock *sk, bool unmark_loss)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (unmark_loss) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb_rbtree_walk(skb, &sk->tcp_rtx_queue) {\n\t\t\tTCP_SKB_CB(skb)->sacked &= ~TCPCB_LOST;\n\t\t}\n\t\ttp->lost_out = 0;\n\t\ttcp_clear_all_retrans_hints(tp);\n\t}\n\n\tif (tp->prior_ssthresh) {\n\t\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\t\ttp->snd_cwnd = icsk->icsk_ca_ops->undo_cwnd(sk);\n\n\t\tif (tp->prior_ssthresh > tp->snd_ssthresh) {\n\t\t\ttp->snd_ssthresh = tp->prior_ssthresh;\n\t\t\ttcp_ecn_withdraw_cwr(tp);\n\t\t}\n\t}\n\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\ttp->undo_marker = 0;\n\ttp->rack.advanced = 1; /* Force RACK to re-exam losses */\n}\n\nstatic inline bool tcp_may_undo(const struct tcp_sock *tp)\n{\n\treturn tp->undo_marker && (!tp->undo_retrans || tcp_packet_delayed(tp));\n}\n\n/* People celebrate: \"We love our President!\" */\nstatic bool tcp_try_undo_recovery(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tcp_may_undo(tp)) {\n\t\tint mib_idx;\n\n\t\t/* Happy end! We did not retransmit anything\n\t\t * or our original transmission succeeded.\n\t\t */\n\t\tDBGUNDO(sk, inet_csk(sk)->icsk_ca_state == TCP_CA_Loss ? \"loss\" : \"retrans\");\n\t\ttcp_undo_cwnd_reduction(sk, false);\n\t\tif (inet_csk(sk)->icsk_ca_state == TCP_CA_Loss)\n\t\t\tmib_idx = LINUX_MIB_TCPLOSSUNDO;\n\t\telse\n\t\t\tmib_idx = LINUX_MIB_TCPFULLUNDO;\n\n\t\tNET_INC_STATS(sock_net(sk), mib_idx);\n\t} else if (tp->rack.reo_wnd_persist) {\n\t\ttp->rack.reo_wnd_persist--;\n\t}\n\tif (tp->snd_una == tp->high_seq && tcp_is_reno(tp)) {\n\t\t/* Hold old state until something *above* high_seq\n\t\t * is ACKed. For Reno it is MUST to prevent false\n\t\t * fast retransmits (RFC2582). SACK TCP is safe. */\n\t\tif (!tcp_any_retrans_done(sk))\n\t\t\ttp->retrans_stamp = 0;\n\t\treturn true;\n\t}\n\ttcp_set_ca_state(sk, TCP_CA_Open);\n\ttp->is_sack_reneg = 0;\n\treturn false;\n}\n\n/* Try to undo cwnd reduction, because D-SACKs acked all retransmitted data */\nstatic bool tcp_try_undo_dsack(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tp->undo_marker && !tp->undo_retrans) {\n\t\ttp->rack.reo_wnd_persist = min(TCP_RACK_RECOVERY_THRESH,\n\t\t\t\t\t       tp->rack.reo_wnd_persist + 1);\n\t\tDBGUNDO(sk, \"D-SACK\");\n\t\ttcp_undo_cwnd_reduction(sk, false);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDSACKUNDO);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/* Undo during loss recovery after partial ACK or using F-RTO. */\nstatic bool tcp_try_undo_loss(struct sock *sk, bool frto_undo)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (frto_undo || tcp_may_undo(tp)) {\n\t\ttcp_undo_cwnd_reduction(sk, true);\n\n\t\tDBGUNDO(sk, \"partial loss\");\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPLOSSUNDO);\n\t\tif (frto_undo)\n\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\tLINUX_MIB_TCPSPURIOUSRTOS);\n\t\tinet_csk(sk)->icsk_retransmits = 0;\n\t\tif (frto_undo || tcp_is_sack(tp)) {\n\t\t\ttcp_set_ca_state(sk, TCP_CA_Open);\n\t\t\ttp->is_sack_reneg = 0;\n\t\t}\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/* The cwnd reduction in CWR and Recovery uses the PRR algorithm in RFC 6937.\n * It computes the number of packets to send (sndcnt) based on packets newly\n * delivered:\n *   1) If the packets in flight is larger than ssthresh, PRR spreads the\n *\tcwnd reductions across a full RTT.\n *   2) Otherwise PRR uses packet conservation to send as much as delivered.\n *      But when the retransmits are acked without further losses, PRR\n *      slow starts cwnd up to ssthresh to speed up the recovery.\n */\nstatic void tcp_init_cwnd_reduction(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttp->high_seq = tp->snd_nxt;\n\ttp->tlp_high_seq = 0;\n\ttp->snd_cwnd_cnt = 0;\n\ttp->prior_cwnd = tp->snd_cwnd;\n\ttp->prr_delivered = 0;\n\ttp->prr_out = 0;\n\ttp->snd_ssthresh = inet_csk(sk)->icsk_ca_ops->ssthresh(sk);\n\ttcp_ecn_queue_cwr(tp);\n}\n\nvoid tcp_cwnd_reduction(struct sock *sk, int newly_acked_sacked, int flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint sndcnt = 0;\n\tint delta = tp->snd_ssthresh - tcp_packets_in_flight(tp);\n\n\tif (newly_acked_sacked <= 0 || WARN_ON_ONCE(!tp->prior_cwnd))\n\t\treturn;\n\n\ttp->prr_delivered += newly_acked_sacked;\n\tif (delta < 0) {\n\t\tu64 dividend = (u64)tp->snd_ssthresh * tp->prr_delivered +\n\t\t\t       tp->prior_cwnd - 1;\n\t\tsndcnt = div_u64(dividend, tp->prior_cwnd) - tp->prr_out;\n\t} else if ((flag & (FLAG_RETRANS_DATA_ACKED | FLAG_LOST_RETRANS)) ==\n\t\t   FLAG_RETRANS_DATA_ACKED) {\n\t\tsndcnt = min_t(int, delta,\n\t\t\t       max_t(int, tp->prr_delivered - tp->prr_out,\n\t\t\t\t     newly_acked_sacked) + 1);\n\t} else {\n\t\tsndcnt = min(delta, newly_acked_sacked);\n\t}\n\t/* Force a fast retransmit upon entering fast recovery */\n\tsndcnt = max(sndcnt, (tp->prr_out ? 0 : 1));\n\ttp->snd_cwnd = tcp_packets_in_flight(tp) + sndcnt;\n}\n\nstatic inline void tcp_end_cwnd_reduction(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (inet_csk(sk)->icsk_ca_ops->cong_control)\n\t\treturn;\n\n\t/* Reset cwnd to ssthresh in CWR or Recovery (unless it's undone) */\n\tif (tp->snd_ssthresh < TCP_INFINITE_SSTHRESH &&\n\t    (inet_csk(sk)->icsk_ca_state == TCP_CA_CWR || tp->undo_marker)) {\n\t\ttp->snd_cwnd = tp->snd_ssthresh;\n\t\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\t}\n\ttcp_ca_event(sk, CA_EVENT_COMPLETE_CWR);\n}\n\n/* Enter CWR state. Disable cwnd undo since congestion is proven with ECN */\nvoid tcp_enter_cwr(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttp->prior_ssthresh = 0;\n\tif (inet_csk(sk)->icsk_ca_state < TCP_CA_CWR) {\n\t\ttp->undo_marker = 0;\n\t\ttcp_init_cwnd_reduction(sk);\n\t\ttcp_set_ca_state(sk, TCP_CA_CWR);\n\t}\n}\nEXPORT_SYMBOL(tcp_enter_cwr);\n\nstatic void tcp_try_keep_open(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint state = TCP_CA_Open;\n\n\tif (tcp_left_out(tp) || tcp_any_retrans_done(sk))\n\t\tstate = TCP_CA_Disorder;\n\n\tif (inet_csk(sk)->icsk_ca_state != state) {\n\t\ttcp_set_ca_state(sk, state);\n\t\ttp->high_seq = tp->snd_nxt;\n\t}\n}\n\nstatic void tcp_try_to_open(struct sock *sk, int flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttcp_verify_left_out(tp);\n\n\tif (!tcp_any_retrans_done(sk))\n\t\ttp->retrans_stamp = 0;\n\n\tif (flag & FLAG_ECE)\n\t\ttcp_enter_cwr(sk);\n\n\tif (inet_csk(sk)->icsk_ca_state != TCP_CA_CWR) {\n\t\ttcp_try_keep_open(sk);\n\t}\n}\n\nstatic void tcp_mtup_probe_failed(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\ticsk->icsk_mtup.search_high = icsk->icsk_mtup.probe_size - 1;\n\ticsk->icsk_mtup.probe_size = 0;\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMTUPFAIL);\n}\n\nstatic void tcp_mtup_probe_success(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\t/* FIXME: breaks with very large cwnd */\n\ttp->prior_ssthresh = tcp_current_ssthresh(sk);\n\ttp->snd_cwnd = tp->snd_cwnd *\n\t\t       tcp_mss_to_mtu(sk, tp->mss_cache) /\n\t\t       icsk->icsk_mtup.probe_size;\n\ttp->snd_cwnd_cnt = 0;\n\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\ttp->snd_ssthresh = tcp_current_ssthresh(sk);\n\n\ticsk->icsk_mtup.search_low = icsk->icsk_mtup.probe_size;\n\ticsk->icsk_mtup.probe_size = 0;\n\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMTUPSUCCESS);\n}\n\n/* Do a simple retransmit without using the backoff mechanisms in\n * tcp_timer. This is used for path mtu discovery.\n * The socket is already locked here.\n */\nvoid tcp_simple_retransmit(struct sock *sk)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\tunsigned int mss = tcp_current_mss(sk);\n\n\tskb_rbtree_walk(skb, &sk->tcp_rtx_queue) {\n\t\tif (tcp_skb_seglen(skb) > mss)\n\t\t\ttcp_mark_skb_lost(sk, skb);\n\t}\n\n\ttcp_clear_retrans_hints_partial(tp);\n\n\tif (!tp->lost_out)\n\t\treturn;\n\n\tif (tcp_is_reno(tp))\n\t\ttcp_limit_reno_sacked(tp);\n\n\ttcp_verify_left_out(tp);\n\n\t/* Don't muck with the congestion window here.\n\t * Reason is that we do not increase amount of _data_\n\t * in network, but units changed and effective\n\t * cwnd/ssthresh really reduced now.\n\t */\n\tif (icsk->icsk_ca_state != TCP_CA_Loss) {\n\t\ttp->high_seq = tp->snd_nxt;\n\t\ttp->snd_ssthresh = tcp_current_ssthresh(sk);\n\t\ttp->prior_ssthresh = 0;\n\t\ttp->undo_marker = 0;\n\t\ttcp_set_ca_state(sk, TCP_CA_Loss);\n\t}\n\ttcp_xmit_retransmit_queue(sk);\n}\nEXPORT_SYMBOL(tcp_simple_retransmit);\n\nvoid tcp_enter_recovery(struct sock *sk, bool ece_ack)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint mib_idx;\n\n\tif (tcp_is_reno(tp))\n\t\tmib_idx = LINUX_MIB_TCPRENORECOVERY;\n\telse\n\t\tmib_idx = LINUX_MIB_TCPSACKRECOVERY;\n\n\tNET_INC_STATS(sock_net(sk), mib_idx);\n\n\ttp->prior_ssthresh = 0;\n\ttcp_init_undo(tp);\n\n\tif (!tcp_in_cwnd_reduction(sk)) {\n\t\tif (!ece_ack)\n\t\t\ttp->prior_ssthresh = tcp_current_ssthresh(sk);\n\t\ttcp_init_cwnd_reduction(sk);\n\t}\n\ttcp_set_ca_state(sk, TCP_CA_Recovery);\n}\n\n/* Process an ACK in CA_Loss state. Move to CA_Open if lost data are\n * recovered or spurious. Otherwise retransmits more on partial ACKs.\n */\nstatic void tcp_process_loss(struct sock *sk, int flag, int num_dupack,\n\t\t\t     int *rexmit)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tbool recovered = !before(tp->snd_una, tp->high_seq);\n\n\tif ((flag & FLAG_SND_UNA_ADVANCED || rcu_access_pointer(tp->fastopen_rsk)) &&\n\t    tcp_try_undo_loss(sk, false))\n\t\treturn;\n\n\tif (tp->frto) { /* F-RTO RFC5682 sec 3.1 (sack enhanced version). */\n\t\t/* Step 3.b. A timeout is spurious if not all data are\n\t\t * lost, i.e., never-retransmitted data are (s)acked.\n\t\t */\n\t\tif ((flag & FLAG_ORIG_SACK_ACKED) &&\n\t\t    tcp_try_undo_loss(sk, true))\n\t\t\treturn;\n\n\t\tif (after(tp->snd_nxt, tp->high_seq)) {\n\t\t\tif (flag & FLAG_DATA_SACKED || num_dupack)\n\t\t\t\ttp->frto = 0; /* Step 3.a. loss was real */\n\t\t} else if (flag & FLAG_SND_UNA_ADVANCED && !recovered) {\n\t\t\ttp->high_seq = tp->snd_nxt;\n\t\t\t/* Step 2.b. Try send new data (but deferred until cwnd\n\t\t\t * is updated in tcp_ack()). Otherwise fall back to\n\t\t\t * the conventional recovery.\n\t\t\t */\n\t\t\tif (!tcp_write_queue_empty(sk) &&\n\t\t\t    after(tcp_wnd_end(tp), tp->snd_nxt)) {\n\t\t\t\t*rexmit = REXMIT_NEW;\n\t\t\t\treturn;\n\t\t\t}\n\t\t\ttp->frto = 0;\n\t\t}\n\t}\n\n\tif (recovered) {\n\t\t/* F-RTO RFC5682 sec 3.1 step 2.a and 1st part of step 3.a */\n\t\ttcp_try_undo_recovery(sk);\n\t\treturn;\n\t}\n\tif (tcp_is_reno(tp)) {\n\t\t/* A Reno DUPACK means new data in F-RTO step 2.b above are\n\t\t * delivered. Lower inflight to clock out (re)tranmissions.\n\t\t */\n\t\tif (after(tp->snd_nxt, tp->high_seq) && num_dupack)\n\t\t\ttcp_add_reno_sack(sk, num_dupack, flag & FLAG_ECE);\n\t\telse if (flag & FLAG_SND_UNA_ADVANCED)\n\t\t\ttcp_reset_reno_sack(tp);\n\t}\n\t*rexmit = REXMIT_LOST;\n}\n\n/* Undo during fast recovery after partial ACK. */\nstatic bool tcp_try_undo_partial(struct sock *sk, u32 prior_snd_una)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tp->undo_marker && tcp_packet_delayed(tp)) {\n\t\t/* Plain luck! Hole if filled with delayed\n\t\t * packet, rather than with a retransmit. Check reordering.\n\t\t */\n\t\ttcp_check_sack_reordering(sk, prior_snd_una, 1);\n\n\t\t/* We are getting evidence that the reordering degree is higher\n\t\t * than we realized. If there are no retransmits out then we\n\t\t * can undo. Otherwise we clock out new packets but do not\n\t\t * mark more packets lost or retransmit more.\n\t\t */\n\t\tif (tp->retrans_out)\n\t\t\treturn true;\n\n\t\tif (!tcp_any_retrans_done(sk))\n\t\t\ttp->retrans_stamp = 0;\n\n\t\tDBGUNDO(sk, \"partial recovery\");\n\t\ttcp_undo_cwnd_reduction(sk, true);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPPARTIALUNDO);\n\t\ttcp_try_keep_open(sk);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void tcp_identify_packet_loss(struct sock *sk, int *ack_flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tcp_rtx_queue_empty(sk))\n\t\treturn;\n\n\tif (unlikely(tcp_is_reno(tp))) {\n\t\ttcp_newreno_mark_lost(sk, *ack_flag & FLAG_SND_UNA_ADVANCED);\n\t} else if (tcp_is_rack(sk)) {\n\t\tu32 prior_retrans = tp->retrans_out;\n\n\t\ttcp_rack_mark_lost(sk);\n\t\tif (prior_retrans > tp->retrans_out)\n\t\t\t*ack_flag |= FLAG_LOST_RETRANS;\n\t}\n}\n\nstatic bool tcp_force_fast_retransmit(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\treturn after(tcp_highest_sack_seq(tp),\n\t\t     tp->snd_una + tp->reordering * tp->mss_cache);\n}\n\n/* Process an event, which can update packets-in-flight not trivially.\n * Main goal of this function is to calculate new estimate for left_out,\n * taking into account both packets sitting in receiver's buffer and\n * packets lost by network.\n *\n * Besides that it updates the congestion state when packet loss or ECN\n * is detected. But it does not reduce the cwnd, it is done by the\n * congestion control later.\n *\n * It does _not_ decide what to send, it is made in function\n * tcp_xmit_retransmit_queue().\n */\nstatic void tcp_fastretrans_alert(struct sock *sk, const u32 prior_snd_una,\n\t\t\t\t  int num_dupack, int *ack_flag, int *rexmit)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint fast_rexmit = 0, flag = *ack_flag;\n\tbool ece_ack = flag & FLAG_ECE;\n\tbool do_lost = num_dupack || ((flag & FLAG_DATA_SACKED) &&\n\t\t\t\t      tcp_force_fast_retransmit(sk));\n\n\tif (!tp->packets_out && tp->sacked_out)\n\t\ttp->sacked_out = 0;\n\n\t/* Now state machine starts.\n\t * A. ECE, hence prohibit cwnd undoing, the reduction is required. */\n\tif (ece_ack)\n\t\ttp->prior_ssthresh = 0;\n\n\t/* B. In all the states check for reneging SACKs. */\n\tif (tcp_check_sack_reneging(sk, flag))\n\t\treturn;\n\n\t/* C. Check consistency of the current state. */\n\ttcp_verify_left_out(tp);\n\n\t/* D. Check state exit conditions. State can be terminated\n\t *    when high_seq is ACKed. */\n\tif (icsk->icsk_ca_state == TCP_CA_Open) {\n\t\tWARN_ON(tp->retrans_out != 0);\n\t\ttp->retrans_stamp = 0;\n\t} else if (!before(tp->snd_una, tp->high_seq)) {\n\t\tswitch (icsk->icsk_ca_state) {\n\t\tcase TCP_CA_CWR:\n\t\t\t/* CWR is to be held something *above* high_seq\n\t\t\t * is ACKed for CWR bit to reach receiver. */\n\t\t\tif (tp->snd_una != tp->high_seq) {\n\t\t\t\ttcp_end_cwnd_reduction(sk);\n\t\t\t\ttcp_set_ca_state(sk, TCP_CA_Open);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TCP_CA_Recovery:\n\t\t\tif (tcp_is_reno(tp))\n\t\t\t\ttcp_reset_reno_sack(tp);\n\t\t\tif (tcp_try_undo_recovery(sk))\n\t\t\t\treturn;\n\t\t\ttcp_end_cwnd_reduction(sk);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* E. Process state. */\n\tswitch (icsk->icsk_ca_state) {\n\tcase TCP_CA_Recovery:\n\t\tif (!(flag & FLAG_SND_UNA_ADVANCED)) {\n\t\t\tif (tcp_is_reno(tp))\n\t\t\t\ttcp_add_reno_sack(sk, num_dupack, ece_ack);\n\t\t} else {\n\t\t\tif (tcp_try_undo_partial(sk, prior_snd_una))\n\t\t\t\treturn;\n\t\t\t/* Partial ACK arrived. Force fast retransmit. */\n\t\t\tdo_lost = tcp_force_fast_retransmit(sk);\n\t\t}\n\t\tif (tcp_try_undo_dsack(sk)) {\n\t\t\ttcp_try_keep_open(sk);\n\t\t\treturn;\n\t\t}\n\t\ttcp_identify_packet_loss(sk, ack_flag);\n\t\tbreak;\n\tcase TCP_CA_Loss:\n\t\ttcp_process_loss(sk, flag, num_dupack, rexmit);\n\t\ttcp_identify_packet_loss(sk, ack_flag);\n\t\tif (!(icsk->icsk_ca_state == TCP_CA_Open ||\n\t\t      (*ack_flag & FLAG_LOST_RETRANS)))\n\t\t\treturn;\n\t\t/* Change state if cwnd is undone or retransmits are lost */\n\t\tfallthrough;\n\tdefault:\n\t\tif (tcp_is_reno(tp)) {\n\t\t\tif (flag & FLAG_SND_UNA_ADVANCED)\n\t\t\t\ttcp_reset_reno_sack(tp);\n\t\t\ttcp_add_reno_sack(sk, num_dupack, ece_ack);\n\t\t}\n\n\t\tif (icsk->icsk_ca_state <= TCP_CA_Disorder)\n\t\t\ttcp_try_undo_dsack(sk);\n\n\t\ttcp_identify_packet_loss(sk, ack_flag);\n\t\tif (!tcp_time_to_recover(sk, flag)) {\n\t\t\ttcp_try_to_open(sk, flag);\n\t\t\treturn;\n\t\t}\n\n\t\t/* MTU probe failure: don't reduce cwnd */\n\t\tif (icsk->icsk_ca_state < TCP_CA_CWR &&\n\t\t    icsk->icsk_mtup.probe_size &&\n\t\t    tp->snd_una == tp->mtu_probe.probe_seq_start) {\n\t\t\ttcp_mtup_probe_failed(sk);\n\t\t\t/* Restores the reduction we did in tcp_mtup_probe() */\n\t\t\ttp->snd_cwnd++;\n\t\t\ttcp_simple_retransmit(sk);\n\t\t\treturn;\n\t\t}\n\n\t\t/* Otherwise enter Recovery state */\n\t\ttcp_enter_recovery(sk, ece_ack);\n\t\tfast_rexmit = 1;\n\t}\n\n\tif (!tcp_is_rack(sk) && do_lost)\n\t\ttcp_update_scoreboard(sk, fast_rexmit);\n\t*rexmit = REXMIT_LOST;\n}\n\nstatic void tcp_update_rtt_min(struct sock *sk, u32 rtt_us, const int flag)\n{\n\tu32 wlen = sock_net(sk)->ipv4.sysctl_tcp_min_rtt_wlen * HZ;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif ((flag & FLAG_ACK_MAYBE_DELAYED) && rtt_us > tcp_min_rtt(tp)) {\n\t\t/* If the remote keeps returning delayed ACKs, eventually\n\t\t * the min filter would pick it up and overestimate the\n\t\t * prop. delay when it expires. Skip suspected delayed ACKs.\n\t\t */\n\t\treturn;\n\t}\n\tminmax_running_min(&tp->rtt_min, wlen, tcp_jiffies32,\n\t\t\t   rtt_us ? : jiffies_to_usecs(1));\n}\n\nstatic bool tcp_ack_update_rtt(struct sock *sk, const int flag,\n\t\t\t       long seq_rtt_us, long sack_rtt_us,\n\t\t\t       long ca_rtt_us, struct rate_sample *rs)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Prefer RTT measured from ACK's timing to TS-ECR. This is because\n\t * broken middle-boxes or peers may corrupt TS-ECR fields. But\n\t * Karn's algorithm forbids taking RTT if some retransmitted data\n\t * is acked (RFC6298).\n\t */\n\tif (seq_rtt_us < 0)\n\t\tseq_rtt_us = sack_rtt_us;\n\n\t/* RTTM Rule: A TSecr value received in a segment is used to\n\t * update the averaged RTT measurement only if the segment\n\t * acknowledges some new data, i.e., only if it advances the\n\t * left edge of the send window.\n\t * See draft-ietf-tcplw-high-performance-00, section 3.3.\n\t */\n\tif (seq_rtt_us < 0 && tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr &&\n\t    flag & FLAG_ACKED) {\n\t\tu32 delta = tcp_time_stamp(tp) - tp->rx_opt.rcv_tsecr;\n\n\t\tif (likely(delta < INT_MAX / (USEC_PER_SEC / TCP_TS_HZ))) {\n\t\t\tif (!delta)\n\t\t\t\tdelta = 1;\n\t\t\tseq_rtt_us = delta * (USEC_PER_SEC / TCP_TS_HZ);\n\t\t\tca_rtt_us = seq_rtt_us;\n\t\t}\n\t}\n\trs->rtt_us = ca_rtt_us; /* RTT of last (S)ACKed packet (or -1) */\n\tif (seq_rtt_us < 0)\n\t\treturn false;\n\n\t/* ca_rtt_us >= 0 is counting on the invariant that ca_rtt_us is\n\t * always taken together with ACK, SACK, or TS-opts. Any negative\n\t * values will be skipped with the seq_rtt_us < 0 check above.\n\t */\n\ttcp_update_rtt_min(sk, ca_rtt_us, flag);\n\ttcp_rtt_estimator(sk, seq_rtt_us);\n\ttcp_set_rto(sk);\n\n\t/* RFC6298: only reset backoff on valid RTT measurement. */\n\tinet_csk(sk)->icsk_backoff = 0;\n\treturn true;\n}\n\n/* Compute time elapsed between (last) SYNACK and the ACK completing 3WHS. */\nvoid tcp_synack_rtt_meas(struct sock *sk, struct request_sock *req)\n{\n\tstruct rate_sample rs;\n\tlong rtt_us = -1L;\n\n\tif (req && !req->num_retrans && tcp_rsk(req)->snt_synack)\n\t\trtt_us = tcp_stamp_us_delta(tcp_clock_us(), tcp_rsk(req)->snt_synack);\n\n\ttcp_ack_update_rtt(sk, FLAG_SYN_ACKED, rtt_us, -1L, rtt_us, &rs);\n}\n\n\nstatic void tcp_cong_avoid(struct sock *sk, u32 ack, u32 acked)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\ticsk->icsk_ca_ops->cong_avoid(sk, ack, acked);\n\ttcp_sk(sk)->snd_cwnd_stamp = tcp_jiffies32;\n}\n\n/* Restart timer after forward progress on connection.\n * RFC2988 recommends to restart timer to now+rto.\n */\nvoid tcp_rearm_rto(struct sock *sk)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* If the retrans timer is currently being used by Fast Open\n\t * for SYN-ACK retrans purpose, stay put.\n\t */\n\tif (rcu_access_pointer(tp->fastopen_rsk))\n\t\treturn;\n\n\tif (!tp->packets_out) {\n\t\tinet_csk_clear_xmit_timer(sk, ICSK_TIME_RETRANS);\n\t} else {\n\t\tu32 rto = inet_csk(sk)->icsk_rto;\n\t\t/* Offset the time elapsed after installing regular RTO */\n\t\tif (icsk->icsk_pending == ICSK_TIME_REO_TIMEOUT ||\n\t\t    icsk->icsk_pending == ICSK_TIME_LOSS_PROBE) {\n\t\t\ts64 delta_us = tcp_rto_delta_us(sk);\n\t\t\t/* delta_us may not be positive if the socket is locked\n\t\t\t * when the retrans timer fires and is rescheduled.\n\t\t\t */\n\t\t\trto = usecs_to_jiffies(max_t(int, delta_us, 1));\n\t\t}\n\t\ttcp_reset_xmit_timer(sk, ICSK_TIME_RETRANS, rto,\n\t\t\t\t     TCP_RTO_MAX);\n\t}\n}\n\n/* Try to schedule a loss probe; if that doesn't work, then schedule an RTO. */\nstatic void tcp_set_xmit_timer(struct sock *sk)\n{\n\tif (!tcp_schedule_loss_probe(sk, true))\n\t\ttcp_rearm_rto(sk);\n}\n\n/* If we get here, the whole TSO packet has not been acked. */\nstatic u32 tcp_tso_acked(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 packets_acked;\n\n\tBUG_ON(!after(TCP_SKB_CB(skb)->end_seq, tp->snd_una));\n\n\tpackets_acked = tcp_skb_pcount(skb);\n\tif (tcp_trim_head(sk, skb, tp->snd_una - TCP_SKB_CB(skb)->seq))\n\t\treturn 0;\n\tpackets_acked -= tcp_skb_pcount(skb);\n\n\tif (packets_acked) {\n\t\tBUG_ON(tcp_skb_pcount(skb) == 0);\n\t\tBUG_ON(!before(TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb)->end_seq));\n\t}\n\n\treturn packets_acked;\n}\n\nstatic void tcp_ack_tstamp(struct sock *sk, struct sk_buff *skb,\n\t\t\t   u32 prior_snd_una)\n{\n\tconst struct skb_shared_info *shinfo;\n\n\t/* Avoid cache line misses to get skb_shinfo() and shinfo->tx_flags */\n\tif (likely(!TCP_SKB_CB(skb)->txstamp_ack))\n\t\treturn;\n\n\tshinfo = skb_shinfo(skb);\n\tif (!before(shinfo->tskey, prior_snd_una) &&\n\t    before(shinfo->tskey, tcp_sk(sk)->snd_una)) {\n\t\ttcp_skb_tsorted_save(skb) {\n\t\t\t__skb_tstamp_tx(skb, NULL, sk, SCM_TSTAMP_ACK);\n\t\t} tcp_skb_tsorted_restore(skb);\n\t}\n}\n\n/* Remove acknowledged frames from the retransmission queue. If our packet\n * is before the ack sequence we can discard it as it's confirmed to have\n * arrived at the other end.\n */\nstatic int tcp_clean_rtx_queue(struct sock *sk, u32 prior_fack,\n\t\t\t       u32 prior_snd_una,\n\t\t\t       struct tcp_sacktag_state *sack, bool ece_ack)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tu64 first_ackt, last_ackt;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 prior_sacked = tp->sacked_out;\n\tu32 reord = tp->snd_nxt; /* lowest acked un-retx un-sacked seq */\n\tstruct sk_buff *skb, *next;\n\tbool fully_acked = true;\n\tlong sack_rtt_us = -1L;\n\tlong seq_rtt_us = -1L;\n\tlong ca_rtt_us = -1L;\n\tu32 pkts_acked = 0;\n\tu32 last_in_flight = 0;\n\tbool rtt_update;\n\tint flag = 0;\n\n\tfirst_ackt = 0;\n\n\tfor (skb = skb_rb_first(&sk->tcp_rtx_queue); skb; skb = next) {\n\t\tstruct tcp_skb_cb *scb = TCP_SKB_CB(skb);\n\t\tconst u32 start_seq = scb->seq;\n\t\tu8 sacked = scb->sacked;\n\t\tu32 acked_pcount;\n\n\t\t/* Determine how many packets and what bytes were acked, tso and else */\n\t\tif (after(scb->end_seq, tp->snd_una)) {\n\t\t\tif (tcp_skb_pcount(skb) == 1 ||\n\t\t\t    !after(tp->snd_una, scb->seq))\n\t\t\t\tbreak;\n\n\t\t\tacked_pcount = tcp_tso_acked(sk, skb);\n\t\t\tif (!acked_pcount)\n\t\t\t\tbreak;\n\t\t\tfully_acked = false;\n\t\t} else {\n\t\t\tacked_pcount = tcp_skb_pcount(skb);\n\t\t}\n\n\t\tif (unlikely(sacked & TCPCB_RETRANS)) {\n\t\t\tif (sacked & TCPCB_SACKED_RETRANS)\n\t\t\t\ttp->retrans_out -= acked_pcount;\n\t\t\tflag |= FLAG_RETRANS_DATA_ACKED;\n\t\t} else if (!(sacked & TCPCB_SACKED_ACKED)) {\n\t\t\tlast_ackt = tcp_skb_timestamp_us(skb);\n\t\t\tWARN_ON_ONCE(last_ackt == 0);\n\t\t\tif (!first_ackt)\n\t\t\t\tfirst_ackt = last_ackt;\n\n\t\t\tlast_in_flight = TCP_SKB_CB(skb)->tx.in_flight;\n\t\t\tif (before(start_seq, reord))\n\t\t\t\treord = start_seq;\n\t\t\tif (!after(scb->end_seq, tp->high_seq))\n\t\t\t\tflag |= FLAG_ORIG_SACK_ACKED;\n\t\t}\n\n\t\tif (sacked & TCPCB_SACKED_ACKED) {\n\t\t\ttp->sacked_out -= acked_pcount;\n\t\t} else if (tcp_is_sack(tp)) {\n\t\t\ttcp_count_delivered(tp, acked_pcount, ece_ack);\n\t\t\tif (!tcp_skb_spurious_retrans(tp, skb))\n\t\t\t\ttcp_rack_advance(tp, sacked, scb->end_seq,\n\t\t\t\t\t\t tcp_skb_timestamp_us(skb));\n\t\t}\n\t\tif (sacked & TCPCB_LOST)\n\t\t\ttp->lost_out -= acked_pcount;\n\n\t\ttp->packets_out -= acked_pcount;\n\t\tpkts_acked += acked_pcount;\n\t\ttcp_rate_skb_delivered(sk, skb, sack->rate);\n\n\t\t/* Initial outgoing SYN's get put onto the write_queue\n\t\t * just like anything else we transmit.  It is not\n\t\t * true data, and if we misinform our callers that\n\t\t * this ACK acks real data, we will erroneously exit\n\t\t * connection startup slow start one packet too\n\t\t * quickly.  This is severely frowned upon behavior.\n\t\t */\n\t\tif (likely(!(scb->tcp_flags & TCPHDR_SYN))) {\n\t\t\tflag |= FLAG_DATA_ACKED;\n\t\t} else {\n\t\t\tflag |= FLAG_SYN_ACKED;\n\t\t\ttp->retrans_stamp = 0;\n\t\t}\n\n\t\tif (!fully_acked)\n\t\t\tbreak;\n\n\t\ttcp_ack_tstamp(sk, skb, prior_snd_una);\n\n\t\tnext = skb_rb_next(skb);\n\t\tif (unlikely(skb == tp->retransmit_skb_hint))\n\t\t\ttp->retransmit_skb_hint = NULL;\n\t\tif (unlikely(skb == tp->lost_skb_hint))\n\t\t\ttp->lost_skb_hint = NULL;\n\t\ttcp_highest_sack_replace(sk, skb, next);\n\t\ttcp_rtx_queue_unlink_and_free(skb, sk);\n\t}\n\n\tif (!skb)\n\t\ttcp_chrono_stop(sk, TCP_CHRONO_BUSY);\n\n\tif (likely(between(tp->snd_up, prior_snd_una, tp->snd_una)))\n\t\ttp->snd_up = tp->snd_una;\n\n\tif (skb) {\n\t\ttcp_ack_tstamp(sk, skb, prior_snd_una);\n\t\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED)\n\t\t\tflag |= FLAG_SACK_RENEGING;\n\t}\n\n\tif (likely(first_ackt) && !(flag & FLAG_RETRANS_DATA_ACKED)) {\n\t\tseq_rtt_us = tcp_stamp_us_delta(tp->tcp_mstamp, first_ackt);\n\t\tca_rtt_us = tcp_stamp_us_delta(tp->tcp_mstamp, last_ackt);\n\n\t\tif (pkts_acked == 1 && last_in_flight < tp->mss_cache &&\n\t\t    last_in_flight && !prior_sacked && fully_acked &&\n\t\t    sack->rate->prior_delivered + 1 == tp->delivered &&\n\t\t    !(flag & (FLAG_CA_ALERT | FLAG_SYN_ACKED))) {\n\t\t\t/* Conservatively mark a delayed ACK. It's typically\n\t\t\t * from a lone runt packet over the round trip to\n\t\t\t * a receiver w/o out-of-order or CE events.\n\t\t\t */\n\t\t\tflag |= FLAG_ACK_MAYBE_DELAYED;\n\t\t}\n\t}\n\tif (sack->first_sackt) {\n\t\tsack_rtt_us = tcp_stamp_us_delta(tp->tcp_mstamp, sack->first_sackt);\n\t\tca_rtt_us = tcp_stamp_us_delta(tp->tcp_mstamp, sack->last_sackt);\n\t}\n\trtt_update = tcp_ack_update_rtt(sk, flag, seq_rtt_us, sack_rtt_us,\n\t\t\t\t\tca_rtt_us, sack->rate);\n\n\tif (flag & FLAG_ACKED) {\n\t\tflag |= FLAG_SET_XMIT_TIMER;  /* set TLP or RTO timer */\n\t\tif (unlikely(icsk->icsk_mtup.probe_size &&\n\t\t\t     !after(tp->mtu_probe.probe_seq_end, tp->snd_una))) {\n\t\t\ttcp_mtup_probe_success(sk);\n\t\t}\n\n\t\tif (tcp_is_reno(tp)) {\n\t\t\ttcp_remove_reno_sacks(sk, pkts_acked, ece_ack);\n\n\t\t\t/* If any of the cumulatively ACKed segments was\n\t\t\t * retransmitted, non-SACK case cannot confirm that\n\t\t\t * progress was due to original transmission due to\n\t\t\t * lack of TCPCB_SACKED_ACKED bits even if some of\n\t\t\t * the packets may have been never retransmitted.\n\t\t\t */\n\t\t\tif (flag & FLAG_RETRANS_DATA_ACKED)\n\t\t\t\tflag &= ~FLAG_ORIG_SACK_ACKED;\n\t\t} else {\n\t\t\tint delta;\n\n\t\t\t/* Non-retransmitted hole got filled? That's reordering */\n\t\t\tif (before(reord, prior_fack))\n\t\t\t\ttcp_check_sack_reordering(sk, reord, 0);\n\n\t\t\tdelta = prior_sacked - tp->sacked_out;\n\t\t\ttp->lost_cnt_hint -= min(tp->lost_cnt_hint, delta);\n\t\t}\n\t} else if (skb && rtt_update && sack_rtt_us >= 0 &&\n\t\t   sack_rtt_us > tcp_stamp_us_delta(tp->tcp_mstamp,\n\t\t\t\t\t\t    tcp_skb_timestamp_us(skb))) {\n\t\t/* Do not re-arm RTO if the sack RTT is measured from data sent\n\t\t * after when the head was last (re)transmitted. Otherwise the\n\t\t * timeout may continue to extend in loss recovery.\n\t\t */\n\t\tflag |= FLAG_SET_XMIT_TIMER;  /* set TLP or RTO timer */\n\t}\n\n\tif (icsk->icsk_ca_ops->pkts_acked) {\n\t\tstruct ack_sample sample = { .pkts_acked = pkts_acked,\n\t\t\t\t\t     .rtt_us = sack->rate->rtt_us,\n\t\t\t\t\t     .in_flight = last_in_flight };\n\n\t\ticsk->icsk_ca_ops->pkts_acked(sk, &sample);\n\t}\n\n#if FASTRETRANS_DEBUG > 0\n\tWARN_ON((int)tp->sacked_out < 0);\n\tWARN_ON((int)tp->lost_out < 0);\n\tWARN_ON((int)tp->retrans_out < 0);\n\tif (!tp->packets_out && tcp_is_sack(tp)) {\n\t\ticsk = inet_csk(sk);\n\t\tif (tp->lost_out) {\n\t\t\tpr_debug(\"Leak l=%u %d\\n\",\n\t\t\t\t tp->lost_out, icsk->icsk_ca_state);\n\t\t\ttp->lost_out = 0;\n\t\t}\n\t\tif (tp->sacked_out) {\n\t\t\tpr_debug(\"Leak s=%u %d\\n\",\n\t\t\t\t tp->sacked_out, icsk->icsk_ca_state);\n\t\t\ttp->sacked_out = 0;\n\t\t}\n\t\tif (tp->retrans_out) {\n\t\t\tpr_debug(\"Leak r=%u %d\\n\",\n\t\t\t\t tp->retrans_out, icsk->icsk_ca_state);\n\t\t\ttp->retrans_out = 0;\n\t\t}\n\t}\n#endif\n\treturn flag;\n}\n\nstatic void tcp_ack_probe(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct sk_buff *head = tcp_send_head(sk);\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Was it a usable window open? */\n\tif (!head)\n\t\treturn;\n\tif (!after(TCP_SKB_CB(head)->end_seq, tcp_wnd_end(tp))) {\n\t\ticsk->icsk_backoff = 0;\n\t\tinet_csk_clear_xmit_timer(sk, ICSK_TIME_PROBE0);\n\t\t/* Socket must be waked up by subsequent tcp_data_snd_check().\n\t\t * This function is not for random using!\n\t\t */\n\t} else {\n\t\tunsigned long when = tcp_probe0_when(sk, TCP_RTO_MAX);\n\n\t\ttcp_reset_xmit_timer(sk, ICSK_TIME_PROBE0,\n\t\t\t\t     when, TCP_RTO_MAX);\n\t}\n}\n\nstatic inline bool tcp_ack_is_dubious(const struct sock *sk, const int flag)\n{\n\treturn !(flag & FLAG_NOT_DUP) || (flag & FLAG_CA_ALERT) ||\n\t\tinet_csk(sk)->icsk_ca_state != TCP_CA_Open;\n}\n\n/* Decide wheather to run the increase function of congestion control. */\nstatic inline bool tcp_may_raise_cwnd(const struct sock *sk, const int flag)\n{\n\t/* If reordering is high then always grow cwnd whenever data is\n\t * delivered regardless of its ordering. Otherwise stay conservative\n\t * and only grow cwnd on in-order delivery (RFC5681). A stretched ACK w/\n\t * new SACK or ECE mark may first advance cwnd here and later reduce\n\t * cwnd in tcp_fastretrans_alert() based on more states.\n\t */\n\tif (tcp_sk(sk)->reordering > sock_net(sk)->ipv4.sysctl_tcp_reordering)\n\t\treturn flag & FLAG_FORWARD_PROGRESS;\n\n\treturn flag & FLAG_DATA_ACKED;\n}\n\n/* The \"ultimate\" congestion control function that aims to replace the rigid\n * cwnd increase and decrease control (tcp_cong_avoid,tcp_*cwnd_reduction).\n * It's called toward the end of processing an ACK with precise rate\n * information. All transmission or retransmission are delayed afterwards.\n */\nstatic void tcp_cong_control(struct sock *sk, u32 ack, u32 acked_sacked,\n\t\t\t     int flag, const struct rate_sample *rs)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (icsk->icsk_ca_ops->cong_control) {\n\t\ticsk->icsk_ca_ops->cong_control(sk, rs);\n\t\treturn;\n\t}\n\n\tif (tcp_in_cwnd_reduction(sk)) {\n\t\t/* Reduce cwnd if state mandates */\n\t\ttcp_cwnd_reduction(sk, acked_sacked, flag);\n\t} else if (tcp_may_raise_cwnd(sk, flag)) {\n\t\t/* Advance cwnd if state allows */\n\t\ttcp_cong_avoid(sk, ack, acked_sacked);\n\t}\n\ttcp_update_pacing_rate(sk);\n}\n\n/* Check that window update is acceptable.\n * The function assumes that snd_una<=ack<=snd_next.\n */\nstatic inline bool tcp_may_update_window(const struct tcp_sock *tp,\n\t\t\t\t\tconst u32 ack, const u32 ack_seq,\n\t\t\t\t\tconst u32 nwin)\n{\n\treturn\tafter(ack, tp->snd_una) ||\n\t\tafter(ack_seq, tp->snd_wl1) ||\n\t\t(ack_seq == tp->snd_wl1 && nwin > tp->snd_wnd);\n}\n\n/* If we update tp->snd_una, also update tp->bytes_acked */\nstatic void tcp_snd_una_update(struct tcp_sock *tp, u32 ack)\n{\n\tu32 delta = ack - tp->snd_una;\n\n\tsock_owned_by_me((struct sock *)tp);\n\ttp->bytes_acked += delta;\n\ttp->snd_una = ack;\n}\n\n/* If we update tp->rcv_nxt, also update tp->bytes_received */\nstatic void tcp_rcv_nxt_update(struct tcp_sock *tp, u32 seq)\n{\n\tu32 delta = seq - tp->rcv_nxt;\n\n\tsock_owned_by_me((struct sock *)tp);\n\ttp->bytes_received += delta;\n\tWRITE_ONCE(tp->rcv_nxt, seq);\n}\n\n/* Update our send window.\n *\n * Window update algorithm, described in RFC793/RFC1122 (used in linux-2.2\n * and in FreeBSD. NetBSD's one is even worse.) is wrong.\n */\nstatic int tcp_ack_update_window(struct sock *sk, const struct sk_buff *skb, u32 ack,\n\t\t\t\t u32 ack_seq)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint flag = 0;\n\tu32 nwin = ntohs(tcp_hdr(skb)->window);\n\n\tif (likely(!tcp_hdr(skb)->syn))\n\t\tnwin <<= tp->rx_opt.snd_wscale;\n\n\tif (tcp_may_update_window(tp, ack, ack_seq, nwin)) {\n\t\tflag |= FLAG_WIN_UPDATE;\n\t\ttcp_update_wl(tp, ack_seq);\n\n\t\tif (tp->snd_wnd != nwin) {\n\t\t\ttp->snd_wnd = nwin;\n\n\t\t\t/* Note, it is the only place, where\n\t\t\t * fast path is recovered for sending TCP.\n\t\t\t */\n\t\t\ttp->pred_flags = 0;\n\t\t\ttcp_fast_path_check(sk);\n\n\t\t\tif (!tcp_write_queue_empty(sk))\n\t\t\t\ttcp_slow_start_after_idle_check(sk);\n\n\t\t\tif (nwin > tp->max_window) {\n\t\t\t\ttp->max_window = nwin;\n\t\t\t\ttcp_sync_mss(sk, inet_csk(sk)->icsk_pmtu_cookie);\n\t\t\t}\n\t\t}\n\t}\n\n\ttcp_snd_una_update(tp, ack);\n\n\treturn flag;\n}\n\nstatic bool __tcp_oow_rate_limited(struct net *net, int mib_idx,\n\t\t\t\t   u32 *last_oow_ack_time)\n{\n\tif (*last_oow_ack_time) {\n\t\ts32 elapsed = (s32)(tcp_jiffies32 - *last_oow_ack_time);\n\n\t\tif (0 <= elapsed && elapsed < net->ipv4.sysctl_tcp_invalid_ratelimit) {\n\t\t\tNET_INC_STATS(net, mib_idx);\n\t\t\treturn true;\t/* rate-limited: don't send yet! */\n\t\t}\n\t}\n\n\t*last_oow_ack_time = tcp_jiffies32;\n\n\treturn false;\t/* not rate-limited: go ahead, send dupack now! */\n}\n\n/* Return true if we're currently rate-limiting out-of-window ACKs and\n * thus shouldn't send a dupack right now. We rate-limit dupacks in\n * response to out-of-window SYNs or ACKs to mitigate ACK loops or DoS\n * attacks that send repeated SYNs or ACKs for the same connection. To\n * do this, we do not send a duplicate SYNACK or ACK if the remote\n * endpoint is sending out-of-window SYNs or pure ACKs at a high rate.\n */\nbool tcp_oow_rate_limited(struct net *net, const struct sk_buff *skb,\n\t\t\t  int mib_idx, u32 *last_oow_ack_time)\n{\n\t/* Data packets without SYNs are not likely part of an ACK loop. */\n\tif ((TCP_SKB_CB(skb)->seq != TCP_SKB_CB(skb)->end_seq) &&\n\t    !tcp_hdr(skb)->syn)\n\t\treturn false;\n\n\treturn __tcp_oow_rate_limited(net, mib_idx, last_oow_ack_time);\n}\n\n/* RFC 5961 7 [ACK Throttling] */\nstatic void tcp_send_challenge_ack(struct sock *sk, const struct sk_buff *skb)\n{\n\t/* unprotected vars, we dont care of overwrites */\n\tstatic u32 challenge_timestamp;\n\tstatic unsigned int challenge_count;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tu32 count, now;\n\n\t/* First check our per-socket dupack rate limit. */\n\tif (__tcp_oow_rate_limited(net,\n\t\t\t\t   LINUX_MIB_TCPACKSKIPPEDCHALLENGE,\n\t\t\t\t   &tp->last_oow_ack_time))\n\t\treturn;\n\n\t/* Then check host-wide RFC 5961 rate limit. */\n\tnow = jiffies / HZ;\n\tif (now != challenge_timestamp) {\n\t\tu32 ack_limit = net->ipv4.sysctl_tcp_challenge_ack_limit;\n\t\tu32 half = (ack_limit + 1) >> 1;\n\n\t\tchallenge_timestamp = now;\n\t\tWRITE_ONCE(challenge_count, half + prandom_u32_max(ack_limit));\n\t}\n\tcount = READ_ONCE(challenge_count);\n\tif (count > 0) {\n\t\tWRITE_ONCE(challenge_count, count - 1);\n\t\tNET_INC_STATS(net, LINUX_MIB_TCPCHALLENGEACK);\n\t\ttcp_send_ack(sk);\n\t}\n}\n\nstatic void tcp_store_ts_recent(struct tcp_sock *tp)\n{\n\ttp->rx_opt.ts_recent = tp->rx_opt.rcv_tsval;\n\ttp->rx_opt.ts_recent_stamp = ktime_get_seconds();\n}\n\nstatic void tcp_replace_ts_recent(struct tcp_sock *tp, u32 seq)\n{\n\tif (tp->rx_opt.saw_tstamp && !after(seq, tp->rcv_wup)) {\n\t\t/* PAWS bug workaround wrt. ACK frames, the PAWS discard\n\t\t * extra check below makes sure this can only happen\n\t\t * for pure ACK frames.  -DaveM\n\t\t *\n\t\t * Not only, also it occurs for expired timestamps.\n\t\t */\n\n\t\tif (tcp_paws_check(&tp->rx_opt, 0))\n\t\t\ttcp_store_ts_recent(tp);\n\t}\n}\n\n/* This routine deals with acks during a TLP episode and ends an episode by\n * resetting tlp_high_seq. Ref: TLP algorithm in draft-ietf-tcpm-rack\n */\nstatic void tcp_process_tlp_ack(struct sock *sk, u32 ack, int flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (before(ack, tp->tlp_high_seq))\n\t\treturn;\n\n\tif (!tp->tlp_retrans) {\n\t\t/* TLP of new data has been acknowledged */\n\t\ttp->tlp_high_seq = 0;\n\t} else if (flag & FLAG_DSACKING_ACK) {\n\t\t/* This DSACK means original and TLP probe arrived; no loss */\n\t\ttp->tlp_high_seq = 0;\n\t} else if (after(ack, tp->tlp_high_seq)) {\n\t\t/* ACK advances: there was a loss, so reduce cwnd. Reset\n\t\t * tlp_high_seq in tcp_init_cwnd_reduction()\n\t\t */\n\t\ttcp_init_cwnd_reduction(sk);\n\t\ttcp_set_ca_state(sk, TCP_CA_CWR);\n\t\ttcp_end_cwnd_reduction(sk);\n\t\ttcp_try_keep_open(sk);\n\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\tLINUX_MIB_TCPLOSSPROBERECOVERY);\n\t} else if (!(flag & (FLAG_SND_UNA_ADVANCED |\n\t\t\t     FLAG_NOT_DUP | FLAG_DATA_SACKED))) {\n\t\t/* Pure dupack: original and TLP probe arrived; no loss */\n\t\ttp->tlp_high_seq = 0;\n\t}\n}\n\nstatic inline void tcp_in_ack_event(struct sock *sk, u32 flags)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (icsk->icsk_ca_ops->in_ack_event)\n\t\ticsk->icsk_ca_ops->in_ack_event(sk, flags);\n}\n\n/* Congestion control has updated the cwnd already. So if we're in\n * loss recovery then now we do any new sends (for FRTO) or\n * retransmits (for CA_Loss or CA_recovery) that make sense.\n */\nstatic void tcp_xmit_recovery(struct sock *sk, int rexmit)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (rexmit == REXMIT_NONE || sk->sk_state == TCP_SYN_SENT)\n\t\treturn;\n\n\tif (unlikely(rexmit == REXMIT_NEW)) {\n\t\t__tcp_push_pending_frames(sk, tcp_current_mss(sk),\n\t\t\t\t\t  TCP_NAGLE_OFF);\n\t\tif (after(tp->snd_nxt, tp->high_seq))\n\t\t\treturn;\n\t\ttp->frto = 0;\n\t}\n\ttcp_xmit_retransmit_queue(sk);\n}\n\n/* Returns the number of packets newly acked or sacked by the current ACK */\nstatic u32 tcp_newly_delivered(struct sock *sk, u32 prior_delivered, int flag)\n{\n\tconst struct net *net = sock_net(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 delivered;\n\n\tdelivered = tp->delivered - prior_delivered;\n\tNET_ADD_STATS(net, LINUX_MIB_TCPDELIVERED, delivered);\n\tif (flag & FLAG_ECE)\n\t\tNET_ADD_STATS(net, LINUX_MIB_TCPDELIVEREDCE, delivered);\n\n\treturn delivered;\n}\n\n/* This routine deals with incoming acks, but not outgoing ones. */\nstatic int tcp_ack(struct sock *sk, const struct sk_buff *skb, int flag)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_sacktag_state sack_state;\n\tstruct rate_sample rs = { .prior_delivered = 0 };\n\tu32 prior_snd_una = tp->snd_una;\n\tbool is_sack_reneg = tp->is_sack_reneg;\n\tu32 ack_seq = TCP_SKB_CB(skb)->seq;\n\tu32 ack = TCP_SKB_CB(skb)->ack_seq;\n\tint num_dupack = 0;\n\tint prior_packets = tp->packets_out;\n\tu32 delivered = tp->delivered;\n\tu32 lost = tp->lost;\n\tint rexmit = REXMIT_NONE; /* Flag to (re)transmit to recover losses */\n\tu32 prior_fack;\n\n\tsack_state.first_sackt = 0;\n\tsack_state.rate = &rs;\n\tsack_state.sack_delivered = 0;\n\n\t/* We very likely will need to access rtx queue. */\n\tprefetch(sk->tcp_rtx_queue.rb_node);\n\n\t/* If the ack is older than previous acks\n\t * then we can probably ignore it.\n\t */\n\tif (before(ack, prior_snd_una)) {\n\t\t/* RFC 5961 5.2 [Blind Data Injection Attack].[Mitigation] */\n\t\tif (before(ack, prior_snd_una - tp->max_window)) {\n\t\t\tif (!(flag & FLAG_NO_CHALLENGE_ACK))\n\t\t\t\ttcp_send_challenge_ack(sk, skb);\n\t\t\treturn -1;\n\t\t}\n\t\tgoto old_ack;\n\t}\n\n\t/* If the ack includes data we haven't sent yet, discard\n\t * this segment (RFC793 Section 3.9).\n\t */\n\tif (after(ack, tp->snd_nxt))\n\t\treturn -1;\n\n\tif (after(ack, prior_snd_una)) {\n\t\tflag |= FLAG_SND_UNA_ADVANCED;\n\t\ticsk->icsk_retransmits = 0;\n\n#if IS_ENABLED(CONFIG_TLS_DEVICE)\n\t\tif (static_branch_unlikely(&clean_acked_data_enabled.key))\n\t\t\tif (icsk->icsk_clean_acked)\n\t\t\t\ticsk->icsk_clean_acked(sk, ack);\n#endif\n\t}\n\n\tprior_fack = tcp_is_sack(tp) ? tcp_highest_sack_seq(tp) : tp->snd_una;\n\trs.prior_in_flight = tcp_packets_in_flight(tp);\n\n\t/* ts_recent update must be made after we are sure that the packet\n\t * is in window.\n\t */\n\tif (flag & FLAG_UPDATE_TS_RECENT)\n\t\ttcp_replace_ts_recent(tp, TCP_SKB_CB(skb)->seq);\n\n\tif ((flag & (FLAG_SLOWPATH | FLAG_SND_UNA_ADVANCED)) ==\n\t    FLAG_SND_UNA_ADVANCED) {\n\t\t/* Window is constant, pure forward advance.\n\t\t * No more checks are required.\n\t\t * Note, we use the fact that SND.UNA>=SND.WL2.\n\t\t */\n\t\ttcp_update_wl(tp, ack_seq);\n\t\ttcp_snd_una_update(tp, ack);\n\t\tflag |= FLAG_WIN_UPDATE;\n\n\t\ttcp_in_ack_event(sk, CA_ACK_WIN_UPDATE);\n\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPHPACKS);\n\t} else {\n\t\tu32 ack_ev_flags = CA_ACK_SLOWPATH;\n\n\t\tif (ack_seq != TCP_SKB_CB(skb)->end_seq)\n\t\t\tflag |= FLAG_DATA;\n\t\telse\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPPUREACKS);\n\n\t\tflag |= tcp_ack_update_window(sk, skb, ack, ack_seq);\n\n\t\tif (TCP_SKB_CB(skb)->sacked)\n\t\t\tflag |= tcp_sacktag_write_queue(sk, skb, prior_snd_una,\n\t\t\t\t\t\t\t&sack_state);\n\n\t\tif (tcp_ecn_rcv_ecn_echo(tp, tcp_hdr(skb))) {\n\t\t\tflag |= FLAG_ECE;\n\t\t\tack_ev_flags |= CA_ACK_ECE;\n\t\t}\n\n\t\tif (sack_state.sack_delivered)\n\t\t\ttcp_count_delivered(tp, sack_state.sack_delivered,\n\t\t\t\t\t    flag & FLAG_ECE);\n\n\t\tif (flag & FLAG_WIN_UPDATE)\n\t\t\tack_ev_flags |= CA_ACK_WIN_UPDATE;\n\n\t\ttcp_in_ack_event(sk, ack_ev_flags);\n\t}\n\n\t/* This is a deviation from RFC3168 since it states that:\n\t * \"When the TCP data sender is ready to set the CWR bit after reducing\n\t * the congestion window, it SHOULD set the CWR bit only on the first\n\t * new data packet that it transmits.\"\n\t * We accept CWR on pure ACKs to be more robust\n\t * with widely-deployed TCP implementations that do this.\n\t */\n\ttcp_ecn_accept_cwr(sk, skb);\n\n\t/* We passed data and got it acked, remove any soft error\n\t * log. Something worked...\n\t */\n\tsk->sk_err_soft = 0;\n\ticsk->icsk_probes_out = 0;\n\ttp->rcv_tstamp = tcp_jiffies32;\n\tif (!prior_packets)\n\t\tgoto no_queue;\n\n\t/* See if we can take anything off of the retransmit queue. */\n\tflag |= tcp_clean_rtx_queue(sk, prior_fack, prior_snd_una, &sack_state,\n\t\t\t\t    flag & FLAG_ECE);\n\n\ttcp_rack_update_reo_wnd(sk, &rs);\n\n\tif (tp->tlp_high_seq)\n\t\ttcp_process_tlp_ack(sk, ack, flag);\n\t/* If needed, reset TLP/RTO timer; RACK may later override this. */\n\tif (flag & FLAG_SET_XMIT_TIMER)\n\t\ttcp_set_xmit_timer(sk);\n\n\tif (tcp_ack_is_dubious(sk, flag)) {\n\t\tif (!(flag & (FLAG_SND_UNA_ADVANCED | FLAG_NOT_DUP))) {\n\t\t\tnum_dupack = 1;\n\t\t\t/* Consider if pure acks were aggregated in tcp_add_backlog() */\n\t\t\tif (!(flag & FLAG_DATA))\n\t\t\t\tnum_dupack = max_t(u16, 1, skb_shinfo(skb)->gso_segs);\n\t\t}\n\t\ttcp_fastretrans_alert(sk, prior_snd_una, num_dupack, &flag,\n\t\t\t\t      &rexmit);\n\t}\n\n\tif ((flag & FLAG_FORWARD_PROGRESS) || !(flag & FLAG_NOT_DUP))\n\t\tsk_dst_confirm(sk);\n\n\tdelivered = tcp_newly_delivered(sk, delivered, flag);\n\tlost = tp->lost - lost;\t\t\t/* freshly marked lost */\n\trs.is_ack_delayed = !!(flag & FLAG_ACK_MAYBE_DELAYED);\n\ttcp_rate_gen(sk, delivered, lost, is_sack_reneg, sack_state.rate);\n\ttcp_cong_control(sk, ack, delivered, flag, sack_state.rate);\n\ttcp_xmit_recovery(sk, rexmit);\n\treturn 1;\n\nno_queue:\n\t/* If data was DSACKed, see if we can undo a cwnd reduction. */\n\tif (flag & FLAG_DSACKING_ACK) {\n\t\ttcp_fastretrans_alert(sk, prior_snd_una, num_dupack, &flag,\n\t\t\t\t      &rexmit);\n\t\ttcp_newly_delivered(sk, delivered, flag);\n\t}\n\t/* If this ack opens up a zero window, clear backoff.  It was\n\t * being used to time the probes, and is probably far higher than\n\t * it needs to be for normal retransmission.\n\t */\n\ttcp_ack_probe(sk);\n\n\tif (tp->tlp_high_seq)\n\t\ttcp_process_tlp_ack(sk, ack, flag);\n\treturn 1;\n\nold_ack:\n\t/* If data was SACKed, tag it and see if we should send more data.\n\t * If data was DSACKed, see if we can undo a cwnd reduction.\n\t */\n\tif (TCP_SKB_CB(skb)->sacked) {\n\t\tflag |= tcp_sacktag_write_queue(sk, skb, prior_snd_una,\n\t\t\t\t\t\t&sack_state);\n\t\ttcp_fastretrans_alert(sk, prior_snd_una, num_dupack, &flag,\n\t\t\t\t      &rexmit);\n\t\ttcp_newly_delivered(sk, delivered, flag);\n\t\ttcp_xmit_recovery(sk, rexmit);\n\t}\n\n\treturn 0;\n}\n\nstatic void tcp_parse_fastopen_option(int len, const unsigned char *cookie,\n\t\t\t\t      bool syn, struct tcp_fastopen_cookie *foc,\n\t\t\t\t      bool exp_opt)\n{\n\t/* Valid only in SYN or SYN-ACK with an even length.  */\n\tif (!foc || !syn || len < 0 || (len & 1))\n\t\treturn;\n\n\tif (len >= TCP_FASTOPEN_COOKIE_MIN &&\n\t    len <= TCP_FASTOPEN_COOKIE_MAX)\n\t\tmemcpy(foc->val, cookie, len);\n\telse if (len != 0)\n\t\tlen = -1;\n\tfoc->len = len;\n\tfoc->exp = exp_opt;\n}\n\nstatic bool smc_parse_options(const struct tcphdr *th,\n\t\t\t      struct tcp_options_received *opt_rx,\n\t\t\t      const unsigned char *ptr,\n\t\t\t      int opsize)\n{\n#if IS_ENABLED(CONFIG_SMC)\n\tif (static_branch_unlikely(&tcp_have_smc)) {\n\t\tif (th->syn && !(opsize & 1) &&\n\t\t    opsize >= TCPOLEN_EXP_SMC_BASE &&\n\t\t    get_unaligned_be32(ptr) == TCPOPT_SMC_MAGIC) {\n\t\t\topt_rx->smc_ok = 1;\n\t\t\treturn true;\n\t\t}\n\t}\n#endif\n\treturn false;\n}\n\n/* Try to parse the MSS option from the TCP header. Return 0 on failure, clamped\n * value on success.\n */\nstatic u16 tcp_parse_mss_option(const struct tcphdr *th, u16 user_mss)\n{\n\tconst unsigned char *ptr = (const unsigned char *)(th + 1);\n\tint length = (th->doff * 4) - sizeof(struct tcphdr);\n\tu16 mss = 0;\n\n\twhile (length > 0) {\n\t\tint opcode = *ptr++;\n\t\tint opsize;\n\n\t\tswitch (opcode) {\n\t\tcase TCPOPT_EOL:\n\t\t\treturn mss;\n\t\tcase TCPOPT_NOP:\t/* Ref: RFC 793 section 3.1 */\n\t\t\tlength--;\n\t\t\tcontinue;\n\t\tdefault:\n\t\t\tif (length < 2)\n\t\t\t\treturn mss;\n\t\t\topsize = *ptr++;\n\t\t\tif (opsize < 2) /* \"silly options\" */\n\t\t\t\treturn mss;\n\t\t\tif (opsize > length)\n\t\t\t\treturn mss;\t/* fail on partial options */\n\t\t\tif (opcode == TCPOPT_MSS && opsize == TCPOLEN_MSS) {\n\t\t\t\tu16 in_mss = get_unaligned_be16(ptr);\n\n\t\t\t\tif (in_mss) {\n\t\t\t\t\tif (user_mss && user_mss < in_mss)\n\t\t\t\t\t\tin_mss = user_mss;\n\t\t\t\t\tmss = in_mss;\n\t\t\t\t}\n\t\t\t}\n\t\t\tptr += opsize - 2;\n\t\t\tlength -= opsize;\n\t\t}\n\t}\n\treturn mss;\n}\n\n/* Look for tcp options. Normally only called on SYN and SYNACK packets.\n * But, this can also be called on packets in the established flow when\n * the fast version below fails.\n */\nvoid tcp_parse_options(const struct net *net,\n\t\t       const struct sk_buff *skb,\n\t\t       struct tcp_options_received *opt_rx, int estab,\n\t\t       struct tcp_fastopen_cookie *foc)\n{\n\tconst unsigned char *ptr;\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tint length = (th->doff * 4) - sizeof(struct tcphdr);\n\n\tptr = (const unsigned char *)(th + 1);\n\topt_rx->saw_tstamp = 0;\n\topt_rx->saw_unknown = 0;\n\n\twhile (length > 0) {\n\t\tint opcode = *ptr++;\n\t\tint opsize;\n\n\t\tswitch (opcode) {\n\t\tcase TCPOPT_EOL:\n\t\t\treturn;\n\t\tcase TCPOPT_NOP:\t/* Ref: RFC 793 section 3.1 */\n\t\t\tlength--;\n\t\t\tcontinue;\n\t\tdefault:\n\t\t\tif (length < 2)\n\t\t\t\treturn;\n\t\t\topsize = *ptr++;\n\t\t\tif (opsize < 2) /* \"silly options\" */\n\t\t\t\treturn;\n\t\t\tif (opsize > length)\n\t\t\t\treturn;\t/* don't parse partial options */\n\t\t\tswitch (opcode) {\n\t\t\tcase TCPOPT_MSS:\n\t\t\t\tif (opsize == TCPOLEN_MSS && th->syn && !estab) {\n\t\t\t\t\tu16 in_mss = get_unaligned_be16(ptr);\n\t\t\t\t\tif (in_mss) {\n\t\t\t\t\t\tif (opt_rx->user_mss &&\n\t\t\t\t\t\t    opt_rx->user_mss < in_mss)\n\t\t\t\t\t\t\tin_mss = opt_rx->user_mss;\n\t\t\t\t\t\topt_rx->mss_clamp = in_mss;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase TCPOPT_WINDOW:\n\t\t\t\tif (opsize == TCPOLEN_WINDOW && th->syn &&\n\t\t\t\t    !estab && net->ipv4.sysctl_tcp_window_scaling) {\n\t\t\t\t\t__u8 snd_wscale = *(__u8 *)ptr;\n\t\t\t\t\topt_rx->wscale_ok = 1;\n\t\t\t\t\tif (snd_wscale > TCP_MAX_WSCALE) {\n\t\t\t\t\t\tnet_info_ratelimited(\"%s: Illegal window scaling value %d > %u received\\n\",\n\t\t\t\t\t\t\t\t     __func__,\n\t\t\t\t\t\t\t\t     snd_wscale,\n\t\t\t\t\t\t\t\t     TCP_MAX_WSCALE);\n\t\t\t\t\t\tsnd_wscale = TCP_MAX_WSCALE;\n\t\t\t\t\t}\n\t\t\t\t\topt_rx->snd_wscale = snd_wscale;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase TCPOPT_TIMESTAMP:\n\t\t\t\tif ((opsize == TCPOLEN_TIMESTAMP) &&\n\t\t\t\t    ((estab && opt_rx->tstamp_ok) ||\n\t\t\t\t     (!estab && net->ipv4.sysctl_tcp_timestamps))) {\n\t\t\t\t\topt_rx->saw_tstamp = 1;\n\t\t\t\t\topt_rx->rcv_tsval = get_unaligned_be32(ptr);\n\t\t\t\t\topt_rx->rcv_tsecr = get_unaligned_be32(ptr + 4);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase TCPOPT_SACK_PERM:\n\t\t\t\tif (opsize == TCPOLEN_SACK_PERM && th->syn &&\n\t\t\t\t    !estab && net->ipv4.sysctl_tcp_sack) {\n\t\t\t\t\topt_rx->sack_ok = TCP_SACK_SEEN;\n\t\t\t\t\ttcp_sack_reset(opt_rx);\n\t\t\t\t}\n\t\t\t\tbreak;\n\n\t\t\tcase TCPOPT_SACK:\n\t\t\t\tif ((opsize >= (TCPOLEN_SACK_BASE + TCPOLEN_SACK_PERBLOCK)) &&\n\t\t\t\t   !((opsize - TCPOLEN_SACK_BASE) % TCPOLEN_SACK_PERBLOCK) &&\n\t\t\t\t   opt_rx->sack_ok) {\n\t\t\t\t\tTCP_SKB_CB(skb)->sacked = (ptr - 2) - (unsigned char *)th;\n\t\t\t\t}\n\t\t\t\tbreak;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\tcase TCPOPT_MD5SIG:\n\t\t\t\t/*\n\t\t\t\t * The MD5 Hash has already been\n\t\t\t\t * checked (see tcp_v{4,6}_do_rcv()).\n\t\t\t\t */\n\t\t\t\tbreak;\n#endif\n\t\t\tcase TCPOPT_FASTOPEN:\n\t\t\t\ttcp_parse_fastopen_option(\n\t\t\t\t\topsize - TCPOLEN_FASTOPEN_BASE,\n\t\t\t\t\tptr, th->syn, foc, false);\n\t\t\t\tbreak;\n\n\t\t\tcase TCPOPT_EXP:\n\t\t\t\t/* Fast Open option shares code 254 using a\n\t\t\t\t * 16 bits magic number.\n\t\t\t\t */\n\t\t\t\tif (opsize >= TCPOLEN_EXP_FASTOPEN_BASE &&\n\t\t\t\t    get_unaligned_be16(ptr) ==\n\t\t\t\t    TCPOPT_FASTOPEN_MAGIC) {\n\t\t\t\t\ttcp_parse_fastopen_option(opsize -\n\t\t\t\t\t\tTCPOLEN_EXP_FASTOPEN_BASE,\n\t\t\t\t\t\tptr + 2, th->syn, foc, true);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tif (smc_parse_options(th, opt_rx, ptr, opsize))\n\t\t\t\t\tbreak;\n\n\t\t\t\topt_rx->saw_unknown = 1;\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\topt_rx->saw_unknown = 1;\n\t\t\t}\n\t\t\tptr += opsize-2;\n\t\t\tlength -= opsize;\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(tcp_parse_options);\n\nstatic bool tcp_parse_aligned_timestamp(struct tcp_sock *tp, const struct tcphdr *th)\n{\n\tconst __be32 *ptr = (const __be32 *)(th + 1);\n\n\tif (*ptr == htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16)\n\t\t\t  | (TCPOPT_TIMESTAMP << 8) | TCPOLEN_TIMESTAMP)) {\n\t\ttp->rx_opt.saw_tstamp = 1;\n\t\t++ptr;\n\t\ttp->rx_opt.rcv_tsval = ntohl(*ptr);\n\t\t++ptr;\n\t\tif (*ptr)\n\t\t\ttp->rx_opt.rcv_tsecr = ntohl(*ptr) - tp->tsoffset;\n\t\telse\n\t\t\ttp->rx_opt.rcv_tsecr = 0;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/* Fast parse options. This hopes to only see timestamps.\n * If it is wrong it falls back on tcp_parse_options().\n */\nstatic bool tcp_fast_parse_options(const struct net *net,\n\t\t\t\t   const struct sk_buff *skb,\n\t\t\t\t   const struct tcphdr *th, struct tcp_sock *tp)\n{\n\t/* In the spirit of fast parsing, compare doff directly to constant\n\t * values.  Because equality is used, short doff can be ignored here.\n\t */\n\tif (th->doff == (sizeof(*th) / 4)) {\n\t\ttp->rx_opt.saw_tstamp = 0;\n\t\treturn false;\n\t} else if (tp->rx_opt.tstamp_ok &&\n\t\t   th->doff == ((sizeof(*th) + TCPOLEN_TSTAMP_ALIGNED) / 4)) {\n\t\tif (tcp_parse_aligned_timestamp(tp, th))\n\t\t\treturn true;\n\t}\n\n\ttcp_parse_options(net, skb, &tp->rx_opt, 1, NULL);\n\tif (tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr)\n\t\ttp->rx_opt.rcv_tsecr -= tp->tsoffset;\n\n\treturn true;\n}\n\n#ifdef CONFIG_TCP_MD5SIG\n/*\n * Parse MD5 Signature option\n */\nconst u8 *tcp_parse_md5sig_option(const struct tcphdr *th)\n{\n\tint length = (th->doff << 2) - sizeof(*th);\n\tconst u8 *ptr = (const u8 *)(th + 1);\n\n\t/* If not enough data remaining, we can short cut */\n\twhile (length >= TCPOLEN_MD5SIG) {\n\t\tint opcode = *ptr++;\n\t\tint opsize;\n\n\t\tswitch (opcode) {\n\t\tcase TCPOPT_EOL:\n\t\t\treturn NULL;\n\t\tcase TCPOPT_NOP:\n\t\t\tlength--;\n\t\t\tcontinue;\n\t\tdefault:\n\t\t\topsize = *ptr++;\n\t\t\tif (opsize < 2 || opsize > length)\n\t\t\t\treturn NULL;\n\t\t\tif (opcode == TCPOPT_MD5SIG)\n\t\t\t\treturn opsize == TCPOLEN_MD5SIG ? ptr : NULL;\n\t\t}\n\t\tptr += opsize - 2;\n\t\tlength -= opsize;\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(tcp_parse_md5sig_option);\n#endif\n\n/* Sorry, PAWS as specified is broken wrt. pure-ACKs -DaveM\n *\n * It is not fatal. If this ACK does _not_ change critical state (seqs, window)\n * it can pass through stack. So, the following predicate verifies that\n * this segment is not used for anything but congestion avoidance or\n * fast retransmit. Moreover, we even are able to eliminate most of such\n * second order effects, if we apply some small \"replay\" window (~RTO)\n * to timestamp space.\n *\n * All these measures still do not guarantee that we reject wrapped ACKs\n * on networks with high bandwidth, when sequence space is recycled fastly,\n * but it guarantees that such events will be very rare and do not affect\n * connection seriously. This doesn't look nice, but alas, PAWS is really\n * buggy extension.\n *\n * [ Later note. Even worse! It is buggy for segments _with_ data. RFC\n * states that events when retransmit arrives after original data are rare.\n * It is a blatant lie. VJ forgot about fast retransmit! 8)8) It is\n * the biggest problem on large power networks even with minor reordering.\n * OK, let's give it small replay window. If peer clock is even 1hz, it is safe\n * up to bandwidth of 18Gigabit/sec. 8) ]\n */\n\nstatic int tcp_disordered_ack(const struct sock *sk, const struct sk_buff *skb)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tu32 seq = TCP_SKB_CB(skb)->seq;\n\tu32 ack = TCP_SKB_CB(skb)->ack_seq;\n\n\treturn (/* 1. Pure ACK with correct sequence number. */\n\t\t(th->ack && seq == TCP_SKB_CB(skb)->end_seq && seq == tp->rcv_nxt) &&\n\n\t\t/* 2. ... and duplicate ACK. */\n\t\tack == tp->snd_una &&\n\n\t\t/* 3. ... and does not update window. */\n\t\t!tcp_may_update_window(tp, ack, seq, ntohs(th->window) << tp->rx_opt.snd_wscale) &&\n\n\t\t/* 4. ... and sits in replay window. */\n\t\t(s32)(tp->rx_opt.ts_recent - tp->rx_opt.rcv_tsval) <= (inet_csk(sk)->icsk_rto * 1024) / HZ);\n}\n\nstatic inline bool tcp_paws_discard(const struct sock *sk,\n\t\t\t\t   const struct sk_buff *skb)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\treturn !tcp_paws_check(&tp->rx_opt, TCP_PAWS_WINDOW) &&\n\t       !tcp_disordered_ack(sk, skb);\n}\n\n/* Check segment sequence number for validity.\n *\n * Segment controls are considered valid, if the segment\n * fits to the window after truncation to the window. Acceptability\n * of data (and SYN, FIN, of course) is checked separately.\n * See tcp_data_queue(), for example.\n *\n * Also, controls (RST is main one) are accepted using RCV.WUP instead\n * of RCV.NXT. Peer still did not advance his SND.UNA when we\n * delayed ACK, so that hisSND.UNA<=ourRCV.WUP.\n * (borrowed from freebsd)\n */\n\nstatic inline bool tcp_sequence(const struct tcp_sock *tp, u32 seq, u32 end_seq)\n{\n\treturn\t!before(end_seq, tp->rcv_wup) &&\n\t\t!after(seq, tp->rcv_nxt + tcp_receive_window(tp));\n}\n\n/* When we get a reset we do this. */\nvoid tcp_reset(struct sock *sk)\n{\n\ttrace_tcp_receive_reset(sk);\n\n\t/* We want the right error as BSD sees it (and indeed as we do). */\n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_SENT:\n\t\tsk->sk_err = ECONNREFUSED;\n\t\tbreak;\n\tcase TCP_CLOSE_WAIT:\n\t\tsk->sk_err = EPIPE;\n\t\tbreak;\n\tcase TCP_CLOSE:\n\t\treturn;\n\tdefault:\n\t\tsk->sk_err = ECONNRESET;\n\t}\n\t/* This barrier is coupled with smp_rmb() in tcp_poll() */\n\tsmp_wmb();\n\n\ttcp_write_queue_purge(sk);\n\ttcp_done(sk);\n\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk->sk_error_report(sk);\n}\n\n/*\n * \tProcess the FIN bit. This now behaves as it is supposed to work\n *\tand the FIN takes effect when it is validly part of sequence\n *\tspace. Not before when we get holes.\n *\n *\tIf we are ESTABLISHED, a received fin moves us to CLOSE-WAIT\n *\t(and thence onto LAST-ACK and finally, CLOSE, we never enter\n *\tTIME-WAIT)\n *\n *\tIf we are in FINWAIT-1, a received FIN indicates simultaneous\n *\tclose and we go into CLOSING (and later onto TIME-WAIT)\n *\n *\tIf we are in FINWAIT-2, a received FIN moves us to TIME-WAIT.\n */\nvoid tcp_fin(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tinet_csk_schedule_ack(sk);\n\n\tsk->sk_shutdown |= RCV_SHUTDOWN;\n\tsock_set_flag(sk, SOCK_DONE);\n\n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_RECV:\n\tcase TCP_ESTABLISHED:\n\t\t/* Move to CLOSE_WAIT */\n\t\ttcp_set_state(sk, TCP_CLOSE_WAIT);\n\t\tinet_csk_enter_pingpong_mode(sk);\n\t\tbreak;\n\n\tcase TCP_CLOSE_WAIT:\n\tcase TCP_CLOSING:\n\t\t/* Received a retransmission of the FIN, do\n\t\t * nothing.\n\t\t */\n\t\tbreak;\n\tcase TCP_LAST_ACK:\n\t\t/* RFC793: Remain in the LAST-ACK state. */\n\t\tbreak;\n\n\tcase TCP_FIN_WAIT1:\n\t\t/* This case occurs when a simultaneous close\n\t\t * happens, we must ack the received FIN and\n\t\t * enter the CLOSING state.\n\t\t */\n\t\ttcp_send_ack(sk);\n\t\ttcp_set_state(sk, TCP_CLOSING);\n\t\tbreak;\n\tcase TCP_FIN_WAIT2:\n\t\t/* Received a FIN -- send ACK and enter TIME_WAIT. */\n\t\ttcp_send_ack(sk);\n\t\ttcp_time_wait(sk, TCP_TIME_WAIT, 0);\n\t\tbreak;\n\tdefault:\n\t\t/* Only TCP_LISTEN and TCP_CLOSE are left, in these\n\t\t * cases we should never reach this piece of code.\n\t\t */\n\t\tpr_err(\"%s: Impossible, sk->sk_state=%d\\n\",\n\t\t       __func__, sk->sk_state);\n\t\tbreak;\n\t}\n\n\t/* It _is_ possible, that we have something out-of-order _after_ FIN.\n\t * Probably, we should reset in this case. For now drop them.\n\t */\n\tskb_rbtree_purge(&tp->out_of_order_queue);\n\tif (tcp_is_sack(tp))\n\t\ttcp_sack_reset(&tp->rx_opt);\n\tsk_mem_reclaim(sk);\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tsk->sk_state_change(sk);\n\n\t\t/* Do not send POLL_HUP for half duplex close. */\n\t\tif (sk->sk_shutdown == SHUTDOWN_MASK ||\n\t\t    sk->sk_state == TCP_CLOSE)\n\t\t\tsk_wake_async(sk, SOCK_WAKE_WAITD, POLL_HUP);\n\t\telse\n\t\t\tsk_wake_async(sk, SOCK_WAKE_WAITD, POLL_IN);\n\t}\n}\n\nstatic inline bool tcp_sack_extend(struct tcp_sack_block *sp, u32 seq,\n\t\t\t\t  u32 end_seq)\n{\n\tif (!after(seq, sp->end_seq) && !after(sp->start_seq, end_seq)) {\n\t\tif (before(seq, sp->start_seq))\n\t\t\tsp->start_seq = seq;\n\t\tif (after(end_seq, sp->end_seq))\n\t\t\tsp->end_seq = end_seq;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void tcp_dsack_set(struct sock *sk, u32 seq, u32 end_seq)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tcp_is_sack(tp) && sock_net(sk)->ipv4.sysctl_tcp_dsack) {\n\t\tint mib_idx;\n\n\t\tif (before(seq, tp->rcv_nxt))\n\t\t\tmib_idx = LINUX_MIB_TCPDSACKOLDSENT;\n\t\telse\n\t\t\tmib_idx = LINUX_MIB_TCPDSACKOFOSENT;\n\n\t\tNET_INC_STATS(sock_net(sk), mib_idx);\n\n\t\ttp->rx_opt.dsack = 1;\n\t\ttp->duplicate_sack[0].start_seq = seq;\n\t\ttp->duplicate_sack[0].end_seq = end_seq;\n\t}\n}\n\nstatic void tcp_dsack_extend(struct sock *sk, u32 seq, u32 end_seq)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (!tp->rx_opt.dsack)\n\t\ttcp_dsack_set(sk, seq, end_seq);\n\telse\n\t\ttcp_sack_extend(tp->duplicate_sack, seq, end_seq);\n}\n\nstatic void tcp_rcv_spurious_retrans(struct sock *sk, const struct sk_buff *skb)\n{\n\t/* When the ACK path fails or drops most ACKs, the sender would\n\t * timeout and spuriously retransmit the same segment repeatedly.\n\t * The receiver remembers and reflects via DSACKs. Leverage the\n\t * DSACK state and change the txhash to re-route speculatively.\n\t */\n\tif (TCP_SKB_CB(skb)->seq == tcp_sk(sk)->duplicate_sack[0].start_seq) {\n\t\tsk_rethink_txhash(sk);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDUPLICATEDATAREHASH);\n\t}\n}\n\nstatic void tcp_send_dupack(struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq &&\n\t    before(TCP_SKB_CB(skb)->seq, tp->rcv_nxt)) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_DELAYEDACKLOST);\n\t\ttcp_enter_quickack_mode(sk, TCP_MAX_QUICKACKS);\n\n\t\tif (tcp_is_sack(tp) && sock_net(sk)->ipv4.sysctl_tcp_dsack) {\n\t\t\tu32 end_seq = TCP_SKB_CB(skb)->end_seq;\n\n\t\t\ttcp_rcv_spurious_retrans(sk, skb);\n\t\t\tif (after(TCP_SKB_CB(skb)->end_seq, tp->rcv_nxt))\n\t\t\t\tend_seq = tp->rcv_nxt;\n\t\t\ttcp_dsack_set(sk, TCP_SKB_CB(skb)->seq, end_seq);\n\t\t}\n\t}\n\n\ttcp_send_ack(sk);\n}\n\n/* These routines update the SACK block as out-of-order packets arrive or\n * in-order packets close up the sequence space.\n */\nstatic void tcp_sack_maybe_coalesce(struct tcp_sock *tp)\n{\n\tint this_sack;\n\tstruct tcp_sack_block *sp = &tp->selective_acks[0];\n\tstruct tcp_sack_block *swalk = sp + 1;\n\n\t/* See if the recent change to the first SACK eats into\n\t * or hits the sequence space of other SACK blocks, if so coalesce.\n\t */\n\tfor (this_sack = 1; this_sack < tp->rx_opt.num_sacks;) {\n\t\tif (tcp_sack_extend(sp, swalk->start_seq, swalk->end_seq)) {\n\t\t\tint i;\n\n\t\t\t/* Zap SWALK, by moving every further SACK up by one slot.\n\t\t\t * Decrease num_sacks.\n\t\t\t */\n\t\t\ttp->rx_opt.num_sacks--;\n\t\t\tfor (i = this_sack; i < tp->rx_opt.num_sacks; i++)\n\t\t\t\tsp[i] = sp[i + 1];\n\t\t\tcontinue;\n\t\t}\n\t\tthis_sack++;\n\t\tswalk++;\n\t}\n}\n\nstatic void tcp_sack_compress_send_ack(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (!tp->compressed_ack)\n\t\treturn;\n\n\tif (hrtimer_try_to_cancel(&tp->compressed_ack_timer) == 1)\n\t\t__sock_put(sk);\n\n\t/* Since we have to send one ack finally,\n\t * substract one from tp->compressed_ack to keep\n\t * LINUX_MIB_TCPACKCOMPRESSED accurate.\n\t */\n\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPACKCOMPRESSED,\n\t\t      tp->compressed_ack - 1);\n\n\ttp->compressed_ack = 0;\n\ttcp_send_ack(sk);\n}\n\n/* Reasonable amount of sack blocks included in TCP SACK option\n * The max is 4, but this becomes 3 if TCP timestamps are there.\n * Given that SACK packets might be lost, be conservative and use 2.\n */\n#define TCP_SACK_BLOCKS_EXPECTED 2\n\nstatic void tcp_sack_new_ofo_skb(struct sock *sk, u32 seq, u32 end_seq)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_sack_block *sp = &tp->selective_acks[0];\n\tint cur_sacks = tp->rx_opt.num_sacks;\n\tint this_sack;\n\n\tif (!cur_sacks)\n\t\tgoto new_sack;\n\n\tfor (this_sack = 0; this_sack < cur_sacks; this_sack++, sp++) {\n\t\tif (tcp_sack_extend(sp, seq, end_seq)) {\n\t\t\tif (this_sack >= TCP_SACK_BLOCKS_EXPECTED)\n\t\t\t\ttcp_sack_compress_send_ack(sk);\n\t\t\t/* Rotate this_sack to the first one. */\n\t\t\tfor (; this_sack > 0; this_sack--, sp--)\n\t\t\t\tswap(*sp, *(sp - 1));\n\t\t\tif (cur_sacks > 1)\n\t\t\t\ttcp_sack_maybe_coalesce(tp);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tif (this_sack >= TCP_SACK_BLOCKS_EXPECTED)\n\t\ttcp_sack_compress_send_ack(sk);\n\n\t/* Could not find an adjacent existing SACK, build a new one,\n\t * put it at the front, and shift everyone else down.  We\n\t * always know there is at least one SACK present already here.\n\t *\n\t * If the sack array is full, forget about the last one.\n\t */\n\tif (this_sack >= TCP_NUM_SACKS) {\n\t\tthis_sack--;\n\t\ttp->rx_opt.num_sacks--;\n\t\tsp--;\n\t}\n\tfor (; this_sack > 0; this_sack--, sp--)\n\t\t*sp = *(sp - 1);\n\nnew_sack:\n\t/* Build the new head SACK, and we're done. */\n\tsp->start_seq = seq;\n\tsp->end_seq = end_seq;\n\ttp->rx_opt.num_sacks++;\n}\n\n/* RCV.NXT advances, some SACKs should be eaten. */\n\nstatic void tcp_sack_remove(struct tcp_sock *tp)\n{\n\tstruct tcp_sack_block *sp = &tp->selective_acks[0];\n\tint num_sacks = tp->rx_opt.num_sacks;\n\tint this_sack;\n\n\t/* Empty ofo queue, hence, all the SACKs are eaten. Clear. */\n\tif (RB_EMPTY_ROOT(&tp->out_of_order_queue)) {\n\t\ttp->rx_opt.num_sacks = 0;\n\t\treturn;\n\t}\n\n\tfor (this_sack = 0; this_sack < num_sacks;) {\n\t\t/* Check if the start of the sack is covered by RCV.NXT. */\n\t\tif (!before(tp->rcv_nxt, sp->start_seq)) {\n\t\t\tint i;\n\n\t\t\t/* RCV.NXT must cover all the block! */\n\t\t\tWARN_ON(before(tp->rcv_nxt, sp->end_seq));\n\n\t\t\t/* Zap this SACK, by moving forward any other SACKS. */\n\t\t\tfor (i = this_sack+1; i < num_sacks; i++)\n\t\t\t\ttp->selective_acks[i-1] = tp->selective_acks[i];\n\t\t\tnum_sacks--;\n\t\t\tcontinue;\n\t\t}\n\t\tthis_sack++;\n\t\tsp++;\n\t}\n\ttp->rx_opt.num_sacks = num_sacks;\n}\n\n/**\n * tcp_try_coalesce - try to merge skb to prior one\n * @sk: socket\n * @to: prior buffer\n * @from: buffer to add in queue\n * @fragstolen: pointer to boolean\n *\n * Before queueing skb @from after @to, try to merge them\n * to reduce overall memory use and queue lengths, if cost is small.\n * Packets in ofo or receive queues can stay a long time.\n * Better try to coalesce them right now to avoid future collapses.\n * Returns true if caller should free @from instead of queueing it\n */\nstatic bool tcp_try_coalesce(struct sock *sk,\n\t\t\t     struct sk_buff *to,\n\t\t\t     struct sk_buff *from,\n\t\t\t     bool *fragstolen)\n{\n\tint delta;\n\n\t*fragstolen = false;\n\n\t/* Its possible this segment overlaps with prior segment in queue */\n\tif (TCP_SKB_CB(from)->seq != TCP_SKB_CB(to)->end_seq)\n\t\treturn false;\n\n\tif (!mptcp_skb_can_collapse(to, from))\n\t\treturn false;\n\n#ifdef CONFIG_TLS_DEVICE\n\tif (from->decrypted != to->decrypted)\n\t\treturn false;\n#endif\n\n\tif (!skb_try_coalesce(to, from, fragstolen, &delta))\n\t\treturn false;\n\n\tatomic_add(delta, &sk->sk_rmem_alloc);\n\tsk_mem_charge(sk, delta);\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPRCVCOALESCE);\n\tTCP_SKB_CB(to)->end_seq = TCP_SKB_CB(from)->end_seq;\n\tTCP_SKB_CB(to)->ack_seq = TCP_SKB_CB(from)->ack_seq;\n\tTCP_SKB_CB(to)->tcp_flags |= TCP_SKB_CB(from)->tcp_flags;\n\n\tif (TCP_SKB_CB(from)->has_rxtstamp) {\n\t\tTCP_SKB_CB(to)->has_rxtstamp = true;\n\t\tto->tstamp = from->tstamp;\n\t\tskb_hwtstamps(to)->hwtstamp = skb_hwtstamps(from)->hwtstamp;\n\t}\n\n\treturn true;\n}\n\nstatic bool tcp_ooo_try_coalesce(struct sock *sk,\n\t\t\t     struct sk_buff *to,\n\t\t\t     struct sk_buff *from,\n\t\t\t     bool *fragstolen)\n{\n\tbool res = tcp_try_coalesce(sk, to, from, fragstolen);\n\n\t/* In case tcp_drop() is called later, update to->gso_segs */\n\tif (res) {\n\t\tu32 gso_segs = max_t(u16, 1, skb_shinfo(to)->gso_segs) +\n\t\t\t       max_t(u16, 1, skb_shinfo(from)->gso_segs);\n\n\t\tskb_shinfo(to)->gso_segs = min_t(u32, gso_segs, 0xFFFF);\n\t}\n\treturn res;\n}\n\nstatic void tcp_drop(struct sock *sk, struct sk_buff *skb)\n{\n\tsk_drops_add(sk, skb);\n\t__kfree_skb(skb);\n}\n\n/* This one checks to see if we can put data from the\n * out_of_order queue into the receive_queue.\n */\nstatic void tcp_ofo_queue(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\t__u32 dsack_high = tp->rcv_nxt;\n\tbool fin, fragstolen, eaten;\n\tstruct sk_buff *skb, *tail;\n\tstruct rb_node *p;\n\n\tp = rb_first(&tp->out_of_order_queue);\n\twhile (p) {\n\t\tskb = rb_to_skb(p);\n\t\tif (after(TCP_SKB_CB(skb)->seq, tp->rcv_nxt))\n\t\t\tbreak;\n\n\t\tif (before(TCP_SKB_CB(skb)->seq, dsack_high)) {\n\t\t\t__u32 dsack = dsack_high;\n\t\t\tif (before(TCP_SKB_CB(skb)->end_seq, dsack_high))\n\t\t\t\tdsack_high = TCP_SKB_CB(skb)->end_seq;\n\t\t\ttcp_dsack_extend(sk, TCP_SKB_CB(skb)->seq, dsack);\n\t\t}\n\t\tp = rb_next(p);\n\t\trb_erase(&skb->rbnode, &tp->out_of_order_queue);\n\n\t\tif (unlikely(!after(TCP_SKB_CB(skb)->end_seq, tp->rcv_nxt))) {\n\t\t\ttcp_drop(sk, skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\ttail = skb_peek_tail(&sk->sk_receive_queue);\n\t\teaten = tail && tcp_try_coalesce(sk, tail, skb, &fragstolen);\n\t\ttcp_rcv_nxt_update(tp, TCP_SKB_CB(skb)->end_seq);\n\t\tfin = TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN;\n\t\tif (!eaten)\n\t\t\t__skb_queue_tail(&sk->sk_receive_queue, skb);\n\t\telse\n\t\t\tkfree_skb_partial(skb, fragstolen);\n\n\t\tif (unlikely(fin)) {\n\t\t\ttcp_fin(sk);\n\t\t\t/* tcp_fin() purges tp->out_of_order_queue,\n\t\t\t * so we must end this loop right now.\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic bool tcp_prune_ofo_queue(struct sock *sk);\nstatic int tcp_prune_queue(struct sock *sk);\n\nstatic int tcp_try_rmem_schedule(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t unsigned int size)\n{\n\tif (atomic_read(&sk->sk_rmem_alloc) > sk->sk_rcvbuf ||\n\t    !sk_rmem_schedule(sk, skb, size)) {\n\n\t\tif (tcp_prune_queue(sk) < 0)\n\t\t\treturn -1;\n\n\t\twhile (!sk_rmem_schedule(sk, skb, size)) {\n\t\t\tif (!tcp_prune_ofo_queue(sk))\n\t\t\t\treturn -1;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic void tcp_data_queue_ofo(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct rb_node **p, *parent;\n\tstruct sk_buff *skb1;\n\tu32 seq, end_seq;\n\tbool fragstolen;\n\n\ttcp_ecn_check_ce(sk, skb);\n\n\tif (unlikely(tcp_try_rmem_schedule(sk, skb, skb->truesize))) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFODROP);\n\t\tsk->sk_data_ready(sk);\n\t\ttcp_drop(sk, skb);\n\t\treturn;\n\t}\n\n\t/* Disable header prediction. */\n\ttp->pred_flags = 0;\n\tinet_csk_schedule_ack(sk);\n\n\ttp->rcv_ooopack += max_t(u16, 1, skb_shinfo(skb)->gso_segs);\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFOQUEUE);\n\tseq = TCP_SKB_CB(skb)->seq;\n\tend_seq = TCP_SKB_CB(skb)->end_seq;\n\n\tp = &tp->out_of_order_queue.rb_node;\n\tif (RB_EMPTY_ROOT(&tp->out_of_order_queue)) {\n\t\t/* Initial out of order segment, build 1 SACK. */\n\t\tif (tcp_is_sack(tp)) {\n\t\t\ttp->rx_opt.num_sacks = 1;\n\t\t\ttp->selective_acks[0].start_seq = seq;\n\t\t\ttp->selective_acks[0].end_seq = end_seq;\n\t\t}\n\t\trb_link_node(&skb->rbnode, NULL, p);\n\t\trb_insert_color(&skb->rbnode, &tp->out_of_order_queue);\n\t\ttp->ooo_last_skb = skb;\n\t\tgoto end;\n\t}\n\n\t/* In the typical case, we are adding an skb to the end of the list.\n\t * Use of ooo_last_skb avoids the O(Log(N)) rbtree lookup.\n\t */\n\tif (tcp_ooo_try_coalesce(sk, tp->ooo_last_skb,\n\t\t\t\t skb, &fragstolen)) {\ncoalesce_done:\n\t\t/* For non sack flows, do not grow window to force DUPACK\n\t\t * and trigger fast retransmit.\n\t\t */\n\t\tif (tcp_is_sack(tp))\n\t\t\ttcp_grow_window(sk, skb);\n\t\tkfree_skb_partial(skb, fragstolen);\n\t\tskb = NULL;\n\t\tgoto add_sack;\n\t}\n\t/* Can avoid an rbtree lookup if we are adding skb after ooo_last_skb */\n\tif (!before(seq, TCP_SKB_CB(tp->ooo_last_skb)->end_seq)) {\n\t\tparent = &tp->ooo_last_skb->rbnode;\n\t\tp = &parent->rb_right;\n\t\tgoto insert;\n\t}\n\n\t/* Find place to insert this segment. Handle overlaps on the way. */\n\tparent = NULL;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tskb1 = rb_to_skb(parent);\n\t\tif (before(seq, TCP_SKB_CB(skb1)->seq)) {\n\t\t\tp = &parent->rb_left;\n\t\t\tcontinue;\n\t\t}\n\t\tif (before(seq, TCP_SKB_CB(skb1)->end_seq)) {\n\t\t\tif (!after(end_seq, TCP_SKB_CB(skb1)->end_seq)) {\n\t\t\t\t/* All the bits are present. Drop. */\n\t\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\t      LINUX_MIB_TCPOFOMERGE);\n\t\t\t\ttcp_drop(sk, skb);\n\t\t\t\tskb = NULL;\n\t\t\t\ttcp_dsack_set(sk, seq, end_seq);\n\t\t\t\tgoto add_sack;\n\t\t\t}\n\t\t\tif (after(seq, TCP_SKB_CB(skb1)->seq)) {\n\t\t\t\t/* Partial overlap. */\n\t\t\t\ttcp_dsack_set(sk, seq, TCP_SKB_CB(skb1)->end_seq);\n\t\t\t} else {\n\t\t\t\t/* skb's seq == skb1's seq and skb covers skb1.\n\t\t\t\t * Replace skb1 with skb.\n\t\t\t\t */\n\t\t\t\trb_replace_node(&skb1->rbnode, &skb->rbnode,\n\t\t\t\t\t\t&tp->out_of_order_queue);\n\t\t\t\ttcp_dsack_extend(sk,\n\t\t\t\t\t\t TCP_SKB_CB(skb1)->seq,\n\t\t\t\t\t\t TCP_SKB_CB(skb1)->end_seq);\n\t\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\t      LINUX_MIB_TCPOFOMERGE);\n\t\t\t\ttcp_drop(sk, skb1);\n\t\t\t\tgoto merge_right;\n\t\t\t}\n\t\t} else if (tcp_ooo_try_coalesce(sk, skb1,\n\t\t\t\t\t\tskb, &fragstolen)) {\n\t\t\tgoto coalesce_done;\n\t\t}\n\t\tp = &parent->rb_right;\n\t}\ninsert:\n\t/* Insert segment into RB tree. */\n\trb_link_node(&skb->rbnode, parent, p);\n\trb_insert_color(&skb->rbnode, &tp->out_of_order_queue);\n\nmerge_right:\n\t/* Remove other segments covered by skb. */\n\twhile ((skb1 = skb_rb_next(skb)) != NULL) {\n\t\tif (!after(end_seq, TCP_SKB_CB(skb1)->seq))\n\t\t\tbreak;\n\t\tif (before(end_seq, TCP_SKB_CB(skb1)->end_seq)) {\n\t\t\ttcp_dsack_extend(sk, TCP_SKB_CB(skb1)->seq,\n\t\t\t\t\t end_seq);\n\t\t\tbreak;\n\t\t}\n\t\trb_erase(&skb1->rbnode, &tp->out_of_order_queue);\n\t\ttcp_dsack_extend(sk, TCP_SKB_CB(skb1)->seq,\n\t\t\t\t TCP_SKB_CB(skb1)->end_seq);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFOMERGE);\n\t\ttcp_drop(sk, skb1);\n\t}\n\t/* If there is no skb after us, we are the last_skb ! */\n\tif (!skb1)\n\t\ttp->ooo_last_skb = skb;\n\nadd_sack:\n\tif (tcp_is_sack(tp))\n\t\ttcp_sack_new_ofo_skb(sk, seq, end_seq);\nend:\n\tif (skb) {\n\t\t/* For non sack flows, do not grow window to force DUPACK\n\t\t * and trigger fast retransmit.\n\t\t */\n\t\tif (tcp_is_sack(tp))\n\t\t\ttcp_grow_window(sk, skb);\n\t\tskb_condense(skb);\n\t\tskb_set_owner_r(skb, sk);\n\t}\n}\n\nstatic int __must_check tcp_queue_rcv(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t      bool *fragstolen)\n{\n\tint eaten;\n\tstruct sk_buff *tail = skb_peek_tail(&sk->sk_receive_queue);\n\n\teaten = (tail &&\n\t\t tcp_try_coalesce(sk, tail,\n\t\t\t\t  skb, fragstolen)) ? 1 : 0;\n\ttcp_rcv_nxt_update(tcp_sk(sk), TCP_SKB_CB(skb)->end_seq);\n\tif (!eaten) {\n\t\t__skb_queue_tail(&sk->sk_receive_queue, skb);\n\t\tskb_set_owner_r(skb, sk);\n\t}\n\treturn eaten;\n}\n\nint tcp_send_rcvq(struct sock *sk, struct msghdr *msg, size_t size)\n{\n\tstruct sk_buff *skb;\n\tint err = -ENOMEM;\n\tint data_len = 0;\n\tbool fragstolen;\n\n\tif (size == 0)\n\t\treturn 0;\n\n\tif (size > PAGE_SIZE) {\n\t\tint npages = min_t(size_t, size >> PAGE_SHIFT, MAX_SKB_FRAGS);\n\n\t\tdata_len = npages << PAGE_SHIFT;\n\t\tsize = data_len + (size & ~PAGE_MASK);\n\t}\n\tskb = alloc_skb_with_frags(size - data_len, data_len,\n\t\t\t\t   PAGE_ALLOC_COSTLY_ORDER,\n\t\t\t\t   &err, sk->sk_allocation);\n\tif (!skb)\n\t\tgoto err;\n\n\tskb_put(skb, size - data_len);\n\tskb->data_len = data_len;\n\tskb->len = size;\n\n\tif (tcp_try_rmem_schedule(sk, skb, skb->truesize)) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPRCVQDROP);\n\t\tgoto err_free;\n\t}\n\n\terr = skb_copy_datagram_from_iter(skb, 0, &msg->msg_iter, size);\n\tif (err)\n\t\tgoto err_free;\n\n\tTCP_SKB_CB(skb)->seq = tcp_sk(sk)->rcv_nxt;\n\tTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(skb)->seq + size;\n\tTCP_SKB_CB(skb)->ack_seq = tcp_sk(sk)->snd_una - 1;\n\n\tif (tcp_queue_rcv(sk, skb, &fragstolen)) {\n\t\tWARN_ON_ONCE(fragstolen); /* should not happen */\n\t\t__kfree_skb(skb);\n\t}\n\treturn size;\n\nerr_free:\n\tkfree_skb(skb);\nerr:\n\treturn err;\n\n}\n\nvoid tcp_data_ready(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tint avail = tp->rcv_nxt - tp->copied_seq;\n\n\tif (avail < sk->sk_rcvlowat && !tcp_rmem_pressure(sk) &&\n\t    !sock_flag(sk, SOCK_DONE) &&\n\t    tcp_receive_window(tp) > inet_csk(sk)->icsk_ack.rcv_mss)\n\t\treturn;\n\n\tsk->sk_data_ready(sk);\n}\n\nstatic void tcp_data_queue(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tbool fragstolen;\n\tint eaten;\n\n\tif (sk_is_mptcp(sk))\n\t\tmptcp_incoming_options(sk, skb);\n\n\tif (TCP_SKB_CB(skb)->seq == TCP_SKB_CB(skb)->end_seq) {\n\t\t__kfree_skb(skb);\n\t\treturn;\n\t}\n\tskb_dst_drop(skb);\n\t__skb_pull(skb, tcp_hdr(skb)->doff * 4);\n\n\ttp->rx_opt.dsack = 0;\n\n\t/*  Queue data for delivery to the user.\n\t *  Packets in sequence go to the receive queue.\n\t *  Out of sequence packets to the out_of_order_queue.\n\t */\n\tif (TCP_SKB_CB(skb)->seq == tp->rcv_nxt) {\n\t\tif (tcp_receive_window(tp) == 0) {\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPZEROWINDOWDROP);\n\t\t\tgoto out_of_window;\n\t\t}\n\n\t\t/* Ok. In sequence. In window. */\nqueue_and_out:\n\t\tif (skb_queue_len(&sk->sk_receive_queue) == 0)\n\t\t\tsk_forced_mem_schedule(sk, skb->truesize);\n\t\telse if (tcp_try_rmem_schedule(sk, skb, skb->truesize)) {\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPRCVQDROP);\n\t\t\tsk->sk_data_ready(sk);\n\t\t\tgoto drop;\n\t\t}\n\n\t\teaten = tcp_queue_rcv(sk, skb, &fragstolen);\n\t\tif (skb->len)\n\t\t\ttcp_event_data_recv(sk, skb);\n\t\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)\n\t\t\ttcp_fin(sk);\n\n\t\tif (!RB_EMPTY_ROOT(&tp->out_of_order_queue)) {\n\t\t\ttcp_ofo_queue(sk);\n\n\t\t\t/* RFC5681. 4.2. SHOULD send immediate ACK, when\n\t\t\t * gap in queue is filled.\n\t\t\t */\n\t\t\tif (RB_EMPTY_ROOT(&tp->out_of_order_queue))\n\t\t\t\tinet_csk(sk)->icsk_ack.pending |= ICSK_ACK_NOW;\n\t\t}\n\n\t\tif (tp->rx_opt.num_sacks)\n\t\t\ttcp_sack_remove(tp);\n\n\t\ttcp_fast_path_check(sk);\n\n\t\tif (eaten > 0)\n\t\t\tkfree_skb_partial(skb, fragstolen);\n\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\ttcp_data_ready(sk);\n\t\treturn;\n\t}\n\n\tif (!after(TCP_SKB_CB(skb)->end_seq, tp->rcv_nxt)) {\n\t\ttcp_rcv_spurious_retrans(sk, skb);\n\t\t/* A retransmit, 2nd most common case.  Force an immediate ack. */\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_DELAYEDACKLOST);\n\t\ttcp_dsack_set(sk, TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb)->end_seq);\n\nout_of_window:\n\t\ttcp_enter_quickack_mode(sk, TCP_MAX_QUICKACKS);\n\t\tinet_csk_schedule_ack(sk);\ndrop:\n\t\ttcp_drop(sk, skb);\n\t\treturn;\n\t}\n\n\t/* Out of window. F.e. zero window probe. */\n\tif (!before(TCP_SKB_CB(skb)->seq, tp->rcv_nxt + tcp_receive_window(tp)))\n\t\tgoto out_of_window;\n\n\tif (before(TCP_SKB_CB(skb)->seq, tp->rcv_nxt)) {\n\t\t/* Partial packet, seq < rcv_next < end_seq */\n\t\ttcp_dsack_set(sk, TCP_SKB_CB(skb)->seq, tp->rcv_nxt);\n\n\t\t/* If window is closed, drop tail of packet. But after\n\t\t * remembering D-SACK for its head made in previous line.\n\t\t */\n\t\tif (!tcp_receive_window(tp)) {\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPZEROWINDOWDROP);\n\t\t\tgoto out_of_window;\n\t\t}\n\t\tgoto queue_and_out;\n\t}\n\n\ttcp_data_queue_ofo(sk, skb);\n}\n\nstatic struct sk_buff *tcp_skb_next(struct sk_buff *skb, struct sk_buff_head *list)\n{\n\tif (list)\n\t\treturn !skb_queue_is_last(list, skb) ? skb->next : NULL;\n\n\treturn skb_rb_next(skb);\n}\n\nstatic struct sk_buff *tcp_collapse_one(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\tstruct sk_buff_head *list,\n\t\t\t\t\tstruct rb_root *root)\n{\n\tstruct sk_buff *next = tcp_skb_next(skb, list);\n\n\tif (list)\n\t\t__skb_unlink(skb, list);\n\telse\n\t\trb_erase(&skb->rbnode, root);\n\n\t__kfree_skb(skb);\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPRCVCOLLAPSED);\n\n\treturn next;\n}\n\n/* Insert skb into rb tree, ordered by TCP_SKB_CB(skb)->seq */\nvoid tcp_rbtree_insert(struct rb_root *root, struct sk_buff *skb)\n{\n\tstruct rb_node **p = &root->rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct sk_buff *skb1;\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tskb1 = rb_to_skb(parent);\n\t\tif (before(TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb1)->seq))\n\t\t\tp = &parent->rb_left;\n\t\telse\n\t\t\tp = &parent->rb_right;\n\t}\n\trb_link_node(&skb->rbnode, parent, p);\n\trb_insert_color(&skb->rbnode, root);\n}\n\n/* Collapse contiguous sequence of skbs head..tail with\n * sequence numbers start..end.\n *\n * If tail is NULL, this means until the end of the queue.\n *\n * Segments with FIN/SYN are not collapsed (only because this\n * simplifies code)\n */\nstatic void\ntcp_collapse(struct sock *sk, struct sk_buff_head *list, struct rb_root *root,\n\t     struct sk_buff *head, struct sk_buff *tail, u32 start, u32 end)\n{\n\tstruct sk_buff *skb = head, *n;\n\tstruct sk_buff_head tmp;\n\tbool end_of_skbs;\n\n\t/* First, check that queue is collapsible and find\n\t * the point where collapsing can be useful.\n\t */\nrestart:\n\tfor (end_of_skbs = true; skb != NULL && skb != tail; skb = n) {\n\t\tn = tcp_skb_next(skb, list);\n\n\t\t/* No new bits? It is possible on ofo queue. */\n\t\tif (!before(start, TCP_SKB_CB(skb)->end_seq)) {\n\t\t\tskb = tcp_collapse_one(sk, skb, list, root);\n\t\t\tif (!skb)\n\t\t\t\tbreak;\n\t\t\tgoto restart;\n\t\t}\n\n\t\t/* The first skb to collapse is:\n\t\t * - not SYN/FIN and\n\t\t * - bloated or contains data before \"start\" or\n\t\t *   overlaps to the next one and mptcp allow collapsing.\n\t\t */\n\t\tif (!(TCP_SKB_CB(skb)->tcp_flags & (TCPHDR_SYN | TCPHDR_FIN)) &&\n\t\t    (tcp_win_from_space(sk, skb->truesize) > skb->len ||\n\t\t     before(TCP_SKB_CB(skb)->seq, start))) {\n\t\t\tend_of_skbs = false;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (n && n != tail && mptcp_skb_can_collapse(skb, n) &&\n\t\t    TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(n)->seq) {\n\t\t\tend_of_skbs = false;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Decided to skip this, advance start seq. */\n\t\tstart = TCP_SKB_CB(skb)->end_seq;\n\t}\n\tif (end_of_skbs ||\n\t    (TCP_SKB_CB(skb)->tcp_flags & (TCPHDR_SYN | TCPHDR_FIN)))\n\t\treturn;\n\n\t__skb_queue_head_init(&tmp);\n\n\twhile (before(start, end)) {\n\t\tint copy = min_t(int, SKB_MAX_ORDER(0, 0), end - start);\n\t\tstruct sk_buff *nskb;\n\n\t\tnskb = alloc_skb(copy, GFP_ATOMIC);\n\t\tif (!nskb)\n\t\t\tbreak;\n\n\t\tmemcpy(nskb->cb, skb->cb, sizeof(skb->cb));\n#ifdef CONFIG_TLS_DEVICE\n\t\tnskb->decrypted = skb->decrypted;\n#endif\n\t\tTCP_SKB_CB(nskb)->seq = TCP_SKB_CB(nskb)->end_seq = start;\n\t\tif (list)\n\t\t\t__skb_queue_before(list, skb, nskb);\n\t\telse\n\t\t\t__skb_queue_tail(&tmp, nskb); /* defer rbtree insertion */\n\t\tskb_set_owner_r(nskb, sk);\n\t\tmptcp_skb_ext_move(nskb, skb);\n\n\t\t/* Copy data, releasing collapsed skbs. */\n\t\twhile (copy > 0) {\n\t\t\tint offset = start - TCP_SKB_CB(skb)->seq;\n\t\t\tint size = TCP_SKB_CB(skb)->end_seq - start;\n\n\t\t\tBUG_ON(offset < 0);\n\t\t\tif (size > 0) {\n\t\t\t\tsize = min(copy, size);\n\t\t\t\tif (skb_copy_bits(skb, offset, skb_put(nskb, size), size))\n\t\t\t\t\tBUG();\n\t\t\t\tTCP_SKB_CB(nskb)->end_seq += size;\n\t\t\t\tcopy -= size;\n\t\t\t\tstart += size;\n\t\t\t}\n\t\t\tif (!before(start, TCP_SKB_CB(skb)->end_seq)) {\n\t\t\t\tskb = tcp_collapse_one(sk, skb, list, root);\n\t\t\t\tif (!skb ||\n\t\t\t\t    skb == tail ||\n\t\t\t\t    !mptcp_skb_can_collapse(nskb, skb) ||\n\t\t\t\t    (TCP_SKB_CB(skb)->tcp_flags & (TCPHDR_SYN | TCPHDR_FIN)))\n\t\t\t\t\tgoto end;\n#ifdef CONFIG_TLS_DEVICE\n\t\t\t\tif (skb->decrypted != nskb->decrypted)\n\t\t\t\t\tgoto end;\n#endif\n\t\t\t}\n\t\t}\n\t}\nend:\n\tskb_queue_walk_safe(&tmp, skb, n)\n\t\ttcp_rbtree_insert(root, skb);\n}\n\n/* Collapse ofo queue. Algorithm: select contiguous sequence of skbs\n * and tcp_collapse() them until all the queue is collapsed.\n */\nstatic void tcp_collapse_ofo_queue(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 range_truesize, sum_tiny = 0;\n\tstruct sk_buff *skb, *head;\n\tu32 start, end;\n\n\tskb = skb_rb_first(&tp->out_of_order_queue);\nnew_range:\n\tif (!skb) {\n\t\ttp->ooo_last_skb = skb_rb_last(&tp->out_of_order_queue);\n\t\treturn;\n\t}\n\tstart = TCP_SKB_CB(skb)->seq;\n\tend = TCP_SKB_CB(skb)->end_seq;\n\trange_truesize = skb->truesize;\n\n\tfor (head = skb;;) {\n\t\tskb = skb_rb_next(skb);\n\n\t\t/* Range is terminated when we see a gap or when\n\t\t * we are at the queue end.\n\t\t */\n\t\tif (!skb ||\n\t\t    after(TCP_SKB_CB(skb)->seq, end) ||\n\t\t    before(TCP_SKB_CB(skb)->end_seq, start)) {\n\t\t\t/* Do not attempt collapsing tiny skbs */\n\t\t\tif (range_truesize != head->truesize ||\n\t\t\t    end - start >= SKB_WITH_OVERHEAD(SK_MEM_QUANTUM)) {\n\t\t\t\ttcp_collapse(sk, NULL, &tp->out_of_order_queue,\n\t\t\t\t\t     head, skb, start, end);\n\t\t\t} else {\n\t\t\t\tsum_tiny += range_truesize;\n\t\t\t\tif (sum_tiny > sk->sk_rcvbuf >> 3)\n\t\t\t\t\treturn;\n\t\t\t}\n\t\t\tgoto new_range;\n\t\t}\n\n\t\trange_truesize += skb->truesize;\n\t\tif (unlikely(before(TCP_SKB_CB(skb)->seq, start)))\n\t\t\tstart = TCP_SKB_CB(skb)->seq;\n\t\tif (after(TCP_SKB_CB(skb)->end_seq, end))\n\t\t\tend = TCP_SKB_CB(skb)->end_seq;\n\t}\n}\n\n/*\n * Clean the out-of-order queue to make room.\n * We drop high sequences packets to :\n * 1) Let a chance for holes to be filled.\n * 2) not add too big latencies if thousands of packets sit there.\n *    (But if application shrinks SO_RCVBUF, we could still end up\n *     freeing whole queue here)\n * 3) Drop at least 12.5 % of sk_rcvbuf to avoid malicious attacks.\n *\n * Return true if queue has shrunk.\n */\nstatic bool tcp_prune_ofo_queue(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct rb_node *node, *prev;\n\tint goal;\n\n\tif (RB_EMPTY_ROOT(&tp->out_of_order_queue))\n\t\treturn false;\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_OFOPRUNED);\n\tgoal = sk->sk_rcvbuf >> 3;\n\tnode = &tp->ooo_last_skb->rbnode;\n\tdo {\n\t\tprev = rb_prev(node);\n\t\trb_erase(node, &tp->out_of_order_queue);\n\t\tgoal -= rb_to_skb(node)->truesize;\n\t\ttcp_drop(sk, rb_to_skb(node));\n\t\tif (!prev || goal <= 0) {\n\t\t\tsk_mem_reclaim(sk);\n\t\t\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf &&\n\t\t\t    !tcp_under_memory_pressure(sk))\n\t\t\t\tbreak;\n\t\t\tgoal = sk->sk_rcvbuf >> 3;\n\t\t}\n\t\tnode = prev;\n\t} while (node);\n\ttp->ooo_last_skb = rb_to_skb(prev);\n\n\t/* Reset SACK state.  A conforming SACK implementation will\n\t * do the same at a timeout based retransmit.  When a connection\n\t * is in a sad state like this, we care only about integrity\n\t * of the connection not performance.\n\t */\n\tif (tp->rx_opt.sack_ok)\n\t\ttcp_sack_reset(&tp->rx_opt);\n\treturn true;\n}\n\n/* Reduce allocated memory if we can, trying to get\n * the socket within its memory limits again.\n *\n * Return less than zero if we should start dropping frames\n * until the socket owning process reads some of the data\n * to stabilize the situation.\n */\nstatic int tcp_prune_queue(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_PRUNECALLED);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\n\t\ttcp_clamp_window(sk);\n\telse if (tcp_under_memory_pressure(sk))\n\t\ttp->rcv_ssthresh = min(tp->rcv_ssthresh, 4U * tp->advmss);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf)\n\t\treturn 0;\n\n\ttcp_collapse_ofo_queue(sk);\n\tif (!skb_queue_empty(&sk->sk_receive_queue))\n\t\ttcp_collapse(sk, &sk->sk_receive_queue, NULL,\n\t\t\t     skb_peek(&sk->sk_receive_queue),\n\t\t\t     NULL,\n\t\t\t     tp->copied_seq, tp->rcv_nxt);\n\tsk_mem_reclaim(sk);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf)\n\t\treturn 0;\n\n\t/* Collapsing did not help, destructive actions follow.\n\t * This must not ever occur. */\n\n\ttcp_prune_ofo_queue(sk);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf)\n\t\treturn 0;\n\n\t/* If we are really being abused, tell the caller to silently\n\t * drop receive data on the floor.  It will get retransmitted\n\t * and hopefully then we'll have sufficient space.\n\t */\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_RCVPRUNED);\n\n\t/* Massive buffer overcommit. */\n\ttp->pred_flags = 0;\n\treturn -1;\n}\n\nstatic bool tcp_should_expand_sndbuf(const struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\t/* If the user specified a specific send buffer setting, do\n\t * not modify it.\n\t */\n\tif (sk->sk_userlocks & SOCK_SNDBUF_LOCK)\n\t\treturn false;\n\n\t/* If we are under global TCP memory pressure, do not expand.  */\n\tif (tcp_under_memory_pressure(sk))\n\t\treturn false;\n\n\t/* If we are under soft global TCP memory pressure, do not expand.  */\n\tif (sk_memory_allocated(sk) >= sk_prot_mem_limits(sk, 0))\n\t\treturn false;\n\n\t/* If we filled the congestion window, do not expand.  */\n\tif (tcp_packets_in_flight(tp) >= tp->snd_cwnd)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void tcp_new_space(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tcp_should_expand_sndbuf(sk)) {\n\t\ttcp_sndbuf_expand(sk);\n\t\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\t}\n\n\tsk->sk_write_space(sk);\n}\n\nstatic void tcp_check_space(struct sock *sk)\n{\n\t/* pairs with tcp_poll() */\n\tsmp_mb();\n\tif (sk->sk_socket &&\n\t    test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\ttcp_new_space(sk);\n\t\tif (!test_bit(SOCK_NOSPACE, &sk->sk_socket->flags))\n\t\t\ttcp_chrono_stop(sk, TCP_CHRONO_SNDBUF_LIMITED);\n\t}\n}\n\nstatic inline void tcp_data_snd_check(struct sock *sk)\n{\n\ttcp_push_pending_frames(sk);\n\ttcp_check_space(sk);\n}\n\n/*\n * Check if sending an ack is needed.\n */\nstatic void __tcp_ack_snd_check(struct sock *sk, int ofo_possible)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned long rtt, delay;\n\n\t    /* More than one full frame received... */\n\tif (((tp->rcv_nxt - tp->rcv_wup) > inet_csk(sk)->icsk_ack.rcv_mss &&\n\t     /* ... and right edge of window advances far enough.\n\t      * (tcp_recvmsg() will send ACK otherwise).\n\t      * If application uses SO_RCVLOWAT, we want send ack now if\n\t      * we have not received enough bytes to satisfy the condition.\n\t      */\n\t    (tp->rcv_nxt - tp->copied_seq < sk->sk_rcvlowat ||\n\t     __tcp_select_window(sk) >= tp->rcv_wnd)) ||\n\t    /* We ACK each frame or... */\n\t    tcp_in_quickack_mode(sk) ||\n\t    /* Protocol state mandates a one-time immediate ACK */\n\t    inet_csk(sk)->icsk_ack.pending & ICSK_ACK_NOW) {\nsend_now:\n\t\ttcp_send_ack(sk);\n\t\treturn;\n\t}\n\n\tif (!ofo_possible || RB_EMPTY_ROOT(&tp->out_of_order_queue)) {\n\t\ttcp_send_delayed_ack(sk);\n\t\treturn;\n\t}\n\n\tif (!tcp_is_sack(tp) ||\n\t    tp->compressed_ack >= sock_net(sk)->ipv4.sysctl_tcp_comp_sack_nr)\n\t\tgoto send_now;\n\n\tif (tp->compressed_ack_rcv_nxt != tp->rcv_nxt) {\n\t\ttp->compressed_ack_rcv_nxt = tp->rcv_nxt;\n\t\ttp->dup_ack_counter = 0;\n\t}\n\tif (tp->dup_ack_counter < TCP_FASTRETRANS_THRESH) {\n\t\ttp->dup_ack_counter++;\n\t\tgoto send_now;\n\t}\n\ttp->compressed_ack++;\n\tif (hrtimer_is_queued(&tp->compressed_ack_timer))\n\t\treturn;\n\n\t/* compress ack timer : 5 % of rtt, but no more than tcp_comp_sack_delay_ns */\n\n\trtt = tp->rcv_rtt_est.rtt_us;\n\tif (tp->srtt_us && tp->srtt_us < rtt)\n\t\trtt = tp->srtt_us;\n\n\tdelay = min_t(unsigned long, sock_net(sk)->ipv4.sysctl_tcp_comp_sack_delay_ns,\n\t\t      rtt * (NSEC_PER_USEC >> 3)/20);\n\tsock_hold(sk);\n\thrtimer_start_range_ns(&tp->compressed_ack_timer, ns_to_ktime(delay),\n\t\t\t       sock_net(sk)->ipv4.sysctl_tcp_comp_sack_slack_ns,\n\t\t\t       HRTIMER_MODE_REL_PINNED_SOFT);\n}\n\nstatic inline void tcp_ack_snd_check(struct sock *sk)\n{\n\tif (!inet_csk_ack_scheduled(sk)) {\n\t\t/* We sent a data segment already. */\n\t\treturn;\n\t}\n\t__tcp_ack_snd_check(sk, 1);\n}\n\n/*\n *\tThis routine is only called when we have urgent data\n *\tsignaled. Its the 'slow' part of tcp_urg. It could be\n *\tmoved inline now as tcp_urg is only called from one\n *\tplace. We handle URGent data wrong. We have to - as\n *\tBSD still doesn't use the correction from RFC961.\n *\tFor 1003.1g we should support a new option TCP_STDURG to permit\n *\teither form (or just set the sysctl tcp_stdurg).\n */\n\nstatic void tcp_check_urg(struct sock *sk, const struct tcphdr *th)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 ptr = ntohs(th->urg_ptr);\n\n\tif (ptr && !sock_net(sk)->ipv4.sysctl_tcp_stdurg)\n\t\tptr--;\n\tptr += ntohl(th->seq);\n\n\t/* Ignore urgent data that we've already seen and read. */\n\tif (after(tp->copied_seq, ptr))\n\t\treturn;\n\n\t/* Do not replay urg ptr.\n\t *\n\t * NOTE: interesting situation not covered by specs.\n\t * Misbehaving sender may send urg ptr, pointing to segment,\n\t * which we already have in ofo queue. We are not able to fetch\n\t * such data and will stay in TCP_URG_NOTYET until will be eaten\n\t * by recvmsg(). Seems, we are not obliged to handle such wicked\n\t * situations. But it is worth to think about possibility of some\n\t * DoSes using some hypothetical application level deadlock.\n\t */\n\tif (before(ptr, tp->rcv_nxt))\n\t\treturn;\n\n\t/* Do we already have a newer (or duplicate) urgent pointer? */\n\tif (tp->urg_data && !after(ptr, tp->urg_seq))\n\t\treturn;\n\n\t/* Tell the world about our new urgent pointer. */\n\tsk_send_sigurg(sk);\n\n\t/* We may be adding urgent data when the last byte read was\n\t * urgent. To do this requires some care. We cannot just ignore\n\t * tp->copied_seq since we would read the last urgent byte again\n\t * as data, nor can we alter copied_seq until this data arrives\n\t * or we break the semantics of SIOCATMARK (and thus sockatmark())\n\t *\n\t * NOTE. Double Dutch. Rendering to plain English: author of comment\n\t * above did something sort of \tsend(\"A\", MSG_OOB); send(\"B\", MSG_OOB);\n\t * and expect that both A and B disappear from stream. This is _wrong_.\n\t * Though this happens in BSD with high probability, this is occasional.\n\t * Any application relying on this is buggy. Note also, that fix \"works\"\n\t * only in this artificial test. Insert some normal data between A and B and we will\n\t * decline of BSD again. Verdict: it is better to remove to trap\n\t * buggy users.\n\t */\n\tif (tp->urg_seq == tp->copied_seq && tp->urg_data &&\n\t    !sock_flag(sk, SOCK_URGINLINE) && tp->copied_seq != tp->rcv_nxt) {\n\t\tstruct sk_buff *skb = skb_peek(&sk->sk_receive_queue);\n\t\ttp->copied_seq++;\n\t\tif (skb && !before(tp->copied_seq, TCP_SKB_CB(skb)->end_seq)) {\n\t\t\t__skb_unlink(skb, &sk->sk_receive_queue);\n\t\t\t__kfree_skb(skb);\n\t\t}\n\t}\n\n\ttp->urg_data = TCP_URG_NOTYET;\n\tWRITE_ONCE(tp->urg_seq, ptr);\n\n\t/* Disable header prediction. */\n\ttp->pred_flags = 0;\n}\n\n/* This is the 'fast' part of urgent handling. */\nstatic void tcp_urg(struct sock *sk, struct sk_buff *skb, const struct tcphdr *th)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Check if we get a new urgent pointer - normally not. */\n\tif (th->urg)\n\t\ttcp_check_urg(sk, th);\n\n\t/* Do we wait for any urgent data? - normally not... */\n\tif (tp->urg_data == TCP_URG_NOTYET) {\n\t\tu32 ptr = tp->urg_seq - ntohl(th->seq) + (th->doff * 4) -\n\t\t\t  th->syn;\n\n\t\t/* Is the urgent pointer pointing into this packet? */\n\t\tif (ptr < skb->len) {\n\t\t\tu8 tmp;\n\t\t\tif (skb_copy_bits(skb, ptr, &tmp, 1))\n\t\t\t\tBUG();\n\t\t\ttp->urg_data = TCP_URG_VALID | tmp;\n\t\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\t\tsk->sk_data_ready(sk);\n\t\t}\n\t}\n}\n\n/* Accept RST for rcv_nxt - 1 after a FIN.\n * When tcp connections are abruptly terminated from Mac OSX (via ^C), a\n * FIN is sent followed by a RST packet. The RST is sent with the same\n * sequence number as the FIN, and thus according to RFC 5961 a challenge\n * ACK should be sent. However, Mac OSX rate limits replies to challenge\n * ACKs on the closed socket. In addition middleboxes can drop either the\n * challenge ACK or a subsequent RST.\n */\nstatic bool tcp_reset_check(const struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\treturn unlikely(TCP_SKB_CB(skb)->seq == (tp->rcv_nxt - 1) &&\n\t\t\t(1 << sk->sk_state) & (TCPF_CLOSE_WAIT | TCPF_LAST_ACK |\n\t\t\t\t\t       TCPF_CLOSING));\n}\n\n/* Does PAWS and seqno based validation of an incoming segment, flags will\n * play significant role here.\n */\nstatic bool tcp_validate_incoming(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  const struct tcphdr *th, int syn_inerr)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tbool rst_seq_match = false;\n\n\t/* RFC1323: H1. Apply PAWS check first. */\n\tif (tcp_fast_parse_options(sock_net(sk), skb, th, tp) &&\n\t    tp->rx_opt.saw_tstamp &&\n\t    tcp_paws_discard(sk, skb)) {\n\t\tif (!th->rst) {\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_PAWSESTABREJECTED);\n\t\t\tif (!tcp_oow_rate_limited(sock_net(sk), skb,\n\t\t\t\t\t\t  LINUX_MIB_TCPACKSKIPPEDPAWS,\n\t\t\t\t\t\t  &tp->last_oow_ack_time))\n\t\t\t\ttcp_send_dupack(sk, skb);\n\t\t\tgoto discard;\n\t\t}\n\t\t/* Reset is accepted even if it did not pass PAWS. */\n\t}\n\n\t/* Step 1: check sequence number */\n\tif (!tcp_sequence(tp, TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb)->end_seq)) {\n\t\t/* RFC793, page 37: \"In all states except SYN-SENT, all reset\n\t\t * (RST) segments are validated by checking their SEQ-fields.\"\n\t\t * And page 69: \"If an incoming segment is not acceptable,\n\t\t * an acknowledgment should be sent in reply (unless the RST\n\t\t * bit is set, if so drop the segment and return)\".\n\t\t */\n\t\tif (!th->rst) {\n\t\t\tif (th->syn)\n\t\t\t\tgoto syn_challenge;\n\t\t\tif (!tcp_oow_rate_limited(sock_net(sk), skb,\n\t\t\t\t\t\t  LINUX_MIB_TCPACKSKIPPEDSEQ,\n\t\t\t\t\t\t  &tp->last_oow_ack_time))\n\t\t\t\ttcp_send_dupack(sk, skb);\n\t\t} else if (tcp_reset_check(sk, skb)) {\n\t\t\ttcp_reset(sk);\n\t\t}\n\t\tgoto discard;\n\t}\n\n\t/* Step 2: check RST bit */\n\tif (th->rst) {\n\t\t/* RFC 5961 3.2 (extend to match against (RCV.NXT - 1) after a\n\t\t * FIN and SACK too if available):\n\t\t * If seq num matches RCV.NXT or (RCV.NXT - 1) after a FIN, or\n\t\t * the right-most SACK block,\n\t\t * then\n\t\t *     RESET the connection\n\t\t * else\n\t\t *     Send a challenge ACK\n\t\t */\n\t\tif (TCP_SKB_CB(skb)->seq == tp->rcv_nxt ||\n\t\t    tcp_reset_check(sk, skb)) {\n\t\t\trst_seq_match = true;\n\t\t} else if (tcp_is_sack(tp) && tp->rx_opt.num_sacks > 0) {\n\t\t\tstruct tcp_sack_block *sp = &tp->selective_acks[0];\n\t\t\tint max_sack = sp[0].end_seq;\n\t\t\tint this_sack;\n\n\t\t\tfor (this_sack = 1; this_sack < tp->rx_opt.num_sacks;\n\t\t\t     ++this_sack) {\n\t\t\t\tmax_sack = after(sp[this_sack].end_seq,\n\t\t\t\t\t\t max_sack) ?\n\t\t\t\t\tsp[this_sack].end_seq : max_sack;\n\t\t\t}\n\n\t\t\tif (TCP_SKB_CB(skb)->seq == max_sack)\n\t\t\t\trst_seq_match = true;\n\t\t}\n\n\t\tif (rst_seq_match)\n\t\t\ttcp_reset(sk);\n\t\telse {\n\t\t\t/* Disable TFO if RST is out-of-order\n\t\t\t * and no data has been received\n\t\t\t * for current active TFO socket\n\t\t\t */\n\t\t\tif (tp->syn_fastopen && !tp->data_segs_in &&\n\t\t\t    sk->sk_state == TCP_ESTABLISHED)\n\t\t\t\ttcp_fastopen_active_disable(sk);\n\t\t\ttcp_send_challenge_ack(sk, skb);\n\t\t}\n\t\tgoto discard;\n\t}\n\n\t/* step 3: check security and precedence [ignored] */\n\n\t/* step 4: Check for a SYN\n\t * RFC 5961 4.2 : Send a challenge ack\n\t */\n\tif (th->syn) {\nsyn_challenge:\n\t\tif (syn_inerr)\n\t\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPSYNCHALLENGE);\n\t\ttcp_send_challenge_ack(sk, skb);\n\t\tgoto discard;\n\t}\n\n\tbpf_skops_parse_hdr(sk, skb);\n\n\treturn true;\n\ndiscard:\n\ttcp_drop(sk, skb);\n\treturn false;\n}\n\n/*\n *\tTCP receive function for the ESTABLISHED state.\n *\n *\tIt is split into a fast path and a slow path. The fast path is\n * \tdisabled when:\n *\t- A zero window was announced from us - zero window probing\n *        is only handled properly in the slow path.\n *\t- Out of order segments arrived.\n *\t- Urgent data is expected.\n *\t- There is no buffer space left\n *\t- Unexpected TCP flags/window values/header lengths are received\n *\t  (detected by checking the TCP header against pred_flags)\n *\t- Data is sent in both directions. Fast path only supports pure senders\n *\t  or pure receivers (this means either the sequence number or the ack\n *\t  value must stay constant)\n *\t- Unexpected TCP option.\n *\n *\tWhen these conditions are not satisfied it drops into a standard\n *\treceive procedure patterned after RFC793 to handle all cases.\n *\tThe first three cases are guaranteed by proper pred_flags setting,\n *\tthe rest is checked inline. Fast processing is turned on in\n *\ttcp_data_queue when everything is OK.\n */\nvoid tcp_rcv_established(struct sock *sk, struct sk_buff *skb)\n{\n\tconst struct tcphdr *th = (const struct tcphdr *)skb->data;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int len = skb->len;\n\n\t/* TCP congestion window tracking */\n\ttrace_tcp_probe(sk, skb);\n\n\ttcp_mstamp_refresh(tp);\n\tif (unlikely(!sk->sk_rx_dst))\n\t\tinet_csk(sk)->icsk_af_ops->sk_rx_dst_set(sk, skb);\n\t/*\n\t *\tHeader prediction.\n\t *\tThe code loosely follows the one in the famous\n\t *\t\"30 instruction TCP receive\" Van Jacobson mail.\n\t *\n\t *\tVan's trick is to deposit buffers into socket queue\n\t *\ton a device interrupt, to call tcp_recv function\n\t *\ton the receive process context and checksum and copy\n\t *\tthe buffer to user space. smart...\n\t *\n\t *\tOur current scheme is not silly either but we take the\n\t *\textra cost of the net_bh soft interrupt processing...\n\t *\tWe do checksum and copy also but from device to kernel.\n\t */\n\n\ttp->rx_opt.saw_tstamp = 0;\n\n\t/*\tpred_flags is 0xS?10 << 16 + snd_wnd\n\t *\tif header_prediction is to be made\n\t *\t'S' will always be tp->tcp_header_len >> 2\n\t *\t'?' will be 0 for the fast path, otherwise pred_flags is 0 to\n\t *  turn it off\t(when there are holes in the receive\n\t *\t space for instance)\n\t *\tPSH flag is ignored.\n\t */\n\n\tif ((tcp_flag_word(th) & TCP_HP_BITS) == tp->pred_flags &&\n\t    TCP_SKB_CB(skb)->seq == tp->rcv_nxt &&\n\t    !after(TCP_SKB_CB(skb)->ack_seq, tp->snd_nxt)) {\n\t\tint tcp_header_len = tp->tcp_header_len;\n\n\t\t/* Timestamp header prediction: tcp_header_len\n\t\t * is automatically equal to th->doff*4 due to pred_flags\n\t\t * match.\n\t\t */\n\n\t\t/* Check timestamp */\n\t\tif (tcp_header_len == sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) {\n\t\t\t/* No? Slow path! */\n\t\t\tif (!tcp_parse_aligned_timestamp(tp, th))\n\t\t\t\tgoto slow_path;\n\n\t\t\t/* If PAWS failed, check it more carefully in slow path */\n\t\t\tif ((s32)(tp->rx_opt.rcv_tsval - tp->rx_opt.ts_recent) < 0)\n\t\t\t\tgoto slow_path;\n\n\t\t\t/* DO NOT update ts_recent here, if checksum fails\n\t\t\t * and timestamp was corrupted part, it will result\n\t\t\t * in a hung connection since we will drop all\n\t\t\t * future packets due to the PAWS test.\n\t\t\t */\n\t\t}\n\n\t\tif (len <= tcp_header_len) {\n\t\t\t/* Bulk data transfer: sender */\n\t\t\tif (len == tcp_header_len) {\n\t\t\t\t/* Predicted packet is in window by definition.\n\t\t\t\t * seq == rcv_nxt and rcv_wup <= rcv_nxt.\n\t\t\t\t * Hence, check seq<=rcv_wup reduces to:\n\t\t\t\t */\n\t\t\t\tif (tcp_header_len ==\n\t\t\t\t    (sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) &&\n\t\t\t\t    tp->rcv_nxt == tp->rcv_wup)\n\t\t\t\t\ttcp_store_ts_recent(tp);\n\n\t\t\t\t/* We know that such packets are checksummed\n\t\t\t\t * on entry.\n\t\t\t\t */\n\t\t\t\ttcp_ack(sk, skb, 0);\n\t\t\t\t__kfree_skb(skb);\n\t\t\t\ttcp_data_snd_check(sk);\n\t\t\t\t/* When receiving pure ack in fast path, update\n\t\t\t\t * last ts ecr directly instead of calling\n\t\t\t\t * tcp_rcv_rtt_measure_ts()\n\t\t\t\t */\n\t\t\t\ttp->rcv_rtt_last_tsecr = tp->rx_opt.rcv_tsecr;\n\t\t\t\treturn;\n\t\t\t} else { /* Header too small */\n\t\t\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);\n\t\t\t\tgoto discard;\n\t\t\t}\n\t\t} else {\n\t\t\tint eaten = 0;\n\t\t\tbool fragstolen = false;\n\n\t\t\tif (tcp_checksum_complete(skb))\n\t\t\t\tgoto csum_error;\n\n\t\t\tif ((int)skb->truesize > sk->sk_forward_alloc)\n\t\t\t\tgoto step5;\n\n\t\t\t/* Predicted packet is in window by definition.\n\t\t\t * seq == rcv_nxt and rcv_wup <= rcv_nxt.\n\t\t\t * Hence, check seq<=rcv_wup reduces to:\n\t\t\t */\n\t\t\tif (tcp_header_len ==\n\t\t\t    (sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) &&\n\t\t\t    tp->rcv_nxt == tp->rcv_wup)\n\t\t\t\ttcp_store_ts_recent(tp);\n\n\t\t\ttcp_rcv_rtt_measure_ts(sk, skb);\n\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPHPHITS);\n\n\t\t\t/* Bulk data transfer: receiver */\n\t\t\t__skb_pull(skb, tcp_header_len);\n\t\t\teaten = tcp_queue_rcv(sk, skb, &fragstolen);\n\n\t\t\ttcp_event_data_recv(sk, skb);\n\n\t\t\tif (TCP_SKB_CB(skb)->ack_seq != tp->snd_una) {\n\t\t\t\t/* Well, only one small jumplet in fast path... */\n\t\t\t\ttcp_ack(sk, skb, FLAG_DATA);\n\t\t\t\ttcp_data_snd_check(sk);\n\t\t\t\tif (!inet_csk_ack_scheduled(sk))\n\t\t\t\t\tgoto no_ack;\n\t\t\t} else {\n\t\t\t\ttcp_update_wl(tp, TCP_SKB_CB(skb)->seq);\n\t\t\t}\n\n\t\t\t__tcp_ack_snd_check(sk, 0);\nno_ack:\n\t\t\tif (eaten)\n\t\t\t\tkfree_skb_partial(skb, fragstolen);\n\t\t\ttcp_data_ready(sk);\n\t\t\treturn;\n\t\t}\n\t}\n\nslow_path:\n\tif (len < (th->doff << 2) || tcp_checksum_complete(skb))\n\t\tgoto csum_error;\n\n\tif (!th->ack && !th->rst && !th->syn)\n\t\tgoto discard;\n\n\t/*\n\t *\tStandard slow path.\n\t */\n\n\tif (!tcp_validate_incoming(sk, skb, th, 1))\n\t\treturn;\n\nstep5:\n\tif (tcp_ack(sk, skb, FLAG_SLOWPATH | FLAG_UPDATE_TS_RECENT) < 0)\n\t\tgoto discard;\n\n\ttcp_rcv_rtt_measure_ts(sk, skb);\n\n\t/* Process urgent data. */\n\ttcp_urg(sk, skb, th);\n\n\t/* step 7: process the segment text */\n\ttcp_data_queue(sk, skb);\n\n\ttcp_data_snd_check(sk);\n\ttcp_ack_snd_check(sk);\n\treturn;\n\ncsum_error:\n\tTCP_INC_STATS(sock_net(sk), TCP_MIB_CSUMERRORS);\n\tTCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);\n\ndiscard:\n\ttcp_drop(sk, skb);\n}\nEXPORT_SYMBOL(tcp_rcv_established);\n\nvoid tcp_init_transfer(struct sock *sk, int bpf_op, struct sk_buff *skb)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttcp_mtup_init(sk);\n\ticsk->icsk_af_ops->rebuild_header(sk);\n\ttcp_init_metrics(sk);\n\n\t/* Initialize the congestion window to start the transfer.\n\t * Cut cwnd down to 1 per RFC5681 if SYN or SYN-ACK has been\n\t * retransmitted. In light of RFC6298 more aggressive 1sec\n\t * initRTO, we only reset cwnd when more than 1 SYN/SYN-ACK\n\t * retransmission has occurred.\n\t */\n\tif (tp->total_retrans > 1 && tp->undo_marker)\n\t\ttp->snd_cwnd = 1;\n\telse\n\t\ttp->snd_cwnd = tcp_init_cwnd(tp, __sk_dst_get(sk));\n\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\n\ticsk->icsk_ca_initialized = 0;\n\tbpf_skops_established(sk, bpf_op, skb);\n\tif (!icsk->icsk_ca_initialized)\n\t\ttcp_init_congestion_control(sk);\n\ttcp_init_buffer_space(sk);\n}\n\nvoid tcp_finish_connect(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\ttcp_set_state(sk, TCP_ESTABLISHED);\n\ticsk->icsk_ack.lrcvtime = tcp_jiffies32;\n\n\tif (skb) {\n\t\ticsk->icsk_af_ops->sk_rx_dst_set(sk, skb);\n\t\tsecurity_inet_conn_established(sk, skb);\n\t\tsk_mark_napi_id(sk, skb);\n\t}\n\n\ttcp_init_transfer(sk, BPF_SOCK_OPS_ACTIVE_ESTABLISHED_CB, skb);\n\n\t/* Prevent spurious tcp_cwnd_restart() on first data\n\t * packet.\n\t */\n\ttp->lsndtime = tcp_jiffies32;\n\n\tif (sock_flag(sk, SOCK_KEEPOPEN))\n\t\tinet_csk_reset_keepalive_timer(sk, keepalive_time_when(tp));\n\n\tif (!tp->rx_opt.snd_wscale)\n\t\t__tcp_fast_path_on(tp, tp->snd_wnd);\n\telse\n\t\ttp->pred_flags = 0;\n}\n\nstatic bool tcp_rcv_fastopen_synack(struct sock *sk, struct sk_buff *synack,\n\t\t\t\t    struct tcp_fastopen_cookie *cookie)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *data = tp->syn_data ? tcp_rtx_queue_head(sk) : NULL;\n\tu16 mss = tp->rx_opt.mss_clamp, try_exp = 0;\n\tbool syn_drop = false;\n\n\tif (mss == tp->rx_opt.user_mss) {\n\t\tstruct tcp_options_received opt;\n\n\t\t/* Get original SYNACK MSS value if user MSS sets mss_clamp */\n\t\ttcp_clear_options(&opt);\n\t\topt.user_mss = opt.mss_clamp = 0;\n\t\ttcp_parse_options(sock_net(sk), synack, &opt, 0, NULL);\n\t\tmss = opt.mss_clamp;\n\t}\n\n\tif (!tp->syn_fastopen) {\n\t\t/* Ignore an unsolicited cookie */\n\t\tcookie->len = -1;\n\t} else if (tp->total_retrans) {\n\t\t/* SYN timed out and the SYN-ACK neither has a cookie nor\n\t\t * acknowledges data. Presumably the remote received only\n\t\t * the retransmitted (regular) SYNs: either the original\n\t\t * SYN-data or the corresponding SYN-ACK was dropped.\n\t\t */\n\t\tsyn_drop = (cookie->len < 0 && data);\n\t} else if (cookie->len < 0 && !tp->syn_data) {\n\t\t/* We requested a cookie but didn't get it. If we did not use\n\t\t * the (old) exp opt format then try so next time (try_exp=1).\n\t\t * Otherwise we go back to use the RFC7413 opt (try_exp=2).\n\t\t */\n\t\ttry_exp = tp->syn_fastopen_exp ? 2 : 1;\n\t}\n\n\ttcp_fastopen_cache_set(sk, mss, cookie, syn_drop, try_exp);\n\n\tif (data) { /* Retransmit unacked data in SYN */\n\t\tif (tp->total_retrans)\n\t\t\ttp->fastopen_client_fail = TFO_SYN_RETRANSMITTED;\n\t\telse\n\t\t\ttp->fastopen_client_fail = TFO_DATA_NOT_ACKED;\n\t\tskb_rbtree_walk_from(data) {\n\t\t\tif (__tcp_retransmit_skb(sk, data, 1))\n\t\t\t\tbreak;\n\t\t}\n\t\ttcp_rearm_rto(sk);\n\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\tLINUX_MIB_TCPFASTOPENACTIVEFAIL);\n\t\treturn true;\n\t}\n\ttp->syn_data_acked = tp->syn_data;\n\tif (tp->syn_data_acked) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPFASTOPENACTIVE);\n\t\t/* SYN-data is counted as two separate packets in tcp_ack() */\n\t\tif (tp->delivered > 1)\n\t\t\t--tp->delivered;\n\t}\n\n\ttcp_fastopen_add_skb(sk, synack);\n\n\treturn false;\n}\n\nstatic void smc_check_reset_syn(struct tcp_sock *tp)\n{\n#if IS_ENABLED(CONFIG_SMC)\n\tif (static_branch_unlikely(&tcp_have_smc)) {\n\t\tif (tp->syn_smc && !tp->rx_opt.smc_ok)\n\t\t\ttp->syn_smc = 0;\n\t}\n#endif\n}\n\nstatic void tcp_try_undo_spurious_syn(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 syn_stamp;\n\n\t/* undo_marker is set when SYN or SYNACK times out. The timeout is\n\t * spurious if the ACK's timestamp option echo value matches the\n\t * original SYN timestamp.\n\t */\n\tsyn_stamp = tp->retrans_stamp;\n\tif (tp->undo_marker && syn_stamp && tp->rx_opt.saw_tstamp &&\n\t    syn_stamp == tp->rx_opt.rcv_tsecr)\n\t\ttp->undo_marker = 0;\n}\n\nstatic int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t const struct tcphdr *th)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_fastopen_cookie foc = { .len = -1 };\n\tint saved_clamp = tp->rx_opt.mss_clamp;\n\tbool fastopen_fail;\n\n\ttcp_parse_options(sock_net(sk), skb, &tp->rx_opt, 0, &foc);\n\tif (tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr)\n\t\ttp->rx_opt.rcv_tsecr -= tp->tsoffset;\n\n\tif (th->ack) {\n\t\t/* rfc793:\n\t\t * \"If the state is SYN-SENT then\n\t\t *    first check the ACK bit\n\t\t *      If the ACK bit is set\n\t\t *\t  If SEG.ACK =< ISS, or SEG.ACK > SND.NXT, send\n\t\t *        a reset (unless the RST bit is set, if so drop\n\t\t *        the segment and return)\"\n\t\t */\n\t\tif (!after(TCP_SKB_CB(skb)->ack_seq, tp->snd_una) ||\n\t\t    after(TCP_SKB_CB(skb)->ack_seq, tp->snd_nxt)) {\n\t\t\t/* Previous FIN/ACK or RST/ACK might be ignored. */\n\t\t\tif (icsk->icsk_retransmits == 0)\n\t\t\t\tinet_csk_reset_xmit_timer(sk,\n\t\t\t\t\t\tICSK_TIME_RETRANS,\n\t\t\t\t\t\tTCP_TIMEOUT_MIN, TCP_RTO_MAX);\n\t\t\tgoto reset_and_undo;\n\t\t}\n\n\t\tif (tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr &&\n\t\t    !between(tp->rx_opt.rcv_tsecr, tp->retrans_stamp,\n\t\t\t     tcp_time_stamp(tp))) {\n\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\tLINUX_MIB_PAWSACTIVEREJECTED);\n\t\t\tgoto reset_and_undo;\n\t\t}\n\n\t\t/* Now ACK is acceptable.\n\t\t *\n\t\t * \"If the RST bit is set\n\t\t *    If the ACK was acceptable then signal the user \"error:\n\t\t *    connection reset\", drop the segment, enter CLOSED state,\n\t\t *    delete TCB, and return.\"\n\t\t */\n\n\t\tif (th->rst) {\n\t\t\ttcp_reset(sk);\n\t\t\tgoto discard;\n\t\t}\n\n\t\t/* rfc793:\n\t\t *   \"fifth, if neither of the SYN or RST bits is set then\n\t\t *    drop the segment and return.\"\n\t\t *\n\t\t *    See note below!\n\t\t *                                        --ANK(990513)\n\t\t */\n\t\tif (!th->syn)\n\t\t\tgoto discard_and_undo;\n\n\t\t/* rfc793:\n\t\t *   \"If the SYN bit is on ...\n\t\t *    are acceptable then ...\n\t\t *    (our SYN has been ACKed), change the connection\n\t\t *    state to ESTABLISHED...\"\n\t\t */\n\n\t\ttcp_ecn_rcv_synack(tp, th);\n\n\t\ttcp_init_wl(tp, TCP_SKB_CB(skb)->seq);\n\t\ttcp_try_undo_spurious_syn(sk);\n\t\ttcp_ack(sk, skb, FLAG_SLOWPATH);\n\n\t\t/* Ok.. it's good. Set up sequence numbers and\n\t\t * move to established.\n\t\t */\n\t\tWRITE_ONCE(tp->rcv_nxt, TCP_SKB_CB(skb)->seq + 1);\n\t\ttp->rcv_wup = TCP_SKB_CB(skb)->seq + 1;\n\n\t\t/* RFC1323: The window in SYN & SYN/ACK segments is\n\t\t * never scaled.\n\t\t */\n\t\ttp->snd_wnd = ntohs(th->window);\n\n\t\tif (!tp->rx_opt.wscale_ok) {\n\t\t\ttp->rx_opt.snd_wscale = tp->rx_opt.rcv_wscale = 0;\n\t\t\ttp->window_clamp = min(tp->window_clamp, 65535U);\n\t\t}\n\n\t\tif (tp->rx_opt.saw_tstamp) {\n\t\t\ttp->rx_opt.tstamp_ok\t   = 1;\n\t\t\ttp->tcp_header_len =\n\t\t\t\tsizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED;\n\t\t\ttp->advmss\t    -= TCPOLEN_TSTAMP_ALIGNED;\n\t\t\ttcp_store_ts_recent(tp);\n\t\t} else {\n\t\t\ttp->tcp_header_len = sizeof(struct tcphdr);\n\t\t}\n\n\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\ttcp_initialize_rcv_mss(sk);\n\n\t\t/* Remember, tcp_poll() does not lock socket!\n\t\t * Change state from SYN-SENT only after copied_seq\n\t\t * is initialized. */\n\t\tWRITE_ONCE(tp->copied_seq, tp->rcv_nxt);\n\n\t\tsmc_check_reset_syn(tp);\n\n\t\tsmp_mb();\n\n\t\ttcp_finish_connect(sk, skb);\n\n\t\tfastopen_fail = (tp->syn_fastopen || tp->syn_data) &&\n\t\t\t\ttcp_rcv_fastopen_synack(sk, skb, &foc);\n\n\t\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\t\tsk->sk_state_change(sk);\n\t\t\tsk_wake_async(sk, SOCK_WAKE_IO, POLL_OUT);\n\t\t}\n\t\tif (fastopen_fail)\n\t\t\treturn -1;\n\t\tif (sk->sk_write_pending ||\n\t\t    icsk->icsk_accept_queue.rskq_defer_accept ||\n\t\t    inet_csk_in_pingpong_mode(sk)) {\n\t\t\t/* Save one ACK. Data will be ready after\n\t\t\t * several ticks, if write_pending is set.\n\t\t\t *\n\t\t\t * It may be deleted, but with this feature tcpdumps\n\t\t\t * look so _wonderfully_ clever, that I was not able\n\t\t\t * to stand against the temptation 8)     --ANK\n\t\t\t */\n\t\t\tinet_csk_schedule_ack(sk);\n\t\t\ttcp_enter_quickack_mode(sk, TCP_MAX_QUICKACKS);\n\t\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK,\n\t\t\t\t\t\t  TCP_DELACK_MAX, TCP_RTO_MAX);\n\ndiscard:\n\t\t\ttcp_drop(sk, skb);\n\t\t\treturn 0;\n\t\t} else {\n\t\t\ttcp_send_ack(sk);\n\t\t}\n\t\treturn -1;\n\t}\n\n\t/* No ACK in the segment */\n\n\tif (th->rst) {\n\t\t/* rfc793:\n\t\t * \"If the RST bit is set\n\t\t *\n\t\t *      Otherwise (no ACK) drop the segment and return.\"\n\t\t */\n\n\t\tgoto discard_and_undo;\n\t}\n\n\t/* PAWS check. */\n\tif (tp->rx_opt.ts_recent_stamp && tp->rx_opt.saw_tstamp &&\n\t    tcp_paws_reject(&tp->rx_opt, 0))\n\t\tgoto discard_and_undo;\n\n\tif (th->syn) {\n\t\t/* We see SYN without ACK. It is attempt of\n\t\t * simultaneous connect with crossed SYNs.\n\t\t * Particularly, it can be connect to self.\n\t\t */\n\t\ttcp_set_state(sk, TCP_SYN_RECV);\n\n\t\tif (tp->rx_opt.saw_tstamp) {\n\t\t\ttp->rx_opt.tstamp_ok = 1;\n\t\t\ttcp_store_ts_recent(tp);\n\t\t\ttp->tcp_header_len =\n\t\t\t\tsizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED;\n\t\t} else {\n\t\t\ttp->tcp_header_len = sizeof(struct tcphdr);\n\t\t}\n\n\t\tWRITE_ONCE(tp->rcv_nxt, TCP_SKB_CB(skb)->seq + 1);\n\t\tWRITE_ONCE(tp->copied_seq, tp->rcv_nxt);\n\t\ttp->rcv_wup = TCP_SKB_CB(skb)->seq + 1;\n\n\t\t/* RFC1323: The window in SYN & SYN/ACK segments is\n\t\t * never scaled.\n\t\t */\n\t\ttp->snd_wnd    = ntohs(th->window);\n\t\ttp->snd_wl1    = TCP_SKB_CB(skb)->seq;\n\t\ttp->max_window = tp->snd_wnd;\n\n\t\ttcp_ecn_rcv_syn(tp, th);\n\n\t\ttcp_mtup_init(sk);\n\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\ttcp_initialize_rcv_mss(sk);\n\n\t\ttcp_send_synack(sk);\n#if 0\n\t\t/* Note, we could accept data and URG from this segment.\n\t\t * There are no obstacles to make this (except that we must\n\t\t * either change tcp_recvmsg() to prevent it from returning data\n\t\t * before 3WHS completes per RFC793, or employ TCP Fast Open).\n\t\t *\n\t\t * However, if we ignore data in ACKless segments sometimes,\n\t\t * we have no reasons to accept it sometimes.\n\t\t * Also, seems the code doing it in step6 of tcp_rcv_state_process\n\t\t * is not flawless. So, discard packet for sanity.\n\t\t * Uncomment this return to process the data.\n\t\t */\n\t\treturn -1;\n#else\n\t\tgoto discard;\n#endif\n\t}\n\t/* \"fifth, if neither of the SYN or RST bits is set then\n\t * drop the segment and return.\"\n\t */\n\ndiscard_and_undo:\n\ttcp_clear_options(&tp->rx_opt);\n\ttp->rx_opt.mss_clamp = saved_clamp;\n\tgoto discard;\n\nreset_and_undo:\n\ttcp_clear_options(&tp->rx_opt);\n\ttp->rx_opt.mss_clamp = saved_clamp;\n\treturn 1;\n}\n\nstatic void tcp_rcv_synrecv_state_fastopen(struct sock *sk)\n{\n\tstruct request_sock *req;\n\n\t/* If we are still handling the SYNACK RTO, see if timestamp ECR allows\n\t * undo. If peer SACKs triggered fast recovery, we can't undo here.\n\t */\n\tif (inet_csk(sk)->icsk_ca_state == TCP_CA_Loss)\n\t\ttcp_try_undo_loss(sk, false);\n\n\t/* Reset rtx states to prevent spurious retransmits_timed_out() */\n\ttcp_sk(sk)->retrans_stamp = 0;\n\tinet_csk(sk)->icsk_retransmits = 0;\n\n\t/* Once we leave TCP_SYN_RECV or TCP_FIN_WAIT_1,\n\t * we no longer need req so release it.\n\t */\n\treq = rcu_dereference_protected(tcp_sk(sk)->fastopen_rsk,\n\t\t\t\t\tlockdep_sock_is_held(sk));\n\treqsk_fastopen_remove(sk, req, false);\n\n\t/* Re-arm the timer because data may have been sent out.\n\t * This is similar to the regular data transmission case\n\t * when new data has just been ack'ed.\n\t *\n\t * (TFO) - we could try to be more aggressive and\n\t * retransmitting any data sooner based on when they\n\t * are sent out.\n\t */\n\ttcp_rearm_rto(sk);\n}\n\n/*\n *\tThis function implements the receiving procedure of RFC 793 for\n *\tall states except ESTABLISHED and TIME_WAIT.\n *\tIt's called from both tcp_v4_rcv and tcp_v6_rcv and should be\n *\taddress independent.\n */\n\nint tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tstruct request_sock *req;\n\tint queued = 0;\n\tbool acceptable;\n\n\tswitch (sk->sk_state) {\n\tcase TCP_CLOSE:\n\t\tgoto discard;\n\n\tcase TCP_LISTEN:\n\t\tif (th->ack)\n\t\t\treturn 1;\n\n\t\tif (th->rst)\n\t\t\tgoto discard;\n\n\t\tif (th->syn) {\n\t\t\tif (th->fin)\n\t\t\t\tgoto discard;\n\t\t\t/* It is possible that we process SYN packets from backlog,\n\t\t\t * so we need to make sure to disable BH and RCU right there.\n\t\t\t */\n\t\t\trcu_read_lock();\n\t\t\tlocal_bh_disable();\n\t\t\tacceptable = icsk->icsk_af_ops->conn_request(sk, skb) >= 0;\n\t\t\tlocal_bh_enable();\n\t\t\trcu_read_unlock();\n\n\t\t\tif (!acceptable)\n\t\t\t\treturn 1;\n\t\t\tconsume_skb(skb);\n\t\t\treturn 0;\n\t\t}\n\t\tgoto discard;\n\n\tcase TCP_SYN_SENT:\n\t\ttp->rx_opt.saw_tstamp = 0;\n\t\ttcp_mstamp_refresh(tp);\n\t\tqueued = tcp_rcv_synsent_state_process(sk, skb, th);\n\t\tif (queued >= 0)\n\t\t\treturn queued;\n\n\t\t/* Do step6 onward by hand. */\n\t\ttcp_urg(sk, skb, th);\n\t\t__kfree_skb(skb);\n\t\ttcp_data_snd_check(sk);\n\t\treturn 0;\n\t}\n\n\ttcp_mstamp_refresh(tp);\n\ttp->rx_opt.saw_tstamp = 0;\n\treq = rcu_dereference_protected(tp->fastopen_rsk,\n\t\t\t\t\tlockdep_sock_is_held(sk));\n\tif (req) {\n\t\tbool req_stolen;\n\n\t\tWARN_ON_ONCE(sk->sk_state != TCP_SYN_RECV &&\n\t\t    sk->sk_state != TCP_FIN_WAIT1);\n\n\t\tif (!tcp_check_req(sk, skb, req, true, &req_stolen))\n\t\t\tgoto discard;\n\t}\n\n\tif (!th->ack && !th->rst && !th->syn)\n\t\tgoto discard;\n\n\tif (!tcp_validate_incoming(sk, skb, th, 0))\n\t\treturn 0;\n\n\t/* step 5: check the ACK field */\n\tacceptable = tcp_ack(sk, skb, FLAG_SLOWPATH |\n\t\t\t\t      FLAG_UPDATE_TS_RECENT |\n\t\t\t\t      FLAG_NO_CHALLENGE_ACK) > 0;\n\n\tif (!acceptable) {\n\t\tif (sk->sk_state == TCP_SYN_RECV)\n\t\t\treturn 1;\t/* send one RST */\n\t\ttcp_send_challenge_ack(sk, skb);\n\t\tgoto discard;\n\t}\n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_RECV:\n\t\ttp->delivered++; /* SYN-ACK delivery isn't tracked in tcp_ack */\n\t\tif (!tp->srtt_us)\n\t\t\ttcp_synack_rtt_meas(sk, req);\n\n\t\tif (req) {\n\t\t\ttcp_rcv_synrecv_state_fastopen(sk);\n\t\t} else {\n\t\t\ttcp_try_undo_spurious_syn(sk);\n\t\t\ttp->retrans_stamp = 0;\n\t\t\ttcp_init_transfer(sk, BPF_SOCK_OPS_PASSIVE_ESTABLISHED_CB,\n\t\t\t\t\t  skb);\n\t\t\tWRITE_ONCE(tp->copied_seq, tp->rcv_nxt);\n\t\t}\n\t\tsmp_mb();\n\t\ttcp_set_state(sk, TCP_ESTABLISHED);\n\t\tsk->sk_state_change(sk);\n\n\t\t/* Note, that this wakeup is only for marginal crossed SYN case.\n\t\t * Passively open sockets are not waked up, because\n\t\t * sk->sk_sleep == NULL and sk->sk_socket == NULL.\n\t\t */\n\t\tif (sk->sk_socket)\n\t\t\tsk_wake_async(sk, SOCK_WAKE_IO, POLL_OUT);\n\n\t\ttp->snd_una = TCP_SKB_CB(skb)->ack_seq;\n\t\ttp->snd_wnd = ntohs(th->window) << tp->rx_opt.snd_wscale;\n\t\ttcp_init_wl(tp, TCP_SKB_CB(skb)->seq);\n\n\t\tif (tp->rx_opt.tstamp_ok)\n\t\t\ttp->advmss -= TCPOLEN_TSTAMP_ALIGNED;\n\n\t\tif (!inet_csk(sk)->icsk_ca_ops->cong_control)\n\t\t\ttcp_update_pacing_rate(sk);\n\n\t\t/* Prevent spurious tcp_cwnd_restart() on first data packet */\n\t\ttp->lsndtime = tcp_jiffies32;\n\n\t\ttcp_initialize_rcv_mss(sk);\n\t\ttcp_fast_path_on(tp);\n\t\tbreak;\n\n\tcase TCP_FIN_WAIT1: {\n\t\tint tmo;\n\n\t\tif (req)\n\t\t\ttcp_rcv_synrecv_state_fastopen(sk);\n\n\t\tif (tp->snd_una != tp->write_seq)\n\t\t\tbreak;\n\n\t\ttcp_set_state(sk, TCP_FIN_WAIT2);\n\t\tsk->sk_shutdown |= SEND_SHUTDOWN;\n\n\t\tsk_dst_confirm(sk);\n\n\t\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\t\t/* Wake up lingering close() */\n\t\t\tsk->sk_state_change(sk);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (tp->linger2 < 0) {\n\t\t\ttcp_done(sk);\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\n\t\t\treturn 1;\n\t\t}\n\t\tif (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq &&\n\t\t    after(TCP_SKB_CB(skb)->end_seq - th->fin, tp->rcv_nxt)) {\n\t\t\t/* Receive out of order FIN after close() */\n\t\t\tif (tp->syn_fastopen && th->fin)\n\t\t\t\ttcp_fastopen_active_disable(sk);\n\t\t\ttcp_done(sk);\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\n\t\t\treturn 1;\n\t\t}\n\n\t\ttmo = tcp_fin_time(sk);\n\t\tif (tmo > TCP_TIMEWAIT_LEN) {\n\t\t\tinet_csk_reset_keepalive_timer(sk, tmo - TCP_TIMEWAIT_LEN);\n\t\t} else if (th->fin || sock_owned_by_user(sk)) {\n\t\t\t/* Bad case. We could lose such FIN otherwise.\n\t\t\t * It is not a big problem, but it looks confusing\n\t\t\t * and not so rare event. We still can lose it now,\n\t\t\t * if it spins in bh_lock_sock(), but it is really\n\t\t\t * marginal case.\n\t\t\t */\n\t\t\tinet_csk_reset_keepalive_timer(sk, tmo);\n\t\t} else {\n\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\n\t\t\tgoto discard;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase TCP_CLOSING:\n\t\tif (tp->snd_una == tp->write_seq) {\n\t\t\ttcp_time_wait(sk, TCP_TIME_WAIT, 0);\n\t\t\tgoto discard;\n\t\t}\n\t\tbreak;\n\n\tcase TCP_LAST_ACK:\n\t\tif (tp->snd_una == tp->write_seq) {\n\t\t\ttcp_update_metrics(sk);\n\t\t\ttcp_done(sk);\n\t\t\tgoto discard;\n\t\t}\n\t\tbreak;\n\t}\n\n\t/* step 6: check the URG bit */\n\ttcp_urg(sk, skb, th);\n\n\t/* step 7: process the segment text */\n\tswitch (sk->sk_state) {\n\tcase TCP_CLOSE_WAIT:\n\tcase TCP_CLOSING:\n\tcase TCP_LAST_ACK:\n\t\tif (!before(TCP_SKB_CB(skb)->seq, tp->rcv_nxt)) {\n\t\t\tif (sk_is_mptcp(sk))\n\t\t\t\tmptcp_incoming_options(sk, skb);\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\tcase TCP_FIN_WAIT1:\n\tcase TCP_FIN_WAIT2:\n\t\t/* RFC 793 says to queue data in these states,\n\t\t * RFC 1122 says we MUST send a reset.\n\t\t * BSD 4.4 also does reset.\n\t\t */\n\t\tif (sk->sk_shutdown & RCV_SHUTDOWN) {\n\t\t\tif (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq &&\n\t\t\t    after(TCP_SKB_CB(skb)->end_seq - th->fin, tp->rcv_nxt)) {\n\t\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\n\t\t\t\ttcp_reset(sk);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t}\n\t\tfallthrough;\n\tcase TCP_ESTABLISHED:\n\t\ttcp_data_queue(sk, skb);\n\t\tqueued = 1;\n\t\tbreak;\n\t}\n\n\t/* tcp_data could move socket to TIME-WAIT */\n\tif (sk->sk_state != TCP_CLOSE) {\n\t\ttcp_data_snd_check(sk);\n\t\ttcp_ack_snd_check(sk);\n\t}\n\n\tif (!queued) {\ndiscard:\n\t\ttcp_drop(sk, skb);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_rcv_state_process);\n\nstatic inline void pr_drop_req(struct request_sock *req, __u16 port, int family)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\n\tif (family == AF_INET)\n\t\tnet_dbg_ratelimited(\"drop open request from %pI4/%u\\n\",\n\t\t\t\t    &ireq->ir_rmt_addr, port);\n#if IS_ENABLED(CONFIG_IPV6)\n\telse if (family == AF_INET6)\n\t\tnet_dbg_ratelimited(\"drop open request from %pI6/%u\\n\",\n\t\t\t\t    &ireq->ir_v6_rmt_addr, port);\n#endif\n}\n\n/* RFC3168 : 6.1.1 SYN packets must not have ECT/ECN bits set\n *\n * If we receive a SYN packet with these bits set, it means a\n * network is playing bad games with TOS bits. In order to\n * avoid possible false congestion notifications, we disable\n * TCP ECN negotiation.\n *\n * Exception: tcp_ca wants ECN. This is required for DCTCP\n * congestion control: Linux DCTCP asserts ECT on all packets,\n * including SYN, which is most optimal solution; however,\n * others, such as FreeBSD do not.\n *\n * Exception: At least one of the reserved bits of the TCP header (th->res1) is\n * set, indicating the use of a future TCP extension (such as AccECN). See\n * RFC8311 \u00a74.3 which updates RFC3168 to allow the development of such\n * extensions.\n */\nstatic void tcp_ecn_create_request(struct request_sock *req,\n\t\t\t\t   const struct sk_buff *skb,\n\t\t\t\t   const struct sock *listen_sk,\n\t\t\t\t   const struct dst_entry *dst)\n{\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tconst struct net *net = sock_net(listen_sk);\n\tbool th_ecn = th->ece && th->cwr;\n\tbool ect, ecn_ok;\n\tu32 ecn_ok_dst;\n\n\tif (!th_ecn)\n\t\treturn;\n\n\tect = !INET_ECN_is_not_ect(TCP_SKB_CB(skb)->ip_dsfield);\n\tecn_ok_dst = dst_feature(dst, DST_FEATURE_ECN_MASK);\n\tecn_ok = net->ipv4.sysctl_tcp_ecn || ecn_ok_dst;\n\n\tif (((!ect || th->res1) && ecn_ok) || tcp_ca_needs_ecn(listen_sk) ||\n\t    (ecn_ok_dst & DST_FEATURE_ECN_CA) ||\n\t    tcp_bpf_ca_needs_ecn((struct sock *)req))\n\t\tinet_rsk(req)->ecn_ok = 1;\n}\n\nstatic void tcp_openreq_init(struct request_sock *req,\n\t\t\t     const struct tcp_options_received *rx_opt,\n\t\t\t     struct sk_buff *skb, const struct sock *sk)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\n\treq->rsk_rcv_wnd = 0;\t\t/* So that tcp_send_synack() knows! */\n\ttcp_rsk(req)->rcv_isn = TCP_SKB_CB(skb)->seq;\n\ttcp_rsk(req)->rcv_nxt = TCP_SKB_CB(skb)->seq + 1;\n\ttcp_rsk(req)->snt_synack = 0;\n\ttcp_rsk(req)->last_oow_ack_time = 0;\n\treq->mss = rx_opt->mss_clamp;\n\treq->ts_recent = rx_opt->saw_tstamp ? rx_opt->rcv_tsval : 0;\n\tireq->tstamp_ok = rx_opt->tstamp_ok;\n\tireq->sack_ok = rx_opt->sack_ok;\n\tireq->snd_wscale = rx_opt->snd_wscale;\n\tireq->wscale_ok = rx_opt->wscale_ok;\n\tireq->acked = 0;\n\tireq->ecn_ok = 0;\n\tireq->ir_rmt_port = tcp_hdr(skb)->source;\n\tireq->ir_num = ntohs(tcp_hdr(skb)->dest);\n\tireq->ir_mark = inet_request_mark(sk, skb);\n#if IS_ENABLED(CONFIG_SMC)\n\tireq->smc_ok = rx_opt->smc_ok;\n#endif\n}\n\nstruct request_sock *inet_reqsk_alloc(const struct request_sock_ops *ops,\n\t\t\t\t      struct sock *sk_listener,\n\t\t\t\t      bool attach_listener)\n{\n\tstruct request_sock *req = reqsk_alloc(ops, sk_listener,\n\t\t\t\t\t       attach_listener);\n\n\tif (req) {\n\t\tstruct inet_request_sock *ireq = inet_rsk(req);\n\n\t\tireq->ireq_opt = NULL;\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tireq->pktopts = NULL;\n#endif\n\t\tatomic64_set(&ireq->ir_cookie, 0);\n\t\tireq->ireq_state = TCP_NEW_SYN_RECV;\n\t\twrite_pnet(&ireq->ireq_net, sock_net(sk_listener));\n\t\tireq->ireq_family = sk_listener->sk_family;\n\t}\n\n\treturn req;\n}\nEXPORT_SYMBOL(inet_reqsk_alloc);\n\n/*\n * Return true if a syncookie should be sent\n */\nstatic bool tcp_syn_flood_action(const struct sock *sk, const char *proto)\n{\n\tstruct request_sock_queue *queue = &inet_csk(sk)->icsk_accept_queue;\n\tconst char *msg = \"Dropping request\";\n\tbool want_cookie = false;\n\tstruct net *net = sock_net(sk);\n\n#ifdef CONFIG_SYN_COOKIES\n\tif (net->ipv4.sysctl_tcp_syncookies) {\n\t\tmsg = \"Sending cookies\";\n\t\twant_cookie = true;\n\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPREQQFULLDOCOOKIES);\n\t} else\n#endif\n\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPREQQFULLDROP);\n\n\tif (!queue->synflood_warned &&\n\t    net->ipv4.sysctl_tcp_syncookies != 2 &&\n\t    xchg(&queue->synflood_warned, 1) == 0)\n\t\tnet_info_ratelimited(\"%s: Possible SYN flooding on port %d. %s.  Check SNMP counters.\\n\",\n\t\t\t\t     proto, sk->sk_num, msg);\n\n\treturn want_cookie;\n}\n\nstatic void tcp_reqsk_record_syn(const struct sock *sk,\n\t\t\t\t struct request_sock *req,\n\t\t\t\t const struct sk_buff *skb)\n{\n\tif (tcp_sk(sk)->save_syn) {\n\t\tu32 len = skb_network_header_len(skb) + tcp_hdrlen(skb);\n\t\tstruct saved_syn *saved_syn;\n\t\tu32 mac_hdrlen;\n\t\tvoid *base;\n\n\t\tif (tcp_sk(sk)->save_syn == 2) {  /* Save full header. */\n\t\t\tbase = skb_mac_header(skb);\n\t\t\tmac_hdrlen = skb_mac_header_len(skb);\n\t\t\tlen += mac_hdrlen;\n\t\t} else {\n\t\t\tbase = skb_network_header(skb);\n\t\t\tmac_hdrlen = 0;\n\t\t}\n\n\t\tsaved_syn = kmalloc(struct_size(saved_syn, data, len),\n\t\t\t\t    GFP_ATOMIC);\n\t\tif (saved_syn) {\n\t\t\tsaved_syn->mac_hdrlen = mac_hdrlen;\n\t\t\tsaved_syn->network_hdrlen = skb_network_header_len(skb);\n\t\t\tsaved_syn->tcp_hdrlen = tcp_hdrlen(skb);\n\t\t\tmemcpy(saved_syn->data, base, len);\n\t\t\treq->saved_syn = saved_syn;\n\t\t}\n\t}\n}\n\n/* If a SYN cookie is required and supported, returns a clamped MSS value to be\n * used for SYN cookie generation.\n */\nu16 tcp_get_syncookie_mss(struct request_sock_ops *rsk_ops,\n\t\t\t  const struct tcp_request_sock_ops *af_ops,\n\t\t\t  struct sock *sk, struct tcphdr *th)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu16 mss;\n\n\tif (sock_net(sk)->ipv4.sysctl_tcp_syncookies != 2 &&\n\t    !inet_csk_reqsk_queue_is_full(sk))\n\t\treturn 0;\n\n\tif (!tcp_syn_flood_action(sk, rsk_ops->slab_name))\n\t\treturn 0;\n\n\tif (sk_acceptq_is_full(sk)) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\n\t\treturn 0;\n\t}\n\n\tmss = tcp_parse_mss_option(th, tp->rx_opt.user_mss);\n\tif (!mss)\n\t\tmss = af_ops->mss_clamp;\n\n\treturn mss;\n}\nEXPORT_SYMBOL_GPL(tcp_get_syncookie_mss);\n\nint tcp_conn_request(struct request_sock_ops *rsk_ops,\n\t\t     const struct tcp_request_sock_ops *af_ops,\n\t\t     struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_fastopen_cookie foc = { .len = -1 };\n\t__u32 isn = TCP_SKB_CB(skb)->tcp_tw_isn;\n\tstruct tcp_options_received tmp_opt;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct sock *fastopen_sk = NULL;\n\tstruct request_sock *req;\n\tbool want_cookie = false;\n\tstruct dst_entry *dst;\n\tstruct flowi fl;\n\n\t/* TW buckets are converted to open requests without\n\t * limitations, they conserve resources and peer is\n\t * evidently real one.\n\t */\n\tif ((net->ipv4.sysctl_tcp_syncookies == 2 ||\n\t     inet_csk_reqsk_queue_is_full(sk)) && !isn) {\n\t\twant_cookie = tcp_syn_flood_action(sk, rsk_ops->slab_name);\n\t\tif (!want_cookie)\n\t\t\tgoto drop;\n\t}\n\n\tif (sk_acceptq_is_full(sk)) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\n\t\tgoto drop;\n\t}\n\n\treq = inet_reqsk_alloc(rsk_ops, sk, !want_cookie);\n\tif (!req)\n\t\tgoto drop;\n\n\treq->syncookie = want_cookie;\n\ttcp_rsk(req)->af_specific = af_ops;\n\ttcp_rsk(req)->ts_off = 0;\n#if IS_ENABLED(CONFIG_MPTCP)\n\ttcp_rsk(req)->is_mptcp = 0;\n#endif\n\n\ttcp_clear_options(&tmp_opt);\n\ttmp_opt.mss_clamp = af_ops->mss_clamp;\n\ttmp_opt.user_mss  = tp->rx_opt.user_mss;\n\ttcp_parse_options(sock_net(sk), skb, &tmp_opt, 0,\n\t\t\t  want_cookie ? NULL : &foc);\n\n\tif (want_cookie && !tmp_opt.saw_tstamp)\n\t\ttcp_clear_options(&tmp_opt);\n\n\tif (IS_ENABLED(CONFIG_SMC) && want_cookie)\n\t\ttmp_opt.smc_ok = 0;\n\n\ttmp_opt.tstamp_ok = tmp_opt.saw_tstamp;\n\ttcp_openreq_init(req, &tmp_opt, skb, sk);\n\tinet_rsk(req)->no_srccheck = inet_sk(sk)->transparent;\n\n\t/* Note: tcp_v6_init_req() might override ir_iif for link locals */\n\tinet_rsk(req)->ir_iif = inet_request_bound_dev_if(sk, skb);\n\n\taf_ops->init_req(req, sk, skb);\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto drop_and_free;\n\n\tif (tmp_opt.tstamp_ok)\n\t\ttcp_rsk(req)->ts_off = af_ops->init_ts_off(net, skb);\n\n\tdst = af_ops->route_req(sk, &fl, req);\n\tif (!dst)\n\t\tgoto drop_and_free;\n\n\tif (!want_cookie && !isn) {\n\t\t/* Kill the following clause, if you dislike this way. */\n\t\tif (!net->ipv4.sysctl_tcp_syncookies &&\n\t\t    (net->ipv4.sysctl_max_syn_backlog - inet_csk_reqsk_queue_len(sk) <\n\t\t     (net->ipv4.sysctl_max_syn_backlog >> 2)) &&\n\t\t    !tcp_peer_is_proven(req, dst)) {\n\t\t\t/* Without syncookies last quarter of\n\t\t\t * backlog is filled with destinations,\n\t\t\t * proven to be alive.\n\t\t\t * It means that we continue to communicate\n\t\t\t * to destinations, already remembered\n\t\t\t * to the moment of synflood.\n\t\t\t */\n\t\t\tpr_drop_req(req, ntohs(tcp_hdr(skb)->source),\n\t\t\t\t    rsk_ops->family);\n\t\t\tgoto drop_and_release;\n\t\t}\n\n\t\tisn = af_ops->init_seq(skb);\n\t}\n\n\ttcp_ecn_create_request(req, skb, sk, dst);\n\n\tif (want_cookie) {\n\t\tisn = cookie_init_sequence(af_ops, sk, skb, &req->mss);\n\t\tif (!tmp_opt.tstamp_ok)\n\t\t\tinet_rsk(req)->ecn_ok = 0;\n\t}\n\n\ttcp_rsk(req)->snt_isn = isn;\n\ttcp_rsk(req)->txhash = net_tx_rndhash();\n\ttcp_rsk(req)->syn_tos = TCP_SKB_CB(skb)->ip_dsfield;\n\ttcp_openreq_init_rwin(req, sk, dst);\n\tsk_rx_queue_set(req_to_sk(req), skb);\n\tif (!want_cookie) {\n\t\ttcp_reqsk_record_syn(sk, req, skb);\n\t\tfastopen_sk = tcp_try_fastopen(sk, skb, req, &foc, dst);\n\t}\n\tif (fastopen_sk) {\n\t\taf_ops->send_synack(fastopen_sk, dst, &fl, req,\n\t\t\t\t    &foc, TCP_SYNACK_FASTOPEN, skb);\n\t\t/* Add the child socket directly into the accept queue */\n\t\tif (!inet_csk_reqsk_queue_add(sk, req, fastopen_sk)) {\n\t\t\treqsk_fastopen_remove(fastopen_sk, req, false);\n\t\t\tbh_unlock_sock(fastopen_sk);\n\t\t\tsock_put(fastopen_sk);\n\t\t\tgoto drop_and_free;\n\t\t}\n\t\tsk->sk_data_ready(sk);\n\t\tbh_unlock_sock(fastopen_sk);\n\t\tsock_put(fastopen_sk);\n\t} else {\n\t\ttcp_rsk(req)->tfo_listener = false;\n\t\tif (!want_cookie)\n\t\t\tinet_csk_reqsk_queue_hash_add(sk, req,\n\t\t\t\ttcp_timeout_init((struct sock *)req));\n\t\taf_ops->send_synack(sk, dst, &fl, req, &foc,\n\t\t\t\t    !want_cookie ? TCP_SYNACK_NORMAL :\n\t\t\t\t\t\t   TCP_SYNACK_COOKIE,\n\t\t\t\t    skb);\n\t\tif (want_cookie) {\n\t\t\treqsk_free(req);\n\t\t\treturn 0;\n\t\t}\n\t}\n\treqsk_put(req);\n\treturn 0;\n\ndrop_and_release:\n\tdst_release(dst);\ndrop_and_free:\n\t__reqsk_free(req);\ndrop:\n\ttcp_listendrop(sk);\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_conn_request);\n"}, "1": {"id": 1, "path": "/src/include/linux/skbuff.h", "content": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/*\n *\tDefinitions for the 'struct sk_buff' memory handlers.\n *\n *\tAuthors:\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\t\tFlorian La Roche, <rzsfl@rz.uni-sb.de>\n */\n\n#ifndef _LINUX_SKBUFF_H\n#define _LINUX_SKBUFF_H\n\n#include <linux/kernel.h>\n#include <linux/compiler.h>\n#include <linux/time.h>\n#include <linux/bug.h>\n#include <linux/bvec.h>\n#include <linux/cache.h>\n#include <linux/rbtree.h>\n#include <linux/socket.h>\n#include <linux/refcount.h>\n\n#include <linux/atomic.h>\n#include <asm/types.h>\n#include <linux/spinlock.h>\n#include <linux/net.h>\n#include <linux/textsearch.h>\n#include <net/checksum.h>\n#include <linux/rcupdate.h>\n#include <linux/hrtimer.h>\n#include <linux/dma-mapping.h>\n#include <linux/netdev_features.h>\n#include <linux/sched.h>\n#include <linux/sched/clock.h>\n#include <net/flow_dissector.h>\n#include <linux/splice.h>\n#include <linux/in6.h>\n#include <linux/if_packet.h>\n#include <net/flow.h>\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n#include <linux/netfilter/nf_conntrack_common.h>\n#endif\n\n/* The interface for checksum offload between the stack and networking drivers\n * is as follows...\n *\n * A. IP checksum related features\n *\n * Drivers advertise checksum offload capabilities in the features of a device.\n * From the stack's point of view these are capabilities offered by the driver.\n * A driver typically only advertises features that it is capable of offloading\n * to its device.\n *\n * The checksum related features are:\n *\n *\tNETIF_F_HW_CSUM\t- The driver (or its device) is able to compute one\n *\t\t\t  IP (one's complement) checksum for any combination\n *\t\t\t  of protocols or protocol layering. The checksum is\n *\t\t\t  computed and set in a packet per the CHECKSUM_PARTIAL\n *\t\t\t  interface (see below).\n *\n *\tNETIF_F_IP_CSUM - Driver (device) is only able to checksum plain\n *\t\t\t  TCP or UDP packets over IPv4. These are specifically\n *\t\t\t  unencapsulated packets of the form IPv4|TCP or\n *\t\t\t  IPv4|UDP where the Protocol field in the IPv4 header\n *\t\t\t  is TCP or UDP. The IPv4 header may contain IP options.\n *\t\t\t  This feature cannot be set in features for a device\n *\t\t\t  with NETIF_F_HW_CSUM also set. This feature is being\n *\t\t\t  DEPRECATED (see below).\n *\n *\tNETIF_F_IPV6_CSUM - Driver (device) is only able to checksum plain\n *\t\t\t  TCP or UDP packets over IPv6. These are specifically\n *\t\t\t  unencapsulated packets of the form IPv6|TCP or\n *\t\t\t  IPv6|UDP where the Next Header field in the IPv6\n *\t\t\t  header is either TCP or UDP. IPv6 extension headers\n *\t\t\t  are not supported with this feature. This feature\n *\t\t\t  cannot be set in features for a device with\n *\t\t\t  NETIF_F_HW_CSUM also set. This feature is being\n *\t\t\t  DEPRECATED (see below).\n *\n *\tNETIF_F_RXCSUM - Driver (device) performs receive checksum offload.\n *\t\t\t This flag is only used to disable the RX checksum\n *\t\t\t feature for a device. The stack will accept receive\n *\t\t\t checksum indication in packets received on a device\n *\t\t\t regardless of whether NETIF_F_RXCSUM is set.\n *\n * B. Checksumming of received packets by device. Indication of checksum\n *    verification is set in skb->ip_summed. Possible values are:\n *\n * CHECKSUM_NONE:\n *\n *   Device did not checksum this packet e.g. due to lack of capabilities.\n *   The packet contains full (though not verified) checksum in packet but\n *   not in skb->csum. Thus, skb->csum is undefined in this case.\n *\n * CHECKSUM_UNNECESSARY:\n *\n *   The hardware you're dealing with doesn't calculate the full checksum\n *   (as in CHECKSUM_COMPLETE), but it does parse headers and verify checksums\n *   for specific protocols. For such packets it will set CHECKSUM_UNNECESSARY\n *   if their checksums are okay. skb->csum is still undefined in this case\n *   though. A driver or device must never modify the checksum field in the\n *   packet even if checksum is verified.\n *\n *   CHECKSUM_UNNECESSARY is applicable to following protocols:\n *     TCP: IPv6 and IPv4.\n *     UDP: IPv4 and IPv6. A device may apply CHECKSUM_UNNECESSARY to a\n *       zero UDP checksum for either IPv4 or IPv6, the networking stack\n *       may perform further validation in this case.\n *     GRE: only if the checksum is present in the header.\n *     SCTP: indicates the CRC in SCTP header has been validated.\n *     FCOE: indicates the CRC in FC frame has been validated.\n *\n *   skb->csum_level indicates the number of consecutive checksums found in\n *   the packet minus one that have been verified as CHECKSUM_UNNECESSARY.\n *   For instance if a device receives an IPv6->UDP->GRE->IPv4->TCP packet\n *   and a device is able to verify the checksums for UDP (possibly zero),\n *   GRE (checksum flag is set) and TCP, skb->csum_level would be set to\n *   two. If the device were only able to verify the UDP checksum and not\n *   GRE, either because it doesn't support GRE checksum or because GRE\n *   checksum is bad, skb->csum_level would be set to zero (TCP checksum is\n *   not considered in this case).\n *\n * CHECKSUM_COMPLETE:\n *\n *   This is the most generic way. The device supplied checksum of the _whole_\n *   packet as seen by netif_rx() and fills in skb->csum. This means the\n *   hardware doesn't need to parse L3/L4 headers to implement this.\n *\n *   Notes:\n *   - Even if device supports only some protocols, but is able to produce\n *     skb->csum, it MUST use CHECKSUM_COMPLETE, not CHECKSUM_UNNECESSARY.\n *   - CHECKSUM_COMPLETE is not applicable to SCTP and FCoE protocols.\n *\n * CHECKSUM_PARTIAL:\n *\n *   A checksum is set up to be offloaded to a device as described in the\n *   output description for CHECKSUM_PARTIAL. This may occur on a packet\n *   received directly from another Linux OS, e.g., a virtualized Linux kernel\n *   on the same host, or it may be set in the input path in GRO or remote\n *   checksum offload. For the purposes of checksum verification, the checksum\n *   referred to by skb->csum_start + skb->csum_offset and any preceding\n *   checksums in the packet are considered verified. Any checksums in the\n *   packet that are after the checksum being offloaded are not considered to\n *   be verified.\n *\n * C. Checksumming on transmit for non-GSO. The stack requests checksum offload\n *    in the skb->ip_summed for a packet. Values are:\n *\n * CHECKSUM_PARTIAL:\n *\n *   The driver is required to checksum the packet as seen by hard_start_xmit()\n *   from skb->csum_start up to the end, and to record/write the checksum at\n *   offset skb->csum_start + skb->csum_offset. A driver may verify that the\n *   csum_start and csum_offset values are valid values given the length and\n *   offset of the packet, but it should not attempt to validate that the\n *   checksum refers to a legitimate transport layer checksum -- it is the\n *   purview of the stack to validate that csum_start and csum_offset are set\n *   correctly.\n *\n *   When the stack requests checksum offload for a packet, the driver MUST\n *   ensure that the checksum is set correctly. A driver can either offload the\n *   checksum calculation to the device, or call skb_checksum_help (in the case\n *   that the device does not support offload for a particular checksum).\n *\n *   NETIF_F_IP_CSUM and NETIF_F_IPV6_CSUM are being deprecated in favor of\n *   NETIF_F_HW_CSUM. New devices should use NETIF_F_HW_CSUM to indicate\n *   checksum offload capability.\n *   skb_csum_hwoffload_help() can be called to resolve CHECKSUM_PARTIAL based\n *   on network device checksumming capabilities: if a packet does not match\n *   them, skb_checksum_help or skb_crc32c_help (depending on the value of\n *   csum_not_inet, see item D.) is called to resolve the checksum.\n *\n * CHECKSUM_NONE:\n *\n *   The skb was already checksummed by the protocol, or a checksum is not\n *   required.\n *\n * CHECKSUM_UNNECESSARY:\n *\n *   This has the same meaning as CHECKSUM_NONE for checksum offload on\n *   output.\n *\n * CHECKSUM_COMPLETE:\n *   Not used in checksum output. If a driver observes a packet with this value\n *   set in skbuff, it should treat the packet as if CHECKSUM_NONE were set.\n *\n * D. Non-IP checksum (CRC) offloads\n *\n *   NETIF_F_SCTP_CRC - This feature indicates that a device is capable of\n *     offloading the SCTP CRC in a packet. To perform this offload the stack\n *     will set csum_start and csum_offset accordingly, set ip_summed to\n *     CHECKSUM_PARTIAL and set csum_not_inet to 1, to provide an indication in\n *     the skbuff that the CHECKSUM_PARTIAL refers to CRC32c.\n *     A driver that supports both IP checksum offload and SCTP CRC32c offload\n *     must verify which offload is configured for a packet by testing the\n *     value of skb->csum_not_inet; skb_crc32c_csum_help is provided to resolve\n *     CHECKSUM_PARTIAL on skbs where csum_not_inet is set to 1.\n *\n *   NETIF_F_FCOE_CRC - This feature indicates that a device is capable of\n *     offloading the FCOE CRC in a packet. To perform this offload the stack\n *     will set ip_summed to CHECKSUM_PARTIAL and set csum_start and csum_offset\n *     accordingly. Note that there is no indication in the skbuff that the\n *     CHECKSUM_PARTIAL refers to an FCOE checksum, so a driver that supports\n *     both IP checksum offload and FCOE CRC offload must verify which offload\n *     is configured for a packet, presumably by inspecting packet headers.\n *\n * E. Checksumming on output with GSO.\n *\n * In the case of a GSO packet (skb_is_gso(skb) is true), checksum offload\n * is implied by the SKB_GSO_* flags in gso_type. Most obviously, if the\n * gso_type is SKB_GSO_TCPV4 or SKB_GSO_TCPV6, TCP checksum offload as\n * part of the GSO operation is implied. If a checksum is being offloaded\n * with GSO then ip_summed is CHECKSUM_PARTIAL, and both csum_start and\n * csum_offset are set to refer to the outermost checksum being offloaded\n * (two offloaded checksums are possible with UDP encapsulation).\n */\n\n/* Don't change this without changing skb_csum_unnecessary! */\n#define CHECKSUM_NONE\t\t0\n#define CHECKSUM_UNNECESSARY\t1\n#define CHECKSUM_COMPLETE\t2\n#define CHECKSUM_PARTIAL\t3\n\n/* Maximum value in skb->csum_level */\n#define SKB_MAX_CSUM_LEVEL\t3\n\n#define SKB_DATA_ALIGN(X)\tALIGN(X, SMP_CACHE_BYTES)\n#define SKB_WITH_OVERHEAD(X)\t\\\n\t((X) - SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))\n#define SKB_MAX_ORDER(X, ORDER) \\\n\tSKB_WITH_OVERHEAD((PAGE_SIZE << (ORDER)) - (X))\n#define SKB_MAX_HEAD(X)\t\t(SKB_MAX_ORDER((X), 0))\n#define SKB_MAX_ALLOC\t\t(SKB_MAX_ORDER(0, 2))\n\n/* return minimum truesize of one skb containing X bytes of data */\n#define SKB_TRUESIZE(X) ((X) +\t\t\t\t\t\t\\\n\t\t\t SKB_DATA_ALIGN(sizeof(struct sk_buff)) +\t\\\n\t\t\t SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))\n\nstruct ahash_request;\nstruct net_device;\nstruct scatterlist;\nstruct pipe_inode_info;\nstruct iov_iter;\nstruct napi_struct;\nstruct bpf_prog;\nunion bpf_attr;\nstruct skb_ext;\n\n#if IS_ENABLED(CONFIG_BRIDGE_NETFILTER)\nstruct nf_bridge_info {\n\tenum {\n\t\tBRNF_PROTO_UNCHANGED,\n\t\tBRNF_PROTO_8021Q,\n\t\tBRNF_PROTO_PPPOE\n\t} orig_proto:8;\n\tu8\t\t\tpkt_otherhost:1;\n\tu8\t\t\tin_prerouting:1;\n\tu8\t\t\tbridged_dnat:1;\n\t__u16\t\t\tfrag_max_size;\n\tstruct net_device\t*physindev;\n\n\t/* always valid & non-NULL from FORWARD on, for physdev match */\n\tstruct net_device\t*physoutdev;\n\tunion {\n\t\t/* prerouting: detect dnat in orig/reply direction */\n\t\t__be32          ipv4_daddr;\n\t\tstruct in6_addr ipv6_daddr;\n\n\t\t/* after prerouting + nat detected: store original source\n\t\t * mac since neigh resolution overwrites it, only used while\n\t\t * skb is out in neigh layer.\n\t\t */\n\t\tchar neigh_header[8];\n\t};\n};\n#endif\n\n#if IS_ENABLED(CONFIG_NET_TC_SKB_EXT)\n/* Chain in tc_skb_ext will be used to share the tc chain with\n * ovs recirc_id. It will be set to the current chain by tc\n * and read by ovs to recirc_id.\n */\nstruct tc_skb_ext {\n\t__u32 chain;\n\t__u16 mru;\n};\n#endif\n\nstruct sk_buff_head {\n\t/* These two members must be first. */\n\tstruct sk_buff\t*next;\n\tstruct sk_buff\t*prev;\n\n\t__u32\t\tqlen;\n\tspinlock_t\tlock;\n};\n\nstruct sk_buff;\n\n/* To allow 64K frame to be packed as single skb without frag_list we\n * require 64K/PAGE_SIZE pages plus 1 additional page to allow for\n * buffers which do not start on a page boundary.\n *\n * Since GRO uses frags we allocate at least 16 regardless of page\n * size.\n */\n#if (65536/PAGE_SIZE + 1) < 16\n#define MAX_SKB_FRAGS 16UL\n#else\n#define MAX_SKB_FRAGS (65536/PAGE_SIZE + 1)\n#endif\nextern int sysctl_max_skb_frags;\n\n/* Set skb_shinfo(skb)->gso_size to this in case you want skb_segment to\n * segment using its current segmentation instead.\n */\n#define GSO_BY_FRAGS\t0xFFFF\n\ntypedef struct bio_vec skb_frag_t;\n\n/**\n * skb_frag_size() - Returns the size of a skb fragment\n * @frag: skb fragment\n */\nstatic inline unsigned int skb_frag_size(const skb_frag_t *frag)\n{\n\treturn frag->bv_len;\n}\n\n/**\n * skb_frag_size_set() - Sets the size of a skb fragment\n * @frag: skb fragment\n * @size: size of fragment\n */\nstatic inline void skb_frag_size_set(skb_frag_t *frag, unsigned int size)\n{\n\tfrag->bv_len = size;\n}\n\n/**\n * skb_frag_size_add() - Increments the size of a skb fragment by @delta\n * @frag: skb fragment\n * @delta: value to add\n */\nstatic inline void skb_frag_size_add(skb_frag_t *frag, int delta)\n{\n\tfrag->bv_len += delta;\n}\n\n/**\n * skb_frag_size_sub() - Decrements the size of a skb fragment by @delta\n * @frag: skb fragment\n * @delta: value to subtract\n */\nstatic inline void skb_frag_size_sub(skb_frag_t *frag, int delta)\n{\n\tfrag->bv_len -= delta;\n}\n\n/**\n * skb_frag_must_loop - Test if %p is a high memory page\n * @p: fragment's page\n */\nstatic inline bool skb_frag_must_loop(struct page *p)\n{\n#if defined(CONFIG_HIGHMEM)\n\tif (PageHighMem(p))\n\t\treturn true;\n#endif\n\treturn false;\n}\n\n/**\n *\tskb_frag_foreach_page - loop over pages in a fragment\n *\n *\t@f:\t\tskb frag to operate on\n *\t@f_off:\t\toffset from start of f->bv_page\n *\t@f_len:\t\tlength from f_off to loop over\n *\t@p:\t\t(temp var) current page\n *\t@p_off:\t\t(temp var) offset from start of current page,\n *\t                           non-zero only on first page.\n *\t@p_len:\t\t(temp var) length in current page,\n *\t\t\t\t   < PAGE_SIZE only on first and last page.\n *\t@copied:\t(temp var) length so far, excluding current p_len.\n *\n *\tA fragment can hold a compound page, in which case per-page\n *\toperations, notably kmap_atomic, must be called for each\n *\tregular page.\n */\n#define skb_frag_foreach_page(f, f_off, f_len, p, p_off, p_len, copied)\t\\\n\tfor (p = skb_frag_page(f) + ((f_off) >> PAGE_SHIFT),\t\t\\\n\t     p_off = (f_off) & (PAGE_SIZE - 1),\t\t\t\t\\\n\t     p_len = skb_frag_must_loop(p) ?\t\t\t\t\\\n\t     min_t(u32, f_len, PAGE_SIZE - p_off) : f_len,\t\t\\\n\t     copied = 0;\t\t\t\t\t\t\\\n\t     copied < f_len;\t\t\t\t\t\t\\\n\t     copied += p_len, p++, p_off = 0,\t\t\t\t\\\n\t     p_len = min_t(u32, f_len - copied, PAGE_SIZE))\t\t\\\n\n#define HAVE_HW_TIME_STAMP\n\n/**\n * struct skb_shared_hwtstamps - hardware time stamps\n * @hwtstamp:\thardware time stamp transformed into duration\n *\t\tsince arbitrary point in time\n *\n * Software time stamps generated by ktime_get_real() are stored in\n * skb->tstamp.\n *\n * hwtstamps can only be compared against other hwtstamps from\n * the same device.\n *\n * This structure is attached to packets as part of the\n * &skb_shared_info. Use skb_hwtstamps() to get a pointer.\n */\nstruct skb_shared_hwtstamps {\n\tktime_t\thwtstamp;\n};\n\n/* Definitions for tx_flags in struct skb_shared_info */\nenum {\n\t/* generate hardware time stamp */\n\tSKBTX_HW_TSTAMP = 1 << 0,\n\n\t/* generate software time stamp when queueing packet to NIC */\n\tSKBTX_SW_TSTAMP = 1 << 1,\n\n\t/* device driver is going to provide hardware time stamp */\n\tSKBTX_IN_PROGRESS = 1 << 2,\n\n\t/* device driver supports TX zero-copy buffers */\n\tSKBTX_DEV_ZEROCOPY = 1 << 3,\n\n\t/* generate wifi status information (where possible) */\n\tSKBTX_WIFI_STATUS = 1 << 4,\n\n\t/* This indicates at least one fragment might be overwritten\n\t * (as in vmsplice(), sendfile() ...)\n\t * If we need to compute a TX checksum, we'll need to copy\n\t * all frags to avoid possible bad checksum\n\t */\n\tSKBTX_SHARED_FRAG = 1 << 5,\n\n\t/* generate software time stamp when entering packet scheduling */\n\tSKBTX_SCHED_TSTAMP = 1 << 6,\n};\n\n#define SKBTX_ZEROCOPY_FRAG\t(SKBTX_DEV_ZEROCOPY | SKBTX_SHARED_FRAG)\n#define SKBTX_ANY_SW_TSTAMP\t(SKBTX_SW_TSTAMP    | \\\n\t\t\t\t SKBTX_SCHED_TSTAMP)\n#define SKBTX_ANY_TSTAMP\t(SKBTX_HW_TSTAMP | SKBTX_ANY_SW_TSTAMP)\n\n/*\n * The callback notifies userspace to release buffers when skb DMA is done in\n * lower device, the skb last reference should be 0 when calling this.\n * The zerocopy_success argument is true if zero copy transmit occurred,\n * false on data copy or out of memory error caused by data copy attempt.\n * The ctx field is used to track device context.\n * The desc field is used to track userspace buffer index.\n */\nstruct ubuf_info {\n\tvoid (*callback)(struct ubuf_info *, bool zerocopy_success);\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long desc;\n\t\t\tvoid *ctx;\n\t\t};\n\t\tstruct {\n\t\t\tu32 id;\n\t\t\tu16 len;\n\t\t\tu16 zerocopy:1;\n\t\t\tu32 bytelen;\n\t\t};\n\t};\n\trefcount_t refcnt;\n\n\tstruct mmpin {\n\t\tstruct user_struct *user;\n\t\tunsigned int num_pg;\n\t} mmp;\n};\n\n#define skb_uarg(SKB)\t((struct ubuf_info *)(skb_shinfo(SKB)->destructor_arg))\n\nint mm_account_pinned_pages(struct mmpin *mmp, size_t size);\nvoid mm_unaccount_pinned_pages(struct mmpin *mmp);\n\nstruct ubuf_info *sock_zerocopy_alloc(struct sock *sk, size_t size);\nstruct ubuf_info *sock_zerocopy_realloc(struct sock *sk, size_t size,\n\t\t\t\t\tstruct ubuf_info *uarg);\n\nstatic inline void sock_zerocopy_get(struct ubuf_info *uarg)\n{\n\trefcount_inc(&uarg->refcnt);\n}\n\nvoid sock_zerocopy_put(struct ubuf_info *uarg);\nvoid sock_zerocopy_put_abort(struct ubuf_info *uarg, bool have_uref);\n\nvoid sock_zerocopy_callback(struct ubuf_info *uarg, bool success);\n\nint skb_zerocopy_iter_dgram(struct sk_buff *skb, struct msghdr *msg, int len);\nint skb_zerocopy_iter_stream(struct sock *sk, struct sk_buff *skb,\n\t\t\t     struct msghdr *msg, int len,\n\t\t\t     struct ubuf_info *uarg);\n\n/* This data is invariant across clones and lives at\n * the end of the header data, ie. at skb->end.\n */\nstruct skb_shared_info {\n\t__u8\t\t__unused;\n\t__u8\t\tmeta_len;\n\t__u8\t\tnr_frags;\n\t__u8\t\ttx_flags;\n\tunsigned short\tgso_size;\n\t/* Warning: this field is not always filled in (UFO)! */\n\tunsigned short\tgso_segs;\n\tstruct sk_buff\t*frag_list;\n\tstruct skb_shared_hwtstamps hwtstamps;\n\tunsigned int\tgso_type;\n\tu32\t\ttskey;\n\n\t/*\n\t * Warning : all fields before dataref are cleared in __alloc_skb()\n\t */\n\tatomic_t\tdataref;\n\n\t/* Intermediate layers must ensure that destructor_arg\n\t * remains valid until skb destructor */\n\tvoid *\t\tdestructor_arg;\n\n\t/* must be last field, see pskb_expand_head() */\n\tskb_frag_t\tfrags[MAX_SKB_FRAGS];\n};\n\n/* We divide dataref into two halves.  The higher 16 bits hold references\n * to the payload part of skb->data.  The lower 16 bits hold references to\n * the entire skb->data.  A clone of a headerless skb holds the length of\n * the header in skb->hdr_len.\n *\n * All users must obey the rule that the skb->data reference count must be\n * greater than or equal to the payload reference count.\n *\n * Holding a reference to the payload part means that the user does not\n * care about modifications to the header part of skb->data.\n */\n#define SKB_DATAREF_SHIFT 16\n#define SKB_DATAREF_MASK ((1 << SKB_DATAREF_SHIFT) - 1)\n\n\nenum {\n\tSKB_FCLONE_UNAVAILABLE,\t/* skb has no fclone (from head_cache) */\n\tSKB_FCLONE_ORIG,\t/* orig skb (from fclone_cache) */\n\tSKB_FCLONE_CLONE,\t/* companion fclone skb (from fclone_cache) */\n};\n\nenum {\n\tSKB_GSO_TCPV4 = 1 << 0,\n\n\t/* This indicates the skb is from an untrusted source. */\n\tSKB_GSO_DODGY = 1 << 1,\n\n\t/* This indicates the tcp segment has CWR set. */\n\tSKB_GSO_TCP_ECN = 1 << 2,\n\n\tSKB_GSO_TCP_FIXEDID = 1 << 3,\n\n\tSKB_GSO_TCPV6 = 1 << 4,\n\n\tSKB_GSO_FCOE = 1 << 5,\n\n\tSKB_GSO_GRE = 1 << 6,\n\n\tSKB_GSO_GRE_CSUM = 1 << 7,\n\n\tSKB_GSO_IPXIP4 = 1 << 8,\n\n\tSKB_GSO_IPXIP6 = 1 << 9,\n\n\tSKB_GSO_UDP_TUNNEL = 1 << 10,\n\n\tSKB_GSO_UDP_TUNNEL_CSUM = 1 << 11,\n\n\tSKB_GSO_PARTIAL = 1 << 12,\n\n\tSKB_GSO_TUNNEL_REMCSUM = 1 << 13,\n\n\tSKB_GSO_SCTP = 1 << 14,\n\n\tSKB_GSO_ESP = 1 << 15,\n\n\tSKB_GSO_UDP = 1 << 16,\n\n\tSKB_GSO_UDP_L4 = 1 << 17,\n\n\tSKB_GSO_FRAGLIST = 1 << 18,\n};\n\n#if BITS_PER_LONG > 32\n#define NET_SKBUFF_DATA_USES_OFFSET 1\n#endif\n\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\ntypedef unsigned int sk_buff_data_t;\n#else\ntypedef unsigned char *sk_buff_data_t;\n#endif\n\n/**\n *\tstruct sk_buff - socket buffer\n *\t@next: Next buffer in list\n *\t@prev: Previous buffer in list\n *\t@tstamp: Time we arrived/left\n *\t@skb_mstamp_ns: (aka @tstamp) earliest departure time; start point\n *\t\tfor retransmit timer\n *\t@rbnode: RB tree node, alternative to next/prev for netem/tcp\n *\t@list: queue head\n *\t@sk: Socket we are owned by\n *\t@ip_defrag_offset: (aka @sk) alternate use of @sk, used in\n *\t\tfragmentation management\n *\t@dev: Device we arrived on/are leaving by\n *\t@dev_scratch: (aka @dev) alternate use of @dev when @dev would be %NULL\n *\t@cb: Control buffer. Free for use by every layer. Put private vars here\n *\t@_skb_refdst: destination entry (with norefcount bit)\n *\t@sp: the security path, used for xfrm\n *\t@len: Length of actual data\n *\t@data_len: Data length\n *\t@mac_len: Length of link layer header\n *\t@hdr_len: writable header length of cloned skb\n *\t@csum: Checksum (must include start/offset pair)\n *\t@csum_start: Offset from skb->head where checksumming should start\n *\t@csum_offset: Offset from csum_start where checksum should be stored\n *\t@priority: Packet queueing priority\n *\t@ignore_df: allow local fragmentation\n *\t@cloned: Head may be cloned (check refcnt to be sure)\n *\t@ip_summed: Driver fed us an IP checksum\n *\t@nohdr: Payload reference only, must not modify header\n *\t@pkt_type: Packet class\n *\t@fclone: skbuff clone status\n *\t@ipvs_property: skbuff is owned by ipvs\n *\t@inner_protocol_type: whether the inner protocol is\n *\t\tENCAP_TYPE_ETHER or ENCAP_TYPE_IPPROTO\n *\t@remcsum_offload: remote checksum offload is enabled\n *\t@offload_fwd_mark: Packet was L2-forwarded in hardware\n *\t@offload_l3_fwd_mark: Packet was L3-forwarded in hardware\n *\t@tc_skip_classify: do not classify packet. set by IFB device\n *\t@tc_at_ingress: used within tc_classify to distinguish in/egress\n *\t@redirected: packet was redirected by packet classifier\n *\t@from_ingress: packet was redirected from the ingress path\n *\t@peeked: this packet has been seen already, so stats have been\n *\t\tdone for it, don't do them again\n *\t@nf_trace: netfilter packet trace flag\n *\t@protocol: Packet protocol from driver\n *\t@destructor: Destruct function\n *\t@tcp_tsorted_anchor: list structure for TCP (tp->tsorted_sent_queue)\n *\t@_nfct: Associated connection, if any (with nfctinfo bits)\n *\t@nf_bridge: Saved data about a bridged frame - see br_netfilter.c\n *\t@skb_iif: ifindex of device we arrived on\n *\t@tc_index: Traffic control index\n *\t@hash: the packet hash\n *\t@queue_mapping: Queue mapping for multiqueue devices\n *\t@head_frag: skb was allocated from page fragments,\n *\t\tnot allocated by kmalloc() or vmalloc().\n *\t@pfmemalloc: skbuff was allocated from PFMEMALLOC reserves\n *\t@active_extensions: active extensions (skb_ext_id types)\n *\t@ndisc_nodetype: router type (from link layer)\n *\t@ooo_okay: allow the mapping of a socket to a queue to be changed\n *\t@l4_hash: indicate hash is a canonical 4-tuple hash over transport\n *\t\tports.\n *\t@sw_hash: indicates hash was computed in software stack\n *\t@wifi_acked_valid: wifi_acked was set\n *\t@wifi_acked: whether frame was acked on wifi or not\n *\t@no_fcs:  Request NIC to treat last 4 bytes as Ethernet FCS\n *\t@encapsulation: indicates the inner headers in the skbuff are valid\n *\t@encap_hdr_csum: software checksum is needed\n *\t@csum_valid: checksum is already valid\n *\t@csum_not_inet: use CRC32c to resolve CHECKSUM_PARTIAL\n *\t@csum_complete_sw: checksum was completed by software\n *\t@csum_level: indicates the number of consecutive checksums found in\n *\t\tthe packet minus one that have been verified as\n *\t\tCHECKSUM_UNNECESSARY (max 3)\n *\t@dst_pending_confirm: need to confirm neighbour\n *\t@decrypted: Decrypted SKB\n *\t@napi_id: id of the NAPI struct this skb came from\n *\t@sender_cpu: (aka @napi_id) source CPU in XPS\n *\t@secmark: security marking\n *\t@mark: Generic packet mark\n *\t@reserved_tailroom: (aka @mark) number of bytes of free space available\n *\t\tat the tail of an sk_buff\n *\t@vlan_present: VLAN tag is present\n *\t@vlan_proto: vlan encapsulation protocol\n *\t@vlan_tci: vlan tag control information\n *\t@inner_protocol: Protocol (encapsulation)\n *\t@inner_ipproto: (aka @inner_protocol) stores ipproto when\n *\t\tskb->inner_protocol_type == ENCAP_TYPE_IPPROTO;\n *\t@inner_transport_header: Inner transport layer header (encapsulation)\n *\t@inner_network_header: Network layer header (encapsulation)\n *\t@inner_mac_header: Link layer header (encapsulation)\n *\t@transport_header: Transport layer header\n *\t@network_header: Network layer header\n *\t@mac_header: Link layer header\n *\t@tail: Tail pointer\n *\t@end: End pointer\n *\t@head: Head of buffer\n *\t@data: Data head pointer\n *\t@truesize: Buffer size\n *\t@users: User count - see {datagram,tcp}.c\n *\t@extensions: allocated extensions, valid if active_extensions is nonzero\n */\n\nstruct sk_buff {\n\tunion {\n\t\tstruct {\n\t\t\t/* These two members must be first. */\n\t\t\tstruct sk_buff\t\t*next;\n\t\t\tstruct sk_buff\t\t*prev;\n\n\t\t\tunion {\n\t\t\t\tstruct net_device\t*dev;\n\t\t\t\t/* Some protocols might use this space to store information,\n\t\t\t\t * while device pointer would be NULL.\n\t\t\t\t * UDP receive path is one user.\n\t\t\t\t */\n\t\t\t\tunsigned long\t\tdev_scratch;\n\t\t\t};\n\t\t};\n\t\tstruct rb_node\t\trbnode; /* used in netem, ip4 defrag, and tcp stack */\n\t\tstruct list_head\tlist;\n\t};\n\n\tunion {\n\t\tstruct sock\t\t*sk;\n\t\tint\t\t\tip_defrag_offset;\n\t};\n\n\tunion {\n\t\tktime_t\t\ttstamp;\n\t\tu64\t\tskb_mstamp_ns; /* earliest departure time */\n\t};\n\t/*\n\t * This is the control buffer. It is free to use for every\n\t * layer. Please put your private variables there. If you\n\t * want to keep them across layers you have to do a skb_clone()\n\t * first. This is owned by whoever has the skb queued ATM.\n\t */\n\tchar\t\t\tcb[48] __aligned(8);\n\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long\t_skb_refdst;\n\t\t\tvoid\t\t(*destructor)(struct sk_buff *skb);\n\t\t};\n\t\tstruct list_head\ttcp_tsorted_anchor;\n\t};\n\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tunsigned long\t\t _nfct;\n#endif\n\tunsigned int\t\tlen,\n\t\t\t\tdata_len;\n\t__u16\t\t\tmac_len,\n\t\t\t\thdr_len;\n\n\t/* Following fields are _not_ copied in __copy_skb_header()\n\t * Note that queue_mapping is here mostly to fill a hole.\n\t */\n\t__u16\t\t\tqueue_mapping;\n\n/* if you move cloned around you also must adapt those constants */\n#ifdef __BIG_ENDIAN_BITFIELD\n#define CLONED_MASK\t(1 << 7)\n#else\n#define CLONED_MASK\t1\n#endif\n#define CLONED_OFFSET()\t\toffsetof(struct sk_buff, __cloned_offset)\n\n\t/* private: */\n\t__u8\t\t\t__cloned_offset[0];\n\t/* public: */\n\t__u8\t\t\tcloned:1,\n\t\t\t\tnohdr:1,\n\t\t\t\tfclone:2,\n\t\t\t\tpeeked:1,\n\t\t\t\thead_frag:1,\n\t\t\t\tpfmemalloc:1;\n#ifdef CONFIG_SKB_EXTENSIONS\n\t__u8\t\t\tactive_extensions;\n#endif\n\t/* fields enclosed in headers_start/headers_end are copied\n\t * using a single memcpy() in __copy_skb_header()\n\t */\n\t/* private: */\n\t__u32\t\t\theaders_start[0];\n\t/* public: */\n\n/* if you move pkt_type around you also must adapt those constants */\n#ifdef __BIG_ENDIAN_BITFIELD\n#define PKT_TYPE_MAX\t(7 << 5)\n#else\n#define PKT_TYPE_MAX\t7\n#endif\n#define PKT_TYPE_OFFSET()\toffsetof(struct sk_buff, __pkt_type_offset)\n\n\t/* private: */\n\t__u8\t\t\t__pkt_type_offset[0];\n\t/* public: */\n\t__u8\t\t\tpkt_type:3;\n\t__u8\t\t\tignore_df:1;\n\t__u8\t\t\tnf_trace:1;\n\t__u8\t\t\tip_summed:2;\n\t__u8\t\t\tooo_okay:1;\n\n\t__u8\t\t\tl4_hash:1;\n\t__u8\t\t\tsw_hash:1;\n\t__u8\t\t\twifi_acked_valid:1;\n\t__u8\t\t\twifi_acked:1;\n\t__u8\t\t\tno_fcs:1;\n\t/* Indicates the inner headers are valid in the skbuff. */\n\t__u8\t\t\tencapsulation:1;\n\t__u8\t\t\tencap_hdr_csum:1;\n\t__u8\t\t\tcsum_valid:1;\n\n#ifdef __BIG_ENDIAN_BITFIELD\n#define PKT_VLAN_PRESENT_BIT\t7\n#else\n#define PKT_VLAN_PRESENT_BIT\t0\n#endif\n#define PKT_VLAN_PRESENT_OFFSET()\toffsetof(struct sk_buff, __pkt_vlan_present_offset)\n\t/* private: */\n\t__u8\t\t\t__pkt_vlan_present_offset[0];\n\t/* public: */\n\t__u8\t\t\tvlan_present:1;\n\t__u8\t\t\tcsum_complete_sw:1;\n\t__u8\t\t\tcsum_level:2;\n\t__u8\t\t\tcsum_not_inet:1;\n\t__u8\t\t\tdst_pending_confirm:1;\n#ifdef CONFIG_IPV6_NDISC_NODETYPE\n\t__u8\t\t\tndisc_nodetype:2;\n#endif\n\n\t__u8\t\t\tipvs_property:1;\n\t__u8\t\t\tinner_protocol_type:1;\n\t__u8\t\t\tremcsum_offload:1;\n#ifdef CONFIG_NET_SWITCHDEV\n\t__u8\t\t\toffload_fwd_mark:1;\n\t__u8\t\t\toffload_l3_fwd_mark:1;\n#endif\n#ifdef CONFIG_NET_CLS_ACT\n\t__u8\t\t\ttc_skip_classify:1;\n\t__u8\t\t\ttc_at_ingress:1;\n#endif\n#ifdef CONFIG_NET_REDIRECT\n\t__u8\t\t\tredirected:1;\n\t__u8\t\t\tfrom_ingress:1;\n#endif\n#ifdef CONFIG_TLS_DEVICE\n\t__u8\t\t\tdecrypted:1;\n#endif\n\n#ifdef CONFIG_NET_SCHED\n\t__u16\t\t\ttc_index;\t/* traffic control index */\n#endif\n\n\tunion {\n\t\t__wsum\t\tcsum;\n\t\tstruct {\n\t\t\t__u16\tcsum_start;\n\t\t\t__u16\tcsum_offset;\n\t\t};\n\t};\n\t__u32\t\t\tpriority;\n\tint\t\t\tskb_iif;\n\t__u32\t\t\thash;\n\t__be16\t\t\tvlan_proto;\n\t__u16\t\t\tvlan_tci;\n#if defined(CONFIG_NET_RX_BUSY_POLL) || defined(CONFIG_XPS)\n\tunion {\n\t\tunsigned int\tnapi_id;\n\t\tunsigned int\tsender_cpu;\n\t};\n#endif\n#ifdef CONFIG_NETWORK_SECMARK\n\t__u32\t\tsecmark;\n#endif\n\n\tunion {\n\t\t__u32\t\tmark;\n\t\t__u32\t\treserved_tailroom;\n\t};\n\n\tunion {\n\t\t__be16\t\tinner_protocol;\n\t\t__u8\t\tinner_ipproto;\n\t};\n\n\t__u16\t\t\tinner_transport_header;\n\t__u16\t\t\tinner_network_header;\n\t__u16\t\t\tinner_mac_header;\n\n\t__be16\t\t\tprotocol;\n\t__u16\t\t\ttransport_header;\n\t__u16\t\t\tnetwork_header;\n\t__u16\t\t\tmac_header;\n\n\t/* private: */\n\t__u32\t\t\theaders_end[0];\n\t/* public: */\n\n\t/* These elements must be at the end, see alloc_skb() for details.  */\n\tsk_buff_data_t\t\ttail;\n\tsk_buff_data_t\t\tend;\n\tunsigned char\t\t*head,\n\t\t\t\t*data;\n\tunsigned int\t\ttruesize;\n\trefcount_t\t\tusers;\n\n#ifdef CONFIG_SKB_EXTENSIONS\n\t/* only useable after checking ->active_extensions != 0 */\n\tstruct skb_ext\t\t*extensions;\n#endif\n};\n\n#ifdef __KERNEL__\n/*\n *\tHandling routines are only of interest to the kernel\n */\n\n#define SKB_ALLOC_FCLONE\t0x01\n#define SKB_ALLOC_RX\t\t0x02\n#define SKB_ALLOC_NAPI\t\t0x04\n\n/**\n * skb_pfmemalloc - Test if the skb was allocated from PFMEMALLOC reserves\n * @skb: buffer\n */\nstatic inline bool skb_pfmemalloc(const struct sk_buff *skb)\n{\n\treturn unlikely(skb->pfmemalloc);\n}\n\n/*\n * skb might have a dst pointer attached, refcounted or not.\n * _skb_refdst low order bit is set if refcount was _not_ taken\n */\n#define SKB_DST_NOREF\t1UL\n#define SKB_DST_PTRMASK\t~(SKB_DST_NOREF)\n\n/**\n * skb_dst - returns skb dst_entry\n * @skb: buffer\n *\n * Returns skb dst_entry, regardless of reference taken or not.\n */\nstatic inline struct dst_entry *skb_dst(const struct sk_buff *skb)\n{\n\t/* If refdst was not refcounted, check we still are in a\n\t * rcu_read_lock section\n\t */\n\tWARN_ON((skb->_skb_refdst & SKB_DST_NOREF) &&\n\t\t!rcu_read_lock_held() &&\n\t\t!rcu_read_lock_bh_held());\n\treturn (struct dst_entry *)(skb->_skb_refdst & SKB_DST_PTRMASK);\n}\n\n/**\n * skb_dst_set - sets skb dst\n * @skb: buffer\n * @dst: dst entry\n *\n * Sets skb dst, assuming a reference was taken on dst and should\n * be released by skb_dst_drop()\n */\nstatic inline void skb_dst_set(struct sk_buff *skb, struct dst_entry *dst)\n{\n\tskb->_skb_refdst = (unsigned long)dst;\n}\n\n/**\n * skb_dst_set_noref - sets skb dst, hopefully, without taking reference\n * @skb: buffer\n * @dst: dst entry\n *\n * Sets skb dst, assuming a reference was not taken on dst.\n * If dst entry is cached, we do not take reference and dst_release\n * will be avoided by refdst_drop. If dst entry is not cached, we take\n * reference, so that last dst_release can destroy the dst immediately.\n */\nstatic inline void skb_dst_set_noref(struct sk_buff *skb, struct dst_entry *dst)\n{\n\tWARN_ON(!rcu_read_lock_held() && !rcu_read_lock_bh_held());\n\tskb->_skb_refdst = (unsigned long)dst | SKB_DST_NOREF;\n}\n\n/**\n * skb_dst_is_noref - Test if skb dst isn't refcounted\n * @skb: buffer\n */\nstatic inline bool skb_dst_is_noref(const struct sk_buff *skb)\n{\n\treturn (skb->_skb_refdst & SKB_DST_NOREF) && skb_dst(skb);\n}\n\n/**\n * skb_rtable - Returns the skb &rtable\n * @skb: buffer\n */\nstatic inline struct rtable *skb_rtable(const struct sk_buff *skb)\n{\n\treturn (struct rtable *)skb_dst(skb);\n}\n\n/* For mangling skb->pkt_type from user space side from applications\n * such as nft, tc, etc, we only allow a conservative subset of\n * possible pkt_types to be set.\n*/\nstatic inline bool skb_pkt_type_ok(u32 ptype)\n{\n\treturn ptype <= PACKET_OTHERHOST;\n}\n\n/**\n * skb_napi_id - Returns the skb's NAPI id\n * @skb: buffer\n */\nstatic inline unsigned int skb_napi_id(const struct sk_buff *skb)\n{\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\treturn skb->napi_id;\n#else\n\treturn 0;\n#endif\n}\n\n/**\n * skb_unref - decrement the skb's reference count\n * @skb: buffer\n *\n * Returns true if we can free the skb.\n */\nstatic inline bool skb_unref(struct sk_buff *skb)\n{\n\tif (unlikely(!skb))\n\t\treturn false;\n\tif (likely(refcount_read(&skb->users) == 1))\n\t\tsmp_rmb();\n\telse if (likely(!refcount_dec_and_test(&skb->users)))\n\t\treturn false;\n\n\treturn true;\n}\n\nvoid skb_release_head_state(struct sk_buff *skb);\nvoid kfree_skb(struct sk_buff *skb);\nvoid kfree_skb_list(struct sk_buff *segs);\nvoid skb_dump(const char *level, const struct sk_buff *skb, bool full_pkt);\nvoid skb_tx_error(struct sk_buff *skb);\n\n#ifdef CONFIG_TRACEPOINTS\nvoid consume_skb(struct sk_buff *skb);\n#else\nstatic inline void consume_skb(struct sk_buff *skb)\n{\n\treturn kfree_skb(skb);\n}\n#endif\n\nvoid __consume_stateless_skb(struct sk_buff *skb);\nvoid  __kfree_skb(struct sk_buff *skb);\nextern struct kmem_cache *skbuff_head_cache;\n\nvoid kfree_skb_partial(struct sk_buff *skb, bool head_stolen);\nbool skb_try_coalesce(struct sk_buff *to, struct sk_buff *from,\n\t\t      bool *fragstolen, int *delta_truesize);\n\nstruct sk_buff *__alloc_skb(unsigned int size, gfp_t priority, int flags,\n\t\t\t    int node);\nstruct sk_buff *__build_skb(void *data, unsigned int frag_size);\nstruct sk_buff *build_skb(void *data, unsigned int frag_size);\nstruct sk_buff *build_skb_around(struct sk_buff *skb,\n\t\t\t\t void *data, unsigned int frag_size);\n\n/**\n * alloc_skb - allocate a network buffer\n * @size: size to allocate\n * @priority: allocation mask\n *\n * This function is a convenient wrapper around __alloc_skb().\n */\nstatic inline struct sk_buff *alloc_skb(unsigned int size,\n\t\t\t\t\tgfp_t priority)\n{\n\treturn __alloc_skb(size, priority, 0, NUMA_NO_NODE);\n}\n\nstruct sk_buff *alloc_skb_with_frags(unsigned long header_len,\n\t\t\t\t     unsigned long data_len,\n\t\t\t\t     int max_page_order,\n\t\t\t\t     int *errcode,\n\t\t\t\t     gfp_t gfp_mask);\nstruct sk_buff *alloc_skb_for_msg(struct sk_buff *first);\n\n/* Layout of fast clones : [skb1][skb2][fclone_ref] */\nstruct sk_buff_fclones {\n\tstruct sk_buff\tskb1;\n\n\tstruct sk_buff\tskb2;\n\n\trefcount_t\tfclone_ref;\n};\n\n/**\n *\tskb_fclone_busy - check if fclone is busy\n *\t@sk: socket\n *\t@skb: buffer\n *\n * Returns true if skb is a fast clone, and its clone is not freed.\n * Some drivers call skb_orphan() in their ndo_start_xmit(),\n * so we also check that this didnt happen.\n */\nstatic inline bool skb_fclone_busy(const struct sock *sk,\n\t\t\t\t   const struct sk_buff *skb)\n{\n\tconst struct sk_buff_fclones *fclones;\n\n\tfclones = container_of(skb, struct sk_buff_fclones, skb1);\n\n\treturn skb->fclone == SKB_FCLONE_ORIG &&\n\t       refcount_read(&fclones->fclone_ref) > 1 &&\n\t       fclones->skb2.sk == sk;\n}\n\n/**\n * alloc_skb_fclone - allocate a network buffer from fclone cache\n * @size: size to allocate\n * @priority: allocation mask\n *\n * This function is a convenient wrapper around __alloc_skb().\n */\nstatic inline struct sk_buff *alloc_skb_fclone(unsigned int size,\n\t\t\t\t\t       gfp_t priority)\n{\n\treturn __alloc_skb(size, priority, SKB_ALLOC_FCLONE, NUMA_NO_NODE);\n}\n\nstruct sk_buff *skb_morph(struct sk_buff *dst, struct sk_buff *src);\nvoid skb_headers_offset_update(struct sk_buff *skb, int off);\nint skb_copy_ubufs(struct sk_buff *skb, gfp_t gfp_mask);\nstruct sk_buff *skb_clone(struct sk_buff *skb, gfp_t priority);\nvoid skb_copy_header(struct sk_buff *new, const struct sk_buff *old);\nstruct sk_buff *skb_copy(const struct sk_buff *skb, gfp_t priority);\nstruct sk_buff *__pskb_copy_fclone(struct sk_buff *skb, int headroom,\n\t\t\t\t   gfp_t gfp_mask, bool fclone);\nstatic inline struct sk_buff *__pskb_copy(struct sk_buff *skb, int headroom,\n\t\t\t\t\t  gfp_t gfp_mask)\n{\n\treturn __pskb_copy_fclone(skb, headroom, gfp_mask, false);\n}\n\nint pskb_expand_head(struct sk_buff *skb, int nhead, int ntail, gfp_t gfp_mask);\nstruct sk_buff *skb_realloc_headroom(struct sk_buff *skb,\n\t\t\t\t     unsigned int headroom);\nstruct sk_buff *skb_copy_expand(const struct sk_buff *skb, int newheadroom,\n\t\t\t\tint newtailroom, gfp_t priority);\nint __must_check skb_to_sgvec_nomark(struct sk_buff *skb, struct scatterlist *sg,\n\t\t\t\t     int offset, int len);\nint __must_check skb_to_sgvec(struct sk_buff *skb, struct scatterlist *sg,\n\t\t\t      int offset, int len);\nint skb_cow_data(struct sk_buff *skb, int tailbits, struct sk_buff **trailer);\nint __skb_pad(struct sk_buff *skb, int pad, bool free_on_error);\n\n/**\n *\tskb_pad\t\t\t-\tzero pad the tail of an skb\n *\t@skb: buffer to pad\n *\t@pad: space to pad\n *\n *\tEnsure that a buffer is followed by a padding area that is zero\n *\tfilled. Used by network drivers which may DMA or transfer data\n *\tbeyond the buffer end onto the wire.\n *\n *\tMay return error in out of memory cases. The skb is freed on error.\n */\nstatic inline int skb_pad(struct sk_buff *skb, int pad)\n{\n\treturn __skb_pad(skb, pad, true);\n}\n#define dev_kfree_skb(a)\tconsume_skb(a)\n\nint skb_append_pagefrags(struct sk_buff *skb, struct page *page,\n\t\t\t int offset, size_t size);\n\nstruct skb_seq_state {\n\t__u32\t\tlower_offset;\n\t__u32\t\tupper_offset;\n\t__u32\t\tfrag_idx;\n\t__u32\t\tstepped_offset;\n\tstruct sk_buff\t*root_skb;\n\tstruct sk_buff\t*cur_skb;\n\t__u8\t\t*frag_data;\n};\n\nvoid skb_prepare_seq_read(struct sk_buff *skb, unsigned int from,\n\t\t\t  unsigned int to, struct skb_seq_state *st);\nunsigned int skb_seq_read(unsigned int consumed, const u8 **data,\n\t\t\t  struct skb_seq_state *st);\nvoid skb_abort_seq_read(struct skb_seq_state *st);\n\nunsigned int skb_find_text(struct sk_buff *skb, unsigned int from,\n\t\t\t   unsigned int to, struct ts_config *config);\n\n/*\n * Packet hash types specify the type of hash in skb_set_hash.\n *\n * Hash types refer to the protocol layer addresses which are used to\n * construct a packet's hash. The hashes are used to differentiate or identify\n * flows of the protocol layer for the hash type. Hash types are either\n * layer-2 (L2), layer-3 (L3), or layer-4 (L4).\n *\n * Properties of hashes:\n *\n * 1) Two packets in different flows have different hash values\n * 2) Two packets in the same flow should have the same hash value\n *\n * A hash at a higher layer is considered to be more specific. A driver should\n * set the most specific hash possible.\n *\n * A driver cannot indicate a more specific hash than the layer at which a hash\n * was computed. For instance an L3 hash cannot be set as an L4 hash.\n *\n * A driver may indicate a hash level which is less specific than the\n * actual layer the hash was computed on. For instance, a hash computed\n * at L4 may be considered an L3 hash. This should only be done if the\n * driver can't unambiguously determine that the HW computed the hash at\n * the higher layer. Note that the \"should\" in the second property above\n * permits this.\n */\nenum pkt_hash_types {\n\tPKT_HASH_TYPE_NONE,\t/* Undefined type */\n\tPKT_HASH_TYPE_L2,\t/* Input: src_MAC, dest_MAC */\n\tPKT_HASH_TYPE_L3,\t/* Input: src_IP, dst_IP */\n\tPKT_HASH_TYPE_L4,\t/* Input: src_IP, dst_IP, src_port, dst_port */\n};\n\nstatic inline void skb_clear_hash(struct sk_buff *skb)\n{\n\tskb->hash = 0;\n\tskb->sw_hash = 0;\n\tskb->l4_hash = 0;\n}\n\nstatic inline void skb_clear_hash_if_not_l4(struct sk_buff *skb)\n{\n\tif (!skb->l4_hash)\n\t\tskb_clear_hash(skb);\n}\n\nstatic inline void\n__skb_set_hash(struct sk_buff *skb, __u32 hash, bool is_sw, bool is_l4)\n{\n\tskb->l4_hash = is_l4;\n\tskb->sw_hash = is_sw;\n\tskb->hash = hash;\n}\n\nstatic inline void\nskb_set_hash(struct sk_buff *skb, __u32 hash, enum pkt_hash_types type)\n{\n\t/* Used by drivers to set hash from HW */\n\t__skb_set_hash(skb, hash, false, type == PKT_HASH_TYPE_L4);\n}\n\nstatic inline void\n__skb_set_sw_hash(struct sk_buff *skb, __u32 hash, bool is_l4)\n{\n\t__skb_set_hash(skb, hash, true, is_l4);\n}\n\nvoid __skb_get_hash(struct sk_buff *skb);\nu32 __skb_get_hash_symmetric(const struct sk_buff *skb);\nu32 skb_get_poff(const struct sk_buff *skb);\nu32 __skb_get_poff(const struct sk_buff *skb, void *data,\n\t\t   const struct flow_keys_basic *keys, int hlen);\n__be32 __skb_flow_get_ports(const struct sk_buff *skb, int thoff, u8 ip_proto,\n\t\t\t    void *data, int hlen_proto);\n\nstatic inline __be32 skb_flow_get_ports(const struct sk_buff *skb,\n\t\t\t\t\tint thoff, u8 ip_proto)\n{\n\treturn __skb_flow_get_ports(skb, thoff, ip_proto, NULL, 0);\n}\n\nvoid skb_flow_dissector_init(struct flow_dissector *flow_dissector,\n\t\t\t     const struct flow_dissector_key *key,\n\t\t\t     unsigned int key_count);\n\nstruct bpf_flow_dissector;\nbool bpf_flow_dissect(struct bpf_prog *prog, struct bpf_flow_dissector *ctx,\n\t\t      __be16 proto, int nhoff, int hlen, unsigned int flags);\n\nbool __skb_flow_dissect(const struct net *net,\n\t\t\tconst struct sk_buff *skb,\n\t\t\tstruct flow_dissector *flow_dissector,\n\t\t\tvoid *target_container,\n\t\t\tvoid *data, __be16 proto, int nhoff, int hlen,\n\t\t\tunsigned int flags);\n\nstatic inline bool skb_flow_dissect(const struct sk_buff *skb,\n\t\t\t\t    struct flow_dissector *flow_dissector,\n\t\t\t\t    void *target_container, unsigned int flags)\n{\n\treturn __skb_flow_dissect(NULL, skb, flow_dissector,\n\t\t\t\t  target_container, NULL, 0, 0, 0, flags);\n}\n\nstatic inline bool skb_flow_dissect_flow_keys(const struct sk_buff *skb,\n\t\t\t\t\t      struct flow_keys *flow,\n\t\t\t\t\t      unsigned int flags)\n{\n\tmemset(flow, 0, sizeof(*flow));\n\treturn __skb_flow_dissect(NULL, skb, &flow_keys_dissector,\n\t\t\t\t  flow, NULL, 0, 0, 0, flags);\n}\n\nstatic inline bool\nskb_flow_dissect_flow_keys_basic(const struct net *net,\n\t\t\t\t const struct sk_buff *skb,\n\t\t\t\t struct flow_keys_basic *flow, void *data,\n\t\t\t\t __be16 proto, int nhoff, int hlen,\n\t\t\t\t unsigned int flags)\n{\n\tmemset(flow, 0, sizeof(*flow));\n\treturn __skb_flow_dissect(net, skb, &flow_keys_basic_dissector, flow,\n\t\t\t\t  data, proto, nhoff, hlen, flags);\n}\n\nvoid skb_flow_dissect_meta(const struct sk_buff *skb,\n\t\t\t   struct flow_dissector *flow_dissector,\n\t\t\t   void *target_container);\n\n/* Gets a skb connection tracking info, ctinfo map should be a\n * map of mapsize to translate enum ip_conntrack_info states\n * to user states.\n */\nvoid\nskb_flow_dissect_ct(const struct sk_buff *skb,\n\t\t    struct flow_dissector *flow_dissector,\n\t\t    void *target_container,\n\t\t    u16 *ctinfo_map,\n\t\t    size_t mapsize);\nvoid\nskb_flow_dissect_tunnel_info(const struct sk_buff *skb,\n\t\t\t     struct flow_dissector *flow_dissector,\n\t\t\t     void *target_container);\n\nvoid skb_flow_dissect_hash(const struct sk_buff *skb,\n\t\t\t   struct flow_dissector *flow_dissector,\n\t\t\t   void *target_container);\n\nstatic inline __u32 skb_get_hash(struct sk_buff *skb)\n{\n\tif (!skb->l4_hash && !skb->sw_hash)\n\t\t__skb_get_hash(skb);\n\n\treturn skb->hash;\n}\n\nstatic inline __u32 skb_get_hash_flowi6(struct sk_buff *skb, const struct flowi6 *fl6)\n{\n\tif (!skb->l4_hash && !skb->sw_hash) {\n\t\tstruct flow_keys keys;\n\t\t__u32 hash = __get_hash_from_flowi6(fl6, &keys);\n\n\t\t__skb_set_sw_hash(skb, hash, flow_keys_have_l4(&keys));\n\t}\n\n\treturn skb->hash;\n}\n\n__u32 skb_get_hash_perturb(const struct sk_buff *skb,\n\t\t\t   const siphash_key_t *perturb);\n\nstatic inline __u32 skb_get_hash_raw(const struct sk_buff *skb)\n{\n\treturn skb->hash;\n}\n\nstatic inline void skb_copy_hash(struct sk_buff *to, const struct sk_buff *from)\n{\n\tto->hash = from->hash;\n\tto->sw_hash = from->sw_hash;\n\tto->l4_hash = from->l4_hash;\n};\n\nstatic inline void skb_copy_decrypted(struct sk_buff *to,\n\t\t\t\t      const struct sk_buff *from)\n{\n#ifdef CONFIG_TLS_DEVICE\n\tto->decrypted = from->decrypted;\n#endif\n}\n\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\nstatic inline unsigned char *skb_end_pointer(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->end;\n}\n\nstatic inline unsigned int skb_end_offset(const struct sk_buff *skb)\n{\n\treturn skb->end;\n}\n#else\nstatic inline unsigned char *skb_end_pointer(const struct sk_buff *skb)\n{\n\treturn skb->end;\n}\n\nstatic inline unsigned int skb_end_offset(const struct sk_buff *skb)\n{\n\treturn skb->end - skb->head;\n}\n#endif\n\n/* Internal */\n#define skb_shinfo(SKB)\t((struct skb_shared_info *)(skb_end_pointer(SKB)))\n\nstatic inline struct skb_shared_hwtstamps *skb_hwtstamps(struct sk_buff *skb)\n{\n\treturn &skb_shinfo(skb)->hwtstamps;\n}\n\nstatic inline struct ubuf_info *skb_zcopy(struct sk_buff *skb)\n{\n\tbool is_zcopy = skb && skb_shinfo(skb)->tx_flags & SKBTX_DEV_ZEROCOPY;\n\n\treturn is_zcopy ? skb_uarg(skb) : NULL;\n}\n\nstatic inline void skb_zcopy_set(struct sk_buff *skb, struct ubuf_info *uarg,\n\t\t\t\t bool *have_ref)\n{\n\tif (skb && uarg && !skb_zcopy(skb)) {\n\t\tif (unlikely(have_ref && *have_ref))\n\t\t\t*have_ref = false;\n\t\telse\n\t\t\tsock_zerocopy_get(uarg);\n\t\tskb_shinfo(skb)->destructor_arg = uarg;\n\t\tskb_shinfo(skb)->tx_flags |= SKBTX_ZEROCOPY_FRAG;\n\t}\n}\n\nstatic inline void skb_zcopy_set_nouarg(struct sk_buff *skb, void *val)\n{\n\tskb_shinfo(skb)->destructor_arg = (void *)((uintptr_t) val | 0x1UL);\n\tskb_shinfo(skb)->tx_flags |= SKBTX_ZEROCOPY_FRAG;\n}\n\nstatic inline bool skb_zcopy_is_nouarg(struct sk_buff *skb)\n{\n\treturn (uintptr_t) skb_shinfo(skb)->destructor_arg & 0x1UL;\n}\n\nstatic inline void *skb_zcopy_get_nouarg(struct sk_buff *skb)\n{\n\treturn (void *)((uintptr_t) skb_shinfo(skb)->destructor_arg & ~0x1UL);\n}\n\n/* Release a reference on a zerocopy structure */\nstatic inline void skb_zcopy_clear(struct sk_buff *skb, bool zerocopy)\n{\n\tstruct ubuf_info *uarg = skb_zcopy(skb);\n\n\tif (uarg) {\n\t\tif (skb_zcopy_is_nouarg(skb)) {\n\t\t\t/* no notification callback */\n\t\t} else if (uarg->callback == sock_zerocopy_callback) {\n\t\t\tuarg->zerocopy = uarg->zerocopy && zerocopy;\n\t\t\tsock_zerocopy_put(uarg);\n\t\t} else {\n\t\t\tuarg->callback(uarg, zerocopy);\n\t\t}\n\n\t\tskb_shinfo(skb)->tx_flags &= ~SKBTX_ZEROCOPY_FRAG;\n\t}\n}\n\n/* Abort a zerocopy operation and revert zckey on error in send syscall */\nstatic inline void skb_zcopy_abort(struct sk_buff *skb)\n{\n\tstruct ubuf_info *uarg = skb_zcopy(skb);\n\n\tif (uarg) {\n\t\tsock_zerocopy_put_abort(uarg, false);\n\t\tskb_shinfo(skb)->tx_flags &= ~SKBTX_ZEROCOPY_FRAG;\n\t}\n}\n\nstatic inline void skb_mark_not_on_list(struct sk_buff *skb)\n{\n\tskb->next = NULL;\n}\n\n/* Iterate through singly-linked GSO fragments of an skb. */\n#define skb_list_walk_safe(first, skb, next_skb)                               \\\n\tfor ((skb) = (first), (next_skb) = (skb) ? (skb)->next : NULL; (skb);  \\\n\t     (skb) = (next_skb), (next_skb) = (skb) ? (skb)->next : NULL)\n\nstatic inline void skb_list_del_init(struct sk_buff *skb)\n{\n\t__list_del_entry(&skb->list);\n\tskb_mark_not_on_list(skb);\n}\n\n/**\n *\tskb_queue_empty - check if a queue is empty\n *\t@list: queue head\n *\n *\tReturns true if the queue is empty, false otherwise.\n */\nstatic inline int skb_queue_empty(const struct sk_buff_head *list)\n{\n\treturn list->next == (const struct sk_buff *) list;\n}\n\n/**\n *\tskb_queue_empty_lockless - check if a queue is empty\n *\t@list: queue head\n *\n *\tReturns true if the queue is empty, false otherwise.\n *\tThis variant can be used in lockless contexts.\n */\nstatic inline bool skb_queue_empty_lockless(const struct sk_buff_head *list)\n{\n\treturn READ_ONCE(list->next) == (const struct sk_buff *) list;\n}\n\n\n/**\n *\tskb_queue_is_last - check if skb is the last entry in the queue\n *\t@list: queue head\n *\t@skb: buffer\n *\n *\tReturns true if @skb is the last buffer on the list.\n */\nstatic inline bool skb_queue_is_last(const struct sk_buff_head *list,\n\t\t\t\t     const struct sk_buff *skb)\n{\n\treturn skb->next == (const struct sk_buff *) list;\n}\n\n/**\n *\tskb_queue_is_first - check if skb is the first entry in the queue\n *\t@list: queue head\n *\t@skb: buffer\n *\n *\tReturns true if @skb is the first buffer on the list.\n */\nstatic inline bool skb_queue_is_first(const struct sk_buff_head *list,\n\t\t\t\t      const struct sk_buff *skb)\n{\n\treturn skb->prev == (const struct sk_buff *) list;\n}\n\n/**\n *\tskb_queue_next - return the next packet in the queue\n *\t@list: queue head\n *\t@skb: current buffer\n *\n *\tReturn the next packet in @list after @skb.  It is only valid to\n *\tcall this if skb_queue_is_last() evaluates to false.\n */\nstatic inline struct sk_buff *skb_queue_next(const struct sk_buff_head *list,\n\t\t\t\t\t     const struct sk_buff *skb)\n{\n\t/* This BUG_ON may seem severe, but if we just return then we\n\t * are going to dereference garbage.\n\t */\n\tBUG_ON(skb_queue_is_last(list, skb));\n\treturn skb->next;\n}\n\n/**\n *\tskb_queue_prev - return the prev packet in the queue\n *\t@list: queue head\n *\t@skb: current buffer\n *\n *\tReturn the prev packet in @list before @skb.  It is only valid to\n *\tcall this if skb_queue_is_first() evaluates to false.\n */\nstatic inline struct sk_buff *skb_queue_prev(const struct sk_buff_head *list,\n\t\t\t\t\t     const struct sk_buff *skb)\n{\n\t/* This BUG_ON may seem severe, but if we just return then we\n\t * are going to dereference garbage.\n\t */\n\tBUG_ON(skb_queue_is_first(list, skb));\n\treturn skb->prev;\n}\n\n/**\n *\tskb_get - reference buffer\n *\t@skb: buffer to reference\n *\n *\tMakes another reference to a socket buffer and returns a pointer\n *\tto the buffer.\n */\nstatic inline struct sk_buff *skb_get(struct sk_buff *skb)\n{\n\trefcount_inc(&skb->users);\n\treturn skb;\n}\n\n/*\n * If users == 1, we are the only owner and can avoid redundant atomic changes.\n */\n\n/**\n *\tskb_cloned - is the buffer a clone\n *\t@skb: buffer to check\n *\n *\tReturns true if the buffer was generated with skb_clone() and is\n *\tone of multiple shared copies of the buffer. Cloned buffers are\n *\tshared data so must not be written to under normal circumstances.\n */\nstatic inline int skb_cloned(const struct sk_buff *skb)\n{\n\treturn skb->cloned &&\n\t       (atomic_read(&skb_shinfo(skb)->dataref) & SKB_DATAREF_MASK) != 1;\n}\n\nstatic inline int skb_unclone(struct sk_buff *skb, gfp_t pri)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(pri));\n\n\tif (skb_cloned(skb))\n\t\treturn pskb_expand_head(skb, 0, 0, pri);\n\n\treturn 0;\n}\n\n/**\n *\tskb_header_cloned - is the header a clone\n *\t@skb: buffer to check\n *\n *\tReturns true if modifying the header part of the buffer requires\n *\tthe data to be copied.\n */\nstatic inline int skb_header_cloned(const struct sk_buff *skb)\n{\n\tint dataref;\n\n\tif (!skb->cloned)\n\t\treturn 0;\n\n\tdataref = atomic_read(&skb_shinfo(skb)->dataref);\n\tdataref = (dataref & SKB_DATAREF_MASK) - (dataref >> SKB_DATAREF_SHIFT);\n\treturn dataref != 1;\n}\n\nstatic inline int skb_header_unclone(struct sk_buff *skb, gfp_t pri)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(pri));\n\n\tif (skb_header_cloned(skb))\n\t\treturn pskb_expand_head(skb, 0, 0, pri);\n\n\treturn 0;\n}\n\n/**\n *\t__skb_header_release - release reference to header\n *\t@skb: buffer to operate on\n */\nstatic inline void __skb_header_release(struct sk_buff *skb)\n{\n\tskb->nohdr = 1;\n\tatomic_set(&skb_shinfo(skb)->dataref, 1 + (1 << SKB_DATAREF_SHIFT));\n}\n\n\n/**\n *\tskb_shared - is the buffer shared\n *\t@skb: buffer to check\n *\n *\tReturns true if more than one person has a reference to this\n *\tbuffer.\n */\nstatic inline int skb_shared(const struct sk_buff *skb)\n{\n\treturn refcount_read(&skb->users) != 1;\n}\n\n/**\n *\tskb_share_check - check if buffer is shared and if so clone it\n *\t@skb: buffer to check\n *\t@pri: priority for memory allocation\n *\n *\tIf the buffer is shared the buffer is cloned and the old copy\n *\tdrops a reference. A new clone with a single reference is returned.\n *\tIf the buffer is not shared the original buffer is returned. When\n *\tbeing called from interrupt status or with spinlocks held pri must\n *\tbe GFP_ATOMIC.\n *\n *\tNULL is returned on a memory allocation failure.\n */\nstatic inline struct sk_buff *skb_share_check(struct sk_buff *skb, gfp_t pri)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(pri));\n\tif (skb_shared(skb)) {\n\t\tstruct sk_buff *nskb = skb_clone(skb, pri);\n\n\t\tif (likely(nskb))\n\t\t\tconsume_skb(skb);\n\t\telse\n\t\t\tkfree_skb(skb);\n\t\tskb = nskb;\n\t}\n\treturn skb;\n}\n\n/*\n *\tCopy shared buffers into a new sk_buff. We effectively do COW on\n *\tpackets to handle cases where we have a local reader and forward\n *\tand a couple of other messy ones. The normal one is tcpdumping\n *\ta packet thats being forwarded.\n */\n\n/**\n *\tskb_unshare - make a copy of a shared buffer\n *\t@skb: buffer to check\n *\t@pri: priority for memory allocation\n *\n *\tIf the socket buffer is a clone then this function creates a new\n *\tcopy of the data, drops a reference count on the old copy and returns\n *\tthe new copy with the reference count at 1. If the buffer is not a clone\n *\tthe original buffer is returned. When called with a spinlock held or\n *\tfrom interrupt state @pri must be %GFP_ATOMIC\n *\n *\t%NULL is returned on a memory allocation failure.\n */\nstatic inline struct sk_buff *skb_unshare(struct sk_buff *skb,\n\t\t\t\t\t  gfp_t pri)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(pri));\n\tif (skb_cloned(skb)) {\n\t\tstruct sk_buff *nskb = skb_copy(skb, pri);\n\n\t\t/* Free our shared copy */\n\t\tif (likely(nskb))\n\t\t\tconsume_skb(skb);\n\t\telse\n\t\t\tkfree_skb(skb);\n\t\tskb = nskb;\n\t}\n\treturn skb;\n}\n\n/**\n *\tskb_peek - peek at the head of an &sk_buff_head\n *\t@list_: list to peek at\n *\n *\tPeek an &sk_buff. Unlike most other operations you _MUST_\n *\tbe careful with this one. A peek leaves the buffer on the\n *\tlist and someone else may run off with it. You must hold\n *\tthe appropriate locks or have a private queue to do this.\n *\n *\tReturns %NULL for an empty list or a pointer to the head element.\n *\tThe reference count is not incremented and the reference is therefore\n *\tvolatile. Use with caution.\n */\nstatic inline struct sk_buff *skb_peek(const struct sk_buff_head *list_)\n{\n\tstruct sk_buff *skb = list_->next;\n\n\tif (skb == (struct sk_buff *)list_)\n\t\tskb = NULL;\n\treturn skb;\n}\n\n/**\n *\t__skb_peek - peek at the head of a non-empty &sk_buff_head\n *\t@list_: list to peek at\n *\n *\tLike skb_peek(), but the caller knows that the list is not empty.\n */\nstatic inline struct sk_buff *__skb_peek(const struct sk_buff_head *list_)\n{\n\treturn list_->next;\n}\n\n/**\n *\tskb_peek_next - peek skb following the given one from a queue\n *\t@skb: skb to start from\n *\t@list_: list to peek at\n *\n *\tReturns %NULL when the end of the list is met or a pointer to the\n *\tnext element. The reference count is not incremented and the\n *\treference is therefore volatile. Use with caution.\n */\nstatic inline struct sk_buff *skb_peek_next(struct sk_buff *skb,\n\t\tconst struct sk_buff_head *list_)\n{\n\tstruct sk_buff *next = skb->next;\n\n\tif (next == (struct sk_buff *)list_)\n\t\tnext = NULL;\n\treturn next;\n}\n\n/**\n *\tskb_peek_tail - peek at the tail of an &sk_buff_head\n *\t@list_: list to peek at\n *\n *\tPeek an &sk_buff. Unlike most other operations you _MUST_\n *\tbe careful with this one. A peek leaves the buffer on the\n *\tlist and someone else may run off with it. You must hold\n *\tthe appropriate locks or have a private queue to do this.\n *\n *\tReturns %NULL for an empty list or a pointer to the tail element.\n *\tThe reference count is not incremented and the reference is therefore\n *\tvolatile. Use with caution.\n */\nstatic inline struct sk_buff *skb_peek_tail(const struct sk_buff_head *list_)\n{\n\tstruct sk_buff *skb = READ_ONCE(list_->prev);\n\n\tif (skb == (struct sk_buff *)list_)\n\t\tskb = NULL;\n\treturn skb;\n\n}\n\n/**\n *\tskb_queue_len\t- get queue length\n *\t@list_: list to measure\n *\n *\tReturn the length of an &sk_buff queue.\n */\nstatic inline __u32 skb_queue_len(const struct sk_buff_head *list_)\n{\n\treturn list_->qlen;\n}\n\n/**\n *\tskb_queue_len_lockless\t- get queue length\n *\t@list_: list to measure\n *\n *\tReturn the length of an &sk_buff queue.\n *\tThis variant can be used in lockless contexts.\n */\nstatic inline __u32 skb_queue_len_lockless(const struct sk_buff_head *list_)\n{\n\treturn READ_ONCE(list_->qlen);\n}\n\n/**\n *\t__skb_queue_head_init - initialize non-spinlock portions of sk_buff_head\n *\t@list: queue to initialize\n *\n *\tThis initializes only the list and queue length aspects of\n *\tan sk_buff_head object.  This allows to initialize the list\n *\taspects of an sk_buff_head without reinitializing things like\n *\tthe spinlock.  It can also be used for on-stack sk_buff_head\n *\tobjects where the spinlock is known to not be used.\n */\nstatic inline void __skb_queue_head_init(struct sk_buff_head *list)\n{\n\tlist->prev = list->next = (struct sk_buff *)list;\n\tlist->qlen = 0;\n}\n\n/*\n * This function creates a split out lock class for each invocation;\n * this is needed for now since a whole lot of users of the skb-queue\n * infrastructure in drivers have different locking usage (in hardirq)\n * than the networking core (in softirq only). In the long run either the\n * network layer or drivers should need annotation to consolidate the\n * main types of usage into 3 classes.\n */\nstatic inline void skb_queue_head_init(struct sk_buff_head *list)\n{\n\tspin_lock_init(&list->lock);\n\t__skb_queue_head_init(list);\n}\n\nstatic inline void skb_queue_head_init_class(struct sk_buff_head *list,\n\t\tstruct lock_class_key *class)\n{\n\tskb_queue_head_init(list);\n\tlockdep_set_class(&list->lock, class);\n}\n\n/*\n *\tInsert an sk_buff on a list.\n *\n *\tThe \"__skb_xxxx()\" functions are the non-atomic ones that\n *\tcan only be called with interrupts disabled.\n */\nstatic inline void __skb_insert(struct sk_buff *newsk,\n\t\t\t\tstruct sk_buff *prev, struct sk_buff *next,\n\t\t\t\tstruct sk_buff_head *list)\n{\n\t/* See skb_queue_empty_lockless() and skb_peek_tail()\n\t * for the opposite READ_ONCE()\n\t */\n\tWRITE_ONCE(newsk->next, next);\n\tWRITE_ONCE(newsk->prev, prev);\n\tWRITE_ONCE(next->prev, newsk);\n\tWRITE_ONCE(prev->next, newsk);\n\tlist->qlen++;\n}\n\nstatic inline void __skb_queue_splice(const struct sk_buff_head *list,\n\t\t\t\t      struct sk_buff *prev,\n\t\t\t\t      struct sk_buff *next)\n{\n\tstruct sk_buff *first = list->next;\n\tstruct sk_buff *last = list->prev;\n\n\tWRITE_ONCE(first->prev, prev);\n\tWRITE_ONCE(prev->next, first);\n\n\tWRITE_ONCE(last->next, next);\n\tWRITE_ONCE(next->prev, last);\n}\n\n/**\n *\tskb_queue_splice - join two skb lists, this is designed for stacks\n *\t@list: the new list to add\n *\t@head: the place to add it in the first list\n */\nstatic inline void skb_queue_splice(const struct sk_buff_head *list,\n\t\t\t\t    struct sk_buff_head *head)\n{\n\tif (!skb_queue_empty(list)) {\n\t\t__skb_queue_splice(list, (struct sk_buff *) head, head->next);\n\t\thead->qlen += list->qlen;\n\t}\n}\n\n/**\n *\tskb_queue_splice_init - join two skb lists and reinitialise the emptied list\n *\t@list: the new list to add\n *\t@head: the place to add it in the first list\n *\n *\tThe list at @list is reinitialised\n */\nstatic inline void skb_queue_splice_init(struct sk_buff_head *list,\n\t\t\t\t\t struct sk_buff_head *head)\n{\n\tif (!skb_queue_empty(list)) {\n\t\t__skb_queue_splice(list, (struct sk_buff *) head, head->next);\n\t\thead->qlen += list->qlen;\n\t\t__skb_queue_head_init(list);\n\t}\n}\n\n/**\n *\tskb_queue_splice_tail - join two skb lists, each list being a queue\n *\t@list: the new list to add\n *\t@head: the place to add it in the first list\n */\nstatic inline void skb_queue_splice_tail(const struct sk_buff_head *list,\n\t\t\t\t\t struct sk_buff_head *head)\n{\n\tif (!skb_queue_empty(list)) {\n\t\t__skb_queue_splice(list, head->prev, (struct sk_buff *) head);\n\t\thead->qlen += list->qlen;\n\t}\n}\n\n/**\n *\tskb_queue_splice_tail_init - join two skb lists and reinitialise the emptied list\n *\t@list: the new list to add\n *\t@head: the place to add it in the first list\n *\n *\tEach of the lists is a queue.\n *\tThe list at @list is reinitialised\n */\nstatic inline void skb_queue_splice_tail_init(struct sk_buff_head *list,\n\t\t\t\t\t      struct sk_buff_head *head)\n{\n\tif (!skb_queue_empty(list)) {\n\t\t__skb_queue_splice(list, head->prev, (struct sk_buff *) head);\n\t\thead->qlen += list->qlen;\n\t\t__skb_queue_head_init(list);\n\t}\n}\n\n/**\n *\t__skb_queue_after - queue a buffer at the list head\n *\t@list: list to use\n *\t@prev: place after this buffer\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer int the middle of a list. This function takes no locks\n *\tand you must therefore hold required locks before calling it.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nstatic inline void __skb_queue_after(struct sk_buff_head *list,\n\t\t\t\t     struct sk_buff *prev,\n\t\t\t\t     struct sk_buff *newsk)\n{\n\t__skb_insert(newsk, prev, prev->next, list);\n}\n\nvoid skb_append(struct sk_buff *old, struct sk_buff *newsk,\n\t\tstruct sk_buff_head *list);\n\nstatic inline void __skb_queue_before(struct sk_buff_head *list,\n\t\t\t\t      struct sk_buff *next,\n\t\t\t\t      struct sk_buff *newsk)\n{\n\t__skb_insert(newsk, next->prev, next, list);\n}\n\n/**\n *\t__skb_queue_head - queue a buffer at the list head\n *\t@list: list to use\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer at the start of a list. This function takes no locks\n *\tand you must therefore hold required locks before calling it.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nstatic inline void __skb_queue_head(struct sk_buff_head *list,\n\t\t\t\t    struct sk_buff *newsk)\n{\n\t__skb_queue_after(list, (struct sk_buff *)list, newsk);\n}\nvoid skb_queue_head(struct sk_buff_head *list, struct sk_buff *newsk);\n\n/**\n *\t__skb_queue_tail - queue a buffer at the list tail\n *\t@list: list to use\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer at the end of a list. This function takes no locks\n *\tand you must therefore hold required locks before calling it.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nstatic inline void __skb_queue_tail(struct sk_buff_head *list,\n\t\t\t\t   struct sk_buff *newsk)\n{\n\t__skb_queue_before(list, (struct sk_buff *)list, newsk);\n}\nvoid skb_queue_tail(struct sk_buff_head *list, struct sk_buff *newsk);\n\n/*\n * remove sk_buff from list. _Must_ be called atomically, and with\n * the list known..\n */\nvoid skb_unlink(struct sk_buff *skb, struct sk_buff_head *list);\nstatic inline void __skb_unlink(struct sk_buff *skb, struct sk_buff_head *list)\n{\n\tstruct sk_buff *next, *prev;\n\n\tWRITE_ONCE(list->qlen, list->qlen - 1);\n\tnext\t   = skb->next;\n\tprev\t   = skb->prev;\n\tskb->next  = skb->prev = NULL;\n\tWRITE_ONCE(next->prev, prev);\n\tWRITE_ONCE(prev->next, next);\n}\n\n/**\n *\t__skb_dequeue - remove from the head of the queue\n *\t@list: list to dequeue from\n *\n *\tRemove the head of the list. This function does not take any locks\n *\tso must be used with appropriate locks held only. The head item is\n *\treturned or %NULL if the list is empty.\n */\nstatic inline struct sk_buff *__skb_dequeue(struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb = skb_peek(list);\n\tif (skb)\n\t\t__skb_unlink(skb, list);\n\treturn skb;\n}\nstruct sk_buff *skb_dequeue(struct sk_buff_head *list);\n\n/**\n *\t__skb_dequeue_tail - remove from the tail of the queue\n *\t@list: list to dequeue from\n *\n *\tRemove the tail of the list. This function does not take any locks\n *\tso must be used with appropriate locks held only. The tail item is\n *\treturned or %NULL if the list is empty.\n */\nstatic inline struct sk_buff *__skb_dequeue_tail(struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb = skb_peek_tail(list);\n\tif (skb)\n\t\t__skb_unlink(skb, list);\n\treturn skb;\n}\nstruct sk_buff *skb_dequeue_tail(struct sk_buff_head *list);\n\n\nstatic inline bool skb_is_nonlinear(const struct sk_buff *skb)\n{\n\treturn skb->data_len;\n}\n\nstatic inline unsigned int skb_headlen(const struct sk_buff *skb)\n{\n\treturn skb->len - skb->data_len;\n}\n\nstatic inline unsigned int __skb_pagelen(const struct sk_buff *skb)\n{\n\tunsigned int i, len = 0;\n\n\tfor (i = skb_shinfo(skb)->nr_frags - 1; (int)i >= 0; i--)\n\t\tlen += skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\treturn len;\n}\n\nstatic inline unsigned int skb_pagelen(const struct sk_buff *skb)\n{\n\treturn skb_headlen(skb) + __skb_pagelen(skb);\n}\n\n/**\n * __skb_fill_page_desc - initialise a paged fragment in an skb\n * @skb: buffer containing fragment to be initialised\n * @i: paged fragment index to initialise\n * @page: the page to use for this fragment\n * @off: the offset to the data with @page\n * @size: the length of the data\n *\n * Initialises the @i'th fragment of @skb to point to &size bytes at\n * offset @off within @page.\n *\n * Does not take any additional reference on the fragment.\n */\nstatic inline void __skb_fill_page_desc(struct sk_buff *skb, int i,\n\t\t\t\t\tstruct page *page, int off, int size)\n{\n\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t/*\n\t * Propagate page pfmemalloc to the skb if we can. The problem is\n\t * that not all callers have unique ownership of the page but rely\n\t * on page_is_pfmemalloc doing the right thing(tm).\n\t */\n\tfrag->bv_page\t\t  = page;\n\tfrag->bv_offset\t\t  = off;\n\tskb_frag_size_set(frag, size);\n\n\tpage = compound_head(page);\n\tif (page_is_pfmemalloc(page))\n\t\tskb->pfmemalloc\t= true;\n}\n\n/**\n * skb_fill_page_desc - initialise a paged fragment in an skb\n * @skb: buffer containing fragment to be initialised\n * @i: paged fragment index to initialise\n * @page: the page to use for this fragment\n * @off: the offset to the data with @page\n * @size: the length of the data\n *\n * As per __skb_fill_page_desc() -- initialises the @i'th fragment of\n * @skb to point to @size bytes at offset @off within @page. In\n * addition updates @skb such that @i is the last fragment.\n *\n * Does not take any additional reference on the fragment.\n */\nstatic inline void skb_fill_page_desc(struct sk_buff *skb, int i,\n\t\t\t\t      struct page *page, int off, int size)\n{\n\t__skb_fill_page_desc(skb, i, page, off, size);\n\tskb_shinfo(skb)->nr_frags = i + 1;\n}\n\nvoid skb_add_rx_frag(struct sk_buff *skb, int i, struct page *page, int off,\n\t\t     int size, unsigned int truesize);\n\nvoid skb_coalesce_rx_frag(struct sk_buff *skb, int i, int size,\n\t\t\t  unsigned int truesize);\n\n#define SKB_LINEAR_ASSERT(skb)  BUG_ON(skb_is_nonlinear(skb))\n\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\nstatic inline unsigned char *skb_tail_pointer(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->tail;\n}\n\nstatic inline void skb_reset_tail_pointer(struct sk_buff *skb)\n{\n\tskb->tail = skb->data - skb->head;\n}\n\nstatic inline void skb_set_tail_pointer(struct sk_buff *skb, const int offset)\n{\n\tskb_reset_tail_pointer(skb);\n\tskb->tail += offset;\n}\n\n#else /* NET_SKBUFF_DATA_USES_OFFSET */\nstatic inline unsigned char *skb_tail_pointer(const struct sk_buff *skb)\n{\n\treturn skb->tail;\n}\n\nstatic inline void skb_reset_tail_pointer(struct sk_buff *skb)\n{\n\tskb->tail = skb->data;\n}\n\nstatic inline void skb_set_tail_pointer(struct sk_buff *skb, const int offset)\n{\n\tskb->tail = skb->data + offset;\n}\n\n#endif /* NET_SKBUFF_DATA_USES_OFFSET */\n\n/*\n *\tAdd data to an sk_buff\n */\nvoid *pskb_put(struct sk_buff *skb, struct sk_buff *tail, int len);\nvoid *skb_put(struct sk_buff *skb, unsigned int len);\nstatic inline void *__skb_put(struct sk_buff *skb, unsigned int len)\n{\n\tvoid *tmp = skb_tail_pointer(skb);\n\tSKB_LINEAR_ASSERT(skb);\n\tskb->tail += len;\n\tskb->len  += len;\n\treturn tmp;\n}\n\nstatic inline void *__skb_put_zero(struct sk_buff *skb, unsigned int len)\n{\n\tvoid *tmp = __skb_put(skb, len);\n\n\tmemset(tmp, 0, len);\n\treturn tmp;\n}\n\nstatic inline void *__skb_put_data(struct sk_buff *skb, const void *data,\n\t\t\t\t   unsigned int len)\n{\n\tvoid *tmp = __skb_put(skb, len);\n\n\tmemcpy(tmp, data, len);\n\treturn tmp;\n}\n\nstatic inline void __skb_put_u8(struct sk_buff *skb, u8 val)\n{\n\t*(u8 *)__skb_put(skb, 1) = val;\n}\n\nstatic inline void *skb_put_zero(struct sk_buff *skb, unsigned int len)\n{\n\tvoid *tmp = skb_put(skb, len);\n\n\tmemset(tmp, 0, len);\n\n\treturn tmp;\n}\n\nstatic inline void *skb_put_data(struct sk_buff *skb, const void *data,\n\t\t\t\t unsigned int len)\n{\n\tvoid *tmp = skb_put(skb, len);\n\n\tmemcpy(tmp, data, len);\n\n\treturn tmp;\n}\n\nstatic inline void skb_put_u8(struct sk_buff *skb, u8 val)\n{\n\t*(u8 *)skb_put(skb, 1) = val;\n}\n\nvoid *skb_push(struct sk_buff *skb, unsigned int len);\nstatic inline void *__skb_push(struct sk_buff *skb, unsigned int len)\n{\n\tskb->data -= len;\n\tskb->len  += len;\n\treturn skb->data;\n}\n\nvoid *skb_pull(struct sk_buff *skb, unsigned int len);\nstatic inline void *__skb_pull(struct sk_buff *skb, unsigned int len)\n{\n\tskb->len -= len;\n\tBUG_ON(skb->len < skb->data_len);\n\treturn skb->data += len;\n}\n\nstatic inline void *skb_pull_inline(struct sk_buff *skb, unsigned int len)\n{\n\treturn unlikely(len > skb->len) ? NULL : __skb_pull(skb, len);\n}\n\nvoid *__pskb_pull_tail(struct sk_buff *skb, int delta);\n\nstatic inline void *__pskb_pull(struct sk_buff *skb, unsigned int len)\n{\n\tif (len > skb_headlen(skb) &&\n\t    !__pskb_pull_tail(skb, len - skb_headlen(skb)))\n\t\treturn NULL;\n\tskb->len -= len;\n\treturn skb->data += len;\n}\n\nstatic inline void *pskb_pull(struct sk_buff *skb, unsigned int len)\n{\n\treturn unlikely(len > skb->len) ? NULL : __pskb_pull(skb, len);\n}\n\nstatic inline bool pskb_may_pull(struct sk_buff *skb, unsigned int len)\n{\n\tif (likely(len <= skb_headlen(skb)))\n\t\treturn true;\n\tif (unlikely(len > skb->len))\n\t\treturn false;\n\treturn __pskb_pull_tail(skb, len - skb_headlen(skb)) != NULL;\n}\n\nvoid skb_condense(struct sk_buff *skb);\n\n/**\n *\tskb_headroom - bytes at buffer head\n *\t@skb: buffer to check\n *\n *\tReturn the number of bytes of free space at the head of an &sk_buff.\n */\nstatic inline unsigned int skb_headroom(const struct sk_buff *skb)\n{\n\treturn skb->data - skb->head;\n}\n\n/**\n *\tskb_tailroom - bytes at buffer end\n *\t@skb: buffer to check\n *\n *\tReturn the number of bytes of free space at the tail of an sk_buff\n */\nstatic inline int skb_tailroom(const struct sk_buff *skb)\n{\n\treturn skb_is_nonlinear(skb) ? 0 : skb->end - skb->tail;\n}\n\n/**\n *\tskb_availroom - bytes at buffer end\n *\t@skb: buffer to check\n *\n *\tReturn the number of bytes of free space at the tail of an sk_buff\n *\tallocated by sk_stream_alloc()\n */\nstatic inline int skb_availroom(const struct sk_buff *skb)\n{\n\tif (skb_is_nonlinear(skb))\n\t\treturn 0;\n\n\treturn skb->end - skb->tail - skb->reserved_tailroom;\n}\n\n/**\n *\tskb_reserve - adjust headroom\n *\t@skb: buffer to alter\n *\t@len: bytes to move\n *\n *\tIncrease the headroom of an empty &sk_buff by reducing the tail\n *\troom. This is only allowed for an empty buffer.\n */\nstatic inline void skb_reserve(struct sk_buff *skb, int len)\n{\n\tskb->data += len;\n\tskb->tail += len;\n}\n\n/**\n *\tskb_tailroom_reserve - adjust reserved_tailroom\n *\t@skb: buffer to alter\n *\t@mtu: maximum amount of headlen permitted\n *\t@needed_tailroom: minimum amount of reserved_tailroom\n *\n *\tSet reserved_tailroom so that headlen can be as large as possible but\n *\tnot larger than mtu and tailroom cannot be smaller than\n *\tneeded_tailroom.\n *\tThe required headroom should already have been reserved before using\n *\tthis function.\n */\nstatic inline void skb_tailroom_reserve(struct sk_buff *skb, unsigned int mtu,\n\t\t\t\t\tunsigned int needed_tailroom)\n{\n\tSKB_LINEAR_ASSERT(skb);\n\tif (mtu < skb_tailroom(skb) - needed_tailroom)\n\t\t/* use at most mtu */\n\t\tskb->reserved_tailroom = skb_tailroom(skb) - mtu;\n\telse\n\t\t/* use up to all available space */\n\t\tskb->reserved_tailroom = needed_tailroom;\n}\n\n#define ENCAP_TYPE_ETHER\t0\n#define ENCAP_TYPE_IPPROTO\t1\n\nstatic inline void skb_set_inner_protocol(struct sk_buff *skb,\n\t\t\t\t\t  __be16 protocol)\n{\n\tskb->inner_protocol = protocol;\n\tskb->inner_protocol_type = ENCAP_TYPE_ETHER;\n}\n\nstatic inline void skb_set_inner_ipproto(struct sk_buff *skb,\n\t\t\t\t\t __u8 ipproto)\n{\n\tskb->inner_ipproto = ipproto;\n\tskb->inner_protocol_type = ENCAP_TYPE_IPPROTO;\n}\n\nstatic inline void skb_reset_inner_headers(struct sk_buff *skb)\n{\n\tskb->inner_mac_header = skb->mac_header;\n\tskb->inner_network_header = skb->network_header;\n\tskb->inner_transport_header = skb->transport_header;\n}\n\nstatic inline void skb_reset_mac_len(struct sk_buff *skb)\n{\n\tskb->mac_len = skb->network_header - skb->mac_header;\n}\n\nstatic inline unsigned char *skb_inner_transport_header(const struct sk_buff\n\t\t\t\t\t\t\t*skb)\n{\n\treturn skb->head + skb->inner_transport_header;\n}\n\nstatic inline int skb_inner_transport_offset(const struct sk_buff *skb)\n{\n\treturn skb_inner_transport_header(skb) - skb->data;\n}\n\nstatic inline void skb_reset_inner_transport_header(struct sk_buff *skb)\n{\n\tskb->inner_transport_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_inner_transport_header(struct sk_buff *skb,\n\t\t\t\t\t\t   const int offset)\n{\n\tskb_reset_inner_transport_header(skb);\n\tskb->inner_transport_header += offset;\n}\n\nstatic inline unsigned char *skb_inner_network_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->inner_network_header;\n}\n\nstatic inline void skb_reset_inner_network_header(struct sk_buff *skb)\n{\n\tskb->inner_network_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_inner_network_header(struct sk_buff *skb,\n\t\t\t\t\t\tconst int offset)\n{\n\tskb_reset_inner_network_header(skb);\n\tskb->inner_network_header += offset;\n}\n\nstatic inline unsigned char *skb_inner_mac_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->inner_mac_header;\n}\n\nstatic inline void skb_reset_inner_mac_header(struct sk_buff *skb)\n{\n\tskb->inner_mac_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_inner_mac_header(struct sk_buff *skb,\n\t\t\t\t\t    const int offset)\n{\n\tskb_reset_inner_mac_header(skb);\n\tskb->inner_mac_header += offset;\n}\nstatic inline bool skb_transport_header_was_set(const struct sk_buff *skb)\n{\n\treturn skb->transport_header != (typeof(skb->transport_header))~0U;\n}\n\nstatic inline unsigned char *skb_transport_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->transport_header;\n}\n\nstatic inline void skb_reset_transport_header(struct sk_buff *skb)\n{\n\tskb->transport_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_transport_header(struct sk_buff *skb,\n\t\t\t\t\t    const int offset)\n{\n\tskb_reset_transport_header(skb);\n\tskb->transport_header += offset;\n}\n\nstatic inline unsigned char *skb_network_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->network_header;\n}\n\nstatic inline void skb_reset_network_header(struct sk_buff *skb)\n{\n\tskb->network_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_network_header(struct sk_buff *skb, const int offset)\n{\n\tskb_reset_network_header(skb);\n\tskb->network_header += offset;\n}\n\nstatic inline unsigned char *skb_mac_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->mac_header;\n}\n\nstatic inline int skb_mac_offset(const struct sk_buff *skb)\n{\n\treturn skb_mac_header(skb) - skb->data;\n}\n\nstatic inline u32 skb_mac_header_len(const struct sk_buff *skb)\n{\n\treturn skb->network_header - skb->mac_header;\n}\n\nstatic inline int skb_mac_header_was_set(const struct sk_buff *skb)\n{\n\treturn skb->mac_header != (typeof(skb->mac_header))~0U;\n}\n\nstatic inline void skb_unset_mac_header(struct sk_buff *skb)\n{\n\tskb->mac_header = (typeof(skb->mac_header))~0U;\n}\n\nstatic inline void skb_reset_mac_header(struct sk_buff *skb)\n{\n\tskb->mac_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_mac_header(struct sk_buff *skb, const int offset)\n{\n\tskb_reset_mac_header(skb);\n\tskb->mac_header += offset;\n}\n\nstatic inline void skb_pop_mac_header(struct sk_buff *skb)\n{\n\tskb->mac_header = skb->network_header;\n}\n\nstatic inline void skb_probe_transport_header(struct sk_buff *skb)\n{\n\tstruct flow_keys_basic keys;\n\n\tif (skb_transport_header_was_set(skb))\n\t\treturn;\n\n\tif (skb_flow_dissect_flow_keys_basic(NULL, skb, &keys,\n\t\t\t\t\t     NULL, 0, 0, 0, 0))\n\t\tskb_set_transport_header(skb, keys.control.thoff);\n}\n\nstatic inline void skb_mac_header_rebuild(struct sk_buff *skb)\n{\n\tif (skb_mac_header_was_set(skb)) {\n\t\tconst unsigned char *old_mac = skb_mac_header(skb);\n\n\t\tskb_set_mac_header(skb, -skb->mac_len);\n\t\tmemmove(skb_mac_header(skb), old_mac, skb->mac_len);\n\t}\n}\n\nstatic inline int skb_checksum_start_offset(const struct sk_buff *skb)\n{\n\treturn skb->csum_start - skb_headroom(skb);\n}\n\nstatic inline unsigned char *skb_checksum_start(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->csum_start;\n}\n\nstatic inline int skb_transport_offset(const struct sk_buff *skb)\n{\n\treturn skb_transport_header(skb) - skb->data;\n}\n\nstatic inline u32 skb_network_header_len(const struct sk_buff *skb)\n{\n\treturn skb->transport_header - skb->network_header;\n}\n\nstatic inline u32 skb_inner_network_header_len(const struct sk_buff *skb)\n{\n\treturn skb->inner_transport_header - skb->inner_network_header;\n}\n\nstatic inline int skb_network_offset(const struct sk_buff *skb)\n{\n\treturn skb_network_header(skb) - skb->data;\n}\n\nstatic inline int skb_inner_network_offset(const struct sk_buff *skb)\n{\n\treturn skb_inner_network_header(skb) - skb->data;\n}\n\nstatic inline int pskb_network_may_pull(struct sk_buff *skb, unsigned int len)\n{\n\treturn pskb_may_pull(skb, skb_network_offset(skb) + len);\n}\n\n/*\n * CPUs often take a performance hit when accessing unaligned memory\n * locations. The actual performance hit varies, it can be small if the\n * hardware handles it or large if we have to take an exception and fix it\n * in software.\n *\n * Since an ethernet header is 14 bytes network drivers often end up with\n * the IP header at an unaligned offset. The IP header can be aligned by\n * shifting the start of the packet by 2 bytes. Drivers should do this\n * with:\n *\n * skb_reserve(skb, NET_IP_ALIGN);\n *\n * The downside to this alignment of the IP header is that the DMA is now\n * unaligned. On some architectures the cost of an unaligned DMA is high\n * and this cost outweighs the gains made by aligning the IP header.\n *\n * Since this trade off varies between architectures, we allow NET_IP_ALIGN\n * to be overridden.\n */\n#ifndef NET_IP_ALIGN\n#define NET_IP_ALIGN\t2\n#endif\n\n/*\n * The networking layer reserves some headroom in skb data (via\n * dev_alloc_skb). This is used to avoid having to reallocate skb data when\n * the header has to grow. In the default case, if the header has to grow\n * 32 bytes or less we avoid the reallocation.\n *\n * Unfortunately this headroom changes the DMA alignment of the resulting\n * network packet. As for NET_IP_ALIGN, this unaligned DMA is expensive\n * on some architectures. An architecture can override this value,\n * perhaps setting it to a cacheline in size (since that will maintain\n * cacheline alignment of the DMA). It must be a power of 2.\n *\n * Various parts of the networking layer expect at least 32 bytes of\n * headroom, you should not reduce this.\n *\n * Using max(32, L1_CACHE_BYTES) makes sense (especially with RPS)\n * to reduce average number of cache lines per packet.\n * get_rps_cpu() for example only access one 64 bytes aligned block :\n * NET_IP_ALIGN(2) + ethernet_header(14) + IP_header(20/40) + ports(8)\n */\n#ifndef NET_SKB_PAD\n#define NET_SKB_PAD\tmax(32, L1_CACHE_BYTES)\n#endif\n\nint ___pskb_trim(struct sk_buff *skb, unsigned int len);\n\nstatic inline void __skb_set_length(struct sk_buff *skb, unsigned int len)\n{\n\tif (WARN_ON(skb_is_nonlinear(skb)))\n\t\treturn;\n\tskb->len = len;\n\tskb_set_tail_pointer(skb, len);\n}\n\nstatic inline void __skb_trim(struct sk_buff *skb, unsigned int len)\n{\n\t__skb_set_length(skb, len);\n}\n\nvoid skb_trim(struct sk_buff *skb, unsigned int len);\n\nstatic inline int __pskb_trim(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->data_len)\n\t\treturn ___pskb_trim(skb, len);\n\t__skb_trim(skb, len);\n\treturn 0;\n}\n\nstatic inline int pskb_trim(struct sk_buff *skb, unsigned int len)\n{\n\treturn (len < skb->len) ? __pskb_trim(skb, len) : 0;\n}\n\n/**\n *\tpskb_trim_unique - remove end from a paged unique (not cloned) buffer\n *\t@skb: buffer to alter\n *\t@len: new length\n *\n *\tThis is identical to pskb_trim except that the caller knows that\n *\tthe skb is not cloned so we should never get an error due to out-\n *\tof-memory.\n */\nstatic inline void pskb_trim_unique(struct sk_buff *skb, unsigned int len)\n{\n\tint err = pskb_trim(skb, len);\n\tBUG_ON(err);\n}\n\nstatic inline int __skb_grow(struct sk_buff *skb, unsigned int len)\n{\n\tunsigned int diff = len - skb->len;\n\n\tif (skb_tailroom(skb) < diff) {\n\t\tint ret = pskb_expand_head(skb, 0, diff - skb_tailroom(skb),\n\t\t\t\t\t   GFP_ATOMIC);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\t__skb_set_length(skb, len);\n\treturn 0;\n}\n\n/**\n *\tskb_orphan - orphan a buffer\n *\t@skb: buffer to orphan\n *\n *\tIf a buffer currently has an owner then we call the owner's\n *\tdestructor function and make the @skb unowned. The buffer continues\n *\tto exist but is no longer charged to its former owner.\n */\nstatic inline void skb_orphan(struct sk_buff *skb)\n{\n\tif (skb->destructor) {\n\t\tskb->destructor(skb);\n\t\tskb->destructor = NULL;\n\t\tskb->sk\t\t= NULL;\n\t} else {\n\t\tBUG_ON(skb->sk);\n\t}\n}\n\n/**\n *\tskb_orphan_frags - orphan the frags contained in a buffer\n *\t@skb: buffer to orphan frags from\n *\t@gfp_mask: allocation mask for replacement pages\n *\n *\tFor each frag in the SKB which needs a destructor (i.e. has an\n *\towner) create a copy of that frag and release the original\n *\tpage by calling the destructor.\n */\nstatic inline int skb_orphan_frags(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tif (likely(!skb_zcopy(skb)))\n\t\treturn 0;\n\tif (!skb_zcopy_is_nouarg(skb) &&\n\t    skb_uarg(skb)->callback == sock_zerocopy_callback)\n\t\treturn 0;\n\treturn skb_copy_ubufs(skb, gfp_mask);\n}\n\n/* Frags must be orphaned, even if refcounted, if skb might loop to rx path */\nstatic inline int skb_orphan_frags_rx(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tif (likely(!skb_zcopy(skb)))\n\t\treturn 0;\n\treturn skb_copy_ubufs(skb, gfp_mask);\n}\n\n/**\n *\t__skb_queue_purge - empty a list\n *\t@list: list to empty\n *\n *\tDelete all buffers on an &sk_buff list. Each buffer is removed from\n *\tthe list and one reference dropped. This function does not take the\n *\tlist lock and the caller must hold the relevant locks to use it.\n */\nstatic inline void __skb_queue_purge(struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb;\n\twhile ((skb = __skb_dequeue(list)) != NULL)\n\t\tkfree_skb(skb);\n}\nvoid skb_queue_purge(struct sk_buff_head *list);\n\nunsigned int skb_rbtree_purge(struct rb_root *root);\n\nvoid *netdev_alloc_frag(unsigned int fragsz);\n\nstruct sk_buff *__netdev_alloc_skb(struct net_device *dev, unsigned int length,\n\t\t\t\t   gfp_t gfp_mask);\n\n/**\n *\tnetdev_alloc_skb - allocate an skbuff for rx on a specific device\n *\t@dev: network device to receive on\n *\t@length: length to allocate\n *\n *\tAllocate a new &sk_buff and assign it a usage count of one. The\n *\tbuffer has unspecified headroom built in. Users should allocate\n *\tthe headroom they think they need without accounting for the\n *\tbuilt in space. The built in space is used for optimisations.\n *\n *\t%NULL is returned if there is no free memory. Although this function\n *\tallocates memory it can be called from an interrupt.\n */\nstatic inline struct sk_buff *netdev_alloc_skb(struct net_device *dev,\n\t\t\t\t\t       unsigned int length)\n{\n\treturn __netdev_alloc_skb(dev, length, GFP_ATOMIC);\n}\n\n/* legacy helper around __netdev_alloc_skb() */\nstatic inline struct sk_buff *__dev_alloc_skb(unsigned int length,\n\t\t\t\t\t      gfp_t gfp_mask)\n{\n\treturn __netdev_alloc_skb(NULL, length, gfp_mask);\n}\n\n/* legacy helper around netdev_alloc_skb() */\nstatic inline struct sk_buff *dev_alloc_skb(unsigned int length)\n{\n\treturn netdev_alloc_skb(NULL, length);\n}\n\n\nstatic inline struct sk_buff *__netdev_alloc_skb_ip_align(struct net_device *dev,\n\t\tunsigned int length, gfp_t gfp)\n{\n\tstruct sk_buff *skb = __netdev_alloc_skb(dev, length + NET_IP_ALIGN, gfp);\n\n\tif (NET_IP_ALIGN && skb)\n\t\tskb_reserve(skb, NET_IP_ALIGN);\n\treturn skb;\n}\n\nstatic inline struct sk_buff *netdev_alloc_skb_ip_align(struct net_device *dev,\n\t\tunsigned int length)\n{\n\treturn __netdev_alloc_skb_ip_align(dev, length, GFP_ATOMIC);\n}\n\nstatic inline void skb_free_frag(void *addr)\n{\n\tpage_frag_free(addr);\n}\n\nvoid *napi_alloc_frag(unsigned int fragsz);\nstruct sk_buff *__napi_alloc_skb(struct napi_struct *napi,\n\t\t\t\t unsigned int length, gfp_t gfp_mask);\nstatic inline struct sk_buff *napi_alloc_skb(struct napi_struct *napi,\n\t\t\t\t\t     unsigned int length)\n{\n\treturn __napi_alloc_skb(napi, length, GFP_ATOMIC);\n}\nvoid napi_consume_skb(struct sk_buff *skb, int budget);\n\nvoid __kfree_skb_flush(void);\nvoid __kfree_skb_defer(struct sk_buff *skb);\n\n/**\n * __dev_alloc_pages - allocate page for network Rx\n * @gfp_mask: allocation priority. Set __GFP_NOMEMALLOC if not for network Rx\n * @order: size of the allocation\n *\n * Allocate a new page.\n *\n * %NULL is returned if there is no free memory.\n*/\nstatic inline struct page *__dev_alloc_pages(gfp_t gfp_mask,\n\t\t\t\t\t     unsigned int order)\n{\n\t/* This piece of code contains several assumptions.\n\t * 1.  This is for device Rx, therefor a cold page is preferred.\n\t * 2.  The expectation is the user wants a compound page.\n\t * 3.  If requesting a order 0 page it will not be compound\n\t *     due to the check to see if order has a value in prep_new_page\n\t * 4.  __GFP_MEMALLOC is ignored if __GFP_NOMEMALLOC is set due to\n\t *     code in gfp_to_alloc_flags that should be enforcing this.\n\t */\n\tgfp_mask |= __GFP_COMP | __GFP_MEMALLOC;\n\n\treturn alloc_pages_node(NUMA_NO_NODE, gfp_mask, order);\n}\n\nstatic inline struct page *dev_alloc_pages(unsigned int order)\n{\n\treturn __dev_alloc_pages(GFP_ATOMIC | __GFP_NOWARN, order);\n}\n\n/**\n * __dev_alloc_page - allocate a page for network Rx\n * @gfp_mask: allocation priority. Set __GFP_NOMEMALLOC if not for network Rx\n *\n * Allocate a new page.\n *\n * %NULL is returned if there is no free memory.\n */\nstatic inline struct page *__dev_alloc_page(gfp_t gfp_mask)\n{\n\treturn __dev_alloc_pages(gfp_mask, 0);\n}\n\nstatic inline struct page *dev_alloc_page(void)\n{\n\treturn dev_alloc_pages(0);\n}\n\n/**\n *\tskb_propagate_pfmemalloc - Propagate pfmemalloc if skb is allocated after RX page\n *\t@page: The page that was allocated from skb_alloc_page\n *\t@skb: The skb that may need pfmemalloc set\n */\nstatic inline void skb_propagate_pfmemalloc(struct page *page,\n\t\t\t\t\t     struct sk_buff *skb)\n{\n\tif (page_is_pfmemalloc(page))\n\t\tskb->pfmemalloc = true;\n}\n\n/**\n * skb_frag_off() - Returns the offset of a skb fragment\n * @frag: the paged fragment\n */\nstatic inline unsigned int skb_frag_off(const skb_frag_t *frag)\n{\n\treturn frag->bv_offset;\n}\n\n/**\n * skb_frag_off_add() - Increments the offset of a skb fragment by @delta\n * @frag: skb fragment\n * @delta: value to add\n */\nstatic inline void skb_frag_off_add(skb_frag_t *frag, int delta)\n{\n\tfrag->bv_offset += delta;\n}\n\n/**\n * skb_frag_off_set() - Sets the offset of a skb fragment\n * @frag: skb fragment\n * @offset: offset of fragment\n */\nstatic inline void skb_frag_off_set(skb_frag_t *frag, unsigned int offset)\n{\n\tfrag->bv_offset = offset;\n}\n\n/**\n * skb_frag_off_copy() - Sets the offset of a skb fragment from another fragment\n * @fragto: skb fragment where offset is set\n * @fragfrom: skb fragment offset is copied from\n */\nstatic inline void skb_frag_off_copy(skb_frag_t *fragto,\n\t\t\t\t     const skb_frag_t *fragfrom)\n{\n\tfragto->bv_offset = fragfrom->bv_offset;\n}\n\n/**\n * skb_frag_page - retrieve the page referred to by a paged fragment\n * @frag: the paged fragment\n *\n * Returns the &struct page associated with @frag.\n */\nstatic inline struct page *skb_frag_page(const skb_frag_t *frag)\n{\n\treturn frag->bv_page;\n}\n\n/**\n * __skb_frag_ref - take an addition reference on a paged fragment.\n * @frag: the paged fragment\n *\n * Takes an additional reference on the paged fragment @frag.\n */\nstatic inline void __skb_frag_ref(skb_frag_t *frag)\n{\n\tget_page(skb_frag_page(frag));\n}\n\n/**\n * skb_frag_ref - take an addition reference on a paged fragment of an skb.\n * @skb: the buffer\n * @f: the fragment offset.\n *\n * Takes an additional reference on the @f'th paged fragment of @skb.\n */\nstatic inline void skb_frag_ref(struct sk_buff *skb, int f)\n{\n\t__skb_frag_ref(&skb_shinfo(skb)->frags[f]);\n}\n\n/**\n * __skb_frag_unref - release a reference on a paged fragment.\n * @frag: the paged fragment\n *\n * Releases a reference on the paged fragment @frag.\n */\nstatic inline void __skb_frag_unref(skb_frag_t *frag)\n{\n\tput_page(skb_frag_page(frag));\n}\n\n/**\n * skb_frag_unref - release a reference on a paged fragment of an skb.\n * @skb: the buffer\n * @f: the fragment offset\n *\n * Releases a reference on the @f'th paged fragment of @skb.\n */\nstatic inline void skb_frag_unref(struct sk_buff *skb, int f)\n{\n\t__skb_frag_unref(&skb_shinfo(skb)->frags[f]);\n}\n\n/**\n * skb_frag_address - gets the address of the data contained in a paged fragment\n * @frag: the paged fragment buffer\n *\n * Returns the address of the data within @frag. The page must already\n * be mapped.\n */\nstatic inline void *skb_frag_address(const skb_frag_t *frag)\n{\n\treturn page_address(skb_frag_page(frag)) + skb_frag_off(frag);\n}\n\n/**\n * skb_frag_address_safe - gets the address of the data contained in a paged fragment\n * @frag: the paged fragment buffer\n *\n * Returns the address of the data within @frag. Checks that the page\n * is mapped and returns %NULL otherwise.\n */\nstatic inline void *skb_frag_address_safe(const skb_frag_t *frag)\n{\n\tvoid *ptr = page_address(skb_frag_page(frag));\n\tif (unlikely(!ptr))\n\t\treturn NULL;\n\n\treturn ptr + skb_frag_off(frag);\n}\n\n/**\n * skb_frag_page_copy() - sets the page in a fragment from another fragment\n * @fragto: skb fragment where page is set\n * @fragfrom: skb fragment page is copied from\n */\nstatic inline void skb_frag_page_copy(skb_frag_t *fragto,\n\t\t\t\t      const skb_frag_t *fragfrom)\n{\n\tfragto->bv_page = fragfrom->bv_page;\n}\n\n/**\n * __skb_frag_set_page - sets the page contained in a paged fragment\n * @frag: the paged fragment\n * @page: the page to set\n *\n * Sets the fragment @frag to contain @page.\n */\nstatic inline void __skb_frag_set_page(skb_frag_t *frag, struct page *page)\n{\n\tfrag->bv_page = page;\n}\n\n/**\n * skb_frag_set_page - sets the page contained in a paged fragment of an skb\n * @skb: the buffer\n * @f: the fragment offset\n * @page: the page to set\n *\n * Sets the @f'th fragment of @skb to contain @page.\n */\nstatic inline void skb_frag_set_page(struct sk_buff *skb, int f,\n\t\t\t\t     struct page *page)\n{\n\t__skb_frag_set_page(&skb_shinfo(skb)->frags[f], page);\n}\n\nbool skb_page_frag_refill(unsigned int sz, struct page_frag *pfrag, gfp_t prio);\n\n/**\n * skb_frag_dma_map - maps a paged fragment via the DMA API\n * @dev: the device to map the fragment to\n * @frag: the paged fragment to map\n * @offset: the offset within the fragment (starting at the\n *          fragment's own offset)\n * @size: the number of bytes to map\n * @dir: the direction of the mapping (``PCI_DMA_*``)\n *\n * Maps the page associated with @frag to @device.\n */\nstatic inline dma_addr_t skb_frag_dma_map(struct device *dev,\n\t\t\t\t\t  const skb_frag_t *frag,\n\t\t\t\t\t  size_t offset, size_t size,\n\t\t\t\t\t  enum dma_data_direction dir)\n{\n\treturn dma_map_page(dev, skb_frag_page(frag),\n\t\t\t    skb_frag_off(frag) + offset, size, dir);\n}\n\nstatic inline struct sk_buff *pskb_copy(struct sk_buff *skb,\n\t\t\t\t\tgfp_t gfp_mask)\n{\n\treturn __pskb_copy(skb, skb_headroom(skb), gfp_mask);\n}\n\n\nstatic inline struct sk_buff *pskb_copy_for_clone(struct sk_buff *skb,\n\t\t\t\t\t\t  gfp_t gfp_mask)\n{\n\treturn __pskb_copy_fclone(skb, skb_headroom(skb), gfp_mask, true);\n}\n\n\n/**\n *\tskb_clone_writable - is the header of a clone writable\n *\t@skb: buffer to check\n *\t@len: length up to which to write\n *\n *\tReturns true if modifying the header part of the cloned buffer\n *\tdoes not requires the data to be copied.\n */\nstatic inline int skb_clone_writable(const struct sk_buff *skb, unsigned int len)\n{\n\treturn !skb_header_cloned(skb) &&\n\t       skb_headroom(skb) + len <= skb->hdr_len;\n}\n\nstatic inline int skb_try_make_writable(struct sk_buff *skb,\n\t\t\t\t\tunsigned int write_len)\n{\n\treturn skb_cloned(skb) && !skb_clone_writable(skb, write_len) &&\n\t       pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n}\n\nstatic inline int __skb_cow(struct sk_buff *skb, unsigned int headroom,\n\t\t\t    int cloned)\n{\n\tint delta = 0;\n\n\tif (headroom > skb_headroom(skb))\n\t\tdelta = headroom - skb_headroom(skb);\n\n\tif (delta || cloned)\n\t\treturn pskb_expand_head(skb, ALIGN(delta, NET_SKB_PAD), 0,\n\t\t\t\t\tGFP_ATOMIC);\n\treturn 0;\n}\n\n/**\n *\tskb_cow - copy header of skb when it is required\n *\t@skb: buffer to cow\n *\t@headroom: needed headroom\n *\n *\tIf the skb passed lacks sufficient headroom or its data part\n *\tis shared, data is reallocated. If reallocation fails, an error\n *\tis returned and original skb is not changed.\n *\n *\tThe result is skb with writable area skb->head...skb->tail\n *\tand at least @headroom of space at head.\n */\nstatic inline int skb_cow(struct sk_buff *skb, unsigned int headroom)\n{\n\treturn __skb_cow(skb, headroom, skb_cloned(skb));\n}\n\n/**\n *\tskb_cow_head - skb_cow but only making the head writable\n *\t@skb: buffer to cow\n *\t@headroom: needed headroom\n *\n *\tThis function is identical to skb_cow except that we replace the\n *\tskb_cloned check by skb_header_cloned.  It should be used when\n *\tyou only need to push on some header and do not need to modify\n *\tthe data.\n */\nstatic inline int skb_cow_head(struct sk_buff *skb, unsigned int headroom)\n{\n\treturn __skb_cow(skb, headroom, skb_header_cloned(skb));\n}\n\n/**\n *\tskb_padto\t- pad an skbuff up to a minimal size\n *\t@skb: buffer to pad\n *\t@len: minimal length\n *\n *\tPads up a buffer to ensure the trailing bytes exist and are\n *\tblanked. If the buffer already contains sufficient data it\n *\tis untouched. Otherwise it is extended. Returns zero on\n *\tsuccess. The skb is freed on error.\n */\nstatic inline int skb_padto(struct sk_buff *skb, unsigned int len)\n{\n\tunsigned int size = skb->len;\n\tif (likely(size >= len))\n\t\treturn 0;\n\treturn skb_pad(skb, len - size);\n}\n\n/**\n *\t__skb_put_padto - increase size and pad an skbuff up to a minimal size\n *\t@skb: buffer to pad\n *\t@len: minimal length\n *\t@free_on_error: free buffer on error\n *\n *\tPads up a buffer to ensure the trailing bytes exist and are\n *\tblanked. If the buffer already contains sufficient data it\n *\tis untouched. Otherwise it is extended. Returns zero on\n *\tsuccess. The skb is freed on error if @free_on_error is true.\n */\nstatic inline int __must_check __skb_put_padto(struct sk_buff *skb,\n\t\t\t\t\t       unsigned int len,\n\t\t\t\t\t       bool free_on_error)\n{\n\tunsigned int size = skb->len;\n\n\tif (unlikely(size < len)) {\n\t\tlen -= size;\n\t\tif (__skb_pad(skb, len, free_on_error))\n\t\t\treturn -ENOMEM;\n\t\t__skb_put(skb, len);\n\t}\n\treturn 0;\n}\n\n/**\n *\tskb_put_padto - increase size and pad an skbuff up to a minimal size\n *\t@skb: buffer to pad\n *\t@len: minimal length\n *\n *\tPads up a buffer to ensure the trailing bytes exist and are\n *\tblanked. If the buffer already contains sufficient data it\n *\tis untouched. Otherwise it is extended. Returns zero on\n *\tsuccess. The skb is freed on error.\n */\nstatic inline int __must_check skb_put_padto(struct sk_buff *skb, unsigned int len)\n{\n\treturn __skb_put_padto(skb, len, true);\n}\n\nstatic inline int skb_add_data(struct sk_buff *skb,\n\t\t\t       struct iov_iter *from, int copy)\n{\n\tconst int off = skb->len;\n\n\tif (skb->ip_summed == CHECKSUM_NONE) {\n\t\t__wsum csum = 0;\n\t\tif (csum_and_copy_from_iter_full(skb_put(skb, copy), copy,\n\t\t\t\t\t         &csum, from)) {\n\t\t\tskb->csum = csum_block_add(skb->csum, csum, off);\n\t\t\treturn 0;\n\t\t}\n\t} else if (copy_from_iter_full(skb_put(skb, copy), copy, from))\n\t\treturn 0;\n\n\t__skb_trim(skb, off);\n\treturn -EFAULT;\n}\n\nstatic inline bool skb_can_coalesce(struct sk_buff *skb, int i,\n\t\t\t\t    const struct page *page, int off)\n{\n\tif (skb_zcopy(skb))\n\t\treturn false;\n\tif (i) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i - 1];\n\n\t\treturn page == skb_frag_page(frag) &&\n\t\t       off == skb_frag_off(frag) + skb_frag_size(frag);\n\t}\n\treturn false;\n}\n\nstatic inline int __skb_linearize(struct sk_buff *skb)\n{\n\treturn __pskb_pull_tail(skb, skb->data_len) ? 0 : -ENOMEM;\n}\n\n/**\n *\tskb_linearize - convert paged skb to linear one\n *\t@skb: buffer to linarize\n *\n *\tIf there is no free memory -ENOMEM is returned, otherwise zero\n *\tis returned and the old skb data released.\n */\nstatic inline int skb_linearize(struct sk_buff *skb)\n{\n\treturn skb_is_nonlinear(skb) ? __skb_linearize(skb) : 0;\n}\n\n/**\n * skb_has_shared_frag - can any frag be overwritten\n * @skb: buffer to test\n *\n * Return true if the skb has at least one frag that might be modified\n * by an external entity (as in vmsplice()/sendfile())\n */\nstatic inline bool skb_has_shared_frag(const struct sk_buff *skb)\n{\n\treturn skb_is_nonlinear(skb) &&\n\t       skb_shinfo(skb)->tx_flags & SKBTX_SHARED_FRAG;\n}\n\n/**\n *\tskb_linearize_cow - make sure skb is linear and writable\n *\t@skb: buffer to process\n *\n *\tIf there is no free memory -ENOMEM is returned, otherwise zero\n *\tis returned and the old skb data released.\n */\nstatic inline int skb_linearize_cow(struct sk_buff *skb)\n{\n\treturn skb_is_nonlinear(skb) || skb_cloned(skb) ?\n\t       __skb_linearize(skb) : 0;\n}\n\nstatic __always_inline void\n__skb_postpull_rcsum(struct sk_buff *skb, const void *start, unsigned int len,\n\t\t     unsigned int off)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->csum = csum_block_sub(skb->csum,\n\t\t\t\t\t   csum_partial(start, len, 0), off);\n\telse if (skb->ip_summed == CHECKSUM_PARTIAL &&\n\t\t skb_checksum_start_offset(skb) < 0)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n}\n\n/**\n *\tskb_postpull_rcsum - update checksum for received skb after pull\n *\t@skb: buffer to update\n *\t@start: start of data before pull\n *\t@len: length of data pulled\n *\n *\tAfter doing a pull on a received packet, you need to call this to\n *\tupdate the CHECKSUM_COMPLETE checksum, or set ip_summed to\n *\tCHECKSUM_NONE so that it can be recomputed from scratch.\n */\nstatic inline void skb_postpull_rcsum(struct sk_buff *skb,\n\t\t\t\t      const void *start, unsigned int len)\n{\n\t__skb_postpull_rcsum(skb, start, len, 0);\n}\n\nstatic __always_inline void\n__skb_postpush_rcsum(struct sk_buff *skb, const void *start, unsigned int len,\n\t\t     unsigned int off)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->csum = csum_block_add(skb->csum,\n\t\t\t\t\t   csum_partial(start, len, 0), off);\n}\n\n/**\n *\tskb_postpush_rcsum - update checksum for received skb after push\n *\t@skb: buffer to update\n *\t@start: start of data after push\n *\t@len: length of data pushed\n *\n *\tAfter doing a push on a received packet, you need to call this to\n *\tupdate the CHECKSUM_COMPLETE checksum.\n */\nstatic inline void skb_postpush_rcsum(struct sk_buff *skb,\n\t\t\t\t      const void *start, unsigned int len)\n{\n\t__skb_postpush_rcsum(skb, start, len, 0);\n}\n\nvoid *skb_pull_rcsum(struct sk_buff *skb, unsigned int len);\n\n/**\n *\tskb_push_rcsum - push skb and update receive checksum\n *\t@skb: buffer to update\n *\t@len: length of data pulled\n *\n *\tThis function performs an skb_push on the packet and updates\n *\tthe CHECKSUM_COMPLETE checksum.  It should be used on\n *\treceive path processing instead of skb_push unless you know\n *\tthat the checksum difference is zero (e.g., a valid IP header)\n *\tor you are setting ip_summed to CHECKSUM_NONE.\n */\nstatic inline void *skb_push_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tskb_push(skb, len);\n\tskb_postpush_rcsum(skb, skb->data, len);\n\treturn skb->data;\n}\n\nint pskb_trim_rcsum_slow(struct sk_buff *skb, unsigned int len);\n/**\n *\tpskb_trim_rcsum - trim received skb and update checksum\n *\t@skb: buffer to trim\n *\t@len: new length\n *\n *\tThis is exactly the same as pskb_trim except that it ensures the\n *\tchecksum of received packets are still valid after the operation.\n *\tIt can change skb pointers.\n */\n\nstatic inline int pskb_trim_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tif (likely(len >= skb->len))\n\t\treturn 0;\n\treturn pskb_trim_rcsum_slow(skb, len);\n}\n\nstatic inline int __skb_trim_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t__skb_trim(skb, len);\n\treturn 0;\n}\n\nstatic inline int __skb_grow_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\treturn __skb_grow(skb, len);\n}\n\n#define rb_to_skb(rb) rb_entry_safe(rb, struct sk_buff, rbnode)\n#define skb_rb_first(root) rb_to_skb(rb_first(root))\n#define skb_rb_last(root)  rb_to_skb(rb_last(root))\n#define skb_rb_next(skb)   rb_to_skb(rb_next(&(skb)->rbnode))\n#define skb_rb_prev(skb)   rb_to_skb(rb_prev(&(skb)->rbnode))\n\n#define skb_queue_walk(queue, skb) \\\n\t\tfor (skb = (queue)->next;\t\t\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = skb->next)\n\n#define skb_queue_walk_safe(queue, skb, tmp)\t\t\t\t\t\\\n\t\tfor (skb = (queue)->next, tmp = skb->next;\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = tmp, tmp = skb->next)\n\n#define skb_queue_walk_from(queue, skb)\t\t\t\t\t\t\\\n\t\tfor (; skb != (struct sk_buff *)(queue);\t\t\t\\\n\t\t     skb = skb->next)\n\n#define skb_rbtree_walk(skb, root)\t\t\t\t\t\t\\\n\t\tfor (skb = skb_rb_first(root); skb != NULL;\t\t\t\\\n\t\t     skb = skb_rb_next(skb))\n\n#define skb_rbtree_walk_from(skb)\t\t\t\t\t\t\\\n\t\tfor (; skb != NULL;\t\t\t\t\t\t\\\n\t\t     skb = skb_rb_next(skb))\n\n#define skb_rbtree_walk_from_safe(skb, tmp)\t\t\t\t\t\\\n\t\tfor (; tmp = skb ? skb_rb_next(skb) : NULL, (skb != NULL);\t\\\n\t\t     skb = tmp)\n\n#define skb_queue_walk_from_safe(queue, skb, tmp)\t\t\t\t\\\n\t\tfor (tmp = skb->next;\t\t\t\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = tmp, tmp = skb->next)\n\n#define skb_queue_reverse_walk(queue, skb) \\\n\t\tfor (skb = (queue)->prev;\t\t\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = skb->prev)\n\n#define skb_queue_reverse_walk_safe(queue, skb, tmp)\t\t\t\t\\\n\t\tfor (skb = (queue)->prev, tmp = skb->prev;\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = tmp, tmp = skb->prev)\n\n#define skb_queue_reverse_walk_from_safe(queue, skb, tmp)\t\t\t\\\n\t\tfor (tmp = skb->prev;\t\t\t\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = tmp, tmp = skb->prev)\n\nstatic inline bool skb_has_frag_list(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->frag_list != NULL;\n}\n\nstatic inline void skb_frag_list_init(struct sk_buff *skb)\n{\n\tskb_shinfo(skb)->frag_list = NULL;\n}\n\n#define skb_walk_frags(skb, iter)\t\\\n\tfor (iter = skb_shinfo(skb)->frag_list; iter; iter = iter->next)\n\n\nint __skb_wait_for_more_packets(struct sock *sk, struct sk_buff_head *queue,\n\t\t\t\tint *err, long *timeo_p,\n\t\t\t\tconst struct sk_buff *skb);\nstruct sk_buff *__skb_try_recv_from_queue(struct sock *sk,\n\t\t\t\t\t  struct sk_buff_head *queue,\n\t\t\t\t\t  unsigned int flags,\n\t\t\t\t\t  int *off, int *err,\n\t\t\t\t\t  struct sk_buff **last);\nstruct sk_buff *__skb_try_recv_datagram(struct sock *sk,\n\t\t\t\t\tstruct sk_buff_head *queue,\n\t\t\t\t\tunsigned int flags, int *off, int *err,\n\t\t\t\t\tstruct sk_buff **last);\nstruct sk_buff *__skb_recv_datagram(struct sock *sk,\n\t\t\t\t    struct sk_buff_head *sk_queue,\n\t\t\t\t    unsigned int flags, int *off, int *err);\nstruct sk_buff *skb_recv_datagram(struct sock *sk, unsigned flags, int noblock,\n\t\t\t\t  int *err);\n__poll_t datagram_poll(struct file *file, struct socket *sock,\n\t\t\t   struct poll_table_struct *wait);\nint skb_copy_datagram_iter(const struct sk_buff *from, int offset,\n\t\t\t   struct iov_iter *to, int size);\nstatic inline int skb_copy_datagram_msg(const struct sk_buff *from, int offset,\n\t\t\t\t\tstruct msghdr *msg, int size)\n{\n\treturn skb_copy_datagram_iter(from, offset, &msg->msg_iter, size);\n}\nint skb_copy_and_csum_datagram_msg(struct sk_buff *skb, int hlen,\n\t\t\t\t   struct msghdr *msg);\nint skb_copy_and_hash_datagram_iter(const struct sk_buff *skb, int offset,\n\t\t\t   struct iov_iter *to, int len,\n\t\t\t   struct ahash_request *hash);\nint skb_copy_datagram_from_iter(struct sk_buff *skb, int offset,\n\t\t\t\t struct iov_iter *from, int len);\nint zerocopy_sg_from_iter(struct sk_buff *skb, struct iov_iter *frm);\nvoid skb_free_datagram(struct sock *sk, struct sk_buff *skb);\nvoid __skb_free_datagram_locked(struct sock *sk, struct sk_buff *skb, int len);\nstatic inline void skb_free_datagram_locked(struct sock *sk,\n\t\t\t\t\t    struct sk_buff *skb)\n{\n\t__skb_free_datagram_locked(sk, skb, 0);\n}\nint skb_kill_datagram(struct sock *sk, struct sk_buff *skb, unsigned int flags);\nint skb_copy_bits(const struct sk_buff *skb, int offset, void *to, int len);\nint skb_store_bits(struct sk_buff *skb, int offset, const void *from, int len);\n__wsum skb_copy_and_csum_bits(const struct sk_buff *skb, int offset, u8 *to,\n\t\t\t      int len);\nint skb_splice_bits(struct sk_buff *skb, struct sock *sk, unsigned int offset,\n\t\t    struct pipe_inode_info *pipe, unsigned int len,\n\t\t    unsigned int flags);\nint skb_send_sock_locked(struct sock *sk, struct sk_buff *skb, int offset,\n\t\t\t int len);\nvoid skb_copy_and_csum_dev(const struct sk_buff *skb, u8 *to);\nunsigned int skb_zerocopy_headlen(const struct sk_buff *from);\nint skb_zerocopy(struct sk_buff *to, struct sk_buff *from,\n\t\t int len, int hlen);\nvoid skb_split(struct sk_buff *skb, struct sk_buff *skb1, const u32 len);\nint skb_shift(struct sk_buff *tgt, struct sk_buff *skb, int shiftlen);\nvoid skb_scrub_packet(struct sk_buff *skb, bool xnet);\nbool skb_gso_validate_network_len(const struct sk_buff *skb, unsigned int mtu);\nbool skb_gso_validate_mac_len(const struct sk_buff *skb, unsigned int len);\nstruct sk_buff *skb_segment(struct sk_buff *skb, netdev_features_t features);\nstruct sk_buff *skb_segment_list(struct sk_buff *skb, netdev_features_t features,\n\t\t\t\t unsigned int offset);\nstruct sk_buff *skb_vlan_untag(struct sk_buff *skb);\nint skb_ensure_writable(struct sk_buff *skb, int write_len);\nint __skb_vlan_pop(struct sk_buff *skb, u16 *vlan_tci);\nint skb_vlan_pop(struct sk_buff *skb);\nint skb_vlan_push(struct sk_buff *skb, __be16 vlan_proto, u16 vlan_tci);\nint skb_eth_pop(struct sk_buff *skb);\nint skb_eth_push(struct sk_buff *skb, const unsigned char *dst,\n\t\t const unsigned char *src);\nint skb_mpls_push(struct sk_buff *skb, __be32 mpls_lse, __be16 mpls_proto,\n\t\t  int mac_len, bool ethernet);\nint skb_mpls_pop(struct sk_buff *skb, __be16 next_proto, int mac_len,\n\t\t bool ethernet);\nint skb_mpls_update_lse(struct sk_buff *skb, __be32 mpls_lse);\nint skb_mpls_dec_ttl(struct sk_buff *skb);\nstruct sk_buff *pskb_extract(struct sk_buff *skb, int off, int to_copy,\n\t\t\t     gfp_t gfp);\n\nstatic inline int memcpy_from_msg(void *data, struct msghdr *msg, int len)\n{\n\treturn copy_from_iter_full(data, len, &msg->msg_iter) ? 0 : -EFAULT;\n}\n\nstatic inline int memcpy_to_msg(struct msghdr *msg, void *data, int len)\n{\n\treturn copy_to_iter(data, len, &msg->msg_iter) == len ? 0 : -EFAULT;\n}\n\nstruct skb_checksum_ops {\n\t__wsum (*update)(const void *mem, int len, __wsum wsum);\n\t__wsum (*combine)(__wsum csum, __wsum csum2, int offset, int len);\n};\n\nextern const struct skb_checksum_ops *crc32c_csum_stub __read_mostly;\n\n__wsum __skb_checksum(const struct sk_buff *skb, int offset, int len,\n\t\t      __wsum csum, const struct skb_checksum_ops *ops);\n__wsum skb_checksum(const struct sk_buff *skb, int offset, int len,\n\t\t    __wsum csum);\n\nstatic inline void * __must_check\n__skb_header_pointer(const struct sk_buff *skb, int offset,\n\t\t     int len, void *data, int hlen, void *buffer)\n{\n\tif (hlen - offset >= len)\n\t\treturn data + offset;\n\n\tif (!skb ||\n\t    skb_copy_bits(skb, offset, buffer, len) < 0)\n\t\treturn NULL;\n\n\treturn buffer;\n}\n\nstatic inline void * __must_check\nskb_header_pointer(const struct sk_buff *skb, int offset, int len, void *buffer)\n{\n\treturn __skb_header_pointer(skb, offset, len, skb->data,\n\t\t\t\t    skb_headlen(skb), buffer);\n}\n\n/**\n *\tskb_needs_linearize - check if we need to linearize a given skb\n *\t\t\t      depending on the given device features.\n *\t@skb: socket buffer to check\n *\t@features: net device features\n *\n *\tReturns true if either:\n *\t1. skb has frag_list and the device doesn't support FRAGLIST, or\n *\t2. skb is fragmented and the device does not support SG.\n */\nstatic inline bool skb_needs_linearize(struct sk_buff *skb,\n\t\t\t\t       netdev_features_t features)\n{\n\treturn skb_is_nonlinear(skb) &&\n\t       ((skb_has_frag_list(skb) && !(features & NETIF_F_FRAGLIST)) ||\n\t\t(skb_shinfo(skb)->nr_frags && !(features & NETIF_F_SG)));\n}\n\nstatic inline void skb_copy_from_linear_data(const struct sk_buff *skb,\n\t\t\t\t\t     void *to,\n\t\t\t\t\t     const unsigned int len)\n{\n\tmemcpy(to, skb->data, len);\n}\n\nstatic inline void skb_copy_from_linear_data_offset(const struct sk_buff *skb,\n\t\t\t\t\t\t    const int offset, void *to,\n\t\t\t\t\t\t    const unsigned int len)\n{\n\tmemcpy(to, skb->data + offset, len);\n}\n\nstatic inline void skb_copy_to_linear_data(struct sk_buff *skb,\n\t\t\t\t\t   const void *from,\n\t\t\t\t\t   const unsigned int len)\n{\n\tmemcpy(skb->data, from, len);\n}\n\nstatic inline void skb_copy_to_linear_data_offset(struct sk_buff *skb,\n\t\t\t\t\t\t  const int offset,\n\t\t\t\t\t\t  const void *from,\n\t\t\t\t\t\t  const unsigned int len)\n{\n\tmemcpy(skb->data + offset, from, len);\n}\n\nvoid skb_init(void);\n\nstatic inline ktime_t skb_get_ktime(const struct sk_buff *skb)\n{\n\treturn skb->tstamp;\n}\n\n/**\n *\tskb_get_timestamp - get timestamp from a skb\n *\t@skb: skb to get stamp from\n *\t@stamp: pointer to struct __kernel_old_timeval to store stamp in\n *\n *\tTimestamps are stored in the skb as offsets to a base timestamp.\n *\tThis function converts the offset back to a struct timeval and stores\n *\tit in stamp.\n */\nstatic inline void skb_get_timestamp(const struct sk_buff *skb,\n\t\t\t\t     struct __kernel_old_timeval *stamp)\n{\n\t*stamp = ns_to_kernel_old_timeval(skb->tstamp);\n}\n\nstatic inline void skb_get_new_timestamp(const struct sk_buff *skb,\n\t\t\t\t\t struct __kernel_sock_timeval *stamp)\n{\n\tstruct timespec64 ts = ktime_to_timespec64(skb->tstamp);\n\n\tstamp->tv_sec = ts.tv_sec;\n\tstamp->tv_usec = ts.tv_nsec / 1000;\n}\n\nstatic inline void skb_get_timestampns(const struct sk_buff *skb,\n\t\t\t\t       struct __kernel_old_timespec *stamp)\n{\n\tstruct timespec64 ts = ktime_to_timespec64(skb->tstamp);\n\n\tstamp->tv_sec = ts.tv_sec;\n\tstamp->tv_nsec = ts.tv_nsec;\n}\n\nstatic inline void skb_get_new_timestampns(const struct sk_buff *skb,\n\t\t\t\t\t   struct __kernel_timespec *stamp)\n{\n\tstruct timespec64 ts = ktime_to_timespec64(skb->tstamp);\n\n\tstamp->tv_sec = ts.tv_sec;\n\tstamp->tv_nsec = ts.tv_nsec;\n}\n\nstatic inline void __net_timestamp(struct sk_buff *skb)\n{\n\tskb->tstamp = ktime_get_real();\n}\n\nstatic inline ktime_t net_timedelta(ktime_t t)\n{\n\treturn ktime_sub(ktime_get_real(), t);\n}\n\nstatic inline ktime_t net_invalid_timestamp(void)\n{\n\treturn 0;\n}\n\nstatic inline u8 skb_metadata_len(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->meta_len;\n}\n\nstatic inline void *skb_metadata_end(const struct sk_buff *skb)\n{\n\treturn skb_mac_header(skb);\n}\n\nstatic inline bool __skb_metadata_differs(const struct sk_buff *skb_a,\n\t\t\t\t\t  const struct sk_buff *skb_b,\n\t\t\t\t\t  u8 meta_len)\n{\n\tconst void *a = skb_metadata_end(skb_a);\n\tconst void *b = skb_metadata_end(skb_b);\n\t/* Using more efficient varaiant than plain call to memcmp(). */\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n\tu64 diffs = 0;\n\n\tswitch (meta_len) {\n#define __it(x, op) (x -= sizeof(u##op))\n#define __it_diff(a, b, op) (*(u##op *)__it(a, op)) ^ (*(u##op *)__it(b, op))\n\tcase 32: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase 24: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase 16: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase  8: diffs |= __it_diff(a, b, 64);\n\t\tbreak;\n\tcase 28: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase 20: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase 12: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase  4: diffs |= __it_diff(a, b, 32);\n\t\tbreak;\n\t}\n\treturn diffs;\n#else\n\treturn memcmp(a - meta_len, b - meta_len, meta_len);\n#endif\n}\n\nstatic inline bool skb_metadata_differs(const struct sk_buff *skb_a,\n\t\t\t\t\tconst struct sk_buff *skb_b)\n{\n\tu8 len_a = skb_metadata_len(skb_a);\n\tu8 len_b = skb_metadata_len(skb_b);\n\n\tif (!(len_a | len_b))\n\t\treturn false;\n\n\treturn len_a != len_b ?\n\t       true : __skb_metadata_differs(skb_a, skb_b, len_a);\n}\n\nstatic inline void skb_metadata_set(struct sk_buff *skb, u8 meta_len)\n{\n\tskb_shinfo(skb)->meta_len = meta_len;\n}\n\nstatic inline void skb_metadata_clear(struct sk_buff *skb)\n{\n\tskb_metadata_set(skb, 0);\n}\n\nstruct sk_buff *skb_clone_sk(struct sk_buff *skb);\n\n#ifdef CONFIG_NETWORK_PHY_TIMESTAMPING\n\nvoid skb_clone_tx_timestamp(struct sk_buff *skb);\nbool skb_defer_rx_timestamp(struct sk_buff *skb);\n\n#else /* CONFIG_NETWORK_PHY_TIMESTAMPING */\n\nstatic inline void skb_clone_tx_timestamp(struct sk_buff *skb)\n{\n}\n\nstatic inline bool skb_defer_rx_timestamp(struct sk_buff *skb)\n{\n\treturn false;\n}\n\n#endif /* !CONFIG_NETWORK_PHY_TIMESTAMPING */\n\n/**\n * skb_complete_tx_timestamp() - deliver cloned skb with tx timestamps\n *\n * PHY drivers may accept clones of transmitted packets for\n * timestamping via their phy_driver.txtstamp method. These drivers\n * must call this function to return the skb back to the stack with a\n * timestamp.\n *\n * @skb: clone of the original outgoing packet\n * @hwtstamps: hardware time stamps\n *\n */\nvoid skb_complete_tx_timestamp(struct sk_buff *skb,\n\t\t\t       struct skb_shared_hwtstamps *hwtstamps);\n\nvoid __skb_tstamp_tx(struct sk_buff *orig_skb,\n\t\t     struct skb_shared_hwtstamps *hwtstamps,\n\t\t     struct sock *sk, int tstype);\n\n/**\n * skb_tstamp_tx - queue clone of skb with send time stamps\n * @orig_skb:\tthe original outgoing packet\n * @hwtstamps:\thardware time stamps, may be NULL if not available\n *\n * If the skb has a socket associated, then this function clones the\n * skb (thus sharing the actual data and optional structures), stores\n * the optional hardware time stamping information (if non NULL) or\n * generates a software time stamp (otherwise), then queues the clone\n * to the error queue of the socket.  Errors are silently ignored.\n */\nvoid skb_tstamp_tx(struct sk_buff *orig_skb,\n\t\t   struct skb_shared_hwtstamps *hwtstamps);\n\n/**\n * skb_tx_timestamp() - Driver hook for transmit timestamping\n *\n * Ethernet MAC Drivers should call this function in their hard_xmit()\n * function immediately before giving the sk_buff to the MAC hardware.\n *\n * Specifically, one should make absolutely sure that this function is\n * called before TX completion of this packet can trigger.  Otherwise\n * the packet could potentially already be freed.\n *\n * @skb: A socket buffer.\n */\nstatic inline void skb_tx_timestamp(struct sk_buff *skb)\n{\n\tskb_clone_tx_timestamp(skb);\n\tif (skb_shinfo(skb)->tx_flags & SKBTX_SW_TSTAMP)\n\t\tskb_tstamp_tx(skb, NULL);\n}\n\n/**\n * skb_complete_wifi_ack - deliver skb with wifi status\n *\n * @skb: the original outgoing packet\n * @acked: ack status\n *\n */\nvoid skb_complete_wifi_ack(struct sk_buff *skb, bool acked);\n\n__sum16 __skb_checksum_complete_head(struct sk_buff *skb, int len);\n__sum16 __skb_checksum_complete(struct sk_buff *skb);\n\nstatic inline int skb_csum_unnecessary(const struct sk_buff *skb)\n{\n\treturn ((skb->ip_summed == CHECKSUM_UNNECESSARY) ||\n\t\tskb->csum_valid ||\n\t\t(skb->ip_summed == CHECKSUM_PARTIAL &&\n\t\t skb_checksum_start_offset(skb) >= 0));\n}\n\n/**\n *\tskb_checksum_complete - Calculate checksum of an entire packet\n *\t@skb: packet to process\n *\n *\tThis function calculates the checksum over the entire packet plus\n *\tthe value of skb->csum.  The latter can be used to supply the\n *\tchecksum of a pseudo header as used by TCP/UDP.  It returns the\n *\tchecksum.\n *\n *\tFor protocols that contain complete checksums such as ICMP/TCP/UDP,\n *\tthis function can be used to verify that checksum on received\n *\tpackets.  In that case the function should return zero if the\n *\tchecksum is correct.  In particular, this function will return zero\n *\tif skb->ip_summed is CHECKSUM_UNNECESSARY which indicates that the\n *\thardware has already verified the correctness of the checksum.\n */\nstatic inline __sum16 skb_checksum_complete(struct sk_buff *skb)\n{\n\treturn skb_csum_unnecessary(skb) ?\n\t       0 : __skb_checksum_complete(skb);\n}\n\nstatic inline void __skb_decr_checksum_unnecessary(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\tif (skb->csum_level == 0)\n\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\telse\n\t\t\tskb->csum_level--;\n\t}\n}\n\nstatic inline void __skb_incr_checksum_unnecessary(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\tif (skb->csum_level < SKB_MAX_CSUM_LEVEL)\n\t\t\tskb->csum_level++;\n\t} else if (skb->ip_summed == CHECKSUM_NONE) {\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\tskb->csum_level = 0;\n\t}\n}\n\nstatic inline void __skb_reset_checksum_unnecessary(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tskb->csum_level = 0;\n\t}\n}\n\n/* Check if we need to perform checksum complete validation.\n *\n * Returns true if checksum complete is needed, false otherwise\n * (either checksum is unnecessary or zero checksum is allowed).\n */\nstatic inline bool __skb_checksum_validate_needed(struct sk_buff *skb,\n\t\t\t\t\t\t  bool zero_okay,\n\t\t\t\t\t\t  __sum16 check)\n{\n\tif (skb_csum_unnecessary(skb) || (zero_okay && !check)) {\n\t\tskb->csum_valid = 1;\n\t\t__skb_decr_checksum_unnecessary(skb);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n/* For small packets <= CHECKSUM_BREAK perform checksum complete directly\n * in checksum_init.\n */\n#define CHECKSUM_BREAK 76\n\n/* Unset checksum-complete\n *\n * Unset checksum complete can be done when packet is being modified\n * (uncompressed for instance) and checksum-complete value is\n * invalidated.\n */\nstatic inline void skb_checksum_complete_unset(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n}\n\n/* Validate (init) checksum based on checksum complete.\n *\n * Return values:\n *   0: checksum is validated or try to in skb_checksum_complete. In the latter\n *\tcase the ip_summed will not be CHECKSUM_UNNECESSARY and the pseudo\n *\tchecksum is stored in skb->csum for use in __skb_checksum_complete\n *   non-zero: value of invalid checksum\n *\n */\nstatic inline __sum16 __skb_checksum_validate_complete(struct sk_buff *skb,\n\t\t\t\t\t\t       bool complete,\n\t\t\t\t\t\t       __wsum psum)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\tif (!csum_fold(csum_add(psum, skb->csum))) {\n\t\t\tskb->csum_valid = 1;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tskb->csum = psum;\n\n\tif (complete || skb->len <= CHECKSUM_BREAK) {\n\t\t__sum16 csum;\n\n\t\tcsum = __skb_checksum_complete(skb);\n\t\tskb->csum_valid = !csum;\n\t\treturn csum;\n\t}\n\n\treturn 0;\n}\n\nstatic inline __wsum null_compute_pseudo(struct sk_buff *skb, int proto)\n{\n\treturn 0;\n}\n\n/* Perform checksum validate (init). Note that this is a macro since we only\n * want to calculate the pseudo header which is an input function if necessary.\n * First we try to validate without any computation (checksum unnecessary) and\n * then calculate based on checksum complete calling the function to compute\n * pseudo header.\n *\n * Return values:\n *   0: checksum is validated or try to in skb_checksum_complete\n *   non-zero: value of invalid checksum\n */\n#define __skb_checksum_validate(skb, proto, complete,\t\t\t\\\n\t\t\t\tzero_okay, check, compute_pseudo)\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__sum16 __ret = 0;\t\t\t\t\t\t\\\n\tskb->csum_valid = 0;\t\t\t\t\t\t\\\n\tif (__skb_checksum_validate_needed(skb, zero_okay, check))\t\\\n\t\t__ret = __skb_checksum_validate_complete(skb,\t\t\\\n\t\t\t\tcomplete, compute_pseudo(skb, proto));\t\\\n\t__ret;\t\t\t\t\t\t\t\t\\\n})\n\n#define skb_checksum_init(skb, proto, compute_pseudo)\t\t\t\\\n\t__skb_checksum_validate(skb, proto, false, false, 0, compute_pseudo)\n\n#define skb_checksum_init_zero_check(skb, proto, check, compute_pseudo)\t\\\n\t__skb_checksum_validate(skb, proto, false, true, check, compute_pseudo)\n\n#define skb_checksum_validate(skb, proto, compute_pseudo)\t\t\\\n\t__skb_checksum_validate(skb, proto, true, false, 0, compute_pseudo)\n\n#define skb_checksum_validate_zero_check(skb, proto, check,\t\t\\\n\t\t\t\t\t compute_pseudo)\t\t\\\n\t__skb_checksum_validate(skb, proto, true, true, check, compute_pseudo)\n\n#define skb_checksum_simple_validate(skb)\t\t\t\t\\\n\t__skb_checksum_validate(skb, 0, true, false, 0, null_compute_pseudo)\n\nstatic inline bool __skb_checksum_convert_check(struct sk_buff *skb)\n{\n\treturn (skb->ip_summed == CHECKSUM_NONE && skb->csum_valid);\n}\n\nstatic inline void __skb_checksum_convert(struct sk_buff *skb, __wsum pseudo)\n{\n\tskb->csum = ~pseudo;\n\tskb->ip_summed = CHECKSUM_COMPLETE;\n}\n\n#define skb_checksum_try_convert(skb, proto, compute_pseudo)\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (__skb_checksum_convert_check(skb))\t\t\t\t\\\n\t\t__skb_checksum_convert(skb, compute_pseudo(skb, proto)); \\\n} while (0)\n\nstatic inline void skb_remcsum_adjust_partial(struct sk_buff *skb, void *ptr,\n\t\t\t\t\t      u16 start, u16 offset)\n{\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\tskb->csum_start = ((unsigned char *)ptr + start) - skb->head;\n\tskb->csum_offset = offset - start;\n}\n\n/* Update skbuf and packet to reflect the remote checksum offload operation.\n * When called, ptr indicates the starting point for skb->csum when\n * ip_summed is CHECKSUM_COMPLETE. If we need create checksum complete\n * here, skb_postpull_rcsum is done so skb->csum start is ptr.\n */\nstatic inline void skb_remcsum_process(struct sk_buff *skb, void *ptr,\n\t\t\t\t       int start, int offset, bool nopartial)\n{\n\t__wsum delta;\n\n\tif (!nopartial) {\n\t\tskb_remcsum_adjust_partial(skb, ptr, start, offset);\n\t\treturn;\n\t}\n\n\t if (unlikely(skb->ip_summed != CHECKSUM_COMPLETE)) {\n\t\t__skb_checksum_complete(skb);\n\t\tskb_postpull_rcsum(skb, skb->data, ptr - (void *)skb->data);\n\t}\n\n\tdelta = remcsum_adjust(ptr, skb->csum, start, offset);\n\n\t/* Adjust skb->csum since we changed the packet */\n\tskb->csum = csum_add(skb->csum, delta);\n}\n\nstatic inline struct nf_conntrack *skb_nfct(const struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\treturn (void *)(skb->_nfct & NFCT_PTRMASK);\n#else\n\treturn NULL;\n#endif\n}\n\nstatic inline unsigned long skb_get_nfct(const struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\treturn skb->_nfct;\n#else\n\treturn 0UL;\n#endif\n}\n\nstatic inline void skb_set_nfct(struct sk_buff *skb, unsigned long nfct)\n{\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\tskb->_nfct = nfct;\n#endif\n}\n\n#ifdef CONFIG_SKB_EXTENSIONS\nenum skb_ext_id {\n#if IS_ENABLED(CONFIG_BRIDGE_NETFILTER)\n\tSKB_EXT_BRIDGE_NF,\n#endif\n#ifdef CONFIG_XFRM\n\tSKB_EXT_SEC_PATH,\n#endif\n#if IS_ENABLED(CONFIG_NET_TC_SKB_EXT)\n\tTC_SKB_EXT,\n#endif\n#if IS_ENABLED(CONFIG_MPTCP)\n\tSKB_EXT_MPTCP,\n#endif\n\tSKB_EXT_NUM, /* must be last */\n};\n\n/**\n *\tstruct skb_ext - sk_buff extensions\n *\t@refcnt: 1 on allocation, deallocated on 0\n *\t@offset: offset to add to @data to obtain extension address\n *\t@chunks: size currently allocated, stored in SKB_EXT_ALIGN_SHIFT units\n *\t@data: start of extension data, variable sized\n *\n *\tNote: offsets/lengths are stored in chunks of 8 bytes, this allows\n *\tto use 'u8' types while allowing up to 2kb worth of extension data.\n */\nstruct skb_ext {\n\trefcount_t refcnt;\n\tu8 offset[SKB_EXT_NUM]; /* in chunks of 8 bytes */\n\tu8 chunks;\t\t/* same */\n\tchar data[] __aligned(8);\n};\n\nstruct skb_ext *__skb_ext_alloc(gfp_t flags);\nvoid *__skb_ext_set(struct sk_buff *skb, enum skb_ext_id id,\n\t\t    struct skb_ext *ext);\nvoid *skb_ext_add(struct sk_buff *skb, enum skb_ext_id id);\nvoid __skb_ext_del(struct sk_buff *skb, enum skb_ext_id id);\nvoid __skb_ext_put(struct skb_ext *ext);\n\nstatic inline void skb_ext_put(struct sk_buff *skb)\n{\n\tif (skb->active_extensions)\n\t\t__skb_ext_put(skb->extensions);\n}\n\nstatic inline void __skb_ext_copy(struct sk_buff *dst,\n\t\t\t\t  const struct sk_buff *src)\n{\n\tdst->active_extensions = src->active_extensions;\n\n\tif (src->active_extensions) {\n\t\tstruct skb_ext *ext = src->extensions;\n\n\t\trefcount_inc(&ext->refcnt);\n\t\tdst->extensions = ext;\n\t}\n}\n\nstatic inline void skb_ext_copy(struct sk_buff *dst, const struct sk_buff *src)\n{\n\tskb_ext_put(dst);\n\t__skb_ext_copy(dst, src);\n}\n\nstatic inline bool __skb_ext_exist(const struct skb_ext *ext, enum skb_ext_id i)\n{\n\treturn !!ext->offset[i];\n}\n\nstatic inline bool skb_ext_exist(const struct sk_buff *skb, enum skb_ext_id id)\n{\n\treturn skb->active_extensions & (1 << id);\n}\n\nstatic inline void skb_ext_del(struct sk_buff *skb, enum skb_ext_id id)\n{\n\tif (skb_ext_exist(skb, id))\n\t\t__skb_ext_del(skb, id);\n}\n\nstatic inline void *skb_ext_find(const struct sk_buff *skb, enum skb_ext_id id)\n{\n\tif (skb_ext_exist(skb, id)) {\n\t\tstruct skb_ext *ext = skb->extensions;\n\n\t\treturn (void *)ext + (ext->offset[id] << 3);\n\t}\n\n\treturn NULL;\n}\n\nstatic inline void skb_ext_reset(struct sk_buff *skb)\n{\n\tif (unlikely(skb->active_extensions)) {\n\t\t__skb_ext_put(skb->extensions);\n\t\tskb->active_extensions = 0;\n\t}\n}\n\nstatic inline bool skb_has_extensions(struct sk_buff *skb)\n{\n\treturn unlikely(skb->active_extensions);\n}\n#else\nstatic inline void skb_ext_put(struct sk_buff *skb) {}\nstatic inline void skb_ext_reset(struct sk_buff *skb) {}\nstatic inline void skb_ext_del(struct sk_buff *skb, int unused) {}\nstatic inline void __skb_ext_copy(struct sk_buff *d, const struct sk_buff *s) {}\nstatic inline void skb_ext_copy(struct sk_buff *dst, const struct sk_buff *s) {}\nstatic inline bool skb_has_extensions(struct sk_buff *skb) { return false; }\n#endif /* CONFIG_SKB_EXTENSIONS */\n\nstatic inline void nf_reset_ct(struct sk_buff *skb)\n{\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tnf_conntrack_put(skb_nfct(skb));\n\tskb->_nfct = 0;\n#endif\n}\n\nstatic inline void nf_reset_trace(struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE) || defined(CONFIG_NF_TABLES)\n\tskb->nf_trace = 0;\n#endif\n}\n\nstatic inline void ipvs_reset(struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_IP_VS)\n\tskb->ipvs_property = 0;\n#endif\n}\n\n/* Note: This doesn't put any conntrack info in dst. */\nstatic inline void __nf_copy(struct sk_buff *dst, const struct sk_buff *src,\n\t\t\t     bool copy)\n{\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tdst->_nfct = src->_nfct;\n\tnf_conntrack_get(skb_nfct(src));\n#endif\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE) || defined(CONFIG_NF_TABLES)\n\tif (copy)\n\t\tdst->nf_trace = src->nf_trace;\n#endif\n}\n\nstatic inline void nf_copy(struct sk_buff *dst, const struct sk_buff *src)\n{\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tnf_conntrack_put(skb_nfct(dst));\n#endif\n\t__nf_copy(dst, src, true);\n}\n\n#ifdef CONFIG_NETWORK_SECMARK\nstatic inline void skb_copy_secmark(struct sk_buff *to, const struct sk_buff *from)\n{\n\tto->secmark = from->secmark;\n}\n\nstatic inline void skb_init_secmark(struct sk_buff *skb)\n{\n\tskb->secmark = 0;\n}\n#else\nstatic inline void skb_copy_secmark(struct sk_buff *to, const struct sk_buff *from)\n{ }\n\nstatic inline void skb_init_secmark(struct sk_buff *skb)\n{ }\n#endif\n\nstatic inline int secpath_exists(const struct sk_buff *skb)\n{\n#ifdef CONFIG_XFRM\n\treturn skb_ext_exist(skb, SKB_EXT_SEC_PATH);\n#else\n\treturn 0;\n#endif\n}\n\nstatic inline bool skb_irq_freeable(const struct sk_buff *skb)\n{\n\treturn !skb->destructor &&\n\t\t!secpath_exists(skb) &&\n\t\t!skb_nfct(skb) &&\n\t\t!skb->_skb_refdst &&\n\t\t!skb_has_frag_list(skb);\n}\n\nstatic inline void skb_set_queue_mapping(struct sk_buff *skb, u16 queue_mapping)\n{\n\tskb->queue_mapping = queue_mapping;\n}\n\nstatic inline u16 skb_get_queue_mapping(const struct sk_buff *skb)\n{\n\treturn skb->queue_mapping;\n}\n\nstatic inline void skb_copy_queue_mapping(struct sk_buff *to, const struct sk_buff *from)\n{\n\tto->queue_mapping = from->queue_mapping;\n}\n\nstatic inline void skb_record_rx_queue(struct sk_buff *skb, u16 rx_queue)\n{\n\tskb->queue_mapping = rx_queue + 1;\n}\n\nstatic inline u16 skb_get_rx_queue(const struct sk_buff *skb)\n{\n\treturn skb->queue_mapping - 1;\n}\n\nstatic inline bool skb_rx_queue_recorded(const struct sk_buff *skb)\n{\n\treturn skb->queue_mapping != 0;\n}\n\nstatic inline void skb_set_dst_pending_confirm(struct sk_buff *skb, u32 val)\n{\n\tskb->dst_pending_confirm = val;\n}\n\nstatic inline bool skb_get_dst_pending_confirm(const struct sk_buff *skb)\n{\n\treturn skb->dst_pending_confirm != 0;\n}\n\nstatic inline struct sec_path *skb_sec_path(const struct sk_buff *skb)\n{\n#ifdef CONFIG_XFRM\n\treturn skb_ext_find(skb, SKB_EXT_SEC_PATH);\n#else\n\treturn NULL;\n#endif\n}\n\n/* Keeps track of mac header offset relative to skb->head.\n * It is useful for TSO of Tunneling protocol. e.g. GRE.\n * For non-tunnel skb it points to skb_mac_header() and for\n * tunnel skb it points to outer mac header.\n * Keeps track of level of encapsulation of network headers.\n */\nstruct skb_gso_cb {\n\tunion {\n\t\tint\tmac_offset;\n\t\tint\tdata_offset;\n\t};\n\tint\tencap_level;\n\t__wsum\tcsum;\n\t__u16\tcsum_start;\n};\n#define SKB_GSO_CB_OFFSET\t32\n#define SKB_GSO_CB(skb) ((struct skb_gso_cb *)((skb)->cb + SKB_GSO_CB_OFFSET))\n\nstatic inline int skb_tnl_header_len(const struct sk_buff *inner_skb)\n{\n\treturn (skb_mac_header(inner_skb) - inner_skb->head) -\n\t\tSKB_GSO_CB(inner_skb)->mac_offset;\n}\n\nstatic inline int gso_pskb_expand_head(struct sk_buff *skb, int extra)\n{\n\tint new_headroom, headroom;\n\tint ret;\n\n\theadroom = skb_headroom(skb);\n\tret = pskb_expand_head(skb, extra, 0, GFP_ATOMIC);\n\tif (ret)\n\t\treturn ret;\n\n\tnew_headroom = skb_headroom(skb);\n\tSKB_GSO_CB(skb)->mac_offset += (new_headroom - headroom);\n\treturn 0;\n}\n\nstatic inline void gso_reset_checksum(struct sk_buff *skb, __wsum res)\n{\n\t/* Do not update partial checksums if remote checksum is enabled. */\n\tif (skb->remcsum_offload)\n\t\treturn;\n\n\tSKB_GSO_CB(skb)->csum = res;\n\tSKB_GSO_CB(skb)->csum_start = skb_checksum_start(skb) - skb->head;\n}\n\n/* Compute the checksum for a gso segment. First compute the checksum value\n * from the start of transport header to SKB_GSO_CB(skb)->csum_start, and\n * then add in skb->csum (checksum from csum_start to end of packet).\n * skb->csum and csum_start are then updated to reflect the checksum of the\n * resultant packet starting from the transport header-- the resultant checksum\n * is in the res argument (i.e. normally zero or ~ of checksum of a pseudo\n * header.\n */\nstatic inline __sum16 gso_make_checksum(struct sk_buff *skb, __wsum res)\n{\n\tunsigned char *csum_start = skb_transport_header(skb);\n\tint plen = (skb->head + SKB_GSO_CB(skb)->csum_start) - csum_start;\n\t__wsum partial = SKB_GSO_CB(skb)->csum;\n\n\tSKB_GSO_CB(skb)->csum = res;\n\tSKB_GSO_CB(skb)->csum_start = csum_start - skb->head;\n\n\treturn csum_fold(csum_partial(csum_start, plen, partial));\n}\n\nstatic inline bool skb_is_gso(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->gso_size;\n}\n\n/* Note: Should be called only if skb_is_gso(skb) is true */\nstatic inline bool skb_is_gso_v6(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->gso_type & SKB_GSO_TCPV6;\n}\n\n/* Note: Should be called only if skb_is_gso(skb) is true */\nstatic inline bool skb_is_gso_sctp(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->gso_type & SKB_GSO_SCTP;\n}\n\n/* Note: Should be called only if skb_is_gso(skb) is true */\nstatic inline bool skb_is_gso_tcp(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6);\n}\n\nstatic inline void skb_gso_reset(struct sk_buff *skb)\n{\n\tskb_shinfo(skb)->gso_size = 0;\n\tskb_shinfo(skb)->gso_segs = 0;\n\tskb_shinfo(skb)->gso_type = 0;\n}\n\nstatic inline void skb_increase_gso_size(struct skb_shared_info *shinfo,\n\t\t\t\t\t u16 increment)\n{\n\tif (WARN_ON_ONCE(shinfo->gso_size == GSO_BY_FRAGS))\n\t\treturn;\n\tshinfo->gso_size += increment;\n}\n\nstatic inline void skb_decrease_gso_size(struct skb_shared_info *shinfo,\n\t\t\t\t\t u16 decrement)\n{\n\tif (WARN_ON_ONCE(shinfo->gso_size == GSO_BY_FRAGS))\n\t\treturn;\n\tshinfo->gso_size -= decrement;\n}\n\nvoid __skb_warn_lro_forwarding(const struct sk_buff *skb);\n\nstatic inline bool skb_warn_if_lro(const struct sk_buff *skb)\n{\n\t/* LRO sets gso_size but not gso_type, whereas if GSO is really\n\t * wanted then gso_type will be set. */\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\tif (skb_is_nonlinear(skb) && shinfo->gso_size != 0 &&\n\t    unlikely(shinfo->gso_type == 0)) {\n\t\t__skb_warn_lro_forwarding(skb);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline void skb_forward_csum(struct sk_buff *skb)\n{\n\t/* Unfortunately we don't support this one.  Any brave souls? */\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n}\n\n/**\n * skb_checksum_none_assert - make sure skb ip_summed is CHECKSUM_NONE\n * @skb: skb to check\n *\n * fresh skbs have their ip_summed set to CHECKSUM_NONE.\n * Instead of forcing ip_summed to CHECKSUM_NONE, we can\n * use this helper, to document places where we make this assertion.\n */\nstatic inline void skb_checksum_none_assert(const struct sk_buff *skb)\n{\n#ifdef DEBUG\n\tBUG_ON(skb->ip_summed != CHECKSUM_NONE);\n#endif\n}\n\nbool skb_partial_csum_set(struct sk_buff *skb, u16 start, u16 off);\n\nint skb_checksum_setup(struct sk_buff *skb, bool recalculate);\nstruct sk_buff *skb_checksum_trimmed(struct sk_buff *skb,\n\t\t\t\t     unsigned int transport_len,\n\t\t\t\t     __sum16(*skb_chkf)(struct sk_buff *skb));\n\n/**\n * skb_head_is_locked - Determine if the skb->head is locked down\n * @skb: skb to check\n *\n * The head on skbs build around a head frag can be removed if they are\n * not cloned.  This function returns true if the skb head is locked down\n * due to either being allocated via kmalloc, or by being a clone with\n * multiple references to the head.\n */\nstatic inline bool skb_head_is_locked(const struct sk_buff *skb)\n{\n\treturn !skb->head_frag || skb_cloned(skb);\n}\n\n/* Local Checksum Offload.\n * Compute outer checksum based on the assumption that the\n * inner checksum will be offloaded later.\n * See Documentation/networking/checksum-offloads.rst for\n * explanation of how this works.\n * Fill in outer checksum adjustment (e.g. with sum of outer\n * pseudo-header) before calling.\n * Also ensure that inner checksum is in linear data area.\n */\nstatic inline __wsum lco_csum(struct sk_buff *skb)\n{\n\tunsigned char *csum_start = skb_checksum_start(skb);\n\tunsigned char *l4_hdr = skb_transport_header(skb);\n\t__wsum partial;\n\n\t/* Start with complement of inner checksum adjustment */\n\tpartial = ~csum_unfold(*(__force __sum16 *)(csum_start +\n\t\t\t\t\t\t    skb->csum_offset));\n\n\t/* Add in checksum of our headers (incl. outer checksum\n\t * adjustment filled in by caller) and return result.\n\t */\n\treturn csum_partial(l4_hdr, csum_start - l4_hdr, partial);\n}\n\nstatic inline bool skb_is_redirected(const struct sk_buff *skb)\n{\n#ifdef CONFIG_NET_REDIRECT\n\treturn skb->redirected;\n#else\n\treturn false;\n#endif\n}\n\nstatic inline void skb_set_redirected(struct sk_buff *skb, bool from_ingress)\n{\n#ifdef CONFIG_NET_REDIRECT\n\tskb->redirected = 1;\n\tskb->from_ingress = from_ingress;\n\tif (skb->from_ingress)\n\t\tskb->tstamp = 0;\n#endif\n}\n\nstatic inline void skb_reset_redirect(struct sk_buff *skb)\n{\n#ifdef CONFIG_NET_REDIRECT\n\tskb->redirected = 0;\n#endif\n}\n\n#endif\t/* __KERNEL__ */\n#endif\t/* _LINUX_SKBUFF_H */\n"}, "2": {"id": 2, "path": "/src/include/linux/rbtree.h", "content": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/*\n  Red Black Trees\n  (C) 1999  Andrea Arcangeli <andrea@suse.de>\n  \n\n  linux/include/linux/rbtree.h\n\n  To use rbtrees you'll have to implement your own insert and search cores.\n  This will avoid us to use callbacks and to drop drammatically performances.\n  I know it's not the cleaner way,  but in C (not in C++) to get\n  performances and genericity...\n\n  See Documentation/core-api/rbtree.rst for documentation and samples.\n*/\n\n#ifndef\t_LINUX_RBTREE_H\n#define\t_LINUX_RBTREE_H\n\n#include <linux/kernel.h>\n#include <linux/stddef.h>\n#include <linux/rcupdate.h>\n\nstruct rb_node {\n\tunsigned long  __rb_parent_color;\n\tstruct rb_node *rb_right;\n\tstruct rb_node *rb_left;\n} __attribute__((aligned(sizeof(long))));\n    /* The alignment might seem pointless, but allegedly CRIS needs it */\n\nstruct rb_root {\n\tstruct rb_node *rb_node;\n};\n\n#define rb_parent(r)   ((struct rb_node *)((r)->__rb_parent_color & ~3))\n\n#define RB_ROOT\t(struct rb_root) { NULL, }\n#define\trb_entry(ptr, type, member) container_of(ptr, type, member)\n\n#define RB_EMPTY_ROOT(root)  (READ_ONCE((root)->rb_node) == NULL)\n\n/* 'empty' nodes are nodes that are known not to be inserted in an rbtree */\n#define RB_EMPTY_NODE(node)  \\\n\t((node)->__rb_parent_color == (unsigned long)(node))\n#define RB_CLEAR_NODE(node)  \\\n\t((node)->__rb_parent_color = (unsigned long)(node))\n\n\nextern void rb_insert_color(struct rb_node *, struct rb_root *);\nextern void rb_erase(struct rb_node *, struct rb_root *);\n\n\n/* Find logical next and previous nodes in a tree */\nextern struct rb_node *rb_next(const struct rb_node *);\nextern struct rb_node *rb_prev(const struct rb_node *);\nextern struct rb_node *rb_first(const struct rb_root *);\nextern struct rb_node *rb_last(const struct rb_root *);\n\n/* Postorder iteration - always visit the parent after its children */\nextern struct rb_node *rb_first_postorder(const struct rb_root *);\nextern struct rb_node *rb_next_postorder(const struct rb_node *);\n\n/* Fast replacement of a single node without remove/rebalance/add/rebalance */\nextern void rb_replace_node(struct rb_node *victim, struct rb_node *new,\n\t\t\t    struct rb_root *root);\nextern void rb_replace_node_rcu(struct rb_node *victim, struct rb_node *new,\n\t\t\t\tstruct rb_root *root);\n\nstatic inline void rb_link_node(struct rb_node *node, struct rb_node *parent,\n\t\t\t\tstruct rb_node **rb_link)\n{\n\tnode->__rb_parent_color = (unsigned long)parent;\n\tnode->rb_left = node->rb_right = NULL;\n\n\t*rb_link = node;\n}\n\nstatic inline void rb_link_node_rcu(struct rb_node *node, struct rb_node *parent,\n\t\t\t\t    struct rb_node **rb_link)\n{\n\tnode->__rb_parent_color = (unsigned long)parent;\n\tnode->rb_left = node->rb_right = NULL;\n\n\trcu_assign_pointer(*rb_link, node);\n}\n\n#define rb_entry_safe(ptr, type, member) \\\n\t({ typeof(ptr) ____ptr = (ptr); \\\n\t   ____ptr ? rb_entry(____ptr, type, member) : NULL; \\\n\t})\n\n/**\n * rbtree_postorder_for_each_entry_safe - iterate in post-order over rb_root of\n * given type allowing the backing memory of @pos to be invalidated\n *\n * @pos:\tthe 'type *' to use as a loop cursor.\n * @n:\t\tanother 'type *' to use as temporary storage\n * @root:\t'rb_root *' of the rbtree.\n * @field:\tthe name of the rb_node field within 'type'.\n *\n * rbtree_postorder_for_each_entry_safe() provides a similar guarantee as\n * list_for_each_entry_safe() and allows the iteration to continue independent\n * of changes to @pos by the body of the loop.\n *\n * Note, however, that it cannot handle other modifications that re-order the\n * rbtree it is iterating over. This includes calling rb_erase() on @pos, as\n * rb_erase() may rebalance the tree, causing us to miss some nodes.\n */\n#define rbtree_postorder_for_each_entry_safe(pos, n, root, field) \\\n\tfor (pos = rb_entry_safe(rb_first_postorder(root), typeof(*pos), field); \\\n\t     pos && ({ n = rb_entry_safe(rb_next_postorder(&pos->field), \\\n\t\t\ttypeof(*pos), field); 1; }); \\\n\t     pos = n)\n\n/*\n * Leftmost-cached rbtrees.\n *\n * We do not cache the rightmost node based on footprint\n * size vs number of potential users that could benefit\n * from O(1) rb_last(). Just not worth it, users that want\n * this feature can always implement the logic explicitly.\n * Furthermore, users that want to cache both pointers may\n * find it a bit asymmetric, but that's ok.\n */\nstruct rb_root_cached {\n\tstruct rb_root rb_root;\n\tstruct rb_node *rb_leftmost;\n};\n\n#define RB_ROOT_CACHED (struct rb_root_cached) { {NULL, }, NULL }\n\n/* Same as rb_first(), but O(1) */\n#define rb_first_cached(root) (root)->rb_leftmost\n\nstatic inline void rb_insert_color_cached(struct rb_node *node,\n\t\t\t\t\t  struct rb_root_cached *root,\n\t\t\t\t\t  bool leftmost)\n{\n\tif (leftmost)\n\t\troot->rb_leftmost = node;\n\trb_insert_color(node, &root->rb_root);\n}\n\nstatic inline void rb_erase_cached(struct rb_node *node,\n\t\t\t\t   struct rb_root_cached *root)\n{\n\tif (root->rb_leftmost == node)\n\t\troot->rb_leftmost = rb_next(node);\n\trb_erase(node, &root->rb_root);\n}\n\nstatic inline void rb_replace_node_cached(struct rb_node *victim,\n\t\t\t\t\t  struct rb_node *new,\n\t\t\t\t\t  struct rb_root_cached *root)\n{\n\tif (root->rb_leftmost == victim)\n\t\troot->rb_leftmost = new;\n\trb_replace_node(victim, new, &root->rb_root);\n}\n\n#endif\t/* _LINUX_RBTREE_H */\n"}, "3": {"id": 3, "path": "/src/include/asm-generic/rwonce.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n/*\n * Prevent the compiler from merging or refetching reads or writes. The\n * compiler is also forbidden from reordering successive instances of\n * READ_ONCE and WRITE_ONCE, but only when the compiler is aware of some\n * particular ordering. One way to make the compiler aware of ordering is to\n * put the two invocations of READ_ONCE or WRITE_ONCE in different C\n * statements.\n *\n * These two macros will also work on aggregate data types like structs or\n * unions.\n *\n * Their two major use cases are: (1) Mediating communication between\n * process-level code and irq/NMI handlers, all running on the same CPU,\n * and (2) Ensuring that the compiler does not fold, spindle, or otherwise\n * mutilate accesses that either do not require ordering or that interact\n * with an explicit memory barrier or atomic instruction that provides the\n * required ordering.\n */\n#ifndef __ASM_GENERIC_RWONCE_H\n#define __ASM_GENERIC_RWONCE_H\n\n#ifndef __ASSEMBLY__\n\n#include <linux/compiler_types.h>\n#include <linux/kasan-checks.h>\n#include <linux/kcsan-checks.h>\n\n/*\n * Yes, this permits 64-bit accesses on 32-bit architectures. These will\n * actually be atomic in some cases (namely Armv7 + LPAE), but for others we\n * rely on the access being split into 2x32-bit accesses for a 32-bit quantity\n * (e.g. a virtual address) and a strong prevailing wind.\n */\n#define compiletime_assert_rwonce_type(t)\t\t\t\t\t\\\n\tcompiletime_assert(__native_word(t) || sizeof(t) == sizeof(long long),\t\\\n\t\t\"Unsupported access size for {READ,WRITE}_ONCE().\")\n\n/*\n * Use __READ_ONCE() instead of READ_ONCE() if you do not require any\n * atomicity. Note that this may result in tears!\n */\n#ifndef __READ_ONCE\n#define __READ_ONCE(x)\t(*(const volatile __unqual_scalar_typeof(x) *)&(x))\n#endif\n\n#define READ_ONCE(x)\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tcompiletime_assert_rwonce_type(x);\t\t\t\t\\\n\t__READ_ONCE(x);\t\t\t\t\t\t\t\\\n})\n\n#define __WRITE_ONCE(x, val)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t*(volatile typeof(x) *)&(x) = (val);\t\t\t\t\\\n} while (0)\n\n#define WRITE_ONCE(x, val)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tcompiletime_assert_rwonce_type(x);\t\t\t\t\\\n\t__WRITE_ONCE(x, val);\t\t\t\t\t\t\\\n} while (0)\n\nstatic __no_sanitize_or_inline\nunsigned long __read_once_word_nocheck(const void *addr)\n{\n\treturn __READ_ONCE(*(unsigned long *)addr);\n}\n\n/*\n * Use READ_ONCE_NOCHECK() instead of READ_ONCE() if you need to load a\n * word from memory atomically but without telling KASAN/KCSAN. This is\n * usually used by unwinding code when walking the stack of a running process.\n */\n#define READ_ONCE_NOCHECK(x)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tcompiletime_assert(sizeof(x) == sizeof(unsigned long),\t\t\\\n\t\t\"Unsupported access size for READ_ONCE_NOCHECK().\");\t\\\n\t(typeof(x))__read_once_word_nocheck(&(x));\t\t\t\\\n})\n\nstatic __no_kasan_or_inline\nunsigned long read_word_at_a_time(const void *addr)\n{\n\tkasan_check_read(addr, 1);\n\treturn *(unsigned long *)addr;\n}\n\n#endif /* __ASSEMBLY__ */\n#endif\t/* __ASM_GENERIC_RWONCE_H */\n"}, "4": {"id": 4, "path": "/src/include/linux/compiler_types.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_COMPILER_TYPES_H\n#define __LINUX_COMPILER_TYPES_H\n\n#ifndef __ASSEMBLY__\n\n#ifdef __CHECKER__\n/* address spaces */\n# define __kernel\t__attribute__((address_space(0)))\n# define __user\t\t__attribute__((noderef, address_space(__user)))\n# define __iomem\t__attribute__((noderef, address_space(__iomem)))\n# define __percpu\t__attribute__((noderef, address_space(__percpu)))\n# define __rcu\t\t__attribute__((noderef, address_space(__rcu)))\nstatic inline void __chk_user_ptr(const volatile void __user *ptr) { }\nstatic inline void __chk_io_ptr(const volatile void __iomem *ptr) { }\n/* context/locking */\n# define __must_hold(x)\t__attribute__((context(x,1,1)))\n# define __acquires(x)\t__attribute__((context(x,0,1)))\n# define __releases(x)\t__attribute__((context(x,1,0)))\n# define __acquire(x)\t__context__(x,1)\n# define __release(x)\t__context__(x,-1)\n# define __cond_lock(x,c)\t((c) ? ({ __acquire(x); 1; }) : 0)\n/* other */\n# define __force\t__attribute__((force))\n# define __nocast\t__attribute__((nocast))\n# define __safe\t\t__attribute__((safe))\n# define __private\t__attribute__((noderef))\n# define ACCESS_PRIVATE(p, member) (*((typeof((p)->member) __force *) &(p)->member))\n#else /* __CHECKER__ */\n/* address spaces */\n# define __kernel\n# ifdef STRUCTLEAK_PLUGIN\n#  define __user\t__attribute__((user))\n# else\n#  define __user\n# endif\n# define __iomem\n# define __percpu\n# define __rcu\n# define __chk_user_ptr(x)\t(void)0\n# define __chk_io_ptr(x)\t(void)0\n/* context/locking */\n# define __must_hold(x)\n# define __acquires(x)\n# define __releases(x)\n# define __acquire(x)\t(void)0\n# define __release(x)\t(void)0\n# define __cond_lock(x,c) (c)\n/* other */\n# define __force\n# define __nocast\n# define __safe\n# define __private\n# define ACCESS_PRIVATE(p, member) ((p)->member)\n# define __builtin_warning(x, y...) (1)\n#endif /* __CHECKER__ */\n\n/* Indirect macros required for expanded argument pasting, eg. __LINE__. */\n#define ___PASTE(a,b) a##b\n#define __PASTE(a,b) ___PASTE(a,b)\n\n#ifdef __KERNEL__\n\n/* Attributes */\n#include <linux/compiler_attributes.h>\n\n/* Compiler specific macros. */\n#ifdef __clang__\n#include <linux/compiler-clang.h>\n#elif defined(__INTEL_COMPILER)\n#include <linux/compiler-intel.h>\n#elif defined(__GNUC__)\n/* The above compilers also define __GNUC__, so order is important here. */\n#include <linux/compiler-gcc.h>\n#else\n#error \"Unknown compiler\"\n#endif\n\n/*\n * Some architectures need to provide custom definitions of macros provided\n * by linux/compiler-*.h, and can do so using asm/compiler.h. We include that\n * conditionally rather than using an asm-generic wrapper in order to avoid\n * build failures if any C compilation, which will include this file via an\n * -include argument in c_flags, occurs prior to the asm-generic wrappers being\n * generated.\n */\n#ifdef CONFIG_HAVE_ARCH_COMPILER_H\n#include <asm/compiler.h>\n#endif\n\nstruct ftrace_branch_data {\n\tconst char *func;\n\tconst char *file;\n\tunsigned line;\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long correct;\n\t\t\tunsigned long incorrect;\n\t\t};\n\t\tstruct {\n\t\t\tunsigned long miss;\n\t\t\tunsigned long hit;\n\t\t};\n\t\tunsigned long miss_hit[2];\n\t};\n};\n\nstruct ftrace_likely_data {\n\tstruct ftrace_branch_data\tdata;\n\tunsigned long\t\t\tconstant;\n};\n\n#ifdef CONFIG_ENABLE_MUST_CHECK\n#define __must_check\t\t__attribute__((__warn_unused_result__))\n#else\n#define __must_check\n#endif\n\n#if defined(CC_USING_HOTPATCH)\n#define notrace\t\t\t__attribute__((hotpatch(0, 0)))\n#elif defined(CC_USING_PATCHABLE_FUNCTION_ENTRY)\n#define notrace\t\t\t__attribute__((patchable_function_entry(0, 0)))\n#else\n#define notrace\t\t\t__attribute__((__no_instrument_function__))\n#endif\n\n/*\n * it doesn't make sense on ARM (currently the only user of __naked)\n * to trace naked functions because then mcount is called without\n * stack and frame pointer being set up and there is no chance to\n * restore the lr register to the value before mcount was called.\n */\n#define __naked\t\t\t__attribute__((__naked__)) notrace\n\n#define __compiler_offsetof(a, b)\t__builtin_offsetof(a, b)\n\n/*\n * Prefer gnu_inline, so that extern inline functions do not emit an\n * externally visible function. This makes extern inline behave as per gnu89\n * semantics rather than c99. This prevents multiple symbol definition errors\n * of extern inline functions at link time.\n * A lot of inline functions can cause havoc with function tracing.\n */\n#define inline inline __gnu_inline __inline_maybe_unused notrace\n\n/*\n * gcc provides both __inline__ and __inline as alternate spellings of\n * the inline keyword, though the latter is undocumented. New kernel\n * code should only use the inline spelling, but some existing code\n * uses __inline__. Since we #define inline above, to ensure\n * __inline__ has the same semantics, we need this #define.\n *\n * However, the spelling __inline is strictly reserved for referring\n * to the bare keyword.\n */\n#define __inline__ inline\n\n/*\n * GCC does not warn about unused static inline functions for -Wunused-function.\n * Suppress the warning in clang as well by using __maybe_unused, but enable it\n * for W=1 build. This will allow clang to find unused functions. Remove the\n * __inline_maybe_unused entirely after fixing most of -Wunused-function warnings.\n */\n#ifdef KBUILD_EXTRA_WARN1\n#define __inline_maybe_unused\n#else\n#define __inline_maybe_unused __maybe_unused\n#endif\n\n/*\n * Rather then using noinline to prevent stack consumption, use\n * noinline_for_stack instead.  For documentation reasons.\n */\n#define noinline_for_stack noinline\n\n/*\n * Sanitizer helper attributes: Because using __always_inline and\n * __no_sanitize_* conflict, provide helper attributes that will either expand\n * to __no_sanitize_* in compilation units where instrumentation is enabled\n * (__SANITIZE_*__), or __always_inline in compilation units without\n * instrumentation (__SANITIZE_*__ undefined).\n */\n#ifdef __SANITIZE_ADDRESS__\n/*\n * We can't declare function 'inline' because __no_sanitize_address conflicts\n * with inlining. Attempt to inline it may cause a build failure.\n *     https://gcc.gnu.org/bugzilla/show_bug.cgi?id=67368\n * '__maybe_unused' allows us to avoid defined-but-not-used warnings.\n */\n# define __no_kasan_or_inline __no_sanitize_address notrace __maybe_unused\n# define __no_sanitize_or_inline __no_kasan_or_inline\n#else\n# define __no_kasan_or_inline __always_inline\n#endif\n\n#define __no_kcsan __no_sanitize_thread\n#ifdef __SANITIZE_THREAD__\n# define __no_sanitize_or_inline __no_kcsan notrace __maybe_unused\n#endif\n\n#ifndef __no_sanitize_or_inline\n#define __no_sanitize_or_inline __always_inline\n#endif\n\n/* Section for code which can't be instrumented at all */\n#define noinstr\t\t\t\t\t\t\t\t\\\n\tnoinline notrace __attribute((__section__(\".noinstr.text\")))\t\\\n\t__no_kcsan __no_sanitize_address\n\n#endif /* __KERNEL__ */\n\n#endif /* __ASSEMBLY__ */\n\n/*\n * The below symbols may be defined for one or more, but not ALL, of the above\n * compilers. We don't consider that to be an error, so set them to nothing.\n * For example, some of them are for compiler specific plugins.\n */\n#ifndef __latent_entropy\n# define __latent_entropy\n#endif\n\n#ifndef __randomize_layout\n# define __randomize_layout __designated_init\n#endif\n\n#ifndef __no_randomize_layout\n# define __no_randomize_layout\n#endif\n\n#ifndef randomized_struct_fields_start\n# define randomized_struct_fields_start\n# define randomized_struct_fields_end\n#endif\n\n#ifndef __noscs\n# define __noscs\n#endif\n\n#ifndef asm_volatile_goto\n#define asm_volatile_goto(x...) asm goto(x)\n#endif\n\n#ifdef CONFIG_CC_HAS_ASM_INLINE\n#define asm_inline asm __inline\n#else\n#define asm_inline asm\n#endif\n\n#ifndef __no_fgcse\n# define __no_fgcse\n#endif\n\n/* Are two types/vars the same type (ignoring qualifiers)? */\n#define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b))\n\n/*\n * __unqual_scalar_typeof(x) - Declare an unqualified scalar type, leaving\n *\t\t\t       non-scalar types unchanged.\n */\n/*\n * Prefer C11 _Generic for better compile-times and simpler code. Note: 'char'\n * is not type-compatible with 'signed char', and we define a separate case.\n */\n#define __scalar_type_to_expr_cases(type)\t\t\t\t\\\n\t\tunsigned type:\t(unsigned type)0,\t\t\t\\\n\t\tsigned type:\t(signed type)0\n\n#define __unqual_scalar_typeof(x) typeof(\t\t\t\t\\\n\t\t_Generic((x),\t\t\t\t\t\t\\\n\t\t\t char:\t(char)0,\t\t\t\t\\\n\t\t\t __scalar_type_to_expr_cases(char),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(short),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(int),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(long),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(long long),\t\\\n\t\t\t default: (x)))\n\n/* Is this type a native word size -- useful for atomic operations */\n#define __native_word(t) \\\n\t(sizeof(t) == sizeof(char) || sizeof(t) == sizeof(short) || \\\n\t sizeof(t) == sizeof(int) || sizeof(t) == sizeof(long))\n\n/* Compile time object size, -1 for unknown */\n#ifndef __compiletime_object_size\n# define __compiletime_object_size(obj) -1\n#endif\n#ifndef __compiletime_warning\n# define __compiletime_warning(message)\n#endif\n#ifndef __compiletime_error\n# define __compiletime_error(message)\n#endif\n\n#ifdef __OPTIMIZE__\n# define __compiletime_assert(condition, msg, prefix, suffix)\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\textern void prefix ## suffix(void) __compiletime_error(msg); \\\n\t\tif (!(condition))\t\t\t\t\t\\\n\t\t\tprefix ## suffix();\t\t\t\t\\\n\t} while (0)\n#else\n# define __compiletime_assert(condition, msg, prefix, suffix) do { } while (0)\n#endif\n\n#define _compiletime_assert(condition, msg, prefix, suffix) \\\n\t__compiletime_assert(condition, msg, prefix, suffix)\n\n/**\n * compiletime_assert - break build and emit msg if condition is false\n * @condition: a compile-time constant condition to check\n * @msg:       a message to emit if condition is false\n *\n * In tradition of POSIX assert, this macro will break the build if the\n * supplied condition is *false*, emitting the supplied error message if the\n * compiler has support to do so.\n */\n#define compiletime_assert(condition, msg) \\\n\t_compiletime_assert(condition, msg, __compiletime_assert_, __COUNTER__)\n\n#define compiletime_assert_atomic_type(t)\t\t\t\t\\\n\tcompiletime_assert(__native_word(t),\t\t\t\t\\\n\t\t\"Need native word sized stores/loads for atomicity.\")\n\n/* Helpers for emitting diagnostics in pragmas. */\n#ifndef __diag\n#define __diag(string)\n#endif\n\n#ifndef __diag_GCC\n#define __diag_GCC(version, severity, string)\n#endif\n\n#define __diag_push()\t__diag(push)\n#define __diag_pop()\t__diag(pop)\n\n#define __diag_ignore(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, ignore, option)\n#define __diag_warn(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, warn, option)\n#define __diag_error(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, error, option)\n\n#endif /* __LINUX_COMPILER_TYPES_H */\n"}, "5": {"id": 5, "path": "/src/include/net/ip.h", "content": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tDefinitions for the IP module.\n *\n * Version:\t@(#)ip.h\t1.0.2\t05/07/93\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\n * Changes:\n *\t\tMike McLagan    :       Routing by source\n */\n#ifndef _IP_H\n#define _IP_H\n\n#include <linux/types.h>\n#include <linux/ip.h>\n#include <linux/in.h>\n#include <linux/skbuff.h>\n#include <linux/jhash.h>\n#include <linux/sockptr.h>\n\n#include <net/inet_sock.h>\n#include <net/route.h>\n#include <net/snmp.h>\n#include <net/flow.h>\n#include <net/flow_dissector.h>\n#include <net/netns/hash.h>\n\n#define IPV4_MAX_PMTU\t\t65535U\t\t/* RFC 2675, Section 5.1 */\n#define IPV4_MIN_MTU\t\t68\t\t\t/* RFC 791 */\n\nextern unsigned int sysctl_fib_sync_mem;\nextern unsigned int sysctl_fib_sync_mem_min;\nextern unsigned int sysctl_fib_sync_mem_max;\n\nstruct sock;\n\nstruct inet_skb_parm {\n\tint\t\t\tiif;\n\tstruct ip_options\topt;\t\t/* Compiled IP options\t\t*/\n\tu16\t\t\tflags;\n\n#define IPSKB_FORWARDED\t\tBIT(0)\n#define IPSKB_XFRM_TUNNEL_SIZE\tBIT(1)\n#define IPSKB_XFRM_TRANSFORMED\tBIT(2)\n#define IPSKB_FRAG_COMPLETE\tBIT(3)\n#define IPSKB_REROUTED\t\tBIT(4)\n#define IPSKB_DOREDIRECT\tBIT(5)\n#define IPSKB_FRAG_PMTU\t\tBIT(6)\n#define IPSKB_L3SLAVE\t\tBIT(7)\n\n\tu16\t\t\tfrag_max_size;\n};\n\nstatic inline bool ipv4_l3mdev_skb(u16 flags)\n{\n\treturn !!(flags & IPSKB_L3SLAVE);\n}\n\nstatic inline unsigned int ip_hdrlen(const struct sk_buff *skb)\n{\n\treturn ip_hdr(skb)->ihl * 4;\n}\n\nstruct ipcm_cookie {\n\tstruct sockcm_cookie\tsockc;\n\t__be32\t\t\taddr;\n\tint\t\t\toif;\n\tstruct ip_options_rcu\t*opt;\n\t__u8\t\t\tttl;\n\t__s16\t\t\ttos;\n\tchar\t\t\tpriority;\n\t__u16\t\t\tgso_size;\n};\n\nstatic inline void ipcm_init(struct ipcm_cookie *ipcm)\n{\n\t*ipcm = (struct ipcm_cookie) { .tos = -1 };\n}\n\nstatic inline void ipcm_init_sk(struct ipcm_cookie *ipcm,\n\t\t\t\tconst struct inet_sock *inet)\n{\n\tipcm_init(ipcm);\n\n\tipcm->sockc.mark = inet->sk.sk_mark;\n\tipcm->sockc.tsflags = inet->sk.sk_tsflags;\n\tipcm->oif = inet->sk.sk_bound_dev_if;\n\tipcm->addr = inet->inet_saddr;\n}\n\n#define IPCB(skb) ((struct inet_skb_parm*)((skb)->cb))\n#define PKTINFO_SKB_CB(skb) ((struct in_pktinfo *)((skb)->cb))\n\n/* return enslaved device index if relevant */\nstatic inline int inet_sdif(struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NET_L3_MASTER_DEV)\n\tif (skb && ipv4_l3mdev_skb(IPCB(skb)->flags))\n\t\treturn IPCB(skb)->iif;\n#endif\n\treturn 0;\n}\n\n/* Special input handler for packets caught by router alert option.\n   They are selected only by protocol field, and then processed likely\n   local ones; but only if someone wants them! Otherwise, router\n   not running rsvpd will kill RSVP.\n\n   It is user level problem, what it will make with them.\n   I have no idea, how it will masquearde or NAT them (it is joke, joke :-)),\n   but receiver should be enough clever f.e. to forward mtrace requests,\n   sent to multicast group to reach destination designated router.\n */\n\nstruct ip_ra_chain {\n\tstruct ip_ra_chain __rcu *next;\n\tstruct sock\t\t*sk;\n\tunion {\n\t\tvoid\t\t\t(*destructor)(struct sock *);\n\t\tstruct sock\t\t*saved_sk;\n\t};\n\tstruct rcu_head\t\trcu;\n};\n\n/* IP flags. */\n#define IP_CE\t\t0x8000\t\t/* Flag: \"Congestion\"\t\t*/\n#define IP_DF\t\t0x4000\t\t/* Flag: \"Don't Fragment\"\t*/\n#define IP_MF\t\t0x2000\t\t/* Flag: \"More Fragments\"\t*/\n#define IP_OFFSET\t0x1FFF\t\t/* \"Fragment Offset\" part\t*/\n\n#define IP_FRAG_TIME\t(30 * HZ)\t\t/* fragment lifetime\t*/\n\nstruct msghdr;\nstruct net_device;\nstruct packet_type;\nstruct rtable;\nstruct sockaddr;\n\nint igmp_mc_init(void);\n\n/*\n *\tFunctions provided by ip.c\n */\n\nint ip_build_and_send_pkt(struct sk_buff *skb, const struct sock *sk,\n\t\t\t  __be32 saddr, __be32 daddr,\n\t\t\t  struct ip_options_rcu *opt, u8 tos);\nint ip_rcv(struct sk_buff *skb, struct net_device *dev, struct packet_type *pt,\n\t   struct net_device *orig_dev);\nvoid ip_list_rcv(struct list_head *head, struct packet_type *pt,\n\t\t struct net_device *orig_dev);\nint ip_local_deliver(struct sk_buff *skb);\nvoid ip_protocol_deliver_rcu(struct net *net, struct sk_buff *skb, int proto);\nint ip_mr_input(struct sk_buff *skb);\nint ip_output(struct net *net, struct sock *sk, struct sk_buff *skb);\nint ip_mc_output(struct net *net, struct sock *sk, struct sk_buff *skb);\nint ip_do_fragment(struct net *net, struct sock *sk, struct sk_buff *skb,\n\t\t   int (*output)(struct net *, struct sock *, struct sk_buff *));\n\nstruct ip_fraglist_iter {\n\tstruct sk_buff\t*frag;\n\tstruct iphdr\t*iph;\n\tint\t\toffset;\n\tunsigned int\thlen;\n};\n\nvoid ip_fraglist_init(struct sk_buff *skb, struct iphdr *iph,\n\t\t      unsigned int hlen, struct ip_fraglist_iter *iter);\nvoid ip_fraglist_prepare(struct sk_buff *skb, struct ip_fraglist_iter *iter);\n\nstatic inline struct sk_buff *ip_fraglist_next(struct ip_fraglist_iter *iter)\n{\n\tstruct sk_buff *skb = iter->frag;\n\n\titer->frag = skb->next;\n\tskb_mark_not_on_list(skb);\n\n\treturn skb;\n}\n\nstruct ip_frag_state {\n\tbool\t\tDF;\n\tunsigned int\thlen;\n\tunsigned int\tll_rs;\n\tunsigned int\tmtu;\n\tunsigned int\tleft;\n\tint\t\toffset;\n\tint\t\tptr;\n\t__be16\t\tnot_last_frag;\n};\n\nvoid ip_frag_init(struct sk_buff *skb, unsigned int hlen, unsigned int ll_rs,\n\t\t  unsigned int mtu, bool DF, struct ip_frag_state *state);\nstruct sk_buff *ip_frag_next(struct sk_buff *skb,\n\t\t\t     struct ip_frag_state *state);\n\nvoid ip_send_check(struct iphdr *ip);\nint __ip_local_out(struct net *net, struct sock *sk, struct sk_buff *skb);\nint ip_local_out(struct net *net, struct sock *sk, struct sk_buff *skb);\n\nint __ip_queue_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl,\n\t\t    __u8 tos);\nvoid ip_init(void);\nint ip_append_data(struct sock *sk, struct flowi4 *fl4,\n\t\t   int getfrag(void *from, char *to, int offset, int len,\n\t\t\t       int odd, struct sk_buff *skb),\n\t\t   void *from, int len, int protolen,\n\t\t   struct ipcm_cookie *ipc,\n\t\t   struct rtable **rt,\n\t\t   unsigned int flags);\nint ip_generic_getfrag(void *from, char *to, int offset, int len, int odd,\n\t\t       struct sk_buff *skb);\nssize_t ip_append_page(struct sock *sk, struct flowi4 *fl4, struct page *page,\n\t\t       int offset, size_t size, int flags);\nstruct sk_buff *__ip_make_skb(struct sock *sk, struct flowi4 *fl4,\n\t\t\t      struct sk_buff_head *queue,\n\t\t\t      struct inet_cork *cork);\nint ip_send_skb(struct net *net, struct sk_buff *skb);\nint ip_push_pending_frames(struct sock *sk, struct flowi4 *fl4);\nvoid ip_flush_pending_frames(struct sock *sk);\nstruct sk_buff *ip_make_skb(struct sock *sk, struct flowi4 *fl4,\n\t\t\t    int getfrag(void *from, char *to, int offset,\n\t\t\t\t\tint len, int odd, struct sk_buff *skb),\n\t\t\t    void *from, int length, int transhdrlen,\n\t\t\t    struct ipcm_cookie *ipc, struct rtable **rtp,\n\t\t\t    struct inet_cork *cork, unsigned int flags);\n\nint ip_queue_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl);\n\nstatic inline struct sk_buff *ip_finish_skb(struct sock *sk, struct flowi4 *fl4)\n{\n\treturn __ip_make_skb(sk, fl4, &sk->sk_write_queue, &inet_sk(sk)->cork.base);\n}\n\nstatic inline __u8 get_rttos(struct ipcm_cookie* ipc, struct inet_sock *inet)\n{\n\treturn (ipc->tos != -1) ? RT_TOS(ipc->tos) : RT_TOS(inet->tos);\n}\n\nstatic inline __u8 get_rtconn_flags(struct ipcm_cookie* ipc, struct sock* sk)\n{\n\treturn (ipc->tos != -1) ? RT_CONN_FLAGS_TOS(sk, ipc->tos) : RT_CONN_FLAGS(sk);\n}\n\n/* datagram.c */\nint __ip4_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len);\nint ip4_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len);\n\nvoid ip4_datagram_release_cb(struct sock *sk);\n\nstruct ip_reply_arg {\n\tstruct kvec iov[1];\n\tint\t    flags;\n\t__wsum \t    csum;\n\tint\t    csumoffset; /* u16 offset of csum in iov[0].iov_base */\n\t\t\t\t/* -1 if not needed */\n\tint\t    bound_dev_if;\n\tu8  \t    tos;\n\tkuid_t\t    uid;\n};\n\n#define IP_REPLY_ARG_NOSRCCHECK 1\n\nstatic inline __u8 ip_reply_arg_flowi_flags(const struct ip_reply_arg *arg)\n{\n\treturn (arg->flags & IP_REPLY_ARG_NOSRCCHECK) ? FLOWI_FLAG_ANYSRC : 0;\n}\n\nvoid ip_send_unicast_reply(struct sock *sk, struct sk_buff *skb,\n\t\t\t   const struct ip_options *sopt,\n\t\t\t   __be32 daddr, __be32 saddr,\n\t\t\t   const struct ip_reply_arg *arg,\n\t\t\t   unsigned int len, u64 transmit_time);\n\n#define IP_INC_STATS(net, field)\tSNMP_INC_STATS64((net)->mib.ip_statistics, field)\n#define __IP_INC_STATS(net, field)\t__SNMP_INC_STATS64((net)->mib.ip_statistics, field)\n#define IP_ADD_STATS(net, field, val)\tSNMP_ADD_STATS64((net)->mib.ip_statistics, field, val)\n#define __IP_ADD_STATS(net, field, val) __SNMP_ADD_STATS64((net)->mib.ip_statistics, field, val)\n#define IP_UPD_PO_STATS(net, field, val) SNMP_UPD_PO_STATS64((net)->mib.ip_statistics, field, val)\n#define __IP_UPD_PO_STATS(net, field, val) __SNMP_UPD_PO_STATS64((net)->mib.ip_statistics, field, val)\n#define NET_INC_STATS(net, field)\tSNMP_INC_STATS((net)->mib.net_statistics, field)\n#define __NET_INC_STATS(net, field)\t__SNMP_INC_STATS((net)->mib.net_statistics, field)\n#define NET_ADD_STATS(net, field, adnd)\tSNMP_ADD_STATS((net)->mib.net_statistics, field, adnd)\n#define __NET_ADD_STATS(net, field, adnd) __SNMP_ADD_STATS((net)->mib.net_statistics, field, adnd)\n\nu64 snmp_get_cpu_field(void __percpu *mib, int cpu, int offct);\nunsigned long snmp_fold_field(void __percpu *mib, int offt);\n#if BITS_PER_LONG==32\nu64 snmp_get_cpu_field64(void __percpu *mib, int cpu, int offct,\n\t\t\t size_t syncp_offset);\nu64 snmp_fold_field64(void __percpu *mib, int offt, size_t sync_off);\n#else\nstatic inline u64  snmp_get_cpu_field64(void __percpu *mib, int cpu, int offct,\n\t\t\t\t\tsize_t syncp_offset)\n{\n\treturn snmp_get_cpu_field(mib, cpu, offct);\n\n}\n\nstatic inline u64 snmp_fold_field64(void __percpu *mib, int offt, size_t syncp_off)\n{\n\treturn snmp_fold_field(mib, offt);\n}\n#endif\n\n#define snmp_get_cpu_field64_batch(buff64, stats_list, mib_statistic, offset) \\\n{ \\\n\tint i, c; \\\n\tfor_each_possible_cpu(c) { \\\n\t\tfor (i = 0; stats_list[i].name; i++) \\\n\t\t\tbuff64[i] += snmp_get_cpu_field64( \\\n\t\t\t\t\tmib_statistic, \\\n\t\t\t\t\tc, stats_list[i].entry, \\\n\t\t\t\t\toffset); \\\n\t} \\\n}\n\n#define snmp_get_cpu_field_batch(buff, stats_list, mib_statistic) \\\n{ \\\n\tint i, c; \\\n\tfor_each_possible_cpu(c) { \\\n\t\tfor (i = 0; stats_list[i].name; i++) \\\n\t\t\tbuff[i] += snmp_get_cpu_field( \\\n\t\t\t\t\t\tmib_statistic, \\\n\t\t\t\t\t\tc, stats_list[i].entry); \\\n\t} \\\n}\n\nvoid inet_get_local_port_range(struct net *net, int *low, int *high);\n\n#ifdef CONFIG_SYSCTL\nstatic inline bool inet_is_local_reserved_port(struct net *net, unsigned short port)\n{\n\tif (!net->ipv4.sysctl_local_reserved_ports)\n\t\treturn false;\n\treturn test_bit(port, net->ipv4.sysctl_local_reserved_ports);\n}\n\nstatic inline bool sysctl_dev_name_is_allowed(const char *name)\n{\n\treturn strcmp(name, \"default\") != 0  && strcmp(name, \"all\") != 0;\n}\n\nstatic inline bool inet_port_requires_bind_service(struct net *net, unsigned short port)\n{\n\treturn port < net->ipv4.sysctl_ip_prot_sock;\n}\n\n#else\nstatic inline bool inet_is_local_reserved_port(struct net *net, unsigned short port)\n{\n\treturn false;\n}\n\nstatic inline bool inet_port_requires_bind_service(struct net *net, unsigned short port)\n{\n\treturn port < PROT_SOCK;\n}\n#endif\n\n__be32 inet_current_timestamp(void);\n\n/* From inetpeer.c */\nextern int inet_peer_threshold;\nextern int inet_peer_minttl;\nextern int inet_peer_maxttl;\n\nvoid ipfrag_init(void);\n\nvoid ip_static_sysctl_init(void);\n\n#define IP4_REPLY_MARK(net, mark) \\\n\t((net)->ipv4.sysctl_fwmark_reflect ? (mark) : 0)\n\nstatic inline bool ip_is_fragment(const struct iphdr *iph)\n{\n\treturn (iph->frag_off & htons(IP_MF | IP_OFFSET)) != 0;\n}\n\n#ifdef CONFIG_INET\n#include <net/dst.h>\n\n/* The function in 2.2 was invalid, producing wrong result for\n * check=0xFEFF. It was noticed by Arthur Skawina _year_ ago. --ANK(000625) */\nstatic inline\nint ip_decrease_ttl(struct iphdr *iph)\n{\n\tu32 check = (__force u32)iph->check;\n\tcheck += (__force u32)htons(0x0100);\n\tiph->check = (__force __sum16)(check + (check>=0xFFFF));\n\treturn --iph->ttl;\n}\n\nstatic inline int ip_mtu_locked(const struct dst_entry *dst)\n{\n\tconst struct rtable *rt = (const struct rtable *)dst;\n\n\treturn rt->rt_mtu_locked || dst_metric_locked(dst, RTAX_MTU);\n}\n\nstatic inline\nint ip_dont_fragment(const struct sock *sk, const struct dst_entry *dst)\n{\n\tu8 pmtudisc = READ_ONCE(inet_sk(sk)->pmtudisc);\n\n\treturn  pmtudisc == IP_PMTUDISC_DO ||\n\t\t(pmtudisc == IP_PMTUDISC_WANT &&\n\t\t !ip_mtu_locked(dst));\n}\n\nstatic inline bool ip_sk_accept_pmtu(const struct sock *sk)\n{\n\treturn inet_sk(sk)->pmtudisc != IP_PMTUDISC_INTERFACE &&\n\t       inet_sk(sk)->pmtudisc != IP_PMTUDISC_OMIT;\n}\n\nstatic inline bool ip_sk_use_pmtu(const struct sock *sk)\n{\n\treturn inet_sk(sk)->pmtudisc < IP_PMTUDISC_PROBE;\n}\n\nstatic inline bool ip_sk_ignore_df(const struct sock *sk)\n{\n\treturn inet_sk(sk)->pmtudisc < IP_PMTUDISC_DO ||\n\t       inet_sk(sk)->pmtudisc == IP_PMTUDISC_OMIT;\n}\n\nstatic inline unsigned int ip_dst_mtu_maybe_forward(const struct dst_entry *dst,\n\t\t\t\t\t\t    bool forwarding)\n{\n\tstruct net *net = dev_net(dst->dev);\n\tunsigned int mtu;\n\n\tif (net->ipv4.sysctl_ip_fwd_use_pmtu ||\n\t    ip_mtu_locked(dst) ||\n\t    !forwarding)\n\t\treturn dst_mtu(dst);\n\n\t/* 'forwarding = true' case should always honour route mtu */\n\tmtu = dst_metric_raw(dst, RTAX_MTU);\n\tif (mtu)\n\t\treturn mtu;\n\n\treturn min(READ_ONCE(dst->dev->mtu), IP_MAX_MTU);\n}\n\nstatic inline unsigned int ip_skb_dst_mtu(struct sock *sk,\n\t\t\t\t\t  const struct sk_buff *skb)\n{\n\tif (!sk || !sk_fullsock(sk) || ip_sk_use_pmtu(sk)) {\n\t\tbool forwarding = IPCB(skb)->flags & IPSKB_FORWARDED;\n\n\t\treturn ip_dst_mtu_maybe_forward(skb_dst(skb), forwarding);\n\t}\n\n\treturn min(READ_ONCE(skb_dst(skb)->dev->mtu), IP_MAX_MTU);\n}\n\nstruct dst_metrics *ip_fib_metrics_init(struct net *net, struct nlattr *fc_mx,\n\t\t\t\t\tint fc_mx_len,\n\t\t\t\t\tstruct netlink_ext_ack *extack);\nstatic inline void ip_fib_metrics_put(struct dst_metrics *fib_metrics)\n{\n\tif (fib_metrics != &dst_default_metrics &&\n\t    refcount_dec_and_test(&fib_metrics->refcnt))\n\t\tkfree(fib_metrics);\n}\n\n/* ipv4 and ipv6 both use refcounted metrics if it is not the default */\nstatic inline\nvoid ip_dst_init_metrics(struct dst_entry *dst, struct dst_metrics *fib_metrics)\n{\n\tdst_init_metrics(dst, fib_metrics->metrics, true);\n\n\tif (fib_metrics != &dst_default_metrics) {\n\t\tdst->_metrics |= DST_METRICS_REFCOUNTED;\n\t\trefcount_inc(&fib_metrics->refcnt);\n\t}\n}\n\nstatic inline\nvoid ip_dst_metrics_put(struct dst_entry *dst)\n{\n\tstruct dst_metrics *p = (struct dst_metrics *)DST_METRICS_PTR(dst);\n\n\tif (p != &dst_default_metrics && refcount_dec_and_test(&p->refcnt))\n\t\tkfree(p);\n}\n\nu32 ip_idents_reserve(u32 hash, int segs);\nvoid __ip_select_ident(struct net *net, struct iphdr *iph, int segs);\n\nstatic inline void ip_select_ident_segs(struct net *net, struct sk_buff *skb,\n\t\t\t\t\tstruct sock *sk, int segs)\n{\n\tstruct iphdr *iph = ip_hdr(skb);\n\n\tif ((iph->frag_off & htons(IP_DF)) && !skb->ignore_df) {\n\t\t/* This is only to work around buggy Windows95/2000\n\t\t * VJ compression implementations.  If the ID field\n\t\t * does not change, they drop every other packet in\n\t\t * a TCP stream using header compression.\n\t\t */\n\t\tif (sk && inet_sk(sk)->inet_daddr) {\n\t\t\tiph->id = htons(inet_sk(sk)->inet_id);\n\t\t\tinet_sk(sk)->inet_id += segs;\n\t\t} else {\n\t\t\tiph->id = 0;\n\t\t}\n\t} else {\n\t\t__ip_select_ident(net, iph, segs);\n\t}\n}\n\nstatic inline void ip_select_ident(struct net *net, struct sk_buff *skb,\n\t\t\t\t   struct sock *sk)\n{\n\tip_select_ident_segs(net, skb, sk, 1);\n}\n\nstatic inline __wsum inet_compute_pseudo(struct sk_buff *skb, int proto)\n{\n\treturn csum_tcpudp_nofold(ip_hdr(skb)->saddr, ip_hdr(skb)->daddr,\n\t\t\t\t  skb->len, proto, 0);\n}\n\n/* copy IPv4 saddr & daddr to flow_keys, possibly using 64bit load/store\n * Equivalent to :\tflow->v4addrs.src = iph->saddr;\n *\t\t\tflow->v4addrs.dst = iph->daddr;\n */\nstatic inline void iph_to_flow_copy_v4addrs(struct flow_keys *flow,\n\t\t\t\t\t    const struct iphdr *iph)\n{\n\tBUILD_BUG_ON(offsetof(typeof(flow->addrs), v4addrs.dst) !=\n\t\t     offsetof(typeof(flow->addrs), v4addrs.src) +\n\t\t\t      sizeof(flow->addrs.v4addrs.src));\n\tmemcpy(&flow->addrs.v4addrs, &iph->saddr, sizeof(flow->addrs.v4addrs));\n\tflow->control.addr_type = FLOW_DISSECTOR_KEY_IPV4_ADDRS;\n}\n\nstatic inline __wsum inet_gro_compute_pseudo(struct sk_buff *skb, int proto)\n{\n\tconst struct iphdr *iph = skb_gro_network_header(skb);\n\n\treturn csum_tcpudp_nofold(iph->saddr, iph->daddr,\n\t\t\t\t  skb_gro_len(skb), proto, 0);\n}\n\n/*\n *\tMap a multicast IP onto multicast MAC for type ethernet.\n */\n\nstatic inline void ip_eth_mc_map(__be32 naddr, char *buf)\n{\n\t__u32 addr=ntohl(naddr);\n\tbuf[0]=0x01;\n\tbuf[1]=0x00;\n\tbuf[2]=0x5e;\n\tbuf[5]=addr&0xFF;\n\taddr>>=8;\n\tbuf[4]=addr&0xFF;\n\taddr>>=8;\n\tbuf[3]=addr&0x7F;\n}\n\n/*\n *\tMap a multicast IP onto multicast MAC for type IP-over-InfiniBand.\n *\tLeave P_Key as 0 to be filled in by driver.\n */\n\nstatic inline void ip_ib_mc_map(__be32 naddr, const unsigned char *broadcast, char *buf)\n{\n\t__u32 addr;\n\tunsigned char scope = broadcast[5] & 0xF;\n\n\tbuf[0]  = 0;\t\t/* Reserved */\n\tbuf[1]  = 0xff;\t\t/* Multicast QPN */\n\tbuf[2]  = 0xff;\n\tbuf[3]  = 0xff;\n\taddr    = ntohl(naddr);\n\tbuf[4]  = 0xff;\n\tbuf[5]  = 0x10 | scope;\t/* scope from broadcast address */\n\tbuf[6]  = 0x40;\t\t/* IPv4 signature */\n\tbuf[7]  = 0x1b;\n\tbuf[8]  = broadcast[8];\t\t/* P_Key */\n\tbuf[9]  = broadcast[9];\n\tbuf[10] = 0;\n\tbuf[11] = 0;\n\tbuf[12] = 0;\n\tbuf[13] = 0;\n\tbuf[14] = 0;\n\tbuf[15] = 0;\n\tbuf[19] = addr & 0xff;\n\taddr  >>= 8;\n\tbuf[18] = addr & 0xff;\n\taddr  >>= 8;\n\tbuf[17] = addr & 0xff;\n\taddr  >>= 8;\n\tbuf[16] = addr & 0x0f;\n}\n\nstatic inline void ip_ipgre_mc_map(__be32 naddr, const unsigned char *broadcast, char *buf)\n{\n\tif ((broadcast[0] | broadcast[1] | broadcast[2] | broadcast[3]) != 0)\n\t\tmemcpy(buf, broadcast, 4);\n\telse\n\t\tmemcpy(buf, &naddr, sizeof(naddr));\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\n#include <linux/ipv6.h>\n#endif\n\nstatic __inline__ void inet_reset_saddr(struct sock *sk)\n{\n\tinet_sk(sk)->inet_rcv_saddr = inet_sk(sk)->inet_saddr = 0;\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (sk->sk_family == PF_INET6) {\n\t\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\n\t\tmemset(&np->saddr, 0, sizeof(np->saddr));\n\t\tmemset(&sk->sk_v6_rcv_saddr, 0, sizeof(sk->sk_v6_rcv_saddr));\n\t}\n#endif\n}\n\n#endif\n\nstatic inline unsigned int ipv4_addr_hash(__be32 ip)\n{\n\treturn (__force unsigned int) ip;\n}\n\nstatic inline u32 ipv4_portaddr_hash(const struct net *net,\n\t\t\t\t     __be32 saddr,\n\t\t\t\t     unsigned int port)\n{\n\treturn jhash_1word((__force u32)saddr, net_hash_mix(net)) ^ port;\n}\n\nbool ip_call_ra_chain(struct sk_buff *skb);\n\n/*\n *\tFunctions provided by ip_fragment.c\n */\n\nenum ip_defrag_users {\n\tIP_DEFRAG_LOCAL_DELIVER,\n\tIP_DEFRAG_CALL_RA_CHAIN,\n\tIP_DEFRAG_CONNTRACK_IN,\n\t__IP_DEFRAG_CONNTRACK_IN_END\t= IP_DEFRAG_CONNTRACK_IN + USHRT_MAX,\n\tIP_DEFRAG_CONNTRACK_OUT,\n\t__IP_DEFRAG_CONNTRACK_OUT_END\t= IP_DEFRAG_CONNTRACK_OUT + USHRT_MAX,\n\tIP_DEFRAG_CONNTRACK_BRIDGE_IN,\n\t__IP_DEFRAG_CONNTRACK_BRIDGE_IN = IP_DEFRAG_CONNTRACK_BRIDGE_IN + USHRT_MAX,\n\tIP_DEFRAG_VS_IN,\n\tIP_DEFRAG_VS_OUT,\n\tIP_DEFRAG_VS_FWD,\n\tIP_DEFRAG_AF_PACKET,\n\tIP_DEFRAG_MACVLAN,\n};\n\n/* Return true if the value of 'user' is between 'lower_bond'\n * and 'upper_bond' inclusively.\n */\nstatic inline bool ip_defrag_user_in_between(u32 user,\n\t\t\t\t\t     enum ip_defrag_users lower_bond,\n\t\t\t\t\t     enum ip_defrag_users upper_bond)\n{\n\treturn user >= lower_bond && user <= upper_bond;\n}\n\nint ip_defrag(struct net *net, struct sk_buff *skb, u32 user);\n#ifdef CONFIG_INET\nstruct sk_buff *ip_check_defrag(struct net *net, struct sk_buff *skb, u32 user);\n#else\nstatic inline struct sk_buff *ip_check_defrag(struct net *net, struct sk_buff *skb, u32 user)\n{\n\treturn skb;\n}\n#endif\n\n/*\n *\tFunctions provided by ip_forward.c\n */\n\nint ip_forward(struct sk_buff *skb);\n\n/*\n *\tFunctions provided by ip_options.c\n */\n\nvoid ip_options_build(struct sk_buff *skb, struct ip_options *opt,\n\t\t      __be32 daddr, struct rtable *rt, int is_frag);\n\nint __ip_options_echo(struct net *net, struct ip_options *dopt,\n\t\t      struct sk_buff *skb, const struct ip_options *sopt);\nstatic inline int ip_options_echo(struct net *net, struct ip_options *dopt,\n\t\t\t\t  struct sk_buff *skb)\n{\n\treturn __ip_options_echo(net, dopt, skb, &IPCB(skb)->opt);\n}\n\nvoid ip_options_fragment(struct sk_buff *skb);\nint __ip_options_compile(struct net *net, struct ip_options *opt,\n\t\t\t struct sk_buff *skb, __be32 *info);\nint ip_options_compile(struct net *net, struct ip_options *opt,\n\t\t       struct sk_buff *skb);\nint ip_options_get(struct net *net, struct ip_options_rcu **optp,\n\t\t   sockptr_t data, int optlen);\nvoid ip_options_undo(struct ip_options *opt);\nvoid ip_forward_options(struct sk_buff *skb);\nint ip_options_rcv_srr(struct sk_buff *skb, struct net_device *dev);\n\n/*\n *\tFunctions provided by ip_sockglue.c\n */\n\nvoid ipv4_pktinfo_prepare(const struct sock *sk, struct sk_buff *skb);\nvoid ip_cmsg_recv_offset(struct msghdr *msg, struct sock *sk,\n\t\t\t struct sk_buff *skb, int tlen, int offset);\nint ip_cmsg_send(struct sock *sk, struct msghdr *msg,\n\t\t struct ipcm_cookie *ipc, bool allow_ipv6);\nint ip_setsockopt(struct sock *sk, int level, int optname, sockptr_t optval,\n\t\t  unsigned int optlen);\nint ip_getsockopt(struct sock *sk, int level, int optname, char __user *optval,\n\t\t  int __user *optlen);\nint ip_ra_control(struct sock *sk, unsigned char on,\n\t\t  void (*destructor)(struct sock *));\n\nint ip_recv_error(struct sock *sk, struct msghdr *msg, int len, int *addr_len);\nvoid ip_icmp_error(struct sock *sk, struct sk_buff *skb, int err, __be16 port,\n\t\t   u32 info, u8 *payload);\nvoid ip_local_error(struct sock *sk, int err, __be32 daddr, __be16 dport,\n\t\t    u32 info);\n\nstatic inline void ip_cmsg_recv(struct msghdr *msg, struct sk_buff *skb)\n{\n\tip_cmsg_recv_offset(msg, skb->sk, skb, 0, 0);\n}\n\nbool icmp_global_allow(void);\nextern int sysctl_icmp_msgs_per_sec;\nextern int sysctl_icmp_msgs_burst;\n\n#ifdef CONFIG_PROC_FS\nint ip_misc_proc_init(void);\n#endif\n\nint rtm_getroute_parse_ip_proto(struct nlattr *attr, u8 *ip_proto, u8 family,\n\t\t\t\tstruct netlink_ext_ack *extack);\n\nstatic inline bool inetdev_valid_mtu(unsigned int mtu)\n{\n\treturn likely(mtu >= IPV4_MIN_MTU);\n}\n\nvoid ip_sock_set_freebind(struct sock *sk);\nint ip_sock_set_mtu_discover(struct sock *sk, int val);\nvoid ip_sock_set_pktinfo(struct sock *sk);\nvoid ip_sock_set_recverr(struct sock *sk);\nvoid ip_sock_set_tos(struct sock *sk, int val);\n\n#endif\t/* _IP_H */\n"}, "6": {"id": 6, "path": "/src/include/net/snmp.h", "content": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/*\n *\n *\t\tSNMP MIB entries for the IP subsystem.\n *\t\t\n *\t\tAlan Cox <gw4pts@gw4pts.ampr.org>\n *\n *\t\tWe don't chose to implement SNMP in the kernel (this would\n *\t\tbe silly as SNMP is a pain in the backside in places). We do\n *\t\thowever need to collect the MIB statistics and export them\n *\t\tout of /proc (eventually)\n */\n \n#ifndef _SNMP_H\n#define _SNMP_H\n\n#include <linux/cache.h>\n#include <linux/snmp.h>\n#include <linux/smp.h>\n\n/*\n * Mibs are stored in array of unsigned long.\n */\n/*\n * struct snmp_mib{}\n *  - list of entries for particular API (such as /proc/net/snmp)\n *  - name of entries.\n */\nstruct snmp_mib {\n\tconst char *name;\n\tint entry;\n};\n\n#define SNMP_MIB_ITEM(_name,_entry)\t{\t\\\n\t.name = _name,\t\t\t\t\\\n\t.entry = _entry,\t\t\t\\\n}\n\n#define SNMP_MIB_SENTINEL {\t\\\n\t.name = NULL,\t\t\\\n\t.entry = 0,\t\t\\\n}\n\n/*\n * We use unsigned longs for most mibs but u64 for ipstats.\n */\n#include <linux/u64_stats_sync.h>\n\n/* IPstats */\n#define IPSTATS_MIB_MAX\t__IPSTATS_MIB_MAX\nstruct ipstats_mib {\n\t/* mibs[] must be first field of struct ipstats_mib */\n\tu64\t\tmibs[IPSTATS_MIB_MAX];\n\tstruct u64_stats_sync syncp;\n};\n\n/* ICMP */\n#define ICMP_MIB_MAX\t__ICMP_MIB_MAX\nstruct icmp_mib {\n\tunsigned long\tmibs[ICMP_MIB_MAX];\n};\n\n#define ICMPMSG_MIB_MAX\t__ICMPMSG_MIB_MAX\nstruct icmpmsg_mib {\n\tatomic_long_t\tmibs[ICMPMSG_MIB_MAX];\n};\n\n/* ICMP6 (IPv6-ICMP) */\n#define ICMP6_MIB_MAX\t__ICMP6_MIB_MAX\n/* per network ns counters */\nstruct icmpv6_mib {\n\tunsigned long\tmibs[ICMP6_MIB_MAX];\n};\n/* per device counters, (shared on all cpus) */\nstruct icmpv6_mib_device {\n\tatomic_long_t\tmibs[ICMP6_MIB_MAX];\n};\n\n#define ICMP6MSG_MIB_MAX  __ICMP6MSG_MIB_MAX\n/* per network ns counters */\nstruct icmpv6msg_mib {\n\tatomic_long_t\tmibs[ICMP6MSG_MIB_MAX];\n};\n/* per device counters, (shared on all cpus) */\nstruct icmpv6msg_mib_device {\n\tatomic_long_t\tmibs[ICMP6MSG_MIB_MAX];\n};\n\n\n/* TCP */\n#define TCP_MIB_MAX\t__TCP_MIB_MAX\nstruct tcp_mib {\n\tunsigned long\tmibs[TCP_MIB_MAX];\n};\n\n/* UDP */\n#define UDP_MIB_MAX\t__UDP_MIB_MAX\nstruct udp_mib {\n\tunsigned long\tmibs[UDP_MIB_MAX];\n};\n\n/* Linux */\n#define LINUX_MIB_MAX\t__LINUX_MIB_MAX\nstruct linux_mib {\n\tunsigned long\tmibs[LINUX_MIB_MAX];\n};\n\n/* Linux Xfrm */\n#define LINUX_MIB_XFRMMAX\t__LINUX_MIB_XFRMMAX\nstruct linux_xfrm_mib {\n\tunsigned long\tmibs[LINUX_MIB_XFRMMAX];\n};\n\n/* Linux TLS */\n#define LINUX_MIB_TLSMAX\t__LINUX_MIB_TLSMAX\nstruct linux_tls_mib {\n\tunsigned long\tmibs[LINUX_MIB_TLSMAX];\n};\n\n#define DEFINE_SNMP_STAT(type, name)\t\\\n\t__typeof__(type) __percpu *name\n#define DEFINE_SNMP_STAT_ATOMIC(type, name)\t\\\n\t__typeof__(type) *name\n#define DECLARE_SNMP_STAT(type, name)\t\\\n\textern __typeof__(type) __percpu *name\n\n#define __SNMP_INC_STATS(mib, field)\t\\\n\t\t\t__this_cpu_inc(mib->mibs[field])\n\n#define SNMP_INC_STATS_ATOMIC_LONG(mib, field)\t\\\n\t\t\tatomic_long_inc(&mib->mibs[field])\n\n#define SNMP_INC_STATS(mib, field)\t\\\n\t\t\tthis_cpu_inc(mib->mibs[field])\n\n#define SNMP_DEC_STATS(mib, field)\t\\\n\t\t\tthis_cpu_dec(mib->mibs[field])\n\n#define __SNMP_ADD_STATS(mib, field, addend)\t\\\n\t\t\t__this_cpu_add(mib->mibs[field], addend)\n\n#define SNMP_ADD_STATS(mib, field, addend)\t\\\n\t\t\tthis_cpu_add(mib->mibs[field], addend)\n#define SNMP_UPD_PO_STATS(mib, basefield, addend)\t\\\n\tdo { \\\n\t\t__typeof__((mib->mibs) + 0) ptr = mib->mibs;\t\\\n\t\tthis_cpu_inc(ptr[basefield##PKTS]);\t\t\\\n\t\tthis_cpu_add(ptr[basefield##OCTETS], addend);\t\\\n\t} while (0)\n#define __SNMP_UPD_PO_STATS(mib, basefield, addend)\t\\\n\tdo { \\\n\t\t__typeof__((mib->mibs) + 0) ptr = mib->mibs;\t\\\n\t\t__this_cpu_inc(ptr[basefield##PKTS]);\t\t\\\n\t\t__this_cpu_add(ptr[basefield##OCTETS], addend);\t\\\n\t} while (0)\n\n\n#if BITS_PER_LONG==32\n\n#define __SNMP_ADD_STATS64(mib, field, addend) \t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\t__typeof__(*mib) *ptr = raw_cpu_ptr(mib);\t\t\\\n\t\tu64_stats_update_begin(&ptr->syncp);\t\t\t\\\n\t\tptr->mibs[field] += addend;\t\t\t\t\\\n\t\tu64_stats_update_end(&ptr->syncp);\t\t\t\\\n\t} while (0)\n\n#define SNMP_ADD_STATS64(mib, field, addend) \t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tlocal_bh_disable();\t\t\t\t\t\\\n\t\t__SNMP_ADD_STATS64(mib, field, addend);\t\t\t\\\n\t\tlocal_bh_enable();\t\t\t\t\\\n\t} while (0)\n\n#define __SNMP_INC_STATS64(mib, field) SNMP_ADD_STATS64(mib, field, 1)\n#define SNMP_INC_STATS64(mib, field) SNMP_ADD_STATS64(mib, field, 1)\n#define __SNMP_UPD_PO_STATS64(mib, basefield, addend)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\t__typeof__(*mib) *ptr;\t\t\t\t\\\n\t\tptr = raw_cpu_ptr((mib));\t\t\t\t\\\n\t\tu64_stats_update_begin(&ptr->syncp);\t\t\t\\\n\t\tptr->mibs[basefield##PKTS]++;\t\t\t\t\\\n\t\tptr->mibs[basefield##OCTETS] += addend;\t\t\t\\\n\t\tu64_stats_update_end(&ptr->syncp);\t\t\t\\\n\t} while (0)\n#define SNMP_UPD_PO_STATS64(mib, basefield, addend)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tlocal_bh_disable();\t\t\t\t\t\\\n\t\t__SNMP_UPD_PO_STATS64(mib, basefield, addend);\t\t\\\n\t\tlocal_bh_enable();\t\t\t\t\\\n\t} while (0)\n#else\n#define __SNMP_INC_STATS64(mib, field)\t\t__SNMP_INC_STATS(mib, field)\n#define SNMP_INC_STATS64(mib, field)\t\tSNMP_INC_STATS(mib, field)\n#define SNMP_DEC_STATS64(mib, field)\t\tSNMP_DEC_STATS(mib, field)\n#define __SNMP_ADD_STATS64(mib, field, addend)\t__SNMP_ADD_STATS(mib, field, addend)\n#define SNMP_ADD_STATS64(mib, field, addend)\tSNMP_ADD_STATS(mib, field, addend)\n#define SNMP_UPD_PO_STATS64(mib, basefield, addend) SNMP_UPD_PO_STATS(mib, basefield, addend)\n#define __SNMP_UPD_PO_STATS64(mib, basefield, addend) __SNMP_UPD_PO_STATS(mib, basefield, addend)\n#endif\n\n#endif\n"}, "7": {"id": 7, "path": "/src/include/linux/percpu-defs.h", "content": "/* SPDX-License-Identifier: GPL-2.0-only */\n/*\n * linux/percpu-defs.h - basic definitions for percpu areas\n *\n * DO NOT INCLUDE DIRECTLY OUTSIDE PERCPU IMPLEMENTATION PROPER.\n *\n * This file is separate from linux/percpu.h to avoid cyclic inclusion\n * dependency from arch header files.  Only to be included from\n * asm/percpu.h.\n *\n * This file includes macros necessary to declare percpu sections and\n * variables, and definitions of percpu accessors and operations.  It\n * should provide enough percpu features to arch header files even when\n * they can only include asm/percpu.h to avoid cyclic inclusion dependency.\n */\n\n#ifndef _LINUX_PERCPU_DEFS_H\n#define _LINUX_PERCPU_DEFS_H\n\n#ifdef CONFIG_SMP\n\n#ifdef MODULE\n#define PER_CPU_SHARED_ALIGNED_SECTION \"\"\n#define PER_CPU_ALIGNED_SECTION \"\"\n#else\n#define PER_CPU_SHARED_ALIGNED_SECTION \"..shared_aligned\"\n#define PER_CPU_ALIGNED_SECTION \"..shared_aligned\"\n#endif\n#define PER_CPU_FIRST_SECTION \"..first\"\n\n#else\n\n#define PER_CPU_SHARED_ALIGNED_SECTION \"\"\n#define PER_CPU_ALIGNED_SECTION \"..shared_aligned\"\n#define PER_CPU_FIRST_SECTION \"\"\n\n#endif\n\n/*\n * Base implementations of per-CPU variable declarations and definitions, where\n * the section in which the variable is to be placed is provided by the\n * 'sec' argument.  This may be used to affect the parameters governing the\n * variable's storage.\n *\n * NOTE!  The sections for the DECLARE and for the DEFINE must match, lest\n * linkage errors occur due the compiler generating the wrong code to access\n * that section.\n */\n#define __PCPU_ATTRS(sec)\t\t\t\t\t\t\\\n\t__percpu __attribute__((section(PER_CPU_BASE_SECTION sec)))\t\\\n\tPER_CPU_ATTRIBUTES\n\n#define __PCPU_DUMMY_ATTRS\t\t\t\t\t\t\\\n\t__attribute__((section(\".discard\"), unused))\n\n/*\n * s390 and alpha modules require percpu variables to be defined as\n * weak to force the compiler to generate GOT based external\n * references for them.  This is necessary because percpu sections\n * will be located outside of the usually addressable area.\n *\n * This definition puts the following two extra restrictions when\n * defining percpu variables.\n *\n * 1. The symbol must be globally unique, even the static ones.\n * 2. Static percpu variables cannot be defined inside a function.\n *\n * Archs which need weak percpu definitions should define\n * ARCH_NEEDS_WEAK_PER_CPU in asm/percpu.h when necessary.\n *\n * To ensure that the generic code observes the above two\n * restrictions, if CONFIG_DEBUG_FORCE_WEAK_PER_CPU is set weak\n * definition is used for all cases.\n */\n#if defined(ARCH_NEEDS_WEAK_PER_CPU) || defined(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)\n/*\n * __pcpu_scope_* dummy variable is used to enforce scope.  It\n * receives the static modifier when it's used in front of\n * DEFINE_PER_CPU() and will trigger build failure if\n * DECLARE_PER_CPU() is used for the same variable.\n *\n * __pcpu_unique_* dummy variable is used to enforce symbol uniqueness\n * such that hidden weak symbol collision, which will cause unrelated\n * variables to share the same address, can be detected during build.\n */\n#define DECLARE_PER_CPU_SECTION(type, name, sec)\t\t\t\\\n\textern __PCPU_DUMMY_ATTRS char __pcpu_scope_##name;\t\t\\\n\textern __PCPU_ATTRS(sec) __typeof__(type) name\n\n#define DEFINE_PER_CPU_SECTION(type, name, sec)\t\t\t\t\\\n\t__PCPU_DUMMY_ATTRS char __pcpu_scope_##name;\t\t\t\\\n\textern __PCPU_DUMMY_ATTRS char __pcpu_unique_##name;\t\t\\\n\t__PCPU_DUMMY_ATTRS char __pcpu_unique_##name;\t\t\t\\\n\textern __PCPU_ATTRS(sec) __typeof__(type) name;\t\t\t\\\n\t__PCPU_ATTRS(sec) __weak __typeof__(type) name\n#else\n/*\n * Normal declaration and definition macros.\n */\n#define DECLARE_PER_CPU_SECTION(type, name, sec)\t\t\t\\\n\textern __PCPU_ATTRS(sec) __typeof__(type) name\n\n#define DEFINE_PER_CPU_SECTION(type, name, sec)\t\t\t\t\\\n\t__PCPU_ATTRS(sec) __typeof__(type) name\n#endif\n\n/*\n * Variant on the per-CPU variable declaration/definition theme used for\n * ordinary per-CPU variables.\n */\n#define DECLARE_PER_CPU(type, name)\t\t\t\t\t\\\n\tDECLARE_PER_CPU_SECTION(type, name, \"\")\n\n#define DEFINE_PER_CPU(type, name)\t\t\t\t\t\\\n\tDEFINE_PER_CPU_SECTION(type, name, \"\")\n\n/*\n * Declaration/definition used for per-CPU variables that must come first in\n * the set of variables.\n */\n#define DECLARE_PER_CPU_FIRST(type, name)\t\t\t\t\\\n\tDECLARE_PER_CPU_SECTION(type, name, PER_CPU_FIRST_SECTION)\n\n#define DEFINE_PER_CPU_FIRST(type, name)\t\t\t\t\\\n\tDEFINE_PER_CPU_SECTION(type, name, PER_CPU_FIRST_SECTION)\n\n/*\n * Declaration/definition used for per-CPU variables that must be cacheline\n * aligned under SMP conditions so that, whilst a particular instance of the\n * data corresponds to a particular CPU, inefficiencies due to direct access by\n * other CPUs are reduced by preventing the data from unnecessarily spanning\n * cachelines.\n *\n * An example of this would be statistical data, where each CPU's set of data\n * is updated by that CPU alone, but the data from across all CPUs is collated\n * by a CPU processing a read from a proc file.\n */\n#define DECLARE_PER_CPU_SHARED_ALIGNED(type, name)\t\t\t\\\n\tDECLARE_PER_CPU_SECTION(type, name, PER_CPU_SHARED_ALIGNED_SECTION) \\\n\t____cacheline_aligned_in_smp\n\n#define DEFINE_PER_CPU_SHARED_ALIGNED(type, name)\t\t\t\\\n\tDEFINE_PER_CPU_SECTION(type, name, PER_CPU_SHARED_ALIGNED_SECTION) \\\n\t____cacheline_aligned_in_smp\n\n#define DECLARE_PER_CPU_ALIGNED(type, name)\t\t\t\t\\\n\tDECLARE_PER_CPU_SECTION(type, name, PER_CPU_ALIGNED_SECTION)\t\\\n\t____cacheline_aligned\n\n#define DEFINE_PER_CPU_ALIGNED(type, name)\t\t\t\t\\\n\tDEFINE_PER_CPU_SECTION(type, name, PER_CPU_ALIGNED_SECTION)\t\\\n\t____cacheline_aligned\n\n/*\n * Declaration/definition used for per-CPU variables that must be page aligned.\n */\n#define DECLARE_PER_CPU_PAGE_ALIGNED(type, name)\t\t\t\\\n\tDECLARE_PER_CPU_SECTION(type, name, \"..page_aligned\")\t\t\\\n\t__aligned(PAGE_SIZE)\n\n#define DEFINE_PER_CPU_PAGE_ALIGNED(type, name)\t\t\t\t\\\n\tDEFINE_PER_CPU_SECTION(type, name, \"..page_aligned\")\t\t\\\n\t__aligned(PAGE_SIZE)\n\n/*\n * Declaration/definition used for per-CPU variables that must be read mostly.\n */\n#define DECLARE_PER_CPU_READ_MOSTLY(type, name)\t\t\t\\\n\tDECLARE_PER_CPU_SECTION(type, name, \"..read_mostly\")\n\n#define DEFINE_PER_CPU_READ_MOSTLY(type, name)\t\t\t\t\\\n\tDEFINE_PER_CPU_SECTION(type, name, \"..read_mostly\")\n\n/*\n * Declaration/definition used for per-CPU variables that should be accessed\n * as decrypted when memory encryption is enabled in the guest.\n */\n#ifdef CONFIG_AMD_MEM_ENCRYPT\n#define DECLARE_PER_CPU_DECRYPTED(type, name)\t\t\t\t\\\n\tDECLARE_PER_CPU_SECTION(type, name, \"..decrypted\")\n\n#define DEFINE_PER_CPU_DECRYPTED(type, name)\t\t\t\t\\\n\tDEFINE_PER_CPU_SECTION(type, name, \"..decrypted\")\n#else\n#define DEFINE_PER_CPU_DECRYPTED(type, name)\tDEFINE_PER_CPU(type, name)\n#endif\n\n/*\n * Intermodule exports for per-CPU variables.  sparse forgets about\n * address space across EXPORT_SYMBOL(), change EXPORT_SYMBOL() to\n * noop if __CHECKER__.\n */\n#ifndef __CHECKER__\n#define EXPORT_PER_CPU_SYMBOL(var) EXPORT_SYMBOL(var)\n#define EXPORT_PER_CPU_SYMBOL_GPL(var) EXPORT_SYMBOL_GPL(var)\n#else\n#define EXPORT_PER_CPU_SYMBOL(var)\n#define EXPORT_PER_CPU_SYMBOL_GPL(var)\n#endif\n\n/*\n * Accessors and operations.\n */\n#ifndef __ASSEMBLY__\n\n/*\n * __verify_pcpu_ptr() verifies @ptr is a percpu pointer without evaluating\n * @ptr and is invoked once before a percpu area is accessed by all\n * accessors and operations.  This is performed in the generic part of\n * percpu and arch overrides don't need to worry about it; however, if an\n * arch wants to implement an arch-specific percpu accessor or operation,\n * it may use __verify_pcpu_ptr() to verify the parameters.\n *\n * + 0 is required in order to convert the pointer type from a\n * potential array type to a pointer to a single item of the array.\n */\n#define __verify_pcpu_ptr(ptr)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tconst void __percpu *__vpp_verify = (typeof((ptr) + 0))NULL;\t\\\n\t(void)__vpp_verify;\t\t\t\t\t\t\\\n} while (0)\n\n#ifdef CONFIG_SMP\n\n/*\n * Add an offset to a pointer but keep the pointer as-is.  Use RELOC_HIDE()\n * to prevent the compiler from making incorrect assumptions about the\n * pointer value.  The weird cast keeps both GCC and sparse happy.\n */\n#define SHIFT_PERCPU_PTR(__p, __offset)\t\t\t\t\t\\\n\tRELOC_HIDE((typeof(*(__p)) __kernel __force *)(__p), (__offset))\n\n#define per_cpu_ptr(ptr, cpu)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__verify_pcpu_ptr(ptr);\t\t\t\t\t\t\\\n\tSHIFT_PERCPU_PTR((ptr), per_cpu_offset((cpu)));\t\t\t\\\n})\n\n#define raw_cpu_ptr(ptr)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__verify_pcpu_ptr(ptr);\t\t\t\t\t\t\\\n\tarch_raw_cpu_ptr(ptr);\t\t\t\t\t\t\\\n})\n\n#ifdef CONFIG_DEBUG_PREEMPT\n#define this_cpu_ptr(ptr)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__verify_pcpu_ptr(ptr);\t\t\t\t\t\t\\\n\tSHIFT_PERCPU_PTR(ptr, my_cpu_offset);\t\t\t\t\\\n})\n#else\n#define this_cpu_ptr(ptr) raw_cpu_ptr(ptr)\n#endif\n\n#else\t/* CONFIG_SMP */\n\n#define VERIFY_PERCPU_PTR(__p)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__verify_pcpu_ptr(__p);\t\t\t\t\t\t\\\n\t(typeof(*(__p)) __kernel __force *)(__p);\t\t\t\\\n})\n\n#define per_cpu_ptr(ptr, cpu)\t({ (void)(cpu); VERIFY_PERCPU_PTR(ptr); })\n#define raw_cpu_ptr(ptr)\tper_cpu_ptr(ptr, 0)\n#define this_cpu_ptr(ptr)\traw_cpu_ptr(ptr)\n\n#endif\t/* CONFIG_SMP */\n\n#define per_cpu(var, cpu)\t(*per_cpu_ptr(&(var), cpu))\n\n/*\n * Must be an lvalue. Since @var must be a simple identifier,\n * we force a syntax error here if it isn't.\n */\n#define get_cpu_var(var)\t\t\t\t\t\t\\\n(*({\t\t\t\t\t\t\t\t\t\\\n\tpreempt_disable();\t\t\t\t\t\t\\\n\tthis_cpu_ptr(&var);\t\t\t\t\t\t\\\n}))\n\n/*\n * The weird & is necessary because sparse considers (void)(var) to be\n * a direct dereference of percpu variable (var).\n */\n#define put_cpu_var(var)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t(void)&(var);\t\t\t\t\t\t\t\\\n\tpreempt_enable();\t\t\t\t\t\t\\\n} while (0)\n\n#define get_cpu_ptr(var)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tpreempt_disable();\t\t\t\t\t\t\\\n\tthis_cpu_ptr(var);\t\t\t\t\t\t\\\n})\n\n#define put_cpu_ptr(var)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t(void)(var);\t\t\t\t\t\t\t\\\n\tpreempt_enable();\t\t\t\t\t\t\\\n} while (0)\n\n/*\n * Branching function to split up a function into a set of functions that\n * are called for different scalar sizes of the objects handled.\n */\n\nextern void __bad_size_call_parameter(void);\n\n#ifdef CONFIG_DEBUG_PREEMPT\nextern void __this_cpu_preempt_check(const char *op);\n#else\nstatic inline void __this_cpu_preempt_check(const char *op) { }\n#endif\n\n#define __pcpu_size_call_return(stem, variable)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\ttypeof(variable) pscr_ret__;\t\t\t\t\t\\\n\t__verify_pcpu_ptr(&(variable));\t\t\t\t\t\\\n\tswitch(sizeof(variable)) {\t\t\t\t\t\\\n\tcase 1: pscr_ret__ = stem##1(variable); break;\t\t\t\\\n\tcase 2: pscr_ret__ = stem##2(variable); break;\t\t\t\\\n\tcase 4: pscr_ret__ = stem##4(variable); break;\t\t\t\\\n\tcase 8: pscr_ret__ = stem##8(variable); break;\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t__bad_size_call_parameter(); break;\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tpscr_ret__;\t\t\t\t\t\t\t\\\n})\n\n#define __pcpu_size_call_return2(stem, variable, ...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\ttypeof(variable) pscr2_ret__;\t\t\t\t\t\\\n\t__verify_pcpu_ptr(&(variable));\t\t\t\t\t\\\n\tswitch(sizeof(variable)) {\t\t\t\t\t\\\n\tcase 1: pscr2_ret__ = stem##1(variable, __VA_ARGS__); break;\t\\\n\tcase 2: pscr2_ret__ = stem##2(variable, __VA_ARGS__); break;\t\\\n\tcase 4: pscr2_ret__ = stem##4(variable, __VA_ARGS__); break;\t\\\n\tcase 8: pscr2_ret__ = stem##8(variable, __VA_ARGS__); break;\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t__bad_size_call_parameter(); break;\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tpscr2_ret__;\t\t\t\t\t\t\t\\\n})\n\n/*\n * Special handling for cmpxchg_double.  cmpxchg_double is passed two\n * percpu variables.  The first has to be aligned to a double word\n * boundary and the second has to follow directly thereafter.\n * We enforce this on all architectures even if they don't support\n * a double cmpxchg instruction, since it's a cheap requirement, and it\n * avoids breaking the requirement for architectures with the instruction.\n */\n#define __pcpu_double_call_return_bool(stem, pcp1, pcp2, ...)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tbool pdcrb_ret__;\t\t\t\t\t\t\\\n\t__verify_pcpu_ptr(&(pcp1));\t\t\t\t\t\\\n\tBUILD_BUG_ON(sizeof(pcp1) != sizeof(pcp2));\t\t\t\\\n\tVM_BUG_ON((unsigned long)(&(pcp1)) % (2 * sizeof(pcp1)));\t\\\n\tVM_BUG_ON((unsigned long)(&(pcp2)) !=\t\t\t\t\\\n\t\t  (unsigned long)(&(pcp1)) + sizeof(pcp1));\t\t\\\n\tswitch(sizeof(pcp1)) {\t\t\t\t\t\t\\\n\tcase 1: pdcrb_ret__ = stem##1(pcp1, pcp2, __VA_ARGS__); break;\t\\\n\tcase 2: pdcrb_ret__ = stem##2(pcp1, pcp2, __VA_ARGS__); break;\t\\\n\tcase 4: pdcrb_ret__ = stem##4(pcp1, pcp2, __VA_ARGS__); break;\t\\\n\tcase 8: pdcrb_ret__ = stem##8(pcp1, pcp2, __VA_ARGS__); break;\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t__bad_size_call_parameter(); break;\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tpdcrb_ret__;\t\t\t\t\t\t\t\\\n})\n\n#define __pcpu_size_call(stem, variable, ...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t__verify_pcpu_ptr(&(variable));\t\t\t\t\t\\\n\tswitch(sizeof(variable)) {\t\t\t\t\t\\\n\t\tcase 1: stem##1(variable, __VA_ARGS__);break;\t\t\\\n\t\tcase 2: stem##2(variable, __VA_ARGS__);break;\t\t\\\n\t\tcase 4: stem##4(variable, __VA_ARGS__);break;\t\t\\\n\t\tcase 8: stem##8(variable, __VA_ARGS__);break;\t\t\\\n\t\tdefault: \t\t\t\t\t\t\\\n\t\t\t__bad_size_call_parameter();break;\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n/*\n * this_cpu operations (C) 2008-2013 Christoph Lameter <cl@linux.com>\n *\n * Optimized manipulation for memory allocated through the per cpu\n * allocator or for addresses of per cpu variables.\n *\n * These operation guarantee exclusivity of access for other operations\n * on the *same* processor. The assumption is that per cpu data is only\n * accessed by a single processor instance (the current one).\n *\n * The arch code can provide optimized implementation by defining macros\n * for certain scalar sizes. F.e. provide this_cpu_add_2() to provide per\n * cpu atomic operations for 2 byte sized RMW actions. If arch code does\n * not provide operations for a scalar size then the fallback in the\n * generic code will be used.\n *\n * cmpxchg_double replaces two adjacent scalars at once.  The first two\n * parameters are per cpu variables which have to be of the same size.  A\n * truth value is returned to indicate success or failure (since a double\n * register result is difficult to handle).  There is very limited hardware\n * support for these operations, so only certain sizes may work.\n */\n\n/*\n * Operations for contexts where we do not want to do any checks for\n * preemptions.  Unless strictly necessary, always use [__]this_cpu_*()\n * instead.\n *\n * If there is no other protection through preempt disable and/or disabling\n * interupts then one of these RMW operations can show unexpected behavior\n * because the execution thread was rescheduled on another processor or an\n * interrupt occurred and the same percpu variable was modified from the\n * interrupt context.\n */\n#define raw_cpu_read(pcp)\t\t__pcpu_size_call_return(raw_cpu_read_, pcp)\n#define raw_cpu_write(pcp, val)\t\t__pcpu_size_call(raw_cpu_write_, pcp, val)\n#define raw_cpu_add(pcp, val)\t\t__pcpu_size_call(raw_cpu_add_, pcp, val)\n#define raw_cpu_and(pcp, val)\t\t__pcpu_size_call(raw_cpu_and_, pcp, val)\n#define raw_cpu_or(pcp, val)\t\t__pcpu_size_call(raw_cpu_or_, pcp, val)\n#define raw_cpu_add_return(pcp, val)\t__pcpu_size_call_return2(raw_cpu_add_return_, pcp, val)\n#define raw_cpu_xchg(pcp, nval)\t\t__pcpu_size_call_return2(raw_cpu_xchg_, pcp, nval)\n#define raw_cpu_cmpxchg(pcp, oval, nval) \\\n\t__pcpu_size_call_return2(raw_cpu_cmpxchg_, pcp, oval, nval)\n#define raw_cpu_cmpxchg_double(pcp1, pcp2, oval1, oval2, nval1, nval2) \\\n\t__pcpu_double_call_return_bool(raw_cpu_cmpxchg_double_, pcp1, pcp2, oval1, oval2, nval1, nval2)\n\n#define raw_cpu_sub(pcp, val)\t\traw_cpu_add(pcp, -(val))\n#define raw_cpu_inc(pcp)\t\traw_cpu_add(pcp, 1)\n#define raw_cpu_dec(pcp)\t\traw_cpu_sub(pcp, 1)\n#define raw_cpu_sub_return(pcp, val)\traw_cpu_add_return(pcp, -(typeof(pcp))(val))\n#define raw_cpu_inc_return(pcp)\t\traw_cpu_add_return(pcp, 1)\n#define raw_cpu_dec_return(pcp)\t\traw_cpu_add_return(pcp, -1)\n\n/*\n * Operations for contexts that are safe from preemption/interrupts.  These\n * operations verify that preemption is disabled.\n */\n#define __this_cpu_read(pcp)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"read\");\t\t\t\t\\\n\traw_cpu_read(pcp);\t\t\t\t\t\t\\\n})\n\n#define __this_cpu_write(pcp, val)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"write\");\t\t\t\t\\\n\traw_cpu_write(pcp, val);\t\t\t\t\t\\\n})\n\n#define __this_cpu_add(pcp, val)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"add\");\t\t\t\t\\\n\traw_cpu_add(pcp, val);\t\t\t\t\t\t\\\n})\n\n#define __this_cpu_and(pcp, val)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"and\");\t\t\t\t\\\n\traw_cpu_and(pcp, val);\t\t\t\t\t\t\\\n})\n\n#define __this_cpu_or(pcp, val)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"or\");\t\t\t\t\t\\\n\traw_cpu_or(pcp, val);\t\t\t\t\t\t\\\n})\n\n#define __this_cpu_add_return(pcp, val)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"add_return\");\t\t\t\t\\\n\traw_cpu_add_return(pcp, val);\t\t\t\t\t\\\n})\n\n#define __this_cpu_xchg(pcp, nval)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"xchg\");\t\t\t\t\\\n\traw_cpu_xchg(pcp, nval);\t\t\t\t\t\\\n})\n\n#define __this_cpu_cmpxchg(pcp, oval, nval)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"cmpxchg\");\t\t\t\t\\\n\traw_cpu_cmpxchg(pcp, oval, nval);\t\t\t\t\\\n})\n\n#define __this_cpu_cmpxchg_double(pcp1, pcp2, oval1, oval2, nval1, nval2) \\\n({\t__this_cpu_preempt_check(\"cmpxchg_double\");\t\t\t\\\n\traw_cpu_cmpxchg_double(pcp1, pcp2, oval1, oval2, nval1, nval2);\t\\\n})\n\n#define __this_cpu_sub(pcp, val)\t__this_cpu_add(pcp, -(typeof(pcp))(val))\n#define __this_cpu_inc(pcp)\t\t__this_cpu_add(pcp, 1)\n#define __this_cpu_dec(pcp)\t\t__this_cpu_sub(pcp, 1)\n#define __this_cpu_sub_return(pcp, val)\t__this_cpu_add_return(pcp, -(typeof(pcp))(val))\n#define __this_cpu_inc_return(pcp)\t__this_cpu_add_return(pcp, 1)\n#define __this_cpu_dec_return(pcp)\t__this_cpu_add_return(pcp, -1)\n\n/*\n * Operations with implied preemption/interrupt protection.  These\n * operations can be used without worrying about preemption or interrupt.\n */\n#define this_cpu_read(pcp)\t\t__pcpu_size_call_return(this_cpu_read_, pcp)\n#define this_cpu_write(pcp, val)\t__pcpu_size_call(this_cpu_write_, pcp, val)\n#define this_cpu_add(pcp, val)\t\t__pcpu_size_call(this_cpu_add_, pcp, val)\n#define this_cpu_and(pcp, val)\t\t__pcpu_size_call(this_cpu_and_, pcp, val)\n#define this_cpu_or(pcp, val)\t\t__pcpu_size_call(this_cpu_or_, pcp, val)\n#define this_cpu_add_return(pcp, val)\t__pcpu_size_call_return2(this_cpu_add_return_, pcp, val)\n#define this_cpu_xchg(pcp, nval)\t__pcpu_size_call_return2(this_cpu_xchg_, pcp, nval)\n#define this_cpu_cmpxchg(pcp, oval, nval) \\\n\t__pcpu_size_call_return2(this_cpu_cmpxchg_, pcp, oval, nval)\n#define this_cpu_cmpxchg_double(pcp1, pcp2, oval1, oval2, nval1, nval2) \\\n\t__pcpu_double_call_return_bool(this_cpu_cmpxchg_double_, pcp1, pcp2, oval1, oval2, nval1, nval2)\n\n#define this_cpu_sub(pcp, val)\t\tthis_cpu_add(pcp, -(typeof(pcp))(val))\n#define this_cpu_inc(pcp)\t\tthis_cpu_add(pcp, 1)\n#define this_cpu_dec(pcp)\t\tthis_cpu_sub(pcp, 1)\n#define this_cpu_sub_return(pcp, val)\tthis_cpu_add_return(pcp, -(typeof(pcp))(val))\n#define this_cpu_inc_return(pcp)\tthis_cpu_add_return(pcp, 1)\n#define this_cpu_dec_return(pcp)\tthis_cpu_add_return(pcp, -1)\n\n#endif /* __ASSEMBLY__ */\n#endif /* _LINUX_PERCPU_DEFS_H */\n"}}, "reports": [{"events": [{"location": {"col": 3, "file": 0, "line": 3340}, "message": "Value stored to 'icsk' is never read"}, {"location": {"col": 3, "file": 0, "line": 3340}, "message": "Value stored to 'icsk' is never read"}], "macros": [], "notes": [], "path": "/src/net/ipv4/tcp_input.c", "reportHash": "5fa854cd0d593ca7ee8d25fc519b9fa6", "checkerName": "clang-analyzer-deadcode.DeadStores", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 23, "file": 1, "line": 3442}, "message": "expanded from macro 'rb_to_skb'"}, {"location": {"col": 2, "file": 2, "line": 88}, "message": "expanded from macro 'rb_entry_safe'"}, {"location": {"col": 6, "file": 0, "line": 5241}, "message": "Left side of '||' is false"}, {"location": {"col": 31, "file": 2, "line": 40}, "message": "expanded from macro 'RB_EMPTY_ROOT'"}, {"location": {"col": 2, "file": 3, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 21, "file": 3, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 4, "line": 281}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 6, "file": 0, "line": 5241}, "message": "Left side of '||' is false"}, {"location": {"col": 31, "file": 2, "line": 40}, "message": "expanded from macro 'RB_EMPTY_ROOT'"}, {"location": {"col": 2, "file": 3, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 21, "file": 3, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 4, "line": 281}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 6, "file": 0, "line": 5241}, "message": "Left side of '||' is false"}, {"location": {"col": 31, "file": 2, "line": 40}, "message": "expanded from macro 'RB_EMPTY_ROOT'"}, {"location": {"col": 2, "file": 3, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 21, "file": 3, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 4, "line": 281}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 6, "file": 0, "line": 5241}, "message": "Left side of '||' is true"}, {"location": {"col": 31, "file": 2, "line": 40}, "message": "expanded from macro 'RB_EMPTY_ROOT'"}, {"location": {"col": 2, "file": 3, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 38, "file": 3, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 6, "file": 0, "line": 5241}, "message": "Taking false branch"}, {"location": {"col": 31, "file": 2, "line": 40}, "message": "expanded from macro 'RB_EMPTY_ROOT'"}, {"location": {"col": 2, "file": 3, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 2, "file": 3, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 4, "line": 319}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 4, "line": 307}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 4, "line": 299}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 6, "file": 0, "line": 5241}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 31, "file": 2, "line": 40}, "message": "expanded from macro 'RB_EMPTY_ROOT'"}, {"location": {"col": 2, "file": 3, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 2, "file": 3, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 4, "line": 319}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 4, "line": 307}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 4, "line": 297}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 6, "file": 0, "line": 5241}, "message": "Assuming the condition is false"}, {"location": {"col": 31, "file": 2, "line": 40}, "message": "expanded from macro 'RB_EMPTY_ROOT'"}, {"location": {"col": 28, "file": 3, "line": 47}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 2, "file": 0, "line": 5241}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 0, "line": 5244}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 35, "file": 5, "line": 288}, "message": "expanded from macro 'NET_INC_STATS'"}, {"location": {"col": 4, "file": 6, "line": 134}, "message": "expanded from macro 'SNMP_INC_STATS'"}, {"location": {"col": 28, "file": 7, "line": 520}, "message": "expanded from macro 'this_cpu_inc'"}, {"location": {"col": 33, "file": 7, "line": 509}, "message": "expanded from macro 'this_cpu_add'"}, {"location": {"col": 2, "file": 7, "line": 375}, "message": "expanded from macro '__pcpu_size_call'"}, {"location": {"col": 37, "file": 7, "line": 217}, "message": "expanded from macro '__verify_pcpu_ptr'"}, {"location": {"col": 2, "file": 0, "line": 5244}, "message": "Control jumps to 'case 8:'  at line 5244"}, {"location": {"col": 35, "file": 5, "line": 288}, "message": "expanded from macro 'NET_INC_STATS'"}, {"location": {"col": 4, "file": 6, "line": 134}, "message": "expanded from macro 'SNMP_INC_STATS'"}, {"location": {"col": 28, "file": 7, "line": 520}, "message": "expanded from macro 'this_cpu_inc'"}, {"location": {"col": 33, "file": 7, "line": 509}, "message": "expanded from macro 'this_cpu_add'"}, {"location": {"col": 2, "file": 7, "line": 376}, "message": "expanded from macro '__pcpu_size_call'"}, {"location": {"col": 2, "file": 0, "line": 5244}, "message": "Left side of '&&' is true"}, {"location": {"col": 35, "file": 5, "line": 288}, "message": "expanded from macro 'NET_INC_STATS'"}, {"location": {"col": 4, "file": 6, "line": 134}, "message": "expanded from macro 'SNMP_INC_STATS'"}, {"location": {"col": 28, "file": 7, "line": 520}, "message": "expanded from macro 'this_cpu_inc'"}, {"location": {"col": 11, "file": 0, "line": 5250}, "message": "Access to field 'truesize' results in a dereference of a null pointer"}], "macros": [], "notes": [], "path": "/src/net/ipv4/tcp_input.c", "reportHash": "e10da1198c23ff88fedc576d30006593", "checkerName": "clang-analyzer-core.NullDereference", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 28, "file": 0, "line": 6549}, "message": "Value stored to 'ireq' during its initialization is never read"}, {"location": {"col": 28, "file": 0, "line": 6549}, "message": "Value stored to 'ireq' during its initialization is never read"}], "macros": [], "notes": [], "path": "/src/net/ipv4/tcp_input.c", "reportHash": "cb3b2259d374a489c5292244f254ff37", "checkerName": "clang-analyzer-deadcode.DeadStores", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
