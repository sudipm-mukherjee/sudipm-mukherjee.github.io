<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"1": {"id": 1, "path": "/src/kernel/audit_watch.c", "content": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* audit_watch.c -- watching inodes\n *\n * Copyright 2003-2009 Red Hat, Inc.\n * Copyright 2005 Hewlett-Packard Development Company, L.P.\n * Copyright 2005 IBM Corporation\n */\n\n#include <linux/file.h>\n#include <linux/kernel.h>\n#include <linux/audit.h>\n#include <linux/kthread.h>\n#include <linux/mutex.h>\n#include <linux/fs.h>\n#include <linux/fsnotify_backend.h>\n#include <linux/namei.h>\n#include <linux/netlink.h>\n#include <linux/refcount.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/security.h>\n#include \"audit.h\"\n\n/*\n * Reference counting:\n *\n * audit_parent: lifetime is from audit_init_parent() to receipt of an FS_IGNORED\n * \tevent.  Each audit_watch holds a reference to its associated parent.\n *\n * audit_watch: if added to lists, lifetime is from audit_init_watch() to\n * \taudit_remove_watch().  Additionally, an audit_watch may exist\n * \ttemporarily to assist in searching existing filter data.  Each\n * \taudit_krule holds a reference to its associated watch.\n */\n\nstruct audit_watch {\n\trefcount_t\t\tcount;\t/* reference count */\n\tdev_t\t\t\tdev;\t/* associated superblock device */\n\tchar\t\t\t*path;\t/* insertion path */\n\tunsigned long\t\tino;\t/* associated inode number */\n\tstruct audit_parent\t*parent; /* associated parent */\n\tstruct list_head\twlist;\t/* entry in parent->watches list */\n\tstruct list_head\trules;\t/* anchor for krule->rlist */\n};\n\nstruct audit_parent {\n\tstruct list_head\twatches; /* anchor for audit_watch->wlist */\n\tstruct fsnotify_mark mark; /* fsnotify mark on the inode */\n};\n\n/* fsnotify handle. */\nstatic struct fsnotify_group *audit_watch_group;\n\n/* fsnotify events we care about. */\n#define AUDIT_FS_WATCH (FS_MOVE | FS_CREATE | FS_DELETE | FS_DELETE_SELF |\\\n\t\t\tFS_MOVE_SELF | FS_UNMOUNT)\n\nstatic void audit_free_parent(struct audit_parent *parent)\n{\n\tWARN_ON(!list_empty(&parent->watches));\n\tkfree(parent);\n}\n\nstatic void audit_watch_free_mark(struct fsnotify_mark *entry)\n{\n\tstruct audit_parent *parent;\n\n\tparent = container_of(entry, struct audit_parent, mark);\n\taudit_free_parent(parent);\n}\n\nstatic void audit_get_parent(struct audit_parent *parent)\n{\n\tif (likely(parent))\n\t\tfsnotify_get_mark(&parent->mark);\n}\n\nstatic void audit_put_parent(struct audit_parent *parent)\n{\n\tif (likely(parent))\n\t\tfsnotify_put_mark(&parent->mark);\n}\n\n/*\n * Find and return the audit_parent on the given inode.  If found a reference\n * is taken on this parent.\n */\nstatic inline struct audit_parent *audit_find_parent(struct inode *inode)\n{\n\tstruct audit_parent *parent = NULL;\n\tstruct fsnotify_mark *entry;\n\n\tentry = fsnotify_find_mark(&inode->i_fsnotify_marks, audit_watch_group);\n\tif (entry)\n\t\tparent = container_of(entry, struct audit_parent, mark);\n\n\treturn parent;\n}\n\nvoid audit_get_watch(struct audit_watch *watch)\n{\n\trefcount_inc(&watch->count);\n}\n\nvoid audit_put_watch(struct audit_watch *watch)\n{\n\tif (refcount_dec_and_test(&watch->count)) {\n\t\tWARN_ON(watch->parent);\n\t\tWARN_ON(!list_empty(&watch->rules));\n\t\tkfree(watch->path);\n\t\tkfree(watch);\n\t}\n}\n\nstatic void audit_remove_watch(struct audit_watch *watch)\n{\n\tlist_del(&watch->wlist);\n\taudit_put_parent(watch->parent);\n\twatch->parent = NULL;\n\taudit_put_watch(watch); /* match initial get */\n}\n\nchar *audit_watch_path(struct audit_watch *watch)\n{\n\treturn watch->path;\n}\n\nint audit_watch_compare(struct audit_watch *watch, unsigned long ino, dev_t dev)\n{\n\treturn (watch->ino != AUDIT_INO_UNSET) &&\n\t\t(watch->ino == ino) &&\n\t\t(watch->dev == dev);\n}\n\n/* Initialize a parent watch entry. */\nstatic struct audit_parent *audit_init_parent(struct path *path)\n{\n\tstruct inode *inode = d_backing_inode(path->dentry);\n\tstruct audit_parent *parent;\n\tint ret;\n\n\tparent = kzalloc(sizeof(*parent), GFP_KERNEL);\n\tif (unlikely(!parent))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tINIT_LIST_HEAD(&parent->watches);\n\n\tfsnotify_init_mark(&parent->mark, audit_watch_group);\n\tparent->mark.mask = AUDIT_FS_WATCH;\n\tret = fsnotify_add_inode_mark(&parent->mark, inode, 0);\n\tif (ret < 0) {\n\t\taudit_free_parent(parent);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn parent;\n}\n\n/* Initialize a watch entry. */\nstatic struct audit_watch *audit_init_watch(char *path)\n{\n\tstruct audit_watch *watch;\n\n\twatch = kzalloc(sizeof(*watch), GFP_KERNEL);\n\tif (unlikely(!watch))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tINIT_LIST_HEAD(&watch->rules);\n\trefcount_set(&watch->count, 1);\n\twatch->path = path;\n\twatch->dev = AUDIT_DEV_UNSET;\n\twatch->ino = AUDIT_INO_UNSET;\n\n\treturn watch;\n}\n\n/* Translate a watch string to kernel representation. */\nint audit_to_watch(struct audit_krule *krule, char *path, int len, u32 op)\n{\n\tstruct audit_watch *watch;\n\n\tif (!audit_watch_group)\n\t\treturn -EOPNOTSUPP;\n\n\tif (path[0] != '/' || path[len-1] == '/' ||\n\t    krule->listnr != AUDIT_FILTER_EXIT ||\n\t    op != Audit_equal ||\n\t    krule->inode_f || krule->watch || krule->tree)\n\t\treturn -EINVAL;\n\n\twatch = audit_init_watch(path);\n\tif (IS_ERR(watch))\n\t\treturn PTR_ERR(watch);\n\n\tkrule->watch = watch;\n\n\treturn 0;\n}\n\n/* Duplicate the given audit watch.  The new watch's rules list is initialized\n * to an empty list and wlist is undefined. */\nstatic struct audit_watch *audit_dupe_watch(struct audit_watch *old)\n{\n\tchar *path;\n\tstruct audit_watch *new;\n\n\tpath = kstrdup(old->path, GFP_KERNEL);\n\tif (unlikely(!path))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tnew = audit_init_watch(path);\n\tif (IS_ERR(new)) {\n\t\tkfree(path);\n\t\tgoto out;\n\t}\n\n\tnew->dev = old->dev;\n\tnew->ino = old->ino;\n\taudit_get_parent(old->parent);\n\tnew->parent = old->parent;\n\nout:\n\treturn new;\n}\n\nstatic void audit_watch_log_rule_change(struct audit_krule *r, struct audit_watch *w, char *op)\n{\n\tstruct audit_buffer *ab;\n\n\tif (!audit_enabled)\n\t\treturn;\n\tab = audit_log_start(audit_context(), GFP_NOFS, AUDIT_CONFIG_CHANGE);\n\tif (!ab)\n\t\treturn;\n\taudit_log_session_info(ab);\n\taudit_log_format(ab, \"op=%s path=\", op);\n\taudit_log_untrustedstring(ab, w->path);\n\taudit_log_key(ab, r->filterkey);\n\taudit_log_format(ab, \" list=%d res=1\", r->listnr);\n\taudit_log_end(ab);\n}\n\n/* Update inode info in audit rules based on filesystem event. */\nstatic void audit_update_watch(struct audit_parent *parent,\n\t\t\t       const struct qstr *dname, dev_t dev,\n\t\t\t       unsigned long ino, unsigned invalidating)\n{\n\tstruct audit_watch *owatch, *nwatch, *nextw;\n\tstruct audit_krule *r, *nextr;\n\tstruct audit_entry *oentry, *nentry;\n\n\tmutex_lock(&audit_filter_mutex);\n\t/* Run all of the watches on this parent looking for the one that\n\t * matches the given dname */\n\tlist_for_each_entry_safe(owatch, nextw, &parent->watches, wlist) {\n\t\tif (audit_compare_dname_path(dname, owatch->path,\n\t\t\t\t\t     AUDIT_NAME_FULL))\n\t\t\tcontinue;\n\n\t\t/* If the update involves invalidating rules, do the inode-based\n\t\t * filtering now, so we don't omit records. */\n\t\tif (invalidating && !audit_dummy_context())\n\t\t\taudit_filter_inodes(current, audit_context());\n\n\t\t/* updating ino will likely change which audit_hash_list we\n\t\t * are on so we need a new watch for the new list */\n\t\tnwatch = audit_dupe_watch(owatch);\n\t\tif (IS_ERR(nwatch)) {\n\t\t\tmutex_unlock(&audit_filter_mutex);\n\t\t\taudit_panic(\"error updating watch, skipping\");\n\t\t\treturn;\n\t\t}\n\t\tnwatch->dev = dev;\n\t\tnwatch->ino = ino;\n\n\t\tlist_for_each_entry_safe(r, nextr, &owatch->rules, rlist) {\n\n\t\t\toentry = container_of(r, struct audit_entry, rule);\n\t\t\tlist_del(&oentry->rule.rlist);\n\t\t\tlist_del_rcu(&oentry->list);\n\n\t\t\tnentry = audit_dupe_rule(&oentry->rule);\n\t\t\tif (IS_ERR(nentry)) {\n\t\t\t\tlist_del(&oentry->rule.list);\n\t\t\t\taudit_panic(\"error updating watch, removing\");\n\t\t\t} else {\n\t\t\t\tint h = audit_hash_ino((u32)ino);\n\n\t\t\t\t/*\n\t\t\t\t * nentry->rule.watch == oentry->rule.watch so\n\t\t\t\t * we must drop that reference and set it to our\n\t\t\t\t * new watch.\n\t\t\t\t */\n\t\t\t\taudit_put_watch(nentry->rule.watch);\n\t\t\t\taudit_get_watch(nwatch);\n\t\t\t\tnentry->rule.watch = nwatch;\n\t\t\t\tlist_add(&nentry->rule.rlist, &nwatch->rules);\n\t\t\t\tlist_add_rcu(&nentry->list, &audit_inode_hash[h]);\n\t\t\t\tlist_replace(&oentry->rule.list,\n\t\t\t\t\t     &nentry->rule.list);\n\t\t\t}\n\t\t\tif (oentry->rule.exe)\n\t\t\t\taudit_remove_mark(oentry->rule.exe);\n\n\t\t\tcall_rcu(&oentry->rcu, audit_free_rule_rcu);\n\t\t}\n\n\t\taudit_remove_watch(owatch);\n\t\tgoto add_watch_to_parent; /* event applies to a single watch */\n\t}\n\tmutex_unlock(&audit_filter_mutex);\n\treturn;\n\nadd_watch_to_parent:\n\tlist_add(&nwatch->wlist, &parent->watches);\n\tmutex_unlock(&audit_filter_mutex);\n\treturn;\n}\n\n/* Remove all watches & rules associated with a parent that is going away. */\nstatic void audit_remove_parent_watches(struct audit_parent *parent)\n{\n\tstruct audit_watch *w, *nextw;\n\tstruct audit_krule *r, *nextr;\n\tstruct audit_entry *e;\n\n\tmutex_lock(&audit_filter_mutex);\n\tlist_for_each_entry_safe(w, nextw, &parent->watches, wlist) {\n\t\tlist_for_each_entry_safe(r, nextr, &w->rules, rlist) {\n\t\t\te = container_of(r, struct audit_entry, rule);\n\t\t\taudit_watch_log_rule_change(r, w, \"remove_rule\");\n\t\t\tif (e->rule.exe)\n\t\t\t\taudit_remove_mark(e->rule.exe);\n\t\t\tlist_del(&r->rlist);\n\t\t\tlist_del(&r->list);\n\t\t\tlist_del_rcu(&e->list);\n\t\t\tcall_rcu(&e->rcu, audit_free_rule_rcu);\n\t\t}\n\t\taudit_remove_watch(w);\n\t}\n\tmutex_unlock(&audit_filter_mutex);\n\n\tfsnotify_destroy_mark(&parent->mark, audit_watch_group);\n}\n\n/* Get path information necessary for adding watches. */\nstatic int audit_get_nd(struct audit_watch *watch, struct path *parent)\n{\n\tstruct dentry *d = kern_path_locked(watch->path, parent);\n\tif (IS_ERR(d))\n\t\treturn PTR_ERR(d);\n\tif (d_is_positive(d)) {\n\t\t/* update watch filter fields */\n\t\twatch->dev = d->d_sb->s_dev;\n\t\twatch->ino = d_backing_inode(d)->i_ino;\n\t}\n\tinode_unlock(d_backing_inode(parent->dentry));\n\tdput(d);\n\treturn 0;\n}\n\n/* Associate the given rule with an existing parent.\n * Caller must hold audit_filter_mutex. */\nstatic void audit_add_to_parent(struct audit_krule *krule,\n\t\t\t\tstruct audit_parent *parent)\n{\n\tstruct audit_watch *w, *watch = krule->watch;\n\tint watch_found = 0;\n\n\tBUG_ON(!mutex_is_locked(&audit_filter_mutex));\n\n\tlist_for_each_entry(w, &parent->watches, wlist) {\n\t\tif (strcmp(watch->path, w->path))\n\t\t\tcontinue;\n\n\t\twatch_found = 1;\n\n\t\t/* put krule's ref to temporary watch */\n\t\taudit_put_watch(watch);\n\n\t\taudit_get_watch(w);\n\t\tkrule->watch = watch = w;\n\n\t\taudit_put_parent(parent);\n\t\tbreak;\n\t}\n\n\tif (!watch_found) {\n\t\twatch->parent = parent;\n\n\t\taudit_get_watch(watch);\n\t\tlist_add(&watch->wlist, &parent->watches);\n\t}\n\tlist_add(&krule->rlist, &watch->rules);\n}\n\n/* Find a matching watch entry, or add this one.\n * Caller must hold audit_filter_mutex. */\nint audit_add_watch(struct audit_krule *krule, struct list_head **list)\n{\n\tstruct audit_watch *watch = krule->watch;\n\tstruct audit_parent *parent;\n\tstruct path parent_path;\n\tint h, ret = 0;\n\n\t/*\n\t * When we will be calling audit_add_to_parent, krule->watch might have\n\t * been updated and watch might have been freed.\n\t * So we need to keep a reference of watch.\n\t */\n\taudit_get_watch(watch);\n\n\tmutex_unlock(&audit_filter_mutex);\n\n\t/* Avoid calling path_lookup under audit_filter_mutex. */\n\tret = audit_get_nd(watch, &parent_path);\n\n\t/* caller expects mutex locked */\n\tmutex_lock(&audit_filter_mutex);\n\n\tif (ret) {\n\t\taudit_put_watch(watch);\n\t\treturn ret;\n\t}\n\n\t/* either find an old parent or attach a new one */\n\tparent = audit_find_parent(d_backing_inode(parent_path.dentry));\n\tif (!parent) {\n\t\tparent = audit_init_parent(&parent_path);\n\t\tif (IS_ERR(parent)) {\n\t\t\tret = PTR_ERR(parent);\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\taudit_add_to_parent(krule, parent);\n\n\th = audit_hash_ino((u32)watch->ino);\n\t*list = &audit_inode_hash[h];\nerror:\n\tpath_put(&parent_path);\n\taudit_put_watch(watch);\n\treturn ret;\n}\n\nvoid audit_remove_watch_rule(struct audit_krule *krule)\n{\n\tstruct audit_watch *watch = krule->watch;\n\tstruct audit_parent *parent = watch->parent;\n\n\tlist_del(&krule->rlist);\n\n\tif (list_empty(&watch->rules)) {\n\t\t/*\n\t\t * audit_remove_watch() drops our reference to 'parent' which\n\t\t * can get freed. Grab our own reference to be safe.\n\t\t */\n\t\taudit_get_parent(parent);\n\t\taudit_remove_watch(watch);\n\t\tif (list_empty(&parent->watches))\n\t\t\tfsnotify_destroy_mark(&parent->mark, audit_watch_group);\n\t\taudit_put_parent(parent);\n\t}\n}\n\n/* Update watch data in audit rules based on fsnotify events. */\nstatic int audit_watch_handle_event(struct fsnotify_mark *inode_mark, u32 mask,\n\t\t\t\t    struct inode *inode, struct inode *dir,\n\t\t\t\t    const struct qstr *dname)\n{\n\tstruct audit_parent *parent;\n\n\tparent = container_of(inode_mark, struct audit_parent, mark);\n\n\tif (WARN_ON_ONCE(inode_mark->group != audit_watch_group) ||\n\t    WARN_ON_ONCE(!inode))\n\t\treturn 0;\n\n\tif (mask & (FS_CREATE|FS_MOVED_TO) && inode)\n\t\taudit_update_watch(parent, dname, inode->i_sb->s_dev, inode->i_ino, 0);\n\telse if (mask & (FS_DELETE|FS_MOVED_FROM))\n\t\taudit_update_watch(parent, dname, AUDIT_DEV_UNSET, AUDIT_INO_UNSET, 1);\n\telse if (mask & (FS_DELETE_SELF|FS_UNMOUNT|FS_MOVE_SELF))\n\t\taudit_remove_parent_watches(parent);\n\n\treturn 0;\n}\n\nstatic const struct fsnotify_ops audit_watch_fsnotify_ops = {\n\t.handle_inode_event =\taudit_watch_handle_event,\n\t.free_mark =\t\taudit_watch_free_mark,\n};\n\nstatic int __init audit_watch_init(void)\n{\n\taudit_watch_group = fsnotify_alloc_group(&audit_watch_fsnotify_ops);\n\tif (IS_ERR(audit_watch_group)) {\n\t\taudit_watch_group = NULL;\n\t\taudit_panic(\"cannot create audit fsnotify group\");\n\t}\n\treturn 0;\n}\ndevice_initcall(audit_watch_init);\n\nint audit_dupe_exe(struct audit_krule *new, struct audit_krule *old)\n{\n\tstruct audit_fsnotify_mark *audit_mark;\n\tchar *pathname;\n\n\tpathname = kstrdup(audit_mark_path(old->exe), GFP_KERNEL);\n\tif (!pathname)\n\t\treturn -ENOMEM;\n\n\taudit_mark = audit_alloc_mark(new, pathname, strlen(pathname));\n\tif (IS_ERR(audit_mark)) {\n\t\tkfree(pathname);\n\t\treturn PTR_ERR(audit_mark);\n\t}\n\tnew->exe = audit_mark;\n\n\treturn 0;\n}\n\nint audit_exe_compare(struct task_struct *tsk, struct audit_fsnotify_mark *mark)\n{\n\tstruct file *exe_file;\n\tunsigned long ino;\n\tdev_t dev;\n\n\texe_file = get_task_exe_file(tsk);\n\tif (!exe_file)\n\t\treturn 0;\n\tino = file_inode(exe_file)->i_ino;\n\tdev = file_inode(exe_file)->i_sb->s_dev;\n\tfput(exe_file);\n\treturn audit_mark_compare(mark, ino, dev);\n}\n"}, "2": {"id": 2, "path": "/src/include/linux/kernel.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_KERNEL_H\n#define _LINUX_KERNEL_H\n\n#include <stdarg.h>\n#include <linux/limits.h>\n#include <linux/linkage.h>\n#include <linux/stddef.h>\n#include <linux/types.h>\n#include <linux/compiler.h>\n#include <linux/bitops.h>\n#include <linux/log2.h>\n#include <linux/math.h>\n#include <linux/minmax.h>\n#include <linux/typecheck.h>\n#include <linux/printk.h>\n#include <linux/build_bug.h>\n\n#include <asm/byteorder.h>\n\n#include <uapi/linux/kernel.h>\n\n#define STACK_MAGIC\t0xdeadbeef\n\n/**\n * REPEAT_BYTE - repeat the value @x multiple times as an unsigned long value\n * @x: value to repeat\n *\n * NOTE: @x is not checked for > 0xff; larger values produce odd results.\n */\n#define REPEAT_BYTE(x)\t((~0ul / 0xff) * (x))\n\n/* @a is a power of 2 value */\n#define ALIGN(x, a)\t\t__ALIGN_KERNEL((x), (a))\n#define ALIGN_DOWN(x, a)\t__ALIGN_KERNEL((x) - ((a) - 1), (a))\n#define __ALIGN_MASK(x, mask)\t__ALIGN_KERNEL_MASK((x), (mask))\n#define PTR_ALIGN(p, a)\t\t((typeof(p))ALIGN((unsigned long)(p), (a)))\n#define PTR_ALIGN_DOWN(p, a)\t((typeof(p))ALIGN_DOWN((unsigned long)(p), (a)))\n#define IS_ALIGNED(x, a)\t\t(((x) & ((typeof(x))(a) - 1)) == 0)\n\n/* generic data direction definitions */\n#define READ\t\t\t0\n#define WRITE\t\t\t1\n\n/**\n * ARRAY_SIZE - get the number of elements in array @arr\n * @arr: array to be sized\n */\n#define ARRAY_SIZE(arr) (sizeof(arr) / sizeof((arr)[0]) + __must_be_array(arr))\n\n#define u64_to_user_ptr(x) (\t\t\\\n{\t\t\t\t\t\\\n\ttypecheck(u64, (x));\t\t\\\n\t(void __user *)(uintptr_t)(x);\t\\\n}\t\t\t\t\t\\\n)\n\n#define typeof_member(T, m)\ttypeof(((T*)0)->m)\n\n#define _RET_IP_\t\t(unsigned long)__builtin_return_address(0)\n#define _THIS_IP_  ({ __label__ __here; __here: (unsigned long)&&__here; })\n\n/**\n * upper_32_bits - return bits 32-63 of a number\n * @n: the number we're accessing\n *\n * A basic shift-right of a 64- or 32-bit quantity.  Use this to suppress\n * the \"right shift count >= width of type\" warning when that quantity is\n * 32-bits.\n */\n#define upper_32_bits(n) ((u32)(((n) >> 16) >> 16))\n\n/**\n * lower_32_bits - return bits 0-31 of a number\n * @n: the number we're accessing\n */\n#define lower_32_bits(n) ((u32)((n) & 0xffffffff))\n\nstruct completion;\nstruct pt_regs;\nstruct user;\n\n#ifdef CONFIG_PREEMPT_VOLUNTARY\nextern int _cond_resched(void);\n# define might_resched() _cond_resched()\n#else\n# define might_resched() do { } while (0)\n#endif\n\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\nextern void ___might_sleep(const char *file, int line, int preempt_offset);\nextern void __might_sleep(const char *file, int line, int preempt_offset);\nextern void __cant_sleep(const char *file, int line, int preempt_offset);\n\n/**\n * might_sleep - annotation for functions that can sleep\n *\n * this macro will print a stack trace if it is executed in an atomic\n * context (spinlock, irq-handler, ...). Additional sections where blocking is\n * not allowed can be annotated with non_block_start() and non_block_end()\n * pairs.\n *\n * This is a useful debugging help to be able to catch problems early and not\n * be bitten later when the calling function happens to sleep when it is not\n * supposed to.\n */\n# define might_sleep() \\\n\tdo { __might_sleep(__FILE__, __LINE__, 0); might_resched(); } while (0)\n/**\n * cant_sleep - annotation for functions that cannot sleep\n *\n * this macro will print a stack trace if it is executed with preemption enabled\n */\n# define cant_sleep() \\\n\tdo { __cant_sleep(__FILE__, __LINE__, 0); } while (0)\n# define sched_annotate_sleep()\t(current->task_state_change = 0)\n/**\n * non_block_start - annotate the start of section where sleeping is prohibited\n *\n * This is on behalf of the oom reaper, specifically when it is calling the mmu\n * notifiers. The problem is that if the notifier were to block on, for example,\n * mutex_lock() and if the process which holds that mutex were to perform a\n * sleeping memory allocation, the oom reaper is now blocked on completion of\n * that memory allocation. Other blocking calls like wait_event() pose similar\n * issues.\n */\n# define non_block_start() (current->non_block_count++)\n/**\n * non_block_end - annotate the end of section where sleeping is prohibited\n *\n * Closes a section opened by non_block_start().\n */\n# define non_block_end() WARN_ON(current->non_block_count-- == 0)\n#else\n  static inline void ___might_sleep(const char *file, int line,\n\t\t\t\t   int preempt_offset) { }\n  static inline void __might_sleep(const char *file, int line,\n\t\t\t\t   int preempt_offset) { }\n# define might_sleep() do { might_resched(); } while (0)\n# define cant_sleep() do { } while (0)\n# define sched_annotate_sleep() do { } while (0)\n# define non_block_start() do { } while (0)\n# define non_block_end() do { } while (0)\n#endif\n\n#define might_sleep_if(cond) do { if (cond) might_sleep(); } while (0)\n\n#ifndef CONFIG_PREEMPT_RT\n# define cant_migrate()\t\tcant_sleep()\n#else\n  /* Placeholder for now */\n# define cant_migrate()\t\tdo { } while (0)\n#endif\n\n#if defined(CONFIG_MMU) && \\\n\t(defined(CONFIG_PROVE_LOCKING) || defined(CONFIG_DEBUG_ATOMIC_SLEEP))\n#define might_fault() __might_fault(__FILE__, __LINE__)\nvoid __might_fault(const char *file, int line);\n#else\nstatic inline void might_fault(void) { }\n#endif\n\nextern struct atomic_notifier_head panic_notifier_list;\nextern long (*panic_blink)(int state);\n__printf(1, 2)\nvoid panic(const char *fmt, ...) __noreturn __cold;\nvoid nmi_panic(struct pt_regs *regs, const char *msg);\nextern void oops_enter(void);\nextern void oops_exit(void);\nextern bool oops_may_print(void);\nvoid do_exit(long error_code) __noreturn;\nvoid complete_and_exit(struct completion *, long) __noreturn;\n\n/* Internal, do not use. */\nint __must_check _kstrtoul(const char *s, unsigned int base, unsigned long *res);\nint __must_check _kstrtol(const char *s, unsigned int base, long *res);\n\nint __must_check kstrtoull(const char *s, unsigned int base, unsigned long long *res);\nint __must_check kstrtoll(const char *s, unsigned int base, long long *res);\n\n/**\n * kstrtoul - convert a string to an unsigned long\n * @s: The start of the string. The string must be null-terminated, and may also\n *  include a single newline before its terminating null. The first character\n *  may also be a plus sign, but not a minus sign.\n * @base: The number base to use. The maximum supported base is 16. If base is\n *  given as 0, then the base of the string is automatically detected with the\n *  conventional semantics - If it begins with 0x the number will be parsed as a\n *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be\n *  parsed as an octal number. Otherwise it will be parsed as a decimal.\n * @res: Where to write the result of the conversion on success.\n *\n * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.\n * Preferred over simple_strtoul(). Return code must be checked.\n*/\nstatic inline int __must_check kstrtoul(const char *s, unsigned int base, unsigned long *res)\n{\n\t/*\n\t * We want to shortcut function call, but\n\t * __builtin_types_compatible_p(unsigned long, unsigned long long) = 0.\n\t */\n\tif (sizeof(unsigned long) == sizeof(unsigned long long) &&\n\t    __alignof__(unsigned long) == __alignof__(unsigned long long))\n\t\treturn kstrtoull(s, base, (unsigned long long *)res);\n\telse\n\t\treturn _kstrtoul(s, base, res);\n}\n\n/**\n * kstrtol - convert a string to a long\n * @s: The start of the string. The string must be null-terminated, and may also\n *  include a single newline before its terminating null. The first character\n *  may also be a plus sign or a minus sign.\n * @base: The number base to use. The maximum supported base is 16. If base is\n *  given as 0, then the base of the string is automatically detected with the\n *  conventional semantics - If it begins with 0x the number will be parsed as a\n *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be\n *  parsed as an octal number. Otherwise it will be parsed as a decimal.\n * @res: Where to write the result of the conversion on success.\n *\n * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.\n * Preferred over simple_strtol(). Return code must be checked.\n */\nstatic inline int __must_check kstrtol(const char *s, unsigned int base, long *res)\n{\n\t/*\n\t * We want to shortcut function call, but\n\t * __builtin_types_compatible_p(long, long long) = 0.\n\t */\n\tif (sizeof(long) == sizeof(long long) &&\n\t    __alignof__(long) == __alignof__(long long))\n\t\treturn kstrtoll(s, base, (long long *)res);\n\telse\n\t\treturn _kstrtol(s, base, res);\n}\n\nint __must_check kstrtouint(const char *s, unsigned int base, unsigned int *res);\nint __must_check kstrtoint(const char *s, unsigned int base, int *res);\n\nstatic inline int __must_check kstrtou64(const char *s, unsigned int base, u64 *res)\n{\n\treturn kstrtoull(s, base, res);\n}\n\nstatic inline int __must_check kstrtos64(const char *s, unsigned int base, s64 *res)\n{\n\treturn kstrtoll(s, base, res);\n}\n\nstatic inline int __must_check kstrtou32(const char *s, unsigned int base, u32 *res)\n{\n\treturn kstrtouint(s, base, res);\n}\n\nstatic inline int __must_check kstrtos32(const char *s, unsigned int base, s32 *res)\n{\n\treturn kstrtoint(s, base, res);\n}\n\nint __must_check kstrtou16(const char *s, unsigned int base, u16 *res);\nint __must_check kstrtos16(const char *s, unsigned int base, s16 *res);\nint __must_check kstrtou8(const char *s, unsigned int base, u8 *res);\nint __must_check kstrtos8(const char *s, unsigned int base, s8 *res);\nint __must_check kstrtobool(const char *s, bool *res);\n\nint __must_check kstrtoull_from_user(const char __user *s, size_t count, unsigned int base, unsigned long long *res);\nint __must_check kstrtoll_from_user(const char __user *s, size_t count, unsigned int base, long long *res);\nint __must_check kstrtoul_from_user(const char __user *s, size_t count, unsigned int base, unsigned long *res);\nint __must_check kstrtol_from_user(const char __user *s, size_t count, unsigned int base, long *res);\nint __must_check kstrtouint_from_user(const char __user *s, size_t count, unsigned int base, unsigned int *res);\nint __must_check kstrtoint_from_user(const char __user *s, size_t count, unsigned int base, int *res);\nint __must_check kstrtou16_from_user(const char __user *s, size_t count, unsigned int base, u16 *res);\nint __must_check kstrtos16_from_user(const char __user *s, size_t count, unsigned int base, s16 *res);\nint __must_check kstrtou8_from_user(const char __user *s, size_t count, unsigned int base, u8 *res);\nint __must_check kstrtos8_from_user(const char __user *s, size_t count, unsigned int base, s8 *res);\nint __must_check kstrtobool_from_user(const char __user *s, size_t count, bool *res);\n\nstatic inline int __must_check kstrtou64_from_user(const char __user *s, size_t count, unsigned int base, u64 *res)\n{\n\treturn kstrtoull_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtos64_from_user(const char __user *s, size_t count, unsigned int base, s64 *res)\n{\n\treturn kstrtoll_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtou32_from_user(const char __user *s, size_t count, unsigned int base, u32 *res)\n{\n\treturn kstrtouint_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtos32_from_user(const char __user *s, size_t count, unsigned int base, s32 *res)\n{\n\treturn kstrtoint_from_user(s, count, base, res);\n}\n\n/*\n * Use kstrto<foo> instead.\n *\n * NOTE: simple_strto<foo> does not check for the range overflow and,\n *\t depending on the input, may give interesting results.\n *\n * Use these functions if and only if you cannot use kstrto<foo>, because\n * the conversion ends on the first non-digit character, which may be far\n * beyond the supported range. It might be useful to parse the strings like\n * 10x50 or 12:21 without altering original string or temporary buffer in use.\n * Keep in mind above caveat.\n */\n\nextern unsigned long simple_strtoul(const char *,char **,unsigned int);\nextern long simple_strtol(const char *,char **,unsigned int);\nextern unsigned long long simple_strtoull(const char *,char **,unsigned int);\nextern long long simple_strtoll(const char *,char **,unsigned int);\n\nextern int num_to_str(char *buf, int size,\n\t\t      unsigned long long num, unsigned int width);\n\n/* lib/printf utilities */\n\nextern __printf(2, 3) int sprintf(char *buf, const char * fmt, ...);\nextern __printf(2, 0) int vsprintf(char *buf, const char *, va_list);\nextern __printf(3, 4)\nint snprintf(char *buf, size_t size, const char *fmt, ...);\nextern __printf(3, 0)\nint vsnprintf(char *buf, size_t size, const char *fmt, va_list args);\nextern __printf(3, 4)\nint scnprintf(char *buf, size_t size, const char *fmt, ...);\nextern __printf(3, 0)\nint vscnprintf(char *buf, size_t size, const char *fmt, va_list args);\nextern __printf(2, 3) __malloc\nchar *kasprintf(gfp_t gfp, const char *fmt, ...);\nextern __printf(2, 0) __malloc\nchar *kvasprintf(gfp_t gfp, const char *fmt, va_list args);\nextern __printf(2, 0)\nconst char *kvasprintf_const(gfp_t gfp, const char *fmt, va_list args);\n\nextern __scanf(2, 3)\nint sscanf(const char *, const char *, ...);\nextern __scanf(2, 0)\nint vsscanf(const char *, const char *, va_list);\n\nextern int get_option(char **str, int *pint);\nextern char *get_options(const char *str, int nints, int *ints);\nextern unsigned long long memparse(const char *ptr, char **retptr);\nextern bool parse_option_str(const char *str, const char *option);\nextern char *next_arg(char *args, char **param, char **val);\n\nextern int core_kernel_text(unsigned long addr);\nextern int init_kernel_text(unsigned long addr);\nextern int core_kernel_data(unsigned long addr);\nextern int __kernel_text_address(unsigned long addr);\nextern int kernel_text_address(unsigned long addr);\nextern int func_ptr_is_kernel_text(void *ptr);\n\n#ifdef CONFIG_SMP\nextern unsigned int sysctl_oops_all_cpu_backtrace;\n#else\n#define sysctl_oops_all_cpu_backtrace 0\n#endif /* CONFIG_SMP */\n\nextern void bust_spinlocks(int yes);\nextern int panic_timeout;\nextern unsigned long panic_print;\nextern int panic_on_oops;\nextern int panic_on_unrecovered_nmi;\nextern int panic_on_io_nmi;\nextern int panic_on_warn;\nextern unsigned long panic_on_taint;\nextern bool panic_on_taint_nousertaint;\nextern int sysctl_panic_on_rcu_stall;\nextern int sysctl_max_rcu_stall_to_panic;\nextern int sysctl_panic_on_stackoverflow;\n\nextern bool crash_kexec_post_notifiers;\n\n/*\n * panic_cpu is used for synchronizing panic() and crash_kexec() execution. It\n * holds a CPU number which is executing panic() currently. A value of\n * PANIC_CPU_INVALID means no CPU has entered panic() or crash_kexec().\n */\nextern atomic_t panic_cpu;\n#define PANIC_CPU_INVALID\t-1\n\n/*\n * Only to be used by arch init code. If the user over-wrote the default\n * CONFIG_PANIC_TIMEOUT, honor it.\n */\nstatic inline void set_arch_panic_timeout(int timeout, int arch_default_timeout)\n{\n\tif (panic_timeout == arch_default_timeout)\n\t\tpanic_timeout = timeout;\n}\nextern const char *print_tainted(void);\nenum lockdep_ok {\n\tLOCKDEP_STILL_OK,\n\tLOCKDEP_NOW_UNRELIABLE\n};\nextern void add_taint(unsigned flag, enum lockdep_ok);\nextern int test_taint(unsigned flag);\nextern unsigned long get_taint(void);\nextern int root_mountflags;\n\nextern bool early_boot_irqs_disabled;\n\n/*\n * Values used for system_state. Ordering of the states must not be changed\n * as code checks for <, <=, >, >= STATE.\n */\nextern enum system_states {\n\tSYSTEM_BOOTING,\n\tSYSTEM_SCHEDULING,\n\tSYSTEM_RUNNING,\n\tSYSTEM_HALT,\n\tSYSTEM_POWER_OFF,\n\tSYSTEM_RESTART,\n\tSYSTEM_SUSPEND,\n} system_state;\n\n/* This cannot be an enum because some may be used in assembly source. */\n#define TAINT_PROPRIETARY_MODULE\t0\n#define TAINT_FORCED_MODULE\t\t1\n#define TAINT_CPU_OUT_OF_SPEC\t\t2\n#define TAINT_FORCED_RMMOD\t\t3\n#define TAINT_MACHINE_CHECK\t\t4\n#define TAINT_BAD_PAGE\t\t\t5\n#define TAINT_USER\t\t\t6\n#define TAINT_DIE\t\t\t7\n#define TAINT_OVERRIDDEN_ACPI_TABLE\t8\n#define TAINT_WARN\t\t\t9\n#define TAINT_CRAP\t\t\t10\n#define TAINT_FIRMWARE_WORKAROUND\t11\n#define TAINT_OOT_MODULE\t\t12\n#define TAINT_UNSIGNED_MODULE\t\t13\n#define TAINT_SOFTLOCKUP\t\t14\n#define TAINT_LIVEPATCH\t\t\t15\n#define TAINT_AUX\t\t\t16\n#define TAINT_RANDSTRUCT\t\t17\n#define TAINT_FLAGS_COUNT\t\t18\n#define TAINT_FLAGS_MAX\t\t\t((1UL << TAINT_FLAGS_COUNT) - 1)\n\nstruct taint_flag {\n\tchar c_true;\t/* character printed when tainted */\n\tchar c_false;\t/* character printed when not tainted */\n\tbool module;\t/* also show as a per-module taint flag */\n};\n\nextern const struct taint_flag taint_flags[TAINT_FLAGS_COUNT];\n\nextern const char hex_asc[];\n#define hex_asc_lo(x)\thex_asc[((x) & 0x0f)]\n#define hex_asc_hi(x)\thex_asc[((x) & 0xf0) >> 4]\n\nstatic inline char *hex_byte_pack(char *buf, u8 byte)\n{\n\t*buf++ = hex_asc_hi(byte);\n\t*buf++ = hex_asc_lo(byte);\n\treturn buf;\n}\n\nextern const char hex_asc_upper[];\n#define hex_asc_upper_lo(x)\thex_asc_upper[((x) & 0x0f)]\n#define hex_asc_upper_hi(x)\thex_asc_upper[((x) & 0xf0) >> 4]\n\nstatic inline char *hex_byte_pack_upper(char *buf, u8 byte)\n{\n\t*buf++ = hex_asc_upper_hi(byte);\n\t*buf++ = hex_asc_upper_lo(byte);\n\treturn buf;\n}\n\nextern int hex_to_bin(char ch);\nextern int __must_check hex2bin(u8 *dst, const char *src, size_t count);\nextern char *bin2hex(char *dst, const void *src, size_t count);\n\nbool mac_pton(const char *s, u8 *mac);\n\n/*\n * General tracing related utility functions - trace_printk(),\n * tracing_on/tracing_off and tracing_start()/tracing_stop\n *\n * Use tracing_on/tracing_off when you want to quickly turn on or off\n * tracing. It simply enables or disables the recording of the trace events.\n * This also corresponds to the user space /sys/kernel/debug/tracing/tracing_on\n * file, which gives a means for the kernel and userspace to interact.\n * Place a tracing_off() in the kernel where you want tracing to end.\n * From user space, examine the trace, and then echo 1 > tracing_on\n * to continue tracing.\n *\n * tracing_stop/tracing_start has slightly more overhead. It is used\n * by things like suspend to ram where disabling the recording of the\n * trace is not enough, but tracing must actually stop because things\n * like calling smp_processor_id() may crash the system.\n *\n * Most likely, you want to use tracing_on/tracing_off.\n */\n\nenum ftrace_dump_mode {\n\tDUMP_NONE,\n\tDUMP_ALL,\n\tDUMP_ORIG,\n};\n\n#ifdef CONFIG_TRACING\nvoid tracing_on(void);\nvoid tracing_off(void);\nint tracing_is_on(void);\nvoid tracing_snapshot(void);\nvoid tracing_snapshot_alloc(void);\n\nextern void tracing_start(void);\nextern void tracing_stop(void);\n\nstatic inline __printf(1, 2)\nvoid ____trace_printk_check_format(const char *fmt, ...)\n{\n}\n#define __trace_printk_check_format(fmt, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\t\\\n\t\t____trace_printk_check_format(fmt, ##args);\t\t\\\n} while (0)\n\n/**\n * trace_printk - printf formatting in the ftrace buffer\n * @fmt: the printf format for printing\n *\n * Note: __trace_printk is an internal function for trace_printk() and\n *       the @ip is passed in via the trace_printk() macro.\n *\n * This function allows a kernel developer to debug fast path sections\n * that printk is not appropriate for. By scattering in various\n * printk like tracing in the code, a developer can quickly see\n * where problems are occurring.\n *\n * This is intended as a debugging tool for the developer only.\n * Please refrain from leaving trace_printks scattered around in\n * your code. (Extra memory is used for special buffers that are\n * allocated when trace_printk() is used.)\n *\n * A little optimization trick is done here. If there's only one\n * argument, there's no need to scan the string for printf formats.\n * The trace_puts() will suffice. But how can we take advantage of\n * using trace_puts() when trace_printk() has only one argument?\n * By stringifying the args and checking the size we can tell\n * whether or not there are args. __stringify((__VA_ARGS__)) will\n * turn into \"()\\0\" with a size of 3 when there are no args, anything\n * else will be bigger. All we need to do is define a string to this,\n * and then take its size and compare to 3. If it's bigger, use\n * do_trace_printk() otherwise, optimize it to trace_puts(). Then just\n * let gcc optimize the rest.\n */\n\n#define trace_printk(fmt, ...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\\\n\tchar _______STR[] = __stringify((__VA_ARGS__));\t\\\n\tif (sizeof(_______STR) > 3)\t\t\t\\\n\t\tdo_trace_printk(fmt, ##__VA_ARGS__);\t\\\n\telse\t\t\t\t\t\t\\\n\t\ttrace_puts(fmt);\t\t\t\\\n} while (0)\n\n#define do_trace_printk(fmt, args...)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tstatic const char *trace_printk_fmt __used\t\t\t\\\n\t\t__section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t__builtin_constant_p(fmt) ? fmt : NULL;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t__trace_printk_check_format(fmt, ##args);\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(fmt))\t\t\t\t\t\\\n\t\t__trace_bprintk(_THIS_IP_, trace_printk_fmt, ##args);\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t__trace_printk(_THIS_IP_, fmt, ##args);\t\t\t\\\n} while (0)\n\nextern __printf(2, 3)\nint __trace_bprintk(unsigned long ip, const char *fmt, ...);\n\nextern __printf(2, 3)\nint __trace_printk(unsigned long ip, const char *fmt, ...);\n\n/**\n * trace_puts - write a string into the ftrace buffer\n * @str: the string to record\n *\n * Note: __trace_bputs is an internal function for trace_puts and\n *       the @ip is passed in via the trace_puts macro.\n *\n * This is similar to trace_printk() but is made for those really fast\n * paths that a developer wants the least amount of \"Heisenbug\" effects,\n * where the processing of the print format is still too much.\n *\n * This function allows a kernel developer to debug fast path sections\n * that printk is not appropriate for. By scattering in various\n * printk like tracing in the code, a developer can quickly see\n * where problems are occurring.\n *\n * This is intended as a debugging tool for the developer only.\n * Please refrain from leaving trace_puts scattered around in\n * your code. (Extra memory is used for special buffers that are\n * allocated when trace_puts() is used.)\n *\n * Returns: 0 if nothing was written, positive # if string was.\n *  (1 when __trace_bputs is used, strlen(str) when __trace_puts is used)\n */\n\n#define trace_puts(str) ({\t\t\t\t\t\t\\\n\tstatic const char *trace_printk_fmt __used\t\t\t\\\n\t\t__section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t__builtin_constant_p(str) ? str : NULL;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(str))\t\t\t\t\t\\\n\t\t__trace_bputs(_THIS_IP_, trace_printk_fmt);\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t__trace_puts(_THIS_IP_, str, strlen(str));\t\t\\\n})\nextern int __trace_bputs(unsigned long ip, const char *str);\nextern int __trace_puts(unsigned long ip, const char *str, int size);\n\nextern void trace_dump_stack(int skip);\n\n/*\n * The double __builtin_constant_p is because gcc will give us an error\n * if we try to allocate the static variable to fmt if it is not a\n * constant. Even with the outer if statement.\n */\n#define ftrace_vprintk(fmt, vargs)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(fmt)) {\t\t\t\t\\\n\t\tstatic const char *trace_printk_fmt __used\t\t\\\n\t\t  __section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t\t__builtin_constant_p(fmt) ? fmt : NULL;\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t__ftrace_vbprintk(_THIS_IP_, trace_printk_fmt, vargs);\t\\\n\t} else\t\t\t\t\t\t\t\t\\\n\t\t__ftrace_vprintk(_THIS_IP_, fmt, vargs);\t\t\\\n} while (0)\n\nextern __printf(2, 0) int\n__ftrace_vbprintk(unsigned long ip, const char *fmt, va_list ap);\n\nextern __printf(2, 0) int\n__ftrace_vprintk(unsigned long ip, const char *fmt, va_list ap);\n\nextern void ftrace_dump(enum ftrace_dump_mode oops_dump_mode);\n#else\nstatic inline void tracing_start(void) { }\nstatic inline void tracing_stop(void) { }\nstatic inline void trace_dump_stack(int skip) { }\n\nstatic inline void tracing_on(void) { }\nstatic inline void tracing_off(void) { }\nstatic inline int tracing_is_on(void) { return 0; }\nstatic inline void tracing_snapshot(void) { }\nstatic inline void tracing_snapshot_alloc(void) { }\n\nstatic inline __printf(1, 2)\nint trace_printk(const char *fmt, ...)\n{\n\treturn 0;\n}\nstatic __printf(1, 0) inline int\nftrace_vprintk(const char *fmt, va_list ap)\n{\n\treturn 0;\n}\nstatic inline void ftrace_dump(enum ftrace_dump_mode oops_dump_mode) { }\n#endif /* CONFIG_TRACING */\n\n/* This counts to 12. Any more, it will return 13th argument. */\n#define __COUNT_ARGS(_0, _1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _n, X...) _n\n#define COUNT_ARGS(X...) __COUNT_ARGS(, ##X, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0)\n\n#define __CONCAT(a, b) a ## b\n#define CONCATENATE(a, b) __CONCAT(a, b)\n\n/**\n * container_of - cast a member of a structure out to the containing structure\n * @ptr:\tthe pointer to the member.\n * @type:\tthe type of the container struct this is embedded in.\n * @member:\tthe name of the member within the struct.\n *\n */\n#define container_of(ptr, type, member) ({\t\t\t\t\\\n\tvoid *__mptr = (void *)(ptr);\t\t\t\t\t\\\n\tBUILD_BUG_ON_MSG(!__same_type(*(ptr), ((type *)0)->member) &&\t\\\n\t\t\t !__same_type(*(ptr), void),\t\t\t\\\n\t\t\t \"pointer type mismatch in container_of()\");\t\\\n\t((type *)(__mptr - offsetof(type, member))); })\n\n/**\n * container_of_safe - cast a member of a structure out to the containing structure\n * @ptr:\tthe pointer to the member.\n * @type:\tthe type of the container struct this is embedded in.\n * @member:\tthe name of the member within the struct.\n *\n * If IS_ERR_OR_NULL(ptr), ptr is returned unchanged.\n */\n#define container_of_safe(ptr, type, member) ({\t\t\t\t\\\n\tvoid *__mptr = (void *)(ptr);\t\t\t\t\t\\\n\tBUILD_BUG_ON_MSG(!__same_type(*(ptr), ((type *)0)->member) &&\t\\\n\t\t\t !__same_type(*(ptr), void),\t\t\t\\\n\t\t\t \"pointer type mismatch in container_of()\");\t\\\n\tIS_ERR_OR_NULL(__mptr) ? ERR_CAST(__mptr) :\t\t\t\\\n\t\t((type *)(__mptr - offsetof(type, member))); })\n\n/* Rebuild everything on CONFIG_FTRACE_MCOUNT_RECORD */\n#ifdef CONFIG_FTRACE_MCOUNT_RECORD\n# define REBUILD_DUE_TO_FTRACE_MCOUNT_RECORD\n#endif\n\n/* Permissions on a sysfs file: you didn't miss the 0 prefix did you? */\n#define VERIFY_OCTAL_PERMISSIONS(perms)\t\t\t\t\t\t\\\n\t(BUILD_BUG_ON_ZERO((perms) < 0) +\t\t\t\t\t\\\n\t BUILD_BUG_ON_ZERO((perms) > 0777) +\t\t\t\t\t\\\n\t /* USER_READABLE >= GROUP_READABLE >= OTHER_READABLE */\t\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 6) & 4) < (((perms) >> 3) & 4)) +\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 3) & 4) < ((perms) & 4)) +\t\t\\\n\t /* USER_WRITABLE >= GROUP_WRITABLE */\t\t\t\t\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 6) & 2) < (((perms) >> 3) & 2)) +\t\\\n\t /* OTHER_WRITABLE?  Generally considered a bad idea. */\t\t\\\n\t BUILD_BUG_ON_ZERO((perms) & 2) +\t\t\t\t\t\\\n\t (perms))\n#endif\n"}, "3": {"id": 3, "path": "/src/include/linux/build_bug.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_BUILD_BUG_H\n#define _LINUX_BUILD_BUG_H\n\n#include <linux/compiler.h>\n\n#ifdef __CHECKER__\n#define BUILD_BUG_ON_ZERO(e) (0)\n#else /* __CHECKER__ */\n/*\n * Force a compilation error if condition is true, but also produce a\n * result (of value 0 and type int), so the expression can be used\n * e.g. in a structure initializer (or where-ever else comma expressions\n * aren't permitted).\n */\n#define BUILD_BUG_ON_ZERO(e) ((int)(sizeof(struct { int:(-!!(e)); })))\n#endif /* __CHECKER__ */\n\n/* Force a compilation error if a constant expression is not a power of 2 */\n#define __BUILD_BUG_ON_NOT_POWER_OF_2(n)\t\\\n\tBUILD_BUG_ON(((n) & ((n) - 1)) != 0)\n#define BUILD_BUG_ON_NOT_POWER_OF_2(n)\t\t\t\\\n\tBUILD_BUG_ON((n) == 0 || (((n) & ((n) - 1)) != 0))\n\n/*\n * BUILD_BUG_ON_INVALID() permits the compiler to check the validity of the\n * expression but avoids the generation of any code, even if that expression\n * has side-effects.\n */\n#define BUILD_BUG_ON_INVALID(e) ((void)(sizeof((__force long)(e))))\n\n/**\n * BUILD_BUG_ON_MSG - break compile if a condition is true & emit supplied\n *\t\t      error message.\n * @condition: the condition which the compiler should know is false.\n *\n * See BUILD_BUG_ON for description.\n */\n#define BUILD_BUG_ON_MSG(cond, msg) compiletime_assert(!(cond), msg)\n\n/**\n * BUILD_BUG_ON - break compile if a condition is true.\n * @condition: the condition which the compiler should know is false.\n *\n * If you have some code which relies on certain constants being equal, or\n * some other compile-time-evaluated condition, you should use BUILD_BUG_ON to\n * detect if someone changes it.\n */\n#define BUILD_BUG_ON(condition) \\\n\tBUILD_BUG_ON_MSG(condition, \"BUILD_BUG_ON failed: \" #condition)\n\n/**\n * BUILD_BUG - break compile if used.\n *\n * If you have some code that you expect the compiler to eliminate at\n * build time, you should use BUILD_BUG to detect if it is\n * unexpectedly used.\n */\n#define BUILD_BUG() BUILD_BUG_ON_MSG(1, \"BUILD_BUG failed\")\n\n/**\n * static_assert - check integer constant expression at build time\n *\n * static_assert() is a wrapper for the C11 _Static_assert, with a\n * little macro magic to make the message optional (defaulting to the\n * stringification of the tested expression).\n *\n * Contrary to BUILD_BUG_ON(), static_assert() can be used at global\n * scope, but requires the expression to be an integer constant\n * expression (i.e., it is not enough that __builtin_constant_p() is\n * true for expr).\n *\n * Also note that BUILD_BUG_ON() fails the build if the condition is\n * true, while static_assert() fails the build if the expression is\n * false.\n */\n#define static_assert(expr, ...) __static_assert(expr, ##__VA_ARGS__, #expr)\n#define __static_assert(expr, msg, ...) _Static_assert(expr, msg)\n\n#endif\t/* _LINUX_BUILD_BUG_H */\n"}, "4": {"id": 4, "path": "/src/include/linux/compiler_types.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_COMPILER_TYPES_H\n#define __LINUX_COMPILER_TYPES_H\n\n#ifndef __ASSEMBLY__\n\n#ifdef __CHECKER__\n/* address spaces */\n# define __kernel\t__attribute__((address_space(0)))\n# define __user\t\t__attribute__((noderef, address_space(__user)))\n# define __iomem\t__attribute__((noderef, address_space(__iomem)))\n# define __percpu\t__attribute__((noderef, address_space(__percpu)))\n# define __rcu\t\t__attribute__((noderef, address_space(__rcu)))\nstatic inline void __chk_user_ptr(const volatile void __user *ptr) { }\nstatic inline void __chk_io_ptr(const volatile void __iomem *ptr) { }\n/* context/locking */\n# define __must_hold(x)\t__attribute__((context(x,1,1)))\n# define __acquires(x)\t__attribute__((context(x,0,1)))\n# define __releases(x)\t__attribute__((context(x,1,0)))\n# define __acquire(x)\t__context__(x,1)\n# define __release(x)\t__context__(x,-1)\n# define __cond_lock(x,c)\t((c) ? ({ __acquire(x); 1; }) : 0)\n/* other */\n# define __force\t__attribute__((force))\n# define __nocast\t__attribute__((nocast))\n# define __safe\t\t__attribute__((safe))\n# define __private\t__attribute__((noderef))\n# define ACCESS_PRIVATE(p, member) (*((typeof((p)->member) __force *) &(p)->member))\n#else /* __CHECKER__ */\n/* address spaces */\n# define __kernel\n# ifdef STRUCTLEAK_PLUGIN\n#  define __user\t__attribute__((user))\n# else\n#  define __user\n# endif\n# define __iomem\n# define __percpu\n# define __rcu\n# define __chk_user_ptr(x)\t(void)0\n# define __chk_io_ptr(x)\t(void)0\n/* context/locking */\n# define __must_hold(x)\n# define __acquires(x)\n# define __releases(x)\n# define __acquire(x)\t(void)0\n# define __release(x)\t(void)0\n# define __cond_lock(x,c) (c)\n/* other */\n# define __force\n# define __nocast\n# define __safe\n# define __private\n# define ACCESS_PRIVATE(p, member) ((p)->member)\n# define __builtin_warning(x, y...) (1)\n#endif /* __CHECKER__ */\n\n/* Indirect macros required for expanded argument pasting, eg. __LINE__. */\n#define ___PASTE(a,b) a##b\n#define __PASTE(a,b) ___PASTE(a,b)\n\n#ifdef __KERNEL__\n\n/* Attributes */\n#include <linux/compiler_attributes.h>\n\n/* Builtins */\n\n/*\n * __has_builtin is supported on gcc >= 10, clang >= 3 and icc >= 21.\n * In the meantime, to support gcc < 10, we implement __has_builtin\n * by hand.\n */\n#ifndef __has_builtin\n#define __has_builtin(x) (0)\n#endif\n\n/* Compiler specific macros. */\n#ifdef __clang__\n#include <linux/compiler-clang.h>\n#elif defined(__INTEL_COMPILER)\n#include <linux/compiler-intel.h>\n#elif defined(__GNUC__)\n/* The above compilers also define __GNUC__, so order is important here. */\n#include <linux/compiler-gcc.h>\n#else\n#error \"Unknown compiler\"\n#endif\n\n/*\n * Some architectures need to provide custom definitions of macros provided\n * by linux/compiler-*.h, and can do so using asm/compiler.h. We include that\n * conditionally rather than using an asm-generic wrapper in order to avoid\n * build failures if any C compilation, which will include this file via an\n * -include argument in c_flags, occurs prior to the asm-generic wrappers being\n * generated.\n */\n#ifdef CONFIG_HAVE_ARCH_COMPILER_H\n#include <asm/compiler.h>\n#endif\n\nstruct ftrace_branch_data {\n\tconst char *func;\n\tconst char *file;\n\tunsigned line;\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long correct;\n\t\t\tunsigned long incorrect;\n\t\t};\n\t\tstruct {\n\t\t\tunsigned long miss;\n\t\t\tunsigned long hit;\n\t\t};\n\t\tunsigned long miss_hit[2];\n\t};\n};\n\nstruct ftrace_likely_data {\n\tstruct ftrace_branch_data\tdata;\n\tunsigned long\t\t\tconstant;\n};\n\n#ifdef CONFIG_ENABLE_MUST_CHECK\n#define __must_check\t\t__attribute__((__warn_unused_result__))\n#else\n#define __must_check\n#endif\n\n#if defined(CC_USING_HOTPATCH)\n#define notrace\t\t\t__attribute__((hotpatch(0, 0)))\n#elif defined(CC_USING_PATCHABLE_FUNCTION_ENTRY)\n#define notrace\t\t\t__attribute__((patchable_function_entry(0, 0)))\n#else\n#define notrace\t\t\t__attribute__((__no_instrument_function__))\n#endif\n\n/*\n * it doesn't make sense on ARM (currently the only user of __naked)\n * to trace naked functions because then mcount is called without\n * stack and frame pointer being set up and there is no chance to\n * restore the lr register to the value before mcount was called.\n */\n#define __naked\t\t\t__attribute__((__naked__)) notrace\n\n#define __compiler_offsetof(a, b)\t__builtin_offsetof(a, b)\n\n/*\n * Prefer gnu_inline, so that extern inline functions do not emit an\n * externally visible function. This makes extern inline behave as per gnu89\n * semantics rather than c99. This prevents multiple symbol definition errors\n * of extern inline functions at link time.\n * A lot of inline functions can cause havoc with function tracing.\n */\n#define inline inline __gnu_inline __inline_maybe_unused notrace\n\n/*\n * gcc provides both __inline__ and __inline as alternate spellings of\n * the inline keyword, though the latter is undocumented. New kernel\n * code should only use the inline spelling, but some existing code\n * uses __inline__. Since we #define inline above, to ensure\n * __inline__ has the same semantics, we need this #define.\n *\n * However, the spelling __inline is strictly reserved for referring\n * to the bare keyword.\n */\n#define __inline__ inline\n\n/*\n * GCC does not warn about unused static inline functions for -Wunused-function.\n * Suppress the warning in clang as well by using __maybe_unused, but enable it\n * for W=1 build. This will allow clang to find unused functions. Remove the\n * __inline_maybe_unused entirely after fixing most of -Wunused-function warnings.\n */\n#ifdef KBUILD_EXTRA_WARN1\n#define __inline_maybe_unused\n#else\n#define __inline_maybe_unused __maybe_unused\n#endif\n\n/*\n * Rather then using noinline to prevent stack consumption, use\n * noinline_for_stack instead.  For documentation reasons.\n */\n#define noinline_for_stack noinline\n\n/*\n * Sanitizer helper attributes: Because using __always_inline and\n * __no_sanitize_* conflict, provide helper attributes that will either expand\n * to __no_sanitize_* in compilation units where instrumentation is enabled\n * (__SANITIZE_*__), or __always_inline in compilation units without\n * instrumentation (__SANITIZE_*__ undefined).\n */\n#ifdef __SANITIZE_ADDRESS__\n/*\n * We can't declare function 'inline' because __no_sanitize_address conflicts\n * with inlining. Attempt to inline it may cause a build failure.\n *     https://gcc.gnu.org/bugzilla/show_bug.cgi?id=67368\n * '__maybe_unused' allows us to avoid defined-but-not-used warnings.\n */\n# define __no_kasan_or_inline __no_sanitize_address notrace __maybe_unused\n# define __no_sanitize_or_inline __no_kasan_or_inline\n#else\n# define __no_kasan_or_inline __always_inline\n#endif\n\n#define __no_kcsan __no_sanitize_thread\n#ifdef __SANITIZE_THREAD__\n# define __no_sanitize_or_inline __no_kcsan notrace __maybe_unused\n#endif\n\n#ifndef __no_sanitize_or_inline\n#define __no_sanitize_or_inline __always_inline\n#endif\n\n/* Section for code which can't be instrumented at all */\n#define noinstr\t\t\t\t\t\t\t\t\\\n\tnoinline notrace __attribute((__section__(\".noinstr.text\")))\t\\\n\t__no_kcsan __no_sanitize_address\n\n#endif /* __KERNEL__ */\n\n#endif /* __ASSEMBLY__ */\n\n/*\n * The below symbols may be defined for one or more, but not ALL, of the above\n * compilers. We don't consider that to be an error, so set them to nothing.\n * For example, some of them are for compiler specific plugins.\n */\n#ifndef __latent_entropy\n# define __latent_entropy\n#endif\n\n#ifndef __randomize_layout\n# define __randomize_layout __designated_init\n#endif\n\n#ifndef __no_randomize_layout\n# define __no_randomize_layout\n#endif\n\n#ifndef randomized_struct_fields_start\n# define randomized_struct_fields_start\n# define randomized_struct_fields_end\n#endif\n\n#ifndef __noscs\n# define __noscs\n#endif\n\n#ifndef asm_volatile_goto\n#define asm_volatile_goto(x...) asm goto(x)\n#endif\n\n#ifdef CONFIG_CC_HAS_ASM_INLINE\n#define asm_inline asm __inline\n#else\n#define asm_inline asm\n#endif\n\n/* Are two types/vars the same type (ignoring qualifiers)? */\n#define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b))\n\n/*\n * __unqual_scalar_typeof(x) - Declare an unqualified scalar type, leaving\n *\t\t\t       non-scalar types unchanged.\n */\n/*\n * Prefer C11 _Generic for better compile-times and simpler code. Note: 'char'\n * is not type-compatible with 'signed char', and we define a separate case.\n */\n#define __scalar_type_to_expr_cases(type)\t\t\t\t\\\n\t\tunsigned type:\t(unsigned type)0,\t\t\t\\\n\t\tsigned type:\t(signed type)0\n\n#define __unqual_scalar_typeof(x) typeof(\t\t\t\t\\\n\t\t_Generic((x),\t\t\t\t\t\t\\\n\t\t\t char:\t(char)0,\t\t\t\t\\\n\t\t\t __scalar_type_to_expr_cases(char),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(short),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(int),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(long),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(long long),\t\\\n\t\t\t default: (x)))\n\n/* Is this type a native word size -- useful for atomic operations */\n#define __native_word(t) \\\n\t(sizeof(t) == sizeof(char) || sizeof(t) == sizeof(short) || \\\n\t sizeof(t) == sizeof(int) || sizeof(t) == sizeof(long))\n\n/* Compile time object size, -1 for unknown */\n#ifndef __compiletime_object_size\n# define __compiletime_object_size(obj) -1\n#endif\n#ifndef __compiletime_warning\n# define __compiletime_warning(message)\n#endif\n#ifndef __compiletime_error\n# define __compiletime_error(message)\n#endif\n\n#ifdef __OPTIMIZE__\n# define __compiletime_assert(condition, msg, prefix, suffix)\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\textern void prefix ## suffix(void) __compiletime_error(msg); \\\n\t\tif (!(condition))\t\t\t\t\t\\\n\t\t\tprefix ## suffix();\t\t\t\t\\\n\t} while (0)\n#else\n# define __compiletime_assert(condition, msg, prefix, suffix) do { } while (0)\n#endif\n\n#define _compiletime_assert(condition, msg, prefix, suffix) \\\n\t__compiletime_assert(condition, msg, prefix, suffix)\n\n/**\n * compiletime_assert - break build and emit msg if condition is false\n * @condition: a compile-time constant condition to check\n * @msg:       a message to emit if condition is false\n *\n * In tradition of POSIX assert, this macro will break the build if the\n * supplied condition is *false*, emitting the supplied error message if the\n * compiler has support to do so.\n */\n#define compiletime_assert(condition, msg) \\\n\t_compiletime_assert(condition, msg, __compiletime_assert_, __COUNTER__)\n\n#define compiletime_assert_atomic_type(t)\t\t\t\t\\\n\tcompiletime_assert(__native_word(t),\t\t\t\t\\\n\t\t\"Need native word sized stores/loads for atomicity.\")\n\n/* Helpers for emitting diagnostics in pragmas. */\n#ifndef __diag\n#define __diag(string)\n#endif\n\n#ifndef __diag_GCC\n#define __diag_GCC(version, severity, string)\n#endif\n\n#define __diag_push()\t__diag(push)\n#define __diag_pop()\t__diag(pop)\n\n#define __diag_ignore(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, ignore, option)\n#define __diag_warn(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, warn, option)\n#define __diag_error(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, error, option)\n\n#endif /* __LINUX_COMPILER_TYPES_H */\n"}, "5": {"id": 5, "path": "/src/include/asm-generic/bug.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_GENERIC_BUG_H\n#define _ASM_GENERIC_BUG_H\n\n#include <linux/compiler.h>\n#include <linux/instrumentation.h>\n\n#define CUT_HERE\t\t\"------------[ cut here ]------------\\n\"\n\n#ifdef CONFIG_GENERIC_BUG\n#define BUGFLAG_WARNING\t\t(1 << 0)\n#define BUGFLAG_ONCE\t\t(1 << 1)\n#define BUGFLAG_DONE\t\t(1 << 2)\n#define BUGFLAG_NO_CUT_HERE\t(1 << 3)\t/* CUT_HERE already sent */\n#define BUGFLAG_TAINT(taint)\t((taint) << 8)\n#define BUG_GET_TAINT(bug)\t((bug)->flags >> 8)\n#endif\n\n#ifndef __ASSEMBLY__\n#include <linux/kernel.h>\n\n#ifdef CONFIG_BUG\n\n#ifdef CONFIG_GENERIC_BUG\nstruct bug_entry {\n#ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS\n\tunsigned long\tbug_addr;\n#else\n\tsigned int\tbug_addr_disp;\n#endif\n#ifdef CONFIG_DEBUG_BUGVERBOSE\n#ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS\n\tconst char\t*file;\n#else\n\tsigned int\tfile_disp;\n#endif\n\tunsigned short\tline;\n#endif\n\tunsigned short\tflags;\n};\n#endif\t/* CONFIG_GENERIC_BUG */\n\n/*\n * Don't use BUG() or BUG_ON() unless there's really no way out; one\n * example might be detecting data structure corruption in the middle\n * of an operation that can't be backed out of.  If the (sub)system\n * can somehow continue operating, perhaps with reduced functionality,\n * it's probably not BUG-worthy.\n *\n * If you're tempted to BUG(), think again:  is completely giving up\n * really the *only* solution?  There are usually better options, where\n * users don't need to reboot ASAP and can mostly shut down cleanly.\n */\n#ifndef HAVE_ARCH_BUG\n#define BUG() do { \\\n\tprintk(\"BUG: failure at %s:%d/%s()!\\n\", __FILE__, __LINE__, __func__); \\\n\tbarrier_before_unreachable(); \\\n\tpanic(\"BUG!\"); \\\n} while (0)\n#endif\n\n#ifndef HAVE_ARCH_BUG_ON\n#define BUG_ON(condition) do { if (unlikely(condition)) BUG(); } while (0)\n#endif\n\n/*\n * WARN(), WARN_ON(), WARN_ON_ONCE, and so on can be used to report\n * significant kernel issues that need prompt attention if they should ever\n * appear at runtime.\n *\n * Do not use these macros when checking for invalid external inputs\n * (e.g. invalid system call arguments, or invalid data coming from\n * network/devices), and on transient conditions like ENOMEM or EAGAIN.\n * These macros should be used for recoverable kernel issues only.\n * For invalid external inputs, transient conditions, etc use\n * pr_err[_once/_ratelimited]() followed by dump_stack(), if necessary.\n * Do not include \"BUG\"/\"WARNING\" in format strings manually to make these\n * conditions distinguishable from kernel issues.\n *\n * Use the versions with printk format strings to provide better diagnostics.\n */\n#ifndef __WARN_FLAGS\nextern __printf(4, 5)\nvoid warn_slowpath_fmt(const char *file, const int line, unsigned taint,\n\t\t       const char *fmt, ...);\n#define __WARN()\t\t__WARN_printf(TAINT_WARN, NULL)\n#define __WARN_printf(taint, arg...) do {\t\t\t\t\\\n\t\tinstrumentation_begin();\t\t\t\t\\\n\t\twarn_slowpath_fmt(__FILE__, __LINE__, taint, arg);\t\\\n\t\tinstrumentation_end();\t\t\t\t\t\\\n\t} while (0)\n#else\nextern __printf(1, 2) void __warn_printk(const char *fmt, ...);\n#define __WARN()\t\t__WARN_FLAGS(BUGFLAG_TAINT(TAINT_WARN))\n#define __WARN_printf(taint, arg...) do {\t\t\t\t\\\n\t\tinstrumentation_begin();\t\t\t\t\\\n\t\t__warn_printk(arg);\t\t\t\t\t\\\n\t\t__WARN_FLAGS(BUGFLAG_NO_CUT_HERE | BUGFLAG_TAINT(taint));\\\n\t\tinstrumentation_end();\t\t\t\t\t\\\n\t} while (0)\n#define WARN_ON_ONCE(condition) ({\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\\\n\t\t__WARN_FLAGS(BUGFLAG_ONCE |\t\t\t\\\n\t\t\t     BUGFLAG_TAINT(TAINT_WARN));\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\\\n})\n#endif\n\n/* used internally by panic.c */\nstruct warn_args;\nstruct pt_regs;\n\nvoid __warn(const char *file, int line, void *caller, unsigned taint,\n\t    struct pt_regs *regs, struct warn_args *args);\n\n#ifndef WARN_ON\n#define WARN_ON(condition) ({\t\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\t\\\n\t\t__WARN();\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#ifndef WARN\n#define WARN(condition, format...) ({\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\t\\\n\t\t__WARN_printf(TAINT_WARN, format);\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#define WARN_TAINT(condition, taint, format...) ({\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\t\\\n\t\t__WARN_printf(taint, format);\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n\n#ifndef WARN_ON_ONCE\n#define WARN_ON_ONCE(condition)\t({\t\t\t\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\\\n\tint __ret_warn_once = !!(condition);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (unlikely(__ret_warn_once && !__warned)) {\t\t\\\n\t\t__warned = true;\t\t\t\t\\\n\t\tWARN_ON(1);\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\\\n})\n#endif\n\n#define WARN_ONCE(condition, format...)\t({\t\t\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\\\n\tint __ret_warn_once = !!(condition);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (unlikely(__ret_warn_once && !__warned)) {\t\t\\\n\t\t__warned = true;\t\t\t\t\\\n\t\tWARN(1, format);\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\\\n})\n\n#define WARN_TAINT_ONCE(condition, taint, format...)\t({\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\\\n\tint __ret_warn_once = !!(condition);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (unlikely(__ret_warn_once && !__warned)) {\t\t\\\n\t\t__warned = true;\t\t\t\t\\\n\t\tWARN_TAINT(1, taint, format);\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\\\n})\n\n#else /* !CONFIG_BUG */\n#ifndef HAVE_ARCH_BUG\n#define BUG() do {} while (1)\n#endif\n\n#ifndef HAVE_ARCH_BUG_ON\n#define BUG_ON(condition) do { if (unlikely(condition)) BUG(); } while (0)\n#endif\n\n#ifndef HAVE_ARCH_WARN_ON\n#define WARN_ON(condition) ({\t\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#ifndef WARN\n#define WARN(condition, format...) ({\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tno_printk(format);\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#define WARN_ON_ONCE(condition) WARN_ON(condition)\n#define WARN_ONCE(condition, format...) WARN(condition, format)\n#define WARN_TAINT(condition, taint, format...) WARN(condition, format)\n#define WARN_TAINT_ONCE(condition, taint, format...) WARN(condition, format)\n\n#endif\n\n/*\n * WARN_ON_SMP() is for cases that the warning is either\n * meaningless for !SMP or may even cause failures.\n * It can also be used with values that are only defined\n * on SMP:\n *\n * struct foo {\n *  [...]\n * #ifdef CONFIG_SMP\n *\tint bar;\n * #endif\n * };\n *\n * void func(struct foo *zoot)\n * {\n *\tWARN_ON_SMP(!zoot->bar);\n *\n * For CONFIG_SMP, WARN_ON_SMP() should act the same as WARN_ON(),\n * and should be a nop and return false for uniprocessor.\n *\n * if (WARN_ON_SMP(x)) returns true only when CONFIG_SMP is set\n * and x is true.\n */\n#ifdef CONFIG_SMP\n# define WARN_ON_SMP(x)\t\t\tWARN_ON(x)\n#else\n/*\n * Use of ({0;}) because WARN_ON_SMP(x) may be used either as\n * a stand alone line statement or as a condition in an if ()\n * statement.\n * A simple \"0\" would cause gcc to give a \"statement has no effect\"\n * warning.\n */\n# define WARN_ON_SMP(x)\t\t\t({0;})\n#endif\n\n#endif /* __ASSEMBLY__ */\n\n#endif\n"}, "0": {"id": 0, "path": "/src/include/linux/list.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_LIST_H\n#define _LINUX_LIST_H\n\n#include <linux/types.h>\n#include <linux/stddef.h>\n#include <linux/poison.h>\n#include <linux/const.h>\n#include <linux/kernel.h>\n\n/*\n * Circular doubly linked list implementation.\n *\n * Some of the internal functions (\"__xxx\") are useful when\n * manipulating whole lists rather than single entries, as\n * sometimes we already know the next/prev entries and we can\n * generate better code by using them directly rather than\n * using the generic single-entry routines.\n */\n\n#define LIST_HEAD_INIT(name) { &(name), &(name) }\n\n#define LIST_HEAD(name) \\\n\tstruct list_head name = LIST_HEAD_INIT(name)\n\n/**\n * INIT_LIST_HEAD - Initialize a list_head structure\n * @list: list_head structure to be initialized.\n *\n * Initializes the list_head to point to itself.  If it is a list header,\n * the result is an empty list.\n */\nstatic inline void INIT_LIST_HEAD(struct list_head *list)\n{\n\tWRITE_ONCE(list->next, list);\n\tlist->prev = list;\n}\n\n#ifdef CONFIG_DEBUG_LIST\nextern bool __list_add_valid(struct list_head *new,\n\t\t\t      struct list_head *prev,\n\t\t\t      struct list_head *next);\nextern bool __list_del_entry_valid(struct list_head *entry);\n#else\nstatic inline bool __list_add_valid(struct list_head *new,\n\t\t\t\tstruct list_head *prev,\n\t\t\t\tstruct list_head *next)\n{\n\treturn true;\n}\nstatic inline bool __list_del_entry_valid(struct list_head *entry)\n{\n\treturn true;\n}\n#endif\n\n/*\n * Insert a new entry between two known consecutive entries.\n *\n * This is only for internal list manipulation where we know\n * the prev/next entries already!\n */\nstatic inline void __list_add(struct list_head *new,\n\t\t\t      struct list_head *prev,\n\t\t\t      struct list_head *next)\n{\n\tif (!__list_add_valid(new, prev, next))\n\t\treturn;\n\n\tnext->prev = new;\n\tnew->next = next;\n\tnew->prev = prev;\n\tWRITE_ONCE(prev->next, new);\n}\n\n/**\n * list_add - add a new entry\n * @new: new entry to be added\n * @head: list head to add it after\n *\n * Insert a new entry after the specified head.\n * This is good for implementing stacks.\n */\nstatic inline void list_add(struct list_head *new, struct list_head *head)\n{\n\t__list_add(new, head, head->next);\n}\n\n\n/**\n * list_add_tail - add a new entry\n * @new: new entry to be added\n * @head: list head to add it before\n *\n * Insert a new entry before the specified head.\n * This is useful for implementing queues.\n */\nstatic inline void list_add_tail(struct list_head *new, struct list_head *head)\n{\n\t__list_add(new, head->prev, head);\n}\n\n/*\n * Delete a list entry by making the prev/next entries\n * point to each other.\n *\n * This is only for internal list manipulation where we know\n * the prev/next entries already!\n */\nstatic inline void __list_del(struct list_head * prev, struct list_head * next)\n{\n\tnext->prev = prev;\n\tWRITE_ONCE(prev->next, next);\n}\n\n/*\n * Delete a list entry and clear the 'prev' pointer.\n *\n * This is a special-purpose list clearing method used in the networking code\n * for lists allocated as per-cpu, where we don't want to incur the extra\n * WRITE_ONCE() overhead of a regular list_del_init(). The code that uses this\n * needs to check the node 'prev' pointer instead of calling list_empty().\n */\nstatic inline void __list_del_clearprev(struct list_head *entry)\n{\n\t__list_del(entry->prev, entry->next);\n\tentry->prev = NULL;\n}\n\nstatic inline void __list_del_entry(struct list_head *entry)\n{\n\tif (!__list_del_entry_valid(entry))\n\t\treturn;\n\n\t__list_del(entry->prev, entry->next);\n}\n\n/**\n * list_del - deletes entry from list.\n * @entry: the element to delete from the list.\n * Note: list_empty() on entry does not return true after this, the entry is\n * in an undefined state.\n */\nstatic inline void list_del(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tentry->next = LIST_POISON1;\n\tentry->prev = LIST_POISON2;\n}\n\n/**\n * list_replace - replace old entry by new one\n * @old : the element to be replaced\n * @new : the new element to insert\n *\n * If @old was empty, it will be overwritten.\n */\nstatic inline void list_replace(struct list_head *old,\n\t\t\t\tstruct list_head *new)\n{\n\tnew->next = old->next;\n\tnew->next->prev = new;\n\tnew->prev = old->prev;\n\tnew->prev->next = new;\n}\n\n/**\n * list_replace_init - replace old entry by new one and initialize the old one\n * @old : the element to be replaced\n * @new : the new element to insert\n *\n * If @old was empty, it will be overwritten.\n */\nstatic inline void list_replace_init(struct list_head *old,\n\t\t\t\t     struct list_head *new)\n{\n\tlist_replace(old, new);\n\tINIT_LIST_HEAD(old);\n}\n\n/**\n * list_swap - replace entry1 with entry2 and re-add entry1 at entry2's position\n * @entry1: the location to place entry2\n * @entry2: the location to place entry1\n */\nstatic inline void list_swap(struct list_head *entry1,\n\t\t\t     struct list_head *entry2)\n{\n\tstruct list_head *pos = entry2->prev;\n\n\tlist_del(entry2);\n\tlist_replace(entry1, entry2);\n\tif (pos == entry1)\n\t\tpos = entry2;\n\tlist_add(entry1, pos);\n}\n\n/**\n * list_del_init - deletes entry from list and reinitialize it.\n * @entry: the element to delete from the list.\n */\nstatic inline void list_del_init(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tINIT_LIST_HEAD(entry);\n}\n\n/**\n * list_move - delete from one list and add as another's head\n * @list: the entry to move\n * @head: the head that will precede our entry\n */\nstatic inline void list_move(struct list_head *list, struct list_head *head)\n{\n\t__list_del_entry(list);\n\tlist_add(list, head);\n}\n\n/**\n * list_move_tail - delete from one list and add as another's tail\n * @list: the entry to move\n * @head: the head that will follow our entry\n */\nstatic inline void list_move_tail(struct list_head *list,\n\t\t\t\t  struct list_head *head)\n{\n\t__list_del_entry(list);\n\tlist_add_tail(list, head);\n}\n\n/**\n * list_bulk_move_tail - move a subsection of a list to its tail\n * @head: the head that will follow our entry\n * @first: first entry to move\n * @last: last entry to move, can be the same as first\n *\n * Move all entries between @first and including @last before @head.\n * All three entries must belong to the same linked list.\n */\nstatic inline void list_bulk_move_tail(struct list_head *head,\n\t\t\t\t       struct list_head *first,\n\t\t\t\t       struct list_head *last)\n{\n\tfirst->prev->next = last->next;\n\tlast->next->prev = first->prev;\n\n\thead->prev->next = first;\n\tfirst->prev = head->prev;\n\n\tlast->next = head;\n\thead->prev = last;\n}\n\n/**\n * list_is_first -- tests whether @list is the first entry in list @head\n * @list: the entry to test\n * @head: the head of the list\n */\nstatic inline int list_is_first(const struct list_head *list,\n\t\t\t\t\tconst struct list_head *head)\n{\n\treturn list->prev == head;\n}\n\n/**\n * list_is_last - tests whether @list is the last entry in list @head\n * @list: the entry to test\n * @head: the head of the list\n */\nstatic inline int list_is_last(const struct list_head *list,\n\t\t\t\tconst struct list_head *head)\n{\n\treturn list->next == head;\n}\n\n/**\n * list_empty - tests whether a list is empty\n * @head: the list to test.\n */\nstatic inline int list_empty(const struct list_head *head)\n{\n\treturn READ_ONCE(head->next) == head;\n}\n\n/**\n * list_del_init_careful - deletes entry from list and reinitialize it.\n * @entry: the element to delete from the list.\n *\n * This is the same as list_del_init(), except designed to be used\n * together with list_empty_careful() in a way to guarantee ordering\n * of other memory operations.\n *\n * Any memory operations done before a list_del_init_careful() are\n * guaranteed to be visible after a list_empty_careful() test.\n */\nstatic inline void list_del_init_careful(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tentry->prev = entry;\n\tsmp_store_release(&entry->next, entry);\n}\n\n/**\n * list_empty_careful - tests whether a list is empty and not being modified\n * @head: the list to test\n *\n * Description:\n * tests whether a list is empty _and_ checks that no other CPU might be\n * in the process of modifying either member (next or prev)\n *\n * NOTE: using list_empty_careful() without synchronization\n * can only be safe if the only activity that can happen\n * to the list entry is list_del_init(). Eg. it cannot be used\n * if another CPU could re-list_add() it.\n */\nstatic inline int list_empty_careful(const struct list_head *head)\n{\n\tstruct list_head *next = smp_load_acquire(&head->next);\n\treturn (next == head) && (next == head->prev);\n}\n\n/**\n * list_rotate_left - rotate the list to the left\n * @head: the head of the list\n */\nstatic inline void list_rotate_left(struct list_head *head)\n{\n\tstruct list_head *first;\n\n\tif (!list_empty(head)) {\n\t\tfirst = head->next;\n\t\tlist_move_tail(first, head);\n\t}\n}\n\n/**\n * list_rotate_to_front() - Rotate list to specific item.\n * @list: The desired new front of the list.\n * @head: The head of the list.\n *\n * Rotates list so that @list becomes the new front of the list.\n */\nstatic inline void list_rotate_to_front(struct list_head *list,\n\t\t\t\t\tstruct list_head *head)\n{\n\t/*\n\t * Deletes the list head from the list denoted by @head and\n\t * places it as the tail of @list, this effectively rotates the\n\t * list so that @list is at the front.\n\t */\n\tlist_move_tail(head, list);\n}\n\n/**\n * list_is_singular - tests whether a list has just one entry.\n * @head: the list to test.\n */\nstatic inline int list_is_singular(const struct list_head *head)\n{\n\treturn !list_empty(head) && (head->next == head->prev);\n}\n\nstatic inline void __list_cut_position(struct list_head *list,\n\t\tstruct list_head *head, struct list_head *entry)\n{\n\tstruct list_head *new_first = entry->next;\n\tlist->next = head->next;\n\tlist->next->prev = list;\n\tlist->prev = entry;\n\tentry->next = list;\n\thead->next = new_first;\n\tnew_first->prev = head;\n}\n\n/**\n * list_cut_position - cut a list into two\n * @list: a new list to add all removed entries\n * @head: a list with entries\n * @entry: an entry within head, could be the head itself\n *\tand if so we won't cut the list\n *\n * This helper moves the initial part of @head, up to and\n * including @entry, from @head to @list. You should\n * pass on @entry an element you know is on @head. @list\n * should be an empty list or a list you do not care about\n * losing its data.\n *\n */\nstatic inline void list_cut_position(struct list_head *list,\n\t\tstruct list_head *head, struct list_head *entry)\n{\n\tif (list_empty(head))\n\t\treturn;\n\tif (list_is_singular(head) &&\n\t\t(head->next != entry && head != entry))\n\t\treturn;\n\tif (entry == head)\n\t\tINIT_LIST_HEAD(list);\n\telse\n\t\t__list_cut_position(list, head, entry);\n}\n\n/**\n * list_cut_before - cut a list into two, before given entry\n * @list: a new list to add all removed entries\n * @head: a list with entries\n * @entry: an entry within head, could be the head itself\n *\n * This helper moves the initial part of @head, up to but\n * excluding @entry, from @head to @list.  You should pass\n * in @entry an element you know is on @head.  @list should\n * be an empty list or a list you do not care about losing\n * its data.\n * If @entry == @head, all entries on @head are moved to\n * @list.\n */\nstatic inline void list_cut_before(struct list_head *list,\n\t\t\t\t   struct list_head *head,\n\t\t\t\t   struct list_head *entry)\n{\n\tif (head->next == entry) {\n\t\tINIT_LIST_HEAD(list);\n\t\treturn;\n\t}\n\tlist->next = head->next;\n\tlist->next->prev = list;\n\tlist->prev = entry->prev;\n\tlist->prev->next = list;\n\thead->next = entry;\n\tentry->prev = head;\n}\n\nstatic inline void __list_splice(const struct list_head *list,\n\t\t\t\t struct list_head *prev,\n\t\t\t\t struct list_head *next)\n{\n\tstruct list_head *first = list->next;\n\tstruct list_head *last = list->prev;\n\n\tfirst->prev = prev;\n\tprev->next = first;\n\n\tlast->next = next;\n\tnext->prev = last;\n}\n\n/**\n * list_splice - join two lists, this is designed for stacks\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n */\nstatic inline void list_splice(const struct list_head *list,\n\t\t\t\tstruct list_head *head)\n{\n\tif (!list_empty(list))\n\t\t__list_splice(list, head, head->next);\n}\n\n/**\n * list_splice_tail - join two lists, each list being a queue\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n */\nstatic inline void list_splice_tail(struct list_head *list,\n\t\t\t\tstruct list_head *head)\n{\n\tif (!list_empty(list))\n\t\t__list_splice(list, head->prev, head);\n}\n\n/**\n * list_splice_init - join two lists and reinitialise the emptied list.\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n *\n * The list at @list is reinitialised\n */\nstatic inline void list_splice_init(struct list_head *list,\n\t\t\t\t    struct list_head *head)\n{\n\tif (!list_empty(list)) {\n\t\t__list_splice(list, head, head->next);\n\t\tINIT_LIST_HEAD(list);\n\t}\n}\n\n/**\n * list_splice_tail_init - join two lists and reinitialise the emptied list\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n *\n * Each of the lists is a queue.\n * The list at @list is reinitialised\n */\nstatic inline void list_splice_tail_init(struct list_head *list,\n\t\t\t\t\t struct list_head *head)\n{\n\tif (!list_empty(list)) {\n\t\t__list_splice(list, head->prev, head);\n\t\tINIT_LIST_HEAD(list);\n\t}\n}\n\n/**\n * list_entry - get the struct for this entry\n * @ptr:\tthe &struct list_head pointer.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_entry(ptr, type, member) \\\n\tcontainer_of(ptr, type, member)\n\n/**\n * list_first_entry - get the first element from a list\n * @ptr:\tthe list head to take the element from.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n *\n * Note, that list is expected to be not empty.\n */\n#define list_first_entry(ptr, type, member) \\\n\tlist_entry((ptr)->next, type, member)\n\n/**\n * list_last_entry - get the last element from a list\n * @ptr:\tthe list head to take the element from.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n *\n * Note, that list is expected to be not empty.\n */\n#define list_last_entry(ptr, type, member) \\\n\tlist_entry((ptr)->prev, type, member)\n\n/**\n * list_first_entry_or_null - get the first element from a list\n * @ptr:\tthe list head to take the element from.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n *\n * Note that if the list is empty, it returns NULL.\n */\n#define list_first_entry_or_null(ptr, type, member) ({ \\\n\tstruct list_head *head__ = (ptr); \\\n\tstruct list_head *pos__ = READ_ONCE(head__->next); \\\n\tpos__ != head__ ? list_entry(pos__, type, member) : NULL; \\\n})\n\n/**\n * list_next_entry - get the next element in list\n * @pos:\tthe type * to cursor\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_next_entry(pos, member) \\\n\tlist_entry((pos)->member.next, typeof(*(pos)), member)\n\n/**\n * list_prev_entry - get the prev element in list\n * @pos:\tthe type * to cursor\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_prev_entry(pos, member) \\\n\tlist_entry((pos)->member.prev, typeof(*(pos)), member)\n\n/**\n * list_for_each\t-\titerate over a list\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @head:\tthe head for your list.\n */\n#define list_for_each(pos, head) \\\n\tfor (pos = (head)->next; pos != (head); pos = pos->next)\n\n/**\n * list_for_each_continue - continue iteration over a list\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @head:\tthe head for your list.\n *\n * Continue to iterate over a list, continuing after the current position.\n */\n#define list_for_each_continue(pos, head) \\\n\tfor (pos = pos->next; pos != (head); pos = pos->next)\n\n/**\n * list_for_each_prev\t-\titerate over a list backwards\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @head:\tthe head for your list.\n */\n#define list_for_each_prev(pos, head) \\\n\tfor (pos = (head)->prev; pos != (head); pos = pos->prev)\n\n/**\n * list_for_each_safe - iterate over a list safe against removal of list entry\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @n:\t\tanother &struct list_head to use as temporary storage\n * @head:\tthe head for your list.\n */\n#define list_for_each_safe(pos, n, head) \\\n\tfor (pos = (head)->next, n = pos->next; pos != (head); \\\n\t\tpos = n, n = pos->next)\n\n/**\n * list_for_each_prev_safe - iterate over a list backwards safe against removal of list entry\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @n:\t\tanother &struct list_head to use as temporary storage\n * @head:\tthe head for your list.\n */\n#define list_for_each_prev_safe(pos, n, head) \\\n\tfor (pos = (head)->prev, n = pos->prev; \\\n\t     pos != (head); \\\n\t     pos = n, n = pos->prev)\n\n/**\n * list_entry_is_head - test if the entry points to the head of the list\n * @pos:\tthe type * to cursor\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_entry_is_head(pos, head, member)\t\t\t\t\\\n\t(&pos->member == (head))\n\n/**\n * list_for_each_entry\t-\titerate over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_for_each_entry(pos, head, member)\t\t\t\t\\\n\tfor (pos = list_first_entry(head, typeof(*pos), member);\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_next_entry(pos, member))\n\n/**\n * list_for_each_entry_reverse - iterate backwards over list of given type.\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_for_each_entry_reverse(pos, head, member)\t\t\t\\\n\tfor (pos = list_last_entry(head, typeof(*pos), member);\t\t\\\n\t     !list_entry_is_head(pos, head, member); \t\t\t\\\n\t     pos = list_prev_entry(pos, member))\n\n/**\n * list_prepare_entry - prepare a pos entry for use in list_for_each_entry_continue()\n * @pos:\tthe type * to use as a start point\n * @head:\tthe head of the list\n * @member:\tthe name of the list_head within the struct.\n *\n * Prepares a pos entry for use as a start point in list_for_each_entry_continue().\n */\n#define list_prepare_entry(pos, head, member) \\\n\t((pos) ? : list_entry(head, typeof(*pos), member))\n\n/**\n * list_for_each_entry_continue - continue iteration over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Continue to iterate over list of given type, continuing after\n * the current position.\n */\n#define list_for_each_entry_continue(pos, head, member) \t\t\\\n\tfor (pos = list_next_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_next_entry(pos, member))\n\n/**\n * list_for_each_entry_continue_reverse - iterate backwards from the given point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Start to iterate over list of given type backwards, continuing after\n * the current position.\n */\n#define list_for_each_entry_continue_reverse(pos, head, member)\t\t\\\n\tfor (pos = list_prev_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_prev_entry(pos, member))\n\n/**\n * list_for_each_entry_from - iterate over list of given type from the current point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate over list of given type, continuing from current position.\n */\n#define list_for_each_entry_from(pos, head, member) \t\t\t\\\n\tfor (; !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_next_entry(pos, member))\n\n/**\n * list_for_each_entry_from_reverse - iterate backwards over list of given type\n *                                    from the current point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate backwards over list of given type, continuing from current position.\n */\n#define list_for_each_entry_from_reverse(pos, head, member)\t\t\\\n\tfor (; !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_prev_entry(pos, member))\n\n/**\n * list_for_each_entry_safe - iterate over list of given type safe against removal of list entry\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_for_each_entry_safe(pos, n, head, member)\t\t\t\\\n\tfor (pos = list_first_entry(head, typeof(*pos), member),\t\\\n\t\tn = list_next_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member); \t\t\t\\\n\t     pos = n, n = list_next_entry(n, member))\n\n/**\n * list_for_each_entry_safe_continue - continue list iteration safe against removal\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate over list of given type, continuing after current point,\n * safe against removal of list entry.\n */\n#define list_for_each_entry_safe_continue(pos, n, head, member) \t\t\\\n\tfor (pos = list_next_entry(pos, member), \t\t\t\t\\\n\t\tn = list_next_entry(pos, member);\t\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\t\\\n\t     pos = n, n = list_next_entry(n, member))\n\n/**\n * list_for_each_entry_safe_from - iterate over list from current point safe against removal\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate over list of given type from current point, safe against\n * removal of list entry.\n */\n#define list_for_each_entry_safe_from(pos, n, head, member) \t\t\t\\\n\tfor (n = list_next_entry(pos, member);\t\t\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\t\\\n\t     pos = n, n = list_next_entry(n, member))\n\n/**\n * list_for_each_entry_safe_reverse - iterate backwards over list safe against removal\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate backwards over list of given type, safe against removal\n * of list entry.\n */\n#define list_for_each_entry_safe_reverse(pos, n, head, member)\t\t\\\n\tfor (pos = list_last_entry(head, typeof(*pos), member),\t\t\\\n\t\tn = list_prev_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member); \t\t\t\\\n\t     pos = n, n = list_prev_entry(n, member))\n\n/**\n * list_safe_reset_next - reset a stale list_for_each_entry_safe loop\n * @pos:\tthe loop cursor used in the list_for_each_entry_safe loop\n * @n:\t\ttemporary storage used in list_for_each_entry_safe\n * @member:\tthe name of the list_head within the struct.\n *\n * list_safe_reset_next is not safe to use in general if the list may be\n * modified concurrently (eg. the lock is dropped in the loop body). An\n * exception to this is if the cursor element (pos) is pinned in the list,\n * and list_safe_reset_next is called after re-taking the lock and before\n * completing the current iteration of the loop body.\n */\n#define list_safe_reset_next(pos, n, member)\t\t\t\t\\\n\tn = list_next_entry(pos, member)\n\n/*\n * Double linked lists with a single pointer list head.\n * Mostly useful for hash tables where the two pointer list head is\n * too wasteful.\n * You lose the ability to access the tail in O(1).\n */\n\n#define HLIST_HEAD_INIT { .first = NULL }\n#define HLIST_HEAD(name) struct hlist_head name = {  .first = NULL }\n#define INIT_HLIST_HEAD(ptr) ((ptr)->first = NULL)\nstatic inline void INIT_HLIST_NODE(struct hlist_node *h)\n{\n\th->next = NULL;\n\th->pprev = NULL;\n}\n\n/**\n * hlist_unhashed - Has node been removed from list and reinitialized?\n * @h: Node to be checked\n *\n * Not that not all removal functions will leave a node in unhashed\n * state.  For example, hlist_nulls_del_init_rcu() does leave the\n * node in unhashed state, but hlist_nulls_del() does not.\n */\nstatic inline int hlist_unhashed(const struct hlist_node *h)\n{\n\treturn !h->pprev;\n}\n\n/**\n * hlist_unhashed_lockless - Version of hlist_unhashed for lockless use\n * @h: Node to be checked\n *\n * This variant of hlist_unhashed() must be used in lockless contexts\n * to avoid potential load-tearing.  The READ_ONCE() is paired with the\n * various WRITE_ONCE() in hlist helpers that are defined below.\n */\nstatic inline int hlist_unhashed_lockless(const struct hlist_node *h)\n{\n\treturn !READ_ONCE(h->pprev);\n}\n\n/**\n * hlist_empty - Is the specified hlist_head structure an empty hlist?\n * @h: Structure to check.\n */\nstatic inline int hlist_empty(const struct hlist_head *h)\n{\n\treturn !READ_ONCE(h->first);\n}\n\nstatic inline void __hlist_del(struct hlist_node *n)\n{\n\tstruct hlist_node *next = n->next;\n\tstruct hlist_node **pprev = n->pprev;\n\n\tWRITE_ONCE(*pprev, next);\n\tif (next)\n\t\tWRITE_ONCE(next->pprev, pprev);\n}\n\n/**\n * hlist_del - Delete the specified hlist_node from its list\n * @n: Node to delete.\n *\n * Note that this function leaves the node in hashed state.  Use\n * hlist_del_init() or similar instead to unhash @n.\n */\nstatic inline void hlist_del(struct hlist_node *n)\n{\n\t__hlist_del(n);\n\tn->next = LIST_POISON1;\n\tn->pprev = LIST_POISON2;\n}\n\n/**\n * hlist_del_init - Delete the specified hlist_node from its list and initialize\n * @n: Node to delete.\n *\n * Note that this function leaves the node in unhashed state.\n */\nstatic inline void hlist_del_init(struct hlist_node *n)\n{\n\tif (!hlist_unhashed(n)) {\n\t\t__hlist_del(n);\n\t\tINIT_HLIST_NODE(n);\n\t}\n}\n\n/**\n * hlist_add_head - add a new entry at the beginning of the hlist\n * @n: new entry to be added\n * @h: hlist head to add it after\n *\n * Insert a new entry after the specified head.\n * This is good for implementing stacks.\n */\nstatic inline void hlist_add_head(struct hlist_node *n, struct hlist_head *h)\n{\n\tstruct hlist_node *first = h->first;\n\tWRITE_ONCE(n->next, first);\n\tif (first)\n\t\tWRITE_ONCE(first->pprev, &n->next);\n\tWRITE_ONCE(h->first, n);\n\tWRITE_ONCE(n->pprev, &h->first);\n}\n\n/**\n * hlist_add_before - add a new entry before the one specified\n * @n: new entry to be added\n * @next: hlist node to add it before, which must be non-NULL\n */\nstatic inline void hlist_add_before(struct hlist_node *n,\n\t\t\t\t    struct hlist_node *next)\n{\n\tWRITE_ONCE(n->pprev, next->pprev);\n\tWRITE_ONCE(n->next, next);\n\tWRITE_ONCE(next->pprev, &n->next);\n\tWRITE_ONCE(*(n->pprev), n);\n}\n\n/**\n * hlist_add_behind - add a new entry after the one specified\n * @n: new entry to be added\n * @prev: hlist node to add it after, which must be non-NULL\n */\nstatic inline void hlist_add_behind(struct hlist_node *n,\n\t\t\t\t    struct hlist_node *prev)\n{\n\tWRITE_ONCE(n->next, prev->next);\n\tWRITE_ONCE(prev->next, n);\n\tWRITE_ONCE(n->pprev, &prev->next);\n\n\tif (n->next)\n\t\tWRITE_ONCE(n->next->pprev, &n->next);\n}\n\n/**\n * hlist_add_fake - create a fake hlist consisting of a single headless node\n * @n: Node to make a fake list out of\n *\n * This makes @n appear to be its own predecessor on a headless hlist.\n * The point of this is to allow things like hlist_del() to work correctly\n * in cases where there is no list.\n */\nstatic inline void hlist_add_fake(struct hlist_node *n)\n{\n\tn->pprev = &n->next;\n}\n\n/**\n * hlist_fake: Is this node a fake hlist?\n * @h: Node to check for being a self-referential fake hlist.\n */\nstatic inline bool hlist_fake(struct hlist_node *h)\n{\n\treturn h->pprev == &h->next;\n}\n\n/**\n * hlist_is_singular_node - is node the only element of the specified hlist?\n * @n: Node to check for singularity.\n * @h: Header for potentially singular list.\n *\n * Check whether the node is the only node of the head without\n * accessing head, thus avoiding unnecessary cache misses.\n */\nstatic inline bool\nhlist_is_singular_node(struct hlist_node *n, struct hlist_head *h)\n{\n\treturn !n->next && n->pprev == &h->first;\n}\n\n/**\n * hlist_move_list - Move an hlist\n * @old: hlist_head for old list.\n * @new: hlist_head for new list.\n *\n * Move a list from one list head to another. Fixup the pprev\n * reference of the first entry if it exists.\n */\nstatic inline void hlist_move_list(struct hlist_head *old,\n\t\t\t\t   struct hlist_head *new)\n{\n\tnew->first = old->first;\n\tif (new->first)\n\t\tnew->first->pprev = &new->first;\n\told->first = NULL;\n}\n\n#define hlist_entry(ptr, type, member) container_of(ptr,type,member)\n\n#define hlist_for_each(pos, head) \\\n\tfor (pos = (head)->first; pos ; pos = pos->next)\n\n#define hlist_for_each_safe(pos, n, head) \\\n\tfor (pos = (head)->first; pos && ({ n = pos->next; 1; }); \\\n\t     pos = n)\n\n#define hlist_entry_safe(ptr, type, member) \\\n\t({ typeof(ptr) ____ptr = (ptr); \\\n\t   ____ptr ? hlist_entry(____ptr, type, member) : NULL; \\\n\t})\n\n/**\n * hlist_for_each_entry\t- iterate over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry(pos, head, member)\t\t\t\t\\\n\tfor (pos = hlist_entry_safe((head)->first, typeof(*(pos)), member);\\\n\t     pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_continue - iterate over a hlist continuing after current point\n * @pos:\tthe type * to use as a loop cursor.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_continue(pos, member)\t\t\t\\\n\tfor (pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member);\\\n\t     pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_from - iterate over a hlist continuing from current point\n * @pos:\tthe type * to use as a loop cursor.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_from(pos, member)\t\t\t\t\\\n\tfor (; pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_safe - iterate over list of given type safe against removal of list entry\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\ta &struct hlist_node to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_safe(pos, n, head, member) \t\t\\\n\tfor (pos = hlist_entry_safe((head)->first, typeof(*pos), member);\\\n\t     pos && ({ n = pos->member.next; 1; });\t\t\t\\\n\t     pos = hlist_entry_safe(n, typeof(*pos), member))\n\n#endif\n"}, "6": {"id": 6, "path": "/src/fs/io_uring.c", "content": "// SPDX-License-Identifier: GPL-2.0\n/*\n * Shared application/kernel submission and completion ring pairs, for\n * supporting fast/efficient IO.\n *\n * A note on the read/write ordering memory barriers that are matched between\n * the application and kernel side.\n *\n * After the application reads the CQ ring tail, it must use an\n * appropriate smp_rmb() to pair with the smp_wmb() the kernel uses\n * before writing the tail (using smp_load_acquire to read the tail will\n * do). It also needs a smp_mb() before updating CQ head (ordering the\n * entry load(s) with the head store), pairing with an implicit barrier\n * through a control-dependency in io_get_cqring (smp_store_release to\n * store head will do). Failure to do so could lead to reading invalid\n * CQ entries.\n *\n * Likewise, the application must use an appropriate smp_wmb() before\n * writing the SQ tail (ordering SQ entry stores with the tail store),\n * which pairs with smp_load_acquire in io_get_sqring (smp_store_release\n * to store the tail will do). And it needs a barrier ordering the SQ\n * head load before writing new SQ entries (smp_load_acquire to read\n * head will do).\n *\n * When using the SQ poll thread (IORING_SETUP_SQPOLL), the application\n * needs to check the SQ flags for IORING_SQ_NEED_WAKEUP *after*\n * updating the SQ tail; a full memory barrier smp_mb() is needed\n * between.\n *\n * Also see the examples in the liburing library:\n *\n *\tgit://git.kernel.dk/liburing\n *\n * io_uring also uses READ/WRITE_ONCE() for _any_ store or load that happens\n * from data shared between the kernel and application. This is done both\n * for ordering purposes, but also to ensure that once a value is loaded from\n * data that the application could potentially modify, it remains stable.\n *\n * Copyright (C) 2018-2019 Jens Axboe\n * Copyright (c) 2018-2019 Christoph Hellwig\n */\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/errno.h>\n#include <linux/syscalls.h>\n#include <linux/compat.h>\n#include <net/compat.h>\n#include <linux/refcount.h>\n#include <linux/uio.h>\n#include <linux/bits.h>\n\n#include <linux/sched/signal.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/fdtable.h>\n#include <linux/mm.h>\n#include <linux/mman.h>\n#include <linux/percpu.h>\n#include <linux/slab.h>\n#include <linux/kthread.h>\n#include <linux/blkdev.h>\n#include <linux/bvec.h>\n#include <linux/net.h>\n#include <net/sock.h>\n#include <net/af_unix.h>\n#include <net/scm.h>\n#include <linux/anon_inodes.h>\n#include <linux/sched/mm.h>\n#include <linux/uaccess.h>\n#include <linux/nospec.h>\n#include <linux/sizes.h>\n#include <linux/hugetlb.h>\n#include <linux/highmem.h>\n#include <linux/namei.h>\n#include <linux/fsnotify.h>\n#include <linux/fadvise.h>\n#include <linux/eventpoll.h>\n#include <linux/fs_struct.h>\n#include <linux/splice.h>\n#include <linux/task_work.h>\n#include <linux/pagemap.h>\n#include <linux/io_uring.h>\n#include <linux/blk-cgroup.h>\n#include <linux/audit.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/io_uring.h>\n\n#include <uapi/linux/io_uring.h>\n\n#include \"internal.h\"\n#include \"io-wq.h\"\n\n#define IORING_MAX_ENTRIES\t32768\n#define IORING_MAX_CQ_ENTRIES\t(2 * IORING_MAX_ENTRIES)\n\n/*\n * Shift of 9 is 512 entries, or exactly one page on 64-bit archs\n */\n#define IORING_FILE_TABLE_SHIFT\t9\n#define IORING_MAX_FILES_TABLE\t(1U << IORING_FILE_TABLE_SHIFT)\n#define IORING_FILE_TABLE_MASK\t(IORING_MAX_FILES_TABLE - 1)\n#define IORING_MAX_FIXED_FILES\t(64 * IORING_MAX_FILES_TABLE)\n#define IORING_MAX_RESTRICTIONS\t(IORING_RESTRICTION_LAST + \\\n\t\t\t\t IORING_REGISTER_LAST + IORING_OP_LAST)\n\nstruct io_uring {\n\tu32 head ____cacheline_aligned_in_smp;\n\tu32 tail ____cacheline_aligned_in_smp;\n};\n\n/*\n * This data is shared with the application through the mmap at offsets\n * IORING_OFF_SQ_RING and IORING_OFF_CQ_RING.\n *\n * The offsets to the member fields are published through struct\n * io_sqring_offsets when calling io_uring_setup.\n */\nstruct io_rings {\n\t/*\n\t * Head and tail offsets into the ring; the offsets need to be\n\t * masked to get valid indices.\n\t *\n\t * The kernel controls head of the sq ring and the tail of the cq ring,\n\t * and the application controls tail of the sq ring and the head of the\n\t * cq ring.\n\t */\n\tstruct io_uring\t\tsq, cq;\n\t/*\n\t * Bitmasks to apply to head and tail offsets (constant, equals\n\t * ring_entries - 1)\n\t */\n\tu32\t\t\tsq_ring_mask, cq_ring_mask;\n\t/* Ring sizes (constant, power of 2) */\n\tu32\t\t\tsq_ring_entries, cq_ring_entries;\n\t/*\n\t * Number of invalid entries dropped by the kernel due to\n\t * invalid index stored in array\n\t *\n\t * Written by the kernel, shouldn't be modified by the\n\t * application (i.e. get number of \"new events\" by comparing to\n\t * cached value).\n\t *\n\t * After a new SQ head value was read by the application this\n\t * counter includes all submissions that were dropped reaching\n\t * the new SQ head (and possibly more).\n\t */\n\tu32\t\t\tsq_dropped;\n\t/*\n\t * Runtime SQ flags\n\t *\n\t * Written by the kernel, shouldn't be modified by the\n\t * application.\n\t *\n\t * The application needs a full memory barrier before checking\n\t * for IORING_SQ_NEED_WAKEUP after updating the sq tail.\n\t */\n\tu32\t\t\tsq_flags;\n\t/*\n\t * Runtime CQ flags\n\t *\n\t * Written by the application, shouldn't be modified by the\n\t * kernel.\n\t */\n\tu32                     cq_flags;\n\t/*\n\t * Number of completion events lost because the queue was full;\n\t * this should be avoided by the application by making sure\n\t * there are not more requests pending than there is space in\n\t * the completion queue.\n\t *\n\t * Written by the kernel, shouldn't be modified by the\n\t * application (i.e. get number of \"new events\" by comparing to\n\t * cached value).\n\t *\n\t * As completion events come in out of order this counter is not\n\t * ordered with any other data.\n\t */\n\tu32\t\t\tcq_overflow;\n\t/*\n\t * Ring buffer of completion events.\n\t *\n\t * The kernel writes completion events fresh every time they are\n\t * produced, so the application is allowed to modify pending\n\t * entries.\n\t */\n\tstruct io_uring_cqe\tcqes[] ____cacheline_aligned_in_smp;\n};\n\nstruct io_mapped_ubuf {\n\tu64\t\tubuf;\n\tsize_t\t\tlen;\n\tstruct\t\tbio_vec *bvec;\n\tunsigned int\tnr_bvecs;\n\tunsigned long\tacct_pages;\n};\n\nstruct fixed_file_table {\n\tstruct file\t\t**files;\n};\n\nstruct fixed_file_ref_node {\n\tstruct percpu_ref\t\trefs;\n\tstruct list_head\t\tnode;\n\tstruct list_head\t\tfile_list;\n\tstruct fixed_file_data\t\t*file_data;\n\tstruct llist_node\t\tllist;\n\tbool\t\t\t\tdone;\n};\n\nstruct fixed_file_data {\n\tstruct fixed_file_table\t\t*table;\n\tstruct io_ring_ctx\t\t*ctx;\n\n\tstruct fixed_file_ref_node\t*node;\n\tstruct percpu_ref\t\trefs;\n\tstruct completion\t\tdone;\n\tstruct list_head\t\tref_list;\n\tspinlock_t\t\t\tlock;\n};\n\nstruct io_buffer {\n\tstruct list_head list;\n\t__u64 addr;\n\t__s32 len;\n\t__u16 bid;\n};\n\nstruct io_restriction {\n\tDECLARE_BITMAP(register_op, IORING_REGISTER_LAST);\n\tDECLARE_BITMAP(sqe_op, IORING_OP_LAST);\n\tu8 sqe_flags_allowed;\n\tu8 sqe_flags_required;\n\tbool registered;\n};\n\nstruct io_sq_data {\n\trefcount_t\t\trefs;\n\tstruct mutex\t\tlock;\n\n\t/* ctx's that are using this sqd */\n\tstruct list_head\tctx_list;\n\tstruct list_head\tctx_new_list;\n\tstruct mutex\t\tctx_lock;\n\n\tstruct task_struct\t*thread;\n\tstruct wait_queue_head\twait;\n\n\tunsigned\t\tsq_thread_idle;\n};\n\nstruct io_ring_ctx {\n\tstruct {\n\t\tstruct percpu_ref\trefs;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct {\n\t\tunsigned int\t\tflags;\n\t\tunsigned int\t\tcompat: 1;\n\t\tunsigned int\t\tlimit_mem: 1;\n\t\tunsigned int\t\tcq_overflow_flushed: 1;\n\t\tunsigned int\t\tdrain_next: 1;\n\t\tunsigned int\t\teventfd_async: 1;\n\t\tunsigned int\t\trestricted: 1;\n\n\t\t/*\n\t\t * Ring buffer of indices into array of io_uring_sqe, which is\n\t\t * mmapped by the application using the IORING_OFF_SQES offset.\n\t\t *\n\t\t * This indirection could e.g. be used to assign fixed\n\t\t * io_uring_sqe entries to operations and only submit them to\n\t\t * the queue when needed.\n\t\t *\n\t\t * The kernel modifies neither the indices array nor the entries\n\t\t * array.\n\t\t */\n\t\tu32\t\t\t*sq_array;\n\t\tunsigned\t\tcached_sq_head;\n\t\tunsigned\t\tsq_entries;\n\t\tunsigned\t\tsq_mask;\n\t\tunsigned\t\tsq_thread_idle;\n\t\tunsigned\t\tcached_sq_dropped;\n\t\tunsigned\t\tcached_cq_overflow;\n\t\tunsigned long\t\tsq_check_overflow;\n\n\t\tstruct list_head\tdefer_list;\n\t\tstruct list_head\ttimeout_list;\n\t\tstruct list_head\tcq_overflow_list;\n\n\t\tstruct io_uring_sqe\t*sq_sqes;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct io_rings\t*rings;\n\n\t/* IO offload */\n\tstruct io_wq\t\t*io_wq;\n\n\t/*\n\t * For SQPOLL usage - we hold a reference to the parent task, so we\n\t * have access to the ->files\n\t */\n\tstruct task_struct\t*sqo_task;\n\n\t/* Only used for accounting purposes */\n\tstruct mm_struct\t*mm_account;\n\n#ifdef CONFIG_BLK_CGROUP\n\tstruct cgroup_subsys_state\t*sqo_blkcg_css;\n#endif\n\n\tstruct io_sq_data\t*sq_data;\t/* if using sq thread polling */\n\n\tstruct wait_queue_head\tsqo_sq_wait;\n\tstruct list_head\tsqd_list;\n\n\t/*\n\t * If used, fixed file set. Writers must ensure that ->refs is dead,\n\t * readers must ensure that ->refs is alive as long as the file* is\n\t * used. Only updated through io_uring_register(2).\n\t */\n\tstruct fixed_file_data\t*file_data;\n\tunsigned\t\tnr_user_files;\n\n\t/* if used, fixed mapped user buffers */\n\tunsigned\t\tnr_user_bufs;\n\tstruct io_mapped_ubuf\t*user_bufs;\n\n\tstruct user_struct\t*user;\n\n\tconst struct cred\t*creds;\n\n#ifdef CONFIG_AUDIT\n\tkuid_t\t\t\tloginuid;\n\tunsigned int\t\tsessionid;\n#endif\n\n\tstruct completion\tref_comp;\n\tstruct completion\tsq_thread_comp;\n\n\t/* if all else fails... */\n\tstruct io_kiocb\t\t*fallback_req;\n\n#if defined(CONFIG_UNIX)\n\tstruct socket\t\t*ring_sock;\n#endif\n\n\tstruct idr\t\tio_buffer_idr;\n\n\tstruct idr\t\tpersonality_idr;\n\n\tstruct {\n\t\tunsigned\t\tcached_cq_tail;\n\t\tunsigned\t\tcq_entries;\n\t\tunsigned\t\tcq_mask;\n\t\tatomic_t\t\tcq_timeouts;\n\t\tunsigned long\t\tcq_check_overflow;\n\t\tstruct wait_queue_head\tcq_wait;\n\t\tstruct fasync_struct\t*cq_fasync;\n\t\tstruct eventfd_ctx\t*cq_ev_fd;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct {\n\t\tstruct mutex\t\turing_lock;\n\t\twait_queue_head_t\twait;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct {\n\t\tspinlock_t\t\tcompletion_lock;\n\n\t\t/*\n\t\t * ->iopoll_list is protected by the ctx->uring_lock for\n\t\t * io_uring instances that don't use IORING_SETUP_SQPOLL.\n\t\t * For SQPOLL, only the single threaded io_sq_thread() will\n\t\t * manipulate the list, hence no extra locking is needed there.\n\t\t */\n\t\tstruct list_head\tiopoll_list;\n\t\tstruct hlist_head\t*cancel_hash;\n\t\tunsigned\t\tcancel_hash_bits;\n\t\tbool\t\t\tpoll_multi_file;\n\n\t\tspinlock_t\t\tinflight_lock;\n\t\tstruct list_head\tinflight_list;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct delayed_work\t\tfile_put_work;\n\tstruct llist_head\t\tfile_put_llist;\n\n\tstruct work_struct\t\texit_work;\n\tstruct io_restriction\t\trestrictions;\n};\n\n/*\n * First field must be the file pointer in all the\n * iocb unions! See also 'struct kiocb' in <linux/fs.h>\n */\nstruct io_poll_iocb {\n\tstruct file\t\t\t*file;\n\tstruct wait_queue_head\t\t*head;\n\t__poll_t\t\t\tevents;\n\tbool\t\t\t\tdone;\n\tbool\t\t\t\tcanceled;\n\tstruct wait_queue_entry\t\twait;\n};\n\nstruct io_poll_remove {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\taddr;\n};\n\nstruct io_close {\n\tstruct file\t\t\t*file;\n\tstruct file\t\t\t*put_file;\n\tint\t\t\t\tfd;\n};\n\nstruct io_timeout_data {\n\tstruct io_kiocb\t\t\t*req;\n\tstruct hrtimer\t\t\ttimer;\n\tstruct timespec64\t\tts;\n\tenum hrtimer_mode\t\tmode;\n};\n\nstruct io_accept {\n\tstruct file\t\t\t*file;\n\tstruct sockaddr __user\t\t*addr;\n\tint __user\t\t\t*addr_len;\n\tint\t\t\t\tflags;\n\tunsigned long\t\t\tnofile;\n};\n\nstruct io_sync {\n\tstruct file\t\t\t*file;\n\tloff_t\t\t\t\tlen;\n\tloff_t\t\t\t\toff;\n\tint\t\t\t\tflags;\n\tint\t\t\t\tmode;\n};\n\nstruct io_cancel {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\taddr;\n};\n\nstruct io_timeout {\n\tstruct file\t\t\t*file;\n\tu32\t\t\t\toff;\n\tu32\t\t\t\ttarget_seq;\n\tstruct list_head\t\tlist;\n\t/* head of the link, used by linked timeouts only */\n\tstruct io_kiocb\t\t\t*head;\n};\n\nstruct io_timeout_rem {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\taddr;\n};\n\nstruct io_rw {\n\t/* NOTE: kiocb has the file as the first member, so don't do it here */\n\tstruct kiocb\t\t\tkiocb;\n\tu64\t\t\t\taddr;\n\tu64\t\t\t\tlen;\n};\n\nstruct io_connect {\n\tstruct file\t\t\t*file;\n\tstruct sockaddr __user\t\t*addr;\n\tint\t\t\t\taddr_len;\n};\n\nstruct io_sr_msg {\n\tstruct file\t\t\t*file;\n\tunion {\n\t\tstruct user_msghdr __user *umsg;\n\t\tvoid __user\t\t*buf;\n\t};\n\tint\t\t\t\tmsg_flags;\n\tint\t\t\t\tbgid;\n\tsize_t\t\t\t\tlen;\n\tstruct io_buffer\t\t*kbuf;\n};\n\nstruct io_open {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\tdfd;\n\tbool\t\t\t\tignore_nonblock;\n\tstruct filename\t\t\t*filename;\n\tstruct open_how\t\t\thow;\n\tunsigned long\t\t\tnofile;\n};\n\nstruct io_files_update {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\targ;\n\tu32\t\t\t\tnr_args;\n\tu32\t\t\t\toffset;\n};\n\nstruct io_fadvise {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\toffset;\n\tu32\t\t\t\tlen;\n\tu32\t\t\t\tadvice;\n};\n\nstruct io_madvise {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\taddr;\n\tu32\t\t\t\tlen;\n\tu32\t\t\t\tadvice;\n};\n\nstruct io_epoll {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\tepfd;\n\tint\t\t\t\top;\n\tint\t\t\t\tfd;\n\tstruct epoll_event\t\tevent;\n};\n\nstruct io_splice {\n\tstruct file\t\t\t*file_out;\n\tstruct file\t\t\t*file_in;\n\tloff_t\t\t\t\toff_out;\n\tloff_t\t\t\t\toff_in;\n\tu64\t\t\t\tlen;\n\tunsigned int\t\t\tflags;\n};\n\nstruct io_provide_buf {\n\tstruct file\t\t\t*file;\n\t__u64\t\t\t\taddr;\n\t__s32\t\t\t\tlen;\n\t__u32\t\t\t\tbgid;\n\t__u16\t\t\t\tnbufs;\n\t__u16\t\t\t\tbid;\n};\n\nstruct io_statx {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\tdfd;\n\tunsigned int\t\t\tmask;\n\tunsigned int\t\t\tflags;\n\tconst char __user\t\t*filename;\n\tstruct statx __user\t\t*buffer;\n};\n\nstruct io_shutdown {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\thow;\n};\n\nstruct io_rename {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\told_dfd;\n\tint\t\t\t\tnew_dfd;\n\tstruct filename\t\t\t*oldpath;\n\tstruct filename\t\t\t*newpath;\n\tint\t\t\t\tflags;\n\tbool\t\t\t\tignore_nonblock;\n};\n\nstruct io_unlink {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\tdfd;\n\tint\t\t\t\tflags;\n\tbool\t\t\t\tignore_nonblock;\n\tstruct filename\t\t\t*filename;\n};\n\nstruct io_completion {\n\tstruct file\t\t\t*file;\n\tstruct list_head\t\tlist;\n\tint\t\t\t\tcflags;\n};\n\nstruct io_async_connect {\n\tstruct sockaddr_storage\t\taddress;\n};\n\nstruct io_async_msghdr {\n\tstruct iovec\t\t\tfast_iov[UIO_FASTIOV];\n\tstruct iovec\t\t\t*iov;\n\tstruct sockaddr __user\t\t*uaddr;\n\tstruct msghdr\t\t\tmsg;\n\tstruct sockaddr_storage\t\taddr;\n};\n\nstruct io_async_rw {\n\tstruct iovec\t\t\tfast_iov[UIO_FASTIOV];\n\tconst struct iovec\t\t*free_iovec;\n\tstruct iov_iter\t\t\titer;\n\tsize_t\t\t\t\tbytes_done;\n\tstruct wait_page_queue\t\twpq;\n};\n\nenum {\n\tREQ_F_FIXED_FILE_BIT\t= IOSQE_FIXED_FILE_BIT,\n\tREQ_F_IO_DRAIN_BIT\t= IOSQE_IO_DRAIN_BIT,\n\tREQ_F_LINK_BIT\t\t= IOSQE_IO_LINK_BIT,\n\tREQ_F_HARDLINK_BIT\t= IOSQE_IO_HARDLINK_BIT,\n\tREQ_F_FORCE_ASYNC_BIT\t= IOSQE_ASYNC_BIT,\n\tREQ_F_BUFFER_SELECT_BIT\t= IOSQE_BUFFER_SELECT_BIT,\n\n\tREQ_F_FAIL_LINK_BIT,\n\tREQ_F_INFLIGHT_BIT,\n\tREQ_F_CUR_POS_BIT,\n\tREQ_F_NOWAIT_BIT,\n\tREQ_F_LINK_TIMEOUT_BIT,\n\tREQ_F_ISREG_BIT,\n\tREQ_F_NEED_CLEANUP_BIT,\n\tREQ_F_POLLED_BIT,\n\tREQ_F_BUFFER_SELECTED_BIT,\n\tREQ_F_NO_FILE_TABLE_BIT,\n\tREQ_F_WORK_INITIALIZED_BIT,\n\tREQ_F_LTIMEOUT_ACTIVE_BIT,\n\n\t/* not a real bit, just to check we're not overflowing the space */\n\t__REQ_F_LAST_BIT,\n};\n\nenum {\n\t/* ctx owns file */\n\tREQ_F_FIXED_FILE\t= BIT(REQ_F_FIXED_FILE_BIT),\n\t/* drain existing IO first */\n\tREQ_F_IO_DRAIN\t\t= BIT(REQ_F_IO_DRAIN_BIT),\n\t/* linked sqes */\n\tREQ_F_LINK\t\t= BIT(REQ_F_LINK_BIT),\n\t/* doesn't sever on completion < 0 */\n\tREQ_F_HARDLINK\t\t= BIT(REQ_F_HARDLINK_BIT),\n\t/* IOSQE_ASYNC */\n\tREQ_F_FORCE_ASYNC\t= BIT(REQ_F_FORCE_ASYNC_BIT),\n\t/* IOSQE_BUFFER_SELECT */\n\tREQ_F_BUFFER_SELECT\t= BIT(REQ_F_BUFFER_SELECT_BIT),\n\n\t/* fail rest of links */\n\tREQ_F_FAIL_LINK\t\t= BIT(REQ_F_FAIL_LINK_BIT),\n\t/* on inflight list */\n\tREQ_F_INFLIGHT\t\t= BIT(REQ_F_INFLIGHT_BIT),\n\t/* read/write uses file position */\n\tREQ_F_CUR_POS\t\t= BIT(REQ_F_CUR_POS_BIT),\n\t/* must not punt to workers */\n\tREQ_F_NOWAIT\t\t= BIT(REQ_F_NOWAIT_BIT),\n\t/* has or had linked timeout */\n\tREQ_F_LINK_TIMEOUT\t= BIT(REQ_F_LINK_TIMEOUT_BIT),\n\t/* regular file */\n\tREQ_F_ISREG\t\t= BIT(REQ_F_ISREG_BIT),\n\t/* needs cleanup */\n\tREQ_F_NEED_CLEANUP\t= BIT(REQ_F_NEED_CLEANUP_BIT),\n\t/* already went through poll handler */\n\tREQ_F_POLLED\t\t= BIT(REQ_F_POLLED_BIT),\n\t/* buffer already selected */\n\tREQ_F_BUFFER_SELECTED\t= BIT(REQ_F_BUFFER_SELECTED_BIT),\n\t/* doesn't need file table for this request */\n\tREQ_F_NO_FILE_TABLE\t= BIT(REQ_F_NO_FILE_TABLE_BIT),\n\t/* io_wq_work is initialized */\n\tREQ_F_WORK_INITIALIZED\t= BIT(REQ_F_WORK_INITIALIZED_BIT),\n\t/* linked timeout is active, i.e. prepared by link's head */\n\tREQ_F_LTIMEOUT_ACTIVE\t= BIT(REQ_F_LTIMEOUT_ACTIVE_BIT),\n};\n\nstruct async_poll {\n\tstruct io_poll_iocb\tpoll;\n\tstruct io_poll_iocb\t*double_poll;\n};\n\n/*\n * NOTE! Each of the iocb union members has the file pointer\n * as the first entry in their struct definition. So you can\n * access the file pointer through any of the sub-structs,\n * or directly as just 'ki_filp' in this struct.\n */\nstruct io_kiocb {\n\tunion {\n\t\tstruct file\t\t*file;\n\t\tstruct io_rw\t\trw;\n\t\tstruct io_poll_iocb\tpoll;\n\t\tstruct io_poll_remove\tpoll_remove;\n\t\tstruct io_accept\taccept;\n\t\tstruct io_sync\t\tsync;\n\t\tstruct io_cancel\tcancel;\n\t\tstruct io_timeout\ttimeout;\n\t\tstruct io_timeout_rem\ttimeout_rem;\n\t\tstruct io_connect\tconnect;\n\t\tstruct io_sr_msg\tsr_msg;\n\t\tstruct io_open\t\topen;\n\t\tstruct io_close\t\tclose;\n\t\tstruct io_files_update\tfiles_update;\n\t\tstruct io_fadvise\tfadvise;\n\t\tstruct io_madvise\tmadvise;\n\t\tstruct io_epoll\t\tepoll;\n\t\tstruct io_splice\tsplice;\n\t\tstruct io_provide_buf\tpbuf;\n\t\tstruct io_statx\t\tstatx;\n\t\tstruct io_shutdown\tshutdown;\n\t\tstruct io_rename\trename;\n\t\tstruct io_unlink\tunlink;\n\t\t/* use only after cleaning per-op data, see io_clean_op() */\n\t\tstruct io_completion\tcompl;\n\t};\n\n\t/* opcode allocated if it needs to store data for async defer */\n\tvoid\t\t\t\t*async_data;\n\tu8\t\t\t\topcode;\n\t/* polled IO has completed */\n\tu8\t\t\t\tiopoll_completed;\n\n\tu16\t\t\t\tbuf_index;\n\tu32\t\t\t\tresult;\n\n\tstruct io_ring_ctx\t\t*ctx;\n\tunsigned int\t\t\tflags;\n\trefcount_t\t\t\trefs;\n\tstruct task_struct\t\t*task;\n\tu64\t\t\t\tuser_data;\n\n\tstruct io_kiocb\t\t\t*link;\n\tstruct percpu_ref\t\t*fixed_file_refs;\n\n\t/*\n\t * 1. used with ctx->iopoll_list with reads/writes\n\t * 2. to track reqs with ->files (see io_op_def::file_table)\n\t */\n\tstruct list_head\t\tinflight_entry;\n\tstruct callback_head\t\ttask_work;\n\t/* for polled requests, i.e. IORING_OP_POLL_ADD and async armed poll */\n\tstruct hlist_node\t\thash_node;\n\tstruct async_poll\t\t*apoll;\n\tstruct io_wq_work\t\twork;\n};\n\nstruct io_defer_entry {\n\tstruct list_head\tlist;\n\tstruct io_kiocb\t\t*req;\n\tu32\t\t\tseq;\n};\n\n#define IO_IOPOLL_BATCH\t\t\t8\n\nstruct io_comp_state {\n\tunsigned int\t\tnr;\n\tstruct list_head\tlist;\n\tstruct io_ring_ctx\t*ctx;\n};\n\nstruct io_submit_state {\n\tstruct blk_plug\t\tplug;\n\n\t/*\n\t * io_kiocb alloc cache\n\t */\n\tvoid\t\t\t*reqs[IO_IOPOLL_BATCH];\n\tunsigned int\t\tfree_reqs;\n\n\tbool\t\t\tplug_started;\n\n\t/*\n\t * Batch completion logic\n\t */\n\tstruct io_comp_state\tcomp;\n\n\t/*\n\t * File reference cache\n\t */\n\tstruct file\t\t*file;\n\tunsigned int\t\tfd;\n\tunsigned int\t\thas_refs;\n\tunsigned int\t\tios_left;\n};\n\nstruct io_op_def {\n\t/* needs req->file assigned */\n\tunsigned\t\tneeds_file : 1;\n\t/* don't fail if file grab fails */\n\tunsigned\t\tneeds_file_no_error : 1;\n\t/* hash wq insertion if file is a regular file */\n\tunsigned\t\thash_reg_file : 1;\n\t/* unbound wq insertion if file is a non-regular file */\n\tunsigned\t\tunbound_nonreg_file : 1;\n\t/* opcode is not supported by this kernel */\n\tunsigned\t\tnot_supported : 1;\n\t/* set if opcode supports polled \"wait\" */\n\tunsigned\t\tpollin : 1;\n\tunsigned\t\tpollout : 1;\n\t/* op supports buffer selection */\n\tunsigned\t\tbuffer_select : 1;\n\t/* must always have async data allocated */\n\tunsigned\t\tneeds_async_data : 1;\n\t/* should block plug */\n\tunsigned\t\tplug : 1;\n\t/* size of async data needed, if any */\n\tunsigned short\t\tasync_size;\n\tunsigned\t\twork_flags;\n};\n\nstatic const struct io_op_def io_op_defs[] = {\n\t[IORING_OP_NOP] = {},\n\t[IORING_OP_READV] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.buffer_select\t\t= 1,\n\t\t.needs_async_data\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_WRITEV] = {\n\t\t.needs_file\t\t= 1,\n\t\t.hash_reg_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.needs_async_data\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_BLKCG |\n\t\t\t\t\t\tIO_WQ_WORK_FSIZE,\n\t},\n\t[IORING_OP_FSYNC] = {\n\t\t.needs_file\t\t= 1,\n\t\t.work_flags\t\t= IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_READ_FIXED] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t\t.work_flags\t\t= IO_WQ_WORK_BLKCG | IO_WQ_WORK_MM,\n\t},\n\t[IORING_OP_WRITE_FIXED] = {\n\t\t.needs_file\t\t= 1,\n\t\t.hash_reg_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t\t.work_flags\t\t= IO_WQ_WORK_BLKCG | IO_WQ_WORK_FSIZE |\n\t\t\t\t\t\tIO_WQ_WORK_MM,\n\t},\n\t[IORING_OP_POLL_ADD] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t},\n\t[IORING_OP_POLL_REMOVE] = {},\n\t[IORING_OP_SYNC_FILE_RANGE] = {\n\t\t.needs_file\t\t= 1,\n\t\t.work_flags\t\t= IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_SENDMSG] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.needs_async_data\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_msghdr),\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_RECVMSG] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.buffer_select\t\t= 1,\n\t\t.needs_async_data\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_msghdr),\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_TIMEOUT] = {\n\t\t.needs_async_data\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_timeout_data),\n\t\t.work_flags\t\t= IO_WQ_WORK_MM,\n\t},\n\t[IORING_OP_TIMEOUT_REMOVE] = {},\n\t[IORING_OP_ACCEPT] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_FILES,\n\t},\n\t[IORING_OP_ASYNC_CANCEL] = {},\n\t[IORING_OP_LINK_TIMEOUT] = {\n\t\t.needs_async_data\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_timeout_data),\n\t\t.work_flags\t\t= IO_WQ_WORK_MM,\n\t},\n\t[IORING_OP_CONNECT] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.needs_async_data\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_connect),\n\t\t.work_flags\t\t= IO_WQ_WORK_MM,\n\t},\n\t[IORING_OP_FALLOCATE] = {\n\t\t.needs_file\t\t= 1,\n\t\t.work_flags\t\t= IO_WQ_WORK_BLKCG | IO_WQ_WORK_FSIZE,\n\t},\n\t[IORING_OP_OPENAT] = {\n\t\t.work_flags\t\t= IO_WQ_WORK_FILES | IO_WQ_WORK_BLKCG |\n\t\t\t\t\t\tIO_WQ_WORK_FS | IO_WQ_WORK_MM,\n\t},\n\t[IORING_OP_CLOSE] = {\n\t\t.needs_file\t\t= 1,\n\t\t.needs_file_no_error\t= 1,\n\t\t.work_flags\t\t= IO_WQ_WORK_FILES | IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_FILES_UPDATE] = {\n\t\t.work_flags\t\t= IO_WQ_WORK_FILES | IO_WQ_WORK_MM,\n\t},\n\t[IORING_OP_STATX] = {\n\t\t.work_flags\t\t= IO_WQ_WORK_FILES | IO_WQ_WORK_MM |\n\t\t\t\t\t\tIO_WQ_WORK_FS | IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_READ] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.buffer_select\t\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_WRITE] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_BLKCG |\n\t\t\t\t\t\tIO_WQ_WORK_FSIZE,\n\t},\n\t[IORING_OP_FADVISE] = {\n\t\t.needs_file\t\t= 1,\n\t\t.work_flags\t\t= IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_MADVISE] = {\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_SEND] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_RECV] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.buffer_select\t\t= 1,\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_OPENAT2] = {\n\t\t.work_flags\t\t= IO_WQ_WORK_FILES | IO_WQ_WORK_FS |\n\t\t\t\t\t\tIO_WQ_WORK_BLKCG | IO_WQ_WORK_MM,\n\t},\n\t[IORING_OP_EPOLL_CTL] = {\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.work_flags\t\t= IO_WQ_WORK_FILES,\n\t},\n\t[IORING_OP_SPLICE] = {\n\t\t.needs_file\t\t= 1,\n\t\t.hash_reg_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.work_flags\t\t= IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_PROVIDE_BUFFERS] = {},\n\t[IORING_OP_REMOVE_BUFFERS] = {},\n\t[IORING_OP_TEE] = {\n\t\t.needs_file\t\t= 1,\n\t\t.hash_reg_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t},\n\t[IORING_OP_SHUTDOWN] = {\n\t\t.needs_file\t\t= 1,\n\t},\n\t[IORING_OP_RENAMEAT] = {\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_FILES |\n\t\t\t\t\t\tIO_WQ_WORK_FS | IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_UNLINKAT] = {\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_FILES |\n\t\t\t\t\t\tIO_WQ_WORK_FS | IO_WQ_WORK_BLKCG,\n\t},\n};\n\nenum io_mem_account {\n\tACCT_LOCKED,\n\tACCT_PINNED,\n};\n\nstatic void __io_complete_rw(struct io_kiocb *req, long res, long res2,\n\t\t\t     struct io_comp_state *cs);\nstatic void io_cqring_fill_event(struct io_kiocb *req, long res);\nstatic void io_put_req(struct io_kiocb *req);\nstatic void io_put_req_deferred(struct io_kiocb *req, int nr);\nstatic void io_double_put_req(struct io_kiocb *req);\nstatic struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req);\nstatic void __io_queue_linked_timeout(struct io_kiocb *req);\nstatic void io_queue_linked_timeout(struct io_kiocb *req);\nstatic int __io_sqe_files_update(struct io_ring_ctx *ctx,\n\t\t\t\t struct io_uring_files_update *ip,\n\t\t\t\t unsigned nr_args);\nstatic void __io_clean_op(struct io_kiocb *req);\nstatic struct file *io_file_get(struct io_submit_state *state,\n\t\t\t\tstruct io_kiocb *req, int fd, bool fixed);\nstatic void __io_queue_sqe(struct io_kiocb *req, struct io_comp_state *cs);\nstatic void io_file_put_work(struct work_struct *work);\n\nstatic ssize_t io_import_iovec(int rw, struct io_kiocb *req,\n\t\t\t       struct iovec **iovec, struct iov_iter *iter,\n\t\t\t       bool needs_lock);\nstatic int io_setup_async_rw(struct io_kiocb *req, const struct iovec *iovec,\n\t\t\t     const struct iovec *fast_iov,\n\t\t\t     struct iov_iter *iter, bool force);\n\nstatic struct kmem_cache *req_cachep;\n\nstatic const struct file_operations io_uring_fops;\n\nstruct sock *io_uring_get_socket(struct file *file)\n{\n#if defined(CONFIG_UNIX)\n\tif (file->f_op == &io_uring_fops) {\n\t\tstruct io_ring_ctx *ctx = file->private_data;\n\n\t\treturn ctx->ring_sock->sk;\n\t}\n#endif\n\treturn NULL;\n}\nEXPORT_SYMBOL(io_uring_get_socket);\n\n#define io_for_each_link(pos, head) \\\n\tfor (pos = (head); pos; pos = pos->link)\n\nstatic inline void io_clean_op(struct io_kiocb *req)\n{\n\tif (req->flags & (REQ_F_NEED_CLEANUP | REQ_F_BUFFER_SELECTED |\n\t\t\t  REQ_F_INFLIGHT))\n\t\t__io_clean_op(req);\n}\n\nstatic inline void io_set_resource_node(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!req->fixed_file_refs) {\n\t\treq->fixed_file_refs = &ctx->file_data->node->refs;\n\t\tpercpu_ref_get(req->fixed_file_refs);\n\t}\n}\n\nstatic bool io_match_task(struct io_kiocb *head,\n\t\t\t  struct task_struct *task,\n\t\t\t  struct files_struct *files)\n{\n\tstruct io_kiocb *req;\n\n\tif (task && head->task != task)\n\t\treturn false;\n\tif (!files)\n\t\treturn true;\n\n\tio_for_each_link(req, head) {\n\t\tif ((req->flags & REQ_F_WORK_INITIALIZED) &&\n\t\t    (req->work.flags & IO_WQ_WORK_FILES) &&\n\t\t    req->work.identity->files == files)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void io_sq_thread_drop_mm_files(void)\n{\n\tstruct files_struct *files = current->files;\n\tstruct mm_struct *mm = current->mm;\n\n\tif (mm) {\n\t\tkthread_unuse_mm(mm);\n\t\tmmput(mm);\n\t\tcurrent->mm = NULL;\n\t}\n\tif (files) {\n\t\tstruct nsproxy *nsproxy = current->nsproxy;\n\t\tstruct pid *thread_pid = current->thread_pid;\n\n\t\ttask_lock(current);\n\t\tcurrent->files = NULL;\n\t\tcurrent->nsproxy = NULL;\n\t\tcurrent->thread_pid = NULL;\n\t\ttask_unlock(current);\n\t\tput_files_struct(files);\n\t\tput_nsproxy(nsproxy);\n\t\tput_pid(thread_pid);\n\t}\n}\n\nstatic int __io_sq_thread_acquire_files(struct io_ring_ctx *ctx)\n{\n\tif (!current->files) {\n\t\tstruct files_struct *files;\n\t\tstruct nsproxy *nsproxy;\n\t\tstruct pid *thread_pid;\n\n\t\ttask_lock(ctx->sqo_task);\n\t\tfiles = ctx->sqo_task->files;\n\t\tif (!files) {\n\t\t\ttask_unlock(ctx->sqo_task);\n\t\t\treturn -EOWNERDEAD;\n\t\t}\n\t\tatomic_inc(&files->count);\n\t\tget_nsproxy(ctx->sqo_task->nsproxy);\n\t\tnsproxy = ctx->sqo_task->nsproxy;\n\t\tthread_pid = get_pid(ctx->sqo_task->thread_pid);\n\t\ttask_unlock(ctx->sqo_task);\n\n\t\ttask_lock(current);\n\t\tcurrent->files = files;\n\t\tcurrent->nsproxy = nsproxy;\n\t\tcurrent->thread_pid = thread_pid;\n\t\ttask_unlock(current);\n\t}\n\treturn 0;\n}\n\nstatic int __io_sq_thread_acquire_mm(struct io_ring_ctx *ctx)\n{\n\tstruct mm_struct *mm;\n\n\tif (current->mm)\n\t\treturn 0;\n\n\t/* Should never happen */\n\tif (unlikely(!(ctx->flags & IORING_SETUP_SQPOLL)))\n\t\treturn -EFAULT;\n\n\ttask_lock(ctx->sqo_task);\n\tmm = ctx->sqo_task->mm;\n\tif (unlikely(!mm || !mmget_not_zero(mm)))\n\t\tmm = NULL;\n\ttask_unlock(ctx->sqo_task);\n\n\tif (mm) {\n\t\tkthread_use_mm(mm);\n\t\treturn 0;\n\t}\n\n\treturn -EFAULT;\n}\n\nstatic int io_sq_thread_acquire_mm_files(struct io_ring_ctx *ctx,\n\t\t\t\t\t struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tint ret;\n\n\tif (def->work_flags & IO_WQ_WORK_MM) {\n\t\tret = __io_sq_thread_acquire_mm(ctx);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t}\n\n\tif (def->needs_file || (def->work_flags & IO_WQ_WORK_FILES)) {\n\t\tret = __io_sq_thread_acquire_files(ctx);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void io_sq_thread_associate_blkcg(struct io_ring_ctx *ctx,\n\t\t\t\t\t struct cgroup_subsys_state **cur_css)\n\n{\n#ifdef CONFIG_BLK_CGROUP\n\t/* puts the old one when swapping */\n\tif (*cur_css != ctx->sqo_blkcg_css) {\n\t\tkthread_associate_blkcg(ctx->sqo_blkcg_css);\n\t\t*cur_css = ctx->sqo_blkcg_css;\n\t}\n#endif\n}\n\nstatic void io_sq_thread_unassociate_blkcg(void)\n{\n#ifdef CONFIG_BLK_CGROUP\n\tkthread_associate_blkcg(NULL);\n#endif\n}\n\nstatic inline void req_set_fail_links(struct io_kiocb *req)\n{\n\tif ((req->flags & (REQ_F_LINK | REQ_F_HARDLINK)) == REQ_F_LINK)\n\t\treq->flags |= REQ_F_FAIL_LINK;\n}\n\n/*\n * None of these are dereferenced, they are simply used to check if any of\n * them have changed. If we're under current and check they are still the\n * same, we're fine to grab references to them for actual out-of-line use.\n */\nstatic void io_init_identity(struct io_identity *id)\n{\n\tid->files = current->files;\n\tid->mm = current->mm;\n#ifdef CONFIG_BLK_CGROUP\n\trcu_read_lock();\n\tid->blkcg_css = blkcg_css();\n\trcu_read_unlock();\n#endif\n\tid->creds = current_cred();\n\tid->nsproxy = current->nsproxy;\n\tid->fs = current->fs;\n\tid->fsize = rlimit(RLIMIT_FSIZE);\n#ifdef CONFIG_AUDIT\n\tid->loginuid = current->loginuid;\n\tid->sessionid = current->sessionid;\n#endif\n\trefcount_set(&id->count, 1);\n}\n\nstatic inline void __io_req_init_async(struct io_kiocb *req)\n{\n\tmemset(&req->work, 0, sizeof(req->work));\n\treq->flags |= REQ_F_WORK_INITIALIZED;\n}\n\n/*\n * Note: must call io_req_init_async() for the first time you\n * touch any members of io_wq_work.\n */\nstatic inline void io_req_init_async(struct io_kiocb *req)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\n\tif (req->flags & REQ_F_WORK_INITIALIZED)\n\t\treturn;\n\n\t__io_req_init_async(req);\n\n\t/* Grab a ref if this isn't our static identity */\n\treq->work.identity = tctx->identity;\n\tif (tctx->identity != &tctx->__identity)\n\t\trefcount_inc(&req->work.identity->count);\n}\n\nstatic inline bool io_async_submit(struct io_ring_ctx *ctx)\n{\n\treturn ctx->flags & IORING_SETUP_SQPOLL;\n}\n\nstatic void io_ring_ctx_ref_free(struct percpu_ref *ref)\n{\n\tstruct io_ring_ctx *ctx = container_of(ref, struct io_ring_ctx, refs);\n\n\tcomplete(&ctx->ref_comp);\n}\n\nstatic inline bool io_is_timeout_noseq(struct io_kiocb *req)\n{\n\treturn !req->timeout.off;\n}\n\nstatic struct io_ring_ctx *io_ring_ctx_alloc(struct io_uring_params *p)\n{\n\tstruct io_ring_ctx *ctx;\n\tint hash_bits;\n\n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn NULL;\n\n\tctx->fallback_req = kmem_cache_alloc(req_cachep, GFP_KERNEL);\n\tif (!ctx->fallback_req)\n\t\tgoto err;\n\n\t/*\n\t * Use 5 bits less than the max cq entries, that should give us around\n\t * 32 entries per hash list if totally full and uniformly spread.\n\t */\n\thash_bits = ilog2(p->cq_entries);\n\thash_bits -= 5;\n\tif (hash_bits <= 0)\n\t\thash_bits = 1;\n\tctx->cancel_hash_bits = hash_bits;\n\tctx->cancel_hash = kmalloc((1U << hash_bits) * sizeof(struct hlist_head),\n\t\t\t\t\tGFP_KERNEL);\n\tif (!ctx->cancel_hash)\n\t\tgoto err;\n\t__hash_init(ctx->cancel_hash, 1U << hash_bits);\n\n\tif (percpu_ref_init(&ctx->refs, io_ring_ctx_ref_free,\n\t\t\t    PERCPU_REF_ALLOW_REINIT, GFP_KERNEL))\n\t\tgoto err;\n\n\tctx->flags = p->flags;\n\tinit_waitqueue_head(&ctx->sqo_sq_wait);\n\tINIT_LIST_HEAD(&ctx->sqd_list);\n\tinit_waitqueue_head(&ctx->cq_wait);\n\tINIT_LIST_HEAD(&ctx->cq_overflow_list);\n\tinit_completion(&ctx->ref_comp);\n\tinit_completion(&ctx->sq_thread_comp);\n\tidr_init(&ctx->io_buffer_idr);\n\tidr_init(&ctx->personality_idr);\n\tmutex_init(&ctx->uring_lock);\n\tinit_waitqueue_head(&ctx->wait);\n\tspin_lock_init(&ctx->completion_lock);\n\tINIT_LIST_HEAD(&ctx->iopoll_list);\n\tINIT_LIST_HEAD(&ctx->defer_list);\n\tINIT_LIST_HEAD(&ctx->timeout_list);\n\tspin_lock_init(&ctx->inflight_lock);\n\tINIT_LIST_HEAD(&ctx->inflight_list);\n\tINIT_DELAYED_WORK(&ctx->file_put_work, io_file_put_work);\n\tinit_llist_head(&ctx->file_put_llist);\n\treturn ctx;\nerr:\n\tif (ctx->fallback_req)\n\t\tkmem_cache_free(req_cachep, ctx->fallback_req);\n\tkfree(ctx->cancel_hash);\n\tkfree(ctx);\n\treturn NULL;\n}\n\nstatic bool req_need_defer(struct io_kiocb *req, u32 seq)\n{\n\tif (unlikely(req->flags & REQ_F_IO_DRAIN)) {\n\t\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t\treturn seq != ctx->cached_cq_tail\n\t\t\t\t+ READ_ONCE(ctx->cached_cq_overflow);\n\t}\n\n\treturn false;\n}\n\nstatic void __io_commit_cqring(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *rings = ctx->rings;\n\n\t/* order cqe stores with ring update */\n\tsmp_store_release(&rings->cq.tail, ctx->cached_cq_tail);\n\n\tif (wq_has_sleeper(&ctx->cq_wait)) {\n\t\twake_up_interruptible(&ctx->cq_wait);\n\t\tkill_fasync(&ctx->cq_fasync, SIGIO, POLL_IN);\n\t}\n}\n\nstatic void io_put_identity(struct io_uring_task *tctx, struct io_kiocb *req)\n{\n\tif (req->work.identity == &tctx->__identity)\n\t\treturn;\n\tif (refcount_dec_and_test(&req->work.identity->count))\n\t\tkfree(req->work.identity);\n}\n\nstatic void io_req_clean_work(struct io_kiocb *req)\n{\n\tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n\t\treturn;\n\n\treq->flags &= ~REQ_F_WORK_INITIALIZED;\n\n\tif (req->work.flags & IO_WQ_WORK_MM) {\n\t\tmmdrop(req->work.identity->mm);\n\t\treq->work.flags &= ~IO_WQ_WORK_MM;\n\t}\n#ifdef CONFIG_BLK_CGROUP\n\tif (req->work.flags & IO_WQ_WORK_BLKCG) {\n\t\tcss_put(req->work.identity->blkcg_css);\n\t\treq->work.flags &= ~IO_WQ_WORK_BLKCG;\n\t}\n#endif\n\tif (req->work.flags & IO_WQ_WORK_CREDS) {\n\t\tput_cred(req->work.identity->creds);\n\t\treq->work.flags &= ~IO_WQ_WORK_CREDS;\n\t}\n\tif (req->work.flags & IO_WQ_WORK_FS) {\n\t\tstruct fs_struct *fs = req->work.identity->fs;\n\n\t\tspin_lock(&req->work.identity->fs->lock);\n\t\tif (--fs->users)\n\t\t\tfs = NULL;\n\t\tspin_unlock(&req->work.identity->fs->lock);\n\t\tif (fs)\n\t\t\tfree_fs_struct(fs);\n\t\treq->work.flags &= ~IO_WQ_WORK_FS;\n\t}\n\n\tio_put_identity(req->task->io_uring, req);\n}\n\n/*\n * Create a private copy of io_identity, since some fields don't match\n * the current context.\n */\nstatic bool io_identity_cow(struct io_kiocb *req)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tconst struct cred *creds = NULL;\n\tstruct io_identity *id;\n\n\tif (req->work.flags & IO_WQ_WORK_CREDS)\n\t\tcreds = req->work.identity->creds;\n\n\tid = kmemdup(req->work.identity, sizeof(*id), GFP_KERNEL);\n\tif (unlikely(!id)) {\n\t\treq->work.flags |= IO_WQ_WORK_CANCEL;\n\t\treturn false;\n\t}\n\n\t/*\n\t * We can safely just re-init the creds we copied  Either the field\n\t * matches the current one, or we haven't grabbed it yet. The only\n\t * exception is ->creds, through registered personalities, so handle\n\t * that one separately.\n\t */\n\tio_init_identity(id);\n\tif (creds)\n\t\treq->work.identity->creds = creds;\n\n\t/* add one for this request */\n\trefcount_inc(&id->count);\n\n\t/* drop tctx and req identity references, if needed */\n\tif (tctx->identity != &tctx->__identity &&\n\t    refcount_dec_and_test(&tctx->identity->count))\n\t\tkfree(tctx->identity);\n\tif (req->work.identity != &tctx->__identity &&\n\t    refcount_dec_and_test(&req->work.identity->count))\n\t\tkfree(req->work.identity);\n\n\treq->work.identity = id;\n\ttctx->identity = id;\n\treturn true;\n}\n\nstatic bool io_grab_identity(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_identity *id = req->work.identity;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (def->work_flags & IO_WQ_WORK_FSIZE) {\n\t\tif (id->fsize != rlimit(RLIMIT_FSIZE))\n\t\t\treturn false;\n\t\treq->work.flags |= IO_WQ_WORK_FSIZE;\n\t}\n\n\tif (!(req->work.flags & IO_WQ_WORK_FILES) &&\n\t    (def->work_flags & IO_WQ_WORK_FILES) &&\n\t    !(req->flags & REQ_F_NO_FILE_TABLE)) {\n\t\tif (id->files != current->files ||\n\t\t    id->nsproxy != current->nsproxy)\n\t\t\treturn false;\n\t\tatomic_inc(&id->files->count);\n\t\tget_nsproxy(id->nsproxy);\n\t\treq->flags |= REQ_F_INFLIGHT;\n\n\t\tspin_lock_irq(&ctx->inflight_lock);\n\t\tlist_add(&req->inflight_entry, &ctx->inflight_list);\n\t\tspin_unlock_irq(&ctx->inflight_lock);\n\t\treq->work.flags |= IO_WQ_WORK_FILES;\n\t}\n#ifdef CONFIG_BLK_CGROUP\n\tif (!(req->work.flags & IO_WQ_WORK_BLKCG) &&\n\t    (def->work_flags & IO_WQ_WORK_BLKCG)) {\n\t\trcu_read_lock();\n\t\tif (id->blkcg_css != blkcg_css()) {\n\t\t\trcu_read_unlock();\n\t\t\treturn false;\n\t\t}\n\t\t/*\n\t\t * This should be rare, either the cgroup is dying or the task\n\t\t * is moving cgroups. Just punt to root for the handful of ios.\n\t\t */\n\t\tif (css_tryget_online(id->blkcg_css))\n\t\t\treq->work.flags |= IO_WQ_WORK_BLKCG;\n\t\trcu_read_unlock();\n\t}\n#endif\n\tif (!(req->work.flags & IO_WQ_WORK_CREDS)) {\n\t\tif (id->creds != current_cred())\n\t\t\treturn false;\n\t\tget_cred(id->creds);\n\t\treq->work.flags |= IO_WQ_WORK_CREDS;\n\t}\n#ifdef CONFIG_AUDIT\n\tif (!uid_eq(current->loginuid, id->loginuid) ||\n\t    current->sessionid != id->sessionid)\n\t\treturn false;\n#endif\n\tif (!(req->work.flags & IO_WQ_WORK_FS) &&\n\t    (def->work_flags & IO_WQ_WORK_FS)) {\n\t\tif (current->fs != id->fs)\n\t\t\treturn false;\n\t\tspin_lock(&id->fs->lock);\n\t\tif (!id->fs->in_exec) {\n\t\t\tid->fs->users++;\n\t\t\treq->work.flags |= IO_WQ_WORK_FS;\n\t\t} else {\n\t\t\treq->work.flags |= IO_WQ_WORK_CANCEL;\n\t\t}\n\t\tspin_unlock(&current->fs->lock);\n\t}\n\n\treturn true;\n}\n\nstatic void io_prep_async_work(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_identity *id;\n\n\tio_req_init_async(req);\n\tid = req->work.identity;\n\n\tif (req->flags & REQ_F_FORCE_ASYNC)\n\t\treq->work.flags |= IO_WQ_WORK_CONCURRENT;\n\n\tif (req->flags & REQ_F_ISREG) {\n\t\tif (def->hash_reg_file || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tio_wq_hash_work(&req->work, file_inode(req->file));\n\t} else {\n\t\tif (def->unbound_nonreg_file)\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n\n\t/* ->mm can never change on us */\n\tif (!(req->work.flags & IO_WQ_WORK_MM) &&\n\t    (def->work_flags & IO_WQ_WORK_MM)) {\n\t\tmmgrab(id->mm);\n\t\treq->work.flags |= IO_WQ_WORK_MM;\n\t}\n\n\t/* if we fail grabbing identity, we must COW, regrab, and retry */\n\tif (io_grab_identity(req))\n\t\treturn;\n\n\tif (!io_identity_cow(req))\n\t\treturn;\n\n\t/* can't fail at this point */\n\tif (!io_grab_identity(req))\n\t\tWARN_ON(1);\n}\n\nstatic void io_prep_async_link(struct io_kiocb *req)\n{\n\tstruct io_kiocb *cur;\n\n\tio_for_each_link(cur, req)\n\t\tio_prep_async_work(cur);\n}\n\nstatic struct io_kiocb *__io_queue_async_work(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_kiocb *link = io_prep_linked_timeout(req);\n\n\ttrace_io_uring_queue_async_work(ctx, io_wq_is_hashed(&req->work), req,\n\t\t\t\t\t&req->work, req->flags);\n\tio_wq_enqueue(ctx->io_wq, &req->work);\n\treturn link;\n}\n\nstatic void io_queue_async_work(struct io_kiocb *req)\n{\n\tstruct io_kiocb *link;\n\n\t/* init ->work of the whole link before punting */\n\tio_prep_async_link(req);\n\tlink = __io_queue_async_work(req);\n\n\tif (link)\n\t\tio_queue_linked_timeout(link);\n}\n\nstatic void io_kill_timeout(struct io_kiocb *req)\n{\n\tstruct io_timeout_data *io = req->async_data;\n\tint ret;\n\n\tret = hrtimer_try_to_cancel(&io->timer);\n\tif (ret != -1) {\n\t\tatomic_set(&req->ctx->cq_timeouts,\n\t\t\tatomic_read(&req->ctx->cq_timeouts) + 1);\n\t\tlist_del_init(&req->timeout.list);\n\t\tio_cqring_fill_event(req, 0);\n\t\tio_put_req_deferred(req, 1);\n\t}\n}\n\n/*\n * Returns true if we found and killed one or more timeouts\n */\nstatic bool io_kill_timeouts(struct io_ring_ctx *ctx, struct task_struct *tsk,\n\t\t\t     struct files_struct *files)\n{\n\tstruct io_kiocb *req, *tmp;\n\tint canceled = 0;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tlist_for_each_entry_safe(req, tmp, &ctx->timeout_list, timeout.list) {\n\t\tif (io_match_task(req, tsk, files)) {\n\t\t\tio_kill_timeout(req);\n\t\t\tcanceled++;\n\t\t}\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\treturn canceled != 0;\n}\n\nstatic void __io_queue_deferred(struct io_ring_ctx *ctx)\n{\n\tdo {\n\t\tstruct io_defer_entry *de = list_first_entry(&ctx->defer_list,\n\t\t\t\t\t\tstruct io_defer_entry, list);\n\t\tstruct io_kiocb *link;\n\n\t\tif (req_need_defer(de->req, de->seq))\n\t\t\tbreak;\n\t\tlist_del_init(&de->list);\n\t\t/* punt-init is done before queueing for defer */\n\t\tlink = __io_queue_async_work(de->req);\n\t\tif (link) {\n\t\t\t__io_queue_linked_timeout(link);\n\t\t\t/* drop submission reference */\n\t\t\tio_put_req_deferred(link, 1);\n\t\t}\n\t\tkfree(de);\n\t} while (!list_empty(&ctx->defer_list));\n}\n\nstatic void io_flush_timeouts(struct io_ring_ctx *ctx)\n{\n\twhile (!list_empty(&ctx->timeout_list)) {\n\t\tstruct io_kiocb *req = list_first_entry(&ctx->timeout_list,\n\t\t\t\t\t\tstruct io_kiocb, timeout.list);\n\n\t\tif (io_is_timeout_noseq(req))\n\t\t\tbreak;\n\t\tif (req->timeout.target_seq != ctx->cached_cq_tail\n\t\t\t\t\t- atomic_read(&ctx->cq_timeouts))\n\t\t\tbreak;\n\n\t\tlist_del_init(&req->timeout.list);\n\t\tio_kill_timeout(req);\n\t}\n}\n\nstatic void io_commit_cqring(struct io_ring_ctx *ctx)\n{\n\tio_flush_timeouts(ctx);\n\t__io_commit_cqring(ctx);\n\n\tif (unlikely(!list_empty(&ctx->defer_list)))\n\t\t__io_queue_deferred(ctx);\n}\n\nstatic inline bool io_sqring_full(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *r = ctx->rings;\n\n\treturn READ_ONCE(r->sq.tail) - ctx->cached_sq_head == r->sq_ring_entries;\n}\n\nstatic struct io_uring_cqe *io_get_cqring(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *rings = ctx->rings;\n\tunsigned tail;\n\n\ttail = ctx->cached_cq_tail;\n\t/*\n\t * writes to the cq entry need to come after reading head; the\n\t * control dependency is enough as we're using WRITE_ONCE to\n\t * fill the cq entry\n\t */\n\tif (tail - READ_ONCE(rings->cq.head) == rings->cq_ring_entries)\n\t\treturn NULL;\n\n\tctx->cached_cq_tail++;\n\treturn &rings->cqes[tail & ctx->cq_mask];\n}\n\nstatic inline bool io_should_trigger_evfd(struct io_ring_ctx *ctx)\n{\n\tif (!ctx->cq_ev_fd)\n\t\treturn false;\n\tif (READ_ONCE(ctx->rings->cq_flags) & IORING_CQ_EVENTFD_DISABLED)\n\t\treturn false;\n\tif (!ctx->eventfd_async)\n\t\treturn true;\n\treturn io_wq_current_is_worker();\n}\n\nstatic void io_cqring_ev_posted(struct io_ring_ctx *ctx)\n{\n\tif (waitqueue_active(&ctx->wait))\n\t\twake_up(&ctx->wait);\n\tif (ctx->sq_data && waitqueue_active(&ctx->sq_data->wait))\n\t\twake_up(&ctx->sq_data->wait);\n\tif (io_should_trigger_evfd(ctx))\n\t\teventfd_signal(ctx->cq_ev_fd, 1);\n}\n\nstatic void io_cqring_mark_overflow(struct io_ring_ctx *ctx)\n{\n\tif (list_empty(&ctx->cq_overflow_list)) {\n\t\tclear_bit(0, &ctx->sq_check_overflow);\n\t\tclear_bit(0, &ctx->cq_check_overflow);\n\t\tctx->rings->sq_flags &= ~IORING_SQ_CQ_OVERFLOW;\n\t}\n}\n\n/* Returns true if there are no backlogged entries after the flush */\nstatic bool io_cqring_overflow_flush(struct io_ring_ctx *ctx, bool force,\n\t\t\t\t     struct task_struct *tsk,\n\t\t\t\t     struct files_struct *files)\n{\n\tstruct io_rings *rings = ctx->rings;\n\tstruct io_kiocb *req, *tmp;\n\tstruct io_uring_cqe *cqe;\n\tunsigned long flags;\n\tLIST_HEAD(list);\n\n\tif (!force) {\n\t\tif (list_empty_careful(&ctx->cq_overflow_list))\n\t\t\treturn true;\n\t\tif ((ctx->cached_cq_tail - READ_ONCE(rings->cq.head) ==\n\t\t    rings->cq_ring_entries))\n\t\t\treturn false;\n\t}\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\n\t/* if force is set, the ring is going away. always drop after that */\n\tif (force)\n\t\tctx->cq_overflow_flushed = 1;\n\n\tcqe = NULL;\n\tlist_for_each_entry_safe(req, tmp, &ctx->cq_overflow_list, compl.list) {\n\t\tif (!io_match_task(req, tsk, files))\n\t\t\tcontinue;\n\n\t\tcqe = io_get_cqring(ctx);\n\t\tif (!cqe && !force)\n\t\t\tbreak;\n\n\t\tlist_move(&req->compl.list, &list);\n\t\tif (cqe) {\n\t\t\tWRITE_ONCE(cqe->user_data, req->user_data);\n\t\t\tWRITE_ONCE(cqe->res, req->result);\n\t\t\tWRITE_ONCE(cqe->flags, req->compl.cflags);\n\t\t} else {\n\t\t\tctx->cached_cq_overflow++;\n\t\t\tWRITE_ONCE(ctx->rings->cq_overflow,\n\t\t\t\t   ctx->cached_cq_overflow);\n\t\t}\n\t}\n\n\tio_commit_cqring(ctx);\n\tio_cqring_mark_overflow(ctx);\n\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\tio_cqring_ev_posted(ctx);\n\n\twhile (!list_empty(&list)) {\n\t\treq = list_first_entry(&list, struct io_kiocb, compl.list);\n\t\tlist_del(&req->compl.list);\n\t\tio_put_req(req);\n\t}\n\n\treturn cqe != NULL;\n}\n\nstatic void __io_cqring_fill_event(struct io_kiocb *req, long res, long cflags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_uring_cqe *cqe;\n\n\ttrace_io_uring_complete(ctx, req->user_data, res);\n\n\t/*\n\t * If we can't get a cq entry, userspace overflowed the\n\t * submission (by quite a lot). Increment the overflow count in\n\t * the ring.\n\t */\n\tcqe = io_get_cqring(ctx);\n\tif (likely(cqe)) {\n\t\tWRITE_ONCE(cqe->user_data, req->user_data);\n\t\tWRITE_ONCE(cqe->res, res);\n\t\tWRITE_ONCE(cqe->flags, cflags);\n\t} else if (ctx->cq_overflow_flushed ||\n\t\t   atomic_read(&req->task->io_uring->in_idle)) {\n\t\t/*\n\t\t * If we're in ring overflow flush mode, or in task cancel mode,\n\t\t * then we cannot store the request for later flushing, we need\n\t\t * to drop it on the floor.\n\t\t */\n\t\tctx->cached_cq_overflow++;\n\t\tWRITE_ONCE(ctx->rings->cq_overflow, ctx->cached_cq_overflow);\n\t} else {\n\t\tif (list_empty(&ctx->cq_overflow_list)) {\n\t\t\tset_bit(0, &ctx->sq_check_overflow);\n\t\t\tset_bit(0, &ctx->cq_check_overflow);\n\t\t\tctx->rings->sq_flags |= IORING_SQ_CQ_OVERFLOW;\n\t\t}\n\t\tio_clean_op(req);\n\t\treq->result = res;\n\t\treq->compl.cflags = cflags;\n\t\trefcount_inc(&req->refs);\n\t\tlist_add_tail(&req->compl.list, &ctx->cq_overflow_list);\n\t}\n}\n\nstatic void io_cqring_fill_event(struct io_kiocb *req, long res)\n{\n\t__io_cqring_fill_event(req, res, 0);\n}\n\nstatic void io_cqring_add_event(struct io_kiocb *req, long res, long cflags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\t__io_cqring_fill_event(req, res, cflags);\n\tio_commit_cqring(ctx);\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\tio_cqring_ev_posted(ctx);\n}\n\nstatic void io_submit_flush_completions(struct io_comp_state *cs)\n{\n\tstruct io_ring_ctx *ctx = cs->ctx;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\twhile (!list_empty(&cs->list)) {\n\t\tstruct io_kiocb *req;\n\n\t\treq = list_first_entry(&cs->list, struct io_kiocb, compl.list);\n\t\tlist_del(&req->compl.list);\n\t\t__io_cqring_fill_event(req, req->result, req->compl.cflags);\n\n\t\t/*\n\t\t * io_free_req() doesn't care about completion_lock unless one\n\t\t * of these flags is set. REQ_F_WORK_INITIALIZED is in the list\n\t\t * because of a potential deadlock with req->work.fs->lock\n\t\t */\n\t\tif (req->flags & (REQ_F_FAIL_LINK|REQ_F_LINK_TIMEOUT\n\t\t\t\t |REQ_F_WORK_INITIALIZED)) {\n\t\t\tspin_unlock_irq(&ctx->completion_lock);\n\t\t\tio_put_req(req);\n\t\t\tspin_lock_irq(&ctx->completion_lock);\n\t\t} else {\n\t\t\tio_put_req(req);\n\t\t}\n\t}\n\tio_commit_cqring(ctx);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tio_cqring_ev_posted(ctx);\n\tcs->nr = 0;\n}\n\nstatic void __io_req_complete(struct io_kiocb *req, long res, unsigned cflags,\n\t\t\t      struct io_comp_state *cs)\n{\n\tif (!cs) {\n\t\tio_cqring_add_event(req, res, cflags);\n\t\tio_put_req(req);\n\t} else {\n\t\tio_clean_op(req);\n\t\treq->result = res;\n\t\treq->compl.cflags = cflags;\n\t\tlist_add_tail(&req->compl.list, &cs->list);\n\t\tif (++cs->nr >= 32)\n\t\t\tio_submit_flush_completions(cs);\n\t}\n}\n\nstatic void io_req_complete(struct io_kiocb *req, long res)\n{\n\t__io_req_complete(req, res, 0, NULL);\n}\n\nstatic inline bool io_is_fallback_req(struct io_kiocb *req)\n{\n\treturn req == (struct io_kiocb *)\n\t\t\t((unsigned long) req->ctx->fallback_req & ~1UL);\n}\n\nstatic struct io_kiocb *io_get_fallback_req(struct io_ring_ctx *ctx)\n{\n\tstruct io_kiocb *req;\n\n\treq = ctx->fallback_req;\n\tif (!test_and_set_bit_lock(0, (unsigned long *) &ctx->fallback_req))\n\t\treturn req;\n\n\treturn NULL;\n}\n\nstatic struct io_kiocb *io_alloc_req(struct io_ring_ctx *ctx,\n\t\t\t\t     struct io_submit_state *state)\n{\n\tif (!state->free_reqs) {\n\t\tgfp_t gfp = GFP_KERNEL | __GFP_NOWARN;\n\t\tsize_t sz;\n\t\tint ret;\n\n\t\tsz = min_t(size_t, state->ios_left, ARRAY_SIZE(state->reqs));\n\t\tret = kmem_cache_alloc_bulk(req_cachep, gfp, sz, state->reqs);\n\n\t\t/*\n\t\t * Bulk alloc is all-or-nothing. If we fail to get a batch,\n\t\t * retry single alloc to be on the safe side.\n\t\t */\n\t\tif (unlikely(ret <= 0)) {\n\t\t\tstate->reqs[0] = kmem_cache_alloc(req_cachep, gfp);\n\t\t\tif (!state->reqs[0])\n\t\t\t\tgoto fallback;\n\t\t\tret = 1;\n\t\t}\n\t\tstate->free_reqs = ret;\n\t}\n\n\tstate->free_reqs--;\n\treturn state->reqs[state->free_reqs];\nfallback:\n\treturn io_get_fallback_req(ctx);\n}\n\nstatic inline void io_put_file(struct io_kiocb *req, struct file *file,\n\t\t\t  bool fixed)\n{\n\tif (!fixed)\n\t\tfput(file);\n}\n\nstatic void io_dismantle_req(struct io_kiocb *req)\n{\n\tio_clean_op(req);\n\n\tif (req->async_data)\n\t\tkfree(req->async_data);\n\tif (req->file)\n\t\tio_put_file(req, req->file, (req->flags & REQ_F_FIXED_FILE));\n\tif (req->fixed_file_refs)\n\t\tpercpu_ref_put(req->fixed_file_refs);\n\tio_req_clean_work(req);\n}\n\nstatic void __io_free_req(struct io_kiocb *req)\n{\n\tstruct io_uring_task *tctx = req->task->io_uring;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_dismantle_req(req);\n\n\tpercpu_counter_dec(&tctx->inflight);\n\tif (atomic_read(&tctx->in_idle))\n\t\twake_up(&tctx->wait);\n\tput_task_struct(req->task);\n\n\tif (likely(!io_is_fallback_req(req)))\n\t\tkmem_cache_free(req_cachep, req);\n\telse\n\t\tclear_bit_unlock(0, (unsigned long *) &ctx->fallback_req);\n\tpercpu_ref_put(&ctx->refs);\n}\n\nstatic inline void io_remove_next_linked(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt = req->link;\n\n\treq->link = nxt->link;\n\tnxt->link = NULL;\n}\n\nstatic void io_kill_linked_timeout(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_kiocb *link;\n\tbool cancelled = false;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tlink = req->link;\n\n\t/*\n\t * Can happen if a linked timeout fired and link had been like\n\t * req -> link t-out -> link t-out [-> ...]\n\t */\n\tif (link && (link->flags & REQ_F_LTIMEOUT_ACTIVE)) {\n\t\tstruct io_timeout_data *io = link->async_data;\n\t\tint ret;\n\n\t\tio_remove_next_linked(req);\n\t\tlink->timeout.head = NULL;\n\t\tret = hrtimer_try_to_cancel(&io->timer);\n\t\tif (ret != -1) {\n\t\t\tio_cqring_fill_event(link, -ECANCELED);\n\t\t\tio_commit_cqring(ctx);\n\t\t\tcancelled = true;\n\t\t}\n\t}\n\treq->flags &= ~REQ_F_LINK_TIMEOUT;\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\tif (cancelled) {\n\t\tio_cqring_ev_posted(ctx);\n\t\tio_put_req(link);\n\t}\n}\n\n\nstatic void io_fail_links(struct io_kiocb *req)\n{\n\tstruct io_kiocb *link, *nxt;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tlink = req->link;\n\treq->link = NULL;\n\n\twhile (link) {\n\t\tnxt = link->link;\n\t\tlink->link = NULL;\n\n\t\ttrace_io_uring_fail_link(req, link);\n\t\tio_cqring_fill_event(link, -ECANCELED);\n\n\t\t/*\n\t\t * It's ok to free under spinlock as they're not linked anymore,\n\t\t * but avoid REQ_F_WORK_INITIALIZED because it may deadlock on\n\t\t * work.fs->lock.\n\t\t */\n\t\tif (link->flags & REQ_F_WORK_INITIALIZED)\n\t\t\tio_put_req_deferred(link, 2);\n\t\telse\n\t\t\tio_double_put_req(link);\n\t\tlink = nxt;\n\t}\n\tio_commit_cqring(ctx);\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\tio_cqring_ev_posted(ctx);\n}\n\nstatic struct io_kiocb *__io_req_find_next(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_LINK_TIMEOUT)\n\t\tio_kill_linked_timeout(req);\n\n\t/*\n\t * If LINK is set, we have dependent requests in this chain. If we\n\t * didn't fail this request, queue the first one up, moving any other\n\t * dependencies to the next request. In case of failure, fail the rest\n\t * of the chain.\n\t */\n\tif (likely(!(req->flags & REQ_F_FAIL_LINK))) {\n\t\tstruct io_kiocb *nxt = req->link;\n\n\t\treq->link = NULL;\n\t\treturn nxt;\n\t}\n\tio_fail_links(req);\n\treturn NULL;\n}\n\nstatic inline struct io_kiocb *io_req_find_next(struct io_kiocb *req)\n{\n\tif (likely(!(req->link) && !(req->flags & REQ_F_LINK_TIMEOUT)))\n\t\treturn NULL;\n\treturn __io_req_find_next(req);\n}\n\nstatic int io_req_task_work_add(struct io_kiocb *req)\n{\n\tstruct task_struct *tsk = req->task;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tenum task_work_notify_mode notify;\n\tint ret;\n\n\tif (tsk->flags & PF_EXITING)\n\t\treturn -ESRCH;\n\n\t/*\n\t * SQPOLL kernel thread doesn't need notification, just a wakeup. For\n\t * all other cases, use TWA_SIGNAL unconditionally to ensure we're\n\t * processing task_work. There's no reliable way to tell if TWA_RESUME\n\t * will do the job.\n\t */\n\tnotify = TWA_NONE;\n\tif (!(ctx->flags & IORING_SETUP_SQPOLL))\n\t\tnotify = TWA_SIGNAL;\n\n\tret = task_work_add(tsk, &req->task_work, notify);\n\tif (!ret)\n\t\twake_up_process(tsk);\n\n\treturn ret;\n}\n\nstatic void __io_req_task_cancel(struct io_kiocb *req, int error)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tio_cqring_fill_event(req, error);\n\tio_commit_cqring(ctx);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tio_cqring_ev_posted(ctx);\n\treq_set_fail_links(req);\n\tio_double_put_req(req);\n}\n\nstatic void io_req_task_cancel(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t__io_req_task_cancel(req, -ECANCELED);\n\tpercpu_ref_put(&ctx->refs);\n}\n\nstatic void __io_req_task_submit(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!__io_sq_thread_acquire_mm(ctx) &&\n\t    !__io_sq_thread_acquire_files(ctx)) {\n\t\tmutex_lock(&ctx->uring_lock);\n\t\t__io_queue_sqe(req, NULL);\n\t\tmutex_unlock(&ctx->uring_lock);\n\t} else {\n\t\t__io_req_task_cancel(req, -EFAULT);\n\t}\n}\n\nstatic void io_req_task_submit(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t__io_req_task_submit(req);\n\tpercpu_ref_put(&ctx->refs);\n}\n\nstatic void io_req_task_queue(struct io_kiocb *req)\n{\n\tint ret;\n\n\tinit_task_work(&req->task_work, io_req_task_submit);\n\tpercpu_ref_get(&req->ctx->refs);\n\n\tret = io_req_task_work_add(req);\n\tif (unlikely(ret)) {\n\t\tstruct task_struct *tsk;\n\n\t\tinit_task_work(&req->task_work, io_req_task_cancel);\n\t\ttsk = io_wq_get_task(req->ctx->io_wq);\n\t\ttask_work_add(tsk, &req->task_work, TWA_NONE);\n\t\twake_up_process(tsk);\n\t}\n}\n\nstatic inline void io_queue_next(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt = io_req_find_next(req);\n\n\tif (nxt)\n\t\tio_req_task_queue(nxt);\n}\n\nstatic void io_free_req(struct io_kiocb *req)\n{\n\tio_queue_next(req);\n\t__io_free_req(req);\n}\n\nstruct req_batch {\n\tvoid *reqs[IO_IOPOLL_BATCH];\n\tint to_free;\n\n\tstruct task_struct\t*task;\n\tint\t\t\ttask_refs;\n};\n\nstatic inline void io_init_req_batch(struct req_batch *rb)\n{\n\trb->to_free = 0;\n\trb->task_refs = 0;\n\trb->task = NULL;\n}\n\nstatic void __io_req_free_batch_flush(struct io_ring_ctx *ctx,\n\t\t\t\t      struct req_batch *rb)\n{\n\tkmem_cache_free_bulk(req_cachep, rb->to_free, rb->reqs);\n\tpercpu_ref_put_many(&ctx->refs, rb->to_free);\n\trb->to_free = 0;\n}\n\nstatic void io_req_free_batch_finish(struct io_ring_ctx *ctx,\n\t\t\t\t     struct req_batch *rb)\n{\n\tif (rb->to_free)\n\t\t__io_req_free_batch_flush(ctx, rb);\n\tif (rb->task) {\n\t\tstruct io_uring_task *tctx = rb->task->io_uring;\n\n\t\tpercpu_counter_sub(&tctx->inflight, rb->task_refs);\n\t\tput_task_struct_many(rb->task, rb->task_refs);\n\t\trb->task = NULL;\n\t}\n}\n\nstatic void io_req_free_batch(struct req_batch *rb, struct io_kiocb *req)\n{\n\tif (unlikely(io_is_fallback_req(req))) {\n\t\tio_free_req(req);\n\t\treturn;\n\t}\n\tio_queue_next(req);\n\n\tif (req->task != rb->task) {\n\t\tif (rb->task) {\n\t\t\tstruct io_uring_task *tctx = rb->task->io_uring;\n\n\t\t\tpercpu_counter_sub(&tctx->inflight, rb->task_refs);\n\t\t\tput_task_struct_many(rb->task, rb->task_refs);\n\t\t}\n\t\trb->task = req->task;\n\t\trb->task_refs = 0;\n\t}\n\trb->task_refs++;\n\n\tio_dismantle_req(req);\n\trb->reqs[rb->to_free++] = req;\n\tif (unlikely(rb->to_free == ARRAY_SIZE(rb->reqs)))\n\t\t__io_req_free_batch_flush(req->ctx, rb);\n}\n\n/*\n * Drop reference to request, return next in chain (if there is one) if this\n * was the last reference to this request.\n */\nstatic struct io_kiocb *io_put_req_find_next(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt = NULL;\n\n\tif (refcount_dec_and_test(&req->refs)) {\n\t\tnxt = io_req_find_next(req);\n\t\t__io_free_req(req);\n\t}\n\treturn nxt;\n}\n\nstatic void io_put_req(struct io_kiocb *req)\n{\n\tif (refcount_dec_and_test(&req->refs))\n\t\tio_free_req(req);\n}\n\nstatic void io_put_req_deferred_cb(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\n\tio_free_req(req);\n}\n\nstatic void io_free_req_deferred(struct io_kiocb *req)\n{\n\tint ret;\n\n\tinit_task_work(&req->task_work, io_put_req_deferred_cb);\n\tret = io_req_task_work_add(req);\n\tif (unlikely(ret)) {\n\t\tstruct task_struct *tsk;\n\n\t\ttsk = io_wq_get_task(req->ctx->io_wq);\n\t\ttask_work_add(tsk, &req->task_work, TWA_NONE);\n\t\twake_up_process(tsk);\n\t}\n}\n\nstatic inline void io_put_req_deferred(struct io_kiocb *req, int refs)\n{\n\tif (refcount_sub_and_test(refs, &req->refs))\n\t\tio_free_req_deferred(req);\n}\n\nstatic struct io_wq_work *io_steal_work(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt;\n\n\t/*\n\t * A ref is owned by io-wq in which context we're. So, if that's the\n\t * last one, it's safe to steal next work. False negatives are Ok,\n\t * it just will be re-punted async in io_put_work()\n\t */\n\tif (refcount_read(&req->refs) != 1)\n\t\treturn NULL;\n\n\tnxt = io_req_find_next(req);\n\treturn nxt ? &nxt->work : NULL;\n}\n\nstatic void io_double_put_req(struct io_kiocb *req)\n{\n\t/* drop both submit and complete references */\n\tif (refcount_sub_and_test(2, &req->refs))\n\t\tio_free_req(req);\n}\n\nstatic unsigned io_cqring_events(struct io_ring_ctx *ctx, bool noflush)\n{\n\tstruct io_rings *rings = ctx->rings;\n\n\tif (test_bit(0, &ctx->cq_check_overflow)) {\n\t\t/*\n\t\t * noflush == true is from the waitqueue handler, just ensure\n\t\t * we wake up the task, and the next invocation will flush the\n\t\t * entries. We cannot safely to it from here.\n\t\t */\n\t\tif (noflush && !list_empty(&ctx->cq_overflow_list))\n\t\t\treturn -1U;\n\n\t\tio_cqring_overflow_flush(ctx, false, NULL, NULL);\n\t}\n\n\t/* See comment at the top of this file */\n\tsmp_rmb();\n\treturn ctx->cached_cq_tail - READ_ONCE(rings->cq.head);\n}\n\nstatic inline unsigned int io_sqring_entries(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *rings = ctx->rings;\n\n\t/* make sure SQ entry isn't read before tail */\n\treturn smp_load_acquire(&rings->sq.tail) - ctx->cached_sq_head;\n}\n\nstatic unsigned int io_put_kbuf(struct io_kiocb *req, struct io_buffer *kbuf)\n{\n\tunsigned int cflags;\n\n\tcflags = kbuf->bid << IORING_CQE_BUFFER_SHIFT;\n\tcflags |= IORING_CQE_F_BUFFER;\n\treq->flags &= ~REQ_F_BUFFER_SELECTED;\n\tkfree(kbuf);\n\treturn cflags;\n}\n\nstatic inline unsigned int io_put_rw_kbuf(struct io_kiocb *req)\n{\n\tstruct io_buffer *kbuf;\n\n\tkbuf = (struct io_buffer *) (unsigned long) req->rw.addr;\n\treturn io_put_kbuf(req, kbuf);\n}\n\nstatic inline bool io_run_task_work(void)\n{\n\t/*\n\t * Not safe to run on exiting task, and the task_work handling will\n\t * not add work to such a task.\n\t */\n\tif (unlikely(current->flags & PF_EXITING))\n\t\treturn false;\n\tif (current->task_works) {\n\t\t__set_current_state(TASK_RUNNING);\n\t\ttask_work_run();\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void io_iopoll_queue(struct list_head *again)\n{\n\tstruct io_kiocb *req;\n\n\tdo {\n\t\treq = list_first_entry(again, struct io_kiocb, inflight_entry);\n\t\tlist_del(&req->inflight_entry);\n\t\t__io_complete_rw(req, -EAGAIN, 0, NULL);\n\t} while (!list_empty(again));\n}\n\n/*\n * Find and free completed poll iocbs\n */\nstatic void io_iopoll_complete(struct io_ring_ctx *ctx, unsigned int *nr_events,\n\t\t\t       struct list_head *done)\n{\n\tstruct req_batch rb;\n\tstruct io_kiocb *req;\n\tLIST_HEAD(again);\n\n\t/* order with ->result store in io_complete_rw_iopoll() */\n\tsmp_rmb();\n\n\tio_init_req_batch(&rb);\n\twhile (!list_empty(done)) {\n\t\tint cflags = 0;\n\n\t\treq = list_first_entry(done, struct io_kiocb, inflight_entry);\n\t\tif (READ_ONCE(req->result) == -EAGAIN) {\n\t\t\treq->result = 0;\n\t\t\treq->iopoll_completed = 0;\n\t\t\tlist_move_tail(&req->inflight_entry, &again);\n\t\t\tcontinue;\n\t\t}\n\t\tlist_del(&req->inflight_entry);\n\n\t\tif (req->flags & REQ_F_BUFFER_SELECTED)\n\t\t\tcflags = io_put_rw_kbuf(req);\n\n\t\t__io_cqring_fill_event(req, req->result, cflags);\n\t\t(*nr_events)++;\n\n\t\tif (refcount_dec_and_test(&req->refs))\n\t\t\tio_req_free_batch(&rb, req);\n\t}\n\n\tio_commit_cqring(ctx);\n\tif (ctx->flags & IORING_SETUP_SQPOLL)\n\t\tio_cqring_ev_posted(ctx);\n\tio_req_free_batch_finish(ctx, &rb);\n\n\tif (!list_empty(&again))\n\t\tio_iopoll_queue(&again);\n}\n\nstatic int io_do_iopoll(struct io_ring_ctx *ctx, unsigned int *nr_events,\n\t\t\tlong min)\n{\n\tstruct io_kiocb *req, *tmp;\n\tLIST_HEAD(done);\n\tbool spin;\n\tint ret;\n\n\t/*\n\t * Only spin for completions if we don't have multiple devices hanging\n\t * off our complete list, and we're under the requested amount.\n\t */\n\tspin = !ctx->poll_multi_file && *nr_events < min;\n\n\tret = 0;\n\tlist_for_each_entry_safe(req, tmp, &ctx->iopoll_list, inflight_entry) {\n\t\tstruct kiocb *kiocb = &req->rw.kiocb;\n\n\t\t/*\n\t\t * Move completed and retryable entries to our local lists.\n\t\t * If we find a request that requires polling, break out\n\t\t * and complete those lists first, if we have entries there.\n\t\t */\n\t\tif (READ_ONCE(req->iopoll_completed)) {\n\t\t\tlist_move_tail(&req->inflight_entry, &done);\n\t\t\tcontinue;\n\t\t}\n\t\tif (!list_empty(&done))\n\t\t\tbreak;\n\n\t\tret = kiocb->ki_filp->f_op->iopoll(kiocb, spin);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\n\t\t/* iopoll may have completed current req */\n\t\tif (READ_ONCE(req->iopoll_completed))\n\t\t\tlist_move_tail(&req->inflight_entry, &done);\n\n\t\tif (ret && spin)\n\t\t\tspin = false;\n\t\tret = 0;\n\t}\n\n\tif (!list_empty(&done))\n\t\tio_iopoll_complete(ctx, nr_events, &done);\n\n\treturn ret;\n}\n\n/*\n * Poll for a minimum of 'min' events. Note that if min == 0 we consider that a\n * non-spinning poll check - we'll still enter the driver poll loop, but only\n * as a non-spinning completion check.\n */\nstatic int io_iopoll_getevents(struct io_ring_ctx *ctx, unsigned int *nr_events,\n\t\t\t\tlong min)\n{\n\twhile (!list_empty(&ctx->iopoll_list) && !need_resched()) {\n\t\tint ret;\n\n\t\tret = io_do_iopoll(ctx, nr_events, min);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tif (*nr_events >= min)\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\n/*\n * We can't just wait for polled events to come to us, we have to actively\n * find and complete them.\n */\nstatic void io_iopoll_try_reap_events(struct io_ring_ctx *ctx)\n{\n\tif (!(ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn;\n\n\tmutex_lock(&ctx->uring_lock);\n\twhile (!list_empty(&ctx->iopoll_list)) {\n\t\tunsigned int nr_events = 0;\n\n\t\tio_do_iopoll(ctx, &nr_events, 0);\n\n\t\t/* let it sleep and repeat later if can't complete a request */\n\t\tif (nr_events == 0)\n\t\t\tbreak;\n\t\t/*\n\t\t * Ensure we allow local-to-the-cpu processing to take place,\n\t\t * in this case we need to ensure that we reap all events.\n\t\t * Also let task_work, etc. to progress by releasing the mutex\n\t\t */\n\t\tif (need_resched()) {\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t\t\tcond_resched();\n\t\t\tmutex_lock(&ctx->uring_lock);\n\t\t}\n\t}\n\tmutex_unlock(&ctx->uring_lock);\n}\n\nstatic int io_iopoll_check(struct io_ring_ctx *ctx, long min)\n{\n\tunsigned int nr_events = 0;\n\tint iters = 0, ret = 0;\n\n\t/*\n\t * We disallow the app entering submit/complete with polling, but we\n\t * still need to lock the ring to prevent racing with polled issue\n\t * that got punted to a workqueue.\n\t */\n\tmutex_lock(&ctx->uring_lock);\n\tdo {\n\t\t/*\n\t\t * Don't enter poll loop if we already have events pending.\n\t\t * If we do, we can potentially be spinning for commands that\n\t\t * already triggered a CQE (eg in error).\n\t\t */\n\t\tif (io_cqring_events(ctx, false))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * If a submit got punted to a workqueue, we can have the\n\t\t * application entering polling for a command before it gets\n\t\t * issued. That app will hold the uring_lock for the duration\n\t\t * of the poll right here, so we need to take a breather every\n\t\t * now and then to ensure that the issue has a chance to add\n\t\t * the poll to the issued list. Otherwise we can spin here\n\t\t * forever, while the workqueue is stuck trying to acquire the\n\t\t * very same mutex.\n\t\t */\n\t\tif (!(++iters & 7)) {\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t\t\tio_run_task_work();\n\t\t\tmutex_lock(&ctx->uring_lock);\n\t\t}\n\n\t\tret = io_iopoll_getevents(ctx, &nr_events, min);\n\t\tif (ret <= 0)\n\t\t\tbreak;\n\t\tret = 0;\n\t} while (min && !nr_events && !need_resched());\n\n\tmutex_unlock(&ctx->uring_lock);\n\treturn ret;\n}\n\nstatic void kiocb_end_write(struct io_kiocb *req)\n{\n\t/*\n\t * Tell lockdep we inherited freeze protection from submission\n\t * thread.\n\t */\n\tif (req->flags & REQ_F_ISREG) {\n\t\tstruct inode *inode = file_inode(req->file);\n\n\t\t__sb_writers_acquired(inode->i_sb, SB_FREEZE_WRITE);\n\t}\n\tfile_end_write(req->file);\n}\n\nstatic void io_complete_rw_common(struct kiocb *kiocb, long res,\n\t\t\t\t  struct io_comp_state *cs)\n{\n\tstruct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw.kiocb);\n\tint cflags = 0;\n\n\tif (kiocb->ki_flags & IOCB_WRITE)\n\t\tkiocb_end_write(req);\n\n\tif (res != req->result)\n\t\treq_set_fail_links(req);\n\tif (req->flags & REQ_F_BUFFER_SELECTED)\n\t\tcflags = io_put_rw_kbuf(req);\n\t__io_req_complete(req, res, cflags, cs);\n}\n\n#ifdef CONFIG_BLOCK\nstatic bool io_resubmit_prep(struct io_kiocb *req, int error)\n{\n\tstruct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;\n\tssize_t ret = -ECANCELED;\n\tstruct iov_iter iter;\n\tint rw;\n\n\tif (error) {\n\t\tret = error;\n\t\tgoto end_req;\n\t}\n\n\tswitch (req->opcode) {\n\tcase IORING_OP_READV:\n\tcase IORING_OP_READ_FIXED:\n\tcase IORING_OP_READ:\n\t\trw = READ;\n\t\tbreak;\n\tcase IORING_OP_WRITEV:\n\tcase IORING_OP_WRITE_FIXED:\n\tcase IORING_OP_WRITE:\n\t\trw = WRITE;\n\t\tbreak;\n\tdefault:\n\t\tprintk_once(KERN_WARNING \"io_uring: bad opcode in resubmit %d\\n\",\n\t\t\t\treq->opcode);\n\t\tgoto end_req;\n\t}\n\n\tif (!req->async_data) {\n\t\tret = io_import_iovec(rw, req, &iovec, &iter, false);\n\t\tif (ret < 0)\n\t\t\tgoto end_req;\n\t\tret = io_setup_async_rw(req, iovec, inline_vecs, &iter, false);\n\t\tif (!ret)\n\t\t\treturn true;\n\t\tkfree(iovec);\n\t} else {\n\t\treturn true;\n\t}\nend_req:\n\treq_set_fail_links(req);\n\treturn false;\n}\n#endif\n\nstatic bool io_rw_reissue(struct io_kiocb *req, long res)\n{\n#ifdef CONFIG_BLOCK\n\tumode_t mode = file_inode(req->file)->i_mode;\n\tint ret;\n\n\tif (!S_ISBLK(mode) && !S_ISREG(mode))\n\t\treturn false;\n\tif ((res != -EAGAIN && res != -EOPNOTSUPP) || io_wq_current_is_worker())\n\t\treturn false;\n\n\tret = io_sq_thread_acquire_mm_files(req->ctx, req);\n\n\tif (io_resubmit_prep(req, ret)) {\n\t\trefcount_inc(&req->refs);\n\t\tio_queue_async_work(req);\n\t\treturn true;\n\t}\n\n#endif\n\treturn false;\n}\n\nstatic void __io_complete_rw(struct io_kiocb *req, long res, long res2,\n\t\t\t     struct io_comp_state *cs)\n{\n\tif (!io_rw_reissue(req, res))\n\t\tio_complete_rw_common(&req->rw.kiocb, res, cs);\n}\n\nstatic void io_complete_rw(struct kiocb *kiocb, long res, long res2)\n{\n\tstruct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw.kiocb);\n\n\t__io_complete_rw(req, res, res2, NULL);\n}\n\nstatic void io_complete_rw_iopoll(struct kiocb *kiocb, long res, long res2)\n{\n\tstruct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw.kiocb);\n\n\tif (kiocb->ki_flags & IOCB_WRITE)\n\t\tkiocb_end_write(req);\n\n\tif (res != -EAGAIN && res != req->result)\n\t\treq_set_fail_links(req);\n\n\tWRITE_ONCE(req->result, res);\n\t/* order with io_poll_complete() checking ->result */\n\tsmp_wmb();\n\tWRITE_ONCE(req->iopoll_completed, 1);\n}\n\n/*\n * After the iocb has been issued, it's safe to be found on the poll list.\n * Adding the kiocb to the list AFTER submission ensures that we don't\n * find it from a io_iopoll_getevents() thread before the issuer is done\n * accessing the kiocb cookie.\n */\nstatic void io_iopoll_req_issued(struct io_kiocb *req, bool in_async)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t/*\n\t * Track whether we have multiple files in our lists. This will impact\n\t * how we do polling eventually, not spinning if we're on potentially\n\t * different devices.\n\t */\n\tif (list_empty(&ctx->iopoll_list)) {\n\t\tctx->poll_multi_file = false;\n\t} else if (!ctx->poll_multi_file) {\n\t\tstruct io_kiocb *list_req;\n\n\t\tlist_req = list_first_entry(&ctx->iopoll_list, struct io_kiocb,\n\t\t\t\t\t\tinflight_entry);\n\t\tif (list_req->file != req->file)\n\t\t\tctx->poll_multi_file = true;\n\t}\n\n\t/*\n\t * For fast devices, IO may have already completed. If it has, add\n\t * it to the front so we find it first.\n\t */\n\tif (READ_ONCE(req->iopoll_completed))\n\t\tlist_add(&req->inflight_entry, &ctx->iopoll_list);\n\telse\n\t\tlist_add_tail(&req->inflight_entry, &ctx->iopoll_list);\n\n\t/*\n\t * If IORING_SETUP_SQPOLL is enabled, sqes are either handled in sq thread\n\t * task context or in io worker task context. If current task context is\n\t * sq thread, we don't need to check whether should wake up sq thread.\n\t */\n\tif (in_async && (ctx->flags & IORING_SETUP_SQPOLL) &&\n\t    wq_has_sleeper(&ctx->sq_data->wait))\n\t\twake_up(&ctx->sq_data->wait);\n}\n\nstatic void __io_state_file_put(struct io_submit_state *state)\n{\n\tif (state->has_refs)\n\t\tfput_many(state->file, state->has_refs);\n\tstate->file = NULL;\n}\n\nstatic inline void io_state_file_put(struct io_submit_state *state)\n{\n\tif (state->file)\n\t\t__io_state_file_put(state);\n}\n\n/*\n * Get as many references to a file as we have IOs left in this submission,\n * assuming most submissions are for one file, or at least that each file\n * has more than one submission.\n */\nstatic struct file *__io_file_get(struct io_submit_state *state, int fd)\n{\n\tif (!state)\n\t\treturn fget(fd);\n\n\tif (state->file) {\n\t\tif (state->fd == fd) {\n\t\t\tstate->has_refs--;\n\t\t\treturn state->file;\n\t\t}\n\t\t__io_state_file_put(state);\n\t}\n\tstate->file = fget_many(fd, state->ios_left);\n\tif (!state->file)\n\t\treturn NULL;\n\n\tstate->fd = fd;\n\tstate->has_refs = state->ios_left - 1;\n\treturn state->file;\n}\n\nstatic bool io_bdev_nowait(struct block_device *bdev)\n{\n#ifdef CONFIG_BLOCK\n\treturn !bdev || blk_queue_nowait(bdev_get_queue(bdev));\n#else\n\treturn true;\n#endif\n}\n\n/*\n * If we tracked the file through the SCM inflight mechanism, we could support\n * any file. For now, just ensure that anything potentially problematic is done\n * inline.\n */\nstatic bool io_file_supports_async(struct file *file, int rw)\n{\n\tumode_t mode = file_inode(file)->i_mode;\n\n\tif (S_ISBLK(mode)) {\n\t\tif (io_bdev_nowait(file->f_inode->i_bdev))\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\tif (S_ISCHR(mode) || S_ISSOCK(mode))\n\t\treturn true;\n\tif (S_ISREG(mode)) {\n\t\tif (io_bdev_nowait(file->f_inode->i_sb->s_bdev) &&\n\t\t    file->f_op != &io_uring_fops)\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\n\t/* any ->read/write should understand O_NONBLOCK */\n\tif (file->f_flags & O_NONBLOCK)\n\t\treturn true;\n\n\tif (!(file->f_mode & FMODE_NOWAIT))\n\t\treturn false;\n\n\tif (rw == READ)\n\t\treturn file->f_op->read_iter != NULL;\n\n\treturn file->f_op->write_iter != NULL;\n}\n\nstatic int io_prep_rw(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct kiocb *kiocb = &req->rw.kiocb;\n\tunsigned ioprio;\n\tint ret;\n\n\tif (S_ISREG(file_inode(req->file)->i_mode))\n\t\treq->flags |= REQ_F_ISREG;\n\n\tkiocb->ki_pos = READ_ONCE(sqe->off);\n\tif (kiocb->ki_pos == -1 && !(req->file->f_mode & FMODE_STREAM)) {\n\t\treq->flags |= REQ_F_CUR_POS;\n\t\tkiocb->ki_pos = req->file->f_pos;\n\t}\n\tkiocb->ki_hint = ki_hint_validate(file_write_hint(kiocb->ki_filp));\n\tkiocb->ki_flags = iocb_flags(kiocb->ki_filp);\n\tret = kiocb_set_rw_flags(kiocb, READ_ONCE(sqe->rw_flags));\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tioprio = READ_ONCE(sqe->ioprio);\n\tif (ioprio) {\n\t\tret = ioprio_check_cap(ioprio);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tkiocb->ki_ioprio = ioprio;\n\t} else\n\t\tkiocb->ki_ioprio = get_current_ioprio();\n\n\t/* don't allow async punt if RWF_NOWAIT was requested */\n\tif (kiocb->ki_flags & IOCB_NOWAIT)\n\t\treq->flags |= REQ_F_NOWAIT;\n\n\tif (ctx->flags & IORING_SETUP_IOPOLL) {\n\t\tif (!(kiocb->ki_flags & IOCB_DIRECT) ||\n\t\t    !kiocb->ki_filp->f_op->iopoll)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tkiocb->ki_flags |= IOCB_HIPRI;\n\t\tkiocb->ki_complete = io_complete_rw_iopoll;\n\t\treq->iopoll_completed = 0;\n\t} else {\n\t\tif (kiocb->ki_flags & IOCB_HIPRI)\n\t\t\treturn -EINVAL;\n\t\tkiocb->ki_complete = io_complete_rw;\n\t}\n\n\treq->rw.addr = READ_ONCE(sqe->addr);\n\treq->rw.len = READ_ONCE(sqe->len);\n\treq->buf_index = READ_ONCE(sqe->buf_index);\n\treturn 0;\n}\n\nstatic inline void io_rw_done(struct kiocb *kiocb, ssize_t ret)\n{\n\tswitch (ret) {\n\tcase -EIOCBQUEUED:\n\t\tbreak;\n\tcase -ERESTARTSYS:\n\tcase -ERESTARTNOINTR:\n\tcase -ERESTARTNOHAND:\n\tcase -ERESTART_RESTARTBLOCK:\n\t\t/*\n\t\t * We can't just restart the syscall, since previously\n\t\t * submitted sqes may already be in progress. Just fail this\n\t\t * IO with EINTR.\n\t\t */\n\t\tret = -EINTR;\n\t\tfallthrough;\n\tdefault:\n\t\tkiocb->ki_complete(kiocb, ret, 0);\n\t}\n}\n\nstatic void kiocb_done(struct kiocb *kiocb, ssize_t ret,\n\t\t       struct io_comp_state *cs)\n{\n\tstruct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw.kiocb);\n\tstruct io_async_rw *io = req->async_data;\n\n\t/* add previously done IO, if any */\n\tif (io && io->bytes_done > 0) {\n\t\tif (ret < 0)\n\t\t\tret = io->bytes_done;\n\t\telse\n\t\t\tret += io->bytes_done;\n\t}\n\n\tif (req->flags & REQ_F_CUR_POS)\n\t\treq->file->f_pos = kiocb->ki_pos;\n\tif (ret >= 0 && kiocb->ki_complete == io_complete_rw)\n\t\t__io_complete_rw(req, ret, 0, cs);\n\telse\n\t\tio_rw_done(kiocb, ret);\n}\n\nstatic ssize_t io_import_fixed(struct io_kiocb *req, int rw,\n\t\t\t       struct iov_iter *iter)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tsize_t len = req->rw.len;\n\tstruct io_mapped_ubuf *imu;\n\tu16 index, buf_index = req->buf_index;\n\tsize_t offset;\n\tu64 buf_addr;\n\n\tif (unlikely(buf_index >= ctx->nr_user_bufs))\n\t\treturn -EFAULT;\n\tindex = array_index_nospec(buf_index, ctx->nr_user_bufs);\n\timu = &ctx->user_bufs[index];\n\tbuf_addr = req->rw.addr;\n\n\t/* overflow */\n\tif (buf_addr + len < buf_addr)\n\t\treturn -EFAULT;\n\t/* not inside the mapped region */\n\tif (buf_addr < imu->ubuf || buf_addr + len > imu->ubuf + imu->len)\n\t\treturn -EFAULT;\n\n\t/*\n\t * May not be a start of buffer, set size appropriately\n\t * and advance us to the beginning.\n\t */\n\toffset = buf_addr - imu->ubuf;\n\tiov_iter_bvec(iter, rw, imu->bvec, imu->nr_bvecs, offset + len);\n\n\tif (offset) {\n\t\t/*\n\t\t * Don't use iov_iter_advance() here, as it's really slow for\n\t\t * using the latter parts of a big fixed buffer - it iterates\n\t\t * over each segment manually. We can cheat a bit here, because\n\t\t * we know that:\n\t\t *\n\t\t * 1) it's a BVEC iter, we set it up\n\t\t * 2) all bvecs are PAGE_SIZE in size, except potentially the\n\t\t *    first and last bvec\n\t\t *\n\t\t * So just find our index, and adjust the iterator afterwards.\n\t\t * If the offset is within the first bvec (or the whole first\n\t\t * bvec, just use iov_iter_advance(). This makes it easier\n\t\t * since we can just skip the first segment, which may not\n\t\t * be PAGE_SIZE aligned.\n\t\t */\n\t\tconst struct bio_vec *bvec = imu->bvec;\n\n\t\tif (offset <= bvec->bv_len) {\n\t\t\tiov_iter_advance(iter, offset);\n\t\t} else {\n\t\t\tunsigned long seg_skip;\n\n\t\t\t/* skip first vec */\n\t\t\toffset -= bvec->bv_len;\n\t\t\tseg_skip = 1 + (offset >> PAGE_SHIFT);\n\n\t\t\titer->bvec = bvec + seg_skip;\n\t\t\titer->nr_segs -= seg_skip;\n\t\t\titer->count -= bvec->bv_len + offset;\n\t\t\titer->iov_offset = offset & ~PAGE_MASK;\n\t\t}\n\t}\n\n\treturn len;\n}\n\nstatic void io_ring_submit_unlock(struct io_ring_ctx *ctx, bool needs_lock)\n{\n\tif (needs_lock)\n\t\tmutex_unlock(&ctx->uring_lock);\n}\n\nstatic void io_ring_submit_lock(struct io_ring_ctx *ctx, bool needs_lock)\n{\n\t/*\n\t * \"Normal\" inline submissions always hold the uring_lock, since we\n\t * grab it from the system call. Same is true for the SQPOLL offload.\n\t * The only exception is when we've detached the request and issue it\n\t * from an async worker thread, grab the lock for that case.\n\t */\n\tif (needs_lock)\n\t\tmutex_lock(&ctx->uring_lock);\n}\n\nstatic struct io_buffer *io_buffer_select(struct io_kiocb *req, size_t *len,\n\t\t\t\t\t  int bgid, struct io_buffer *kbuf,\n\t\t\t\t\t  bool needs_lock)\n{\n\tstruct io_buffer *head;\n\n\tif (req->flags & REQ_F_BUFFER_SELECTED)\n\t\treturn kbuf;\n\n\tio_ring_submit_lock(req->ctx, needs_lock);\n\n\tlockdep_assert_held(&req->ctx->uring_lock);\n\n\thead = idr_find(&req->ctx->io_buffer_idr, bgid);\n\tif (head) {\n\t\tif (!list_empty(&head->list)) {\n\t\t\tkbuf = list_last_entry(&head->list, struct io_buffer,\n\t\t\t\t\t\t\tlist);\n\t\t\tlist_del(&kbuf->list);\n\t\t} else {\n\t\t\tkbuf = head;\n\t\t\tidr_remove(&req->ctx->io_buffer_idr, bgid);\n\t\t}\n\t\tif (*len > kbuf->len)\n\t\t\t*len = kbuf->len;\n\t} else {\n\t\tkbuf = ERR_PTR(-ENOBUFS);\n\t}\n\n\tio_ring_submit_unlock(req->ctx, needs_lock);\n\n\treturn kbuf;\n}\n\nstatic void __user *io_rw_buffer_select(struct io_kiocb *req, size_t *len,\n\t\t\t\t\tbool needs_lock)\n{\n\tstruct io_buffer *kbuf;\n\tu16 bgid;\n\n\tkbuf = (struct io_buffer *) (unsigned long) req->rw.addr;\n\tbgid = req->buf_index;\n\tkbuf = io_buffer_select(req, len, bgid, kbuf, needs_lock);\n\tif (IS_ERR(kbuf))\n\t\treturn kbuf;\n\treq->rw.addr = (u64) (unsigned long) kbuf;\n\treq->flags |= REQ_F_BUFFER_SELECTED;\n\treturn u64_to_user_ptr(kbuf->addr);\n}\n\n#ifdef CONFIG_COMPAT\nstatic ssize_t io_compat_import(struct io_kiocb *req, struct iovec *iov,\n\t\t\t\tbool needs_lock)\n{\n\tstruct compat_iovec __user *uiov;\n\tcompat_ssize_t clen;\n\tvoid __user *buf;\n\tssize_t len;\n\n\tuiov = u64_to_user_ptr(req->rw.addr);\n\tif (!access_ok(uiov, sizeof(*uiov)))\n\t\treturn -EFAULT;\n\tif (__get_user(clen, &uiov->iov_len))\n\t\treturn -EFAULT;\n\tif (clen < 0)\n\t\treturn -EINVAL;\n\n\tlen = clen;\n\tbuf = io_rw_buffer_select(req, &len, needs_lock);\n\tif (IS_ERR(buf))\n\t\treturn PTR_ERR(buf);\n\tiov[0].iov_base = buf;\n\tiov[0].iov_len = (compat_size_t) len;\n\treturn 0;\n}\n#endif\n\nstatic ssize_t __io_iov_buffer_select(struct io_kiocb *req, struct iovec *iov,\n\t\t\t\t      bool needs_lock)\n{\n\tstruct iovec __user *uiov = u64_to_user_ptr(req->rw.addr);\n\tvoid __user *buf;\n\tssize_t len;\n\n\tif (copy_from_user(iov, uiov, sizeof(*uiov)))\n\t\treturn -EFAULT;\n\n\tlen = iov[0].iov_len;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\tbuf = io_rw_buffer_select(req, &len, needs_lock);\n\tif (IS_ERR(buf))\n\t\treturn PTR_ERR(buf);\n\tiov[0].iov_base = buf;\n\tiov[0].iov_len = len;\n\treturn 0;\n}\n\nstatic ssize_t io_iov_buffer_select(struct io_kiocb *req, struct iovec *iov,\n\t\t\t\t    bool needs_lock)\n{\n\tif (req->flags & REQ_F_BUFFER_SELECTED) {\n\t\tstruct io_buffer *kbuf;\n\n\t\tkbuf = (struct io_buffer *) (unsigned long) req->rw.addr;\n\t\tiov[0].iov_base = u64_to_user_ptr(kbuf->addr);\n\t\tiov[0].iov_len = kbuf->len;\n\t\treturn 0;\n\t}\n\tif (!req->rw.len)\n\t\treturn 0;\n\telse if (req->rw.len > 1)\n\t\treturn -EINVAL;\n\n#ifdef CONFIG_COMPAT\n\tif (req->ctx->compat)\n\t\treturn io_compat_import(req, iov, needs_lock);\n#endif\n\n\treturn __io_iov_buffer_select(req, iov, needs_lock);\n}\n\nstatic ssize_t io_import_iovec(int rw, struct io_kiocb *req,\n\t\t\t\t struct iovec **iovec, struct iov_iter *iter,\n\t\t\t\t bool needs_lock)\n{\n\tvoid __user *buf = u64_to_user_ptr(req->rw.addr);\n\tsize_t sqe_len = req->rw.len;\n\tssize_t ret;\n\tu8 opcode;\n\n\topcode = req->opcode;\n\tif (opcode == IORING_OP_READ_FIXED || opcode == IORING_OP_WRITE_FIXED) {\n\t\t*iovec = NULL;\n\t\treturn io_import_fixed(req, rw, iter);\n\t}\n\n\t/* buffer index only valid with fixed read/write, or buffer select  */\n\tif (req->buf_index && !(req->flags & REQ_F_BUFFER_SELECT))\n\t\treturn -EINVAL;\n\n\tif (opcode == IORING_OP_READ || opcode == IORING_OP_WRITE) {\n\t\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\t\tbuf = io_rw_buffer_select(req, &sqe_len, needs_lock);\n\t\t\tif (IS_ERR(buf))\n\t\t\t\treturn PTR_ERR(buf);\n\t\t\treq->rw.len = sqe_len;\n\t\t}\n\n\t\tret = import_single_range(rw, buf, sqe_len, *iovec, iter);\n\t\t*iovec = NULL;\n\t\treturn ret;\n\t}\n\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tret = io_iov_buffer_select(req, *iovec, needs_lock);\n\t\tif (!ret) {\n\t\t\tret = (*iovec)->iov_len;\n\t\t\tiov_iter_init(iter, rw, *iovec, 1, ret);\n\t\t}\n\t\t*iovec = NULL;\n\t\treturn ret;\n\t}\n\n\treturn __import_iovec(rw, buf, sqe_len, UIO_FASTIOV, iovec, iter,\n\t\t\t      req->ctx->compat);\n}\n\nstatic inline loff_t *io_kiocb_ppos(struct kiocb *kiocb)\n{\n\treturn (kiocb->ki_filp->f_mode & FMODE_STREAM) ? NULL : &kiocb->ki_pos;\n}\n\n/*\n * For files that don't have ->read_iter() and ->write_iter(), handle them\n * by looping over ->read() or ->write() manually.\n */\nstatic ssize_t loop_rw_iter(int rw, struct io_kiocb *req, struct iov_iter *iter)\n{\n\tstruct kiocb *kiocb = &req->rw.kiocb;\n\tstruct file *file = req->file;\n\tssize_t ret = 0;\n\n\t/*\n\t * Don't support polled IO through this interface, and we can't\n\t * support non-blocking either. For the latter, this just causes\n\t * the kiocb to be handled from an async context.\n\t */\n\tif (kiocb->ki_flags & IOCB_HIPRI)\n\t\treturn -EOPNOTSUPP;\n\tif (kiocb->ki_flags & IOCB_NOWAIT)\n\t\treturn -EAGAIN;\n\n\twhile (iov_iter_count(iter)) {\n\t\tstruct iovec iovec;\n\t\tssize_t nr;\n\n\t\tif (!iov_iter_is_bvec(iter)) {\n\t\t\tiovec = iov_iter_iovec(iter);\n\t\t} else {\n\t\t\tiovec.iov_base = u64_to_user_ptr(req->rw.addr);\n\t\t\tiovec.iov_len = req->rw.len;\n\t\t}\n\n\t\tif (rw == READ) {\n\t\t\tnr = file->f_op->read(file, iovec.iov_base,\n\t\t\t\t\t      iovec.iov_len, io_kiocb_ppos(kiocb));\n\t\t} else {\n\t\t\tnr = file->f_op->write(file, iovec.iov_base,\n\t\t\t\t\t       iovec.iov_len, io_kiocb_ppos(kiocb));\n\t\t}\n\n\t\tif (nr < 0) {\n\t\t\tif (!ret)\n\t\t\t\tret = nr;\n\t\t\tbreak;\n\t\t}\n\t\tret += nr;\n\t\tif (nr != iovec.iov_len)\n\t\t\tbreak;\n\t\treq->rw.len -= nr;\n\t\treq->rw.addr += nr;\n\t\tiov_iter_advance(iter, nr);\n\t}\n\n\treturn ret;\n}\n\nstatic void io_req_map_rw(struct io_kiocb *req, const struct iovec *iovec,\n\t\t\t  const struct iovec *fast_iov, struct iov_iter *iter)\n{\n\tstruct io_async_rw *rw = req->async_data;\n\n\tmemcpy(&rw->iter, iter, sizeof(*iter));\n\trw->free_iovec = iovec;\n\trw->bytes_done = 0;\n\t/* can only be fixed buffers, no need to do anything */\n\tif (iter->type == ITER_BVEC)\n\t\treturn;\n\tif (!iovec) {\n\t\tunsigned iov_off = 0;\n\n\t\trw->iter.iov = rw->fast_iov;\n\t\tif (iter->iov != fast_iov) {\n\t\t\tiov_off = iter->iov - fast_iov;\n\t\t\trw->iter.iov += iov_off;\n\t\t}\n\t\tif (rw->fast_iov != fast_iov)\n\t\t\tmemcpy(rw->fast_iov + iov_off, fast_iov + iov_off,\n\t\t\t       sizeof(struct iovec) * iter->nr_segs);\n\t} else {\n\t\treq->flags |= REQ_F_NEED_CLEANUP;\n\t}\n}\n\nstatic inline int __io_alloc_async_data(struct io_kiocb *req)\n{\n\tWARN_ON_ONCE(!io_op_defs[req->opcode].async_size);\n\treq->async_data = kmalloc(io_op_defs[req->opcode].async_size, GFP_KERNEL);\n\treturn req->async_data == NULL;\n}\n\nstatic int io_alloc_async_data(struct io_kiocb *req)\n{\n\tif (!io_op_defs[req->opcode].needs_async_data)\n\t\treturn 0;\n\n\treturn  __io_alloc_async_data(req);\n}\n\nstatic int io_setup_async_rw(struct io_kiocb *req, const struct iovec *iovec,\n\t\t\t     const struct iovec *fast_iov,\n\t\t\t     struct iov_iter *iter, bool force)\n{\n\tif (!force && !io_op_defs[req->opcode].needs_async_data)\n\t\treturn 0;\n\tif (!req->async_data) {\n\t\tif (__io_alloc_async_data(req))\n\t\t\treturn -ENOMEM;\n\n\t\tio_req_map_rw(req, iovec, fast_iov, iter);\n\t}\n\treturn 0;\n}\n\nstatic inline int io_rw_prep_async(struct io_kiocb *req, int rw)\n{\n\tstruct io_async_rw *iorw = req->async_data;\n\tstruct iovec *iov = iorw->fast_iov;\n\tssize_t ret;\n\n\tret = io_import_iovec(rw, req, &iov, &iorw->iter, false);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tiorw->bytes_done = 0;\n\tiorw->free_iovec = iov;\n\tif (iov)\n\t\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn 0;\n}\n\nstatic int io_read_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tssize_t ret;\n\n\tret = io_prep_rw(req, sqe);\n\tif (ret)\n\t\treturn ret;\n\n\tif (unlikely(!(req->file->f_mode & FMODE_READ)))\n\t\treturn -EBADF;\n\n\t/* either don't need iovec imported or already have it */\n\tif (!req->async_data)\n\t\treturn 0;\n\treturn io_rw_prep_async(req, READ);\n}\n\n/*\n * This is our waitqueue callback handler, registered through lock_page_async()\n * when we initially tried to do the IO with the iocb armed our waitqueue.\n * This gets called when the page is unlocked, and we generally expect that to\n * happen when the page IO is completed and the page is now uptodate. This will\n * queue a task_work based retry of the operation, attempting to copy the data\n * again. If the latter fails because the page was NOT uptodate, then we will\n * do a thread based blocking retry of the operation. That's the unexpected\n * slow path.\n */\nstatic int io_async_buf_func(struct wait_queue_entry *wait, unsigned mode,\n\t\t\t     int sync, void *arg)\n{\n\tstruct wait_page_queue *wpq;\n\tstruct io_kiocb *req = wait->private;\n\tstruct wait_page_key *key = arg;\n\tint ret;\n\n\twpq = container_of(wait, struct wait_page_queue, wait);\n\n\tif (!wake_page_match(wpq, key))\n\t\treturn 0;\n\n\treq->rw.kiocb.ki_flags &= ~IOCB_WAITQ;\n\tlist_del_init(&wait->entry);\n\n\tinit_task_work(&req->task_work, io_req_task_submit);\n\tpercpu_ref_get(&req->ctx->refs);\n\n\t/* submit ref gets dropped, acquire a new one */\n\trefcount_inc(&req->refs);\n\tret = io_req_task_work_add(req);\n\tif (unlikely(ret)) {\n\t\tstruct task_struct *tsk;\n\n\t\t/* queue just for cancelation */\n\t\tinit_task_work(&req->task_work, io_req_task_cancel);\n\t\ttsk = io_wq_get_task(req->ctx->io_wq);\n\t\ttask_work_add(tsk, &req->task_work, TWA_NONE);\n\t\twake_up_process(tsk);\n\t}\n\treturn 1;\n}\n\n/*\n * This controls whether a given IO request should be armed for async page\n * based retry. If we return false here, the request is handed to the async\n * worker threads for retry. If we're doing buffered reads on a regular file,\n * we prepare a private wait_page_queue entry and retry the operation. This\n * will either succeed because the page is now uptodate and unlocked, or it\n * will register a callback when the page is unlocked at IO completion. Through\n * that callback, io_uring uses task_work to setup a retry of the operation.\n * That retry will attempt the buffered read again. The retry will generally\n * succeed, or in rare cases where it fails, we then fall back to using the\n * async worker threads for a blocking retry.\n */\nstatic bool io_rw_should_retry(struct io_kiocb *req)\n{\n\tstruct io_async_rw *rw = req->async_data;\n\tstruct wait_page_queue *wait = &rw->wpq;\n\tstruct kiocb *kiocb = &req->rw.kiocb;\n\n\t/* never retry for NOWAIT, we just complete with -EAGAIN */\n\tif (req->flags & REQ_F_NOWAIT)\n\t\treturn false;\n\n\t/* Only for buffered IO */\n\tif (kiocb->ki_flags & (IOCB_DIRECT | IOCB_HIPRI))\n\t\treturn false;\n\n\t/*\n\t * just use poll if we can, and don't attempt if the fs doesn't\n\t * support callback based unlocks\n\t */\n\tif (file_can_poll(req->file) || !(req->file->f_mode & FMODE_BUF_RASYNC))\n\t\treturn false;\n\n\twait->wait.func = io_async_buf_func;\n\twait->wait.private = req;\n\twait->wait.flags = 0;\n\tINIT_LIST_HEAD(&wait->wait.entry);\n\tkiocb->ki_flags |= IOCB_WAITQ;\n\tkiocb->ki_flags &= ~IOCB_NOWAIT;\n\tkiocb->ki_waitq = wait;\n\treturn true;\n}\n\nstatic int io_iter_do_read(struct io_kiocb *req, struct iov_iter *iter)\n{\n\tif (req->file->f_op->read_iter)\n\t\treturn call_read_iter(req->file, &req->rw.kiocb, iter);\n\telse if (req->file->f_op->read)\n\t\treturn loop_rw_iter(READ, req, iter);\n\telse\n\t\treturn -EINVAL;\n}\n\nstatic int io_read(struct io_kiocb *req, bool force_nonblock,\n\t\t   struct io_comp_state *cs)\n{\n\tstruct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;\n\tstruct kiocb *kiocb = &req->rw.kiocb;\n\tstruct iov_iter __iter, *iter = &__iter;\n\tstruct io_async_rw *rw = req->async_data;\n\tssize_t io_size, ret, ret2;\n\tbool no_async;\n\n\tif (rw) {\n\t\titer = &rw->iter;\n\t\tiovec = NULL;\n\t} else {\n\t\tret = io_import_iovec(READ, req, &iovec, iter, !force_nonblock);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\tio_size = iov_iter_count(iter);\n\treq->result = io_size;\n\tret = 0;\n\n\t/* Ensure we clear previously set non-block flag */\n\tif (!force_nonblock)\n\t\tkiocb->ki_flags &= ~IOCB_NOWAIT;\n\telse\n\t\tkiocb->ki_flags |= IOCB_NOWAIT;\n\n\n\t/* If the file doesn't support async, just async punt */\n\tno_async = force_nonblock && !io_file_supports_async(req->file, READ);\n\tif (no_async)\n\t\tgoto copy_iov;\n\n\tret = rw_verify_area(READ, req->file, io_kiocb_ppos(kiocb), io_size);\n\tif (unlikely(ret))\n\t\tgoto out_free;\n\n\tret = io_iter_do_read(req, iter);\n\n\tif (!ret) {\n\t\tgoto done;\n\t} else if (ret == -EIOCBQUEUED) {\n\t\tret = 0;\n\t\tgoto out_free;\n\t} else if (ret == -EAGAIN) {\n\t\t/* IOPOLL retry should happen for io-wq threads */\n\t\tif (!force_nonblock && !(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tgoto done;\n\t\t/* no retry on NONBLOCK marked file */\n\t\tif (req->file->f_flags & O_NONBLOCK)\n\t\t\tgoto done;\n\t\t/* some cases will consume bytes even on error returns */\n\t\tiov_iter_revert(iter, io_size - iov_iter_count(iter));\n\t\tret = 0;\n\t\tgoto copy_iov;\n\t} else if (ret < 0) {\n\t\t/* make sure -ERESTARTSYS -> -EINTR is done */\n\t\tgoto done;\n\t}\n\n\t/* read it all, or we did blocking attempt. no retry. */\n\tif (!iov_iter_count(iter) || !force_nonblock ||\n\t    (req->file->f_flags & O_NONBLOCK))\n\t\tgoto done;\n\n\tio_size -= ret;\ncopy_iov:\n\tret2 = io_setup_async_rw(req, iovec, inline_vecs, iter, true);\n\tif (ret2) {\n\t\tret = ret2;\n\t\tgoto out_free;\n\t}\n\tif (no_async)\n\t\treturn -EAGAIN;\n\trw = req->async_data;\n\t/* it's copied and will be cleaned with ->io */\n\tiovec = NULL;\n\t/* now use our persistent iterator, if we aren't already */\n\titer = &rw->iter;\nretry:\n\trw->bytes_done += ret;\n\t/* if we can retry, do so with the callbacks armed */\n\tif (!io_rw_should_retry(req)) {\n\t\tkiocb->ki_flags &= ~IOCB_WAITQ;\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * Now retry read with the IOCB_WAITQ parts set in the iocb. If we\n\t * get -EIOCBQUEUED, then we'll get a notification when the desired\n\t * page gets unlocked. We can also get a partial read here, and if we\n\t * do, then just retry at the new offset.\n\t */\n\tret = io_iter_do_read(req, iter);\n\tif (ret == -EIOCBQUEUED) {\n\t\tret = 0;\n\t\tgoto out_free;\n\t} else if (ret > 0 && ret < io_size) {\n\t\t/* we got some bytes, but not all. retry. */\n\t\tgoto retry;\n\t}\ndone:\n\tkiocb_done(kiocb, ret, cs);\n\tret = 0;\nout_free:\n\t/* it's reportedly faster than delegating the null check to kfree() */\n\tif (iovec)\n\t\tkfree(iovec);\n\treturn ret;\n}\n\nstatic int io_write_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tssize_t ret;\n\n\tret = io_prep_rw(req, sqe);\n\tif (ret)\n\t\treturn ret;\n\n\tif (unlikely(!(req->file->f_mode & FMODE_WRITE)))\n\t\treturn -EBADF;\n\n\t/* either don't need iovec imported or already have it */\n\tif (!req->async_data)\n\t\treturn 0;\n\treturn io_rw_prep_async(req, WRITE);\n}\n\nstatic int io_write(struct io_kiocb *req, bool force_nonblock,\n\t\t    struct io_comp_state *cs)\n{\n\tstruct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;\n\tstruct kiocb *kiocb = &req->rw.kiocb;\n\tstruct iov_iter __iter, *iter = &__iter;\n\tstruct io_async_rw *rw = req->async_data;\n\tssize_t ret, ret2, io_size;\n\n\tif (rw) {\n\t\titer = &rw->iter;\n\t\tiovec = NULL;\n\t} else {\n\t\tret = io_import_iovec(WRITE, req, &iovec, iter, !force_nonblock);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\tio_size = iov_iter_count(iter);\n\treq->result = io_size;\n\n\t/* Ensure we clear previously set non-block flag */\n\tif (!force_nonblock)\n\t\tkiocb->ki_flags &= ~IOCB_NOWAIT;\n\telse\n\t\tkiocb->ki_flags |= IOCB_NOWAIT;\n\n\t/* If the file doesn't support async, just async punt */\n\tif (force_nonblock && !io_file_supports_async(req->file, WRITE))\n\t\tgoto copy_iov;\n\n\t/* file path doesn't support NOWAIT for non-direct_IO */\n\tif (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT) &&\n\t    (req->flags & REQ_F_ISREG))\n\t\tgoto copy_iov;\n\n\tret = rw_verify_area(WRITE, req->file, io_kiocb_ppos(kiocb), io_size);\n\tif (unlikely(ret))\n\t\tgoto out_free;\n\n\t/*\n\t * Open-code file_start_write here to grab freeze protection,\n\t * which will be released by another thread in\n\t * io_complete_rw().  Fool lockdep by telling it the lock got\n\t * released so that it doesn't complain about the held lock when\n\t * we return to userspace.\n\t */\n\tif (req->flags & REQ_F_ISREG) {\n\t\tsb_start_write(file_inode(req->file)->i_sb);\n\t\t__sb_writers_release(file_inode(req->file)->i_sb,\n\t\t\t\t\tSB_FREEZE_WRITE);\n\t}\n\tkiocb->ki_flags |= IOCB_WRITE;\n\n\tif (req->file->f_op->write_iter)\n\t\tret2 = call_write_iter(req->file, kiocb, iter);\n\telse if (req->file->f_op->write)\n\t\tret2 = loop_rw_iter(WRITE, req, iter);\n\telse\n\t\tret2 = -EINVAL;\n\n\t/*\n\t * Raw bdev writes will return -EOPNOTSUPP for IOCB_NOWAIT. Just\n\t * retry them without IOCB_NOWAIT.\n\t */\n\tif (ret2 == -EOPNOTSUPP && (kiocb->ki_flags & IOCB_NOWAIT))\n\t\tret2 = -EAGAIN;\n\t/* no retry on NONBLOCK marked file */\n\tif (ret2 == -EAGAIN && (req->file->f_flags & O_NONBLOCK))\n\t\tgoto done;\n\tif (!force_nonblock || ret2 != -EAGAIN) {\n\t\t/* IOPOLL retry should happen for io-wq threads */\n\t\tif ((req->ctx->flags & IORING_SETUP_IOPOLL) && ret2 == -EAGAIN)\n\t\t\tgoto copy_iov;\ndone:\n\t\tkiocb_done(kiocb, ret2, cs);\n\t} else {\ncopy_iov:\n\t\t/* some cases will consume bytes even on error returns */\n\t\tiov_iter_revert(iter, io_size - iov_iter_count(iter));\n\t\tret = io_setup_async_rw(req, iovec, inline_vecs, iter, false);\n\t\tif (!ret)\n\t\t\treturn -EAGAIN;\n\t}\nout_free:\n\t/* it's reportedly faster than delegating the null check to kfree() */\n\tif (iovec)\n\t\tkfree(iovec);\n\treturn ret;\n}\n\nstatic int io_renameat_prep(struct io_kiocb *req,\n\t\t\t    const struct io_uring_sqe *sqe)\n{\n\tstruct io_rename *ren = &req->rename;\n\tconst char __user *oldf, *newf;\n\n\tif (unlikely(req->flags & REQ_F_FIXED_FILE))\n\t\treturn -EBADF;\n\n\tren->old_dfd = READ_ONCE(sqe->fd);\n\toldf = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\tnewf = u64_to_user_ptr(READ_ONCE(sqe->addr2));\n\tren->new_dfd = READ_ONCE(sqe->len);\n\tren->flags = READ_ONCE(sqe->rename_flags);\n\n\tren->oldpath = getname(oldf);\n\tif (IS_ERR(ren->oldpath))\n\t\treturn PTR_ERR(ren->oldpath);\n\n\tren->newpath = getname(newf);\n\tif (IS_ERR(ren->newpath)) {\n\t\tputname(ren->oldpath);\n\t\treturn PTR_ERR(ren->newpath);\n\t}\n\n\tren->ignore_nonblock = false;\n\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn 0;\n}\n\nstatic int io_renameat(struct io_kiocb *req, bool force_nonblock)\n{\n\tstruct io_rename *ren = &req->rename;\n\tint ret;\n\n\tif (force_nonblock && !ren->ignore_nonblock)\n\t\treturn -EAGAIN;\n\n\tren->oldpath->refcnt++;\n\tren->newpath->refcnt++;\n\n\tret = do_renameat2(ren->old_dfd, ren->oldpath, ren->new_dfd,\n\t\t\t\tren->newpath, ren->flags);\n\n\t/* see io_openat2() comment */\n\tif (ret == -EOPNOTSUPP && io_wq_current_is_worker()) {\n\t\tren->ignore_nonblock = true;\n\t\trefcount_inc(&req->refs);\n\t\tio_req_task_queue(req);\n\t\treturn 0;\n\t}\n\n\tputname(ren->oldpath);\n\tputname(ren->newpath);\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_unlinkat_prep(struct io_kiocb *req,\n\t\t\t    const struct io_uring_sqe *sqe)\n{\n\tstruct io_unlink *un = &req->unlink;\n\tconst char __user *fname;\n\n\tif (unlikely(req->flags & REQ_F_FIXED_FILE))\n\t\treturn -EBADF;\n\n\tun->dfd = READ_ONCE(sqe->fd);\n\n\tun->flags = READ_ONCE(sqe->unlink_flags);\n\tif (un->flags & ~AT_REMOVEDIR)\n\t\treturn -EINVAL;\n\n\tfname = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\tun->filename = getname(fname);\n\tif (IS_ERR(un->filename))\n\t\treturn PTR_ERR(un->filename);\n\n\tun->ignore_nonblock = false;\n\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn 0;\n}\n\nstatic int io_unlinkat(struct io_kiocb *req, bool force_nonblock)\n{\n\tstruct io_unlink *un = &req->unlink;\n\tint ret;\n\n\tif (force_nonblock && !un->ignore_nonblock)\n\t\treturn -EAGAIN;\n\n\tun->filename->refcnt++;\n\tif (un->flags & AT_REMOVEDIR)\n\t\tret = do_rmdir(un->dfd, un->filename);\n\telse\n\t\tret = do_unlinkat(un->dfd, un->filename);\n\n\t/* see io_openat2() comment */\n\tif (ret == -EOPNOTSUPP && io_wq_current_is_worker()) {\n\t\tun->ignore_nonblock = true;\n\t\trefcount_inc(&req->refs);\n\t\tio_req_task_queue(req);\n\t\treturn 0;\n\t}\n\n\tputname(un->filename);\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_shutdown_prep(struct io_kiocb *req,\n\t\t\t    const struct io_uring_sqe *sqe)\n{\n#if defined(CONFIG_NET)\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->off || sqe->addr || sqe->rw_flags ||\n\t    sqe->buf_index)\n\t\treturn -EINVAL;\n\n\treq->shutdown.how = READ_ONCE(sqe->len);\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int io_shutdown(struct io_kiocb *req, bool force_nonblock)\n{\n#if defined(CONFIG_NET)\n\tstruct socket *sock;\n\tint ret;\n\n\tif (force_nonblock)\n\t\treturn -EAGAIN;\n\n\tsock = sock_from_file(req->file, &ret);\n\tif (unlikely(!sock))\n\t\treturn ret;\n\n\tret = __sys_shutdown_sock(sock, req->shutdown.how);\n\tio_req_complete(req, ret);\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int __io_splice_prep(struct io_kiocb *req,\n\t\t\t    const struct io_uring_sqe *sqe)\n{\n\tstruct io_splice* sp = &req->splice;\n\tunsigned int valid_flags = SPLICE_F_FD_IN_FIXED | SPLICE_F_ALL;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\tsp->file_in = NULL;\n\tsp->len = READ_ONCE(sqe->len);\n\tsp->flags = READ_ONCE(sqe->splice_flags);\n\n\tif (unlikely(sp->flags & ~valid_flags))\n\t\treturn -EINVAL;\n\n\tsp->file_in = io_file_get(NULL, req, READ_ONCE(sqe->splice_fd_in),\n\t\t\t\t  (sp->flags & SPLICE_F_FD_IN_FIXED));\n\tif (!sp->file_in)\n\t\treturn -EBADF;\n\treq->flags |= REQ_F_NEED_CLEANUP;\n\n\tif (!S_ISREG(file_inode(sp->file_in)->i_mode)) {\n\t\t/*\n\t\t * Splice operation will be punted aync, and here need to\n\t\t * modify io_wq_work.flags, so initialize io_wq_work firstly.\n\t\t */\n\t\tio_req_init_async(req);\n\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n\n\treturn 0;\n}\n\nstatic int io_tee_prep(struct io_kiocb *req,\n\t\t       const struct io_uring_sqe *sqe)\n{\n\tif (READ_ONCE(sqe->splice_off_in) || READ_ONCE(sqe->off))\n\t\treturn -EINVAL;\n\treturn __io_splice_prep(req, sqe);\n}\n\nstatic int io_tee(struct io_kiocb *req, bool force_nonblock)\n{\n\tstruct io_splice *sp = &req->splice;\n\tstruct file *in = sp->file_in;\n\tstruct file *out = sp->file_out;\n\tunsigned int flags = sp->flags & ~SPLICE_F_FD_IN_FIXED;\n\tlong ret = 0;\n\n\tif (force_nonblock)\n\t\treturn -EAGAIN;\n\tif (sp->len)\n\t\tret = do_tee(in, out, sp->len, flags);\n\n\tio_put_file(req, in, (sp->flags & SPLICE_F_FD_IN_FIXED));\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\n\tif (ret != sp->len)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_splice_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_splice* sp = &req->splice;\n\n\tsp->off_in = READ_ONCE(sqe->splice_off_in);\n\tsp->off_out = READ_ONCE(sqe->off);\n\treturn __io_splice_prep(req, sqe);\n}\n\nstatic int io_splice(struct io_kiocb *req, bool force_nonblock)\n{\n\tstruct io_splice *sp = &req->splice;\n\tstruct file *in = sp->file_in;\n\tstruct file *out = sp->file_out;\n\tunsigned int flags = sp->flags & ~SPLICE_F_FD_IN_FIXED;\n\tloff_t *poff_in, *poff_out;\n\tlong ret = 0;\n\n\tif (force_nonblock)\n\t\treturn -EAGAIN;\n\n\tpoff_in = (sp->off_in == -1) ? NULL : &sp->off_in;\n\tpoff_out = (sp->off_out == -1) ? NULL : &sp->off_out;\n\n\tif (sp->len)\n\t\tret = do_splice(in, poff_in, out, poff_out, sp->len, flags);\n\n\tio_put_file(req, in, (sp->flags & SPLICE_F_FD_IN_FIXED));\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\n\tif (ret != sp->len)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\n/*\n * IORING_OP_NOP just posts a completion event, nothing else.\n */\nstatic int io_nop(struct io_kiocb *req, struct io_comp_state *cs)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (unlikely(ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\t__io_req_complete(req, 0, 0, cs);\n\treturn 0;\n}\n\nstatic int io_prep_fsync(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!req->file)\n\t\treturn -EBADF;\n\n\tif (unlikely(ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (unlikely(sqe->addr || sqe->ioprio || sqe->buf_index))\n\t\treturn -EINVAL;\n\n\treq->sync.flags = READ_ONCE(sqe->fsync_flags);\n\tif (unlikely(req->sync.flags & ~IORING_FSYNC_DATASYNC))\n\t\treturn -EINVAL;\n\n\treq->sync.off = READ_ONCE(sqe->off);\n\treq->sync.len = READ_ONCE(sqe->len);\n\treturn 0;\n}\n\nstatic int io_fsync(struct io_kiocb *req, bool force_nonblock)\n{\n\tloff_t end = req->sync.off + req->sync.len;\n\tint ret;\n\n\t/* fsync always requires a blocking context */\n\tif (force_nonblock)\n\t\treturn -EAGAIN;\n\n\tret = vfs_fsync_range(req->file, req->sync.off,\n\t\t\t\tend > 0 ? end : LLONG_MAX,\n\t\t\t\treq->sync.flags & IORING_FSYNC_DATASYNC);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_fallocate_prep(struct io_kiocb *req,\n\t\t\t     const struct io_uring_sqe *sqe)\n{\n\tif (sqe->ioprio || sqe->buf_index || sqe->rw_flags)\n\t\treturn -EINVAL;\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\treq->sync.off = READ_ONCE(sqe->off);\n\treq->sync.len = READ_ONCE(sqe->addr);\n\treq->sync.mode = READ_ONCE(sqe->len);\n\treturn 0;\n}\n\nstatic int io_fallocate(struct io_kiocb *req, bool force_nonblock)\n{\n\tint ret;\n\n\t/* fallocate always requiring blocking context */\n\tif (force_nonblock)\n\t\treturn -EAGAIN;\n\tret = vfs_fallocate(req->file, req->sync.mode, req->sync.off,\n\t\t\t\treq->sync.len);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int __io_openat_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tconst char __user *fname;\n\tint ret;\n\n\tif (unlikely(sqe->ioprio || sqe->buf_index))\n\t\treturn -EINVAL;\n\tif (unlikely(req->flags & REQ_F_FIXED_FILE))\n\t\treturn -EBADF;\n\n\t/* open.how should be already initialised */\n\tif (!(req->open.how.flags & O_PATH) && force_o_largefile())\n\t\treq->open.how.flags |= O_LARGEFILE;\n\n\treq->open.dfd = READ_ONCE(sqe->fd);\n\tfname = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\treq->open.filename = getname(fname);\n\tif (IS_ERR(req->open.filename)) {\n\t\tret = PTR_ERR(req->open.filename);\n\t\treq->open.filename = NULL;\n\t\treturn ret;\n\t}\n\treq->open.nofile = rlimit(RLIMIT_NOFILE);\n\treq->open.ignore_nonblock = false;\n\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn 0;\n}\n\nstatic int io_openat_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tu64 flags, mode;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tmode = READ_ONCE(sqe->len);\n\tflags = READ_ONCE(sqe->open_flags);\n\treq->open.how = build_open_how(flags, mode);\n\treturn __io_openat_prep(req, sqe);\n}\n\nstatic int io_openat2_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct open_how __user *how;\n\tsize_t len;\n\tint ret;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\thow = u64_to_user_ptr(READ_ONCE(sqe->addr2));\n\tlen = READ_ONCE(sqe->len);\n\tif (len < OPEN_HOW_SIZE_VER0)\n\t\treturn -EINVAL;\n\n\tret = copy_struct_from_user(&req->open.how, sizeof(req->open.how), how,\n\t\t\t\t\tlen);\n\tif (ret)\n\t\treturn ret;\n\n\treturn __io_openat_prep(req, sqe);\n}\n\nstatic int io_openat2(struct io_kiocb *req, bool force_nonblock)\n{\n\tstruct open_flags op;\n\tstruct file *file;\n\tint ret;\n\n\tif (force_nonblock && !req->open.ignore_nonblock)\n\t\treturn -EAGAIN;\n\n\tret = build_open_flags(&req->open.how, &op);\n\tif (ret)\n\t\tgoto err;\n\n\tret = __get_unused_fd_flags(req->open.how.flags, req->open.nofile);\n\tif (ret < 0)\n\t\tgoto err;\n\n\tfile = do_filp_open(req->open.dfd, req->open.filename, &op);\n\tif (IS_ERR(file)) {\n\t\tput_unused_fd(ret);\n\t\tret = PTR_ERR(file);\n\t\t/*\n\t\t * A work-around to ensure that /proc/self works that way\n\t\t * that it should - if we get -EOPNOTSUPP back, then assume\n\t\t * that proc_self_get_link() failed us because we're in async\n\t\t * context. We should be safe to retry this from the task\n\t\t * itself with force_nonblock == false set, as it should not\n\t\t * block on lookup. Would be nice to know this upfront and\n\t\t * avoid the async dance, but doesn't seem feasible.\n\t\t */\n\t\tif (ret == -EOPNOTSUPP && io_wq_current_is_worker()) {\n\t\t\treq->open.ignore_nonblock = true;\n\t\t\trefcount_inc(&req->refs);\n\t\t\tio_req_task_queue(req);\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\tfsnotify_open(file);\n\t\tfd_install(ret, file);\n\t}\nerr:\n\tputname(req->open.filename);\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_openat(struct io_kiocb *req, bool force_nonblock)\n{\n\treturn io_openat2(req, force_nonblock);\n}\n\nstatic int io_remove_buffers_prep(struct io_kiocb *req,\n\t\t\t\t  const struct io_uring_sqe *sqe)\n{\n\tstruct io_provide_buf *p = &req->pbuf;\n\tu64 tmp;\n\n\tif (sqe->ioprio || sqe->rw_flags || sqe->addr || sqe->len || sqe->off)\n\t\treturn -EINVAL;\n\n\ttmp = READ_ONCE(sqe->fd);\n\tif (!tmp || tmp > USHRT_MAX)\n\t\treturn -EINVAL;\n\n\tmemset(p, 0, sizeof(*p));\n\tp->nbufs = tmp;\n\tp->bgid = READ_ONCE(sqe->buf_group);\n\treturn 0;\n}\n\nstatic int __io_remove_buffers(struct io_ring_ctx *ctx, struct io_buffer *buf,\n\t\t\t       int bgid, unsigned nbufs)\n{\n\tunsigned i = 0;\n\n\t/* shouldn't happen */\n\tif (!nbufs)\n\t\treturn 0;\n\n\t/* the head kbuf is the list itself */\n\twhile (!list_empty(&buf->list)) {\n\t\tstruct io_buffer *nxt;\n\n\t\tnxt = list_first_entry(&buf->list, struct io_buffer, list);\n\t\tlist_del(&nxt->list);\n\t\tkfree(nxt);\n\t\tif (++i == nbufs)\n\t\t\treturn i;\n\t}\n\ti++;\n\tkfree(buf);\n\tidr_remove(&ctx->io_buffer_idr, bgid);\n\n\treturn i;\n}\n\nstatic int io_remove_buffers(struct io_kiocb *req, bool force_nonblock,\n\t\t\t     struct io_comp_state *cs)\n{\n\tstruct io_provide_buf *p = &req->pbuf;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_buffer *head;\n\tint ret = 0;\n\n\tio_ring_submit_lock(ctx, !force_nonblock);\n\n\tlockdep_assert_held(&ctx->uring_lock);\n\n\tret = -ENOENT;\n\thead = idr_find(&ctx->io_buffer_idr, p->bgid);\n\tif (head)\n\t\tret = __io_remove_buffers(ctx, head, p->bgid, p->nbufs);\n\n\tio_ring_submit_lock(ctx, !force_nonblock);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, ret, 0, cs);\n\treturn 0;\n}\n\nstatic int io_provide_buffers_prep(struct io_kiocb *req,\n\t\t\t\t   const struct io_uring_sqe *sqe)\n{\n\tstruct io_provide_buf *p = &req->pbuf;\n\tu64 tmp;\n\n\tif (sqe->ioprio || sqe->rw_flags)\n\t\treturn -EINVAL;\n\n\ttmp = READ_ONCE(sqe->fd);\n\tif (!tmp || tmp > USHRT_MAX)\n\t\treturn -E2BIG;\n\tp->nbufs = tmp;\n\tp->addr = READ_ONCE(sqe->addr);\n\tp->len = READ_ONCE(sqe->len);\n\n\tif (!access_ok(u64_to_user_ptr(p->addr), (p->len * p->nbufs)))\n\t\treturn -EFAULT;\n\n\tp->bgid = READ_ONCE(sqe->buf_group);\n\ttmp = READ_ONCE(sqe->off);\n\tif (tmp > USHRT_MAX)\n\t\treturn -E2BIG;\n\tp->bid = tmp;\n\treturn 0;\n}\n\nstatic int io_add_buffers(struct io_provide_buf *pbuf, struct io_buffer **head)\n{\n\tstruct io_buffer *buf;\n\tu64 addr = pbuf->addr;\n\tint i, bid = pbuf->bid;\n\n\tfor (i = 0; i < pbuf->nbufs; i++) {\n\t\tbuf = kmalloc(sizeof(*buf), GFP_KERNEL);\n\t\tif (!buf)\n\t\t\tbreak;\n\n\t\tbuf->addr = addr;\n\t\tbuf->len = pbuf->len;\n\t\tbuf->bid = bid;\n\t\taddr += pbuf->len;\n\t\tbid++;\n\t\tif (!*head) {\n\t\t\tINIT_LIST_HEAD(&buf->list);\n\t\t\t*head = buf;\n\t\t} else {\n\t\t\tlist_add_tail(&buf->list, &(*head)->list);\n\t\t}\n\t}\n\n\treturn i ? i : -ENOMEM;\n}\n\nstatic int io_provide_buffers(struct io_kiocb *req, bool force_nonblock,\n\t\t\t      struct io_comp_state *cs)\n{\n\tstruct io_provide_buf *p = &req->pbuf;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_buffer *head, *list;\n\tint ret = 0;\n\n\tio_ring_submit_lock(ctx, !force_nonblock);\n\n\tlockdep_assert_held(&ctx->uring_lock);\n\n\tlist = head = idr_find(&ctx->io_buffer_idr, p->bgid);\n\n\tret = io_add_buffers(p, &head);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (!list) {\n\t\tret = idr_alloc(&ctx->io_buffer_idr, head, p->bgid, p->bgid + 1,\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (ret < 0) {\n\t\t\t__io_remove_buffers(ctx, head, p->bgid, -1U);\n\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tio_ring_submit_unlock(ctx, !force_nonblock);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, ret, 0, cs);\n\treturn 0;\n}\n\nstatic int io_epoll_ctl_prep(struct io_kiocb *req,\n\t\t\t     const struct io_uring_sqe *sqe)\n{\n#if defined(CONFIG_EPOLL)\n\tif (sqe->ioprio || sqe->buf_index)\n\t\treturn -EINVAL;\n\tif (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL)))\n\t\treturn -EINVAL;\n\n\treq->epoll.epfd = READ_ONCE(sqe->fd);\n\treq->epoll.op = READ_ONCE(sqe->len);\n\treq->epoll.fd = READ_ONCE(sqe->off);\n\n\tif (ep_op_has_event(req->epoll.op)) {\n\t\tstruct epoll_event __user *ev;\n\n\t\tev = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\t\tif (copy_from_user(&req->epoll.event, ev, sizeof(*ev)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int io_epoll_ctl(struct io_kiocb *req, bool force_nonblock,\n\t\t\tstruct io_comp_state *cs)\n{\n#if defined(CONFIG_EPOLL)\n\tstruct io_epoll *ie = &req->epoll;\n\tint ret;\n\n\tret = do_epoll_ctl(ie->epfd, ie->op, ie->fd, &ie->event, force_nonblock);\n\tif (force_nonblock && ret == -EAGAIN)\n\t\treturn -EAGAIN;\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, ret, 0, cs);\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int io_madvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n#if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)\n\tif (sqe->ioprio || sqe->buf_index || sqe->off)\n\t\treturn -EINVAL;\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\treq->madvise.addr = READ_ONCE(sqe->addr);\n\treq->madvise.len = READ_ONCE(sqe->len);\n\treq->madvise.advice = READ_ONCE(sqe->fadvise_advice);\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int io_madvise(struct io_kiocb *req, bool force_nonblock)\n{\n#if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)\n\tstruct io_madvise *ma = &req->madvise;\n\tint ret;\n\n\tif (force_nonblock)\n\t\treturn -EAGAIN;\n\n\tret = do_madvise(current->mm, ma->addr, ma->len, ma->advice);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int io_fadvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tif (sqe->ioprio || sqe->buf_index || sqe->addr)\n\t\treturn -EINVAL;\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\treq->fadvise.offset = READ_ONCE(sqe->off);\n\treq->fadvise.len = READ_ONCE(sqe->len);\n\treq->fadvise.advice = READ_ONCE(sqe->fadvise_advice);\n\treturn 0;\n}\n\nstatic int io_fadvise(struct io_kiocb *req, bool force_nonblock)\n{\n\tstruct io_fadvise *fa = &req->fadvise;\n\tint ret;\n\n\tif (force_nonblock) {\n\t\tswitch (fa->advice) {\n\t\tcase POSIX_FADV_NORMAL:\n\t\tcase POSIX_FADV_RANDOM:\n\t\tcase POSIX_FADV_SEQUENTIAL:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\n\tret = vfs_fadvise(req->file, fa->offset, fa->len, fa->advice);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_statx_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tif (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL)))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->buf_index)\n\t\treturn -EINVAL;\n\tif (req->flags & REQ_F_FIXED_FILE)\n\t\treturn -EBADF;\n\n\treq->statx.dfd = READ_ONCE(sqe->fd);\n\treq->statx.mask = READ_ONCE(sqe->len);\n\treq->statx.filename = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\treq->statx.buffer = u64_to_user_ptr(READ_ONCE(sqe->addr2));\n\treq->statx.flags = READ_ONCE(sqe->statx_flags);\n\n\treturn 0;\n}\n\nstatic int io_statx(struct io_kiocb *req, bool force_nonblock)\n{\n\tstruct io_statx *ctx = &req->statx;\n\tint ret;\n\n\tif (force_nonblock) {\n\t\t/* only need file table for an actual valid fd */\n\t\tif (ctx->dfd == -1 || ctx->dfd == AT_FDCWD)\n\t\t\treq->flags |= REQ_F_NO_FILE_TABLE;\n\t\treturn -EAGAIN;\n\t}\n\n\tret = do_statx(ctx->dfd, ctx->filename, ctx->flags, ctx->mask,\n\t\t       ctx->buffer);\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_close_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\t/*\n\t * If we queue this for async, it must not be cancellable. That would\n\t * leave the 'file' in an undeterminate state, and here need to modify\n\t * io_wq_work.flags, so initialize io_wq_work firstly.\n\t */\n\tio_req_init_async(req);\n\treq->work.flags |= IO_WQ_WORK_NO_CANCEL;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||\n\t    sqe->rw_flags || sqe->buf_index)\n\t\treturn -EINVAL;\n\tif (req->flags & REQ_F_FIXED_FILE)\n\t\treturn -EBADF;\n\n\treq->close.fd = READ_ONCE(sqe->fd);\n\tif ((req->file && req->file->f_op == &io_uring_fops))\n\t\treturn -EBADF;\n\n\treq->close.put_file = NULL;\n\treturn 0;\n}\n\nstatic int io_close(struct io_kiocb *req, bool force_nonblock,\n\t\t    struct io_comp_state *cs)\n{\n\tstruct io_close *close = &req->close;\n\tint ret;\n\n\t/* might be already done during nonblock submission */\n\tif (!close->put_file) {\n\t\tret = __close_fd_get_file(close->fd, &close->put_file);\n\t\tif (ret < 0)\n\t\t\treturn (ret == -ENOENT) ? -EBADF : ret;\n\t}\n\n\t/* if the file has a flush method, be safe and punt to async */\n\tif (close->put_file->f_op->flush && force_nonblock) {\n\t\t/* was never set, but play safe */\n\t\treq->flags &= ~REQ_F_NOWAIT;\n\t\t/* avoid grabbing files - we don't need the files */\n\t\treq->flags |= REQ_F_NO_FILE_TABLE;\n\t\treturn -EAGAIN;\n\t}\n\n\t/* No ->flush() or already async, safely close from here */\n\tret = filp_close(close->put_file, req->work.identity->files);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tfput(close->put_file);\n\tclose->put_file = NULL;\n\t__io_req_complete(req, ret, 0, cs);\n\treturn 0;\n}\n\nstatic int io_prep_sfr(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!req->file)\n\t\treturn -EBADF;\n\n\tif (unlikely(ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (unlikely(sqe->addr || sqe->ioprio || sqe->buf_index))\n\t\treturn -EINVAL;\n\n\treq->sync.off = READ_ONCE(sqe->off);\n\treq->sync.len = READ_ONCE(sqe->len);\n\treq->sync.flags = READ_ONCE(sqe->sync_range_flags);\n\treturn 0;\n}\n\nstatic int io_sync_file_range(struct io_kiocb *req, bool force_nonblock)\n{\n\tint ret;\n\n\t/* sync_file_range always requires a blocking context */\n\tif (force_nonblock)\n\t\treturn -EAGAIN;\n\n\tret = sync_file_range(req->file, req->sync.off, req->sync.len,\n\t\t\t\treq->sync.flags);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\n#if defined(CONFIG_NET)\nstatic int io_setup_async_msg(struct io_kiocb *req,\n\t\t\t      struct io_async_msghdr *kmsg)\n{\n\tstruct io_async_msghdr *async_msg = req->async_data;\n\n\tif (async_msg)\n\t\treturn -EAGAIN;\n\tif (io_alloc_async_data(req)) {\n\t\tif (kmsg->iov != kmsg->fast_iov)\n\t\t\tkfree(kmsg->iov);\n\t\treturn -ENOMEM;\n\t}\n\tasync_msg = req->async_data;\n\treq->flags |= REQ_F_NEED_CLEANUP;\n\tmemcpy(async_msg, kmsg, sizeof(*kmsg));\n\treturn -EAGAIN;\n}\n\nstatic int io_sendmsg_copy_hdr(struct io_kiocb *req,\n\t\t\t       struct io_async_msghdr *iomsg)\n{\n\tiomsg->iov = iomsg->fast_iov;\n\tiomsg->msg.msg_name = &iomsg->addr;\n\treturn sendmsg_copy_msghdr(&iomsg->msg, req->sr_msg.umsg,\n\t\t\t\t   req->sr_msg.msg_flags, &iomsg->iov);\n}\n\nstatic int io_sendmsg_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_async_msghdr *async_msg = req->async_data;\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tint ret;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\tsr->msg_flags = READ_ONCE(sqe->msg_flags);\n\tsr->umsg = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\tsr->len = READ_ONCE(sqe->len);\n\n#ifdef CONFIG_COMPAT\n\tif (req->ctx->compat)\n\t\tsr->msg_flags |= MSG_CMSG_COMPAT;\n#endif\n\n\tif (!async_msg || !io_op_defs[req->opcode].needs_async_data)\n\t\treturn 0;\n\tret = io_sendmsg_copy_hdr(req, async_msg);\n\tif (!ret)\n\t\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn ret;\n}\n\nstatic int io_sendmsg(struct io_kiocb *req, bool force_nonblock,\n\t\t      struct io_comp_state *cs)\n{\n\tstruct io_async_msghdr iomsg, *kmsg;\n\tstruct socket *sock;\n\tunsigned flags;\n\tint ret;\n\n\tsock = sock_from_file(req->file, &ret);\n\tif (unlikely(!sock))\n\t\treturn ret;\n\n\tif (req->async_data) {\n\t\tkmsg = req->async_data;\n\t\tkmsg->msg.msg_name = &kmsg->addr;\n\t\t/* if iov is set, it's allocated already */\n\t\tif (!kmsg->iov)\n\t\t\tkmsg->iov = kmsg->fast_iov;\n\t\tkmsg->msg.msg_iter.iov = kmsg->iov;\n\t} else {\n\t\tret = io_sendmsg_copy_hdr(req, &iomsg);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tkmsg = &iomsg;\n\t}\n\n\tflags = req->sr_msg.msg_flags;\n\tif (flags & MSG_DONTWAIT)\n\t\treq->flags |= REQ_F_NOWAIT;\n\telse if (force_nonblock)\n\t\tflags |= MSG_DONTWAIT;\n\n\tret = __sys_sendmsg_sock(sock, &kmsg->msg, flags);\n\tif (force_nonblock && ret == -EAGAIN)\n\t\treturn io_setup_async_msg(req, kmsg);\n\tif (ret == -ERESTARTSYS)\n\t\tret = -EINTR;\n\n\tif (kmsg->iov != kmsg->fast_iov)\n\t\tkfree(kmsg->iov);\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, ret, 0, cs);\n\treturn 0;\n}\n\nstatic int io_send(struct io_kiocb *req, bool force_nonblock,\n\t\t   struct io_comp_state *cs)\n{\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tstruct msghdr msg;\n\tstruct iovec iov;\n\tstruct socket *sock;\n\tunsigned flags;\n\tint ret;\n\n\tsock = sock_from_file(req->file, &ret);\n\tif (unlikely(!sock))\n\t\treturn ret;\n\n\tret = import_single_range(WRITE, sr->buf, sr->len, &iov, &msg.msg_iter);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tmsg.msg_name = NULL;\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\tmsg.msg_namelen = 0;\n\n\tflags = req->sr_msg.msg_flags;\n\tif (flags & MSG_DONTWAIT)\n\t\treq->flags |= REQ_F_NOWAIT;\n\telse if (force_nonblock)\n\t\tflags |= MSG_DONTWAIT;\n\n\tmsg.msg_flags = flags;\n\tret = sock_sendmsg(sock, &msg);\n\tif (force_nonblock && ret == -EAGAIN)\n\t\treturn -EAGAIN;\n\tif (ret == -ERESTARTSYS)\n\t\tret = -EINTR;\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, ret, 0, cs);\n\treturn 0;\n}\n\nstatic int __io_recvmsg_copy_hdr(struct io_kiocb *req,\n\t\t\t\t struct io_async_msghdr *iomsg)\n{\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tstruct iovec __user *uiov;\n\tsize_t iov_len;\n\tint ret;\n\n\tret = __copy_msghdr_from_user(&iomsg->msg, sr->umsg,\n\t\t\t\t\t&iomsg->uaddr, &uiov, &iov_len);\n\tif (ret)\n\t\treturn ret;\n\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tif (iov_len > 1)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(iomsg->iov, uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\t\tsr->len = iomsg->iov[0].iov_len;\n\t\tiov_iter_init(&iomsg->msg.msg_iter, READ, iomsg->iov, 1,\n\t\t\t\tsr->len);\n\t\tiomsg->iov = NULL;\n\t} else {\n\t\tret = __import_iovec(READ, uiov, iov_len, UIO_FASTIOV,\n\t\t\t\t     &iomsg->iov, &iomsg->msg.msg_iter,\n\t\t\t\t     false);\n\t\tif (ret > 0)\n\t\t\tret = 0;\n\t}\n\n\treturn ret;\n}\n\n#ifdef CONFIG_COMPAT\nstatic int __io_compat_recvmsg_copy_hdr(struct io_kiocb *req,\n\t\t\t\t\tstruct io_async_msghdr *iomsg)\n{\n\tstruct compat_msghdr __user *msg_compat;\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tstruct compat_iovec __user *uiov;\n\tcompat_uptr_t ptr;\n\tcompat_size_t len;\n\tint ret;\n\n\tmsg_compat = (struct compat_msghdr __user *) sr->umsg;\n\tret = __get_compat_msghdr(&iomsg->msg, msg_compat, &iomsg->uaddr,\n\t\t\t\t\t&ptr, &len);\n\tif (ret)\n\t\treturn ret;\n\n\tuiov = compat_ptr(ptr);\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tcompat_ssize_t clen;\n\n\t\tif (len > 1)\n\t\t\treturn -EINVAL;\n\t\tif (!access_ok(uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\t\tif (__get_user(clen, &uiov->iov_len))\n\t\t\treturn -EFAULT;\n\t\tif (clen < 0)\n\t\t\treturn -EINVAL;\n\t\tsr->len = iomsg->iov[0].iov_len;\n\t\tiomsg->iov = NULL;\n\t} else {\n\t\tret = __import_iovec(READ, (struct iovec __user *)uiov, len,\n\t\t\t\t   UIO_FASTIOV, &iomsg->iov,\n\t\t\t\t   &iomsg->msg.msg_iter, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n#endif\n\nstatic int io_recvmsg_copy_hdr(struct io_kiocb *req,\n\t\t\t       struct io_async_msghdr *iomsg)\n{\n\tiomsg->msg.msg_name = &iomsg->addr;\n\tiomsg->iov = iomsg->fast_iov;\n\n#ifdef CONFIG_COMPAT\n\tif (req->ctx->compat)\n\t\treturn __io_compat_recvmsg_copy_hdr(req, iomsg);\n#endif\n\n\treturn __io_recvmsg_copy_hdr(req, iomsg);\n}\n\nstatic struct io_buffer *io_recv_buffer_select(struct io_kiocb *req,\n\t\t\t\t\t       bool needs_lock)\n{\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tstruct io_buffer *kbuf;\n\n\tkbuf = io_buffer_select(req, &sr->len, sr->bgid, sr->kbuf, needs_lock);\n\tif (IS_ERR(kbuf))\n\t\treturn kbuf;\n\n\tsr->kbuf = kbuf;\n\treq->flags |= REQ_F_BUFFER_SELECTED;\n\treturn kbuf;\n}\n\nstatic inline unsigned int io_put_recv_kbuf(struct io_kiocb *req)\n{\n\treturn io_put_kbuf(req, req->sr_msg.kbuf);\n}\n\nstatic int io_recvmsg_prep(struct io_kiocb *req,\n\t\t\t   const struct io_uring_sqe *sqe)\n{\n\tstruct io_async_msghdr *async_msg = req->async_data;\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tint ret;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\tsr->msg_flags = READ_ONCE(sqe->msg_flags);\n\tsr->umsg = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\tsr->len = READ_ONCE(sqe->len);\n\tsr->bgid = READ_ONCE(sqe->buf_group);\n\n#ifdef CONFIG_COMPAT\n\tif (req->ctx->compat)\n\t\tsr->msg_flags |= MSG_CMSG_COMPAT;\n#endif\n\n\tif (!async_msg || !io_op_defs[req->opcode].needs_async_data)\n\t\treturn 0;\n\tret = io_recvmsg_copy_hdr(req, async_msg);\n\tif (!ret)\n\t\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn ret;\n}\n\nstatic int io_recvmsg(struct io_kiocb *req, bool force_nonblock,\n\t\t      struct io_comp_state *cs)\n{\n\tstruct io_async_msghdr iomsg, *kmsg;\n\tstruct socket *sock;\n\tstruct io_buffer *kbuf;\n\tunsigned flags;\n\tint ret, cflags = 0;\n\n\tsock = sock_from_file(req->file, &ret);\n\tif (unlikely(!sock))\n\t\treturn ret;\n\n\tif (req->async_data) {\n\t\tkmsg = req->async_data;\n\t\tkmsg->msg.msg_name = &kmsg->addr;\n\t\t/* if iov is set, it's allocated already */\n\t\tif (!kmsg->iov)\n\t\t\tkmsg->iov = kmsg->fast_iov;\n\t\tkmsg->msg.msg_iter.iov = kmsg->iov;\n\t} else {\n\t\tret = io_recvmsg_copy_hdr(req, &iomsg);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tkmsg = &iomsg;\n\t}\n\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tkbuf = io_recv_buffer_select(req, !force_nonblock);\n\t\tif (IS_ERR(kbuf))\n\t\t\treturn PTR_ERR(kbuf);\n\t\tkmsg->fast_iov[0].iov_base = u64_to_user_ptr(kbuf->addr);\n\t\tiov_iter_init(&kmsg->msg.msg_iter, READ, kmsg->iov,\n\t\t\t\t1, req->sr_msg.len);\n\t}\n\n\tflags = req->sr_msg.msg_flags;\n\tif (flags & MSG_DONTWAIT)\n\t\treq->flags |= REQ_F_NOWAIT;\n\telse if (force_nonblock)\n\t\tflags |= MSG_DONTWAIT;\n\n\tret = __sys_recvmsg_sock(sock, &kmsg->msg, req->sr_msg.umsg,\n\t\t\t\t\tkmsg->uaddr, flags);\n\tif (force_nonblock && ret == -EAGAIN)\n\t\treturn io_setup_async_msg(req, kmsg);\n\tif (ret == -ERESTARTSYS)\n\t\tret = -EINTR;\n\n\tif (req->flags & REQ_F_BUFFER_SELECTED)\n\t\tcflags = io_put_recv_kbuf(req);\n\tif (kmsg->iov != kmsg->fast_iov)\n\t\tkfree(kmsg->iov);\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, ret, cflags, cs);\n\treturn 0;\n}\n\nstatic int io_recv(struct io_kiocb *req, bool force_nonblock,\n\t\t   struct io_comp_state *cs)\n{\n\tstruct io_buffer *kbuf;\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tstruct msghdr msg;\n\tvoid __user *buf = sr->buf;\n\tstruct socket *sock;\n\tstruct iovec iov;\n\tunsigned flags;\n\tint ret, cflags = 0;\n\n\tsock = sock_from_file(req->file, &ret);\n\tif (unlikely(!sock))\n\t\treturn ret;\n\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tkbuf = io_recv_buffer_select(req, !force_nonblock);\n\t\tif (IS_ERR(kbuf))\n\t\t\treturn PTR_ERR(kbuf);\n\t\tbuf = u64_to_user_ptr(kbuf->addr);\n\t}\n\n\tret = import_single_range(READ, buf, sr->len, &iov, &msg.msg_iter);\n\tif (unlikely(ret))\n\t\tgoto out_free;\n\n\tmsg.msg_name = NULL;\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\tmsg.msg_namelen = 0;\n\tmsg.msg_iocb = NULL;\n\tmsg.msg_flags = 0;\n\n\tflags = req->sr_msg.msg_flags;\n\tif (flags & MSG_DONTWAIT)\n\t\treq->flags |= REQ_F_NOWAIT;\n\telse if (force_nonblock)\n\t\tflags |= MSG_DONTWAIT;\n\n\tret = sock_recvmsg(sock, &msg, flags);\n\tif (force_nonblock && ret == -EAGAIN)\n\t\treturn -EAGAIN;\n\tif (ret == -ERESTARTSYS)\n\t\tret = -EINTR;\nout_free:\n\tif (req->flags & REQ_F_BUFFER_SELECTED)\n\t\tcflags = io_put_recv_kbuf(req);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, ret, cflags, cs);\n\treturn 0;\n}\n\nstatic int io_accept_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_accept *accept = &req->accept;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->len || sqe->buf_index)\n\t\treturn -EINVAL;\n\n\taccept->addr = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\taccept->addr_len = u64_to_user_ptr(READ_ONCE(sqe->addr2));\n\taccept->flags = READ_ONCE(sqe->accept_flags);\n\taccept->nofile = rlimit(RLIMIT_NOFILE);\n\treturn 0;\n}\n\nstatic int io_accept(struct io_kiocb *req, bool force_nonblock,\n\t\t     struct io_comp_state *cs)\n{\n\tstruct io_accept *accept = &req->accept;\n\tunsigned int file_flags = force_nonblock ? O_NONBLOCK : 0;\n\tint ret;\n\n\tif (req->file->f_flags & O_NONBLOCK)\n\t\treq->flags |= REQ_F_NOWAIT;\n\n\tret = __sys_accept4_file(req->file, file_flags, accept->addr,\n\t\t\t\t\taccept->addr_len, accept->flags,\n\t\t\t\t\taccept->nofile);\n\tif (ret == -EAGAIN && force_nonblock)\n\t\treturn -EAGAIN;\n\tif (ret < 0) {\n\t\tif (ret == -ERESTARTSYS)\n\t\t\tret = -EINTR;\n\t\treq_set_fail_links(req);\n\t}\n\t__io_req_complete(req, ret, 0, cs);\n\treturn 0;\n}\n\nstatic int io_connect_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_connect *conn = &req->connect;\n\tstruct io_async_connect *io = req->async_data;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->len || sqe->buf_index || sqe->rw_flags)\n\t\treturn -EINVAL;\n\n\tconn->addr = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\tconn->addr_len =  READ_ONCE(sqe->addr2);\n\n\tif (!io)\n\t\treturn 0;\n\n\treturn move_addr_to_kernel(conn->addr, conn->addr_len,\n\t\t\t\t\t&io->address);\n}\n\nstatic int io_connect(struct io_kiocb *req, bool force_nonblock,\n\t\t      struct io_comp_state *cs)\n{\n\tstruct io_async_connect __io, *io;\n\tunsigned file_flags;\n\tint ret;\n\n\tif (req->async_data) {\n\t\tio = req->async_data;\n\t} else {\n\t\tret = move_addr_to_kernel(req->connect.addr,\n\t\t\t\t\t\treq->connect.addr_len,\n\t\t\t\t\t\t&__io.address);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tio = &__io;\n\t}\n\n\tfile_flags = force_nonblock ? O_NONBLOCK : 0;\n\n\tret = __sys_connect_file(req->file, &io->address,\n\t\t\t\t\treq->connect.addr_len, file_flags);\n\tif ((ret == -EAGAIN || ret == -EINPROGRESS) && force_nonblock) {\n\t\tif (req->async_data)\n\t\t\treturn -EAGAIN;\n\t\tif (io_alloc_async_data(req)) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tio = req->async_data;\n\t\tmemcpy(req->async_data, &__io, sizeof(__io));\n\t\treturn -EAGAIN;\n\t}\n\tif (ret == -ERESTARTSYS)\n\t\tret = -EINTR;\nout:\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, ret, 0, cs);\n\treturn 0;\n}\n#else /* !CONFIG_NET */\nstatic int io_sendmsg_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic int io_sendmsg(struct io_kiocb *req, bool force_nonblock,\n\t\t      struct io_comp_state *cs)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic int io_send(struct io_kiocb *req, bool force_nonblock,\n\t\t   struct io_comp_state *cs)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic int io_recvmsg_prep(struct io_kiocb *req,\n\t\t\t   const struct io_uring_sqe *sqe)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic int io_recvmsg(struct io_kiocb *req, bool force_nonblock,\n\t\t      struct io_comp_state *cs)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic int io_recv(struct io_kiocb *req, bool force_nonblock,\n\t\t   struct io_comp_state *cs)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic int io_accept_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic int io_accept(struct io_kiocb *req, bool force_nonblock,\n\t\t     struct io_comp_state *cs)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic int io_connect_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic int io_connect(struct io_kiocb *req, bool force_nonblock,\n\t\t      struct io_comp_state *cs)\n{\n\treturn -EOPNOTSUPP;\n}\n#endif /* CONFIG_NET */\n\nstruct io_poll_table {\n\tstruct poll_table_struct pt;\n\tstruct io_kiocb *req;\n\tint error;\n};\n\nstatic int __io_async_wake(struct io_kiocb *req, struct io_poll_iocb *poll,\n\t\t\t   __poll_t mask, task_work_func_t func)\n{\n\tint ret;\n\n\t/* for instances that support it check for an event match first: */\n\tif (mask && !(mask & poll->events))\n\t\treturn 0;\n\n\ttrace_io_uring_task_add(req->ctx, req->opcode, req->user_data, mask);\n\n\tlist_del_init(&poll->wait.entry);\n\n\treq->result = mask;\n\tinit_task_work(&req->task_work, func);\n\tpercpu_ref_get(&req->ctx->refs);\n\n\t/*\n\t * If this fails, then the task is exiting. When a task exits, the\n\t * work gets canceled, so just cancel this request as well instead\n\t * of executing it. We can't safely execute it anyway, as we may not\n\t * have the needed state needed for it anyway.\n\t */\n\tret = io_req_task_work_add(req);\n\tif (unlikely(ret)) {\n\t\tstruct task_struct *tsk;\n\n\t\tWRITE_ONCE(poll->canceled, true);\n\t\ttsk = io_wq_get_task(req->ctx->io_wq);\n\t\ttask_work_add(tsk, &req->task_work, TWA_NONE);\n\t\twake_up_process(tsk);\n\t}\n\treturn 1;\n}\n\nstatic bool io_poll_rewait(struct io_kiocb *req, struct io_poll_iocb *poll)\n\t__acquires(&req->ctx->completion_lock)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!req->result && !READ_ONCE(poll->canceled)) {\n\t\tstruct poll_table_struct pt = { ._key = poll->events };\n\n\t\treq->result = vfs_poll(req->file, &pt) & poll->events;\n\t}\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tif (!req->result && !READ_ONCE(poll->canceled)) {\n\t\tadd_wait_queue(poll->head, &poll->wait);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic struct io_poll_iocb *io_poll_get_double(struct io_kiocb *req)\n{\n\t/* pure poll stashes this in ->async_data, poll driven retry elsewhere */\n\tif (req->opcode == IORING_OP_POLL_ADD)\n\t\treturn req->async_data;\n\treturn req->apoll->double_poll;\n}\n\nstatic struct io_poll_iocb *io_poll_get_single(struct io_kiocb *req)\n{\n\tif (req->opcode == IORING_OP_POLL_ADD)\n\t\treturn &req->poll;\n\treturn &req->apoll->poll;\n}\n\nstatic void io_poll_remove_double(struct io_kiocb *req)\n{\n\tstruct io_poll_iocb *poll = io_poll_get_double(req);\n\n\tlockdep_assert_held(&req->ctx->completion_lock);\n\n\tif (poll && poll->head) {\n\t\tstruct wait_queue_head *head = poll->head;\n\n\t\tspin_lock(&head->lock);\n\t\tlist_del_init(&poll->wait.entry);\n\t\tif (poll->wait.private)\n\t\t\trefcount_dec(&req->refs);\n\t\tpoll->head = NULL;\n\t\tspin_unlock(&head->lock);\n\t}\n}\n\nstatic void io_poll_complete(struct io_kiocb *req, __poll_t mask, int error)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_poll_remove_double(req);\n\treq->poll.done = true;\n\tio_cqring_fill_event(req, error ? error : mangle_poll(mask));\n\tio_commit_cqring(ctx);\n}\n\nstatic void io_poll_task_func(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_kiocb *nxt;\n\n\tif (io_poll_rewait(req, &req->poll)) {\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t} else {\n\t\thash_del(&req->hash_node);\n\t\tio_poll_complete(req, req->result, 0);\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\n\t\tnxt = io_put_req_find_next(req);\n\t\tio_cqring_ev_posted(ctx);\n\t\tif (nxt)\n\t\t\t__io_req_task_submit(nxt);\n\t}\n\n\tpercpu_ref_put(&ctx->refs);\n}\n\nstatic int io_poll_double_wake(struct wait_queue_entry *wait, unsigned mode,\n\t\t\t       int sync, void *key)\n{\n\tstruct io_kiocb *req = wait->private;\n\tstruct io_poll_iocb *poll = io_poll_get_single(req);\n\t__poll_t mask = key_to_poll(key);\n\n\t/* for instances that support it check for an event match first: */\n\tif (mask && !(mask & poll->events))\n\t\treturn 0;\n\n\tlist_del_init(&wait->entry);\n\n\tif (poll && poll->head) {\n\t\tbool done;\n\n\t\tspin_lock(&poll->head->lock);\n\t\tdone = list_empty(&poll->wait.entry);\n\t\tif (!done)\n\t\t\tlist_del_init(&poll->wait.entry);\n\t\t/* make sure double remove sees this as being gone */\n\t\twait->private = NULL;\n\t\tspin_unlock(&poll->head->lock);\n\t\tif (!done) {\n\t\t\t/* use wait func handler, so it matches the rq type */\n\t\t\tpoll->wait.func(&poll->wait, mode, sync, key);\n\t\t}\n\t}\n\trefcount_dec(&req->refs);\n\treturn 1;\n}\n\nstatic void io_init_poll_iocb(struct io_poll_iocb *poll, __poll_t events,\n\t\t\t      wait_queue_func_t wake_func)\n{\n\tpoll->head = NULL;\n\tpoll->done = false;\n\tpoll->canceled = false;\n\tpoll->events = events;\n\tINIT_LIST_HEAD(&poll->wait.entry);\n\tinit_waitqueue_func_entry(&poll->wait, wake_func);\n}\n\nstatic void __io_queue_proc(struct io_poll_iocb *poll, struct io_poll_table *pt,\n\t\t\t    struct wait_queue_head *head,\n\t\t\t    struct io_poll_iocb **poll_ptr)\n{\n\tstruct io_kiocb *req = pt->req;\n\n\t/*\n\t * If poll->head is already set, it's because the file being polled\n\t * uses multiple waitqueues for poll handling (eg one for read, one\n\t * for write). Setup a separate io_poll_iocb if this happens.\n\t */\n\tif (unlikely(poll->head)) {\n\t\tstruct io_poll_iocb *poll_one = poll;\n\n\t\t/* already have a 2nd entry, fail a third attempt */\n\t\tif (*poll_ptr) {\n\t\t\tpt->error = -EINVAL;\n\t\t\treturn;\n\t\t}\n\t\tpoll = kmalloc(sizeof(*poll), GFP_ATOMIC);\n\t\tif (!poll) {\n\t\t\tpt->error = -ENOMEM;\n\t\t\treturn;\n\t\t}\n\t\tio_init_poll_iocb(poll, poll_one->events, io_poll_double_wake);\n\t\trefcount_inc(&req->refs);\n\t\tpoll->wait.private = req;\n\t\t*poll_ptr = poll;\n\t}\n\n\tpt->error = 0;\n\tpoll->head = head;\n\n\tif (poll->events & EPOLLEXCLUSIVE)\n\t\tadd_wait_queue_exclusive(head, &poll->wait);\n\telse\n\t\tadd_wait_queue(head, &poll->wait);\n}\n\nstatic void io_async_queue_proc(struct file *file, struct wait_queue_head *head,\n\t\t\t       struct poll_table_struct *p)\n{\n\tstruct io_poll_table *pt = container_of(p, struct io_poll_table, pt);\n\tstruct async_poll *apoll = pt->req->apoll;\n\n\t__io_queue_proc(&apoll->poll, pt, head, &apoll->double_poll);\n}\n\nstatic void io_async_task_func(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct async_poll *apoll = req->apoll;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\ttrace_io_uring_task_run(req->ctx, req->opcode, req->user_data);\n\n\tif (io_poll_rewait(req, &apoll->poll)) {\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t\tpercpu_ref_put(&ctx->refs);\n\t\treturn;\n\t}\n\n\t/* If req is still hashed, it cannot have been canceled. Don't check. */\n\tif (hash_hashed(&req->hash_node))\n\t\thash_del(&req->hash_node);\n\n\tio_poll_remove_double(req);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tif (!READ_ONCE(apoll->poll.canceled))\n\t\t__io_req_task_submit(req);\n\telse\n\t\t__io_req_task_cancel(req, -ECANCELED);\n\n\tpercpu_ref_put(&ctx->refs);\n\tkfree(apoll->double_poll);\n\tkfree(apoll);\n}\n\nstatic int io_async_wake(struct wait_queue_entry *wait, unsigned mode, int sync,\n\t\t\tvoid *key)\n{\n\tstruct io_kiocb *req = wait->private;\n\tstruct io_poll_iocb *poll = &req->apoll->poll;\n\n\ttrace_io_uring_poll_wake(req->ctx, req->opcode, req->user_data,\n\t\t\t\t\tkey_to_poll(key));\n\n\treturn __io_async_wake(req, poll, key_to_poll(key), io_async_task_func);\n}\n\nstatic void io_poll_req_insert(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct hlist_head *list;\n\n\tlist = &ctx->cancel_hash[hash_long(req->user_data, ctx->cancel_hash_bits)];\n\thlist_add_head(&req->hash_node, list);\n}\n\nstatic __poll_t __io_arm_poll_handler(struct io_kiocb *req,\n\t\t\t\t      struct io_poll_iocb *poll,\n\t\t\t\t      struct io_poll_table *ipt, __poll_t mask,\n\t\t\t\t      wait_queue_func_t wake_func)\n\t__acquires(&ctx->completion_lock)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tbool cancel = false;\n\n\tINIT_HLIST_NODE(&req->hash_node);\n\tio_init_poll_iocb(poll, mask, wake_func);\n\tpoll->file = req->file;\n\tpoll->wait.private = req;\n\n\tipt->pt._key = mask;\n\tipt->req = req;\n\tipt->error = -EINVAL;\n\n\tmask = vfs_poll(req->file, &ipt->pt) & poll->events;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tif (likely(poll->head)) {\n\t\tspin_lock(&poll->head->lock);\n\t\tif (unlikely(list_empty(&poll->wait.entry))) {\n\t\t\tif (ipt->error)\n\t\t\t\tcancel = true;\n\t\t\tipt->error = 0;\n\t\t\tmask = 0;\n\t\t}\n\t\tif (mask || ipt->error)\n\t\t\tlist_del_init(&poll->wait.entry);\n\t\telse if (cancel)\n\t\t\tWRITE_ONCE(poll->canceled, true);\n\t\telse if (!poll->done) /* actually waiting for an event */\n\t\t\tio_poll_req_insert(req);\n\t\tspin_unlock(&poll->head->lock);\n\t}\n\n\treturn mask;\n}\n\nstatic bool io_arm_poll_handler(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct async_poll *apoll;\n\tstruct io_poll_table ipt;\n\t__poll_t mask, ret;\n\tint rw;\n\n\tif (!req->file || !file_can_poll(req->file))\n\t\treturn false;\n\tif (req->flags & REQ_F_POLLED)\n\t\treturn false;\n\tif (def->pollin)\n\t\trw = READ;\n\telse if (def->pollout)\n\t\trw = WRITE;\n\telse\n\t\treturn false;\n\t/* if we can't nonblock try, then no point in arming a poll handler */\n\tif (!io_file_supports_async(req->file, rw))\n\t\treturn false;\n\n\tapoll = kmalloc(sizeof(*apoll), GFP_ATOMIC);\n\tif (unlikely(!apoll))\n\t\treturn false;\n\tapoll->double_poll = NULL;\n\n\treq->flags |= REQ_F_POLLED;\n\treq->apoll = apoll;\n\n\tmask = 0;\n\tif (def->pollin)\n\t\tmask |= POLLIN | POLLRDNORM;\n\tif (def->pollout)\n\t\tmask |= POLLOUT | POLLWRNORM;\n\n\t/* If reading from MSG_ERRQUEUE using recvmsg, ignore POLLIN */\n\tif ((req->opcode == IORING_OP_RECVMSG) &&\n\t    (req->sr_msg.msg_flags & MSG_ERRQUEUE))\n\t\tmask &= ~POLLIN;\n\n\tmask |= POLLERR | POLLPRI;\n\n\tipt.pt._qproc = io_async_queue_proc;\n\n\tret = __io_arm_poll_handler(req, &apoll->poll, &ipt, mask,\n\t\t\t\t\tio_async_wake);\n\tif (ret || ipt.error) {\n\t\tio_poll_remove_double(req);\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t\tkfree(apoll->double_poll);\n\t\tkfree(apoll);\n\t\treturn false;\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\ttrace_io_uring_poll_arm(ctx, req->opcode, req->user_data, mask,\n\t\t\t\t\tapoll->poll.events);\n\treturn true;\n}\n\nstatic bool __io_poll_remove_one(struct io_kiocb *req,\n\t\t\t\t struct io_poll_iocb *poll)\n{\n\tbool do_complete = false;\n\n\tspin_lock(&poll->head->lock);\n\tWRITE_ONCE(poll->canceled, true);\n\tif (!list_empty(&poll->wait.entry)) {\n\t\tlist_del_init(&poll->wait.entry);\n\t\tdo_complete = true;\n\t}\n\tspin_unlock(&poll->head->lock);\n\thash_del(&req->hash_node);\n\treturn do_complete;\n}\n\nstatic bool io_poll_remove_one(struct io_kiocb *req)\n{\n\tbool do_complete;\n\n\tio_poll_remove_double(req);\n\n\tif (req->opcode == IORING_OP_POLL_ADD) {\n\t\tdo_complete = __io_poll_remove_one(req, &req->poll);\n\t} else {\n\t\tstruct async_poll *apoll = req->apoll;\n\n\t\t/* non-poll requests have submit ref still */\n\t\tdo_complete = __io_poll_remove_one(req, &apoll->poll);\n\t\tif (do_complete) {\n\t\t\tio_put_req(req);\n\t\t\tkfree(apoll->double_poll);\n\t\t\tkfree(apoll);\n\t\t}\n\t}\n\n\tif (do_complete) {\n\t\tio_cqring_fill_event(req, -ECANCELED);\n\t\tio_commit_cqring(req->ctx);\n\t\treq_set_fail_links(req);\n\t\tio_put_req_deferred(req, 1);\n\t}\n\n\treturn do_complete;\n}\n\n/*\n * Returns true if we found and killed one or more poll requests\n */\nstatic bool io_poll_remove_all(struct io_ring_ctx *ctx, struct task_struct *tsk,\n\t\t\t       struct files_struct *files)\n{\n\tstruct hlist_node *tmp;\n\tstruct io_kiocb *req;\n\tint posted = 0, i;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tfor (i = 0; i < (1U << ctx->cancel_hash_bits); i++) {\n\t\tstruct hlist_head *list;\n\n\t\tlist = &ctx->cancel_hash[i];\n\t\thlist_for_each_entry_safe(req, tmp, list, hash_node) {\n\t\t\tif (io_match_task(req, tsk, files))\n\t\t\t\tposted += io_poll_remove_one(req);\n\t\t}\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tif (posted)\n\t\tio_cqring_ev_posted(ctx);\n\n\treturn posted != 0;\n}\n\nstatic int io_poll_cancel(struct io_ring_ctx *ctx, __u64 sqe_addr)\n{\n\tstruct hlist_head *list;\n\tstruct io_kiocb *req;\n\n\tlist = &ctx->cancel_hash[hash_long(sqe_addr, ctx->cancel_hash_bits)];\n\thlist_for_each_entry(req, list, hash_node) {\n\t\tif (sqe_addr != req->user_data)\n\t\t\tcontinue;\n\t\tif (io_poll_remove_one(req))\n\t\t\treturn 0;\n\t\treturn -EALREADY;\n\t}\n\n\treturn -ENOENT;\n}\n\nstatic int io_poll_remove_prep(struct io_kiocb *req,\n\t\t\t       const struct io_uring_sqe *sqe)\n{\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->off || sqe->len || sqe->buf_index ||\n\t    sqe->poll_events)\n\t\treturn -EINVAL;\n\n\treq->poll_remove.addr = READ_ONCE(sqe->addr);\n\treturn 0;\n}\n\n/*\n * Find a running poll command that matches one specified in sqe->addr,\n * and remove it if found.\n */\nstatic int io_poll_remove(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tret = io_poll_cancel(ctx, req->poll_remove.addr);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_poll_wake(struct wait_queue_entry *wait, unsigned mode, int sync,\n\t\t\tvoid *key)\n{\n\tstruct io_kiocb *req = wait->private;\n\tstruct io_poll_iocb *poll = &req->poll;\n\n\treturn __io_async_wake(req, poll, key_to_poll(key), io_poll_task_func);\n}\n\nstatic void io_poll_queue_proc(struct file *file, struct wait_queue_head *head,\n\t\t\t       struct poll_table_struct *p)\n{\n\tstruct io_poll_table *pt = container_of(p, struct io_poll_table, pt);\n\n\t__io_queue_proc(&pt->req->poll, pt, head, (struct io_poll_iocb **) &pt->req->async_data);\n}\n\nstatic int io_poll_add_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_poll_iocb *poll = &req->poll;\n\tu32 events;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->addr || sqe->ioprio || sqe->off || sqe->len || sqe->buf_index)\n\t\treturn -EINVAL;\n\n\tevents = READ_ONCE(sqe->poll32_events);\n#ifdef __BIG_ENDIAN\n\tevents = swahw32(events);\n#endif\n\tpoll->events = demangle_poll(events) | EPOLLERR | EPOLLHUP |\n\t\t       (events & EPOLLEXCLUSIVE);\n\treturn 0;\n}\n\nstatic int io_poll_add(struct io_kiocb *req)\n{\n\tstruct io_poll_iocb *poll = &req->poll;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_poll_table ipt;\n\t__poll_t mask;\n\n\tipt.pt._qproc = io_poll_queue_proc;\n\n\tmask = __io_arm_poll_handler(req, &req->poll, &ipt, poll->events,\n\t\t\t\t\tio_poll_wake);\n\n\tif (mask) { /* no async, we'd stolen it */\n\t\tipt.error = 0;\n\t\tio_poll_complete(req, mask, 0);\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tif (mask) {\n\t\tio_cqring_ev_posted(ctx);\n\t\tio_put_req(req);\n\t}\n\treturn ipt.error;\n}\n\nstatic enum hrtimer_restart io_timeout_fn(struct hrtimer *timer)\n{\n\tstruct io_timeout_data *data = container_of(timer,\n\t\t\t\t\t\tstruct io_timeout_data, timer);\n\tstruct io_kiocb *req = data->req;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tlist_del_init(&req->timeout.list);\n\tatomic_set(&req->ctx->cq_timeouts,\n\t\tatomic_read(&req->ctx->cq_timeouts) + 1);\n\n\tio_cqring_fill_event(req, -ETIME);\n\tio_commit_cqring(ctx);\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\tio_cqring_ev_posted(ctx);\n\treq_set_fail_links(req);\n\tio_put_req(req);\n\treturn HRTIMER_NORESTART;\n}\n\nstatic int __io_timeout_cancel(struct io_kiocb *req)\n{\n\tstruct io_timeout_data *io = req->async_data;\n\tint ret;\n\n\tret = hrtimer_try_to_cancel(&io->timer);\n\tif (ret == -1)\n\t\treturn -EALREADY;\n\tlist_del_init(&req->timeout.list);\n\n\treq_set_fail_links(req);\n\tio_cqring_fill_event(req, -ECANCELED);\n\tio_put_req_deferred(req, 1);\n\treturn 0;\n}\n\nstatic int io_timeout_cancel(struct io_ring_ctx *ctx, __u64 user_data)\n{\n\tstruct io_kiocb *req;\n\tint ret = -ENOENT;\n\n\tlist_for_each_entry(req, &ctx->timeout_list, timeout.list) {\n\t\tif (user_data == req->user_data) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (ret == -ENOENT)\n\t\treturn ret;\n\n\treturn __io_timeout_cancel(req);\n}\n\nstatic int io_timeout_remove_prep(struct io_kiocb *req,\n\t\t\t\t  const struct io_uring_sqe *sqe)\n{\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (unlikely(req->flags & (REQ_F_FIXED_FILE | REQ_F_BUFFER_SELECT)))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->buf_index || sqe->len || sqe->timeout_flags)\n\t\treturn -EINVAL;\n\n\treq->timeout_rem.addr = READ_ONCE(sqe->addr);\n\treturn 0;\n}\n\n/*\n * Remove or update an existing timeout command\n */\nstatic int io_timeout_remove(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tret = io_timeout_cancel(ctx, req->timeout_rem.addr);\n\n\tio_cqring_fill_event(req, ret);\n\tio_commit_cqring(ctx);\n\tspin_unlock_irq(&ctx->completion_lock);\n\tio_cqring_ev_posted(ctx);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_put_req(req);\n\treturn 0;\n}\n\nstatic int io_timeout_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,\n\t\t\t   bool is_timeout_link)\n{\n\tstruct io_timeout_data *data;\n\tunsigned flags;\n\tu32 off = READ_ONCE(sqe->off);\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->buf_index || sqe->len != 1)\n\t\treturn -EINVAL;\n\tif (off && is_timeout_link)\n\t\treturn -EINVAL;\n\tflags = READ_ONCE(sqe->timeout_flags);\n\tif (flags & ~IORING_TIMEOUT_ABS)\n\t\treturn -EINVAL;\n\n\treq->timeout.off = off;\n\n\tif (!req->async_data && io_alloc_async_data(req))\n\t\treturn -ENOMEM;\n\n\tdata = req->async_data;\n\tdata->req = req;\n\n\tif (get_timespec64(&data->ts, u64_to_user_ptr(sqe->addr)))\n\t\treturn -EFAULT;\n\n\tif (flags & IORING_TIMEOUT_ABS)\n\t\tdata->mode = HRTIMER_MODE_ABS;\n\telse\n\t\tdata->mode = HRTIMER_MODE_REL;\n\n\thrtimer_init(&data->timer, CLOCK_MONOTONIC, data->mode);\n\treturn 0;\n}\n\nstatic int io_timeout(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_timeout_data *data = req->async_data;\n\tstruct list_head *entry;\n\tu32 tail, off = req->timeout.off;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\n\t/*\n\t * sqe->off holds how many events that need to occur for this\n\t * timeout event to be satisfied. If it isn't set, then this is\n\t * a pure timeout request, sequence isn't used.\n\t */\n\tif (io_is_timeout_noseq(req)) {\n\t\tentry = ctx->timeout_list.prev;\n\t\tgoto add;\n\t}\n\n\ttail = ctx->cached_cq_tail - atomic_read(&ctx->cq_timeouts);\n\treq->timeout.target_seq = tail + off;\n\n\t/*\n\t * Insertion sort, ensuring the first entry in the list is always\n\t * the one we need first.\n\t */\n\tlist_for_each_prev(entry, &ctx->timeout_list) {\n\t\tstruct io_kiocb *nxt = list_entry(entry, struct io_kiocb,\n\t\t\t\t\t\t  timeout.list);\n\n\t\tif (io_is_timeout_noseq(nxt))\n\t\t\tcontinue;\n\t\t/* nxt.seq is behind @tail, otherwise would've been completed */\n\t\tif (off >= nxt->timeout.target_seq - tail)\n\t\t\tbreak;\n\t}\nadd:\n\tlist_add(&req->timeout.list, entry);\n\tdata->timer.function = io_timeout_fn;\n\thrtimer_start(&data->timer, timespec64_to_ktime(data->ts), data->mode);\n\tspin_unlock_irq(&ctx->completion_lock);\n\treturn 0;\n}\n\nstatic bool io_cancel_cb(struct io_wq_work *work, void *data)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\n\treturn req->user_data == (unsigned long) data;\n}\n\nstatic int io_async_cancel_one(struct io_ring_ctx *ctx, void *sqe_addr)\n{\n\tenum io_wq_cancel cancel_ret;\n\tint ret = 0;\n\n\tcancel_ret = io_wq_cancel_cb(ctx->io_wq, io_cancel_cb, sqe_addr, false);\n\tswitch (cancel_ret) {\n\tcase IO_WQ_CANCEL_OK:\n\t\tret = 0;\n\t\tbreak;\n\tcase IO_WQ_CANCEL_RUNNING:\n\t\tret = -EALREADY;\n\t\tbreak;\n\tcase IO_WQ_CANCEL_NOTFOUND:\n\t\tret = -ENOENT;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic void io_async_find_and_cancel(struct io_ring_ctx *ctx,\n\t\t\t\t     struct io_kiocb *req, __u64 sqe_addr,\n\t\t\t\t     int success_ret)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tret = io_async_cancel_one(ctx, (void *) (unsigned long) sqe_addr);\n\tif (ret != -ENOENT) {\n\t\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\t\tgoto done;\n\t}\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tret = io_timeout_cancel(ctx, sqe_addr);\n\tif (ret != -ENOENT)\n\t\tgoto done;\n\tret = io_poll_cancel(ctx, sqe_addr);\ndone:\n\tif (!ret)\n\t\tret = success_ret;\n\tio_cqring_fill_event(req, ret);\n\tio_commit_cqring(ctx);\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\tio_cqring_ev_posted(ctx);\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_put_req(req);\n}\n\nstatic int io_async_cancel_prep(struct io_kiocb *req,\n\t\t\t\tconst struct io_uring_sqe *sqe)\n{\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (unlikely(req->flags & (REQ_F_FIXED_FILE | REQ_F_BUFFER_SELECT)))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->off || sqe->len || sqe->cancel_flags)\n\t\treturn -EINVAL;\n\n\treq->cancel.addr = READ_ONCE(sqe->addr);\n\treturn 0;\n}\n\nstatic int io_async_cancel(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_async_find_and_cancel(ctx, req, req->cancel.addr, 0);\n\treturn 0;\n}\n\nstatic int io_files_update_prep(struct io_kiocb *req,\n\t\t\t\tconst struct io_uring_sqe *sqe)\n{\n\tif (unlikely(req->ctx->flags & IORING_SETUP_SQPOLL))\n\t\treturn -EINVAL;\n\tif (unlikely(req->flags & (REQ_F_FIXED_FILE | REQ_F_BUFFER_SELECT)))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->rw_flags)\n\t\treturn -EINVAL;\n\n\treq->files_update.offset = READ_ONCE(sqe->off);\n\treq->files_update.nr_args = READ_ONCE(sqe->len);\n\tif (!req->files_update.nr_args)\n\t\treturn -EINVAL;\n\treq->files_update.arg = READ_ONCE(sqe->addr);\n\treturn 0;\n}\n\nstatic int io_files_update(struct io_kiocb *req, bool force_nonblock,\n\t\t\t   struct io_comp_state *cs)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_uring_files_update up;\n\tint ret;\n\n\tif (force_nonblock)\n\t\treturn -EAGAIN;\n\n\tup.offset = req->files_update.offset;\n\tup.fds = req->files_update.arg;\n\n\tmutex_lock(&ctx->uring_lock);\n\tret = __io_sqe_files_update(ctx, &up, req->files_update.nr_args);\n\tmutex_unlock(&ctx->uring_lock);\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, ret, 0, cs);\n\treturn 0;\n}\n\nstatic int io_req_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tswitch (req->opcode) {\n\tcase IORING_OP_NOP:\n\t\treturn 0;\n\tcase IORING_OP_READV:\n\tcase IORING_OP_READ_FIXED:\n\tcase IORING_OP_READ:\n\t\treturn io_read_prep(req, sqe);\n\tcase IORING_OP_WRITEV:\n\tcase IORING_OP_WRITE_FIXED:\n\tcase IORING_OP_WRITE:\n\t\treturn io_write_prep(req, sqe);\n\tcase IORING_OP_POLL_ADD:\n\t\treturn io_poll_add_prep(req, sqe);\n\tcase IORING_OP_POLL_REMOVE:\n\t\treturn io_poll_remove_prep(req, sqe);\n\tcase IORING_OP_FSYNC:\n\t\treturn io_prep_fsync(req, sqe);\n\tcase IORING_OP_SYNC_FILE_RANGE:\n\t\treturn io_prep_sfr(req, sqe);\n\tcase IORING_OP_SENDMSG:\n\tcase IORING_OP_SEND:\n\t\treturn io_sendmsg_prep(req, sqe);\n\tcase IORING_OP_RECVMSG:\n\tcase IORING_OP_RECV:\n\t\treturn io_recvmsg_prep(req, sqe);\n\tcase IORING_OP_CONNECT:\n\t\treturn io_connect_prep(req, sqe);\n\tcase IORING_OP_TIMEOUT:\n\t\treturn io_timeout_prep(req, sqe, false);\n\tcase IORING_OP_TIMEOUT_REMOVE:\n\t\treturn io_timeout_remove_prep(req, sqe);\n\tcase IORING_OP_ASYNC_CANCEL:\n\t\treturn io_async_cancel_prep(req, sqe);\n\tcase IORING_OP_LINK_TIMEOUT:\n\t\treturn io_timeout_prep(req, sqe, true);\n\tcase IORING_OP_ACCEPT:\n\t\treturn io_accept_prep(req, sqe);\n\tcase IORING_OP_FALLOCATE:\n\t\treturn io_fallocate_prep(req, sqe);\n\tcase IORING_OP_OPENAT:\n\t\treturn io_openat_prep(req, sqe);\n\tcase IORING_OP_CLOSE:\n\t\treturn io_close_prep(req, sqe);\n\tcase IORING_OP_FILES_UPDATE:\n\t\treturn io_files_update_prep(req, sqe);\n\tcase IORING_OP_STATX:\n\t\treturn io_statx_prep(req, sqe);\n\tcase IORING_OP_FADVISE:\n\t\treturn io_fadvise_prep(req, sqe);\n\tcase IORING_OP_MADVISE:\n\t\treturn io_madvise_prep(req, sqe);\n\tcase IORING_OP_OPENAT2:\n\t\treturn io_openat2_prep(req, sqe);\n\tcase IORING_OP_EPOLL_CTL:\n\t\treturn io_epoll_ctl_prep(req, sqe);\n\tcase IORING_OP_SPLICE:\n\t\treturn io_splice_prep(req, sqe);\n\tcase IORING_OP_PROVIDE_BUFFERS:\n\t\treturn io_provide_buffers_prep(req, sqe);\n\tcase IORING_OP_REMOVE_BUFFERS:\n\t\treturn io_remove_buffers_prep(req, sqe);\n\tcase IORING_OP_TEE:\n\t\treturn io_tee_prep(req, sqe);\n\tcase IORING_OP_SHUTDOWN:\n\t\treturn io_shutdown_prep(req, sqe);\n\tcase IORING_OP_RENAMEAT:\n\t\treturn io_renameat_prep(req, sqe);\n\tcase IORING_OP_UNLINKAT:\n\t\treturn io_unlinkat_prep(req, sqe);\n\t}\n\n\tprintk_once(KERN_WARNING \"io_uring: unhandled opcode %d\\n\",\n\t\t\treq->opcode);\n\treturn-EINVAL;\n}\n\nstatic int io_req_defer_prep(struct io_kiocb *req,\n\t\t\t     const struct io_uring_sqe *sqe)\n{\n\tif (!sqe)\n\t\treturn 0;\n\tif (io_alloc_async_data(req))\n\t\treturn -EAGAIN;\n\treturn io_req_prep(req, sqe);\n}\n\nstatic u32 io_get_sequence(struct io_kiocb *req)\n{\n\tstruct io_kiocb *pos;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tu32 total_submitted, nr_reqs = 0;\n\n\tio_for_each_link(pos, req)\n\t\tnr_reqs++;\n\n\ttotal_submitted = ctx->cached_sq_head - ctx->cached_sq_dropped;\n\treturn total_submitted - nr_reqs;\n}\n\nstatic int io_req_defer(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_defer_entry *de;\n\tint ret;\n\tu32 seq;\n\n\t/* Still need defer if there is pending req in defer list. */\n\tif (likely(list_empty_careful(&ctx->defer_list) &&\n\t\t!(req->flags & REQ_F_IO_DRAIN)))\n\t\treturn 0;\n\n\tseq = io_get_sequence(req);\n\t/* Still a chance to pass the sequence check */\n\tif (!req_need_defer(req, seq) && list_empty_careful(&ctx->defer_list))\n\t\treturn 0;\n\n\tif (!req->async_data) {\n\t\tret = io_req_defer_prep(req, sqe);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\tio_prep_async_link(req);\n\tde = kmalloc(sizeof(*de), GFP_KERNEL);\n\tif (!de)\n\t\treturn -ENOMEM;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tif (!req_need_defer(req, seq) && list_empty(&ctx->defer_list)) {\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t\tkfree(de);\n\t\tio_queue_async_work(req);\n\t\treturn -EIOCBQUEUED;\n\t}\n\n\ttrace_io_uring_defer(ctx, req, req->user_data);\n\tde->req = req;\n\tde->seq = seq;\n\tlist_add_tail(&de->list, &ctx->defer_list);\n\tspin_unlock_irq(&ctx->completion_lock);\n\treturn -EIOCBQUEUED;\n}\n\nstatic void io_req_drop_files(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_uring_task *tctx = req->task->io_uring;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->inflight_lock, flags);\n\tlist_del(&req->inflight_entry);\n\tif (atomic_read(&tctx->in_idle))\n\t\twake_up(&tctx->wait);\n\tspin_unlock_irqrestore(&ctx->inflight_lock, flags);\n\treq->flags &= ~REQ_F_INFLIGHT;\n\tput_files_struct(req->work.identity->files);\n\tput_nsproxy(req->work.identity->nsproxy);\n\treq->work.flags &= ~IO_WQ_WORK_FILES;\n}\n\nstatic void __io_clean_op(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_BUFFER_SELECTED) {\n\t\tswitch (req->opcode) {\n\t\tcase IORING_OP_READV:\n\t\tcase IORING_OP_READ_FIXED:\n\t\tcase IORING_OP_READ:\n\t\t\tkfree((void *)(unsigned long)req->rw.addr);\n\t\t\tbreak;\n\t\tcase IORING_OP_RECVMSG:\n\t\tcase IORING_OP_RECV:\n\t\t\tkfree(req->sr_msg.kbuf);\n\t\t\tbreak;\n\t\t}\n\t\treq->flags &= ~REQ_F_BUFFER_SELECTED;\n\t}\n\n\tif (req->flags & REQ_F_NEED_CLEANUP) {\n\t\tswitch (req->opcode) {\n\t\tcase IORING_OP_READV:\n\t\tcase IORING_OP_READ_FIXED:\n\t\tcase IORING_OP_READ:\n\t\tcase IORING_OP_WRITEV:\n\t\tcase IORING_OP_WRITE_FIXED:\n\t\tcase IORING_OP_WRITE: {\n\t\t\tstruct io_async_rw *io = req->async_data;\n\t\t\tif (io->free_iovec)\n\t\t\t\tkfree(io->free_iovec);\n\t\t\tbreak;\n\t\t\t}\n\t\tcase IORING_OP_RECVMSG:\n\t\tcase IORING_OP_SENDMSG: {\n\t\t\tstruct io_async_msghdr *io = req->async_data;\n\t\t\tif (io->iov != io->fast_iov)\n\t\t\t\tkfree(io->iov);\n\t\t\tbreak;\n\t\t\t}\n\t\tcase IORING_OP_SPLICE:\n\t\tcase IORING_OP_TEE:\n\t\t\tio_put_file(req, req->splice.file_in,\n\t\t\t\t    (req->splice.flags & SPLICE_F_FD_IN_FIXED));\n\t\t\tbreak;\n\t\tcase IORING_OP_OPENAT:\n\t\tcase IORING_OP_OPENAT2:\n\t\t\tif (req->open.filename)\n\t\t\t\tputname(req->open.filename);\n\t\t\tbreak;\n\t\tcase IORING_OP_RENAMEAT:\n\t\t\tputname(req->rename.oldpath);\n\t\t\tputname(req->rename.newpath);\n\t\t\tbreak;\n\t\tcase IORING_OP_UNLINKAT:\n\t\t\tputname(req->unlink.filename);\n\t\t\tbreak;\n\t\t}\n\t\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\t}\n\n\tif (req->flags & REQ_F_INFLIGHT)\n\t\tio_req_drop_files(req);\n}\n\nstatic int io_issue_sqe(struct io_kiocb *req, bool force_nonblock,\n\t\t\tstruct io_comp_state *cs)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tswitch (req->opcode) {\n\tcase IORING_OP_NOP:\n\t\tret = io_nop(req, cs);\n\t\tbreak;\n\tcase IORING_OP_READV:\n\tcase IORING_OP_READ_FIXED:\n\tcase IORING_OP_READ:\n\t\tret = io_read(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_WRITEV:\n\tcase IORING_OP_WRITE_FIXED:\n\tcase IORING_OP_WRITE:\n\t\tret = io_write(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_FSYNC:\n\t\tret = io_fsync(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_POLL_ADD:\n\t\tret = io_poll_add(req);\n\t\tbreak;\n\tcase IORING_OP_POLL_REMOVE:\n\t\tret = io_poll_remove(req);\n\t\tbreak;\n\tcase IORING_OP_SYNC_FILE_RANGE:\n\t\tret = io_sync_file_range(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_SENDMSG:\n\t\tret = io_sendmsg(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_SEND:\n\t\tret = io_send(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_RECVMSG:\n\t\tret = io_recvmsg(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_RECV:\n\t\tret = io_recv(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_TIMEOUT:\n\t\tret = io_timeout(req);\n\t\tbreak;\n\tcase IORING_OP_TIMEOUT_REMOVE:\n\t\tret = io_timeout_remove(req);\n\t\tbreak;\n\tcase IORING_OP_ACCEPT:\n\t\tret = io_accept(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_CONNECT:\n\t\tret = io_connect(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_ASYNC_CANCEL:\n\t\tret = io_async_cancel(req);\n\t\tbreak;\n\tcase IORING_OP_FALLOCATE:\n\t\tret = io_fallocate(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_OPENAT:\n\t\tret = io_openat(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_CLOSE:\n\t\tret = io_close(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_FILES_UPDATE:\n\t\tret = io_files_update(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_STATX:\n\t\tret = io_statx(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_FADVISE:\n\t\tret = io_fadvise(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_MADVISE:\n\t\tret = io_madvise(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_OPENAT2:\n\t\tret = io_openat2(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_EPOLL_CTL:\n\t\tret = io_epoll_ctl(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_SPLICE:\n\t\tret = io_splice(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_PROVIDE_BUFFERS:\n\t\tret = io_provide_buffers(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_REMOVE_BUFFERS:\n\t\tret = io_remove_buffers(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_TEE:\n\t\tret = io_tee(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_SHUTDOWN:\n\t\tret = io_shutdown(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_RENAMEAT:\n\t\tret = io_renameat(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_UNLINKAT:\n\t\tret = io_unlinkat(req, force_nonblock);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\tif (ret)\n\t\treturn ret;\n\n\t/* If the op doesn't have a file, we're not polling for it */\n\tif ((ctx->flags & IORING_SETUP_IOPOLL) && req->file) {\n\t\tconst bool in_async = io_wq_current_is_worker();\n\n\t\t/* workqueue context doesn't hold uring_lock, grab it now */\n\t\tif (in_async)\n\t\t\tmutex_lock(&ctx->uring_lock);\n\n\t\tio_iopoll_req_issued(req, in_async);\n\n\t\tif (in_async)\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t}\n\n\treturn 0;\n}\n\nstatic struct io_wq_work *io_wq_submit_work(struct io_wq_work *work)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\tstruct io_kiocb *timeout;\n\tint ret = 0;\n\n\ttimeout = io_prep_linked_timeout(req);\n\tif (timeout)\n\t\tio_queue_linked_timeout(timeout);\n\n\t/* if NO_CANCEL is set, we must still run the work */\n\tif ((work->flags & (IO_WQ_WORK_CANCEL|IO_WQ_WORK_NO_CANCEL)) ==\n\t\t\t\tIO_WQ_WORK_CANCEL) {\n\t\tret = -ECANCELED;\n\t}\n\n\tif (!ret) {\n\t\tdo {\n\t\t\tret = io_issue_sqe(req, false, NULL);\n\t\t\t/*\n\t\t\t * We can get EAGAIN for polled IO even though we're\n\t\t\t * forcing a sync submission from here, since we can't\n\t\t\t * wait for request slots on the block side.\n\t\t\t */\n\t\t\tif (ret != -EAGAIN)\n\t\t\t\tbreak;\n\t\t\tcond_resched();\n\t\t} while (1);\n\t}\n\n\tif (ret) {\n\t\treq_set_fail_links(req);\n\t\tio_req_complete(req, ret);\n\t}\n\n\treturn io_steal_work(req);\n}\n\nstatic inline struct file *io_file_from_index(struct io_ring_ctx *ctx,\n\t\t\t\t\t      int index)\n{\n\tstruct fixed_file_table *table;\n\n\ttable = &ctx->file_data->table[index >> IORING_FILE_TABLE_SHIFT];\n\treturn table->files[index & IORING_FILE_TABLE_MASK];\n}\n\nstatic struct file *io_file_get(struct io_submit_state *state,\n\t\t\t\tstruct io_kiocb *req, int fd, bool fixed)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct file *file;\n\n\tif (fixed) {\n\t\tif (unlikely((unsigned int)fd >= ctx->nr_user_files))\n\t\t\treturn NULL;\n\t\tfd = array_index_nospec(fd, ctx->nr_user_files);\n\t\tfile = io_file_from_index(ctx, fd);\n\t\tio_set_resource_node(req);\n\t} else {\n\t\ttrace_io_uring_file_get(ctx, fd);\n\t\tfile = __io_file_get(state, fd);\n\t}\n\n\treturn file;\n}\n\nstatic int io_req_set_file(struct io_submit_state *state, struct io_kiocb *req,\n\t\t\t   int fd)\n{\n\treq->file = io_file_get(state, req, fd, req->flags & REQ_F_FIXED_FILE);\n\tif (req->file || io_op_defs[req->opcode].needs_file_no_error)\n\t\treturn 0;\n\treturn -EBADF;\n}\n\nstatic enum hrtimer_restart io_link_timeout_fn(struct hrtimer *timer)\n{\n\tstruct io_timeout_data *data = container_of(timer,\n\t\t\t\t\t\tstruct io_timeout_data, timer);\n\tstruct io_kiocb *prev, *req = data->req;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tprev = req->timeout.head;\n\treq->timeout.head = NULL;\n\n\t/*\n\t * We don't expect the list to be empty, that will only happen if we\n\t * race with the completion of the linked work.\n\t */\n\tif (prev && refcount_inc_not_zero(&prev->refs))\n\t\tio_remove_next_linked(prev);\n\telse\n\t\tprev = NULL;\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\tif (prev) {\n\t\treq_set_fail_links(prev);\n\t\tio_async_find_and_cancel(ctx, req, prev->user_data, -ETIME);\n\t\tio_put_req(prev);\n\t} else {\n\t\tio_req_complete(req, -ETIME);\n\t}\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void __io_queue_linked_timeout(struct io_kiocb *req)\n{\n\t/*\n\t * If the back reference is NULL, then our linked request finished\n\t * before we got a chance to setup the timer\n\t */\n\tif (req->timeout.head) {\n\t\tstruct io_timeout_data *data = req->async_data;\n\n\t\tdata->timer.function = io_link_timeout_fn;\n\t\thrtimer_start(&data->timer, timespec64_to_ktime(data->ts),\n\t\t\t\tdata->mode);\n\t}\n}\n\nstatic void io_queue_linked_timeout(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\t__io_queue_linked_timeout(req);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\t/* drop submission reference */\n\tio_put_req(req);\n}\n\nstatic struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt = req->link;\n\n\tif (!nxt || (req->flags & REQ_F_LINK_TIMEOUT) ||\n\t    nxt->opcode != IORING_OP_LINK_TIMEOUT)\n\t\treturn NULL;\n\n\tnxt->timeout.head = req;\n\tnxt->flags |= REQ_F_LTIMEOUT_ACTIVE;\n\treq->flags |= REQ_F_LINK_TIMEOUT;\n\treturn nxt;\n}\n\nstatic void __io_queue_sqe(struct io_kiocb *req, struct io_comp_state *cs)\n{\n\tstruct io_kiocb *linked_timeout;\n\tconst struct cred *old_creds = NULL;\n\tint ret;\n\nagain:\n\tlinked_timeout = io_prep_linked_timeout(req);\n\n\tif ((req->flags & REQ_F_WORK_INITIALIZED) &&\n\t    (req->work.flags & IO_WQ_WORK_CREDS) &&\n\t    req->work.identity->creds != current_cred()) {\n\t\tif (old_creds)\n\t\t\trevert_creds(old_creds);\n\t\tif (old_creds == req->work.identity->creds)\n\t\t\told_creds = NULL; /* restored original creds */\n\t\telse\n\t\t\told_creds = override_creds(req->work.identity->creds);\n\t}\n\n\tret = io_issue_sqe(req, true, cs);\n\n\t/*\n\t * We async punt it if the file wasn't marked NOWAIT, or if the file\n\t * doesn't support non-blocking read/write attempts\n\t */\n\tif (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {\n\t\tif (!io_arm_poll_handler(req)) {\n\t\t\t/*\n\t\t\t * Queued up for async execution, worker will release\n\t\t\t * submit reference when the iocb is actually submitted.\n\t\t\t */\n\t\t\tio_queue_async_work(req);\n\t\t}\n\n\t\tif (linked_timeout)\n\t\t\tio_queue_linked_timeout(linked_timeout);\n\t} else if (likely(!ret)) {\n\t\t/* drop submission reference */\n\t\treq = io_put_req_find_next(req);\n\t\tif (linked_timeout)\n\t\t\tio_queue_linked_timeout(linked_timeout);\n\n\t\tif (req) {\n\t\t\tif (!(req->flags & REQ_F_FORCE_ASYNC))\n\t\t\t\tgoto again;\n\t\t\tio_queue_async_work(req);\n\t\t}\n\t} else {\n\t\t/* un-prep timeout, so it'll be killed as any other linked */\n\t\treq->flags &= ~REQ_F_LINK_TIMEOUT;\n\t\treq_set_fail_links(req);\n\t\tio_put_req(req);\n\t\tio_req_complete(req, ret);\n\t}\n\n\tif (old_creds)\n\t\trevert_creds(old_creds);\n}\n\nstatic void io_queue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,\n\t\t\t struct io_comp_state *cs)\n{\n\tint ret;\n\n\tret = io_req_defer(req, sqe);\n\tif (ret) {\n\t\tif (ret != -EIOCBQUEUED) {\nfail_req:\n\t\t\treq_set_fail_links(req);\n\t\t\tio_put_req(req);\n\t\t\tio_req_complete(req, ret);\n\t\t}\n\t} else if (req->flags & REQ_F_FORCE_ASYNC) {\n\t\tif (!req->async_data) {\n\t\t\tret = io_req_defer_prep(req, sqe);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto fail_req;\n\t\t}\n\t\tio_queue_async_work(req);\n\t} else {\n\t\tif (sqe) {\n\t\t\tret = io_req_prep(req, sqe);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto fail_req;\n\t\t}\n\t\t__io_queue_sqe(req, cs);\n\t}\n}\n\nstatic inline void io_queue_link_head(struct io_kiocb *req,\n\t\t\t\t      struct io_comp_state *cs)\n{\n\tif (unlikely(req->flags & REQ_F_FAIL_LINK)) {\n\t\tio_put_req(req);\n\t\tio_req_complete(req, -ECANCELED);\n\t} else\n\t\tio_queue_sqe(req, NULL, cs);\n}\n\nstruct io_submit_link {\n\tstruct io_kiocb *head;\n\tstruct io_kiocb *last;\n};\n\nstatic int io_submit_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,\n\t\t\t struct io_submit_link *link, struct io_comp_state *cs)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\t/*\n\t * If we already have a head request, queue this one for async\n\t * submittal once the head completes. If we don't have a head but\n\t * IOSQE_IO_LINK is set in the sqe, start a new head. This one will be\n\t * submitted sync once the chain is complete. If none of those\n\t * conditions are true (normal request), then just queue it.\n\t */\n\tif (link->head) {\n\t\tstruct io_kiocb *head = link->head;\n\n\t\t/*\n\t\t * Taking sequential execution of a link, draining both sides\n\t\t * of the link also fullfils IOSQE_IO_DRAIN semantics for all\n\t\t * requests in the link. So, it drains the head and the\n\t\t * next after the link request. The last one is done via\n\t\t * drain_next flag to persist the effect across calls.\n\t\t */\n\t\tif (req->flags & REQ_F_IO_DRAIN) {\n\t\t\thead->flags |= REQ_F_IO_DRAIN;\n\t\t\tctx->drain_next = 1;\n\t\t}\n\t\tret = io_req_defer_prep(req, sqe);\n\t\tif (unlikely(ret)) {\n\t\t\t/* fail even hard links since we don't submit */\n\t\t\thead->flags |= REQ_F_FAIL_LINK;\n\t\t\treturn ret;\n\t\t}\n\t\ttrace_io_uring_link(ctx, req, head);\n\t\tlink->last->link = req;\n\t\tlink->last = req;\n\n\t\t/* last request of a link, enqueue the link */\n\t\tif (!(req->flags & (REQ_F_LINK | REQ_F_HARDLINK))) {\n\t\t\tio_queue_link_head(head, cs);\n\t\t\tlink->head = NULL;\n\t\t}\n\t} else {\n\t\tif (unlikely(ctx->drain_next)) {\n\t\t\treq->flags |= REQ_F_IO_DRAIN;\n\t\t\tctx->drain_next = 0;\n\t\t}\n\t\tif (req->flags & (REQ_F_LINK | REQ_F_HARDLINK)) {\n\t\t\tret = io_req_defer_prep(req, sqe);\n\t\t\tif (unlikely(ret))\n\t\t\t\treq->flags |= REQ_F_FAIL_LINK;\n\t\t\tlink->head = req;\n\t\t\tlink->last = req;\n\t\t} else {\n\t\t\tio_queue_sqe(req, sqe, cs);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n * Batched submission is done, ensure local IO is flushed out.\n */\nstatic void io_submit_state_end(struct io_submit_state *state)\n{\n\tif (!list_empty(&state->comp.list))\n\t\tio_submit_flush_completions(&state->comp);\n\tif (state->plug_started)\n\t\tblk_finish_plug(&state->plug);\n\tio_state_file_put(state);\n\tif (state->free_reqs)\n\t\tkmem_cache_free_bulk(req_cachep, state->free_reqs, state->reqs);\n}\n\n/*\n * Start submission side cache.\n */\nstatic void io_submit_state_start(struct io_submit_state *state,\n\t\t\t\t  struct io_ring_ctx *ctx, unsigned int max_ios)\n{\n\tstate->plug_started = false;\n\tstate->comp.nr = 0;\n\tINIT_LIST_HEAD(&state->comp.list);\n\tstate->comp.ctx = ctx;\n\tstate->free_reqs = 0;\n\tstate->file = NULL;\n\tstate->ios_left = max_ios;\n}\n\nstatic void io_commit_sqring(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *rings = ctx->rings;\n\n\t/*\n\t * Ensure any loads from the SQEs are done at this point,\n\t * since once we write the new head, the application could\n\t * write new data to them.\n\t */\n\tsmp_store_release(&rings->sq.head, ctx->cached_sq_head);\n}\n\n/*\n * Fetch an sqe, if one is available. Note that sqe_ptr will point to memory\n * that is mapped by userspace. This means that care needs to be taken to\n * ensure that reads are stable, as we cannot rely on userspace always\n * being a good citizen. If members of the sqe are validated and then later\n * used, it's important that those reads are done through READ_ONCE() to\n * prevent a re-load down the line.\n */\nstatic const struct io_uring_sqe *io_get_sqe(struct io_ring_ctx *ctx)\n{\n\tu32 *sq_array = ctx->sq_array;\n\tunsigned head;\n\n\t/*\n\t * The cached sq head (or cq tail) serves two purposes:\n\t *\n\t * 1) allows us to batch the cost of updating the user visible\n\t *    head updates.\n\t * 2) allows the kernel side to track the head on its own, even\n\t *    though the application is the one updating it.\n\t */\n\thead = READ_ONCE(sq_array[ctx->cached_sq_head & ctx->sq_mask]);\n\tif (likely(head < ctx->sq_entries))\n\t\treturn &ctx->sq_sqes[head];\n\n\t/* drop invalid entries */\n\tctx->cached_sq_dropped++;\n\tWRITE_ONCE(ctx->rings->sq_dropped, ctx->cached_sq_dropped);\n\treturn NULL;\n}\n\nstatic inline void io_consume_sqe(struct io_ring_ctx *ctx)\n{\n\tctx->cached_sq_head++;\n}\n\n/*\n * Check SQE restrictions (opcode and flags).\n *\n * Returns 'true' if SQE is allowed, 'false' otherwise.\n */\nstatic inline bool io_check_restriction(struct io_ring_ctx *ctx,\n\t\t\t\t\tstruct io_kiocb *req,\n\t\t\t\t\tunsigned int sqe_flags)\n{\n\tif (!ctx->restricted)\n\t\treturn true;\n\n\tif (!test_bit(req->opcode, ctx->restrictions.sqe_op))\n\t\treturn false;\n\n\tif ((sqe_flags & ctx->restrictions.sqe_flags_required) !=\n\t    ctx->restrictions.sqe_flags_required)\n\t\treturn false;\n\n\tif (sqe_flags & ~(ctx->restrictions.sqe_flags_allowed |\n\t\t\t  ctx->restrictions.sqe_flags_required))\n\t\treturn false;\n\n\treturn true;\n}\n\n#define SQE_VALID_FLAGS\t(IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK|\t\\\n\t\t\t\tIOSQE_IO_HARDLINK | IOSQE_ASYNC | \\\n\t\t\t\tIOSQE_BUFFER_SELECT)\n\nstatic int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,\n\t\t       const struct io_uring_sqe *sqe,\n\t\t       struct io_submit_state *state)\n{\n\tunsigned int sqe_flags;\n\tint id, ret;\n\n\treq->opcode = READ_ONCE(sqe->opcode);\n\treq->user_data = READ_ONCE(sqe->user_data);\n\treq->async_data = NULL;\n\treq->file = NULL;\n\treq->ctx = ctx;\n\treq->flags = 0;\n\treq->link = NULL;\n\treq->fixed_file_refs = NULL;\n\t/* one is dropped after submission, the other at completion */\n\trefcount_set(&req->refs, 2);\n\treq->task = current;\n\treq->result = 0;\n\n\tif (unlikely(req->opcode >= IORING_OP_LAST))\n\t\treturn -EINVAL;\n\n\tif (unlikely(io_sq_thread_acquire_mm_files(ctx, req)))\n\t\treturn -EFAULT;\n\n\tsqe_flags = READ_ONCE(sqe->flags);\n\t/* enforce forwards compatibility on users */\n\tif (unlikely(sqe_flags & ~SQE_VALID_FLAGS))\n\t\treturn -EINVAL;\n\n\tif (unlikely(!io_check_restriction(ctx, req, sqe_flags)))\n\t\treturn -EACCES;\n\n\tif ((sqe_flags & IOSQE_BUFFER_SELECT) &&\n\t    !io_op_defs[req->opcode].buffer_select)\n\t\treturn -EOPNOTSUPP;\n\n\tid = READ_ONCE(sqe->personality);\n\tif (id) {\n\t\tstruct io_identity *iod;\n\n\t\tiod = idr_find(&ctx->personality_idr, id);\n\t\tif (unlikely(!iod))\n\t\t\treturn -EINVAL;\n\t\trefcount_inc(&iod->count);\n\n\t\t__io_req_init_async(req);\n\t\tget_cred(iod->creds);\n\t\treq->work.identity = iod;\n\t\treq->work.flags |= IO_WQ_WORK_CREDS;\n\t}\n\n\t/* same numerical values with corresponding REQ_F_*, safe to copy */\n\treq->flags |= sqe_flags;\n\n\t/*\n\t * Plug now if we have more than 1 IO left after this, and the target\n\t * is potentially a read/write to block based storage.\n\t */\n\tif (!state->plug_started && state->ios_left > 1 &&\n\t    io_op_defs[req->opcode].plug) {\n\t\tblk_start_plug(&state->plug);\n\t\tstate->plug_started = true;\n\t}\n\n\tif (!io_op_defs[req->opcode].needs_file)\n\t\treturn 0;\n\n\tret = io_req_set_file(state, req, READ_ONCE(sqe->fd));\n\tstate->ios_left--;\n\treturn ret;\n}\n\nstatic int io_submit_sqes(struct io_ring_ctx *ctx, unsigned int nr)\n{\n\tstruct io_submit_state state;\n\tstruct io_submit_link link;\n\tint i, submitted = 0;\n\n\t/* if we have a backlog and couldn't flush it all, return BUSY */\n\tif (test_bit(0, &ctx->sq_check_overflow)) {\n\t\tif (!list_empty(&ctx->cq_overflow_list) &&\n\t\t    !io_cqring_overflow_flush(ctx, false, NULL, NULL))\n\t\t\treturn -EBUSY;\n\t}\n\n\t/* make sure SQ entry isn't read before tail */\n\tnr = min3(nr, ctx->sq_entries, io_sqring_entries(ctx));\n\n\tif (!percpu_ref_tryget_many(&ctx->refs, nr))\n\t\treturn -EAGAIN;\n\n\tpercpu_counter_add(&current->io_uring->inflight, nr);\n\trefcount_add(nr, &current->usage);\n\n\tio_submit_state_start(&state, ctx, nr);\n\tlink.head = NULL;\n\n\tfor (i = 0; i < nr; i++) {\n\t\tconst struct io_uring_sqe *sqe;\n\t\tstruct io_kiocb *req;\n\t\tint err;\n\n\t\tsqe = io_get_sqe(ctx);\n\t\tif (unlikely(!sqe)) {\n\t\t\tio_consume_sqe(ctx);\n\t\t\tbreak;\n\t\t}\n\t\treq = io_alloc_req(ctx, &state);\n\t\tif (unlikely(!req)) {\n\t\t\tif (!submitted)\n\t\t\t\tsubmitted = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tio_consume_sqe(ctx);\n\t\t/* will complete beyond this point, count as submitted */\n\t\tsubmitted++;\n\n\t\terr = io_init_req(ctx, req, sqe, &state);\n\t\tif (unlikely(err)) {\nfail_req:\n\t\t\tio_put_req(req);\n\t\t\tio_req_complete(req, err);\n\t\t\tbreak;\n\t\t}\n\n\t\ttrace_io_uring_submit_sqe(ctx, req->opcode, req->user_data,\n\t\t\t\t\t\ttrue, io_async_submit(ctx));\n\t\terr = io_submit_sqe(req, sqe, &link, &state.comp);\n\t\tif (err)\n\t\t\tgoto fail_req;\n\t}\n\n\tif (unlikely(submitted != nr)) {\n\t\tint ref_used = (submitted == -EAGAIN) ? 0 : submitted;\n\t\tstruct io_uring_task *tctx = current->io_uring;\n\t\tint unused = nr - ref_used;\n\n\t\tpercpu_ref_put_many(&ctx->refs, unused);\n\t\tpercpu_counter_sub(&tctx->inflight, unused);\n\t\tput_task_struct_many(current, unused);\n\t}\n\tif (link.head)\n\t\tio_queue_link_head(link.head, &state.comp);\n\tio_submit_state_end(&state);\n\n\t /* Commit SQ ring head once we've consumed and submitted all SQEs */\n\tio_commit_sqring(ctx);\n\n\treturn submitted;\n}\n\nstatic inline void io_ring_set_wakeup_flag(struct io_ring_ctx *ctx)\n{\n\t/* Tell userspace we may need a wakeup call */\n\tspin_lock_irq(&ctx->completion_lock);\n\tctx->rings->sq_flags |= IORING_SQ_NEED_WAKEUP;\n\tspin_unlock_irq(&ctx->completion_lock);\n}\n\nstatic inline void io_ring_clear_wakeup_flag(struct io_ring_ctx *ctx)\n{\n\tspin_lock_irq(&ctx->completion_lock);\n\tctx->rings->sq_flags &= ~IORING_SQ_NEED_WAKEUP;\n\tspin_unlock_irq(&ctx->completion_lock);\n}\n\nstatic int __io_sq_thread(struct io_ring_ctx *ctx, bool cap_entries)\n{\n\tunsigned int to_submit;\n\tint ret = 0;\n\n\tto_submit = io_sqring_entries(ctx);\n\t/* if we're handling multiple rings, cap submit size for fairness */\n\tif (cap_entries && to_submit > 8)\n\t\tto_submit = 8;\n\n\tif (!list_empty(&ctx->iopoll_list) || to_submit) {\n\t\tunsigned nr_events = 0;\n\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tif (!list_empty(&ctx->iopoll_list))\n\t\t\tio_do_iopoll(ctx, &nr_events, 0);\n\n\t\tif (to_submit && likely(!percpu_ref_is_dying(&ctx->refs)))\n\t\t\tret = io_submit_sqes(ctx, to_submit);\n\t\tmutex_unlock(&ctx->uring_lock);\n\t}\n\n\tif (!io_sqring_full(ctx) && wq_has_sleeper(&ctx->sqo_sq_wait))\n\t\twake_up(&ctx->sqo_sq_wait);\n\n\treturn ret;\n}\n\nstatic void io_sqd_update_thread_idle(struct io_sq_data *sqd)\n{\n\tstruct io_ring_ctx *ctx;\n\tunsigned sq_thread_idle = 0;\n\n\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list) {\n\t\tif (sq_thread_idle < ctx->sq_thread_idle)\n\t\t\tsq_thread_idle = ctx->sq_thread_idle;\n\t}\n\n\tsqd->sq_thread_idle = sq_thread_idle;\n}\n\nstatic void io_sqd_init_new(struct io_sq_data *sqd)\n{\n\tstruct io_ring_ctx *ctx;\n\n\twhile (!list_empty(&sqd->ctx_new_list)) {\n\t\tctx = list_first_entry(&sqd->ctx_new_list, struct io_ring_ctx, sqd_list);\n\t\tlist_move_tail(&ctx->sqd_list, &sqd->ctx_list);\n\t\tcomplete(&ctx->sq_thread_comp);\n\t}\n\n\tio_sqd_update_thread_idle(sqd);\n}\n\nstatic int io_sq_thread(void *data)\n{\n\tstruct cgroup_subsys_state *cur_css = NULL;\n\tstruct files_struct *old_files = current->files;\n\tstruct nsproxy *old_nsproxy = current->nsproxy;\n\tstruct pid *old_thread_pid = current->thread_pid;\n\tconst struct cred *old_cred = NULL;\n\tstruct io_sq_data *sqd = data;\n\tstruct io_ring_ctx *ctx;\n\tunsigned long timeout = 0;\n\tDEFINE_WAIT(wait);\n\n\ttask_lock(current);\n\tcurrent->files = NULL;\n\tcurrent->nsproxy = NULL;\n\tcurrent->thread_pid = NULL;\n\ttask_unlock(current);\n\n\twhile (!kthread_should_stop()) {\n\t\tint ret;\n\t\tbool cap_entries, sqt_spin, needs_sched;\n\n\t\t/*\n\t\t * Any changes to the sqd lists are synchronized through the\n\t\t * kthread parking. This synchronizes the thread vs users,\n\t\t * the users are synchronized on the sqd->ctx_lock.\n\t\t */\n\t\tif (kthread_should_park()) {\n\t\t\tkthread_parkme();\n\t\t\t/*\n\t\t\t * When sq thread is unparked, in case the previous park operation\n\t\t\t * comes from io_put_sq_data(), which means that sq thread is going\n\t\t\t * to be stopped, so here needs to have a check.\n\t\t\t */\n\t\t\tif (kthread_should_stop())\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (unlikely(!list_empty(&sqd->ctx_new_list))) {\n\t\t\tio_sqd_init_new(sqd);\n\t\t\ttimeout = jiffies + sqd->sq_thread_idle;\n\t\t}\n\n\t\tsqt_spin = false;\n\t\tcap_entries = !list_is_singular(&sqd->ctx_list);\n\t\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list) {\n\t\t\tif (current->cred != ctx->creds) {\n\t\t\t\tif (old_cred)\n\t\t\t\t\trevert_creds(old_cred);\n\t\t\t\told_cred = override_creds(ctx->creds);\n\t\t\t}\n\t\t\tio_sq_thread_associate_blkcg(ctx, &cur_css);\n#ifdef CONFIG_AUDIT\n\t\t\tcurrent->loginuid = ctx->loginuid;\n\t\t\tcurrent->sessionid = ctx->sessionid;\n#endif\n\n\t\t\tret = __io_sq_thread(ctx, cap_entries);\n\t\t\tif (!sqt_spin && (ret > 0 || !list_empty(&ctx->iopoll_list)))\n\t\t\t\tsqt_spin = true;\n\n\t\t\tio_sq_thread_drop_mm_files();\n\t\t}\n\n\t\tif (sqt_spin || !time_after(jiffies, timeout)) {\n\t\t\tio_run_task_work();\n\t\t\tcond_resched();\n\t\t\tif (sqt_spin)\n\t\t\t\ttimeout = jiffies + sqd->sq_thread_idle;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (kthread_should_park())\n\t\t\tcontinue;\n\n\t\tneeds_sched = true;\n\t\tprepare_to_wait(&sqd->wait, &wait, TASK_INTERRUPTIBLE);\n\t\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list) {\n\t\t\tif ((ctx->flags & IORING_SETUP_IOPOLL) &&\n\t\t\t    !list_empty_careful(&ctx->iopoll_list)) {\n\t\t\t\tneeds_sched = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (io_sqring_entries(ctx)) {\n\t\t\t\tneeds_sched = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (needs_sched) {\n\t\t\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list)\n\t\t\t\tio_ring_set_wakeup_flag(ctx);\n\n\t\t\tschedule();\n\t\t\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list)\n\t\t\t\tio_ring_clear_wakeup_flag(ctx);\n\t\t}\n\n\t\tfinish_wait(&sqd->wait, &wait);\n\t\ttimeout = jiffies + sqd->sq_thread_idle;\n\t}\n\n\tio_run_task_work();\n\n\tif (cur_css)\n\t\tio_sq_thread_unassociate_blkcg();\n\tif (old_cred)\n\t\trevert_creds(old_cred);\n\n\ttask_lock(current);\n\tcurrent->files = old_files;\n\tcurrent->nsproxy = old_nsproxy;\n\tcurrent->thread_pid = old_thread_pid;\n\ttask_unlock(current);\n\n\tkthread_parkme();\n\n\treturn 0;\n}\n\nstruct io_wait_queue {\n\tstruct wait_queue_entry wq;\n\tstruct io_ring_ctx *ctx;\n\tunsigned to_wait;\n\tunsigned nr_timeouts;\n};\n\nstatic inline bool io_should_wake(struct io_wait_queue *iowq, bool noflush)\n{\n\tstruct io_ring_ctx *ctx = iowq->ctx;\n\n\t/*\n\t * Wake up if we have enough events, or if a timeout occurred since we\n\t * started waiting. For timeouts, we always want to return to userspace,\n\t * regardless of event count.\n\t */\n\treturn io_cqring_events(ctx, noflush) >= iowq->to_wait ||\n\t\t\tatomic_read(&ctx->cq_timeouts) != iowq->nr_timeouts;\n}\n\nstatic int io_wake_function(struct wait_queue_entry *curr, unsigned int mode,\n\t\t\t    int wake_flags, void *key)\n{\n\tstruct io_wait_queue *iowq = container_of(curr, struct io_wait_queue,\n\t\t\t\t\t\t\twq);\n\n\t/* use noflush == true, as we can't safely rely on locking context */\n\tif (!io_should_wake(iowq, true))\n\t\treturn -1;\n\n\treturn autoremove_wake_function(curr, mode, wake_flags, key);\n}\n\nstatic int io_run_task_work_sig(void)\n{\n\tif (io_run_task_work())\n\t\treturn 1;\n\tif (!signal_pending(current))\n\t\treturn 0;\n\tif (test_tsk_thread_flag(current, TIF_NOTIFY_SIGNAL))\n\t\treturn -ERESTARTSYS;\n\treturn -EINTR;\n}\n\n/*\n * Wait until events become available, if we don't already have some. The\n * application must reap them itself, as they reside on the shared cq ring.\n */\nstatic int io_cqring_wait(struct io_ring_ctx *ctx, int min_events,\n\t\t\t  const sigset_t __user *sig, size_t sigsz,\n\t\t\t  struct __kernel_timespec __user *uts)\n{\n\tstruct io_wait_queue iowq = {\n\t\t.wq = {\n\t\t\t.private\t= current,\n\t\t\t.func\t\t= io_wake_function,\n\t\t\t.entry\t\t= LIST_HEAD_INIT(iowq.wq.entry),\n\t\t},\n\t\t.ctx\t\t= ctx,\n\t\t.to_wait\t= min_events,\n\t};\n\tstruct io_rings *rings = ctx->rings;\n\tstruct timespec64 ts;\n\tsigned long timeout = 0;\n\tint ret = 0;\n\n\tdo {\n\t\tif (io_cqring_events(ctx, false) >= min_events)\n\t\t\treturn 0;\n\t\tif (!io_run_task_work())\n\t\t\tbreak;\n\t} while (1);\n\n\tif (sig) {\n#ifdef CONFIG_COMPAT\n\t\tif (in_compat_syscall())\n\t\t\tret = set_compat_user_sigmask((const compat_sigset_t __user *)sig,\n\t\t\t\t\t\t      sigsz);\n\t\telse\n#endif\n\t\t\tret = set_user_sigmask(sig, sigsz);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (uts) {\n\t\tif (get_timespec64(&ts, uts))\n\t\t\treturn -EFAULT;\n\t\ttimeout = timespec64_to_jiffies(&ts);\n\t}\n\n\tiowq.nr_timeouts = atomic_read(&ctx->cq_timeouts);\n\ttrace_io_uring_cqring_wait(ctx, min_events);\n\tdo {\n\t\tprepare_to_wait_exclusive(&ctx->wait, &iowq.wq,\n\t\t\t\t\t\tTASK_INTERRUPTIBLE);\n\t\t/* make sure we run task_work before checking for signals */\n\t\tret = io_run_task_work_sig();\n\t\tif (ret > 0)\n\t\t\tcontinue;\n\t\telse if (ret < 0)\n\t\t\tbreak;\n\t\tif (io_should_wake(&iowq, false))\n\t\t\tbreak;\n\t\tif (uts) {\n\t\t\ttimeout = schedule_timeout(timeout);\n\t\t\tif (timeout == 0) {\n\t\t\t\tret = -ETIME;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tschedule();\n\t\t}\n\t} while (1);\n\tfinish_wait(&ctx->wait, &iowq.wq);\n\n\trestore_saved_sigmask_unless(ret == -EINTR);\n\n\treturn READ_ONCE(rings->cq.head) == READ_ONCE(rings->cq.tail) ? ret : 0;\n}\n\nstatic void __io_sqe_files_unregister(struct io_ring_ctx *ctx)\n{\n#if defined(CONFIG_UNIX)\n\tif (ctx->ring_sock) {\n\t\tstruct sock *sock = ctx->ring_sock->sk;\n\t\tstruct sk_buff *skb;\n\n\t\twhile ((skb = skb_dequeue(&sock->sk_receive_queue)) != NULL)\n\t\t\tkfree_skb(skb);\n\t}\n#else\n\tint i;\n\n\tfor (i = 0; i < ctx->nr_user_files; i++) {\n\t\tstruct file *file;\n\n\t\tfile = io_file_from_index(ctx, i);\n\t\tif (file)\n\t\t\tfput(file);\n\t}\n#endif\n}\n\nstatic void io_file_ref_kill(struct percpu_ref *ref)\n{\n\tstruct fixed_file_data *data;\n\n\tdata = container_of(ref, struct fixed_file_data, refs);\n\tcomplete(&data->done);\n}\n\nstatic int io_sqe_files_unregister(struct io_ring_ctx *ctx)\n{\n\tstruct fixed_file_data *data = ctx->file_data;\n\tstruct fixed_file_ref_node *ref_node = NULL;\n\tunsigned nr_tables, i;\n\n\tif (!data)\n\t\treturn -ENXIO;\n\n\tspin_lock(&data->lock);\n\tref_node = data->node;\n\tspin_unlock(&data->lock);\n\tif (ref_node)\n\t\tpercpu_ref_kill(&ref_node->refs);\n\n\tpercpu_ref_kill(&data->refs);\n\n\t/* wait for all refs nodes to complete */\n\tflush_delayed_work(&ctx->file_put_work);\n\twait_for_completion(&data->done);\n\n\t__io_sqe_files_unregister(ctx);\n\tnr_tables = DIV_ROUND_UP(ctx->nr_user_files, IORING_MAX_FILES_TABLE);\n\tfor (i = 0; i < nr_tables; i++)\n\t\tkfree(data->table[i].files);\n\tkfree(data->table);\n\tpercpu_ref_exit(&data->refs);\n\tkfree(data);\n\tctx->file_data = NULL;\n\tctx->nr_user_files = 0;\n\treturn 0;\n}\n\nstatic void io_put_sq_data(struct io_sq_data *sqd)\n{\n\tif (refcount_dec_and_test(&sqd->refs)) {\n\t\t/*\n\t\t * The park is a bit of a work-around, without it we get\n\t\t * warning spews on shutdown with SQPOLL set and affinity\n\t\t * set to a single CPU.\n\t\t */\n\t\tif (sqd->thread) {\n\t\t\tkthread_park(sqd->thread);\n\t\t\tkthread_stop(sqd->thread);\n\t\t}\n\n\t\tkfree(sqd);\n\t}\n}\n\nstatic struct io_sq_data *io_attach_sq_data(struct io_uring_params *p)\n{\n\tstruct io_ring_ctx *ctx_attach;\n\tstruct io_sq_data *sqd;\n\tstruct fd f;\n\n\tf = fdget(p->wq_fd);\n\tif (!f.file)\n\t\treturn ERR_PTR(-ENXIO);\n\tif (f.file->f_op != &io_uring_fops) {\n\t\tfdput(f);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tctx_attach = f.file->private_data;\n\tsqd = ctx_attach->sq_data;\n\tif (!sqd) {\n\t\tfdput(f);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\trefcount_inc(&sqd->refs);\n\tfdput(f);\n\treturn sqd;\n}\n\nstatic struct io_sq_data *io_get_sq_data(struct io_uring_params *p)\n{\n\tstruct io_sq_data *sqd;\n\n\tif (p->flags & IORING_SETUP_ATTACH_WQ)\n\t\treturn io_attach_sq_data(p);\n\n\tsqd = kzalloc(sizeof(*sqd), GFP_KERNEL);\n\tif (!sqd)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\trefcount_set(&sqd->refs, 1);\n\tINIT_LIST_HEAD(&sqd->ctx_list);\n\tINIT_LIST_HEAD(&sqd->ctx_new_list);\n\tmutex_init(&sqd->ctx_lock);\n\tmutex_init(&sqd->lock);\n\tinit_waitqueue_head(&sqd->wait);\n\treturn sqd;\n}\n\nstatic void io_sq_thread_unpark(struct io_sq_data *sqd)\n\t__releases(&sqd->lock)\n{\n\tif (!sqd->thread)\n\t\treturn;\n\tkthread_unpark(sqd->thread);\n\tmutex_unlock(&sqd->lock);\n}\n\nstatic void io_sq_thread_park(struct io_sq_data *sqd)\n\t__acquires(&sqd->lock)\n{\n\tif (!sqd->thread)\n\t\treturn;\n\tmutex_lock(&sqd->lock);\n\tkthread_park(sqd->thread);\n}\n\nstatic void io_sq_thread_stop(struct io_ring_ctx *ctx)\n{\n\tstruct io_sq_data *sqd = ctx->sq_data;\n\n\tif (sqd) {\n\t\tif (sqd->thread) {\n\t\t\t/*\n\t\t\t * We may arrive here from the error branch in\n\t\t\t * io_sq_offload_create() where the kthread is created\n\t\t\t * without being waked up, thus wake it up now to make\n\t\t\t * sure the wait will complete.\n\t\t\t */\n\t\t\twake_up_process(sqd->thread);\n\t\t\twait_for_completion(&ctx->sq_thread_comp);\n\n\t\t\tio_sq_thread_park(sqd);\n\t\t}\n\n\t\tmutex_lock(&sqd->ctx_lock);\n\t\tlist_del(&ctx->sqd_list);\n\t\tio_sqd_update_thread_idle(sqd);\n\t\tmutex_unlock(&sqd->ctx_lock);\n\n\t\tif (sqd->thread)\n\t\t\tio_sq_thread_unpark(sqd);\n\n\t\tio_put_sq_data(sqd);\n\t\tctx->sq_data = NULL;\n\t}\n}\n\nstatic void io_finish_async(struct io_ring_ctx *ctx)\n{\n\tio_sq_thread_stop(ctx);\n\n\tif (ctx->io_wq) {\n\t\tio_wq_destroy(ctx->io_wq);\n\t\tctx->io_wq = NULL;\n\t}\n}\n\n#if defined(CONFIG_UNIX)\n/*\n * Ensure the UNIX gc is aware of our file set, so we are certain that\n * the io_uring can be safely unregistered on process exit, even if we have\n * loops in the file referencing.\n */\nstatic int __io_sqe_files_scm(struct io_ring_ctx *ctx, int nr, int offset)\n{\n\tstruct sock *sk = ctx->ring_sock->sk;\n\tstruct scm_fp_list *fpl;\n\tstruct sk_buff *skb;\n\tint i, nr_files;\n\n\tfpl = kzalloc(sizeof(*fpl), GFP_KERNEL);\n\tif (!fpl)\n\t\treturn -ENOMEM;\n\n\tskb = alloc_skb(0, GFP_KERNEL);\n\tif (!skb) {\n\t\tkfree(fpl);\n\t\treturn -ENOMEM;\n\t}\n\n\tskb->sk = sk;\n\n\tnr_files = 0;\n\tfpl->user = get_uid(ctx->user);\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct file *file = io_file_from_index(ctx, i + offset);\n\n\t\tif (!file)\n\t\t\tcontinue;\n\t\tfpl->fp[nr_files] = get_file(file);\n\t\tunix_inflight(fpl->user, fpl->fp[nr_files]);\n\t\tnr_files++;\n\t}\n\n\tif (nr_files) {\n\t\tfpl->max = SCM_MAX_FD;\n\t\tfpl->count = nr_files;\n\t\tUNIXCB(skb).fp = fpl;\n\t\tskb->destructor = unix_destruct_scm;\n\t\trefcount_add(skb->truesize, &sk->sk_wmem_alloc);\n\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\n\t\tfor (i = 0; i < nr_files; i++)\n\t\t\tfput(fpl->fp[i]);\n\t} else {\n\t\tkfree_skb(skb);\n\t\tkfree(fpl);\n\t}\n\n\treturn 0;\n}\n\n/*\n * If UNIX sockets are enabled, fd passing can cause a reference cycle which\n * causes regular reference counting to break down. We rely on the UNIX\n * garbage collection to take care of this problem for us.\n */\nstatic int io_sqe_files_scm(struct io_ring_ctx *ctx)\n{\n\tunsigned left, total;\n\tint ret = 0;\n\n\ttotal = 0;\n\tleft = ctx->nr_user_files;\n\twhile (left) {\n\t\tunsigned this_files = min_t(unsigned, left, SCM_MAX_FD);\n\n\t\tret = __io_sqe_files_scm(ctx, this_files, total);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tleft -= this_files;\n\t\ttotal += this_files;\n\t}\n\n\tif (!ret)\n\t\treturn 0;\n\n\twhile (total < ctx->nr_user_files) {\n\t\tstruct file *file = io_file_from_index(ctx, total);\n\n\t\tif (file)\n\t\t\tfput(file);\n\t\ttotal++;\n\t}\n\n\treturn ret;\n}\n#else\nstatic int io_sqe_files_scm(struct io_ring_ctx *ctx)\n{\n\treturn 0;\n}\n#endif\n\nstatic int io_sqe_alloc_file_tables(struct fixed_file_data *file_data,\n\t\t\t\t    unsigned nr_tables, unsigned nr_files)\n{\n\tint i;\n\n\tfor (i = 0; i < nr_tables; i++) {\n\t\tstruct fixed_file_table *table = &file_data->table[i];\n\t\tunsigned this_files;\n\n\t\tthis_files = min(nr_files, IORING_MAX_FILES_TABLE);\n\t\ttable->files = kcalloc(this_files, sizeof(struct file *),\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (!table->files)\n\t\t\tbreak;\n\t\tnr_files -= this_files;\n\t}\n\n\tif (i == nr_tables)\n\t\treturn 0;\n\n\tfor (i = 0; i < nr_tables; i++) {\n\t\tstruct fixed_file_table *table = &file_data->table[i];\n\t\tkfree(table->files);\n\t}\n\treturn 1;\n}\n\nstatic void io_ring_file_put(struct io_ring_ctx *ctx, struct file *file)\n{\n#if defined(CONFIG_UNIX)\n\tstruct sock *sock = ctx->ring_sock->sk;\n\tstruct sk_buff_head list, *head = &sock->sk_receive_queue;\n\tstruct sk_buff *skb;\n\tint i;\n\n\t__skb_queue_head_init(&list);\n\n\t/*\n\t * Find the skb that holds this file in its SCM_RIGHTS. When found,\n\t * remove this entry and rearrange the file array.\n\t */\n\tskb = skb_dequeue(head);\n\twhile (skb) {\n\t\tstruct scm_fp_list *fp;\n\n\t\tfp = UNIXCB(skb).fp;\n\t\tfor (i = 0; i < fp->count; i++) {\n\t\t\tint left;\n\n\t\t\tif (fp->fp[i] != file)\n\t\t\t\tcontinue;\n\n\t\t\tunix_notinflight(fp->user, fp->fp[i]);\n\t\t\tleft = fp->count - 1 - i;\n\t\t\tif (left) {\n\t\t\t\tmemmove(&fp->fp[i], &fp->fp[i + 1],\n\t\t\t\t\t\tleft * sizeof(struct file *));\n\t\t\t}\n\t\t\tfp->count--;\n\t\t\tif (!fp->count) {\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tskb = NULL;\n\t\t\t} else {\n\t\t\t\t__skb_queue_tail(&list, skb);\n\t\t\t}\n\t\t\tfput(file);\n\t\t\tfile = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!file)\n\t\t\tbreak;\n\n\t\t__skb_queue_tail(&list, skb);\n\n\t\tskb = skb_dequeue(head);\n\t}\n\n\tif (skb_peek(&list)) {\n\t\tspin_lock_irq(&head->lock);\n\t\twhile ((skb = __skb_dequeue(&list)) != NULL)\n\t\t\t__skb_queue_tail(head, skb);\n\t\tspin_unlock_irq(&head->lock);\n\t}\n#else\n\tfput(file);\n#endif\n}\n\nstruct io_file_put {\n\tstruct list_head list;\n\tstruct file *file;\n};\n\nstatic void __io_file_put_work(struct fixed_file_ref_node *ref_node)\n{\n\tstruct fixed_file_data *file_data = ref_node->file_data;\n\tstruct io_ring_ctx *ctx = file_data->ctx;\n\tstruct io_file_put *pfile, *tmp;\n\n\tlist_for_each_entry_safe(pfile, tmp, &ref_node->file_list, list) {\n\t\tlist_del(&pfile->list);\n\t\tio_ring_file_put(ctx, pfile->file);\n\t\tkfree(pfile);\n\t}\n\n\tpercpu_ref_exit(&ref_node->refs);\n\tkfree(ref_node);\n\tpercpu_ref_put(&file_data->refs);\n}\n\nstatic void io_file_put_work(struct work_struct *work)\n{\n\tstruct io_ring_ctx *ctx;\n\tstruct llist_node *node;\n\n\tctx = container_of(work, struct io_ring_ctx, file_put_work.work);\n\tnode = llist_del_all(&ctx->file_put_llist);\n\n\twhile (node) {\n\t\tstruct fixed_file_ref_node *ref_node;\n\t\tstruct llist_node *next = node->next;\n\n\t\tref_node = llist_entry(node, struct fixed_file_ref_node, llist);\n\t\t__io_file_put_work(ref_node);\n\t\tnode = next;\n\t}\n}\n\nstatic void io_file_data_ref_zero(struct percpu_ref *ref)\n{\n\tstruct fixed_file_ref_node *ref_node;\n\tstruct fixed_file_data *data;\n\tstruct io_ring_ctx *ctx;\n\tbool first_add = false;\n\tint delay = HZ;\n\n\tref_node = container_of(ref, struct fixed_file_ref_node, refs);\n\tdata = ref_node->file_data;\n\tctx = data->ctx;\n\n\tspin_lock(&data->lock);\n\tref_node->done = true;\n\n\twhile (!list_empty(&data->ref_list)) {\n\t\tref_node = list_first_entry(&data->ref_list,\n\t\t\t\t\tstruct fixed_file_ref_node, node);\n\t\t/* recycle ref nodes in order */\n\t\tif (!ref_node->done)\n\t\t\tbreak;\n\t\tlist_del(&ref_node->node);\n\t\tfirst_add |= llist_add(&ref_node->llist, &ctx->file_put_llist);\n\t}\n\tspin_unlock(&data->lock);\n\n\tif (percpu_ref_is_dying(&data->refs))\n\t\tdelay = 0;\n\n\tif (!delay)\n\t\tmod_delayed_work(system_wq, &ctx->file_put_work, 0);\n\telse if (first_add)\n\t\tqueue_delayed_work(system_wq, &ctx->file_put_work, delay);\n}\n\nstatic struct fixed_file_ref_node *alloc_fixed_file_ref_node(\n\t\t\tstruct io_ring_ctx *ctx)\n{\n\tstruct fixed_file_ref_node *ref_node;\n\n\tref_node = kzalloc(sizeof(*ref_node), GFP_KERNEL);\n\tif (!ref_node)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (percpu_ref_init(&ref_node->refs, io_file_data_ref_zero,\n\t\t\t    0, GFP_KERNEL)) {\n\t\tkfree(ref_node);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tINIT_LIST_HEAD(&ref_node->node);\n\tINIT_LIST_HEAD(&ref_node->file_list);\n\tref_node->file_data = ctx->file_data;\n\tref_node->done = false;\n\treturn ref_node;\n}\n\nstatic void destroy_fixed_file_ref_node(struct fixed_file_ref_node *ref_node)\n{\n\tpercpu_ref_exit(&ref_node->refs);\n\tkfree(ref_node);\n}\n\nstatic int io_sqe_files_register(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t\t unsigned nr_args)\n{\n\t__s32 __user *fds = (__s32 __user *) arg;\n\tunsigned nr_tables, i;\n\tstruct file *file;\n\tint fd, ret = -ENOMEM;\n\tstruct fixed_file_ref_node *ref_node;\n\tstruct fixed_file_data *file_data;\n\n\tif (ctx->file_data)\n\t\treturn -EBUSY;\n\tif (!nr_args)\n\t\treturn -EINVAL;\n\tif (nr_args > IORING_MAX_FIXED_FILES)\n\t\treturn -EMFILE;\n\n\tfile_data = kzalloc(sizeof(*ctx->file_data), GFP_KERNEL);\n\tif (!file_data)\n\t\treturn -ENOMEM;\n\tfile_data->ctx = ctx;\n\tinit_completion(&file_data->done);\n\tINIT_LIST_HEAD(&file_data->ref_list);\n\tspin_lock_init(&file_data->lock);\n\n\tnr_tables = DIV_ROUND_UP(nr_args, IORING_MAX_FILES_TABLE);\n\tfile_data->table = kcalloc(nr_tables, sizeof(*file_data->table),\n\t\t\t\t   GFP_KERNEL);\n\tif (!file_data->table)\n\t\tgoto out_free;\n\n\tif (percpu_ref_init(&file_data->refs, io_file_ref_kill,\n\t\t\t\tPERCPU_REF_ALLOW_REINIT, GFP_KERNEL))\n\t\tgoto out_free;\n\n\tif (io_sqe_alloc_file_tables(file_data, nr_tables, nr_args))\n\t\tgoto out_ref;\n\tctx->file_data = file_data;\n\n\tfor (i = 0; i < nr_args; i++, ctx->nr_user_files++) {\n\t\tstruct fixed_file_table *table;\n\t\tunsigned index;\n\n\t\tif (copy_from_user(&fd, &fds[i], sizeof(fd))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out_fput;\n\t\t}\n\t\t/* allow sparse sets */\n\t\tif (fd == -1)\n\t\t\tcontinue;\n\n\t\tfile = fget(fd);\n\t\tret = -EBADF;\n\t\tif (!file)\n\t\t\tgoto out_fput;\n\n\t\t/*\n\t\t * Don't allow io_uring instances to be registered. If UNIX\n\t\t * isn't enabled, then this causes a reference cycle and this\n\t\t * instance can never get freed. If UNIX is enabled we'll\n\t\t * handle it just fine, but there's still no point in allowing\n\t\t * a ring fd as it doesn't support regular read/write anyway.\n\t\t */\n\t\tif (file->f_op == &io_uring_fops) {\n\t\t\tfput(file);\n\t\t\tgoto out_fput;\n\t\t}\n\t\ttable = &file_data->table[i >> IORING_FILE_TABLE_SHIFT];\n\t\tindex = i & IORING_FILE_TABLE_MASK;\n\t\ttable->files[index] = file;\n\t}\n\n\tret = io_sqe_files_scm(ctx);\n\tif (ret) {\n\t\tio_sqe_files_unregister(ctx);\n\t\treturn ret;\n\t}\n\n\tref_node = alloc_fixed_file_ref_node(ctx);\n\tif (IS_ERR(ref_node)) {\n\t\tio_sqe_files_unregister(ctx);\n\t\treturn PTR_ERR(ref_node);\n\t}\n\n\tfile_data->node = ref_node;\n\tspin_lock(&file_data->lock);\n\tlist_add_tail(&ref_node->node, &file_data->ref_list);\n\tspin_unlock(&file_data->lock);\n\tpercpu_ref_get(&file_data->refs);\n\treturn ret;\nout_fput:\n\tfor (i = 0; i < ctx->nr_user_files; i++) {\n\t\tfile = io_file_from_index(ctx, i);\n\t\tif (file)\n\t\t\tfput(file);\n\t}\n\tfor (i = 0; i < nr_tables; i++)\n\t\tkfree(file_data->table[i].files);\n\tctx->nr_user_files = 0;\nout_ref:\n\tpercpu_ref_exit(&file_data->refs);\nout_free:\n\tkfree(file_data->table);\n\tkfree(file_data);\n\tctx->file_data = NULL;\n\treturn ret;\n}\n\nstatic int io_sqe_file_register(struct io_ring_ctx *ctx, struct file *file,\n\t\t\t\tint index)\n{\n#if defined(CONFIG_UNIX)\n\tstruct sock *sock = ctx->ring_sock->sk;\n\tstruct sk_buff_head *head = &sock->sk_receive_queue;\n\tstruct sk_buff *skb;\n\n\t/*\n\t * See if we can merge this file into an existing skb SCM_RIGHTS\n\t * file set. If there's no room, fall back to allocating a new skb\n\t * and filling it in.\n\t */\n\tspin_lock_irq(&head->lock);\n\tskb = skb_peek(head);\n\tif (skb) {\n\t\tstruct scm_fp_list *fpl = UNIXCB(skb).fp;\n\n\t\tif (fpl->count < SCM_MAX_FD) {\n\t\t\t__skb_unlink(skb, head);\n\t\t\tspin_unlock_irq(&head->lock);\n\t\t\tfpl->fp[fpl->count] = get_file(file);\n\t\t\tunix_inflight(fpl->user, fpl->fp[fpl->count]);\n\t\t\tfpl->count++;\n\t\t\tspin_lock_irq(&head->lock);\n\t\t\t__skb_queue_head(head, skb);\n\t\t} else {\n\t\t\tskb = NULL;\n\t\t}\n\t}\n\tspin_unlock_irq(&head->lock);\n\n\tif (skb) {\n\t\tfput(file);\n\t\treturn 0;\n\t}\n\n\treturn __io_sqe_files_scm(ctx, 1, index);\n#else\n\treturn 0;\n#endif\n}\n\nstatic int io_queue_file_removal(struct fixed_file_data *data,\n\t\t\t\t struct file *file)\n{\n\tstruct io_file_put *pfile;\n\tstruct fixed_file_ref_node *ref_node = data->node;\n\n\tpfile = kzalloc(sizeof(*pfile), GFP_KERNEL);\n\tif (!pfile)\n\t\treturn -ENOMEM;\n\n\tpfile->file = file;\n\tlist_add(&pfile->list, &ref_node->file_list);\n\n\treturn 0;\n}\n\nstatic int __io_sqe_files_update(struct io_ring_ctx *ctx,\n\t\t\t\t struct io_uring_files_update *up,\n\t\t\t\t unsigned nr_args)\n{\n\tstruct fixed_file_data *data = ctx->file_data;\n\tstruct fixed_file_ref_node *ref_node;\n\tstruct file *file;\n\t__s32 __user *fds;\n\tint fd, i, err;\n\t__u32 done;\n\tbool needs_switch = false;\n\n\tif (check_add_overflow(up->offset, nr_args, &done))\n\t\treturn -EOVERFLOW;\n\tif (done > ctx->nr_user_files)\n\t\treturn -EINVAL;\n\n\tref_node = alloc_fixed_file_ref_node(ctx);\n\tif (IS_ERR(ref_node))\n\t\treturn PTR_ERR(ref_node);\n\n\tdone = 0;\n\tfds = u64_to_user_ptr(up->fds);\n\twhile (nr_args) {\n\t\tstruct fixed_file_table *table;\n\t\tunsigned index;\n\n\t\terr = 0;\n\t\tif (copy_from_user(&fd, &fds[done], sizeof(fd))) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\ti = array_index_nospec(up->offset, ctx->nr_user_files);\n\t\ttable = &ctx->file_data->table[i >> IORING_FILE_TABLE_SHIFT];\n\t\tindex = i & IORING_FILE_TABLE_MASK;\n\t\tif (table->files[index]) {\n\t\t\tfile = table->files[index];\n\t\t\terr = io_queue_file_removal(data, file);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\ttable->files[index] = NULL;\n\t\t\tneeds_switch = true;\n\t\t}\n\t\tif (fd != -1) {\n\t\t\tfile = fget(fd);\n\t\t\tif (!file) {\n\t\t\t\terr = -EBADF;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Don't allow io_uring instances to be registered. If\n\t\t\t * UNIX isn't enabled, then this causes a reference\n\t\t\t * cycle and this instance can never get freed. If UNIX\n\t\t\t * is enabled we'll handle it just fine, but there's\n\t\t\t * still no point in allowing a ring fd as it doesn't\n\t\t\t * support regular read/write anyway.\n\t\t\t */\n\t\t\tif (file->f_op == &io_uring_fops) {\n\t\t\t\tfput(file);\n\t\t\t\terr = -EBADF;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttable->files[index] = file;\n\t\t\terr = io_sqe_file_register(ctx, file, i);\n\t\t\tif (err) {\n\t\t\t\ttable->files[index] = NULL;\n\t\t\t\tfput(file);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tnr_args--;\n\t\tdone++;\n\t\tup->offset++;\n\t}\n\n\tif (needs_switch) {\n\t\tpercpu_ref_kill(&data->node->refs);\n\t\tspin_lock(&data->lock);\n\t\tlist_add_tail(&ref_node->node, &data->ref_list);\n\t\tdata->node = ref_node;\n\t\tspin_unlock(&data->lock);\n\t\tpercpu_ref_get(&ctx->file_data->refs);\n\t} else\n\t\tdestroy_fixed_file_ref_node(ref_node);\n\n\treturn done ? done : err;\n}\n\nstatic int io_sqe_files_update(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t       unsigned nr_args)\n{\n\tstruct io_uring_files_update up;\n\n\tif (!ctx->file_data)\n\t\treturn -ENXIO;\n\tif (!nr_args)\n\t\treturn -EINVAL;\n\tif (copy_from_user(&up, arg, sizeof(up)))\n\t\treturn -EFAULT;\n\tif (up.resv)\n\t\treturn -EINVAL;\n\n\treturn __io_sqe_files_update(ctx, &up, nr_args);\n}\n\nstatic void io_free_work(struct io_wq_work *work)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\n\t/* Consider that io_steal_work() relies on this ref */\n\tio_put_req(req);\n}\n\nstatic int io_init_wq_offload(struct io_ring_ctx *ctx,\n\t\t\t      struct io_uring_params *p)\n{\n\tstruct io_wq_data data;\n\tstruct fd f;\n\tstruct io_ring_ctx *ctx_attach;\n\tunsigned int concurrency;\n\tint ret = 0;\n\n\tdata.user = ctx->user;\n\tdata.free_work = io_free_work;\n\tdata.do_work = io_wq_submit_work;\n\n\tif (!(p->flags & IORING_SETUP_ATTACH_WQ)) {\n\t\t/* Do QD, or 4 * CPUS, whatever is smallest */\n\t\tconcurrency = min(ctx->sq_entries, 4 * num_online_cpus());\n\n\t\tctx->io_wq = io_wq_create(concurrency, &data);\n\t\tif (IS_ERR(ctx->io_wq)) {\n\t\t\tret = PTR_ERR(ctx->io_wq);\n\t\t\tctx->io_wq = NULL;\n\t\t}\n\t\treturn ret;\n\t}\n\n\tf = fdget(p->wq_fd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tif (f.file->f_op != &io_uring_fops) {\n\t\tret = -EINVAL;\n\t\tgoto out_fput;\n\t}\n\n\tctx_attach = f.file->private_data;\n\t/* @io_wq is protected by holding the fd */\n\tif (!io_wq_get(ctx_attach->io_wq, &data)) {\n\t\tret = -EINVAL;\n\t\tgoto out_fput;\n\t}\n\n\tctx->io_wq = ctx_attach->io_wq;\nout_fput:\n\tfdput(f);\n\treturn ret;\n}\n\nstatic int io_uring_alloc_task_context(struct task_struct *task)\n{\n\tstruct io_uring_task *tctx;\n\tint ret;\n\n\ttctx = kmalloc(sizeof(*tctx), GFP_KERNEL);\n\tif (unlikely(!tctx))\n\t\treturn -ENOMEM;\n\n\tret = percpu_counter_init(&tctx->inflight, 0, GFP_KERNEL);\n\tif (unlikely(ret)) {\n\t\tkfree(tctx);\n\t\treturn ret;\n\t}\n\n\txa_init(&tctx->xa);\n\tinit_waitqueue_head(&tctx->wait);\n\ttctx->last = NULL;\n\tatomic_set(&tctx->in_idle, 0);\n\ttctx->sqpoll = false;\n\tio_init_identity(&tctx->__identity);\n\ttctx->identity = &tctx->__identity;\n\ttask->io_uring = tctx;\n\treturn 0;\n}\n\nvoid __io_uring_free(struct task_struct *tsk)\n{\n\tstruct io_uring_task *tctx = tsk->io_uring;\n\n\tWARN_ON_ONCE(!xa_empty(&tctx->xa));\n\tWARN_ON_ONCE(refcount_read(&tctx->identity->count) != 1);\n\tif (tctx->identity != &tctx->__identity)\n\t\tkfree(tctx->identity);\n\tpercpu_counter_destroy(&tctx->inflight);\n\tkfree(tctx);\n\ttsk->io_uring = NULL;\n}\n\nstatic int io_sq_offload_create(struct io_ring_ctx *ctx,\n\t\t\t\tstruct io_uring_params *p)\n{\n\tint ret;\n\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\tstruct io_sq_data *sqd;\n\n\t\tret = -EPERM;\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_NICE))\n\t\t\tgoto err;\n\n\t\tsqd = io_get_sq_data(p);\n\t\tif (IS_ERR(sqd)) {\n\t\t\tret = PTR_ERR(sqd);\n\t\t\tgoto err;\n\t\t}\n\n\t\tctx->sq_data = sqd;\n\t\tio_sq_thread_park(sqd);\n\t\tmutex_lock(&sqd->ctx_lock);\n\t\tlist_add(&ctx->sqd_list, &sqd->ctx_new_list);\n\t\tmutex_unlock(&sqd->ctx_lock);\n\t\tio_sq_thread_unpark(sqd);\n\n\t\tctx->sq_thread_idle = msecs_to_jiffies(p->sq_thread_idle);\n\t\tif (!ctx->sq_thread_idle)\n\t\t\tctx->sq_thread_idle = HZ;\n\n\t\tif (sqd->thread)\n\t\t\tgoto done;\n\n\t\tif (p->flags & IORING_SETUP_SQ_AFF) {\n\t\t\tint cpu = p->sq_thread_cpu;\n\n\t\t\tret = -EINVAL;\n\t\t\tif (cpu >= nr_cpu_ids)\n\t\t\t\tgoto err;\n\t\t\tif (!cpu_online(cpu))\n\t\t\t\tgoto err;\n\n\t\t\tsqd->thread = kthread_create_on_cpu(io_sq_thread, sqd,\n\t\t\t\t\t\t\tcpu, \"io_uring-sq\");\n\t\t} else {\n\t\t\tsqd->thread = kthread_create(io_sq_thread, sqd,\n\t\t\t\t\t\t\t\"io_uring-sq\");\n\t\t}\n\t\tif (IS_ERR(sqd->thread)) {\n\t\t\tret = PTR_ERR(sqd->thread);\n\t\t\tsqd->thread = NULL;\n\t\t\tgoto err;\n\t\t}\n\t\tret = io_uring_alloc_task_context(sqd->thread);\n\t\tif (ret)\n\t\t\tgoto err;\n\t} else if (p->flags & IORING_SETUP_SQ_AFF) {\n\t\t/* Can't have SQ_AFF without SQPOLL */\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\ndone:\n\tret = io_init_wq_offload(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\tio_finish_async(ctx);\n\treturn ret;\n}\n\nstatic void io_sq_offload_start(struct io_ring_ctx *ctx)\n{\n\tstruct io_sq_data *sqd = ctx->sq_data;\n\n\tif ((ctx->flags & IORING_SETUP_SQPOLL) && sqd->thread)\n\t\twake_up_process(sqd->thread);\n}\n\nstatic inline void __io_unaccount_mem(struct user_struct *user,\n\t\t\t\t      unsigned long nr_pages)\n{\n\tatomic_long_sub(nr_pages, &user->locked_vm);\n}\n\nstatic inline int __io_account_mem(struct user_struct *user,\n\t\t\t\t   unsigned long nr_pages)\n{\n\tunsigned long page_limit, cur_pages, new_pages;\n\n\t/* Don't allow more pages than we can safely lock */\n\tpage_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\n\n\tdo {\n\t\tcur_pages = atomic_long_read(&user->locked_vm);\n\t\tnew_pages = cur_pages + nr_pages;\n\t\tif (new_pages > page_limit)\n\t\t\treturn -ENOMEM;\n\t} while (atomic_long_cmpxchg(&user->locked_vm, cur_pages,\n\t\t\t\t\tnew_pages) != cur_pages);\n\n\treturn 0;\n}\n\nstatic void io_unaccount_mem(struct io_ring_ctx *ctx, unsigned long nr_pages,\n\t\t\t     enum io_mem_account acct)\n{\n\tif (ctx->limit_mem)\n\t\t__io_unaccount_mem(ctx->user, nr_pages);\n\n\tif (ctx->mm_account) {\n\t\tif (acct == ACCT_LOCKED)\n\t\t\tctx->mm_account->locked_vm -= nr_pages;\n\t\telse if (acct == ACCT_PINNED)\n\t\t\tatomic64_sub(nr_pages, &ctx->mm_account->pinned_vm);\n\t}\n}\n\nstatic int io_account_mem(struct io_ring_ctx *ctx, unsigned long nr_pages,\n\t\t\t  enum io_mem_account acct)\n{\n\tint ret;\n\n\tif (ctx->limit_mem) {\n\t\tret = __io_account_mem(ctx->user, nr_pages);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (ctx->mm_account) {\n\t\tif (acct == ACCT_LOCKED)\n\t\t\tctx->mm_account->locked_vm += nr_pages;\n\t\telse if (acct == ACCT_PINNED)\n\t\t\tatomic64_add(nr_pages, &ctx->mm_account->pinned_vm);\n\t}\n\n\treturn 0;\n}\n\nstatic void io_mem_free(void *ptr)\n{\n\tstruct page *page;\n\n\tif (!ptr)\n\t\treturn;\n\n\tpage = virt_to_head_page(ptr);\n\tif (put_page_testzero(page))\n\t\tfree_compound_page(page);\n}\n\nstatic void *io_mem_alloc(size_t size)\n{\n\tgfp_t gfp_flags = GFP_KERNEL | __GFP_ZERO | __GFP_NOWARN | __GFP_COMP |\n\t\t\t\t__GFP_NORETRY;\n\n\treturn (void *) __get_free_pages(gfp_flags, get_order(size));\n}\n\nstatic unsigned long rings_size(unsigned sq_entries, unsigned cq_entries,\n\t\t\t\tsize_t *sq_offset)\n{\n\tstruct io_rings *rings;\n\tsize_t off, sq_array_size;\n\n\toff = struct_size(rings, cqes, cq_entries);\n\tif (off == SIZE_MAX)\n\t\treturn SIZE_MAX;\n\n#ifdef CONFIG_SMP\n\toff = ALIGN(off, SMP_CACHE_BYTES);\n\tif (off == 0)\n\t\treturn SIZE_MAX;\n#endif\n\n\tif (sq_offset)\n\t\t*sq_offset = off;\n\n\tsq_array_size = array_size(sizeof(u32), sq_entries);\n\tif (sq_array_size == SIZE_MAX)\n\t\treturn SIZE_MAX;\n\n\tif (check_add_overflow(off, sq_array_size, &off))\n\t\treturn SIZE_MAX;\n\n\treturn off;\n}\n\nstatic unsigned long ring_pages(unsigned sq_entries, unsigned cq_entries)\n{\n\tsize_t pages;\n\n\tpages = (size_t)1 << get_order(\n\t\trings_size(sq_entries, cq_entries, NULL));\n\tpages += (size_t)1 << get_order(\n\t\tarray_size(sizeof(struct io_uring_sqe), sq_entries));\n\n\treturn pages;\n}\n\nstatic int io_sqe_buffer_unregister(struct io_ring_ctx *ctx)\n{\n\tint i, j;\n\n\tif (!ctx->user_bufs)\n\t\treturn -ENXIO;\n\n\tfor (i = 0; i < ctx->nr_user_bufs; i++) {\n\t\tstruct io_mapped_ubuf *imu = &ctx->user_bufs[i];\n\n\t\tfor (j = 0; j < imu->nr_bvecs; j++)\n\t\t\tunpin_user_page(imu->bvec[j].bv_page);\n\n\t\tif (imu->acct_pages)\n\t\t\tio_unaccount_mem(ctx, imu->acct_pages, ACCT_PINNED);\n\t\tkvfree(imu->bvec);\n\t\timu->nr_bvecs = 0;\n\t}\n\n\tkfree(ctx->user_bufs);\n\tctx->user_bufs = NULL;\n\tctx->nr_user_bufs = 0;\n\treturn 0;\n}\n\nstatic int io_copy_iov(struct io_ring_ctx *ctx, struct iovec *dst,\n\t\t       void __user *arg, unsigned index)\n{\n\tstruct iovec __user *src;\n\n#ifdef CONFIG_COMPAT\n\tif (ctx->compat) {\n\t\tstruct compat_iovec __user *ciovs;\n\t\tstruct compat_iovec ciov;\n\n\t\tciovs = (struct compat_iovec __user *) arg;\n\t\tif (copy_from_user(&ciov, &ciovs[index], sizeof(ciov)))\n\t\t\treturn -EFAULT;\n\n\t\tdst->iov_base = u64_to_user_ptr((u64)ciov.iov_base);\n\t\tdst->iov_len = ciov.iov_len;\n\t\treturn 0;\n\t}\n#endif\n\tsrc = (struct iovec __user *) arg;\n\tif (copy_from_user(dst, &src[index], sizeof(*dst)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n/*\n * Not super efficient, but this is just a registration time. And we do cache\n * the last compound head, so generally we'll only do a full search if we don't\n * match that one.\n *\n * We check if the given compound head page has already been accounted, to\n * avoid double accounting it. This allows us to account the full size of the\n * page, not just the constituent pages of a huge page.\n */\nstatic bool headpage_already_acct(struct io_ring_ctx *ctx, struct page **pages,\n\t\t\t\t  int nr_pages, struct page *hpage)\n{\n\tint i, j;\n\n\t/* check current page array */\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tif (!PageCompound(pages[i]))\n\t\t\tcontinue;\n\t\tif (compound_head(pages[i]) == hpage)\n\t\t\treturn true;\n\t}\n\n\t/* check previously registered pages */\n\tfor (i = 0; i < ctx->nr_user_bufs; i++) {\n\t\tstruct io_mapped_ubuf *imu = &ctx->user_bufs[i];\n\n\t\tfor (j = 0; j < imu->nr_bvecs; j++) {\n\t\t\tif (!PageCompound(imu->bvec[j].bv_page))\n\t\t\t\tcontinue;\n\t\t\tif (compound_head(imu->bvec[j].bv_page) == hpage)\n\t\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic int io_buffer_account_pin(struct io_ring_ctx *ctx, struct page **pages,\n\t\t\t\t int nr_pages, struct io_mapped_ubuf *imu,\n\t\t\t\t struct page **last_hpage)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tif (!PageCompound(pages[i])) {\n\t\t\timu->acct_pages++;\n\t\t} else {\n\t\t\tstruct page *hpage;\n\n\t\t\thpage = compound_head(pages[i]);\n\t\t\tif (hpage == *last_hpage)\n\t\t\t\tcontinue;\n\t\t\t*last_hpage = hpage;\n\t\t\tif (headpage_already_acct(ctx, pages, i, hpage))\n\t\t\t\tcontinue;\n\t\t\timu->acct_pages += page_size(hpage) >> PAGE_SHIFT;\n\t\t}\n\t}\n\n\tif (!imu->acct_pages)\n\t\treturn 0;\n\n\tret = io_account_mem(ctx, imu->acct_pages, ACCT_PINNED);\n\tif (ret)\n\t\timu->acct_pages = 0;\n\treturn ret;\n}\n\nstatic int io_sqe_buffer_register(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t\t  unsigned nr_args)\n{\n\tstruct vm_area_struct **vmas = NULL;\n\tstruct page **pages = NULL;\n\tstruct page *last_hpage = NULL;\n\tint i, j, got_pages = 0;\n\tint ret = -EINVAL;\n\n\tif (ctx->user_bufs)\n\t\treturn -EBUSY;\n\tif (!nr_args || nr_args > UIO_MAXIOV)\n\t\treturn -EINVAL;\n\n\tctx->user_bufs = kcalloc(nr_args, sizeof(struct io_mapped_ubuf),\n\t\t\t\t\tGFP_KERNEL);\n\tif (!ctx->user_bufs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < nr_args; i++) {\n\t\tstruct io_mapped_ubuf *imu = &ctx->user_bufs[i];\n\t\tunsigned long off, start, end, ubuf;\n\t\tint pret, nr_pages;\n\t\tstruct iovec iov;\n\t\tsize_t size;\n\n\t\tret = io_copy_iov(ctx, &iov, arg, i);\n\t\tif (ret)\n\t\t\tgoto err;\n\n\t\t/*\n\t\t * Don't impose further limits on the size and buffer\n\t\t * constraints here, we'll -EINVAL later when IO is\n\t\t * submitted if they are wrong.\n\t\t */\n\t\tret = -EFAULT;\n\t\tif (!iov.iov_base || !iov.iov_len)\n\t\t\tgoto err;\n\n\t\t/* arbitrary limit, but we need something */\n\t\tif (iov.iov_len > SZ_1G)\n\t\t\tgoto err;\n\n\t\tubuf = (unsigned long) iov.iov_base;\n\t\tend = (ubuf + iov.iov_len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tstart = ubuf >> PAGE_SHIFT;\n\t\tnr_pages = end - start;\n\n\t\tret = 0;\n\t\tif (!pages || nr_pages > got_pages) {\n\t\t\tkvfree(vmas);\n\t\t\tkvfree(pages);\n\t\t\tpages = kvmalloc_array(nr_pages, sizeof(struct page *),\n\t\t\t\t\t\tGFP_KERNEL);\n\t\t\tvmas = kvmalloc_array(nr_pages,\n\t\t\t\t\tsizeof(struct vm_area_struct *),\n\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!pages || !vmas) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tgot_pages = nr_pages;\n\t\t}\n\n\t\timu->bvec = kvmalloc_array(nr_pages, sizeof(struct bio_vec),\n\t\t\t\t\t\tGFP_KERNEL);\n\t\tret = -ENOMEM;\n\t\tif (!imu->bvec)\n\t\t\tgoto err;\n\n\t\tret = 0;\n\t\tmmap_read_lock(current->mm);\n\t\tpret = pin_user_pages(ubuf, nr_pages,\n\t\t\t\t      FOLL_WRITE | FOLL_LONGTERM,\n\t\t\t\t      pages, vmas);\n\t\tif (pret == nr_pages) {\n\t\t\t/* don't support file backed memory */\n\t\t\tfor (j = 0; j < nr_pages; j++) {\n\t\t\t\tstruct vm_area_struct *vma = vmas[j];\n\n\t\t\t\tif (vma->vm_file &&\n\t\t\t\t    !is_file_hugepages(vma->vm_file)) {\n\t\t\t\t\tret = -EOPNOTSUPP;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tret = pret < 0 ? pret : -EFAULT;\n\t\t}\n\t\tmmap_read_unlock(current->mm);\n\t\tif (ret) {\n\t\t\t/*\n\t\t\t * if we did partial map, or found file backed vmas,\n\t\t\t * release any pages we did get\n\t\t\t */\n\t\t\tif (pret > 0)\n\t\t\t\tunpin_user_pages(pages, pret);\n\t\t\tkvfree(imu->bvec);\n\t\t\tgoto err;\n\t\t}\n\n\t\tret = io_buffer_account_pin(ctx, pages, pret, imu, &last_hpage);\n\t\tif (ret) {\n\t\t\tunpin_user_pages(pages, pret);\n\t\t\tkvfree(imu->bvec);\n\t\t\tgoto err;\n\t\t}\n\n\t\toff = ubuf & ~PAGE_MASK;\n\t\tsize = iov.iov_len;\n\t\tfor (j = 0; j < nr_pages; j++) {\n\t\t\tsize_t vec_len;\n\n\t\t\tvec_len = min_t(size_t, size, PAGE_SIZE - off);\n\t\t\timu->bvec[j].bv_page = pages[j];\n\t\t\timu->bvec[j].bv_len = vec_len;\n\t\t\timu->bvec[j].bv_offset = off;\n\t\t\toff = 0;\n\t\t\tsize -= vec_len;\n\t\t}\n\t\t/* store original address for later verification */\n\t\timu->ubuf = ubuf;\n\t\timu->len = iov.iov_len;\n\t\timu->nr_bvecs = nr_pages;\n\n\t\tctx->nr_user_bufs++;\n\t}\n\tkvfree(pages);\n\tkvfree(vmas);\n\treturn 0;\nerr:\n\tkvfree(pages);\n\tkvfree(vmas);\n\tio_sqe_buffer_unregister(ctx);\n\treturn ret;\n}\n\nstatic int io_eventfd_register(struct io_ring_ctx *ctx, void __user *arg)\n{\n\t__s32 __user *fds = arg;\n\tint fd;\n\n\tif (ctx->cq_ev_fd)\n\t\treturn -EBUSY;\n\n\tif (copy_from_user(&fd, fds, sizeof(*fds)))\n\t\treturn -EFAULT;\n\n\tctx->cq_ev_fd = eventfd_ctx_fdget(fd);\n\tif (IS_ERR(ctx->cq_ev_fd)) {\n\t\tint ret = PTR_ERR(ctx->cq_ev_fd);\n\t\tctx->cq_ev_fd = NULL;\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int io_eventfd_unregister(struct io_ring_ctx *ctx)\n{\n\tif (ctx->cq_ev_fd) {\n\t\teventfd_ctx_put(ctx->cq_ev_fd);\n\t\tctx->cq_ev_fd = NULL;\n\t\treturn 0;\n\t}\n\n\treturn -ENXIO;\n}\n\nstatic int __io_destroy_buffers(int id, void *p, void *data)\n{\n\tstruct io_ring_ctx *ctx = data;\n\tstruct io_buffer *buf = p;\n\n\t__io_remove_buffers(ctx, buf, id, -1U);\n\treturn 0;\n}\n\nstatic void io_destroy_buffers(struct io_ring_ctx *ctx)\n{\n\tidr_for_each(&ctx->io_buffer_idr, __io_destroy_buffers, ctx);\n\tidr_destroy(&ctx->io_buffer_idr);\n}\n\nstatic void io_ring_ctx_free(struct io_ring_ctx *ctx)\n{\n\tio_finish_async(ctx);\n\tio_sqe_buffer_unregister(ctx);\n\n\tif (ctx->sqo_task) {\n\t\tput_task_struct(ctx->sqo_task);\n\t\tctx->sqo_task = NULL;\n\t\tmmdrop(ctx->mm_account);\n\t\tctx->mm_account = NULL;\n\t}\n\n#ifdef CONFIG_BLK_CGROUP\n\tif (ctx->sqo_blkcg_css)\n\t\tcss_put(ctx->sqo_blkcg_css);\n#endif\n\n\tio_sqe_files_unregister(ctx);\n\tio_eventfd_unregister(ctx);\n\tio_destroy_buffers(ctx);\n\tidr_destroy(&ctx->personality_idr);\n\n#if defined(CONFIG_UNIX)\n\tif (ctx->ring_sock) {\n\t\tctx->ring_sock->file = NULL; /* so that iput() is called */\n\t\tsock_release(ctx->ring_sock);\n\t}\n#endif\n\n\tio_mem_free(ctx->rings);\n\tio_mem_free(ctx->sq_sqes);\n\n\tpercpu_ref_exit(&ctx->refs);\n\tfree_uid(ctx->user);\n\tput_cred(ctx->creds);\n\tkfree(ctx->cancel_hash);\n\tkmem_cache_free(req_cachep, ctx->fallback_req);\n\tkfree(ctx);\n}\n\nstatic __poll_t io_uring_poll(struct file *file, poll_table *wait)\n{\n\tstruct io_ring_ctx *ctx = file->private_data;\n\t__poll_t mask = 0;\n\n\tpoll_wait(file, &ctx->cq_wait, wait);\n\t/*\n\t * synchronizes with barrier from wq_has_sleeper call in\n\t * io_commit_cqring\n\t */\n\tsmp_rmb();\n\tif (!io_sqring_full(ctx))\n\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\tif (io_cqring_events(ctx, false))\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\treturn mask;\n}\n\nstatic int io_uring_fasync(int fd, struct file *file, int on)\n{\n\tstruct io_ring_ctx *ctx = file->private_data;\n\n\treturn fasync_helper(fd, file, on, &ctx->cq_fasync);\n}\n\nstatic int io_remove_personalities(int id, void *p, void *data)\n{\n\tstruct io_ring_ctx *ctx = data;\n\tstruct io_identity *iod;\n\n\tiod = idr_remove(&ctx->personality_idr, id);\n\tif (iod) {\n\t\tput_cred(iod->creds);\n\t\tif (refcount_dec_and_test(&iod->count))\n\t\t\tkfree(iod);\n\t}\n\treturn 0;\n}\n\nstatic void io_ring_exit_work(struct work_struct *work)\n{\n\tstruct io_ring_ctx *ctx = container_of(work, struct io_ring_ctx,\n\t\t\t\t\t       exit_work);\n\n\t/*\n\t * If we're doing polled IO and end up having requests being\n\t * submitted async (out-of-line), then completions can come in while\n\t * we're waiting for refs to drop. We need to reap these manually,\n\t * as nobody else will be looking for them.\n\t */\n\tdo {\n\t\tif (ctx->rings)\n\t\t\tio_cqring_overflow_flush(ctx, true, NULL, NULL);\n\t\tio_iopoll_try_reap_events(ctx);\n\t} while (!wait_for_completion_timeout(&ctx->ref_comp, HZ/20));\n\tio_ring_ctx_free(ctx);\n}\n\nstatic void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)\n{\n\tmutex_lock(&ctx->uring_lock);\n\tpercpu_ref_kill(&ctx->refs);\n\tmutex_unlock(&ctx->uring_lock);\n\n\tio_kill_timeouts(ctx, NULL, NULL);\n\tio_poll_remove_all(ctx, NULL, NULL);\n\n\tif (ctx->io_wq)\n\t\tio_wq_cancel_all(ctx->io_wq);\n\n\t/* if we failed setting up the ctx, we might not have any rings */\n\tif (ctx->rings)\n\t\tio_cqring_overflow_flush(ctx, true, NULL, NULL);\n\tio_iopoll_try_reap_events(ctx);\n\tidr_for_each(&ctx->personality_idr, io_remove_personalities, ctx);\n\n\t/*\n\t * Do this upfront, so we won't have a grace period where the ring\n\t * is closed but resources aren't reaped yet. This can cause\n\t * spurious failure in setting up a new ring.\n\t */\n\tio_unaccount_mem(ctx, ring_pages(ctx->sq_entries, ctx->cq_entries),\n\t\t\t ACCT_LOCKED);\n\n\tINIT_WORK(&ctx->exit_work, io_ring_exit_work);\n\t/*\n\t * Use system_unbound_wq to avoid spawning tons of event kworkers\n\t * if we're exiting a ton of rings at the same time. It just adds\n\t * noise and overhead, there's no discernable change in runtime\n\t * over using system_wq.\n\t */\n\tqueue_work(system_unbound_wq, &ctx->exit_work);\n}\n\nstatic int io_uring_release(struct inode *inode, struct file *file)\n{\n\tstruct io_ring_ctx *ctx = file->private_data;\n\n\tfile->private_data = NULL;\n\tio_ring_ctx_wait_and_kill(ctx);\n\treturn 0;\n}\n\nstruct io_task_cancel {\n\tstruct task_struct *task;\n\tstruct files_struct *files;\n};\n\nstatic bool io_cancel_task_cb(struct io_wq_work *work, void *data)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\tstruct io_task_cancel *cancel = data;\n\tbool ret;\n\n\tif (cancel->files && (req->flags & REQ_F_LINK_TIMEOUT)) {\n\t\tunsigned long flags;\n\t\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t\t/* protect against races with linked timeouts */\n\t\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\t\tret = io_match_task(req, cancel->task, cancel->files);\n\t\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\t} else {\n\t\tret = io_match_task(req, cancel->task, cancel->files);\n\t}\n\treturn ret;\n}\n\nstatic void io_cancel_defer_files(struct io_ring_ctx *ctx,\n\t\t\t\t  struct task_struct *task,\n\t\t\t\t  struct files_struct *files)\n{\n\tstruct io_defer_entry *de = NULL;\n\tLIST_HEAD(list);\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tlist_for_each_entry_reverse(de, &ctx->defer_list, list) {\n\t\tif (io_match_task(de->req, task, files)) {\n\t\t\tlist_cut_position(&list, &ctx->defer_list, &de->list);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\twhile (!list_empty(&list)) {\n\t\tde = list_first_entry(&list, struct io_defer_entry, list);\n\t\tlist_del_init(&de->list);\n\t\treq_set_fail_links(de->req);\n\t\tio_put_req(de->req);\n\t\tio_req_complete(de->req, -ECANCELED);\n\t\tkfree(de);\n\t}\n}\n\nstatic void io_uring_cancel_files(struct io_ring_ctx *ctx,\n\t\t\t\t  struct task_struct *task,\n\t\t\t\t  struct files_struct *files)\n{\n\twhile (!list_empty_careful(&ctx->inflight_list)) {\n\t\tstruct io_task_cancel cancel = { .task = task, .files = NULL, };\n\t\tstruct io_kiocb *req;\n\t\tDEFINE_WAIT(wait);\n\t\tbool found = false;\n\n\t\tspin_lock_irq(&ctx->inflight_lock);\n\t\tlist_for_each_entry(req, &ctx->inflight_list, inflight_entry) {\n\t\t\tif (req->task == task &&\n\t\t\t    (req->work.flags & IO_WQ_WORK_FILES) &&\n\t\t\t    req->work.identity->files != files)\n\t\t\t\tcontinue;\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t\tif (found)\n\t\t\tprepare_to_wait(&task->io_uring->wait, &wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tspin_unlock_irq(&ctx->inflight_lock);\n\n\t\t/* We need to keep going until we don't find a matching req */\n\t\tif (!found)\n\t\t\tbreak;\n\n\t\tio_wq_cancel_cb(ctx->io_wq, io_cancel_task_cb, &cancel, true);\n\t\tio_poll_remove_all(ctx, task, files);\n\t\tio_kill_timeouts(ctx, task, files);\n\t\t/* cancellations _may_ trigger task work */\n\t\tio_run_task_work();\n\t\tschedule();\n\t\tfinish_wait(&task->io_uring->wait, &wait);\n\t}\n}\n\nstatic void __io_uring_cancel_task_requests(struct io_ring_ctx *ctx,\n\t\t\t\t\t    struct task_struct *task)\n{\n\twhile (1) {\n\t\tstruct io_task_cancel cancel = { .task = task, .files = NULL, };\n\t\tenum io_wq_cancel cret;\n\t\tbool ret = false;\n\n\t\tcret = io_wq_cancel_cb(ctx->io_wq, io_cancel_task_cb, &cancel, true);\n\t\tif (cret != IO_WQ_CANCEL_NOTFOUND)\n\t\t\tret = true;\n\n\t\t/* SQPOLL thread does its own polling */\n\t\tif (!(ctx->flags & IORING_SETUP_SQPOLL)) {\n\t\t\twhile (!list_empty_careful(&ctx->iopoll_list)) {\n\t\t\t\tio_iopoll_try_reap_events(ctx);\n\t\t\t\tret = true;\n\t\t\t}\n\t\t}\n\n\t\tret |= io_poll_remove_all(ctx, task, NULL);\n\t\tret |= io_kill_timeouts(ctx, task, NULL);\n\t\tif (!ret)\n\t\t\tbreak;\n\t\tio_run_task_work();\n\t\tcond_resched();\n\t}\n}\n\n/*\n * We need to iteratively cancel requests, in case a request has dependent\n * hard links. These persist even for failure of cancelations, hence keep\n * looping until none are found.\n */\nstatic void io_uring_cancel_task_requests(struct io_ring_ctx *ctx,\n\t\t\t\t\t  struct files_struct *files)\n{\n\tstruct task_struct *task = current;\n\n\tif ((ctx->flags & IORING_SETUP_SQPOLL) && ctx->sq_data) {\n\t\ttask = ctx->sq_data->thread;\n\t\tatomic_inc(&task->io_uring->in_idle);\n\t\tio_sq_thread_park(ctx->sq_data);\n\t}\n\n\tio_cancel_defer_files(ctx, task, files);\n\tio_cqring_overflow_flush(ctx, true, task, files);\n\tio_uring_cancel_files(ctx, task, files);\n\n\tif (!files)\n\t\t__io_uring_cancel_task_requests(ctx, task);\n\n\tif ((ctx->flags & IORING_SETUP_SQPOLL) && ctx->sq_data) {\n\t\tatomic_dec(&task->io_uring->in_idle);\n\t\t/*\n\t\t * If the files that are going away are the ones in the thread\n\t\t * identity, clear them out.\n\t\t */\n\t\tif (task->io_uring->identity->files == files)\n\t\t\ttask->io_uring->identity->files = NULL;\n\t\tio_sq_thread_unpark(ctx->sq_data);\n\t}\n}\n\n/*\n * Note that this task has used io_uring. We use it for cancelation purposes.\n */\nstatic int io_uring_add_task_file(struct io_ring_ctx *ctx, struct file *file)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\n\tif (unlikely(!tctx)) {\n\t\tint ret;\n\n\t\tret = io_uring_alloc_task_context(current);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t\ttctx = current->io_uring;\n\t}\n\tif (tctx->last != file) {\n\t\tvoid *old = xa_load(&tctx->xa, (unsigned long)file);\n\n\t\tif (!old) {\n\t\t\tget_file(file);\n\t\t\txa_store(&tctx->xa, (unsigned long)file, file, GFP_KERNEL);\n\t\t}\n\t\ttctx->last = file;\n\t}\n\n\t/*\n\t * This is race safe in that the task itself is doing this, hence it\n\t * cannot be going through the exit/cancel paths at the same time.\n\t * This cannot be modified while exit/cancel is running.\n\t */\n\tif (!tctx->sqpoll && (ctx->flags & IORING_SETUP_SQPOLL))\n\t\ttctx->sqpoll = true;\n\n\treturn 0;\n}\n\n/*\n * Remove this io_uring_file -> task mapping.\n */\nstatic void io_uring_del_task_file(struct file *file)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\n\tif (tctx->last == file)\n\t\ttctx->last = NULL;\n\tfile = xa_erase(&tctx->xa, (unsigned long)file);\n\tif (file)\n\t\tfput(file);\n}\n\n/*\n * Drop task note for this file if we're the only ones that hold it after\n * pending fput()\n */\nstatic void io_uring_attempt_task_drop(struct file *file)\n{\n\tif (!current->io_uring)\n\t\treturn;\n\t/*\n\t * fput() is pending, will be 2 if the only other ref is our potential\n\t * task file note. If the task is exiting, drop regardless of count.\n\t */\n\tif (fatal_signal_pending(current) || (current->flags & PF_EXITING) ||\n\t    atomic_long_read(&file->f_count) == 2)\n\t\tio_uring_del_task_file(file);\n}\n\nvoid __io_uring_files_cancel(struct files_struct *files)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tstruct file *file;\n\tunsigned long index;\n\n\t/* make sure overflow events are dropped */\n\tatomic_inc(&tctx->in_idle);\n\n\txa_for_each(&tctx->xa, index, file) {\n\t\tstruct io_ring_ctx *ctx = file->private_data;\n\n\t\tio_uring_cancel_task_requests(ctx, files);\n\t\tif (files)\n\t\t\tio_uring_del_task_file(file);\n\t}\n\n\tatomic_dec(&tctx->in_idle);\n}\n\nstatic s64 tctx_inflight(struct io_uring_task *tctx)\n{\n\tunsigned long index;\n\tstruct file *file;\n\ts64 inflight;\n\n\tinflight = percpu_counter_sum(&tctx->inflight);\n\tif (!tctx->sqpoll)\n\t\treturn inflight;\n\n\t/*\n\t * If we have SQPOLL rings, then we need to iterate and find them, and\n\t * add the pending count for those.\n\t */\n\txa_for_each(&tctx->xa, index, file) {\n\t\tstruct io_ring_ctx *ctx = file->private_data;\n\n\t\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\t\tstruct io_uring_task *__tctx = ctx->sqo_task->io_uring;\n\n\t\t\tinflight += percpu_counter_sum(&__tctx->inflight);\n\t\t}\n\t}\n\n\treturn inflight;\n}\n\n/*\n * Find any io_uring fd that this task has registered or done IO on, and cancel\n * requests.\n */\nvoid __io_uring_task_cancel(void)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tDEFINE_WAIT(wait);\n\ts64 inflight;\n\n\t/* make sure overflow events are dropped */\n\tatomic_inc(&tctx->in_idle);\n\n\tdo {\n\t\t/* read completions before cancelations */\n\t\tinflight = tctx_inflight(tctx);\n\t\tif (!inflight)\n\t\t\tbreak;\n\t\t__io_uring_files_cancel(NULL);\n\n\t\tprepare_to_wait(&tctx->wait, &wait, TASK_UNINTERRUPTIBLE);\n\n\t\t/*\n\t\t * If we've seen completions, retry. This avoids a race where\n\t\t * a completion comes in before we did prepare_to_wait().\n\t\t */\n\t\tif (inflight != tctx_inflight(tctx))\n\t\t\tcontinue;\n\t\tschedule();\n\t} while (1);\n\n\tfinish_wait(&tctx->wait, &wait);\n\tatomic_dec(&tctx->in_idle);\n}\n\nstatic int io_uring_flush(struct file *file, void *data)\n{\n\tio_uring_attempt_task_drop(file);\n\treturn 0;\n}\n\nstatic void *io_uring_validate_mmap_request(struct file *file,\n\t\t\t\t\t    loff_t pgoff, size_t sz)\n{\n\tstruct io_ring_ctx *ctx = file->private_data;\n\tloff_t offset = pgoff << PAGE_SHIFT;\n\tstruct page *page;\n\tvoid *ptr;\n\n\tswitch (offset) {\n\tcase IORING_OFF_SQ_RING:\n\tcase IORING_OFF_CQ_RING:\n\t\tptr = ctx->rings;\n\t\tbreak;\n\tcase IORING_OFF_SQES:\n\t\tptr = ctx->sq_sqes;\n\t\tbreak;\n\tdefault:\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tpage = virt_to_head_page(ptr);\n\tif (sz > page_size(page))\n\t\treturn ERR_PTR(-EINVAL);\n\n\treturn ptr;\n}\n\n#ifdef CONFIG_MMU\n\nstatic int io_uring_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tsize_t sz = vma->vm_end - vma->vm_start;\n\tunsigned long pfn;\n\tvoid *ptr;\n\n\tptr = io_uring_validate_mmap_request(file, vma->vm_pgoff, sz);\n\tif (IS_ERR(ptr))\n\t\treturn PTR_ERR(ptr);\n\n\tpfn = virt_to_phys(ptr) >> PAGE_SHIFT;\n\treturn remap_pfn_range(vma, vma->vm_start, pfn, sz, vma->vm_page_prot);\n}\n\n#else /* !CONFIG_MMU */\n\nstatic int io_uring_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\treturn vma->vm_flags & (VM_SHARED | VM_MAYSHARE) ? 0 : -EINVAL;\n}\n\nstatic unsigned int io_uring_nommu_mmap_capabilities(struct file *file)\n{\n\treturn NOMMU_MAP_DIRECT | NOMMU_MAP_READ | NOMMU_MAP_WRITE;\n}\n\nstatic unsigned long io_uring_nommu_get_unmapped_area(struct file *file,\n\tunsigned long addr, unsigned long len,\n\tunsigned long pgoff, unsigned long flags)\n{\n\tvoid *ptr;\n\n\tptr = io_uring_validate_mmap_request(file, pgoff, len);\n\tif (IS_ERR(ptr))\n\t\treturn PTR_ERR(ptr);\n\n\treturn (unsigned long) ptr;\n}\n\n#endif /* !CONFIG_MMU */\n\nstatic void io_sqpoll_wait_sq(struct io_ring_ctx *ctx)\n{\n\tDEFINE_WAIT(wait);\n\n\tdo {\n\t\tif (!io_sqring_full(ctx))\n\t\t\tbreak;\n\n\t\tprepare_to_wait(&ctx->sqo_sq_wait, &wait, TASK_INTERRUPTIBLE);\n\n\t\tif (!io_sqring_full(ctx))\n\t\t\tbreak;\n\n\t\tschedule();\n\t} while (!signal_pending(current));\n\n\tfinish_wait(&ctx->sqo_sq_wait, &wait);\n}\n\nstatic int io_get_ext_arg(unsigned flags, const void __user *argp, size_t *argsz,\n\t\t\t  struct __kernel_timespec __user **ts,\n\t\t\t  const sigset_t __user **sig)\n{\n\tstruct io_uring_getevents_arg arg;\n\n\t/*\n\t * If EXT_ARG isn't set, then we have no timespec and the argp pointer\n\t * is just a pointer to the sigset_t.\n\t */\n\tif (!(flags & IORING_ENTER_EXT_ARG)) {\n\t\t*sig = (const sigset_t __user *) argp;\n\t\t*ts = NULL;\n\t\treturn 0;\n\t}\n\n\t/*\n\t * EXT_ARG is set - ensure we agree on the size of it and copy in our\n\t * timespec and sigset_t pointers if good.\n\t */\n\tif (*argsz != sizeof(arg))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&arg, argp, sizeof(arg)))\n\t\treturn -EFAULT;\n\t*sig = u64_to_user_ptr(arg.sigmask);\n\t*argsz = arg.sigmask_sz;\n\t*ts = u64_to_user_ptr(arg.ts);\n\treturn 0;\n}\n\nSYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,\n\t\tu32, min_complete, u32, flags, const void __user *, argp,\n\t\tsize_t, argsz)\n{\n\tstruct io_ring_ctx *ctx;\n\tlong ret = -EBADF;\n\tint submitted = 0;\n\tstruct fd f;\n\n\tio_run_task_work();\n\n\tif (flags & ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP |\n\t\t\tIORING_ENTER_SQ_WAIT | IORING_ENTER_EXT_ARG))\n\t\treturn -EINVAL;\n\n\tf = fdget(fd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tret = -EOPNOTSUPP;\n\tif (f.file->f_op != &io_uring_fops)\n\t\tgoto out_fput;\n\n\tret = -ENXIO;\n\tctx = f.file->private_data;\n\tif (!percpu_ref_tryget(&ctx->refs))\n\t\tgoto out_fput;\n\n\tret = -EBADFD;\n\tif (ctx->flags & IORING_SETUP_R_DISABLED)\n\t\tgoto out;\n\n\t/*\n\t * For SQ polling, the thread will do all submissions and completions.\n\t * Just return the requested submit count, and wake the thread if\n\t * we were asked to.\n\t */\n\tret = 0;\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\tif (!list_empty_careful(&ctx->cq_overflow_list))\n\t\t\tio_cqring_overflow_flush(ctx, false, NULL, NULL);\n\t\tif (flags & IORING_ENTER_SQ_WAKEUP)\n\t\t\twake_up(&ctx->sq_data->wait);\n\t\tif (flags & IORING_ENTER_SQ_WAIT)\n\t\t\tio_sqpoll_wait_sq(ctx);\n\t\tsubmitted = to_submit;\n\t} else if (to_submit) {\n\t\tret = io_uring_add_task_file(ctx, f.file);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tsubmitted = io_submit_sqes(ctx, to_submit);\n\t\tmutex_unlock(&ctx->uring_lock);\n\n\t\tif (submitted != to_submit)\n\t\t\tgoto out;\n\t}\n\tif (flags & IORING_ENTER_GETEVENTS) {\n\t\tconst sigset_t __user *sig;\n\t\tstruct __kernel_timespec __user *ts;\n\n\t\tret = io_get_ext_arg(flags, argp, &argsz, &ts, &sig);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\n\t\tmin_complete = min(min_complete, ctx->cq_entries);\n\n\t\t/*\n\t\t * When SETUP_IOPOLL and SETUP_SQPOLL are both enabled, user\n\t\t * space applications don't need to do io completion events\n\t\t * polling again, they can rely on io_sq_thread to do polling\n\t\t * work, which can reduce cpu usage and uring_lock contention.\n\t\t */\n\t\tif (ctx->flags & IORING_SETUP_IOPOLL &&\n\t\t    !(ctx->flags & IORING_SETUP_SQPOLL)) {\n\t\t\tret = io_iopoll_check(ctx, min_complete);\n\t\t} else {\n\t\t\tret = io_cqring_wait(ctx, min_complete, sig, argsz, ts);\n\t\t}\n\t}\n\nout:\n\tpercpu_ref_put(&ctx->refs);\nout_fput:\n\tfdput(f);\n\treturn submitted ? submitted : ret;\n}\n\n#ifdef CONFIG_PROC_FS\nstatic int io_uring_show_cred(int id, void *p, void *data)\n{\n\tstruct io_identity *iod = p;\n\tconst struct cred *cred = iod->creds;\n\tstruct seq_file *m = data;\n\tstruct user_namespace *uns = seq_user_ns(m);\n\tstruct group_info *gi;\n\tkernel_cap_t cap;\n\tunsigned __capi;\n\tint g;\n\n\tseq_printf(m, \"%5d\\n\", id);\n\tseq_put_decimal_ull(m, \"\\tUid:\\t\", from_kuid_munged(uns, cred->uid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->euid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->suid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->fsuid));\n\tseq_put_decimal_ull(m, \"\\n\\tGid:\\t\", from_kgid_munged(uns, cred->gid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->egid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->sgid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->fsgid));\n\tseq_puts(m, \"\\n\\tGroups:\\t\");\n\tgi = cred->group_info;\n\tfor (g = 0; g < gi->ngroups; g++) {\n\t\tseq_put_decimal_ull(m, g ? \" \" : \"\",\n\t\t\t\t\tfrom_kgid_munged(uns, gi->gid[g]));\n\t}\n\tseq_puts(m, \"\\n\\tCapEff:\\t\");\n\tcap = cred->cap_effective;\n\tCAP_FOR_EACH_U32(__capi)\n\t\tseq_put_hex_ll(m, NULL, cap.cap[CAP_LAST_U32 - __capi], 8);\n\tseq_putc(m, '\\n');\n\treturn 0;\n}\n\nstatic void __io_uring_show_fdinfo(struct io_ring_ctx *ctx, struct seq_file *m)\n{\n\tstruct io_sq_data *sq = NULL;\n\tbool has_lock;\n\tint i;\n\n\t/*\n\t * Avoid ABBA deadlock between the seq lock and the io_uring mutex,\n\t * since fdinfo case grabs it in the opposite direction of normal use\n\t * cases. If we fail to get the lock, we just don't iterate any\n\t * structures that could be going away outside the io_uring mutex.\n\t */\n\thas_lock = mutex_trylock(&ctx->uring_lock);\n\n\tif (has_lock && (ctx->flags & IORING_SETUP_SQPOLL))\n\t\tsq = ctx->sq_data;\n\n\tseq_printf(m, \"SqThread:\\t%d\\n\", sq ? task_pid_nr(sq->thread) : -1);\n\tseq_printf(m, \"SqThreadCpu:\\t%d\\n\", sq ? task_cpu(sq->thread) : -1);\n\tseq_printf(m, \"UserFiles:\\t%u\\n\", ctx->nr_user_files);\n\tfor (i = 0; has_lock && i < ctx->nr_user_files; i++) {\n\t\tstruct fixed_file_table *table;\n\t\tstruct file *f;\n\n\t\ttable = &ctx->file_data->table[i >> IORING_FILE_TABLE_SHIFT];\n\t\tf = table->files[i & IORING_FILE_TABLE_MASK];\n\t\tif (f)\n\t\t\tseq_printf(m, \"%5u: %s\\n\", i, file_dentry(f)->d_iname);\n\t\telse\n\t\t\tseq_printf(m, \"%5u: <none>\\n\", i);\n\t}\n\tseq_printf(m, \"UserBufs:\\t%u\\n\", ctx->nr_user_bufs);\n\tfor (i = 0; has_lock && i < ctx->nr_user_bufs; i++) {\n\t\tstruct io_mapped_ubuf *buf = &ctx->user_bufs[i];\n\n\t\tseq_printf(m, \"%5u: 0x%llx/%u\\n\", i, buf->ubuf,\n\t\t\t\t\t\t(unsigned int) buf->len);\n\t}\n\tif (has_lock && !idr_is_empty(&ctx->personality_idr)) {\n\t\tseq_printf(m, \"Personalities:\\n\");\n\t\tidr_for_each(&ctx->personality_idr, io_uring_show_cred, m);\n\t}\n\tseq_printf(m, \"PollList:\\n\");\n\tspin_lock_irq(&ctx->completion_lock);\n\tfor (i = 0; i < (1U << ctx->cancel_hash_bits); i++) {\n\t\tstruct hlist_head *list = &ctx->cancel_hash[i];\n\t\tstruct io_kiocb *req;\n\n\t\thlist_for_each_entry(req, list, hash_node)\n\t\t\tseq_printf(m, \"  op=%d, task_works=%d\\n\", req->opcode,\n\t\t\t\t\treq->task->task_works != NULL);\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\tif (has_lock)\n\t\tmutex_unlock(&ctx->uring_lock);\n}\n\nstatic void io_uring_show_fdinfo(struct seq_file *m, struct file *f)\n{\n\tstruct io_ring_ctx *ctx = f->private_data;\n\n\tif (percpu_ref_tryget(&ctx->refs)) {\n\t\t__io_uring_show_fdinfo(ctx, m);\n\t\tpercpu_ref_put(&ctx->refs);\n\t}\n}\n#endif\n\nstatic const struct file_operations io_uring_fops = {\n\t.release\t= io_uring_release,\n\t.flush\t\t= io_uring_flush,\n\t.mmap\t\t= io_uring_mmap,\n#ifndef CONFIG_MMU\n\t.get_unmapped_area = io_uring_nommu_get_unmapped_area,\n\t.mmap_capabilities = io_uring_nommu_mmap_capabilities,\n#endif\n\t.poll\t\t= io_uring_poll,\n\t.fasync\t\t= io_uring_fasync,\n#ifdef CONFIG_PROC_FS\n\t.show_fdinfo\t= io_uring_show_fdinfo,\n#endif\n};\n\nstatic int io_allocate_scq_urings(struct io_ring_ctx *ctx,\n\t\t\t\t  struct io_uring_params *p)\n{\n\tstruct io_rings *rings;\n\tsize_t size, sq_array_offset;\n\n\t/* make sure these are sane, as we already accounted them */\n\tctx->sq_entries = p->sq_entries;\n\tctx->cq_entries = p->cq_entries;\n\n\tsize = rings_size(p->sq_entries, p->cq_entries, &sq_array_offset);\n\tif (size == SIZE_MAX)\n\t\treturn -EOVERFLOW;\n\n\trings = io_mem_alloc(size);\n\tif (!rings)\n\t\treturn -ENOMEM;\n\n\tctx->rings = rings;\n\tctx->sq_array = (u32 *)((char *)rings + sq_array_offset);\n\trings->sq_ring_mask = p->sq_entries - 1;\n\trings->cq_ring_mask = p->cq_entries - 1;\n\trings->sq_ring_entries = p->sq_entries;\n\trings->cq_ring_entries = p->cq_entries;\n\tctx->sq_mask = rings->sq_ring_mask;\n\tctx->cq_mask = rings->cq_ring_mask;\n\n\tsize = array_size(sizeof(struct io_uring_sqe), p->sq_entries);\n\tif (size == SIZE_MAX) {\n\t\tio_mem_free(ctx->rings);\n\t\tctx->rings = NULL;\n\t\treturn -EOVERFLOW;\n\t}\n\n\tctx->sq_sqes = io_mem_alloc(size);\n\tif (!ctx->sq_sqes) {\n\t\tio_mem_free(ctx->rings);\n\t\tctx->rings = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Allocate an anonymous fd, this is what constitutes the application\n * visible backing of an io_uring instance. The application mmaps this\n * fd to gain access to the SQ/CQ ring details. If UNIX sockets are enabled,\n * we have to tie this fd to a socket for file garbage collection purposes.\n */\nstatic int io_uring_get_fd(struct io_ring_ctx *ctx)\n{\n\tstruct file *file;\n\tint ret;\n\n#if defined(CONFIG_UNIX)\n\tret = sock_create_kern(&init_net, PF_UNIX, SOCK_RAW, IPPROTO_IP,\n\t\t\t\t&ctx->ring_sock);\n\tif (ret)\n\t\treturn ret;\n#endif\n\n\tret = get_unused_fd_flags(O_RDWR | O_CLOEXEC);\n\tif (ret < 0)\n\t\tgoto err;\n\n\tfile = anon_inode_getfile(\"[io_uring]\", &io_uring_fops, ctx,\n\t\t\t\t\tO_RDWR | O_CLOEXEC);\n\tif (IS_ERR(file)) {\nerr_fd:\n\t\tput_unused_fd(ret);\n\t\tret = PTR_ERR(file);\n\t\tgoto err;\n\t}\n\n#if defined(CONFIG_UNIX)\n\tctx->ring_sock->file = file;\n#endif\n\tif (unlikely(io_uring_add_task_file(ctx, file))) {\n\t\tfile = ERR_PTR(-ENOMEM);\n\t\tgoto err_fd;\n\t}\n\tfd_install(ret, file);\n\treturn ret;\nerr:\n#if defined(CONFIG_UNIX)\n\tsock_release(ctx->ring_sock);\n\tctx->ring_sock = NULL;\n#endif\n\treturn ret;\n}\n\nstatic int io_uring_create(unsigned entries, struct io_uring_params *p,\n\t\t\t   struct io_uring_params __user *params)\n{\n\tstruct user_struct *user = NULL;\n\tstruct io_ring_ctx *ctx;\n\tbool limit_mem;\n\tint ret;\n\n\tif (!entries)\n\t\treturn -EINVAL;\n\tif (entries > IORING_MAX_ENTRIES) {\n\t\tif (!(p->flags & IORING_SETUP_CLAMP))\n\t\t\treturn -EINVAL;\n\t\tentries = IORING_MAX_ENTRIES;\n\t}\n\n\t/*\n\t * Use twice as many entries for the CQ ring. It's possible for the\n\t * application to drive a higher depth than the size of the SQ ring,\n\t * since the sqes are only used at submission time. This allows for\n\t * some flexibility in overcommitting a bit. If the application has\n\t * set IORING_SETUP_CQSIZE, it will have passed in the desired number\n\t * of CQ ring entries manually.\n\t */\n\tp->sq_entries = roundup_pow_of_two(entries);\n\tif (p->flags & IORING_SETUP_CQSIZE) {\n\t\t/*\n\t\t * If IORING_SETUP_CQSIZE is set, we do the same roundup\n\t\t * to a power-of-two, if it isn't already. We do NOT impose\n\t\t * any cq vs sq ring sizing.\n\t\t */\n\t\tp->cq_entries = roundup_pow_of_two(p->cq_entries);\n\t\tif (p->cq_entries < p->sq_entries)\n\t\t\treturn -EINVAL;\n\t\tif (p->cq_entries > IORING_MAX_CQ_ENTRIES) {\n\t\t\tif (!(p->flags & IORING_SETUP_CLAMP))\n\t\t\t\treturn -EINVAL;\n\t\t\tp->cq_entries = IORING_MAX_CQ_ENTRIES;\n\t\t}\n\t} else {\n\t\tp->cq_entries = 2 * p->sq_entries;\n\t}\n\n\tuser = get_uid(current_user());\n\tlimit_mem = !capable(CAP_IPC_LOCK);\n\n\tif (limit_mem) {\n\t\tret = __io_account_mem(user,\n\t\t\t\tring_pages(p->sq_entries, p->cq_entries));\n\t\tif (ret) {\n\t\t\tfree_uid(user);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tctx = io_ring_ctx_alloc(p);\n\tif (!ctx) {\n\t\tif (limit_mem)\n\t\t\t__io_unaccount_mem(user, ring_pages(p->sq_entries,\n\t\t\t\t\t\t\t\tp->cq_entries));\n\t\tfree_uid(user);\n\t\treturn -ENOMEM;\n\t}\n\tctx->compat = in_compat_syscall();\n\tctx->user = user;\n\tctx->creds = get_current_cred();\n#ifdef CONFIG_AUDIT\n\tctx->loginuid = current->loginuid;\n\tctx->sessionid = current->sessionid;\n#endif\n\tctx->sqo_task = get_task_struct(current);\n\n\t/*\n\t * This is just grabbed for accounting purposes. When a process exits,\n\t * the mm is exited and dropped before the files, hence we need to hang\n\t * on to this mm purely for the purposes of being able to unaccount\n\t * memory (locked/pinned vm). It's not used for anything else.\n\t */\n\tmmgrab(current->mm);\n\tctx->mm_account = current->mm;\n\n#ifdef CONFIG_BLK_CGROUP\n\t/*\n\t * The sq thread will belong to the original cgroup it was inited in.\n\t * If the cgroup goes offline (e.g. disabling the io controller), then\n\t * issued bios will be associated with the closest cgroup later in the\n\t * block layer.\n\t */\n\trcu_read_lock();\n\tctx->sqo_blkcg_css = blkcg_css();\n\tret = css_tryget_online(ctx->sqo_blkcg_css);\n\trcu_read_unlock();\n\tif (!ret) {\n\t\t/* don't init against a dying cgroup, have the user try again */\n\t\tctx->sqo_blkcg_css = NULL;\n\t\tret = -ENODEV;\n\t\tgoto err;\n\t}\n#endif\n\n\t/*\n\t * Account memory _before_ installing the file descriptor. Once\n\t * the descriptor is installed, it can get closed at any time. Also\n\t * do this before hitting the general error path, as ring freeing\n\t * will un-account as well.\n\t */\n\tio_account_mem(ctx, ring_pages(p->sq_entries, p->cq_entries),\n\t\t       ACCT_LOCKED);\n\tctx->limit_mem = limit_mem;\n\n\tret = io_allocate_scq_urings(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\tret = io_sq_offload_create(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\tif (!(p->flags & IORING_SETUP_R_DISABLED))\n\t\tio_sq_offload_start(ctx);\n\n\tmemset(&p->sq_off, 0, sizeof(p->sq_off));\n\tp->sq_off.head = offsetof(struct io_rings, sq.head);\n\tp->sq_off.tail = offsetof(struct io_rings, sq.tail);\n\tp->sq_off.ring_mask = offsetof(struct io_rings, sq_ring_mask);\n\tp->sq_off.ring_entries = offsetof(struct io_rings, sq_ring_entries);\n\tp->sq_off.flags = offsetof(struct io_rings, sq_flags);\n\tp->sq_off.dropped = offsetof(struct io_rings, sq_dropped);\n\tp->sq_off.array = (char *)ctx->sq_array - (char *)ctx->rings;\n\n\tmemset(&p->cq_off, 0, sizeof(p->cq_off));\n\tp->cq_off.head = offsetof(struct io_rings, cq.head);\n\tp->cq_off.tail = offsetof(struct io_rings, cq.tail);\n\tp->cq_off.ring_mask = offsetof(struct io_rings, cq_ring_mask);\n\tp->cq_off.ring_entries = offsetof(struct io_rings, cq_ring_entries);\n\tp->cq_off.overflow = offsetof(struct io_rings, cq_overflow);\n\tp->cq_off.cqes = offsetof(struct io_rings, cqes);\n\tp->cq_off.flags = offsetof(struct io_rings, cq_flags);\n\n\tp->features = IORING_FEAT_SINGLE_MMAP | IORING_FEAT_NODROP |\n\t\t\tIORING_FEAT_SUBMIT_STABLE | IORING_FEAT_RW_CUR_POS |\n\t\t\tIORING_FEAT_CUR_PERSONALITY | IORING_FEAT_FAST_POLL |\n\t\t\tIORING_FEAT_POLL_32BITS | IORING_FEAT_SQPOLL_NONFIXED |\n\t\t\tIORING_FEAT_EXT_ARG;\n\n\tif (copy_to_user(params, p, sizeof(*p))) {\n\t\tret = -EFAULT;\n\t\tgoto err;\n\t}\n\n\t/*\n\t * Install ring fd as the very last thing, so we don't risk someone\n\t * having closed it before we finish setup\n\t */\n\tret = io_uring_get_fd(ctx);\n\tif (ret < 0)\n\t\tgoto err;\n\n\ttrace_io_uring_create(ret, ctx, p->sq_entries, p->cq_entries, p->flags);\n\treturn ret;\nerr:\n\tio_ring_ctx_wait_and_kill(ctx);\n\treturn ret;\n}\n\n/*\n * Sets up an aio uring context, and returns the fd. Applications asks for a\n * ring size, we return the actual sq/cq ring sizes (among other things) in the\n * params structure passed in.\n */\nstatic long io_uring_setup(u32 entries, struct io_uring_params __user *params)\n{\n\tstruct io_uring_params p;\n\tint i;\n\n\tif (copy_from_user(&p, params, sizeof(p)))\n\t\treturn -EFAULT;\n\tfor (i = 0; i < ARRAY_SIZE(p.resv); i++) {\n\t\tif (p.resv[i])\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (p.flags & ~(IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL |\n\t\t\tIORING_SETUP_SQ_AFF | IORING_SETUP_CQSIZE |\n\t\t\tIORING_SETUP_CLAMP | IORING_SETUP_ATTACH_WQ |\n\t\t\tIORING_SETUP_R_DISABLED))\n\t\treturn -EINVAL;\n\n\treturn  io_uring_create(entries, &p, params);\n}\n\nSYSCALL_DEFINE2(io_uring_setup, u32, entries,\n\t\tstruct io_uring_params __user *, params)\n{\n\treturn io_uring_setup(entries, params);\n}\n\nstatic int io_probe(struct io_ring_ctx *ctx, void __user *arg, unsigned nr_args)\n{\n\tstruct io_uring_probe *p;\n\tsize_t size;\n\tint i, ret;\n\n\tsize = struct_size(p, ops, nr_args);\n\tif (size == SIZE_MAX)\n\t\treturn -EOVERFLOW;\n\tp = kzalloc(size, GFP_KERNEL);\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tret = -EFAULT;\n\tif (copy_from_user(p, arg, size))\n\t\tgoto out;\n\tret = -EINVAL;\n\tif (memchr_inv(p, 0, size))\n\t\tgoto out;\n\n\tp->last_op = IORING_OP_LAST - 1;\n\tif (nr_args > IORING_OP_LAST)\n\t\tnr_args = IORING_OP_LAST;\n\n\tfor (i = 0; i < nr_args; i++) {\n\t\tp->ops[i].op = i;\n\t\tif (!io_op_defs[i].not_supported)\n\t\t\tp->ops[i].flags = IO_URING_OP_SUPPORTED;\n\t}\n\tp->ops_len = i;\n\n\tret = 0;\n\tif (copy_to_user(arg, p, size))\n\t\tret = -EFAULT;\nout:\n\tkfree(p);\n\treturn ret;\n}\n\nstatic int io_register_personality(struct io_ring_ctx *ctx)\n{\n\tstruct io_identity *id;\n\tint ret;\n\n\tid = kmalloc(sizeof(*id), GFP_KERNEL);\n\tif (unlikely(!id))\n\t\treturn -ENOMEM;\n\n\tio_init_identity(id);\n\tid->creds = get_current_cred();\n\n\tret = idr_alloc_cyclic(&ctx->personality_idr, id, 1, USHRT_MAX, GFP_KERNEL);\n\tif (ret < 0) {\n\t\tput_cred(id->creds);\n\t\tkfree(id);\n\t}\n\treturn ret;\n}\n\nstatic int io_unregister_personality(struct io_ring_ctx *ctx, unsigned id)\n{\n\tstruct io_identity *iod;\n\n\tiod = idr_remove(&ctx->personality_idr, id);\n\tif (iod) {\n\t\tput_cred(iod->creds);\n\t\tif (refcount_dec_and_test(&iod->count))\n\t\t\tkfree(iod);\n\t\treturn 0;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int io_register_restrictions(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t\t    unsigned int nr_args)\n{\n\tstruct io_uring_restriction *res;\n\tsize_t size;\n\tint i, ret;\n\n\t/* Restrictions allowed only if rings started disabled */\n\tif (!(ctx->flags & IORING_SETUP_R_DISABLED))\n\t\treturn -EBADFD;\n\n\t/* We allow only a single restrictions registration */\n\tif (ctx->restrictions.registered)\n\t\treturn -EBUSY;\n\n\tif (!arg || nr_args > IORING_MAX_RESTRICTIONS)\n\t\treturn -EINVAL;\n\n\tsize = array_size(nr_args, sizeof(*res));\n\tif (size == SIZE_MAX)\n\t\treturn -EOVERFLOW;\n\n\tres = memdup_user(arg, size);\n\tif (IS_ERR(res))\n\t\treturn PTR_ERR(res);\n\n\tret = 0;\n\n\tfor (i = 0; i < nr_args; i++) {\n\t\tswitch (res[i].opcode) {\n\t\tcase IORING_RESTRICTION_REGISTER_OP:\n\t\t\tif (res[i].register_op >= IORING_REGISTER_LAST) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t__set_bit(res[i].register_op,\n\t\t\t\t  ctx->restrictions.register_op);\n\t\t\tbreak;\n\t\tcase IORING_RESTRICTION_SQE_OP:\n\t\t\tif (res[i].sqe_op >= IORING_OP_LAST) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t__set_bit(res[i].sqe_op, ctx->restrictions.sqe_op);\n\t\t\tbreak;\n\t\tcase IORING_RESTRICTION_SQE_FLAGS_ALLOWED:\n\t\t\tctx->restrictions.sqe_flags_allowed = res[i].sqe_flags;\n\t\t\tbreak;\n\t\tcase IORING_RESTRICTION_SQE_FLAGS_REQUIRED:\n\t\t\tctx->restrictions.sqe_flags_required = res[i].sqe_flags;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\t/* Reset all restrictions if an error happened */\n\tif (ret != 0)\n\t\tmemset(&ctx->restrictions, 0, sizeof(ctx->restrictions));\n\telse\n\t\tctx->restrictions.registered = true;\n\n\tkfree(res);\n\treturn ret;\n}\n\nstatic int io_register_enable_rings(struct io_ring_ctx *ctx)\n{\n\tif (!(ctx->flags & IORING_SETUP_R_DISABLED))\n\t\treturn -EBADFD;\n\n\tif (ctx->restrictions.registered)\n\t\tctx->restricted = 1;\n\n\tctx->flags &= ~IORING_SETUP_R_DISABLED;\n\n\tio_sq_offload_start(ctx);\n\n\treturn 0;\n}\n\nstatic bool io_register_op_must_quiesce(int op)\n{\n\tswitch (op) {\n\tcase IORING_UNREGISTER_FILES:\n\tcase IORING_REGISTER_FILES_UPDATE:\n\tcase IORING_REGISTER_PROBE:\n\tcase IORING_REGISTER_PERSONALITY:\n\tcase IORING_UNREGISTER_PERSONALITY:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}\n\nstatic int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,\n\t\t\t       void __user *arg, unsigned nr_args)\n\t__releases(ctx->uring_lock)\n\t__acquires(ctx->uring_lock)\n{\n\tint ret;\n\n\t/*\n\t * We're inside the ring mutex, if the ref is already dying, then\n\t * someone else killed the ctx or is already going through\n\t * io_uring_register().\n\t */\n\tif (percpu_ref_is_dying(&ctx->refs))\n\t\treturn -ENXIO;\n\n\tif (io_register_op_must_quiesce(opcode)) {\n\t\tpercpu_ref_kill(&ctx->refs);\n\n\t\t/*\n\t\t * Drop uring mutex before waiting for references to exit. If\n\t\t * another thread is currently inside io_uring_enter() it might\n\t\t * need to grab the uring_lock to make progress. If we hold it\n\t\t * here across the drain wait, then we can deadlock. It's safe\n\t\t * to drop the mutex here, since no new references will come in\n\t\t * after we've killed the percpu ref.\n\t\t */\n\t\tmutex_unlock(&ctx->uring_lock);\n\t\tdo {\n\t\t\tret = wait_for_completion_interruptible(&ctx->ref_comp);\n\t\t\tif (!ret)\n\t\t\t\tbreak;\n\t\t\tret = io_run_task_work_sig();\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t} while (1);\n\n\t\tmutex_lock(&ctx->uring_lock);\n\n\t\tif (ret) {\n\t\t\tpercpu_ref_resurrect(&ctx->refs);\n\t\t\tgoto out_quiesce;\n\t\t}\n\t}\n\n\tif (ctx->restricted) {\n\t\tif (opcode >= IORING_REGISTER_LAST) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (!test_bit(opcode, ctx->restrictions.register_op)) {\n\t\t\tret = -EACCES;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tswitch (opcode) {\n\tcase IORING_REGISTER_BUFFERS:\n\t\tret = io_sqe_buffer_register(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_UNREGISTER_BUFFERS:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_sqe_buffer_unregister(ctx);\n\t\tbreak;\n\tcase IORING_REGISTER_FILES:\n\t\tret = io_sqe_files_register(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_UNREGISTER_FILES:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_sqe_files_unregister(ctx);\n\t\tbreak;\n\tcase IORING_REGISTER_FILES_UPDATE:\n\t\tret = io_sqe_files_update(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_REGISTER_EVENTFD:\n\tcase IORING_REGISTER_EVENTFD_ASYNC:\n\t\tret = -EINVAL;\n\t\tif (nr_args != 1)\n\t\t\tbreak;\n\t\tret = io_eventfd_register(ctx, arg);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (opcode == IORING_REGISTER_EVENTFD_ASYNC)\n\t\t\tctx->eventfd_async = 1;\n\t\telse\n\t\t\tctx->eventfd_async = 0;\n\t\tbreak;\n\tcase IORING_UNREGISTER_EVENTFD:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_eventfd_unregister(ctx);\n\t\tbreak;\n\tcase IORING_REGISTER_PROBE:\n\t\tret = -EINVAL;\n\t\tif (!arg || nr_args > 256)\n\t\t\tbreak;\n\t\tret = io_probe(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_REGISTER_PERSONALITY:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_register_personality(ctx);\n\t\tbreak;\n\tcase IORING_UNREGISTER_PERSONALITY:\n\t\tret = -EINVAL;\n\t\tif (arg)\n\t\t\tbreak;\n\t\tret = io_unregister_personality(ctx, nr_args);\n\t\tbreak;\n\tcase IORING_REGISTER_ENABLE_RINGS:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_register_enable_rings(ctx);\n\t\tbreak;\n\tcase IORING_REGISTER_RESTRICTIONS:\n\t\tret = io_register_restrictions(ctx, arg, nr_args);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\nout:\n\tif (io_register_op_must_quiesce(opcode)) {\n\t\t/* bring the ctx back to life */\n\t\tpercpu_ref_reinit(&ctx->refs);\nout_quiesce:\n\t\treinit_completion(&ctx->ref_comp);\n\t}\n\treturn ret;\n}\n\nSYSCALL_DEFINE4(io_uring_register, unsigned int, fd, unsigned int, opcode,\n\t\tvoid __user *, arg, unsigned int, nr_args)\n{\n\tstruct io_ring_ctx *ctx;\n\tlong ret = -EBADF;\n\tstruct fd f;\n\n\tf = fdget(fd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tret = -EOPNOTSUPP;\n\tif (f.file->f_op != &io_uring_fops)\n\t\tgoto out_fput;\n\n\tctx = f.file->private_data;\n\n\tmutex_lock(&ctx->uring_lock);\n\tret = __io_uring_register(ctx, opcode, arg, nr_args);\n\tmutex_unlock(&ctx->uring_lock);\n\ttrace_io_uring_register(ctx, opcode, ctx->nr_user_files, ctx->nr_user_bufs,\n\t\t\t\t\t\t\tctx->cq_ev_fd != NULL, ret);\nout_fput:\n\tfdput(f);\n\treturn ret;\n}\n\nstatic int __init io_uring_init(void)\n{\n#define __BUILD_BUG_VERIFY_ELEMENT(stype, eoffset, etype, ename) do { \\\n\tBUILD_BUG_ON(offsetof(stype, ename) != eoffset); \\\n\tBUILD_BUG_ON(sizeof(etype) != sizeof_field(stype, ename)); \\\n} while (0)\n\n#define BUILD_BUG_SQE_ELEM(eoffset, etype, ename) \\\n\t__BUILD_BUG_VERIFY_ELEMENT(struct io_uring_sqe, eoffset, etype, ename)\n\tBUILD_BUG_ON(sizeof(struct io_uring_sqe) != 64);\n\tBUILD_BUG_SQE_ELEM(0,  __u8,   opcode);\n\tBUILD_BUG_SQE_ELEM(1,  __u8,   flags);\n\tBUILD_BUG_SQE_ELEM(2,  __u16,  ioprio);\n\tBUILD_BUG_SQE_ELEM(4,  __s32,  fd);\n\tBUILD_BUG_SQE_ELEM(8,  __u64,  off);\n\tBUILD_BUG_SQE_ELEM(8,  __u64,  addr2);\n\tBUILD_BUG_SQE_ELEM(16, __u64,  addr);\n\tBUILD_BUG_SQE_ELEM(16, __u64,  splice_off_in);\n\tBUILD_BUG_SQE_ELEM(24, __u32,  len);\n\tBUILD_BUG_SQE_ELEM(28,     __kernel_rwf_t, rw_flags);\n\tBUILD_BUG_SQE_ELEM(28, /* compat */   int, rw_flags);\n\tBUILD_BUG_SQE_ELEM(28, /* compat */ __u32, rw_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  fsync_flags);\n\tBUILD_BUG_SQE_ELEM(28, /* compat */ __u16,  poll_events);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  poll32_events);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  sync_range_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  msg_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  timeout_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  accept_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  cancel_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  open_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  statx_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  fadvise_advice);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  splice_flags);\n\tBUILD_BUG_SQE_ELEM(32, __u64,  user_data);\n\tBUILD_BUG_SQE_ELEM(40, __u16,  buf_index);\n\tBUILD_BUG_SQE_ELEM(42, __u16,  personality);\n\tBUILD_BUG_SQE_ELEM(44, __s32,  splice_fd_in);\n\n\tBUILD_BUG_ON(ARRAY_SIZE(io_op_defs) != IORING_OP_LAST);\n\tBUILD_BUG_ON(__REQ_F_LAST_BIT >= 8 * sizeof(int));\n\treq_cachep = KMEM_CACHE(io_kiocb, SLAB_HWCACHE_ALIGN | SLAB_PANIC);\n\treturn 0;\n};\n__initcall(io_uring_init);\n"}, "7": {"id": 7, "path": "/src/include/linux/lockdep.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n/*\n * Runtime locking correctness validator\n *\n *  Copyright (C) 2006,2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>\n *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra\n *\n * see Documentation/locking/lockdep-design.rst for more details.\n */\n#ifndef __LINUX_LOCKDEP_H\n#define __LINUX_LOCKDEP_H\n\n#include <linux/lockdep_types.h>\n#include <linux/smp.h>\n#include <asm/percpu.h>\n\nstruct task_struct;\n\n/* for sysctl */\nextern int prove_locking;\nextern int lock_stat;\n\n#ifdef CONFIG_LOCKDEP\n\n#include <linux/linkage.h>\n#include <linux/list.h>\n#include <linux/debug_locks.h>\n#include <linux/stacktrace.h>\n\nstatic inline void lockdep_copy_map(struct lockdep_map *to,\n\t\t\t\t    struct lockdep_map *from)\n{\n\tint i;\n\n\t*to = *from;\n\t/*\n\t * Since the class cache can be modified concurrently we could observe\n\t * half pointers (64bit arch using 32bit copy insns). Therefore clear\n\t * the caches and take the performance hit.\n\t *\n\t * XXX it doesn't work well with lockdep_set_class_and_subclass(), since\n\t *     that relies on cache abuse.\n\t */\n\tfor (i = 0; i < NR_LOCKDEP_CACHING_CLASSES; i++)\n\t\tto->class_cache[i] = NULL;\n}\n\n/*\n * Every lock has a list of other locks that were taken after it.\n * We only grow the list, never remove from it:\n */\nstruct lock_list {\n\tstruct list_head\t\tentry;\n\tstruct lock_class\t\t*class;\n\tstruct lock_class\t\t*links_to;\n\tconst struct lock_trace\t\t*trace;\n\tu16\t\t\t\tdistance;\n\t/* bitmap of different dependencies from head to this */\n\tu8\t\t\t\tdep;\n\t/* used by BFS to record whether \"prev -> this\" only has -(*R)-> */\n\tu8\t\t\t\tonly_xr;\n\n\t/*\n\t * The parent field is used to implement breadth-first search, and the\n\t * bit 0 is reused to indicate if the lock has been accessed in BFS.\n\t */\n\tstruct lock_list\t\t*parent;\n};\n\n/**\n * struct lock_chain - lock dependency chain record\n *\n * @irq_context: the same as irq_context in held_lock below\n * @depth:       the number of held locks in this chain\n * @base:        the index in chain_hlocks for this chain\n * @entry:       the collided lock chains in lock_chain hash list\n * @chain_key:   the hash key of this lock_chain\n */\nstruct lock_chain {\n\t/* see BUILD_BUG_ON()s in add_chain_cache() */\n\tunsigned int\t\t\tirq_context :  2,\n\t\t\t\t\tdepth       :  6,\n\t\t\t\t\tbase\t    : 24;\n\t/* 4 byte hole */\n\tstruct hlist_node\t\tentry;\n\tu64\t\t\t\tchain_key;\n};\n\n#define MAX_LOCKDEP_KEYS_BITS\t\t13\n#define MAX_LOCKDEP_KEYS\t\t(1UL << MAX_LOCKDEP_KEYS_BITS)\n#define INITIAL_CHAIN_KEY\t\t-1\n\nstruct held_lock {\n\t/*\n\t * One-way hash of the dependency chain up to this point. We\n\t * hash the hashes step by step as the dependency chain grows.\n\t *\n\t * We use it for dependency-caching and we skip detection\n\t * passes and dependency-updates if there is a cache-hit, so\n\t * it is absolutely critical for 100% coverage of the validator\n\t * to have a unique key value for every unique dependency path\n\t * that can occur in the system, to make a unique hash value\n\t * as likely as possible - hence the 64-bit width.\n\t *\n\t * The task struct holds the current hash value (initialized\n\t * with zero), here we store the previous hash value:\n\t */\n\tu64\t\t\t\tprev_chain_key;\n\tunsigned long\t\t\tacquire_ip;\n\tstruct lockdep_map\t\t*instance;\n\tstruct lockdep_map\t\t*nest_lock;\n#ifdef CONFIG_LOCK_STAT\n\tu64 \t\t\t\twaittime_stamp;\n\tu64\t\t\t\tholdtime_stamp;\n#endif\n\t/*\n\t * class_idx is zero-indexed; it points to the element in\n\t * lock_classes this held lock instance belongs to. class_idx is in\n\t * the range from 0 to (MAX_LOCKDEP_KEYS-1) inclusive.\n\t */\n\tunsigned int\t\t\tclass_idx:MAX_LOCKDEP_KEYS_BITS;\n\t/*\n\t * The lock-stack is unified in that the lock chains of interrupt\n\t * contexts nest ontop of process context chains, but we 'separate'\n\t * the hashes by starting with 0 if we cross into an interrupt\n\t * context, and we also keep do not add cross-context lock\n\t * dependencies - the lock usage graph walking covers that area\n\t * anyway, and we'd just unnecessarily increase the number of\n\t * dependencies otherwise. [Note: hardirq and softirq contexts\n\t * are separated from each other too.]\n\t *\n\t * The following field is used to detect when we cross into an\n\t * interrupt context:\n\t */\n\tunsigned int irq_context:2; /* bit 0 - soft, bit 1 - hard */\n\tunsigned int trylock:1;\t\t\t\t\t\t/* 16 bits */\n\n\tunsigned int read:2;        /* see lock_acquire() comment */\n\tunsigned int check:1;       /* see lock_acquire() comment */\n\tunsigned int hardirqs_off:1;\n\tunsigned int references:12;\t\t\t\t\t/* 32 bits */\n\tunsigned int pin_count;\n};\n\n/*\n * Initialization, self-test and debugging-output methods:\n */\nextern void lockdep_init(void);\nextern void lockdep_reset(void);\nextern void lockdep_reset_lock(struct lockdep_map *lock);\nextern void lockdep_free_key_range(void *start, unsigned long size);\nextern asmlinkage void lockdep_sys_exit(void);\nextern void lockdep_set_selftest_task(struct task_struct *task);\n\nextern void lockdep_init_task(struct task_struct *task);\n\n/*\n * Split the recrursion counter in two to readily detect 'off' vs recursion.\n */\n#define LOCKDEP_RECURSION_BITS\t16\n#define LOCKDEP_OFF\t\t(1U << LOCKDEP_RECURSION_BITS)\n#define LOCKDEP_RECURSION_MASK\t(LOCKDEP_OFF - 1)\n\n/*\n * lockdep_{off,on}() are macros to avoid tracing and kprobes; not inlines due\n * to header dependencies.\n */\n\n#define lockdep_off()\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\\\n\tcurrent->lockdep_recursion += LOCKDEP_OFF;\t\\\n} while (0)\n\n#define lockdep_on()\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\\\n\tcurrent->lockdep_recursion -= LOCKDEP_OFF;\t\\\n} while (0)\n\nextern void lockdep_register_key(struct lock_class_key *key);\nextern void lockdep_unregister_key(struct lock_class_key *key);\n\n/*\n * These methods are used by specific locking variants (spinlocks,\n * rwlocks, mutexes and rwsems) to pass init/acquire/release events\n * to lockdep:\n */\n\nextern void lockdep_init_map_waits(struct lockdep_map *lock, const char *name,\n\tstruct lock_class_key *key, int subclass, short inner, short outer);\n\nstatic inline void\nlockdep_init_map_wait(struct lockdep_map *lock, const char *name,\n\t\t      struct lock_class_key *key, int subclass, short inner)\n{\n\tlockdep_init_map_waits(lock, name, key, subclass, inner, LD_WAIT_INV);\n}\n\nstatic inline void lockdep_init_map(struct lockdep_map *lock, const char *name,\n\t\t\t     struct lock_class_key *key, int subclass)\n{\n\tlockdep_init_map_wait(lock, name, key, subclass, LD_WAIT_INV);\n}\n\n/*\n * Reinitialize a lock key - for cases where there is special locking or\n * special initialization of locks so that the validator gets the scope\n * of dependencies wrong: they are either too broad (they need a class-split)\n * or they are too narrow (they suffer from a false class-split):\n */\n#define lockdep_set_class(lock, key)\t\t\t\t\\\n\tlockdep_init_map_waits(&(lock)->dep_map, #key, key, 0,\t\\\n\t\t\t       (lock)->dep_map.wait_type_inner,\t\\\n\t\t\t       (lock)->dep_map.wait_type_outer)\n\n#define lockdep_set_class_and_name(lock, key, name)\t\t\\\n\tlockdep_init_map_waits(&(lock)->dep_map, name, key, 0,\t\\\n\t\t\t       (lock)->dep_map.wait_type_inner,\t\\\n\t\t\t       (lock)->dep_map.wait_type_outer)\n\n#define lockdep_set_class_and_subclass(lock, key, sub)\t\t\\\n\tlockdep_init_map_waits(&(lock)->dep_map, #key, key, sub,\\\n\t\t\t       (lock)->dep_map.wait_type_inner,\t\\\n\t\t\t       (lock)->dep_map.wait_type_outer)\n\n#define lockdep_set_subclass(lock, sub)\t\t\t\t\t\\\n\tlockdep_init_map_waits(&(lock)->dep_map, #lock, (lock)->dep_map.key, sub,\\\n\t\t\t       (lock)->dep_map.wait_type_inner,\t\t\\\n\t\t\t       (lock)->dep_map.wait_type_outer)\n\n#define lockdep_set_novalidate_class(lock) \\\n\tlockdep_set_class_and_name(lock, &__lockdep_no_validate__, #lock)\n\n/*\n * Compare locking classes\n */\n#define lockdep_match_class(lock, key) lockdep_match_key(&(lock)->dep_map, key)\n\nstatic inline int lockdep_match_key(struct lockdep_map *lock,\n\t\t\t\t    struct lock_class_key *key)\n{\n\treturn lock->key == key;\n}\n\n/*\n * Acquire a lock.\n *\n * Values for \"read\":\n *\n *   0: exclusive (write) acquire\n *   1: read-acquire (no recursion allowed)\n *   2: read-acquire with same-instance recursion allowed\n *\n * Values for check:\n *\n *   0: simple checks (freeing, held-at-exit-time, etc.)\n *   1: full validation\n */\nextern void lock_acquire(struct lockdep_map *lock, unsigned int subclass,\n\t\t\t int trylock, int read, int check,\n\t\t\t struct lockdep_map *nest_lock, unsigned long ip);\n\nextern void lock_release(struct lockdep_map *lock, unsigned long ip);\n\n/*\n * Same \"read\" as for lock_acquire(), except -1 means any.\n */\nextern int lock_is_held_type(const struct lockdep_map *lock, int read);\n\nstatic inline int lock_is_held(const struct lockdep_map *lock)\n{\n\treturn lock_is_held_type(lock, -1);\n}\n\n#define lockdep_is_held(lock)\t\tlock_is_held(&(lock)->dep_map)\n#define lockdep_is_held_type(lock, r)\tlock_is_held_type(&(lock)->dep_map, (r))\n\nextern void lock_set_class(struct lockdep_map *lock, const char *name,\n\t\t\t   struct lock_class_key *key, unsigned int subclass,\n\t\t\t   unsigned long ip);\n\nstatic inline void lock_set_subclass(struct lockdep_map *lock,\n\t\tunsigned int subclass, unsigned long ip)\n{\n\tlock_set_class(lock, lock->name, lock->key, subclass, ip);\n}\n\nextern void lock_downgrade(struct lockdep_map *lock, unsigned long ip);\n\n#define NIL_COOKIE (struct pin_cookie){ .val = 0U, }\n\nextern struct pin_cookie lock_pin_lock(struct lockdep_map *lock);\nextern void lock_repin_lock(struct lockdep_map *lock, struct pin_cookie);\nextern void lock_unpin_lock(struct lockdep_map *lock, struct pin_cookie);\n\n#define lockdep_depth(tsk)\t(debug_locks ? (tsk)->lockdep_depth : 0)\n\n#define lockdep_assert_held(l)\tdo {\t\t\t\t\\\n\t\tWARN_ON(debug_locks && !lockdep_is_held(l));\t\\\n\t} while (0)\n\n#define lockdep_assert_held_write(l)\tdo {\t\t\t\\\n\t\tWARN_ON(debug_locks && !lockdep_is_held_type(l, 0));\t\\\n\t} while (0)\n\n#define lockdep_assert_held_read(l)\tdo {\t\t\t\t\\\n\t\tWARN_ON(debug_locks && !lockdep_is_held_type(l, 1));\t\\\n\t} while (0)\n\n#define lockdep_assert_held_once(l)\tdo {\t\t\t\t\\\n\t\tWARN_ON_ONCE(debug_locks && !lockdep_is_held(l));\t\\\n\t} while (0)\n\n#define lockdep_recursing(tsk)\t((tsk)->lockdep_recursion)\n\n#define lockdep_pin_lock(l)\tlock_pin_lock(&(l)->dep_map)\n#define lockdep_repin_lock(l,c)\tlock_repin_lock(&(l)->dep_map, (c))\n#define lockdep_unpin_lock(l,c)\tlock_unpin_lock(&(l)->dep_map, (c))\n\n#else /* !CONFIG_LOCKDEP */\n\nstatic inline void lockdep_init_task(struct task_struct *task)\n{\n}\n\nstatic inline void lockdep_off(void)\n{\n}\n\nstatic inline void lockdep_on(void)\n{\n}\n\nstatic inline void lockdep_set_selftest_task(struct task_struct *task)\n{\n}\n\n# define lock_acquire(l, s, t, r, c, n, i)\tdo { } while (0)\n# define lock_release(l, i)\t\t\tdo { } while (0)\n# define lock_downgrade(l, i)\t\t\tdo { } while (0)\n# define lock_set_class(l, n, k, s, i)\t\tdo { } while (0)\n# define lock_set_subclass(l, s, i)\t\tdo { } while (0)\n# define lockdep_init()\t\t\t\tdo { } while (0)\n# define lockdep_init_map_waits(lock, name, key, sub, inner, outer) \\\n\t\tdo { (void)(name); (void)(key); } while (0)\n# define lockdep_init_map_wait(lock, name, key, sub, inner) \\\n\t\tdo { (void)(name); (void)(key); } while (0)\n# define lockdep_init_map(lock, name, key, sub) \\\n\t\tdo { (void)(name); (void)(key); } while (0)\n# define lockdep_set_class(lock, key)\t\tdo { (void)(key); } while (0)\n# define lockdep_set_class_and_name(lock, key, name) \\\n\t\tdo { (void)(key); (void)(name); } while (0)\n#define lockdep_set_class_and_subclass(lock, key, sub) \\\n\t\tdo { (void)(key); } while (0)\n#define lockdep_set_subclass(lock, sub)\t\tdo { } while (0)\n\n#define lockdep_set_novalidate_class(lock) do { } while (0)\n\n/*\n * We don't define lockdep_match_class() and lockdep_match_key() for !LOCKDEP\n * case since the result is not well defined and the caller should rather\n * #ifdef the call himself.\n */\n\n# define lockdep_reset()\t\tdo { debug_locks = 1; } while (0)\n# define lockdep_free_key_range(start, size)\tdo { } while (0)\n# define lockdep_sys_exit() \t\t\tdo { } while (0)\n\nstatic inline void lockdep_register_key(struct lock_class_key *key)\n{\n}\n\nstatic inline void lockdep_unregister_key(struct lock_class_key *key)\n{\n}\n\n#define lockdep_depth(tsk)\t(0)\n\n/*\n * Dummy forward declarations, allow users to write less ifdef-y code\n * and depend on dead code elimination.\n */\nextern int lock_is_held(const void *);\nextern int lockdep_is_held(const void *);\n#define lockdep_is_held_type(l, r)\t\t(1)\n\n#define lockdep_assert_held(l)\t\t\tdo { (void)(l); } while (0)\n#define lockdep_assert_held_write(l)\tdo { (void)(l); } while (0)\n#define lockdep_assert_held_read(l)\t\tdo { (void)(l); } while (0)\n#define lockdep_assert_held_once(l)\t\tdo { (void)(l); } while (0)\n\n#define lockdep_recursing(tsk)\t\t\t(0)\n\n#define NIL_COOKIE (struct pin_cookie){ }\n\n#define lockdep_pin_lock(l)\t\t\t({ struct pin_cookie cookie = { }; cookie; })\n#define lockdep_repin_lock(l, c)\t\tdo { (void)(l); (void)(c); } while (0)\n#define lockdep_unpin_lock(l, c)\t\tdo { (void)(l); (void)(c); } while (0)\n\n#endif /* !LOCKDEP */\n\nenum xhlock_context_t {\n\tXHLOCK_HARD,\n\tXHLOCK_SOFT,\n\tXHLOCK_CTX_NR,\n};\n\n#define lockdep_init_map_crosslock(m, n, k, s) do {} while (0)\n/*\n * To initialize a lockdep_map statically use this macro.\n * Note that _name must not be NULL.\n */\n#define STATIC_LOCKDEP_MAP_INIT(_name, _key) \\\n\t{ .name = (_name), .key = (void *)(_key), }\n\nstatic inline void lockdep_invariant_state(bool force) {}\nstatic inline void lockdep_free_task(struct task_struct *task) {}\n\n#ifdef CONFIG_LOCK_STAT\n\nextern void lock_contended(struct lockdep_map *lock, unsigned long ip);\nextern void lock_acquired(struct lockdep_map *lock, unsigned long ip);\n\n#define LOCK_CONTENDED(_lock, try, lock)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (!try(_lock)) {\t\t\t\t\t\\\n\t\tlock_contended(&(_lock)->dep_map, _RET_IP_);\t\\\n\t\tlock(_lock);\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tlock_acquired(&(_lock)->dep_map, _RET_IP_);\t\t\t\\\n} while (0)\n\n#define LOCK_CONTENDED_RETURN(_lock, try, lock)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tint ____err = 0;\t\t\t\t\t\\\n\tif (!try(_lock)) {\t\t\t\t\t\\\n\t\tlock_contended(&(_lock)->dep_map, _RET_IP_);\t\\\n\t\t____err = lock(_lock);\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tif (!____err)\t\t\t\t\t\t\\\n\t\tlock_acquired(&(_lock)->dep_map, _RET_IP_);\t\\\n\t____err;\t\t\t\t\t\t\\\n})\n\n#else /* CONFIG_LOCK_STAT */\n\n#define lock_contended(lockdep_map, ip) do {} while (0)\n#define lock_acquired(lockdep_map, ip) do {} while (0)\n\n#define LOCK_CONTENDED(_lock, try, lock) \\\n\tlock(_lock)\n\n#define LOCK_CONTENDED_RETURN(_lock, try, lock) \\\n\tlock(_lock)\n\n#endif /* CONFIG_LOCK_STAT */\n\n#ifdef CONFIG_LOCKDEP\n\n/*\n * On lockdep we dont want the hand-coded irq-enable of\n * _raw_*_lock_flags() code, because lockdep assumes\n * that interrupts are not re-enabled during lock-acquire:\n */\n#define LOCK_CONTENDED_FLAGS(_lock, try, lock, lockfl, flags) \\\n\tLOCK_CONTENDED((_lock), (try), (lock))\n\n#else /* CONFIG_LOCKDEP */\n\n#define LOCK_CONTENDED_FLAGS(_lock, try, lock, lockfl, flags) \\\n\tlockfl((_lock), (flags))\n\n#endif /* CONFIG_LOCKDEP */\n\n#ifdef CONFIG_PROVE_LOCKING\nextern void print_irqtrace_events(struct task_struct *curr);\n#else\nstatic inline void print_irqtrace_events(struct task_struct *curr)\n{\n}\n#endif\n\n/* Variable used to make lockdep treat read_lock() as recursive in selftests */\n#ifdef CONFIG_DEBUG_LOCKING_API_SELFTESTS\nextern unsigned int force_read_lock_recursive;\n#else /* CONFIG_DEBUG_LOCKING_API_SELFTESTS */\n#define force_read_lock_recursive 0\n#endif /* CONFIG_DEBUG_LOCKING_API_SELFTESTS */\n\n#ifdef CONFIG_LOCKDEP\nextern bool read_lock_is_recursive(void);\n#else /* CONFIG_LOCKDEP */\n/* If !LOCKDEP, the value is meaningless */\n#define read_lock_is_recursive() 0\n#endif\n\n/*\n * For trivial one-depth nesting of a lock-class, the following\n * global define can be used. (Subsystems with multiple levels\n * of nesting should define their own lock-nesting subclasses.)\n */\n#define SINGLE_DEPTH_NESTING\t\t\t1\n\n/*\n * Map the dependency ops to NOP or to real lockdep ops, depending\n * on the per lock-class debug mode:\n */\n\n#define lock_acquire_exclusive(l, s, t, n, i)\t\tlock_acquire(l, s, t, 0, 1, n, i)\n#define lock_acquire_shared(l, s, t, n, i)\t\tlock_acquire(l, s, t, 1, 1, n, i)\n#define lock_acquire_shared_recursive(l, s, t, n, i)\tlock_acquire(l, s, t, 2, 1, n, i)\n\n#define spin_acquire(l, s, t, i)\t\tlock_acquire_exclusive(l, s, t, NULL, i)\n#define spin_acquire_nest(l, s, t, n, i)\tlock_acquire_exclusive(l, s, t, n, i)\n#define spin_release(l, i)\t\t\tlock_release(l, i)\n\n#define rwlock_acquire(l, s, t, i)\t\tlock_acquire_exclusive(l, s, t, NULL, i)\n#define rwlock_acquire_read(l, s, t, i)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (read_lock_is_recursive())\t\t\t\t\t\\\n\t\tlock_acquire_shared_recursive(l, s, t, NULL, i);\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\tlock_acquire_shared(l, s, t, NULL, i);\t\t\t\\\n} while (0)\n\n#define rwlock_release(l, i)\t\t\tlock_release(l, i)\n\n#define seqcount_acquire(l, s, t, i)\t\tlock_acquire_exclusive(l, s, t, NULL, i)\n#define seqcount_acquire_read(l, s, t, i)\tlock_acquire_shared_recursive(l, s, t, NULL, i)\n#define seqcount_release(l, i)\t\t\tlock_release(l, i)\n\n#define mutex_acquire(l, s, t, i)\t\tlock_acquire_exclusive(l, s, t, NULL, i)\n#define mutex_acquire_nest(l, s, t, n, i)\tlock_acquire_exclusive(l, s, t, n, i)\n#define mutex_release(l, i)\t\t\tlock_release(l, i)\n\n#define rwsem_acquire(l, s, t, i)\t\tlock_acquire_exclusive(l, s, t, NULL, i)\n#define rwsem_acquire_nest(l, s, t, n, i)\tlock_acquire_exclusive(l, s, t, n, i)\n#define rwsem_acquire_read(l, s, t, i)\t\tlock_acquire_shared(l, s, t, NULL, i)\n#define rwsem_release(l, i)\t\t\tlock_release(l, i)\n\n#define lock_map_acquire(l)\t\t\tlock_acquire_exclusive(l, 0, 0, NULL, _THIS_IP_)\n#define lock_map_acquire_read(l)\t\tlock_acquire_shared_recursive(l, 0, 0, NULL, _THIS_IP_)\n#define lock_map_acquire_tryread(l)\t\tlock_acquire_shared_recursive(l, 0, 1, NULL, _THIS_IP_)\n#define lock_map_release(l)\t\t\tlock_release(l, _THIS_IP_)\n\n#ifdef CONFIG_PROVE_LOCKING\n# define might_lock(lock)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\ttypecheck(struct lockdep_map *, &(lock)->dep_map);\t\t\\\n\tlock_acquire(&(lock)->dep_map, 0, 0, 0, 1, NULL, _THIS_IP_);\t\\\n\tlock_release(&(lock)->dep_map, _THIS_IP_);\t\t\t\\\n} while (0)\n# define might_lock_read(lock)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\ttypecheck(struct lockdep_map *, &(lock)->dep_map);\t\t\\\n\tlock_acquire(&(lock)->dep_map, 0, 0, 1, 1, NULL, _THIS_IP_);\t\\\n\tlock_release(&(lock)->dep_map, _THIS_IP_);\t\t\t\\\n} while (0)\n# define might_lock_nested(lock, subclass)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\ttypecheck(struct lockdep_map *, &(lock)->dep_map);\t\t\\\n\tlock_acquire(&(lock)->dep_map, subclass, 0, 1, 1, NULL,\t\t\\\n\t\t     _THIS_IP_);\t\t\t\t\t\\\n\tlock_release(&(lock)->dep_map, _THIS_IP_);\t\t\t\\\n} while (0)\n\nDECLARE_PER_CPU(int, hardirqs_enabled);\nDECLARE_PER_CPU(int, hardirq_context);\nDECLARE_PER_CPU(unsigned int, lockdep_recursion);\n\n#define __lockdep_enabled\t(debug_locks && !this_cpu_read(lockdep_recursion))\n\n#define lockdep_assert_irqs_enabled()\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tWARN_ON_ONCE(__lockdep_enabled && !this_cpu_read(hardirqs_enabled)); \\\n} while (0)\n\n#define lockdep_assert_irqs_disabled()\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tWARN_ON_ONCE(__lockdep_enabled && this_cpu_read(hardirqs_enabled)); \\\n} while (0)\n\n#define lockdep_assert_in_irq()\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tWARN_ON_ONCE(__lockdep_enabled && !this_cpu_read(hardirq_context)); \\\n} while (0)\n\n#define lockdep_assert_preemption_enabled()\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tWARN_ON_ONCE(__lockdep_enabled\t\t\t&&\t\t\\\n\t\t     (preempt_count() != 0\t\t||\t\t\\\n\t\t      !this_cpu_read(hardirqs_enabled)));\t\t\\\n} while (0)\n\n#define lockdep_assert_preemption_disabled()\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tWARN_ON_ONCE(__lockdep_enabled\t\t\t&&\t\t\\\n\t\t     (preempt_count() == 0\t\t&&\t\t\\\n\t\t      this_cpu_read(hardirqs_enabled)));\t\t\\\n} while (0)\n\n#else\n# define might_lock(lock) do { } while (0)\n# define might_lock_read(lock) do { } while (0)\n# define might_lock_nested(lock, subclass) do { } while (0)\n\n# define lockdep_assert_irqs_enabled() do { } while (0)\n# define lockdep_assert_irqs_disabled() do { } while (0)\n# define lockdep_assert_in_irq() do { } while (0)\n\n# define lockdep_assert_preemption_enabled() do { } while (0)\n# define lockdep_assert_preemption_disabled() do { } while (0)\n#endif\n\n#ifdef CONFIG_PROVE_RAW_LOCK_NESTING\n\n# define lockdep_assert_RT_in_threaded_ctx() do {\t\t\t\\\n\t\tWARN_ONCE(debug_locks && !current->lockdep_recursion &&\t\\\n\t\t\t  lockdep_hardirq_context() &&\t\t\t\\\n\t\t\t  !(current->hardirq_threaded || current->irq_config),\t\\\n\t\t\t  \"Not in threaded context on PREEMPT_RT as expected\\n\");\t\\\n} while (0)\n\n#else\n\n# define lockdep_assert_RT_in_threaded_ctx() do { } while (0)\n\n#endif\n\n#ifdef CONFIG_LOCKDEP\nvoid lockdep_rcu_suspicious(const char *file, const int line, const char *s);\n#else\nstatic inline void\nlockdep_rcu_suspicious(const char *file, const int line, const char *s)\n{\n}\n#endif\n\n#endif /* __LINUX_LOCKDEP_H */\n"}, "8": {"id": 8, "path": "/src/mm/compaction.c", "content": "// SPDX-License-Identifier: GPL-2.0\n/*\n * linux/mm/compaction.c\n *\n * Memory compaction for the reduction of external fragmentation. Note that\n * this heavily depends upon page migration to do all the real heavy\n * lifting\n *\n * Copyright IBM Corp. 2007-2010 Mel Gorman <mel@csn.ul.ie>\n */\n#include <linux/cpu.h>\n#include <linux/swap.h>\n#include <linux/migrate.h>\n#include <linux/compaction.h>\n#include <linux/mm_inline.h>\n#include <linux/sched/signal.h>\n#include <linux/backing-dev.h>\n#include <linux/sysctl.h>\n#include <linux/sysfs.h>\n#include <linux/page-isolation.h>\n#include <linux/kasan.h>\n#include <linux/kthread.h>\n#include <linux/freezer.h>\n#include <linux/page_owner.h>\n#include <linux/psi.h>\n#include \"internal.h\"\n\n#ifdef CONFIG_COMPACTION\nstatic inline void count_compact_event(enum vm_event_item item)\n{\n\tcount_vm_event(item);\n}\n\nstatic inline void count_compact_events(enum vm_event_item item, long delta)\n{\n\tcount_vm_events(item, delta);\n}\n#else\n#define count_compact_event(item) do { } while (0)\n#define count_compact_events(item, delta) do { } while (0)\n#endif\n\n#if defined CONFIG_COMPACTION || defined CONFIG_CMA\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/compaction.h>\n\n#define block_start_pfn(pfn, order)\tround_down(pfn, 1UL << (order))\n#define block_end_pfn(pfn, order)\tALIGN((pfn) + 1, 1UL << (order))\n#define pageblock_start_pfn(pfn)\tblock_start_pfn(pfn, pageblock_order)\n#define pageblock_end_pfn(pfn)\t\tblock_end_pfn(pfn, pageblock_order)\n\n/*\n * Fragmentation score check interval for proactive compaction purposes.\n */\nstatic const unsigned int HPAGE_FRAG_CHECK_INTERVAL_MSEC = 500;\n\n/*\n * Page order with-respect-to which proactive compaction\n * calculates external fragmentation, which is used as\n * the \"fragmentation score\" of a node/zone.\n */\n#if defined CONFIG_TRANSPARENT_HUGEPAGE\n#define COMPACTION_HPAGE_ORDER\tHPAGE_PMD_ORDER\n#elif defined CONFIG_HUGETLBFS\n#define COMPACTION_HPAGE_ORDER\tHUGETLB_PAGE_ORDER\n#else\n#define COMPACTION_HPAGE_ORDER\t(PMD_SHIFT - PAGE_SHIFT)\n#endif\n\nstatic unsigned long release_freepages(struct list_head *freelist)\n{\n\tstruct page *page, *next;\n\tunsigned long high_pfn = 0;\n\n\tlist_for_each_entry_safe(page, next, freelist, lru) {\n\t\tunsigned long pfn = page_to_pfn(page);\n\t\tlist_del(&page->lru);\n\t\t__free_page(page);\n\t\tif (pfn > high_pfn)\n\t\t\thigh_pfn = pfn;\n\t}\n\n\treturn high_pfn;\n}\n\nstatic void split_map_pages(struct list_head *list)\n{\n\tunsigned int i, order, nr_pages;\n\tstruct page *page, *next;\n\tLIST_HEAD(tmp_list);\n\n\tlist_for_each_entry_safe(page, next, list, lru) {\n\t\tlist_del(&page->lru);\n\n\t\torder = page_private(page);\n\t\tnr_pages = 1 << order;\n\n\t\tpost_alloc_hook(page, order, __GFP_MOVABLE);\n\t\tif (order)\n\t\t\tsplit_page(page, order);\n\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tlist_add(&page->lru, &tmp_list);\n\t\t\tpage++;\n\t\t}\n\t}\n\n\tlist_splice(&tmp_list, list);\n}\n\n#ifdef CONFIG_COMPACTION\n\nint PageMovable(struct page *page)\n{\n\tstruct address_space *mapping;\n\n\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\tif (!__PageMovable(page))\n\t\treturn 0;\n\n\tmapping = page_mapping(page);\n\tif (mapping && mapping->a_ops && mapping->a_ops->isolate_page)\n\t\treturn 1;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(PageMovable);\n\nvoid __SetPageMovable(struct page *page, struct address_space *mapping)\n{\n\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\tVM_BUG_ON_PAGE((unsigned long)mapping & PAGE_MAPPING_MOVABLE, page);\n\tpage->mapping = (void *)((unsigned long)mapping | PAGE_MAPPING_MOVABLE);\n}\nEXPORT_SYMBOL(__SetPageMovable);\n\nvoid __ClearPageMovable(struct page *page)\n{\n\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\tVM_BUG_ON_PAGE(!PageMovable(page), page);\n\t/*\n\t * Clear registered address_space val with keeping PAGE_MAPPING_MOVABLE\n\t * flag so that VM can catch up released page by driver after isolation.\n\t * With it, VM migration doesn't try to put it back.\n\t */\n\tpage->mapping = (void *)((unsigned long)page->mapping &\n\t\t\t\tPAGE_MAPPING_MOVABLE);\n}\nEXPORT_SYMBOL(__ClearPageMovable);\n\n/* Do not skip compaction more than 64 times */\n#define COMPACT_MAX_DEFER_SHIFT 6\n\n/*\n * Compaction is deferred when compaction fails to result in a page\n * allocation success. 1 << compact_defer_shift, compactions are skipped up\n * to a limit of 1 << COMPACT_MAX_DEFER_SHIFT\n */\nstatic void defer_compaction(struct zone *zone, int order)\n{\n\tzone->compact_considered = 0;\n\tzone->compact_defer_shift++;\n\n\tif (order < zone->compact_order_failed)\n\t\tzone->compact_order_failed = order;\n\n\tif (zone->compact_defer_shift > COMPACT_MAX_DEFER_SHIFT)\n\t\tzone->compact_defer_shift = COMPACT_MAX_DEFER_SHIFT;\n\n\ttrace_mm_compaction_defer_compaction(zone, order);\n}\n\n/* Returns true if compaction should be skipped this time */\nstatic bool compaction_deferred(struct zone *zone, int order)\n{\n\tunsigned long defer_limit = 1UL << zone->compact_defer_shift;\n\n\tif (order < zone->compact_order_failed)\n\t\treturn false;\n\n\t/* Avoid possible overflow */\n\tif (++zone->compact_considered >= defer_limit) {\n\t\tzone->compact_considered = defer_limit;\n\t\treturn false;\n\t}\n\n\ttrace_mm_compaction_deferred(zone, order);\n\n\treturn true;\n}\n\n/*\n * Update defer tracking counters after successful compaction of given order,\n * which means an allocation either succeeded (alloc_success == true) or is\n * expected to succeed.\n */\nvoid compaction_defer_reset(struct zone *zone, int order,\n\t\tbool alloc_success)\n{\n\tif (alloc_success) {\n\t\tzone->compact_considered = 0;\n\t\tzone->compact_defer_shift = 0;\n\t}\n\tif (order >= zone->compact_order_failed)\n\t\tzone->compact_order_failed = order + 1;\n\n\ttrace_mm_compaction_defer_reset(zone, order);\n}\n\n/* Returns true if restarting compaction after many failures */\nstatic bool compaction_restarting(struct zone *zone, int order)\n{\n\tif (order < zone->compact_order_failed)\n\t\treturn false;\n\n\treturn zone->compact_defer_shift == COMPACT_MAX_DEFER_SHIFT &&\n\t\tzone->compact_considered >= 1UL << zone->compact_defer_shift;\n}\n\n/* Returns true if the pageblock should be scanned for pages to isolate. */\nstatic inline bool isolation_suitable(struct compact_control *cc,\n\t\t\t\t\tstruct page *page)\n{\n\tif (cc->ignore_skip_hint)\n\t\treturn true;\n\n\treturn !get_pageblock_skip(page);\n}\n\nstatic void reset_cached_positions(struct zone *zone)\n{\n\tzone->compact_cached_migrate_pfn[0] = zone->zone_start_pfn;\n\tzone->compact_cached_migrate_pfn[1] = zone->zone_start_pfn;\n\tzone->compact_cached_free_pfn =\n\t\t\t\tpageblock_start_pfn(zone_end_pfn(zone) - 1);\n}\n\n/*\n * Compound pages of >= pageblock_order should consistently be skipped until\n * released. It is always pointless to compact pages of such order (if they are\n * migratable), and the pageblocks they occupy cannot contain any free pages.\n */\nstatic bool pageblock_skip_persistent(struct page *page)\n{\n\tif (!PageCompound(page))\n\t\treturn false;\n\n\tpage = compound_head(page);\n\n\tif (compound_order(page) >= pageblock_order)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool\n__reset_isolation_pfn(struct zone *zone, unsigned long pfn, bool check_source,\n\t\t\t\t\t\t\tbool check_target)\n{\n\tstruct page *page = pfn_to_online_page(pfn);\n\tstruct page *block_page;\n\tstruct page *end_page;\n\tunsigned long block_pfn;\n\n\tif (!page)\n\t\treturn false;\n\tif (zone != page_zone(page))\n\t\treturn false;\n\tif (pageblock_skip_persistent(page))\n\t\treturn false;\n\n\t/*\n\t * If skip is already cleared do no further checking once the\n\t * restart points have been set.\n\t */\n\tif (check_source && check_target && !get_pageblock_skip(page))\n\t\treturn true;\n\n\t/*\n\t * If clearing skip for the target scanner, do not select a\n\t * non-movable pageblock as the starting point.\n\t */\n\tif (!check_source && check_target &&\n\t    get_pageblock_migratetype(page) != MIGRATE_MOVABLE)\n\t\treturn false;\n\n\t/* Ensure the start of the pageblock or zone is online and valid */\n\tblock_pfn = pageblock_start_pfn(pfn);\n\tblock_pfn = max(block_pfn, zone->zone_start_pfn);\n\tblock_page = pfn_to_online_page(block_pfn);\n\tif (block_page) {\n\t\tpage = block_page;\n\t\tpfn = block_pfn;\n\t}\n\n\t/* Ensure the end of the pageblock or zone is online and valid */\n\tblock_pfn = pageblock_end_pfn(pfn) - 1;\n\tblock_pfn = min(block_pfn, zone_end_pfn(zone) - 1);\n\tend_page = pfn_to_online_page(block_pfn);\n\tif (!end_page)\n\t\treturn false;\n\n\t/*\n\t * Only clear the hint if a sample indicates there is either a\n\t * free page or an LRU page in the block. One or other condition\n\t * is necessary for the block to be a migration source/target.\n\t */\n\tdo {\n\t\tif (pfn_valid_within(pfn)) {\n\t\t\tif (check_source && PageLRU(page)) {\n\t\t\t\tclear_pageblock_skip(page);\n\t\t\t\treturn true;\n\t\t\t}\n\n\t\t\tif (check_target && PageBuddy(page)) {\n\t\t\t\tclear_pageblock_skip(page);\n\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\n\t\tpage += (1 << PAGE_ALLOC_COSTLY_ORDER);\n\t\tpfn += (1 << PAGE_ALLOC_COSTLY_ORDER);\n\t} while (page <= end_page);\n\n\treturn false;\n}\n\n/*\n * This function is called to clear all cached information on pageblocks that\n * should be skipped for page isolation when the migrate and free page scanner\n * meet.\n */\nstatic void __reset_isolation_suitable(struct zone *zone)\n{\n\tunsigned long migrate_pfn = zone->zone_start_pfn;\n\tunsigned long free_pfn = zone_end_pfn(zone) - 1;\n\tunsigned long reset_migrate = free_pfn;\n\tunsigned long reset_free = migrate_pfn;\n\tbool source_set = false;\n\tbool free_set = false;\n\n\tif (!zone->compact_blockskip_flush)\n\t\treturn;\n\n\tzone->compact_blockskip_flush = false;\n\n\t/*\n\t * Walk the zone and update pageblock skip information. Source looks\n\t * for PageLRU while target looks for PageBuddy. When the scanner\n\t * is found, both PageBuddy and PageLRU are checked as the pageblock\n\t * is suitable as both source and target.\n\t */\n\tfor (; migrate_pfn < free_pfn; migrate_pfn += pageblock_nr_pages,\n\t\t\t\t\tfree_pfn -= pageblock_nr_pages) {\n\t\tcond_resched();\n\n\t\t/* Update the migrate PFN */\n\t\tif (__reset_isolation_pfn(zone, migrate_pfn, true, source_set) &&\n\t\t    migrate_pfn < reset_migrate) {\n\t\t\tsource_set = true;\n\t\t\treset_migrate = migrate_pfn;\n\t\t\tzone->compact_init_migrate_pfn = reset_migrate;\n\t\t\tzone->compact_cached_migrate_pfn[0] = reset_migrate;\n\t\t\tzone->compact_cached_migrate_pfn[1] = reset_migrate;\n\t\t}\n\n\t\t/* Update the free PFN */\n\t\tif (__reset_isolation_pfn(zone, free_pfn, free_set, true) &&\n\t\t    free_pfn > reset_free) {\n\t\t\tfree_set = true;\n\t\t\treset_free = free_pfn;\n\t\t\tzone->compact_init_free_pfn = reset_free;\n\t\t\tzone->compact_cached_free_pfn = reset_free;\n\t\t}\n\t}\n\n\t/* Leave no distance if no suitable block was reset */\n\tif (reset_migrate >= reset_free) {\n\t\tzone->compact_cached_migrate_pfn[0] = migrate_pfn;\n\t\tzone->compact_cached_migrate_pfn[1] = migrate_pfn;\n\t\tzone->compact_cached_free_pfn = free_pfn;\n\t}\n}\n\nvoid reset_isolation_suitable(pg_data_t *pgdat)\n{\n\tint zoneid;\n\n\tfor (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {\n\t\tstruct zone *zone = &pgdat->node_zones[zoneid];\n\t\tif (!populated_zone(zone))\n\t\t\tcontinue;\n\n\t\t/* Only flush if a full compaction finished recently */\n\t\tif (zone->compact_blockskip_flush)\n\t\t\t__reset_isolation_suitable(zone);\n\t}\n}\n\n/*\n * Sets the pageblock skip bit if it was clear. Note that this is a hint as\n * locks are not required for read/writers. Returns true if it was already set.\n */\nstatic bool test_and_set_skip(struct compact_control *cc, struct page *page,\n\t\t\t\t\t\t\tunsigned long pfn)\n{\n\tbool skip;\n\n\t/* Do no update if skip hint is being ignored */\n\tif (cc->ignore_skip_hint)\n\t\treturn false;\n\n\tif (!IS_ALIGNED(pfn, pageblock_nr_pages))\n\t\treturn false;\n\n\tskip = get_pageblock_skip(page);\n\tif (!skip && !cc->no_set_skip_hint)\n\t\tset_pageblock_skip(page);\n\n\treturn skip;\n}\n\nstatic void update_cached_migrate(struct compact_control *cc, unsigned long pfn)\n{\n\tstruct zone *zone = cc->zone;\n\n\tpfn = pageblock_end_pfn(pfn);\n\n\t/* Set for isolation rather than compaction */\n\tif (cc->no_set_skip_hint)\n\t\treturn;\n\n\tif (pfn > zone->compact_cached_migrate_pfn[0])\n\t\tzone->compact_cached_migrate_pfn[0] = pfn;\n\tif (cc->mode != MIGRATE_ASYNC &&\n\t    pfn > zone->compact_cached_migrate_pfn[1])\n\t\tzone->compact_cached_migrate_pfn[1] = pfn;\n}\n\n/*\n * If no pages were isolated then mark this pageblock to be skipped in the\n * future. The information is later cleared by __reset_isolation_suitable().\n */\nstatic void update_pageblock_skip(struct compact_control *cc,\n\t\t\tstruct page *page, unsigned long pfn)\n{\n\tstruct zone *zone = cc->zone;\n\n\tif (cc->no_set_skip_hint)\n\t\treturn;\n\n\tif (!page)\n\t\treturn;\n\n\tset_pageblock_skip(page);\n\n\t/* Update where async and sync compaction should restart */\n\tif (pfn < zone->compact_cached_free_pfn)\n\t\tzone->compact_cached_free_pfn = pfn;\n}\n#else\nstatic inline bool isolation_suitable(struct compact_control *cc,\n\t\t\t\t\tstruct page *page)\n{\n\treturn true;\n}\n\nstatic inline bool pageblock_skip_persistent(struct page *page)\n{\n\treturn false;\n}\n\nstatic inline void update_pageblock_skip(struct compact_control *cc,\n\t\t\tstruct page *page, unsigned long pfn)\n{\n}\n\nstatic void update_cached_migrate(struct compact_control *cc, unsigned long pfn)\n{\n}\n\nstatic bool test_and_set_skip(struct compact_control *cc, struct page *page,\n\t\t\t\t\t\t\tunsigned long pfn)\n{\n\treturn false;\n}\n#endif /* CONFIG_COMPACTION */\n\n/*\n * Compaction requires the taking of some coarse locks that are potentially\n * very heavily contended. For async compaction, trylock and record if the\n * lock is contended. The lock will still be acquired but compaction will\n * abort when the current block is finished regardless of success rate.\n * Sync compaction acquires the lock.\n *\n * Always returns true which makes it easier to track lock state in callers.\n */\nstatic bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n\t\t\t\t\t\tstruct compact_control *cc)\n\t__acquires(lock)\n{\n\t/* Track if the lock is contended in async mode */\n\tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n\t\tif (spin_trylock_irqsave(lock, *flags))\n\t\t\treturn true;\n\n\t\tcc->contended = true;\n\t}\n\n\tspin_lock_irqsave(lock, *flags);\n\treturn true;\n}\n\n/*\n * Compaction requires the taking of some coarse locks that are potentially\n * very heavily contended. The lock should be periodically unlocked to avoid\n * having disabled IRQs for a long time, even when there is nobody waiting on\n * the lock. It might also be that allowing the IRQs will result in\n * need_resched() becoming true. If scheduling is needed, async compaction\n * aborts. Sync compaction schedules.\n * Either compaction type will also abort if a fatal signal is pending.\n * In either case if the lock was locked, it is dropped and not regained.\n *\n * Returns true if compaction should abort due to fatal signal pending, or\n *\t\tasync compaction due to need_resched()\n * Returns false when compaction can continue (sync compaction might have\n *\t\tscheduled)\n */\nstatic bool compact_unlock_should_abort(spinlock_t *lock,\n\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n{\n\tif (*locked) {\n\t\tspin_unlock_irqrestore(lock, flags);\n\t\t*locked = false;\n\t}\n\n\tif (fatal_signal_pending(current)) {\n\t\tcc->contended = true;\n\t\treturn true;\n\t}\n\n\tcond_resched();\n\n\treturn false;\n}\n\n/*\n * Isolate free pages onto a private freelist. If @strict is true, will abort\n * returning 0 on any invalid PFNs or non-free pages inside of the pageblock\n * (even though it may still end up isolating some pages).\n */\nstatic unsigned long isolate_freepages_block(struct compact_control *cc,\n\t\t\t\tunsigned long *start_pfn,\n\t\t\t\tunsigned long end_pfn,\n\t\t\t\tstruct list_head *freelist,\n\t\t\t\tunsigned int stride,\n\t\t\t\tbool strict)\n{\n\tint nr_scanned = 0, total_isolated = 0;\n\tstruct page *cursor;\n\tunsigned long flags = 0;\n\tbool locked = false;\n\tunsigned long blockpfn = *start_pfn;\n\tunsigned int order;\n\n\t/* Strict mode is for isolation, speed is secondary */\n\tif (strict)\n\t\tstride = 1;\n\n\tcursor = pfn_to_page(blockpfn);\n\n\t/* Isolate free pages. */\n\tfor (; blockpfn < end_pfn; blockpfn += stride, cursor += stride) {\n\t\tint isolated;\n\t\tstruct page *page = cursor;\n\n\t\t/*\n\t\t * Periodically drop the lock (if held) regardless of its\n\t\t * contention, to give chance to IRQs. Abort if fatal signal\n\t\t * pending or async compaction detects need_resched()\n\t\t */\n\t\tif (!(blockpfn % SWAP_CLUSTER_MAX)\n\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n\t\t\t\t\t\t\t\t&locked, cc))\n\t\t\tbreak;\n\n\t\tnr_scanned++;\n\t\tif (!pfn_valid_within(blockpfn))\n\t\t\tgoto isolate_fail;\n\n\t\t/*\n\t\t * For compound pages such as THP and hugetlbfs, we can save\n\t\t * potentially a lot of iterations if we skip them at once.\n\t\t * The check is racy, but we can consider only valid values\n\t\t * and the only danger is skipping too much.\n\t\t */\n\t\tif (PageCompound(page)) {\n\t\t\tconst unsigned int order = compound_order(page);\n\n\t\t\tif (likely(order < MAX_ORDER)) {\n\t\t\t\tblockpfn += (1UL << order) - 1;\n\t\t\t\tcursor += (1UL << order) - 1;\n\t\t\t}\n\t\t\tgoto isolate_fail;\n\t\t}\n\n\t\tif (!PageBuddy(page))\n\t\t\tgoto isolate_fail;\n\n\t\t/*\n\t\t * If we already hold the lock, we can skip some rechecking.\n\t\t * Note that if we hold the lock now, checked_pageblock was\n\t\t * already set in some previous iteration (or strict is true),\n\t\t * so it is correct to skip the suitable migration target\n\t\t * recheck as well.\n\t\t */\n\t\tif (!locked) {\n\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n\t\t\t\t\t\t\t\t&flags, cc);\n\n\t\t\t/* Recheck this is a buddy page under lock */\n\t\t\tif (!PageBuddy(page))\n\t\t\t\tgoto isolate_fail;\n\t\t}\n\n\t\t/* Found a free page, will break it into order-0 pages */\n\t\torder = buddy_order(page);\n\t\tisolated = __isolate_free_page(page, order);\n\t\tif (!isolated)\n\t\t\tbreak;\n\t\tset_page_private(page, order);\n\n\t\ttotal_isolated += isolated;\n\t\tcc->nr_freepages += isolated;\n\t\tlist_add_tail(&page->lru, freelist);\n\n\t\tif (!strict && cc->nr_migratepages <= cc->nr_freepages) {\n\t\t\tblockpfn += isolated;\n\t\t\tbreak;\n\t\t}\n\t\t/* Advance to the end of split page */\n\t\tblockpfn += isolated - 1;\n\t\tcursor += isolated - 1;\n\t\tcontinue;\n\nisolate_fail:\n\t\tif (strict)\n\t\t\tbreak;\n\t\telse\n\t\t\tcontinue;\n\n\t}\n\n\tif (locked)\n\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n\n\t/*\n\t * There is a tiny chance that we have read bogus compound_order(),\n\t * so be careful to not go outside of the pageblock.\n\t */\n\tif (unlikely(blockpfn > end_pfn))\n\t\tblockpfn = end_pfn;\n\n\ttrace_mm_compaction_isolate_freepages(*start_pfn, blockpfn,\n\t\t\t\t\tnr_scanned, total_isolated);\n\n\t/* Record how far we have got within the block */\n\t*start_pfn = blockpfn;\n\n\t/*\n\t * If strict isolation is requested by CMA then check that all the\n\t * pages requested were isolated. If there were any failures, 0 is\n\t * returned and CMA will fail.\n\t */\n\tif (strict && blockpfn < end_pfn)\n\t\ttotal_isolated = 0;\n\n\tcc->total_free_scanned += nr_scanned;\n\tif (total_isolated)\n\t\tcount_compact_events(COMPACTISOLATED, total_isolated);\n\treturn total_isolated;\n}\n\n/**\n * isolate_freepages_range() - isolate free pages.\n * @cc:        Compaction control structure.\n * @start_pfn: The first PFN to start isolating.\n * @end_pfn:   The one-past-last PFN.\n *\n * Non-free pages, invalid PFNs, or zone boundaries within the\n * [start_pfn, end_pfn) range are considered errors, cause function to\n * undo its actions and return zero.\n *\n * Otherwise, function returns one-past-the-last PFN of isolated page\n * (which may be greater then end_pfn if end fell in a middle of\n * a free page).\n */\nunsigned long\nisolate_freepages_range(struct compact_control *cc,\n\t\t\tunsigned long start_pfn, unsigned long end_pfn)\n{\n\tunsigned long isolated, pfn, block_start_pfn, block_end_pfn;\n\tLIST_HEAD(freelist);\n\n\tpfn = start_pfn;\n\tblock_start_pfn = pageblock_start_pfn(pfn);\n\tif (block_start_pfn < cc->zone->zone_start_pfn)\n\t\tblock_start_pfn = cc->zone->zone_start_pfn;\n\tblock_end_pfn = pageblock_end_pfn(pfn);\n\n\tfor (; pfn < end_pfn; pfn += isolated,\n\t\t\t\tblock_start_pfn = block_end_pfn,\n\t\t\t\tblock_end_pfn += pageblock_nr_pages) {\n\t\t/* Protect pfn from changing by isolate_freepages_block */\n\t\tunsigned long isolate_start_pfn = pfn;\n\n\t\tblock_end_pfn = min(block_end_pfn, end_pfn);\n\n\t\t/*\n\t\t * pfn could pass the block_end_pfn if isolated freepage\n\t\t * is more than pageblock order. In this case, we adjust\n\t\t * scanning range to right one.\n\t\t */\n\t\tif (pfn >= block_end_pfn) {\n\t\t\tblock_start_pfn = pageblock_start_pfn(pfn);\n\t\t\tblock_end_pfn = pageblock_end_pfn(pfn);\n\t\t\tblock_end_pfn = min(block_end_pfn, end_pfn);\n\t\t}\n\n\t\tif (!pageblock_pfn_to_page(block_start_pfn,\n\t\t\t\t\tblock_end_pfn, cc->zone))\n\t\t\tbreak;\n\n\t\tisolated = isolate_freepages_block(cc, &isolate_start_pfn,\n\t\t\t\t\tblock_end_pfn, &freelist, 0, true);\n\n\t\t/*\n\t\t * In strict mode, isolate_freepages_block() returns 0 if\n\t\t * there are any holes in the block (ie. invalid PFNs or\n\t\t * non-free pages).\n\t\t */\n\t\tif (!isolated)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * If we managed to isolate pages, it is always (1 << n) *\n\t\t * pageblock_nr_pages for some non-negative n.  (Max order\n\t\t * page may span two pageblocks).\n\t\t */\n\t}\n\n\t/* __isolate_free_page() does not map the pages */\n\tsplit_map_pages(&freelist);\n\n\tif (pfn < end_pfn) {\n\t\t/* Loop terminated early, cleanup. */\n\t\trelease_freepages(&freelist);\n\t\treturn 0;\n\t}\n\n\t/* We don't use freelists for anything. */\n\treturn pfn;\n}\n\n/* Similar to reclaim, but different enough that they don't share logic */\nstatic bool too_many_isolated(pg_data_t *pgdat)\n{\n\tunsigned long active, inactive, isolated;\n\n\tinactive = node_page_state(pgdat, NR_INACTIVE_FILE) +\n\t\t\tnode_page_state(pgdat, NR_INACTIVE_ANON);\n\tactive = node_page_state(pgdat, NR_ACTIVE_FILE) +\n\t\t\tnode_page_state(pgdat, NR_ACTIVE_ANON);\n\tisolated = node_page_state(pgdat, NR_ISOLATED_FILE) +\n\t\t\tnode_page_state(pgdat, NR_ISOLATED_ANON);\n\n\treturn isolated > (inactive + active) / 2;\n}\n\n/**\n * isolate_migratepages_block() - isolate all migrate-able pages within\n *\t\t\t\t  a single pageblock\n * @cc:\t\tCompaction control structure.\n * @low_pfn:\tThe first PFN to isolate\n * @end_pfn:\tThe one-past-the-last PFN to isolate, within same pageblock\n * @isolate_mode: Isolation mode to be used.\n *\n * Isolate all pages that can be migrated from the range specified by\n * [low_pfn, end_pfn). The range is expected to be within same pageblock.\n * Returns zero if there is a fatal signal pending, otherwise PFN of the\n * first page that was not scanned (which may be both less, equal to or more\n * than end_pfn).\n *\n * The pages are isolated on cc->migratepages list (not required to be empty),\n * and cc->nr_migratepages is updated accordingly. The cc->migrate_pfn field\n * is neither read nor updated.\n */\nstatic unsigned long\nisolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n\t\t\tunsigned long end_pfn, isolate_mode_t isolate_mode)\n{\n\tpg_data_t *pgdat = cc->zone->zone_pgdat;\n\tunsigned long nr_scanned = 0, nr_isolated = 0;\n\tstruct lruvec *lruvec;\n\tunsigned long flags = 0;\n\tstruct lruvec *locked = NULL;\n\tstruct page *page = NULL, *valid_page = NULL;\n\tunsigned long start_pfn = low_pfn;\n\tbool skip_on_failure = false;\n\tunsigned long next_skip_pfn = 0;\n\tbool skip_updated = false;\n\n\t/*\n\t * Ensure that there are not too many pages isolated from the LRU\n\t * list by either parallel reclaimers or compaction. If there are,\n\t * delay for some time until fewer pages are isolated\n\t */\n\twhile (unlikely(too_many_isolated(pgdat))) {\n\t\t/* stop isolation if there are still pages not migrated */\n\t\tif (cc->nr_migratepages)\n\t\t\treturn 0;\n\n\t\t/* async migration should just abort */\n\t\tif (cc->mode == MIGRATE_ASYNC)\n\t\t\treturn 0;\n\n\t\tcongestion_wait(BLK_RW_ASYNC, HZ/10);\n\n\t\tif (fatal_signal_pending(current))\n\t\t\treturn 0;\n\t}\n\n\tcond_resched();\n\n\tif (cc->direct_compaction && (cc->mode == MIGRATE_ASYNC)) {\n\t\tskip_on_failure = true;\n\t\tnext_skip_pfn = block_end_pfn(low_pfn, cc->order);\n\t}\n\n\t/* Time to isolate some pages for migration */\n\tfor (; low_pfn < end_pfn; low_pfn++) {\n\n\t\tif (skip_on_failure && low_pfn >= next_skip_pfn) {\n\t\t\t/*\n\t\t\t * We have isolated all migration candidates in the\n\t\t\t * previous order-aligned block, and did not skip it due\n\t\t\t * to failure. We should migrate the pages now and\n\t\t\t * hopefully succeed compaction.\n\t\t\t */\n\t\t\tif (nr_isolated)\n\t\t\t\tbreak;\n\n\t\t\t/*\n\t\t\t * We failed to isolate in the previous order-aligned\n\t\t\t * block. Set the new boundary to the end of the\n\t\t\t * current block. Note we can't simply increase\n\t\t\t * next_skip_pfn by 1 << order, as low_pfn might have\n\t\t\t * been incremented by a higher number due to skipping\n\t\t\t * a compound or a high-order buddy page in the\n\t\t\t * previous loop iteration.\n\t\t\t */\n\t\t\tnext_skip_pfn = block_end_pfn(low_pfn, cc->order);\n\t\t}\n\n\t\t/*\n\t\t * Periodically drop the lock (if held) regardless of its\n\t\t * contention, to give chance to IRQs. Abort completely if\n\t\t * a fatal signal is pending.\n\t\t */\n\t\tif (!(low_pfn % SWAP_CLUSTER_MAX)) {\n\t\t\tif (locked) {\n\t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n\t\t\t\tlocked = NULL;\n\t\t\t}\n\n\t\t\tif (fatal_signal_pending(current)) {\n\t\t\t\tcc->contended = true;\n\n\t\t\t\tlow_pfn = 0;\n\t\t\t\tgoto fatal_pending;\n\t\t\t}\n\n\t\t\tcond_resched();\n\t\t}\n\n\t\tif (!pfn_valid_within(low_pfn))\n\t\t\tgoto isolate_fail;\n\t\tnr_scanned++;\n\n\t\tpage = pfn_to_page(low_pfn);\n\n\t\t/*\n\t\t * Check if the pageblock has already been marked skipped.\n\t\t * Only the aligned PFN is checked as the caller isolates\n\t\t * COMPACT_CLUSTER_MAX at a time so the second call must\n\t\t * not falsely conclude that the block should be skipped.\n\t\t */\n\t\tif (!valid_page && IS_ALIGNED(low_pfn, pageblock_nr_pages)) {\n\t\t\tif (!cc->ignore_skip_hint && get_pageblock_skip(page)) {\n\t\t\t\tlow_pfn = end_pfn;\n\t\t\t\tpage = NULL;\n\t\t\t\tgoto isolate_abort;\n\t\t\t}\n\t\t\tvalid_page = page;\n\t\t}\n\n\t\t/*\n\t\t * Skip if free. We read page order here without zone lock\n\t\t * which is generally unsafe, but the race window is small and\n\t\t * the worst thing that can happen is that we skip some\n\t\t * potential isolation targets.\n\t\t */\n\t\tif (PageBuddy(page)) {\n\t\t\tunsigned long freepage_order = buddy_order_unsafe(page);\n\n\t\t\t/*\n\t\t\t * Without lock, we cannot be sure that what we got is\n\t\t\t * a valid page order. Consider only values in the\n\t\t\t * valid order range to prevent low_pfn overflow.\n\t\t\t */\n\t\t\tif (freepage_order > 0 && freepage_order < MAX_ORDER)\n\t\t\t\tlow_pfn += (1UL << freepage_order) - 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * Regardless of being on LRU, compound pages such as THP and\n\t\t * hugetlbfs are not to be compacted unless we are attempting\n\t\t * an allocation much larger than the huge page size (eg CMA).\n\t\t * We can potentially save a lot of iterations if we skip them\n\t\t * at once. The check is racy, but we can consider only valid\n\t\t * values and the only danger is skipping too much.\n\t\t */\n\t\tif (PageCompound(page) && !cc->alloc_contig) {\n\t\t\tconst unsigned int order = compound_order(page);\n\n\t\t\tif (likely(order < MAX_ORDER))\n\t\t\t\tlow_pfn += (1UL << order) - 1;\n\t\t\tgoto isolate_fail;\n\t\t}\n\n\t\t/*\n\t\t * Check may be lockless but that's ok as we recheck later.\n\t\t * It's possible to migrate LRU and non-lru movable pages.\n\t\t * Skip any other type of page\n\t\t */\n\t\tif (!PageLRU(page)) {\n\t\t\t/*\n\t\t\t * __PageMovable can return false positive so we need\n\t\t\t * to verify it under page_lock.\n\t\t\t */\n\t\t\tif (unlikely(__PageMovable(page)) &&\n\t\t\t\t\t!PageIsolated(page)) {\n\t\t\t\tif (locked) {\n\t\t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n\t\t\t\t\tlocked = NULL;\n\t\t\t\t}\n\n\t\t\t\tif (!isolate_movable_page(page, isolate_mode))\n\t\t\t\t\tgoto isolate_success;\n\t\t\t}\n\n\t\t\tgoto isolate_fail;\n\t\t}\n\n\t\t/*\n\t\t * Migration will fail if an anonymous page is pinned in memory,\n\t\t * so avoid taking lru_lock and isolating it unnecessarily in an\n\t\t * admittedly racy check.\n\t\t */\n\t\tif (!page_mapping(page) &&\n\t\t    page_count(page) > page_mapcount(page))\n\t\t\tgoto isolate_fail;\n\n\t\t/*\n\t\t * Only allow to migrate anonymous pages in GFP_NOFS context\n\t\t * because those do not depend on fs locks.\n\t\t */\n\t\tif (!(cc->gfp_mask & __GFP_FS) && page_mapping(page))\n\t\t\tgoto isolate_fail;\n\n\t\t/*\n\t\t * Be careful not to clear PageLRU until after we're\n\t\t * sure the page is not being freed elsewhere -- the\n\t\t * page release code relies on it.\n\t\t */\n\t\tif (unlikely(!get_page_unless_zero(page)))\n\t\t\tgoto isolate_fail;\n\n\t\tif (!__isolate_lru_page_prepare(page, isolate_mode))\n\t\t\tgoto isolate_fail_put;\n\n\t\t/* Try isolate the page */\n\t\tif (!TestClearPageLRU(page))\n\t\t\tgoto isolate_fail_put;\n\n\t\trcu_read_lock();\n\t\tlruvec = mem_cgroup_page_lruvec(page, pgdat);\n\n\t\t/* If we already hold the lock, we can skip some rechecking */\n\t\tif (lruvec != locked) {\n\t\t\tif (locked)\n\t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n\n\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n\t\t\tlocked = lruvec;\n\t\t\trcu_read_unlock();\n\n\t\t\tlruvec_memcg_debug(lruvec, page);\n\n\t\t\t/* Try get exclusive access under lock */\n\t\t\tif (!skip_updated) {\n\t\t\t\tskip_updated = true;\n\t\t\t\tif (test_and_set_skip(cc, page, low_pfn))\n\t\t\t\t\tgoto isolate_abort;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Page become compound since the non-locked check,\n\t\t\t * and it's on LRU. It can only be a THP so the order\n\t\t\t * is safe to read and it's 0 for tail pages.\n\t\t\t */\n\t\t\tif (unlikely(PageCompound(page) && !cc->alloc_contig)) {\n\t\t\t\tlow_pfn += compound_nr(page) - 1;\n\t\t\t\tSetPageLRU(page);\n\t\t\t\tgoto isolate_fail_put;\n\t\t\t}\n\t\t} else\n\t\t\trcu_read_unlock();\n\n\t\t/* The whole page is taken off the LRU; skip the tail pages. */\n\t\tif (PageCompound(page))\n\t\t\tlow_pfn += compound_nr(page) - 1;\n\n\t\t/* Successfully isolated */\n\t\tdel_page_from_lru_list(page, lruvec, page_lru(page));\n\t\tmod_node_page_state(page_pgdat(page),\n\t\t\t\tNR_ISOLATED_ANON + page_is_file_lru(page),\n\t\t\t\tthp_nr_pages(page));\n\nisolate_success:\n\t\tlist_add(&page->lru, &cc->migratepages);\n\t\tcc->nr_migratepages += compound_nr(page);\n\t\tnr_isolated += compound_nr(page);\n\n\t\t/*\n\t\t * Avoid isolating too much unless this block is being\n\t\t * rescanned (e.g. dirty/writeback pages, parallel allocation)\n\t\t * or a lock is contended. For contention, isolate quickly to\n\t\t * potentially remove one source of contention.\n\t\t */\n\t\tif (cc->nr_migratepages >= COMPACT_CLUSTER_MAX &&\n\t\t    !cc->rescan && !cc->contended) {\n\t\t\t++low_pfn;\n\t\t\tbreak;\n\t\t}\n\n\t\tcontinue;\n\nisolate_fail_put:\n\t\t/* Avoid potential deadlock in freeing page under lru_lock */\n\t\tif (locked) {\n\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n\t\t\tlocked = NULL;\n\t\t}\n\t\tput_page(page);\n\nisolate_fail:\n\t\tif (!skip_on_failure)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * We have isolated some pages, but then failed. Release them\n\t\t * instead of migrating, as we cannot form the cc->order buddy\n\t\t * page anyway.\n\t\t */\n\t\tif (nr_isolated) {\n\t\t\tif (locked) {\n\t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n\t\t\t\tlocked = NULL;\n\t\t\t}\n\t\t\tputback_movable_pages(&cc->migratepages);\n\t\t\tcc->nr_migratepages = 0;\n\t\t\tnr_isolated = 0;\n\t\t}\n\n\t\tif (low_pfn < next_skip_pfn) {\n\t\t\tlow_pfn = next_skip_pfn - 1;\n\t\t\t/*\n\t\t\t * The check near the loop beginning would have updated\n\t\t\t * next_skip_pfn too, but this is a bit simpler.\n\t\t\t */\n\t\t\tnext_skip_pfn += 1UL << cc->order;\n\t\t}\n\t}\n\n\t/*\n\t * The PageBuddy() check could have potentially brought us outside\n\t * the range to be scanned.\n\t */\n\tif (unlikely(low_pfn > end_pfn))\n\t\tlow_pfn = end_pfn;\n\n\tpage = NULL;\n\nisolate_abort:\n\tif (locked)\n\t\tunlock_page_lruvec_irqrestore(locked, flags);\n\tif (page) {\n\t\tSetPageLRU(page);\n\t\tput_page(page);\n\t}\n\n\t/*\n\t * Updated the cached scanner pfn once the pageblock has been scanned\n\t * Pages will either be migrated in which case there is no point\n\t * scanning in the near future or migration failed in which case the\n\t * failure reason may persist. The block is marked for skipping if\n\t * there were no pages isolated in the block or if the block is\n\t * rescanned twice in a row.\n\t */\n\tif (low_pfn == end_pfn && (!nr_isolated || cc->rescan)) {\n\t\tif (valid_page && !skip_updated)\n\t\t\tset_pageblock_skip(valid_page);\n\t\tupdate_cached_migrate(cc, low_pfn);\n\t}\n\n\ttrace_mm_compaction_isolate_migratepages(start_pfn, low_pfn,\n\t\t\t\t\t\tnr_scanned, nr_isolated);\n\nfatal_pending:\n\tcc->total_migrate_scanned += nr_scanned;\n\tif (nr_isolated)\n\t\tcount_compact_events(COMPACTISOLATED, nr_isolated);\n\n\treturn low_pfn;\n}\n\n/**\n * isolate_migratepages_range() - isolate migrate-able pages in a PFN range\n * @cc:        Compaction control structure.\n * @start_pfn: The first PFN to start isolating.\n * @end_pfn:   The one-past-last PFN.\n *\n * Returns zero if isolation fails fatally due to e.g. pending signal.\n * Otherwise, function returns one-past-the-last PFN of isolated page\n * (which may be greater than end_pfn if end fell in a middle of a THP page).\n */\nunsigned long\nisolate_migratepages_range(struct compact_control *cc, unsigned long start_pfn,\n\t\t\t\t\t\t\tunsigned long end_pfn)\n{\n\tunsigned long pfn, block_start_pfn, block_end_pfn;\n\n\t/* Scan block by block. First and last block may be incomplete */\n\tpfn = start_pfn;\n\tblock_start_pfn = pageblock_start_pfn(pfn);\n\tif (block_start_pfn < cc->zone->zone_start_pfn)\n\t\tblock_start_pfn = cc->zone->zone_start_pfn;\n\tblock_end_pfn = pageblock_end_pfn(pfn);\n\n\tfor (; pfn < end_pfn; pfn = block_end_pfn,\n\t\t\t\tblock_start_pfn = block_end_pfn,\n\t\t\t\tblock_end_pfn += pageblock_nr_pages) {\n\n\t\tblock_end_pfn = min(block_end_pfn, end_pfn);\n\n\t\tif (!pageblock_pfn_to_page(block_start_pfn,\n\t\t\t\t\tblock_end_pfn, cc->zone))\n\t\t\tcontinue;\n\n\t\tpfn = isolate_migratepages_block(cc, pfn, block_end_pfn,\n\t\t\t\t\t\t\tISOLATE_UNEVICTABLE);\n\n\t\tif (!pfn)\n\t\t\tbreak;\n\n\t\tif (cc->nr_migratepages >= COMPACT_CLUSTER_MAX)\n\t\t\tbreak;\n\t}\n\n\treturn pfn;\n}\n\n#endif /* CONFIG_COMPACTION || CONFIG_CMA */\n#ifdef CONFIG_COMPACTION\n\nstatic bool suitable_migration_source(struct compact_control *cc,\n\t\t\t\t\t\t\tstruct page *page)\n{\n\tint block_mt;\n\n\tif (pageblock_skip_persistent(page))\n\t\treturn false;\n\n\tif ((cc->mode != MIGRATE_ASYNC) || !cc->direct_compaction)\n\t\treturn true;\n\n\tblock_mt = get_pageblock_migratetype(page);\n\n\tif (cc->migratetype == MIGRATE_MOVABLE)\n\t\treturn is_migrate_movable(block_mt);\n\telse\n\t\treturn block_mt == cc->migratetype;\n}\n\n/* Returns true if the page is within a block suitable for migration to */\nstatic bool suitable_migration_target(struct compact_control *cc,\n\t\t\t\t\t\t\tstruct page *page)\n{\n\t/* If the page is a large free page, then disallow migration */\n\tif (PageBuddy(page)) {\n\t\t/*\n\t\t * We are checking page_order without zone->lock taken. But\n\t\t * the only small danger is that we skip a potentially suitable\n\t\t * pageblock, so it's not worth to check order for valid range.\n\t\t */\n\t\tif (buddy_order_unsafe(page) >= pageblock_order)\n\t\t\treturn false;\n\t}\n\n\tif (cc->ignore_block_suitable)\n\t\treturn true;\n\n\t/* If the block is MIGRATE_MOVABLE or MIGRATE_CMA, allow migration */\n\tif (is_migrate_movable(get_pageblock_migratetype(page)))\n\t\treturn true;\n\n\t/* Otherwise skip the block */\n\treturn false;\n}\n\nstatic inline unsigned int\nfreelist_scan_limit(struct compact_control *cc)\n{\n\tunsigned short shift = BITS_PER_LONG - 1;\n\n\treturn (COMPACT_CLUSTER_MAX >> min(shift, cc->fast_search_fail)) + 1;\n}\n\n/*\n * Test whether the free scanner has reached the same or lower pageblock than\n * the migration scanner, and compaction should thus terminate.\n */\nstatic inline bool compact_scanners_met(struct compact_control *cc)\n{\n\treturn (cc->free_pfn >> pageblock_order)\n\t\t<= (cc->migrate_pfn >> pageblock_order);\n}\n\n/*\n * Used when scanning for a suitable migration target which scans freelists\n * in reverse. Reorders the list such as the unscanned pages are scanned\n * first on the next iteration of the free scanner\n */\nstatic void\nmove_freelist_head(struct list_head *freelist, struct page *freepage)\n{\n\tLIST_HEAD(sublist);\n\n\tif (!list_is_last(freelist, &freepage->lru)) {\n\t\tlist_cut_before(&sublist, freelist, &freepage->lru);\n\t\tif (!list_empty(&sublist))\n\t\t\tlist_splice_tail(&sublist, freelist);\n\t}\n}\n\n/*\n * Similar to move_freelist_head except used by the migration scanner\n * when scanning forward. It's possible for these list operations to\n * move against each other if they search the free list exactly in\n * lockstep.\n */\nstatic void\nmove_freelist_tail(struct list_head *freelist, struct page *freepage)\n{\n\tLIST_HEAD(sublist);\n\n\tif (!list_is_first(freelist, &freepage->lru)) {\n\t\tlist_cut_position(&sublist, freelist, &freepage->lru);\n\t\tif (!list_empty(&sublist))\n\t\t\tlist_splice_tail(&sublist, freelist);\n\t}\n}\n\nstatic void\nfast_isolate_around(struct compact_control *cc, unsigned long pfn, unsigned long nr_isolated)\n{\n\tunsigned long start_pfn, end_pfn;\n\tstruct page *page = pfn_to_page(pfn);\n\n\t/* Do not search around if there are enough pages already */\n\tif (cc->nr_freepages >= cc->nr_migratepages)\n\t\treturn;\n\n\t/* Minimise scanning during async compaction */\n\tif (cc->direct_compaction && cc->mode == MIGRATE_ASYNC)\n\t\treturn;\n\n\t/* Pageblock boundaries */\n\tstart_pfn = pageblock_start_pfn(pfn);\n\tend_pfn = min(pageblock_end_pfn(pfn), zone_end_pfn(cc->zone)) - 1;\n\n\t/* Scan before */\n\tif (start_pfn != pfn) {\n\t\tisolate_freepages_block(cc, &start_pfn, pfn, &cc->freepages, 1, false);\n\t\tif (cc->nr_freepages >= cc->nr_migratepages)\n\t\t\treturn;\n\t}\n\n\t/* Scan after */\n\tstart_pfn = pfn + nr_isolated;\n\tif (start_pfn < end_pfn)\n\t\tisolate_freepages_block(cc, &start_pfn, end_pfn, &cc->freepages, 1, false);\n\n\t/* Skip this pageblock in the future as it's full or nearly full */\n\tif (cc->nr_freepages < cc->nr_migratepages)\n\t\tset_pageblock_skip(page);\n}\n\n/* Search orders in round-robin fashion */\nstatic int next_search_order(struct compact_control *cc, int order)\n{\n\torder--;\n\tif (order < 0)\n\t\torder = cc->order - 1;\n\n\t/* Search wrapped around? */\n\tif (order == cc->search_order) {\n\t\tcc->search_order--;\n\t\tif (cc->search_order < 0)\n\t\t\tcc->search_order = cc->order - 1;\n\t\treturn -1;\n\t}\n\n\treturn order;\n}\n\nstatic unsigned long\nfast_isolate_freepages(struct compact_control *cc)\n{\n\tunsigned int limit = min(1U, freelist_scan_limit(cc) >> 1);\n\tunsigned int nr_scanned = 0;\n\tunsigned long low_pfn, min_pfn, high_pfn = 0, highest = 0;\n\tunsigned long nr_isolated = 0;\n\tunsigned long distance;\n\tstruct page *page = NULL;\n\tbool scan_start = false;\n\tint order;\n\n\t/* Full compaction passes in a negative order */\n\tif (cc->order <= 0)\n\t\treturn cc->free_pfn;\n\n\t/*\n\t * If starting the scan, use a deeper search and use the highest\n\t * PFN found if a suitable one is not found.\n\t */\n\tif (cc->free_pfn >= cc->zone->compact_init_free_pfn) {\n\t\tlimit = pageblock_nr_pages >> 1;\n\t\tscan_start = true;\n\t}\n\n\t/*\n\t * Preferred point is in the top quarter of the scan space but take\n\t * a pfn from the top half if the search is problematic.\n\t */\n\tdistance = (cc->free_pfn - cc->migrate_pfn);\n\tlow_pfn = pageblock_start_pfn(cc->free_pfn - (distance >> 2));\n\tmin_pfn = pageblock_start_pfn(cc->free_pfn - (distance >> 1));\n\n\tif (WARN_ON_ONCE(min_pfn > low_pfn))\n\t\tlow_pfn = min_pfn;\n\n\t/*\n\t * Search starts from the last successful isolation order or the next\n\t * order to search after a previous failure\n\t */\n\tcc->search_order = min_t(unsigned int, cc->order - 1, cc->search_order);\n\n\tfor (order = cc->search_order;\n\t     !page && order >= 0;\n\t     order = next_search_order(cc, order)) {\n\t\tstruct free_area *area = &cc->zone->free_area[order];\n\t\tstruct list_head *freelist;\n\t\tstruct page *freepage;\n\t\tunsigned long flags;\n\t\tunsigned int order_scanned = 0;\n\n\t\tif (!area->nr_free)\n\t\t\tcontinue;\n\n\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n\t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n\t\tlist_for_each_entry_reverse(freepage, freelist, lru) {\n\t\t\tunsigned long pfn;\n\n\t\t\torder_scanned++;\n\t\t\tnr_scanned++;\n\t\t\tpfn = page_to_pfn(freepage);\n\n\t\t\tif (pfn >= highest)\n\t\t\t\thighest = pageblock_start_pfn(pfn);\n\n\t\t\tif (pfn >= low_pfn) {\n\t\t\t\tcc->fast_search_fail = 0;\n\t\t\t\tcc->search_order = order;\n\t\t\t\tpage = freepage;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (pfn >= min_pfn && pfn > high_pfn) {\n\t\t\t\thigh_pfn = pfn;\n\n\t\t\t\t/* Shorten the scan if a candidate is found */\n\t\t\t\tlimit >>= 1;\n\t\t\t}\n\n\t\t\tif (order_scanned >= limit)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t/* Use a minimum pfn if a preferred one was not found */\n\t\tif (!page && high_pfn) {\n\t\t\tpage = pfn_to_page(high_pfn);\n\n\t\t\t/* Update freepage for the list reorder below */\n\t\t\tfreepage = page;\n\t\t}\n\n\t\t/* Reorder to so a future search skips recent pages */\n\t\tmove_freelist_head(freelist, freepage);\n\n\t\t/* Isolate the page if available */\n\t\tif (page) {\n\t\t\tif (__isolate_free_page(page, order)) {\n\t\t\t\tset_page_private(page, order);\n\t\t\t\tnr_isolated = 1 << order;\n\t\t\t\tcc->nr_freepages += nr_isolated;\n\t\t\t\tlist_add_tail(&page->lru, &cc->freepages);\n\t\t\t\tcount_compact_events(COMPACTISOLATED, nr_isolated);\n\t\t\t} else {\n\t\t\t\t/* If isolation fails, abort the search */\n\t\t\t\torder = cc->search_order + 1;\n\t\t\t\tpage = NULL;\n\t\t\t}\n\t\t}\n\n\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n\n\t\t/*\n\t\t * Smaller scan on next order so the total scan ig related\n\t\t * to freelist_scan_limit.\n\t\t */\n\t\tif (order_scanned >= limit)\n\t\t\tlimit = min(1U, limit >> 1);\n\t}\n\n\tif (!page) {\n\t\tcc->fast_search_fail++;\n\t\tif (scan_start) {\n\t\t\t/*\n\t\t\t * Use the highest PFN found above min. If one was\n\t\t\t * not found, be pessimistic for direct compaction\n\t\t\t * and use the min mark.\n\t\t\t */\n\t\t\tif (highest) {\n\t\t\t\tpage = pfn_to_page(highest);\n\t\t\t\tcc->free_pfn = highest;\n\t\t\t} else {\n\t\t\t\tif (cc->direct_compaction && pfn_valid(min_pfn)) {\n\t\t\t\t\tpage = pageblock_pfn_to_page(min_pfn,\n\t\t\t\t\t\tpageblock_end_pfn(min_pfn),\n\t\t\t\t\t\tcc->zone);\n\t\t\t\t\tcc->free_pfn = min_pfn;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (highest && highest >= cc->zone->compact_cached_free_pfn) {\n\t\thighest -= pageblock_nr_pages;\n\t\tcc->zone->compact_cached_free_pfn = highest;\n\t}\n\n\tcc->total_free_scanned += nr_scanned;\n\tif (!page)\n\t\treturn cc->free_pfn;\n\n\tlow_pfn = page_to_pfn(page);\n\tfast_isolate_around(cc, low_pfn, nr_isolated);\n\treturn low_pfn;\n}\n\n/*\n * Based on information in the current compact_control, find blocks\n * suitable for isolating free pages from and then isolate them.\n */\nstatic void isolate_freepages(struct compact_control *cc)\n{\n\tstruct zone *zone = cc->zone;\n\tstruct page *page;\n\tunsigned long block_start_pfn;\t/* start of current pageblock */\n\tunsigned long isolate_start_pfn; /* exact pfn we start at */\n\tunsigned long block_end_pfn;\t/* end of current pageblock */\n\tunsigned long low_pfn;\t     /* lowest pfn scanner is able to scan */\n\tstruct list_head *freelist = &cc->freepages;\n\tunsigned int stride;\n\n\t/* Try a small search of the free lists for a candidate */\n\tisolate_start_pfn = fast_isolate_freepages(cc);\n\tif (cc->nr_freepages)\n\t\tgoto splitmap;\n\n\t/*\n\t * Initialise the free scanner. The starting point is where we last\n\t * successfully isolated from, zone-cached value, or the end of the\n\t * zone when isolating for the first time. For looping we also need\n\t * this pfn aligned down to the pageblock boundary, because we do\n\t * block_start_pfn -= pageblock_nr_pages in the for loop.\n\t * For ending point, take care when isolating in last pageblock of a\n\t * zone which ends in the middle of a pageblock.\n\t * The low boundary is the end of the pageblock the migration scanner\n\t * is using.\n\t */\n\tisolate_start_pfn = cc->free_pfn;\n\tblock_start_pfn = pageblock_start_pfn(isolate_start_pfn);\n\tblock_end_pfn = min(block_start_pfn + pageblock_nr_pages,\n\t\t\t\t\t\tzone_end_pfn(zone));\n\tlow_pfn = pageblock_end_pfn(cc->migrate_pfn);\n\tstride = cc->mode == MIGRATE_ASYNC ? COMPACT_CLUSTER_MAX : 1;\n\n\t/*\n\t * Isolate free pages until enough are available to migrate the\n\t * pages on cc->migratepages. We stop searching if the migrate\n\t * and free page scanners meet or enough free pages are isolated.\n\t */\n\tfor (; block_start_pfn >= low_pfn;\n\t\t\t\tblock_end_pfn = block_start_pfn,\n\t\t\t\tblock_start_pfn -= pageblock_nr_pages,\n\t\t\t\tisolate_start_pfn = block_start_pfn) {\n\t\tunsigned long nr_isolated;\n\n\t\t/*\n\t\t * This can iterate a massively long zone without finding any\n\t\t * suitable migration targets, so periodically check resched.\n\t\t */\n\t\tif (!(block_start_pfn % (SWAP_CLUSTER_MAX * pageblock_nr_pages)))\n\t\t\tcond_resched();\n\n\t\tpage = pageblock_pfn_to_page(block_start_pfn, block_end_pfn,\n\t\t\t\t\t\t\t\t\tzone);\n\t\tif (!page)\n\t\t\tcontinue;\n\n\t\t/* Check the block is suitable for migration */\n\t\tif (!suitable_migration_target(cc, page))\n\t\t\tcontinue;\n\n\t\t/* If isolation recently failed, do not retry */\n\t\tif (!isolation_suitable(cc, page))\n\t\t\tcontinue;\n\n\t\t/* Found a block suitable for isolating free pages from. */\n\t\tnr_isolated = isolate_freepages_block(cc, &isolate_start_pfn,\n\t\t\t\t\tblock_end_pfn, freelist, stride, false);\n\n\t\t/* Update the skip hint if the full pageblock was scanned */\n\t\tif (isolate_start_pfn == block_end_pfn)\n\t\t\tupdate_pageblock_skip(cc, page, block_start_pfn);\n\n\t\t/* Are enough freepages isolated? */\n\t\tif (cc->nr_freepages >= cc->nr_migratepages) {\n\t\t\tif (isolate_start_pfn >= block_end_pfn) {\n\t\t\t\t/*\n\t\t\t\t * Restart at previous pageblock if more\n\t\t\t\t * freepages can be isolated next time.\n\t\t\t\t */\n\t\t\t\tisolate_start_pfn =\n\t\t\t\t\tblock_start_pfn - pageblock_nr_pages;\n\t\t\t}\n\t\t\tbreak;\n\t\t} else if (isolate_start_pfn < block_end_pfn) {\n\t\t\t/*\n\t\t\t * If isolation failed early, do not continue\n\t\t\t * needlessly.\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Adjust stride depending on isolation */\n\t\tif (nr_isolated) {\n\t\t\tstride = 1;\n\t\t\tcontinue;\n\t\t}\n\t\tstride = min_t(unsigned int, COMPACT_CLUSTER_MAX, stride << 1);\n\t}\n\n\t/*\n\t * Record where the free scanner will restart next time. Either we\n\t * broke from the loop and set isolate_start_pfn based on the last\n\t * call to isolate_freepages_block(), or we met the migration scanner\n\t * and the loop terminated due to isolate_start_pfn < low_pfn\n\t */\n\tcc->free_pfn = isolate_start_pfn;\n\nsplitmap:\n\t/* __isolate_free_page() does not map the pages */\n\tsplit_map_pages(freelist);\n}\n\n/*\n * This is a migrate-callback that \"allocates\" freepages by taking pages\n * from the isolated freelists in the block we are migrating to.\n */\nstatic struct page *compaction_alloc(struct page *migratepage,\n\t\t\t\t\tunsigned long data)\n{\n\tstruct compact_control *cc = (struct compact_control *)data;\n\tstruct page *freepage;\n\n\tif (list_empty(&cc->freepages)) {\n\t\tisolate_freepages(cc);\n\n\t\tif (list_empty(&cc->freepages))\n\t\t\treturn NULL;\n\t}\n\n\tfreepage = list_entry(cc->freepages.next, struct page, lru);\n\tlist_del(&freepage->lru);\n\tcc->nr_freepages--;\n\n\treturn freepage;\n}\n\n/*\n * This is a migrate-callback that \"frees\" freepages back to the isolated\n * freelist.  All pages on the freelist are from the same zone, so there is no\n * special handling needed for NUMA.\n */\nstatic void compaction_free(struct page *page, unsigned long data)\n{\n\tstruct compact_control *cc = (struct compact_control *)data;\n\n\tlist_add(&page->lru, &cc->freepages);\n\tcc->nr_freepages++;\n}\n\n/* possible outcome of isolate_migratepages */\ntypedef enum {\n\tISOLATE_ABORT,\t\t/* Abort compaction now */\n\tISOLATE_NONE,\t\t/* No pages isolated, continue scanning */\n\tISOLATE_SUCCESS,\t/* Pages isolated, migrate */\n} isolate_migrate_t;\n\n/*\n * Allow userspace to control policy on scanning the unevictable LRU for\n * compactable pages.\n */\n#ifdef CONFIG_PREEMPT_RT\nint sysctl_compact_unevictable_allowed __read_mostly = 0;\n#else\nint sysctl_compact_unevictable_allowed __read_mostly = 1;\n#endif\n\nstatic inline void\nupdate_fast_start_pfn(struct compact_control *cc, unsigned long pfn)\n{\n\tif (cc->fast_start_pfn == ULONG_MAX)\n\t\treturn;\n\n\tif (!cc->fast_start_pfn)\n\t\tcc->fast_start_pfn = pfn;\n\n\tcc->fast_start_pfn = min(cc->fast_start_pfn, pfn);\n}\n\nstatic inline unsigned long\nreinit_migrate_pfn(struct compact_control *cc)\n{\n\tif (!cc->fast_start_pfn || cc->fast_start_pfn == ULONG_MAX)\n\t\treturn cc->migrate_pfn;\n\n\tcc->migrate_pfn = cc->fast_start_pfn;\n\tcc->fast_start_pfn = ULONG_MAX;\n\n\treturn cc->migrate_pfn;\n}\n\n/*\n * Briefly search the free lists for a migration source that already has\n * some free pages to reduce the number of pages that need migration\n * before a pageblock is free.\n */\nstatic unsigned long fast_find_migrateblock(struct compact_control *cc)\n{\n\tunsigned int limit = freelist_scan_limit(cc);\n\tunsigned int nr_scanned = 0;\n\tunsigned long distance;\n\tunsigned long pfn = cc->migrate_pfn;\n\tunsigned long high_pfn;\n\tint order;\n\n\t/* Skip hints are relied on to avoid repeats on the fast search */\n\tif (cc->ignore_skip_hint)\n\t\treturn pfn;\n\n\t/*\n\t * If the migrate_pfn is not at the start of a zone or the start\n\t * of a pageblock then assume this is a continuation of a previous\n\t * scan restarted due to COMPACT_CLUSTER_MAX.\n\t */\n\tif (pfn != cc->zone->zone_start_pfn && pfn != pageblock_start_pfn(pfn))\n\t\treturn pfn;\n\n\t/*\n\t * For smaller orders, just linearly scan as the number of pages\n\t * to migrate should be relatively small and does not necessarily\n\t * justify freeing up a large block for a small allocation.\n\t */\n\tif (cc->order <= PAGE_ALLOC_COSTLY_ORDER)\n\t\treturn pfn;\n\n\t/*\n\t * Only allow kcompactd and direct requests for movable pages to\n\t * quickly clear out a MOVABLE pageblock for allocation. This\n\t * reduces the risk that a large movable pageblock is freed for\n\t * an unmovable/reclaimable small allocation.\n\t */\n\tif (cc->direct_compaction && cc->migratetype != MIGRATE_MOVABLE)\n\t\treturn pfn;\n\n\t/*\n\t * When starting the migration scanner, pick any pageblock within the\n\t * first half of the search space. Otherwise try and pick a pageblock\n\t * within the first eighth to reduce the chances that a migration\n\t * target later becomes a source.\n\t */\n\tdistance = (cc->free_pfn - cc->migrate_pfn) >> 1;\n\tif (cc->migrate_pfn != cc->zone->zone_start_pfn)\n\t\tdistance >>= 2;\n\thigh_pfn = pageblock_start_pfn(cc->migrate_pfn + distance);\n\n\tfor (order = cc->order - 1;\n\t     order >= PAGE_ALLOC_COSTLY_ORDER && pfn == cc->migrate_pfn && nr_scanned < limit;\n\t     order--) {\n\t\tstruct free_area *area = &cc->zone->free_area[order];\n\t\tstruct list_head *freelist;\n\t\tunsigned long flags;\n\t\tstruct page *freepage;\n\n\t\tif (!area->nr_free)\n\t\t\tcontinue;\n\n\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n\t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n\t\tlist_for_each_entry(freepage, freelist, lru) {\n\t\t\tunsigned long free_pfn;\n\n\t\t\tnr_scanned++;\n\t\t\tfree_pfn = page_to_pfn(freepage);\n\t\t\tif (free_pfn < high_pfn) {\n\t\t\t\t/*\n\t\t\t\t * Avoid if skipped recently. Ideally it would\n\t\t\t\t * move to the tail but even safe iteration of\n\t\t\t\t * the list assumes an entry is deleted, not\n\t\t\t\t * reordered.\n\t\t\t\t */\n\t\t\t\tif (get_pageblock_skip(freepage)) {\n\t\t\t\t\tif (list_is_last(freelist, &freepage->lru))\n\t\t\t\t\t\tbreak;\n\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\t/* Reorder to so a future search skips recent pages */\n\t\t\t\tmove_freelist_tail(freelist, freepage);\n\n\t\t\t\tupdate_fast_start_pfn(cc, free_pfn);\n\t\t\t\tpfn = pageblock_start_pfn(free_pfn);\n\t\t\t\tcc->fast_search_fail = 0;\n\t\t\t\tset_pageblock_skip(freepage);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (nr_scanned >= limit) {\n\t\t\t\tcc->fast_search_fail++;\n\t\t\t\tmove_freelist_tail(freelist, freepage);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n\t}\n\n\tcc->total_migrate_scanned += nr_scanned;\n\n\t/*\n\t * If fast scanning failed then use a cached entry for a page block\n\t * that had free pages as the basis for starting a linear scan.\n\t */\n\tif (pfn == cc->migrate_pfn)\n\t\tpfn = reinit_migrate_pfn(cc);\n\n\treturn pfn;\n}\n\n/*\n * Isolate all pages that can be migrated from the first suitable block,\n * starting at the block pointed to by the migrate scanner pfn within\n * compact_control.\n */\nstatic isolate_migrate_t isolate_migratepages(struct compact_control *cc)\n{\n\tunsigned long block_start_pfn;\n\tunsigned long block_end_pfn;\n\tunsigned long low_pfn;\n\tstruct page *page;\n\tconst isolate_mode_t isolate_mode =\n\t\t(sysctl_compact_unevictable_allowed ? ISOLATE_UNEVICTABLE : 0) |\n\t\t(cc->mode != MIGRATE_SYNC ? ISOLATE_ASYNC_MIGRATE : 0);\n\tbool fast_find_block;\n\n\t/*\n\t * Start at where we last stopped, or beginning of the zone as\n\t * initialized by compact_zone(). The first failure will use\n\t * the lowest PFN as the starting point for linear scanning.\n\t */\n\tlow_pfn = fast_find_migrateblock(cc);\n\tblock_start_pfn = pageblock_start_pfn(low_pfn);\n\tif (block_start_pfn < cc->zone->zone_start_pfn)\n\t\tblock_start_pfn = cc->zone->zone_start_pfn;\n\n\t/*\n\t * fast_find_migrateblock marks a pageblock skipped so to avoid\n\t * the isolation_suitable check below, check whether the fast\n\t * search was successful.\n\t */\n\tfast_find_block = low_pfn != cc->migrate_pfn && !cc->fast_search_fail;\n\n\t/* Only scan within a pageblock boundary */\n\tblock_end_pfn = pageblock_end_pfn(low_pfn);\n\n\t/*\n\t * Iterate over whole pageblocks until we find the first suitable.\n\t * Do not cross the free scanner.\n\t */\n\tfor (; block_end_pfn <= cc->free_pfn;\n\t\t\tfast_find_block = false,\n\t\t\tlow_pfn = block_end_pfn,\n\t\t\tblock_start_pfn = block_end_pfn,\n\t\t\tblock_end_pfn += pageblock_nr_pages) {\n\n\t\t/*\n\t\t * This can potentially iterate a massively long zone with\n\t\t * many pageblocks unsuitable, so periodically check if we\n\t\t * need to schedule.\n\t\t */\n\t\tif (!(low_pfn % (SWAP_CLUSTER_MAX * pageblock_nr_pages)))\n\t\t\tcond_resched();\n\n\t\tpage = pageblock_pfn_to_page(block_start_pfn,\n\t\t\t\t\t\tblock_end_pfn, cc->zone);\n\t\tif (!page)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * If isolation recently failed, do not retry. Only check the\n\t\t * pageblock once. COMPACT_CLUSTER_MAX causes a pageblock\n\t\t * to be visited multiple times. Assume skip was checked\n\t\t * before making it \"skip\" so other compaction instances do\n\t\t * not scan the same block.\n\t\t */\n\t\tif (IS_ALIGNED(low_pfn, pageblock_nr_pages) &&\n\t\t    !fast_find_block && !isolation_suitable(cc, page))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * For async compaction, also only scan in MOVABLE blocks\n\t\t * without huge pages. Async compaction is optimistic to see\n\t\t * if the minimum amount of work satisfies the allocation.\n\t\t * The cached PFN is updated as it's possible that all\n\t\t * remaining blocks between source and target are unsuitable\n\t\t * and the compaction scanners fail to meet.\n\t\t */\n\t\tif (!suitable_migration_source(cc, page)) {\n\t\t\tupdate_cached_migrate(cc, block_end_pfn);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Perform the isolation */\n\t\tlow_pfn = isolate_migratepages_block(cc, low_pfn,\n\t\t\t\t\t\tblock_end_pfn, isolate_mode);\n\n\t\tif (!low_pfn)\n\t\t\treturn ISOLATE_ABORT;\n\n\t\t/*\n\t\t * Either we isolated something and proceed with migration. Or\n\t\t * we failed and compact_zone should decide if we should\n\t\t * continue or not.\n\t\t */\n\t\tbreak;\n\t}\n\n\t/* Record where migration scanner will be restarted. */\n\tcc->migrate_pfn = low_pfn;\n\n\treturn cc->nr_migratepages ? ISOLATE_SUCCESS : ISOLATE_NONE;\n}\n\n/*\n * order == -1 is expected when compacting via\n * /proc/sys/vm/compact_memory\n */\nstatic inline bool is_via_compact_memory(int order)\n{\n\treturn order == -1;\n}\n\nstatic bool kswapd_is_running(pg_data_t *pgdat)\n{\n\treturn pgdat->kswapd && (pgdat->kswapd->state == TASK_RUNNING);\n}\n\n/*\n * A zone's fragmentation score is the external fragmentation wrt to the\n * COMPACTION_HPAGE_ORDER scaled by the zone's size. It returns a value\n * in the range [0, 100].\n *\n * The scaling factor ensures that proactive compaction focuses on larger\n * zones like ZONE_NORMAL, rather than smaller, specialized zones like\n * ZONE_DMA32. For smaller zones, the score value remains close to zero,\n * and thus never exceeds the high threshold for proactive compaction.\n */\nstatic unsigned int fragmentation_score_zone(struct zone *zone)\n{\n\tunsigned long score;\n\n\tscore = zone->present_pages *\n\t\t\textfrag_for_order(zone, COMPACTION_HPAGE_ORDER);\n\treturn div64_ul(score, zone->zone_pgdat->node_present_pages + 1);\n}\n\n/*\n * The per-node proactive (background) compaction process is started by its\n * corresponding kcompactd thread when the node's fragmentation score\n * exceeds the high threshold. The compaction process remains active till\n * the node's score falls below the low threshold, or one of the back-off\n * conditions is met.\n */\nstatic unsigned int fragmentation_score_node(pg_data_t *pgdat)\n{\n\tunsigned int score = 0;\n\tint zoneid;\n\n\tfor (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {\n\t\tstruct zone *zone;\n\n\t\tzone = &pgdat->node_zones[zoneid];\n\t\tscore += fragmentation_score_zone(zone);\n\t}\n\n\treturn score;\n}\n\nstatic unsigned int fragmentation_score_wmark(pg_data_t *pgdat, bool low)\n{\n\tunsigned int wmark_low;\n\n\t/*\n\t * Cap the low watermak to avoid excessive compaction\n\t * activity in case a user sets the proactivess tunable\n\t * close to 100 (maximum).\n\t */\n\twmark_low = max(100U - sysctl_compaction_proactiveness, 5U);\n\treturn low ? wmark_low : min(wmark_low + 10, 100U);\n}\n\nstatic bool should_proactive_compact_node(pg_data_t *pgdat)\n{\n\tint wmark_high;\n\n\tif (!sysctl_compaction_proactiveness || kswapd_is_running(pgdat))\n\t\treturn false;\n\n\twmark_high = fragmentation_score_wmark(pgdat, false);\n\treturn fragmentation_score_node(pgdat) > wmark_high;\n}\n\nstatic enum compact_result __compact_finished(struct compact_control *cc)\n{\n\tunsigned int order;\n\tconst int migratetype = cc->migratetype;\n\tint ret;\n\n\t/* Compaction run completes if the migrate and free scanner meet */\n\tif (compact_scanners_met(cc)) {\n\t\t/* Let the next compaction start anew. */\n\t\treset_cached_positions(cc->zone);\n\n\t\t/*\n\t\t * Mark that the PG_migrate_skip information should be cleared\n\t\t * by kswapd when it goes to sleep. kcompactd does not set the\n\t\t * flag itself as the decision to be clear should be directly\n\t\t * based on an allocation request.\n\t\t */\n\t\tif (cc->direct_compaction)\n\t\t\tcc->zone->compact_blockskip_flush = true;\n\n\t\tif (cc->whole_zone)\n\t\t\treturn COMPACT_COMPLETE;\n\t\telse\n\t\t\treturn COMPACT_PARTIAL_SKIPPED;\n\t}\n\n\tif (cc->proactive_compaction) {\n\t\tint score, wmark_low;\n\t\tpg_data_t *pgdat;\n\n\t\tpgdat = cc->zone->zone_pgdat;\n\t\tif (kswapd_is_running(pgdat))\n\t\t\treturn COMPACT_PARTIAL_SKIPPED;\n\n\t\tscore = fragmentation_score_zone(cc->zone);\n\t\twmark_low = fragmentation_score_wmark(pgdat, true);\n\n\t\tif (score > wmark_low)\n\t\t\tret = COMPACT_CONTINUE;\n\t\telse\n\t\t\tret = COMPACT_SUCCESS;\n\n\t\tgoto out;\n\t}\n\n\tif (is_via_compact_memory(cc->order))\n\t\treturn COMPACT_CONTINUE;\n\n\t/*\n\t * Always finish scanning a pageblock to reduce the possibility of\n\t * fallbacks in the future. This is particularly important when\n\t * migration source is unmovable/reclaimable but it's not worth\n\t * special casing.\n\t */\n\tif (!IS_ALIGNED(cc->migrate_pfn, pageblock_nr_pages))\n\t\treturn COMPACT_CONTINUE;\n\n\t/* Direct compactor: Is a suitable page free? */\n\tret = COMPACT_NO_SUITABLE_PAGE;\n\tfor (order = cc->order; order < MAX_ORDER; order++) {\n\t\tstruct free_area *area = &cc->zone->free_area[order];\n\t\tbool can_steal;\n\n\t\t/* Job done if page is free of the right migratetype */\n\t\tif (!free_area_empty(area, migratetype))\n\t\t\treturn COMPACT_SUCCESS;\n\n#ifdef CONFIG_CMA\n\t\t/* MIGRATE_MOVABLE can fallback on MIGRATE_CMA */\n\t\tif (migratetype == MIGRATE_MOVABLE &&\n\t\t\t!free_area_empty(area, MIGRATE_CMA))\n\t\t\treturn COMPACT_SUCCESS;\n#endif\n\t\t/*\n\t\t * Job done if allocation would steal freepages from\n\t\t * other migratetype buddy lists.\n\t\t */\n\t\tif (find_suitable_fallback(area, order, migratetype,\n\t\t\t\t\t\ttrue, &can_steal) != -1) {\n\n\t\t\t/* movable pages are OK in any pageblock */\n\t\t\tif (migratetype == MIGRATE_MOVABLE)\n\t\t\t\treturn COMPACT_SUCCESS;\n\n\t\t\t/*\n\t\t\t * We are stealing for a non-movable allocation. Make\n\t\t\t * sure we finish compacting the current pageblock\n\t\t\t * first so it is as free as possible and we won't\n\t\t\t * have to steal another one soon. This only applies\n\t\t\t * to sync compaction, as async compaction operates\n\t\t\t * on pageblocks of the same migratetype.\n\t\t\t */\n\t\t\tif (cc->mode == MIGRATE_ASYNC ||\n\t\t\t\t\tIS_ALIGNED(cc->migrate_pfn,\n\t\t\t\t\t\t\tpageblock_nr_pages)) {\n\t\t\t\treturn COMPACT_SUCCESS;\n\t\t\t}\n\n\t\t\tret = COMPACT_CONTINUE;\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\tif (cc->contended || fatal_signal_pending(current))\n\t\tret = COMPACT_CONTENDED;\n\n\treturn ret;\n}\n\nstatic enum compact_result compact_finished(struct compact_control *cc)\n{\n\tint ret;\n\n\tret = __compact_finished(cc);\n\ttrace_mm_compaction_finished(cc->zone, cc->order, ret);\n\tif (ret == COMPACT_NO_SUITABLE_PAGE)\n\t\tret = COMPACT_CONTINUE;\n\n\treturn ret;\n}\n\nstatic enum compact_result __compaction_suitable(struct zone *zone, int order,\n\t\t\t\t\tunsigned int alloc_flags,\n\t\t\t\t\tint highest_zoneidx,\n\t\t\t\t\tunsigned long wmark_target)\n{\n\tunsigned long watermark;\n\n\tif (is_via_compact_memory(order))\n\t\treturn COMPACT_CONTINUE;\n\n\twatermark = wmark_pages(zone, alloc_flags & ALLOC_WMARK_MASK);\n\t/*\n\t * If watermarks for high-order allocation are already met, there\n\t * should be no need for compaction at all.\n\t */\n\tif (zone_watermark_ok(zone, order, watermark, highest_zoneidx,\n\t\t\t\t\t\t\t\talloc_flags))\n\t\treturn COMPACT_SUCCESS;\n\n\t/*\n\t * Watermarks for order-0 must be met for compaction to be able to\n\t * isolate free pages for migration targets. This means that the\n\t * watermark and alloc_flags have to match, or be more pessimistic than\n\t * the check in __isolate_free_page(). We don't use the direct\n\t * compactor's alloc_flags, as they are not relevant for freepage\n\t * isolation. We however do use the direct compactor's highest_zoneidx\n\t * to skip over zones where lowmem reserves would prevent allocation\n\t * even if compaction succeeds.\n\t * For costly orders, we require low watermark instead of min for\n\t * compaction to proceed to increase its chances.\n\t * ALLOC_CMA is used, as pages in CMA pageblocks are considered\n\t * suitable migration targets\n\t */\n\twatermark = (order > PAGE_ALLOC_COSTLY_ORDER) ?\n\t\t\t\tlow_wmark_pages(zone) : min_wmark_pages(zone);\n\twatermark += compact_gap(order);\n\tif (!__zone_watermark_ok(zone, 0, watermark, highest_zoneidx,\n\t\t\t\t\t\tALLOC_CMA, wmark_target))\n\t\treturn COMPACT_SKIPPED;\n\n\treturn COMPACT_CONTINUE;\n}\n\n/*\n * compaction_suitable: Is this suitable to run compaction on this zone now?\n * Returns\n *   COMPACT_SKIPPED  - If there are too few free pages for compaction\n *   COMPACT_SUCCESS  - If the allocation would succeed without compaction\n *   COMPACT_CONTINUE - If compaction should run now\n */\nenum compact_result compaction_suitable(struct zone *zone, int order,\n\t\t\t\t\tunsigned int alloc_flags,\n\t\t\t\t\tint highest_zoneidx)\n{\n\tenum compact_result ret;\n\tint fragindex;\n\n\tret = __compaction_suitable(zone, order, alloc_flags, highest_zoneidx,\n\t\t\t\t    zone_page_state(zone, NR_FREE_PAGES));\n\t/*\n\t * fragmentation index determines if allocation failures are due to\n\t * low memory or external fragmentation\n\t *\n\t * index of -1000 would imply allocations might succeed depending on\n\t * watermarks, but we already failed the high-order watermark check\n\t * index towards 0 implies failure is due to lack of memory\n\t * index towards 1000 implies failure is due to fragmentation\n\t *\n\t * Only compact if a failure would be due to fragmentation. Also\n\t * ignore fragindex for non-costly orders where the alternative to\n\t * a successful reclaim/compaction is OOM. Fragindex and the\n\t * vm.extfrag_threshold sysctl is meant as a heuristic to prevent\n\t * excessive compaction for costly orders, but it should not be at the\n\t * expense of system stability.\n\t */\n\tif (ret == COMPACT_CONTINUE && (order > PAGE_ALLOC_COSTLY_ORDER)) {\n\t\tfragindex = fragmentation_index(zone, order);\n\t\tif (fragindex >= 0 && fragindex <= sysctl_extfrag_threshold)\n\t\t\tret = COMPACT_NOT_SUITABLE_ZONE;\n\t}\n\n\ttrace_mm_compaction_suitable(zone, order, ret);\n\tif (ret == COMPACT_NOT_SUITABLE_ZONE)\n\t\tret = COMPACT_SKIPPED;\n\n\treturn ret;\n}\n\nbool compaction_zonelist_suitable(struct alloc_context *ac, int order,\n\t\tint alloc_flags)\n{\n\tstruct zone *zone;\n\tstruct zoneref *z;\n\n\t/*\n\t * Make sure at least one zone would pass __compaction_suitable if we continue\n\t * retrying the reclaim.\n\t */\n\tfor_each_zone_zonelist_nodemask(zone, z, ac->zonelist,\n\t\t\t\tac->highest_zoneidx, ac->nodemask) {\n\t\tunsigned long available;\n\t\tenum compact_result compact_result;\n\n\t\t/*\n\t\t * Do not consider all the reclaimable memory because we do not\n\t\t * want to trash just for a single high order allocation which\n\t\t * is even not guaranteed to appear even if __compaction_suitable\n\t\t * is happy about the watermark check.\n\t\t */\n\t\tavailable = zone_reclaimable_pages(zone) / order;\n\t\tavailable += zone_page_state_snapshot(zone, NR_FREE_PAGES);\n\t\tcompact_result = __compaction_suitable(zone, order, alloc_flags,\n\t\t\t\tac->highest_zoneidx, available);\n\t\tif (compact_result != COMPACT_SKIPPED)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic enum compact_result\ncompact_zone(struct compact_control *cc, struct capture_control *capc)\n{\n\tenum compact_result ret;\n\tunsigned long start_pfn = cc->zone->zone_start_pfn;\n\tunsigned long end_pfn = zone_end_pfn(cc->zone);\n\tunsigned long last_migrated_pfn;\n\tconst bool sync = cc->mode != MIGRATE_ASYNC;\n\tbool update_cached;\n\n\t/*\n\t * These counters track activities during zone compaction.  Initialize\n\t * them before compacting a new zone.\n\t */\n\tcc->total_migrate_scanned = 0;\n\tcc->total_free_scanned = 0;\n\tcc->nr_migratepages = 0;\n\tcc->nr_freepages = 0;\n\tINIT_LIST_HEAD(&cc->freepages);\n\tINIT_LIST_HEAD(&cc->migratepages);\n\n\tcc->migratetype = gfp_migratetype(cc->gfp_mask);\n\tret = compaction_suitable(cc->zone, cc->order, cc->alloc_flags,\n\t\t\t\t\t\t\tcc->highest_zoneidx);\n\t/* Compaction is likely to fail */\n\tif (ret == COMPACT_SUCCESS || ret == COMPACT_SKIPPED)\n\t\treturn ret;\n\n\t/* huh, compaction_suitable is returning something unexpected */\n\tVM_BUG_ON(ret != COMPACT_CONTINUE);\n\n\t/*\n\t * Clear pageblock skip if there were failures recently and compaction\n\t * is about to be retried after being deferred.\n\t */\n\tif (compaction_restarting(cc->zone, cc->order))\n\t\t__reset_isolation_suitable(cc->zone);\n\n\t/*\n\t * Setup to move all movable pages to the end of the zone. Used cached\n\t * information on where the scanners should start (unless we explicitly\n\t * want to compact the whole zone), but check that it is initialised\n\t * by ensuring the values are within zone boundaries.\n\t */\n\tcc->fast_start_pfn = 0;\n\tif (cc->whole_zone) {\n\t\tcc->migrate_pfn = start_pfn;\n\t\tcc->free_pfn = pageblock_start_pfn(end_pfn - 1);\n\t} else {\n\t\tcc->migrate_pfn = cc->zone->compact_cached_migrate_pfn[sync];\n\t\tcc->free_pfn = cc->zone->compact_cached_free_pfn;\n\t\tif (cc->free_pfn < start_pfn || cc->free_pfn >= end_pfn) {\n\t\t\tcc->free_pfn = pageblock_start_pfn(end_pfn - 1);\n\t\t\tcc->zone->compact_cached_free_pfn = cc->free_pfn;\n\t\t}\n\t\tif (cc->migrate_pfn < start_pfn || cc->migrate_pfn >= end_pfn) {\n\t\t\tcc->migrate_pfn = start_pfn;\n\t\t\tcc->zone->compact_cached_migrate_pfn[0] = cc->migrate_pfn;\n\t\t\tcc->zone->compact_cached_migrate_pfn[1] = cc->migrate_pfn;\n\t\t}\n\n\t\tif (cc->migrate_pfn <= cc->zone->compact_init_migrate_pfn)\n\t\t\tcc->whole_zone = true;\n\t}\n\n\tlast_migrated_pfn = 0;\n\n\t/*\n\t * Migrate has separate cached PFNs for ASYNC and SYNC* migration on\n\t * the basis that some migrations will fail in ASYNC mode. However,\n\t * if the cached PFNs match and pageblocks are skipped due to having\n\t * no isolation candidates, then the sync state does not matter.\n\t * Until a pageblock with isolation candidates is found, keep the\n\t * cached PFNs in sync to avoid revisiting the same blocks.\n\t */\n\tupdate_cached = !sync &&\n\t\tcc->zone->compact_cached_migrate_pfn[0] == cc->zone->compact_cached_migrate_pfn[1];\n\n\ttrace_mm_compaction_begin(start_pfn, cc->migrate_pfn,\n\t\t\t\tcc->free_pfn, end_pfn, sync);\n\n\tmigrate_prep_local();\n\n\twhile ((ret = compact_finished(cc)) == COMPACT_CONTINUE) {\n\t\tint err;\n\t\tunsigned long iteration_start_pfn = cc->migrate_pfn;\n\n\t\t/*\n\t\t * Avoid multiple rescans which can happen if a page cannot be\n\t\t * isolated (dirty/writeback in async mode) or if the migrated\n\t\t * pages are being allocated before the pageblock is cleared.\n\t\t * The first rescan will capture the entire pageblock for\n\t\t * migration. If it fails, it'll be marked skip and scanning\n\t\t * will proceed as normal.\n\t\t */\n\t\tcc->rescan = false;\n\t\tif (pageblock_start_pfn(last_migrated_pfn) ==\n\t\t    pageblock_start_pfn(iteration_start_pfn)) {\n\t\t\tcc->rescan = true;\n\t\t}\n\n\t\tswitch (isolate_migratepages(cc)) {\n\t\tcase ISOLATE_ABORT:\n\t\t\tret = COMPACT_CONTENDED;\n\t\t\tputback_movable_pages(&cc->migratepages);\n\t\t\tcc->nr_migratepages = 0;\n\t\t\tgoto out;\n\t\tcase ISOLATE_NONE:\n\t\t\tif (update_cached) {\n\t\t\t\tcc->zone->compact_cached_migrate_pfn[1] =\n\t\t\t\t\tcc->zone->compact_cached_migrate_pfn[0];\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * We haven't isolated and migrated anything, but\n\t\t\t * there might still be unflushed migrations from\n\t\t\t * previous cc->order aligned block.\n\t\t\t */\n\t\t\tgoto check_drain;\n\t\tcase ISOLATE_SUCCESS:\n\t\t\tupdate_cached = false;\n\t\t\tlast_migrated_pfn = iteration_start_pfn;\n\t\t}\n\n\t\terr = migrate_pages(&cc->migratepages, compaction_alloc,\n\t\t\t\tcompaction_free, (unsigned long)cc, cc->mode,\n\t\t\t\tMR_COMPACTION);\n\n\t\ttrace_mm_compaction_migratepages(cc->nr_migratepages, err,\n\t\t\t\t\t\t\t&cc->migratepages);\n\n\t\t/* All pages were either migrated or will be released */\n\t\tcc->nr_migratepages = 0;\n\t\tif (err) {\n\t\t\tputback_movable_pages(&cc->migratepages);\n\t\t\t/*\n\t\t\t * migrate_pages() may return -ENOMEM when scanners meet\n\t\t\t * and we want compact_finished() to detect it\n\t\t\t */\n\t\t\tif (err == -ENOMEM && !compact_scanners_met(cc)) {\n\t\t\t\tret = COMPACT_CONTENDED;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\t/*\n\t\t\t * We failed to migrate at least one page in the current\n\t\t\t * order-aligned block, so skip the rest of it.\n\t\t\t */\n\t\t\tif (cc->direct_compaction &&\n\t\t\t\t\t\t(cc->mode == MIGRATE_ASYNC)) {\n\t\t\t\tcc->migrate_pfn = block_end_pfn(\n\t\t\t\t\t\tcc->migrate_pfn - 1, cc->order);\n\t\t\t\t/* Draining pcplists is useless in this case */\n\t\t\t\tlast_migrated_pfn = 0;\n\t\t\t}\n\t\t}\n\ncheck_drain:\n\t\t/*\n\t\t * Has the migration scanner moved away from the previous\n\t\t * cc->order aligned block where we migrated from? If yes,\n\t\t * flush the pages that were freed, so that they can merge and\n\t\t * compact_finished() can detect immediately if allocation\n\t\t * would succeed.\n\t\t */\n\t\tif (cc->order > 0 && last_migrated_pfn) {\n\t\t\tunsigned long current_block_start =\n\t\t\t\tblock_start_pfn(cc->migrate_pfn, cc->order);\n\n\t\t\tif (last_migrated_pfn < current_block_start) {\n\t\t\t\tlru_add_drain_cpu_zone(cc->zone);\n\t\t\t\t/* No more flushing until we migrate again */\n\t\t\t\tlast_migrated_pfn = 0;\n\t\t\t}\n\t\t}\n\n\t\t/* Stop if a page has been captured */\n\t\tif (capc && capc->page) {\n\t\t\tret = COMPACT_SUCCESS;\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\t/*\n\t * Release free pages and update where the free scanner should restart,\n\t * so we don't leave any returned pages behind in the next attempt.\n\t */\n\tif (cc->nr_freepages > 0) {\n\t\tunsigned long free_pfn = release_freepages(&cc->freepages);\n\n\t\tcc->nr_freepages = 0;\n\t\tVM_BUG_ON(free_pfn == 0);\n\t\t/* The cached pfn is always the first in a pageblock */\n\t\tfree_pfn = pageblock_start_pfn(free_pfn);\n\t\t/*\n\t\t * Only go back, not forward. The cached pfn might have been\n\t\t * already reset to zone end in compact_finished()\n\t\t */\n\t\tif (free_pfn > cc->zone->compact_cached_free_pfn)\n\t\t\tcc->zone->compact_cached_free_pfn = free_pfn;\n\t}\n\n\tcount_compact_events(COMPACTMIGRATE_SCANNED, cc->total_migrate_scanned);\n\tcount_compact_events(COMPACTFREE_SCANNED, cc->total_free_scanned);\n\n\ttrace_mm_compaction_end(start_pfn, cc->migrate_pfn,\n\t\t\t\tcc->free_pfn, end_pfn, sync, ret);\n\n\treturn ret;\n}\n\nstatic enum compact_result compact_zone_order(struct zone *zone, int order,\n\t\tgfp_t gfp_mask, enum compact_priority prio,\n\t\tunsigned int alloc_flags, int highest_zoneidx,\n\t\tstruct page **capture)\n{\n\tenum compact_result ret;\n\tstruct compact_control cc = {\n\t\t.order = order,\n\t\t.search_order = order,\n\t\t.gfp_mask = gfp_mask,\n\t\t.zone = zone,\n\t\t.mode = (prio == COMPACT_PRIO_ASYNC) ?\n\t\t\t\t\tMIGRATE_ASYNC :\tMIGRATE_SYNC_LIGHT,\n\t\t.alloc_flags = alloc_flags,\n\t\t.highest_zoneidx = highest_zoneidx,\n\t\t.direct_compaction = true,\n\t\t.whole_zone = (prio == MIN_COMPACT_PRIORITY),\n\t\t.ignore_skip_hint = (prio == MIN_COMPACT_PRIORITY),\n\t\t.ignore_block_suitable = (prio == MIN_COMPACT_PRIORITY)\n\t};\n\tstruct capture_control capc = {\n\t\t.cc = &cc,\n\t\t.page = NULL,\n\t};\n\n\t/*\n\t * Make sure the structs are really initialized before we expose the\n\t * capture control, in case we are interrupted and the interrupt handler\n\t * frees a page.\n\t */\n\tbarrier();\n\tWRITE_ONCE(current->capture_control, &capc);\n\n\tret = compact_zone(&cc, &capc);\n\n\tVM_BUG_ON(!list_empty(&cc.freepages));\n\tVM_BUG_ON(!list_empty(&cc.migratepages));\n\n\t/*\n\t * Make sure we hide capture control first before we read the captured\n\t * page pointer, otherwise an interrupt could free and capture a page\n\t * and we would leak it.\n\t */\n\tWRITE_ONCE(current->capture_control, NULL);\n\t*capture = READ_ONCE(capc.page);\n\n\treturn ret;\n}\n\nint sysctl_extfrag_threshold = 500;\n\n/**\n * try_to_compact_pages - Direct compact to satisfy a high-order allocation\n * @gfp_mask: The GFP mask of the current allocation\n * @order: The order of the current allocation\n * @alloc_flags: The allocation flags of the current allocation\n * @ac: The context of current allocation\n * @prio: Determines how hard direct compaction should try to succeed\n * @capture: Pointer to free page created by compaction will be stored here\n *\n * This is the main entry point for direct page compaction.\n */\nenum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,\n\t\tunsigned int alloc_flags, const struct alloc_context *ac,\n\t\tenum compact_priority prio, struct page **capture)\n{\n\tint may_perform_io = gfp_mask & __GFP_IO;\n\tstruct zoneref *z;\n\tstruct zone *zone;\n\tenum compact_result rc = COMPACT_SKIPPED;\n\n\t/*\n\t * Check if the GFP flags allow compaction - GFP_NOIO is really\n\t * tricky context because the migration might require IO\n\t */\n\tif (!may_perform_io)\n\t\treturn COMPACT_SKIPPED;\n\n\ttrace_mm_compaction_try_to_compact_pages(order, gfp_mask, prio);\n\n\t/* Compact each zone in the list */\n\tfor_each_zone_zonelist_nodemask(zone, z, ac->zonelist,\n\t\t\t\t\tac->highest_zoneidx, ac->nodemask) {\n\t\tenum compact_result status;\n\n\t\tif (prio > MIN_COMPACT_PRIORITY\n\t\t\t\t\t&& compaction_deferred(zone, order)) {\n\t\t\trc = max_t(enum compact_result, COMPACT_DEFERRED, rc);\n\t\t\tcontinue;\n\t\t}\n\n\t\tstatus = compact_zone_order(zone, order, gfp_mask, prio,\n\t\t\t\talloc_flags, ac->highest_zoneidx, capture);\n\t\trc = max(status, rc);\n\n\t\t/* The allocation should succeed, stop compacting */\n\t\tif (status == COMPACT_SUCCESS) {\n\t\t\t/*\n\t\t\t * We think the allocation will succeed in this zone,\n\t\t\t * but it is not certain, hence the false. The caller\n\t\t\t * will repeat this with true if allocation indeed\n\t\t\t * succeeds in this zone.\n\t\t\t */\n\t\t\tcompaction_defer_reset(zone, order, false);\n\n\t\t\tbreak;\n\t\t}\n\n\t\tif (prio != COMPACT_PRIO_ASYNC && (status == COMPACT_COMPLETE ||\n\t\t\t\t\tstatus == COMPACT_PARTIAL_SKIPPED))\n\t\t\t/*\n\t\t\t * We think that allocation won't succeed in this zone\n\t\t\t * so we defer compaction there. If it ends up\n\t\t\t * succeeding after all, it will be reset.\n\t\t\t */\n\t\t\tdefer_compaction(zone, order);\n\n\t\t/*\n\t\t * We might have stopped compacting due to need_resched() in\n\t\t * async compaction, or due to a fatal signal detected. In that\n\t\t * case do not try further zones\n\t\t */\n\t\tif ((prio == COMPACT_PRIO_ASYNC && need_resched())\n\t\t\t\t\t|| fatal_signal_pending(current))\n\t\t\tbreak;\n\t}\n\n\treturn rc;\n}\n\n/*\n * Compact all zones within a node till each zone's fragmentation score\n * reaches within proactive compaction thresholds (as determined by the\n * proactiveness tunable).\n *\n * It is possible that the function returns before reaching score targets\n * due to various back-off conditions, such as, contention on per-node or\n * per-zone locks.\n */\nstatic void proactive_compact_node(pg_data_t *pgdat)\n{\n\tint zoneid;\n\tstruct zone *zone;\n\tstruct compact_control cc = {\n\t\t.order = -1,\n\t\t.mode = MIGRATE_SYNC_LIGHT,\n\t\t.ignore_skip_hint = true,\n\t\t.whole_zone = true,\n\t\t.gfp_mask = GFP_KERNEL,\n\t\t.proactive_compaction = true,\n\t};\n\n\tfor (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {\n\t\tzone = &pgdat->node_zones[zoneid];\n\t\tif (!populated_zone(zone))\n\t\t\tcontinue;\n\n\t\tcc.zone = zone;\n\n\t\tcompact_zone(&cc, NULL);\n\n\t\tVM_BUG_ON(!list_empty(&cc.freepages));\n\t\tVM_BUG_ON(!list_empty(&cc.migratepages));\n\t}\n}\n\n/* Compact all zones within a node */\nstatic void compact_node(int nid)\n{\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\tint zoneid;\n\tstruct zone *zone;\n\tstruct compact_control cc = {\n\t\t.order = -1,\n\t\t.mode = MIGRATE_SYNC,\n\t\t.ignore_skip_hint = true,\n\t\t.whole_zone = true,\n\t\t.gfp_mask = GFP_KERNEL,\n\t};\n\n\n\tfor (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {\n\n\t\tzone = &pgdat->node_zones[zoneid];\n\t\tif (!populated_zone(zone))\n\t\t\tcontinue;\n\n\t\tcc.zone = zone;\n\n\t\tcompact_zone(&cc, NULL);\n\n\t\tVM_BUG_ON(!list_empty(&cc.freepages));\n\t\tVM_BUG_ON(!list_empty(&cc.migratepages));\n\t}\n}\n\n/* Compact all nodes in the system */\nstatic void compact_nodes(void)\n{\n\tint nid;\n\n\t/* Flush pending updates to the LRU lists */\n\tlru_add_drain_all();\n\n\tfor_each_online_node(nid)\n\t\tcompact_node(nid);\n}\n\n/* The written value is actually unused, all memory is compacted */\nint sysctl_compact_memory;\n\n/*\n * Tunable for proactive compaction. It determines how\n * aggressively the kernel should compact memory in the\n * background. It takes values in the range [0, 100].\n */\nunsigned int __read_mostly sysctl_compaction_proactiveness = 20;\n\n/*\n * This is the entry point for compacting all nodes via\n * /proc/sys/vm/compact_memory\n */\nint sysctl_compaction_handler(struct ctl_table *table, int write,\n\t\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tif (write)\n\t\tcompact_nodes();\n\n\treturn 0;\n}\n\n#if defined(CONFIG_SYSFS) && defined(CONFIG_NUMA)\nstatic ssize_t sysfs_compact_node(struct device *dev,\n\t\t\tstruct device_attribute *attr,\n\t\t\tconst char *buf, size_t count)\n{\n\tint nid = dev->id;\n\n\tif (nid >= 0 && nid < nr_node_ids && node_online(nid)) {\n\t\t/* Flush pending updates to the LRU lists */\n\t\tlru_add_drain_all();\n\n\t\tcompact_node(nid);\n\t}\n\n\treturn count;\n}\nstatic DEVICE_ATTR(compact, 0200, NULL, sysfs_compact_node);\n\nint compaction_register_node(struct node *node)\n{\n\treturn device_create_file(&node->dev, &dev_attr_compact);\n}\n\nvoid compaction_unregister_node(struct node *node)\n{\n\treturn device_remove_file(&node->dev, &dev_attr_compact);\n}\n#endif /* CONFIG_SYSFS && CONFIG_NUMA */\n\nstatic inline bool kcompactd_work_requested(pg_data_t *pgdat)\n{\n\treturn pgdat->kcompactd_max_order > 0 || kthread_should_stop();\n}\n\nstatic bool kcompactd_node_suitable(pg_data_t *pgdat)\n{\n\tint zoneid;\n\tstruct zone *zone;\n\tenum zone_type highest_zoneidx = pgdat->kcompactd_highest_zoneidx;\n\n\tfor (zoneid = 0; zoneid <= highest_zoneidx; zoneid++) {\n\t\tzone = &pgdat->node_zones[zoneid];\n\n\t\tif (!populated_zone(zone))\n\t\t\tcontinue;\n\n\t\tif (compaction_suitable(zone, pgdat->kcompactd_max_order, 0,\n\t\t\t\t\thighest_zoneidx) == COMPACT_CONTINUE)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void kcompactd_do_work(pg_data_t *pgdat)\n{\n\t/*\n\t * With no special task, compact all zones so that a page of requested\n\t * order is allocatable.\n\t */\n\tint zoneid;\n\tstruct zone *zone;\n\tstruct compact_control cc = {\n\t\t.order = pgdat->kcompactd_max_order,\n\t\t.search_order = pgdat->kcompactd_max_order,\n\t\t.highest_zoneidx = pgdat->kcompactd_highest_zoneidx,\n\t\t.mode = MIGRATE_SYNC_LIGHT,\n\t\t.ignore_skip_hint = false,\n\t\t.gfp_mask = GFP_KERNEL,\n\t};\n\ttrace_mm_compaction_kcompactd_wake(pgdat->node_id, cc.order,\n\t\t\t\t\t\t\tcc.highest_zoneidx);\n\tcount_compact_event(KCOMPACTD_WAKE);\n\n\tfor (zoneid = 0; zoneid <= cc.highest_zoneidx; zoneid++) {\n\t\tint status;\n\n\t\tzone = &pgdat->node_zones[zoneid];\n\t\tif (!populated_zone(zone))\n\t\t\tcontinue;\n\n\t\tif (compaction_deferred(zone, cc.order))\n\t\t\tcontinue;\n\n\t\tif (compaction_suitable(zone, cc.order, 0, zoneid) !=\n\t\t\t\t\t\t\tCOMPACT_CONTINUE)\n\t\t\tcontinue;\n\n\t\tif (kthread_should_stop())\n\t\t\treturn;\n\n\t\tcc.zone = zone;\n\t\tstatus = compact_zone(&cc, NULL);\n\n\t\tif (status == COMPACT_SUCCESS) {\n\t\t\tcompaction_defer_reset(zone, cc.order, false);\n\t\t} else if (status == COMPACT_PARTIAL_SKIPPED || status == COMPACT_COMPLETE) {\n\t\t\t/*\n\t\t\t * Buddy pages may become stranded on pcps that could\n\t\t\t * otherwise coalesce on the zone's free area for\n\t\t\t * order >= cc.order.  This is ratelimited by the\n\t\t\t * upcoming deferral.\n\t\t\t */\n\t\t\tdrain_all_pages(zone);\n\n\t\t\t/*\n\t\t\t * We use sync migration mode here, so we defer like\n\t\t\t * sync direct compaction does.\n\t\t\t */\n\t\t\tdefer_compaction(zone, cc.order);\n\t\t}\n\n\t\tcount_compact_events(KCOMPACTD_MIGRATE_SCANNED,\n\t\t\t\t     cc.total_migrate_scanned);\n\t\tcount_compact_events(KCOMPACTD_FREE_SCANNED,\n\t\t\t\t     cc.total_free_scanned);\n\n\t\tVM_BUG_ON(!list_empty(&cc.freepages));\n\t\tVM_BUG_ON(!list_empty(&cc.migratepages));\n\t}\n\n\t/*\n\t * Regardless of success, we are done until woken up next. But remember\n\t * the requested order/highest_zoneidx in case it was higher/tighter\n\t * than our current ones\n\t */\n\tif (pgdat->kcompactd_max_order <= cc.order)\n\t\tpgdat->kcompactd_max_order = 0;\n\tif (pgdat->kcompactd_highest_zoneidx >= cc.highest_zoneidx)\n\t\tpgdat->kcompactd_highest_zoneidx = pgdat->nr_zones - 1;\n}\n\nvoid wakeup_kcompactd(pg_data_t *pgdat, int order, int highest_zoneidx)\n{\n\tif (!order)\n\t\treturn;\n\n\tif (pgdat->kcompactd_max_order < order)\n\t\tpgdat->kcompactd_max_order = order;\n\n\tif (pgdat->kcompactd_highest_zoneidx > highest_zoneidx)\n\t\tpgdat->kcompactd_highest_zoneidx = highest_zoneidx;\n\n\t/*\n\t * Pairs with implicit barrier in wait_event_freezable()\n\t * such that wakeups are not missed.\n\t */\n\tif (!wq_has_sleeper(&pgdat->kcompactd_wait))\n\t\treturn;\n\n\tif (!kcompactd_node_suitable(pgdat))\n\t\treturn;\n\n\ttrace_mm_compaction_wakeup_kcompactd(pgdat->node_id, order,\n\t\t\t\t\t\t\thighest_zoneidx);\n\twake_up_interruptible(&pgdat->kcompactd_wait);\n}\n\n/*\n * The background compaction daemon, started as a kernel thread\n * from the init process.\n */\nstatic int kcompactd(void *p)\n{\n\tpg_data_t *pgdat = (pg_data_t*)p;\n\tstruct task_struct *tsk = current;\n\tunsigned int proactive_defer = 0;\n\n\tconst struct cpumask *cpumask = cpumask_of_node(pgdat->node_id);\n\n\tif (!cpumask_empty(cpumask))\n\t\tset_cpus_allowed_ptr(tsk, cpumask);\n\n\tset_freezable();\n\n\tpgdat->kcompactd_max_order = 0;\n\tpgdat->kcompactd_highest_zoneidx = pgdat->nr_zones - 1;\n\n\twhile (!kthread_should_stop()) {\n\t\tunsigned long pflags;\n\n\t\ttrace_mm_compaction_kcompactd_sleep(pgdat->node_id);\n\t\tif (wait_event_freezable_timeout(pgdat->kcompactd_wait,\n\t\t\tkcompactd_work_requested(pgdat),\n\t\t\tmsecs_to_jiffies(HPAGE_FRAG_CHECK_INTERVAL_MSEC))) {\n\n\t\t\tpsi_memstall_enter(&pflags);\n\t\t\tkcompactd_do_work(pgdat);\n\t\t\tpsi_memstall_leave(&pflags);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* kcompactd wait timeout */\n\t\tif (should_proactive_compact_node(pgdat)) {\n\t\t\tunsigned int prev_score, score;\n\n\t\t\tif (proactive_defer) {\n\t\t\t\tproactive_defer--;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tprev_score = fragmentation_score_node(pgdat);\n\t\t\tproactive_compact_node(pgdat);\n\t\t\tscore = fragmentation_score_node(pgdat);\n\t\t\t/*\n\t\t\t * Defer proactive compaction if the fragmentation\n\t\t\t * score did not go down i.e. no progress made.\n\t\t\t */\n\t\t\tproactive_defer = score < prev_score ?\n\t\t\t\t\t0 : 1 << COMPACT_MAX_DEFER_SHIFT;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n * This kcompactd start function will be called by init and node-hot-add.\n * On node-hot-add, kcompactd will moved to proper cpus if cpus are hot-added.\n */\nint kcompactd_run(int nid)\n{\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\tint ret = 0;\n\n\tif (pgdat->kcompactd)\n\t\treturn 0;\n\n\tpgdat->kcompactd = kthread_run(kcompactd, pgdat, \"kcompactd%d\", nid);\n\tif (IS_ERR(pgdat->kcompactd)) {\n\t\tpr_err(\"Failed to start kcompactd on node %d\\n\", nid);\n\t\tret = PTR_ERR(pgdat->kcompactd);\n\t\tpgdat->kcompactd = NULL;\n\t}\n\treturn ret;\n}\n\n/*\n * Called by memory hotplug when all memory in a node is offlined. Caller must\n * hold mem_hotplug_begin/end().\n */\nvoid kcompactd_stop(int nid)\n{\n\tstruct task_struct *kcompactd = NODE_DATA(nid)->kcompactd;\n\n\tif (kcompactd) {\n\t\tkthread_stop(kcompactd);\n\t\tNODE_DATA(nid)->kcompactd = NULL;\n\t}\n}\n\n/*\n * It's optimal to keep kcompactd on the same CPUs as their memory, but\n * not required for correctness. So if the last cpu in a node goes\n * away, we get changed to run anywhere: as the first one comes back,\n * restore their cpu bindings.\n */\nstatic int kcompactd_cpu_online(unsigned int cpu)\n{\n\tint nid;\n\n\tfor_each_node_state(nid, N_MEMORY) {\n\t\tpg_data_t *pgdat = NODE_DATA(nid);\n\t\tconst struct cpumask *mask;\n\n\t\tmask = cpumask_of_node(pgdat->node_id);\n\n\t\tif (cpumask_any_and(cpu_online_mask, mask) < nr_cpu_ids)\n\t\t\t/* One of our CPUs online: restore mask */\n\t\t\tset_cpus_allowed_ptr(pgdat->kcompactd, mask);\n\t}\n\treturn 0;\n}\n\nstatic int __init kcompactd_init(void)\n{\n\tint nid;\n\tint ret;\n\n\tret = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN,\n\t\t\t\t\t\"mm/compaction:online\",\n\t\t\t\t\tkcompactd_cpu_online, NULL);\n\tif (ret < 0) {\n\t\tpr_err(\"kcompactd: failed to register hotplug callbacks.\\n\");\n\t\treturn ret;\n\t}\n\n\tfor_each_node_state(nid, N_MEMORY)\n\t\tkcompactd_run(nid);\n\treturn 0;\n}\nsubsys_initcall(kcompactd_init)\n\n#endif /* CONFIG_COMPACTION */\n"}, "9": {"id": 9, "path": "/src/include/linux/minmax.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_MINMAX_H\n#define _LINUX_MINMAX_H\n\n/*\n * min()/max()/clamp() macros must accomplish three things:\n *\n * - avoid multiple evaluations of the arguments (so side-effects like\n *   \"x++\" happen only once) when non-constant.\n * - perform strict type-checking (to generate warnings instead of\n *   nasty runtime surprises). See the \"unnecessary\" pointer comparison\n *   in __typecheck().\n * - retain result as a constant expressions when called with only\n *   constant expressions (to avoid tripping VLA warnings in stack\n *   allocation usage).\n */\n#define __typecheck(x, y) \\\n\t(!!(sizeof((typeof(x) *)1 == (typeof(y) *)1)))\n\n/*\n * This returns a constant expression while determining if an argument is\n * a constant expression, most importantly without evaluating the argument.\n * Glory to Martin Uecker <Martin.Uecker@med.uni-goettingen.de>\n */\n#define __is_constexpr(x) \\\n\t(sizeof(int) == sizeof(*(8 ? ((void *)((long)(x) * 0l)) : (int *)8)))\n\n#define __no_side_effects(x, y) \\\n\t\t(__is_constexpr(x) && __is_constexpr(y))\n\n#define __safe_cmp(x, y) \\\n\t\t(__typecheck(x, y) && __no_side_effects(x, y))\n\n#define __cmp(x, y, op)\t((x) op (y) ? (x) : (y))\n\n#define __cmp_once(x, y, unique_x, unique_y, op) ({\t\\\n\t\ttypeof(x) unique_x = (x);\t\t\\\n\t\ttypeof(y) unique_y = (y);\t\t\\\n\t\t__cmp(unique_x, unique_y, op); })\n\n#define __careful_cmp(x, y, op) \\\n\t__builtin_choose_expr(__safe_cmp(x, y), \\\n\t\t__cmp(x, y, op), \\\n\t\t__cmp_once(x, y, __UNIQUE_ID(__x), __UNIQUE_ID(__y), op))\n\n/**\n * min - return minimum of two values of the same or compatible types\n * @x: first value\n * @y: second value\n */\n#define min(x, y)\t__careful_cmp(x, y, <)\n\n/**\n * max - return maximum of two values of the same or compatible types\n * @x: first value\n * @y: second value\n */\n#define max(x, y)\t__careful_cmp(x, y, >)\n\n/**\n * min3 - return minimum of three values\n * @x: first value\n * @y: second value\n * @z: third value\n */\n#define min3(x, y, z) min((typeof(x))min(x, y), z)\n\n/**\n * max3 - return maximum of three values\n * @x: first value\n * @y: second value\n * @z: third value\n */\n#define max3(x, y, z) max((typeof(x))max(x, y), z)\n\n/**\n * min_not_zero - return the minimum that is _not_ zero, unless both are zero\n * @x: value1\n * @y: value2\n */\n#define min_not_zero(x, y) ({\t\t\t\\\n\ttypeof(x) __x = (x);\t\t\t\\\n\ttypeof(y) __y = (y);\t\t\t\\\n\t__x == 0 ? __y : ((__y == 0) ? __x : min(__x, __y)); })\n\n/**\n * clamp - return a value clamped to a given range with strict typechecking\n * @val: current value\n * @lo: lowest allowable value\n * @hi: highest allowable value\n *\n * This macro does strict typechecking of @lo/@hi to make sure they are of the\n * same type as @val.  See the unnecessary pointer comparisons.\n */\n#define clamp(val, lo, hi) min((typeof(val))max(val, lo), hi)\n\n/*\n * ..and if you can't take the strict\n * types, you can specify one yourself.\n *\n * Or not use min/max/clamp at all, of course.\n */\n\n/**\n * min_t - return minimum of two values, using the specified type\n * @type: data type to use\n * @x: first value\n * @y: second value\n */\n#define min_t(type, x, y)\t__careful_cmp((type)(x), (type)(y), <)\n\n/**\n * max_t - return maximum of two values, using the specified type\n * @type: data type to use\n * @x: first value\n * @y: second value\n */\n#define max_t(type, x, y)\t__careful_cmp((type)(x), (type)(y), >)\n\n/**\n * clamp_t - return a value clamped to a given range using a given type\n * @type: the type of variable to use\n * @val: current value\n * @lo: minimum allowable value\n * @hi: maximum allowable value\n *\n * This macro does no typechecking and uses temporary variables of type\n * @type to make all the comparisons.\n */\n#define clamp_t(type, val, lo, hi) min_t(type, max_t(type, val, lo), hi)\n\n/**\n * clamp_val - return a value clamped to a given range using val's type\n * @val: current value\n * @lo: minimum allowable value\n * @hi: maximum allowable value\n *\n * This macro does no typechecking and uses temporary variables of whatever\n * type the input argument @val is.  This is useful when @val is an unsigned\n * type and @lo and @hi are literals that will otherwise be assigned a signed\n * integer type.\n */\n#define clamp_val(val, lo, hi) clamp_t(typeof(val), val, lo, hi)\n\n/**\n * swap - swap values of @a and @b\n * @a: first value\n * @b: second value\n */\n#define swap(a, b) \\\n\tdo { typeof(a) __tmp = (a); (a) = (b); (b) = __tmp; } while (0)\n\n#endif\t/* _LINUX_MINMAX_H */\n"}, "10": {"id": 10, "path": "/src/include/linux/spinlock.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_SPINLOCK_H\n#define __LINUX_SPINLOCK_H\n\n/*\n * include/linux/spinlock.h - generic spinlock/rwlock declarations\n *\n * here's the role of the various spinlock/rwlock related include files:\n *\n * on SMP builds:\n *\n *  asm/spinlock_types.h: contains the arch_spinlock_t/arch_rwlock_t and the\n *                        initializers\n *\n *  linux/spinlock_types.h:\n *                        defines the generic type and initializers\n *\n *  asm/spinlock.h:       contains the arch_spin_*()/etc. lowlevel\n *                        implementations, mostly inline assembly code\n *\n *   (also included on UP-debug builds:)\n *\n *  linux/spinlock_api_smp.h:\n *                        contains the prototypes for the _spin_*() APIs.\n *\n *  linux/spinlock.h:     builds the final spin_*() APIs.\n *\n * on UP builds:\n *\n *  linux/spinlock_type_up.h:\n *                        contains the generic, simplified UP spinlock type.\n *                        (which is an empty structure on non-debug builds)\n *\n *  linux/spinlock_types.h:\n *                        defines the generic type and initializers\n *\n *  linux/spinlock_up.h:\n *                        contains the arch_spin_*()/etc. version of UP\n *                        builds. (which are NOPs on non-debug, non-preempt\n *                        builds)\n *\n *   (included on UP-non-debug builds:)\n *\n *  linux/spinlock_api_up.h:\n *                        builds the _spin_*() APIs.\n *\n *  linux/spinlock.h:     builds the final spin_*() APIs.\n */\n\n#include <linux/typecheck.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n#include <linux/compiler.h>\n#include <linux/irqflags.h>\n#include <linux/thread_info.h>\n#include <linux/kernel.h>\n#include <linux/stringify.h>\n#include <linux/bottom_half.h>\n#include <linux/lockdep.h>\n#include <asm/barrier.h>\n#include <asm/mmiowb.h>\n\n\n/*\n * Must define these before including other files, inline functions need them\n */\n#define LOCK_SECTION_NAME \".text..lock.\"KBUILD_BASENAME\n\n#define LOCK_SECTION_START(extra)               \\\n        \".subsection 1\\n\\t\"                     \\\n        extra                                   \\\n        \".ifndef \" LOCK_SECTION_NAME \"\\n\\t\"     \\\n        LOCK_SECTION_NAME \":\\n\\t\"               \\\n        \".endif\\n\"\n\n#define LOCK_SECTION_END                        \\\n        \".previous\\n\\t\"\n\n#define __lockfunc __section(\".spinlock.text\")\n\n/*\n * Pull the arch_spinlock_t and arch_rwlock_t definitions:\n */\n#include <linux/spinlock_types.h>\n\n/*\n * Pull the arch_spin*() functions/declarations (UP-nondebug doesn't need them):\n */\n#ifdef CONFIG_SMP\n# include <asm/spinlock.h>\n#else\n# include <linux/spinlock_up.h>\n#endif\n\n#ifdef CONFIG_DEBUG_SPINLOCK\n  extern void __raw_spin_lock_init(raw_spinlock_t *lock, const char *name,\n\t\t\t\t   struct lock_class_key *key, short inner);\n\n# define raw_spin_lock_init(lock)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tstatic struct lock_class_key __key;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t__raw_spin_lock_init((lock), #lock, &__key, LD_WAIT_SPIN);\t\\\n} while (0)\n\n#else\n# define raw_spin_lock_init(lock)\t\t\t\t\\\n\tdo { *(lock) = __RAW_SPIN_LOCK_UNLOCKED(lock); } while (0)\n#endif\n\n#define raw_spin_is_locked(lock)\tarch_spin_is_locked(&(lock)->raw_lock)\n\n#ifdef arch_spin_is_contended\n#define raw_spin_is_contended(lock)\tarch_spin_is_contended(&(lock)->raw_lock)\n#else\n#define raw_spin_is_contended(lock)\t(((void)(lock), 0))\n#endif /*arch_spin_is_contended*/\n\n/*\n * smp_mb__after_spinlock() provides the equivalent of a full memory barrier\n * between program-order earlier lock acquisitions and program-order later\n * memory accesses.\n *\n * This guarantees that the following two properties hold:\n *\n *   1) Given the snippet:\n *\n *\t  { X = 0;  Y = 0; }\n *\n *\t  CPU0\t\t\t\tCPU1\n *\n *\t  WRITE_ONCE(X, 1);\t\tWRITE_ONCE(Y, 1);\n *\t  spin_lock(S);\t\t\tsmp_mb();\n *\t  smp_mb__after_spinlock();\tr1 = READ_ONCE(X);\n *\t  r0 = READ_ONCE(Y);\n *\t  spin_unlock(S);\n *\n *      it is forbidden that CPU0 does not observe CPU1's store to Y (r0 = 0)\n *      and CPU1 does not observe CPU0's store to X (r1 = 0); see the comments\n *      preceding the call to smp_mb__after_spinlock() in __schedule() and in\n *      try_to_wake_up().\n *\n *   2) Given the snippet:\n *\n *  { X = 0;  Y = 0; }\n *\n *  CPU0\t\tCPU1\t\t\t\tCPU2\n *\n *  spin_lock(S);\tspin_lock(S);\t\t\tr1 = READ_ONCE(Y);\n *  WRITE_ONCE(X, 1);\tsmp_mb__after_spinlock();\tsmp_rmb();\n *  spin_unlock(S);\tr0 = READ_ONCE(X);\t\tr2 = READ_ONCE(X);\n *\t\t\tWRITE_ONCE(Y, 1);\n *\t\t\tspin_unlock(S);\n *\n *      it is forbidden that CPU0's critical section executes before CPU1's\n *      critical section (r0 = 1), CPU2 observes CPU1's store to Y (r1 = 1)\n *      and CPU2 does not observe CPU0's store to X (r2 = 0); see the comments\n *      preceding the calls to smp_rmb() in try_to_wake_up() for similar\n *      snippets but \"projected\" onto two CPUs.\n *\n * Property (2) upgrades the lock to an RCsc lock.\n *\n * Since most load-store architectures implement ACQUIRE with an smp_mb() after\n * the LL/SC loop, they need no further barriers. Similarly all our TSO\n * architectures imply an smp_mb() for each atomic instruction and equally don't\n * need more.\n *\n * Architectures that can implement ACQUIRE better need to take care.\n */\n#ifndef smp_mb__after_spinlock\n#define smp_mb__after_spinlock()\tdo { } while (0)\n#endif\n\n#ifdef CONFIG_DEBUG_SPINLOCK\n extern void do_raw_spin_lock(raw_spinlock_t *lock) __acquires(lock);\n#define do_raw_spin_lock_flags(lock, flags) do_raw_spin_lock(lock)\n extern int do_raw_spin_trylock(raw_spinlock_t *lock);\n extern void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock);\n#else\nstatic inline void do_raw_spin_lock(raw_spinlock_t *lock) __acquires(lock)\n{\n\t__acquire(lock);\n\tarch_spin_lock(&lock->raw_lock);\n\tmmiowb_spin_lock();\n}\n\n#ifndef arch_spin_lock_flags\n#define arch_spin_lock_flags(lock, flags)\tarch_spin_lock(lock)\n#endif\n\nstatic inline void\ndo_raw_spin_lock_flags(raw_spinlock_t *lock, unsigned long *flags) __acquires(lock)\n{\n\t__acquire(lock);\n\tarch_spin_lock_flags(&lock->raw_lock, *flags);\n\tmmiowb_spin_lock();\n}\n\nstatic inline int do_raw_spin_trylock(raw_spinlock_t *lock)\n{\n\tint ret = arch_spin_trylock(&(lock)->raw_lock);\n\n\tif (ret)\n\t\tmmiowb_spin_lock();\n\n\treturn ret;\n}\n\nstatic inline void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock)\n{\n\tmmiowb_spin_unlock();\n\tarch_spin_unlock(&lock->raw_lock);\n\t__release(lock);\n}\n#endif\n\n/*\n * Define the various spin_lock methods.  Note we define these\n * regardless of whether CONFIG_SMP or CONFIG_PREEMPTION are set. The\n * various methods are defined as nops in the case they are not\n * required.\n */\n#define raw_spin_trylock(lock)\t__cond_lock(lock, _raw_spin_trylock(lock))\n\n#define raw_spin_lock(lock)\t_raw_spin_lock(lock)\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n# define raw_spin_lock_nested(lock, subclass) \\\n\t_raw_spin_lock_nested(lock, subclass)\n\n# define raw_spin_lock_nest_lock(lock, nest_lock)\t\t\t\\\n\t do {\t\t\t\t\t\t\t\t\\\n\t\t typecheck(struct lockdep_map *, &(nest_lock)->dep_map);\\\n\t\t _raw_spin_lock_nest_lock(lock, &(nest_lock)->dep_map);\t\\\n\t } while (0)\n#else\n/*\n * Always evaluate the 'subclass' argument to avoid that the compiler\n * warns about set-but-not-used variables when building with\n * CONFIG_DEBUG_LOCK_ALLOC=n and with W=1.\n */\n# define raw_spin_lock_nested(lock, subclass)\t\t\\\n\t_raw_spin_lock(((void)(subclass), (lock)))\n# define raw_spin_lock_nest_lock(lock, nest_lock)\t_raw_spin_lock(lock)\n#endif\n\n#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)\n\n#define raw_spin_lock_irqsave(lock, flags)\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\\\n\t\tflags = _raw_spin_lock_irqsave(lock);\t\\\n\t} while (0)\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n#define raw_spin_lock_irqsave_nested(lock, flags, subclass)\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\t\t\\\n\t\tflags = _raw_spin_lock_irqsave_nested(lock, subclass);\t\\\n\t} while (0)\n#else\n#define raw_spin_lock_irqsave_nested(lock, flags, subclass)\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\t\t\\\n\t\tflags = _raw_spin_lock_irqsave(lock);\t\t\t\\\n\t} while (0)\n#endif\n\n#else\n\n#define raw_spin_lock_irqsave(lock, flags)\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\\\n\t\t_raw_spin_lock_irqsave(lock, flags);\t\\\n\t} while (0)\n\n#define raw_spin_lock_irqsave_nested(lock, flags, subclass)\t\\\n\traw_spin_lock_irqsave(lock, flags)\n\n#endif\n\n#define raw_spin_lock_irq(lock)\t\t_raw_spin_lock_irq(lock)\n#define raw_spin_lock_bh(lock)\t\t_raw_spin_lock_bh(lock)\n#define raw_spin_unlock(lock)\t\t_raw_spin_unlock(lock)\n#define raw_spin_unlock_irq(lock)\t_raw_spin_unlock_irq(lock)\n\n#define raw_spin_unlock_irqrestore(lock, flags)\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\t\\\n\t\t_raw_spin_unlock_irqrestore(lock, flags);\t\\\n\t} while (0)\n#define raw_spin_unlock_bh(lock)\t_raw_spin_unlock_bh(lock)\n\n#define raw_spin_trylock_bh(lock) \\\n\t__cond_lock(lock, _raw_spin_trylock_bh(lock))\n\n#define raw_spin_trylock_irq(lock) \\\n({ \\\n\tlocal_irq_disable(); \\\n\traw_spin_trylock(lock) ? \\\n\t1 : ({ local_irq_enable(); 0;  }); \\\n})\n\n#define raw_spin_trylock_irqsave(lock, flags) \\\n({ \\\n\tlocal_irq_save(flags); \\\n\traw_spin_trylock(lock) ? \\\n\t1 : ({ local_irq_restore(flags); 0; }); \\\n})\n\n/* Include rwlock functions */\n#include <linux/rwlock.h>\n\n/*\n * Pull the _spin_*()/_read_*()/_write_*() functions/declarations:\n */\n#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)\n# include <linux/spinlock_api_smp.h>\n#else\n# include <linux/spinlock_api_up.h>\n#endif\n\n/*\n * Map the spin_lock functions to the raw variants for PREEMPT_RT=n\n */\n\nstatic __always_inline raw_spinlock_t *spinlock_check(spinlock_t *lock)\n{\n\treturn &lock->rlock;\n}\n\n#ifdef CONFIG_DEBUG_SPINLOCK\n\n# define spin_lock_init(lock)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tstatic struct lock_class_key __key;\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\t__raw_spin_lock_init(spinlock_check(lock),\t\t\\\n\t\t\t     #lock, &__key, LD_WAIT_CONFIG);\t\\\n} while (0)\n\n#else\n\n# define spin_lock_init(_lock)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tspinlock_check(_lock);\t\t\t\\\n\t*(_lock) = __SPIN_LOCK_UNLOCKED(_lock);\t\\\n} while (0)\n\n#endif\n\nstatic __always_inline void spin_lock(spinlock_t *lock)\n{\n\traw_spin_lock(&lock->rlock);\n}\n\nstatic __always_inline void spin_lock_bh(spinlock_t *lock)\n{\n\traw_spin_lock_bh(&lock->rlock);\n}\n\nstatic __always_inline int spin_trylock(spinlock_t *lock)\n{\n\treturn raw_spin_trylock(&lock->rlock);\n}\n\n#define spin_lock_nested(lock, subclass)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_nested(spinlock_check(lock), subclass);\t\\\n} while (0)\n\n#define spin_lock_nest_lock(lock, nest_lock)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_nest_lock(spinlock_check(lock), nest_lock);\t\\\n} while (0)\n\nstatic __always_inline void spin_lock_irq(spinlock_t *lock)\n{\n\traw_spin_lock_irq(&lock->rlock);\n}\n\n#define spin_lock_irqsave(lock, flags)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_irqsave(spinlock_check(lock), flags);\t\\\n} while (0)\n\n#define spin_lock_irqsave_nested(lock, flags, subclass)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \\\n} while (0)\n\nstatic __always_inline void spin_unlock(spinlock_t *lock)\n{\n\traw_spin_unlock(&lock->rlock);\n}\n\nstatic __always_inline void spin_unlock_bh(spinlock_t *lock)\n{\n\traw_spin_unlock_bh(&lock->rlock);\n}\n\nstatic __always_inline void spin_unlock_irq(spinlock_t *lock)\n{\n\traw_spin_unlock_irq(&lock->rlock);\n}\n\nstatic __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)\n{\n\traw_spin_unlock_irqrestore(&lock->rlock, flags);\n}\n\nstatic __always_inline int spin_trylock_bh(spinlock_t *lock)\n{\n\treturn raw_spin_trylock_bh(&lock->rlock);\n}\n\nstatic __always_inline int spin_trylock_irq(spinlock_t *lock)\n{\n\treturn raw_spin_trylock_irq(&lock->rlock);\n}\n\n#define spin_trylock_irqsave(lock, flags)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\traw_spin_trylock_irqsave(spinlock_check(lock), flags); \\\n})\n\n/**\n * spin_is_locked() - Check whether a spinlock is locked.\n * @lock: Pointer to the spinlock.\n *\n * This function is NOT required to provide any memory ordering\n * guarantees; it could be used for debugging purposes or, when\n * additional synchronization is needed, accompanied with other\n * constructs (memory barriers) enforcing the synchronization.\n *\n * Returns: 1 if @lock is locked, 0 otherwise.\n *\n * Note that the function only tells you that the spinlock is\n * seen to be locked, not that it is locked on your CPU.\n *\n * Further, on CONFIG_SMP=n builds with CONFIG_DEBUG_SPINLOCK=n,\n * the return value is always 0 (see include/linux/spinlock_up.h).\n * Therefore you should not rely heavily on the return value.\n */\nstatic __always_inline int spin_is_locked(spinlock_t *lock)\n{\n\treturn raw_spin_is_locked(&lock->rlock);\n}\n\nstatic __always_inline int spin_is_contended(spinlock_t *lock)\n{\n\treturn raw_spin_is_contended(&lock->rlock);\n}\n\n#define assert_spin_locked(lock)\tassert_raw_spin_locked(&(lock)->rlock)\n\n/*\n * Pull the atomic_t declaration:\n * (asm-mips/atomic.h needs above definitions)\n */\n#include <linux/atomic.h>\n/**\n * atomic_dec_and_lock - lock on reaching reference count zero\n * @atomic: the atomic counter\n * @lock: the spinlock in question\n *\n * Decrements @atomic by 1.  If the result is 0, returns true and locks\n * @lock.  Returns false for all other cases.\n */\nextern int _atomic_dec_and_lock(atomic_t *atomic, spinlock_t *lock);\n#define atomic_dec_and_lock(atomic, lock) \\\n\t\t__cond_lock(lock, _atomic_dec_and_lock(atomic, lock))\n\nextern int _atomic_dec_and_lock_irqsave(atomic_t *atomic, spinlock_t *lock,\n\t\t\t\t\tunsigned long *flags);\n#define atomic_dec_and_lock_irqsave(atomic, lock, flags) \\\n\t\t__cond_lock(lock, _atomic_dec_and_lock_irqsave(atomic, lock, &(flags)))\n\nint __alloc_bucket_spinlocks(spinlock_t **locks, unsigned int *lock_mask,\n\t\t\t     size_t max_size, unsigned int cpu_mult,\n\t\t\t     gfp_t gfp, const char *name,\n\t\t\t     struct lock_class_key *key);\n\n#define alloc_bucket_spinlocks(locks, lock_mask, max_size, cpu_mult, gfp)    \\\n\t({\t\t\t\t\t\t\t\t     \\\n\t\tstatic struct lock_class_key key;\t\t\t     \\\n\t\tint ret;\t\t\t\t\t\t     \\\n\t\t\t\t\t\t\t\t\t     \\\n\t\tret = __alloc_bucket_spinlocks(locks, lock_mask, max_size,   \\\n\t\t\t\t\t       cpu_mult, gfp, #locks, &key); \\\n\t\tret;\t\t\t\t\t\t\t     \\\n\t})\n\nvoid free_bucket_spinlocks(spinlock_t *locks);\n\n#endif /* __LINUX_SPINLOCK_H */\n"}, "11": {"id": 11, "path": "/src/mm/hugetlb.c", "content": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n * Generic hugetlb support.\n * (C) Nadia Yvette Chambers, April 2004\n */\n#include <linux/list.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/seq_file.h>\n#include <linux/sysctl.h>\n#include <linux/highmem.h>\n#include <linux/mmu_notifier.h>\n#include <linux/nodemask.h>\n#include <linux/pagemap.h>\n#include <linux/mempolicy.h>\n#include <linux/compiler.h>\n#include <linux/cpuset.h>\n#include <linux/mutex.h>\n#include <linux/memblock.h>\n#include <linux/sysfs.h>\n#include <linux/slab.h>\n#include <linux/sched/mm.h>\n#include <linux/mmdebug.h>\n#include <linux/sched/signal.h>\n#include <linux/rmap.h>\n#include <linux/string_helpers.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/jhash.h>\n#include <linux/numa.h>\n#include <linux/llist.h>\n#include <linux/cma.h>\n\n#include <asm/page.h>\n#include <asm/pgalloc.h>\n#include <asm/tlb.h>\n\n#include <linux/io.h>\n#include <linux/hugetlb.h>\n#include <linux/hugetlb_cgroup.h>\n#include <linux/node.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/page_owner.h>\n#include \"internal.h\"\n\nint hugetlb_max_hstate __read_mostly;\nunsigned int default_hstate_idx;\nstruct hstate hstates[HUGE_MAX_HSTATE];\n\n#ifdef CONFIG_CMA\nstatic struct cma *hugetlb_cma[MAX_NUMNODES];\n#endif\nstatic unsigned long hugetlb_cma_size __initdata;\n\n/*\n * Minimum page order among possible hugepage sizes, set to a proper value\n * at boot time.\n */\nstatic unsigned int minimum_order __read_mostly = UINT_MAX;\n\n__initdata LIST_HEAD(huge_boot_pages);\n\n/* for command line parsing */\nstatic struct hstate * __initdata parsed_hstate;\nstatic unsigned long __initdata default_hstate_max_huge_pages;\nstatic bool __initdata parsed_valid_hugepagesz = true;\nstatic bool __initdata parsed_default_hugepagesz;\n\n/*\n * Protects updates to hugepage_freelists, hugepage_activelist, nr_huge_pages,\n * free_huge_pages, and surplus_huge_pages.\n */\nDEFINE_SPINLOCK(hugetlb_lock);\n\n/*\n * Serializes faults on the same logical page.  This is used to\n * prevent spurious OOMs when the hugepage pool is fully utilized.\n */\nstatic int num_fault_mutexes;\nstruct mutex *hugetlb_fault_mutex_table ____cacheline_aligned_in_smp;\n\n/* Forward declaration */\nstatic int hugetlb_acct_memory(struct hstate *h, long delta);\n\nstatic inline void unlock_or_release_subpool(struct hugepage_subpool *spool)\n{\n\tbool free = (spool->count == 0) && (spool->used_hpages == 0);\n\n\tspin_unlock(&spool->lock);\n\n\t/* If no pages are used, and no other handles to the subpool\n\t * remain, give up any reservations based on minimum size and\n\t * free the subpool */\n\tif (free) {\n\t\tif (spool->min_hpages != -1)\n\t\t\thugetlb_acct_memory(spool->hstate,\n\t\t\t\t\t\t-spool->min_hpages);\n\t\tkfree(spool);\n\t}\n}\n\nstruct hugepage_subpool *hugepage_new_subpool(struct hstate *h, long max_hpages,\n\t\t\t\t\t\tlong min_hpages)\n{\n\tstruct hugepage_subpool *spool;\n\n\tspool = kzalloc(sizeof(*spool), GFP_KERNEL);\n\tif (!spool)\n\t\treturn NULL;\n\n\tspin_lock_init(&spool->lock);\n\tspool->count = 1;\n\tspool->max_hpages = max_hpages;\n\tspool->hstate = h;\n\tspool->min_hpages = min_hpages;\n\n\tif (min_hpages != -1 && hugetlb_acct_memory(h, min_hpages)) {\n\t\tkfree(spool);\n\t\treturn NULL;\n\t}\n\tspool->rsv_hpages = min_hpages;\n\n\treturn spool;\n}\n\nvoid hugepage_put_subpool(struct hugepage_subpool *spool)\n{\n\tspin_lock(&spool->lock);\n\tBUG_ON(!spool->count);\n\tspool->count--;\n\tunlock_or_release_subpool(spool);\n}\n\n/*\n * Subpool accounting for allocating and reserving pages.\n * Return -ENOMEM if there are not enough resources to satisfy the\n * request.  Otherwise, return the number of pages by which the\n * global pools must be adjusted (upward).  The returned value may\n * only be different than the passed value (delta) in the case where\n * a subpool minimum size must be maintained.\n */\nstatic long hugepage_subpool_get_pages(struct hugepage_subpool *spool,\n\t\t\t\t      long delta)\n{\n\tlong ret = delta;\n\n\tif (!spool)\n\t\treturn ret;\n\n\tspin_lock(&spool->lock);\n\n\tif (spool->max_hpages != -1) {\t\t/* maximum size accounting */\n\t\tif ((spool->used_hpages + delta) <= spool->max_hpages)\n\t\t\tspool->used_hpages += delta;\n\t\telse {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto unlock_ret;\n\t\t}\n\t}\n\n\t/* minimum size accounting */\n\tif (spool->min_hpages != -1 && spool->rsv_hpages) {\n\t\tif (delta > spool->rsv_hpages) {\n\t\t\t/*\n\t\t\t * Asking for more reserves than those already taken on\n\t\t\t * behalf of subpool.  Return difference.\n\t\t\t */\n\t\t\tret = delta - spool->rsv_hpages;\n\t\t\tspool->rsv_hpages = 0;\n\t\t} else {\n\t\t\tret = 0;\t/* reserves already accounted for */\n\t\t\tspool->rsv_hpages -= delta;\n\t\t}\n\t}\n\nunlock_ret:\n\tspin_unlock(&spool->lock);\n\treturn ret;\n}\n\n/*\n * Subpool accounting for freeing and unreserving pages.\n * Return the number of global page reservations that must be dropped.\n * The return value may only be different than the passed value (delta)\n * in the case where a subpool minimum size must be maintained.\n */\nstatic long hugepage_subpool_put_pages(struct hugepage_subpool *spool,\n\t\t\t\t       long delta)\n{\n\tlong ret = delta;\n\n\tif (!spool)\n\t\treturn delta;\n\n\tspin_lock(&spool->lock);\n\n\tif (spool->max_hpages != -1)\t\t/* maximum size accounting */\n\t\tspool->used_hpages -= delta;\n\n\t /* minimum size accounting */\n\tif (spool->min_hpages != -1 && spool->used_hpages < spool->min_hpages) {\n\t\tif (spool->rsv_hpages + delta <= spool->min_hpages)\n\t\t\tret = 0;\n\t\telse\n\t\t\tret = spool->rsv_hpages + delta - spool->min_hpages;\n\n\t\tspool->rsv_hpages += delta;\n\t\tif (spool->rsv_hpages > spool->min_hpages)\n\t\t\tspool->rsv_hpages = spool->min_hpages;\n\t}\n\n\t/*\n\t * If hugetlbfs_put_super couldn't free spool due to an outstanding\n\t * quota reference, free it now.\n\t */\n\tunlock_or_release_subpool(spool);\n\n\treturn ret;\n}\n\nstatic inline struct hugepage_subpool *subpool_inode(struct inode *inode)\n{\n\treturn HUGETLBFS_SB(inode->i_sb)->spool;\n}\n\nstatic inline struct hugepage_subpool *subpool_vma(struct vm_area_struct *vma)\n{\n\treturn subpool_inode(file_inode(vma->vm_file));\n}\n\n/* Helper that removes a struct file_region from the resv_map cache and returns\n * it for use.\n */\nstatic struct file_region *\nget_file_region_entry_from_cache(struct resv_map *resv, long from, long to)\n{\n\tstruct file_region *nrg = NULL;\n\n\tVM_BUG_ON(resv->region_cache_count <= 0);\n\n\tresv->region_cache_count--;\n\tnrg = list_first_entry(&resv->region_cache, struct file_region, link);\n\tlist_del(&nrg->link);\n\n\tnrg->from = from;\n\tnrg->to = to;\n\n\treturn nrg;\n}\n\nstatic void copy_hugetlb_cgroup_uncharge_info(struct file_region *nrg,\n\t\t\t\t\t      struct file_region *rg)\n{\n#ifdef CONFIG_CGROUP_HUGETLB\n\tnrg->reservation_counter = rg->reservation_counter;\n\tnrg->css = rg->css;\n\tif (rg->css)\n\t\tcss_get(rg->css);\n#endif\n}\n\n/* Helper that records hugetlb_cgroup uncharge info. */\nstatic void record_hugetlb_cgroup_uncharge_info(struct hugetlb_cgroup *h_cg,\n\t\t\t\t\t\tstruct hstate *h,\n\t\t\t\t\t\tstruct resv_map *resv,\n\t\t\t\t\t\tstruct file_region *nrg)\n{\n#ifdef CONFIG_CGROUP_HUGETLB\n\tif (h_cg) {\n\t\tnrg->reservation_counter =\n\t\t\t&h_cg->rsvd_hugepage[hstate_index(h)];\n\t\tnrg->css = &h_cg->css;\n\t\tif (!resv->pages_per_hpage)\n\t\t\tresv->pages_per_hpage = pages_per_huge_page(h);\n\t\t/* pages_per_hpage should be the same for all entries in\n\t\t * a resv_map.\n\t\t */\n\t\tVM_BUG_ON(resv->pages_per_hpage != pages_per_huge_page(h));\n\t} else {\n\t\tnrg->reservation_counter = NULL;\n\t\tnrg->css = NULL;\n\t}\n#endif\n}\n\nstatic bool has_same_uncharge_info(struct file_region *rg,\n\t\t\t\t   struct file_region *org)\n{\n#ifdef CONFIG_CGROUP_HUGETLB\n\treturn rg && org &&\n\t       rg->reservation_counter == org->reservation_counter &&\n\t       rg->css == org->css;\n\n#else\n\treturn true;\n#endif\n}\n\nstatic void coalesce_file_region(struct resv_map *resv, struct file_region *rg)\n{\n\tstruct file_region *nrg = NULL, *prg = NULL;\n\n\tprg = list_prev_entry(rg, link);\n\tif (&prg->link != &resv->regions && prg->to == rg->from &&\n\t    has_same_uncharge_info(prg, rg)) {\n\t\tprg->to = rg->to;\n\n\t\tlist_del(&rg->link);\n\t\tkfree(rg);\n\n\t\trg = prg;\n\t}\n\n\tnrg = list_next_entry(rg, link);\n\tif (&nrg->link != &resv->regions && nrg->from == rg->to &&\n\t    has_same_uncharge_info(nrg, rg)) {\n\t\tnrg->from = rg->from;\n\n\t\tlist_del(&rg->link);\n\t\tkfree(rg);\n\t}\n}\n\n/*\n * Must be called with resv->lock held.\n *\n * Calling this with regions_needed != NULL will count the number of pages\n * to be added but will not modify the linked list. And regions_needed will\n * indicate the number of file_regions needed in the cache to carry out to add\n * the regions for this range.\n */\nstatic long add_reservation_in_range(struct resv_map *resv, long f, long t,\n\t\t\t\t     struct hugetlb_cgroup *h_cg,\n\t\t\t\t     struct hstate *h, long *regions_needed)\n{\n\tlong add = 0;\n\tstruct list_head *head = &resv->regions;\n\tlong last_accounted_offset = f;\n\tstruct file_region *rg = NULL, *trg = NULL, *nrg = NULL;\n\n\tif (regions_needed)\n\t\t*regions_needed = 0;\n\n\t/* In this loop, we essentially handle an entry for the range\n\t * [last_accounted_offset, rg->from), at every iteration, with some\n\t * bounds checking.\n\t */\n\tlist_for_each_entry_safe(rg, trg, head, link) {\n\t\t/* Skip irrelevant regions that start before our range. */\n\t\tif (rg->from < f) {\n\t\t\t/* If this region ends after the last accounted offset,\n\t\t\t * then we need to update last_accounted_offset.\n\t\t\t */\n\t\t\tif (rg->to > last_accounted_offset)\n\t\t\t\tlast_accounted_offset = rg->to;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* When we find a region that starts beyond our range, we've\n\t\t * finished.\n\t\t */\n\t\tif (rg->from > t)\n\t\t\tbreak;\n\n\t\t/* Add an entry for last_accounted_offset -> rg->from, and\n\t\t * update last_accounted_offset.\n\t\t */\n\t\tif (rg->from > last_accounted_offset) {\n\t\t\tadd += rg->from - last_accounted_offset;\n\t\t\tif (!regions_needed) {\n\t\t\t\tnrg = get_file_region_entry_from_cache(\n\t\t\t\t\tresv, last_accounted_offset, rg->from);\n\t\t\t\trecord_hugetlb_cgroup_uncharge_info(h_cg, h,\n\t\t\t\t\t\t\t\t    resv, nrg);\n\t\t\t\tlist_add(&nrg->link, rg->link.prev);\n\t\t\t\tcoalesce_file_region(resv, nrg);\n\t\t\t} else\n\t\t\t\t*regions_needed += 1;\n\t\t}\n\n\t\tlast_accounted_offset = rg->to;\n\t}\n\n\t/* Handle the case where our range extends beyond\n\t * last_accounted_offset.\n\t */\n\tif (last_accounted_offset < t) {\n\t\tadd += t - last_accounted_offset;\n\t\tif (!regions_needed) {\n\t\t\tnrg = get_file_region_entry_from_cache(\n\t\t\t\tresv, last_accounted_offset, t);\n\t\t\trecord_hugetlb_cgroup_uncharge_info(h_cg, h, resv, nrg);\n\t\t\tlist_add(&nrg->link, rg->link.prev);\n\t\t\tcoalesce_file_region(resv, nrg);\n\t\t} else\n\t\t\t*regions_needed += 1;\n\t}\n\n\tVM_BUG_ON(add < 0);\n\treturn add;\n}\n\n/* Must be called with resv->lock acquired. Will drop lock to allocate entries.\n */\nstatic int allocate_file_region_entries(struct resv_map *resv,\n\t\t\t\t\tint regions_needed)\n\t__must_hold(&resv->lock)\n{\n\tstruct list_head allocated_regions;\n\tint to_allocate = 0, i = 0;\n\tstruct file_region *trg = NULL, *rg = NULL;\n\n\tVM_BUG_ON(regions_needed < 0);\n\n\tINIT_LIST_HEAD(&allocated_regions);\n\n\t/*\n\t * Check for sufficient descriptors in the cache to accommodate\n\t * the number of in progress add operations plus regions_needed.\n\t *\n\t * This is a while loop because when we drop the lock, some other call\n\t * to region_add or region_del may have consumed some region_entries,\n\t * so we keep looping here until we finally have enough entries for\n\t * (adds_in_progress + regions_needed).\n\t */\n\twhile (resv->region_cache_count <\n\t       (resv->adds_in_progress + regions_needed)) {\n\t\tto_allocate = resv->adds_in_progress + regions_needed -\n\t\t\t      resv->region_cache_count;\n\n\t\t/* At this point, we should have enough entries in the cache\n\t\t * for all the existings adds_in_progress. We should only be\n\t\t * needing to allocate for regions_needed.\n\t\t */\n\t\tVM_BUG_ON(resv->region_cache_count < resv->adds_in_progress);\n\n\t\tspin_unlock(&resv->lock);\n\t\tfor (i = 0; i < to_allocate; i++) {\n\t\t\ttrg = kmalloc(sizeof(*trg), GFP_KERNEL);\n\t\t\tif (!trg)\n\t\t\t\tgoto out_of_memory;\n\t\t\tlist_add(&trg->link, &allocated_regions);\n\t\t}\n\n\t\tspin_lock(&resv->lock);\n\n\t\tlist_splice(&allocated_regions, &resv->region_cache);\n\t\tresv->region_cache_count += to_allocate;\n\t}\n\n\treturn 0;\n\nout_of_memory:\n\tlist_for_each_entry_safe(rg, trg, &allocated_regions, link) {\n\t\tlist_del(&rg->link);\n\t\tkfree(rg);\n\t}\n\treturn -ENOMEM;\n}\n\n/*\n * Add the huge page range represented by [f, t) to the reserve\n * map.  Regions will be taken from the cache to fill in this range.\n * Sufficient regions should exist in the cache due to the previous\n * call to region_chg with the same range, but in some cases the cache will not\n * have sufficient entries due to races with other code doing region_add or\n * region_del.  The extra needed entries will be allocated.\n *\n * regions_needed is the out value provided by a previous call to region_chg.\n *\n * Return the number of new huge pages added to the map.  This number is greater\n * than or equal to zero.  If file_region entries needed to be allocated for\n * this operation and we were not able to allocate, it returns -ENOMEM.\n * region_add of regions of length 1 never allocate file_regions and cannot\n * fail; region_chg will always allocate at least 1 entry and a region_add for\n * 1 page will only require at most 1 entry.\n */\nstatic long region_add(struct resv_map *resv, long f, long t,\n\t\t       long in_regions_needed, struct hstate *h,\n\t\t       struct hugetlb_cgroup *h_cg)\n{\n\tlong add = 0, actual_regions_needed = 0;\n\n\tspin_lock(&resv->lock);\nretry:\n\n\t/* Count how many regions are actually needed to execute this add. */\n\tadd_reservation_in_range(resv, f, t, NULL, NULL,\n\t\t\t\t &actual_regions_needed);\n\n\t/*\n\t * Check for sufficient descriptors in the cache to accommodate\n\t * this add operation. Note that actual_regions_needed may be greater\n\t * than in_regions_needed, as the resv_map may have been modified since\n\t * the region_chg call. In this case, we need to make sure that we\n\t * allocate extra entries, such that we have enough for all the\n\t * existing adds_in_progress, plus the excess needed for this\n\t * operation.\n\t */\n\tif (actual_regions_needed > in_regions_needed &&\n\t    resv->region_cache_count <\n\t\t    resv->adds_in_progress +\n\t\t\t    (actual_regions_needed - in_regions_needed)) {\n\t\t/* region_add operation of range 1 should never need to\n\t\t * allocate file_region entries.\n\t\t */\n\t\tVM_BUG_ON(t - f <= 1);\n\n\t\tif (allocate_file_region_entries(\n\t\t\t    resv, actual_regions_needed - in_regions_needed)) {\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tgoto retry;\n\t}\n\n\tadd = add_reservation_in_range(resv, f, t, h_cg, h, NULL);\n\n\tresv->adds_in_progress -= in_regions_needed;\n\n\tspin_unlock(&resv->lock);\n\tVM_BUG_ON(add < 0);\n\treturn add;\n}\n\n/*\n * Examine the existing reserve map and determine how many\n * huge pages in the specified range [f, t) are NOT currently\n * represented.  This routine is called before a subsequent\n * call to region_add that will actually modify the reserve\n * map to add the specified range [f, t).  region_chg does\n * not change the number of huge pages represented by the\n * map.  A number of new file_region structures is added to the cache as a\n * placeholder, for the subsequent region_add call to use. At least 1\n * file_region structure is added.\n *\n * out_regions_needed is the number of regions added to the\n * resv->adds_in_progress.  This value needs to be provided to a follow up call\n * to region_add or region_abort for proper accounting.\n *\n * Returns the number of huge pages that need to be added to the existing\n * reservation map for the range [f, t).  This number is greater or equal to\n * zero.  -ENOMEM is returned if a new file_region structure or cache entry\n * is needed and can not be allocated.\n */\nstatic long region_chg(struct resv_map *resv, long f, long t,\n\t\t       long *out_regions_needed)\n{\n\tlong chg = 0;\n\n\tspin_lock(&resv->lock);\n\n\t/* Count how many hugepages in this range are NOT represented. */\n\tchg = add_reservation_in_range(resv, f, t, NULL, NULL,\n\t\t\t\t       out_regions_needed);\n\n\tif (*out_regions_needed == 0)\n\t\t*out_regions_needed = 1;\n\n\tif (allocate_file_region_entries(resv, *out_regions_needed))\n\t\treturn -ENOMEM;\n\n\tresv->adds_in_progress += *out_regions_needed;\n\n\tspin_unlock(&resv->lock);\n\treturn chg;\n}\n\n/*\n * Abort the in progress add operation.  The adds_in_progress field\n * of the resv_map keeps track of the operations in progress between\n * calls to region_chg and region_add.  Operations are sometimes\n * aborted after the call to region_chg.  In such cases, region_abort\n * is called to decrement the adds_in_progress counter. regions_needed\n * is the value returned by the region_chg call, it is used to decrement\n * the adds_in_progress counter.\n *\n * NOTE: The range arguments [f, t) are not needed or used in this\n * routine.  They are kept to make reading the calling code easier as\n * arguments will match the associated region_chg call.\n */\nstatic void region_abort(struct resv_map *resv, long f, long t,\n\t\t\t long regions_needed)\n{\n\tspin_lock(&resv->lock);\n\tVM_BUG_ON(!resv->region_cache_count);\n\tresv->adds_in_progress -= regions_needed;\n\tspin_unlock(&resv->lock);\n}\n\n/*\n * Delete the specified range [f, t) from the reserve map.  If the\n * t parameter is LONG_MAX, this indicates that ALL regions after f\n * should be deleted.  Locate the regions which intersect [f, t)\n * and either trim, delete or split the existing regions.\n *\n * Returns the number of huge pages deleted from the reserve map.\n * In the normal case, the return value is zero or more.  In the\n * case where a region must be split, a new region descriptor must\n * be allocated.  If the allocation fails, -ENOMEM will be returned.\n * NOTE: If the parameter t == LONG_MAX, then we will never split\n * a region and possibly return -ENOMEM.  Callers specifying\n * t == LONG_MAX do not need to check for -ENOMEM error.\n */\nstatic long region_del(struct resv_map *resv, long f, long t)\n{\n\tstruct list_head *head = &resv->regions;\n\tstruct file_region *rg, *trg;\n\tstruct file_region *nrg = NULL;\n\tlong del = 0;\n\nretry:\n\tspin_lock(&resv->lock);\n\tlist_for_each_entry_safe(rg, trg, head, link) {\n\t\t/*\n\t\t * Skip regions before the range to be deleted.  file_region\n\t\t * ranges are normally of the form [from, to).  However, there\n\t\t * may be a \"placeholder\" entry in the map which is of the form\n\t\t * (from, to) with from == to.  Check for placeholder entries\n\t\t * at the beginning of the range to be deleted.\n\t\t */\n\t\tif (rg->to <= f && (rg->to != rg->from || rg->to != f))\n\t\t\tcontinue;\n\n\t\tif (rg->from >= t)\n\t\t\tbreak;\n\n\t\tif (f > rg->from && t < rg->to) { /* Must split region */\n\t\t\t/*\n\t\t\t * Check for an entry in the cache before dropping\n\t\t\t * lock and attempting allocation.\n\t\t\t */\n\t\t\tif (!nrg &&\n\t\t\t    resv->region_cache_count > resv->adds_in_progress) {\n\t\t\t\tnrg = list_first_entry(&resv->region_cache,\n\t\t\t\t\t\t\tstruct file_region,\n\t\t\t\t\t\t\tlink);\n\t\t\t\tlist_del(&nrg->link);\n\t\t\t\tresv->region_cache_count--;\n\t\t\t}\n\n\t\t\tif (!nrg) {\n\t\t\t\tspin_unlock(&resv->lock);\n\t\t\t\tnrg = kmalloc(sizeof(*nrg), GFP_KERNEL);\n\t\t\t\tif (!nrg)\n\t\t\t\t\treturn -ENOMEM;\n\t\t\t\tgoto retry;\n\t\t\t}\n\n\t\t\tdel += t - f;\n\t\t\thugetlb_cgroup_uncharge_file_region(\n\t\t\t\tresv, rg, t - f);\n\n\t\t\t/* New entry for end of split region */\n\t\t\tnrg->from = t;\n\t\t\tnrg->to = rg->to;\n\n\t\t\tcopy_hugetlb_cgroup_uncharge_info(nrg, rg);\n\n\t\t\tINIT_LIST_HEAD(&nrg->link);\n\n\t\t\t/* Original entry is trimmed */\n\t\t\trg->to = f;\n\n\t\t\tlist_add(&nrg->link, &rg->link);\n\t\t\tnrg = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (f <= rg->from && t >= rg->to) { /* Remove entire region */\n\t\t\tdel += rg->to - rg->from;\n\t\t\thugetlb_cgroup_uncharge_file_region(resv, rg,\n\t\t\t\t\t\t\t    rg->to - rg->from);\n\t\t\tlist_del(&rg->link);\n\t\t\tkfree(rg);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (f <= rg->from) {\t/* Trim beginning of region */\n\t\t\thugetlb_cgroup_uncharge_file_region(resv, rg,\n\t\t\t\t\t\t\t    t - rg->from);\n\n\t\t\tdel += t - rg->from;\n\t\t\trg->from = t;\n\t\t} else {\t\t/* Trim end of region */\n\t\t\thugetlb_cgroup_uncharge_file_region(resv, rg,\n\t\t\t\t\t\t\t    rg->to - f);\n\n\t\t\tdel += rg->to - f;\n\t\t\trg->to = f;\n\t\t}\n\t}\n\n\tspin_unlock(&resv->lock);\n\tkfree(nrg);\n\treturn del;\n}\n\n/*\n * A rare out of memory error was encountered which prevented removal of\n * the reserve map region for a page.  The huge page itself was free'ed\n * and removed from the page cache.  This routine will adjust the subpool\n * usage count, and the global reserve count if needed.  By incrementing\n * these counts, the reserve map entry which could not be deleted will\n * appear as a \"reserved\" entry instead of simply dangling with incorrect\n * counts.\n */\nvoid hugetlb_fix_reserve_counts(struct inode *inode)\n{\n\tstruct hugepage_subpool *spool = subpool_inode(inode);\n\tlong rsv_adjust;\n\n\trsv_adjust = hugepage_subpool_get_pages(spool, 1);\n\tif (rsv_adjust) {\n\t\tstruct hstate *h = hstate_inode(inode);\n\n\t\thugetlb_acct_memory(h, 1);\n\t}\n}\n\n/*\n * Count and return the number of huge pages in the reserve map\n * that intersect with the range [f, t).\n */\nstatic long region_count(struct resv_map *resv, long f, long t)\n{\n\tstruct list_head *head = &resv->regions;\n\tstruct file_region *rg;\n\tlong chg = 0;\n\n\tspin_lock(&resv->lock);\n\t/* Locate each segment we overlap with, and count that overlap. */\n\tlist_for_each_entry(rg, head, link) {\n\t\tlong seg_from;\n\t\tlong seg_to;\n\n\t\tif (rg->to <= f)\n\t\t\tcontinue;\n\t\tif (rg->from >= t)\n\t\t\tbreak;\n\n\t\tseg_from = max(rg->from, f);\n\t\tseg_to = min(rg->to, t);\n\n\t\tchg += seg_to - seg_from;\n\t}\n\tspin_unlock(&resv->lock);\n\n\treturn chg;\n}\n\n/*\n * Convert the address within this vma to the page offset within\n * the mapping, in pagecache page units; huge pages here.\n */\nstatic pgoff_t vma_hugecache_offset(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\treturn ((address - vma->vm_start) >> huge_page_shift(h)) +\n\t\t\t(vma->vm_pgoff >> huge_page_order(h));\n}\n\npgoff_t linear_hugepage_index(struct vm_area_struct *vma,\n\t\t\t\t     unsigned long address)\n{\n\treturn vma_hugecache_offset(hstate_vma(vma), vma, address);\n}\nEXPORT_SYMBOL_GPL(linear_hugepage_index);\n\n/*\n * Return the size of the pages allocated when backing a VMA. In the majority\n * cases this will be same size as used by the page table entries.\n */\nunsigned long vma_kernel_pagesize(struct vm_area_struct *vma)\n{\n\tif (vma->vm_ops && vma->vm_ops->pagesize)\n\t\treturn vma->vm_ops->pagesize(vma);\n\treturn PAGE_SIZE;\n}\nEXPORT_SYMBOL_GPL(vma_kernel_pagesize);\n\n/*\n * Return the page size being used by the MMU to back a VMA. In the majority\n * of cases, the page size used by the kernel matches the MMU size. On\n * architectures where it differs, an architecture-specific 'strong'\n * version of this symbol is required.\n */\n__weak unsigned long vma_mmu_pagesize(struct vm_area_struct *vma)\n{\n\treturn vma_kernel_pagesize(vma);\n}\n\n/*\n * Flags for MAP_PRIVATE reservations.  These are stored in the bottom\n * bits of the reservation map pointer, which are always clear due to\n * alignment.\n */\n#define HPAGE_RESV_OWNER    (1UL << 0)\n#define HPAGE_RESV_UNMAPPED (1UL << 1)\n#define HPAGE_RESV_MASK (HPAGE_RESV_OWNER | HPAGE_RESV_UNMAPPED)\n\n/*\n * These helpers are used to track how many pages are reserved for\n * faults in a MAP_PRIVATE mapping. Only the process that called mmap()\n * is guaranteed to have their future faults succeed.\n *\n * With the exception of reset_vma_resv_huge_pages() which is called at fork(),\n * the reserve counters are updated with the hugetlb_lock held. It is safe\n * to reset the VMA at fork() time as it is not in use yet and there is no\n * chance of the global counters getting corrupted as a result of the values.\n *\n * The private mapping reservation is represented in a subtly different\n * manner to a shared mapping.  A shared mapping has a region map associated\n * with the underlying file, this region map represents the backing file\n * pages which have ever had a reservation assigned which this persists even\n * after the page is instantiated.  A private mapping has a region map\n * associated with the original mmap which is attached to all VMAs which\n * reference it, this region map represents those offsets which have consumed\n * reservation ie. where pages have been instantiated.\n */\nstatic unsigned long get_vma_private_data(struct vm_area_struct *vma)\n{\n\treturn (unsigned long)vma->vm_private_data;\n}\n\nstatic void set_vma_private_data(struct vm_area_struct *vma,\n\t\t\t\t\t\t\tunsigned long value)\n{\n\tvma->vm_private_data = (void *)value;\n}\n\nstatic void\nresv_map_set_hugetlb_cgroup_uncharge_info(struct resv_map *resv_map,\n\t\t\t\t\t  struct hugetlb_cgroup *h_cg,\n\t\t\t\t\t  struct hstate *h)\n{\n#ifdef CONFIG_CGROUP_HUGETLB\n\tif (!h_cg || !h) {\n\t\tresv_map->reservation_counter = NULL;\n\t\tresv_map->pages_per_hpage = 0;\n\t\tresv_map->css = NULL;\n\t} else {\n\t\tresv_map->reservation_counter =\n\t\t\t&h_cg->rsvd_hugepage[hstate_index(h)];\n\t\tresv_map->pages_per_hpage = pages_per_huge_page(h);\n\t\tresv_map->css = &h_cg->css;\n\t}\n#endif\n}\n\nstruct resv_map *resv_map_alloc(void)\n{\n\tstruct resv_map *resv_map = kmalloc(sizeof(*resv_map), GFP_KERNEL);\n\tstruct file_region *rg = kmalloc(sizeof(*rg), GFP_KERNEL);\n\n\tif (!resv_map || !rg) {\n\t\tkfree(resv_map);\n\t\tkfree(rg);\n\t\treturn NULL;\n\t}\n\n\tkref_init(&resv_map->refs);\n\tspin_lock_init(&resv_map->lock);\n\tINIT_LIST_HEAD(&resv_map->regions);\n\n\tresv_map->adds_in_progress = 0;\n\t/*\n\t * Initialize these to 0. On shared mappings, 0's here indicate these\n\t * fields don't do cgroup accounting. On private mappings, these will be\n\t * re-initialized to the proper values, to indicate that hugetlb cgroup\n\t * reservations are to be un-charged from here.\n\t */\n\tresv_map_set_hugetlb_cgroup_uncharge_info(resv_map, NULL, NULL);\n\n\tINIT_LIST_HEAD(&resv_map->region_cache);\n\tlist_add(&rg->link, &resv_map->region_cache);\n\tresv_map->region_cache_count = 1;\n\n\treturn resv_map;\n}\n\nvoid resv_map_release(struct kref *ref)\n{\n\tstruct resv_map *resv_map = container_of(ref, struct resv_map, refs);\n\tstruct list_head *head = &resv_map->region_cache;\n\tstruct file_region *rg, *trg;\n\n\t/* Clear out any active regions before we release the map. */\n\tregion_del(resv_map, 0, LONG_MAX);\n\n\t/* ... and any entries left in the cache */\n\tlist_for_each_entry_safe(rg, trg, head, link) {\n\t\tlist_del(&rg->link);\n\t\tkfree(rg);\n\t}\n\n\tVM_BUG_ON(resv_map->adds_in_progress);\n\n\tkfree(resv_map);\n}\n\nstatic inline struct resv_map *inode_resv_map(struct inode *inode)\n{\n\t/*\n\t * At inode evict time, i_mapping may not point to the original\n\t * address space within the inode.  This original address space\n\t * contains the pointer to the resv_map.  So, always use the\n\t * address space embedded within the inode.\n\t * The VERY common case is inode->mapping == &inode->i_data but,\n\t * this may not be true for device special inodes.\n\t */\n\treturn (struct resv_map *)(&inode->i_data)->private_data;\n}\n\nstatic struct resv_map *vma_resv_map(struct vm_area_struct *vma)\n{\n\tVM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);\n\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\tstruct address_space *mapping = vma->vm_file->f_mapping;\n\t\tstruct inode *inode = mapping->host;\n\n\t\treturn inode_resv_map(inode);\n\n\t} else {\n\t\treturn (struct resv_map *)(get_vma_private_data(vma) &\n\t\t\t\t\t\t\t~HPAGE_RESV_MASK);\n\t}\n}\n\nstatic void set_vma_resv_map(struct vm_area_struct *vma, struct resv_map *map)\n{\n\tVM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);\n\tVM_BUG_ON_VMA(vma->vm_flags & VM_MAYSHARE, vma);\n\n\tset_vma_private_data(vma, (get_vma_private_data(vma) &\n\t\t\t\tHPAGE_RESV_MASK) | (unsigned long)map);\n}\n\nstatic void set_vma_resv_flags(struct vm_area_struct *vma, unsigned long flags)\n{\n\tVM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);\n\tVM_BUG_ON_VMA(vma->vm_flags & VM_MAYSHARE, vma);\n\n\tset_vma_private_data(vma, get_vma_private_data(vma) | flags);\n}\n\nstatic int is_vma_resv_set(struct vm_area_struct *vma, unsigned long flag)\n{\n\tVM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);\n\n\treturn (get_vma_private_data(vma) & flag) != 0;\n}\n\n/* Reset counters to 0 and clear all HPAGE_RESV_* flags */\nvoid reset_vma_resv_huge_pages(struct vm_area_struct *vma)\n{\n\tVM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);\n\tif (!(vma->vm_flags & VM_MAYSHARE))\n\t\tvma->vm_private_data = (void *)0;\n}\n\n/* Returns true if the VMA has associated reserve pages */\nstatic bool vma_has_reserves(struct vm_area_struct *vma, long chg)\n{\n\tif (vma->vm_flags & VM_NORESERVE) {\n\t\t/*\n\t\t * This address is already reserved by other process(chg == 0),\n\t\t * so, we should decrement reserved count. Without decrementing,\n\t\t * reserve count remains after releasing inode, because this\n\t\t * allocated page will go into page cache and is regarded as\n\t\t * coming from reserved pool in releasing step.  Currently, we\n\t\t * don't have any other solution to deal with this situation\n\t\t * properly, so add work-around here.\n\t\t */\n\t\tif (vma->vm_flags & VM_MAYSHARE && chg == 0)\n\t\t\treturn true;\n\t\telse\n\t\t\treturn false;\n\t}\n\n\t/* Shared mappings always use reserves */\n\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\t/*\n\t\t * We know VM_NORESERVE is not set.  Therefore, there SHOULD\n\t\t * be a region map for all pages.  The only situation where\n\t\t * there is no region map is if a hole was punched via\n\t\t * fallocate.  In this case, there really are no reserves to\n\t\t * use.  This situation is indicated if chg != 0.\n\t\t */\n\t\tif (chg)\n\t\t\treturn false;\n\t\telse\n\t\t\treturn true;\n\t}\n\n\t/*\n\t * Only the process that called mmap() has reserves for\n\t * private mappings.\n\t */\n\tif (is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {\n\t\t/*\n\t\t * Like the shared case above, a hole punch or truncate\n\t\t * could have been performed on the private mapping.\n\t\t * Examine the value of chg to determine if reserves\n\t\t * actually exist or were previously consumed.\n\t\t * Very Subtle - The value of chg comes from a previous\n\t\t * call to vma_needs_reserves().  The reserve map for\n\t\t * private mappings has different (opposite) semantics\n\t\t * than that of shared mappings.  vma_needs_reserves()\n\t\t * has already taken this difference in semantics into\n\t\t * account.  Therefore, the meaning of chg is the same\n\t\t * as in the shared case above.  Code could easily be\n\t\t * combined, but keeping it separate draws attention to\n\t\t * subtle differences.\n\t\t */\n\t\tif (chg)\n\t\t\treturn false;\n\t\telse\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void enqueue_huge_page(struct hstate *h, struct page *page)\n{\n\tint nid = page_to_nid(page);\n\tlist_move(&page->lru, &h->hugepage_freelists[nid]);\n\th->free_huge_pages++;\n\th->free_huge_pages_node[nid]++;\n}\n\nstatic struct page *dequeue_huge_page_node_exact(struct hstate *h, int nid)\n{\n\tstruct page *page;\n\tbool nocma = !!(current->flags & PF_MEMALLOC_NOCMA);\n\n\tlist_for_each_entry(page, &h->hugepage_freelists[nid], lru) {\n\t\tif (nocma && is_migrate_cma_page(page))\n\t\t\tcontinue;\n\n\t\tif (PageHWPoison(page))\n\t\t\tcontinue;\n\n\t\tlist_move(&page->lru, &h->hugepage_activelist);\n\t\tset_page_refcounted(page);\n\t\th->free_huge_pages--;\n\t\th->free_huge_pages_node[nid]--;\n\t\treturn page;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct page *dequeue_huge_page_nodemask(struct hstate *h, gfp_t gfp_mask, int nid,\n\t\tnodemask_t *nmask)\n{\n\tunsigned int cpuset_mems_cookie;\n\tstruct zonelist *zonelist;\n\tstruct zone *zone;\n\tstruct zoneref *z;\n\tint node = NUMA_NO_NODE;\n\n\tzonelist = node_zonelist(nid, gfp_mask);\n\nretry_cpuset:\n\tcpuset_mems_cookie = read_mems_allowed_begin();\n\tfor_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nmask) {\n\t\tstruct page *page;\n\n\t\tif (!cpuset_zone_allowed(zone, gfp_mask))\n\t\t\tcontinue;\n\t\t/*\n\t\t * no need to ask again on the same node. Pool is node rather than\n\t\t * zone aware\n\t\t */\n\t\tif (zone_to_nid(zone) == node)\n\t\t\tcontinue;\n\t\tnode = zone_to_nid(zone);\n\n\t\tpage = dequeue_huge_page_node_exact(h, node);\n\t\tif (page)\n\t\t\treturn page;\n\t}\n\tif (unlikely(read_mems_allowed_retry(cpuset_mems_cookie)))\n\t\tgoto retry_cpuset;\n\n\treturn NULL;\n}\n\nstatic struct page *dequeue_huge_page_vma(struct hstate *h,\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tunsigned long address, int avoid_reserve,\n\t\t\t\tlong chg)\n{\n\tstruct page *page;\n\tstruct mempolicy *mpol;\n\tgfp_t gfp_mask;\n\tnodemask_t *nodemask;\n\tint nid;\n\n\t/*\n\t * A child process with MAP_PRIVATE mappings created by their parent\n\t * have no page reserves. This check ensures that reservations are\n\t * not \"stolen\". The child may still get SIGKILLed\n\t */\n\tif (!vma_has_reserves(vma, chg) &&\n\t\t\th->free_huge_pages - h->resv_huge_pages == 0)\n\t\tgoto err;\n\n\t/* If reserves cannot be used, ensure enough pages are in the pool */\n\tif (avoid_reserve && h->free_huge_pages - h->resv_huge_pages == 0)\n\t\tgoto err;\n\n\tgfp_mask = htlb_alloc_mask(h);\n\tnid = huge_node(vma, address, gfp_mask, &mpol, &nodemask);\n\tpage = dequeue_huge_page_nodemask(h, gfp_mask, nid, nodemask);\n\tif (page && !avoid_reserve && vma_has_reserves(vma, chg)) {\n\t\tSetPagePrivate(page);\n\t\th->resv_huge_pages--;\n\t}\n\n\tmpol_cond_put(mpol);\n\treturn page;\n\nerr:\n\treturn NULL;\n}\n\n/*\n * common helper functions for hstate_next_node_to_{alloc|free}.\n * We may have allocated or freed a huge page based on a different\n * nodes_allowed previously, so h->next_node_to_{alloc|free} might\n * be outside of *nodes_allowed.  Ensure that we use an allowed\n * node for alloc or free.\n */\nstatic int next_node_allowed(int nid, nodemask_t *nodes_allowed)\n{\n\tnid = next_node_in(nid, *nodes_allowed);\n\tVM_BUG_ON(nid >= MAX_NUMNODES);\n\n\treturn nid;\n}\n\nstatic int get_valid_node_allowed(int nid, nodemask_t *nodes_allowed)\n{\n\tif (!node_isset(nid, *nodes_allowed))\n\t\tnid = next_node_allowed(nid, nodes_allowed);\n\treturn nid;\n}\n\n/*\n * returns the previously saved node [\"this node\"] from which to\n * allocate a persistent huge page for the pool and advance the\n * next node from which to allocate, handling wrap at end of node\n * mask.\n */\nstatic int hstate_next_node_to_alloc(struct hstate *h,\n\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n\tint nid;\n\n\tVM_BUG_ON(!nodes_allowed);\n\n\tnid = get_valid_node_allowed(h->next_nid_to_alloc, nodes_allowed);\n\th->next_nid_to_alloc = next_node_allowed(nid, nodes_allowed);\n\n\treturn nid;\n}\n\n/*\n * helper for free_pool_huge_page() - return the previously saved\n * node [\"this node\"] from which to free a huge page.  Advance the\n * next node id whether or not we find a free huge page to free so\n * that the next attempt to free addresses the next node.\n */\nstatic int hstate_next_node_to_free(struct hstate *h, nodemask_t *nodes_allowed)\n{\n\tint nid;\n\n\tVM_BUG_ON(!nodes_allowed);\n\n\tnid = get_valid_node_allowed(h->next_nid_to_free, nodes_allowed);\n\th->next_nid_to_free = next_node_allowed(nid, nodes_allowed);\n\n\treturn nid;\n}\n\n#define for_each_node_mask_to_alloc(hs, nr_nodes, node, mask)\t\t\\\n\tfor (nr_nodes = nodes_weight(*mask);\t\t\t\t\\\n\t\tnr_nodes > 0 &&\t\t\t\t\t\t\\\n\t\t((node = hstate_next_node_to_alloc(hs, mask)) || 1);\t\\\n\t\tnr_nodes--)\n\n#define for_each_node_mask_to_free(hs, nr_nodes, node, mask)\t\t\\\n\tfor (nr_nodes = nodes_weight(*mask);\t\t\t\t\\\n\t\tnr_nodes > 0 &&\t\t\t\t\t\t\\\n\t\t((node = hstate_next_node_to_free(hs, mask)) || 1);\t\\\n\t\tnr_nodes--)\n\n#ifdef CONFIG_ARCH_HAS_GIGANTIC_PAGE\nstatic void destroy_compound_gigantic_page(struct page *page,\n\t\t\t\t\tunsigned int order)\n{\n\tint i;\n\tint nr_pages = 1 << order;\n\tstruct page *p = page + 1;\n\n\tatomic_set(compound_mapcount_ptr(page), 0);\n\tif (hpage_pincount_available(page))\n\t\tatomic_set(compound_pincount_ptr(page), 0);\n\n\tfor (i = 1; i < nr_pages; i++, p = mem_map_next(p, page, i)) {\n\t\tclear_compound_head(p);\n\t\tset_page_refcounted(p);\n\t}\n\n\tset_compound_order(page, 0);\n\t__ClearPageHead(page);\n}\n\nstatic void free_gigantic_page(struct page *page, unsigned int order)\n{\n\t/*\n\t * If the page isn't allocated using the cma allocator,\n\t * cma_release() returns false.\n\t */\n#ifdef CONFIG_CMA\n\tif (cma_release(hugetlb_cma[page_to_nid(page)], page, 1 << order))\n\t\treturn;\n#endif\n\n\tfree_contig_range(page_to_pfn(page), 1 << order);\n}\n\n#ifdef CONFIG_CONTIG_ALLOC\nstatic struct page *alloc_gigantic_page(struct hstate *h, gfp_t gfp_mask,\n\t\tint nid, nodemask_t *nodemask)\n{\n\tunsigned long nr_pages = 1UL << huge_page_order(h);\n\tif (nid == NUMA_NO_NODE)\n\t\tnid = numa_mem_id();\n\n#ifdef CONFIG_CMA\n\t{\n\t\tstruct page *page;\n\t\tint node;\n\n\t\tif (hugetlb_cma[nid]) {\n\t\t\tpage = cma_alloc(hugetlb_cma[nid], nr_pages,\n\t\t\t\t\thuge_page_order(h), true);\n\t\t\tif (page)\n\t\t\t\treturn page;\n\t\t}\n\n\t\tif (!(gfp_mask & __GFP_THISNODE)) {\n\t\t\tfor_each_node_mask(node, *nodemask) {\n\t\t\t\tif (node == nid || !hugetlb_cma[node])\n\t\t\t\t\tcontinue;\n\n\t\t\t\tpage = cma_alloc(hugetlb_cma[node], nr_pages,\n\t\t\t\t\t\thuge_page_order(h), true);\n\t\t\t\tif (page)\n\t\t\t\t\treturn page;\n\t\t\t}\n\t\t}\n\t}\n#endif\n\n\treturn alloc_contig_pages(nr_pages, gfp_mask, nid, nodemask);\n}\n\nstatic void prep_new_huge_page(struct hstate *h, struct page *page, int nid);\nstatic void prep_compound_gigantic_page(struct page *page, unsigned int order);\n#else /* !CONFIG_CONTIG_ALLOC */\nstatic struct page *alloc_gigantic_page(struct hstate *h, gfp_t gfp_mask,\n\t\t\t\t\tint nid, nodemask_t *nodemask)\n{\n\treturn NULL;\n}\n#endif /* CONFIG_CONTIG_ALLOC */\n\n#else /* !CONFIG_ARCH_HAS_GIGANTIC_PAGE */\nstatic struct page *alloc_gigantic_page(struct hstate *h, gfp_t gfp_mask,\n\t\t\t\t\tint nid, nodemask_t *nodemask)\n{\n\treturn NULL;\n}\nstatic inline void free_gigantic_page(struct page *page, unsigned int order) { }\nstatic inline void destroy_compound_gigantic_page(struct page *page,\n\t\t\t\t\t\tunsigned int order) { }\n#endif\n\nstatic void update_and_free_page(struct hstate *h, struct page *page)\n{\n\tint i;\n\n\tif (hstate_is_gigantic(h) && !gigantic_page_runtime_supported())\n\t\treturn;\n\n\th->nr_huge_pages--;\n\th->nr_huge_pages_node[page_to_nid(page)]--;\n\tfor (i = 0; i < pages_per_huge_page(h); i++) {\n\t\tpage[i].flags &= ~(1 << PG_locked | 1 << PG_error |\n\t\t\t\t1 << PG_referenced | 1 << PG_dirty |\n\t\t\t\t1 << PG_active | 1 << PG_private |\n\t\t\t\t1 << PG_writeback);\n\t}\n\tVM_BUG_ON_PAGE(hugetlb_cgroup_from_page(page), page);\n\tVM_BUG_ON_PAGE(hugetlb_cgroup_from_page_rsvd(page), page);\n\tset_compound_page_dtor(page, NULL_COMPOUND_DTOR);\n\tset_page_refcounted(page);\n\tif (hstate_is_gigantic(h)) {\n\t\t/*\n\t\t * Temporarily drop the hugetlb_lock, because\n\t\t * we might block in free_gigantic_page().\n\t\t */\n\t\tspin_unlock(&hugetlb_lock);\n\t\tdestroy_compound_gigantic_page(page, huge_page_order(h));\n\t\tfree_gigantic_page(page, huge_page_order(h));\n\t\tspin_lock(&hugetlb_lock);\n\t} else {\n\t\t__free_pages(page, huge_page_order(h));\n\t}\n}\n\nstruct hstate *size_to_hstate(unsigned long size)\n{\n\tstruct hstate *h;\n\n\tfor_each_hstate(h) {\n\t\tif (huge_page_size(h) == size)\n\t\t\treturn h;\n\t}\n\treturn NULL;\n}\n\n/*\n * Test to determine whether the hugepage is \"active/in-use\" (i.e. being linked\n * to hstate->hugepage_activelist.)\n *\n * This function can be called for tail pages, but never returns true for them.\n */\nbool page_huge_active(struct page *page)\n{\n\tVM_BUG_ON_PAGE(!PageHuge(page), page);\n\treturn PageHead(page) && PagePrivate(&page[1]);\n}\n\n/* never called for tail page */\nstatic void set_page_huge_active(struct page *page)\n{\n\tVM_BUG_ON_PAGE(!PageHeadHuge(page), page);\n\tSetPagePrivate(&page[1]);\n}\n\nstatic void clear_page_huge_active(struct page *page)\n{\n\tVM_BUG_ON_PAGE(!PageHeadHuge(page), page);\n\tClearPagePrivate(&page[1]);\n}\n\n/*\n * Internal hugetlb specific page flag. Do not use outside of the hugetlb\n * code\n */\nstatic inline bool PageHugeTemporary(struct page *page)\n{\n\tif (!PageHuge(page))\n\t\treturn false;\n\n\treturn (unsigned long)page[2].mapping == -1U;\n}\n\nstatic inline void SetPageHugeTemporary(struct page *page)\n{\n\tpage[2].mapping = (void *)-1U;\n}\n\nstatic inline void ClearPageHugeTemporary(struct page *page)\n{\n\tpage[2].mapping = NULL;\n}\n\nstatic void __free_huge_page(struct page *page)\n{\n\t/*\n\t * Can't pass hstate in here because it is called from the\n\t * compound page destructor.\n\t */\n\tstruct hstate *h = page_hstate(page);\n\tint nid = page_to_nid(page);\n\tstruct hugepage_subpool *spool =\n\t\t(struct hugepage_subpool *)page_private(page);\n\tbool restore_reserve;\n\n\tVM_BUG_ON_PAGE(page_count(page), page);\n\tVM_BUG_ON_PAGE(page_mapcount(page), page);\n\n\tset_page_private(page, 0);\n\tpage->mapping = NULL;\n\trestore_reserve = PagePrivate(page);\n\tClearPagePrivate(page);\n\n\t/*\n\t * If PagePrivate() was set on page, page allocation consumed a\n\t * reservation.  If the page was associated with a subpool, there\n\t * would have been a page reserved in the subpool before allocation\n\t * via hugepage_subpool_get_pages().  Since we are 'restoring' the\n\t * reservtion, do not call hugepage_subpool_put_pages() as this will\n\t * remove the reserved page from the subpool.\n\t */\n\tif (!restore_reserve) {\n\t\t/*\n\t\t * A return code of zero implies that the subpool will be\n\t\t * under its minimum size if the reservation is not restored\n\t\t * after page is free.  Therefore, force restore_reserve\n\t\t * operation.\n\t\t */\n\t\tif (hugepage_subpool_put_pages(spool, 1) == 0)\n\t\t\trestore_reserve = true;\n\t}\n\n\tspin_lock(&hugetlb_lock);\n\tclear_page_huge_active(page);\n\thugetlb_cgroup_uncharge_page(hstate_index(h),\n\t\t\t\t     pages_per_huge_page(h), page);\n\thugetlb_cgroup_uncharge_page_rsvd(hstate_index(h),\n\t\t\t\t\t  pages_per_huge_page(h), page);\n\tif (restore_reserve)\n\t\th->resv_huge_pages++;\n\n\tif (PageHugeTemporary(page)) {\n\t\tlist_del(&page->lru);\n\t\tClearPageHugeTemporary(page);\n\t\tupdate_and_free_page(h, page);\n\t} else if (h->surplus_huge_pages_node[nid]) {\n\t\t/* remove the page from active list */\n\t\tlist_del(&page->lru);\n\t\tupdate_and_free_page(h, page);\n\t\th->surplus_huge_pages--;\n\t\th->surplus_huge_pages_node[nid]--;\n\t} else {\n\t\tarch_clear_hugepage_flags(page);\n\t\tenqueue_huge_page(h, page);\n\t}\n\tspin_unlock(&hugetlb_lock);\n}\n\n/*\n * As free_huge_page() can be called from a non-task context, we have\n * to defer the actual freeing in a workqueue to prevent potential\n * hugetlb_lock deadlock.\n *\n * free_hpage_workfn() locklessly retrieves the linked list of pages to\n * be freed and frees them one-by-one. As the page->mapping pointer is\n * going to be cleared in __free_huge_page() anyway, it is reused as the\n * llist_node structure of a lockless linked list of huge pages to be freed.\n */\nstatic LLIST_HEAD(hpage_freelist);\n\nstatic void free_hpage_workfn(struct work_struct *work)\n{\n\tstruct llist_node *node;\n\tstruct page *page;\n\n\tnode = llist_del_all(&hpage_freelist);\n\n\twhile (node) {\n\t\tpage = container_of((struct address_space **)node,\n\t\t\t\t     struct page, mapping);\n\t\tnode = node->next;\n\t\t__free_huge_page(page);\n\t}\n}\nstatic DECLARE_WORK(free_hpage_work, free_hpage_workfn);\n\nvoid free_huge_page(struct page *page)\n{\n\t/*\n\t * Defer freeing if in non-task context to avoid hugetlb_lock deadlock.\n\t */\n\tif (!in_task()) {\n\t\t/*\n\t\t * Only call schedule_work() if hpage_freelist is previously\n\t\t * empty. Otherwise, schedule_work() had been called but the\n\t\t * workfn hasn't retrieved the list yet.\n\t\t */\n\t\tif (llist_add((struct llist_node *)&page->mapping,\n\t\t\t      &hpage_freelist))\n\t\t\tschedule_work(&free_hpage_work);\n\t\treturn;\n\t}\n\n\t__free_huge_page(page);\n}\n\nstatic void prep_new_huge_page(struct hstate *h, struct page *page, int nid)\n{\n\tINIT_LIST_HEAD(&page->lru);\n\tset_compound_page_dtor(page, HUGETLB_PAGE_DTOR);\n\tset_hugetlb_cgroup(page, NULL);\n\tset_hugetlb_cgroup_rsvd(page, NULL);\n\tspin_lock(&hugetlb_lock);\n\th->nr_huge_pages++;\n\th->nr_huge_pages_node[nid]++;\n\tspin_unlock(&hugetlb_lock);\n}\n\nstatic void prep_compound_gigantic_page(struct page *page, unsigned int order)\n{\n\tint i;\n\tint nr_pages = 1 << order;\n\tstruct page *p = page + 1;\n\n\t/* we rely on prep_new_huge_page to set the destructor */\n\tset_compound_order(page, order);\n\t__ClearPageReserved(page);\n\t__SetPageHead(page);\n\tfor (i = 1; i < nr_pages; i++, p = mem_map_next(p, page, i)) {\n\t\t/*\n\t\t * For gigantic hugepages allocated through bootmem at\n\t\t * boot, it's safer to be consistent with the not-gigantic\n\t\t * hugepages and clear the PG_reserved bit from all tail pages\n\t\t * too.  Otherwise drivers using get_user_pages() to access tail\n\t\t * pages may get the reference counting wrong if they see\n\t\t * PG_reserved set on a tail page (despite the head page not\n\t\t * having PG_reserved set).  Enforcing this consistency between\n\t\t * head and tail pages allows drivers to optimize away a check\n\t\t * on the head page when they need know if put_page() is needed\n\t\t * after get_user_pages().\n\t\t */\n\t\t__ClearPageReserved(p);\n\t\tset_page_count(p, 0);\n\t\tset_compound_head(p, page);\n\t}\n\tatomic_set(compound_mapcount_ptr(page), -1);\n\n\tif (hpage_pincount_available(page))\n\t\tatomic_set(compound_pincount_ptr(page), 0);\n}\n\n/*\n * PageHuge() only returns true for hugetlbfs pages, but not for normal or\n * transparent huge pages.  See the PageTransHuge() documentation for more\n * details.\n */\nint PageHuge(struct page *page)\n{\n\tif (!PageCompound(page))\n\t\treturn 0;\n\n\tpage = compound_head(page);\n\treturn page[1].compound_dtor == HUGETLB_PAGE_DTOR;\n}\nEXPORT_SYMBOL_GPL(PageHuge);\n\n/*\n * PageHeadHuge() only returns true for hugetlbfs head page, but not for\n * normal or transparent huge pages.\n */\nint PageHeadHuge(struct page *page_head)\n{\n\tif (!PageHead(page_head))\n\t\treturn 0;\n\n\treturn page_head[1].compound_dtor == HUGETLB_PAGE_DTOR;\n}\n\n/*\n * Find and lock address space (mapping) in write mode.\n *\n * Upon entry, the page is locked which means that page_mapping() is\n * stable.  Due to locking order, we can only trylock_write.  If we can\n * not get the lock, simply return NULL to caller.\n */\nstruct address_space *hugetlb_page_mapping_lock_write(struct page *hpage)\n{\n\tstruct address_space *mapping = page_mapping(hpage);\n\n\tif (!mapping)\n\t\treturn mapping;\n\n\tif (i_mmap_trylock_write(mapping))\n\t\treturn mapping;\n\n\treturn NULL;\n}\n\npgoff_t __basepage_index(struct page *page)\n{\n\tstruct page *page_head = compound_head(page);\n\tpgoff_t index = page_index(page_head);\n\tunsigned long compound_idx;\n\n\tif (!PageHuge(page_head))\n\t\treturn page_index(page);\n\n\tif (compound_order(page_head) >= MAX_ORDER)\n\t\tcompound_idx = page_to_pfn(page) - page_to_pfn(page_head);\n\telse\n\t\tcompound_idx = page - page_head;\n\n\treturn (index << compound_order(page_head)) + compound_idx;\n}\n\nstatic struct page *alloc_buddy_huge_page(struct hstate *h,\n\t\tgfp_t gfp_mask, int nid, nodemask_t *nmask,\n\t\tnodemask_t *node_alloc_noretry)\n{\n\tint order = huge_page_order(h);\n\tstruct page *page;\n\tbool alloc_try_hard = true;\n\n\t/*\n\t * By default we always try hard to allocate the page with\n\t * __GFP_RETRY_MAYFAIL flag.  However, if we are allocating pages in\n\t * a loop (to adjust global huge page counts) and previous allocation\n\t * failed, do not continue to try hard on the same node.  Use the\n\t * node_alloc_noretry bitmap to manage this state information.\n\t */\n\tif (node_alloc_noretry && node_isset(nid, *node_alloc_noretry))\n\t\talloc_try_hard = false;\n\tgfp_mask |= __GFP_COMP|__GFP_NOWARN;\n\tif (alloc_try_hard)\n\t\tgfp_mask |= __GFP_RETRY_MAYFAIL;\n\tif (nid == NUMA_NO_NODE)\n\t\tnid = numa_mem_id();\n\tpage = __alloc_pages_nodemask(gfp_mask, order, nid, nmask);\n\tif (page)\n\t\t__count_vm_event(HTLB_BUDDY_PGALLOC);\n\telse\n\t\t__count_vm_event(HTLB_BUDDY_PGALLOC_FAIL);\n\n\t/*\n\t * If we did not specify __GFP_RETRY_MAYFAIL, but still got a page this\n\t * indicates an overall state change.  Clear bit so that we resume\n\t * normal 'try hard' allocations.\n\t */\n\tif (node_alloc_noretry && page && !alloc_try_hard)\n\t\tnode_clear(nid, *node_alloc_noretry);\n\n\t/*\n\t * If we tried hard to get a page but failed, set bit so that\n\t * subsequent attempts will not try as hard until there is an\n\t * overall state change.\n\t */\n\tif (node_alloc_noretry && !page && alloc_try_hard)\n\t\tnode_set(nid, *node_alloc_noretry);\n\n\treturn page;\n}\n\n/*\n * Common helper to allocate a fresh hugetlb page. All specific allocators\n * should use this function to get new hugetlb pages\n */\nstatic struct page *alloc_fresh_huge_page(struct hstate *h,\n\t\tgfp_t gfp_mask, int nid, nodemask_t *nmask,\n\t\tnodemask_t *node_alloc_noretry)\n{\n\tstruct page *page;\n\n\tif (hstate_is_gigantic(h))\n\t\tpage = alloc_gigantic_page(h, gfp_mask, nid, nmask);\n\telse\n\t\tpage = alloc_buddy_huge_page(h, gfp_mask,\n\t\t\t\tnid, nmask, node_alloc_noretry);\n\tif (!page)\n\t\treturn NULL;\n\n\tif (hstate_is_gigantic(h))\n\t\tprep_compound_gigantic_page(page, huge_page_order(h));\n\tprep_new_huge_page(h, page, page_to_nid(page));\n\n\treturn page;\n}\n\n/*\n * Allocates a fresh page to the hugetlb allocator pool in the node interleaved\n * manner.\n */\nstatic int alloc_pool_huge_page(struct hstate *h, nodemask_t *nodes_allowed,\n\t\t\t\tnodemask_t *node_alloc_noretry)\n{\n\tstruct page *page;\n\tint nr_nodes, node;\n\tgfp_t gfp_mask = htlb_alloc_mask(h) | __GFP_THISNODE;\n\n\tfor_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {\n\t\tpage = alloc_fresh_huge_page(h, gfp_mask, node, nodes_allowed,\n\t\t\t\t\t\tnode_alloc_noretry);\n\t\tif (page)\n\t\t\tbreak;\n\t}\n\n\tif (!page)\n\t\treturn 0;\n\n\tput_page(page); /* free it into the hugepage allocator */\n\n\treturn 1;\n}\n\n/*\n * Free huge page from pool from next node to free.\n * Attempt to keep persistent huge pages more or less\n * balanced over allowed nodes.\n * Called with hugetlb_lock locked.\n */\nstatic int free_pool_huge_page(struct hstate *h, nodemask_t *nodes_allowed,\n\t\t\t\t\t\t\t bool acct_surplus)\n{\n\tint nr_nodes, node;\n\tint ret = 0;\n\n\tfor_each_node_mask_to_free(h, nr_nodes, node, nodes_allowed) {\n\t\t/*\n\t\t * If we're returning unused surplus pages, only examine\n\t\t * nodes with surplus pages.\n\t\t */\n\t\tif ((!acct_surplus || h->surplus_huge_pages_node[node]) &&\n\t\t    !list_empty(&h->hugepage_freelists[node])) {\n\t\t\tstruct page *page =\n\t\t\t\tlist_entry(h->hugepage_freelists[node].next,\n\t\t\t\t\t  struct page, lru);\n\t\t\tlist_del(&page->lru);\n\t\t\th->free_huge_pages--;\n\t\t\th->free_huge_pages_node[node]--;\n\t\t\tif (acct_surplus) {\n\t\t\t\th->surplus_huge_pages--;\n\t\t\t\th->surplus_huge_pages_node[node]--;\n\t\t\t}\n\t\t\tupdate_and_free_page(h, page);\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n/*\n * Dissolve a given free hugepage into free buddy pages. This function does\n * nothing for in-use hugepages and non-hugepages.\n * This function returns values like below:\n *\n *  -EBUSY: failed to dissolved free hugepages or the hugepage is in-use\n *          (allocated or reserved.)\n *       0: successfully dissolved free hugepages or the page is not a\n *          hugepage (considered as already dissolved)\n */\nint dissolve_free_huge_page(struct page *page)\n{\n\tint rc = -EBUSY;\n\n\t/* Not to disrupt normal path by vainly holding hugetlb_lock */\n\tif (!PageHuge(page))\n\t\treturn 0;\n\n\tspin_lock(&hugetlb_lock);\n\tif (!PageHuge(page)) {\n\t\trc = 0;\n\t\tgoto out;\n\t}\n\n\tif (!page_count(page)) {\n\t\tstruct page *head = compound_head(page);\n\t\tstruct hstate *h = page_hstate(head);\n\t\tint nid = page_to_nid(head);\n\t\tif (h->free_huge_pages - h->resv_huge_pages == 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * Move PageHWPoison flag from head page to the raw error page,\n\t\t * which makes any subpages rather than the error page reusable.\n\t\t */\n\t\tif (PageHWPoison(head) && page != head) {\n\t\t\tSetPageHWPoison(page);\n\t\t\tClearPageHWPoison(head);\n\t\t}\n\t\tlist_del(&head->lru);\n\t\th->free_huge_pages--;\n\t\th->free_huge_pages_node[nid]--;\n\t\th->max_huge_pages--;\n\t\tupdate_and_free_page(h, head);\n\t\trc = 0;\n\t}\nout:\n\tspin_unlock(&hugetlb_lock);\n\treturn rc;\n}\n\n/*\n * Dissolve free hugepages in a given pfn range. Used by memory hotplug to\n * make specified memory blocks removable from the system.\n * Note that this will dissolve a free gigantic hugepage completely, if any\n * part of it lies within the given range.\n * Also note that if dissolve_free_huge_page() returns with an error, all\n * free hugepages that were dissolved before that error are lost.\n */\nint dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)\n{\n\tunsigned long pfn;\n\tstruct page *page;\n\tint rc = 0;\n\n\tif (!hugepages_supported())\n\t\treturn rc;\n\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn += 1 << minimum_order) {\n\t\tpage = pfn_to_page(pfn);\n\t\trc = dissolve_free_huge_page(page);\n\t\tif (rc)\n\t\t\tbreak;\n\t}\n\n\treturn rc;\n}\n\n/*\n * Allocates a fresh surplus page from the page allocator.\n */\nstatic struct page *alloc_surplus_huge_page(struct hstate *h, gfp_t gfp_mask,\n\t\tint nid, nodemask_t *nmask)\n{\n\tstruct page *page = NULL;\n\n\tif (hstate_is_gigantic(h))\n\t\treturn NULL;\n\n\tspin_lock(&hugetlb_lock);\n\tif (h->surplus_huge_pages >= h->nr_overcommit_huge_pages)\n\t\tgoto out_unlock;\n\tspin_unlock(&hugetlb_lock);\n\n\tpage = alloc_fresh_huge_page(h, gfp_mask, nid, nmask, NULL);\n\tif (!page)\n\t\treturn NULL;\n\n\tspin_lock(&hugetlb_lock);\n\t/*\n\t * We could have raced with the pool size change.\n\t * Double check that and simply deallocate the new page\n\t * if we would end up overcommiting the surpluses. Abuse\n\t * temporary page to workaround the nasty free_huge_page\n\t * codeflow\n\t */\n\tif (h->surplus_huge_pages >= h->nr_overcommit_huge_pages) {\n\t\tSetPageHugeTemporary(page);\n\t\tspin_unlock(&hugetlb_lock);\n\t\tput_page(page);\n\t\treturn NULL;\n\t} else {\n\t\th->surplus_huge_pages++;\n\t\th->surplus_huge_pages_node[page_to_nid(page)]++;\n\t}\n\nout_unlock:\n\tspin_unlock(&hugetlb_lock);\n\n\treturn page;\n}\n\nstatic struct page *alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,\n\t\t\t\t     int nid, nodemask_t *nmask)\n{\n\tstruct page *page;\n\n\tif (hstate_is_gigantic(h))\n\t\treturn NULL;\n\n\tpage = alloc_fresh_huge_page(h, gfp_mask, nid, nmask, NULL);\n\tif (!page)\n\t\treturn NULL;\n\n\t/*\n\t * We do not account these pages as surplus because they are only\n\t * temporary and will be released properly on the last reference\n\t */\n\tSetPageHugeTemporary(page);\n\n\treturn page;\n}\n\n/*\n * Use the VMA's mpolicy to allocate a huge page from the buddy.\n */\nstatic\nstruct page *alloc_buddy_huge_page_with_mpol(struct hstate *h,\n\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\tstruct page *page;\n\tstruct mempolicy *mpol;\n\tgfp_t gfp_mask = htlb_alloc_mask(h);\n\tint nid;\n\tnodemask_t *nodemask;\n\n\tnid = huge_node(vma, addr, gfp_mask, &mpol, &nodemask);\n\tpage = alloc_surplus_huge_page(h, gfp_mask, nid, nodemask);\n\tmpol_cond_put(mpol);\n\n\treturn page;\n}\n\n/* page migration callback function */\nstruct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,\n\t\tnodemask_t *nmask, gfp_t gfp_mask)\n{\n\tspin_lock(&hugetlb_lock);\n\tif (h->free_huge_pages - h->resv_huge_pages > 0) {\n\t\tstruct page *page;\n\n\t\tpage = dequeue_huge_page_nodemask(h, gfp_mask, preferred_nid, nmask);\n\t\tif (page) {\n\t\t\tspin_unlock(&hugetlb_lock);\n\t\t\treturn page;\n\t\t}\n\t}\n\tspin_unlock(&hugetlb_lock);\n\n\treturn alloc_migrate_huge_page(h, gfp_mask, preferred_nid, nmask);\n}\n\n/* mempolicy aware migration callback */\nstruct page *alloc_huge_page_vma(struct hstate *h, struct vm_area_struct *vma,\n\t\tunsigned long address)\n{\n\tstruct mempolicy *mpol;\n\tnodemask_t *nodemask;\n\tstruct page *page;\n\tgfp_t gfp_mask;\n\tint node;\n\n\tgfp_mask = htlb_alloc_mask(h);\n\tnode = huge_node(vma, address, gfp_mask, &mpol, &nodemask);\n\tpage = alloc_huge_page_nodemask(h, node, nodemask, gfp_mask);\n\tmpol_cond_put(mpol);\n\n\treturn page;\n}\n\n/*\n * Increase the hugetlb pool such that it can accommodate a reservation\n * of size 'delta'.\n */\nstatic int gather_surplus_pages(struct hstate *h, long delta)\n\t__must_hold(&hugetlb_lock)\n{\n\tstruct list_head surplus_list;\n\tstruct page *page, *tmp;\n\tint ret;\n\tlong i;\n\tlong needed, allocated;\n\tbool alloc_ok = true;\n\n\tneeded = (h->resv_huge_pages + delta) - h->free_huge_pages;\n\tif (needed <= 0) {\n\t\th->resv_huge_pages += delta;\n\t\treturn 0;\n\t}\n\n\tallocated = 0;\n\tINIT_LIST_HEAD(&surplus_list);\n\n\tret = -ENOMEM;\nretry:\n\tspin_unlock(&hugetlb_lock);\n\tfor (i = 0; i < needed; i++) {\n\t\tpage = alloc_surplus_huge_page(h, htlb_alloc_mask(h),\n\t\t\t\tNUMA_NO_NODE, NULL);\n\t\tif (!page) {\n\t\t\talloc_ok = false;\n\t\t\tbreak;\n\t\t}\n\t\tlist_add(&page->lru, &surplus_list);\n\t\tcond_resched();\n\t}\n\tallocated += i;\n\n\t/*\n\t * After retaking hugetlb_lock, we need to recalculate 'needed'\n\t * because either resv_huge_pages or free_huge_pages may have changed.\n\t */\n\tspin_lock(&hugetlb_lock);\n\tneeded = (h->resv_huge_pages + delta) -\n\t\t\t(h->free_huge_pages + allocated);\n\tif (needed > 0) {\n\t\tif (alloc_ok)\n\t\t\tgoto retry;\n\t\t/*\n\t\t * We were not able to allocate enough pages to\n\t\t * satisfy the entire reservation so we free what\n\t\t * we've allocated so far.\n\t\t */\n\t\tgoto free;\n\t}\n\t/*\n\t * The surplus_list now contains _at_least_ the number of extra pages\n\t * needed to accommodate the reservation.  Add the appropriate number\n\t * of pages to the hugetlb pool and free the extras back to the buddy\n\t * allocator.  Commit the entire reservation here to prevent another\n\t * process from stealing the pages as they are added to the pool but\n\t * before they are reserved.\n\t */\n\tneeded += allocated;\n\th->resv_huge_pages += delta;\n\tret = 0;\n\n\t/* Free the needed pages to the hugetlb pool */\n\tlist_for_each_entry_safe(page, tmp, &surplus_list, lru) {\n\t\tif ((--needed) < 0)\n\t\t\tbreak;\n\t\t/*\n\t\t * This page is now managed by the hugetlb allocator and has\n\t\t * no users -- drop the buddy allocator's reference.\n\t\t */\n\t\tVM_BUG_ON_PAGE(!put_page_testzero(page), page);\n\t\tenqueue_huge_page(h, page);\n\t}\nfree:\n\tspin_unlock(&hugetlb_lock);\n\n\t/* Free unnecessary surplus pages to the buddy allocator */\n\tlist_for_each_entry_safe(page, tmp, &surplus_list, lru)\n\t\tput_page(page);\n\tspin_lock(&hugetlb_lock);\n\n\treturn ret;\n}\n\n/*\n * This routine has two main purposes:\n * 1) Decrement the reservation count (resv_huge_pages) by the value passed\n *    in unused_resv_pages.  This corresponds to the prior adjustments made\n *    to the associated reservation map.\n * 2) Free any unused surplus pages that may have been allocated to satisfy\n *    the reservation.  As many as unused_resv_pages may be freed.\n *\n * Called with hugetlb_lock held.  However, the lock could be dropped (and\n * reacquired) during calls to cond_resched_lock.  Whenever dropping the lock,\n * we must make sure nobody else can claim pages we are in the process of\n * freeing.  Do this by ensuring resv_huge_page always is greater than the\n * number of huge pages we plan to free when dropping the lock.\n */\nstatic void return_unused_surplus_pages(struct hstate *h,\n\t\t\t\t\tunsigned long unused_resv_pages)\n{\n\tunsigned long nr_pages;\n\n\t/* Cannot return gigantic pages currently */\n\tif (hstate_is_gigantic(h))\n\t\tgoto out;\n\n\t/*\n\t * Part (or even all) of the reservation could have been backed\n\t * by pre-allocated pages. Only free surplus pages.\n\t */\n\tnr_pages = min(unused_resv_pages, h->surplus_huge_pages);\n\n\t/*\n\t * We want to release as many surplus pages as possible, spread\n\t * evenly across all nodes with memory. Iterate across these nodes\n\t * until we can no longer free unreserved surplus pages. This occurs\n\t * when the nodes with surplus pages have no free pages.\n\t * free_pool_huge_page() will balance the freed pages across the\n\t * on-line nodes with memory and will handle the hstate accounting.\n\t *\n\t * Note that we decrement resv_huge_pages as we free the pages.  If\n\t * we drop the lock, resv_huge_pages will still be sufficiently large\n\t * to cover subsequent pages we may free.\n\t */\n\twhile (nr_pages--) {\n\t\th->resv_huge_pages--;\n\t\tunused_resv_pages--;\n\t\tif (!free_pool_huge_page(h, &node_states[N_MEMORY], 1))\n\t\t\tgoto out;\n\t\tcond_resched_lock(&hugetlb_lock);\n\t}\n\nout:\n\t/* Fully uncommit the reservation */\n\th->resv_huge_pages -= unused_resv_pages;\n}\n\n\n/*\n * vma_needs_reservation, vma_commit_reservation and vma_end_reservation\n * are used by the huge page allocation routines to manage reservations.\n *\n * vma_needs_reservation is called to determine if the huge page at addr\n * within the vma has an associated reservation.  If a reservation is\n * needed, the value 1 is returned.  The caller is then responsible for\n * managing the global reservation and subpool usage counts.  After\n * the huge page has been allocated, vma_commit_reservation is called\n * to add the page to the reservation map.  If the page allocation fails,\n * the reservation must be ended instead of committed.  vma_end_reservation\n * is called in such cases.\n *\n * In the normal case, vma_commit_reservation returns the same value\n * as the preceding vma_needs_reservation call.  The only time this\n * is not the case is if a reserve map was changed between calls.  It\n * is the responsibility of the caller to notice the difference and\n * take appropriate action.\n *\n * vma_add_reservation is used in error paths where a reservation must\n * be restored when a newly allocated huge page must be freed.  It is\n * to be called after calling vma_needs_reservation to determine if a\n * reservation exists.\n */\nenum vma_resv_mode {\n\tVMA_NEEDS_RESV,\n\tVMA_COMMIT_RESV,\n\tVMA_END_RESV,\n\tVMA_ADD_RESV,\n};\nstatic long __vma_reservation_common(struct hstate *h,\n\t\t\t\tstruct vm_area_struct *vma, unsigned long addr,\n\t\t\t\tenum vma_resv_mode mode)\n{\n\tstruct resv_map *resv;\n\tpgoff_t idx;\n\tlong ret;\n\tlong dummy_out_regions_needed;\n\n\tresv = vma_resv_map(vma);\n\tif (!resv)\n\t\treturn 1;\n\n\tidx = vma_hugecache_offset(h, vma, addr);\n\tswitch (mode) {\n\tcase VMA_NEEDS_RESV:\n\t\tret = region_chg(resv, idx, idx + 1, &dummy_out_regions_needed);\n\t\t/* We assume that vma_reservation_* routines always operate on\n\t\t * 1 page, and that adding to resv map a 1 page entry can only\n\t\t * ever require 1 region.\n\t\t */\n\t\tVM_BUG_ON(dummy_out_regions_needed != 1);\n\t\tbreak;\n\tcase VMA_COMMIT_RESV:\n\t\tret = region_add(resv, idx, idx + 1, 1, NULL, NULL);\n\t\t/* region_add calls of range 1 should never fail. */\n\t\tVM_BUG_ON(ret < 0);\n\t\tbreak;\n\tcase VMA_END_RESV:\n\t\tregion_abort(resv, idx, idx + 1, 1);\n\t\tret = 0;\n\t\tbreak;\n\tcase VMA_ADD_RESV:\n\t\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\t\tret = region_add(resv, idx, idx + 1, 1, NULL, NULL);\n\t\t\t/* region_add calls of range 1 should never fail. */\n\t\t\tVM_BUG_ON(ret < 0);\n\t\t} else {\n\t\t\tregion_abort(resv, idx, idx + 1, 1);\n\t\t\tret = region_del(resv, idx, idx + 1);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\treturn ret;\n\telse if (is_vma_resv_set(vma, HPAGE_RESV_OWNER) && ret >= 0) {\n\t\t/*\n\t\t * In most cases, reserves always exist for private mappings.\n\t\t * However, a file associated with mapping could have been\n\t\t * hole punched or truncated after reserves were consumed.\n\t\t * As subsequent fault on such a range will not use reserves.\n\t\t * Subtle - The reserve map for private mappings has the\n\t\t * opposite meaning than that of shared mappings.  If NO\n\t\t * entry is in the reserve map, it means a reservation exists.\n\t\t * If an entry exists in the reserve map, it means the\n\t\t * reservation has already been consumed.  As a result, the\n\t\t * return value of this routine is the opposite of the\n\t\t * value returned from reserve map manipulation routines above.\n\t\t */\n\t\tif (ret)\n\t\t\treturn 0;\n\t\telse\n\t\t\treturn 1;\n\t}\n\telse\n\t\treturn ret < 0 ? ret : 0;\n}\n\nstatic long vma_needs_reservation(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\treturn __vma_reservation_common(h, vma, addr, VMA_NEEDS_RESV);\n}\n\nstatic long vma_commit_reservation(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\treturn __vma_reservation_common(h, vma, addr, VMA_COMMIT_RESV);\n}\n\nstatic void vma_end_reservation(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\t(void)__vma_reservation_common(h, vma, addr, VMA_END_RESV);\n}\n\nstatic long vma_add_reservation(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\treturn __vma_reservation_common(h, vma, addr, VMA_ADD_RESV);\n}\n\n/*\n * This routine is called to restore a reservation on error paths.  In the\n * specific error paths, a huge page was allocated (via alloc_huge_page)\n * and is about to be freed.  If a reservation for the page existed,\n * alloc_huge_page would have consumed the reservation and set PagePrivate\n * in the newly allocated page.  When the page is freed via free_huge_page,\n * the global reservation count will be incremented if PagePrivate is set.\n * However, free_huge_page can not adjust the reserve map.  Adjust the\n * reserve map here to be consistent with global reserve count adjustments\n * to be made by free_huge_page.\n */\nstatic void restore_reserve_on_error(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long address,\n\t\t\tstruct page *page)\n{\n\tif (unlikely(PagePrivate(page))) {\n\t\tlong rc = vma_needs_reservation(h, vma, address);\n\n\t\tif (unlikely(rc < 0)) {\n\t\t\t/*\n\t\t\t * Rare out of memory condition in reserve map\n\t\t\t * manipulation.  Clear PagePrivate so that\n\t\t\t * global reserve count will not be incremented\n\t\t\t * by free_huge_page.  This will make it appear\n\t\t\t * as though the reservation for this page was\n\t\t\t * consumed.  This may prevent the task from\n\t\t\t * faulting in the page at a later time.  This\n\t\t\t * is better than inconsistent global huge page\n\t\t\t * accounting of reserve counts.\n\t\t\t */\n\t\t\tClearPagePrivate(page);\n\t\t} else if (rc) {\n\t\t\trc = vma_add_reservation(h, vma, address);\n\t\t\tif (unlikely(rc < 0))\n\t\t\t\t/*\n\t\t\t\t * See above comment about rare out of\n\t\t\t\t * memory condition.\n\t\t\t\t */\n\t\t\t\tClearPagePrivate(page);\n\t\t} else\n\t\t\tvma_end_reservation(h, vma, address);\n\t}\n}\n\nstruct page *alloc_huge_page(struct vm_area_struct *vma,\n\t\t\t\t    unsigned long addr, int avoid_reserve)\n{\n\tstruct hugepage_subpool *spool = subpool_vma(vma);\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct page *page;\n\tlong map_chg, map_commit;\n\tlong gbl_chg;\n\tint ret, idx;\n\tstruct hugetlb_cgroup *h_cg;\n\tbool deferred_reserve;\n\n\tidx = hstate_index(h);\n\t/*\n\t * Examine the region/reserve map to determine if the process\n\t * has a reservation for the page to be allocated.  A return\n\t * code of zero indicates a reservation exists (no change).\n\t */\n\tmap_chg = gbl_chg = vma_needs_reservation(h, vma, addr);\n\tif (map_chg < 0)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t/*\n\t * Processes that did not create the mapping will have no\n\t * reserves as indicated by the region/reserve map. Check\n\t * that the allocation will not exceed the subpool limit.\n\t * Allocations for MAP_NORESERVE mappings also need to be\n\t * checked against any subpool limit.\n\t */\n\tif (map_chg || avoid_reserve) {\n\t\tgbl_chg = hugepage_subpool_get_pages(spool, 1);\n\t\tif (gbl_chg < 0) {\n\t\t\tvma_end_reservation(h, vma, addr);\n\t\t\treturn ERR_PTR(-ENOSPC);\n\t\t}\n\n\t\t/*\n\t\t * Even though there was no reservation in the region/reserve\n\t\t * map, there could be reservations associated with the\n\t\t * subpool that can be used.  This would be indicated if the\n\t\t * return value of hugepage_subpool_get_pages() is zero.\n\t\t * However, if avoid_reserve is specified we still avoid even\n\t\t * the subpool reservations.\n\t\t */\n\t\tif (avoid_reserve)\n\t\t\tgbl_chg = 1;\n\t}\n\n\t/* If this allocation is not consuming a reservation, charge it now.\n\t */\n\tdeferred_reserve = map_chg || avoid_reserve || !vma_resv_map(vma);\n\tif (deferred_reserve) {\n\t\tret = hugetlb_cgroup_charge_cgroup_rsvd(\n\t\t\tidx, pages_per_huge_page(h), &h_cg);\n\t\tif (ret)\n\t\t\tgoto out_subpool_put;\n\t}\n\n\tret = hugetlb_cgroup_charge_cgroup(idx, pages_per_huge_page(h), &h_cg);\n\tif (ret)\n\t\tgoto out_uncharge_cgroup_reservation;\n\n\tspin_lock(&hugetlb_lock);\n\t/*\n\t * glb_chg is passed to indicate whether or not a page must be taken\n\t * from the global free pool (global change).  gbl_chg == 0 indicates\n\t * a reservation exists for the allocation.\n\t */\n\tpage = dequeue_huge_page_vma(h, vma, addr, avoid_reserve, gbl_chg);\n\tif (!page) {\n\t\tspin_unlock(&hugetlb_lock);\n\t\tpage = alloc_buddy_huge_page_with_mpol(h, vma, addr);\n\t\tif (!page)\n\t\t\tgoto out_uncharge_cgroup;\n\t\tif (!avoid_reserve && vma_has_reserves(vma, gbl_chg)) {\n\t\t\tSetPagePrivate(page);\n\t\t\th->resv_huge_pages--;\n\t\t}\n\t\tspin_lock(&hugetlb_lock);\n\t\tlist_add(&page->lru, &h->hugepage_activelist);\n\t\t/* Fall through */\n\t}\n\thugetlb_cgroup_commit_charge(idx, pages_per_huge_page(h), h_cg, page);\n\t/* If allocation is not consuming a reservation, also store the\n\t * hugetlb_cgroup pointer on the page.\n\t */\n\tif (deferred_reserve) {\n\t\thugetlb_cgroup_commit_charge_rsvd(idx, pages_per_huge_page(h),\n\t\t\t\t\t\t  h_cg, page);\n\t}\n\n\tspin_unlock(&hugetlb_lock);\n\n\tset_page_private(page, (unsigned long)spool);\n\n\tmap_commit = vma_commit_reservation(h, vma, addr);\n\tif (unlikely(map_chg > map_commit)) {\n\t\t/*\n\t\t * The page was added to the reservation map between\n\t\t * vma_needs_reservation and vma_commit_reservation.\n\t\t * This indicates a race with hugetlb_reserve_pages.\n\t\t * Adjust for the subpool count incremented above AND\n\t\t * in hugetlb_reserve_pages for the same page.  Also,\n\t\t * the reservation count added in hugetlb_reserve_pages\n\t\t * no longer applies.\n\t\t */\n\t\tlong rsv_adjust;\n\n\t\trsv_adjust = hugepage_subpool_put_pages(spool, 1);\n\t\thugetlb_acct_memory(h, -rsv_adjust);\n\t\tif (deferred_reserve)\n\t\t\thugetlb_cgroup_uncharge_page_rsvd(hstate_index(h),\n\t\t\t\t\tpages_per_huge_page(h), page);\n\t}\n\treturn page;\n\nout_uncharge_cgroup:\n\thugetlb_cgroup_uncharge_cgroup(idx, pages_per_huge_page(h), h_cg);\nout_uncharge_cgroup_reservation:\n\tif (deferred_reserve)\n\t\thugetlb_cgroup_uncharge_cgroup_rsvd(idx, pages_per_huge_page(h),\n\t\t\t\t\t\t    h_cg);\nout_subpool_put:\n\tif (map_chg || avoid_reserve)\n\t\thugepage_subpool_put_pages(spool, 1);\n\tvma_end_reservation(h, vma, addr);\n\treturn ERR_PTR(-ENOSPC);\n}\n\nint alloc_bootmem_huge_page(struct hstate *h)\n\t__attribute__ ((weak, alias(\"__alloc_bootmem_huge_page\")));\nint __alloc_bootmem_huge_page(struct hstate *h)\n{\n\tstruct huge_bootmem_page *m;\n\tint nr_nodes, node;\n\n\tfor_each_node_mask_to_alloc(h, nr_nodes, node, &node_states[N_MEMORY]) {\n\t\tvoid *addr;\n\n\t\taddr = memblock_alloc_try_nid_raw(\n\t\t\t\thuge_page_size(h), huge_page_size(h),\n\t\t\t\t0, MEMBLOCK_ALLOC_ACCESSIBLE, node);\n\t\tif (addr) {\n\t\t\t/*\n\t\t\t * Use the beginning of the huge page to store the\n\t\t\t * huge_bootmem_page struct (until gather_bootmem\n\t\t\t * puts them into the mem_map).\n\t\t\t */\n\t\t\tm = addr;\n\t\t\tgoto found;\n\t\t}\n\t}\n\treturn 0;\n\nfound:\n\tBUG_ON(!IS_ALIGNED(virt_to_phys(m), huge_page_size(h)));\n\t/* Put them into a private list first because mem_map is not up yet */\n\tINIT_LIST_HEAD(&m->list);\n\tlist_add(&m->list, &huge_boot_pages);\n\tm->hstate = h;\n\treturn 1;\n}\n\nstatic void __init prep_compound_huge_page(struct page *page,\n\t\tunsigned int order)\n{\n\tif (unlikely(order > (MAX_ORDER - 1)))\n\t\tprep_compound_gigantic_page(page, order);\n\telse\n\t\tprep_compound_page(page, order);\n}\n\n/* Put bootmem huge pages into the standard lists after mem_map is up */\nstatic void __init gather_bootmem_prealloc(void)\n{\n\tstruct huge_bootmem_page *m;\n\n\tlist_for_each_entry(m, &huge_boot_pages, list) {\n\t\tstruct page *page = virt_to_page(m);\n\t\tstruct hstate *h = m->hstate;\n\n\t\tWARN_ON(page_count(page) != 1);\n\t\tprep_compound_huge_page(page, h->order);\n\t\tWARN_ON(PageReserved(page));\n\t\tprep_new_huge_page(h, page, page_to_nid(page));\n\t\tput_page(page); /* free it into the hugepage allocator */\n\n\t\t/*\n\t\t * If we had gigantic hugepages allocated at boot time, we need\n\t\t * to restore the 'stolen' pages to totalram_pages in order to\n\t\t * fix confusing memory reports from free(1) and another\n\t\t * side-effects, like CommitLimit going negative.\n\t\t */\n\t\tif (hstate_is_gigantic(h))\n\t\t\tadjust_managed_page_count(page, 1 << h->order);\n\t\tcond_resched();\n\t}\n}\n\nstatic void __init hugetlb_hstate_alloc_pages(struct hstate *h)\n{\n\tunsigned long i;\n\tnodemask_t *node_alloc_noretry;\n\n\tif (!hstate_is_gigantic(h)) {\n\t\t/*\n\t\t * Bit mask controlling how hard we retry per-node allocations.\n\t\t * Ignore errors as lower level routines can deal with\n\t\t * node_alloc_noretry == NULL.  If this kmalloc fails at boot\n\t\t * time, we are likely in bigger trouble.\n\t\t */\n\t\tnode_alloc_noretry = kmalloc(sizeof(*node_alloc_noretry),\n\t\t\t\t\t\tGFP_KERNEL);\n\t} else {\n\t\t/* allocations done at boot time */\n\t\tnode_alloc_noretry = NULL;\n\t}\n\n\t/* bit mask controlling how hard we retry per-node allocations */\n\tif (node_alloc_noretry)\n\t\tnodes_clear(*node_alloc_noretry);\n\n\tfor (i = 0; i < h->max_huge_pages; ++i) {\n\t\tif (hstate_is_gigantic(h)) {\n\t\t\tif (hugetlb_cma_size) {\n\t\t\t\tpr_warn_once(\"HugeTLB: hugetlb_cma is enabled, skip boot time allocation\\n\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!alloc_bootmem_huge_page(h))\n\t\t\t\tbreak;\n\t\t} else if (!alloc_pool_huge_page(h,\n\t\t\t\t\t &node_states[N_MEMORY],\n\t\t\t\t\t node_alloc_noretry))\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n\tif (i < h->max_huge_pages) {\n\t\tchar buf[32];\n\n\t\tstring_get_size(huge_page_size(h), 1, STRING_UNITS_2, buf, 32);\n\t\tpr_warn(\"HugeTLB: allocating %lu of page size %s failed.  Only allocated %lu hugepages.\\n\",\n\t\t\th->max_huge_pages, buf, i);\n\t\th->max_huge_pages = i;\n\t}\n\n\tkfree(node_alloc_noretry);\n}\n\nstatic void __init hugetlb_init_hstates(void)\n{\n\tstruct hstate *h;\n\n\tfor_each_hstate(h) {\n\t\tif (minimum_order > huge_page_order(h))\n\t\t\tminimum_order = huge_page_order(h);\n\n\t\t/* oversize hugepages were init'ed in early boot */\n\t\tif (!hstate_is_gigantic(h))\n\t\t\thugetlb_hstate_alloc_pages(h);\n\t}\n\tVM_BUG_ON(minimum_order == UINT_MAX);\n}\n\nstatic void __init report_hugepages(void)\n{\n\tstruct hstate *h;\n\n\tfor_each_hstate(h) {\n\t\tchar buf[32];\n\n\t\tstring_get_size(huge_page_size(h), 1, STRING_UNITS_2, buf, 32);\n\t\tpr_info(\"HugeTLB registered %s page size, pre-allocated %ld pages\\n\",\n\t\t\tbuf, h->free_huge_pages);\n\t}\n}\n\n#ifdef CONFIG_HIGHMEM\nstatic void try_to_free_low(struct hstate *h, unsigned long count,\n\t\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n\tint i;\n\n\tif (hstate_is_gigantic(h))\n\t\treturn;\n\n\tfor_each_node_mask(i, *nodes_allowed) {\n\t\tstruct page *page, *next;\n\t\tstruct list_head *freel = &h->hugepage_freelists[i];\n\t\tlist_for_each_entry_safe(page, next, freel, lru) {\n\t\t\tif (count >= h->nr_huge_pages)\n\t\t\t\treturn;\n\t\t\tif (PageHighMem(page))\n\t\t\t\tcontinue;\n\t\t\tlist_del(&page->lru);\n\t\t\tupdate_and_free_page(h, page);\n\t\t\th->free_huge_pages--;\n\t\t\th->free_huge_pages_node[page_to_nid(page)]--;\n\t\t}\n\t}\n}\n#else\nstatic inline void try_to_free_low(struct hstate *h, unsigned long count,\n\t\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n}\n#endif\n\n/*\n * Increment or decrement surplus_huge_pages.  Keep node-specific counters\n * balanced by operating on them in a round-robin fashion.\n * Returns 1 if an adjustment was made.\n */\nstatic int adjust_pool_surplus(struct hstate *h, nodemask_t *nodes_allowed,\n\t\t\t\tint delta)\n{\n\tint nr_nodes, node;\n\n\tVM_BUG_ON(delta != -1 && delta != 1);\n\n\tif (delta < 0) {\n\t\tfor_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {\n\t\t\tif (h->surplus_huge_pages_node[node])\n\t\t\t\tgoto found;\n\t\t}\n\t} else {\n\t\tfor_each_node_mask_to_free(h, nr_nodes, node, nodes_allowed) {\n\t\t\tif (h->surplus_huge_pages_node[node] <\n\t\t\t\t\th->nr_huge_pages_node[node])\n\t\t\t\tgoto found;\n\t\t}\n\t}\n\treturn 0;\n\nfound:\n\th->surplus_huge_pages += delta;\n\th->surplus_huge_pages_node[node] += delta;\n\treturn 1;\n}\n\n#define persistent_huge_pages(h) (h->nr_huge_pages - h->surplus_huge_pages)\nstatic int set_max_huge_pages(struct hstate *h, unsigned long count, int nid,\n\t\t\t      nodemask_t *nodes_allowed)\n{\n\tunsigned long min_count, ret;\n\tNODEMASK_ALLOC(nodemask_t, node_alloc_noretry, GFP_KERNEL);\n\n\t/*\n\t * Bit mask controlling how hard we retry per-node allocations.\n\t * If we can not allocate the bit mask, do not attempt to allocate\n\t * the requested huge pages.\n\t */\n\tif (node_alloc_noretry)\n\t\tnodes_clear(*node_alloc_noretry);\n\telse\n\t\treturn -ENOMEM;\n\n\tspin_lock(&hugetlb_lock);\n\n\t/*\n\t * Check for a node specific request.\n\t * Changing node specific huge page count may require a corresponding\n\t * change to the global count.  In any case, the passed node mask\n\t * (nodes_allowed) will restrict alloc/free to the specified node.\n\t */\n\tif (nid != NUMA_NO_NODE) {\n\t\tunsigned long old_count = count;\n\n\t\tcount += h->nr_huge_pages - h->nr_huge_pages_node[nid];\n\t\t/*\n\t\t * User may have specified a large count value which caused the\n\t\t * above calculation to overflow.  In this case, they wanted\n\t\t * to allocate as many huge pages as possible.  Set count to\n\t\t * largest possible value to align with their intention.\n\t\t */\n\t\tif (count < old_count)\n\t\t\tcount = ULONG_MAX;\n\t}\n\n\t/*\n\t * Gigantic pages runtime allocation depend on the capability for large\n\t * page range allocation.\n\t * If the system does not provide this feature, return an error when\n\t * the user tries to allocate gigantic pages but let the user free the\n\t * boottime allocated gigantic pages.\n\t */\n\tif (hstate_is_gigantic(h) && !IS_ENABLED(CONFIG_CONTIG_ALLOC)) {\n\t\tif (count > persistent_huge_pages(h)) {\n\t\t\tspin_unlock(&hugetlb_lock);\n\t\t\tNODEMASK_FREE(node_alloc_noretry);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t/* Fall through to decrease pool */\n\t}\n\n\t/*\n\t * Increase the pool size\n\t * First take pages out of surplus state.  Then make up the\n\t * remaining difference by allocating fresh huge pages.\n\t *\n\t * We might race with alloc_surplus_huge_page() here and be unable\n\t * to convert a surplus huge page to a normal huge page. That is\n\t * not critical, though, it just means the overall size of the\n\t * pool might be one hugepage larger than it needs to be, but\n\t * within all the constraints specified by the sysctls.\n\t */\n\twhile (h->surplus_huge_pages && count > persistent_huge_pages(h)) {\n\t\tif (!adjust_pool_surplus(h, nodes_allowed, -1))\n\t\t\tbreak;\n\t}\n\n\twhile (count > persistent_huge_pages(h)) {\n\t\t/*\n\t\t * If this allocation races such that we no longer need the\n\t\t * page, free_huge_page will handle it by freeing the page\n\t\t * and reducing the surplus.\n\t\t */\n\t\tspin_unlock(&hugetlb_lock);\n\n\t\t/* yield cpu to avoid soft lockup */\n\t\tcond_resched();\n\n\t\tret = alloc_pool_huge_page(h, nodes_allowed,\n\t\t\t\t\t\tnode_alloc_noretry);\n\t\tspin_lock(&hugetlb_lock);\n\t\tif (!ret)\n\t\t\tgoto out;\n\n\t\t/* Bail for signals. Probably ctrl-c from user */\n\t\tif (signal_pending(current))\n\t\t\tgoto out;\n\t}\n\n\t/*\n\t * Decrease the pool size\n\t * First return free pages to the buddy allocator (being careful\n\t * to keep enough around to satisfy reservations).  Then place\n\t * pages into surplus state as needed so the pool will shrink\n\t * to the desired size as pages become free.\n\t *\n\t * By placing pages into the surplus state independent of the\n\t * overcommit value, we are allowing the surplus pool size to\n\t * exceed overcommit. There are few sane options here. Since\n\t * alloc_surplus_huge_page() is checking the global counter,\n\t * though, we'll note that we're not allowed to exceed surplus\n\t * and won't grow the pool anywhere else. Not until one of the\n\t * sysctls are changed, or the surplus pages go out of use.\n\t */\n\tmin_count = h->resv_huge_pages + h->nr_huge_pages - h->free_huge_pages;\n\tmin_count = max(count, min_count);\n\ttry_to_free_low(h, min_count, nodes_allowed);\n\twhile (min_count < persistent_huge_pages(h)) {\n\t\tif (!free_pool_huge_page(h, nodes_allowed, 0))\n\t\t\tbreak;\n\t\tcond_resched_lock(&hugetlb_lock);\n\t}\n\twhile (count < persistent_huge_pages(h)) {\n\t\tif (!adjust_pool_surplus(h, nodes_allowed, 1))\n\t\t\tbreak;\n\t}\nout:\n\th->max_huge_pages = persistent_huge_pages(h);\n\tspin_unlock(&hugetlb_lock);\n\n\tNODEMASK_FREE(node_alloc_noretry);\n\n\treturn 0;\n}\n\n#define HSTATE_ATTR_RO(_name) \\\n\tstatic struct kobj_attribute _name##_attr = __ATTR_RO(_name)\n\n#define HSTATE_ATTR(_name) \\\n\tstatic struct kobj_attribute _name##_attr = \\\n\t\t__ATTR(_name, 0644, _name##_show, _name##_store)\n\nstatic struct kobject *hugepages_kobj;\nstatic struct kobject *hstate_kobjs[HUGE_MAX_HSTATE];\n\nstatic struct hstate *kobj_to_node_hstate(struct kobject *kobj, int *nidp);\n\nstatic struct hstate *kobj_to_hstate(struct kobject *kobj, int *nidp)\n{\n\tint i;\n\n\tfor (i = 0; i < HUGE_MAX_HSTATE; i++)\n\t\tif (hstate_kobjs[i] == kobj) {\n\t\t\tif (nidp)\n\t\t\t\t*nidp = NUMA_NO_NODE;\n\t\t\treturn &hstates[i];\n\t\t}\n\n\treturn kobj_to_node_hstate(kobj, nidp);\n}\n\nstatic ssize_t nr_hugepages_show_common(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h;\n\tunsigned long nr_huge_pages;\n\tint nid;\n\n\th = kobj_to_hstate(kobj, &nid);\n\tif (nid == NUMA_NO_NODE)\n\t\tnr_huge_pages = h->nr_huge_pages;\n\telse\n\t\tnr_huge_pages = h->nr_huge_pages_node[nid];\n\n\treturn sysfs_emit(buf, \"%lu\\n\", nr_huge_pages);\n}\n\nstatic ssize_t __nr_hugepages_store_common(bool obey_mempolicy,\n\t\t\t\t\t   struct hstate *h, int nid,\n\t\t\t\t\t   unsigned long count, size_t len)\n{\n\tint err;\n\tnodemask_t nodes_allowed, *n_mask;\n\n\tif (hstate_is_gigantic(h) && !gigantic_page_runtime_supported())\n\t\treturn -EINVAL;\n\n\tif (nid == NUMA_NO_NODE) {\n\t\t/*\n\t\t * global hstate attribute\n\t\t */\n\t\tif (!(obey_mempolicy &&\n\t\t\t\tinit_nodemask_of_mempolicy(&nodes_allowed)))\n\t\t\tn_mask = &node_states[N_MEMORY];\n\t\telse\n\t\t\tn_mask = &nodes_allowed;\n\t} else {\n\t\t/*\n\t\t * Node specific request.  count adjustment happens in\n\t\t * set_max_huge_pages() after acquiring hugetlb_lock.\n\t\t */\n\t\tinit_nodemask_of_node(&nodes_allowed, nid);\n\t\tn_mask = &nodes_allowed;\n\t}\n\n\terr = set_max_huge_pages(h, count, nid, n_mask);\n\n\treturn err ? err : len;\n}\n\nstatic ssize_t nr_hugepages_store_common(bool obey_mempolicy,\n\t\t\t\t\t struct kobject *kobj, const char *buf,\n\t\t\t\t\t size_t len)\n{\n\tstruct hstate *h;\n\tunsigned long count;\n\tint nid;\n\tint err;\n\n\terr = kstrtoul(buf, 10, &count);\n\tif (err)\n\t\treturn err;\n\n\th = kobj_to_hstate(kobj, &nid);\n\treturn __nr_hugepages_store_common(obey_mempolicy, h, nid, count, len);\n}\n\nstatic ssize_t nr_hugepages_show(struct kobject *kobj,\n\t\t\t\t       struct kobj_attribute *attr, char *buf)\n{\n\treturn nr_hugepages_show_common(kobj, attr, buf);\n}\n\nstatic ssize_t nr_hugepages_store(struct kobject *kobj,\n\t       struct kobj_attribute *attr, const char *buf, size_t len)\n{\n\treturn nr_hugepages_store_common(false, kobj, buf, len);\n}\nHSTATE_ATTR(nr_hugepages);\n\n#ifdef CONFIG_NUMA\n\n/*\n * hstate attribute for optionally mempolicy-based constraint on persistent\n * huge page alloc/free.\n */\nstatic ssize_t nr_hugepages_mempolicy_show(struct kobject *kobj,\n\t\t\t\t\t   struct kobj_attribute *attr,\n\t\t\t\t\t   char *buf)\n{\n\treturn nr_hugepages_show_common(kobj, attr, buf);\n}\n\nstatic ssize_t nr_hugepages_mempolicy_store(struct kobject *kobj,\n\t       struct kobj_attribute *attr, const char *buf, size_t len)\n{\n\treturn nr_hugepages_store_common(true, kobj, buf, len);\n}\nHSTATE_ATTR(nr_hugepages_mempolicy);\n#endif\n\n\nstatic ssize_t nr_overcommit_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h = kobj_to_hstate(kobj, NULL);\n\treturn sysfs_emit(buf, \"%lu\\n\", h->nr_overcommit_huge_pages);\n}\n\nstatic ssize_t nr_overcommit_hugepages_store(struct kobject *kobj,\n\t\tstruct kobj_attribute *attr, const char *buf, size_t count)\n{\n\tint err;\n\tunsigned long input;\n\tstruct hstate *h = kobj_to_hstate(kobj, NULL);\n\n\tif (hstate_is_gigantic(h))\n\t\treturn -EINVAL;\n\n\terr = kstrtoul(buf, 10, &input);\n\tif (err)\n\t\treturn err;\n\n\tspin_lock(&hugetlb_lock);\n\th->nr_overcommit_huge_pages = input;\n\tspin_unlock(&hugetlb_lock);\n\n\treturn count;\n}\nHSTATE_ATTR(nr_overcommit_hugepages);\n\nstatic ssize_t free_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h;\n\tunsigned long free_huge_pages;\n\tint nid;\n\n\th = kobj_to_hstate(kobj, &nid);\n\tif (nid == NUMA_NO_NODE)\n\t\tfree_huge_pages = h->free_huge_pages;\n\telse\n\t\tfree_huge_pages = h->free_huge_pages_node[nid];\n\n\treturn sysfs_emit(buf, \"%lu\\n\", free_huge_pages);\n}\nHSTATE_ATTR_RO(free_hugepages);\n\nstatic ssize_t resv_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h = kobj_to_hstate(kobj, NULL);\n\treturn sysfs_emit(buf, \"%lu\\n\", h->resv_huge_pages);\n}\nHSTATE_ATTR_RO(resv_hugepages);\n\nstatic ssize_t surplus_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h;\n\tunsigned long surplus_huge_pages;\n\tint nid;\n\n\th = kobj_to_hstate(kobj, &nid);\n\tif (nid == NUMA_NO_NODE)\n\t\tsurplus_huge_pages = h->surplus_huge_pages;\n\telse\n\t\tsurplus_huge_pages = h->surplus_huge_pages_node[nid];\n\n\treturn sysfs_emit(buf, \"%lu\\n\", surplus_huge_pages);\n}\nHSTATE_ATTR_RO(surplus_hugepages);\n\nstatic struct attribute *hstate_attrs[] = {\n\t&nr_hugepages_attr.attr,\n\t&nr_overcommit_hugepages_attr.attr,\n\t&free_hugepages_attr.attr,\n\t&resv_hugepages_attr.attr,\n\t&surplus_hugepages_attr.attr,\n#ifdef CONFIG_NUMA\n\t&nr_hugepages_mempolicy_attr.attr,\n#endif\n\tNULL,\n};\n\nstatic const struct attribute_group hstate_attr_group = {\n\t.attrs = hstate_attrs,\n};\n\nstatic int hugetlb_sysfs_add_hstate(struct hstate *h, struct kobject *parent,\n\t\t\t\t    struct kobject **hstate_kobjs,\n\t\t\t\t    const struct attribute_group *hstate_attr_group)\n{\n\tint retval;\n\tint hi = hstate_index(h);\n\n\thstate_kobjs[hi] = kobject_create_and_add(h->name, parent);\n\tif (!hstate_kobjs[hi])\n\t\treturn -ENOMEM;\n\n\tretval = sysfs_create_group(hstate_kobjs[hi], hstate_attr_group);\n\tif (retval)\n\t\tkobject_put(hstate_kobjs[hi]);\n\n\treturn retval;\n}\n\nstatic void __init hugetlb_sysfs_init(void)\n{\n\tstruct hstate *h;\n\tint err;\n\n\thugepages_kobj = kobject_create_and_add(\"hugepages\", mm_kobj);\n\tif (!hugepages_kobj)\n\t\treturn;\n\n\tfor_each_hstate(h) {\n\t\terr = hugetlb_sysfs_add_hstate(h, hugepages_kobj,\n\t\t\t\t\t hstate_kobjs, &hstate_attr_group);\n\t\tif (err)\n\t\t\tpr_err(\"HugeTLB: Unable to add hstate %s\", h->name);\n\t}\n}\n\n#ifdef CONFIG_NUMA\n\n/*\n * node_hstate/s - associate per node hstate attributes, via their kobjects,\n * with node devices in node_devices[] using a parallel array.  The array\n * index of a node device or _hstate == node id.\n * This is here to avoid any static dependency of the node device driver, in\n * the base kernel, on the hugetlb module.\n */\nstruct node_hstate {\n\tstruct kobject\t\t*hugepages_kobj;\n\tstruct kobject\t\t*hstate_kobjs[HUGE_MAX_HSTATE];\n};\nstatic struct node_hstate node_hstates[MAX_NUMNODES];\n\n/*\n * A subset of global hstate attributes for node devices\n */\nstatic struct attribute *per_node_hstate_attrs[] = {\n\t&nr_hugepages_attr.attr,\n\t&free_hugepages_attr.attr,\n\t&surplus_hugepages_attr.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group per_node_hstate_attr_group = {\n\t.attrs = per_node_hstate_attrs,\n};\n\n/*\n * kobj_to_node_hstate - lookup global hstate for node device hstate attr kobj.\n * Returns node id via non-NULL nidp.\n */\nstatic struct hstate *kobj_to_node_hstate(struct kobject *kobj, int *nidp)\n{\n\tint nid;\n\n\tfor (nid = 0; nid < nr_node_ids; nid++) {\n\t\tstruct node_hstate *nhs = &node_hstates[nid];\n\t\tint i;\n\t\tfor (i = 0; i < HUGE_MAX_HSTATE; i++)\n\t\t\tif (nhs->hstate_kobjs[i] == kobj) {\n\t\t\t\tif (nidp)\n\t\t\t\t\t*nidp = nid;\n\t\t\t\treturn &hstates[i];\n\t\t\t}\n\t}\n\n\tBUG();\n\treturn NULL;\n}\n\n/*\n * Unregister hstate attributes from a single node device.\n * No-op if no hstate attributes attached.\n */\nstatic void hugetlb_unregister_node(struct node *node)\n{\n\tstruct hstate *h;\n\tstruct node_hstate *nhs = &node_hstates[node->dev.id];\n\n\tif (!nhs->hugepages_kobj)\n\t\treturn;\t\t/* no hstate attributes */\n\n\tfor_each_hstate(h) {\n\t\tint idx = hstate_index(h);\n\t\tif (nhs->hstate_kobjs[idx]) {\n\t\t\tkobject_put(nhs->hstate_kobjs[idx]);\n\t\t\tnhs->hstate_kobjs[idx] = NULL;\n\t\t}\n\t}\n\n\tkobject_put(nhs->hugepages_kobj);\n\tnhs->hugepages_kobj = NULL;\n}\n\n\n/*\n * Register hstate attributes for a single node device.\n * No-op if attributes already registered.\n */\nstatic void hugetlb_register_node(struct node *node)\n{\n\tstruct hstate *h;\n\tstruct node_hstate *nhs = &node_hstates[node->dev.id];\n\tint err;\n\n\tif (nhs->hugepages_kobj)\n\t\treturn;\t\t/* already allocated */\n\n\tnhs->hugepages_kobj = kobject_create_and_add(\"hugepages\",\n\t\t\t\t\t\t\t&node->dev.kobj);\n\tif (!nhs->hugepages_kobj)\n\t\treturn;\n\n\tfor_each_hstate(h) {\n\t\terr = hugetlb_sysfs_add_hstate(h, nhs->hugepages_kobj,\n\t\t\t\t\t\tnhs->hstate_kobjs,\n\t\t\t\t\t\t&per_node_hstate_attr_group);\n\t\tif (err) {\n\t\t\tpr_err(\"HugeTLB: Unable to add hstate %s for node %d\\n\",\n\t\t\t\th->name, node->dev.id);\n\t\t\thugetlb_unregister_node(node);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n/*\n * hugetlb init time:  register hstate attributes for all registered node\n * devices of nodes that have memory.  All on-line nodes should have\n * registered their associated device by this time.\n */\nstatic void __init hugetlb_register_all_nodes(void)\n{\n\tint nid;\n\n\tfor_each_node_state(nid, N_MEMORY) {\n\t\tstruct node *node = node_devices[nid];\n\t\tif (node->dev.id == nid)\n\t\t\thugetlb_register_node(node);\n\t}\n\n\t/*\n\t * Let the node device driver know we're here so it can\n\t * [un]register hstate attributes on node hotplug.\n\t */\n\tregister_hugetlbfs_with_node(hugetlb_register_node,\n\t\t\t\t     hugetlb_unregister_node);\n}\n#else\t/* !CONFIG_NUMA */\n\nstatic struct hstate *kobj_to_node_hstate(struct kobject *kobj, int *nidp)\n{\n\tBUG();\n\tif (nidp)\n\t\t*nidp = -1;\n\treturn NULL;\n}\n\nstatic void hugetlb_register_all_nodes(void) { }\n\n#endif\n\nstatic int __init hugetlb_init(void)\n{\n\tint i;\n\n\tif (!hugepages_supported()) {\n\t\tif (hugetlb_max_hstate || default_hstate_max_huge_pages)\n\t\t\tpr_warn(\"HugeTLB: huge pages not supported, ignoring associated command-line parameters\\n\");\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Make sure HPAGE_SIZE (HUGETLB_PAGE_ORDER) hstate exists.  Some\n\t * architectures depend on setup being done here.\n\t */\n\thugetlb_add_hstate(HUGETLB_PAGE_ORDER);\n\tif (!parsed_default_hugepagesz) {\n\t\t/*\n\t\t * If we did not parse a default huge page size, set\n\t\t * default_hstate_idx to HPAGE_SIZE hstate. And, if the\n\t\t * number of huge pages for this default size was implicitly\n\t\t * specified, set that here as well.\n\t\t * Note that the implicit setting will overwrite an explicit\n\t\t * setting.  A warning will be printed in this case.\n\t\t */\n\t\tdefault_hstate_idx = hstate_index(size_to_hstate(HPAGE_SIZE));\n\t\tif (default_hstate_max_huge_pages) {\n\t\t\tif (default_hstate.max_huge_pages) {\n\t\t\t\tchar buf[32];\n\n\t\t\t\tstring_get_size(huge_page_size(&default_hstate),\n\t\t\t\t\t1, STRING_UNITS_2, buf, 32);\n\t\t\t\tpr_warn(\"HugeTLB: Ignoring hugepages=%lu associated with %s page size\\n\",\n\t\t\t\t\tdefault_hstate.max_huge_pages, buf);\n\t\t\t\tpr_warn(\"HugeTLB: Using hugepages=%lu for number of default huge pages\\n\",\n\t\t\t\t\tdefault_hstate_max_huge_pages);\n\t\t\t}\n\t\t\tdefault_hstate.max_huge_pages =\n\t\t\t\tdefault_hstate_max_huge_pages;\n\t\t}\n\t}\n\n\thugetlb_cma_check();\n\thugetlb_init_hstates();\n\tgather_bootmem_prealloc();\n\treport_hugepages();\n\n\thugetlb_sysfs_init();\n\thugetlb_register_all_nodes();\n\thugetlb_cgroup_file_init();\n\n#ifdef CONFIG_SMP\n\tnum_fault_mutexes = roundup_pow_of_two(8 * num_possible_cpus());\n#else\n\tnum_fault_mutexes = 1;\n#endif\n\thugetlb_fault_mutex_table =\n\t\tkmalloc_array(num_fault_mutexes, sizeof(struct mutex),\n\t\t\t      GFP_KERNEL);\n\tBUG_ON(!hugetlb_fault_mutex_table);\n\n\tfor (i = 0; i < num_fault_mutexes; i++)\n\t\tmutex_init(&hugetlb_fault_mutex_table[i]);\n\treturn 0;\n}\nsubsys_initcall(hugetlb_init);\n\n/* Overwritten by architectures with more huge page sizes */\nbool __init __attribute((weak)) arch_hugetlb_valid_size(unsigned long size)\n{\n\treturn size == HPAGE_SIZE;\n}\n\nvoid __init hugetlb_add_hstate(unsigned int order)\n{\n\tstruct hstate *h;\n\tunsigned long i;\n\n\tif (size_to_hstate(PAGE_SIZE << order)) {\n\t\treturn;\n\t}\n\tBUG_ON(hugetlb_max_hstate >= HUGE_MAX_HSTATE);\n\tBUG_ON(order == 0);\n\th = &hstates[hugetlb_max_hstate++];\n\th->order = order;\n\th->mask = ~((1ULL << (order + PAGE_SHIFT)) - 1);\n\tfor (i = 0; i < MAX_NUMNODES; ++i)\n\t\tINIT_LIST_HEAD(&h->hugepage_freelists[i]);\n\tINIT_LIST_HEAD(&h->hugepage_activelist);\n\th->next_nid_to_alloc = first_memory_node;\n\th->next_nid_to_free = first_memory_node;\n\tsnprintf(h->name, HSTATE_NAME_LEN, \"hugepages-%lukB\",\n\t\t\t\t\thuge_page_size(h)/1024);\n\n\tparsed_hstate = h;\n}\n\n/*\n * hugepages command line processing\n * hugepages normally follows a valid hugepagsz or default_hugepagsz\n * specification.  If not, ignore the hugepages value.  hugepages can also\n * be the first huge page command line  option in which case it implicitly\n * specifies the number of huge pages for the default size.\n */\nstatic int __init hugepages_setup(char *s)\n{\n\tunsigned long *mhp;\n\tstatic unsigned long *last_mhp;\n\n\tif (!parsed_valid_hugepagesz) {\n\t\tpr_warn(\"HugeTLB: hugepages=%s does not follow a valid hugepagesz, ignoring\\n\", s);\n\t\tparsed_valid_hugepagesz = true;\n\t\treturn 0;\n\t}\n\n\t/*\n\t * !hugetlb_max_hstate means we haven't parsed a hugepagesz= parameter\n\t * yet, so this hugepages= parameter goes to the \"default hstate\".\n\t * Otherwise, it goes with the previously parsed hugepagesz or\n\t * default_hugepagesz.\n\t */\n\telse if (!hugetlb_max_hstate)\n\t\tmhp = &default_hstate_max_huge_pages;\n\telse\n\t\tmhp = &parsed_hstate->max_huge_pages;\n\n\tif (mhp == last_mhp) {\n\t\tpr_warn(\"HugeTLB: hugepages= specified twice without interleaving hugepagesz=, ignoring hugepages=%s\\n\", s);\n\t\treturn 0;\n\t}\n\n\tif (sscanf(s, \"%lu\", mhp) <= 0)\n\t\t*mhp = 0;\n\n\t/*\n\t * Global state is always initialized later in hugetlb_init.\n\t * But we need to allocate >= MAX_ORDER hstates here early to still\n\t * use the bootmem allocator.\n\t */\n\tif (hugetlb_max_hstate && parsed_hstate->order >= MAX_ORDER)\n\t\thugetlb_hstate_alloc_pages(parsed_hstate);\n\n\tlast_mhp = mhp;\n\n\treturn 1;\n}\n__setup(\"hugepages=\", hugepages_setup);\n\n/*\n * hugepagesz command line processing\n * A specific huge page size can only be specified once with hugepagesz.\n * hugepagesz is followed by hugepages on the command line.  The global\n * variable 'parsed_valid_hugepagesz' is used to determine if prior\n * hugepagesz argument was valid.\n */\nstatic int __init hugepagesz_setup(char *s)\n{\n\tunsigned long size;\n\tstruct hstate *h;\n\n\tparsed_valid_hugepagesz = false;\n\tsize = (unsigned long)memparse(s, NULL);\n\n\tif (!arch_hugetlb_valid_size(size)) {\n\t\tpr_err(\"HugeTLB: unsupported hugepagesz=%s\\n\", s);\n\t\treturn 0;\n\t}\n\n\th = size_to_hstate(size);\n\tif (h) {\n\t\t/*\n\t\t * hstate for this size already exists.  This is normally\n\t\t * an error, but is allowed if the existing hstate is the\n\t\t * default hstate.  More specifically, it is only allowed if\n\t\t * the number of huge pages for the default hstate was not\n\t\t * previously specified.\n\t\t */\n\t\tif (!parsed_default_hugepagesz ||  h != &default_hstate ||\n\t\t    default_hstate.max_huge_pages) {\n\t\t\tpr_warn(\"HugeTLB: hugepagesz=%s specified twice, ignoring\\n\", s);\n\t\t\treturn 0;\n\t\t}\n\n\t\t/*\n\t\t * No need to call hugetlb_add_hstate() as hstate already\n\t\t * exists.  But, do set parsed_hstate so that a following\n\t\t * hugepages= parameter will be applied to this hstate.\n\t\t */\n\t\tparsed_hstate = h;\n\t\tparsed_valid_hugepagesz = true;\n\t\treturn 1;\n\t}\n\n\thugetlb_add_hstate(ilog2(size) - PAGE_SHIFT);\n\tparsed_valid_hugepagesz = true;\n\treturn 1;\n}\n__setup(\"hugepagesz=\", hugepagesz_setup);\n\n/*\n * default_hugepagesz command line input\n * Only one instance of default_hugepagesz allowed on command line.\n */\nstatic int __init default_hugepagesz_setup(char *s)\n{\n\tunsigned long size;\n\n\tparsed_valid_hugepagesz = false;\n\tif (parsed_default_hugepagesz) {\n\t\tpr_err(\"HugeTLB: default_hugepagesz previously specified, ignoring %s\\n\", s);\n\t\treturn 0;\n\t}\n\n\tsize = (unsigned long)memparse(s, NULL);\n\n\tif (!arch_hugetlb_valid_size(size)) {\n\t\tpr_err(\"HugeTLB: unsupported default_hugepagesz=%s\\n\", s);\n\t\treturn 0;\n\t}\n\n\thugetlb_add_hstate(ilog2(size) - PAGE_SHIFT);\n\tparsed_valid_hugepagesz = true;\n\tparsed_default_hugepagesz = true;\n\tdefault_hstate_idx = hstate_index(size_to_hstate(size));\n\n\t/*\n\t * The number of default huge pages (for this size) could have been\n\t * specified as the first hugetlb parameter: hugepages=X.  If so,\n\t * then default_hstate_max_huge_pages is set.  If the default huge\n\t * page size is gigantic (>= MAX_ORDER), then the pages must be\n\t * allocated here from bootmem allocator.\n\t */\n\tif (default_hstate_max_huge_pages) {\n\t\tdefault_hstate.max_huge_pages = default_hstate_max_huge_pages;\n\t\tif (hstate_is_gigantic(&default_hstate))\n\t\t\thugetlb_hstate_alloc_pages(&default_hstate);\n\t\tdefault_hstate_max_huge_pages = 0;\n\t}\n\n\treturn 1;\n}\n__setup(\"default_hugepagesz=\", default_hugepagesz_setup);\n\nstatic unsigned int allowed_mems_nr(struct hstate *h)\n{\n\tint node;\n\tunsigned int nr = 0;\n\tnodemask_t *mpol_allowed;\n\tunsigned int *array = h->free_huge_pages_node;\n\tgfp_t gfp_mask = htlb_alloc_mask(h);\n\n\tmpol_allowed = policy_nodemask_current(gfp_mask);\n\n\tfor_each_node_mask(node, cpuset_current_mems_allowed) {\n\t\tif (!mpol_allowed ||\n\t\t    (mpol_allowed && node_isset(node, *mpol_allowed)))\n\t\t\tnr += array[node];\n\t}\n\n\treturn nr;\n}\n\n#ifdef CONFIG_SYSCTL\nstatic int proc_hugetlb_doulongvec_minmax(struct ctl_table *table, int write,\n\t\t\t\t\t  void *buffer, size_t *length,\n\t\t\t\t\t  loff_t *ppos, unsigned long *out)\n{\n\tstruct ctl_table dup_table;\n\n\t/*\n\t * In order to avoid races with __do_proc_doulongvec_minmax(), we\n\t * can duplicate the @table and alter the duplicate of it.\n\t */\n\tdup_table = *table;\n\tdup_table.data = out;\n\n\treturn proc_doulongvec_minmax(&dup_table, write, buffer, length, ppos);\n}\n\nstatic int hugetlb_sysctl_handler_common(bool obey_mempolicy,\n\t\t\t struct ctl_table *table, int write,\n\t\t\t void *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp = h->max_huge_pages;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,\n\t\t\t\t\t     &tmp);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write)\n\t\tret = __nr_hugepages_store_common(obey_mempolicy, h,\n\t\t\t\t\t\t  NUMA_NO_NODE, tmp, *length);\nout:\n\treturn ret;\n}\n\nint hugetlb_sysctl_handler(struct ctl_table *table, int write,\n\t\t\t  void *buffer, size_t *length, loff_t *ppos)\n{\n\n\treturn hugetlb_sysctl_handler_common(false, table, write,\n\t\t\t\t\t\t\tbuffer, length, ppos);\n}\n\n#ifdef CONFIG_NUMA\nint hugetlb_mempolicy_sysctl_handler(struct ctl_table *table, int write,\n\t\t\t  void *buffer, size_t *length, loff_t *ppos)\n{\n\treturn hugetlb_sysctl_handler_common(true, table, write,\n\t\t\t\t\t\t\tbuffer, length, ppos);\n}\n#endif /* CONFIG_NUMA */\n\nint hugetlb_overcommit_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\ttmp = h->nr_overcommit_huge_pages;\n\n\tif (write && hstate_is_gigantic(h))\n\t\treturn -EINVAL;\n\n\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,\n\t\t\t\t\t     &tmp);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write) {\n\t\tspin_lock(&hugetlb_lock);\n\t\th->nr_overcommit_huge_pages = tmp;\n\t\tspin_unlock(&hugetlb_lock);\n\t}\nout:\n\treturn ret;\n}\n\n#endif /* CONFIG_SYSCTL */\n\nvoid hugetlb_report_meminfo(struct seq_file *m)\n{\n\tstruct hstate *h;\n\tunsigned long total = 0;\n\n\tif (!hugepages_supported())\n\t\treturn;\n\n\tfor_each_hstate(h) {\n\t\tunsigned long count = h->nr_huge_pages;\n\n\t\ttotal += (PAGE_SIZE << huge_page_order(h)) * count;\n\n\t\tif (h == &default_hstate)\n\t\t\tseq_printf(m,\n\t\t\t\t   \"HugePages_Total:   %5lu\\n\"\n\t\t\t\t   \"HugePages_Free:    %5lu\\n\"\n\t\t\t\t   \"HugePages_Rsvd:    %5lu\\n\"\n\t\t\t\t   \"HugePages_Surp:    %5lu\\n\"\n\t\t\t\t   \"Hugepagesize:   %8lu kB\\n\",\n\t\t\t\t   count,\n\t\t\t\t   h->free_huge_pages,\n\t\t\t\t   h->resv_huge_pages,\n\t\t\t\t   h->surplus_huge_pages,\n\t\t\t\t   (PAGE_SIZE << huge_page_order(h)) / 1024);\n\t}\n\n\tseq_printf(m, \"Hugetlb:        %8lu kB\\n\", total / 1024);\n}\n\nint hugetlb_report_node_meminfo(char *buf, int len, int nid)\n{\n\tstruct hstate *h = &default_hstate;\n\n\tif (!hugepages_supported())\n\t\treturn 0;\n\n\treturn sysfs_emit_at(buf, len,\n\t\t\t     \"Node %d HugePages_Total: %5u\\n\"\n\t\t\t     \"Node %d HugePages_Free:  %5u\\n\"\n\t\t\t     \"Node %d HugePages_Surp:  %5u\\n\",\n\t\t\t     nid, h->nr_huge_pages_node[nid],\n\t\t\t     nid, h->free_huge_pages_node[nid],\n\t\t\t     nid, h->surplus_huge_pages_node[nid]);\n}\n\nvoid hugetlb_show_meminfo(void)\n{\n\tstruct hstate *h;\n\tint nid;\n\n\tif (!hugepages_supported())\n\t\treturn;\n\n\tfor_each_node_state(nid, N_MEMORY)\n\t\tfor_each_hstate(h)\n\t\t\tpr_info(\"Node %d hugepages_total=%u hugepages_free=%u hugepages_surp=%u hugepages_size=%lukB\\n\",\n\t\t\t\tnid,\n\t\t\t\th->nr_huge_pages_node[nid],\n\t\t\t\th->free_huge_pages_node[nid],\n\t\t\t\th->surplus_huge_pages_node[nid],\n\t\t\t\t1UL << (huge_page_order(h) + PAGE_SHIFT - 10));\n}\n\nvoid hugetlb_report_usage(struct seq_file *m, struct mm_struct *mm)\n{\n\tseq_printf(m, \"HugetlbPages:\\t%8lu kB\\n\",\n\t\t   atomic_long_read(&mm->hugetlb_usage) << (PAGE_SHIFT - 10));\n}\n\n/* Return the number pages of memory we physically have, in PAGE_SIZE units. */\nunsigned long hugetlb_total_pages(void)\n{\n\tstruct hstate *h;\n\tunsigned long nr_total_pages = 0;\n\n\tfor_each_hstate(h)\n\t\tnr_total_pages += h->nr_huge_pages * pages_per_huge_page(h);\n\treturn nr_total_pages;\n}\n\nstatic int hugetlb_acct_memory(struct hstate *h, long delta)\n{\n\tint ret = -ENOMEM;\n\n\tspin_lock(&hugetlb_lock);\n\t/*\n\t * When cpuset is configured, it breaks the strict hugetlb page\n\t * reservation as the accounting is done on a global variable. Such\n\t * reservation is completely rubbish in the presence of cpuset because\n\t * the reservation is not checked against page availability for the\n\t * current cpuset. Application can still potentially OOM'ed by kernel\n\t * with lack of free htlb page in cpuset that the task is in.\n\t * Attempt to enforce strict accounting with cpuset is almost\n\t * impossible (or too ugly) because cpuset is too fluid that\n\t * task or memory node can be dynamically moved between cpusets.\n\t *\n\t * The change of semantics for shared hugetlb mapping with cpuset is\n\t * undesirable. However, in order to preserve some of the semantics,\n\t * we fall back to check against current free page availability as\n\t * a best attempt and hopefully to minimize the impact of changing\n\t * semantics that cpuset has.\n\t *\n\t * Apart from cpuset, we also have memory policy mechanism that\n\t * also determines from which node the kernel will allocate memory\n\t * in a NUMA system. So similar to cpuset, we also should consider\n\t * the memory policy of the current task. Similar to the description\n\t * above.\n\t */\n\tif (delta > 0) {\n\t\tif (gather_surplus_pages(h, delta) < 0)\n\t\t\tgoto out;\n\n\t\tif (delta > allowed_mems_nr(h)) {\n\t\t\treturn_unused_surplus_pages(h, delta);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = 0;\n\tif (delta < 0)\n\t\treturn_unused_surplus_pages(h, (unsigned long) -delta);\n\nout:\n\tspin_unlock(&hugetlb_lock);\n\treturn ret;\n}\n\nstatic void hugetlb_vm_op_open(struct vm_area_struct *vma)\n{\n\tstruct resv_map *resv = vma_resv_map(vma);\n\n\t/*\n\t * This new VMA should share its siblings reservation map if present.\n\t * The VMA will only ever have a valid reservation map pointer where\n\t * it is being copied for another still existing VMA.  As that VMA\n\t * has a reference to the reservation map it cannot disappear until\n\t * after this open call completes.  It is therefore safe to take a\n\t * new reference here without additional locking.\n\t */\n\tif (resv && is_vma_resv_set(vma, HPAGE_RESV_OWNER))\n\t\tkref_get(&resv->refs);\n}\n\nstatic void hugetlb_vm_op_close(struct vm_area_struct *vma)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct resv_map *resv = vma_resv_map(vma);\n\tstruct hugepage_subpool *spool = subpool_vma(vma);\n\tunsigned long reserve, start, end;\n\tlong gbl_reserve;\n\n\tif (!resv || !is_vma_resv_set(vma, HPAGE_RESV_OWNER))\n\t\treturn;\n\n\tstart = vma_hugecache_offset(h, vma, vma->vm_start);\n\tend = vma_hugecache_offset(h, vma, vma->vm_end);\n\n\treserve = (end - start) - region_count(resv, start, end);\n\thugetlb_cgroup_uncharge_counter(resv, start, end);\n\tif (reserve) {\n\t\t/*\n\t\t * Decrement reserve counts.  The global reserve count may be\n\t\t * adjusted if the subpool has a minimum size.\n\t\t */\n\t\tgbl_reserve = hugepage_subpool_put_pages(spool, reserve);\n\t\thugetlb_acct_memory(h, -gbl_reserve);\n\t}\n\n\tkref_put(&resv->refs, resv_map_release);\n}\n\nstatic int hugetlb_vm_op_split(struct vm_area_struct *vma, unsigned long addr)\n{\n\tif (addr & ~(huge_page_mask(hstate_vma(vma))))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic unsigned long hugetlb_vm_op_pagesize(struct vm_area_struct *vma)\n{\n\tstruct hstate *hstate = hstate_vma(vma);\n\n\treturn 1UL << huge_page_shift(hstate);\n}\n\n/*\n * We cannot handle pagefaults against hugetlb pages at all.  They cause\n * handle_mm_fault() to try to instantiate regular-sized pages in the\n * hugegpage VMA.  do_page_fault() is supposed to trap this, so BUG is we get\n * this far.\n */\nstatic vm_fault_t hugetlb_vm_op_fault(struct vm_fault *vmf)\n{\n\tBUG();\n\treturn 0;\n}\n\n/*\n * When a new function is introduced to vm_operations_struct and added\n * to hugetlb_vm_ops, please consider adding the function to shm_vm_ops.\n * This is because under System V memory model, mappings created via\n * shmget/shmat with \"huge page\" specified are backed by hugetlbfs files,\n * their original vm_ops are overwritten with shm_vm_ops.\n */\nconst struct vm_operations_struct hugetlb_vm_ops = {\n\t.fault = hugetlb_vm_op_fault,\n\t.open = hugetlb_vm_op_open,\n\t.close = hugetlb_vm_op_close,\n\t.may_split = hugetlb_vm_op_split,\n\t.pagesize = hugetlb_vm_op_pagesize,\n};\n\nstatic pte_t make_huge_pte(struct vm_area_struct *vma, struct page *page,\n\t\t\t\tint writable)\n{\n\tpte_t entry;\n\n\tif (writable) {\n\t\tentry = huge_pte_mkwrite(huge_pte_mkdirty(mk_huge_pte(page,\n\t\t\t\t\t vma->vm_page_prot)));\n\t} else {\n\t\tentry = huge_pte_wrprotect(mk_huge_pte(page,\n\t\t\t\t\t   vma->vm_page_prot));\n\t}\n\tentry = pte_mkyoung(entry);\n\tentry = pte_mkhuge(entry);\n\tentry = arch_make_huge_pte(entry, vma, page, writable);\n\n\treturn entry;\n}\n\nstatic void set_huge_ptep_writable(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long address, pte_t *ptep)\n{\n\tpte_t entry;\n\n\tentry = huge_pte_mkwrite(huge_pte_mkdirty(huge_ptep_get(ptep)));\n\tif (huge_ptep_set_access_flags(vma, address, ptep, entry, 1))\n\t\tupdate_mmu_cache(vma, address, ptep);\n}\n\nbool is_hugetlb_entry_migration(pte_t pte)\n{\n\tswp_entry_t swp;\n\n\tif (huge_pte_none(pte) || pte_present(pte))\n\t\treturn false;\n\tswp = pte_to_swp_entry(pte);\n\tif (is_migration_entry(swp))\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nstatic bool is_hugetlb_entry_hwpoisoned(pte_t pte)\n{\n\tswp_entry_t swp;\n\n\tif (huge_pte_none(pte) || pte_present(pte))\n\t\treturn false;\n\tswp = pte_to_swp_entry(pte);\n\tif (is_hwpoison_entry(swp))\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nint copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,\n\t\t\t    struct vm_area_struct *vma)\n{\n\tpte_t *src_pte, *dst_pte, entry, dst_entry;\n\tstruct page *ptepage;\n\tunsigned long addr;\n\tint cow;\n\tstruct hstate *h = hstate_vma(vma);\n\tunsigned long sz = huge_page_size(h);\n\tstruct address_space *mapping = vma->vm_file->f_mapping;\n\tstruct mmu_notifier_range range;\n\tint ret = 0;\n\n\tcow = (vma->vm_flags & (VM_SHARED | VM_MAYWRITE)) == VM_MAYWRITE;\n\n\tif (cow) {\n\t\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, src,\n\t\t\t\t\tvma->vm_start,\n\t\t\t\t\tvma->vm_end);\n\t\tmmu_notifier_invalidate_range_start(&range);\n\t} else {\n\t\t/*\n\t\t * For shared mappings i_mmap_rwsem must be held to call\n\t\t * huge_pte_alloc, otherwise the returned ptep could go\n\t\t * away if part of a shared pmd and another thread calls\n\t\t * huge_pmd_unshare.\n\t\t */\n\t\ti_mmap_lock_read(mapping);\n\t}\n\n\tfor (addr = vma->vm_start; addr < vma->vm_end; addr += sz) {\n\t\tspinlock_t *src_ptl, *dst_ptl;\n\t\tsrc_pte = huge_pte_offset(src, addr, sz);\n\t\tif (!src_pte)\n\t\t\tcontinue;\n\t\tdst_pte = huge_pte_alloc(dst, addr, sz);\n\t\tif (!dst_pte) {\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * If the pagetables are shared don't copy or take references.\n\t\t * dst_pte == src_pte is the common case of src/dest sharing.\n\t\t *\n\t\t * However, src could have 'unshared' and dst shares with\n\t\t * another vma.  If dst_pte !none, this implies sharing.\n\t\t * Check here before taking page table lock, and once again\n\t\t * after taking the lock below.\n\t\t */\n\t\tdst_entry = huge_ptep_get(dst_pte);\n\t\tif ((dst_pte == src_pte) || !huge_pte_none(dst_entry))\n\t\t\tcontinue;\n\n\t\tdst_ptl = huge_pte_lock(h, dst, dst_pte);\n\t\tsrc_ptl = huge_pte_lockptr(h, src, src_pte);\n\t\tspin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);\n\t\tentry = huge_ptep_get(src_pte);\n\t\tdst_entry = huge_ptep_get(dst_pte);\n\t\tif (huge_pte_none(entry) || !huge_pte_none(dst_entry)) {\n\t\t\t/*\n\t\t\t * Skip if src entry none.  Also, skip in the\n\t\t\t * unlikely case dst entry !none as this implies\n\t\t\t * sharing with another vma.\n\t\t\t */\n\t\t\t;\n\t\t} else if (unlikely(is_hugetlb_entry_migration(entry) ||\n\t\t\t\t    is_hugetlb_entry_hwpoisoned(entry))) {\n\t\t\tswp_entry_t swp_entry = pte_to_swp_entry(entry);\n\n\t\t\tif (is_write_migration_entry(swp_entry) && cow) {\n\t\t\t\t/*\n\t\t\t\t * COW mappings require pages in both\n\t\t\t\t * parent and child to be set to read.\n\t\t\t\t */\n\t\t\t\tmake_migration_entry_read(&swp_entry);\n\t\t\t\tentry = swp_entry_to_pte(swp_entry);\n\t\t\t\tset_huge_swap_pte_at(src, addr, src_pte,\n\t\t\t\t\t\t     entry, sz);\n\t\t\t}\n\t\t\tset_huge_swap_pte_at(dst, addr, dst_pte, entry, sz);\n\t\t} else {\n\t\t\tif (cow) {\n\t\t\t\t/*\n\t\t\t\t * No need to notify as we are downgrading page\n\t\t\t\t * table protection not changing it to point\n\t\t\t\t * to a new page.\n\t\t\t\t *\n\t\t\t\t * See Documentation/vm/mmu_notifier.rst\n\t\t\t\t */\n\t\t\t\thuge_ptep_set_wrprotect(src, addr, src_pte);\n\t\t\t}\n\t\t\tentry = huge_ptep_get(src_pte);\n\t\t\tptepage = pte_page(entry);\n\t\t\tget_page(ptepage);\n\t\t\tpage_dup_rmap(ptepage, true);\n\t\t\tset_huge_pte_at(dst, addr, dst_pte, entry);\n\t\t\thugetlb_count_add(pages_per_huge_page(h), dst);\n\t\t}\n\t\tspin_unlock(src_ptl);\n\t\tspin_unlock(dst_ptl);\n\t}\n\n\tif (cow)\n\t\tmmu_notifier_invalidate_range_end(&range);\n\telse\n\t\ti_mmap_unlock_read(mapping);\n\n\treturn ret;\n}\n\nvoid __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct *vma,\n\t\t\t    unsigned long start, unsigned long end,\n\t\t\t    struct page *ref_page)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long address;\n\tpte_t *ptep;\n\tpte_t pte;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\tstruct hstate *h = hstate_vma(vma);\n\tunsigned long sz = huge_page_size(h);\n\tstruct mmu_notifier_range range;\n\n\tWARN_ON(!is_vm_hugetlb_page(vma));\n\tBUG_ON(start & ~huge_page_mask(h));\n\tBUG_ON(end & ~huge_page_mask(h));\n\n\t/*\n\t * This is a hugetlb vma, all the pte entries should point\n\t * to huge page.\n\t */\n\ttlb_change_page_size(tlb, sz);\n\ttlb_start_vma(tlb, vma);\n\n\t/*\n\t * If sharing possible, alert mmu notifiers of worst case.\n\t */\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_UNMAP, 0, vma, mm, start,\n\t\t\t\tend);\n\tadjust_range_if_pmd_sharing_possible(vma, &range.start, &range.end);\n\tmmu_notifier_invalidate_range_start(&range);\n\taddress = start;\n\tfor (; address < end; address += sz) {\n\t\tptep = huge_pte_offset(mm, address, sz);\n\t\tif (!ptep)\n\t\t\tcontinue;\n\n\t\tptl = huge_pte_lock(h, mm, ptep);\n\t\tif (huge_pmd_unshare(mm, vma, &address, ptep)) {\n\t\t\tspin_unlock(ptl);\n\t\t\t/*\n\t\t\t * We just unmapped a page of PMDs by clearing a PUD.\n\t\t\t * The caller's TLB flush range should cover this area.\n\t\t\t */\n\t\t\tcontinue;\n\t\t}\n\n\t\tpte = huge_ptep_get(ptep);\n\t\tif (huge_pte_none(pte)) {\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * Migrating hugepage or HWPoisoned hugepage is already\n\t\t * unmapped and its refcount is dropped, so just clear pte here.\n\t\t */\n\t\tif (unlikely(!pte_present(pte))) {\n\t\t\thuge_pte_clear(mm, address, ptep, sz);\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\n\t\tpage = pte_page(pte);\n\t\t/*\n\t\t * If a reference page is supplied, it is because a specific\n\t\t * page is being unmapped, not a range. Ensure the page we\n\t\t * are about to unmap is the actual page of interest.\n\t\t */\n\t\tif (ref_page) {\n\t\t\tif (page != ref_page) {\n\t\t\t\tspin_unlock(ptl);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Mark the VMA as having unmapped its page so that\n\t\t\t * future faults in this VMA will fail rather than\n\t\t\t * looking like data was lost\n\t\t\t */\n\t\t\tset_vma_resv_flags(vma, HPAGE_RESV_UNMAPPED);\n\t\t}\n\n\t\tpte = huge_ptep_get_and_clear(mm, address, ptep);\n\t\ttlb_remove_huge_tlb_entry(h, tlb, ptep, address);\n\t\tif (huge_pte_dirty(pte))\n\t\t\tset_page_dirty(page);\n\n\t\thugetlb_count_sub(pages_per_huge_page(h), mm);\n\t\tpage_remove_rmap(page, true);\n\n\t\tspin_unlock(ptl);\n\t\ttlb_remove_page_size(tlb, page, huge_page_size(h));\n\t\t/*\n\t\t * Bail out after unmapping reference page if supplied\n\t\t */\n\t\tif (ref_page)\n\t\t\tbreak;\n\t}\n\tmmu_notifier_invalidate_range_end(&range);\n\ttlb_end_vma(tlb, vma);\n}\n\nvoid __unmap_hugepage_range_final(struct mmu_gather *tlb,\n\t\t\t  struct vm_area_struct *vma, unsigned long start,\n\t\t\t  unsigned long end, struct page *ref_page)\n{\n\t__unmap_hugepage_range(tlb, vma, start, end, ref_page);\n\n\t/*\n\t * Clear this flag so that x86's huge_pmd_share page_table_shareable\n\t * test will fail on a vma being torn down, and not grab a page table\n\t * on its way out.  We're lucky that the flag has such an appropriate\n\t * name, and can in fact be safely cleared here. We could clear it\n\t * before the __unmap_hugepage_range above, but all that's necessary\n\t * is to clear it before releasing the i_mmap_rwsem. This works\n\t * because in the context this is called, the VMA is about to be\n\t * destroyed and the i_mmap_rwsem is held.\n\t */\n\tvma->vm_flags &= ~VM_MAYSHARE;\n}\n\nvoid unmap_hugepage_range(struct vm_area_struct *vma, unsigned long start,\n\t\t\t  unsigned long end, struct page *ref_page)\n{\n\tstruct mm_struct *mm;\n\tstruct mmu_gather tlb;\n\tunsigned long tlb_start = start;\n\tunsigned long tlb_end = end;\n\n\t/*\n\t * If shared PMDs were possibly used within this vma range, adjust\n\t * start/end for worst case tlb flushing.\n\t * Note that we can not be sure if PMDs are shared until we try to\n\t * unmap pages.  However, we want to make sure TLB flushing covers\n\t * the largest possible range.\n\t */\n\tadjust_range_if_pmd_sharing_possible(vma, &tlb_start, &tlb_end);\n\n\tmm = vma->vm_mm;\n\n\ttlb_gather_mmu(&tlb, mm, tlb_start, tlb_end);\n\t__unmap_hugepage_range(&tlb, vma, start, end, ref_page);\n\ttlb_finish_mmu(&tlb, tlb_start, tlb_end);\n}\n\n/*\n * This is called when the original mapper is failing to COW a MAP_PRIVATE\n * mappping it owns the reserve page for. The intention is to unmap the page\n * from other VMAs and let the children be SIGKILLed if they are faulting the\n * same region.\n */\nstatic void unmap_ref_private(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t      struct page *page, unsigned long address)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct vm_area_struct *iter_vma;\n\tstruct address_space *mapping;\n\tpgoff_t pgoff;\n\n\t/*\n\t * vm_pgoff is in PAGE_SIZE units, hence the different calculation\n\t * from page cache lookup which is in HPAGE_SIZE units.\n\t */\n\taddress = address & huge_page_mask(h);\n\tpgoff = ((address - vma->vm_start) >> PAGE_SHIFT) +\n\t\t\tvma->vm_pgoff;\n\tmapping = vma->vm_file->f_mapping;\n\n\t/*\n\t * Take the mapping lock for the duration of the table walk. As\n\t * this mapping should be shared between all the VMAs,\n\t * __unmap_hugepage_range() is called as the lock is already held\n\t */\n\ti_mmap_lock_write(mapping);\n\tvma_interval_tree_foreach(iter_vma, &mapping->i_mmap, pgoff, pgoff) {\n\t\t/* Do not unmap the current VMA */\n\t\tif (iter_vma == vma)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Shared VMAs have their own reserves and do not affect\n\t\t * MAP_PRIVATE accounting but it is possible that a shared\n\t\t * VMA is using the same page so check and skip such VMAs.\n\t\t */\n\t\tif (iter_vma->vm_flags & VM_MAYSHARE)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Unmap the page from other VMAs without their own reserves.\n\t\t * They get marked to be SIGKILLed if they fault in these\n\t\t * areas. This is because a future no-page fault on this VMA\n\t\t * could insert a zeroed page instead of the data existing\n\t\t * from the time of fork. This would look like data corruption\n\t\t */\n\t\tif (!is_vma_resv_set(iter_vma, HPAGE_RESV_OWNER))\n\t\t\tunmap_hugepage_range(iter_vma, address,\n\t\t\t\t\t     address + huge_page_size(h), page);\n\t}\n\ti_mmap_unlock_write(mapping);\n}\n\n/*\n * Hugetlb_cow() should be called with page lock of the original hugepage held.\n * Called with hugetlb_instantiation_mutex held and pte_page locked so we\n * cannot race with other handlers or page migration.\n * Keep the pte_same checks anyway to make transition from the mutex easier.\n */\nstatic vm_fault_t hugetlb_cow(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t       unsigned long address, pte_t *ptep,\n\t\t       struct page *pagecache_page, spinlock_t *ptl)\n{\n\tpte_t pte;\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct page *old_page, *new_page;\n\tint outside_reserve = 0;\n\tvm_fault_t ret = 0;\n\tunsigned long haddr = address & huge_page_mask(h);\n\tstruct mmu_notifier_range range;\n\n\tpte = huge_ptep_get(ptep);\n\told_page = pte_page(pte);\n\nretry_avoidcopy:\n\t/* If no-one else is actually using this page, avoid the copy\n\t * and just make the page writable */\n\tif (page_mapcount(old_page) == 1 && PageAnon(old_page)) {\n\t\tpage_move_anon_rmap(old_page, vma);\n\t\tset_huge_ptep_writable(vma, haddr, ptep);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * If the process that created a MAP_PRIVATE mapping is about to\n\t * perform a COW due to a shared page count, attempt to satisfy\n\t * the allocation without using the existing reserves. The pagecache\n\t * page is used to determine if the reserve at this address was\n\t * consumed or not. If reserves were used, a partial faulted mapping\n\t * at the time of fork() could consume its reserves on COW instead\n\t * of the full address range.\n\t */\n\tif (is_vma_resv_set(vma, HPAGE_RESV_OWNER) &&\n\t\t\told_page != pagecache_page)\n\t\toutside_reserve = 1;\n\n\tget_page(old_page);\n\n\t/*\n\t * Drop page table lock as buddy allocator may be called. It will\n\t * be acquired again before returning to the caller, as expected.\n\t */\n\tspin_unlock(ptl);\n\tnew_page = alloc_huge_page(vma, haddr, outside_reserve);\n\n\tif (IS_ERR(new_page)) {\n\t\t/*\n\t\t * If a process owning a MAP_PRIVATE mapping fails to COW,\n\t\t * it is due to references held by a child and an insufficient\n\t\t * huge page pool. To guarantee the original mappers\n\t\t * reliability, unmap the page from child processes. The child\n\t\t * may get SIGKILLed if it later faults.\n\t\t */\n\t\tif (outside_reserve) {\n\t\t\tput_page(old_page);\n\t\t\tBUG_ON(huge_pte_none(pte));\n\t\t\tunmap_ref_private(mm, vma, old_page, haddr);\n\t\t\tBUG_ON(huge_pte_none(pte));\n\t\t\tspin_lock(ptl);\n\t\t\tptep = huge_pte_offset(mm, haddr, huge_page_size(h));\n\t\t\tif (likely(ptep &&\n\t\t\t\t   pte_same(huge_ptep_get(ptep), pte)))\n\t\t\t\tgoto retry_avoidcopy;\n\t\t\t/*\n\t\t\t * race occurs while re-acquiring page table\n\t\t\t * lock, and our job is done.\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\n\t\tret = vmf_error(PTR_ERR(new_page));\n\t\tgoto out_release_old;\n\t}\n\n\t/*\n\t * When the original hugepage is shared one, it does not have\n\t * anon_vma prepared.\n\t */\n\tif (unlikely(anon_vma_prepare(vma))) {\n\t\tret = VM_FAULT_OOM;\n\t\tgoto out_release_all;\n\t}\n\n\tcopy_user_huge_page(new_page, old_page, address, vma,\n\t\t\t    pages_per_huge_page(h));\n\t__SetPageUptodate(new_page);\n\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, mm, haddr,\n\t\t\t\thaddr + huge_page_size(h));\n\tmmu_notifier_invalidate_range_start(&range);\n\n\t/*\n\t * Retake the page table lock to check for racing updates\n\t * before the page tables are altered\n\t */\n\tspin_lock(ptl);\n\tptep = huge_pte_offset(mm, haddr, huge_page_size(h));\n\tif (likely(ptep && pte_same(huge_ptep_get(ptep), pte))) {\n\t\tClearPagePrivate(new_page);\n\n\t\t/* Break COW */\n\t\thuge_ptep_clear_flush(vma, haddr, ptep);\n\t\tmmu_notifier_invalidate_range(mm, range.start, range.end);\n\t\tset_huge_pte_at(mm, haddr, ptep,\n\t\t\t\tmake_huge_pte(vma, new_page, 1));\n\t\tpage_remove_rmap(old_page, true);\n\t\thugepage_add_new_anon_rmap(new_page, vma, haddr);\n\t\tset_page_huge_active(new_page);\n\t\t/* Make the old page be freed below */\n\t\tnew_page = old_page;\n\t}\n\tspin_unlock(ptl);\n\tmmu_notifier_invalidate_range_end(&range);\nout_release_all:\n\trestore_reserve_on_error(h, vma, haddr, new_page);\n\tput_page(new_page);\nout_release_old:\n\tput_page(old_page);\n\n\tspin_lock(ptl); /* Caller expects lock to be held */\n\treturn ret;\n}\n\n/* Return the pagecache page at a given address within a VMA */\nstatic struct page *hugetlbfs_pagecache_page(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tstruct address_space *mapping;\n\tpgoff_t idx;\n\n\tmapping = vma->vm_file->f_mapping;\n\tidx = vma_hugecache_offset(h, vma, address);\n\n\treturn find_lock_page(mapping, idx);\n}\n\n/*\n * Return whether there is a pagecache page to back given address within VMA.\n * Caller follow_hugetlb_page() holds page_table_lock so we cannot lock_page.\n */\nstatic bool hugetlbfs_pagecache_present(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tstruct address_space *mapping;\n\tpgoff_t idx;\n\tstruct page *page;\n\n\tmapping = vma->vm_file->f_mapping;\n\tidx = vma_hugecache_offset(h, vma, address);\n\n\tpage = find_get_page(mapping, idx);\n\tif (page)\n\t\tput_page(page);\n\treturn page != NULL;\n}\n\nint huge_add_to_page_cache(struct page *page, struct address_space *mapping,\n\t\t\t   pgoff_t idx)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct hstate *h = hstate_inode(inode);\n\tint err = add_to_page_cache(page, mapping, idx, GFP_KERNEL);\n\n\tif (err)\n\t\treturn err;\n\tClearPagePrivate(page);\n\n\t/*\n\t * set page dirty so that it will not be removed from cache/file\n\t * by non-hugetlbfs specific code paths.\n\t */\n\tset_page_dirty(page);\n\n\tspin_lock(&inode->i_lock);\n\tinode->i_blocks += blocks_per_huge_page(h);\n\tspin_unlock(&inode->i_lock);\n\treturn 0;\n}\n\nstatic vm_fault_t hugetlb_no_page(struct mm_struct *mm,\n\t\t\tstruct vm_area_struct *vma,\n\t\t\tstruct address_space *mapping, pgoff_t idx,\n\t\t\tunsigned long address, pte_t *ptep, unsigned int flags)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tvm_fault_t ret = VM_FAULT_SIGBUS;\n\tint anon_rmap = 0;\n\tunsigned long size;\n\tstruct page *page;\n\tpte_t new_pte;\n\tspinlock_t *ptl;\n\tunsigned long haddr = address & huge_page_mask(h);\n\tbool new_page = false;\n\n\t/*\n\t * Currently, we are forced to kill the process in the event the\n\t * original mapper has unmapped pages from the child due to a failed\n\t * COW. Warn that such a situation has occurred as it may not be obvious\n\t */\n\tif (is_vma_resv_set(vma, HPAGE_RESV_UNMAPPED)) {\n\t\tpr_warn_ratelimited(\"PID %d killed due to inadequate hugepage pool\\n\",\n\t\t\t   current->pid);\n\t\treturn ret;\n\t}\n\n\t/*\n\t * We can not race with truncation due to holding i_mmap_rwsem.\n\t * i_size is modified when holding i_mmap_rwsem, so check here\n\t * once for faults beyond end of file.\n\t */\n\tsize = i_size_read(mapping->host) >> huge_page_shift(h);\n\tif (idx >= size)\n\t\tgoto out;\n\nretry:\n\tpage = find_lock_page(mapping, idx);\n\tif (!page) {\n\t\t/*\n\t\t * Check for page in userfault range\n\t\t */\n\t\tif (userfaultfd_missing(vma)) {\n\t\t\tu32 hash;\n\t\t\tstruct vm_fault vmf = {\n\t\t\t\t.vma = vma,\n\t\t\t\t.address = haddr,\n\t\t\t\t.flags = flags,\n\t\t\t\t/*\n\t\t\t\t * Hard to debug if it ends up being\n\t\t\t\t * used by a callee that assumes\n\t\t\t\t * something about the other\n\t\t\t\t * uninitialized fields... same as in\n\t\t\t\t * memory.c\n\t\t\t\t */\n\t\t\t};\n\n\t\t\t/*\n\t\t\t * hugetlb_fault_mutex and i_mmap_rwsem must be\n\t\t\t * dropped before handling userfault.  Reacquire\n\t\t\t * after handling fault to make calling code simpler.\n\t\t\t */\n\t\t\thash = hugetlb_fault_mutex_hash(mapping, idx);\n\t\t\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\t\t\ti_mmap_unlock_read(mapping);\n\t\t\tret = handle_userfault(&vmf, VM_UFFD_MISSING);\n\t\t\ti_mmap_lock_read(mapping);\n\t\t\tmutex_lock(&hugetlb_fault_mutex_table[hash]);\n\t\t\tgoto out;\n\t\t}\n\n\t\tpage = alloc_huge_page(vma, haddr, 0);\n\t\tif (IS_ERR(page)) {\n\t\t\t/*\n\t\t\t * Returning error will result in faulting task being\n\t\t\t * sent SIGBUS.  The hugetlb fault mutex prevents two\n\t\t\t * tasks from racing to fault in the same page which\n\t\t\t * could result in false unable to allocate errors.\n\t\t\t * Page migration does not take the fault mutex, but\n\t\t\t * does a clear then write of pte's under page table\n\t\t\t * lock.  Page fault code could race with migration,\n\t\t\t * notice the clear pte and try to allocate a page\n\t\t\t * here.  Before returning error, get ptl and make\n\t\t\t * sure there really is no pte entry.\n\t\t\t */\n\t\t\tptl = huge_pte_lock(h, mm, ptep);\n\t\t\tif (!huge_pte_none(huge_ptep_get(ptep))) {\n\t\t\t\tret = 0;\n\t\t\t\tspin_unlock(ptl);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tspin_unlock(ptl);\n\t\t\tret = vmf_error(PTR_ERR(page));\n\t\t\tgoto out;\n\t\t}\n\t\tclear_huge_page(page, address, pages_per_huge_page(h));\n\t\t__SetPageUptodate(page);\n\t\tnew_page = true;\n\n\t\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\t\tint err = huge_add_to_page_cache(page, mapping, idx);\n\t\t\tif (err) {\n\t\t\t\tput_page(page);\n\t\t\t\tif (err == -EEXIST)\n\t\t\t\t\tgoto retry;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else {\n\t\t\tlock_page(page);\n\t\t\tif (unlikely(anon_vma_prepare(vma))) {\n\t\t\t\tret = VM_FAULT_OOM;\n\t\t\t\tgoto backout_unlocked;\n\t\t\t}\n\t\t\tanon_rmap = 1;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * If memory error occurs between mmap() and fault, some process\n\t\t * don't have hwpoisoned swap entry for errored virtual address.\n\t\t * So we need to block hugepage fault by PG_hwpoison bit check.\n\t\t */\n\t\tif (unlikely(PageHWPoison(page))) {\n\t\t\tret = VM_FAULT_HWPOISON |\n\t\t\t\tVM_FAULT_SET_HINDEX(hstate_index(h));\n\t\t\tgoto backout_unlocked;\n\t\t}\n\t}\n\n\t/*\n\t * If we are going to COW a private mapping later, we examine the\n\t * pending reservations for this page now. This will ensure that\n\t * any allocations necessary to record that reservation occur outside\n\t * the spinlock.\n\t */\n\tif ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {\n\t\tif (vma_needs_reservation(h, vma, haddr) < 0) {\n\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto backout_unlocked;\n\t\t}\n\t\t/* Just decrements count, does not deallocate */\n\t\tvma_end_reservation(h, vma, haddr);\n\t}\n\n\tptl = huge_pte_lock(h, mm, ptep);\n\tret = 0;\n\tif (!huge_pte_none(huge_ptep_get(ptep)))\n\t\tgoto backout;\n\n\tif (anon_rmap) {\n\t\tClearPagePrivate(page);\n\t\thugepage_add_new_anon_rmap(page, vma, haddr);\n\t} else\n\t\tpage_dup_rmap(page, true);\n\tnew_pte = make_huge_pte(vma, page, ((vma->vm_flags & VM_WRITE)\n\t\t\t\t&& (vma->vm_flags & VM_SHARED)));\n\tset_huge_pte_at(mm, haddr, ptep, new_pte);\n\n\thugetlb_count_add(pages_per_huge_page(h), mm);\n\tif ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {\n\t\t/* Optimization, do the COW without a second fault */\n\t\tret = hugetlb_cow(mm, vma, address, ptep, page, ptl);\n\t}\n\n\tspin_unlock(ptl);\n\n\t/*\n\t * Only make newly allocated pages active.  Existing pages found\n\t * in the pagecache could be !page_huge_active() if they have been\n\t * isolated for migration.\n\t */\n\tif (new_page)\n\t\tset_page_huge_active(page);\n\n\tunlock_page(page);\nout:\n\treturn ret;\n\nbackout:\n\tspin_unlock(ptl);\nbackout_unlocked:\n\tunlock_page(page);\n\trestore_reserve_on_error(h, vma, haddr, page);\n\tput_page(page);\n\tgoto out;\n}\n\n#ifdef CONFIG_SMP\nu32 hugetlb_fault_mutex_hash(struct address_space *mapping, pgoff_t idx)\n{\n\tunsigned long key[2];\n\tu32 hash;\n\n\tkey[0] = (unsigned long) mapping;\n\tkey[1] = idx;\n\n\thash = jhash2((u32 *)&key, sizeof(key)/(sizeof(u32)), 0);\n\n\treturn hash & (num_fault_mutexes - 1);\n}\n#else\n/*\n * For uniprocesor systems we always use a single mutex, so just\n * return 0 and avoid the hashing overhead.\n */\nu32 hugetlb_fault_mutex_hash(struct address_space *mapping, pgoff_t idx)\n{\n\treturn 0;\n}\n#endif\n\nvm_fault_t hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\tunsigned long address, unsigned int flags)\n{\n\tpte_t *ptep, entry;\n\tspinlock_t *ptl;\n\tvm_fault_t ret;\n\tu32 hash;\n\tpgoff_t idx;\n\tstruct page *page = NULL;\n\tstruct page *pagecache_page = NULL;\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct address_space *mapping;\n\tint need_wait_lock = 0;\n\tunsigned long haddr = address & huge_page_mask(h);\n\n\tptep = huge_pte_offset(mm, haddr, huge_page_size(h));\n\tif (ptep) {\n\t\t/*\n\t\t * Since we hold no locks, ptep could be stale.  That is\n\t\t * OK as we are only making decisions based on content and\n\t\t * not actually modifying content here.\n\t\t */\n\t\tentry = huge_ptep_get(ptep);\n\t\tif (unlikely(is_hugetlb_entry_migration(entry))) {\n\t\t\tmigration_entry_wait_huge(vma, mm, ptep);\n\t\t\treturn 0;\n\t\t} else if (unlikely(is_hugetlb_entry_hwpoisoned(entry)))\n\t\t\treturn VM_FAULT_HWPOISON_LARGE |\n\t\t\t\tVM_FAULT_SET_HINDEX(hstate_index(h));\n\t}\n\n\t/*\n\t * Acquire i_mmap_rwsem before calling huge_pte_alloc and hold\n\t * until finished with ptep.  This serves two purposes:\n\t * 1) It prevents huge_pmd_unshare from being called elsewhere\n\t *    and making the ptep no longer valid.\n\t * 2) It synchronizes us with i_size modifications during truncation.\n\t *\n\t * ptep could have already be assigned via huge_pte_offset.  That\n\t * is OK, as huge_pte_alloc will return the same value unless\n\t * something has changed.\n\t */\n\tmapping = vma->vm_file->f_mapping;\n\ti_mmap_lock_read(mapping);\n\tptep = huge_pte_alloc(mm, haddr, huge_page_size(h));\n\tif (!ptep) {\n\t\ti_mmap_unlock_read(mapping);\n\t\treturn VM_FAULT_OOM;\n\t}\n\n\t/*\n\t * Serialize hugepage allocation and instantiation, so that we don't\n\t * get spurious allocation failures if two CPUs race to instantiate\n\t * the same page in the page cache.\n\t */\n\tidx = vma_hugecache_offset(h, vma, haddr);\n\thash = hugetlb_fault_mutex_hash(mapping, idx);\n\tmutex_lock(&hugetlb_fault_mutex_table[hash]);\n\n\tentry = huge_ptep_get(ptep);\n\tif (huge_pte_none(entry)) {\n\t\tret = hugetlb_no_page(mm, vma, mapping, idx, address, ptep, flags);\n\t\tgoto out_mutex;\n\t}\n\n\tret = 0;\n\n\t/*\n\t * entry could be a migration/hwpoison entry at this point, so this\n\t * check prevents the kernel from going below assuming that we have\n\t * an active hugepage in pagecache. This goto expects the 2nd page\n\t * fault, and is_hugetlb_entry_(migration|hwpoisoned) check will\n\t * properly handle it.\n\t */\n\tif (!pte_present(entry))\n\t\tgoto out_mutex;\n\n\t/*\n\t * If we are going to COW the mapping later, we examine the pending\n\t * reservations for this page now. This will ensure that any\n\t * allocations necessary to record that reservation occur outside the\n\t * spinlock. For private mappings, we also lookup the pagecache\n\t * page now as it is used to determine if a reservation has been\n\t * consumed.\n\t */\n\tif ((flags & FAULT_FLAG_WRITE) && !huge_pte_write(entry)) {\n\t\tif (vma_needs_reservation(h, vma, haddr) < 0) {\n\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto out_mutex;\n\t\t}\n\t\t/* Just decrements count, does not deallocate */\n\t\tvma_end_reservation(h, vma, haddr);\n\n\t\tif (!(vma->vm_flags & VM_MAYSHARE))\n\t\t\tpagecache_page = hugetlbfs_pagecache_page(h,\n\t\t\t\t\t\t\t\tvma, haddr);\n\t}\n\n\tptl = huge_pte_lock(h, mm, ptep);\n\n\t/* Check for a racing update before calling hugetlb_cow */\n\tif (unlikely(!pte_same(entry, huge_ptep_get(ptep))))\n\t\tgoto out_ptl;\n\n\t/*\n\t * hugetlb_cow() requires page locks of pte_page(entry) and\n\t * pagecache_page, so here we need take the former one\n\t * when page != pagecache_page or !pagecache_page.\n\t */\n\tpage = pte_page(entry);\n\tif (page != pagecache_page)\n\t\tif (!trylock_page(page)) {\n\t\t\tneed_wait_lock = 1;\n\t\t\tgoto out_ptl;\n\t\t}\n\n\tget_page(page);\n\n\tif (flags & FAULT_FLAG_WRITE) {\n\t\tif (!huge_pte_write(entry)) {\n\t\t\tret = hugetlb_cow(mm, vma, address, ptep,\n\t\t\t\t\t  pagecache_page, ptl);\n\t\t\tgoto out_put_page;\n\t\t}\n\t\tentry = huge_pte_mkdirty(entry);\n\t}\n\tentry = pte_mkyoung(entry);\n\tif (huge_ptep_set_access_flags(vma, haddr, ptep, entry,\n\t\t\t\t\t\tflags & FAULT_FLAG_WRITE))\n\t\tupdate_mmu_cache(vma, haddr, ptep);\nout_put_page:\n\tif (page != pagecache_page)\n\t\tunlock_page(page);\n\tput_page(page);\nout_ptl:\n\tspin_unlock(ptl);\n\n\tif (pagecache_page) {\n\t\tunlock_page(pagecache_page);\n\t\tput_page(pagecache_page);\n\t}\nout_mutex:\n\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);\n\ti_mmap_unlock_read(mapping);\n\t/*\n\t * Generally it's safe to hold refcount during waiting page lock. But\n\t * here we just wait to defer the next page fault to avoid busy loop and\n\t * the page is not used after unlocked before returning from the current\n\t * page fault. So we are safe from accessing freed page, even if we wait\n\t * here without taking refcount.\n\t */\n\tif (need_wait_lock)\n\t\twait_on_page_locked(page);\n\treturn ret;\n}\n\n/*\n * Used by userfaultfd UFFDIO_COPY.  Based on mcopy_atomic_pte with\n * modifications for huge pages.\n */\nint hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm,\n\t\t\t    pte_t *dst_pte,\n\t\t\t    struct vm_area_struct *dst_vma,\n\t\t\t    unsigned long dst_addr,\n\t\t\t    unsigned long src_addr,\n\t\t\t    struct page **pagep)\n{\n\tstruct address_space *mapping;\n\tpgoff_t idx;\n\tunsigned long size;\n\tint vm_shared = dst_vma->vm_flags & VM_SHARED;\n\tstruct hstate *h = hstate_vma(dst_vma);\n\tpte_t _dst_pte;\n\tspinlock_t *ptl;\n\tint ret;\n\tstruct page *page;\n\n\tif (!*pagep) {\n\t\tret = -ENOMEM;\n\t\tpage = alloc_huge_page(dst_vma, dst_addr, 0);\n\t\tif (IS_ERR(page))\n\t\t\tgoto out;\n\n\t\tret = copy_huge_page_from_user(page,\n\t\t\t\t\t\t(const void __user *) src_addr,\n\t\t\t\t\t\tpages_per_huge_page(h), false);\n\n\t\t/* fallback to copy_from_user outside mmap_lock */\n\t\tif (unlikely(ret)) {\n\t\t\tret = -ENOENT;\n\t\t\t*pagep = page;\n\t\t\t/* don't free the page */\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tpage = *pagep;\n\t\t*pagep = NULL;\n\t}\n\n\t/*\n\t * The memory barrier inside __SetPageUptodate makes sure that\n\t * preceding stores to the page contents become visible before\n\t * the set_pte_at() write.\n\t */\n\t__SetPageUptodate(page);\n\n\tmapping = dst_vma->vm_file->f_mapping;\n\tidx = vma_hugecache_offset(h, dst_vma, dst_addr);\n\n\t/*\n\t * If shared, add to page cache\n\t */\n\tif (vm_shared) {\n\t\tsize = i_size_read(mapping->host) >> huge_page_shift(h);\n\t\tret = -EFAULT;\n\t\tif (idx >= size)\n\t\t\tgoto out_release_nounlock;\n\n\t\t/*\n\t\t * Serialization between remove_inode_hugepages() and\n\t\t * huge_add_to_page_cache() below happens through the\n\t\t * hugetlb_fault_mutex_table that here must be hold by\n\t\t * the caller.\n\t\t */\n\t\tret = huge_add_to_page_cache(page, mapping, idx);\n\t\tif (ret)\n\t\t\tgoto out_release_nounlock;\n\t}\n\n\tptl = huge_pte_lockptr(h, dst_mm, dst_pte);\n\tspin_lock(ptl);\n\n\t/*\n\t * Recheck the i_size after holding PT lock to make sure not\n\t * to leave any page mapped (as page_mapped()) beyond the end\n\t * of the i_size (remove_inode_hugepages() is strict about\n\t * enforcing that). If we bail out here, we'll also leave a\n\t * page in the radix tree in the vm_shared case beyond the end\n\t * of the i_size, but remove_inode_hugepages() will take care\n\t * of it as soon as we drop the hugetlb_fault_mutex_table.\n\t */\n\tsize = i_size_read(mapping->host) >> huge_page_shift(h);\n\tret = -EFAULT;\n\tif (idx >= size)\n\t\tgoto out_release_unlock;\n\n\tret = -EEXIST;\n\tif (!huge_pte_none(huge_ptep_get(dst_pte)))\n\t\tgoto out_release_unlock;\n\n\tif (vm_shared) {\n\t\tpage_dup_rmap(page, true);\n\t} else {\n\t\tClearPagePrivate(page);\n\t\thugepage_add_new_anon_rmap(page, dst_vma, dst_addr);\n\t}\n\n\t_dst_pte = make_huge_pte(dst_vma, page, dst_vma->vm_flags & VM_WRITE);\n\tif (dst_vma->vm_flags & VM_WRITE)\n\t\t_dst_pte = huge_pte_mkdirty(_dst_pte);\n\t_dst_pte = pte_mkyoung(_dst_pte);\n\n\tset_huge_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte);\n\n\t(void)huge_ptep_set_access_flags(dst_vma, dst_addr, dst_pte, _dst_pte,\n\t\t\t\t\tdst_vma->vm_flags & VM_WRITE);\n\thugetlb_count_add(pages_per_huge_page(h), dst_mm);\n\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(dst_vma, dst_addr, dst_pte);\n\n\tspin_unlock(ptl);\n\tset_page_huge_active(page);\n\tif (vm_shared)\n\t\tunlock_page(page);\n\tret = 0;\nout:\n\treturn ret;\nout_release_unlock:\n\tspin_unlock(ptl);\n\tif (vm_shared)\n\t\tunlock_page(page);\nout_release_nounlock:\n\tput_page(page);\n\tgoto out;\n}\n\nlong follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t struct page **pages, struct vm_area_struct **vmas,\n\t\t\t unsigned long *position, unsigned long *nr_pages,\n\t\t\t long i, unsigned int flags, int *locked)\n{\n\tunsigned long pfn_offset;\n\tunsigned long vaddr = *position;\n\tunsigned long remainder = *nr_pages;\n\tstruct hstate *h = hstate_vma(vma);\n\tint err = -EFAULT;\n\n\twhile (vaddr < vma->vm_end && remainder) {\n\t\tpte_t *pte;\n\t\tspinlock_t *ptl = NULL;\n\t\tint absent;\n\t\tstruct page *page;\n\n\t\t/*\n\t\t * If we have a pending SIGKILL, don't keep faulting pages and\n\t\t * potentially allocating memory.\n\t\t */\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tremainder = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Some archs (sparc64, sh*) have multiple pte_ts to\n\t\t * each hugepage.  We have to make sure we get the\n\t\t * first, for the page indexing below to work.\n\t\t *\n\t\t * Note that page table lock is not held when pte is null.\n\t\t */\n\t\tpte = huge_pte_offset(mm, vaddr & huge_page_mask(h),\n\t\t\t\t      huge_page_size(h));\n\t\tif (pte)\n\t\t\tptl = huge_pte_lock(h, mm, pte);\n\t\tabsent = !pte || huge_pte_none(huge_ptep_get(pte));\n\n\t\t/*\n\t\t * When coredumping, it suits get_dump_page if we just return\n\t\t * an error where there's an empty slot with no huge pagecache\n\t\t * to back it.  This way, we avoid allocating a hugepage, and\n\t\t * the sparse dumpfile avoids allocating disk blocks, but its\n\t\t * huge holes still show up with zeroes where they need to be.\n\t\t */\n\t\tif (absent && (flags & FOLL_DUMP) &&\n\t\t    !hugetlbfs_pagecache_present(h, vma, vaddr)) {\n\t\t\tif (pte)\n\t\t\t\tspin_unlock(ptl);\n\t\t\tremainder = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * We need call hugetlb_fault for both hugepages under migration\n\t\t * (in which case hugetlb_fault waits for the migration,) and\n\t\t * hwpoisoned hugepages (in which case we need to prevent the\n\t\t * caller from accessing to them.) In order to do this, we use\n\t\t * here is_swap_pte instead of is_hugetlb_entry_migration and\n\t\t * is_hugetlb_entry_hwpoisoned. This is because it simply covers\n\t\t * both cases, and because we can't follow correct pages\n\t\t * directly from any kind of swap entries.\n\t\t */\n\t\tif (absent || is_swap_pte(huge_ptep_get(pte)) ||\n\t\t    ((flags & FOLL_WRITE) &&\n\t\t      !huge_pte_write(huge_ptep_get(pte)))) {\n\t\t\tvm_fault_t ret;\n\t\t\tunsigned int fault_flags = 0;\n\n\t\t\tif (pte)\n\t\t\t\tspin_unlock(ptl);\n\t\t\tif (flags & FOLL_WRITE)\n\t\t\t\tfault_flags |= FAULT_FLAG_WRITE;\n\t\t\tif (locked)\n\t\t\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY |\n\t\t\t\t\tFAULT_FLAG_KILLABLE;\n\t\t\tif (flags & FOLL_NOWAIT)\n\t\t\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY |\n\t\t\t\t\tFAULT_FLAG_RETRY_NOWAIT;\n\t\t\tif (flags & FOLL_TRIED) {\n\t\t\t\t/*\n\t\t\t\t * Note: FAULT_FLAG_ALLOW_RETRY and\n\t\t\t\t * FAULT_FLAG_TRIED can co-exist\n\t\t\t\t */\n\t\t\t\tfault_flags |= FAULT_FLAG_TRIED;\n\t\t\t}\n\t\t\tret = hugetlb_fault(mm, vma, vaddr, fault_flags);\n\t\t\tif (ret & VM_FAULT_ERROR) {\n\t\t\t\terr = vm_fault_to_errno(ret, flags);\n\t\t\t\tremainder = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ret & VM_FAULT_RETRY) {\n\t\t\t\tif (locked &&\n\t\t\t\t    !(fault_flags & FAULT_FLAG_RETRY_NOWAIT))\n\t\t\t\t\t*locked = 0;\n\t\t\t\t*nr_pages = 0;\n\t\t\t\t/*\n\t\t\t\t * VM_FAULT_RETRY must not return an\n\t\t\t\t * error, it will return zero\n\t\t\t\t * instead.\n\t\t\t\t *\n\t\t\t\t * No need to update \"position\" as the\n\t\t\t\t * caller will not check it after\n\t\t\t\t * *nr_pages is set to 0.\n\t\t\t\t */\n\t\t\t\treturn i;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tpfn_offset = (vaddr & ~huge_page_mask(h)) >> PAGE_SHIFT;\n\t\tpage = pte_page(huge_ptep_get(pte));\n\n\t\t/*\n\t\t * If subpage information not requested, update counters\n\t\t * and skip the same_page loop below.\n\t\t */\n\t\tif (!pages && !vmas && !pfn_offset &&\n\t\t    (vaddr + huge_page_size(h) < vma->vm_end) &&\n\t\t    (remainder >= pages_per_huge_page(h))) {\n\t\t\tvaddr += huge_page_size(h);\n\t\t\tremainder -= pages_per_huge_page(h);\n\t\t\ti += pages_per_huge_page(h);\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\nsame_page:\n\t\tif (pages) {\n\t\t\tpages[i] = mem_map_offset(page, pfn_offset);\n\t\t\t/*\n\t\t\t * try_grab_page() should always succeed here, because:\n\t\t\t * a) we hold the ptl lock, and b) we've just checked\n\t\t\t * that the huge page is present in the page tables. If\n\t\t\t * the huge page is present, then the tail pages must\n\t\t\t * also be present. The ptl prevents the head page and\n\t\t\t * tail pages from being rearranged in any way. So this\n\t\t\t * page must be available at this point, unless the page\n\t\t\t * refcount overflowed:\n\t\t\t */\n\t\t\tif (WARN_ON_ONCE(!try_grab_page(pages[i], flags))) {\n\t\t\t\tspin_unlock(ptl);\n\t\t\t\tremainder = 0;\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (vmas)\n\t\t\tvmas[i] = vma;\n\n\t\tvaddr += PAGE_SIZE;\n\t\t++pfn_offset;\n\t\t--remainder;\n\t\t++i;\n\t\tif (vaddr < vma->vm_end && remainder &&\n\t\t\t\tpfn_offset < pages_per_huge_page(h)) {\n\t\t\t/*\n\t\t\t * We use pfn_offset to avoid touching the pageframes\n\t\t\t * of this compound page.\n\t\t\t */\n\t\t\tgoto same_page;\n\t\t}\n\t\tspin_unlock(ptl);\n\t}\n\t*nr_pages = remainder;\n\t/*\n\t * setting position is actually required only if remainder is\n\t * not zero but it's faster not to add a \"if (remainder)\"\n\t * branch.\n\t */\n\t*position = vaddr;\n\n\treturn i ? i : err;\n}\n\n#ifndef __HAVE_ARCH_FLUSH_HUGETLB_TLB_RANGE\n/*\n * ARCHes with special requirements for evicting HUGETLB backing TLB entries can\n * implement this.\n */\n#define flush_hugetlb_tlb_range(vma, addr, end)\tflush_tlb_range(vma, addr, end)\n#endif\n\nunsigned long hugetlb_change_protection(struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned long end, pgprot_t newprot)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long start = address;\n\tpte_t *ptep;\n\tpte_t pte;\n\tstruct hstate *h = hstate_vma(vma);\n\tunsigned long pages = 0;\n\tbool shared_pmd = false;\n\tstruct mmu_notifier_range range;\n\n\t/*\n\t * In the case of shared PMDs, the area to flush could be beyond\n\t * start/end.  Set range.start/range.end to cover the maximum possible\n\t * range if PMD sharing is possible.\n\t */\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_PROTECTION_VMA,\n\t\t\t\t0, vma, mm, start, end);\n\tadjust_range_if_pmd_sharing_possible(vma, &range.start, &range.end);\n\n\tBUG_ON(address >= end);\n\tflush_cache_range(vma, range.start, range.end);\n\n\tmmu_notifier_invalidate_range_start(&range);\n\ti_mmap_lock_write(vma->vm_file->f_mapping);\n\tfor (; address < end; address += huge_page_size(h)) {\n\t\tspinlock_t *ptl;\n\t\tptep = huge_pte_offset(mm, address, huge_page_size(h));\n\t\tif (!ptep)\n\t\t\tcontinue;\n\t\tptl = huge_pte_lock(h, mm, ptep);\n\t\tif (huge_pmd_unshare(mm, vma, &address, ptep)) {\n\t\t\tpages++;\n\t\t\tspin_unlock(ptl);\n\t\t\tshared_pmd = true;\n\t\t\tcontinue;\n\t\t}\n\t\tpte = huge_ptep_get(ptep);\n\t\tif (unlikely(is_hugetlb_entry_hwpoisoned(pte))) {\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\t\tif (unlikely(is_hugetlb_entry_migration(pte))) {\n\t\t\tswp_entry_t entry = pte_to_swp_entry(pte);\n\n\t\t\tif (is_write_migration_entry(entry)) {\n\t\t\t\tpte_t newpte;\n\n\t\t\t\tmake_migration_entry_read(&entry);\n\t\t\t\tnewpte = swp_entry_to_pte(entry);\n\t\t\t\tset_huge_swap_pte_at(mm, address, ptep,\n\t\t\t\t\t\t     newpte, huge_page_size(h));\n\t\t\t\tpages++;\n\t\t\t}\n\t\t\tspin_unlock(ptl);\n\t\t\tcontinue;\n\t\t}\n\t\tif (!huge_pte_none(pte)) {\n\t\t\tpte_t old_pte;\n\n\t\t\told_pte = huge_ptep_modify_prot_start(vma, address, ptep);\n\t\t\tpte = pte_mkhuge(huge_pte_modify(old_pte, newprot));\n\t\t\tpte = arch_make_huge_pte(pte, vma, NULL, 0);\n\t\t\thuge_ptep_modify_prot_commit(vma, address, ptep, old_pte, pte);\n\t\t\tpages++;\n\t\t}\n\t\tspin_unlock(ptl);\n\t}\n\t/*\n\t * Must flush TLB before releasing i_mmap_rwsem: x86's huge_pmd_unshare\n\t * may have cleared our pud entry and done put_page on the page table:\n\t * once we release i_mmap_rwsem, another task can do the final put_page\n\t * and that page table be reused and filled with junk.  If we actually\n\t * did unshare a page of pmds, flush the range corresponding to the pud.\n\t */\n\tif (shared_pmd)\n\t\tflush_hugetlb_tlb_range(vma, range.start, range.end);\n\telse\n\t\tflush_hugetlb_tlb_range(vma, start, end);\n\t/*\n\t * No need to call mmu_notifier_invalidate_range() we are downgrading\n\t * page table protection not changing it to point to a new page.\n\t *\n\t * See Documentation/vm/mmu_notifier.rst\n\t */\n\ti_mmap_unlock_write(vma->vm_file->f_mapping);\n\tmmu_notifier_invalidate_range_end(&range);\n\n\treturn pages << h->order;\n}\n\nint hugetlb_reserve_pages(struct inode *inode,\n\t\t\t\t\tlong from, long to,\n\t\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\t\tvm_flags_t vm_flags)\n{\n\tlong ret, chg, add = -1;\n\tstruct hstate *h = hstate_inode(inode);\n\tstruct hugepage_subpool *spool = subpool_inode(inode);\n\tstruct resv_map *resv_map;\n\tstruct hugetlb_cgroup *h_cg = NULL;\n\tlong gbl_reserve, regions_needed = 0;\n\n\t/* This should never happen */\n\tif (from > to) {\n\t\tVM_WARN(1, \"%s called with a negative range\\n\", __func__);\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Only apply hugepage reservation if asked. At fault time, an\n\t * attempt will be made for VM_NORESERVE to allocate a page\n\t * without using reserves\n\t */\n\tif (vm_flags & VM_NORESERVE)\n\t\treturn 0;\n\n\t/*\n\t * Shared mappings base their reservation on the number of pages that\n\t * are already allocated on behalf of the file. Private mappings need\n\t * to reserve the full area even if read-only as mprotect() may be\n\t * called to make the mapping read-write. Assume !vma is a shm mapping\n\t */\n\tif (!vma || vma->vm_flags & VM_MAYSHARE) {\n\t\t/*\n\t\t * resv_map can not be NULL as hugetlb_reserve_pages is only\n\t\t * called for inodes for which resv_maps were created (see\n\t\t * hugetlbfs_get_inode).\n\t\t */\n\t\tresv_map = inode_resv_map(inode);\n\n\t\tchg = region_chg(resv_map, from, to, &regions_needed);\n\n\t} else {\n\t\t/* Private mapping. */\n\t\tresv_map = resv_map_alloc();\n\t\tif (!resv_map)\n\t\t\treturn -ENOMEM;\n\n\t\tchg = to - from;\n\n\t\tset_vma_resv_map(vma, resv_map);\n\t\tset_vma_resv_flags(vma, HPAGE_RESV_OWNER);\n\t}\n\n\tif (chg < 0) {\n\t\tret = chg;\n\t\tgoto out_err;\n\t}\n\n\tret = hugetlb_cgroup_charge_cgroup_rsvd(\n\t\thstate_index(h), chg * pages_per_huge_page(h), &h_cg);\n\n\tif (ret < 0) {\n\t\tret = -ENOMEM;\n\t\tgoto out_err;\n\t}\n\n\tif (vma && !(vma->vm_flags & VM_MAYSHARE) && h_cg) {\n\t\t/* For private mappings, the hugetlb_cgroup uncharge info hangs\n\t\t * of the resv_map.\n\t\t */\n\t\tresv_map_set_hugetlb_cgroup_uncharge_info(resv_map, h_cg, h);\n\t}\n\n\t/*\n\t * There must be enough pages in the subpool for the mapping. If\n\t * the subpool has a minimum size, there may be some global\n\t * reservations already in place (gbl_reserve).\n\t */\n\tgbl_reserve = hugepage_subpool_get_pages(spool, chg);\n\tif (gbl_reserve < 0) {\n\t\tret = -ENOSPC;\n\t\tgoto out_uncharge_cgroup;\n\t}\n\n\t/*\n\t * Check enough hugepages are available for the reservation.\n\t * Hand the pages back to the subpool if there are not\n\t */\n\tret = hugetlb_acct_memory(h, gbl_reserve);\n\tif (ret < 0) {\n\t\tgoto out_put_pages;\n\t}\n\n\t/*\n\t * Account for the reservations made. Shared mappings record regions\n\t * that have reservations as they are shared by multiple VMAs.\n\t * When the last VMA disappears, the region map says how much\n\t * the reservation was and the page cache tells how much of\n\t * the reservation was consumed. Private mappings are per-VMA and\n\t * only the consumed reservations are tracked. When the VMA\n\t * disappears, the original reservation is the VMA size and the\n\t * consumed reservations are stored in the map. Hence, nothing\n\t * else has to be done for private mappings here\n\t */\n\tif (!vma || vma->vm_flags & VM_MAYSHARE) {\n\t\tadd = region_add(resv_map, from, to, regions_needed, h, h_cg);\n\n\t\tif (unlikely(add < 0)) {\n\t\t\thugetlb_acct_memory(h, -gbl_reserve);\n\t\t\tgoto out_put_pages;\n\t\t} else if (unlikely(chg > add)) {\n\t\t\t/*\n\t\t\t * pages in this range were added to the reserve\n\t\t\t * map between region_chg and region_add.  This\n\t\t\t * indicates a race with alloc_huge_page.  Adjust\n\t\t\t * the subpool and reserve counts modified above\n\t\t\t * based on the difference.\n\t\t\t */\n\t\t\tlong rsv_adjust;\n\n\t\t\thugetlb_cgroup_uncharge_cgroup_rsvd(\n\t\t\t\thstate_index(h),\n\t\t\t\t(chg - add) * pages_per_huge_page(h), h_cg);\n\n\t\t\trsv_adjust = hugepage_subpool_put_pages(spool,\n\t\t\t\t\t\t\t\tchg - add);\n\t\t\thugetlb_acct_memory(h, -rsv_adjust);\n\t\t}\n\t}\n\treturn 0;\nout_put_pages:\n\t/* put back original number of pages, chg */\n\t(void)hugepage_subpool_put_pages(spool, chg);\nout_uncharge_cgroup:\n\thugetlb_cgroup_uncharge_cgroup_rsvd(hstate_index(h),\n\t\t\t\t\t    chg * pages_per_huge_page(h), h_cg);\nout_err:\n\tif (!vma || vma->vm_flags & VM_MAYSHARE)\n\t\t/* Only call region_abort if the region_chg succeeded but the\n\t\t * region_add failed or didn't run.\n\t\t */\n\t\tif (chg >= 0 && add < 0)\n\t\t\tregion_abort(resv_map, from, to, regions_needed);\n\tif (vma && is_vma_resv_set(vma, HPAGE_RESV_OWNER))\n\t\tkref_put(&resv_map->refs, resv_map_release);\n\treturn ret;\n}\n\nlong hugetlb_unreserve_pages(struct inode *inode, long start, long end,\n\t\t\t\t\t\t\t\tlong freed)\n{\n\tstruct hstate *h = hstate_inode(inode);\n\tstruct resv_map *resv_map = inode_resv_map(inode);\n\tlong chg = 0;\n\tstruct hugepage_subpool *spool = subpool_inode(inode);\n\tlong gbl_reserve;\n\n\t/*\n\t * Since this routine can be called in the evict inode path for all\n\t * hugetlbfs inodes, resv_map could be NULL.\n\t */\n\tif (resv_map) {\n\t\tchg = region_del(resv_map, start, end);\n\t\t/*\n\t\t * region_del() can fail in the rare case where a region\n\t\t * must be split and another region descriptor can not be\n\t\t * allocated.  If end == LONG_MAX, it will not fail.\n\t\t */\n\t\tif (chg < 0)\n\t\t\treturn chg;\n\t}\n\n\tspin_lock(&inode->i_lock);\n\tinode->i_blocks -= (blocks_per_huge_page(h) * freed);\n\tspin_unlock(&inode->i_lock);\n\n\t/*\n\t * If the subpool has a minimum size, the number of global\n\t * reservations to be released may be adjusted.\n\t */\n\tgbl_reserve = hugepage_subpool_put_pages(spool, (chg - freed));\n\thugetlb_acct_memory(h, -gbl_reserve);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_ARCH_WANT_HUGE_PMD_SHARE\nstatic unsigned long page_table_shareable(struct vm_area_struct *svma,\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tunsigned long addr, pgoff_t idx)\n{\n\tunsigned long saddr = ((idx - svma->vm_pgoff) << PAGE_SHIFT) +\n\t\t\t\tsvma->vm_start;\n\tunsigned long sbase = saddr & PUD_MASK;\n\tunsigned long s_end = sbase + PUD_SIZE;\n\n\t/* Allow segments to share if only one is marked locked */\n\tunsigned long vm_flags = vma->vm_flags & VM_LOCKED_CLEAR_MASK;\n\tunsigned long svm_flags = svma->vm_flags & VM_LOCKED_CLEAR_MASK;\n\n\t/*\n\t * match the virtual addresses, permission and the alignment of the\n\t * page table page.\n\t */\n\tif (pmd_index(addr) != pmd_index(saddr) ||\n\t    vm_flags != svm_flags ||\n\t    sbase < svma->vm_start || svma->vm_end < s_end)\n\t\treturn 0;\n\n\treturn saddr;\n}\n\nstatic bool vma_shareable(struct vm_area_struct *vma, unsigned long addr)\n{\n\tunsigned long base = addr & PUD_MASK;\n\tunsigned long end = base + PUD_SIZE;\n\n\t/*\n\t * check on proper vm_flags and page table alignment\n\t */\n\tif (vma->vm_flags & VM_MAYSHARE && range_in_vma(vma, base, end))\n\t\treturn true;\n\treturn false;\n}\n\n/*\n * Determine if start,end range within vma could be mapped by shared pmd.\n * If yes, adjust start and end to cover range associated with possible\n * shared pmd mappings.\n */\nvoid adjust_range_if_pmd_sharing_possible(struct vm_area_struct *vma,\n\t\t\t\tunsigned long *start, unsigned long *end)\n{\n\tunsigned long a_start, a_end;\n\n\tif (!(vma->vm_flags & VM_MAYSHARE))\n\t\treturn;\n\n\t/* Extend the range to be PUD aligned for a worst case scenario */\n\ta_start = ALIGN_DOWN(*start, PUD_SIZE);\n\ta_end = ALIGN(*end, PUD_SIZE);\n\n\t/*\n\t * Intersect the range with the vma range, since pmd sharing won't be\n\t * across vma after all\n\t */\n\t*start = max(vma->vm_start, a_start);\n\t*end = min(vma->vm_end, a_end);\n}\n\n/*\n * Search for a shareable pmd page for hugetlb. In any case calls pmd_alloc()\n * and returns the corresponding pte. While this is not necessary for the\n * !shared pmd case because we can allocate the pmd later as well, it makes the\n * code much cleaner.\n *\n * This routine must be called with i_mmap_rwsem held in at least read mode if\n * sharing is possible.  For hugetlbfs, this prevents removal of any page\n * table entries associated with the address space.  This is important as we\n * are setting up sharing based on existing page table entries (mappings).\n *\n * NOTE: This routine is only called from huge_pte_alloc.  Some callers of\n * huge_pte_alloc know that sharing is not possible and do not take\n * i_mmap_rwsem as a performance optimization.  This is handled by the\n * if !vma_shareable check at the beginning of the routine. i_mmap_rwsem is\n * only required for subsequent processing.\n */\npte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud)\n{\n\tstruct vm_area_struct *vma = find_vma(mm, addr);\n\tstruct address_space *mapping = vma->vm_file->f_mapping;\n\tpgoff_t idx = ((addr - vma->vm_start) >> PAGE_SHIFT) +\n\t\t\tvma->vm_pgoff;\n\tstruct vm_area_struct *svma;\n\tunsigned long saddr;\n\tpte_t *spte = NULL;\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\n\tif (!vma_shareable(vma, addr))\n\t\treturn (pte_t *)pmd_alloc(mm, pud, addr);\n\n\ti_mmap_assert_locked(mapping);\n\tvma_interval_tree_foreach(svma, &mapping->i_mmap, idx, idx) {\n\t\tif (svma == vma)\n\t\t\tcontinue;\n\n\t\tsaddr = page_table_shareable(svma, vma, addr, idx);\n\t\tif (saddr) {\n\t\t\tspte = huge_pte_offset(svma->vm_mm, saddr,\n\t\t\t\t\t       vma_mmu_pagesize(svma));\n\t\t\tif (spte) {\n\t\t\t\tget_page(virt_to_page(spte));\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!spte)\n\t\tgoto out;\n\n\tptl = huge_pte_lock(hstate_vma(vma), mm, spte);\n\tif (pud_none(*pud)) {\n\t\tpud_populate(mm, pud,\n\t\t\t\t(pmd_t *)((unsigned long)spte & PAGE_MASK));\n\t\tmm_inc_nr_pmds(mm);\n\t} else {\n\t\tput_page(virt_to_page(spte));\n\t}\n\tspin_unlock(ptl);\nout:\n\tpte = (pte_t *)pmd_alloc(mm, pud, addr);\n\treturn pte;\n}\n\n/*\n * unmap huge page backed by shared pte.\n *\n * Hugetlb pte page is ref counted at the time of mapping.  If pte is shared\n * indicated by page_count > 1, unmap is achieved by clearing pud and\n * decrementing the ref count. If count == 1, the pte page is not shared.\n *\n * Called with page table lock held and i_mmap_rwsem held in write mode.\n *\n * returns: 1 successfully unmapped a shared pte page\n *\t    0 the underlying pte page is not shared, or it is the last user\n */\nint huge_pmd_unshare(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t\t\tunsigned long *addr, pte_t *ptep)\n{\n\tpgd_t *pgd = pgd_offset(mm, *addr);\n\tp4d_t *p4d = p4d_offset(pgd, *addr);\n\tpud_t *pud = pud_offset(p4d, *addr);\n\n\ti_mmap_assert_write_locked(vma->vm_file->f_mapping);\n\tBUG_ON(page_count(virt_to_page(ptep)) == 0);\n\tif (page_count(virt_to_page(ptep)) == 1)\n\t\treturn 0;\n\n\tpud_clear(pud);\n\tput_page(virt_to_page(ptep));\n\tmm_dec_nr_pmds(mm);\n\t*addr = ALIGN(*addr, HPAGE_SIZE * PTRS_PER_PTE) - HPAGE_SIZE;\n\treturn 1;\n}\n#define want_pmd_share()\t(1)\n#else /* !CONFIG_ARCH_WANT_HUGE_PMD_SHARE */\npte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud)\n{\n\treturn NULL;\n}\n\nint huge_pmd_unshare(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t\tunsigned long *addr, pte_t *ptep)\n{\n\treturn 0;\n}\n\nvoid adjust_range_if_pmd_sharing_possible(struct vm_area_struct *vma,\n\t\t\t\tunsigned long *start, unsigned long *end)\n{\n}\n#define want_pmd_share()\t(0)\n#endif /* CONFIG_ARCH_WANT_HUGE_PMD_SHARE */\n\n#ifdef CONFIG_ARCH_WANT_GENERAL_HUGETLB\npte_t *huge_pte_alloc(struct mm_struct *mm,\n\t\t\tunsigned long addr, unsigned long sz)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpte_t *pte = NULL;\n\n\tpgd = pgd_offset(mm, addr);\n\tp4d = p4d_alloc(mm, pgd, addr);\n\tif (!p4d)\n\t\treturn NULL;\n\tpud = pud_alloc(mm, p4d, addr);\n\tif (pud) {\n\t\tif (sz == PUD_SIZE) {\n\t\t\tpte = (pte_t *)pud;\n\t\t} else {\n\t\t\tBUG_ON(sz != PMD_SIZE);\n\t\t\tif (want_pmd_share() && pud_none(*pud))\n\t\t\t\tpte = huge_pmd_share(mm, addr, pud);\n\t\t\telse\n\t\t\t\tpte = (pte_t *)pmd_alloc(mm, pud, addr);\n\t\t}\n\t}\n\tBUG_ON(pte && pte_present(*pte) && !pte_huge(*pte));\n\n\treturn pte;\n}\n\n/*\n * huge_pte_offset() - Walk the page table to resolve the hugepage\n * entry at address @addr\n *\n * Return: Pointer to page table entry (PUD or PMD) for\n * address @addr, or NULL if a !p*d_present() entry is encountered and the\n * size @sz doesn't match the hugepage size at this level of the page\n * table.\n */\npte_t *huge_pte_offset(struct mm_struct *mm,\n\t\t       unsigned long addr, unsigned long sz)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\n\tpgd = pgd_offset(mm, addr);\n\tif (!pgd_present(*pgd))\n\t\treturn NULL;\n\tp4d = p4d_offset(pgd, addr);\n\tif (!p4d_present(*p4d))\n\t\treturn NULL;\n\n\tpud = pud_offset(p4d, addr);\n\tif (sz == PUD_SIZE)\n\t\t/* must be pud huge, non-present or none */\n\t\treturn (pte_t *)pud;\n\tif (!pud_present(*pud))\n\t\treturn NULL;\n\t/* must have a valid entry and size to go further */\n\n\tpmd = pmd_offset(pud, addr);\n\t/* must be pmd huge, non-present or none */\n\treturn (pte_t *)pmd;\n}\n\n#endif /* CONFIG_ARCH_WANT_GENERAL_HUGETLB */\n\n/*\n * These functions are overwritable if your architecture needs its own\n * behavior.\n */\nstruct page * __weak\nfollow_huge_addr(struct mm_struct *mm, unsigned long address,\n\t\t\t      int write)\n{\n\treturn ERR_PTR(-EINVAL);\n}\n\nstruct page * __weak\nfollow_huge_pd(struct vm_area_struct *vma,\n\t       unsigned long address, hugepd_t hpd, int flags, int pdshift)\n{\n\tWARN(1, \"hugepd follow called with no support for hugepage directory format\\n\");\n\treturn NULL;\n}\n\nstruct page * __weak\nfollow_huge_pmd(struct mm_struct *mm, unsigned long address,\n\t\tpmd_t *pmd, int flags)\n{\n\tstruct page *page = NULL;\n\tspinlock_t *ptl;\n\tpte_t pte;\n\n\t/* FOLL_GET and FOLL_PIN are mutually exclusive. */\n\tif (WARN_ON_ONCE((flags & (FOLL_PIN | FOLL_GET)) ==\n\t\t\t (FOLL_PIN | FOLL_GET)))\n\t\treturn NULL;\n\nretry:\n\tptl = pmd_lockptr(mm, pmd);\n\tspin_lock(ptl);\n\t/*\n\t * make sure that the address range covered by this pmd is not\n\t * unmapped from other threads.\n\t */\n\tif (!pmd_huge(*pmd))\n\t\tgoto out;\n\tpte = huge_ptep_get((pte_t *)pmd);\n\tif (pte_present(pte)) {\n\t\tpage = pmd_page(*pmd) + ((address & ~PMD_MASK) >> PAGE_SHIFT);\n\t\t/*\n\t\t * try_grab_page() should always succeed here, because: a) we\n\t\t * hold the pmd (ptl) lock, and b) we've just checked that the\n\t\t * huge pmd (head) page is present in the page tables. The ptl\n\t\t * prevents the head page and tail pages from being rearranged\n\t\t * in any way. So this page must be available at this point,\n\t\t * unless the page refcount overflowed:\n\t\t */\n\t\tif (WARN_ON_ONCE(!try_grab_page(page, flags))) {\n\t\t\tpage = NULL;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tif (is_hugetlb_entry_migration(pte)) {\n\t\t\tspin_unlock(ptl);\n\t\t\t__migration_entry_wait(mm, (pte_t *)pmd, ptl);\n\t\t\tgoto retry;\n\t\t}\n\t\t/*\n\t\t * hwpoisoned entry is treated as no_page_table in\n\t\t * follow_page_mask().\n\t\t */\n\t}\nout:\n\tspin_unlock(ptl);\n\treturn page;\n}\n\nstruct page * __weak\nfollow_huge_pud(struct mm_struct *mm, unsigned long address,\n\t\tpud_t *pud, int flags)\n{\n\tif (flags & (FOLL_GET | FOLL_PIN))\n\t\treturn NULL;\n\n\treturn pte_page(*(pte_t *)pud) + ((address & ~PUD_MASK) >> PAGE_SHIFT);\n}\n\nstruct page * __weak\nfollow_huge_pgd(struct mm_struct *mm, unsigned long address, pgd_t *pgd, int flags)\n{\n\tif (flags & (FOLL_GET | FOLL_PIN))\n\t\treturn NULL;\n\n\treturn pte_page(*(pte_t *)pgd) + ((address & ~PGDIR_MASK) >> PAGE_SHIFT);\n}\n\nbool isolate_huge_page(struct page *page, struct list_head *list)\n{\n\tbool ret = true;\n\n\tVM_BUG_ON_PAGE(!PageHead(page), page);\n\tspin_lock(&hugetlb_lock);\n\tif (!page_huge_active(page) || !get_page_unless_zero(page)) {\n\t\tret = false;\n\t\tgoto unlock;\n\t}\n\tclear_page_huge_active(page);\n\tlist_move_tail(&page->lru, list);\nunlock:\n\tspin_unlock(&hugetlb_lock);\n\treturn ret;\n}\n\nvoid putback_active_hugepage(struct page *page)\n{\n\tVM_BUG_ON_PAGE(!PageHead(page), page);\n\tspin_lock(&hugetlb_lock);\n\tset_page_huge_active(page);\n\tlist_move_tail(&page->lru, &(page_hstate(page))->hugepage_activelist);\n\tspin_unlock(&hugetlb_lock);\n\tput_page(page);\n}\n\nvoid move_hugetlb_state(struct page *oldpage, struct page *newpage, int reason)\n{\n\tstruct hstate *h = page_hstate(oldpage);\n\n\thugetlb_cgroup_migrate(oldpage, newpage);\n\tset_page_owner_migrate_reason(newpage, reason);\n\n\t/*\n\t * transfer temporary state of the new huge page. This is\n\t * reverse to other transitions because the newpage is going to\n\t * be final while the old one will be freed so it takes over\n\t * the temporary status.\n\t *\n\t * Also note that we have to transfer the per-node surplus state\n\t * here as well otherwise the global surplus count will not match\n\t * the per-node's.\n\t */\n\tif (PageHugeTemporary(newpage)) {\n\t\tint old_nid = page_to_nid(oldpage);\n\t\tint new_nid = page_to_nid(newpage);\n\n\t\tSetPageHugeTemporary(oldpage);\n\t\tClearPageHugeTemporary(newpage);\n\n\t\tspin_lock(&hugetlb_lock);\n\t\tif (h->surplus_huge_pages_node[old_nid]) {\n\t\t\th->surplus_huge_pages_node[old_nid]--;\n\t\t\th->surplus_huge_pages_node[new_nid]++;\n\t\t}\n\t\tspin_unlock(&hugetlb_lock);\n\t}\n}\n\n#ifdef CONFIG_CMA\nstatic bool cma_reserve_called __initdata;\n\nstatic int __init cmdline_parse_hugetlb_cma(char *p)\n{\n\thugetlb_cma_size = memparse(p, &p);\n\treturn 0;\n}\n\nearly_param(\"hugetlb_cma\", cmdline_parse_hugetlb_cma);\n\nvoid __init hugetlb_cma_reserve(int order)\n{\n\tunsigned long size, reserved, per_node;\n\tint nid;\n\n\tcma_reserve_called = true;\n\n\tif (!hugetlb_cma_size)\n\t\treturn;\n\n\tif (hugetlb_cma_size < (PAGE_SIZE << order)) {\n\t\tpr_warn(\"hugetlb_cma: cma area should be at least %lu MiB\\n\",\n\t\t\t(PAGE_SIZE << order) / SZ_1M);\n\t\treturn;\n\t}\n\n\t/*\n\t * If 3 GB area is requested on a machine with 4 numa nodes,\n\t * let's allocate 1 GB on first three nodes and ignore the last one.\n\t */\n\tper_node = DIV_ROUND_UP(hugetlb_cma_size, nr_online_nodes);\n\tpr_info(\"hugetlb_cma: reserve %lu MiB, up to %lu MiB per node\\n\",\n\t\thugetlb_cma_size / SZ_1M, per_node / SZ_1M);\n\n\treserved = 0;\n\tfor_each_node_state(nid, N_ONLINE) {\n\t\tint res;\n\t\tchar name[CMA_MAX_NAME];\n\n\t\tsize = min(per_node, hugetlb_cma_size - reserved);\n\t\tsize = round_up(size, PAGE_SIZE << order);\n\n\t\tsnprintf(name, sizeof(name), \"hugetlb%d\", nid);\n\t\tres = cma_declare_contiguous_nid(0, size, 0, PAGE_SIZE << order,\n\t\t\t\t\t\t 0, false, name,\n\t\t\t\t\t\t &hugetlb_cma[nid], nid);\n\t\tif (res) {\n\t\t\tpr_warn(\"hugetlb_cma: reservation failed: err %d, node %d\",\n\t\t\t\tres, nid);\n\t\t\tcontinue;\n\t\t}\n\n\t\treserved += size;\n\t\tpr_info(\"hugetlb_cma: reserved %lu MiB on node %d\\n\",\n\t\t\tsize / SZ_1M, nid);\n\n\t\tif (reserved >= hugetlb_cma_size)\n\t\t\tbreak;\n\t}\n}\n\nvoid __init hugetlb_cma_check(void)\n{\n\tif (!hugetlb_cma_size || cma_reserve_called)\n\t\treturn;\n\n\tpr_warn(\"hugetlb_cma: the option isn't supported by current arch\\n\");\n}\n\n#endif /* CONFIG_CMA */\n"}, "12": {"id": 12, "path": "/src/fs/xattr.c", "content": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n  File: fs/xattr.c\n\n  Extended attribute handling.\n\n  Copyright (C) 2001 by Andreas Gruenbacher <a.gruenbacher@computer.org>\n  Copyright (C) 2001 SGI - Silicon Graphics, Inc <linux-xfs@oss.sgi.com>\n  Copyright (c) 2004 Red Hat, Inc., James Morris <jmorris@redhat.com>\n */\n#include <linux/fs.h>\n#include <linux/slab.h>\n#include <linux/file.h>\n#include <linux/xattr.h>\n#include <linux/mount.h>\n#include <linux/namei.h>\n#include <linux/security.h>\n#include <linux/evm.h>\n#include <linux/syscalls.h>\n#include <linux/export.h>\n#include <linux/fsnotify.h>\n#include <linux/audit.h>\n#include <linux/vmalloc.h>\n#include <linux/posix_acl_xattr.h>\n\n#include <linux/uaccess.h>\n\nstatic const char *\nstrcmp_prefix(const char *a, const char *a_prefix)\n{\n\twhile (*a_prefix && *a == *a_prefix) {\n\t\ta++;\n\t\ta_prefix++;\n\t}\n\treturn *a_prefix ? NULL : a;\n}\n\n/*\n * In order to implement different sets of xattr operations for each xattr\n * prefix, a filesystem should create a null-terminated array of struct\n * xattr_handler (one for each prefix) and hang a pointer to it off of the\n * s_xattr field of the superblock.\n */\n#define for_each_xattr_handler(handlers, handler)\t\t\\\n\tif (handlers)\t\t\t\t\t\t\\\n\t\tfor ((handler) = *(handlers)++;\t\t\t\\\n\t\t\t(handler) != NULL;\t\t\t\\\n\t\t\t(handler) = *(handlers)++)\n\n/*\n * Find the xattr_handler with the matching prefix.\n */\nstatic const struct xattr_handler *\nxattr_resolve_name(struct inode *inode, const char **name)\n{\n\tconst struct xattr_handler **handlers = inode->i_sb->s_xattr;\n\tconst struct xattr_handler *handler;\n\n\tif (!(inode->i_opflags & IOP_XATTR)) {\n\t\tif (unlikely(is_bad_inode(inode)))\n\t\t\treturn ERR_PTR(-EIO);\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\t}\n\tfor_each_xattr_handler(handlers, handler) {\n\t\tconst char *n;\n\n\t\tn = strcmp_prefix(*name, xattr_prefix(handler));\n\t\tif (n) {\n\t\t\tif (!handler->prefix ^ !*n) {\n\t\t\t\tif (*n)\n\t\t\t\t\tcontinue;\n\t\t\t\treturn ERR_PTR(-EINVAL);\n\t\t\t}\n\t\t\t*name = n;\n\t\t\treturn handler;\n\t\t}\n\t}\n\treturn ERR_PTR(-EOPNOTSUPP);\n}\n\n/*\n * Check permissions for extended attribute access.  This is a bit complicated\n * because different namespaces have very different rules.\n */\nstatic int\nxattr_permission(struct inode *inode, const char *name, int mask)\n{\n\t/*\n\t * We can never set or remove an extended attribute on a read-only\n\t * filesystem  or on an immutable / append-only inode.\n\t */\n\tif (mask & MAY_WRITE) {\n\t\tif (IS_IMMUTABLE(inode) || IS_APPEND(inode))\n\t\t\treturn -EPERM;\n\t\t/*\n\t\t * Updating an xattr will likely cause i_uid and i_gid\n\t\t * to be writen back improperly if their true value is\n\t\t * unknown to the vfs.\n\t\t */\n\t\tif (HAS_UNMAPPED_ID(inode))\n\t\t\treturn -EPERM;\n\t}\n\n\t/*\n\t * No restriction for security.* and system.* from the VFS.  Decision\n\t * on these is left to the underlying filesystem / security module.\n\t */\n\tif (!strncmp(name, XATTR_SECURITY_PREFIX, XATTR_SECURITY_PREFIX_LEN) ||\n\t    !strncmp(name, XATTR_SYSTEM_PREFIX, XATTR_SYSTEM_PREFIX_LEN))\n\t\treturn 0;\n\n\t/*\n\t * The trusted.* namespace can only be accessed by privileged users.\n\t */\n\tif (!strncmp(name, XATTR_TRUSTED_PREFIX, XATTR_TRUSTED_PREFIX_LEN)) {\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn (mask & MAY_WRITE) ? -EPERM : -ENODATA;\n\t\treturn 0;\n\t}\n\n\t/*\n\t * In the user.* namespace, only regular files and directories can have\n\t * extended attributes. For sticky directories, only the owner and\n\t * privileged users can write attributes.\n\t */\n\tif (!strncmp(name, XATTR_USER_PREFIX, XATTR_USER_PREFIX_LEN)) {\n\t\tif (!S_ISREG(inode->i_mode) && !S_ISDIR(inode->i_mode))\n\t\t\treturn (mask & MAY_WRITE) ? -EPERM : -ENODATA;\n\t\tif (S_ISDIR(inode->i_mode) && (inode->i_mode & S_ISVTX) &&\n\t\t    (mask & MAY_WRITE) && !inode_owner_or_capable(inode))\n\t\t\treturn -EPERM;\n\t}\n\n\treturn inode_permission(inode, mask);\n}\n\n/*\n * Look for any handler that deals with the specified namespace.\n */\nint\nxattr_supported_namespace(struct inode *inode, const char *prefix)\n{\n\tconst struct xattr_handler **handlers = inode->i_sb->s_xattr;\n\tconst struct xattr_handler *handler;\n\tsize_t preflen;\n\n\tif (!(inode->i_opflags & IOP_XATTR)) {\n\t\tif (unlikely(is_bad_inode(inode)))\n\t\t\treturn -EIO;\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tpreflen = strlen(prefix);\n\n\tfor_each_xattr_handler(handlers, handler) {\n\t\tif (!strncmp(xattr_prefix(handler), prefix, preflen))\n\t\t\treturn 0;\n\t}\n\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(xattr_supported_namespace);\n\nint\n__vfs_setxattr(struct dentry *dentry, struct inode *inode, const char *name,\n\t       const void *value, size_t size, int flags)\n{\n\tconst struct xattr_handler *handler;\n\n\thandler = xattr_resolve_name(inode, &name);\n\tif (IS_ERR(handler))\n\t\treturn PTR_ERR(handler);\n\tif (!handler->set)\n\t\treturn -EOPNOTSUPP;\n\tif (size == 0)\n\t\tvalue = \"\";  /* empty EA, do not remove */\n\treturn handler->set(handler, dentry, inode, name, value, size, flags);\n}\nEXPORT_SYMBOL(__vfs_setxattr);\n\n/**\n *  __vfs_setxattr_noperm - perform setxattr operation without performing\n *  permission checks.\n *\n *  @dentry - object to perform setxattr on\n *  @name - xattr name to set\n *  @value - value to set @name to\n *  @size - size of @value\n *  @flags - flags to pass into filesystem operations\n *\n *  returns the result of the internal setxattr or setsecurity operations.\n *\n *  This function requires the caller to lock the inode's i_mutex before it\n *  is executed. It also assumes that the caller will make the appropriate\n *  permission checks.\n */\nint __vfs_setxattr_noperm(struct dentry *dentry, const char *name,\n\t\tconst void *value, size_t size, int flags)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tint error = -EAGAIN;\n\tint issec = !strncmp(name, XATTR_SECURITY_PREFIX,\n\t\t\t\t   XATTR_SECURITY_PREFIX_LEN);\n\n\tif (issec)\n\t\tinode->i_flags &= ~S_NOSEC;\n\tif (inode->i_opflags & IOP_XATTR) {\n\t\terror = __vfs_setxattr(dentry, inode, name, value, size, flags);\n\t\tif (!error) {\n\t\t\tfsnotify_xattr(dentry);\n\t\t\tsecurity_inode_post_setxattr(dentry, name, value,\n\t\t\t\t\t\t     size, flags);\n\t\t}\n\t} else {\n\t\tif (unlikely(is_bad_inode(inode)))\n\t\t\treturn -EIO;\n\t}\n\tif (error == -EAGAIN) {\n\t\terror = -EOPNOTSUPP;\n\n\t\tif (issec) {\n\t\t\tconst char *suffix = name + XATTR_SECURITY_PREFIX_LEN;\n\n\t\t\terror = security_inode_setsecurity(inode, suffix, value,\n\t\t\t\t\t\t\t   size, flags);\n\t\t\tif (!error)\n\t\t\t\tfsnotify_xattr(dentry);\n\t\t}\n\t}\n\n\treturn error;\n}\n\n/**\n * __vfs_setxattr_locked - set an extended attribute while holding the inode\n * lock\n *\n *  @dentry: object to perform setxattr on\n *  @name: xattr name to set\n *  @value: value to set @name to\n *  @size: size of @value\n *  @flags: flags to pass into filesystem operations\n *  @delegated_inode: on return, will contain an inode pointer that\n *  a delegation was broken on, NULL if none.\n */\nint\n__vfs_setxattr_locked(struct dentry *dentry, const char *name,\n\t\tconst void *value, size_t size, int flags,\n\t\tstruct inode **delegated_inode)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tint error;\n\n\terror = xattr_permission(inode, name, MAY_WRITE);\n\tif (error)\n\t\treturn error;\n\n\terror = security_inode_setxattr(dentry, name, value, size, flags);\n\tif (error)\n\t\tgoto out;\n\n\terror = try_break_deleg(inode, delegated_inode);\n\tif (error)\n\t\tgoto out;\n\n\terror = __vfs_setxattr_noperm(dentry, name, value, size, flags);\n\nout:\n\treturn error;\n}\nEXPORT_SYMBOL_GPL(__vfs_setxattr_locked);\n\nint\nvfs_setxattr(struct dentry *dentry, const char *name, const void *value,\n\t\tsize_t size, int flags)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tstruct inode *delegated_inode = NULL;\n\tint error;\n\nretry_deleg:\n\tinode_lock(inode);\n\terror = __vfs_setxattr_locked(dentry, name, value, size, flags,\n\t    &delegated_inode);\n\tinode_unlock(inode);\n\n\tif (delegated_inode) {\n\t\terror = break_deleg_wait(&delegated_inode);\n\t\tif (!error)\n\t\t\tgoto retry_deleg;\n\t}\n\treturn error;\n}\nEXPORT_SYMBOL_GPL(vfs_setxattr);\n\nstatic ssize_t\nxattr_getsecurity(struct inode *inode, const char *name, void *value,\n\t\t\tsize_t size)\n{\n\tvoid *buffer = NULL;\n\tssize_t len;\n\n\tif (!value || !size) {\n\t\tlen = security_inode_getsecurity(inode, name, &buffer, false);\n\t\tgoto out_noalloc;\n\t}\n\n\tlen = security_inode_getsecurity(inode, name, &buffer, true);\n\tif (len < 0)\n\t\treturn len;\n\tif (size < len) {\n\t\tlen = -ERANGE;\n\t\tgoto out;\n\t}\n\tmemcpy(value, buffer, len);\nout:\n\tkfree(buffer);\nout_noalloc:\n\treturn len;\n}\n\n/*\n * vfs_getxattr_alloc - allocate memory, if necessary, before calling getxattr\n *\n * Allocate memory, if not already allocated, or re-allocate correct size,\n * before retrieving the extended attribute.\n *\n * Returns the result of alloc, if failed, or the getxattr operation.\n */\nssize_t\nvfs_getxattr_alloc(struct dentry *dentry, const char *name, char **xattr_value,\n\t\t   size_t xattr_size, gfp_t flags)\n{\n\tconst struct xattr_handler *handler;\n\tstruct inode *inode = dentry->d_inode;\n\tchar *value = *xattr_value;\n\tint error;\n\n\terror = xattr_permission(inode, name, MAY_READ);\n\tif (error)\n\t\treturn error;\n\n\thandler = xattr_resolve_name(inode, &name);\n\tif (IS_ERR(handler))\n\t\treturn PTR_ERR(handler);\n\tif (!handler->get)\n\t\treturn -EOPNOTSUPP;\n\terror = handler->get(handler, dentry, inode, name, NULL, 0);\n\tif (error < 0)\n\t\treturn error;\n\n\tif (!value || (error > xattr_size)) {\n\t\tvalue = krealloc(*xattr_value, error + 1, flags);\n\t\tif (!value)\n\t\t\treturn -ENOMEM;\n\t\tmemset(value, 0, error + 1);\n\t}\n\n\terror = handler->get(handler, dentry, inode, name, value, error);\n\t*xattr_value = value;\n\treturn error;\n}\n\nssize_t\n__vfs_getxattr(struct dentry *dentry, struct inode *inode, const char *name,\n\t       void *value, size_t size)\n{\n\tconst struct xattr_handler *handler;\n\n\thandler = xattr_resolve_name(inode, &name);\n\tif (IS_ERR(handler))\n\t\treturn PTR_ERR(handler);\n\tif (!handler->get)\n\t\treturn -EOPNOTSUPP;\n\treturn handler->get(handler, dentry, inode, name, value, size);\n}\nEXPORT_SYMBOL(__vfs_getxattr);\n\nssize_t\nvfs_getxattr(struct dentry *dentry, const char *name, void *value, size_t size)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tint error;\n\n\terror = xattr_permission(inode, name, MAY_READ);\n\tif (error)\n\t\treturn error;\n\n\terror = security_inode_getxattr(dentry, name);\n\tif (error)\n\t\treturn error;\n\n\tif (!strncmp(name, XATTR_SECURITY_PREFIX,\n\t\t\t\tXATTR_SECURITY_PREFIX_LEN)) {\n\t\tconst char *suffix = name + XATTR_SECURITY_PREFIX_LEN;\n\t\tint ret = xattr_getsecurity(inode, suffix, value, size);\n\t\t/*\n\t\t * Only overwrite the return value if a security module\n\t\t * is actually active.\n\t\t */\n\t\tif (ret == -EOPNOTSUPP)\n\t\t\tgoto nolsm;\n\t\treturn ret;\n\t}\nnolsm:\n\treturn __vfs_getxattr(dentry, inode, name, value, size);\n}\nEXPORT_SYMBOL_GPL(vfs_getxattr);\n\nssize_t\nvfs_listxattr(struct dentry *dentry, char *list, size_t size)\n{\n\tstruct inode *inode = d_inode(dentry);\n\tssize_t error;\n\n\terror = security_inode_listxattr(dentry);\n\tif (error)\n\t\treturn error;\n\tif (inode->i_op->listxattr && (inode->i_opflags & IOP_XATTR)) {\n\t\terror = inode->i_op->listxattr(dentry, list, size);\n\t} else {\n\t\terror = security_inode_listsecurity(inode, list, size);\n\t\tif (size && error > size)\n\t\t\terror = -ERANGE;\n\t}\n\treturn error;\n}\nEXPORT_SYMBOL_GPL(vfs_listxattr);\n\nint\n__vfs_removexattr(struct dentry *dentry, const char *name)\n{\n\tstruct inode *inode = d_inode(dentry);\n\tconst struct xattr_handler *handler;\n\n\thandler = xattr_resolve_name(inode, &name);\n\tif (IS_ERR(handler))\n\t\treturn PTR_ERR(handler);\n\tif (!handler->set)\n\t\treturn -EOPNOTSUPP;\n\treturn handler->set(handler, dentry, inode, name, NULL, 0, XATTR_REPLACE);\n}\nEXPORT_SYMBOL(__vfs_removexattr);\n\n/**\n * __vfs_removexattr_locked - set an extended attribute while holding the inode\n * lock\n *\n *  @dentry: object to perform setxattr on\n *  @name: name of xattr to remove\n *  @delegated_inode: on return, will contain an inode pointer that\n *  a delegation was broken on, NULL if none.\n */\nint\n__vfs_removexattr_locked(struct dentry *dentry, const char *name,\n\t\tstruct inode **delegated_inode)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tint error;\n\n\terror = xattr_permission(inode, name, MAY_WRITE);\n\tif (error)\n\t\treturn error;\n\n\terror = security_inode_removexattr(dentry, name);\n\tif (error)\n\t\tgoto out;\n\n\terror = try_break_deleg(inode, delegated_inode);\n\tif (error)\n\t\tgoto out;\n\n\terror = __vfs_removexattr(dentry, name);\n\n\tif (!error) {\n\t\tfsnotify_xattr(dentry);\n\t\tevm_inode_post_removexattr(dentry, name);\n\t}\n\nout:\n\treturn error;\n}\nEXPORT_SYMBOL_GPL(__vfs_removexattr_locked);\n\nint\nvfs_removexattr(struct dentry *dentry, const char *name)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tstruct inode *delegated_inode = NULL;\n\tint error;\n\nretry_deleg:\n\tinode_lock(inode);\n\terror = __vfs_removexattr_locked(dentry, name, &delegated_inode);\n\tinode_unlock(inode);\n\n\tif (delegated_inode) {\n\t\terror = break_deleg_wait(&delegated_inode);\n\t\tif (!error)\n\t\t\tgoto retry_deleg;\n\t}\n\n\treturn error;\n}\nEXPORT_SYMBOL_GPL(vfs_removexattr);\n\n/*\n * Extended attribute SET operations\n */\nstatic long\nsetxattr(struct dentry *d, const char __user *name, const void __user *value,\n\t size_t size, int flags)\n{\n\tint error;\n\tvoid *kvalue = NULL;\n\tchar kname[XATTR_NAME_MAX + 1];\n\n\tif (flags & ~(XATTR_CREATE|XATTR_REPLACE))\n\t\treturn -EINVAL;\n\n\terror = strncpy_from_user(kname, name, sizeof(kname));\n\tif (error == 0 || error == sizeof(kname))\n\t\terror = -ERANGE;\n\tif (error < 0)\n\t\treturn error;\n\n\tif (size) {\n\t\tif (size > XATTR_SIZE_MAX)\n\t\t\treturn -E2BIG;\n\t\tkvalue = kvmalloc(size, GFP_KERNEL);\n\t\tif (!kvalue)\n\t\t\treturn -ENOMEM;\n\t\tif (copy_from_user(kvalue, value, size)) {\n\t\t\terror = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tif ((strcmp(kname, XATTR_NAME_POSIX_ACL_ACCESS) == 0) ||\n\t\t    (strcmp(kname, XATTR_NAME_POSIX_ACL_DEFAULT) == 0))\n\t\t\tposix_acl_fix_xattr_from_user(kvalue, size);\n\t\telse if (strcmp(kname, XATTR_NAME_CAPS) == 0) {\n\t\t\terror = cap_convert_nscap(d, &kvalue, size);\n\t\t\tif (error < 0)\n\t\t\t\tgoto out;\n\t\t\tsize = error;\n\t\t}\n\t}\n\n\terror = vfs_setxattr(d, kname, kvalue, size, flags);\nout:\n\tkvfree(kvalue);\n\n\treturn error;\n}\n\nstatic int path_setxattr(const char __user *pathname,\n\t\t\t const char __user *name, const void __user *value,\n\t\t\t size_t size, int flags, unsigned int lookup_flags)\n{\n\tstruct path path;\n\tint error;\nretry:\n\terror = user_path_at(AT_FDCWD, pathname, lookup_flags, &path);\n\tif (error)\n\t\treturn error;\n\terror = mnt_want_write(path.mnt);\n\tif (!error) {\n\t\terror = setxattr(path.dentry, name, value, size, flags);\n\t\tmnt_drop_write(path.mnt);\n\t}\n\tpath_put(&path);\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\n\treturn error;\n}\n\nSYSCALL_DEFINE5(setxattr, const char __user *, pathname,\n\t\tconst char __user *, name, const void __user *, value,\n\t\tsize_t, size, int, flags)\n{\n\treturn path_setxattr(pathname, name, value, size, flags, LOOKUP_FOLLOW);\n}\n\nSYSCALL_DEFINE5(lsetxattr, const char __user *, pathname,\n\t\tconst char __user *, name, const void __user *, value,\n\t\tsize_t, size, int, flags)\n{\n\treturn path_setxattr(pathname, name, value, size, flags, 0);\n}\n\nSYSCALL_DEFINE5(fsetxattr, int, fd, const char __user *, name,\n\t\tconst void __user *,value, size_t, size, int, flags)\n{\n\tstruct fd f = fdget(fd);\n\tint error = -EBADF;\n\n\tif (!f.file)\n\t\treturn error;\n\taudit_file(f.file);\n\terror = mnt_want_write_file(f.file);\n\tif (!error) {\n\t\terror = setxattr(f.file->f_path.dentry, name, value, size, flags);\n\t\tmnt_drop_write_file(f.file);\n\t}\n\tfdput(f);\n\treturn error;\n}\n\n/*\n * Extended attribute GET operations\n */\nstatic ssize_t\ngetxattr(struct dentry *d, const char __user *name, void __user *value,\n\t size_t size)\n{\n\tssize_t error;\n\tvoid *kvalue = NULL;\n\tchar kname[XATTR_NAME_MAX + 1];\n\n\terror = strncpy_from_user(kname, name, sizeof(kname));\n\tif (error == 0 || error == sizeof(kname))\n\t\terror = -ERANGE;\n\tif (error < 0)\n\t\treturn error;\n\n\tif (size) {\n\t\tif (size > XATTR_SIZE_MAX)\n\t\t\tsize = XATTR_SIZE_MAX;\n\t\tkvalue = kvzalloc(size, GFP_KERNEL);\n\t\tif (!kvalue)\n\t\t\treturn -ENOMEM;\n\t}\n\n\terror = vfs_getxattr(d, kname, kvalue, size);\n\tif (error > 0) {\n\t\tif ((strcmp(kname, XATTR_NAME_POSIX_ACL_ACCESS) == 0) ||\n\t\t    (strcmp(kname, XATTR_NAME_POSIX_ACL_DEFAULT) == 0))\n\t\t\tposix_acl_fix_xattr_to_user(kvalue, error);\n\t\tif (size && copy_to_user(value, kvalue, error))\n\t\t\terror = -EFAULT;\n\t} else if (error == -ERANGE && size >= XATTR_SIZE_MAX) {\n\t\t/* The file system tried to returned a value bigger\n\t\t   than XATTR_SIZE_MAX bytes. Not possible. */\n\t\terror = -E2BIG;\n\t}\n\n\tkvfree(kvalue);\n\n\treturn error;\n}\n\nstatic ssize_t path_getxattr(const char __user *pathname,\n\t\t\t     const char __user *name, void __user *value,\n\t\t\t     size_t size, unsigned int lookup_flags)\n{\n\tstruct path path;\n\tssize_t error;\nretry:\n\terror = user_path_at(AT_FDCWD, pathname, lookup_flags, &path);\n\tif (error)\n\t\treturn error;\n\terror = getxattr(path.dentry, name, value, size);\n\tpath_put(&path);\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\n\treturn error;\n}\n\nSYSCALL_DEFINE4(getxattr, const char __user *, pathname,\n\t\tconst char __user *, name, void __user *, value, size_t, size)\n{\n\treturn path_getxattr(pathname, name, value, size, LOOKUP_FOLLOW);\n}\n\nSYSCALL_DEFINE4(lgetxattr, const char __user *, pathname,\n\t\tconst char __user *, name, void __user *, value, size_t, size)\n{\n\treturn path_getxattr(pathname, name, value, size, 0);\n}\n\nSYSCALL_DEFINE4(fgetxattr, int, fd, const char __user *, name,\n\t\tvoid __user *, value, size_t, size)\n{\n\tstruct fd f = fdget(fd);\n\tssize_t error = -EBADF;\n\n\tif (!f.file)\n\t\treturn error;\n\taudit_file(f.file);\n\terror = getxattr(f.file->f_path.dentry, name, value, size);\n\tfdput(f);\n\treturn error;\n}\n\n/*\n * Extended attribute LIST operations\n */\nstatic ssize_t\nlistxattr(struct dentry *d, char __user *list, size_t size)\n{\n\tssize_t error;\n\tchar *klist = NULL;\n\n\tif (size) {\n\t\tif (size > XATTR_LIST_MAX)\n\t\t\tsize = XATTR_LIST_MAX;\n\t\tklist = kvmalloc(size, GFP_KERNEL);\n\t\tif (!klist)\n\t\t\treturn -ENOMEM;\n\t}\n\n\terror = vfs_listxattr(d, klist, size);\n\tif (error > 0) {\n\t\tif (size && copy_to_user(list, klist, error))\n\t\t\terror = -EFAULT;\n\t} else if (error == -ERANGE && size >= XATTR_LIST_MAX) {\n\t\t/* The file system tried to returned a list bigger\n\t\t   than XATTR_LIST_MAX bytes. Not possible. */\n\t\terror = -E2BIG;\n\t}\n\n\tkvfree(klist);\n\n\treturn error;\n}\n\nstatic ssize_t path_listxattr(const char __user *pathname, char __user *list,\n\t\t\t      size_t size, unsigned int lookup_flags)\n{\n\tstruct path path;\n\tssize_t error;\nretry:\n\terror = user_path_at(AT_FDCWD, pathname, lookup_flags, &path);\n\tif (error)\n\t\treturn error;\n\terror = listxattr(path.dentry, list, size);\n\tpath_put(&path);\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\n\treturn error;\n}\n\nSYSCALL_DEFINE3(listxattr, const char __user *, pathname, char __user *, list,\n\t\tsize_t, size)\n{\n\treturn path_listxattr(pathname, list, size, LOOKUP_FOLLOW);\n}\n\nSYSCALL_DEFINE3(llistxattr, const char __user *, pathname, char __user *, list,\n\t\tsize_t, size)\n{\n\treturn path_listxattr(pathname, list, size, 0);\n}\n\nSYSCALL_DEFINE3(flistxattr, int, fd, char __user *, list, size_t, size)\n{\n\tstruct fd f = fdget(fd);\n\tssize_t error = -EBADF;\n\n\tif (!f.file)\n\t\treturn error;\n\taudit_file(f.file);\n\terror = listxattr(f.file->f_path.dentry, list, size);\n\tfdput(f);\n\treturn error;\n}\n\n/*\n * Extended attribute REMOVE operations\n */\nstatic long\nremovexattr(struct dentry *d, const char __user *name)\n{\n\tint error;\n\tchar kname[XATTR_NAME_MAX + 1];\n\n\terror = strncpy_from_user(kname, name, sizeof(kname));\n\tif (error == 0 || error == sizeof(kname))\n\t\terror = -ERANGE;\n\tif (error < 0)\n\t\treturn error;\n\n\treturn vfs_removexattr(d, kname);\n}\n\nstatic int path_removexattr(const char __user *pathname,\n\t\t\t    const char __user *name, unsigned int lookup_flags)\n{\n\tstruct path path;\n\tint error;\nretry:\n\terror = user_path_at(AT_FDCWD, pathname, lookup_flags, &path);\n\tif (error)\n\t\treturn error;\n\terror = mnt_want_write(path.mnt);\n\tif (!error) {\n\t\terror = removexattr(path.dentry, name);\n\t\tmnt_drop_write(path.mnt);\n\t}\n\tpath_put(&path);\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\n\treturn error;\n}\n\nSYSCALL_DEFINE2(removexattr, const char __user *, pathname,\n\t\tconst char __user *, name)\n{\n\treturn path_removexattr(pathname, name, LOOKUP_FOLLOW);\n}\n\nSYSCALL_DEFINE2(lremovexattr, const char __user *, pathname,\n\t\tconst char __user *, name)\n{\n\treturn path_removexattr(pathname, name, 0);\n}\n\nSYSCALL_DEFINE2(fremovexattr, int, fd, const char __user *, name)\n{\n\tstruct fd f = fdget(fd);\n\tint error = -EBADF;\n\n\tif (!f.file)\n\t\treturn error;\n\taudit_file(f.file);\n\terror = mnt_want_write_file(f.file);\n\tif (!error) {\n\t\terror = removexattr(f.file->f_path.dentry, name);\n\t\tmnt_drop_write_file(f.file);\n\t}\n\tfdput(f);\n\treturn error;\n}\n\n/*\n * Combine the results of the list() operation from every xattr_handler in the\n * list.\n */\nssize_t\ngeneric_listxattr(struct dentry *dentry, char *buffer, size_t buffer_size)\n{\n\tconst struct xattr_handler *handler, **handlers = dentry->d_sb->s_xattr;\n\tunsigned int size = 0;\n\n\tif (!buffer) {\n\t\tfor_each_xattr_handler(handlers, handler) {\n\t\t\tif (!handler->name ||\n\t\t\t    (handler->list && !handler->list(dentry)))\n\t\t\t\tcontinue;\n\t\t\tsize += strlen(handler->name) + 1;\n\t\t}\n\t} else {\n\t\tchar *buf = buffer;\n\t\tsize_t len;\n\n\t\tfor_each_xattr_handler(handlers, handler) {\n\t\t\tif (!handler->name ||\n\t\t\t    (handler->list && !handler->list(dentry)))\n\t\t\t\tcontinue;\n\t\t\tlen = strlen(handler->name);\n\t\t\tif (len + 1 > buffer_size)\n\t\t\t\treturn -ERANGE;\n\t\t\tmemcpy(buf, handler->name, len + 1);\n\t\t\tbuf += len + 1;\n\t\t\tbuffer_size -= len + 1;\n\t\t}\n\t\tsize = buf - buffer;\n\t}\n\treturn size;\n}\nEXPORT_SYMBOL(generic_listxattr);\n\n/**\n * xattr_full_name  -  Compute full attribute name from suffix\n *\n * @handler:\thandler of the xattr_handler operation\n * @name:\tname passed to the xattr_handler operation\n *\n * The get and set xattr handler operations are called with the remainder of\n * the attribute name after skipping the handler's prefix: for example, \"foo\"\n * is passed to the get operation of a handler with prefix \"user.\" to get\n * attribute \"user.foo\".  The full name is still \"there\" in the name though.\n *\n * Note: the list xattr handler operation when called from the vfs is passed a\n * NULL name; some file systems use this operation internally, with varying\n * semantics.\n */\nconst char *xattr_full_name(const struct xattr_handler *handler,\n\t\t\t    const char *name)\n{\n\tsize_t prefix_len = strlen(xattr_prefix(handler));\n\n\treturn name - prefix_len;\n}\nEXPORT_SYMBOL(xattr_full_name);\n\n/*\n * Allocate new xattr and copy in the value; but leave the name to callers.\n */\nstruct simple_xattr *simple_xattr_alloc(const void *value, size_t size)\n{\n\tstruct simple_xattr *new_xattr;\n\tsize_t len;\n\n\t/* wrap around? */\n\tlen = sizeof(*new_xattr) + size;\n\tif (len < sizeof(*new_xattr))\n\t\treturn NULL;\n\n\tnew_xattr = kvmalloc(len, GFP_KERNEL);\n\tif (!new_xattr)\n\t\treturn NULL;\n\n\tnew_xattr->size = size;\n\tmemcpy(new_xattr->value, value, size);\n\treturn new_xattr;\n}\n\n/*\n * xattr GET operation for in-memory/pseudo filesystems\n */\nint simple_xattr_get(struct simple_xattrs *xattrs, const char *name,\n\t\t     void *buffer, size_t size)\n{\n\tstruct simple_xattr *xattr;\n\tint ret = -ENODATA;\n\n\tspin_lock(&xattrs->lock);\n\tlist_for_each_entry(xattr, &xattrs->head, list) {\n\t\tif (strcmp(name, xattr->name))\n\t\t\tcontinue;\n\n\t\tret = xattr->size;\n\t\tif (buffer) {\n\t\t\tif (size < xattr->size)\n\t\t\t\tret = -ERANGE;\n\t\t\telse\n\t\t\t\tmemcpy(buffer, xattr->value, xattr->size);\n\t\t}\n\t\tbreak;\n\t}\n\tspin_unlock(&xattrs->lock);\n\treturn ret;\n}\n\n/**\n * simple_xattr_set - xattr SET operation for in-memory/pseudo filesystems\n * @xattrs: target simple_xattr list\n * @name: name of the extended attribute\n * @value: value of the xattr. If %NULL, will remove the attribute.\n * @size: size of the new xattr\n * @flags: %XATTR_{CREATE|REPLACE}\n * @removed_size: returns size of the removed xattr, -1 if none removed\n *\n * %XATTR_CREATE is set, the xattr shouldn't exist already; otherwise fails\n * with -EEXIST.  If %XATTR_REPLACE is set, the xattr should exist;\n * otherwise, fails with -ENODATA.\n *\n * Returns 0 on success, -errno on failure.\n */\nint simple_xattr_set(struct simple_xattrs *xattrs, const char *name,\n\t\t     const void *value, size_t size, int flags,\n\t\t     ssize_t *removed_size)\n{\n\tstruct simple_xattr *xattr;\n\tstruct simple_xattr *new_xattr = NULL;\n\tint err = 0;\n\n\tif (removed_size)\n\t\t*removed_size = -1;\n\n\t/* value == NULL means remove */\n\tif (value) {\n\t\tnew_xattr = simple_xattr_alloc(value, size);\n\t\tif (!new_xattr)\n\t\t\treturn -ENOMEM;\n\n\t\tnew_xattr->name = kstrdup(name, GFP_KERNEL);\n\t\tif (!new_xattr->name) {\n\t\t\tkvfree(new_xattr);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tspin_lock(&xattrs->lock);\n\tlist_for_each_entry(xattr, &xattrs->head, list) {\n\t\tif (!strcmp(name, xattr->name)) {\n\t\t\tif (flags & XATTR_CREATE) {\n\t\t\t\txattr = new_xattr;\n\t\t\t\terr = -EEXIST;\n\t\t\t} else if (new_xattr) {\n\t\t\t\tlist_replace(&xattr->list, &new_xattr->list);\n\t\t\t\tif (removed_size)\n\t\t\t\t\t*removed_size = xattr->size;\n\t\t\t} else {\n\t\t\t\tlist_del(&xattr->list);\n\t\t\t\tif (removed_size)\n\t\t\t\t\t*removed_size = xattr->size;\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\t}\n\tif (flags & XATTR_REPLACE) {\n\t\txattr = new_xattr;\n\t\terr = -ENODATA;\n\t} else {\n\t\tlist_add(&new_xattr->list, &xattrs->head);\n\t\txattr = NULL;\n\t}\nout:\n\tspin_unlock(&xattrs->lock);\n\tif (xattr) {\n\t\tkfree(xattr->name);\n\t\tkvfree(xattr);\n\t}\n\treturn err;\n\n}\n\nstatic bool xattr_is_trusted(const char *name)\n{\n\treturn !strncmp(name, XATTR_TRUSTED_PREFIX, XATTR_TRUSTED_PREFIX_LEN);\n}\n\nstatic int xattr_list_one(char **buffer, ssize_t *remaining_size,\n\t\t\t  const char *name)\n{\n\tsize_t len = strlen(name) + 1;\n\tif (*buffer) {\n\t\tif (*remaining_size < len)\n\t\t\treturn -ERANGE;\n\t\tmemcpy(*buffer, name, len);\n\t\t*buffer += len;\n\t}\n\t*remaining_size -= len;\n\treturn 0;\n}\n\n/*\n * xattr LIST operation for in-memory/pseudo filesystems\n */\nssize_t simple_xattr_list(struct inode *inode, struct simple_xattrs *xattrs,\n\t\t\t  char *buffer, size_t size)\n{\n\tbool trusted = capable(CAP_SYS_ADMIN);\n\tstruct simple_xattr *xattr;\n\tssize_t remaining_size = size;\n\tint err = 0;\n\n#ifdef CONFIG_FS_POSIX_ACL\n\tif (IS_POSIXACL(inode)) {\n\t\tif (inode->i_acl) {\n\t\t\terr = xattr_list_one(&buffer, &remaining_size,\n\t\t\t\t\t     XATTR_NAME_POSIX_ACL_ACCESS);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\tif (inode->i_default_acl) {\n\t\t\terr = xattr_list_one(&buffer, &remaining_size,\n\t\t\t\t\t     XATTR_NAME_POSIX_ACL_DEFAULT);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t}\n#endif\n\n\tspin_lock(&xattrs->lock);\n\tlist_for_each_entry(xattr, &xattrs->head, list) {\n\t\t/* skip \"trusted.\" attributes for unprivileged callers */\n\t\tif (!trusted && xattr_is_trusted(xattr->name))\n\t\t\tcontinue;\n\n\t\terr = xattr_list_one(&buffer, &remaining_size, xattr->name);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\tspin_unlock(&xattrs->lock);\n\n\treturn err ? err : size - remaining_size;\n}\n\n/*\n * Adds an extended attribute to the list\n */\nvoid simple_xattr_list_add(struct simple_xattrs *xattrs,\n\t\t\t   struct simple_xattr *new_xattr)\n{\n\tspin_lock(&xattrs->lock);\n\tlist_add(&new_xattr->list, &xattrs->head);\n\tspin_unlock(&xattrs->lock);\n}\n"}, "13": {"id": 13, "path": "/src/fs/buffer.c", "content": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n *  linux/fs/buffer.c\n *\n *  Copyright (C) 1991, 1992, 2002  Linus Torvalds\n */\n\n/*\n * Start bdflush() with kernel_thread not syscall - Paul Gortmaker, 12/95\n *\n * Removed a lot of unnecessary code and simplified things now that\n * the buffer cache isn't our primary cache - Andrew Tridgell 12/96\n *\n * Speed up hash, lru, and free list operations.  Use gfp() for allocating\n * hash table, use SLAB cache for buffer heads. SMP threading.  -DaveM\n *\n * Added 32k buffer block sizes - these are required older ARM systems. - RMK\n *\n * async buffer flushing, 1999 Andrea Arcangeli <andrea@suse.de>\n */\n\n#include <linux/kernel.h>\n#include <linux/sched/signal.h>\n#include <linux/syscalls.h>\n#include <linux/fs.h>\n#include <linux/iomap.h>\n#include <linux/mm.h>\n#include <linux/percpu.h>\n#include <linux/slab.h>\n#include <linux/capability.h>\n#include <linux/blkdev.h>\n#include <linux/file.h>\n#include <linux/quotaops.h>\n#include <linux/highmem.h>\n#include <linux/export.h>\n#include <linux/backing-dev.h>\n#include <linux/writeback.h>\n#include <linux/hash.h>\n#include <linux/suspend.h>\n#include <linux/buffer_head.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/bio.h>\n#include <linux/cpu.h>\n#include <linux/bitops.h>\n#include <linux/mpage.h>\n#include <linux/bit_spinlock.h>\n#include <linux/pagevec.h>\n#include <linux/sched/mm.h>\n#include <trace/events/block.h>\n#include <linux/fscrypt.h>\n\n#include \"internal.h\"\n\nstatic int fsync_buffers_list(spinlock_t *lock, struct list_head *list);\nstatic int submit_bh_wbc(int op, int op_flags, struct buffer_head *bh,\n\t\t\t enum rw_hint hint, struct writeback_control *wbc);\n\n#define BH_ENTRY(list) list_entry((list), struct buffer_head, b_assoc_buffers)\n\ninline void touch_buffer(struct buffer_head *bh)\n{\n\ttrace_block_touch_buffer(bh);\n\tmark_page_accessed(bh->b_page);\n}\nEXPORT_SYMBOL(touch_buffer);\n\nvoid __lock_buffer(struct buffer_head *bh)\n{\n\twait_on_bit_lock_io(&bh->b_state, BH_Lock, TASK_UNINTERRUPTIBLE);\n}\nEXPORT_SYMBOL(__lock_buffer);\n\nvoid unlock_buffer(struct buffer_head *bh)\n{\n\tclear_bit_unlock(BH_Lock, &bh->b_state);\n\tsmp_mb__after_atomic();\n\twake_up_bit(&bh->b_state, BH_Lock);\n}\nEXPORT_SYMBOL(unlock_buffer);\n\n/*\n * Returns if the page has dirty or writeback buffers. If all the buffers\n * are unlocked and clean then the PageDirty information is stale. If\n * any of the pages are locked, it is assumed they are locked for IO.\n */\nvoid buffer_check_dirty_writeback(struct page *page,\n\t\t\t\t     bool *dirty, bool *writeback)\n{\n\tstruct buffer_head *head, *bh;\n\t*dirty = false;\n\t*writeback = false;\n\n\tBUG_ON(!PageLocked(page));\n\n\tif (!page_has_buffers(page))\n\t\treturn;\n\n\tif (PageWriteback(page))\n\t\t*writeback = true;\n\n\thead = page_buffers(page);\n\tbh = head;\n\tdo {\n\t\tif (buffer_locked(bh))\n\t\t\t*writeback = true;\n\n\t\tif (buffer_dirty(bh))\n\t\t\t*dirty = true;\n\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n}\nEXPORT_SYMBOL(buffer_check_dirty_writeback);\n\n/*\n * Block until a buffer comes unlocked.  This doesn't stop it\n * from becoming locked again - you have to lock it yourself\n * if you want to preserve its state.\n */\nvoid __wait_on_buffer(struct buffer_head * bh)\n{\n\twait_on_bit_io(&bh->b_state, BH_Lock, TASK_UNINTERRUPTIBLE);\n}\nEXPORT_SYMBOL(__wait_on_buffer);\n\nstatic void buffer_io_error(struct buffer_head *bh, char *msg)\n{\n\tif (!test_bit(BH_Quiet, &bh->b_state))\n\t\tprintk_ratelimited(KERN_ERR\n\t\t\t\"Buffer I/O error on dev %pg, logical block %llu%s\\n\",\n\t\t\tbh->b_bdev, (unsigned long long)bh->b_blocknr, msg);\n}\n\n/*\n * End-of-IO handler helper function which does not touch the bh after\n * unlocking it.\n * Note: unlock_buffer() sort-of does touch the bh after unlocking it, but\n * a race there is benign: unlock_buffer() only use the bh's address for\n * hashing after unlocking the buffer, so it doesn't actually touch the bh\n * itself.\n */\nstatic void __end_buffer_read_notouch(struct buffer_head *bh, int uptodate)\n{\n\tif (uptodate) {\n\t\tset_buffer_uptodate(bh);\n\t} else {\n\t\t/* This happens, due to failed read-ahead attempts. */\n\t\tclear_buffer_uptodate(bh);\n\t}\n\tunlock_buffer(bh);\n}\n\n/*\n * Default synchronous end-of-IO handler..  Just mark it up-to-date and\n * unlock the buffer. This is what ll_rw_block uses too.\n */\nvoid end_buffer_read_sync(struct buffer_head *bh, int uptodate)\n{\n\t__end_buffer_read_notouch(bh, uptodate);\n\tput_bh(bh);\n}\nEXPORT_SYMBOL(end_buffer_read_sync);\n\nvoid end_buffer_write_sync(struct buffer_head *bh, int uptodate)\n{\n\tif (uptodate) {\n\t\tset_buffer_uptodate(bh);\n\t} else {\n\t\tbuffer_io_error(bh, \", lost sync page write\");\n\t\tmark_buffer_write_io_error(bh);\n\t\tclear_buffer_uptodate(bh);\n\t}\n\tunlock_buffer(bh);\n\tput_bh(bh);\n}\nEXPORT_SYMBOL(end_buffer_write_sync);\n\n/*\n * Various filesystems appear to want __find_get_block to be non-blocking.\n * But it's the page lock which protects the buffers.  To get around this,\n * we get exclusion from try_to_free_buffers with the blockdev mapping's\n * private_lock.\n *\n * Hack idea: for the blockdev mapping, private_lock contention\n * may be quite high.  This code could TryLock the page, and if that\n * succeeds, there is no need to take private_lock.\n */\nstatic struct buffer_head *\n__find_get_block_slow(struct block_device *bdev, sector_t block)\n{\n\tstruct inode *bd_inode = bdev->bd_inode;\n\tstruct address_space *bd_mapping = bd_inode->i_mapping;\n\tstruct buffer_head *ret = NULL;\n\tpgoff_t index;\n\tstruct buffer_head *bh;\n\tstruct buffer_head *head;\n\tstruct page *page;\n\tint all_mapped = 1;\n\tstatic DEFINE_RATELIMIT_STATE(last_warned, HZ, 1);\n\n\tindex = block >> (PAGE_SHIFT - bd_inode->i_blkbits);\n\tpage = find_get_page_flags(bd_mapping, index, FGP_ACCESSED);\n\tif (!page)\n\t\tgoto out;\n\n\tspin_lock(&bd_mapping->private_lock);\n\tif (!page_has_buffers(page))\n\t\tgoto out_unlock;\n\thead = page_buffers(page);\n\tbh = head;\n\tdo {\n\t\tif (!buffer_mapped(bh))\n\t\t\tall_mapped = 0;\n\t\telse if (bh->b_blocknr == block) {\n\t\t\tret = bh;\n\t\t\tget_bh(bh);\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\n\t/* we might be here because some of the buffers on this page are\n\t * not mapped.  This is due to various races between\n\t * file io on the block device and getblk.  It gets dealt with\n\t * elsewhere, don't buffer_error if we had some unmapped buffers\n\t */\n\tratelimit_set_flags(&last_warned, RATELIMIT_MSG_ON_RELEASE);\n\tif (all_mapped && __ratelimit(&last_warned)) {\n\t\tprintk(\"__find_get_block_slow() failed. block=%llu, \"\n\t\t       \"b_blocknr=%llu, b_state=0x%08lx, b_size=%zu, \"\n\t\t       \"device %pg blocksize: %d\\n\",\n\t\t       (unsigned long long)block,\n\t\t       (unsigned long long)bh->b_blocknr,\n\t\t       bh->b_state, bh->b_size, bdev,\n\t\t       1 << bd_inode->i_blkbits);\n\t}\nout_unlock:\n\tspin_unlock(&bd_mapping->private_lock);\n\tput_page(page);\nout:\n\treturn ret;\n}\n\nstatic void end_buffer_async_read(struct buffer_head *bh, int uptodate)\n{\n\tunsigned long flags;\n\tstruct buffer_head *first;\n\tstruct buffer_head *tmp;\n\tstruct page *page;\n\tint page_uptodate = 1;\n\n\tBUG_ON(!buffer_async_read(bh));\n\n\tpage = bh->b_page;\n\tif (uptodate) {\n\t\tset_buffer_uptodate(bh);\n\t} else {\n\t\tclear_buffer_uptodate(bh);\n\t\tbuffer_io_error(bh, \", async page read\");\n\t\tSetPageError(page);\n\t}\n\n\t/*\n\t * Be _very_ careful from here on. Bad things can happen if\n\t * two buffer heads end IO at almost the same time and both\n\t * decide that the page is now completely done.\n\t */\n\tfirst = page_buffers(page);\n\tspin_lock_irqsave(&first->b_uptodate_lock, flags);\n\tclear_buffer_async_read(bh);\n\tunlock_buffer(bh);\n\ttmp = bh;\n\tdo {\n\t\tif (!buffer_uptodate(tmp))\n\t\t\tpage_uptodate = 0;\n\t\tif (buffer_async_read(tmp)) {\n\t\t\tBUG_ON(!buffer_locked(tmp));\n\t\t\tgoto still_busy;\n\t\t}\n\t\ttmp = tmp->b_this_page;\n\t} while (tmp != bh);\n\tspin_unlock_irqrestore(&first->b_uptodate_lock, flags);\n\n\t/*\n\t * If none of the buffers had errors and they are all\n\t * uptodate then we can set the page uptodate.\n\t */\n\tif (page_uptodate && !PageError(page))\n\t\tSetPageUptodate(page);\n\tunlock_page(page);\n\treturn;\n\nstill_busy:\n\tspin_unlock_irqrestore(&first->b_uptodate_lock, flags);\n\treturn;\n}\n\nstruct decrypt_bh_ctx {\n\tstruct work_struct work;\n\tstruct buffer_head *bh;\n};\n\nstatic void decrypt_bh(struct work_struct *work)\n{\n\tstruct decrypt_bh_ctx *ctx =\n\t\tcontainer_of(work, struct decrypt_bh_ctx, work);\n\tstruct buffer_head *bh = ctx->bh;\n\tint err;\n\n\terr = fscrypt_decrypt_pagecache_blocks(bh->b_page, bh->b_size,\n\t\t\t\t\t       bh_offset(bh));\n\tend_buffer_async_read(bh, err == 0);\n\tkfree(ctx);\n}\n\n/*\n * I/O completion handler for block_read_full_page() - pages\n * which come unlocked at the end of I/O.\n */\nstatic void end_buffer_async_read_io(struct buffer_head *bh, int uptodate)\n{\n\t/* Decrypt if needed */\n\tif (uptodate &&\n\t    fscrypt_inode_uses_fs_layer_crypto(bh->b_page->mapping->host)) {\n\t\tstruct decrypt_bh_ctx *ctx = kmalloc(sizeof(*ctx), GFP_ATOMIC);\n\n\t\tif (ctx) {\n\t\t\tINIT_WORK(&ctx->work, decrypt_bh);\n\t\t\tctx->bh = bh;\n\t\t\tfscrypt_enqueue_decrypt_work(&ctx->work);\n\t\t\treturn;\n\t\t}\n\t\tuptodate = 0;\n\t}\n\tend_buffer_async_read(bh, uptodate);\n}\n\n/*\n * Completion handler for block_write_full_page() - pages which are unlocked\n * during I/O, and which have PageWriteback cleared upon I/O completion.\n */\nvoid end_buffer_async_write(struct buffer_head *bh, int uptodate)\n{\n\tunsigned long flags;\n\tstruct buffer_head *first;\n\tstruct buffer_head *tmp;\n\tstruct page *page;\n\n\tBUG_ON(!buffer_async_write(bh));\n\n\tpage = bh->b_page;\n\tif (uptodate) {\n\t\tset_buffer_uptodate(bh);\n\t} else {\n\t\tbuffer_io_error(bh, \", lost async page write\");\n\t\tmark_buffer_write_io_error(bh);\n\t\tclear_buffer_uptodate(bh);\n\t\tSetPageError(page);\n\t}\n\n\tfirst = page_buffers(page);\n\tspin_lock_irqsave(&first->b_uptodate_lock, flags);\n\n\tclear_buffer_async_write(bh);\n\tunlock_buffer(bh);\n\ttmp = bh->b_this_page;\n\twhile (tmp != bh) {\n\t\tif (buffer_async_write(tmp)) {\n\t\t\tBUG_ON(!buffer_locked(tmp));\n\t\t\tgoto still_busy;\n\t\t}\n\t\ttmp = tmp->b_this_page;\n\t}\n\tspin_unlock_irqrestore(&first->b_uptodate_lock, flags);\n\tend_page_writeback(page);\n\treturn;\n\nstill_busy:\n\tspin_unlock_irqrestore(&first->b_uptodate_lock, flags);\n\treturn;\n}\nEXPORT_SYMBOL(end_buffer_async_write);\n\n/*\n * If a page's buffers are under async readin (end_buffer_async_read\n * completion) then there is a possibility that another thread of\n * control could lock one of the buffers after it has completed\n * but while some of the other buffers have not completed.  This\n * locked buffer would confuse end_buffer_async_read() into not unlocking\n * the page.  So the absence of BH_Async_Read tells end_buffer_async_read()\n * that this buffer is not under async I/O.\n *\n * The page comes unlocked when it has no locked buffer_async buffers\n * left.\n *\n * PageLocked prevents anyone starting new async I/O reads any of\n * the buffers.\n *\n * PageWriteback is used to prevent simultaneous writeout of the same\n * page.\n *\n * PageLocked prevents anyone from starting writeback of a page which is\n * under read I/O (PageWriteback is only ever set against a locked page).\n */\nstatic void mark_buffer_async_read(struct buffer_head *bh)\n{\n\tbh->b_end_io = end_buffer_async_read_io;\n\tset_buffer_async_read(bh);\n}\n\nstatic void mark_buffer_async_write_endio(struct buffer_head *bh,\n\t\t\t\t\t  bh_end_io_t *handler)\n{\n\tbh->b_end_io = handler;\n\tset_buffer_async_write(bh);\n}\n\nvoid mark_buffer_async_write(struct buffer_head *bh)\n{\n\tmark_buffer_async_write_endio(bh, end_buffer_async_write);\n}\nEXPORT_SYMBOL(mark_buffer_async_write);\n\n\n/*\n * fs/buffer.c contains helper functions for buffer-backed address space's\n * fsync functions.  A common requirement for buffer-based filesystems is\n * that certain data from the backing blockdev needs to be written out for\n * a successful fsync().  For example, ext2 indirect blocks need to be\n * written back and waited upon before fsync() returns.\n *\n * The functions mark_buffer_inode_dirty(), fsync_inode_buffers(),\n * inode_has_buffers() and invalidate_inode_buffers() are provided for the\n * management of a list of dependent buffers at ->i_mapping->private_list.\n *\n * Locking is a little subtle: try_to_free_buffers() will remove buffers\n * from their controlling inode's queue when they are being freed.  But\n * try_to_free_buffers() will be operating against the *blockdev* mapping\n * at the time, not against the S_ISREG file which depends on those buffers.\n * So the locking for private_list is via the private_lock in the address_space\n * which backs the buffers.  Which is different from the address_space \n * against which the buffers are listed.  So for a particular address_space,\n * mapping->private_lock does *not* protect mapping->private_list!  In fact,\n * mapping->private_list will always be protected by the backing blockdev's\n * ->private_lock.\n *\n * Which introduces a requirement: all buffers on an address_space's\n * ->private_list must be from the same address_space: the blockdev's.\n *\n * address_spaces which do not place buffers at ->private_list via these\n * utility functions are free to use private_lock and private_list for\n * whatever they want.  The only requirement is that list_empty(private_list)\n * be true at clear_inode() time.\n *\n * FIXME: clear_inode should not call invalidate_inode_buffers().  The\n * filesystems should do that.  invalidate_inode_buffers() should just go\n * BUG_ON(!list_empty).\n *\n * FIXME: mark_buffer_dirty_inode() is a data-plane operation.  It should\n * take an address_space, not an inode.  And it should be called\n * mark_buffer_dirty_fsync() to clearly define why those buffers are being\n * queued up.\n *\n * FIXME: mark_buffer_dirty_inode() doesn't need to add the buffer to the\n * list if it is already on a list.  Because if the buffer is on a list,\n * it *must* already be on the right one.  If not, the filesystem is being\n * silly.  This will save a ton of locking.  But first we have to ensure\n * that buffers are taken *off* the old inode's list when they are freed\n * (presumably in truncate).  That requires careful auditing of all\n * filesystems (do it inside bforget()).  It could also be done by bringing\n * b_inode back.\n */\n\n/*\n * The buffer's backing address_space's private_lock must be held\n */\nstatic void __remove_assoc_queue(struct buffer_head *bh)\n{\n\tlist_del_init(&bh->b_assoc_buffers);\n\tWARN_ON(!bh->b_assoc_map);\n\tbh->b_assoc_map = NULL;\n}\n\nint inode_has_buffers(struct inode *inode)\n{\n\treturn !list_empty(&inode->i_data.private_list);\n}\n\n/*\n * osync is designed to support O_SYNC io.  It waits synchronously for\n * all already-submitted IO to complete, but does not queue any new\n * writes to the disk.\n *\n * To do O_SYNC writes, just queue the buffer writes with ll_rw_block as\n * you dirty the buffers, and then use osync_inode_buffers to wait for\n * completion.  Any other dirty buffers which are not yet queued for\n * write will not be flushed to disk by the osync.\n */\nstatic int osync_buffers_list(spinlock_t *lock, struct list_head *list)\n{\n\tstruct buffer_head *bh;\n\tstruct list_head *p;\n\tint err = 0;\n\n\tspin_lock(lock);\nrepeat:\n\tlist_for_each_prev(p, list) {\n\t\tbh = BH_ENTRY(p);\n\t\tif (buffer_locked(bh)) {\n\t\t\tget_bh(bh);\n\t\t\tspin_unlock(lock);\n\t\t\twait_on_buffer(bh);\n\t\t\tif (!buffer_uptodate(bh))\n\t\t\t\terr = -EIO;\n\t\t\tbrelse(bh);\n\t\t\tspin_lock(lock);\n\t\t\tgoto repeat;\n\t\t}\n\t}\n\tspin_unlock(lock);\n\treturn err;\n}\n\nvoid emergency_thaw_bdev(struct super_block *sb)\n{\n\twhile (sb->s_bdev && !thaw_bdev(sb->s_bdev, sb))\n\t\tprintk(KERN_WARNING \"Emergency Thaw on %pg\\n\", sb->s_bdev);\n}\n\n/**\n * sync_mapping_buffers - write out & wait upon a mapping's \"associated\" buffers\n * @mapping: the mapping which wants those buffers written\n *\n * Starts I/O against the buffers at mapping->private_list, and waits upon\n * that I/O.\n *\n * Basically, this is a convenience function for fsync().\n * @mapping is a file or directory which needs those buffers to be written for\n * a successful fsync().\n */\nint sync_mapping_buffers(struct address_space *mapping)\n{\n\tstruct address_space *buffer_mapping = mapping->private_data;\n\n\tif (buffer_mapping == NULL || list_empty(&mapping->private_list))\n\t\treturn 0;\n\n\treturn fsync_buffers_list(&buffer_mapping->private_lock,\n\t\t\t\t\t&mapping->private_list);\n}\nEXPORT_SYMBOL(sync_mapping_buffers);\n\n/*\n * Called when we've recently written block `bblock', and it is known that\n * `bblock' was for a buffer_boundary() buffer.  This means that the block at\n * `bblock + 1' is probably a dirty indirect block.  Hunt it down and, if it's\n * dirty, schedule it for IO.  So that indirects merge nicely with their data.\n */\nvoid write_boundary_block(struct block_device *bdev,\n\t\t\tsector_t bblock, unsigned blocksize)\n{\n\tstruct buffer_head *bh = __find_get_block(bdev, bblock + 1, blocksize);\n\tif (bh) {\n\t\tif (buffer_dirty(bh))\n\t\t\tll_rw_block(REQ_OP_WRITE, 0, 1, &bh);\n\t\tput_bh(bh);\n\t}\n}\n\nvoid mark_buffer_dirty_inode(struct buffer_head *bh, struct inode *inode)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct address_space *buffer_mapping = bh->b_page->mapping;\n\n\tmark_buffer_dirty(bh);\n\tif (!mapping->private_data) {\n\t\tmapping->private_data = buffer_mapping;\n\t} else {\n\t\tBUG_ON(mapping->private_data != buffer_mapping);\n\t}\n\tif (!bh->b_assoc_map) {\n\t\tspin_lock(&buffer_mapping->private_lock);\n\t\tlist_move_tail(&bh->b_assoc_buffers,\n\t\t\t\t&mapping->private_list);\n\t\tbh->b_assoc_map = mapping;\n\t\tspin_unlock(&buffer_mapping->private_lock);\n\t}\n}\nEXPORT_SYMBOL(mark_buffer_dirty_inode);\n\n/*\n * Mark the page dirty, and set it dirty in the page cache, and mark the inode\n * dirty.\n *\n * If warn is true, then emit a warning if the page is not uptodate and has\n * not been truncated.\n *\n * The caller must hold lock_page_memcg().\n */\nvoid __set_page_dirty(struct page *page, struct address_space *mapping,\n\t\t\t     int warn)\n{\n\tunsigned long flags;\n\n\txa_lock_irqsave(&mapping->i_pages, flags);\n\tif (page->mapping) {\t/* Race with truncate? */\n\t\tWARN_ON_ONCE(warn && !PageUptodate(page));\n\t\taccount_page_dirtied(page, mapping);\n\t\t__xa_set_mark(&mapping->i_pages, page_index(page),\n\t\t\t\tPAGECACHE_TAG_DIRTY);\n\t}\n\txa_unlock_irqrestore(&mapping->i_pages, flags);\n}\nEXPORT_SYMBOL_GPL(__set_page_dirty);\n\n/*\n * Add a page to the dirty page list.\n *\n * It is a sad fact of life that this function is called from several places\n * deeply under spinlocking.  It may not sleep.\n *\n * If the page has buffers, the uptodate buffers are set dirty, to preserve\n * dirty-state coherency between the page and the buffers.  It the page does\n * not have buffers then when they are later attached they will all be set\n * dirty.\n *\n * The buffers are dirtied before the page is dirtied.  There's a small race\n * window in which a writepage caller may see the page cleanness but not the\n * buffer dirtiness.  That's fine.  If this code were to set the page dirty\n * before the buffers, a concurrent writepage caller could clear the page dirty\n * bit, see a bunch of clean buffers and we'd end up with dirty buffers/clean\n * page on the dirty page list.\n *\n * We use private_lock to lock against try_to_free_buffers while using the\n * page's buffer list.  Also use this to protect against clean buffers being\n * added to the page after it was set dirty.\n *\n * FIXME: may need to call ->reservepage here as well.  That's rather up to the\n * address_space though.\n */\nint __set_page_dirty_buffers(struct page *page)\n{\n\tint newly_dirty;\n\tstruct address_space *mapping = page_mapping(page);\n\n\tif (unlikely(!mapping))\n\t\treturn !TestSetPageDirty(page);\n\n\tspin_lock(&mapping->private_lock);\n\tif (page_has_buffers(page)) {\n\t\tstruct buffer_head *head = page_buffers(page);\n\t\tstruct buffer_head *bh = head;\n\n\t\tdo {\n\t\t\tset_buffer_dirty(bh);\n\t\t\tbh = bh->b_this_page;\n\t\t} while (bh != head);\n\t}\n\t/*\n\t * Lock out page's memcg migration to keep PageDirty\n\t * synchronized with per-memcg dirty page counters.\n\t */\n\tlock_page_memcg(page);\n\tnewly_dirty = !TestSetPageDirty(page);\n\tspin_unlock(&mapping->private_lock);\n\n\tif (newly_dirty)\n\t\t__set_page_dirty(page, mapping, 1);\n\n\tunlock_page_memcg(page);\n\n\tif (newly_dirty)\n\t\t__mark_inode_dirty(mapping->host, I_DIRTY_PAGES);\n\n\treturn newly_dirty;\n}\nEXPORT_SYMBOL(__set_page_dirty_buffers);\n\n/*\n * Write out and wait upon a list of buffers.\n *\n * We have conflicting pressures: we want to make sure that all\n * initially dirty buffers get waited on, but that any subsequently\n * dirtied buffers don't.  After all, we don't want fsync to last\n * forever if somebody is actively writing to the file.\n *\n * Do this in two main stages: first we copy dirty buffers to a\n * temporary inode list, queueing the writes as we go.  Then we clean\n * up, waiting for those writes to complete.\n * \n * During this second stage, any subsequent updates to the file may end\n * up refiling the buffer on the original inode's dirty list again, so\n * there is a chance we will end up with a buffer queued for write but\n * not yet completed on that list.  So, as a final cleanup we go through\n * the osync code to catch these locked, dirty buffers without requeuing\n * any newly dirty buffers for write.\n */\nstatic int fsync_buffers_list(spinlock_t *lock, struct list_head *list)\n{\n\tstruct buffer_head *bh;\n\tstruct list_head tmp;\n\tstruct address_space *mapping;\n\tint err = 0, err2;\n\tstruct blk_plug plug;\n\n\tINIT_LIST_HEAD(&tmp);\n\tblk_start_plug(&plug);\n\n\tspin_lock(lock);\n\twhile (!list_empty(list)) {\n\t\tbh = BH_ENTRY(list->next);\n\t\tmapping = bh->b_assoc_map;\n\t\t__remove_assoc_queue(bh);\n\t\t/* Avoid race with mark_buffer_dirty_inode() which does\n\t\t * a lockless check and we rely on seeing the dirty bit */\n\t\tsmp_mb();\n\t\tif (buffer_dirty(bh) || buffer_locked(bh)) {\n\t\t\tlist_add(&bh->b_assoc_buffers, &tmp);\n\t\t\tbh->b_assoc_map = mapping;\n\t\t\tif (buffer_dirty(bh)) {\n\t\t\t\tget_bh(bh);\n\t\t\t\tspin_unlock(lock);\n\t\t\t\t/*\n\t\t\t\t * Ensure any pending I/O completes so that\n\t\t\t\t * write_dirty_buffer() actually writes the\n\t\t\t\t * current contents - it is a noop if I/O is\n\t\t\t\t * still in flight on potentially older\n\t\t\t\t * contents.\n\t\t\t\t */\n\t\t\t\twrite_dirty_buffer(bh, REQ_SYNC);\n\n\t\t\t\t/*\n\t\t\t\t * Kick off IO for the previous mapping. Note\n\t\t\t\t * that we will not run the very last mapping,\n\t\t\t\t * wait_on_buffer() will do that for us\n\t\t\t\t * through sync_buffer().\n\t\t\t\t */\n\t\t\t\tbrelse(bh);\n\t\t\t\tspin_lock(lock);\n\t\t\t}\n\t\t}\n\t}\n\n\tspin_unlock(lock);\n\tblk_finish_plug(&plug);\n\tspin_lock(lock);\n\n\twhile (!list_empty(&tmp)) {\n\t\tbh = BH_ENTRY(tmp.prev);\n\t\tget_bh(bh);\n\t\tmapping = bh->b_assoc_map;\n\t\t__remove_assoc_queue(bh);\n\t\t/* Avoid race with mark_buffer_dirty_inode() which does\n\t\t * a lockless check and we rely on seeing the dirty bit */\n\t\tsmp_mb();\n\t\tif (buffer_dirty(bh)) {\n\t\t\tlist_add(&bh->b_assoc_buffers,\n\t\t\t\t &mapping->private_list);\n\t\t\tbh->b_assoc_map = mapping;\n\t\t}\n\t\tspin_unlock(lock);\n\t\twait_on_buffer(bh);\n\t\tif (!buffer_uptodate(bh))\n\t\t\terr = -EIO;\n\t\tbrelse(bh);\n\t\tspin_lock(lock);\n\t}\n\t\n\tspin_unlock(lock);\n\terr2 = osync_buffers_list(lock, list);\n\tif (err)\n\t\treturn err;\n\telse\n\t\treturn err2;\n}\n\n/*\n * Invalidate any and all dirty buffers on a given inode.  We are\n * probably unmounting the fs, but that doesn't mean we have already\n * done a sync().  Just drop the buffers from the inode list.\n *\n * NOTE: we take the inode's blockdev's mapping's private_lock.  Which\n * assumes that all the buffers are against the blockdev.  Not true\n * for reiserfs.\n */\nvoid invalidate_inode_buffers(struct inode *inode)\n{\n\tif (inode_has_buffers(inode)) {\n\t\tstruct address_space *mapping = &inode->i_data;\n\t\tstruct list_head *list = &mapping->private_list;\n\t\tstruct address_space *buffer_mapping = mapping->private_data;\n\n\t\tspin_lock(&buffer_mapping->private_lock);\n\t\twhile (!list_empty(list))\n\t\t\t__remove_assoc_queue(BH_ENTRY(list->next));\n\t\tspin_unlock(&buffer_mapping->private_lock);\n\t}\n}\nEXPORT_SYMBOL(invalidate_inode_buffers);\n\n/*\n * Remove any clean buffers from the inode's buffer list.  This is called\n * when we're trying to free the inode itself.  Those buffers can pin it.\n *\n * Returns true if all buffers were removed.\n */\nint remove_inode_buffers(struct inode *inode)\n{\n\tint ret = 1;\n\n\tif (inode_has_buffers(inode)) {\n\t\tstruct address_space *mapping = &inode->i_data;\n\t\tstruct list_head *list = &mapping->private_list;\n\t\tstruct address_space *buffer_mapping = mapping->private_data;\n\n\t\tspin_lock(&buffer_mapping->private_lock);\n\t\twhile (!list_empty(list)) {\n\t\t\tstruct buffer_head *bh = BH_ENTRY(list->next);\n\t\t\tif (buffer_dirty(bh)) {\n\t\t\t\tret = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t__remove_assoc_queue(bh);\n\t\t}\n\t\tspin_unlock(&buffer_mapping->private_lock);\n\t}\n\treturn ret;\n}\n\n/*\n * Create the appropriate buffers when given a page for data area and\n * the size of each buffer.. Use the bh->b_this_page linked list to\n * follow the buffers created.  Return NULL if unable to create more\n * buffers.\n *\n * The retry flag is used to differentiate async IO (paging, swapping)\n * which may not fail from ordinary buffer allocations.\n */\nstruct buffer_head *alloc_page_buffers(struct page *page, unsigned long size,\n\t\tbool retry)\n{\n\tstruct buffer_head *bh, *head;\n\tgfp_t gfp = GFP_NOFS | __GFP_ACCOUNT;\n\tlong offset;\n\tstruct mem_cgroup *memcg, *old_memcg;\n\n\tif (retry)\n\t\tgfp |= __GFP_NOFAIL;\n\n\tmemcg = get_mem_cgroup_from_page(page);\n\told_memcg = set_active_memcg(memcg);\n\n\thead = NULL;\n\toffset = PAGE_SIZE;\n\twhile ((offset -= size) >= 0) {\n\t\tbh = alloc_buffer_head(gfp);\n\t\tif (!bh)\n\t\t\tgoto no_grow;\n\n\t\tbh->b_this_page = head;\n\t\tbh->b_blocknr = -1;\n\t\thead = bh;\n\n\t\tbh->b_size = size;\n\n\t\t/* Link the buffer to its page */\n\t\tset_bh_page(bh, page, offset);\n\t}\nout:\n\tset_active_memcg(old_memcg);\n\tmem_cgroup_put(memcg);\n\treturn head;\n/*\n * In case anything failed, we just free everything we got.\n */\nno_grow:\n\tif (head) {\n\t\tdo {\n\t\t\tbh = head;\n\t\t\thead = head->b_this_page;\n\t\t\tfree_buffer_head(bh);\n\t\t} while (head);\n\t}\n\n\tgoto out;\n}\nEXPORT_SYMBOL_GPL(alloc_page_buffers);\n\nstatic inline void\nlink_dev_buffers(struct page *page, struct buffer_head *head)\n{\n\tstruct buffer_head *bh, *tail;\n\n\tbh = head;\n\tdo {\n\t\ttail = bh;\n\t\tbh = bh->b_this_page;\n\t} while (bh);\n\ttail->b_this_page = head;\n\tattach_page_private(page, head);\n}\n\nstatic sector_t blkdev_max_block(struct block_device *bdev, unsigned int size)\n{\n\tsector_t retval = ~((sector_t)0);\n\tloff_t sz = i_size_read(bdev->bd_inode);\n\n\tif (sz) {\n\t\tunsigned int sizebits = blksize_bits(size);\n\t\tretval = (sz >> sizebits);\n\t}\n\treturn retval;\n}\n\n/*\n * Initialise the state of a blockdev page's buffers.\n */ \nstatic sector_t\ninit_page_buffers(struct page *page, struct block_device *bdev,\n\t\t\tsector_t block, int size)\n{\n\tstruct buffer_head *head = page_buffers(page);\n\tstruct buffer_head *bh = head;\n\tint uptodate = PageUptodate(page);\n\tsector_t end_block = blkdev_max_block(I_BDEV(bdev->bd_inode), size);\n\n\tdo {\n\t\tif (!buffer_mapped(bh)) {\n\t\t\tbh->b_end_io = NULL;\n\t\t\tbh->b_private = NULL;\n\t\t\tbh->b_bdev = bdev;\n\t\t\tbh->b_blocknr = block;\n\t\t\tif (uptodate)\n\t\t\t\tset_buffer_uptodate(bh);\n\t\t\tif (block < end_block)\n\t\t\t\tset_buffer_mapped(bh);\n\t\t}\n\t\tblock++;\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\n\t/*\n\t * Caller needs to validate requested block against end of device.\n\t */\n\treturn end_block;\n}\n\n/*\n * Create the page-cache page that contains the requested block.\n *\n * This is used purely for blockdev mappings.\n */\nstatic int\ngrow_dev_page(struct block_device *bdev, sector_t block,\n\t      pgoff_t index, int size, int sizebits, gfp_t gfp)\n{\n\tstruct inode *inode = bdev->bd_inode;\n\tstruct page *page;\n\tstruct buffer_head *bh;\n\tsector_t end_block;\n\tint ret = 0;\n\tgfp_t gfp_mask;\n\n\tgfp_mask = mapping_gfp_constraint(inode->i_mapping, ~__GFP_FS) | gfp;\n\n\t/*\n\t * XXX: __getblk_slow() can not really deal with failure and\n\t * will endlessly loop on improvised global reclaim.  Prefer\n\t * looping in the allocator rather than here, at least that\n\t * code knows what it's doing.\n\t */\n\tgfp_mask |= __GFP_NOFAIL;\n\n\tpage = find_or_create_page(inode->i_mapping, index, gfp_mask);\n\n\tBUG_ON(!PageLocked(page));\n\n\tif (page_has_buffers(page)) {\n\t\tbh = page_buffers(page);\n\t\tif (bh->b_size == size) {\n\t\t\tend_block = init_page_buffers(page, bdev,\n\t\t\t\t\t\t(sector_t)index << sizebits,\n\t\t\t\t\t\tsize);\n#ifdef CONFIG_DEBUG_AID_FOR_SYZBOT\n\t\t\tcurrent->getblk_executed |= 0x01;\n#endif\n\t\t\tgoto done;\n\t\t}\n\t\tif (!try_to_free_buffers(page)) {\n#ifdef CONFIG_DEBUG_AID_FOR_SYZBOT\n\t\t\tcurrent->getblk_executed |= 0x02;\n#endif\n\t\t\tgoto failed;\n\t\t}\n#ifdef CONFIG_DEBUG_AID_FOR_SYZBOT\n\t\tcurrent->getblk_executed |= 0x04;\n#endif\n\t}\n\n\t/*\n\t * Allocate some buffers for this page\n\t */\n\tbh = alloc_page_buffers(page, size, true);\n\n\t/*\n\t * Link the page to the buffers and initialise them.  Take the\n\t * lock to be atomic wrt __find_get_block(), which does not\n\t * run under the page lock.\n\t */\n\tspin_lock(&inode->i_mapping->private_lock);\n\tlink_dev_buffers(page, bh);\n\tend_block = init_page_buffers(page, bdev, (sector_t)index << sizebits,\n\t\t\tsize);\n\tspin_unlock(&inode->i_mapping->private_lock);\ndone:\n\tret = (block < end_block) ? 1 : -ENXIO;\n#ifdef CONFIG_DEBUG_AID_FOR_SYZBOT\n\tcurrent->getblk_executed |= 0x08;\n#endif\nfailed:\n\tunlock_page(page);\n\tput_page(page);\n\treturn ret;\n}\n\n/*\n * Create buffers for the specified block device block's page.  If\n * that page was dirty, the buffers are set dirty also.\n */\nstatic int\ngrow_buffers(struct block_device *bdev, sector_t block, int size, gfp_t gfp)\n{\n\tpgoff_t index;\n\tint sizebits;\n\n\tsizebits = -1;\n\tdo {\n\t\tsizebits++;\n\t} while ((size << sizebits) < PAGE_SIZE);\n\n\tindex = block >> sizebits;\n\n\t/*\n\t * Check for a block which wants to lie outside our maximum possible\n\t * pagecache index.  (this comparison is done using sector_t types).\n\t */\n\tif (unlikely(index != block >> sizebits)) {\n\t\tprintk(KERN_ERR \"%s: requested out-of-range block %llu for \"\n\t\t\t\"device %pg\\n\",\n\t\t\t__func__, (unsigned long long)block,\n\t\t\tbdev);\n\t\treturn -EIO;\n\t}\n\n\t/* Create a page with the proper size buffers.. */\n\treturn grow_dev_page(bdev, block, index, size, sizebits, gfp);\n}\n\nstatic struct buffer_head *\n__getblk_slow(struct block_device *bdev, sector_t block,\n\t     unsigned size, gfp_t gfp)\n{\n\t/* Size must be multiple of hard sectorsize */\n\tif (unlikely(size & (bdev_logical_block_size(bdev)-1) ||\n\t\t\t(size < 512 || size > PAGE_SIZE))) {\n\t\tprintk(KERN_ERR \"getblk(): invalid block size %d requested\\n\",\n\t\t\t\t\tsize);\n\t\tprintk(KERN_ERR \"logical block size: %d\\n\",\n\t\t\t\t\tbdev_logical_block_size(bdev));\n\n\t\tdump_stack();\n\t\treturn NULL;\n\t}\n\n#ifdef CONFIG_DEBUG_AID_FOR_SYZBOT\n\tcurrent->getblk_stamp = jiffies;\n\tcurrent->getblk_executed = 0;\n\tcurrent->getblk_bh_count = 0;\n\tcurrent->getblk_bh_state = 0;\n#endif\n\tfor (;;) {\n\t\tstruct buffer_head *bh;\n\t\tint ret;\n\n\t\tbh = __find_get_block(bdev, block, size);\n\t\tif (bh)\n\t\t\treturn bh;\n\n\t\tret = grow_buffers(bdev, block, size, gfp);\n\t\tif (ret < 0)\n\t\t\treturn NULL;\n\n#ifdef CONFIG_DEBUG_AID_FOR_SYZBOT\n\t\tif (!time_after(jiffies, current->getblk_stamp + 3 * HZ))\n\t\t\tcontinue;\n\t\tprintk(KERN_ERR \"%s(%u): getblk(): executed=%x bh_count=%d bh_state=%lx bdev_super_blocksize=%ld size=%u bdev_super_blocksize_bits=%d bdev_inode_blkbits=%d\\n\",\n\t\t       current->comm, current->pid, current->getblk_executed,\n\t\t       current->getblk_bh_count, current->getblk_bh_state,\n\t\t       IS_ERR_OR_NULL(bdev->bd_super) ? -1L :\n\t\t       bdev->bd_super->s_blocksize, size,\n\t\t       IS_ERR_OR_NULL(bdev->bd_super) ? -1 :\n\t\t       bdev->bd_super->s_blocksize_bits,\n\t\t       IS_ERR_OR_NULL(bdev->bd_inode) ? -1 :\n\t\t       bdev->bd_inode->i_blkbits);\n\t\tcurrent->getblk_executed = 0;\n\t\tcurrent->getblk_bh_count = 0;\n\t\tcurrent->getblk_bh_state = 0;\n\t\tcurrent->getblk_stamp = jiffies;\n#endif\n\t}\n}\n\n/*\n * The relationship between dirty buffers and dirty pages:\n *\n * Whenever a page has any dirty buffers, the page's dirty bit is set, and\n * the page is tagged dirty in the page cache.\n *\n * At all times, the dirtiness of the buffers represents the dirtiness of\n * subsections of the page.  If the page has buffers, the page dirty bit is\n * merely a hint about the true dirty state.\n *\n * When a page is set dirty in its entirety, all its buffers are marked dirty\n * (if the page has buffers).\n *\n * When a buffer is marked dirty, its page is dirtied, but the page's other\n * buffers are not.\n *\n * Also.  When blockdev buffers are explicitly read with bread(), they\n * individually become uptodate.  But their backing page remains not\n * uptodate - even if all of its buffers are uptodate.  A subsequent\n * block_read_full_page() against that page will discover all the uptodate\n * buffers, will set the page uptodate and will perform no I/O.\n */\n\n/**\n * mark_buffer_dirty - mark a buffer_head as needing writeout\n * @bh: the buffer_head to mark dirty\n *\n * mark_buffer_dirty() will set the dirty bit against the buffer, then set\n * its backing page dirty, then tag the page as dirty in the page cache\n * and then attach the address_space's inode to its superblock's dirty\n * inode list.\n *\n * mark_buffer_dirty() is atomic.  It takes bh->b_page->mapping->private_lock,\n * i_pages lock and mapping->host->i_lock.\n */\nvoid mark_buffer_dirty(struct buffer_head *bh)\n{\n\tWARN_ON_ONCE(!buffer_uptodate(bh));\n\n\ttrace_block_dirty_buffer(bh);\n\n\t/*\n\t * Very *carefully* optimize the it-is-already-dirty case.\n\t *\n\t * Don't let the final \"is it dirty\" escape to before we\n\t * perhaps modified the buffer.\n\t */\n\tif (buffer_dirty(bh)) {\n\t\tsmp_mb();\n\t\tif (buffer_dirty(bh))\n\t\t\treturn;\n\t}\n\n\tif (!test_set_buffer_dirty(bh)) {\n\t\tstruct page *page = bh->b_page;\n\t\tstruct address_space *mapping = NULL;\n\n\t\tlock_page_memcg(page);\n\t\tif (!TestSetPageDirty(page)) {\n\t\t\tmapping = page_mapping(page);\n\t\t\tif (mapping)\n\t\t\t\t__set_page_dirty(page, mapping, 0);\n\t\t}\n\t\tunlock_page_memcg(page);\n\t\tif (mapping)\n\t\t\t__mark_inode_dirty(mapping->host, I_DIRTY_PAGES);\n\t}\n}\nEXPORT_SYMBOL(mark_buffer_dirty);\n\nvoid mark_buffer_write_io_error(struct buffer_head *bh)\n{\n\tstruct super_block *sb;\n\n\tset_buffer_write_io_error(bh);\n\t/* FIXME: do we need to set this in both places? */\n\tif (bh->b_page && bh->b_page->mapping)\n\t\tmapping_set_error(bh->b_page->mapping, -EIO);\n\tif (bh->b_assoc_map)\n\t\tmapping_set_error(bh->b_assoc_map, -EIO);\n\trcu_read_lock();\n\tsb = READ_ONCE(bh->b_bdev->bd_super);\n\tif (sb)\n\t\terrseq_set(&sb->s_wb_err, -EIO);\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(mark_buffer_write_io_error);\n\n/*\n * Decrement a buffer_head's reference count.  If all buffers against a page\n * have zero reference count, are clean and unlocked, and if the page is clean\n * and unlocked then try_to_free_buffers() may strip the buffers from the page\n * in preparation for freeing it (sometimes, rarely, buffers are removed from\n * a page but it ends up not being freed, and buffers may later be reattached).\n */\nvoid __brelse(struct buffer_head * buf)\n{\n\tif (atomic_read(&buf->b_count)) {\n\t\tput_bh(buf);\n\t\treturn;\n\t}\n\tWARN(1, KERN_ERR \"VFS: brelse: Trying to free free buffer\\n\");\n}\nEXPORT_SYMBOL(__brelse);\n\n/*\n * bforget() is like brelse(), except it discards any\n * potentially dirty data.\n */\nvoid __bforget(struct buffer_head *bh)\n{\n\tclear_buffer_dirty(bh);\n\tif (bh->b_assoc_map) {\n\t\tstruct address_space *buffer_mapping = bh->b_page->mapping;\n\n\t\tspin_lock(&buffer_mapping->private_lock);\n\t\tlist_del_init(&bh->b_assoc_buffers);\n\t\tbh->b_assoc_map = NULL;\n\t\tspin_unlock(&buffer_mapping->private_lock);\n\t}\n\t__brelse(bh);\n}\nEXPORT_SYMBOL(__bforget);\n\nstatic struct buffer_head *__bread_slow(struct buffer_head *bh)\n{\n\tlock_buffer(bh);\n\tif (buffer_uptodate(bh)) {\n\t\tunlock_buffer(bh);\n\t\treturn bh;\n\t} else {\n\t\tget_bh(bh);\n\t\tbh->b_end_io = end_buffer_read_sync;\n\t\tsubmit_bh(REQ_OP_READ, 0, bh);\n\t\twait_on_buffer(bh);\n\t\tif (buffer_uptodate(bh))\n\t\t\treturn bh;\n\t}\n\tbrelse(bh);\n\treturn NULL;\n}\n\n/*\n * Per-cpu buffer LRU implementation.  To reduce the cost of __find_get_block().\n * The bhs[] array is sorted - newest buffer is at bhs[0].  Buffers have their\n * refcount elevated by one when they're in an LRU.  A buffer can only appear\n * once in a particular CPU's LRU.  A single buffer can be present in multiple\n * CPU's LRUs at the same time.\n *\n * This is a transparent caching front-end to sb_bread(), sb_getblk() and\n * sb_find_get_block().\n *\n * The LRUs themselves only need locking against invalidate_bh_lrus.  We use\n * a local interrupt disable for that.\n */\n\n#define BH_LRU_SIZE\t16\n\nstruct bh_lru {\n\tstruct buffer_head *bhs[BH_LRU_SIZE];\n};\n\nstatic DEFINE_PER_CPU(struct bh_lru, bh_lrus) = {{ NULL }};\n\n#ifdef CONFIG_SMP\n#define bh_lru_lock()\tlocal_irq_disable()\n#define bh_lru_unlock()\tlocal_irq_enable()\n#else\n#define bh_lru_lock()\tpreempt_disable()\n#define bh_lru_unlock()\tpreempt_enable()\n#endif\n\nstatic inline void check_irqs_on(void)\n{\n#ifdef irqs_disabled\n\tBUG_ON(irqs_disabled());\n#endif\n}\n\n/*\n * Install a buffer_head into this cpu's LRU.  If not already in the LRU, it is\n * inserted at the front, and the buffer_head at the back if any is evicted.\n * Or, if already in the LRU it is moved to the front.\n */\nstatic void bh_lru_install(struct buffer_head *bh)\n{\n\tstruct buffer_head *evictee = bh;\n\tstruct bh_lru *b;\n\tint i;\n\n\tcheck_irqs_on();\n\tbh_lru_lock();\n\n\tb = this_cpu_ptr(&bh_lrus);\n\tfor (i = 0; i < BH_LRU_SIZE; i++) {\n\t\tswap(evictee, b->bhs[i]);\n\t\tif (evictee == bh) {\n\t\t\tbh_lru_unlock();\n\t\t\treturn;\n\t\t}\n\t}\n\n\tget_bh(bh);\n\tbh_lru_unlock();\n\tbrelse(evictee);\n}\n\n/*\n * Look up the bh in this cpu's LRU.  If it's there, move it to the head.\n */\nstatic struct buffer_head *\nlookup_bh_lru(struct block_device *bdev, sector_t block, unsigned size)\n{\n\tstruct buffer_head *ret = NULL;\n\tunsigned int i;\n\n\tcheck_irqs_on();\n\tbh_lru_lock();\n\tfor (i = 0; i < BH_LRU_SIZE; i++) {\n\t\tstruct buffer_head *bh = __this_cpu_read(bh_lrus.bhs[i]);\n\n\t\tif (bh && bh->b_blocknr == block && bh->b_bdev == bdev &&\n\t\t    bh->b_size == size) {\n\t\t\tif (i) {\n\t\t\t\twhile (i) {\n\t\t\t\t\t__this_cpu_write(bh_lrus.bhs[i],\n\t\t\t\t\t\t__this_cpu_read(bh_lrus.bhs[i - 1]));\n\t\t\t\t\ti--;\n\t\t\t\t}\n\t\t\t\t__this_cpu_write(bh_lrus.bhs[0], bh);\n\t\t\t}\n\t\t\tget_bh(bh);\n\t\t\tret = bh;\n\t\t\tbreak;\n\t\t}\n\t}\n\tbh_lru_unlock();\n\treturn ret;\n}\n\n/*\n * Perform a pagecache lookup for the matching buffer.  If it's there, refresh\n * it in the LRU and mark it as accessed.  If it is not present then return\n * NULL\n */\nstruct buffer_head *\n__find_get_block(struct block_device *bdev, sector_t block, unsigned size)\n{\n\tstruct buffer_head *bh = lookup_bh_lru(bdev, block, size);\n\n\tif (bh == NULL) {\n\t\t/* __find_get_block_slow will mark the page accessed */\n\t\tbh = __find_get_block_slow(bdev, block);\n\t\tif (bh)\n\t\t\tbh_lru_install(bh);\n\t} else\n\t\ttouch_buffer(bh);\n\n\treturn bh;\n}\nEXPORT_SYMBOL(__find_get_block);\n\n/*\n * __getblk_gfp() will locate (and, if necessary, create) the buffer_head\n * which corresponds to the passed block_device, block and size. The\n * returned buffer has its reference count incremented.\n *\n * __getblk_gfp() will lock up the machine if grow_dev_page's\n * try_to_free_buffers() attempt is failing.  FIXME, perhaps?\n */\nstruct buffer_head *\n__getblk_gfp(struct block_device *bdev, sector_t block,\n\t     unsigned size, gfp_t gfp)\n{\n\tstruct buffer_head *bh = __find_get_block(bdev, block, size);\n\n\tmight_sleep();\n\tif (bh == NULL)\n\t\tbh = __getblk_slow(bdev, block, size, gfp);\n\treturn bh;\n}\nEXPORT_SYMBOL(__getblk_gfp);\n\n/*\n * Do async read-ahead on a buffer..\n */\nvoid __breadahead(struct block_device *bdev, sector_t block, unsigned size)\n{\n\tstruct buffer_head *bh = __getblk(bdev, block, size);\n\tif (likely(bh)) {\n\t\tll_rw_block(REQ_OP_READ, REQ_RAHEAD, 1, &bh);\n\t\tbrelse(bh);\n\t}\n}\nEXPORT_SYMBOL(__breadahead);\n\nvoid __breadahead_gfp(struct block_device *bdev, sector_t block, unsigned size,\n\t\t      gfp_t gfp)\n{\n\tstruct buffer_head *bh = __getblk_gfp(bdev, block, size, gfp);\n\tif (likely(bh)) {\n\t\tll_rw_block(REQ_OP_READ, REQ_RAHEAD, 1, &bh);\n\t\tbrelse(bh);\n\t}\n}\nEXPORT_SYMBOL(__breadahead_gfp);\n\n/**\n *  __bread_gfp() - reads a specified block and returns the bh\n *  @bdev: the block_device to read from\n *  @block: number of block\n *  @size: size (in bytes) to read\n *  @gfp: page allocation flag\n *\n *  Reads a specified block, and returns buffer head that contains it.\n *  The page cache can be allocated from non-movable area\n *  not to prevent page migration if you set gfp to zero.\n *  It returns NULL if the block was unreadable.\n */\nstruct buffer_head *\n__bread_gfp(struct block_device *bdev, sector_t block,\n\t\t   unsigned size, gfp_t gfp)\n{\n\tstruct buffer_head *bh = __getblk_gfp(bdev, block, size, gfp);\n\n\tif (likely(bh) && !buffer_uptodate(bh))\n\t\tbh = __bread_slow(bh);\n\treturn bh;\n}\nEXPORT_SYMBOL(__bread_gfp);\n\n/*\n * invalidate_bh_lrus() is called rarely - but not only at unmount.\n * This doesn't race because it runs in each cpu either in irq\n * or with preempt disabled.\n */\nstatic void invalidate_bh_lru(void *arg)\n{\n\tstruct bh_lru *b = &get_cpu_var(bh_lrus);\n\tint i;\n\n\tfor (i = 0; i < BH_LRU_SIZE; i++) {\n\t\tbrelse(b->bhs[i]);\n\t\tb->bhs[i] = NULL;\n\t}\n\tput_cpu_var(bh_lrus);\n}\n\nstatic bool has_bh_in_lru(int cpu, void *dummy)\n{\n\tstruct bh_lru *b = per_cpu_ptr(&bh_lrus, cpu);\n\tint i;\n\t\n\tfor (i = 0; i < BH_LRU_SIZE; i++) {\n\t\tif (b->bhs[i])\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nvoid invalidate_bh_lrus(void)\n{\n\ton_each_cpu_cond(has_bh_in_lru, invalidate_bh_lru, NULL, 1);\n}\nEXPORT_SYMBOL_GPL(invalidate_bh_lrus);\n\nvoid set_bh_page(struct buffer_head *bh,\n\t\tstruct page *page, unsigned long offset)\n{\n\tbh->b_page = page;\n\tBUG_ON(offset >= PAGE_SIZE);\n\tif (PageHighMem(page))\n\t\t/*\n\t\t * This catches illegal uses and preserves the offset:\n\t\t */\n\t\tbh->b_data = (char *)(0 + offset);\n\telse\n\t\tbh->b_data = page_address(page) + offset;\n}\nEXPORT_SYMBOL(set_bh_page);\n\n/*\n * Called when truncating a buffer on a page completely.\n */\n\n/* Bits that are cleared during an invalidate */\n#define BUFFER_FLAGS_DISCARD \\\n\t(1 << BH_Mapped | 1 << BH_New | 1 << BH_Req | \\\n\t 1 << BH_Delay | 1 << BH_Unwritten)\n\nstatic void discard_buffer(struct buffer_head * bh)\n{\n\tunsigned long b_state, b_state_old;\n\n\tlock_buffer(bh);\n\tclear_buffer_dirty(bh);\n\tbh->b_bdev = NULL;\n\tb_state = bh->b_state;\n\tfor (;;) {\n\t\tb_state_old = cmpxchg(&bh->b_state, b_state,\n\t\t\t\t      (b_state & ~BUFFER_FLAGS_DISCARD));\n\t\tif (b_state_old == b_state)\n\t\t\tbreak;\n\t\tb_state = b_state_old;\n\t}\n\tunlock_buffer(bh);\n}\n\n/**\n * block_invalidatepage - invalidate part or all of a buffer-backed page\n *\n * @page: the page which is affected\n * @offset: start of the range to invalidate\n * @length: length of the range to invalidate\n *\n * block_invalidatepage() is called when all or part of the page has become\n * invalidated by a truncate operation.\n *\n * block_invalidatepage() does not have to release all buffers, but it must\n * ensure that no dirty buffer is left outside @offset and that no I/O\n * is underway against any of the blocks which are outside the truncation\n * point.  Because the caller is about to free (and possibly reuse) those\n * blocks on-disk.\n */\nvoid block_invalidatepage(struct page *page, unsigned int offset,\n\t\t\t  unsigned int length)\n{\n\tstruct buffer_head *head, *bh, *next;\n\tunsigned int curr_off = 0;\n\tunsigned int stop = length + offset;\n\n\tBUG_ON(!PageLocked(page));\n\tif (!page_has_buffers(page))\n\t\tgoto out;\n\n\t/*\n\t * Check for overflow\n\t */\n\tBUG_ON(stop > PAGE_SIZE || stop < length);\n\n\thead = page_buffers(page);\n\tbh = head;\n\tdo {\n\t\tunsigned int next_off = curr_off + bh->b_size;\n\t\tnext = bh->b_this_page;\n\n\t\t/*\n\t\t * Are we still fully in range ?\n\t\t */\n\t\tif (next_off > stop)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * is this block fully invalidated?\n\t\t */\n\t\tif (offset <= curr_off)\n\t\t\tdiscard_buffer(bh);\n\t\tcurr_off = next_off;\n\t\tbh = next;\n\t} while (bh != head);\n\n\t/*\n\t * We release buffers only if the entire page is being invalidated.\n\t * The get_block cached value has been unconditionally invalidated,\n\t * so real IO is not possible anymore.\n\t */\n\tif (length == PAGE_SIZE)\n\t\ttry_to_release_page(page, 0);\nout:\n\treturn;\n}\nEXPORT_SYMBOL(block_invalidatepage);\n\n\n/*\n * We attach and possibly dirty the buffers atomically wrt\n * __set_page_dirty_buffers() via private_lock.  try_to_free_buffers\n * is already excluded via the page lock.\n */\nvoid create_empty_buffers(struct page *page,\n\t\t\tunsigned long blocksize, unsigned long b_state)\n{\n\tstruct buffer_head *bh, *head, *tail;\n\n\thead = alloc_page_buffers(page, blocksize, true);\n\tbh = head;\n\tdo {\n\t\tbh->b_state |= b_state;\n\t\ttail = bh;\n\t\tbh = bh->b_this_page;\n\t} while (bh);\n\ttail->b_this_page = head;\n\n\tspin_lock(&page->mapping->private_lock);\n\tif (PageUptodate(page) || PageDirty(page)) {\n\t\tbh = head;\n\t\tdo {\n\t\t\tif (PageDirty(page))\n\t\t\t\tset_buffer_dirty(bh);\n\t\t\tif (PageUptodate(page))\n\t\t\t\tset_buffer_uptodate(bh);\n\t\t\tbh = bh->b_this_page;\n\t\t} while (bh != head);\n\t}\n\tattach_page_private(page, head);\n\tspin_unlock(&page->mapping->private_lock);\n}\nEXPORT_SYMBOL(create_empty_buffers);\n\n/**\n * clean_bdev_aliases: clean a range of buffers in block device\n * @bdev: Block device to clean buffers in\n * @block: Start of a range of blocks to clean\n * @len: Number of blocks to clean\n *\n * We are taking a range of blocks for data and we don't want writeback of any\n * buffer-cache aliases starting from return from this function and until the\n * moment when something will explicitly mark the buffer dirty (hopefully that\n * will not happen until we will free that block ;-) We don't even need to mark\n * it not-uptodate - nobody can expect anything from a newly allocated buffer\n * anyway. We used to use unmap_buffer() for such invalidation, but that was\n * wrong. We definitely don't want to mark the alias unmapped, for example - it\n * would confuse anyone who might pick it with bread() afterwards...\n *\n * Also..  Note that bforget() doesn't lock the buffer.  So there can be\n * writeout I/O going on against recently-freed buffers.  We don't wait on that\n * I/O in bforget() - it's more efficient to wait on the I/O only if we really\n * need to.  That happens here.\n */\nvoid clean_bdev_aliases(struct block_device *bdev, sector_t block, sector_t len)\n{\n\tstruct inode *bd_inode = bdev->bd_inode;\n\tstruct address_space *bd_mapping = bd_inode->i_mapping;\n\tstruct pagevec pvec;\n\tpgoff_t index = block >> (PAGE_SHIFT - bd_inode->i_blkbits);\n\tpgoff_t end;\n\tint i, count;\n\tstruct buffer_head *bh;\n\tstruct buffer_head *head;\n\n\tend = (block + len - 1) >> (PAGE_SHIFT - bd_inode->i_blkbits);\n\tpagevec_init(&pvec);\n\twhile (pagevec_lookup_range(&pvec, bd_mapping, &index, end)) {\n\t\tcount = pagevec_count(&pvec);\n\t\tfor (i = 0; i < count; i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\n\t\t\tif (!page_has_buffers(page))\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * We use page lock instead of bd_mapping->private_lock\n\t\t\t * to pin buffers here since we can afford to sleep and\n\t\t\t * it scales better than a global spinlock lock.\n\t\t\t */\n\t\t\tlock_page(page);\n\t\t\t/* Recheck when the page is locked which pins bhs */\n\t\t\tif (!page_has_buffers(page))\n\t\t\t\tgoto unlock_page;\n\t\t\thead = page_buffers(page);\n\t\t\tbh = head;\n\t\t\tdo {\n\t\t\t\tif (!buffer_mapped(bh) || (bh->b_blocknr < block))\n\t\t\t\t\tgoto next;\n\t\t\t\tif (bh->b_blocknr >= block + len)\n\t\t\t\t\tbreak;\n\t\t\t\tclear_buffer_dirty(bh);\n\t\t\t\twait_on_buffer(bh);\n\t\t\t\tclear_buffer_req(bh);\nnext:\n\t\t\t\tbh = bh->b_this_page;\n\t\t\t} while (bh != head);\nunlock_page:\n\t\t\tunlock_page(page);\n\t\t}\n\t\tpagevec_release(&pvec);\n\t\tcond_resched();\n\t\t/* End of range already reached? */\n\t\tif (index > end || !index)\n\t\t\tbreak;\n\t}\n}\nEXPORT_SYMBOL(clean_bdev_aliases);\n\n/*\n * Size is a power-of-two in the range 512..PAGE_SIZE,\n * and the case we care about most is PAGE_SIZE.\n *\n * So this *could* possibly be written with those\n * constraints in mind (relevant mostly if some\n * architecture has a slow bit-scan instruction)\n */\nstatic inline int block_size_bits(unsigned int blocksize)\n{\n\treturn ilog2(blocksize);\n}\n\nstatic struct buffer_head *create_page_buffers(struct page *page, struct inode *inode, unsigned int b_state)\n{\n\tBUG_ON(!PageLocked(page));\n\n\tif (!page_has_buffers(page))\n\t\tcreate_empty_buffers(page, 1 << READ_ONCE(inode->i_blkbits),\n\t\t\t\t     b_state);\n\treturn page_buffers(page);\n}\n\n/*\n * NOTE! All mapped/uptodate combinations are valid:\n *\n *\tMapped\tUptodate\tMeaning\n *\n *\tNo\tNo\t\t\"unknown\" - must do get_block()\n *\tNo\tYes\t\t\"hole\" - zero-filled\n *\tYes\tNo\t\t\"allocated\" - allocated on disk, not read in\n *\tYes\tYes\t\t\"valid\" - allocated and up-to-date in memory.\n *\n * \"Dirty\" is valid only with the last case (mapped+uptodate).\n */\n\n/*\n * While block_write_full_page is writing back the dirty buffers under\n * the page lock, whoever dirtied the buffers may decide to clean them\n * again at any time.  We handle that by only looking at the buffer\n * state inside lock_buffer().\n *\n * If block_write_full_page() is called for regular writeback\n * (wbc->sync_mode == WB_SYNC_NONE) then it will redirty a page which has a\n * locked buffer.   This only can happen if someone has written the buffer\n * directly, with submit_bh().  At the address_space level PageWriteback\n * prevents this contention from occurring.\n *\n * If block_write_full_page() is called with wbc->sync_mode ==\n * WB_SYNC_ALL, the writes are posted using REQ_SYNC; this\n * causes the writes to be flagged as synchronous writes.\n */\nint __block_write_full_page(struct inode *inode, struct page *page,\n\t\t\tget_block_t *get_block, struct writeback_control *wbc,\n\t\t\tbh_end_io_t *handler)\n{\n\tint err;\n\tsector_t block;\n\tsector_t last_block;\n\tstruct buffer_head *bh, *head;\n\tunsigned int blocksize, bbits;\n\tint nr_underway = 0;\n\tint write_flags = wbc_to_write_flags(wbc);\n\n\thead = create_page_buffers(page, inode,\n\t\t\t\t\t(1 << BH_Dirty)|(1 << BH_Uptodate));\n\n\t/*\n\t * Be very careful.  We have no exclusion from __set_page_dirty_buffers\n\t * here, and the (potentially unmapped) buffers may become dirty at\n\t * any time.  If a buffer becomes dirty here after we've inspected it\n\t * then we just miss that fact, and the page stays dirty.\n\t *\n\t * Buffers outside i_size may be dirtied by __set_page_dirty_buffers;\n\t * handle that here by just cleaning them.\n\t */\n\n\tbh = head;\n\tblocksize = bh->b_size;\n\tbbits = block_size_bits(blocksize);\n\n\tblock = (sector_t)page->index << (PAGE_SHIFT - bbits);\n\tlast_block = (i_size_read(inode) - 1) >> bbits;\n\n\t/*\n\t * Get all the dirty buffers mapped to disk addresses and\n\t * handle any aliases from the underlying blockdev's mapping.\n\t */\n\tdo {\n\t\tif (block > last_block) {\n\t\t\t/*\n\t\t\t * mapped buffers outside i_size will occur, because\n\t\t\t * this page can be outside i_size when there is a\n\t\t\t * truncate in progress.\n\t\t\t */\n\t\t\t/*\n\t\t\t * The buffer was zeroed by block_write_full_page()\n\t\t\t */\n\t\t\tclear_buffer_dirty(bh);\n\t\t\tset_buffer_uptodate(bh);\n\t\t} else if ((!buffer_mapped(bh) || buffer_delay(bh)) &&\n\t\t\t   buffer_dirty(bh)) {\n\t\t\tWARN_ON(bh->b_size != blocksize);\n\t\t\terr = get_block(inode, block, bh, 1);\n\t\t\tif (err)\n\t\t\t\tgoto recover;\n\t\t\tclear_buffer_delay(bh);\n\t\t\tif (buffer_new(bh)) {\n\t\t\t\t/* blockdev mappings never come here */\n\t\t\t\tclear_buffer_new(bh);\n\t\t\t\tclean_bdev_bh_alias(bh);\n\t\t\t}\n\t\t}\n\t\tbh = bh->b_this_page;\n\t\tblock++;\n\t} while (bh != head);\n\n\tdo {\n\t\tif (!buffer_mapped(bh))\n\t\t\tcontinue;\n\t\t/*\n\t\t * If it's a fully non-blocking write attempt and we cannot\n\t\t * lock the buffer then redirty the page.  Note that this can\n\t\t * potentially cause a busy-wait loop from writeback threads\n\t\t * and kswapd activity, but those code paths have their own\n\t\t * higher-level throttling.\n\t\t */\n\t\tif (wbc->sync_mode != WB_SYNC_NONE) {\n\t\t\tlock_buffer(bh);\n\t\t} else if (!trylock_buffer(bh)) {\n\t\t\tredirty_page_for_writepage(wbc, page);\n\t\t\tcontinue;\n\t\t}\n\t\tif (test_clear_buffer_dirty(bh)) {\n\t\t\tmark_buffer_async_write_endio(bh, handler);\n\t\t} else {\n\t\t\tunlock_buffer(bh);\n\t\t}\n\t} while ((bh = bh->b_this_page) != head);\n\n\t/*\n\t * The page and its buffers are protected by PageWriteback(), so we can\n\t * drop the bh refcounts early.\n\t */\n\tBUG_ON(PageWriteback(page));\n\tset_page_writeback(page);\n\n\tdo {\n\t\tstruct buffer_head *next = bh->b_this_page;\n\t\tif (buffer_async_write(bh)) {\n\t\t\tsubmit_bh_wbc(REQ_OP_WRITE, write_flags, bh,\n\t\t\t\t\tinode->i_write_hint, wbc);\n\t\t\tnr_underway++;\n\t\t}\n\t\tbh = next;\n\t} while (bh != head);\n\tunlock_page(page);\n\n\terr = 0;\ndone:\n\tif (nr_underway == 0) {\n\t\t/*\n\t\t * The page was marked dirty, but the buffers were\n\t\t * clean.  Someone wrote them back by hand with\n\t\t * ll_rw_block/submit_bh.  A rare case.\n\t\t */\n\t\tend_page_writeback(page);\n\n\t\t/*\n\t\t * The page and buffer_heads can be released at any time from\n\t\t * here on.\n\t\t */\n\t}\n\treturn err;\n\nrecover:\n\t/*\n\t * ENOSPC, or some other error.  We may already have added some\n\t * blocks to the file, so we need to write these out to avoid\n\t * exposing stale data.\n\t * The page is currently locked and not marked for writeback\n\t */\n\tbh = head;\n\t/* Recovery: lock and submit the mapped buffers */\n\tdo {\n\t\tif (buffer_mapped(bh) && buffer_dirty(bh) &&\n\t\t    !buffer_delay(bh)) {\n\t\t\tlock_buffer(bh);\n\t\t\tmark_buffer_async_write_endio(bh, handler);\n\t\t} else {\n\t\t\t/*\n\t\t\t * The buffer may have been set dirty during\n\t\t\t * attachment to a dirty page.\n\t\t\t */\n\t\t\tclear_buffer_dirty(bh);\n\t\t}\n\t} while ((bh = bh->b_this_page) != head);\n\tSetPageError(page);\n\tBUG_ON(PageWriteback(page));\n\tmapping_set_error(page->mapping, err);\n\tset_page_writeback(page);\n\tdo {\n\t\tstruct buffer_head *next = bh->b_this_page;\n\t\tif (buffer_async_write(bh)) {\n\t\t\tclear_buffer_dirty(bh);\n\t\t\tsubmit_bh_wbc(REQ_OP_WRITE, write_flags, bh,\n\t\t\t\t\tinode->i_write_hint, wbc);\n\t\t\tnr_underway++;\n\t\t}\n\t\tbh = next;\n\t} while (bh != head);\n\tunlock_page(page);\n\tgoto done;\n}\nEXPORT_SYMBOL(__block_write_full_page);\n\n/*\n * If a page has any new buffers, zero them out here, and mark them uptodate\n * and dirty so they'll be written out (in order to prevent uninitialised\n * block data from leaking). And clear the new bit.\n */\nvoid page_zero_new_buffers(struct page *page, unsigned from, unsigned to)\n{\n\tunsigned int block_start, block_end;\n\tstruct buffer_head *head, *bh;\n\n\tBUG_ON(!PageLocked(page));\n\tif (!page_has_buffers(page))\n\t\treturn;\n\n\tbh = head = page_buffers(page);\n\tblock_start = 0;\n\tdo {\n\t\tblock_end = block_start + bh->b_size;\n\n\t\tif (buffer_new(bh)) {\n\t\t\tif (block_end > from && block_start < to) {\n\t\t\t\tif (!PageUptodate(page)) {\n\t\t\t\t\tunsigned start, size;\n\n\t\t\t\t\tstart = max(from, block_start);\n\t\t\t\t\tsize = min(to, block_end) - start;\n\n\t\t\t\t\tzero_user(page, start, size);\n\t\t\t\t\tset_buffer_uptodate(bh);\n\t\t\t\t}\n\n\t\t\t\tclear_buffer_new(bh);\n\t\t\t\tmark_buffer_dirty(bh);\n\t\t\t}\n\t\t}\n\n\t\tblock_start = block_end;\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n}\nEXPORT_SYMBOL(page_zero_new_buffers);\n\nstatic void\niomap_to_bh(struct inode *inode, sector_t block, struct buffer_head *bh,\n\t\tstruct iomap *iomap)\n{\n\tloff_t offset = block << inode->i_blkbits;\n\n\tbh->b_bdev = iomap->bdev;\n\n\t/*\n\t * Block points to offset in file we need to map, iomap contains\n\t * the offset at which the map starts. If the map ends before the\n\t * current block, then do not map the buffer and let the caller\n\t * handle it.\n\t */\n\tBUG_ON(offset >= iomap->offset + iomap->length);\n\n\tswitch (iomap->type) {\n\tcase IOMAP_HOLE:\n\t\t/*\n\t\t * If the buffer is not up to date or beyond the current EOF,\n\t\t * we need to mark it as new to ensure sub-block zeroing is\n\t\t * executed if necessary.\n\t\t */\n\t\tif (!buffer_uptodate(bh) ||\n\t\t    (offset >= i_size_read(inode)))\n\t\t\tset_buffer_new(bh);\n\t\tbreak;\n\tcase IOMAP_DELALLOC:\n\t\tif (!buffer_uptodate(bh) ||\n\t\t    (offset >= i_size_read(inode)))\n\t\t\tset_buffer_new(bh);\n\t\tset_buffer_uptodate(bh);\n\t\tset_buffer_mapped(bh);\n\t\tset_buffer_delay(bh);\n\t\tbreak;\n\tcase IOMAP_UNWRITTEN:\n\t\t/*\n\t\t * For unwritten regions, we always need to ensure that regions\n\t\t * in the block we are not writing to are zeroed. Mark the\n\t\t * buffer as new to ensure this.\n\t\t */\n\t\tset_buffer_new(bh);\n\t\tset_buffer_unwritten(bh);\n\t\tfallthrough;\n\tcase IOMAP_MAPPED:\n\t\tif ((iomap->flags & IOMAP_F_NEW) ||\n\t\t    offset >= i_size_read(inode))\n\t\t\tset_buffer_new(bh);\n\t\tbh->b_blocknr = (iomap->addr + offset - iomap->offset) >>\n\t\t\t\tinode->i_blkbits;\n\t\tset_buffer_mapped(bh);\n\t\tbreak;\n\t}\n}\n\nint __block_write_begin_int(struct page *page, loff_t pos, unsigned len,\n\t\tget_block_t *get_block, struct iomap *iomap)\n{\n\tunsigned from = pos & (PAGE_SIZE - 1);\n\tunsigned to = from + len;\n\tstruct inode *inode = page->mapping->host;\n\tunsigned block_start, block_end;\n\tsector_t block;\n\tint err = 0;\n\tunsigned blocksize, bbits;\n\tstruct buffer_head *bh, *head, *wait[2], **wait_bh=wait;\n\n\tBUG_ON(!PageLocked(page));\n\tBUG_ON(from > PAGE_SIZE);\n\tBUG_ON(to > PAGE_SIZE);\n\tBUG_ON(from > to);\n\n\thead = create_page_buffers(page, inode, 0);\n\tblocksize = head->b_size;\n\tbbits = block_size_bits(blocksize);\n\n\tblock = (sector_t)page->index << (PAGE_SHIFT - bbits);\n\n\tfor(bh = head, block_start = 0; bh != head || !block_start;\n\t    block++, block_start=block_end, bh = bh->b_this_page) {\n\t\tblock_end = block_start + blocksize;\n\t\tif (block_end <= from || block_start >= to) {\n\t\t\tif (PageUptodate(page)) {\n\t\t\t\tif (!buffer_uptodate(bh))\n\t\t\t\t\tset_buffer_uptodate(bh);\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\tif (buffer_new(bh))\n\t\t\tclear_buffer_new(bh);\n\t\tif (!buffer_mapped(bh)) {\n\t\t\tWARN_ON(bh->b_size != blocksize);\n\t\t\tif (get_block) {\n\t\t\t\terr = get_block(inode, block, bh, 1);\n\t\t\t\tif (err)\n\t\t\t\t\tbreak;\n\t\t\t} else {\n\t\t\t\tiomap_to_bh(inode, block, bh, iomap);\n\t\t\t}\n\n\t\t\tif (buffer_new(bh)) {\n\t\t\t\tclean_bdev_bh_alias(bh);\n\t\t\t\tif (PageUptodate(page)) {\n\t\t\t\t\tclear_buffer_new(bh);\n\t\t\t\t\tset_buffer_uptodate(bh);\n\t\t\t\t\tmark_buffer_dirty(bh);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tif (block_end > to || block_start < from)\n\t\t\t\t\tzero_user_segments(page,\n\t\t\t\t\t\tto, block_end,\n\t\t\t\t\t\tblock_start, from);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tif (PageUptodate(page)) {\n\t\t\tif (!buffer_uptodate(bh))\n\t\t\t\tset_buffer_uptodate(bh);\n\t\t\tcontinue; \n\t\t}\n\t\tif (!buffer_uptodate(bh) && !buffer_delay(bh) &&\n\t\t    !buffer_unwritten(bh) &&\n\t\t     (block_start < from || block_end > to)) {\n\t\t\tll_rw_block(REQ_OP_READ, 0, 1, &bh);\n\t\t\t*wait_bh++=bh;\n\t\t}\n\t}\n\t/*\n\t * If we issued read requests - let them complete.\n\t */\n\twhile(wait_bh > wait) {\n\t\twait_on_buffer(*--wait_bh);\n\t\tif (!buffer_uptodate(*wait_bh))\n\t\t\terr = -EIO;\n\t}\n\tif (unlikely(err))\n\t\tpage_zero_new_buffers(page, from, to);\n\treturn err;\n}\n\nint __block_write_begin(struct page *page, loff_t pos, unsigned len,\n\t\tget_block_t *get_block)\n{\n\treturn __block_write_begin_int(page, pos, len, get_block, NULL);\n}\nEXPORT_SYMBOL(__block_write_begin);\n\nstatic int __block_commit_write(struct inode *inode, struct page *page,\n\t\tunsigned from, unsigned to)\n{\n\tunsigned block_start, block_end;\n\tint partial = 0;\n\tunsigned blocksize;\n\tstruct buffer_head *bh, *head;\n\n\tbh = head = page_buffers(page);\n\tblocksize = bh->b_size;\n\n\tblock_start = 0;\n\tdo {\n\t\tblock_end = block_start + blocksize;\n\t\tif (block_end <= from || block_start >= to) {\n\t\t\tif (!buffer_uptodate(bh))\n\t\t\t\tpartial = 1;\n\t\t} else {\n\t\t\tset_buffer_uptodate(bh);\n\t\t\tmark_buffer_dirty(bh);\n\t\t}\n\t\tclear_buffer_new(bh);\n\n\t\tblock_start = block_end;\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\n\t/*\n\t * If this is a partial write which happened to make all buffers\n\t * uptodate then we can optimize away a bogus readpage() for\n\t * the next read(). Here we 'discover' whether the page went\n\t * uptodate as a result of this (potentially partial) write.\n\t */\n\tif (!partial)\n\t\tSetPageUptodate(page);\n\treturn 0;\n}\n\n/*\n * block_write_begin takes care of the basic task of block allocation and\n * bringing partial write blocks uptodate first.\n *\n * The filesystem needs to handle block truncation upon failure.\n */\nint block_write_begin(struct address_space *mapping, loff_t pos, unsigned len,\n\t\tunsigned flags, struct page **pagep, get_block_t *get_block)\n{\n\tpgoff_t index = pos >> PAGE_SHIFT;\n\tstruct page *page;\n\tint status;\n\n\tpage = grab_cache_page_write_begin(mapping, index, flags);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tstatus = __block_write_begin(page, pos, len, get_block);\n\tif (unlikely(status)) {\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\tpage = NULL;\n\t}\n\n\t*pagep = page;\n\treturn status;\n}\nEXPORT_SYMBOL(block_write_begin);\n\nint block_write_end(struct file *file, struct address_space *mapping,\n\t\t\tloff_t pos, unsigned len, unsigned copied,\n\t\t\tstruct page *page, void *fsdata)\n{\n\tstruct inode *inode = mapping->host;\n\tunsigned start;\n\n\tstart = pos & (PAGE_SIZE - 1);\n\n\tif (unlikely(copied < len)) {\n\t\t/*\n\t\t * The buffers that were written will now be uptodate, so we\n\t\t * don't have to worry about a readpage reading them and\n\t\t * overwriting a partial write. However if we have encountered\n\t\t * a short write and only partially written into a buffer, it\n\t\t * will not be marked uptodate, so a readpage might come in and\n\t\t * destroy our partial write.\n\t\t *\n\t\t * Do the simplest thing, and just treat any short write to a\n\t\t * non uptodate page as a zero-length write, and force the\n\t\t * caller to redo the whole thing.\n\t\t */\n\t\tif (!PageUptodate(page))\n\t\t\tcopied = 0;\n\n\t\tpage_zero_new_buffers(page, start+copied, start+len);\n\t}\n\tflush_dcache_page(page);\n\n\t/* This could be a short (even 0-length) commit */\n\t__block_commit_write(inode, page, start, start+copied);\n\n\treturn copied;\n}\nEXPORT_SYMBOL(block_write_end);\n\nint generic_write_end(struct file *file, struct address_space *mapping,\n\t\t\tloff_t pos, unsigned len, unsigned copied,\n\t\t\tstruct page *page, void *fsdata)\n{\n\tstruct inode *inode = mapping->host;\n\tloff_t old_size = inode->i_size;\n\tbool i_size_changed = false;\n\n\tcopied = block_write_end(file, mapping, pos, len, copied, page, fsdata);\n\n\t/*\n\t * No need to use i_size_read() here, the i_size cannot change under us\n\t * because we hold i_rwsem.\n\t *\n\t * But it's important to update i_size while still holding page lock:\n\t * page writeout could otherwise come in and zero beyond i_size.\n\t */\n\tif (pos + copied > inode->i_size) {\n\t\ti_size_write(inode, pos + copied);\n\t\ti_size_changed = true;\n\t}\n\n\tunlock_page(page);\n\tput_page(page);\n\n\tif (old_size < pos)\n\t\tpagecache_isize_extended(inode, old_size, pos);\n\t/*\n\t * Don't mark the inode dirty under page lock. First, it unnecessarily\n\t * makes the holding time of page lock longer. Second, it forces lock\n\t * ordering of page lock and transaction start for journaling\n\t * filesystems.\n\t */\n\tif (i_size_changed)\n\t\tmark_inode_dirty(inode);\n\treturn copied;\n}\nEXPORT_SYMBOL(generic_write_end);\n\n/*\n * block_is_partially_uptodate checks whether buffers within a page are\n * uptodate or not.\n *\n * Returns true if all buffers which correspond to a file portion\n * we want to read are uptodate.\n */\nint block_is_partially_uptodate(struct page *page, unsigned long from,\n\t\t\t\t\tunsigned long count)\n{\n\tunsigned block_start, block_end, blocksize;\n\tunsigned to;\n\tstruct buffer_head *bh, *head;\n\tint ret = 1;\n\n\tif (!page_has_buffers(page))\n\t\treturn 0;\n\n\thead = page_buffers(page);\n\tblocksize = head->b_size;\n\tto = min_t(unsigned, PAGE_SIZE - from, count);\n\tto = from + to;\n\tif (from < blocksize && to > PAGE_SIZE - blocksize)\n\t\treturn 0;\n\n\tbh = head;\n\tblock_start = 0;\n\tdo {\n\t\tblock_end = block_start + blocksize;\n\t\tif (block_end > from && block_start < to) {\n\t\t\tif (!buffer_uptodate(bh)) {\n\t\t\t\tret = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (block_end >= to)\n\t\t\t\tbreak;\n\t\t}\n\t\tblock_start = block_end;\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(block_is_partially_uptodate);\n\n/*\n * Generic \"read page\" function for block devices that have the normal\n * get_block functionality. This is most of the block device filesystems.\n * Reads the page asynchronously --- the unlock_buffer() and\n * set/clear_buffer_uptodate() functions propagate buffer state into the\n * page struct once IO has completed.\n */\nint block_read_full_page(struct page *page, get_block_t *get_block)\n{\n\tstruct inode *inode = page->mapping->host;\n\tsector_t iblock, lblock;\n\tstruct buffer_head *bh, *head, *arr[MAX_BUF_PER_PAGE];\n\tunsigned int blocksize, bbits;\n\tint nr, i;\n\tint fully_mapped = 1;\n\n\thead = create_page_buffers(page, inode, 0);\n\tblocksize = head->b_size;\n\tbbits = block_size_bits(blocksize);\n\n\tiblock = (sector_t)page->index << (PAGE_SHIFT - bbits);\n\tlblock = (i_size_read(inode)+blocksize-1) >> bbits;\n\tbh = head;\n\tnr = 0;\n\ti = 0;\n\n\tdo {\n\t\tif (buffer_uptodate(bh))\n\t\t\tcontinue;\n\n\t\tif (!buffer_mapped(bh)) {\n\t\t\tint err = 0;\n\n\t\t\tfully_mapped = 0;\n\t\t\tif (iblock < lblock) {\n\t\t\t\tWARN_ON(bh->b_size != blocksize);\n\t\t\t\terr = get_block(inode, iblock, bh, 0);\n\t\t\t\tif (err)\n\t\t\t\t\tSetPageError(page);\n\t\t\t}\n\t\t\tif (!buffer_mapped(bh)) {\n\t\t\t\tzero_user(page, i * blocksize, blocksize);\n\t\t\t\tif (!err)\n\t\t\t\t\tset_buffer_uptodate(bh);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t/*\n\t\t\t * get_block() might have updated the buffer\n\t\t\t * synchronously\n\t\t\t */\n\t\t\tif (buffer_uptodate(bh))\n\t\t\t\tcontinue;\n\t\t}\n\t\tarr[nr++] = bh;\n\t} while (i++, iblock++, (bh = bh->b_this_page) != head);\n\n\tif (fully_mapped)\n\t\tSetPageMappedToDisk(page);\n\n\tif (!nr) {\n\t\t/*\n\t\t * All buffers are uptodate - we can set the page uptodate\n\t\t * as well. But not if get_block() returned an error.\n\t\t */\n\t\tif (!PageError(page))\n\t\t\tSetPageUptodate(page);\n\t\tunlock_page(page);\n\t\treturn 0;\n\t}\n\n\t/* Stage two: lock the buffers */\n\tfor (i = 0; i < nr; i++) {\n\t\tbh = arr[i];\n\t\tlock_buffer(bh);\n\t\tmark_buffer_async_read(bh);\n\t}\n\n\t/*\n\t * Stage 3: start the IO.  Check for uptodateness\n\t * inside the buffer lock in case another process reading\n\t * the underlying blockdev brought it uptodate (the sct fix).\n\t */\n\tfor (i = 0; i < nr; i++) {\n\t\tbh = arr[i];\n\t\tif (buffer_uptodate(bh))\n\t\t\tend_buffer_async_read(bh, 1);\n\t\telse\n\t\t\tsubmit_bh(REQ_OP_READ, 0, bh);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(block_read_full_page);\n\n/* utility function for filesystems that need to do work on expanding\n * truncates.  Uses filesystem pagecache writes to allow the filesystem to\n * deal with the hole.  \n */\nint generic_cont_expand_simple(struct inode *inode, loff_t size)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct page *page;\n\tvoid *fsdata;\n\tint err;\n\n\terr = inode_newsize_ok(inode, size);\n\tif (err)\n\t\tgoto out;\n\n\terr = pagecache_write_begin(NULL, mapping, size, 0,\n\t\t\t\t    AOP_FLAG_CONT_EXPAND, &page, &fsdata);\n\tif (err)\n\t\tgoto out;\n\n\terr = pagecache_write_end(NULL, mapping, size, 0, 0, page, fsdata);\n\tBUG_ON(err > 0);\n\nout:\n\treturn err;\n}\nEXPORT_SYMBOL(generic_cont_expand_simple);\n\nstatic int cont_expand_zero(struct file *file, struct address_space *mapping,\n\t\t\t    loff_t pos, loff_t *bytes)\n{\n\tstruct inode *inode = mapping->host;\n\tunsigned int blocksize = i_blocksize(inode);\n\tstruct page *page;\n\tvoid *fsdata;\n\tpgoff_t index, curidx;\n\tloff_t curpos;\n\tunsigned zerofrom, offset, len;\n\tint err = 0;\n\n\tindex = pos >> PAGE_SHIFT;\n\toffset = pos & ~PAGE_MASK;\n\n\twhile (index > (curidx = (curpos = *bytes)>>PAGE_SHIFT)) {\n\t\tzerofrom = curpos & ~PAGE_MASK;\n\t\tif (zerofrom & (blocksize-1)) {\n\t\t\t*bytes |= (blocksize-1);\n\t\t\t(*bytes)++;\n\t\t}\n\t\tlen = PAGE_SIZE - zerofrom;\n\n\t\terr = pagecache_write_begin(file, mapping, curpos, len, 0,\n\t\t\t\t\t    &page, &fsdata);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tzero_user(page, zerofrom, len);\n\t\terr = pagecache_write_end(file, mapping, curpos, len, len,\n\t\t\t\t\t\tpage, fsdata);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t\tBUG_ON(err != len);\n\t\terr = 0;\n\n\t\tbalance_dirty_pages_ratelimited(mapping);\n\n\t\tif (fatal_signal_pending(current)) {\n\t\t\terr = -EINTR;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* page covers the boundary, find the boundary offset */\n\tif (index == curidx) {\n\t\tzerofrom = curpos & ~PAGE_MASK;\n\t\t/* if we will expand the thing last block will be filled */\n\t\tif (offset <= zerofrom) {\n\t\t\tgoto out;\n\t\t}\n\t\tif (zerofrom & (blocksize-1)) {\n\t\t\t*bytes |= (blocksize-1);\n\t\t\t(*bytes)++;\n\t\t}\n\t\tlen = offset - zerofrom;\n\n\t\terr = pagecache_write_begin(file, mapping, curpos, len, 0,\n\t\t\t\t\t    &page, &fsdata);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tzero_user(page, zerofrom, len);\n\t\terr = pagecache_write_end(file, mapping, curpos, len, len,\n\t\t\t\t\t\tpage, fsdata);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t\tBUG_ON(err != len);\n\t\terr = 0;\n\t}\nout:\n\treturn err;\n}\n\n/*\n * For moronic filesystems that do not allow holes in file.\n * We may have to extend the file.\n */\nint cont_write_begin(struct file *file, struct address_space *mapping,\n\t\t\tloff_t pos, unsigned len, unsigned flags,\n\t\t\tstruct page **pagep, void **fsdata,\n\t\t\tget_block_t *get_block, loff_t *bytes)\n{\n\tstruct inode *inode = mapping->host;\n\tunsigned int blocksize = i_blocksize(inode);\n\tunsigned int zerofrom;\n\tint err;\n\n\terr = cont_expand_zero(file, mapping, pos, bytes);\n\tif (err)\n\t\treturn err;\n\n\tzerofrom = *bytes & ~PAGE_MASK;\n\tif (pos+len > *bytes && zerofrom & (blocksize-1)) {\n\t\t*bytes |= (blocksize-1);\n\t\t(*bytes)++;\n\t}\n\n\treturn block_write_begin(mapping, pos, len, flags, pagep, get_block);\n}\nEXPORT_SYMBOL(cont_write_begin);\n\nint block_commit_write(struct page *page, unsigned from, unsigned to)\n{\n\tstruct inode *inode = page->mapping->host;\n\t__block_commit_write(inode,page,from,to);\n\treturn 0;\n}\nEXPORT_SYMBOL(block_commit_write);\n\n/*\n * block_page_mkwrite() is not allowed to change the file size as it gets\n * called from a page fault handler when a page is first dirtied. Hence we must\n * be careful to check for EOF conditions here. We set the page up correctly\n * for a written page which means we get ENOSPC checking when writing into\n * holes and correct delalloc and unwritten extent mapping on filesystems that\n * support these features.\n *\n * We are not allowed to take the i_mutex here so we have to play games to\n * protect against truncate races as the page could now be beyond EOF.  Because\n * truncate writes the inode size before removing pages, once we have the\n * page lock we can determine safely if the page is beyond EOF. If it is not\n * beyond EOF, then the page is guaranteed safe against truncation until we\n * unlock the page.\n *\n * Direct callers of this function should protect against filesystem freezing\n * using sb_start_pagefault() - sb_end_pagefault() functions.\n */\nint block_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf,\n\t\t\t get_block_t get_block)\n{\n\tstruct page *page = vmf->page;\n\tstruct inode *inode = file_inode(vma->vm_file);\n\tunsigned long end;\n\tloff_t size;\n\tint ret;\n\n\tlock_page(page);\n\tsize = i_size_read(inode);\n\tif ((page->mapping != inode->i_mapping) ||\n\t    (page_offset(page) > size)) {\n\t\t/* We overload EFAULT to mean page got truncated */\n\t\tret = -EFAULT;\n\t\tgoto out_unlock;\n\t}\n\n\t/* page is wholly or partially inside EOF */\n\tif (((page->index + 1) << PAGE_SHIFT) > size)\n\t\tend = size & ~PAGE_MASK;\n\telse\n\t\tend = PAGE_SIZE;\n\n\tret = __block_write_begin(page, 0, end, get_block);\n\tif (!ret)\n\t\tret = block_commit_write(page, 0, end);\n\n\tif (unlikely(ret < 0))\n\t\tgoto out_unlock;\n\tset_page_dirty(page);\n\twait_for_stable_page(page);\n\treturn 0;\nout_unlock:\n\tunlock_page(page);\n\treturn ret;\n}\nEXPORT_SYMBOL(block_page_mkwrite);\n\n/*\n * nobh_write_begin()'s prereads are special: the buffer_heads are freed\n * immediately, while under the page lock.  So it needs a special end_io\n * handler which does not touch the bh after unlocking it.\n */\nstatic void end_buffer_read_nobh(struct buffer_head *bh, int uptodate)\n{\n\t__end_buffer_read_notouch(bh, uptodate);\n}\n\n/*\n * Attach the singly-linked list of buffers created by nobh_write_begin, to\n * the page (converting it to circular linked list and taking care of page\n * dirty races).\n */\nstatic void attach_nobh_buffers(struct page *page, struct buffer_head *head)\n{\n\tstruct buffer_head *bh;\n\n\tBUG_ON(!PageLocked(page));\n\n\tspin_lock(&page->mapping->private_lock);\n\tbh = head;\n\tdo {\n\t\tif (PageDirty(page))\n\t\t\tset_buffer_dirty(bh);\n\t\tif (!bh->b_this_page)\n\t\t\tbh->b_this_page = head;\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\tattach_page_private(page, head);\n\tspin_unlock(&page->mapping->private_lock);\n}\n\n/*\n * On entry, the page is fully not uptodate.\n * On exit the page is fully uptodate in the areas outside (from,to)\n * The filesystem needs to handle block truncation upon failure.\n */\nint nobh_write_begin(struct address_space *mapping,\n\t\t\tloff_t pos, unsigned len, unsigned flags,\n\t\t\tstruct page **pagep, void **fsdata,\n\t\t\tget_block_t *get_block)\n{\n\tstruct inode *inode = mapping->host;\n\tconst unsigned blkbits = inode->i_blkbits;\n\tconst unsigned blocksize = 1 << blkbits;\n\tstruct buffer_head *head, *bh;\n\tstruct page *page;\n\tpgoff_t index;\n\tunsigned from, to;\n\tunsigned block_in_page;\n\tunsigned block_start, block_end;\n\tsector_t block_in_file;\n\tint nr_reads = 0;\n\tint ret = 0;\n\tint is_mapped_to_disk = 1;\n\n\tindex = pos >> PAGE_SHIFT;\n\tfrom = pos & (PAGE_SIZE - 1);\n\tto = from + len;\n\n\tpage = grab_cache_page_write_begin(mapping, index, flags);\n\tif (!page)\n\t\treturn -ENOMEM;\n\t*pagep = page;\n\t*fsdata = NULL;\n\n\tif (page_has_buffers(page)) {\n\t\tret = __block_write_begin(page, pos, len, get_block);\n\t\tif (unlikely(ret))\n\t\t\tgoto out_release;\n\t\treturn ret;\n\t}\n\n\tif (PageMappedToDisk(page))\n\t\treturn 0;\n\n\t/*\n\t * Allocate buffers so that we can keep track of state, and potentially\n\t * attach them to the page if an error occurs. In the common case of\n\t * no error, they will just be freed again without ever being attached\n\t * to the page (which is all OK, because we're under the page lock).\n\t *\n\t * Be careful: the buffer linked list is a NULL terminated one, rather\n\t * than the circular one we're used to.\n\t */\n\thead = alloc_page_buffers(page, blocksize, false);\n\tif (!head) {\n\t\tret = -ENOMEM;\n\t\tgoto out_release;\n\t}\n\n\tblock_in_file = (sector_t)page->index << (PAGE_SHIFT - blkbits);\n\n\t/*\n\t * We loop across all blocks in the page, whether or not they are\n\t * part of the affected region.  This is so we can discover if the\n\t * page is fully mapped-to-disk.\n\t */\n\tfor (block_start = 0, block_in_page = 0, bh = head;\n\t\t  block_start < PAGE_SIZE;\n\t\t  block_in_page++, block_start += blocksize, bh = bh->b_this_page) {\n\t\tint create;\n\n\t\tblock_end = block_start + blocksize;\n\t\tbh->b_state = 0;\n\t\tcreate = 1;\n\t\tif (block_start >= to)\n\t\t\tcreate = 0;\n\t\tret = get_block(inode, block_in_file + block_in_page,\n\t\t\t\t\tbh, create);\n\t\tif (ret)\n\t\t\tgoto failed;\n\t\tif (!buffer_mapped(bh))\n\t\t\tis_mapped_to_disk = 0;\n\t\tif (buffer_new(bh))\n\t\t\tclean_bdev_bh_alias(bh);\n\t\tif (PageUptodate(page)) {\n\t\t\tset_buffer_uptodate(bh);\n\t\t\tcontinue;\n\t\t}\n\t\tif (buffer_new(bh) || !buffer_mapped(bh)) {\n\t\t\tzero_user_segments(page, block_start, from,\n\t\t\t\t\t\t\tto, block_end);\n\t\t\tcontinue;\n\t\t}\n\t\tif (buffer_uptodate(bh))\n\t\t\tcontinue;\t/* reiserfs does this */\n\t\tif (block_start < from || block_end > to) {\n\t\t\tlock_buffer(bh);\n\t\t\tbh->b_end_io = end_buffer_read_nobh;\n\t\t\tsubmit_bh(REQ_OP_READ, 0, bh);\n\t\t\tnr_reads++;\n\t\t}\n\t}\n\n\tif (nr_reads) {\n\t\t/*\n\t\t * The page is locked, so these buffers are protected from\n\t\t * any VM or truncate activity.  Hence we don't need to care\n\t\t * for the buffer_head refcounts.\n\t\t */\n\t\tfor (bh = head; bh; bh = bh->b_this_page) {\n\t\t\twait_on_buffer(bh);\n\t\t\tif (!buffer_uptodate(bh))\n\t\t\t\tret = -EIO;\n\t\t}\n\t\tif (ret)\n\t\t\tgoto failed;\n\t}\n\n\tif (is_mapped_to_disk)\n\t\tSetPageMappedToDisk(page);\n\n\t*fsdata = head; /* to be released by nobh_write_end */\n\n\treturn 0;\n\nfailed:\n\tBUG_ON(!ret);\n\t/*\n\t * Error recovery is a bit difficult. We need to zero out blocks that\n\t * were newly allocated, and dirty them to ensure they get written out.\n\t * Buffers need to be attached to the page at this point, otherwise\n\t * the handling of potential IO errors during writeout would be hard\n\t * (could try doing synchronous writeout, but what if that fails too?)\n\t */\n\tattach_nobh_buffers(page, head);\n\tpage_zero_new_buffers(page, from, to);\n\nout_release:\n\tunlock_page(page);\n\tput_page(page);\n\t*pagep = NULL;\n\n\treturn ret;\n}\nEXPORT_SYMBOL(nobh_write_begin);\n\nint nobh_write_end(struct file *file, struct address_space *mapping,\n\t\t\tloff_t pos, unsigned len, unsigned copied,\n\t\t\tstruct page *page, void *fsdata)\n{\n\tstruct inode *inode = page->mapping->host;\n\tstruct buffer_head *head = fsdata;\n\tstruct buffer_head *bh;\n\tBUG_ON(fsdata != NULL && page_has_buffers(page));\n\n\tif (unlikely(copied < len) && head)\n\t\tattach_nobh_buffers(page, head);\n\tif (page_has_buffers(page))\n\t\treturn generic_write_end(file, mapping, pos, len,\n\t\t\t\t\tcopied, page, fsdata);\n\n\tSetPageUptodate(page);\n\tset_page_dirty(page);\n\tif (pos+copied > inode->i_size) {\n\t\ti_size_write(inode, pos+copied);\n\t\tmark_inode_dirty(inode);\n\t}\n\n\tunlock_page(page);\n\tput_page(page);\n\n\twhile (head) {\n\t\tbh = head;\n\t\thead = head->b_this_page;\n\t\tfree_buffer_head(bh);\n\t}\n\n\treturn copied;\n}\nEXPORT_SYMBOL(nobh_write_end);\n\n/*\n * nobh_writepage() - based on block_full_write_page() except\n * that it tries to operate without attaching bufferheads to\n * the page.\n */\nint nobh_writepage(struct page *page, get_block_t *get_block,\n\t\t\tstruct writeback_control *wbc)\n{\n\tstruct inode * const inode = page->mapping->host;\n\tloff_t i_size = i_size_read(inode);\n\tconst pgoff_t end_index = i_size >> PAGE_SHIFT;\n\tunsigned offset;\n\tint ret;\n\n\t/* Is the page fully inside i_size? */\n\tif (page->index < end_index)\n\t\tgoto out;\n\n\t/* Is the page fully outside i_size? (truncate in progress) */\n\toffset = i_size & (PAGE_SIZE-1);\n\tif (page->index >= end_index+1 || !offset) {\n\t\tunlock_page(page);\n\t\treturn 0; /* don't care */\n\t}\n\n\t/*\n\t * The page straddles i_size.  It must be zeroed out on each and every\n\t * writepage invocation because it may be mmapped.  \"A file is mapped\n\t * in multiples of the page size.  For a file that is not a multiple of\n\t * the  page size, the remaining memory is zeroed when mapped, and\n\t * writes to that region are not written out to the file.\"\n\t */\n\tzero_user_segment(page, offset, PAGE_SIZE);\nout:\n\tret = mpage_writepage(page, get_block, wbc);\n\tif (ret == -EAGAIN)\n\t\tret = __block_write_full_page(inode, page, get_block, wbc,\n\t\t\t\t\t      end_buffer_async_write);\n\treturn ret;\n}\nEXPORT_SYMBOL(nobh_writepage);\n\nint nobh_truncate_page(struct address_space *mapping,\n\t\t\tloff_t from, get_block_t *get_block)\n{\n\tpgoff_t index = from >> PAGE_SHIFT;\n\tunsigned offset = from & (PAGE_SIZE-1);\n\tunsigned blocksize;\n\tsector_t iblock;\n\tunsigned length, pos;\n\tstruct inode *inode = mapping->host;\n\tstruct page *page;\n\tstruct buffer_head map_bh;\n\tint err;\n\n\tblocksize = i_blocksize(inode);\n\tlength = offset & (blocksize - 1);\n\n\t/* Block boundary? Nothing to do */\n\tif (!length)\n\t\treturn 0;\n\n\tlength = blocksize - length;\n\tiblock = (sector_t)index << (PAGE_SHIFT - inode->i_blkbits);\n\n\tpage = grab_cache_page(mapping, index);\n\terr = -ENOMEM;\n\tif (!page)\n\t\tgoto out;\n\n\tif (page_has_buffers(page)) {\nhas_buffers:\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\treturn block_truncate_page(mapping, from, get_block);\n\t}\n\n\t/* Find the buffer that contains \"offset\" */\n\tpos = blocksize;\n\twhile (offset >= pos) {\n\t\tiblock++;\n\t\tpos += blocksize;\n\t}\n\n\tmap_bh.b_size = blocksize;\n\tmap_bh.b_state = 0;\n\terr = get_block(inode, iblock, &map_bh, 0);\n\tif (err)\n\t\tgoto unlock;\n\t/* unmapped? It's a hole - nothing to do */\n\tif (!buffer_mapped(&map_bh))\n\t\tgoto unlock;\n\n\t/* Ok, it's mapped. Make sure it's up-to-date */\n\tif (!PageUptodate(page)) {\n\t\terr = mapping->a_ops->readpage(NULL, page);\n\t\tif (err) {\n\t\t\tput_page(page);\n\t\t\tgoto out;\n\t\t}\n\t\tlock_page(page);\n\t\tif (!PageUptodate(page)) {\n\t\t\terr = -EIO;\n\t\t\tgoto unlock;\n\t\t}\n\t\tif (page_has_buffers(page))\n\t\t\tgoto has_buffers;\n\t}\n\tzero_user(page, offset, length);\n\tset_page_dirty(page);\n\terr = 0;\n\nunlock:\n\tunlock_page(page);\n\tput_page(page);\nout:\n\treturn err;\n}\nEXPORT_SYMBOL(nobh_truncate_page);\n\nint block_truncate_page(struct address_space *mapping,\n\t\t\tloff_t from, get_block_t *get_block)\n{\n\tpgoff_t index = from >> PAGE_SHIFT;\n\tunsigned offset = from & (PAGE_SIZE-1);\n\tunsigned blocksize;\n\tsector_t iblock;\n\tunsigned length, pos;\n\tstruct inode *inode = mapping->host;\n\tstruct page *page;\n\tstruct buffer_head *bh;\n\tint err;\n\n\tblocksize = i_blocksize(inode);\n\tlength = offset & (blocksize - 1);\n\n\t/* Block boundary? Nothing to do */\n\tif (!length)\n\t\treturn 0;\n\n\tlength = blocksize - length;\n\tiblock = (sector_t)index << (PAGE_SHIFT - inode->i_blkbits);\n\t\n\tpage = grab_cache_page(mapping, index);\n\terr = -ENOMEM;\n\tif (!page)\n\t\tgoto out;\n\n\tif (!page_has_buffers(page))\n\t\tcreate_empty_buffers(page, blocksize, 0);\n\n\t/* Find the buffer that contains \"offset\" */\n\tbh = page_buffers(page);\n\tpos = blocksize;\n\twhile (offset >= pos) {\n\t\tbh = bh->b_this_page;\n\t\tiblock++;\n\t\tpos += blocksize;\n\t}\n\n\terr = 0;\n\tif (!buffer_mapped(bh)) {\n\t\tWARN_ON(bh->b_size != blocksize);\n\t\terr = get_block(inode, iblock, bh, 0);\n\t\tif (err)\n\t\t\tgoto unlock;\n\t\t/* unmapped? It's a hole - nothing to do */\n\t\tif (!buffer_mapped(bh))\n\t\t\tgoto unlock;\n\t}\n\n\t/* Ok, it's mapped. Make sure it's up-to-date */\n\tif (PageUptodate(page))\n\t\tset_buffer_uptodate(bh);\n\n\tif (!buffer_uptodate(bh) && !buffer_delay(bh) && !buffer_unwritten(bh)) {\n\t\terr = -EIO;\n\t\tll_rw_block(REQ_OP_READ, 0, 1, &bh);\n\t\twait_on_buffer(bh);\n\t\t/* Uhhuh. Read error. Complain and punt. */\n\t\tif (!buffer_uptodate(bh))\n\t\t\tgoto unlock;\n\t}\n\n\tzero_user(page, offset, length);\n\tmark_buffer_dirty(bh);\n\terr = 0;\n\nunlock:\n\tunlock_page(page);\n\tput_page(page);\nout:\n\treturn err;\n}\nEXPORT_SYMBOL(block_truncate_page);\n\n/*\n * The generic ->writepage function for buffer-backed address_spaces\n */\nint block_write_full_page(struct page *page, get_block_t *get_block,\n\t\t\tstruct writeback_control *wbc)\n{\n\tstruct inode * const inode = page->mapping->host;\n\tloff_t i_size = i_size_read(inode);\n\tconst pgoff_t end_index = i_size >> PAGE_SHIFT;\n\tunsigned offset;\n\n\t/* Is the page fully inside i_size? */\n\tif (page->index < end_index)\n\t\treturn __block_write_full_page(inode, page, get_block, wbc,\n\t\t\t\t\t       end_buffer_async_write);\n\n\t/* Is the page fully outside i_size? (truncate in progress) */\n\toffset = i_size & (PAGE_SIZE-1);\n\tif (page->index >= end_index+1 || !offset) {\n\t\tunlock_page(page);\n\t\treturn 0; /* don't care */\n\t}\n\n\t/*\n\t * The page straddles i_size.  It must be zeroed out on each and every\n\t * writepage invocation because it may be mmapped.  \"A file is mapped\n\t * in multiples of the page size.  For a file that is not a multiple of\n\t * the  page size, the remaining memory is zeroed when mapped, and\n\t * writes to that region are not written out to the file.\"\n\t */\n\tzero_user_segment(page, offset, PAGE_SIZE);\n\treturn __block_write_full_page(inode, page, get_block, wbc,\n\t\t\t\t\t\t\tend_buffer_async_write);\n}\nEXPORT_SYMBOL(block_write_full_page);\n\nsector_t generic_block_bmap(struct address_space *mapping, sector_t block,\n\t\t\t    get_block_t *get_block)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct buffer_head tmp = {\n\t\t.b_size = i_blocksize(inode),\n\t};\n\n\tget_block(inode, block, &tmp, 0);\n\treturn tmp.b_blocknr;\n}\nEXPORT_SYMBOL(generic_block_bmap);\n\nstatic void end_bio_bh_io_sync(struct bio *bio)\n{\n\tstruct buffer_head *bh = bio->bi_private;\n\n\tif (unlikely(bio_flagged(bio, BIO_QUIET)))\n\t\tset_bit(BH_Quiet, &bh->b_state);\n\n\tbh->b_end_io(bh, !bio->bi_status);\n\tbio_put(bio);\n}\n\nstatic int submit_bh_wbc(int op, int op_flags, struct buffer_head *bh,\n\t\t\t enum rw_hint write_hint, struct writeback_control *wbc)\n{\n\tstruct bio *bio;\n\n\tBUG_ON(!buffer_locked(bh));\n\tBUG_ON(!buffer_mapped(bh));\n\tBUG_ON(!bh->b_end_io);\n\tBUG_ON(buffer_delay(bh));\n\tBUG_ON(buffer_unwritten(bh));\n\n\t/*\n\t * Only clear out a write error when rewriting\n\t */\n\tif (test_set_buffer_req(bh) && (op == REQ_OP_WRITE))\n\t\tclear_buffer_write_io_error(bh);\n\n\tbio = bio_alloc(GFP_NOIO, 1);\n\n\tfscrypt_set_bio_crypt_ctx_bh(bio, bh, GFP_NOIO);\n\n\tbio->bi_iter.bi_sector = bh->b_blocknr * (bh->b_size >> 9);\n\tbio_set_dev(bio, bh->b_bdev);\n\tbio->bi_write_hint = write_hint;\n\n\tbio_add_page(bio, bh->b_page, bh->b_size, bh_offset(bh));\n\tBUG_ON(bio->bi_iter.bi_size != bh->b_size);\n\n\tbio->bi_end_io = end_bio_bh_io_sync;\n\tbio->bi_private = bh;\n\n\tif (buffer_meta(bh))\n\t\top_flags |= REQ_META;\n\tif (buffer_prio(bh))\n\t\top_flags |= REQ_PRIO;\n\tbio_set_op_attrs(bio, op, op_flags);\n\n\t/* Take care of bh's that straddle the end of the device */\n\tguard_bio_eod(bio);\n\n\tif (wbc) {\n\t\twbc_init_bio(wbc, bio);\n\t\twbc_account_cgroup_owner(wbc, bh->b_page, bh->b_size);\n\t}\n\n\tsubmit_bio(bio);\n\treturn 0;\n}\n\nint submit_bh(int op, int op_flags, struct buffer_head *bh)\n{\n\treturn submit_bh_wbc(op, op_flags, bh, 0, NULL);\n}\nEXPORT_SYMBOL(submit_bh);\n\n/**\n * ll_rw_block: low-level access to block devices (DEPRECATED)\n * @op: whether to %READ or %WRITE\n * @op_flags: req_flag_bits\n * @nr: number of &struct buffer_heads in the array\n * @bhs: array of pointers to &struct buffer_head\n *\n * ll_rw_block() takes an array of pointers to &struct buffer_heads, and\n * requests an I/O operation on them, either a %REQ_OP_READ or a %REQ_OP_WRITE.\n * @op_flags contains flags modifying the detailed I/O behavior, most notably\n * %REQ_RAHEAD.\n *\n * This function drops any buffer that it cannot get a lock on (with the\n * BH_Lock state bit), any buffer that appears to be clean when doing a write\n * request, and any buffer that appears to be up-to-date when doing read\n * request.  Further it marks as clean buffers that are processed for\n * writing (the buffer cache won't assume that they are actually clean\n * until the buffer gets unlocked).\n *\n * ll_rw_block sets b_end_io to simple completion handler that marks\n * the buffer up-to-date (if appropriate), unlocks the buffer and wakes\n * any waiters. \n *\n * All of the buffers must be for the same device, and must also be a\n * multiple of the current approved size for the device.\n */\nvoid ll_rw_block(int op, int op_flags,  int nr, struct buffer_head *bhs[])\n{\n\tint i;\n\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct buffer_head *bh = bhs[i];\n\n\t\tif (!trylock_buffer(bh))\n\t\t\tcontinue;\n\t\tif (op == WRITE) {\n\t\t\tif (test_clear_buffer_dirty(bh)) {\n\t\t\t\tbh->b_end_io = end_buffer_write_sync;\n\t\t\t\tget_bh(bh);\n\t\t\t\tsubmit_bh(op, op_flags, bh);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!buffer_uptodate(bh)) {\n\t\t\t\tbh->b_end_io = end_buffer_read_sync;\n\t\t\t\tget_bh(bh);\n\t\t\t\tsubmit_bh(op, op_flags, bh);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tunlock_buffer(bh);\n\t}\n}\nEXPORT_SYMBOL(ll_rw_block);\n\nvoid write_dirty_buffer(struct buffer_head *bh, int op_flags)\n{\n\tlock_buffer(bh);\n\tif (!test_clear_buffer_dirty(bh)) {\n\t\tunlock_buffer(bh);\n\t\treturn;\n\t}\n\tbh->b_end_io = end_buffer_write_sync;\n\tget_bh(bh);\n\tsubmit_bh(REQ_OP_WRITE, op_flags, bh);\n}\nEXPORT_SYMBOL(write_dirty_buffer);\n\n/*\n * For a data-integrity writeout, we need to wait upon any in-progress I/O\n * and then start new I/O and then wait upon it.  The caller must have a ref on\n * the buffer_head.\n */\nint __sync_dirty_buffer(struct buffer_head *bh, int op_flags)\n{\n\tint ret = 0;\n\n\tWARN_ON(atomic_read(&bh->b_count) < 1);\n\tlock_buffer(bh);\n\tif (test_clear_buffer_dirty(bh)) {\n\t\t/*\n\t\t * The bh should be mapped, but it might not be if the\n\t\t * device was hot-removed. Not much we can do but fail the I/O.\n\t\t */\n\t\tif (!buffer_mapped(bh)) {\n\t\t\tunlock_buffer(bh);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tget_bh(bh);\n\t\tbh->b_end_io = end_buffer_write_sync;\n\t\tret = submit_bh(REQ_OP_WRITE, op_flags, bh);\n\t\twait_on_buffer(bh);\n\t\tif (!ret && !buffer_uptodate(bh))\n\t\t\tret = -EIO;\n\t} else {\n\t\tunlock_buffer(bh);\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(__sync_dirty_buffer);\n\nint sync_dirty_buffer(struct buffer_head *bh)\n{\n\treturn __sync_dirty_buffer(bh, REQ_SYNC);\n}\nEXPORT_SYMBOL(sync_dirty_buffer);\n\n/*\n * try_to_free_buffers() checks if all the buffers on this particular page\n * are unused, and releases them if so.\n *\n * Exclusion against try_to_free_buffers may be obtained by either\n * locking the page or by holding its mapping's private_lock.\n *\n * If the page is dirty but all the buffers are clean then we need to\n * be sure to mark the page clean as well.  This is because the page\n * may be against a block device, and a later reattachment of buffers\n * to a dirty page will set *all* buffers dirty.  Which would corrupt\n * filesystem data on the same device.\n *\n * The same applies to regular filesystem pages: if all the buffers are\n * clean then we set the page clean and proceed.  To do that, we require\n * total exclusion from __set_page_dirty_buffers().  That is obtained with\n * private_lock.\n *\n * try_to_free_buffers() is non-blocking.\n */\nstatic inline int buffer_busy(struct buffer_head *bh)\n{\n#ifdef CONFIG_DEBUG_AID_FOR_SYZBOT\n\tcurrent->getblk_executed |= 0x80;\n\tcurrent->getblk_bh_count = atomic_read(&bh->b_count);\n\tcurrent->getblk_bh_state = bh->b_state;\n#endif\n\treturn atomic_read(&bh->b_count) |\n\t\t(bh->b_state & ((1 << BH_Dirty) | (1 << BH_Lock)));\n}\n\nstatic int\ndrop_buffers(struct page *page, struct buffer_head **buffers_to_free)\n{\n\tstruct buffer_head *head = page_buffers(page);\n\tstruct buffer_head *bh;\n\n\tbh = head;\n\tdo {\n\t\tif (buffer_busy(bh))\n\t\t\tgoto failed;\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\n\tdo {\n\t\tstruct buffer_head *next = bh->b_this_page;\n\n\t\tif (bh->b_assoc_map)\n\t\t\t__remove_assoc_queue(bh);\n\t\tbh = next;\n\t} while (bh != head);\n\t*buffers_to_free = head;\n\tdetach_page_private(page);\n\treturn 1;\nfailed:\n\treturn 0;\n}\n\nint try_to_free_buffers(struct page *page)\n{\n\tstruct address_space * const mapping = page->mapping;\n\tstruct buffer_head *buffers_to_free = NULL;\n\tint ret = 0;\n\n\tBUG_ON(!PageLocked(page));\n\tif (PageWriteback(page)) {\n#ifdef CONFIG_DEBUG_AID_FOR_SYZBOT\n\t\tcurrent->getblk_executed |= 0x10;\n#endif\n\t\treturn 0;\n\t}\n\n\tif (mapping == NULL) {\t\t/* can this still happen? */\n\t\tret = drop_buffers(page, &buffers_to_free);\n#ifdef CONFIG_DEBUG_AID_FOR_SYZBOT\n\t\tcurrent->getblk_executed |= 0x20;\n#endif\n\t\tgoto out;\n\t}\n\n\tspin_lock(&mapping->private_lock);\n\tret = drop_buffers(page, &buffers_to_free);\n\n\t/*\n\t * If the filesystem writes its buffers by hand (eg ext3)\n\t * then we can have clean buffers against a dirty page.  We\n\t * clean the page here; otherwise the VM will never notice\n\t * that the filesystem did any IO at all.\n\t *\n\t * Also, during truncate, discard_buffer will have marked all\n\t * the page's buffers clean.  We discover that here and clean\n\t * the page also.\n\t *\n\t * private_lock must be held over this entire operation in order\n\t * to synchronise against __set_page_dirty_buffers and prevent the\n\t * dirty bit from being lost.\n\t */\n\tif (ret)\n\t\tcancel_dirty_page(page);\n\tspin_unlock(&mapping->private_lock);\n#ifdef CONFIG_DEBUG_AID_FOR_SYZBOT\n\tcurrent->getblk_executed |= 0x40;\n#endif\nout:\n\tif (buffers_to_free) {\n\t\tstruct buffer_head *bh = buffers_to_free;\n\n\t\tdo {\n\t\t\tstruct buffer_head *next = bh->b_this_page;\n\t\t\tfree_buffer_head(bh);\n\t\t\tbh = next;\n\t\t} while (bh != buffers_to_free);\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(try_to_free_buffers);\n\n/*\n * There are no bdflush tunables left.  But distributions are\n * still running obsolete flush daemons, so we terminate them here.\n *\n * Use of bdflush() is deprecated and will be removed in a future kernel.\n * The `flush-X' kernel threads fully replace bdflush daemons and this call.\n */\nSYSCALL_DEFINE2(bdflush, int, func, long, data)\n{\n\tstatic int msg_count;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (msg_count < 5) {\n\t\tmsg_count++;\n\t\tprintk(KERN_INFO\n\t\t\t\"warning: process `%s' used the obsolete bdflush\"\n\t\t\t\" system call\\n\", current->comm);\n\t\tprintk(KERN_INFO \"Fix your initscripts?\\n\");\n\t}\n\n\tif (func == 1)\n\t\tdo_exit(0);\n\treturn 0;\n}\n\n/*\n * Buffer-head allocation\n */\nstatic struct kmem_cache *bh_cachep __read_mostly;\n\n/*\n * Once the number of bh's in the machine exceeds this level, we start\n * stripping them in writeback.\n */\nstatic unsigned long max_buffer_heads;\n\nint buffer_heads_over_limit;\n\nstruct bh_accounting {\n\tint nr;\t\t\t/* Number of live bh's */\n\tint ratelimit;\t\t/* Limit cacheline bouncing */\n};\n\nstatic DEFINE_PER_CPU(struct bh_accounting, bh_accounting) = {0, 0};\n\nstatic void recalc_bh_state(void)\n{\n\tint i;\n\tint tot = 0;\n\n\tif (__this_cpu_inc_return(bh_accounting.ratelimit) - 1 < 4096)\n\t\treturn;\n\t__this_cpu_write(bh_accounting.ratelimit, 0);\n\tfor_each_online_cpu(i)\n\t\ttot += per_cpu(bh_accounting, i).nr;\n\tbuffer_heads_over_limit = (tot > max_buffer_heads);\n}\n\nstruct buffer_head *alloc_buffer_head(gfp_t gfp_flags)\n{\n\tstruct buffer_head *ret = kmem_cache_zalloc(bh_cachep, gfp_flags);\n\tif (ret) {\n\t\tINIT_LIST_HEAD(&ret->b_assoc_buffers);\n\t\tspin_lock_init(&ret->b_uptodate_lock);\n\t\tpreempt_disable();\n\t\t__this_cpu_inc(bh_accounting.nr);\n\t\trecalc_bh_state();\n\t\tpreempt_enable();\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(alloc_buffer_head);\n\nvoid free_buffer_head(struct buffer_head *bh)\n{\n\tBUG_ON(!list_empty(&bh->b_assoc_buffers));\n\tkmem_cache_free(bh_cachep, bh);\n\tpreempt_disable();\n\t__this_cpu_dec(bh_accounting.nr);\n\trecalc_bh_state();\n\tpreempt_enable();\n}\nEXPORT_SYMBOL(free_buffer_head);\n\nstatic int buffer_exit_cpu_dead(unsigned int cpu)\n{\n\tint i;\n\tstruct bh_lru *b = &per_cpu(bh_lrus, cpu);\n\n\tfor (i = 0; i < BH_LRU_SIZE; i++) {\n\t\tbrelse(b->bhs[i]);\n\t\tb->bhs[i] = NULL;\n\t}\n\tthis_cpu_add(bh_accounting.nr, per_cpu(bh_accounting, cpu).nr);\n\tper_cpu(bh_accounting, cpu).nr = 0;\n\treturn 0;\n}\n\n/**\n * bh_uptodate_or_lock - Test whether the buffer is uptodate\n * @bh: struct buffer_head\n *\n * Return true if the buffer is up-to-date and false,\n * with the buffer locked, if not.\n */\nint bh_uptodate_or_lock(struct buffer_head *bh)\n{\n\tif (!buffer_uptodate(bh)) {\n\t\tlock_buffer(bh);\n\t\tif (!buffer_uptodate(bh))\n\t\t\treturn 0;\n\t\tunlock_buffer(bh);\n\t}\n\treturn 1;\n}\nEXPORT_SYMBOL(bh_uptodate_or_lock);\n\n/**\n * bh_submit_read - Submit a locked buffer for reading\n * @bh: struct buffer_head\n *\n * Returns zero on success and -EIO on error.\n */\nint bh_submit_read(struct buffer_head *bh)\n{\n\tBUG_ON(!buffer_locked(bh));\n\n\tif (buffer_uptodate(bh)) {\n\t\tunlock_buffer(bh);\n\t\treturn 0;\n\t}\n\n\tget_bh(bh);\n\tbh->b_end_io = end_buffer_read_sync;\n\tsubmit_bh(REQ_OP_READ, 0, bh);\n\twait_on_buffer(bh);\n\tif (buffer_uptodate(bh))\n\t\treturn 0;\n\treturn -EIO;\n}\nEXPORT_SYMBOL(bh_submit_read);\n\nvoid __init buffer_init(void)\n{\n\tunsigned long nrpages;\n\tint ret;\n\n\tbh_cachep = kmem_cache_create(\"buffer_head\",\n\t\t\tsizeof(struct buffer_head), 0,\n\t\t\t\t(SLAB_RECLAIM_ACCOUNT|SLAB_PANIC|\n\t\t\t\tSLAB_MEM_SPREAD),\n\t\t\t\tNULL);\n\n\t/*\n\t * Limit the bh occupancy to 10% of ZONE_NORMAL\n\t */\n\tnrpages = (nr_free_buffer_pages() * 10) / 100;\n\tmax_buffer_heads = nrpages * (PAGE_SIZE / sizeof(struct buffer_head));\n\tret = cpuhp_setup_state_nocalls(CPUHP_FS_BUFF_DEAD, \"fs/buffer:dead\",\n\t\t\t\t\tNULL, buffer_exit_cpu_dead);\n\tWARN_ON(ret < 0);\n}\n"}, "14": {"id": 14, "path": "/src/drivers/acpi/pci_root.c", "content": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n *  pci_root.c - ACPI PCI Root Bridge Driver ($Revision: 40 $)\n *\n *  Copyright (C) 2001, 2002 Andy Grover <andrew.grover@intel.com>\n *  Copyright (C) 2001, 2002 Paul Diefenbaugh <paul.s.diefenbaugh@intel.com>\n */\n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/types.h>\n#include <linux/mutex.h>\n#include <linux/pm.h>\n#include <linux/pm_runtime.h>\n#include <linux/pci.h>\n#include <linux/pci-acpi.h>\n#include <linux/dmar.h>\n#include <linux/acpi.h>\n#include <linux/slab.h>\n#include <linux/dmi.h>\n#include <linux/platform_data/x86/apple.h>\n#include <acpi/apei.h>\t/* for acpi_hest_init() */\n\n#include \"internal.h\"\n\n#define ACPI_PCI_ROOT_CLASS\t\t\"pci_bridge\"\n#define ACPI_PCI_ROOT_DEVICE_NAME\t\"PCI Root Bridge\"\nstatic int acpi_pci_root_add(struct acpi_device *device,\n\t\t\t     const struct acpi_device_id *not_used);\nstatic void acpi_pci_root_remove(struct acpi_device *device);\n\nstatic int acpi_pci_root_scan_dependent(struct acpi_device *adev)\n{\n\tacpiphp_check_host_bridge(adev);\n\treturn 0;\n}\n\n#define ACPI_PCIE_REQ_SUPPORT (OSC_PCI_EXT_CONFIG_SUPPORT \\\n\t\t\t\t| OSC_PCI_ASPM_SUPPORT \\\n\t\t\t\t| OSC_PCI_CLOCK_PM_SUPPORT \\\n\t\t\t\t| OSC_PCI_MSI_SUPPORT)\n\nstatic const struct acpi_device_id root_device_ids[] = {\n\t{\"PNP0A03\", 0},\n\t{\"\", 0},\n};\n\nstatic struct acpi_scan_handler pci_root_handler = {\n\t.ids = root_device_ids,\n\t.attach = acpi_pci_root_add,\n\t.detach = acpi_pci_root_remove,\n\t.hotplug = {\n\t\t.enabled = true,\n\t\t.scan_dependent = acpi_pci_root_scan_dependent,\n\t},\n};\n\nstatic DEFINE_MUTEX(osc_lock);\n\n/**\n * acpi_is_root_bridge - determine whether an ACPI CA node is a PCI root bridge\n * @handle:  the ACPI CA node in question.\n *\n * Note: we could make this API take a struct acpi_device * instead, but\n * for now, it's more convenient to operate on an acpi_handle.\n */\nint acpi_is_root_bridge(acpi_handle handle)\n{\n\tint ret;\n\tstruct acpi_device *device;\n\n\tret = acpi_bus_get_device(handle, &device);\n\tif (ret)\n\t\treturn 0;\n\n\tret = acpi_match_device_ids(device, root_device_ids);\n\tif (ret)\n\t\treturn 0;\n\telse\n\t\treturn 1;\n}\nEXPORT_SYMBOL_GPL(acpi_is_root_bridge);\n\nstatic acpi_status\nget_root_bridge_busnr_callback(struct acpi_resource *resource, void *data)\n{\n\tstruct resource *res = data;\n\tstruct acpi_resource_address64 address;\n\tacpi_status status;\n\n\tstatus = acpi_resource_to_address64(resource, &address);\n\tif (ACPI_FAILURE(status))\n\t\treturn AE_OK;\n\n\tif ((address.address.address_length > 0) &&\n\t    (address.resource_type == ACPI_BUS_NUMBER_RANGE)) {\n\t\tres->start = address.address.minimum;\n\t\tres->end = address.address.minimum + address.address.address_length - 1;\n\t}\n\n\treturn AE_OK;\n}\n\nstatic acpi_status try_get_root_bridge_busnr(acpi_handle handle,\n\t\t\t\t\t     struct resource *res)\n{\n\tacpi_status status;\n\n\tres->start = -1;\n\tstatus =\n\t    acpi_walk_resources(handle, METHOD_NAME__CRS,\n\t\t\t\tget_root_bridge_busnr_callback, res);\n\tif (ACPI_FAILURE(status))\n\t\treturn status;\n\tif (res->start == -1)\n\t\treturn AE_ERROR;\n\treturn AE_OK;\n}\n\nstruct pci_osc_bit_struct {\n\tu32 bit;\n\tchar *desc;\n};\n\nstatic struct pci_osc_bit_struct pci_osc_support_bit[] = {\n\t{ OSC_PCI_EXT_CONFIG_SUPPORT, \"ExtendedConfig\" },\n\t{ OSC_PCI_ASPM_SUPPORT, \"ASPM\" },\n\t{ OSC_PCI_CLOCK_PM_SUPPORT, \"ClockPM\" },\n\t{ OSC_PCI_SEGMENT_GROUPS_SUPPORT, \"Segments\" },\n\t{ OSC_PCI_MSI_SUPPORT, \"MSI\" },\n\t{ OSC_PCI_EDR_SUPPORT, \"EDR\" },\n\t{ OSC_PCI_HPX_TYPE_3_SUPPORT, \"HPX-Type3\" },\n};\n\nstatic struct pci_osc_bit_struct pci_osc_control_bit[] = {\n\t{ OSC_PCI_EXPRESS_NATIVE_HP_CONTROL, \"PCIeHotplug\" },\n\t{ OSC_PCI_SHPC_NATIVE_HP_CONTROL, \"SHPCHotplug\" },\n\t{ OSC_PCI_EXPRESS_PME_CONTROL, \"PME\" },\n\t{ OSC_PCI_EXPRESS_AER_CONTROL, \"AER\" },\n\t{ OSC_PCI_EXPRESS_CAPABILITY_CONTROL, \"PCIeCapability\" },\n\t{ OSC_PCI_EXPRESS_LTR_CONTROL, \"LTR\" },\n\t{ OSC_PCI_EXPRESS_DPC_CONTROL, \"DPC\" },\n};\n\nstatic void decode_osc_bits(struct acpi_pci_root *root, char *msg, u32 word,\n\t\t\t    struct pci_osc_bit_struct *table, int size)\n{\n\tchar buf[80];\n\tint i, len = 0;\n\tstruct pci_osc_bit_struct *entry;\n\n\tbuf[0] = '\\0';\n\tfor (i = 0, entry = table; i < size; i++, entry++)\n\t\tif (word & entry->bit)\n\t\t\tlen += scnprintf(buf + len, sizeof(buf) - len, \"%s%s\",\n\t\t\t\t\tlen ? \" \" : \"\", entry->desc);\n\n\tdev_info(&root->device->dev, \"_OSC: %s [%s]\\n\", msg, buf);\n}\n\nstatic void decode_osc_support(struct acpi_pci_root *root, char *msg, u32 word)\n{\n\tdecode_osc_bits(root, msg, word, pci_osc_support_bit,\n\t\t\tARRAY_SIZE(pci_osc_support_bit));\n}\n\nstatic void decode_osc_control(struct acpi_pci_root *root, char *msg, u32 word)\n{\n\tdecode_osc_bits(root, msg, word, pci_osc_control_bit,\n\t\t\tARRAY_SIZE(pci_osc_control_bit));\n}\n\nstatic u8 pci_osc_uuid_str[] = \"33DB4D5B-1FF7-401C-9657-7441C03DD766\";\n\nstatic acpi_status acpi_pci_run_osc(acpi_handle handle,\n\t\t\t\t    const u32 *capbuf, u32 *retval)\n{\n\tstruct acpi_osc_context context = {\n\t\t.uuid_str = pci_osc_uuid_str,\n\t\t.rev = 1,\n\t\t.cap.length = 12,\n\t\t.cap.pointer = (void *)capbuf,\n\t};\n\tacpi_status status;\n\n\tstatus = acpi_run_osc(handle, &context);\n\tif (ACPI_SUCCESS(status)) {\n\t\t*retval = *((u32 *)(context.ret.pointer + 8));\n\t\tkfree(context.ret.pointer);\n\t}\n\treturn status;\n}\n\nstatic acpi_status acpi_pci_query_osc(struct acpi_pci_root *root,\n\t\t\t\t\tu32 support,\n\t\t\t\t\tu32 *control)\n{\n\tacpi_status status;\n\tu32 result, capbuf[3];\n\n\tsupport &= OSC_PCI_SUPPORT_MASKS;\n\tsupport |= root->osc_support_set;\n\n\tcapbuf[OSC_QUERY_DWORD] = OSC_QUERY_ENABLE;\n\tcapbuf[OSC_SUPPORT_DWORD] = support;\n\tif (control) {\n\t\t*control &= OSC_PCI_CONTROL_MASKS;\n\t\tcapbuf[OSC_CONTROL_DWORD] = *control | root->osc_control_set;\n\t} else {\n\t\t/* Run _OSC query only with existing controls. */\n\t\tcapbuf[OSC_CONTROL_DWORD] = root->osc_control_set;\n\t}\n\n\tstatus = acpi_pci_run_osc(root->device->handle, capbuf, &result);\n\tif (ACPI_SUCCESS(status)) {\n\t\troot->osc_support_set = support;\n\t\tif (control)\n\t\t\t*control = result;\n\t}\n\treturn status;\n}\n\nstatic acpi_status acpi_pci_osc_support(struct acpi_pci_root *root, u32 flags)\n{\n\tacpi_status status;\n\n\tmutex_lock(&osc_lock);\n\tstatus = acpi_pci_query_osc(root, flags, NULL);\n\tmutex_unlock(&osc_lock);\n\treturn status;\n}\n\nstruct acpi_pci_root *acpi_pci_find_root(acpi_handle handle)\n{\n\tstruct acpi_pci_root *root;\n\tstruct acpi_device *device;\n\n\tif (acpi_bus_get_device(handle, &device) ||\n\t    acpi_match_device_ids(device, root_device_ids))\n\t\treturn NULL;\n\n\troot = acpi_driver_data(device);\n\n\treturn root;\n}\nEXPORT_SYMBOL_GPL(acpi_pci_find_root);\n\nstruct acpi_handle_node {\n\tstruct list_head node;\n\tacpi_handle handle;\n};\n\n/**\n * acpi_get_pci_dev - convert ACPI CA handle to struct pci_dev\n * @handle: the handle in question\n *\n * Given an ACPI CA handle, the desired PCI device is located in the\n * list of PCI devices.\n *\n * If the device is found, its reference count is increased and this\n * function returns a pointer to its data structure.  The caller must\n * decrement the reference count by calling pci_dev_put().\n * If no device is found, %NULL is returned.\n */\nstruct pci_dev *acpi_get_pci_dev(acpi_handle handle)\n{\n\tint dev, fn;\n\tunsigned long long adr;\n\tacpi_status status;\n\tacpi_handle phandle;\n\tstruct pci_bus *pbus;\n\tstruct pci_dev *pdev = NULL;\n\tstruct acpi_handle_node *node, *tmp;\n\tstruct acpi_pci_root *root;\n\tLIST_HEAD(device_list);\n\n\t/*\n\t * Walk up the ACPI CA namespace until we reach a PCI root bridge.\n\t */\n\tphandle = handle;\n\twhile (!acpi_is_root_bridge(phandle)) {\n\t\tnode = kzalloc(sizeof(struct acpi_handle_node), GFP_KERNEL);\n\t\tif (!node)\n\t\t\tgoto out;\n\n\t\tINIT_LIST_HEAD(&node->node);\n\t\tnode->handle = phandle;\n\t\tlist_add(&node->node, &device_list);\n\n\t\tstatus = acpi_get_parent(phandle, &phandle);\n\t\tif (ACPI_FAILURE(status))\n\t\t\tgoto out;\n\t}\n\n\troot = acpi_pci_find_root(phandle);\n\tif (!root)\n\t\tgoto out;\n\n\tpbus = root->bus;\n\n\t/*\n\t * Now, walk back down the PCI device tree until we return to our\n\t * original handle. Assumes that everything between the PCI root\n\t * bridge and the device we're looking for must be a P2P bridge.\n\t */\n\tlist_for_each_entry(node, &device_list, node) {\n\t\tacpi_handle hnd = node->handle;\n\t\tstatus = acpi_evaluate_integer(hnd, \"_ADR\", NULL, &adr);\n\t\tif (ACPI_FAILURE(status))\n\t\t\tgoto out;\n\t\tdev = (adr >> 16) & 0xffff;\n\t\tfn  = adr & 0xffff;\n\n\t\tpdev = pci_get_slot(pbus, PCI_DEVFN(dev, fn));\n\t\tif (!pdev || hnd == handle)\n\t\t\tbreak;\n\n\t\tpbus = pdev->subordinate;\n\t\tpci_dev_put(pdev);\n\n\t\t/*\n\t\t * This function may be called for a non-PCI device that has a\n\t\t * PCI parent (eg. a disk under a PCI SATA controller).  In that\n\t\t * case pdev->subordinate will be NULL for the parent.\n\t\t */\n\t\tif (!pbus) {\n\t\t\tdev_dbg(&pdev->dev, \"Not a PCI-to-PCI bridge\\n\");\n\t\t\tpdev = NULL;\n\t\t\tbreak;\n\t\t}\n\t}\nout:\n\tlist_for_each_entry_safe(node, tmp, &device_list, node)\n\t\tkfree(node);\n\n\treturn pdev;\n}\nEXPORT_SYMBOL_GPL(acpi_get_pci_dev);\n\n/**\n * acpi_pci_osc_control_set - Request control of PCI root _OSC features.\n * @handle: ACPI handle of a PCI root bridge (or PCIe Root Complex).\n * @mask: Mask of _OSC bits to request control of, place to store control mask.\n * @req: Mask of _OSC bits the control of is essential to the caller.\n *\n * Run _OSC query for @mask and if that is successful, compare the returned\n * mask of control bits with @req.  If all of the @req bits are set in the\n * returned mask, run _OSC request for it.\n *\n * The variable at the @mask address may be modified regardless of whether or\n * not the function returns success.  On success it will contain the mask of\n * _OSC bits the BIOS has granted control of, but its contents are meaningless\n * on failure.\n **/\nacpi_status acpi_pci_osc_control_set(acpi_handle handle, u32 *mask, u32 req)\n{\n\tstruct acpi_pci_root *root;\n\tacpi_status status = AE_OK;\n\tu32 ctrl, capbuf[3];\n\n\tif (!mask)\n\t\treturn AE_BAD_PARAMETER;\n\n\tctrl = *mask & OSC_PCI_CONTROL_MASKS;\n\tif ((ctrl & req) != req)\n\t\treturn AE_TYPE;\n\n\troot = acpi_pci_find_root(handle);\n\tif (!root)\n\t\treturn AE_NOT_EXIST;\n\n\tmutex_lock(&osc_lock);\n\n\t*mask = ctrl | root->osc_control_set;\n\t/* No need to evaluate _OSC if the control was already granted. */\n\tif ((root->osc_control_set & ctrl) == ctrl)\n\t\tgoto out;\n\n\t/* Need to check the available controls bits before requesting them. */\n\twhile (*mask) {\n\t\tstatus = acpi_pci_query_osc(root, root->osc_support_set, mask);\n\t\tif (ACPI_FAILURE(status))\n\t\t\tgoto out;\n\t\tif (ctrl == *mask)\n\t\t\tbreak;\n\t\tdecode_osc_control(root, \"platform does not support\",\n\t\t\t\t   ctrl & ~(*mask));\n\t\tctrl = *mask;\n\t}\n\n\tif ((ctrl & req) != req) {\n\t\tdecode_osc_control(root, \"not requesting control; platform does not support\",\n\t\t\t\t   req & ~(ctrl));\n\t\tstatus = AE_SUPPORT;\n\t\tgoto out;\n\t}\n\n\tcapbuf[OSC_QUERY_DWORD] = 0;\n\tcapbuf[OSC_SUPPORT_DWORD] = root->osc_support_set;\n\tcapbuf[OSC_CONTROL_DWORD] = ctrl;\n\tstatus = acpi_pci_run_osc(handle, capbuf, mask);\n\tif (ACPI_SUCCESS(status))\n\t\troot->osc_control_set = *mask;\nout:\n\tmutex_unlock(&osc_lock);\n\treturn status;\n}\nEXPORT_SYMBOL(acpi_pci_osc_control_set);\n\nstatic void negotiate_os_control(struct acpi_pci_root *root, int *no_aspm,\n\t\t\t\t bool is_pcie)\n{\n\tu32 support, control, requested;\n\tacpi_status status;\n\tstruct acpi_device *device = root->device;\n\tacpi_handle handle = device->handle;\n\n\t/*\n\t * Apple always return failure on _OSC calls when _OSI(\"Darwin\") has\n\t * been called successfully. We know the feature set supported by the\n\t * platform, so avoid calling _OSC at all\n\t */\n\tif (x86_apple_machine) {\n\t\troot->osc_control_set = ~OSC_PCI_EXPRESS_PME_CONTROL;\n\t\tdecode_osc_control(root, \"OS assumes control of\",\n\t\t\t\t   root->osc_control_set);\n\t\treturn;\n\t}\n\n\t/*\n\t * All supported architectures that use ACPI have support for\n\t * PCI domains, so we indicate this in _OSC support capabilities.\n\t */\n\tsupport = OSC_PCI_SEGMENT_GROUPS_SUPPORT;\n\tsupport |= OSC_PCI_HPX_TYPE_3_SUPPORT;\n\tif (pci_ext_cfg_avail())\n\t\tsupport |= OSC_PCI_EXT_CONFIG_SUPPORT;\n\tif (pcie_aspm_support_enabled())\n\t\tsupport |= OSC_PCI_ASPM_SUPPORT | OSC_PCI_CLOCK_PM_SUPPORT;\n\tif (pci_msi_enabled())\n\t\tsupport |= OSC_PCI_MSI_SUPPORT;\n\tif (IS_ENABLED(CONFIG_PCIE_EDR))\n\t\tsupport |= OSC_PCI_EDR_SUPPORT;\n\n\tdecode_osc_support(root, \"OS supports\", support);\n\tstatus = acpi_pci_osc_support(root, support);\n\tif (ACPI_FAILURE(status)) {\n\t\t*no_aspm = 1;\n\n\t\t/* _OSC is optional for PCI host bridges */\n\t\tif ((status == AE_NOT_FOUND) && !is_pcie)\n\t\t\treturn;\n\n\t\tdev_info(&device->dev, \"_OSC failed (%s)%s\\n\",\n\t\t\t acpi_format_exception(status),\n\t\t\t pcie_aspm_support_enabled() ? \"; disabling ASPM\" : \"\");\n\t\treturn;\n\t}\n\n\tif (pcie_ports_disabled) {\n\t\tdev_info(&device->dev, \"PCIe port services disabled; not requesting _OSC control\\n\");\n\t\treturn;\n\t}\n\n\tif ((support & ACPI_PCIE_REQ_SUPPORT) != ACPI_PCIE_REQ_SUPPORT) {\n\t\tdecode_osc_support(root, \"not requesting OS control; OS requires\",\n\t\t\t\t   ACPI_PCIE_REQ_SUPPORT);\n\t\treturn;\n\t}\n\n\tcontrol = OSC_PCI_EXPRESS_CAPABILITY_CONTROL\n\t\t| OSC_PCI_EXPRESS_PME_CONTROL;\n\n\tif (IS_ENABLED(CONFIG_PCIEASPM))\n\t\tcontrol |= OSC_PCI_EXPRESS_LTR_CONTROL;\n\n\tif (IS_ENABLED(CONFIG_HOTPLUG_PCI_PCIE))\n\t\tcontrol |= OSC_PCI_EXPRESS_NATIVE_HP_CONTROL;\n\n\tif (IS_ENABLED(CONFIG_HOTPLUG_PCI_SHPC))\n\t\tcontrol |= OSC_PCI_SHPC_NATIVE_HP_CONTROL;\n\n\tif (pci_aer_available())\n\t\tcontrol |= OSC_PCI_EXPRESS_AER_CONTROL;\n\n\t/*\n\t * Per the Downstream Port Containment Related Enhancements ECN to\n\t * the PCI Firmware Spec, r3.2, sec 4.5.1, table 4-5,\n\t * OSC_PCI_EXPRESS_DPC_CONTROL indicates the OS supports both DPC\n\t * and EDR.\n\t */\n\tif (IS_ENABLED(CONFIG_PCIE_DPC) && IS_ENABLED(CONFIG_PCIE_EDR))\n\t\tcontrol |= OSC_PCI_EXPRESS_DPC_CONTROL;\n\n\trequested = control;\n\tstatus = acpi_pci_osc_control_set(handle, &control,\n\t\t\t\t\t  OSC_PCI_EXPRESS_CAPABILITY_CONTROL);\n\tif (ACPI_SUCCESS(status)) {\n\t\tdecode_osc_control(root, \"OS now controls\", control);\n\t\tif (acpi_gbl_FADT.boot_flags & ACPI_FADT_NO_ASPM) {\n\t\t\t/*\n\t\t\t * We have ASPM control, but the FADT indicates that\n\t\t\t * it's unsupported. Leave existing configuration\n\t\t\t * intact and prevent the OS from touching it.\n\t\t\t */\n\t\t\tdev_info(&device->dev, \"FADT indicates ASPM is unsupported, using BIOS configuration\\n\");\n\t\t\t*no_aspm = 1;\n\t\t}\n\t} else {\n\t\tdecode_osc_control(root, \"OS requested\", requested);\n\t\tdecode_osc_control(root, \"platform willing to grant\", control);\n\t\tdev_info(&device->dev, \"_OSC failed (%s); disabling ASPM\\n\",\n\t\t\tacpi_format_exception(status));\n\n\t\t/*\n\t\t * We want to disable ASPM here, but aspm_disabled\n\t\t * needs to remain in its state from boot so that we\n\t\t * properly handle PCIe 1.1 devices.  So we set this\n\t\t * flag here, to defer the action until after the ACPI\n\t\t * root scan.\n\t\t */\n\t\t*no_aspm = 1;\n\t}\n}\n\nstatic int acpi_pci_root_add(struct acpi_device *device,\n\t\t\t     const struct acpi_device_id *not_used)\n{\n\tunsigned long long segment, bus;\n\tacpi_status status;\n\tint result;\n\tstruct acpi_pci_root *root;\n\tacpi_handle handle = device->handle;\n\tint no_aspm = 0;\n\tbool hotadd = system_state == SYSTEM_RUNNING;\n\tbool is_pcie;\n\n\troot = kzalloc(sizeof(struct acpi_pci_root), GFP_KERNEL);\n\tif (!root)\n\t\treturn -ENOMEM;\n\n\tsegment = 0;\n\tstatus = acpi_evaluate_integer(handle, METHOD_NAME__SEG, NULL,\n\t\t\t\t       &segment);\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tdev_err(&device->dev,  \"can't evaluate _SEG\\n\");\n\t\tresult = -ENODEV;\n\t\tgoto end;\n\t}\n\n\t/* Check _CRS first, then _BBN.  If no _BBN, default to zero. */\n\troot->secondary.flags = IORESOURCE_BUS;\n\tstatus = try_get_root_bridge_busnr(handle, &root->secondary);\n\tif (ACPI_FAILURE(status)) {\n\t\t/*\n\t\t * We need both the start and end of the downstream bus range\n\t\t * to interpret _CBA (MMCONFIG base address), so it really is\n\t\t * supposed to be in _CRS.  If we don't find it there, all we\n\t\t * can do is assume [_BBN-0xFF] or [0-0xFF].\n\t\t */\n\t\troot->secondary.end = 0xFF;\n\t\tdev_warn(&device->dev,\n\t\t\t FW_BUG \"no secondary bus range in _CRS\\n\");\n\t\tstatus = acpi_evaluate_integer(handle, METHOD_NAME__BBN,\n\t\t\t\t\t       NULL, &bus);\n\t\tif (ACPI_SUCCESS(status))\n\t\t\troot->secondary.start = bus;\n\t\telse if (status == AE_NOT_FOUND)\n\t\t\troot->secondary.start = 0;\n\t\telse {\n\t\t\tdev_err(&device->dev, \"can't evaluate _BBN\\n\");\n\t\t\tresult = -ENODEV;\n\t\t\tgoto end;\n\t\t}\n\t}\n\n\troot->device = device;\n\troot->segment = segment & 0xFFFF;\n\tstrcpy(acpi_device_name(device), ACPI_PCI_ROOT_DEVICE_NAME);\n\tstrcpy(acpi_device_class(device), ACPI_PCI_ROOT_CLASS);\n\tdevice->driver_data = root;\n\n\tif (hotadd && dmar_device_add(handle)) {\n\t\tresult = -ENXIO;\n\t\tgoto end;\n\t}\n\n\tpr_info(PREFIX \"%s [%s] (domain %04x %pR)\\n\",\n\t       acpi_device_name(device), acpi_device_bid(device),\n\t       root->segment, &root->secondary);\n\n\troot->mcfg_addr = acpi_pci_root_get_mcfg_addr(handle);\n\n\tis_pcie = strcmp(acpi_device_hid(device), \"PNP0A08\") == 0;\n\tnegotiate_os_control(root, &no_aspm, is_pcie);\n\n\t/*\n\t * TBD: Need PCI interface for enumeration/configuration of roots.\n\t */\n\n\t/*\n\t * Scan the Root Bridge\n\t * --------------------\n\t * Must do this prior to any attempt to bind the root device, as the\n\t * PCI namespace does not get created until this call is made (and\n\t * thus the root bridge's pci_dev does not exist).\n\t */\n\troot->bus = pci_acpi_scan_root(root);\n\tif (!root->bus) {\n\t\tdev_err(&device->dev,\n\t\t\t\"Bus %04x:%02x not present in PCI namespace\\n\",\n\t\t\troot->segment, (unsigned int)root->secondary.start);\n\t\tdevice->driver_data = NULL;\n\t\tresult = -ENODEV;\n\t\tgoto remove_dmar;\n\t}\n\n\tif (no_aspm)\n\t\tpcie_no_aspm();\n\n\tpci_acpi_add_bus_pm_notifier(device);\n\tdevice_set_wakeup_capable(root->bus->bridge, device->wakeup.flags.valid);\n\n\tif (hotadd) {\n\t\tpcibios_resource_survey_bus(root->bus);\n\t\tpci_assign_unassigned_root_bus_resources(root->bus);\n\t\t/*\n\t\t * This is only called for the hotadd case. For the boot-time\n\t\t * case, we need to wait until after PCI initialization in\n\t\t * order to deal with IOAPICs mapped in on a PCI BAR.\n\t\t *\n\t\t * This is currently x86-specific, because acpi_ioapic_add()\n\t\t * is an empty function without CONFIG_ACPI_HOTPLUG_IOAPIC.\n\t\t * And CONFIG_ACPI_HOTPLUG_IOAPIC depends on CONFIG_X86_IO_APIC\n\t\t * (see drivers/acpi/Kconfig).\n\t\t */\n\t\tacpi_ioapic_add(root->device->handle);\n\t}\n\n\tpci_lock_rescan_remove();\n\tpci_bus_add_devices(root->bus);\n\tpci_unlock_rescan_remove();\n\treturn 1;\n\nremove_dmar:\n\tif (hotadd)\n\t\tdmar_device_remove(handle);\nend:\n\tkfree(root);\n\treturn result;\n}\n\nstatic void acpi_pci_root_remove(struct acpi_device *device)\n{\n\tstruct acpi_pci_root *root = acpi_driver_data(device);\n\n\tpci_lock_rescan_remove();\n\n\tpci_stop_root_bus(root->bus);\n\n\tpci_ioapic_remove(root);\n\tdevice_set_wakeup_capable(root->bus->bridge, false);\n\tpci_acpi_remove_bus_pm_notifier(device);\n\n\tpci_remove_root_bus(root->bus);\n\tWARN_ON(acpi_ioapic_remove(root));\n\n\tdmar_device_remove(device->handle);\n\n\tpci_unlock_rescan_remove();\n\n\tkfree(root);\n}\n\n/*\n * Following code to support acpi_pci_root_create() is copied from\n * arch/x86/pci/acpi.c and modified so it could be reused by x86, IA64\n * and ARM64.\n */\nstatic void acpi_pci_root_validate_resources(struct device *dev,\n\t\t\t\t\t     struct list_head *resources,\n\t\t\t\t\t     unsigned long type)\n{\n\tLIST_HEAD(list);\n\tstruct resource *res1, *res2, *root = NULL;\n\tstruct resource_entry *tmp, *entry, *entry2;\n\n\tBUG_ON((type & (IORESOURCE_MEM | IORESOURCE_IO)) == 0);\n\troot = (type & IORESOURCE_MEM) ? &iomem_resource : &ioport_resource;\n\n\tlist_splice_init(resources, &list);\n\tresource_list_for_each_entry_safe(entry, tmp, &list) {\n\t\tbool free = false;\n\t\tresource_size_t end;\n\n\t\tres1 = entry->res;\n\t\tif (!(res1->flags & type))\n\t\t\tgoto next;\n\n\t\t/* Exclude non-addressable range or non-addressable portion */\n\t\tend = min(res1->end, root->end);\n\t\tif (end <= res1->start) {\n\t\t\tdev_info(dev, \"host bridge window %pR (ignored, not CPU addressable)\\n\",\n\t\t\t\t res1);\n\t\t\tfree = true;\n\t\t\tgoto next;\n\t\t} else if (res1->end != end) {\n\t\t\tdev_info(dev, \"host bridge window %pR ([%#llx-%#llx] ignored, not CPU addressable)\\n\",\n\t\t\t\t res1, (unsigned long long)end + 1,\n\t\t\t\t (unsigned long long)res1->end);\n\t\t\tres1->end = end;\n\t\t}\n\n\t\tresource_list_for_each_entry(entry2, resources) {\n\t\t\tres2 = entry2->res;\n\t\t\tif (!(res2->flags & type))\n\t\t\t\tcontinue;\n\n\t\t\t/*\n\t\t\t * I don't like throwing away windows because then\n\t\t\t * our resources no longer match the ACPI _CRS, but\n\t\t\t * the kernel resource tree doesn't allow overlaps.\n\t\t\t */\n\t\t\tif (resource_union(res1, res2, res2)) {\n\t\t\t\tdev_info(dev, \"host bridge window expanded to %pR; %pR ignored\\n\",\n\t\t\t\t\t res2, res1);\n\t\t\t\tfree = true;\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t}\n\nnext:\n\t\tresource_list_del(entry);\n\t\tif (free)\n\t\t\tresource_list_free_entry(entry);\n\t\telse\n\t\t\tresource_list_add_tail(entry, resources);\n\t}\n}\n\nstatic void acpi_pci_root_remap_iospace(struct fwnode_handle *fwnode,\n\t\t\tstruct resource_entry *entry)\n{\n#ifdef PCI_IOBASE\n\tstruct resource *res = entry->res;\n\tresource_size_t cpu_addr = res->start;\n\tresource_size_t pci_addr = cpu_addr - entry->offset;\n\tresource_size_t length = resource_size(res);\n\tunsigned long port;\n\n\tif (pci_register_io_range(fwnode, cpu_addr, length))\n\t\tgoto err;\n\n\tport = pci_address_to_pio(cpu_addr);\n\tif (port == (unsigned long)-1)\n\t\tgoto err;\n\n\tres->start = port;\n\tres->end = port + length - 1;\n\tentry->offset = port - pci_addr;\n\n\tif (pci_remap_iospace(res, cpu_addr) < 0)\n\t\tgoto err;\n\n\tpr_info(\"Remapped I/O %pa to %pR\\n\", &cpu_addr, res);\n\treturn;\nerr:\n\tres->flags |= IORESOURCE_DISABLED;\n#endif\n}\n\nint acpi_pci_probe_root_resources(struct acpi_pci_root_info *info)\n{\n\tint ret;\n\tstruct list_head *list = &info->resources;\n\tstruct acpi_device *device = info->bridge;\n\tstruct resource_entry *entry, *tmp;\n\tunsigned long flags;\n\n\tflags = IORESOURCE_IO | IORESOURCE_MEM | IORESOURCE_MEM_8AND16BIT;\n\tret = acpi_dev_get_resources(device, list,\n\t\t\t\t     acpi_dev_filter_resource_type_cb,\n\t\t\t\t     (void *)flags);\n\tif (ret < 0)\n\t\tdev_warn(&device->dev,\n\t\t\t \"failed to parse _CRS method, error code %d\\n\", ret);\n\telse if (ret == 0)\n\t\tdev_dbg(&device->dev,\n\t\t\t\"no IO and memory resources present in _CRS\\n\");\n\telse {\n\t\tresource_list_for_each_entry_safe(entry, tmp, list) {\n\t\t\tif (entry->res->flags & IORESOURCE_IO)\n\t\t\t\tacpi_pci_root_remap_iospace(&device->fwnode,\n\t\t\t\t\t\tentry);\n\n\t\t\tif (entry->res->flags & IORESOURCE_DISABLED)\n\t\t\t\tresource_list_destroy_entry(entry);\n\t\t\telse\n\t\t\t\tentry->res->name = info->name;\n\t\t}\n\t\tacpi_pci_root_validate_resources(&device->dev, list,\n\t\t\t\t\t\t IORESOURCE_MEM);\n\t\tacpi_pci_root_validate_resources(&device->dev, list,\n\t\t\t\t\t\t IORESOURCE_IO);\n\t}\n\n\treturn ret;\n}\n\nstatic void pci_acpi_root_add_resources(struct acpi_pci_root_info *info)\n{\n\tstruct resource_entry *entry, *tmp;\n\tstruct resource *res, *conflict, *root = NULL;\n\n\tresource_list_for_each_entry_safe(entry, tmp, &info->resources) {\n\t\tres = entry->res;\n\t\tif (res->flags & IORESOURCE_MEM)\n\t\t\troot = &iomem_resource;\n\t\telse if (res->flags & IORESOURCE_IO)\n\t\t\troot = &ioport_resource;\n\t\telse\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Some legacy x86 host bridge drivers use iomem_resource and\n\t\t * ioport_resource as default resource pool, skip it.\n\t\t */\n\t\tif (res == root)\n\t\t\tcontinue;\n\n\t\tconflict = insert_resource_conflict(root, res);\n\t\tif (conflict) {\n\t\t\tdev_info(&info->bridge->dev,\n\t\t\t\t \"ignoring host bridge window %pR (conflicts with %s %pR)\\n\",\n\t\t\t\t res, conflict->name, conflict);\n\t\t\tresource_list_destroy_entry(entry);\n\t\t}\n\t}\n}\n\nstatic void __acpi_pci_root_release_info(struct acpi_pci_root_info *info)\n{\n\tstruct resource *res;\n\tstruct resource_entry *entry, *tmp;\n\n\tif (!info)\n\t\treturn;\n\n\tresource_list_for_each_entry_safe(entry, tmp, &info->resources) {\n\t\tres = entry->res;\n\t\tif (res->parent &&\n\t\t    (res->flags & (IORESOURCE_MEM | IORESOURCE_IO)))\n\t\t\trelease_resource(res);\n\t\tresource_list_destroy_entry(entry);\n\t}\n\n\tinfo->ops->release_info(info);\n}\n\nstatic void acpi_pci_root_release_info(struct pci_host_bridge *bridge)\n{\n\tstruct resource *res;\n\tstruct resource_entry *entry;\n\n\tresource_list_for_each_entry(entry, &bridge->windows) {\n\t\tres = entry->res;\n\t\tif (res->flags & IORESOURCE_IO)\n\t\t\tpci_unmap_iospace(res);\n\t\tif (res->parent &&\n\t\t    (res->flags & (IORESOURCE_MEM | IORESOURCE_IO)))\n\t\t\trelease_resource(res);\n\t}\n\t__acpi_pci_root_release_info(bridge->release_data);\n}\n\nstruct pci_bus *acpi_pci_root_create(struct acpi_pci_root *root,\n\t\t\t\t     struct acpi_pci_root_ops *ops,\n\t\t\t\t     struct acpi_pci_root_info *info,\n\t\t\t\t     void *sysdata)\n{\n\tint ret, busnum = root->secondary.start;\n\tstruct acpi_device *device = root->device;\n\tint node = acpi_get_node(device->handle);\n\tstruct pci_bus *bus;\n\tstruct pci_host_bridge *host_bridge;\n\tunion acpi_object *obj;\n\n\tinfo->root = root;\n\tinfo->bridge = device;\n\tinfo->ops = ops;\n\tINIT_LIST_HEAD(&info->resources);\n\tsnprintf(info->name, sizeof(info->name), \"PCI Bus %04x:%02x\",\n\t\t root->segment, busnum);\n\n\tif (ops->init_info && ops->init_info(info))\n\t\tgoto out_release_info;\n\tif (ops->prepare_resources)\n\t\tret = ops->prepare_resources(info);\n\telse\n\t\tret = acpi_pci_probe_root_resources(info);\n\tif (ret < 0)\n\t\tgoto out_release_info;\n\n\tpci_acpi_root_add_resources(info);\n\tpci_add_resource(&info->resources, &root->secondary);\n\tbus = pci_create_root_bus(NULL, busnum, ops->pci_ops,\n\t\t\t\t  sysdata, &info->resources);\n\tif (!bus)\n\t\tgoto out_release_info;\n\n\thost_bridge = to_pci_host_bridge(bus->bridge);\n\tif (!(root->osc_control_set & OSC_PCI_EXPRESS_NATIVE_HP_CONTROL))\n\t\thost_bridge->native_pcie_hotplug = 0;\n\tif (!(root->osc_control_set & OSC_PCI_SHPC_NATIVE_HP_CONTROL))\n\t\thost_bridge->native_shpc_hotplug = 0;\n\tif (!(root->osc_control_set & OSC_PCI_EXPRESS_AER_CONTROL))\n\t\thost_bridge->native_aer = 0;\n\tif (!(root->osc_control_set & OSC_PCI_EXPRESS_PME_CONTROL))\n\t\thost_bridge->native_pme = 0;\n\tif (!(root->osc_control_set & OSC_PCI_EXPRESS_LTR_CONTROL))\n\t\thost_bridge->native_ltr = 0;\n\tif (!(root->osc_control_set & OSC_PCI_EXPRESS_DPC_CONTROL))\n\t\thost_bridge->native_dpc = 0;\n\n\t/*\n\t * Evaluate the \"PCI Boot Configuration\" _DSM Function.  If it\n\t * exists and returns 0, we must preserve any PCI resource\n\t * assignments made by firmware for this host bridge.\n\t */\n\tobj = acpi_evaluate_dsm(ACPI_HANDLE(bus->bridge), &pci_acpi_dsm_guid, 1,\n\t\t\t\tDSM_PCI_PRESERVE_BOOT_CONFIG, NULL);\n\tif (obj && obj->type == ACPI_TYPE_INTEGER && obj->integer.value == 0)\n\t\thost_bridge->preserve_config = 1;\n\tACPI_FREE(obj);\n\n\tpci_scan_child_bus(bus);\n\tpci_set_host_bridge_release(host_bridge, acpi_pci_root_release_info,\n\t\t\t\t    info);\n\tif (node != NUMA_NO_NODE)\n\t\tdev_printk(KERN_DEBUG, &bus->dev, \"on NUMA node %d\\n\", node);\n\treturn bus;\n\nout_release_info:\n\t__acpi_pci_root_release_info(info);\n\treturn NULL;\n}\n\nvoid __init acpi_pci_root_init(void)\n{\n\tacpi_hest_init();\n\tif (acpi_pci_disabled)\n\t\treturn;\n\n\tpci_acpi_crs_quirks();\n\tacpi_scan_add_handler_with_hotplug(&pci_root_handler, \"pci_root\");\n}\n"}, "15": {"id": 15, "path": "/src/include/linux/resource_ext.h", "content": "/* SPDX-License-Identifier: GPL-2.0-only */\n/*\n * Copyright (C) 2015, Intel Corporation\n * Author: Jiang Liu <jiang.liu@linux.intel.com>\n */\n#ifndef _LINUX_RESOURCE_EXT_H\n#define _LINUX_RESOURCE_EXT_H\n#include <linux/types.h>\n#include <linux/list.h>\n#include <linux/ioport.h>\n#include <linux/slab.h>\n\n/* Represent resource window for bridge devices */\nstruct resource_win {\n\tstruct resource res;\t\t/* In master (CPU) address space */\n\tresource_size_t offset;\t\t/* Translation offset for bridge */\n};\n\n/*\n * Common resource list management data structure and interfaces to support\n * ACPI, PNP and PCI host bridge etc.\n */\nstruct resource_entry {\n\tstruct list_head\tnode;\n\tstruct resource\t\t*res;\t/* In master (CPU) address space */\n\tresource_size_t\t\toffset;\t/* Translation offset for bridge */\n\tstruct resource\t\t__res;\t/* Default storage for res */\n};\n\nextern struct resource_entry *\nresource_list_create_entry(struct resource *res, size_t extra_size);\nextern void resource_list_free(struct list_head *head);\n\nstatic inline void resource_list_add(struct resource_entry *entry,\n\t\t\t\t     struct list_head *head)\n{\n\tlist_add(&entry->node, head);\n}\n\nstatic inline void resource_list_add_tail(struct resource_entry *entry,\n\t\t\t\t\t  struct list_head *head)\n{\n\tlist_add_tail(&entry->node, head);\n}\n\nstatic inline void resource_list_del(struct resource_entry *entry)\n{\n\tlist_del(&entry->node);\n}\n\nstatic inline void resource_list_free_entry(struct resource_entry *entry)\n{\n\tkfree(entry);\n}\n\nstatic inline void\nresource_list_destroy_entry(struct resource_entry *entry)\n{\n\tresource_list_del(entry);\n\tresource_list_free_entry(entry);\n}\n\n#define resource_list_for_each_entry(entry, list)\t\\\n\tlist_for_each_entry((entry), (list), node)\n\n#define resource_list_for_each_entry_safe(entry, tmp, list)\t\\\n\tlist_for_each_entry_safe((entry), (tmp), (list), node)\n\nstatic inline struct resource_entry *\nresource_list_first_type(struct list_head *list, unsigned long type)\n{\n\tstruct resource_entry *entry;\n\n\tresource_list_for_each_entry(entry, list) {\n\t\tif (resource_type(entry->res) == type)\n\t\t\treturn entry;\n\t}\n\treturn NULL;\n}\n\n#endif /* _LINUX_RESOURCE_EXT_H */\n"}, "16": {"id": 16, "path": "/src/net/ipv4/fib_semantics.c", "content": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tIPv4 Forwarding Information Base: semantics.\n *\n * Authors:\tAlexey Kuznetsov, <kuznet@ms2.inr.ac.ru>\n */\n\n#include <linux/uaccess.h>\n#include <linux/bitops.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/jiffies.h>\n#include <linux/mm.h>\n#include <linux/string.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/errno.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/inetdevice.h>\n#include <linux/netdevice.h>\n#include <linux/if_arp.h>\n#include <linux/proc_fs.h>\n#include <linux/skbuff.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <linux/netlink.h>\n\n#include <net/arp.h>\n#include <net/ip.h>\n#include <net/protocol.h>\n#include <net/route.h>\n#include <net/tcp.h>\n#include <net/sock.h>\n#include <net/ip_fib.h>\n#include <net/ip6_fib.h>\n#include <net/nexthop.h>\n#include <net/netlink.h>\n#include <net/rtnh.h>\n#include <net/lwtunnel.h>\n#include <net/fib_notifier.h>\n#include <net/addrconf.h>\n\n#include \"fib_lookup.h\"\n\nstatic DEFINE_SPINLOCK(fib_info_lock);\nstatic struct hlist_head *fib_info_hash;\nstatic struct hlist_head *fib_info_laddrhash;\nstatic unsigned int fib_info_hash_size;\nstatic unsigned int fib_info_cnt;\n\n#define DEVINDEX_HASHBITS 8\n#define DEVINDEX_HASHSIZE (1U << DEVINDEX_HASHBITS)\nstatic struct hlist_head fib_info_devhash[DEVINDEX_HASHSIZE];\n\n/* for_nexthops and change_nexthops only used when nexthop object\n * is not set in a fib_info. The logic within can reference fib_nh.\n */\n#ifdef CONFIG_IP_ROUTE_MULTIPATH\n\n#define for_nexthops(fi) {\t\t\t\t\t\t\\\n\tint nhsel; const struct fib_nh *nh;\t\t\t\t\\\n\tfor (nhsel = 0, nh = (fi)->fib_nh;\t\t\t\t\\\n\t     nhsel < fib_info_num_path((fi));\t\t\t\t\\\n\t     nh++, nhsel++)\n\n#define change_nexthops(fi) {\t\t\t\t\t\t\\\n\tint nhsel; struct fib_nh *nexthop_nh;\t\t\t\t\\\n\tfor (nhsel = 0,\tnexthop_nh = (struct fib_nh *)((fi)->fib_nh);\t\\\n\t     nhsel < fib_info_num_path((fi));\t\t\t\t\\\n\t     nexthop_nh++, nhsel++)\n\n#else /* CONFIG_IP_ROUTE_MULTIPATH */\n\n/* Hope, that gcc will optimize it to get rid of dummy loop */\n\n#define for_nexthops(fi) {\t\t\t\t\t\t\\\n\tint nhsel; const struct fib_nh *nh = (fi)->fib_nh;\t\t\\\n\tfor (nhsel = 0; nhsel < 1; nhsel++)\n\n#define change_nexthops(fi) {\t\t\t\t\t\t\\\n\tint nhsel;\t\t\t\t\t\t\t\\\n\tstruct fib_nh *nexthop_nh = (struct fib_nh *)((fi)->fib_nh);\t\\\n\tfor (nhsel = 0; nhsel < 1; nhsel++)\n\n#endif /* CONFIG_IP_ROUTE_MULTIPATH */\n\n#define endfor_nexthops(fi) }\n\n\nconst struct fib_prop fib_props[RTN_MAX + 1] = {\n\t[RTN_UNSPEC] = {\n\t\t.error\t= 0,\n\t\t.scope\t= RT_SCOPE_NOWHERE,\n\t},\n\t[RTN_UNICAST] = {\n\t\t.error\t= 0,\n\t\t.scope\t= RT_SCOPE_UNIVERSE,\n\t},\n\t[RTN_LOCAL] = {\n\t\t.error\t= 0,\n\t\t.scope\t= RT_SCOPE_HOST,\n\t},\n\t[RTN_BROADCAST] = {\n\t\t.error\t= 0,\n\t\t.scope\t= RT_SCOPE_LINK,\n\t},\n\t[RTN_ANYCAST] = {\n\t\t.error\t= 0,\n\t\t.scope\t= RT_SCOPE_LINK,\n\t},\n\t[RTN_MULTICAST] = {\n\t\t.error\t= 0,\n\t\t.scope\t= RT_SCOPE_UNIVERSE,\n\t},\n\t[RTN_BLACKHOLE] = {\n\t\t.error\t= -EINVAL,\n\t\t.scope\t= RT_SCOPE_UNIVERSE,\n\t},\n\t[RTN_UNREACHABLE] = {\n\t\t.error\t= -EHOSTUNREACH,\n\t\t.scope\t= RT_SCOPE_UNIVERSE,\n\t},\n\t[RTN_PROHIBIT] = {\n\t\t.error\t= -EACCES,\n\t\t.scope\t= RT_SCOPE_UNIVERSE,\n\t},\n\t[RTN_THROW] = {\n\t\t.error\t= -EAGAIN,\n\t\t.scope\t= RT_SCOPE_UNIVERSE,\n\t},\n\t[RTN_NAT] = {\n\t\t.error\t= -EINVAL,\n\t\t.scope\t= RT_SCOPE_NOWHERE,\n\t},\n\t[RTN_XRESOLVE] = {\n\t\t.error\t= -EINVAL,\n\t\t.scope\t= RT_SCOPE_NOWHERE,\n\t},\n};\n\nstatic void rt_fibinfo_free(struct rtable __rcu **rtp)\n{\n\tstruct rtable *rt = rcu_dereference_protected(*rtp, 1);\n\n\tif (!rt)\n\t\treturn;\n\n\t/* Not even needed : RCU_INIT_POINTER(*rtp, NULL);\n\t * because we waited an RCU grace period before calling\n\t * free_fib_info_rcu()\n\t */\n\n\tdst_dev_put(&rt->dst);\n\tdst_release_immediate(&rt->dst);\n}\n\nstatic void free_nh_exceptions(struct fib_nh_common *nhc)\n{\n\tstruct fnhe_hash_bucket *hash;\n\tint i;\n\n\thash = rcu_dereference_protected(nhc->nhc_exceptions, 1);\n\tif (!hash)\n\t\treturn;\n\tfor (i = 0; i < FNHE_HASH_SIZE; i++) {\n\t\tstruct fib_nh_exception *fnhe;\n\n\t\tfnhe = rcu_dereference_protected(hash[i].chain, 1);\n\t\twhile (fnhe) {\n\t\t\tstruct fib_nh_exception *next;\n\n\t\t\tnext = rcu_dereference_protected(fnhe->fnhe_next, 1);\n\n\t\t\trt_fibinfo_free(&fnhe->fnhe_rth_input);\n\t\t\trt_fibinfo_free(&fnhe->fnhe_rth_output);\n\n\t\t\tkfree(fnhe);\n\n\t\t\tfnhe = next;\n\t\t}\n\t}\n\tkfree(hash);\n}\n\nstatic void rt_fibinfo_free_cpus(struct rtable __rcu * __percpu *rtp)\n{\n\tint cpu;\n\n\tif (!rtp)\n\t\treturn;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct rtable *rt;\n\n\t\trt = rcu_dereference_protected(*per_cpu_ptr(rtp, cpu), 1);\n\t\tif (rt) {\n\t\t\tdst_dev_put(&rt->dst);\n\t\t\tdst_release_immediate(&rt->dst);\n\t\t}\n\t}\n\tfree_percpu(rtp);\n}\n\nvoid fib_nh_common_release(struct fib_nh_common *nhc)\n{\n\tif (nhc->nhc_dev)\n\t\tdev_put(nhc->nhc_dev);\n\n\tlwtstate_put(nhc->nhc_lwtstate);\n\trt_fibinfo_free_cpus(nhc->nhc_pcpu_rth_output);\n\trt_fibinfo_free(&nhc->nhc_rth_input);\n\tfree_nh_exceptions(nhc);\n}\nEXPORT_SYMBOL_GPL(fib_nh_common_release);\n\nvoid fib_nh_release(struct net *net, struct fib_nh *fib_nh)\n{\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\tif (fib_nh->nh_tclassid)\n\t\tnet->ipv4.fib_num_tclassid_users--;\n#endif\n\tfib_nh_common_release(&fib_nh->nh_common);\n}\n\n/* Release a nexthop info record */\nstatic void free_fib_info_rcu(struct rcu_head *head)\n{\n\tstruct fib_info *fi = container_of(head, struct fib_info, rcu);\n\n\tif (fi->nh) {\n\t\tnexthop_put(fi->nh);\n\t} else {\n\t\tchange_nexthops(fi) {\n\t\t\tfib_nh_release(fi->fib_net, nexthop_nh);\n\t\t} endfor_nexthops(fi);\n\t}\n\n\tip_fib_metrics_put(fi->fib_metrics);\n\n\tkfree(fi);\n}\n\nvoid free_fib_info(struct fib_info *fi)\n{\n\tif (fi->fib_dead == 0) {\n\t\tpr_warn(\"Freeing alive fib_info %p\\n\", fi);\n\t\treturn;\n\t}\n\tfib_info_cnt--;\n\n\tcall_rcu(&fi->rcu, free_fib_info_rcu);\n}\nEXPORT_SYMBOL_GPL(free_fib_info);\n\nvoid fib_release_info(struct fib_info *fi)\n{\n\tspin_lock_bh(&fib_info_lock);\n\tif (fi && --fi->fib_treeref == 0) {\n\t\thlist_del(&fi->fib_hash);\n\t\tif (fi->fib_prefsrc)\n\t\t\thlist_del(&fi->fib_lhash);\n\t\tif (fi->nh) {\n\t\t\tlist_del(&fi->nh_list);\n\t\t} else {\n\t\t\tchange_nexthops(fi) {\n\t\t\t\tif (!nexthop_nh->fib_nh_dev)\n\t\t\t\t\tcontinue;\n\t\t\t\thlist_del(&nexthop_nh->nh_hash);\n\t\t\t} endfor_nexthops(fi)\n\t\t}\n\t\tfi->fib_dead = 1;\n\t\tfib_info_put(fi);\n\t}\n\tspin_unlock_bh(&fib_info_lock);\n}\n\nstatic inline int nh_comp(struct fib_info *fi, struct fib_info *ofi)\n{\n\tconst struct fib_nh *onh;\n\n\tif (fi->nh || ofi->nh)\n\t\treturn nexthop_cmp(fi->nh, ofi->nh) ? 0 : -1;\n\n\tif (ofi->fib_nhs == 0)\n\t\treturn 0;\n\n\tfor_nexthops(fi) {\n\t\tonh = fib_info_nh(ofi, nhsel);\n\n\t\tif (nh->fib_nh_oif != onh->fib_nh_oif ||\n\t\t    nh->fib_nh_gw_family != onh->fib_nh_gw_family ||\n\t\t    nh->fib_nh_scope != onh->fib_nh_scope ||\n#ifdef CONFIG_IP_ROUTE_MULTIPATH\n\t\t    nh->fib_nh_weight != onh->fib_nh_weight ||\n#endif\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\t\t    nh->nh_tclassid != onh->nh_tclassid ||\n#endif\n\t\t    lwtunnel_cmp_encap(nh->fib_nh_lws, onh->fib_nh_lws) ||\n\t\t    ((nh->fib_nh_flags ^ onh->fib_nh_flags) & ~RTNH_COMPARE_MASK))\n\t\t\treturn -1;\n\n\t\tif (nh->fib_nh_gw_family == AF_INET &&\n\t\t    nh->fib_nh_gw4 != onh->fib_nh_gw4)\n\t\t\treturn -1;\n\n\t\tif (nh->fib_nh_gw_family == AF_INET6 &&\n\t\t    ipv6_addr_cmp(&nh->fib_nh_gw6, &onh->fib_nh_gw6))\n\t\t\treturn -1;\n\t} endfor_nexthops(fi);\n\treturn 0;\n}\n\nstatic inline unsigned int fib_devindex_hashfn(unsigned int val)\n{\n\tunsigned int mask = DEVINDEX_HASHSIZE - 1;\n\n\treturn (val ^\n\t\t(val >> DEVINDEX_HASHBITS) ^\n\t\t(val >> (DEVINDEX_HASHBITS * 2))) & mask;\n}\n\nstatic unsigned int fib_info_hashfn_1(int init_val, u8 protocol, u8 scope,\n\t\t\t\t      u32 prefsrc, u32 priority)\n{\n\tunsigned int val = init_val;\n\n\tval ^= (protocol << 8) | scope;\n\tval ^= prefsrc;\n\tval ^= priority;\n\n\treturn val;\n}\n\nstatic unsigned int fib_info_hashfn_result(unsigned int val)\n{\n\tunsigned int mask = (fib_info_hash_size - 1);\n\n\treturn (val ^ (val >> 7) ^ (val >> 12)) & mask;\n}\n\nstatic inline unsigned int fib_info_hashfn(struct fib_info *fi)\n{\n\tunsigned int val;\n\n\tval = fib_info_hashfn_1(fi->fib_nhs, fi->fib_protocol,\n\t\t\t\tfi->fib_scope, (__force u32)fi->fib_prefsrc,\n\t\t\t\tfi->fib_priority);\n\n\tif (fi->nh) {\n\t\tval ^= fib_devindex_hashfn(fi->nh->id);\n\t} else {\n\t\tfor_nexthops(fi) {\n\t\t\tval ^= fib_devindex_hashfn(nh->fib_nh_oif);\n\t\t} endfor_nexthops(fi)\n\t}\n\n\treturn fib_info_hashfn_result(val);\n}\n\n/* no metrics, only nexthop id */\nstatic struct fib_info *fib_find_info_nh(struct net *net,\n\t\t\t\t\t const struct fib_config *cfg)\n{\n\tstruct hlist_head *head;\n\tstruct fib_info *fi;\n\tunsigned int hash;\n\n\thash = fib_info_hashfn_1(fib_devindex_hashfn(cfg->fc_nh_id),\n\t\t\t\t cfg->fc_protocol, cfg->fc_scope,\n\t\t\t\t (__force u32)cfg->fc_prefsrc,\n\t\t\t\t cfg->fc_priority);\n\thash = fib_info_hashfn_result(hash);\n\thead = &fib_info_hash[hash];\n\n\thlist_for_each_entry(fi, head, fib_hash) {\n\t\tif (!net_eq(fi->fib_net, net))\n\t\t\tcontinue;\n\t\tif (!fi->nh || fi->nh->id != cfg->fc_nh_id)\n\t\t\tcontinue;\n\t\tif (cfg->fc_protocol == fi->fib_protocol &&\n\t\t    cfg->fc_scope == fi->fib_scope &&\n\t\t    cfg->fc_prefsrc == fi->fib_prefsrc &&\n\t\t    cfg->fc_priority == fi->fib_priority &&\n\t\t    cfg->fc_type == fi->fib_type &&\n\t\t    cfg->fc_table == fi->fib_tb_id &&\n\t\t    !((cfg->fc_flags ^ fi->fib_flags) & ~RTNH_COMPARE_MASK))\n\t\t\treturn fi;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct fib_info *fib_find_info(struct fib_info *nfi)\n{\n\tstruct hlist_head *head;\n\tstruct fib_info *fi;\n\tunsigned int hash;\n\n\thash = fib_info_hashfn(nfi);\n\thead = &fib_info_hash[hash];\n\n\thlist_for_each_entry(fi, head, fib_hash) {\n\t\tif (!net_eq(fi->fib_net, nfi->fib_net))\n\t\t\tcontinue;\n\t\tif (fi->fib_nhs != nfi->fib_nhs)\n\t\t\tcontinue;\n\t\tif (nfi->fib_protocol == fi->fib_protocol &&\n\t\t    nfi->fib_scope == fi->fib_scope &&\n\t\t    nfi->fib_prefsrc == fi->fib_prefsrc &&\n\t\t    nfi->fib_priority == fi->fib_priority &&\n\t\t    nfi->fib_type == fi->fib_type &&\n\t\t    memcmp(nfi->fib_metrics, fi->fib_metrics,\n\t\t\t   sizeof(u32) * RTAX_MAX) == 0 &&\n\t\t    !((nfi->fib_flags ^ fi->fib_flags) & ~RTNH_COMPARE_MASK) &&\n\t\t    nh_comp(fi, nfi) == 0)\n\t\t\treturn fi;\n\t}\n\n\treturn NULL;\n}\n\n/* Check, that the gateway is already configured.\n * Used only by redirect accept routine.\n */\nint ip_fib_check_default(__be32 gw, struct net_device *dev)\n{\n\tstruct hlist_head *head;\n\tstruct fib_nh *nh;\n\tunsigned int hash;\n\n\tspin_lock(&fib_info_lock);\n\n\thash = fib_devindex_hashfn(dev->ifindex);\n\thead = &fib_info_devhash[hash];\n\thlist_for_each_entry(nh, head, nh_hash) {\n\t\tif (nh->fib_nh_dev == dev &&\n\t\t    nh->fib_nh_gw4 == gw &&\n\t\t    !(nh->fib_nh_flags & RTNH_F_DEAD)) {\n\t\t\tspin_unlock(&fib_info_lock);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tspin_unlock(&fib_info_lock);\n\n\treturn -1;\n}\n\nstatic inline size_t fib_nlmsg_size(struct fib_info *fi)\n{\n\tsize_t payload = NLMSG_ALIGN(sizeof(struct rtmsg))\n\t\t\t + nla_total_size(4) /* RTA_TABLE */\n\t\t\t + nla_total_size(4) /* RTA_DST */\n\t\t\t + nla_total_size(4) /* RTA_PRIORITY */\n\t\t\t + nla_total_size(4) /* RTA_PREFSRC */\n\t\t\t + nla_total_size(TCP_CA_NAME_MAX); /* RTAX_CC_ALGO */\n\tunsigned int nhs = fib_info_num_path(fi);\n\n\t/* space for nested metrics */\n\tpayload += nla_total_size((RTAX_MAX * nla_total_size(4)));\n\n\tif (fi->nh)\n\t\tpayload += nla_total_size(4); /* RTA_NH_ID */\n\n\tif (nhs) {\n\t\tsize_t nh_encapsize = 0;\n\t\t/* Also handles the special case nhs == 1 */\n\n\t\t/* each nexthop is packed in an attribute */\n\t\tsize_t nhsize = nla_total_size(sizeof(struct rtnexthop));\n\t\tunsigned int i;\n\n\t\t/* may contain flow and gateway attribute */\n\t\tnhsize += 2 * nla_total_size(4);\n\n\t\t/* grab encap info */\n\t\tfor (i = 0; i < fib_info_num_path(fi); i++) {\n\t\t\tstruct fib_nh_common *nhc = fib_info_nhc(fi, i);\n\n\t\t\tif (nhc->nhc_lwtstate) {\n\t\t\t\t/* RTA_ENCAP_TYPE */\n\t\t\t\tnh_encapsize += lwtunnel_get_encap_size(\n\t\t\t\t\t\tnhc->nhc_lwtstate);\n\t\t\t\t/* RTA_ENCAP */\n\t\t\t\tnh_encapsize +=  nla_total_size(2);\n\t\t\t}\n\t\t}\n\n\t\t/* all nexthops are packed in a nested attribute */\n\t\tpayload += nla_total_size((nhs * nhsize) + nh_encapsize);\n\n\t}\n\n\treturn payload;\n}\n\nvoid rtmsg_fib(int event, __be32 key, struct fib_alias *fa,\n\t       int dst_len, u32 tb_id, const struct nl_info *info,\n\t       unsigned int nlm_flags)\n{\n\tstruct fib_rt_info fri;\n\tstruct sk_buff *skb;\n\tu32 seq = info->nlh ? info->nlh->nlmsg_seq : 0;\n\tint err = -ENOBUFS;\n\n\tskb = nlmsg_new(fib_nlmsg_size(fa->fa_info), GFP_KERNEL);\n\tif (!skb)\n\t\tgoto errout;\n\n\tfri.fi = fa->fa_info;\n\tfri.tb_id = tb_id;\n\tfri.dst = key;\n\tfri.dst_len = dst_len;\n\tfri.tos = fa->fa_tos;\n\tfri.type = fa->fa_type;\n\tfri.offload = fa->offload;\n\tfri.trap = fa->trap;\n\terr = fib_dump_info(skb, info->portid, seq, event, &fri, nlm_flags);\n\tif (err < 0) {\n\t\t/* -EMSGSIZE implies BUG in fib_nlmsg_size() */\n\t\tWARN_ON(err == -EMSGSIZE);\n\t\tkfree_skb(skb);\n\t\tgoto errout;\n\t}\n\trtnl_notify(skb, info->nl_net, info->portid, RTNLGRP_IPV4_ROUTE,\n\t\t    info->nlh, GFP_KERNEL);\n\treturn;\nerrout:\n\tif (err < 0)\n\t\trtnl_set_sk_err(info->nl_net, RTNLGRP_IPV4_ROUTE, err);\n}\n\nstatic int fib_detect_death(struct fib_info *fi, int order,\n\t\t\t    struct fib_info **last_resort, int *last_idx,\n\t\t\t    int dflt)\n{\n\tconst struct fib_nh_common *nhc = fib_info_nhc(fi, 0);\n\tstruct neighbour *n;\n\tint state = NUD_NONE;\n\n\tif (likely(nhc->nhc_gw_family == AF_INET))\n\t\tn = neigh_lookup(&arp_tbl, &nhc->nhc_gw.ipv4, nhc->nhc_dev);\n\telse if (nhc->nhc_gw_family == AF_INET6)\n\t\tn = neigh_lookup(ipv6_stub->nd_tbl, &nhc->nhc_gw.ipv6,\n\t\t\t\t nhc->nhc_dev);\n\telse\n\t\tn = NULL;\n\n\tif (n) {\n\t\tstate = n->nud_state;\n\t\tneigh_release(n);\n\t} else {\n\t\treturn 0;\n\t}\n\tif (state == NUD_REACHABLE)\n\t\treturn 0;\n\tif ((state & NUD_VALID) && order != dflt)\n\t\treturn 0;\n\tif ((state & NUD_VALID) ||\n\t    (*last_idx < 0 && order > dflt && state != NUD_INCOMPLETE)) {\n\t\t*last_resort = fi;\n\t\t*last_idx = order;\n\t}\n\treturn 1;\n}\n\nint fib_nh_common_init(struct net *net, struct fib_nh_common *nhc,\n\t\t       struct nlattr *encap, u16 encap_type,\n\t\t       void *cfg, gfp_t gfp_flags,\n\t\t       struct netlink_ext_ack *extack)\n{\n\tint err;\n\n\tnhc->nhc_pcpu_rth_output = alloc_percpu_gfp(struct rtable __rcu *,\n\t\t\t\t\t\t    gfp_flags);\n\tif (!nhc->nhc_pcpu_rth_output)\n\t\treturn -ENOMEM;\n\n\tif (encap) {\n\t\tstruct lwtunnel_state *lwtstate;\n\n\t\tif (encap_type == LWTUNNEL_ENCAP_NONE) {\n\t\t\tNL_SET_ERR_MSG(extack, \"LWT encap type not specified\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto lwt_failure;\n\t\t}\n\t\terr = lwtunnel_build_state(net, encap_type, encap,\n\t\t\t\t\t   nhc->nhc_family, cfg, &lwtstate,\n\t\t\t\t\t   extack);\n\t\tif (err)\n\t\t\tgoto lwt_failure;\n\n\t\tnhc->nhc_lwtstate = lwtstate_get(lwtstate);\n\t}\n\n\treturn 0;\n\nlwt_failure:\n\trt_fibinfo_free_cpus(nhc->nhc_pcpu_rth_output);\n\tnhc->nhc_pcpu_rth_output = NULL;\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(fib_nh_common_init);\n\nint fib_nh_init(struct net *net, struct fib_nh *nh,\n\t\tstruct fib_config *cfg, int nh_weight,\n\t\tstruct netlink_ext_ack *extack)\n{\n\tint err;\n\n\tnh->fib_nh_family = AF_INET;\n\n\terr = fib_nh_common_init(net, &nh->nh_common, cfg->fc_encap,\n\t\t\t\t cfg->fc_encap_type, cfg, GFP_KERNEL, extack);\n\tif (err)\n\t\treturn err;\n\n\tnh->fib_nh_oif = cfg->fc_oif;\n\tnh->fib_nh_gw_family = cfg->fc_gw_family;\n\tif (cfg->fc_gw_family == AF_INET)\n\t\tnh->fib_nh_gw4 = cfg->fc_gw4;\n\telse if (cfg->fc_gw_family == AF_INET6)\n\t\tnh->fib_nh_gw6 = cfg->fc_gw6;\n\n\tnh->fib_nh_flags = cfg->fc_flags;\n\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\tnh->nh_tclassid = cfg->fc_flow;\n\tif (nh->nh_tclassid)\n\t\tnet->ipv4.fib_num_tclassid_users++;\n#endif\n#ifdef CONFIG_IP_ROUTE_MULTIPATH\n\tnh->fib_nh_weight = nh_weight;\n#endif\n\treturn 0;\n}\n\n#ifdef CONFIG_IP_ROUTE_MULTIPATH\n\nstatic int fib_count_nexthops(struct rtnexthop *rtnh, int remaining,\n\t\t\t      struct netlink_ext_ack *extack)\n{\n\tint nhs = 0;\n\n\twhile (rtnh_ok(rtnh, remaining)) {\n\t\tnhs++;\n\t\trtnh = rtnh_next(rtnh, &remaining);\n\t}\n\n\t/* leftover implies invalid nexthop configuration, discard it */\n\tif (remaining > 0) {\n\t\tNL_SET_ERR_MSG(extack,\n\t\t\t       \"Invalid nexthop configuration - extra data after nexthops\");\n\t\tnhs = 0;\n\t}\n\n\treturn nhs;\n}\n\n/* only called when fib_nh is integrated into fib_info */\nstatic int fib_get_nhs(struct fib_info *fi, struct rtnexthop *rtnh,\n\t\t       int remaining, struct fib_config *cfg,\n\t\t       struct netlink_ext_ack *extack)\n{\n\tstruct net *net = fi->fib_net;\n\tstruct fib_config fib_cfg;\n\tstruct fib_nh *nh;\n\tint ret;\n\n\tchange_nexthops(fi) {\n\t\tint attrlen;\n\n\t\tmemset(&fib_cfg, 0, sizeof(fib_cfg));\n\n\t\tif (!rtnh_ok(rtnh, remaining)) {\n\t\t\tNL_SET_ERR_MSG(extack,\n\t\t\t\t       \"Invalid nexthop configuration - extra data after nexthop\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (rtnh->rtnh_flags & (RTNH_F_DEAD | RTNH_F_LINKDOWN)) {\n\t\t\tNL_SET_ERR_MSG(extack,\n\t\t\t\t       \"Invalid flags for nexthop - can not contain DEAD or LINKDOWN\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tfib_cfg.fc_flags = (cfg->fc_flags & ~0xFF) | rtnh->rtnh_flags;\n\t\tfib_cfg.fc_oif = rtnh->rtnh_ifindex;\n\n\t\tattrlen = rtnh_attrlen(rtnh);\n\t\tif (attrlen > 0) {\n\t\t\tstruct nlattr *nla, *nlav, *attrs = rtnh_attrs(rtnh);\n\n\t\t\tnla = nla_find(attrs, attrlen, RTA_GATEWAY);\n\t\t\tnlav = nla_find(attrs, attrlen, RTA_VIA);\n\t\t\tif (nla && nlav) {\n\t\t\t\tNL_SET_ERR_MSG(extack,\n\t\t\t\t\t       \"Nexthop configuration can not contain both GATEWAY and VIA\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (nla) {\n\t\t\t\tfib_cfg.fc_gw4 = nla_get_in_addr(nla);\n\t\t\t\tif (fib_cfg.fc_gw4)\n\t\t\t\t\tfib_cfg.fc_gw_family = AF_INET;\n\t\t\t} else if (nlav) {\n\t\t\t\tret = fib_gw_from_via(&fib_cfg, nlav, extack);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto errout;\n\t\t\t}\n\n\t\t\tnla = nla_find(attrs, attrlen, RTA_FLOW);\n\t\t\tif (nla)\n\t\t\t\tfib_cfg.fc_flow = nla_get_u32(nla);\n\n\t\t\tfib_cfg.fc_encap = nla_find(attrs, attrlen, RTA_ENCAP);\n\t\t\tnla = nla_find(attrs, attrlen, RTA_ENCAP_TYPE);\n\t\t\tif (nla)\n\t\t\t\tfib_cfg.fc_encap_type = nla_get_u16(nla);\n\t\t}\n\n\t\tret = fib_nh_init(net, nexthop_nh, &fib_cfg,\n\t\t\t\t  rtnh->rtnh_hops + 1, extack);\n\t\tif (ret)\n\t\t\tgoto errout;\n\n\t\trtnh = rtnh_next(rtnh, &remaining);\n\t} endfor_nexthops(fi);\n\n\tret = -EINVAL;\n\tnh = fib_info_nh(fi, 0);\n\tif (cfg->fc_oif && nh->fib_nh_oif != cfg->fc_oif) {\n\t\tNL_SET_ERR_MSG(extack,\n\t\t\t       \"Nexthop device index does not match RTA_OIF\");\n\t\tgoto errout;\n\t}\n\tif (cfg->fc_gw_family) {\n\t\tif (cfg->fc_gw_family != nh->fib_nh_gw_family ||\n\t\t    (cfg->fc_gw_family == AF_INET &&\n\t\t     nh->fib_nh_gw4 != cfg->fc_gw4) ||\n\t\t    (cfg->fc_gw_family == AF_INET6 &&\n\t\t     ipv6_addr_cmp(&nh->fib_nh_gw6, &cfg->fc_gw6))) {\n\t\t\tNL_SET_ERR_MSG(extack,\n\t\t\t\t       \"Nexthop gateway does not match RTA_GATEWAY or RTA_VIA\");\n\t\t\tgoto errout;\n\t\t}\n\t}\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\tif (cfg->fc_flow && nh->nh_tclassid != cfg->fc_flow) {\n\t\tNL_SET_ERR_MSG(extack,\n\t\t\t       \"Nexthop class id does not match RTA_FLOW\");\n\t\tgoto errout;\n\t}\n#endif\n\tret = 0;\nerrout:\n\treturn ret;\n}\n\n/* only called when fib_nh is integrated into fib_info */\nstatic void fib_rebalance(struct fib_info *fi)\n{\n\tint total;\n\tint w;\n\n\tif (fib_info_num_path(fi) < 2)\n\t\treturn;\n\n\ttotal = 0;\n\tfor_nexthops(fi) {\n\t\tif (nh->fib_nh_flags & RTNH_F_DEAD)\n\t\t\tcontinue;\n\n\t\tif (ip_ignore_linkdown(nh->fib_nh_dev) &&\n\t\t    nh->fib_nh_flags & RTNH_F_LINKDOWN)\n\t\t\tcontinue;\n\n\t\ttotal += nh->fib_nh_weight;\n\t} endfor_nexthops(fi);\n\n\tw = 0;\n\tchange_nexthops(fi) {\n\t\tint upper_bound;\n\n\t\tif (nexthop_nh->fib_nh_flags & RTNH_F_DEAD) {\n\t\t\tupper_bound = -1;\n\t\t} else if (ip_ignore_linkdown(nexthop_nh->fib_nh_dev) &&\n\t\t\t   nexthop_nh->fib_nh_flags & RTNH_F_LINKDOWN) {\n\t\t\tupper_bound = -1;\n\t\t} else {\n\t\t\tw += nexthop_nh->fib_nh_weight;\n\t\t\tupper_bound = DIV_ROUND_CLOSEST_ULL((u64)w << 31,\n\t\t\t\t\t\t\t    total) - 1;\n\t\t}\n\n\t\tatomic_set(&nexthop_nh->fib_nh_upper_bound, upper_bound);\n\t} endfor_nexthops(fi);\n}\n#else /* CONFIG_IP_ROUTE_MULTIPATH */\n\nstatic int fib_get_nhs(struct fib_info *fi, struct rtnexthop *rtnh,\n\t\t       int remaining, struct fib_config *cfg,\n\t\t       struct netlink_ext_ack *extack)\n{\n\tNL_SET_ERR_MSG(extack, \"Multipath support not enabled in kernel\");\n\n\treturn -EINVAL;\n}\n\n#define fib_rebalance(fi) do { } while (0)\n\n#endif /* CONFIG_IP_ROUTE_MULTIPATH */\n\nstatic int fib_encap_match(struct net *net, u16 encap_type,\n\t\t\t   struct nlattr *encap,\n\t\t\t   const struct fib_nh *nh,\n\t\t\t   const struct fib_config *cfg,\n\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct lwtunnel_state *lwtstate;\n\tint ret, result = 0;\n\n\tif (encap_type == LWTUNNEL_ENCAP_NONE)\n\t\treturn 0;\n\n\tret = lwtunnel_build_state(net, encap_type, encap, AF_INET,\n\t\t\t\t   cfg, &lwtstate, extack);\n\tif (!ret) {\n\t\tresult = lwtunnel_cmp_encap(lwtstate, nh->fib_nh_lws);\n\t\tlwtstate_free(lwtstate);\n\t}\n\n\treturn result;\n}\n\nint fib_nh_match(struct net *net, struct fib_config *cfg, struct fib_info *fi,\n\t\t struct netlink_ext_ack *extack)\n{\n#ifdef CONFIG_IP_ROUTE_MULTIPATH\n\tstruct rtnexthop *rtnh;\n\tint remaining;\n#endif\n\n\tif (cfg->fc_priority && cfg->fc_priority != fi->fib_priority)\n\t\treturn 1;\n\n\tif (cfg->fc_nh_id) {\n\t\tif (fi->nh && cfg->fc_nh_id == fi->nh->id)\n\t\t\treturn 0;\n\t\treturn 1;\n\t}\n\n\tif (cfg->fc_oif || cfg->fc_gw_family) {\n\t\tstruct fib_nh *nh = fib_info_nh(fi, 0);\n\n\t\tif (cfg->fc_encap) {\n\t\t\tif (fib_encap_match(net, cfg->fc_encap_type,\n\t\t\t\t\t    cfg->fc_encap, nh, cfg, extack))\n\t\t\t\treturn 1;\n\t\t}\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\t\tif (cfg->fc_flow &&\n\t\t    cfg->fc_flow != nh->nh_tclassid)\n\t\t\treturn 1;\n#endif\n\t\tif ((cfg->fc_oif && cfg->fc_oif != nh->fib_nh_oif) ||\n\t\t    (cfg->fc_gw_family &&\n\t\t     cfg->fc_gw_family != nh->fib_nh_gw_family))\n\t\t\treturn 1;\n\n\t\tif (cfg->fc_gw_family == AF_INET &&\n\t\t    cfg->fc_gw4 != nh->fib_nh_gw4)\n\t\t\treturn 1;\n\n\t\tif (cfg->fc_gw_family == AF_INET6 &&\n\t\t    ipv6_addr_cmp(&cfg->fc_gw6, &nh->fib_nh_gw6))\n\t\t\treturn 1;\n\n\t\treturn 0;\n\t}\n\n#ifdef CONFIG_IP_ROUTE_MULTIPATH\n\tif (!cfg->fc_mp)\n\t\treturn 0;\n\n\trtnh = cfg->fc_mp;\n\tremaining = cfg->fc_mp_len;\n\n\tfor_nexthops(fi) {\n\t\tint attrlen;\n\n\t\tif (!rtnh_ok(rtnh, remaining))\n\t\t\treturn -EINVAL;\n\n\t\tif (rtnh->rtnh_ifindex && rtnh->rtnh_ifindex != nh->fib_nh_oif)\n\t\t\treturn 1;\n\n\t\tattrlen = rtnh_attrlen(rtnh);\n\t\tif (attrlen > 0) {\n\t\t\tstruct nlattr *nla, *nlav, *attrs = rtnh_attrs(rtnh);\n\n\t\t\tnla = nla_find(attrs, attrlen, RTA_GATEWAY);\n\t\t\tnlav = nla_find(attrs, attrlen, RTA_VIA);\n\t\t\tif (nla && nlav) {\n\t\t\t\tNL_SET_ERR_MSG(extack,\n\t\t\t\t\t       \"Nexthop configuration can not contain both GATEWAY and VIA\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (nla) {\n\t\t\t\tif (nh->fib_nh_gw_family != AF_INET ||\n\t\t\t\t    nla_get_in_addr(nla) != nh->fib_nh_gw4)\n\t\t\t\t\treturn 1;\n\t\t\t} else if (nlav) {\n\t\t\t\tstruct fib_config cfg2;\n\t\t\t\tint err;\n\n\t\t\t\terr = fib_gw_from_via(&cfg2, nlav, extack);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tswitch (nh->fib_nh_gw_family) {\n\t\t\t\tcase AF_INET:\n\t\t\t\t\tif (cfg2.fc_gw_family != AF_INET ||\n\t\t\t\t\t    cfg2.fc_gw4 != nh->fib_nh_gw4)\n\t\t\t\t\t\treturn 1;\n\t\t\t\t\tbreak;\n\t\t\t\tcase AF_INET6:\n\t\t\t\t\tif (cfg2.fc_gw_family != AF_INET6 ||\n\t\t\t\t\t    ipv6_addr_cmp(&cfg2.fc_gw6,\n\t\t\t\t\t\t\t  &nh->fib_nh_gw6))\n\t\t\t\t\t\treturn 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\t\t\tnla = nla_find(attrs, attrlen, RTA_FLOW);\n\t\t\tif (nla && nla_get_u32(nla) != nh->nh_tclassid)\n\t\t\t\treturn 1;\n#endif\n\t\t}\n\n\t\trtnh = rtnh_next(rtnh, &remaining);\n\t} endfor_nexthops(fi);\n#endif\n\treturn 0;\n}\n\nbool fib_metrics_match(struct fib_config *cfg, struct fib_info *fi)\n{\n\tstruct nlattr *nla;\n\tint remaining;\n\n\tif (!cfg->fc_mx)\n\t\treturn true;\n\n\tnla_for_each_attr(nla, cfg->fc_mx, cfg->fc_mx_len, remaining) {\n\t\tint type = nla_type(nla);\n\t\tu32 fi_val, val;\n\n\t\tif (!type)\n\t\t\tcontinue;\n\t\tif (type > RTAX_MAX)\n\t\t\treturn false;\n\n\t\tif (type == RTAX_CC_ALGO) {\n\t\t\tchar tmp[TCP_CA_NAME_MAX];\n\t\t\tbool ecn_ca = false;\n\n\t\t\tnla_strscpy(tmp, nla, sizeof(tmp));\n\t\t\tval = tcp_ca_get_key_by_name(fi->fib_net, tmp, &ecn_ca);\n\t\t} else {\n\t\t\tif (nla_len(nla) != sizeof(u32))\n\t\t\t\treturn false;\n\t\t\tval = nla_get_u32(nla);\n\t\t}\n\n\t\tfi_val = fi->fib_metrics->metrics[type - 1];\n\t\tif (type == RTAX_FEATURES)\n\t\t\tfi_val &= ~DST_FEATURE_ECN_CA;\n\n\t\tif (fi_val != val)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int fib_check_nh_v6_gw(struct net *net, struct fib_nh *nh,\n\t\t\t      u32 table, struct netlink_ext_ack *extack)\n{\n\tstruct fib6_config cfg = {\n\t\t.fc_table = table,\n\t\t.fc_flags = nh->fib_nh_flags | RTF_GATEWAY,\n\t\t.fc_ifindex = nh->fib_nh_oif,\n\t\t.fc_gateway = nh->fib_nh_gw6,\n\t};\n\tstruct fib6_nh fib6_nh = {};\n\tint err;\n\n\terr = ipv6_stub->fib6_nh_init(net, &fib6_nh, &cfg, GFP_KERNEL, extack);\n\tif (!err) {\n\t\tnh->fib_nh_dev = fib6_nh.fib_nh_dev;\n\t\tdev_hold(nh->fib_nh_dev);\n\t\tnh->fib_nh_oif = nh->fib_nh_dev->ifindex;\n\t\tnh->fib_nh_scope = RT_SCOPE_LINK;\n\n\t\tipv6_stub->fib6_nh_release(&fib6_nh);\n\t}\n\n\treturn err;\n}\n\n/*\n * Picture\n * -------\n *\n * Semantics of nexthop is very messy by historical reasons.\n * We have to take into account, that:\n * a) gateway can be actually local interface address,\n *    so that gatewayed route is direct.\n * b) gateway must be on-link address, possibly\n *    described not by an ifaddr, but also by a direct route.\n * c) If both gateway and interface are specified, they should not\n *    contradict.\n * d) If we use tunnel routes, gateway could be not on-link.\n *\n * Attempt to reconcile all of these (alas, self-contradictory) conditions\n * results in pretty ugly and hairy code with obscure logic.\n *\n * I chose to generalized it instead, so that the size\n * of code does not increase practically, but it becomes\n * much more general.\n * Every prefix is assigned a \"scope\" value: \"host\" is local address,\n * \"link\" is direct route,\n * [ ... \"site\" ... \"interior\" ... ]\n * and \"universe\" is true gateway route with global meaning.\n *\n * Every prefix refers to a set of \"nexthop\"s (gw, oif),\n * where gw must have narrower scope. This recursion stops\n * when gw has LOCAL scope or if \"nexthop\" is declared ONLINK,\n * which means that gw is forced to be on link.\n *\n * Code is still hairy, but now it is apparently logically\n * consistent and very flexible. F.e. as by-product it allows\n * to co-exists in peace independent exterior and interior\n * routing processes.\n *\n * Normally it looks as following.\n *\n * {universe prefix}  -> (gw, oif) [scope link]\n *\t\t  |\n *\t\t  |-> {link prefix} -> (gw, oif) [scope local]\n *\t\t\t\t\t|\n *\t\t\t\t\t|-> {local prefix} (terminal node)\n */\nstatic int fib_check_nh_v4_gw(struct net *net, struct fib_nh *nh, u32 table,\n\t\t\t      u8 scope, struct netlink_ext_ack *extack)\n{\n\tstruct net_device *dev;\n\tstruct fib_result res;\n\tint err = 0;\n\n\tif (nh->fib_nh_flags & RTNH_F_ONLINK) {\n\t\tunsigned int addr_type;\n\n\t\tif (scope >= RT_SCOPE_LINK) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Nexthop has invalid scope\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tdev = __dev_get_by_index(net, nh->fib_nh_oif);\n\t\tif (!dev) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Nexthop device required for onlink\");\n\t\t\treturn -ENODEV;\n\t\t}\n\t\tif (!(dev->flags & IFF_UP)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Nexthop device is not up\");\n\t\t\treturn -ENETDOWN;\n\t\t}\n\t\taddr_type = inet_addr_type_dev_table(net, dev, nh->fib_nh_gw4);\n\t\tif (addr_type != RTN_UNICAST) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Nexthop has invalid gateway\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!netif_carrier_ok(dev))\n\t\t\tnh->fib_nh_flags |= RTNH_F_LINKDOWN;\n\t\tnh->fib_nh_dev = dev;\n\t\tdev_hold(dev);\n\t\tnh->fib_nh_scope = RT_SCOPE_LINK;\n\t\treturn 0;\n\t}\n\trcu_read_lock();\n\t{\n\t\tstruct fib_table *tbl = NULL;\n\t\tstruct flowi4 fl4 = {\n\t\t\t.daddr = nh->fib_nh_gw4,\n\t\t\t.flowi4_scope = scope + 1,\n\t\t\t.flowi4_oif = nh->fib_nh_oif,\n\t\t\t.flowi4_iif = LOOPBACK_IFINDEX,\n\t\t};\n\n\t\t/* It is not necessary, but requires a bit of thinking */\n\t\tif (fl4.flowi4_scope < RT_SCOPE_LINK)\n\t\t\tfl4.flowi4_scope = RT_SCOPE_LINK;\n\n\t\tif (table && table != RT_TABLE_MAIN)\n\t\t\ttbl = fib_get_table(net, table);\n\n\t\tif (tbl)\n\t\t\terr = fib_table_lookup(tbl, &fl4, &res,\n\t\t\t\t\t       FIB_LOOKUP_IGNORE_LINKSTATE |\n\t\t\t\t\t       FIB_LOOKUP_NOREF);\n\n\t\t/* on error or if no table given do full lookup. This\n\t\t * is needed for example when nexthops are in the local\n\t\t * table rather than the given table\n\t\t */\n\t\tif (!tbl || err) {\n\t\t\terr = fib_lookup(net, &fl4, &res,\n\t\t\t\t\t FIB_LOOKUP_IGNORE_LINKSTATE);\n\t\t}\n\n\t\tif (err) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Nexthop has invalid gateway\");\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\terr = -EINVAL;\n\tif (res.type != RTN_UNICAST && res.type != RTN_LOCAL) {\n\t\tNL_SET_ERR_MSG(extack, \"Nexthop has invalid gateway\");\n\t\tgoto out;\n\t}\n\tnh->fib_nh_scope = res.scope;\n\tnh->fib_nh_oif = FIB_RES_OIF(res);\n\tnh->fib_nh_dev = dev = FIB_RES_DEV(res);\n\tif (!dev) {\n\t\tNL_SET_ERR_MSG(extack,\n\t\t\t       \"No egress device for nexthop gateway\");\n\t\tgoto out;\n\t}\n\tdev_hold(dev);\n\tif (!netif_carrier_ok(dev))\n\t\tnh->fib_nh_flags |= RTNH_F_LINKDOWN;\n\terr = (dev->flags & IFF_UP) ? 0 : -ENETDOWN;\nout:\n\trcu_read_unlock();\n\treturn err;\n}\n\nstatic int fib_check_nh_nongw(struct net *net, struct fib_nh *nh,\n\t\t\t      struct netlink_ext_ack *extack)\n{\n\tstruct in_device *in_dev;\n\tint err;\n\n\tif (nh->fib_nh_flags & (RTNH_F_PERVASIVE | RTNH_F_ONLINK)) {\n\t\tNL_SET_ERR_MSG(extack,\n\t\t\t       \"Invalid flags for nexthop - PERVASIVE and ONLINK can not be set\");\n\t\treturn -EINVAL;\n\t}\n\n\trcu_read_lock();\n\n\terr = -ENODEV;\n\tin_dev = inetdev_by_index(net, nh->fib_nh_oif);\n\tif (!in_dev)\n\t\tgoto out;\n\terr = -ENETDOWN;\n\tif (!(in_dev->dev->flags & IFF_UP)) {\n\t\tNL_SET_ERR_MSG(extack, \"Device for nexthop is not up\");\n\t\tgoto out;\n\t}\n\n\tnh->fib_nh_dev = in_dev->dev;\n\tdev_hold(nh->fib_nh_dev);\n\tnh->fib_nh_scope = RT_SCOPE_HOST;\n\tif (!netif_carrier_ok(nh->fib_nh_dev))\n\t\tnh->fib_nh_flags |= RTNH_F_LINKDOWN;\n\terr = 0;\nout:\n\trcu_read_unlock();\n\treturn err;\n}\n\nint fib_check_nh(struct net *net, struct fib_nh *nh, u32 table, u8 scope,\n\t\t struct netlink_ext_ack *extack)\n{\n\tint err;\n\n\tif (nh->fib_nh_gw_family == AF_INET)\n\t\terr = fib_check_nh_v4_gw(net, nh, table, scope, extack);\n\telse if (nh->fib_nh_gw_family == AF_INET6)\n\t\terr = fib_check_nh_v6_gw(net, nh, table, extack);\n\telse\n\t\terr = fib_check_nh_nongw(net, nh, extack);\n\n\treturn err;\n}\n\nstatic inline unsigned int fib_laddr_hashfn(__be32 val)\n{\n\tunsigned int mask = (fib_info_hash_size - 1);\n\n\treturn ((__force u32)val ^\n\t\t((__force u32)val >> 7) ^\n\t\t((__force u32)val >> 14)) & mask;\n}\n\nstatic struct hlist_head *fib_info_hash_alloc(int bytes)\n{\n\tif (bytes <= PAGE_SIZE)\n\t\treturn kzalloc(bytes, GFP_KERNEL);\n\telse\n\t\treturn (struct hlist_head *)\n\t\t\t__get_free_pages(GFP_KERNEL | __GFP_ZERO,\n\t\t\t\t\t get_order(bytes));\n}\n\nstatic void fib_info_hash_free(struct hlist_head *hash, int bytes)\n{\n\tif (!hash)\n\t\treturn;\n\n\tif (bytes <= PAGE_SIZE)\n\t\tkfree(hash);\n\telse\n\t\tfree_pages((unsigned long) hash, get_order(bytes));\n}\n\nstatic void fib_info_hash_move(struct hlist_head *new_info_hash,\n\t\t\t       struct hlist_head *new_laddrhash,\n\t\t\t       unsigned int new_size)\n{\n\tstruct hlist_head *old_info_hash, *old_laddrhash;\n\tunsigned int old_size = fib_info_hash_size;\n\tunsigned int i, bytes;\n\n\tspin_lock_bh(&fib_info_lock);\n\told_info_hash = fib_info_hash;\n\told_laddrhash = fib_info_laddrhash;\n\tfib_info_hash_size = new_size;\n\n\tfor (i = 0; i < old_size; i++) {\n\t\tstruct hlist_head *head = &fib_info_hash[i];\n\t\tstruct hlist_node *n;\n\t\tstruct fib_info *fi;\n\n\t\thlist_for_each_entry_safe(fi, n, head, fib_hash) {\n\t\t\tstruct hlist_head *dest;\n\t\t\tunsigned int new_hash;\n\n\t\t\tnew_hash = fib_info_hashfn(fi);\n\t\t\tdest = &new_info_hash[new_hash];\n\t\t\thlist_add_head(&fi->fib_hash, dest);\n\t\t}\n\t}\n\tfib_info_hash = new_info_hash;\n\n\tfor (i = 0; i < old_size; i++) {\n\t\tstruct hlist_head *lhead = &fib_info_laddrhash[i];\n\t\tstruct hlist_node *n;\n\t\tstruct fib_info *fi;\n\n\t\thlist_for_each_entry_safe(fi, n, lhead, fib_lhash) {\n\t\t\tstruct hlist_head *ldest;\n\t\t\tunsigned int new_hash;\n\n\t\t\tnew_hash = fib_laddr_hashfn(fi->fib_prefsrc);\n\t\t\tldest = &new_laddrhash[new_hash];\n\t\t\thlist_add_head(&fi->fib_lhash, ldest);\n\t\t}\n\t}\n\tfib_info_laddrhash = new_laddrhash;\n\n\tspin_unlock_bh(&fib_info_lock);\n\n\tbytes = old_size * sizeof(struct hlist_head *);\n\tfib_info_hash_free(old_info_hash, bytes);\n\tfib_info_hash_free(old_laddrhash, bytes);\n}\n\n__be32 fib_info_update_nhc_saddr(struct net *net, struct fib_nh_common *nhc,\n\t\t\t\t unsigned char scope)\n{\n\tstruct fib_nh *nh;\n\n\tif (nhc->nhc_family != AF_INET)\n\t\treturn inet_select_addr(nhc->nhc_dev, 0, scope);\n\n\tnh = container_of(nhc, struct fib_nh, nh_common);\n\tnh->nh_saddr = inet_select_addr(nh->fib_nh_dev, nh->fib_nh_gw4, scope);\n\tnh->nh_saddr_genid = atomic_read(&net->ipv4.dev_addr_genid);\n\n\treturn nh->nh_saddr;\n}\n\n__be32 fib_result_prefsrc(struct net *net, struct fib_result *res)\n{\n\tstruct fib_nh_common *nhc = res->nhc;\n\n\tif (res->fi->fib_prefsrc)\n\t\treturn res->fi->fib_prefsrc;\n\n\tif (nhc->nhc_family == AF_INET) {\n\t\tstruct fib_nh *nh;\n\n\t\tnh = container_of(nhc, struct fib_nh, nh_common);\n\t\tif (nh->nh_saddr_genid == atomic_read(&net->ipv4.dev_addr_genid))\n\t\t\treturn nh->nh_saddr;\n\t}\n\n\treturn fib_info_update_nhc_saddr(net, nhc, res->fi->fib_scope);\n}\n\nstatic bool fib_valid_prefsrc(struct fib_config *cfg, __be32 fib_prefsrc)\n{\n\tif (cfg->fc_type != RTN_LOCAL || !cfg->fc_dst ||\n\t    fib_prefsrc != cfg->fc_dst) {\n\t\tu32 tb_id = cfg->fc_table;\n\t\tint rc;\n\n\t\tif (tb_id == RT_TABLE_MAIN)\n\t\t\ttb_id = RT_TABLE_LOCAL;\n\n\t\trc = inet_addr_type_table(cfg->fc_nlinfo.nl_net,\n\t\t\t\t\t  fib_prefsrc, tb_id);\n\n\t\tif (rc != RTN_LOCAL && tb_id != RT_TABLE_LOCAL) {\n\t\t\trc = inet_addr_type_table(cfg->fc_nlinfo.nl_net,\n\t\t\t\t\t\t  fib_prefsrc, RT_TABLE_LOCAL);\n\t\t}\n\n\t\tif (rc != RTN_LOCAL)\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nstruct fib_info *fib_create_info(struct fib_config *cfg,\n\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tint err;\n\tstruct fib_info *fi = NULL;\n\tstruct nexthop *nh = NULL;\n\tstruct fib_info *ofi;\n\tint nhs = 1;\n\tstruct net *net = cfg->fc_nlinfo.nl_net;\n\n\tif (cfg->fc_type > RTN_MAX)\n\t\tgoto err_inval;\n\n\t/* Fast check to catch the most weird cases */\n\tif (fib_props[cfg->fc_type].scope > cfg->fc_scope) {\n\t\tNL_SET_ERR_MSG(extack, \"Invalid scope\");\n\t\tgoto err_inval;\n\t}\n\n\tif (cfg->fc_flags & (RTNH_F_DEAD | RTNH_F_LINKDOWN)) {\n\t\tNL_SET_ERR_MSG(extack,\n\t\t\t       \"Invalid rtm_flags - can not contain DEAD or LINKDOWN\");\n\t\tgoto err_inval;\n\t}\n\n\tif (cfg->fc_nh_id) {\n\t\tif (!cfg->fc_mx) {\n\t\t\tfi = fib_find_info_nh(net, cfg);\n\t\t\tif (fi) {\n\t\t\t\tfi->fib_treeref++;\n\t\t\t\treturn fi;\n\t\t\t}\n\t\t}\n\n\t\tnh = nexthop_find_by_id(net, cfg->fc_nh_id);\n\t\tif (!nh) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Nexthop id does not exist\");\n\t\t\tgoto err_inval;\n\t\t}\n\t\tnhs = 0;\n\t}\n\n#ifdef CONFIG_IP_ROUTE_MULTIPATH\n\tif (cfg->fc_mp) {\n\t\tnhs = fib_count_nexthops(cfg->fc_mp, cfg->fc_mp_len, extack);\n\t\tif (nhs == 0)\n\t\t\tgoto err_inval;\n\t}\n#endif\n\n\terr = -ENOBUFS;\n\tif (fib_info_cnt >= fib_info_hash_size) {\n\t\tunsigned int new_size = fib_info_hash_size << 1;\n\t\tstruct hlist_head *new_info_hash;\n\t\tstruct hlist_head *new_laddrhash;\n\t\tunsigned int bytes;\n\n\t\tif (!new_size)\n\t\t\tnew_size = 16;\n\t\tbytes = new_size * sizeof(struct hlist_head *);\n\t\tnew_info_hash = fib_info_hash_alloc(bytes);\n\t\tnew_laddrhash = fib_info_hash_alloc(bytes);\n\t\tif (!new_info_hash || !new_laddrhash) {\n\t\t\tfib_info_hash_free(new_info_hash, bytes);\n\t\t\tfib_info_hash_free(new_laddrhash, bytes);\n\t\t} else\n\t\t\tfib_info_hash_move(new_info_hash, new_laddrhash, new_size);\n\n\t\tif (!fib_info_hash_size)\n\t\t\tgoto failure;\n\t}\n\n\tfi = kzalloc(struct_size(fi, fib_nh, nhs), GFP_KERNEL);\n\tif (!fi)\n\t\tgoto failure;\n\tfi->fib_metrics = ip_fib_metrics_init(fi->fib_net, cfg->fc_mx,\n\t\t\t\t\t      cfg->fc_mx_len, extack);\n\tif (IS_ERR(fi->fib_metrics)) {\n\t\terr = PTR_ERR(fi->fib_metrics);\n\t\tkfree(fi);\n\t\treturn ERR_PTR(err);\n\t}\n\n\tfib_info_cnt++;\n\tfi->fib_net = net;\n\tfi->fib_protocol = cfg->fc_protocol;\n\tfi->fib_scope = cfg->fc_scope;\n\tfi->fib_flags = cfg->fc_flags;\n\tfi->fib_priority = cfg->fc_priority;\n\tfi->fib_prefsrc = cfg->fc_prefsrc;\n\tfi->fib_type = cfg->fc_type;\n\tfi->fib_tb_id = cfg->fc_table;\n\n\tfi->fib_nhs = nhs;\n\tif (nh) {\n\t\tif (!nexthop_get(nh)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Nexthop has been deleted\");\n\t\t\terr = -EINVAL;\n\t\t} else {\n\t\t\terr = 0;\n\t\t\tfi->nh = nh;\n\t\t}\n\t} else {\n\t\tchange_nexthops(fi) {\n\t\t\tnexthop_nh->nh_parent = fi;\n\t\t} endfor_nexthops(fi)\n\n\t\tif (cfg->fc_mp)\n\t\t\terr = fib_get_nhs(fi, cfg->fc_mp, cfg->fc_mp_len, cfg,\n\t\t\t\t\t  extack);\n\t\telse\n\t\t\terr = fib_nh_init(net, fi->fib_nh, cfg, 1, extack);\n\t}\n\n\tif (err != 0)\n\t\tgoto failure;\n\n\tif (fib_props[cfg->fc_type].error) {\n\t\tif (cfg->fc_gw_family || cfg->fc_oif || cfg->fc_mp) {\n\t\t\tNL_SET_ERR_MSG(extack,\n\t\t\t\t       \"Gateway, device and multipath can not be specified for this route type\");\n\t\t\tgoto err_inval;\n\t\t}\n\t\tgoto link_it;\n\t} else {\n\t\tswitch (cfg->fc_type) {\n\t\tcase RTN_UNICAST:\n\t\tcase RTN_LOCAL:\n\t\tcase RTN_BROADCAST:\n\t\tcase RTN_ANYCAST:\n\t\tcase RTN_MULTICAST:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tNL_SET_ERR_MSG(extack, \"Invalid route type\");\n\t\t\tgoto err_inval;\n\t\t}\n\t}\n\n\tif (cfg->fc_scope > RT_SCOPE_HOST) {\n\t\tNL_SET_ERR_MSG(extack, \"Invalid scope\");\n\t\tgoto err_inval;\n\t}\n\n\tif (fi->nh) {\n\t\terr = fib_check_nexthop(fi->nh, cfg->fc_scope, extack);\n\t\tif (err)\n\t\t\tgoto failure;\n\t} else if (cfg->fc_scope == RT_SCOPE_HOST) {\n\t\tstruct fib_nh *nh = fi->fib_nh;\n\n\t\t/* Local address is added. */\n\t\tif (nhs != 1) {\n\t\t\tNL_SET_ERR_MSG(extack,\n\t\t\t\t       \"Route with host scope can not have multiple nexthops\");\n\t\t\tgoto err_inval;\n\t\t}\n\t\tif (nh->fib_nh_gw_family) {\n\t\t\tNL_SET_ERR_MSG(extack,\n\t\t\t\t       \"Route with host scope can not have a gateway\");\n\t\t\tgoto err_inval;\n\t\t}\n\t\tnh->fib_nh_scope = RT_SCOPE_NOWHERE;\n\t\tnh->fib_nh_dev = dev_get_by_index(net, nh->fib_nh_oif);\n\t\terr = -ENODEV;\n\t\tif (!nh->fib_nh_dev)\n\t\t\tgoto failure;\n\t} else {\n\t\tint linkdown = 0;\n\n\t\tchange_nexthops(fi) {\n\t\t\terr = fib_check_nh(cfg->fc_nlinfo.nl_net, nexthop_nh,\n\t\t\t\t\t   cfg->fc_table, cfg->fc_scope,\n\t\t\t\t\t   extack);\n\t\t\tif (err != 0)\n\t\t\t\tgoto failure;\n\t\t\tif (nexthop_nh->fib_nh_flags & RTNH_F_LINKDOWN)\n\t\t\t\tlinkdown++;\n\t\t} endfor_nexthops(fi)\n\t\tif (linkdown == fi->fib_nhs)\n\t\t\tfi->fib_flags |= RTNH_F_LINKDOWN;\n\t}\n\n\tif (fi->fib_prefsrc && !fib_valid_prefsrc(cfg, fi->fib_prefsrc)) {\n\t\tNL_SET_ERR_MSG(extack, \"Invalid prefsrc address\");\n\t\tgoto err_inval;\n\t}\n\n\tif (!fi->nh) {\n\t\tchange_nexthops(fi) {\n\t\t\tfib_info_update_nhc_saddr(net, &nexthop_nh->nh_common,\n\t\t\t\t\t\t  fi->fib_scope);\n\t\t\tif (nexthop_nh->fib_nh_gw_family == AF_INET6)\n\t\t\t\tfi->fib_nh_is_v6 = true;\n\t\t} endfor_nexthops(fi)\n\n\t\tfib_rebalance(fi);\n\t}\n\nlink_it:\n\tofi = fib_find_info(fi);\n\tif (ofi) {\n\t\tfi->fib_dead = 1;\n\t\tfree_fib_info(fi);\n\t\tofi->fib_treeref++;\n\t\treturn ofi;\n\t}\n\n\tfi->fib_treeref++;\n\trefcount_set(&fi->fib_clntref, 1);\n\tspin_lock_bh(&fib_info_lock);\n\thlist_add_head(&fi->fib_hash,\n\t\t       &fib_info_hash[fib_info_hashfn(fi)]);\n\tif (fi->fib_prefsrc) {\n\t\tstruct hlist_head *head;\n\n\t\thead = &fib_info_laddrhash[fib_laddr_hashfn(fi->fib_prefsrc)];\n\t\thlist_add_head(&fi->fib_lhash, head);\n\t}\n\tif (fi->nh) {\n\t\tlist_add(&fi->nh_list, &nh->fi_list);\n\t} else {\n\t\tchange_nexthops(fi) {\n\t\t\tstruct hlist_head *head;\n\t\t\tunsigned int hash;\n\n\t\t\tif (!nexthop_nh->fib_nh_dev)\n\t\t\t\tcontinue;\n\t\t\thash = fib_devindex_hashfn(nexthop_nh->fib_nh_dev->ifindex);\n\t\t\thead = &fib_info_devhash[hash];\n\t\t\thlist_add_head(&nexthop_nh->nh_hash, head);\n\t\t} endfor_nexthops(fi)\n\t}\n\tspin_unlock_bh(&fib_info_lock);\n\treturn fi;\n\nerr_inval:\n\terr = -EINVAL;\n\nfailure:\n\tif (fi) {\n\t\tfi->fib_dead = 1;\n\t\tfree_fib_info(fi);\n\t}\n\n\treturn ERR_PTR(err);\n}\n\nint fib_nexthop_info(struct sk_buff *skb, const struct fib_nh_common *nhc,\n\t\t     u8 rt_family, unsigned char *flags, bool skip_oif)\n{\n\tif (nhc->nhc_flags & RTNH_F_DEAD)\n\t\t*flags |= RTNH_F_DEAD;\n\n\tif (nhc->nhc_flags & RTNH_F_LINKDOWN) {\n\t\t*flags |= RTNH_F_LINKDOWN;\n\n\t\trcu_read_lock();\n\t\tswitch (nhc->nhc_family) {\n\t\tcase AF_INET:\n\t\t\tif (ip_ignore_linkdown(nhc->nhc_dev))\n\t\t\t\t*flags |= RTNH_F_DEAD;\n\t\t\tbreak;\n\t\tcase AF_INET6:\n\t\t\tif (ip6_ignore_linkdown(nhc->nhc_dev))\n\t\t\t\t*flags |= RTNH_F_DEAD;\n\t\t\tbreak;\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tswitch (nhc->nhc_gw_family) {\n\tcase AF_INET:\n\t\tif (nla_put_in_addr(skb, RTA_GATEWAY, nhc->nhc_gw.ipv4))\n\t\t\tgoto nla_put_failure;\n\t\tbreak;\n\tcase AF_INET6:\n\t\t/* if gateway family does not match nexthop family\n\t\t * gateway is encoded as RTA_VIA\n\t\t */\n\t\tif (rt_family != nhc->nhc_gw_family) {\n\t\t\tint alen = sizeof(struct in6_addr);\n\t\t\tstruct nlattr *nla;\n\t\t\tstruct rtvia *via;\n\n\t\t\tnla = nla_reserve(skb, RTA_VIA, alen + 2);\n\t\t\tif (!nla)\n\t\t\t\tgoto nla_put_failure;\n\n\t\t\tvia = nla_data(nla);\n\t\t\tvia->rtvia_family = AF_INET6;\n\t\t\tmemcpy(via->rtvia_addr, &nhc->nhc_gw.ipv6, alen);\n\t\t} else if (nla_put_in6_addr(skb, RTA_GATEWAY,\n\t\t\t\t\t    &nhc->nhc_gw.ipv6) < 0) {\n\t\t\tgoto nla_put_failure;\n\t\t}\n\t\tbreak;\n\t}\n\n\t*flags |= (nhc->nhc_flags &\n\t\t   (RTNH_F_ONLINK | RTNH_F_OFFLOAD | RTNH_F_TRAP));\n\n\tif (!skip_oif && nhc->nhc_dev &&\n\t    nla_put_u32(skb, RTA_OIF, nhc->nhc_dev->ifindex))\n\t\tgoto nla_put_failure;\n\n\tif (nhc->nhc_lwtstate &&\n\t    lwtunnel_fill_encap(skb, nhc->nhc_lwtstate,\n\t\t\t\tRTA_ENCAP, RTA_ENCAP_TYPE) < 0)\n\t\tgoto nla_put_failure;\n\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}\nEXPORT_SYMBOL_GPL(fib_nexthop_info);\n\n#if IS_ENABLED(CONFIG_IP_ROUTE_MULTIPATH) || IS_ENABLED(CONFIG_IPV6)\nint fib_add_nexthop(struct sk_buff *skb, const struct fib_nh_common *nhc,\n\t\t    int nh_weight, u8 rt_family)\n{\n\tconst struct net_device *dev = nhc->nhc_dev;\n\tstruct rtnexthop *rtnh;\n\tunsigned char flags = 0;\n\n\trtnh = nla_reserve_nohdr(skb, sizeof(*rtnh));\n\tif (!rtnh)\n\t\tgoto nla_put_failure;\n\n\trtnh->rtnh_hops = nh_weight - 1;\n\trtnh->rtnh_ifindex = dev ? dev->ifindex : 0;\n\n\tif (fib_nexthop_info(skb, nhc, rt_family, &flags, true) < 0)\n\t\tgoto nla_put_failure;\n\n\trtnh->rtnh_flags = flags;\n\n\t/* length of rtnetlink header + attributes */\n\trtnh->rtnh_len = nlmsg_get_pos(skb) - (void *)rtnh;\n\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}\nEXPORT_SYMBOL_GPL(fib_add_nexthop);\n#endif\n\n#ifdef CONFIG_IP_ROUTE_MULTIPATH\nstatic int fib_add_multipath(struct sk_buff *skb, struct fib_info *fi)\n{\n\tstruct nlattr *mp;\n\n\tmp = nla_nest_start_noflag(skb, RTA_MULTIPATH);\n\tif (!mp)\n\t\tgoto nla_put_failure;\n\n\tif (unlikely(fi->nh)) {\n\t\tif (nexthop_mpath_fill_node(skb, fi->nh, AF_INET) < 0)\n\t\t\tgoto nla_put_failure;\n\t\tgoto mp_end;\n\t}\n\n\tfor_nexthops(fi) {\n\t\tif (fib_add_nexthop(skb, &nh->nh_common, nh->fib_nh_weight,\n\t\t\t\t    AF_INET) < 0)\n\t\t\tgoto nla_put_failure;\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\t\tif (nh->nh_tclassid &&\n\t\t    nla_put_u32(skb, RTA_FLOW, nh->nh_tclassid))\n\t\t\tgoto nla_put_failure;\n#endif\n\t} endfor_nexthops(fi);\n\nmp_end:\n\tnla_nest_end(skb, mp);\n\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}\n#else\nstatic int fib_add_multipath(struct sk_buff *skb, struct fib_info *fi)\n{\n\treturn 0;\n}\n#endif\n\nint fib_dump_info(struct sk_buff *skb, u32 portid, u32 seq, int event,\n\t\t  struct fib_rt_info *fri, unsigned int flags)\n{\n\tunsigned int nhs = fib_info_num_path(fri->fi);\n\tstruct fib_info *fi = fri->fi;\n\tu32 tb_id = fri->tb_id;\n\tstruct nlmsghdr *nlh;\n\tstruct rtmsg *rtm;\n\n\tnlh = nlmsg_put(skb, portid, seq, event, sizeof(*rtm), flags);\n\tif (!nlh)\n\t\treturn -EMSGSIZE;\n\n\trtm = nlmsg_data(nlh);\n\trtm->rtm_family = AF_INET;\n\trtm->rtm_dst_len = fri->dst_len;\n\trtm->rtm_src_len = 0;\n\trtm->rtm_tos = fri->tos;\n\tif (tb_id < 256)\n\t\trtm->rtm_table = tb_id;\n\telse\n\t\trtm->rtm_table = RT_TABLE_COMPAT;\n\tif (nla_put_u32(skb, RTA_TABLE, tb_id))\n\t\tgoto nla_put_failure;\n\trtm->rtm_type = fri->type;\n\trtm->rtm_flags = fi->fib_flags;\n\trtm->rtm_scope = fi->fib_scope;\n\trtm->rtm_protocol = fi->fib_protocol;\n\n\tif (rtm->rtm_dst_len &&\n\t    nla_put_in_addr(skb, RTA_DST, fri->dst))\n\t\tgoto nla_put_failure;\n\tif (fi->fib_priority &&\n\t    nla_put_u32(skb, RTA_PRIORITY, fi->fib_priority))\n\t\tgoto nla_put_failure;\n\tif (rtnetlink_put_metrics(skb, fi->fib_metrics->metrics) < 0)\n\t\tgoto nla_put_failure;\n\n\tif (fi->fib_prefsrc &&\n\t    nla_put_in_addr(skb, RTA_PREFSRC, fi->fib_prefsrc))\n\t\tgoto nla_put_failure;\n\n\tif (fi->nh) {\n\t\tif (nla_put_u32(skb, RTA_NH_ID, fi->nh->id))\n\t\t\tgoto nla_put_failure;\n\t\tif (nexthop_is_blackhole(fi->nh))\n\t\t\trtm->rtm_type = RTN_BLACKHOLE;\n\t\tif (!fi->fib_net->ipv4.sysctl_nexthop_compat_mode)\n\t\t\tgoto offload;\n\t}\n\n\tif (nhs == 1) {\n\t\tconst struct fib_nh_common *nhc = fib_info_nhc(fi, 0);\n\t\tunsigned char flags = 0;\n\n\t\tif (fib_nexthop_info(skb, nhc, AF_INET, &flags, false) < 0)\n\t\t\tgoto nla_put_failure;\n\n\t\trtm->rtm_flags = flags;\n#ifdef CONFIG_IP_ROUTE_CLASSID\n\t\tif (nhc->nhc_family == AF_INET) {\n\t\t\tstruct fib_nh *nh;\n\n\t\t\tnh = container_of(nhc, struct fib_nh, nh_common);\n\t\t\tif (nh->nh_tclassid &&\n\t\t\t    nla_put_u32(skb, RTA_FLOW, nh->nh_tclassid))\n\t\t\t\tgoto nla_put_failure;\n\t\t}\n#endif\n\t} else {\n\t\tif (fib_add_multipath(skb, fi) < 0)\n\t\t\tgoto nla_put_failure;\n\t}\n\noffload:\n\tif (fri->offload)\n\t\trtm->rtm_flags |= RTM_F_OFFLOAD;\n\tif (fri->trap)\n\t\trtm->rtm_flags |= RTM_F_TRAP;\n\n\tnlmsg_end(skb, nlh);\n\treturn 0;\n\nnla_put_failure:\n\tnlmsg_cancel(skb, nlh);\n\treturn -EMSGSIZE;\n}\n\n/*\n * Update FIB if:\n * - local address disappeared -> we must delete all the entries\n *   referring to it.\n * - device went down -> we must shutdown all nexthops going via it.\n */\nint fib_sync_down_addr(struct net_device *dev, __be32 local)\n{\n\tint ret = 0;\n\tunsigned int hash = fib_laddr_hashfn(local);\n\tstruct hlist_head *head = &fib_info_laddrhash[hash];\n\tint tb_id = l3mdev_fib_table(dev) ? : RT_TABLE_MAIN;\n\tstruct net *net = dev_net(dev);\n\tstruct fib_info *fi;\n\n\tif (!fib_info_laddrhash || local == 0)\n\t\treturn 0;\n\n\thlist_for_each_entry(fi, head, fib_lhash) {\n\t\tif (!net_eq(fi->fib_net, net) ||\n\t\t    fi->fib_tb_id != tb_id)\n\t\t\tcontinue;\n\t\tif (fi->fib_prefsrc == local) {\n\t\t\tfi->fib_flags |= RTNH_F_DEAD;\n\t\t\tret++;\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic int call_fib_nh_notifiers(struct fib_nh *nh,\n\t\t\t\t enum fib_event_type event_type)\n{\n\tbool ignore_link_down = ip_ignore_linkdown(nh->fib_nh_dev);\n\tstruct fib_nh_notifier_info info = {\n\t\t.fib_nh = nh,\n\t};\n\n\tswitch (event_type) {\n\tcase FIB_EVENT_NH_ADD:\n\t\tif (nh->fib_nh_flags & RTNH_F_DEAD)\n\t\t\tbreak;\n\t\tif (ignore_link_down && nh->fib_nh_flags & RTNH_F_LINKDOWN)\n\t\t\tbreak;\n\t\treturn call_fib4_notifiers(dev_net(nh->fib_nh_dev), event_type,\n\t\t\t\t\t   &info.info);\n\tcase FIB_EVENT_NH_DEL:\n\t\tif ((ignore_link_down && nh->fib_nh_flags & RTNH_F_LINKDOWN) ||\n\t\t    (nh->fib_nh_flags & RTNH_F_DEAD))\n\t\t\treturn call_fib4_notifiers(dev_net(nh->fib_nh_dev),\n\t\t\t\t\t\t   event_type, &info.info);\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn NOTIFY_DONE;\n}\n\n/* Update the PMTU of exceptions when:\n * - the new MTU of the first hop becomes smaller than the PMTU\n * - the old MTU was the same as the PMTU, and it limited discovery of\n *   larger MTUs on the path. With that limit raised, we can now\n *   discover larger MTUs\n * A special case is locked exceptions, for which the PMTU is smaller\n * than the minimal accepted PMTU:\n * - if the new MTU is greater than the PMTU, don't make any change\n * - otherwise, unlock and set PMTU\n */\nvoid fib_nhc_update_mtu(struct fib_nh_common *nhc, u32 new, u32 orig)\n{\n\tstruct fnhe_hash_bucket *bucket;\n\tint i;\n\n\tbucket = rcu_dereference_protected(nhc->nhc_exceptions, 1);\n\tif (!bucket)\n\t\treturn;\n\n\tfor (i = 0; i < FNHE_HASH_SIZE; i++) {\n\t\tstruct fib_nh_exception *fnhe;\n\n\t\tfor (fnhe = rcu_dereference_protected(bucket[i].chain, 1);\n\t\t     fnhe;\n\t\t     fnhe = rcu_dereference_protected(fnhe->fnhe_next, 1)) {\n\t\t\tif (fnhe->fnhe_mtu_locked) {\n\t\t\t\tif (new <= fnhe->fnhe_pmtu) {\n\t\t\t\t\tfnhe->fnhe_pmtu = new;\n\t\t\t\t\tfnhe->fnhe_mtu_locked = false;\n\t\t\t\t}\n\t\t\t} else if (new < fnhe->fnhe_pmtu ||\n\t\t\t\t   orig == fnhe->fnhe_pmtu) {\n\t\t\t\tfnhe->fnhe_pmtu = new;\n\t\t\t}\n\t\t}\n\t}\n}\n\nvoid fib_sync_mtu(struct net_device *dev, u32 orig_mtu)\n{\n\tunsigned int hash = fib_devindex_hashfn(dev->ifindex);\n\tstruct hlist_head *head = &fib_info_devhash[hash];\n\tstruct fib_nh *nh;\n\n\thlist_for_each_entry(nh, head, nh_hash) {\n\t\tif (nh->fib_nh_dev == dev)\n\t\t\tfib_nhc_update_mtu(&nh->nh_common, dev->mtu, orig_mtu);\n\t}\n}\n\n/* Event              force Flags           Description\n * NETDEV_CHANGE      0     LINKDOWN        Carrier OFF, not for scope host\n * NETDEV_DOWN        0     LINKDOWN|DEAD   Link down, not for scope host\n * NETDEV_DOWN        1     LINKDOWN|DEAD   Last address removed\n * NETDEV_UNREGISTER  1     LINKDOWN|DEAD   Device removed\n *\n * only used when fib_nh is built into fib_info\n */\nint fib_sync_down_dev(struct net_device *dev, unsigned long event, bool force)\n{\n\tint ret = 0;\n\tint scope = RT_SCOPE_NOWHERE;\n\tstruct fib_info *prev_fi = NULL;\n\tunsigned int hash = fib_devindex_hashfn(dev->ifindex);\n\tstruct hlist_head *head = &fib_info_devhash[hash];\n\tstruct fib_nh *nh;\n\n\tif (force)\n\t\tscope = -1;\n\n\thlist_for_each_entry(nh, head, nh_hash) {\n\t\tstruct fib_info *fi = nh->nh_parent;\n\t\tint dead;\n\n\t\tBUG_ON(!fi->fib_nhs);\n\t\tif (nh->fib_nh_dev != dev || fi == prev_fi)\n\t\t\tcontinue;\n\t\tprev_fi = fi;\n\t\tdead = 0;\n\t\tchange_nexthops(fi) {\n\t\t\tif (nexthop_nh->fib_nh_flags & RTNH_F_DEAD)\n\t\t\t\tdead++;\n\t\t\telse if (nexthop_nh->fib_nh_dev == dev &&\n\t\t\t\t nexthop_nh->fib_nh_scope != scope) {\n\t\t\t\tswitch (event) {\n\t\t\t\tcase NETDEV_DOWN:\n\t\t\t\tcase NETDEV_UNREGISTER:\n\t\t\t\t\tnexthop_nh->fib_nh_flags |= RTNH_F_DEAD;\n\t\t\t\t\tfallthrough;\n\t\t\t\tcase NETDEV_CHANGE:\n\t\t\t\t\tnexthop_nh->fib_nh_flags |= RTNH_F_LINKDOWN;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tcall_fib_nh_notifiers(nexthop_nh,\n\t\t\t\t\t\t      FIB_EVENT_NH_DEL);\n\t\t\t\tdead++;\n\t\t\t}\n#ifdef CONFIG_IP_ROUTE_MULTIPATH\n\t\t\tif (event == NETDEV_UNREGISTER &&\n\t\t\t    nexthop_nh->fib_nh_dev == dev) {\n\t\t\t\tdead = fi->fib_nhs;\n\t\t\t\tbreak;\n\t\t\t}\n#endif\n\t\t} endfor_nexthops(fi)\n\t\tif (dead == fi->fib_nhs) {\n\t\t\tswitch (event) {\n\t\t\tcase NETDEV_DOWN:\n\t\t\tcase NETDEV_UNREGISTER:\n\t\t\t\tfi->fib_flags |= RTNH_F_DEAD;\n\t\t\t\tfallthrough;\n\t\t\tcase NETDEV_CHANGE:\n\t\t\t\tfi->fib_flags |= RTNH_F_LINKDOWN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tret++;\n\t\t}\n\n\t\tfib_rebalance(fi);\n\t}\n\n\treturn ret;\n}\n\n/* Must be invoked inside of an RCU protected region.  */\nstatic void fib_select_default(const struct flowi4 *flp, struct fib_result *res)\n{\n\tstruct fib_info *fi = NULL, *last_resort = NULL;\n\tstruct hlist_head *fa_head = res->fa_head;\n\tstruct fib_table *tb = res->table;\n\tu8 slen = 32 - res->prefixlen;\n\tint order = -1, last_idx = -1;\n\tstruct fib_alias *fa, *fa1 = NULL;\n\tu32 last_prio = res->fi->fib_priority;\n\tu8 last_tos = 0;\n\n\thlist_for_each_entry_rcu(fa, fa_head, fa_list) {\n\t\tstruct fib_info *next_fi = fa->fa_info;\n\t\tstruct fib_nh_common *nhc;\n\n\t\tif (fa->fa_slen != slen)\n\t\t\tcontinue;\n\t\tif (fa->fa_tos && fa->fa_tos != flp->flowi4_tos)\n\t\t\tcontinue;\n\t\tif (fa->tb_id != tb->tb_id)\n\t\t\tcontinue;\n\t\tif (next_fi->fib_priority > last_prio &&\n\t\t    fa->fa_tos == last_tos) {\n\t\t\tif (last_tos)\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\t}\n\t\tif (next_fi->fib_flags & RTNH_F_DEAD)\n\t\t\tcontinue;\n\t\tlast_tos = fa->fa_tos;\n\t\tlast_prio = next_fi->fib_priority;\n\n\t\tif (next_fi->fib_scope != res->scope ||\n\t\t    fa->fa_type != RTN_UNICAST)\n\t\t\tcontinue;\n\n\t\tnhc = fib_info_nhc(next_fi, 0);\n\t\tif (!nhc->nhc_gw_family || nhc->nhc_scope != RT_SCOPE_LINK)\n\t\t\tcontinue;\n\n\t\tfib_alias_accessed(fa);\n\n\t\tif (!fi) {\n\t\t\tif (next_fi != res->fi)\n\t\t\t\tbreak;\n\t\t\tfa1 = fa;\n\t\t} else if (!fib_detect_death(fi, order, &last_resort,\n\t\t\t\t\t     &last_idx, fa1->fa_default)) {\n\t\t\tfib_result_assign(res, fi);\n\t\t\tfa1->fa_default = order;\n\t\t\tgoto out;\n\t\t}\n\t\tfi = next_fi;\n\t\torder++;\n\t}\n\n\tif (order <= 0 || !fi) {\n\t\tif (fa1)\n\t\t\tfa1->fa_default = -1;\n\t\tgoto out;\n\t}\n\n\tif (!fib_detect_death(fi, order, &last_resort, &last_idx,\n\t\t\t      fa1->fa_default)) {\n\t\tfib_result_assign(res, fi);\n\t\tfa1->fa_default = order;\n\t\tgoto out;\n\t}\n\n\tif (last_idx >= 0)\n\t\tfib_result_assign(res, last_resort);\n\tfa1->fa_default = last_idx;\nout:\n\treturn;\n}\n\n/*\n * Dead device goes up. We wake up dead nexthops.\n * It takes sense only on multipath routes.\n *\n * only used when fib_nh is built into fib_info\n */\nint fib_sync_up(struct net_device *dev, unsigned char nh_flags)\n{\n\tstruct fib_info *prev_fi;\n\tunsigned int hash;\n\tstruct hlist_head *head;\n\tstruct fib_nh *nh;\n\tint ret;\n\n\tif (!(dev->flags & IFF_UP))\n\t\treturn 0;\n\n\tif (nh_flags & RTNH_F_DEAD) {\n\t\tunsigned int flags = dev_get_flags(dev);\n\n\t\tif (flags & (IFF_RUNNING | IFF_LOWER_UP))\n\t\t\tnh_flags |= RTNH_F_LINKDOWN;\n\t}\n\n\tprev_fi = NULL;\n\thash = fib_devindex_hashfn(dev->ifindex);\n\thead = &fib_info_devhash[hash];\n\tret = 0;\n\n\thlist_for_each_entry(nh, head, nh_hash) {\n\t\tstruct fib_info *fi = nh->nh_parent;\n\t\tint alive;\n\n\t\tBUG_ON(!fi->fib_nhs);\n\t\tif (nh->fib_nh_dev != dev || fi == prev_fi)\n\t\t\tcontinue;\n\n\t\tprev_fi = fi;\n\t\talive = 0;\n\t\tchange_nexthops(fi) {\n\t\t\tif (!(nexthop_nh->fib_nh_flags & nh_flags)) {\n\t\t\t\talive++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!nexthop_nh->fib_nh_dev ||\n\t\t\t    !(nexthop_nh->fib_nh_dev->flags & IFF_UP))\n\t\t\t\tcontinue;\n\t\t\tif (nexthop_nh->fib_nh_dev != dev ||\n\t\t\t    !__in_dev_get_rtnl(dev))\n\t\t\t\tcontinue;\n\t\t\talive++;\n\t\t\tnexthop_nh->fib_nh_flags &= ~nh_flags;\n\t\t\tcall_fib_nh_notifiers(nexthop_nh, FIB_EVENT_NH_ADD);\n\t\t} endfor_nexthops(fi)\n\n\t\tif (alive > 0) {\n\t\t\tfi->fib_flags &= ~nh_flags;\n\t\t\tret++;\n\t\t}\n\n\t\tfib_rebalance(fi);\n\t}\n\n\treturn ret;\n}\n\n#ifdef CONFIG_IP_ROUTE_MULTIPATH\nstatic bool fib_good_nh(const struct fib_nh *nh)\n{\n\tint state = NUD_REACHABLE;\n\n\tif (nh->fib_nh_scope == RT_SCOPE_LINK) {\n\t\tstruct neighbour *n;\n\n\t\trcu_read_lock_bh();\n\n\t\tif (likely(nh->fib_nh_gw_family == AF_INET))\n\t\t\tn = __ipv4_neigh_lookup_noref(nh->fib_nh_dev,\n\t\t\t\t\t\t   (__force u32)nh->fib_nh_gw4);\n\t\telse if (nh->fib_nh_gw_family == AF_INET6)\n\t\t\tn = __ipv6_neigh_lookup_noref_stub(nh->fib_nh_dev,\n\t\t\t\t\t\t\t   &nh->fib_nh_gw6);\n\t\telse\n\t\t\tn = NULL;\n\t\tif (n)\n\t\t\tstate = n->nud_state;\n\n\t\trcu_read_unlock_bh();\n\t}\n\n\treturn !!(state & NUD_VALID);\n}\n\nvoid fib_select_multipath(struct fib_result *res, int hash)\n{\n\tstruct fib_info *fi = res->fi;\n\tstruct net *net = fi->fib_net;\n\tbool first = false;\n\n\tif (unlikely(res->fi->nh)) {\n\t\tnexthop_path_fib_result(res, hash);\n\t\treturn;\n\t}\n\n\tchange_nexthops(fi) {\n\t\tif (net->ipv4.sysctl_fib_multipath_use_neigh) {\n\t\t\tif (!fib_good_nh(nexthop_nh))\n\t\t\t\tcontinue;\n\t\t\tif (!first) {\n\t\t\t\tres->nh_sel = nhsel;\n\t\t\t\tres->nhc = &nexthop_nh->nh_common;\n\t\t\t\tfirst = true;\n\t\t\t}\n\t\t}\n\n\t\tif (hash > atomic_read(&nexthop_nh->fib_nh_upper_bound))\n\t\t\tcontinue;\n\n\t\tres->nh_sel = nhsel;\n\t\tres->nhc = &nexthop_nh->nh_common;\n\t\treturn;\n\t} endfor_nexthops(fi);\n}\n#endif\n\nvoid fib_select_path(struct net *net, struct fib_result *res,\n\t\t     struct flowi4 *fl4, const struct sk_buff *skb)\n{\n\tif (fl4->flowi4_oif && !(fl4->flowi4_flags & FLOWI_FLAG_SKIP_NH_OIF))\n\t\tgoto check_saddr;\n\n#ifdef CONFIG_IP_ROUTE_MULTIPATH\n\tif (fib_info_num_path(res->fi) > 1) {\n\t\tint h = fib_multipath_hash(net, fl4, skb, NULL);\n\n\t\tfib_select_multipath(res, h);\n\t}\n\telse\n#endif\n\tif (!res->prefixlen &&\n\t    res->table->tb_num_default > 1 &&\n\t    res->type == RTN_UNICAST)\n\t\tfib_select_default(fl4, res);\n\ncheck_saddr:\n\tif (!fl4->saddr)\n\t\tfl4->saddr = fib_result_prefsrc(net, res);\n}\n"}, "17": {"id": 17, "path": "/src/include/linux/slab.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n/*\n * Written by Mark Hemment, 1996 (markhe@nextd.demon.co.uk).\n *\n * (C) SGI 2006, Christoph Lameter\n * \tCleaned up and restructured to ease the addition of alternative\n * \timplementations of SLAB allocators.\n * (C) Linux Foundation 2008-2013\n *      Unified interface for all slab allocators\n */\n\n#ifndef _LINUX_SLAB_H\n#define\t_LINUX_SLAB_H\n\n#include <linux/gfp.h>\n#include <linux/overflow.h>\n#include <linux/types.h>\n#include <linux/workqueue.h>\n#include <linux/percpu-refcount.h>\n\n\n/*\n * Flags to pass to kmem_cache_create().\n * The ones marked DEBUG are only valid if CONFIG_DEBUG_SLAB is set.\n */\n/* DEBUG: Perform (expensive) checks on alloc/free */\n#define SLAB_CONSISTENCY_CHECKS\t((slab_flags_t __force)0x00000100U)\n/* DEBUG: Red zone objs in a cache */\n#define SLAB_RED_ZONE\t\t((slab_flags_t __force)0x00000400U)\n/* DEBUG: Poison objects */\n#define SLAB_POISON\t\t((slab_flags_t __force)0x00000800U)\n/* Align objs on cache lines */\n#define SLAB_HWCACHE_ALIGN\t((slab_flags_t __force)0x00002000U)\n/* Use GFP_DMA memory */\n#define SLAB_CACHE_DMA\t\t((slab_flags_t __force)0x00004000U)\n/* Use GFP_DMA32 memory */\n#define SLAB_CACHE_DMA32\t((slab_flags_t __force)0x00008000U)\n/* DEBUG: Store the last owner for bug hunting */\n#define SLAB_STORE_USER\t\t((slab_flags_t __force)0x00010000U)\n/* Panic if kmem_cache_create() fails */\n#define SLAB_PANIC\t\t((slab_flags_t __force)0x00040000U)\n/*\n * SLAB_TYPESAFE_BY_RCU - **WARNING** READ THIS!\n *\n * This delays freeing the SLAB page by a grace period, it does _NOT_\n * delay object freeing. This means that if you do kmem_cache_free()\n * that memory location is free to be reused at any time. Thus it may\n * be possible to see another object there in the same RCU grace period.\n *\n * This feature only ensures the memory location backing the object\n * stays valid, the trick to using this is relying on an independent\n * object validation pass. Something like:\n *\n *  rcu_read_lock()\n * again:\n *  obj = lockless_lookup(key);\n *  if (obj) {\n *    if (!try_get_ref(obj)) // might fail for free objects\n *      goto again;\n *\n *    if (obj->key != key) { // not the object we expected\n *      put_ref(obj);\n *      goto again;\n *    }\n *  }\n *  rcu_read_unlock();\n *\n * This is useful if we need to approach a kernel structure obliquely,\n * from its address obtained without the usual locking. We can lock\n * the structure to stabilize it and check it's still at the given address,\n * only if we can be sure that the memory has not been meanwhile reused\n * for some other kind of object (which our subsystem's lock might corrupt).\n *\n * rcu_read_lock before reading the address, then rcu_read_unlock after\n * taking the spinlock within the structure expected at that address.\n *\n * Note that SLAB_TYPESAFE_BY_RCU was originally named SLAB_DESTROY_BY_RCU.\n */\n/* Defer freeing slabs to RCU */\n#define SLAB_TYPESAFE_BY_RCU\t((slab_flags_t __force)0x00080000U)\n/* Spread some memory over cpuset */\n#define SLAB_MEM_SPREAD\t\t((slab_flags_t __force)0x00100000U)\n/* Trace allocations and frees */\n#define SLAB_TRACE\t\t((slab_flags_t __force)0x00200000U)\n\n/* Flag to prevent checks on free */\n#ifdef CONFIG_DEBUG_OBJECTS\n# define SLAB_DEBUG_OBJECTS\t((slab_flags_t __force)0x00400000U)\n#else\n# define SLAB_DEBUG_OBJECTS\t0\n#endif\n\n/* Avoid kmemleak tracing */\n#define SLAB_NOLEAKTRACE\t((slab_flags_t __force)0x00800000U)\n\n/* Fault injection mark */\n#ifdef CONFIG_FAILSLAB\n# define SLAB_FAILSLAB\t\t((slab_flags_t __force)0x02000000U)\n#else\n# define SLAB_FAILSLAB\t\t0\n#endif\n/* Account to memcg */\n#ifdef CONFIG_MEMCG_KMEM\n# define SLAB_ACCOUNT\t\t((slab_flags_t __force)0x04000000U)\n#else\n# define SLAB_ACCOUNT\t\t0\n#endif\n\n#ifdef CONFIG_KASAN\n#define SLAB_KASAN\t\t((slab_flags_t __force)0x08000000U)\n#else\n#define SLAB_KASAN\t\t0\n#endif\n\n/* The following flags affect the page allocator grouping pages by mobility */\n/* Objects are reclaimable */\n#define SLAB_RECLAIM_ACCOUNT\t((slab_flags_t __force)0x00020000U)\n#define SLAB_TEMPORARY\t\tSLAB_RECLAIM_ACCOUNT\t/* Objects are short-lived */\n\n/* Slab deactivation flag */\n#define SLAB_DEACTIVATED\t((slab_flags_t __force)0x10000000U)\n\n/*\n * ZERO_SIZE_PTR will be returned for zero sized kmalloc requests.\n *\n * Dereferencing ZERO_SIZE_PTR will lead to a distinct access fault.\n *\n * ZERO_SIZE_PTR can be passed to kfree though in the same way that NULL can.\n * Both make kfree a no-op.\n */\n#define ZERO_SIZE_PTR ((void *)16)\n\n#define ZERO_OR_NULL_PTR(x) ((unsigned long)(x) <= \\\n\t\t\t\t(unsigned long)ZERO_SIZE_PTR)\n\n#include <linux/kasan.h>\n\nstruct mem_cgroup;\n/*\n * struct kmem_cache related prototypes\n */\nvoid __init kmem_cache_init(void);\nbool slab_is_available(void);\n\nextern bool usercopy_fallback;\n\nstruct kmem_cache *kmem_cache_create(const char *name, unsigned int size,\n\t\t\tunsigned int align, slab_flags_t flags,\n\t\t\tvoid (*ctor)(void *));\nstruct kmem_cache *kmem_cache_create_usercopy(const char *name,\n\t\t\tunsigned int size, unsigned int align,\n\t\t\tslab_flags_t flags,\n\t\t\tunsigned int useroffset, unsigned int usersize,\n\t\t\tvoid (*ctor)(void *));\nvoid kmem_cache_destroy(struct kmem_cache *);\nint kmem_cache_shrink(struct kmem_cache *);\n\n/*\n * Please use this macro to create slab caches. Simply specify the\n * name of the structure and maybe some flags that are listed above.\n *\n * The alignment of the struct determines object alignment. If you\n * f.e. add ____cacheline_aligned_in_smp to the struct declaration\n * then the objects will be properly aligned in SMP configurations.\n */\n#define KMEM_CACHE(__struct, __flags)\t\t\t\t\t\\\n\t\tkmem_cache_create(#__struct, sizeof(struct __struct),\t\\\n\t\t\t__alignof__(struct __struct), (__flags), NULL)\n\n/*\n * To whitelist a single field for copying to/from usercopy, use this\n * macro instead for KMEM_CACHE() above.\n */\n#define KMEM_CACHE_USERCOPY(__struct, __flags, __field)\t\t\t\\\n\t\tkmem_cache_create_usercopy(#__struct,\t\t\t\\\n\t\t\tsizeof(struct __struct),\t\t\t\\\n\t\t\t__alignof__(struct __struct), (__flags),\t\\\n\t\t\toffsetof(struct __struct, __field),\t\t\\\n\t\t\tsizeof_field(struct __struct, __field), NULL)\n\n/*\n * Common kmalloc functions provided by all allocators\n */\nvoid * __must_check krealloc(const void *, size_t, gfp_t);\nvoid kfree(const void *);\nvoid kfree_sensitive(const void *);\nsize_t __ksize(const void *);\nsize_t ksize(const void *);\n\n#ifdef CONFIG_HAVE_HARDENED_USERCOPY_ALLOCATOR\nvoid __check_heap_object(const void *ptr, unsigned long n, struct page *page,\n\t\t\tbool to_user);\n#else\nstatic inline void __check_heap_object(const void *ptr, unsigned long n,\n\t\t\t\t       struct page *page, bool to_user) { }\n#endif\n\n/*\n * Some archs want to perform DMA into kmalloc caches and need a guaranteed\n * alignment larger than the alignment of a 64-bit integer.\n * Setting ARCH_KMALLOC_MINALIGN in arch headers allows that.\n */\n#if defined(ARCH_DMA_MINALIGN) && ARCH_DMA_MINALIGN > 8\n#define ARCH_KMALLOC_MINALIGN ARCH_DMA_MINALIGN\n#define KMALLOC_MIN_SIZE ARCH_DMA_MINALIGN\n#define KMALLOC_SHIFT_LOW ilog2(ARCH_DMA_MINALIGN)\n#else\n#define ARCH_KMALLOC_MINALIGN __alignof__(unsigned long long)\n#endif\n\n/*\n * Setting ARCH_SLAB_MINALIGN in arch headers allows a different alignment.\n * Intended for arches that get misalignment faults even for 64 bit integer\n * aligned buffers.\n */\n#ifndef ARCH_SLAB_MINALIGN\n#define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)\n#endif\n\n/*\n * kmalloc and friends return ARCH_KMALLOC_MINALIGN aligned\n * pointers. kmem_cache_alloc and friends return ARCH_SLAB_MINALIGN\n * aligned pointers.\n */\n#define __assume_kmalloc_alignment __assume_aligned(ARCH_KMALLOC_MINALIGN)\n#define __assume_slab_alignment __assume_aligned(ARCH_SLAB_MINALIGN)\n#define __assume_page_alignment __assume_aligned(PAGE_SIZE)\n\n/*\n * Kmalloc array related definitions\n */\n\n#ifdef CONFIG_SLAB\n/*\n * The largest kmalloc size supported by the SLAB allocators is\n * 32 megabyte (2^25) or the maximum allocatable page order if that is\n * less than 32 MB.\n *\n * WARNING: Its not easy to increase this value since the allocators have\n * to do various tricks to work around compiler limitations in order to\n * ensure proper constant folding.\n */\n#define KMALLOC_SHIFT_HIGH\t((MAX_ORDER + PAGE_SHIFT - 1) <= 25 ? \\\n\t\t\t\t(MAX_ORDER + PAGE_SHIFT - 1) : 25)\n#define KMALLOC_SHIFT_MAX\tKMALLOC_SHIFT_HIGH\n#ifndef KMALLOC_SHIFT_LOW\n#define KMALLOC_SHIFT_LOW\t5\n#endif\n#endif\n\n#ifdef CONFIG_SLUB\n/*\n * SLUB directly allocates requests fitting in to an order-1 page\n * (PAGE_SIZE*2).  Larger requests are passed to the page allocator.\n */\n#define KMALLOC_SHIFT_HIGH\t(PAGE_SHIFT + 1)\n#define KMALLOC_SHIFT_MAX\t(MAX_ORDER + PAGE_SHIFT - 1)\n#ifndef KMALLOC_SHIFT_LOW\n#define KMALLOC_SHIFT_LOW\t3\n#endif\n#endif\n\n#ifdef CONFIG_SLOB\n/*\n * SLOB passes all requests larger than one page to the page allocator.\n * No kmalloc array is necessary since objects of different sizes can\n * be allocated from the same page.\n */\n#define KMALLOC_SHIFT_HIGH\tPAGE_SHIFT\n#define KMALLOC_SHIFT_MAX\t(MAX_ORDER + PAGE_SHIFT - 1)\n#ifndef KMALLOC_SHIFT_LOW\n#define KMALLOC_SHIFT_LOW\t3\n#endif\n#endif\n\n/* Maximum allocatable size */\n#define KMALLOC_MAX_SIZE\t(1UL << KMALLOC_SHIFT_MAX)\n/* Maximum size for which we actually use a slab cache */\n#define KMALLOC_MAX_CACHE_SIZE\t(1UL << KMALLOC_SHIFT_HIGH)\n/* Maximum order allocatable via the slab allocator */\n#define KMALLOC_MAX_ORDER\t(KMALLOC_SHIFT_MAX - PAGE_SHIFT)\n\n/*\n * Kmalloc subsystem.\n */\n#ifndef KMALLOC_MIN_SIZE\n#define KMALLOC_MIN_SIZE (1 << KMALLOC_SHIFT_LOW)\n#endif\n\n/*\n * This restriction comes from byte sized index implementation.\n * Page size is normally 2^12 bytes and, in this case, if we want to use\n * byte sized index which can represent 2^8 entries, the size of the object\n * should be equal or greater to 2^12 / 2^8 = 2^4 = 16.\n * If minimum size of kmalloc is less than 16, we use it as minimum object\n * size and give up to use byte sized index.\n */\n#define SLAB_OBJ_MIN_SIZE      (KMALLOC_MIN_SIZE < 16 ? \\\n                               (KMALLOC_MIN_SIZE) : 16)\n\n/*\n * Whenever changing this, take care of that kmalloc_type() and\n * create_kmalloc_caches() still work as intended.\n */\nenum kmalloc_cache_type {\n\tKMALLOC_NORMAL = 0,\n\tKMALLOC_RECLAIM,\n#ifdef CONFIG_ZONE_DMA\n\tKMALLOC_DMA,\n#endif\n\tNR_KMALLOC_TYPES\n};\n\n#ifndef CONFIG_SLOB\nextern struct kmem_cache *\nkmalloc_caches[NR_KMALLOC_TYPES][KMALLOC_SHIFT_HIGH + 1];\n\nstatic __always_inline enum kmalloc_cache_type kmalloc_type(gfp_t flags)\n{\n#ifdef CONFIG_ZONE_DMA\n\t/*\n\t * The most common case is KMALLOC_NORMAL, so test for it\n\t * with a single branch for both flags.\n\t */\n\tif (likely((flags & (__GFP_DMA | __GFP_RECLAIMABLE)) == 0))\n\t\treturn KMALLOC_NORMAL;\n\n\t/*\n\t * At least one of the flags has to be set. If both are, __GFP_DMA\n\t * is more important.\n\t */\n\treturn flags & __GFP_DMA ? KMALLOC_DMA : KMALLOC_RECLAIM;\n#else\n\treturn flags & __GFP_RECLAIMABLE ? KMALLOC_RECLAIM : KMALLOC_NORMAL;\n#endif\n}\n\n/*\n * Figure out which kmalloc slab an allocation of a certain size\n * belongs to.\n * 0 = zero alloc\n * 1 =  65 .. 96 bytes\n * 2 = 129 .. 192 bytes\n * n = 2^(n-1)+1 .. 2^n\n */\nstatic __always_inline unsigned int kmalloc_index(size_t size)\n{\n\tif (!size)\n\t\treturn 0;\n\n\tif (size <= KMALLOC_MIN_SIZE)\n\t\treturn KMALLOC_SHIFT_LOW;\n\n\tif (KMALLOC_MIN_SIZE <= 32 && size > 64 && size <= 96)\n\t\treturn 1;\n\tif (KMALLOC_MIN_SIZE <= 64 && size > 128 && size <= 192)\n\t\treturn 2;\n\tif (size <=          8) return 3;\n\tif (size <=         16) return 4;\n\tif (size <=         32) return 5;\n\tif (size <=         64) return 6;\n\tif (size <=        128) return 7;\n\tif (size <=        256) return 8;\n\tif (size <=        512) return 9;\n\tif (size <=       1024) return 10;\n\tif (size <=   2 * 1024) return 11;\n\tif (size <=   4 * 1024) return 12;\n\tif (size <=   8 * 1024) return 13;\n\tif (size <=  16 * 1024) return 14;\n\tif (size <=  32 * 1024) return 15;\n\tif (size <=  64 * 1024) return 16;\n\tif (size <= 128 * 1024) return 17;\n\tif (size <= 256 * 1024) return 18;\n\tif (size <= 512 * 1024) return 19;\n\tif (size <= 1024 * 1024) return 20;\n\tif (size <=  2 * 1024 * 1024) return 21;\n\tif (size <=  4 * 1024 * 1024) return 22;\n\tif (size <=  8 * 1024 * 1024) return 23;\n\tif (size <=  16 * 1024 * 1024) return 24;\n\tif (size <=  32 * 1024 * 1024) return 25;\n\tif (size <=  64 * 1024 * 1024) return 26;\n\tBUG();\n\n\t/* Will never be reached. Needed because the compiler may complain */\n\treturn -1;\n}\n#endif /* !CONFIG_SLOB */\n\nvoid *__kmalloc(size_t size, gfp_t flags) __assume_kmalloc_alignment __malloc;\nvoid *kmem_cache_alloc(struct kmem_cache *, gfp_t flags) __assume_slab_alignment __malloc;\nvoid kmem_cache_free(struct kmem_cache *, void *);\n\n/*\n * Bulk allocation and freeing operations. These are accelerated in an\n * allocator specific way to avoid taking locks repeatedly or building\n * metadata structures unnecessarily.\n *\n * Note that interrupts must be enabled when calling these functions.\n */\nvoid kmem_cache_free_bulk(struct kmem_cache *, size_t, void **);\nint kmem_cache_alloc_bulk(struct kmem_cache *, gfp_t, size_t, void **);\n\n/*\n * Caller must not use kfree_bulk() on memory not originally allocated\n * by kmalloc(), because the SLOB allocator cannot handle this.\n */\nstatic __always_inline void kfree_bulk(size_t size, void **p)\n{\n\tkmem_cache_free_bulk(NULL, size, p);\n}\n\n#ifdef CONFIG_NUMA\nvoid *__kmalloc_node(size_t size, gfp_t flags, int node) __assume_kmalloc_alignment __malloc;\nvoid *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node) __assume_slab_alignment __malloc;\n#else\nstatic __always_inline void *__kmalloc_node(size_t size, gfp_t flags, int node)\n{\n\treturn __kmalloc(size, flags);\n}\n\nstatic __always_inline void *kmem_cache_alloc_node(struct kmem_cache *s, gfp_t flags, int node)\n{\n\treturn kmem_cache_alloc(s, flags);\n}\n#endif\n\n#ifdef CONFIG_TRACING\nextern void *kmem_cache_alloc_trace(struct kmem_cache *, gfp_t, size_t) __assume_slab_alignment __malloc;\n\n#ifdef CONFIG_NUMA\nextern void *kmem_cache_alloc_node_trace(struct kmem_cache *s,\n\t\t\t\t\t   gfp_t gfpflags,\n\t\t\t\t\t   int node, size_t size) __assume_slab_alignment __malloc;\n#else\nstatic __always_inline void *\nkmem_cache_alloc_node_trace(struct kmem_cache *s,\n\t\t\t      gfp_t gfpflags,\n\t\t\t      int node, size_t size)\n{\n\treturn kmem_cache_alloc_trace(s, gfpflags, size);\n}\n#endif /* CONFIG_NUMA */\n\n#else /* CONFIG_TRACING */\nstatic __always_inline void *kmem_cache_alloc_trace(struct kmem_cache *s,\n\t\tgfp_t flags, size_t size)\n{\n\tvoid *ret = kmem_cache_alloc(s, flags);\n\n\tret = kasan_kmalloc(s, ret, size, flags);\n\treturn ret;\n}\n\nstatic __always_inline void *\nkmem_cache_alloc_node_trace(struct kmem_cache *s,\n\t\t\t      gfp_t gfpflags,\n\t\t\t      int node, size_t size)\n{\n\tvoid *ret = kmem_cache_alloc_node(s, gfpflags, node);\n\n\tret = kasan_kmalloc(s, ret, size, gfpflags);\n\treturn ret;\n}\n#endif /* CONFIG_TRACING */\n\nextern void *kmalloc_order(size_t size, gfp_t flags, unsigned int order) __assume_page_alignment __malloc;\n\n#ifdef CONFIG_TRACING\nextern void *kmalloc_order_trace(size_t size, gfp_t flags, unsigned int order) __assume_page_alignment __malloc;\n#else\nstatic __always_inline void *\nkmalloc_order_trace(size_t size, gfp_t flags, unsigned int order)\n{\n\treturn kmalloc_order(size, flags, order);\n}\n#endif\n\nstatic __always_inline void *kmalloc_large(size_t size, gfp_t flags)\n{\n\tunsigned int order = get_order(size);\n\treturn kmalloc_order_trace(size, flags, order);\n}\n\n/**\n * kmalloc - allocate memory\n * @size: how many bytes of memory are required.\n * @flags: the type of memory to allocate.\n *\n * kmalloc is the normal method of allocating memory\n * for objects smaller than page size in the kernel.\n *\n * The allocated object address is aligned to at least ARCH_KMALLOC_MINALIGN\n * bytes. For @size of power of two bytes, the alignment is also guaranteed\n * to be at least to the size.\n *\n * The @flags argument may be one of the GFP flags defined at\n * include/linux/gfp.h and described at\n * :ref:`Documentation/core-api/mm-api.rst <mm-api-gfp-flags>`\n *\n * The recommended usage of the @flags is described at\n * :ref:`Documentation/core-api/memory-allocation.rst <memory_allocation>`\n *\n * Below is a brief outline of the most useful GFP flags\n *\n * %GFP_KERNEL\n *\tAllocate normal kernel ram. May sleep.\n *\n * %GFP_NOWAIT\n *\tAllocation will not sleep.\n *\n * %GFP_ATOMIC\n *\tAllocation will not sleep.  May use emergency pools.\n *\n * %GFP_HIGHUSER\n *\tAllocate memory from high memory on behalf of user.\n *\n * Also it is possible to set different flags by OR'ing\n * in one or more of the following additional @flags:\n *\n * %__GFP_HIGH\n *\tThis allocation has high priority and may use emergency pools.\n *\n * %__GFP_NOFAIL\n *\tIndicate that this allocation is in no way allowed to fail\n *\t(think twice before using).\n *\n * %__GFP_NORETRY\n *\tIf memory is not immediately available,\n *\tthen give up at once.\n *\n * %__GFP_NOWARN\n *\tIf allocation fails, don't issue any warnings.\n *\n * %__GFP_RETRY_MAYFAIL\n *\tTry really hard to succeed the allocation but fail\n *\teventually.\n */\nstatic __always_inline void *kmalloc(size_t size, gfp_t flags)\n{\n\tif (__builtin_constant_p(size)) {\n#ifndef CONFIG_SLOB\n\t\tunsigned int index;\n#endif\n\t\tif (size > KMALLOC_MAX_CACHE_SIZE)\n\t\t\treturn kmalloc_large(size, flags);\n#ifndef CONFIG_SLOB\n\t\tindex = kmalloc_index(size);\n\n\t\tif (!index)\n\t\t\treturn ZERO_SIZE_PTR;\n\n\t\treturn kmem_cache_alloc_trace(\n\t\t\t\tkmalloc_caches[kmalloc_type(flags)][index],\n\t\t\t\tflags, size);\n#endif\n\t}\n\treturn __kmalloc(size, flags);\n}\n\nstatic __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)\n{\n#ifndef CONFIG_SLOB\n\tif (__builtin_constant_p(size) &&\n\t\tsize <= KMALLOC_MAX_CACHE_SIZE) {\n\t\tunsigned int i = kmalloc_index(size);\n\n\t\tif (!i)\n\t\t\treturn ZERO_SIZE_PTR;\n\n\t\treturn kmem_cache_alloc_node_trace(\n\t\t\t\tkmalloc_caches[kmalloc_type(flags)][i],\n\t\t\t\t\t\tflags, node, size);\n\t}\n#endif\n\treturn __kmalloc_node(size, flags, node);\n}\n\n/**\n * kmalloc_array - allocate memory for an array.\n * @n: number of elements.\n * @size: element size.\n * @flags: the type of memory to allocate (see kmalloc).\n */\nstatic inline void *kmalloc_array(size_t n, size_t size, gfp_t flags)\n{\n\tsize_t bytes;\n\n\tif (unlikely(check_mul_overflow(n, size, &bytes)))\n\t\treturn NULL;\n\tif (__builtin_constant_p(n) && __builtin_constant_p(size))\n\t\treturn kmalloc(bytes, flags);\n\treturn __kmalloc(bytes, flags);\n}\n\n/**\n * krealloc_array - reallocate memory for an array.\n * @p: pointer to the memory chunk to reallocate\n * @new_n: new number of elements to alloc\n * @new_size: new size of a single member of the array\n * @flags: the type of memory to allocate (see kmalloc)\n */\nstatic __must_check inline void *\nkrealloc_array(void *p, size_t new_n, size_t new_size, gfp_t flags)\n{\n\tsize_t bytes;\n\n\tif (unlikely(check_mul_overflow(new_n, new_size, &bytes)))\n\t\treturn NULL;\n\n\treturn krealloc(p, bytes, flags);\n}\n\n/**\n * kcalloc - allocate memory for an array. The memory is set to zero.\n * @n: number of elements.\n * @size: element size.\n * @flags: the type of memory to allocate (see kmalloc).\n */\nstatic inline void *kcalloc(size_t n, size_t size, gfp_t flags)\n{\n\treturn kmalloc_array(n, size, flags | __GFP_ZERO);\n}\n\n/*\n * kmalloc_track_caller is a special version of kmalloc that records the\n * calling function of the routine calling it for slab leak tracking instead\n * of just the calling function (confusing, eh?).\n * It's useful when the call to kmalloc comes from a widely-used standard\n * allocator where we care about the real place the memory allocation\n * request comes from.\n */\nextern void *__kmalloc_track_caller(size_t, gfp_t, unsigned long);\n#define kmalloc_track_caller(size, flags) \\\n\t__kmalloc_track_caller(size, flags, _RET_IP_)\n\nstatic inline void *kmalloc_array_node(size_t n, size_t size, gfp_t flags,\n\t\t\t\t       int node)\n{\n\tsize_t bytes;\n\n\tif (unlikely(check_mul_overflow(n, size, &bytes)))\n\t\treturn NULL;\n\tif (__builtin_constant_p(n) && __builtin_constant_p(size))\n\t\treturn kmalloc_node(bytes, flags, node);\n\treturn __kmalloc_node(bytes, flags, node);\n}\n\nstatic inline void *kcalloc_node(size_t n, size_t size, gfp_t flags, int node)\n{\n\treturn kmalloc_array_node(n, size, flags | __GFP_ZERO, node);\n}\n\n\n#ifdef CONFIG_NUMA\nextern void *__kmalloc_node_track_caller(size_t, gfp_t, int, unsigned long);\n#define kmalloc_node_track_caller(size, flags, node) \\\n\t__kmalloc_node_track_caller(size, flags, node, \\\n\t\t\t_RET_IP_)\n\n#else /* CONFIG_NUMA */\n\n#define kmalloc_node_track_caller(size, flags, node) \\\n\tkmalloc_track_caller(size, flags)\n\n#endif /* CONFIG_NUMA */\n\n/*\n * Shortcuts\n */\nstatic inline void *kmem_cache_zalloc(struct kmem_cache *k, gfp_t flags)\n{\n\treturn kmem_cache_alloc(k, flags | __GFP_ZERO);\n}\n\n/**\n * kzalloc - allocate memory. The memory is set to zero.\n * @size: how many bytes of memory are required.\n * @flags: the type of memory to allocate (see kmalloc).\n */\nstatic inline void *kzalloc(size_t size, gfp_t flags)\n{\n\treturn kmalloc(size, flags | __GFP_ZERO);\n}\n\n/**\n * kzalloc_node - allocate zeroed memory from a particular memory node.\n * @size: how many bytes of memory are required.\n * @flags: the type of memory to allocate (see kmalloc).\n * @node: memory node from which to allocate\n */\nstatic inline void *kzalloc_node(size_t size, gfp_t flags, int node)\n{\n\treturn kmalloc_node(size, flags | __GFP_ZERO, node);\n}\n\nunsigned int kmem_cache_size(struct kmem_cache *s);\nvoid __init kmem_cache_init_late(void);\n\n#if defined(CONFIG_SMP) && defined(CONFIG_SLAB)\nint slab_prepare_cpu(unsigned int cpu);\nint slab_dead_cpu(unsigned int cpu);\n#else\n#define slab_prepare_cpu\tNULL\n#define slab_dead_cpu\t\tNULL\n#endif\n\n#endif\t/* _LINUX_SLAB_H */\n"}, "18": {"id": 18, "path": "/src/include/linux/err.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_ERR_H\n#define _LINUX_ERR_H\n\n#include <linux/compiler.h>\n#include <linux/types.h>\n\n#include <asm/errno.h>\n\n/*\n * Kernel pointers have redundant information, so we can use a\n * scheme where we can return either an error code or a normal\n * pointer with the same return value.\n *\n * This should be a per-architecture thing, to allow different\n * error and pointer decisions.\n */\n#define MAX_ERRNO\t4095\n\n#ifndef __ASSEMBLY__\n\n#define IS_ERR_VALUE(x) unlikely((unsigned long)(void *)(x) >= (unsigned long)-MAX_ERRNO)\n\nstatic inline void * __must_check ERR_PTR(long error)\n{\n\treturn (void *) error;\n}\n\nstatic inline long __must_check PTR_ERR(__force const void *ptr)\n{\n\treturn (long) ptr;\n}\n\nstatic inline bool __must_check IS_ERR(__force const void *ptr)\n{\n\treturn IS_ERR_VALUE((unsigned long)ptr);\n}\n\nstatic inline bool __must_check IS_ERR_OR_NULL(__force const void *ptr)\n{\n\treturn unlikely(!ptr) || IS_ERR_VALUE((unsigned long)ptr);\n}\n\n/**\n * ERR_CAST - Explicitly cast an error-valued pointer to another pointer type\n * @ptr: The pointer to cast.\n *\n * Explicitly cast an error-valued pointer to another pointer type in such a\n * way as to make it clear that's what's going on.\n */\nstatic inline void * __must_check ERR_CAST(__force const void *ptr)\n{\n\t/* cast away the const */\n\treturn (void *) ptr;\n}\n\nstatic inline int __must_check PTR_ERR_OR_ZERO(__force const void *ptr)\n{\n\tif (IS_ERR(ptr))\n\t\treturn PTR_ERR(ptr);\n\telse\n\t\treturn 0;\n}\n\n#endif\n\n#endif /* _LINUX_ERR_H */\n"}, "19": {"id": 19, "path": "/src/include/linux/compiler.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_COMPILER_H\n#define __LINUX_COMPILER_H\n\n#include <linux/compiler_types.h>\n\n#ifndef __ASSEMBLY__\n\n#ifdef __KERNEL__\n\n/*\n * Note: DISABLE_BRANCH_PROFILING can be used by special lowlevel code\n * to disable branch tracing on a per file basis.\n */\n#if defined(CONFIG_TRACE_BRANCH_PROFILING) \\\n    && !defined(DISABLE_BRANCH_PROFILING) && !defined(__CHECKER__)\nvoid ftrace_likely_update(struct ftrace_likely_data *f, int val,\n\t\t\t  int expect, int is_constant);\n\n#define likely_notrace(x)\t__builtin_expect(!!(x), 1)\n#define unlikely_notrace(x)\t__builtin_expect(!!(x), 0)\n\n#define __branch_check__(x, expect, is_constant) ({\t\t\t\\\n\t\t\tlong ______r;\t\t\t\t\t\\\n\t\t\tstatic struct ftrace_likely_data\t\t\\\n\t\t\t\t__aligned(4)\t\t\t\t\\\n\t\t\t\t__section(\"_ftrace_annotated_branch\")\t\\\n\t\t\t\t______f = {\t\t\t\t\\\n\t\t\t\t.data.func = __func__,\t\t\t\\\n\t\t\t\t.data.file = __FILE__,\t\t\t\\\n\t\t\t\t.data.line = __LINE__,\t\t\t\\\n\t\t\t};\t\t\t\t\t\t\\\n\t\t\t______r = __builtin_expect(!!(x), expect);\t\\\n\t\t\tftrace_likely_update(&______f, ______r,\t\t\\\n\t\t\t\t\t     expect, is_constant);\t\\\n\t\t\t______r;\t\t\t\t\t\\\n\t\t})\n\n/*\n * Using __builtin_constant_p(x) to ignore cases where the return\n * value is always the same.  This idea is taken from a similar patch\n * written by Daniel Walker.\n */\n# ifndef likely\n#  define likely(x)\t(__branch_check__(x, 1, __builtin_constant_p(x)))\n# endif\n# ifndef unlikely\n#  define unlikely(x)\t(__branch_check__(x, 0, __builtin_constant_p(x)))\n# endif\n\n#ifdef CONFIG_PROFILE_ALL_BRANCHES\n/*\n * \"Define 'is'\", Bill Clinton\n * \"Define 'if'\", Steven Rostedt\n */\n#define if(cond, ...) if ( __trace_if_var( !!(cond , ## __VA_ARGS__) ) )\n\n#define __trace_if_var(cond) (__builtin_constant_p(cond) ? (cond) : __trace_if_value(cond))\n\n#define __trace_if_value(cond) ({\t\t\t\\\n\tstatic struct ftrace_branch_data\t\t\\\n\t\t__aligned(4)\t\t\t\t\\\n\t\t__section(\"_ftrace_branch\")\t\t\\\n\t\t__if_trace = {\t\t\t\t\\\n\t\t\t.func = __func__,\t\t\\\n\t\t\t.file = __FILE__,\t\t\\\n\t\t\t.line = __LINE__,\t\t\\\n\t\t};\t\t\t\t\t\\\n\t(cond) ?\t\t\t\t\t\\\n\t\t(__if_trace.miss_hit[1]++,1) :\t\t\\\n\t\t(__if_trace.miss_hit[0]++,0);\t\t\\\n})\n\n#endif /* CONFIG_PROFILE_ALL_BRANCHES */\n\n#else\n# define likely(x)\t__builtin_expect(!!(x), 1)\n# define unlikely(x)\t__builtin_expect(!!(x), 0)\n#endif\n\n/* Optimization barrier */\n#ifndef barrier\n/* The \"volatile\" is due to gcc bugs */\n# define barrier() __asm__ __volatile__(\"\": : :\"memory\")\n#endif\n\n#ifndef barrier_data\n/*\n * This version is i.e. to prevent dead stores elimination on @ptr\n * where gcc and llvm may behave differently when otherwise using\n * normal barrier(): while gcc behavior gets along with a normal\n * barrier(), llvm needs an explicit input variable to be assumed\n * clobbered. The issue is as follows: while the inline asm might\n * access any memory it wants, the compiler could have fit all of\n * @ptr into memory registers instead, and since @ptr never escaped\n * from that, it proved that the inline asm wasn't touching any of\n * it. This version works well with both compilers, i.e. we're telling\n * the compiler that the inline asm absolutely may see the contents\n * of @ptr. See also: https://llvm.org/bugs/show_bug.cgi?id=15495\n */\n# define barrier_data(ptr) __asm__ __volatile__(\"\": :\"r\"(ptr) :\"memory\")\n#endif\n\n/* workaround for GCC PR82365 if needed */\n#ifndef barrier_before_unreachable\n# define barrier_before_unreachable() do { } while (0)\n#endif\n\n/* Unreachable code */\n#ifdef CONFIG_STACK_VALIDATION\n/*\n * These macros help objtool understand GCC code flow for unreachable code.\n * The __COUNTER__ based labels are a hack to make each instance of the macros\n * unique, to convince GCC not to merge duplicate inline asm statements.\n */\n#define annotate_reachable() ({\t\t\t\t\t\t\\\n\tasm volatile(\"%c0:\\n\\t\"\t\t\t\t\t\t\\\n\t\t     \".pushsection .discard.reachable\\n\\t\"\t\t\\\n\t\t     \".long %c0b - .\\n\\t\"\t\t\t\t\\\n\t\t     \".popsection\\n\\t\" : : \"i\" (__COUNTER__));\t\t\\\n})\n#define annotate_unreachable() ({\t\t\t\t\t\\\n\tasm volatile(\"%c0:\\n\\t\"\t\t\t\t\t\t\\\n\t\t     \".pushsection .discard.unreachable\\n\\t\"\t\t\\\n\t\t     \".long %c0b - .\\n\\t\"\t\t\t\t\\\n\t\t     \".popsection\\n\\t\" : : \"i\" (__COUNTER__));\t\t\\\n})\n#define ASM_UNREACHABLE\t\t\t\t\t\t\t\\\n\t\"999:\\n\\t\"\t\t\t\t\t\t\t\\\n\t\".pushsection .discard.unreachable\\n\\t\"\t\t\t\t\\\n\t\".long 999b - .\\n\\t\"\t\t\t\t\t\t\\\n\t\".popsection\\n\\t\"\n\n/* Annotate a C jump table to allow objtool to follow the code flow */\n#define __annotate_jump_table __section(\".rodata..c_jump_table\")\n\n#else\n#define annotate_reachable()\n#define annotate_unreachable()\n#define __annotate_jump_table\n#endif\n\n#ifndef ASM_UNREACHABLE\n# define ASM_UNREACHABLE\n#endif\n#ifndef unreachable\n# define unreachable() do {\t\t\\\n\tannotate_unreachable();\t\t\\\n\t__builtin_unreachable();\t\\\n} while (0)\n#endif\n\n/*\n * KENTRY - kernel entry point\n * This can be used to annotate symbols (functions or data) that are used\n * without their linker symbol being referenced explicitly. For example,\n * interrupt vector handlers, or functions in the kernel image that are found\n * programatically.\n *\n * Not required for symbols exported with EXPORT_SYMBOL, or initcalls. Those\n * are handled in their own way (with KEEP() in linker scripts).\n *\n * KENTRY can be avoided if the symbols in question are marked as KEEP() in the\n * linker script. For example an architecture could KEEP() its entire\n * boot/exception vector code rather than annotate each function and data.\n */\n#ifndef KENTRY\n# define KENTRY(sym)\t\t\t\t\t\t\\\n\textern typeof(sym) sym;\t\t\t\t\t\\\n\tstatic const unsigned long __kentry_##sym\t\t\\\n\t__used\t\t\t\t\t\t\t\\\n\t__attribute__((__section__(\"___kentry+\" #sym)))\t\t\\\n\t= (unsigned long)&sym;\n#endif\n\n#ifndef RELOC_HIDE\n# define RELOC_HIDE(ptr, off)\t\t\t\t\t\\\n  ({ unsigned long __ptr;\t\t\t\t\t\\\n     __ptr = (unsigned long) (ptr);\t\t\t\t\\\n    (typeof(ptr)) (__ptr + (off)); })\n#endif\n\n#ifndef OPTIMIZER_HIDE_VAR\n/* Make the optimizer believe the variable can be manipulated arbitrarily. */\n#define OPTIMIZER_HIDE_VAR(var)\t\t\t\t\t\t\\\n\t__asm__ (\"\" : \"=r\" (var) : \"0\" (var))\n#endif\n\n/* Not-quite-unique ID. */\n#ifndef __UNIQUE_ID\n# define __UNIQUE_ID(prefix) __PASTE(__PASTE(__UNIQUE_ID_, prefix), __LINE__)\n#endif\n\n/**\n * data_race - mark an expression as containing intentional data races\n *\n * This data_race() macro is useful for situations in which data races\n * should be forgiven.  One example is diagnostic code that accesses\n * shared variables but is not a part of the core synchronization design.\n *\n * This macro *does not* affect normal code generation, but is a hint\n * to tooling that data races here are to be ignored.\n */\n#define data_race(expr)\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__unqual_scalar_typeof(({ expr; })) __v = ({\t\t\t\\\n\t\t__kcsan_disable_current();\t\t\t\t\\\n\t\texpr;\t\t\t\t\t\t\t\\\n\t});\t\t\t\t\t\t\t\t\\\n\t__kcsan_enable_current();\t\t\t\t\t\\\n\t__v;\t\t\t\t\t\t\t\t\\\n})\n\n#endif /* __KERNEL__ */\n\n/*\n * Force the compiler to emit 'sym' as a symbol, so that we can reference\n * it from inline assembler. Necessary in case 'sym' could be inlined\n * otherwise, or eliminated entirely due to lack of references that are\n * visible to the compiler.\n */\n#define __ADDRESSABLE(sym) \\\n\tstatic void * __section(\".discard.addressable\") __used \\\n\t\t__UNIQUE_ID(__PASTE(__addressable_,sym)) = (void *)&sym;\n\n/**\n * offset_to_ptr - convert a relative memory offset to an absolute pointer\n * @off:\tthe address of the 32-bit offset value\n */\nstatic inline void *offset_to_ptr(const int *off)\n{\n\treturn (void *)((unsigned long)off + *off);\n}\n\n#endif /* __ASSEMBLY__ */\n\n/* &a[0] degrades to a pointer: a different type from an array */\n#define __must_be_array(a)\tBUILD_BUG_ON_ZERO(__same_type((a), &(a)[0]))\n\n/*\n * This is needed in functions which generate the stack canary, see\n * arch/x86/kernel/smpboot.c::start_secondary() for an example.\n */\n#define prevent_tail_call_optimization()\tmb()\n\n#include <asm/rwonce.h>\n\n#endif /* __LINUX_COMPILER_H */\n"}, "20": {"id": 20, "path": "/src/include/linux/percpu.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_PERCPU_H\n#define __LINUX_PERCPU_H\n\n#include <linux/mmdebug.h>\n#include <linux/preempt.h>\n#include <linux/smp.h>\n#include <linux/cpumask.h>\n#include <linux/printk.h>\n#include <linux/pfn.h>\n#include <linux/init.h>\n\n#include <asm/percpu.h>\n\n/* enough to cover all DEFINE_PER_CPUs in modules */\n#ifdef CONFIG_MODULES\n#define PERCPU_MODULE_RESERVE\t\t(8 << 10)\n#else\n#define PERCPU_MODULE_RESERVE\t\t0\n#endif\n\n/* minimum unit size, also is the maximum supported allocation size */\n#define PCPU_MIN_UNIT_SIZE\t\tPFN_ALIGN(32 << 10)\n\n/* minimum allocation size and shift in bytes */\n#define PCPU_MIN_ALLOC_SHIFT\t\t2\n#define PCPU_MIN_ALLOC_SIZE\t\t(1 << PCPU_MIN_ALLOC_SHIFT)\n\n/*\n * The PCPU_BITMAP_BLOCK_SIZE must be the same size as PAGE_SIZE as the\n * updating of hints is used to manage the nr_empty_pop_pages in both\n * the chunk and globally.\n */\n#define PCPU_BITMAP_BLOCK_SIZE\t\tPAGE_SIZE\n#define PCPU_BITMAP_BLOCK_BITS\t\t(PCPU_BITMAP_BLOCK_SIZE >>\t\\\n\t\t\t\t\t PCPU_MIN_ALLOC_SHIFT)\n\n/*\n * Percpu allocator can serve percpu allocations before slab is\n * initialized which allows slab to depend on the percpu allocator.\n * The following two parameters decide how much resource to\n * preallocate for this.  Keep PERCPU_DYNAMIC_RESERVE equal to or\n * larger than PERCPU_DYNAMIC_EARLY_SIZE.\n */\n#define PERCPU_DYNAMIC_EARLY_SLOTS\t128\n#define PERCPU_DYNAMIC_EARLY_SIZE\t(12 << 10)\n\n/*\n * PERCPU_DYNAMIC_RESERVE indicates the amount of free area to piggy\n * back on the first chunk for dynamic percpu allocation if arch is\n * manually allocating and mapping it for faster access (as a part of\n * large page mapping for example).\n *\n * The following values give between one and two pages of free space\n * after typical minimal boot (2-way SMP, single disk and NIC) with\n * both defconfig and a distro config on x86_64 and 32.  More\n * intelligent way to determine this would be nice.\n */\n#if BITS_PER_LONG > 32\n#define PERCPU_DYNAMIC_RESERVE\t\t(28 << 10)\n#else\n#define PERCPU_DYNAMIC_RESERVE\t\t(20 << 10)\n#endif\n\nextern void *pcpu_base_addr;\nextern const unsigned long *pcpu_unit_offsets;\n\nstruct pcpu_group_info {\n\tint\t\t\tnr_units;\t/* aligned # of units */\n\tunsigned long\t\tbase_offset;\t/* base address offset */\n\tunsigned int\t\t*cpu_map;\t/* unit->cpu map, empty\n\t\t\t\t\t\t * entries contain NR_CPUS */\n};\n\nstruct pcpu_alloc_info {\n\tsize_t\t\t\tstatic_size;\n\tsize_t\t\t\treserved_size;\n\tsize_t\t\t\tdyn_size;\n\tsize_t\t\t\tunit_size;\n\tsize_t\t\t\tatom_size;\n\tsize_t\t\t\talloc_size;\n\tsize_t\t\t\t__ai_size;\t/* internal, don't use */\n\tint\t\t\tnr_groups;\t/* 0 if grouping unnecessary */\n\tstruct pcpu_group_info\tgroups[];\n};\n\nenum pcpu_fc {\n\tPCPU_FC_AUTO,\n\tPCPU_FC_EMBED,\n\tPCPU_FC_PAGE,\n\n\tPCPU_FC_NR,\n};\nextern const char * const pcpu_fc_names[PCPU_FC_NR];\n\nextern enum pcpu_fc pcpu_chosen_fc;\n\ntypedef void * (*pcpu_fc_alloc_fn_t)(unsigned int cpu, size_t size,\n\t\t\t\t     size_t align);\ntypedef void (*pcpu_fc_free_fn_t)(void *ptr, size_t size);\ntypedef void (*pcpu_fc_populate_pte_fn_t)(unsigned long addr);\ntypedef int (pcpu_fc_cpu_distance_fn_t)(unsigned int from, unsigned int to);\n\nextern struct pcpu_alloc_info * __init pcpu_alloc_alloc_info(int nr_groups,\n\t\t\t\t\t\t\t     int nr_units);\nextern void __init pcpu_free_alloc_info(struct pcpu_alloc_info *ai);\n\nextern void __init pcpu_setup_first_chunk(const struct pcpu_alloc_info *ai,\n\t\t\t\t\t void *base_addr);\n\n#ifdef CONFIG_NEED_PER_CPU_EMBED_FIRST_CHUNK\nextern int __init pcpu_embed_first_chunk(size_t reserved_size, size_t dyn_size,\n\t\t\t\tsize_t atom_size,\n\t\t\t\tpcpu_fc_cpu_distance_fn_t cpu_distance_fn,\n\t\t\t\tpcpu_fc_alloc_fn_t alloc_fn,\n\t\t\t\tpcpu_fc_free_fn_t free_fn);\n#endif\n\n#ifdef CONFIG_NEED_PER_CPU_PAGE_FIRST_CHUNK\nextern int __init pcpu_page_first_chunk(size_t reserved_size,\n\t\t\t\tpcpu_fc_alloc_fn_t alloc_fn,\n\t\t\t\tpcpu_fc_free_fn_t free_fn,\n\t\t\t\tpcpu_fc_populate_pte_fn_t populate_pte_fn);\n#endif\n\nextern void __percpu *__alloc_reserved_percpu(size_t size, size_t align);\nextern bool __is_kernel_percpu_address(unsigned long addr, unsigned long *can_addr);\nextern bool is_kernel_percpu_address(unsigned long addr);\n\n#if !defined(CONFIG_SMP) || !defined(CONFIG_HAVE_SETUP_PER_CPU_AREA)\nextern void __init setup_per_cpu_areas(void);\n#endif\n\nextern void __percpu *__alloc_percpu_gfp(size_t size, size_t align, gfp_t gfp);\nextern void __percpu *__alloc_percpu(size_t size, size_t align);\nextern void free_percpu(void __percpu *__pdata);\nextern phys_addr_t per_cpu_ptr_to_phys(void *addr);\n\n#define alloc_percpu_gfp(type, gfp)\t\t\t\t\t\\\n\t(typeof(type) __percpu *)__alloc_percpu_gfp(sizeof(type),\t\\\n\t\t\t\t\t\t__alignof__(type), gfp)\n#define alloc_percpu(type)\t\t\t\t\t\t\\\n\t(typeof(type) __percpu *)__alloc_percpu(sizeof(type),\t\t\\\n\t\t\t\t\t\t__alignof__(type))\n\nextern unsigned long pcpu_nr_pages(void);\n\n#endif /* __LINUX_PERCPU_H */\n"}, "21": {"id": 21, "path": "/src/net/sunrpc/xprt.c", "content": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n *  linux/net/sunrpc/xprt.c\n *\n *  This is a generic RPC call interface supporting congestion avoidance,\n *  and asynchronous calls.\n *\n *  The interface works like this:\n *\n *  -\tWhen a process places a call, it allocates a request slot if\n *\tone is available. Otherwise, it sleeps on the backlog queue\n *\t(xprt_reserve).\n *  -\tNext, the caller puts together the RPC message, stuffs it into\n *\tthe request struct, and calls xprt_transmit().\n *  -\txprt_transmit sends the message and installs the caller on the\n *\ttransport's wait list. At the same time, if a reply is expected,\n *\tit installs a timer that is run after the packet's timeout has\n *\texpired.\n *  -\tWhen a packet arrives, the data_ready handler walks the list of\n *\tpending requests for that transport. If a matching XID is found, the\n *\tcaller is woken up, and the timer removed.\n *  -\tWhen no reply arrives within the timeout interval, the timer is\n *\tfired by the kernel and runs xprt_timer(). It either adjusts the\n *\ttimeout values (minor timeout) or wakes up the caller with a status\n *\tof -ETIMEDOUT.\n *  -\tWhen the caller receives a notification from RPC that a reply arrived,\n *\tit should release the RPC slot, and process the reply.\n *\tIf the call timed out, it may choose to retry the operation by\n *\tadjusting the initial timeout value, and simply calling rpc_call\n *\tagain.\n *\n *  Support for async RPC is done through a set of RPC-specific scheduling\n *  primitives that `transparently' work for processes as well as async\n *  tasks that rely on callbacks.\n *\n *  Copyright (C) 1995-1997, Olaf Kirch <okir@monad.swb.de>\n *\n *  Transport switch API copyright (C) 2005, Chuck Lever <cel@netapp.com>\n */\n\n#include <linux/module.h>\n\n#include <linux/types.h>\n#include <linux/interrupt.h>\n#include <linux/workqueue.h>\n#include <linux/net.h>\n#include <linux/ktime.h>\n\n#include <linux/sunrpc/clnt.h>\n#include <linux/sunrpc/metrics.h>\n#include <linux/sunrpc/bc_xprt.h>\n#include <linux/rcupdate.h>\n#include <linux/sched/mm.h>\n\n#include <trace/events/sunrpc.h>\n\n#include \"sunrpc.h\"\n\n/*\n * Local variables\n */\n\n#if IS_ENABLED(CONFIG_SUNRPC_DEBUG)\n# define RPCDBG_FACILITY\tRPCDBG_XPRT\n#endif\n\n/*\n * Local functions\n */\nstatic void\t xprt_init(struct rpc_xprt *xprt, struct net *net);\nstatic __be32\txprt_alloc_xid(struct rpc_xprt *xprt);\nstatic void\t xprt_destroy(struct rpc_xprt *xprt);\n\nstatic DEFINE_SPINLOCK(xprt_list_lock);\nstatic LIST_HEAD(xprt_list);\n\nstatic unsigned long xprt_request_timeout(const struct rpc_rqst *req)\n{\n\tunsigned long timeout = jiffies + req->rq_timeout;\n\n\tif (time_before(timeout, req->rq_majortimeo))\n\t\treturn timeout;\n\treturn req->rq_majortimeo;\n}\n\n/**\n * xprt_register_transport - register a transport implementation\n * @transport: transport to register\n *\n * If a transport implementation is loaded as a kernel module, it can\n * call this interface to make itself known to the RPC client.\n *\n * Returns:\n * 0:\t\ttransport successfully registered\n * -EEXIST:\ttransport already registered\n * -EINVAL:\ttransport module being unloaded\n */\nint xprt_register_transport(struct xprt_class *transport)\n{\n\tstruct xprt_class *t;\n\tint result;\n\n\tresult = -EEXIST;\n\tspin_lock(&xprt_list_lock);\n\tlist_for_each_entry(t, &xprt_list, list) {\n\t\t/* don't register the same transport class twice */\n\t\tif (t->ident == transport->ident)\n\t\t\tgoto out;\n\t}\n\n\tlist_add_tail(&transport->list, &xprt_list);\n\tprintk(KERN_INFO \"RPC: Registered %s transport module.\\n\",\n\t       transport->name);\n\tresult = 0;\n\nout:\n\tspin_unlock(&xprt_list_lock);\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(xprt_register_transport);\n\n/**\n * xprt_unregister_transport - unregister a transport implementation\n * @transport: transport to unregister\n *\n * Returns:\n * 0:\t\ttransport successfully unregistered\n * -ENOENT:\ttransport never registered\n */\nint xprt_unregister_transport(struct xprt_class *transport)\n{\n\tstruct xprt_class *t;\n\tint result;\n\n\tresult = 0;\n\tspin_lock(&xprt_list_lock);\n\tlist_for_each_entry(t, &xprt_list, list) {\n\t\tif (t == transport) {\n\t\t\tprintk(KERN_INFO\n\t\t\t\t\"RPC: Unregistered %s transport module.\\n\",\n\t\t\t\ttransport->name);\n\t\t\tlist_del_init(&transport->list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\tresult = -ENOENT;\n\nout:\n\tspin_unlock(&xprt_list_lock);\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(xprt_unregister_transport);\n\n/**\n * xprt_load_transport - load a transport implementation\n * @transport_name: transport to load\n *\n * Returns:\n * 0:\t\ttransport successfully loaded\n * -ENOENT:\ttransport module not available\n */\nint xprt_load_transport(const char *transport_name)\n{\n\tstruct xprt_class *t;\n\tint result;\n\n\tresult = 0;\n\tspin_lock(&xprt_list_lock);\n\tlist_for_each_entry(t, &xprt_list, list) {\n\t\tif (strcmp(t->name, transport_name) == 0) {\n\t\t\tspin_unlock(&xprt_list_lock);\n\t\t\tgoto out;\n\t\t}\n\t}\n\tspin_unlock(&xprt_list_lock);\n\tresult = request_module(\"xprt%s\", transport_name);\nout:\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(xprt_load_transport);\n\nstatic void xprt_clear_locked(struct rpc_xprt *xprt)\n{\n\txprt->snd_task = NULL;\n\tif (!test_bit(XPRT_CLOSE_WAIT, &xprt->state)) {\n\t\tsmp_mb__before_atomic();\n\t\tclear_bit(XPRT_LOCKED, &xprt->state);\n\t\tsmp_mb__after_atomic();\n\t} else\n\t\tqueue_work(xprtiod_workqueue, &xprt->task_cleanup);\n}\n\n/**\n * xprt_reserve_xprt - serialize write access to transports\n * @task: task that is requesting access to the transport\n * @xprt: pointer to the target transport\n *\n * This prevents mixing the payload of separate requests, and prevents\n * transport connects from colliding with writes.  No congestion control\n * is provided.\n */\nint xprt_reserve_xprt(struct rpc_xprt *xprt, struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\n\tif (test_and_set_bit(XPRT_LOCKED, &xprt->state)) {\n\t\tif (task == xprt->snd_task)\n\t\t\tgoto out_locked;\n\t\tgoto out_sleep;\n\t}\n\tif (test_bit(XPRT_WRITE_SPACE, &xprt->state))\n\t\tgoto out_unlock;\n\txprt->snd_task = task;\n\nout_locked:\n\ttrace_xprt_reserve_xprt(xprt, task);\n\treturn 1;\n\nout_unlock:\n\txprt_clear_locked(xprt);\nout_sleep:\n\ttask->tk_status = -EAGAIN;\n\tif  (RPC_IS_SOFT(task))\n\t\trpc_sleep_on_timeout(&xprt->sending, task, NULL,\n\t\t\t\txprt_request_timeout(req));\n\telse\n\t\trpc_sleep_on(&xprt->sending, task, NULL);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(xprt_reserve_xprt);\n\nstatic bool\nxprt_need_congestion_window_wait(struct rpc_xprt *xprt)\n{\n\treturn test_bit(XPRT_CWND_WAIT, &xprt->state);\n}\n\nstatic void\nxprt_set_congestion_window_wait(struct rpc_xprt *xprt)\n{\n\tif (!list_empty(&xprt->xmit_queue)) {\n\t\t/* Peek at head of queue to see if it can make progress */\n\t\tif (list_first_entry(&xprt->xmit_queue, struct rpc_rqst,\n\t\t\t\t\trq_xmit)->rq_cong)\n\t\t\treturn;\n\t}\n\tset_bit(XPRT_CWND_WAIT, &xprt->state);\n}\n\nstatic void\nxprt_test_and_clear_congestion_window_wait(struct rpc_xprt *xprt)\n{\n\tif (!RPCXPRT_CONGESTED(xprt))\n\t\tclear_bit(XPRT_CWND_WAIT, &xprt->state);\n}\n\n/*\n * xprt_reserve_xprt_cong - serialize write access to transports\n * @task: task that is requesting access to the transport\n *\n * Same as xprt_reserve_xprt, but Van Jacobson congestion control is\n * integrated into the decision of whether a request is allowed to be\n * woken up and given access to the transport.\n * Note that the lock is only granted if we know there are free slots.\n */\nint xprt_reserve_xprt_cong(struct rpc_xprt *xprt, struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\n\tif (test_and_set_bit(XPRT_LOCKED, &xprt->state)) {\n\t\tif (task == xprt->snd_task)\n\t\t\tgoto out_locked;\n\t\tgoto out_sleep;\n\t}\n\tif (req == NULL) {\n\t\txprt->snd_task = task;\n\t\tgoto out_locked;\n\t}\n\tif (test_bit(XPRT_WRITE_SPACE, &xprt->state))\n\t\tgoto out_unlock;\n\tif (!xprt_need_congestion_window_wait(xprt)) {\n\t\txprt->snd_task = task;\n\t\tgoto out_locked;\n\t}\nout_unlock:\n\txprt_clear_locked(xprt);\nout_sleep:\n\ttask->tk_status = -EAGAIN;\n\tif (RPC_IS_SOFT(task))\n\t\trpc_sleep_on_timeout(&xprt->sending, task, NULL,\n\t\t\t\txprt_request_timeout(req));\n\telse\n\t\trpc_sleep_on(&xprt->sending, task, NULL);\n\treturn 0;\nout_locked:\n\ttrace_xprt_reserve_cong(xprt, task);\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(xprt_reserve_xprt_cong);\n\nstatic inline int xprt_lock_write(struct rpc_xprt *xprt, struct rpc_task *task)\n{\n\tint retval;\n\n\tif (test_bit(XPRT_LOCKED, &xprt->state) && xprt->snd_task == task)\n\t\treturn 1;\n\tspin_lock(&xprt->transport_lock);\n\tretval = xprt->ops->reserve_xprt(xprt, task);\n\tspin_unlock(&xprt->transport_lock);\n\treturn retval;\n}\n\nstatic bool __xprt_lock_write_func(struct rpc_task *task, void *data)\n{\n\tstruct rpc_xprt *xprt = data;\n\n\txprt->snd_task = task;\n\treturn true;\n}\n\nstatic void __xprt_lock_write_next(struct rpc_xprt *xprt)\n{\n\tif (test_and_set_bit(XPRT_LOCKED, &xprt->state))\n\t\treturn;\n\tif (test_bit(XPRT_WRITE_SPACE, &xprt->state))\n\t\tgoto out_unlock;\n\tif (rpc_wake_up_first_on_wq(xprtiod_workqueue, &xprt->sending,\n\t\t\t\t__xprt_lock_write_func, xprt))\n\t\treturn;\nout_unlock:\n\txprt_clear_locked(xprt);\n}\n\nstatic void __xprt_lock_write_next_cong(struct rpc_xprt *xprt)\n{\n\tif (test_and_set_bit(XPRT_LOCKED, &xprt->state))\n\t\treturn;\n\tif (test_bit(XPRT_WRITE_SPACE, &xprt->state))\n\t\tgoto out_unlock;\n\tif (xprt_need_congestion_window_wait(xprt))\n\t\tgoto out_unlock;\n\tif (rpc_wake_up_first_on_wq(xprtiod_workqueue, &xprt->sending,\n\t\t\t\t__xprt_lock_write_func, xprt))\n\t\treturn;\nout_unlock:\n\txprt_clear_locked(xprt);\n}\n\n/**\n * xprt_release_xprt - allow other requests to use a transport\n * @xprt: transport with other tasks potentially waiting\n * @task: task that is releasing access to the transport\n *\n * Note that \"task\" can be NULL.  No congestion control is provided.\n */\nvoid xprt_release_xprt(struct rpc_xprt *xprt, struct rpc_task *task)\n{\n\tif (xprt->snd_task == task) {\n\t\txprt_clear_locked(xprt);\n\t\t__xprt_lock_write_next(xprt);\n\t}\n\ttrace_xprt_release_xprt(xprt, task);\n}\nEXPORT_SYMBOL_GPL(xprt_release_xprt);\n\n/**\n * xprt_release_xprt_cong - allow other requests to use a transport\n * @xprt: transport with other tasks potentially waiting\n * @task: task that is releasing access to the transport\n *\n * Note that \"task\" can be NULL.  Another task is awoken to use the\n * transport if the transport's congestion window allows it.\n */\nvoid xprt_release_xprt_cong(struct rpc_xprt *xprt, struct rpc_task *task)\n{\n\tif (xprt->snd_task == task) {\n\t\txprt_clear_locked(xprt);\n\t\t__xprt_lock_write_next_cong(xprt);\n\t}\n\ttrace_xprt_release_cong(xprt, task);\n}\nEXPORT_SYMBOL_GPL(xprt_release_xprt_cong);\n\nstatic inline void xprt_release_write(struct rpc_xprt *xprt, struct rpc_task *task)\n{\n\tif (xprt->snd_task != task)\n\t\treturn;\n\tspin_lock(&xprt->transport_lock);\n\txprt->ops->release_xprt(xprt, task);\n\tspin_unlock(&xprt->transport_lock);\n}\n\n/*\n * Van Jacobson congestion avoidance. Check if the congestion window\n * overflowed. Put the task to sleep if this is the case.\n */\nstatic int\n__xprt_get_cong(struct rpc_xprt *xprt, struct rpc_rqst *req)\n{\n\tif (req->rq_cong)\n\t\treturn 1;\n\ttrace_xprt_get_cong(xprt, req->rq_task);\n\tif (RPCXPRT_CONGESTED(xprt)) {\n\t\txprt_set_congestion_window_wait(xprt);\n\t\treturn 0;\n\t}\n\treq->rq_cong = 1;\n\txprt->cong += RPC_CWNDSCALE;\n\treturn 1;\n}\n\n/*\n * Adjust the congestion window, and wake up the next task\n * that has been sleeping due to congestion\n */\nstatic void\n__xprt_put_cong(struct rpc_xprt *xprt, struct rpc_rqst *req)\n{\n\tif (!req->rq_cong)\n\t\treturn;\n\treq->rq_cong = 0;\n\txprt->cong -= RPC_CWNDSCALE;\n\txprt_test_and_clear_congestion_window_wait(xprt);\n\ttrace_xprt_put_cong(xprt, req->rq_task);\n\t__xprt_lock_write_next_cong(xprt);\n}\n\n/**\n * xprt_request_get_cong - Request congestion control credits\n * @xprt: pointer to transport\n * @req: pointer to RPC request\n *\n * Useful for transports that require congestion control.\n */\nbool\nxprt_request_get_cong(struct rpc_xprt *xprt, struct rpc_rqst *req)\n{\n\tbool ret = false;\n\n\tif (req->rq_cong)\n\t\treturn true;\n\tspin_lock(&xprt->transport_lock);\n\tret = __xprt_get_cong(xprt, req) != 0;\n\tspin_unlock(&xprt->transport_lock);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(xprt_request_get_cong);\n\n/**\n * xprt_release_rqst_cong - housekeeping when request is complete\n * @task: RPC request that recently completed\n *\n * Useful for transports that require congestion control.\n */\nvoid xprt_release_rqst_cong(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\n\t__xprt_put_cong(req->rq_xprt, req);\n}\nEXPORT_SYMBOL_GPL(xprt_release_rqst_cong);\n\nstatic void xprt_clear_congestion_window_wait_locked(struct rpc_xprt *xprt)\n{\n\tif (test_and_clear_bit(XPRT_CWND_WAIT, &xprt->state))\n\t\t__xprt_lock_write_next_cong(xprt);\n}\n\n/*\n * Clear the congestion window wait flag and wake up the next\n * entry on xprt->sending\n */\nstatic void\nxprt_clear_congestion_window_wait(struct rpc_xprt *xprt)\n{\n\tif (test_and_clear_bit(XPRT_CWND_WAIT, &xprt->state)) {\n\t\tspin_lock(&xprt->transport_lock);\n\t\t__xprt_lock_write_next_cong(xprt);\n\t\tspin_unlock(&xprt->transport_lock);\n\t}\n}\n\n/**\n * xprt_adjust_cwnd - adjust transport congestion window\n * @xprt: pointer to xprt\n * @task: recently completed RPC request used to adjust window\n * @result: result code of completed RPC request\n *\n * The transport code maintains an estimate on the maximum number of out-\n * standing RPC requests, using a smoothed version of the congestion\n * avoidance implemented in 44BSD. This is basically the Van Jacobson\n * congestion algorithm: If a retransmit occurs, the congestion window is\n * halved; otherwise, it is incremented by 1/cwnd when\n *\n *\t-\ta reply is received and\n *\t-\ta full number of requests are outstanding and\n *\t-\tthe congestion window hasn't been updated recently.\n */\nvoid xprt_adjust_cwnd(struct rpc_xprt *xprt, struct rpc_task *task, int result)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\tunsigned long cwnd = xprt->cwnd;\n\n\tif (result >= 0 && cwnd <= xprt->cong) {\n\t\t/* The (cwnd >> 1) term makes sure\n\t\t * the result gets rounded properly. */\n\t\tcwnd += (RPC_CWNDSCALE * RPC_CWNDSCALE + (cwnd >> 1)) / cwnd;\n\t\tif (cwnd > RPC_MAXCWND(xprt))\n\t\t\tcwnd = RPC_MAXCWND(xprt);\n\t\t__xprt_lock_write_next_cong(xprt);\n\t} else if (result == -ETIMEDOUT) {\n\t\tcwnd >>= 1;\n\t\tif (cwnd < RPC_CWNDSCALE)\n\t\t\tcwnd = RPC_CWNDSCALE;\n\t}\n\tdprintk(\"RPC:       cong %ld, cwnd was %ld, now %ld\\n\",\n\t\t\txprt->cong, xprt->cwnd, cwnd);\n\txprt->cwnd = cwnd;\n\t__xprt_put_cong(xprt, req);\n}\nEXPORT_SYMBOL_GPL(xprt_adjust_cwnd);\n\n/**\n * xprt_wake_pending_tasks - wake all tasks on a transport's pending queue\n * @xprt: transport with waiting tasks\n * @status: result code to plant in each task before waking it\n *\n */\nvoid xprt_wake_pending_tasks(struct rpc_xprt *xprt, int status)\n{\n\tif (status < 0)\n\t\trpc_wake_up_status(&xprt->pending, status);\n\telse\n\t\trpc_wake_up(&xprt->pending);\n}\nEXPORT_SYMBOL_GPL(xprt_wake_pending_tasks);\n\n/**\n * xprt_wait_for_buffer_space - wait for transport output buffer to clear\n * @xprt: transport\n *\n * Note that we only set the timer for the case of RPC_IS_SOFT(), since\n * we don't in general want to force a socket disconnection due to\n * an incomplete RPC call transmission.\n */\nvoid xprt_wait_for_buffer_space(struct rpc_xprt *xprt)\n{\n\tset_bit(XPRT_WRITE_SPACE, &xprt->state);\n}\nEXPORT_SYMBOL_GPL(xprt_wait_for_buffer_space);\n\nstatic bool\nxprt_clear_write_space_locked(struct rpc_xprt *xprt)\n{\n\tif (test_and_clear_bit(XPRT_WRITE_SPACE, &xprt->state)) {\n\t\t__xprt_lock_write_next(xprt);\n\t\tdprintk(\"RPC:       write space: waking waiting task on \"\n\t\t\t\t\"xprt %p\\n\", xprt);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/**\n * xprt_write_space - wake the task waiting for transport output buffer space\n * @xprt: transport with waiting tasks\n *\n * Can be called in a soft IRQ context, so xprt_write_space never sleeps.\n */\nbool xprt_write_space(struct rpc_xprt *xprt)\n{\n\tbool ret;\n\n\tif (!test_bit(XPRT_WRITE_SPACE, &xprt->state))\n\t\treturn false;\n\tspin_lock(&xprt->transport_lock);\n\tret = xprt_clear_write_space_locked(xprt);\n\tspin_unlock(&xprt->transport_lock);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(xprt_write_space);\n\nstatic unsigned long xprt_abs_ktime_to_jiffies(ktime_t abstime)\n{\n\ts64 delta = ktime_to_ns(ktime_get() - abstime);\n\treturn likely(delta >= 0) ?\n\t\tjiffies - nsecs_to_jiffies(delta) :\n\t\tjiffies + nsecs_to_jiffies(-delta);\n}\n\nstatic unsigned long xprt_calc_majortimeo(struct rpc_rqst *req)\n{\n\tconst struct rpc_timeout *to = req->rq_task->tk_client->cl_timeout;\n\tunsigned long majortimeo = req->rq_timeout;\n\n\tif (to->to_exponential)\n\t\tmajortimeo <<= to->to_retries;\n\telse\n\t\tmajortimeo += to->to_increment * to->to_retries;\n\tif (majortimeo > to->to_maxval || majortimeo == 0)\n\t\tmajortimeo = to->to_maxval;\n\treturn majortimeo;\n}\n\nstatic void xprt_reset_majortimeo(struct rpc_rqst *req)\n{\n\treq->rq_majortimeo += xprt_calc_majortimeo(req);\n}\n\nstatic void xprt_reset_minortimeo(struct rpc_rqst *req)\n{\n\treq->rq_minortimeo += req->rq_timeout;\n}\n\nstatic void xprt_init_majortimeo(struct rpc_task *task, struct rpc_rqst *req)\n{\n\tunsigned long time_init;\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\n\tif (likely(xprt && xprt_connected(xprt)))\n\t\ttime_init = jiffies;\n\telse\n\t\ttime_init = xprt_abs_ktime_to_jiffies(task->tk_start);\n\treq->rq_timeout = task->tk_client->cl_timeout->to_initval;\n\treq->rq_majortimeo = time_init + xprt_calc_majortimeo(req);\n\treq->rq_minortimeo = time_init + req->rq_timeout;\n}\n\n/**\n * xprt_adjust_timeout - adjust timeout values for next retransmit\n * @req: RPC request containing parameters to use for the adjustment\n *\n */\nint xprt_adjust_timeout(struct rpc_rqst *req)\n{\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\tconst struct rpc_timeout *to = req->rq_task->tk_client->cl_timeout;\n\tint status = 0;\n\n\tif (time_before(jiffies, req->rq_minortimeo))\n\t\treturn status;\n\tif (time_before(jiffies, req->rq_majortimeo)) {\n\t\tif (to->to_exponential)\n\t\t\treq->rq_timeout <<= 1;\n\t\telse\n\t\t\treq->rq_timeout += to->to_increment;\n\t\tif (to->to_maxval && req->rq_timeout >= to->to_maxval)\n\t\t\treq->rq_timeout = to->to_maxval;\n\t\treq->rq_retries++;\n\t} else {\n\t\treq->rq_timeout = to->to_initval;\n\t\treq->rq_retries = 0;\n\t\txprt_reset_majortimeo(req);\n\t\t/* Reset the RTT counters == \"slow start\" */\n\t\tspin_lock(&xprt->transport_lock);\n\t\trpc_init_rtt(req->rq_task->tk_client->cl_rtt, to->to_initval);\n\t\tspin_unlock(&xprt->transport_lock);\n\t\tstatus = -ETIMEDOUT;\n\t}\n\txprt_reset_minortimeo(req);\n\n\tif (req->rq_timeout == 0) {\n\t\tprintk(KERN_WARNING \"xprt_adjust_timeout: rq_timeout = 0!\\n\");\n\t\treq->rq_timeout = 5 * HZ;\n\t}\n\treturn status;\n}\n\nstatic void xprt_autoclose(struct work_struct *work)\n{\n\tstruct rpc_xprt *xprt =\n\t\tcontainer_of(work, struct rpc_xprt, task_cleanup);\n\tunsigned int pflags = memalloc_nofs_save();\n\n\ttrace_xprt_disconnect_auto(xprt);\n\tclear_bit(XPRT_CLOSE_WAIT, &xprt->state);\n\txprt->ops->close(xprt);\n\txprt_release_write(xprt, NULL);\n\twake_up_bit(&xprt->state, XPRT_LOCKED);\n\tmemalloc_nofs_restore(pflags);\n}\n\n/**\n * xprt_disconnect_done - mark a transport as disconnected\n * @xprt: transport to flag for disconnect\n *\n */\nvoid xprt_disconnect_done(struct rpc_xprt *xprt)\n{\n\ttrace_xprt_disconnect_done(xprt);\n\tspin_lock(&xprt->transport_lock);\n\txprt_clear_connected(xprt);\n\txprt_clear_write_space_locked(xprt);\n\txprt_clear_congestion_window_wait_locked(xprt);\n\txprt_wake_pending_tasks(xprt, -ENOTCONN);\n\tspin_unlock(&xprt->transport_lock);\n}\nEXPORT_SYMBOL_GPL(xprt_disconnect_done);\n\n/**\n * xprt_force_disconnect - force a transport to disconnect\n * @xprt: transport to disconnect\n *\n */\nvoid xprt_force_disconnect(struct rpc_xprt *xprt)\n{\n\ttrace_xprt_disconnect_force(xprt);\n\n\t/* Don't race with the test_bit() in xprt_clear_locked() */\n\tspin_lock(&xprt->transport_lock);\n\tset_bit(XPRT_CLOSE_WAIT, &xprt->state);\n\t/* Try to schedule an autoclose RPC call */\n\tif (test_and_set_bit(XPRT_LOCKED, &xprt->state) == 0)\n\t\tqueue_work(xprtiod_workqueue, &xprt->task_cleanup);\n\telse if (xprt->snd_task)\n\t\trpc_wake_up_queued_task_set_status(&xprt->pending,\n\t\t\t\txprt->snd_task, -ENOTCONN);\n\tspin_unlock(&xprt->transport_lock);\n}\nEXPORT_SYMBOL_GPL(xprt_force_disconnect);\n\nstatic unsigned int\nxprt_connect_cookie(struct rpc_xprt *xprt)\n{\n\treturn READ_ONCE(xprt->connect_cookie);\n}\n\nstatic bool\nxprt_request_retransmit_after_disconnect(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\n\treturn req->rq_connect_cookie != xprt_connect_cookie(xprt) ||\n\t\t!xprt_connected(xprt);\n}\n\n/**\n * xprt_conditional_disconnect - force a transport to disconnect\n * @xprt: transport to disconnect\n * @cookie: 'connection cookie'\n *\n * This attempts to break the connection if and only if 'cookie' matches\n * the current transport 'connection cookie'. It ensures that we don't\n * try to break the connection more than once when we need to retransmit\n * a batch of RPC requests.\n *\n */\nvoid xprt_conditional_disconnect(struct rpc_xprt *xprt, unsigned int cookie)\n{\n\t/* Don't race with the test_bit() in xprt_clear_locked() */\n\tspin_lock(&xprt->transport_lock);\n\tif (cookie != xprt->connect_cookie)\n\t\tgoto out;\n\tif (test_bit(XPRT_CLOSING, &xprt->state))\n\t\tgoto out;\n\tset_bit(XPRT_CLOSE_WAIT, &xprt->state);\n\t/* Try to schedule an autoclose RPC call */\n\tif (test_and_set_bit(XPRT_LOCKED, &xprt->state) == 0)\n\t\tqueue_work(xprtiod_workqueue, &xprt->task_cleanup);\n\txprt_wake_pending_tasks(xprt, -EAGAIN);\nout:\n\tspin_unlock(&xprt->transport_lock);\n}\n\nstatic bool\nxprt_has_timer(const struct rpc_xprt *xprt)\n{\n\treturn xprt->idle_timeout != 0;\n}\n\nstatic void\nxprt_schedule_autodisconnect(struct rpc_xprt *xprt)\n\t__must_hold(&xprt->transport_lock)\n{\n\txprt->last_used = jiffies;\n\tif (RB_EMPTY_ROOT(&xprt->recv_queue) && xprt_has_timer(xprt))\n\t\tmod_timer(&xprt->timer, xprt->last_used + xprt->idle_timeout);\n}\n\nstatic void\nxprt_init_autodisconnect(struct timer_list *t)\n{\n\tstruct rpc_xprt *xprt = from_timer(xprt, t, timer);\n\n\tif (!RB_EMPTY_ROOT(&xprt->recv_queue))\n\t\treturn;\n\t/* Reset xprt->last_used to avoid connect/autodisconnect cycling */\n\txprt->last_used = jiffies;\n\tif (test_and_set_bit(XPRT_LOCKED, &xprt->state))\n\t\treturn;\n\tqueue_work(xprtiod_workqueue, &xprt->task_cleanup);\n}\n\nbool xprt_lock_connect(struct rpc_xprt *xprt,\n\t\tstruct rpc_task *task,\n\t\tvoid *cookie)\n{\n\tbool ret = false;\n\n\tspin_lock(&xprt->transport_lock);\n\tif (!test_bit(XPRT_LOCKED, &xprt->state))\n\t\tgoto out;\n\tif (xprt->snd_task != task)\n\t\tgoto out;\n\txprt->snd_task = cookie;\n\tret = true;\nout:\n\tspin_unlock(&xprt->transport_lock);\n\treturn ret;\n}\n\nvoid xprt_unlock_connect(struct rpc_xprt *xprt, void *cookie)\n{\n\tspin_lock(&xprt->transport_lock);\n\tif (xprt->snd_task != cookie)\n\t\tgoto out;\n\tif (!test_bit(XPRT_LOCKED, &xprt->state))\n\t\tgoto out;\n\txprt->snd_task =NULL;\n\txprt->ops->release_xprt(xprt, NULL);\n\txprt_schedule_autodisconnect(xprt);\nout:\n\tspin_unlock(&xprt->transport_lock);\n\twake_up_bit(&xprt->state, XPRT_LOCKED);\n}\n\n/**\n * xprt_connect - schedule a transport connect operation\n * @task: RPC task that is requesting the connect\n *\n */\nvoid xprt_connect(struct rpc_task *task)\n{\n\tstruct rpc_xprt\t*xprt = task->tk_rqstp->rq_xprt;\n\n\ttrace_xprt_connect(xprt);\n\n\tif (!xprt_bound(xprt)) {\n\t\ttask->tk_status = -EAGAIN;\n\t\treturn;\n\t}\n\tif (!xprt_lock_write(xprt, task))\n\t\treturn;\n\n\tif (test_and_clear_bit(XPRT_CLOSE_WAIT, &xprt->state)) {\n\t\ttrace_xprt_disconnect_cleanup(xprt);\n\t\txprt->ops->close(xprt);\n\t}\n\n\tif (!xprt_connected(xprt)) {\n\t\ttask->tk_rqstp->rq_connect_cookie = xprt->connect_cookie;\n\t\trpc_sleep_on_timeout(&xprt->pending, task, NULL,\n\t\t\t\txprt_request_timeout(task->tk_rqstp));\n\n\t\tif (test_bit(XPRT_CLOSING, &xprt->state))\n\t\t\treturn;\n\t\tif (xprt_test_and_set_connecting(xprt))\n\t\t\treturn;\n\t\t/* Race breaker */\n\t\tif (!xprt_connected(xprt)) {\n\t\t\txprt->stat.connect_start = jiffies;\n\t\t\txprt->ops->connect(xprt, task);\n\t\t} else {\n\t\t\txprt_clear_connecting(xprt);\n\t\t\ttask->tk_status = 0;\n\t\t\trpc_wake_up_queued_task(&xprt->pending, task);\n\t\t}\n\t}\n\txprt_release_write(xprt, task);\n}\n\n/**\n * xprt_reconnect_delay - compute the wait before scheduling a connect\n * @xprt: transport instance\n *\n */\nunsigned long xprt_reconnect_delay(const struct rpc_xprt *xprt)\n{\n\tunsigned long start, now = jiffies;\n\n\tstart = xprt->stat.connect_start + xprt->reestablish_timeout;\n\tif (time_after(start, now))\n\t\treturn start - now;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(xprt_reconnect_delay);\n\n/**\n * xprt_reconnect_backoff - compute the new re-establish timeout\n * @xprt: transport instance\n * @init_to: initial reestablish timeout\n *\n */\nvoid xprt_reconnect_backoff(struct rpc_xprt *xprt, unsigned long init_to)\n{\n\txprt->reestablish_timeout <<= 1;\n\tif (xprt->reestablish_timeout > xprt->max_reconnect_timeout)\n\t\txprt->reestablish_timeout = xprt->max_reconnect_timeout;\n\tif (xprt->reestablish_timeout < init_to)\n\t\txprt->reestablish_timeout = init_to;\n}\nEXPORT_SYMBOL_GPL(xprt_reconnect_backoff);\n\nenum xprt_xid_rb_cmp {\n\tXID_RB_EQUAL,\n\tXID_RB_LEFT,\n\tXID_RB_RIGHT,\n};\nstatic enum xprt_xid_rb_cmp\nxprt_xid_cmp(__be32 xid1, __be32 xid2)\n{\n\tif (xid1 == xid2)\n\t\treturn XID_RB_EQUAL;\n\tif ((__force u32)xid1 < (__force u32)xid2)\n\t\treturn XID_RB_LEFT;\n\treturn XID_RB_RIGHT;\n}\n\nstatic struct rpc_rqst *\nxprt_request_rb_find(struct rpc_xprt *xprt, __be32 xid)\n{\n\tstruct rb_node *n = xprt->recv_queue.rb_node;\n\tstruct rpc_rqst *req;\n\n\twhile (n != NULL) {\n\t\treq = rb_entry(n, struct rpc_rqst, rq_recv);\n\t\tswitch (xprt_xid_cmp(xid, req->rq_xid)) {\n\t\tcase XID_RB_LEFT:\n\t\t\tn = n->rb_left;\n\t\t\tbreak;\n\t\tcase XID_RB_RIGHT:\n\t\t\tn = n->rb_right;\n\t\t\tbreak;\n\t\tcase XID_RB_EQUAL:\n\t\t\treturn req;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic void\nxprt_request_rb_insert(struct rpc_xprt *xprt, struct rpc_rqst *new)\n{\n\tstruct rb_node **p = &xprt->recv_queue.rb_node;\n\tstruct rb_node *n = NULL;\n\tstruct rpc_rqst *req;\n\n\twhile (*p != NULL) {\n\t\tn = *p;\n\t\treq = rb_entry(n, struct rpc_rqst, rq_recv);\n\t\tswitch(xprt_xid_cmp(new->rq_xid, req->rq_xid)) {\n\t\tcase XID_RB_LEFT:\n\t\t\tp = &n->rb_left;\n\t\t\tbreak;\n\t\tcase XID_RB_RIGHT:\n\t\t\tp = &n->rb_right;\n\t\t\tbreak;\n\t\tcase XID_RB_EQUAL:\n\t\t\tWARN_ON_ONCE(new != req);\n\t\t\treturn;\n\t\t}\n\t}\n\trb_link_node(&new->rq_recv, n, p);\n\trb_insert_color(&new->rq_recv, &xprt->recv_queue);\n}\n\nstatic void\nxprt_request_rb_remove(struct rpc_xprt *xprt, struct rpc_rqst *req)\n{\n\trb_erase(&req->rq_recv, &xprt->recv_queue);\n}\n\n/**\n * xprt_lookup_rqst - find an RPC request corresponding to an XID\n * @xprt: transport on which the original request was transmitted\n * @xid: RPC XID of incoming reply\n *\n * Caller holds xprt->queue_lock.\n */\nstruct rpc_rqst *xprt_lookup_rqst(struct rpc_xprt *xprt, __be32 xid)\n{\n\tstruct rpc_rqst *entry;\n\n\tentry = xprt_request_rb_find(xprt, xid);\n\tif (entry != NULL) {\n\t\ttrace_xprt_lookup_rqst(xprt, xid, 0);\n\t\tentry->rq_rtt = ktime_sub(ktime_get(), entry->rq_xtime);\n\t\treturn entry;\n\t}\n\n\tdprintk(\"RPC:       xprt_lookup_rqst did not find xid %08x\\n\",\n\t\t\tntohl(xid));\n\ttrace_xprt_lookup_rqst(xprt, xid, -ENOENT);\n\txprt->stat.bad_xids++;\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(xprt_lookup_rqst);\n\nstatic bool\nxprt_is_pinned_rqst(struct rpc_rqst *req)\n{\n\treturn atomic_read(&req->rq_pin) != 0;\n}\n\n/**\n * xprt_pin_rqst - Pin a request on the transport receive list\n * @req: Request to pin\n *\n * Caller must ensure this is atomic with the call to xprt_lookup_rqst()\n * so should be holding xprt->queue_lock.\n */\nvoid xprt_pin_rqst(struct rpc_rqst *req)\n{\n\tatomic_inc(&req->rq_pin);\n}\nEXPORT_SYMBOL_GPL(xprt_pin_rqst);\n\n/**\n * xprt_unpin_rqst - Unpin a request on the transport receive list\n * @req: Request to pin\n *\n * Caller should be holding xprt->queue_lock.\n */\nvoid xprt_unpin_rqst(struct rpc_rqst *req)\n{\n\tif (!test_bit(RPC_TASK_MSG_PIN_WAIT, &req->rq_task->tk_runstate)) {\n\t\tatomic_dec(&req->rq_pin);\n\t\treturn;\n\t}\n\tif (atomic_dec_and_test(&req->rq_pin))\n\t\twake_up_var(&req->rq_pin);\n}\nEXPORT_SYMBOL_GPL(xprt_unpin_rqst);\n\nstatic void xprt_wait_on_pinned_rqst(struct rpc_rqst *req)\n{\n\twait_var_event(&req->rq_pin, !xprt_is_pinned_rqst(req));\n}\n\nstatic bool\nxprt_request_data_received(struct rpc_task *task)\n{\n\treturn !test_bit(RPC_TASK_NEED_RECV, &task->tk_runstate) &&\n\t\tREAD_ONCE(task->tk_rqstp->rq_reply_bytes_recvd) != 0;\n}\n\nstatic bool\nxprt_request_need_enqueue_receive(struct rpc_task *task, struct rpc_rqst *req)\n{\n\treturn !test_bit(RPC_TASK_NEED_RECV, &task->tk_runstate) &&\n\t\tREAD_ONCE(task->tk_rqstp->rq_reply_bytes_recvd) == 0;\n}\n\n/**\n * xprt_request_enqueue_receive - Add an request to the receive queue\n * @task: RPC task\n *\n */\nvoid\nxprt_request_enqueue_receive(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\n\tif (!xprt_request_need_enqueue_receive(task, req))\n\t\treturn;\n\n\txprt_request_prepare(task->tk_rqstp);\n\tspin_lock(&xprt->queue_lock);\n\n\t/* Update the softirq receive buffer */\n\tmemcpy(&req->rq_private_buf, &req->rq_rcv_buf,\n\t\t\tsizeof(req->rq_private_buf));\n\n\t/* Add request to the receive list */\n\txprt_request_rb_insert(xprt, req);\n\tset_bit(RPC_TASK_NEED_RECV, &task->tk_runstate);\n\tspin_unlock(&xprt->queue_lock);\n\n\t/* Turn off autodisconnect */\n\tdel_singleshot_timer_sync(&xprt->timer);\n}\n\n/**\n * xprt_request_dequeue_receive_locked - Remove a request from the receive queue\n * @task: RPC task\n *\n * Caller must hold xprt->queue_lock.\n */\nstatic void\nxprt_request_dequeue_receive_locked(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\n\tif (test_and_clear_bit(RPC_TASK_NEED_RECV, &task->tk_runstate))\n\t\txprt_request_rb_remove(req->rq_xprt, req);\n}\n\n/**\n * xprt_update_rtt - Update RPC RTT statistics\n * @task: RPC request that recently completed\n *\n * Caller holds xprt->queue_lock.\n */\nvoid xprt_update_rtt(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\tstruct rpc_rtt *rtt = task->tk_client->cl_rtt;\n\tunsigned int timer = task->tk_msg.rpc_proc->p_timer;\n\tlong m = usecs_to_jiffies(ktime_to_us(req->rq_rtt));\n\n\tif (timer) {\n\t\tif (req->rq_ntrans == 1)\n\t\t\trpc_update_rtt(rtt, timer, m);\n\t\trpc_set_timeo(rtt, timer, req->rq_ntrans - 1);\n\t}\n}\nEXPORT_SYMBOL_GPL(xprt_update_rtt);\n\n/**\n * xprt_complete_rqst - called when reply processing is complete\n * @task: RPC request that recently completed\n * @copied: actual number of bytes received from the transport\n *\n * Caller holds xprt->queue_lock.\n */\nvoid xprt_complete_rqst(struct rpc_task *task, int copied)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\n\txprt->stat.recvs++;\n\n\treq->rq_private_buf.len = copied;\n\t/* Ensure all writes are done before we update */\n\t/* req->rq_reply_bytes_recvd */\n\tsmp_wmb();\n\treq->rq_reply_bytes_recvd = copied;\n\txprt_request_dequeue_receive_locked(task);\n\trpc_wake_up_queued_task(&xprt->pending, task);\n}\nEXPORT_SYMBOL_GPL(xprt_complete_rqst);\n\nstatic void xprt_timer(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\n\tif (task->tk_status != -ETIMEDOUT)\n\t\treturn;\n\n\ttrace_xprt_timer(xprt, req->rq_xid, task->tk_status);\n\tif (!req->rq_reply_bytes_recvd) {\n\t\tif (xprt->ops->timer)\n\t\t\txprt->ops->timer(xprt, task);\n\t} else\n\t\ttask->tk_status = 0;\n}\n\n/**\n * xprt_wait_for_reply_request_def - wait for reply\n * @task: pointer to rpc_task\n *\n * Set a request's retransmit timeout based on the transport's\n * default timeout parameters.  Used by transports that don't adjust\n * the retransmit timeout based on round-trip time estimation,\n * and put the task to sleep on the pending queue.\n */\nvoid xprt_wait_for_reply_request_def(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\n\trpc_sleep_on_timeout(&req->rq_xprt->pending, task, xprt_timer,\n\t\t\txprt_request_timeout(req));\n}\nEXPORT_SYMBOL_GPL(xprt_wait_for_reply_request_def);\n\n/**\n * xprt_wait_for_reply_request_rtt - wait for reply using RTT estimator\n * @task: pointer to rpc_task\n *\n * Set a request's retransmit timeout using the RTT estimator,\n * and put the task to sleep on the pending queue.\n */\nvoid xprt_wait_for_reply_request_rtt(struct rpc_task *task)\n{\n\tint timer = task->tk_msg.rpc_proc->p_timer;\n\tstruct rpc_clnt *clnt = task->tk_client;\n\tstruct rpc_rtt *rtt = clnt->cl_rtt;\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\tunsigned long max_timeout = clnt->cl_timeout->to_maxval;\n\tunsigned long timeout;\n\n\ttimeout = rpc_calc_rto(rtt, timer);\n\ttimeout <<= rpc_ntimeo(rtt, timer) + req->rq_retries;\n\tif (timeout > max_timeout || timeout == 0)\n\t\ttimeout = max_timeout;\n\trpc_sleep_on_timeout(&req->rq_xprt->pending, task, xprt_timer,\n\t\t\tjiffies + timeout);\n}\nEXPORT_SYMBOL_GPL(xprt_wait_for_reply_request_rtt);\n\n/**\n * xprt_request_wait_receive - wait for the reply to an RPC request\n * @task: RPC task about to send a request\n *\n */\nvoid xprt_request_wait_receive(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\n\tif (!test_bit(RPC_TASK_NEED_RECV, &task->tk_runstate))\n\t\treturn;\n\t/*\n\t * Sleep on the pending queue if we're expecting a reply.\n\t * The spinlock ensures atomicity between the test of\n\t * req->rq_reply_bytes_recvd, and the call to rpc_sleep_on().\n\t */\n\tspin_lock(&xprt->queue_lock);\n\tif (test_bit(RPC_TASK_NEED_RECV, &task->tk_runstate)) {\n\t\txprt->ops->wait_for_reply_request(task);\n\t\t/*\n\t\t * Send an extra queue wakeup call if the\n\t\t * connection was dropped in case the call to\n\t\t * rpc_sleep_on() raced.\n\t\t */\n\t\tif (xprt_request_retransmit_after_disconnect(task))\n\t\t\trpc_wake_up_queued_task_set_status(&xprt->pending,\n\t\t\t\t\ttask, -ENOTCONN);\n\t}\n\tspin_unlock(&xprt->queue_lock);\n}\n\nstatic bool\nxprt_request_need_enqueue_transmit(struct rpc_task *task, struct rpc_rqst *req)\n{\n\treturn !test_bit(RPC_TASK_NEED_XMIT, &task->tk_runstate);\n}\n\n/**\n * xprt_request_enqueue_transmit - queue a task for transmission\n * @task: pointer to rpc_task\n *\n * Add a task to the transmission queue.\n */\nvoid\nxprt_request_enqueue_transmit(struct rpc_task *task)\n{\n\tstruct rpc_rqst *pos, *req = task->tk_rqstp;\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\n\tif (xprt_request_need_enqueue_transmit(task, req)) {\n\t\treq->rq_bytes_sent = 0;\n\t\tspin_lock(&xprt->queue_lock);\n\t\t/*\n\t\t * Requests that carry congestion control credits are added\n\t\t * to the head of the list to avoid starvation issues.\n\t\t */\n\t\tif (req->rq_cong) {\n\t\t\txprt_clear_congestion_window_wait(xprt);\n\t\t\tlist_for_each_entry(pos, &xprt->xmit_queue, rq_xmit) {\n\t\t\t\tif (pos->rq_cong)\n\t\t\t\t\tcontinue;\n\t\t\t\t/* Note: req is added _before_ pos */\n\t\t\t\tlist_add_tail(&req->rq_xmit, &pos->rq_xmit);\n\t\t\t\tINIT_LIST_HEAD(&req->rq_xmit2);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else if (RPC_IS_SWAPPER(task)) {\n\t\t\tlist_for_each_entry(pos, &xprt->xmit_queue, rq_xmit) {\n\t\t\t\tif (pos->rq_cong || pos->rq_bytes_sent)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (RPC_IS_SWAPPER(pos->rq_task))\n\t\t\t\t\tcontinue;\n\t\t\t\t/* Note: req is added _before_ pos */\n\t\t\t\tlist_add_tail(&req->rq_xmit, &pos->rq_xmit);\n\t\t\t\tINIT_LIST_HEAD(&req->rq_xmit2);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else if (!req->rq_seqno) {\n\t\t\tlist_for_each_entry(pos, &xprt->xmit_queue, rq_xmit) {\n\t\t\t\tif (pos->rq_task->tk_owner != task->tk_owner)\n\t\t\t\t\tcontinue;\n\t\t\t\tlist_add_tail(&req->rq_xmit2, &pos->rq_xmit2);\n\t\t\t\tINIT_LIST_HEAD(&req->rq_xmit);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tlist_add_tail(&req->rq_xmit, &xprt->xmit_queue);\n\t\tINIT_LIST_HEAD(&req->rq_xmit2);\nout:\n\t\tset_bit(RPC_TASK_NEED_XMIT, &task->tk_runstate);\n\t\tspin_unlock(&xprt->queue_lock);\n\t}\n}\n\n/**\n * xprt_request_dequeue_transmit_locked - remove a task from the transmission queue\n * @task: pointer to rpc_task\n *\n * Remove a task from the transmission queue\n * Caller must hold xprt->queue_lock\n */\nstatic void\nxprt_request_dequeue_transmit_locked(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\n\tif (!test_and_clear_bit(RPC_TASK_NEED_XMIT, &task->tk_runstate))\n\t\treturn;\n\tif (!list_empty(&req->rq_xmit)) {\n\t\tlist_del(&req->rq_xmit);\n\t\tif (!list_empty(&req->rq_xmit2)) {\n\t\t\tstruct rpc_rqst *next = list_first_entry(&req->rq_xmit2,\n\t\t\t\t\tstruct rpc_rqst, rq_xmit2);\n\t\t\tlist_del(&req->rq_xmit2);\n\t\t\tlist_add_tail(&next->rq_xmit, &next->rq_xprt->xmit_queue);\n\t\t}\n\t} else\n\t\tlist_del(&req->rq_xmit2);\n}\n\n/**\n * xprt_request_dequeue_transmit - remove a task from the transmission queue\n * @task: pointer to rpc_task\n *\n * Remove a task from the transmission queue\n */\nstatic void\nxprt_request_dequeue_transmit(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\n\tspin_lock(&xprt->queue_lock);\n\txprt_request_dequeue_transmit_locked(task);\n\tspin_unlock(&xprt->queue_lock);\n}\n\n/**\n * xprt_request_dequeue_xprt - remove a task from the transmit+receive queue\n * @task: pointer to rpc_task\n *\n * Remove a task from the transmit and receive queues, and ensure that\n * it is not pinned by the receive work item.\n */\nvoid\nxprt_request_dequeue_xprt(struct rpc_task *task)\n{\n\tstruct rpc_rqst\t*req = task->tk_rqstp;\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\n\tif (test_bit(RPC_TASK_NEED_XMIT, &task->tk_runstate) ||\n\t    test_bit(RPC_TASK_NEED_RECV, &task->tk_runstate) ||\n\t    xprt_is_pinned_rqst(req)) {\n\t\tspin_lock(&xprt->queue_lock);\n\t\txprt_request_dequeue_transmit_locked(task);\n\t\txprt_request_dequeue_receive_locked(task);\n\t\twhile (xprt_is_pinned_rqst(req)) {\n\t\t\tset_bit(RPC_TASK_MSG_PIN_WAIT, &task->tk_runstate);\n\t\t\tspin_unlock(&xprt->queue_lock);\n\t\t\txprt_wait_on_pinned_rqst(req);\n\t\t\tspin_lock(&xprt->queue_lock);\n\t\t\tclear_bit(RPC_TASK_MSG_PIN_WAIT, &task->tk_runstate);\n\t\t}\n\t\tspin_unlock(&xprt->queue_lock);\n\t}\n}\n\n/**\n * xprt_request_prepare - prepare an encoded request for transport\n * @req: pointer to rpc_rqst\n *\n * Calls into the transport layer to do whatever is needed to prepare\n * the request for transmission or receive.\n */\nvoid\nxprt_request_prepare(struct rpc_rqst *req)\n{\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\n\tif (xprt->ops->prepare_request)\n\t\txprt->ops->prepare_request(req);\n}\n\n/**\n * xprt_request_need_retransmit - Test if a task needs retransmission\n * @task: pointer to rpc_task\n *\n * Test for whether a connection breakage requires the task to retransmit\n */\nbool\nxprt_request_need_retransmit(struct rpc_task *task)\n{\n\treturn xprt_request_retransmit_after_disconnect(task);\n}\n\n/**\n * xprt_prepare_transmit - reserve the transport before sending a request\n * @task: RPC task about to send a request\n *\n */\nbool xprt_prepare_transmit(struct rpc_task *task)\n{\n\tstruct rpc_rqst\t*req = task->tk_rqstp;\n\tstruct rpc_xprt\t*xprt = req->rq_xprt;\n\n\tif (!xprt_lock_write(xprt, task)) {\n\t\ttrace_xprt_transmit_queued(xprt, task);\n\n\t\t/* Race breaker: someone may have transmitted us */\n\t\tif (!test_bit(RPC_TASK_NEED_XMIT, &task->tk_runstate))\n\t\t\trpc_wake_up_queued_task_set_status(&xprt->sending,\n\t\t\t\t\ttask, 0);\n\t\treturn false;\n\n\t}\n\treturn true;\n}\n\nvoid xprt_end_transmit(struct rpc_task *task)\n{\n\txprt_release_write(task->tk_rqstp->rq_xprt, task);\n}\n\n/**\n * xprt_request_transmit - send an RPC request on a transport\n * @req: pointer to request to transmit\n * @snd_task: RPC task that owns the transport lock\n *\n * This performs the transmission of a single request.\n * Note that if the request is not the same as snd_task, then it\n * does need to be pinned.\n * Returns '0' on success.\n */\nstatic int\nxprt_request_transmit(struct rpc_rqst *req, struct rpc_task *snd_task)\n{\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\tstruct rpc_task *task = req->rq_task;\n\tunsigned int connect_cookie;\n\tint is_retrans = RPC_WAS_SENT(task);\n\tint status;\n\n\tif (!req->rq_bytes_sent) {\n\t\tif (xprt_request_data_received(task)) {\n\t\t\tstatus = 0;\n\t\t\tgoto out_dequeue;\n\t\t}\n\t\t/* Verify that our message lies in the RPCSEC_GSS window */\n\t\tif (rpcauth_xmit_need_reencode(task)) {\n\t\t\tstatus = -EBADMSG;\n\t\t\tgoto out_dequeue;\n\t\t}\n\t\tif (RPC_SIGNALLED(task)) {\n\t\t\tstatus = -ERESTARTSYS;\n\t\t\tgoto out_dequeue;\n\t\t}\n\t}\n\n\t/*\n\t * Update req->rq_ntrans before transmitting to avoid races with\n\t * xprt_update_rtt(), which needs to know that it is recording a\n\t * reply to the first transmission.\n\t */\n\treq->rq_ntrans++;\n\n\ttrace_rpc_xdr_sendto(task, &req->rq_snd_buf);\n\tconnect_cookie = xprt->connect_cookie;\n\tstatus = xprt->ops->send_request(req);\n\tif (status != 0) {\n\t\treq->rq_ntrans--;\n\t\ttrace_xprt_transmit(req, status);\n\t\treturn status;\n\t}\n\n\tif (is_retrans)\n\t\ttask->tk_client->cl_stats->rpcretrans++;\n\n\txprt_inject_disconnect(xprt);\n\n\ttask->tk_flags |= RPC_TASK_SENT;\n\tspin_lock(&xprt->transport_lock);\n\n\txprt->stat.sends++;\n\txprt->stat.req_u += xprt->stat.sends - xprt->stat.recvs;\n\txprt->stat.bklog_u += xprt->backlog.qlen;\n\txprt->stat.sending_u += xprt->sending.qlen;\n\txprt->stat.pending_u += xprt->pending.qlen;\n\tspin_unlock(&xprt->transport_lock);\n\n\treq->rq_connect_cookie = connect_cookie;\nout_dequeue:\n\ttrace_xprt_transmit(req, status);\n\txprt_request_dequeue_transmit(task);\n\trpc_wake_up_queued_task_set_status(&xprt->sending, task, status);\n\treturn status;\n}\n\n/**\n * xprt_transmit - send an RPC request on a transport\n * @task: controlling RPC task\n *\n * Attempts to drain the transmit queue. On exit, either the transport\n * signalled an error that needs to be handled before transmission can\n * resume, or @task finished transmitting, and detected that it already\n * received a reply.\n */\nvoid\nxprt_transmit(struct rpc_task *task)\n{\n\tstruct rpc_rqst *next, *req = task->tk_rqstp;\n\tstruct rpc_xprt\t*xprt = req->rq_xprt;\n\tint counter, status;\n\n\tspin_lock(&xprt->queue_lock);\n\tcounter = 0;\n\twhile (!list_empty(&xprt->xmit_queue)) {\n\t\tif (++counter == 20)\n\t\t\tbreak;\n\t\tnext = list_first_entry(&xprt->xmit_queue,\n\t\t\t\tstruct rpc_rqst, rq_xmit);\n\t\txprt_pin_rqst(next);\n\t\tspin_unlock(&xprt->queue_lock);\n\t\tstatus = xprt_request_transmit(next, task);\n\t\tif (status == -EBADMSG && next != req)\n\t\t\tstatus = 0;\n\t\tspin_lock(&xprt->queue_lock);\n\t\txprt_unpin_rqst(next);\n\t\tif (status == 0) {\n\t\t\tif (!xprt_request_data_received(task) ||\n\t\t\t    test_bit(RPC_TASK_NEED_XMIT, &task->tk_runstate))\n\t\t\t\tcontinue;\n\t\t} else if (test_bit(RPC_TASK_NEED_XMIT, &task->tk_runstate))\n\t\t\ttask->tk_status = status;\n\t\tbreak;\n\t}\n\tspin_unlock(&xprt->queue_lock);\n}\n\nstatic void xprt_add_backlog(struct rpc_xprt *xprt, struct rpc_task *task)\n{\n\tset_bit(XPRT_CONGESTED, &xprt->state);\n\trpc_sleep_on(&xprt->backlog, task, NULL);\n}\n\nstatic void xprt_wake_up_backlog(struct rpc_xprt *xprt)\n{\n\tif (rpc_wake_up_next(&xprt->backlog) == NULL)\n\t\tclear_bit(XPRT_CONGESTED, &xprt->state);\n}\n\nstatic bool xprt_throttle_congested(struct rpc_xprt *xprt, struct rpc_task *task)\n{\n\tbool ret = false;\n\n\tif (!test_bit(XPRT_CONGESTED, &xprt->state))\n\t\tgoto out;\n\tspin_lock(&xprt->reserve_lock);\n\tif (test_bit(XPRT_CONGESTED, &xprt->state)) {\n\t\trpc_sleep_on(&xprt->backlog, task, NULL);\n\t\tret = true;\n\t}\n\tspin_unlock(&xprt->reserve_lock);\nout:\n\treturn ret;\n}\n\nstatic struct rpc_rqst *xprt_dynamic_alloc_slot(struct rpc_xprt *xprt)\n{\n\tstruct rpc_rqst *req = ERR_PTR(-EAGAIN);\n\n\tif (xprt->num_reqs >= xprt->max_reqs)\n\t\tgoto out;\n\t++xprt->num_reqs;\n\tspin_unlock(&xprt->reserve_lock);\n\treq = kzalloc(sizeof(struct rpc_rqst), GFP_NOFS);\n\tspin_lock(&xprt->reserve_lock);\n\tif (req != NULL)\n\t\tgoto out;\n\t--xprt->num_reqs;\n\treq = ERR_PTR(-ENOMEM);\nout:\n\treturn req;\n}\n\nstatic bool xprt_dynamic_free_slot(struct rpc_xprt *xprt, struct rpc_rqst *req)\n{\n\tif (xprt->num_reqs > xprt->min_reqs) {\n\t\t--xprt->num_reqs;\n\t\tkfree(req);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nvoid xprt_alloc_slot(struct rpc_xprt *xprt, struct rpc_task *task)\n{\n\tstruct rpc_rqst *req;\n\n\tspin_lock(&xprt->reserve_lock);\n\tif (!list_empty(&xprt->free)) {\n\t\treq = list_entry(xprt->free.next, struct rpc_rqst, rq_list);\n\t\tlist_del(&req->rq_list);\n\t\tgoto out_init_req;\n\t}\n\treq = xprt_dynamic_alloc_slot(xprt);\n\tif (!IS_ERR(req))\n\t\tgoto out_init_req;\n\tswitch (PTR_ERR(req)) {\n\tcase -ENOMEM:\n\t\tdprintk(\"RPC:       dynamic allocation of request slot \"\n\t\t\t\t\"failed! Retrying\\n\");\n\t\ttask->tk_status = -ENOMEM;\n\t\tbreak;\n\tcase -EAGAIN:\n\t\txprt_add_backlog(xprt, task);\n\t\tdprintk(\"RPC:       waiting for request slot\\n\");\n\t\tfallthrough;\n\tdefault:\n\t\ttask->tk_status = -EAGAIN;\n\t}\n\tspin_unlock(&xprt->reserve_lock);\n\treturn;\nout_init_req:\n\txprt->stat.max_slots = max_t(unsigned int, xprt->stat.max_slots,\n\t\t\t\t     xprt->num_reqs);\n\tspin_unlock(&xprt->reserve_lock);\n\n\ttask->tk_status = 0;\n\ttask->tk_rqstp = req;\n}\nEXPORT_SYMBOL_GPL(xprt_alloc_slot);\n\nvoid xprt_free_slot(struct rpc_xprt *xprt, struct rpc_rqst *req)\n{\n\tspin_lock(&xprt->reserve_lock);\n\tif (!xprt_dynamic_free_slot(xprt, req)) {\n\t\tmemset(req, 0, sizeof(*req));\t/* mark unused */\n\t\tlist_add(&req->rq_list, &xprt->free);\n\t}\n\txprt_wake_up_backlog(xprt);\n\tspin_unlock(&xprt->reserve_lock);\n}\nEXPORT_SYMBOL_GPL(xprt_free_slot);\n\nstatic void xprt_free_all_slots(struct rpc_xprt *xprt)\n{\n\tstruct rpc_rqst *req;\n\twhile (!list_empty(&xprt->free)) {\n\t\treq = list_first_entry(&xprt->free, struct rpc_rqst, rq_list);\n\t\tlist_del(&req->rq_list);\n\t\tkfree(req);\n\t}\n}\n\nstruct rpc_xprt *xprt_alloc(struct net *net, size_t size,\n\t\tunsigned int num_prealloc,\n\t\tunsigned int max_alloc)\n{\n\tstruct rpc_xprt *xprt;\n\tstruct rpc_rqst *req;\n\tint i;\n\n\txprt = kzalloc(size, GFP_KERNEL);\n\tif (xprt == NULL)\n\t\tgoto out;\n\n\txprt_init(xprt, net);\n\n\tfor (i = 0; i < num_prealloc; i++) {\n\t\treq = kzalloc(sizeof(struct rpc_rqst), GFP_KERNEL);\n\t\tif (!req)\n\t\t\tgoto out_free;\n\t\tlist_add(&req->rq_list, &xprt->free);\n\t}\n\tif (max_alloc > num_prealloc)\n\t\txprt->max_reqs = max_alloc;\n\telse\n\t\txprt->max_reqs = num_prealloc;\n\txprt->min_reqs = num_prealloc;\n\txprt->num_reqs = num_prealloc;\n\n\treturn xprt;\n\nout_free:\n\txprt_free(xprt);\nout:\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(xprt_alloc);\n\nvoid xprt_free(struct rpc_xprt *xprt)\n{\n\tput_net(xprt->xprt_net);\n\txprt_free_all_slots(xprt);\n\tkfree_rcu(xprt, rcu);\n}\nEXPORT_SYMBOL_GPL(xprt_free);\n\nstatic void\nxprt_init_connect_cookie(struct rpc_rqst *req, struct rpc_xprt *xprt)\n{\n\treq->rq_connect_cookie = xprt_connect_cookie(xprt) - 1;\n}\n\nstatic __be32\nxprt_alloc_xid(struct rpc_xprt *xprt)\n{\n\t__be32 xid;\n\n\tspin_lock(&xprt->reserve_lock);\n\txid = (__force __be32)xprt->xid++;\n\tspin_unlock(&xprt->reserve_lock);\n\treturn xid;\n}\n\nstatic void\nxprt_init_xid(struct rpc_xprt *xprt)\n{\n\txprt->xid = prandom_u32();\n}\n\nstatic void\nxprt_request_init(struct rpc_task *task)\n{\n\tstruct rpc_xprt *xprt = task->tk_xprt;\n\tstruct rpc_rqst\t*req = task->tk_rqstp;\n\n\treq->rq_task\t= task;\n\treq->rq_xprt    = xprt;\n\treq->rq_buffer  = NULL;\n\treq->rq_xid\t= xprt_alloc_xid(xprt);\n\txprt_init_connect_cookie(req, xprt);\n\treq->rq_snd_buf.len = 0;\n\treq->rq_snd_buf.buflen = 0;\n\treq->rq_rcv_buf.len = 0;\n\treq->rq_rcv_buf.buflen = 0;\n\treq->rq_snd_buf.bvec = NULL;\n\treq->rq_rcv_buf.bvec = NULL;\n\treq->rq_release_snd_buf = NULL;\n\txprt_init_majortimeo(task, req);\n\n\ttrace_xprt_reserve(req);\n}\n\nstatic void\nxprt_do_reserve(struct rpc_xprt *xprt, struct rpc_task *task)\n{\n\txprt->ops->alloc_slot(xprt, task);\n\tif (task->tk_rqstp != NULL)\n\t\txprt_request_init(task);\n}\n\n/**\n * xprt_reserve - allocate an RPC request slot\n * @task: RPC task requesting a slot allocation\n *\n * If the transport is marked as being congested, or if no more\n * slots are available, place the task on the transport's\n * backlog queue.\n */\nvoid xprt_reserve(struct rpc_task *task)\n{\n\tstruct rpc_xprt *xprt = task->tk_xprt;\n\n\ttask->tk_status = 0;\n\tif (task->tk_rqstp != NULL)\n\t\treturn;\n\n\ttask->tk_status = -EAGAIN;\n\tif (!xprt_throttle_congested(xprt, task))\n\t\txprt_do_reserve(xprt, task);\n}\n\n/**\n * xprt_retry_reserve - allocate an RPC request slot\n * @task: RPC task requesting a slot allocation\n *\n * If no more slots are available, place the task on the transport's\n * backlog queue.\n * Note that the only difference with xprt_reserve is that we now\n * ignore the value of the XPRT_CONGESTED flag.\n */\nvoid xprt_retry_reserve(struct rpc_task *task)\n{\n\tstruct rpc_xprt *xprt = task->tk_xprt;\n\n\ttask->tk_status = 0;\n\tif (task->tk_rqstp != NULL)\n\t\treturn;\n\n\ttask->tk_status = -EAGAIN;\n\txprt_do_reserve(xprt, task);\n}\n\n/**\n * xprt_release - release an RPC request slot\n * @task: task which is finished with the slot\n *\n */\nvoid xprt_release(struct rpc_task *task)\n{\n\tstruct rpc_xprt\t*xprt;\n\tstruct rpc_rqst\t*req = task->tk_rqstp;\n\n\tif (req == NULL) {\n\t\tif (task->tk_client) {\n\t\t\txprt = task->tk_xprt;\n\t\t\txprt_release_write(xprt, task);\n\t\t}\n\t\treturn;\n\t}\n\n\txprt = req->rq_xprt;\n\txprt_request_dequeue_xprt(task);\n\tspin_lock(&xprt->transport_lock);\n\txprt->ops->release_xprt(xprt, task);\n\tif (xprt->ops->release_request)\n\t\txprt->ops->release_request(task);\n\txprt_schedule_autodisconnect(xprt);\n\tspin_unlock(&xprt->transport_lock);\n\tif (req->rq_buffer)\n\t\txprt->ops->buf_free(task);\n\txprt_inject_disconnect(xprt);\n\txdr_free_bvec(&req->rq_rcv_buf);\n\txdr_free_bvec(&req->rq_snd_buf);\n\tif (req->rq_cred != NULL)\n\t\tput_rpccred(req->rq_cred);\n\ttask->tk_rqstp = NULL;\n\tif (req->rq_release_snd_buf)\n\t\treq->rq_release_snd_buf(req);\n\n\tif (likely(!bc_prealloc(req)))\n\t\txprt->ops->free_slot(xprt, req);\n\telse\n\t\txprt_free_bc_request(req);\n}\n\n#ifdef CONFIG_SUNRPC_BACKCHANNEL\nvoid\nxprt_init_bc_request(struct rpc_rqst *req, struct rpc_task *task)\n{\n\tstruct xdr_buf *xbufp = &req->rq_snd_buf;\n\n\ttask->tk_rqstp = req;\n\treq->rq_task = task;\n\txprt_init_connect_cookie(req, req->rq_xprt);\n\t/*\n\t * Set up the xdr_buf length.\n\t * This also indicates that the buffer is XDR encoded already.\n\t */\n\txbufp->len = xbufp->head[0].iov_len + xbufp->page_len +\n\t\txbufp->tail[0].iov_len;\n}\n#endif\n\nstatic void xprt_init(struct rpc_xprt *xprt, struct net *net)\n{\n\tkref_init(&xprt->kref);\n\n\tspin_lock_init(&xprt->transport_lock);\n\tspin_lock_init(&xprt->reserve_lock);\n\tspin_lock_init(&xprt->queue_lock);\n\n\tINIT_LIST_HEAD(&xprt->free);\n\txprt->recv_queue = RB_ROOT;\n\tINIT_LIST_HEAD(&xprt->xmit_queue);\n#if defined(CONFIG_SUNRPC_BACKCHANNEL)\n\tspin_lock_init(&xprt->bc_pa_lock);\n\tINIT_LIST_HEAD(&xprt->bc_pa_list);\n#endif /* CONFIG_SUNRPC_BACKCHANNEL */\n\tINIT_LIST_HEAD(&xprt->xprt_switch);\n\n\txprt->last_used = jiffies;\n\txprt->cwnd = RPC_INITCWND;\n\txprt->bind_index = 0;\n\n\trpc_init_wait_queue(&xprt->binding, \"xprt_binding\");\n\trpc_init_wait_queue(&xprt->pending, \"xprt_pending\");\n\trpc_init_wait_queue(&xprt->sending, \"xprt_sending\");\n\trpc_init_priority_wait_queue(&xprt->backlog, \"xprt_backlog\");\n\n\txprt_init_xid(xprt);\n\n\txprt->xprt_net = get_net(net);\n}\n\n/**\n * xprt_create_transport - create an RPC transport\n * @args: rpc transport creation arguments\n *\n */\nstruct rpc_xprt *xprt_create_transport(struct xprt_create *args)\n{\n\tstruct rpc_xprt\t*xprt;\n\tstruct xprt_class *t;\n\n\tspin_lock(&xprt_list_lock);\n\tlist_for_each_entry(t, &xprt_list, list) {\n\t\tif (t->ident == args->ident) {\n\t\t\tspin_unlock(&xprt_list_lock);\n\t\t\tgoto found;\n\t\t}\n\t}\n\tspin_unlock(&xprt_list_lock);\n\tdprintk(\"RPC: transport (%d) not supported\\n\", args->ident);\n\treturn ERR_PTR(-EIO);\n\nfound:\n\txprt = t->setup(args);\n\tif (IS_ERR(xprt))\n\t\tgoto out;\n\tif (args->flags & XPRT_CREATE_NO_IDLE_TIMEOUT)\n\t\txprt->idle_timeout = 0;\n\tINIT_WORK(&xprt->task_cleanup, xprt_autoclose);\n\tif (xprt_has_timer(xprt))\n\t\ttimer_setup(&xprt->timer, xprt_init_autodisconnect, 0);\n\telse\n\t\ttimer_setup(&xprt->timer, NULL, 0);\n\n\tif (strlen(args->servername) > RPC_MAXNETNAMELEN) {\n\t\txprt_destroy(xprt);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\txprt->servername = kstrdup(args->servername, GFP_KERNEL);\n\tif (xprt->servername == NULL) {\n\t\txprt_destroy(xprt);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\trpc_xprt_debugfs_register(xprt);\n\n\ttrace_xprt_create(xprt);\nout:\n\treturn xprt;\n}\n\nstatic void xprt_destroy_cb(struct work_struct *work)\n{\n\tstruct rpc_xprt *xprt =\n\t\tcontainer_of(work, struct rpc_xprt, task_cleanup);\n\n\ttrace_xprt_destroy(xprt);\n\n\trpc_xprt_debugfs_unregister(xprt);\n\trpc_destroy_wait_queue(&xprt->binding);\n\trpc_destroy_wait_queue(&xprt->pending);\n\trpc_destroy_wait_queue(&xprt->sending);\n\trpc_destroy_wait_queue(&xprt->backlog);\n\tkfree(xprt->servername);\n\t/*\n\t * Destroy any existing back channel\n\t */\n\txprt_destroy_backchannel(xprt, UINT_MAX);\n\n\t/*\n\t * Tear down transport state and free the rpc_xprt\n\t */\n\txprt->ops->destroy(xprt);\n}\n\n/**\n * xprt_destroy - destroy an RPC transport, killing off all requests.\n * @xprt: transport to destroy\n *\n */\nstatic void xprt_destroy(struct rpc_xprt *xprt)\n{\n\t/*\n\t * Exclude transport connect/disconnect handlers and autoclose\n\t */\n\twait_on_bit_lock(&xprt->state, XPRT_LOCKED, TASK_UNINTERRUPTIBLE);\n\n\tdel_timer_sync(&xprt->timer);\n\n\t/*\n\t * Destroy sockets etc from the system workqueue so they can\n\t * safely flush receive work running on rpciod.\n\t */\n\tINIT_WORK(&xprt->task_cleanup, xprt_destroy_cb);\n\tschedule_work(&xprt->task_cleanup);\n}\n\nstatic void xprt_destroy_kref(struct kref *kref)\n{\n\txprt_destroy(container_of(kref, struct rpc_xprt, kref));\n}\n\n/**\n * xprt_get - return a reference to an RPC transport.\n * @xprt: pointer to the transport\n *\n */\nstruct rpc_xprt *xprt_get(struct rpc_xprt *xprt)\n{\n\tif (xprt != NULL && kref_get_unless_zero(&xprt->kref))\n\t\treturn xprt;\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(xprt_get);\n\n/**\n * xprt_put - release a reference to an RPC transport.\n * @xprt: pointer to the transport\n *\n */\nvoid xprt_put(struct rpc_xprt *xprt)\n{\n\tif (xprt != NULL)\n\t\tkref_put(&xprt->kref, xprt_destroy_kref);\n}\nEXPORT_SYMBOL_GPL(xprt_put);\n"}, "22": {"id": 22, "path": "/src/net/sunrpc/cache.c", "content": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n * net/sunrpc/cache.c\n *\n * Generic code for various authentication-related caches\n * used by sunrpc clients and servers.\n *\n * Copyright (C) 2002 Neil Brown <neilb@cse.unsw.edu.au>\n */\n\n#include <linux/types.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/slab.h>\n#include <linux/signal.h>\n#include <linux/sched.h>\n#include <linux/kmod.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/ctype.h>\n#include <linux/string_helpers.h>\n#include <linux/uaccess.h>\n#include <linux/poll.h>\n#include <linux/seq_file.h>\n#include <linux/proc_fs.h>\n#include <linux/net.h>\n#include <linux/workqueue.h>\n#include <linux/mutex.h>\n#include <linux/pagemap.h>\n#include <asm/ioctls.h>\n#include <linux/sunrpc/types.h>\n#include <linux/sunrpc/cache.h>\n#include <linux/sunrpc/stats.h>\n#include <linux/sunrpc/rpc_pipe_fs.h>\n#include <trace/events/sunrpc.h>\n#include \"netns.h\"\n\n#define\t RPCDBG_FACILITY RPCDBG_CACHE\n\nstatic bool cache_defer_req(struct cache_req *req, struct cache_head *item);\nstatic void cache_revisit_request(struct cache_head *item);\n\nstatic void cache_init(struct cache_head *h, struct cache_detail *detail)\n{\n\ttime64_t now = seconds_since_boot();\n\tINIT_HLIST_NODE(&h->cache_list);\n\th->flags = 0;\n\tkref_init(&h->ref);\n\th->expiry_time = now + CACHE_NEW_EXPIRY;\n\tif (now <= detail->flush_time)\n\t\t/* ensure it isn't already expired */\n\t\tnow = detail->flush_time + 1;\n\th->last_refresh = now;\n}\n\nstatic void cache_fresh_unlocked(struct cache_head *head,\n\t\t\t\tstruct cache_detail *detail);\n\nstatic struct cache_head *sunrpc_cache_find_rcu(struct cache_detail *detail,\n\t\t\t\t\t\tstruct cache_head *key,\n\t\t\t\t\t\tint hash)\n{\n\tstruct hlist_head *head = &detail->hash_table[hash];\n\tstruct cache_head *tmp;\n\n\trcu_read_lock();\n\thlist_for_each_entry_rcu(tmp, head, cache_list) {\n\t\tif (!detail->match(tmp, key))\n\t\t\tcontinue;\n\t\tif (test_bit(CACHE_VALID, &tmp->flags) &&\n\t\t    cache_is_expired(detail, tmp))\n\t\t\tcontinue;\n\t\ttmp = cache_get_rcu(tmp);\n\t\trcu_read_unlock();\n\t\treturn tmp;\n\t}\n\trcu_read_unlock();\n\treturn NULL;\n}\n\nstatic void sunrpc_begin_cache_remove_entry(struct cache_head *ch,\n\t\t\t\t\t    struct cache_detail *cd)\n{\n\t/* Must be called under cd->hash_lock */\n\thlist_del_init_rcu(&ch->cache_list);\n\tset_bit(CACHE_CLEANED, &ch->flags);\n\tcd->entries --;\n}\n\nstatic void sunrpc_end_cache_remove_entry(struct cache_head *ch,\n\t\t\t\t\t  struct cache_detail *cd)\n{\n\tcache_fresh_unlocked(ch, cd);\n\tcache_put(ch, cd);\n}\n\nstatic struct cache_head *sunrpc_cache_add_entry(struct cache_detail *detail,\n\t\t\t\t\t\t struct cache_head *key,\n\t\t\t\t\t\t int hash)\n{\n\tstruct cache_head *new, *tmp, *freeme = NULL;\n\tstruct hlist_head *head = &detail->hash_table[hash];\n\n\tnew = detail->alloc();\n\tif (!new)\n\t\treturn NULL;\n\t/* must fully initialise 'new', else\n\t * we might get lose if we need to\n\t * cache_put it soon.\n\t */\n\tcache_init(new, detail);\n\tdetail->init(new, key);\n\n\tspin_lock(&detail->hash_lock);\n\n\t/* check if entry appeared while we slept */\n\thlist_for_each_entry_rcu(tmp, head, cache_list,\n\t\t\t\t lockdep_is_held(&detail->hash_lock)) {\n\t\tif (!detail->match(tmp, key))\n\t\t\tcontinue;\n\t\tif (test_bit(CACHE_VALID, &tmp->flags) &&\n\t\t    cache_is_expired(detail, tmp)) {\n\t\t\tsunrpc_begin_cache_remove_entry(tmp, detail);\n\t\t\ttrace_cache_entry_expired(detail, tmp);\n\t\t\tfreeme = tmp;\n\t\t\tbreak;\n\t\t}\n\t\tcache_get(tmp);\n\t\tspin_unlock(&detail->hash_lock);\n\t\tcache_put(new, detail);\n\t\treturn tmp;\n\t}\n\n\thlist_add_head_rcu(&new->cache_list, head);\n\tdetail->entries++;\n\tcache_get(new);\n\tspin_unlock(&detail->hash_lock);\n\n\tif (freeme)\n\t\tsunrpc_end_cache_remove_entry(freeme, detail);\n\treturn new;\n}\n\nstruct cache_head *sunrpc_cache_lookup_rcu(struct cache_detail *detail,\n\t\t\t\t\t   struct cache_head *key, int hash)\n{\n\tstruct cache_head *ret;\n\n\tret = sunrpc_cache_find_rcu(detail, key, hash);\n\tif (ret)\n\t\treturn ret;\n\t/* Didn't find anything, insert an empty entry */\n\treturn sunrpc_cache_add_entry(detail, key, hash);\n}\nEXPORT_SYMBOL_GPL(sunrpc_cache_lookup_rcu);\n\nstatic void cache_dequeue(struct cache_detail *detail, struct cache_head *ch);\n\nstatic void cache_fresh_locked(struct cache_head *head, time64_t expiry,\n\t\t\t       struct cache_detail *detail)\n{\n\ttime64_t now = seconds_since_boot();\n\tif (now <= detail->flush_time)\n\t\t/* ensure it isn't immediately treated as expired */\n\t\tnow = detail->flush_time + 1;\n\thead->expiry_time = expiry;\n\thead->last_refresh = now;\n\tsmp_wmb(); /* paired with smp_rmb() in cache_is_valid() */\n\tset_bit(CACHE_VALID, &head->flags);\n}\n\nstatic void cache_fresh_unlocked(struct cache_head *head,\n\t\t\t\t struct cache_detail *detail)\n{\n\tif (test_and_clear_bit(CACHE_PENDING, &head->flags)) {\n\t\tcache_revisit_request(head);\n\t\tcache_dequeue(detail, head);\n\t}\n}\n\nstatic void cache_make_negative(struct cache_detail *detail,\n\t\t\t\tstruct cache_head *h)\n{\n\tset_bit(CACHE_NEGATIVE, &h->flags);\n\ttrace_cache_entry_make_negative(detail, h);\n}\n\nstatic void cache_entry_update(struct cache_detail *detail,\n\t\t\t       struct cache_head *h,\n\t\t\t       struct cache_head *new)\n{\n\tif (!test_bit(CACHE_NEGATIVE, &new->flags)) {\n\t\tdetail->update(h, new);\n\t\ttrace_cache_entry_update(detail, h);\n\t} else {\n\t\tcache_make_negative(detail, h);\n\t}\n}\n\nstruct cache_head *sunrpc_cache_update(struct cache_detail *detail,\n\t\t\t\t       struct cache_head *new, struct cache_head *old, int hash)\n{\n\t/* The 'old' entry is to be replaced by 'new'.\n\t * If 'old' is not VALID, we update it directly,\n\t * otherwise we need to replace it\n\t */\n\tstruct cache_head *tmp;\n\n\tif (!test_bit(CACHE_VALID, &old->flags)) {\n\t\tspin_lock(&detail->hash_lock);\n\t\tif (!test_bit(CACHE_VALID, &old->flags)) {\n\t\t\tcache_entry_update(detail, old, new);\n\t\t\tcache_fresh_locked(old, new->expiry_time, detail);\n\t\t\tspin_unlock(&detail->hash_lock);\n\t\t\tcache_fresh_unlocked(old, detail);\n\t\t\treturn old;\n\t\t}\n\t\tspin_unlock(&detail->hash_lock);\n\t}\n\t/* We need to insert a new entry */\n\ttmp = detail->alloc();\n\tif (!tmp) {\n\t\tcache_put(old, detail);\n\t\treturn NULL;\n\t}\n\tcache_init(tmp, detail);\n\tdetail->init(tmp, old);\n\n\tspin_lock(&detail->hash_lock);\n\tcache_entry_update(detail, tmp, new);\n\thlist_add_head(&tmp->cache_list, &detail->hash_table[hash]);\n\tdetail->entries++;\n\tcache_get(tmp);\n\tcache_fresh_locked(tmp, new->expiry_time, detail);\n\tcache_fresh_locked(old, 0, detail);\n\tspin_unlock(&detail->hash_lock);\n\tcache_fresh_unlocked(tmp, detail);\n\tcache_fresh_unlocked(old, detail);\n\tcache_put(old, detail);\n\treturn tmp;\n}\nEXPORT_SYMBOL_GPL(sunrpc_cache_update);\n\nstatic inline int cache_is_valid(struct cache_head *h)\n{\n\tif (!test_bit(CACHE_VALID, &h->flags))\n\t\treturn -EAGAIN;\n\telse {\n\t\t/* entry is valid */\n\t\tif (test_bit(CACHE_NEGATIVE, &h->flags))\n\t\t\treturn -ENOENT;\n\t\telse {\n\t\t\t/*\n\t\t\t * In combination with write barrier in\n\t\t\t * sunrpc_cache_update, ensures that anyone\n\t\t\t * using the cache entry after this sees the\n\t\t\t * updated contents:\n\t\t\t */\n\t\t\tsmp_rmb();\n\t\t\treturn 0;\n\t\t}\n\t}\n}\n\nstatic int try_to_negate_entry(struct cache_detail *detail, struct cache_head *h)\n{\n\tint rv;\n\n\tspin_lock(&detail->hash_lock);\n\trv = cache_is_valid(h);\n\tif (rv == -EAGAIN) {\n\t\tcache_make_negative(detail, h);\n\t\tcache_fresh_locked(h, seconds_since_boot()+CACHE_NEW_EXPIRY,\n\t\t\t\t   detail);\n\t\trv = -ENOENT;\n\t}\n\tspin_unlock(&detail->hash_lock);\n\tcache_fresh_unlocked(h, detail);\n\treturn rv;\n}\n\n/*\n * This is the generic cache management routine for all\n * the authentication caches.\n * It checks the currency of a cache item and will (later)\n * initiate an upcall to fill it if needed.\n *\n *\n * Returns 0 if the cache_head can be used, or cache_puts it and returns\n * -EAGAIN if upcall is pending and request has been queued\n * -ETIMEDOUT if upcall failed or request could not be queue or\n *           upcall completed but item is still invalid (implying that\n *           the cache item has been replaced with a newer one).\n * -ENOENT if cache entry was negative\n */\nint cache_check(struct cache_detail *detail,\n\t\t    struct cache_head *h, struct cache_req *rqstp)\n{\n\tint rv;\n\ttime64_t refresh_age, age;\n\n\t/* First decide return status as best we can */\n\trv = cache_is_valid(h);\n\n\t/* now see if we want to start an upcall */\n\trefresh_age = (h->expiry_time - h->last_refresh);\n\tage = seconds_since_boot() - h->last_refresh;\n\n\tif (rqstp == NULL) {\n\t\tif (rv == -EAGAIN)\n\t\t\trv = -ENOENT;\n\t} else if (rv == -EAGAIN ||\n\t\t   (h->expiry_time != 0 && age > refresh_age/2)) {\n\t\tdprintk(\"RPC:       Want update, refage=%lld, age=%lld\\n\",\n\t\t\t\trefresh_age, age);\n\t\tswitch (detail->cache_upcall(detail, h)) {\n\t\tcase -EINVAL:\n\t\t\trv = try_to_negate_entry(detail, h);\n\t\t\tbreak;\n\t\tcase -EAGAIN:\n\t\t\tcache_fresh_unlocked(h, detail);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (rv == -EAGAIN) {\n\t\tif (!cache_defer_req(rqstp, h)) {\n\t\t\t/*\n\t\t\t * Request was not deferred; handle it as best\n\t\t\t * we can ourselves:\n\t\t\t */\n\t\t\trv = cache_is_valid(h);\n\t\t\tif (rv == -EAGAIN)\n\t\t\t\trv = -ETIMEDOUT;\n\t\t}\n\t}\n\tif (rv)\n\t\tcache_put(h, detail);\n\treturn rv;\n}\nEXPORT_SYMBOL_GPL(cache_check);\n\n/*\n * caches need to be periodically cleaned.\n * For this we maintain a list of cache_detail and\n * a current pointer into that list and into the table\n * for that entry.\n *\n * Each time cache_clean is called it finds the next non-empty entry\n * in the current table and walks the list in that entry\n * looking for entries that can be removed.\n *\n * An entry gets removed if:\n * - The expiry is before current time\n * - The last_refresh time is before the flush_time for that cache\n *\n * later we might drop old entries with non-NEVER expiry if that table\n * is getting 'full' for some definition of 'full'\n *\n * The question of \"how often to scan a table\" is an interesting one\n * and is answered in part by the use of the \"nextcheck\" field in the\n * cache_detail.\n * When a scan of a table begins, the nextcheck field is set to a time\n * that is well into the future.\n * While scanning, if an expiry time is found that is earlier than the\n * current nextcheck time, nextcheck is set to that expiry time.\n * If the flush_time is ever set to a time earlier than the nextcheck\n * time, the nextcheck time is then set to that flush_time.\n *\n * A table is then only scanned if the current time is at least\n * the nextcheck time.\n *\n */\n\nstatic LIST_HEAD(cache_list);\nstatic DEFINE_SPINLOCK(cache_list_lock);\nstatic struct cache_detail *current_detail;\nstatic int current_index;\n\nstatic void do_cache_clean(struct work_struct *work);\nstatic struct delayed_work cache_cleaner;\n\nvoid sunrpc_init_cache_detail(struct cache_detail *cd)\n{\n\tspin_lock_init(&cd->hash_lock);\n\tINIT_LIST_HEAD(&cd->queue);\n\tspin_lock(&cache_list_lock);\n\tcd->nextcheck = 0;\n\tcd->entries = 0;\n\tatomic_set(&cd->writers, 0);\n\tcd->last_close = 0;\n\tcd->last_warn = -1;\n\tlist_add(&cd->others, &cache_list);\n\tspin_unlock(&cache_list_lock);\n\n\t/* start the cleaning process */\n\tqueue_delayed_work(system_power_efficient_wq, &cache_cleaner, 0);\n}\nEXPORT_SYMBOL_GPL(sunrpc_init_cache_detail);\n\nvoid sunrpc_destroy_cache_detail(struct cache_detail *cd)\n{\n\tcache_purge(cd);\n\tspin_lock(&cache_list_lock);\n\tspin_lock(&cd->hash_lock);\n\tif (current_detail == cd)\n\t\tcurrent_detail = NULL;\n\tlist_del_init(&cd->others);\n\tspin_unlock(&cd->hash_lock);\n\tspin_unlock(&cache_list_lock);\n\tif (list_empty(&cache_list)) {\n\t\t/* module must be being unloaded so its safe to kill the worker */\n\t\tcancel_delayed_work_sync(&cache_cleaner);\n\t}\n}\nEXPORT_SYMBOL_GPL(sunrpc_destroy_cache_detail);\n\n/* clean cache tries to find something to clean\n * and cleans it.\n * It returns 1 if it cleaned something,\n *            0 if it didn't find anything this time\n *           -1 if it fell off the end of the list.\n */\nstatic int cache_clean(void)\n{\n\tint rv = 0;\n\tstruct list_head *next;\n\n\tspin_lock(&cache_list_lock);\n\n\t/* find a suitable table if we don't already have one */\n\twhile (current_detail == NULL ||\n\t    current_index >= current_detail->hash_size) {\n\t\tif (current_detail)\n\t\t\tnext = current_detail->others.next;\n\t\telse\n\t\t\tnext = cache_list.next;\n\t\tif (next == &cache_list) {\n\t\t\tcurrent_detail = NULL;\n\t\t\tspin_unlock(&cache_list_lock);\n\t\t\treturn -1;\n\t\t}\n\t\tcurrent_detail = list_entry(next, struct cache_detail, others);\n\t\tif (current_detail->nextcheck > seconds_since_boot())\n\t\t\tcurrent_index = current_detail->hash_size;\n\t\telse {\n\t\t\tcurrent_index = 0;\n\t\t\tcurrent_detail->nextcheck = seconds_since_boot()+30*60;\n\t\t}\n\t}\n\n\t/* find a non-empty bucket in the table */\n\twhile (current_detail &&\n\t       current_index < current_detail->hash_size &&\n\t       hlist_empty(&current_detail->hash_table[current_index]))\n\t\tcurrent_index++;\n\n\t/* find a cleanable entry in the bucket and clean it, or set to next bucket */\n\n\tif (current_detail && current_index < current_detail->hash_size) {\n\t\tstruct cache_head *ch = NULL;\n\t\tstruct cache_detail *d;\n\t\tstruct hlist_head *head;\n\t\tstruct hlist_node *tmp;\n\n\t\tspin_lock(&current_detail->hash_lock);\n\n\t\t/* Ok, now to clean this strand */\n\n\t\thead = &current_detail->hash_table[current_index];\n\t\thlist_for_each_entry_safe(ch, tmp, head, cache_list) {\n\t\t\tif (current_detail->nextcheck > ch->expiry_time)\n\t\t\t\tcurrent_detail->nextcheck = ch->expiry_time+1;\n\t\t\tif (!cache_is_expired(current_detail, ch))\n\t\t\t\tcontinue;\n\n\t\t\tsunrpc_begin_cache_remove_entry(ch, current_detail);\n\t\t\ttrace_cache_entry_expired(current_detail, ch);\n\t\t\trv = 1;\n\t\t\tbreak;\n\t\t}\n\n\t\tspin_unlock(&current_detail->hash_lock);\n\t\td = current_detail;\n\t\tif (!ch)\n\t\t\tcurrent_index ++;\n\t\tspin_unlock(&cache_list_lock);\n\t\tif (ch)\n\t\t\tsunrpc_end_cache_remove_entry(ch, d);\n\t} else\n\t\tspin_unlock(&cache_list_lock);\n\n\treturn rv;\n}\n\n/*\n * We want to regularly clean the cache, so we need to schedule some work ...\n */\nstatic void do_cache_clean(struct work_struct *work)\n{\n\tint delay;\n\n\tif (list_empty(&cache_list))\n\t\treturn;\n\n\tif (cache_clean() == -1)\n\t\tdelay = round_jiffies_relative(30*HZ);\n\telse\n\t\tdelay = 5;\n\n\tqueue_delayed_work(system_power_efficient_wq, &cache_cleaner, delay);\n}\n\n\n/*\n * Clean all caches promptly.  This just calls cache_clean\n * repeatedly until we are sure that every cache has had a chance to\n * be fully cleaned\n */\nvoid cache_flush(void)\n{\n\twhile (cache_clean() != -1)\n\t\tcond_resched();\n\twhile (cache_clean() != -1)\n\t\tcond_resched();\n}\nEXPORT_SYMBOL_GPL(cache_flush);\n\nvoid cache_purge(struct cache_detail *detail)\n{\n\tstruct cache_head *ch = NULL;\n\tstruct hlist_head *head = NULL;\n\tint i = 0;\n\n\tspin_lock(&detail->hash_lock);\n\tif (!detail->entries) {\n\t\tspin_unlock(&detail->hash_lock);\n\t\treturn;\n\t}\n\n\tdprintk(\"RPC: %d entries in %s cache\\n\", detail->entries, detail->name);\n\tfor (i = 0; i < detail->hash_size; i++) {\n\t\thead = &detail->hash_table[i];\n\t\twhile (!hlist_empty(head)) {\n\t\t\tch = hlist_entry(head->first, struct cache_head,\n\t\t\t\t\t cache_list);\n\t\t\tsunrpc_begin_cache_remove_entry(ch, detail);\n\t\t\tspin_unlock(&detail->hash_lock);\n\t\t\tsunrpc_end_cache_remove_entry(ch, detail);\n\t\t\tspin_lock(&detail->hash_lock);\n\t\t}\n\t}\n\tspin_unlock(&detail->hash_lock);\n}\nEXPORT_SYMBOL_GPL(cache_purge);\n\n\n/*\n * Deferral and Revisiting of Requests.\n *\n * If a cache lookup finds a pending entry, we\n * need to defer the request and revisit it later.\n * All deferred requests are stored in a hash table,\n * indexed by \"struct cache_head *\".\n * As it may be wasteful to store a whole request\n * structure, we allow the request to provide a\n * deferred form, which must contain a\n * 'struct cache_deferred_req'\n * This cache_deferred_req contains a method to allow\n * it to be revisited when cache info is available\n */\n\n#define\tDFR_HASHSIZE\t(PAGE_SIZE/sizeof(struct list_head))\n#define\tDFR_HASH(item)\t((((long)item)>>4 ^ (((long)item)>>13)) % DFR_HASHSIZE)\n\n#define\tDFR_MAX\t300\t/* ??? */\n\nstatic DEFINE_SPINLOCK(cache_defer_lock);\nstatic LIST_HEAD(cache_defer_list);\nstatic struct hlist_head cache_defer_hash[DFR_HASHSIZE];\nstatic int cache_defer_cnt;\n\nstatic void __unhash_deferred_req(struct cache_deferred_req *dreq)\n{\n\thlist_del_init(&dreq->hash);\n\tif (!list_empty(&dreq->recent)) {\n\t\tlist_del_init(&dreq->recent);\n\t\tcache_defer_cnt--;\n\t}\n}\n\nstatic void __hash_deferred_req(struct cache_deferred_req *dreq, struct cache_head *item)\n{\n\tint hash = DFR_HASH(item);\n\n\tINIT_LIST_HEAD(&dreq->recent);\n\thlist_add_head(&dreq->hash, &cache_defer_hash[hash]);\n}\n\nstatic void setup_deferral(struct cache_deferred_req *dreq,\n\t\t\t   struct cache_head *item,\n\t\t\t   int count_me)\n{\n\n\tdreq->item = item;\n\n\tspin_lock(&cache_defer_lock);\n\n\t__hash_deferred_req(dreq, item);\n\n\tif (count_me) {\n\t\tcache_defer_cnt++;\n\t\tlist_add(&dreq->recent, &cache_defer_list);\n\t}\n\n\tspin_unlock(&cache_defer_lock);\n\n}\n\nstruct thread_deferred_req {\n\tstruct cache_deferred_req handle;\n\tstruct completion completion;\n};\n\nstatic void cache_restart_thread(struct cache_deferred_req *dreq, int too_many)\n{\n\tstruct thread_deferred_req *dr =\n\t\tcontainer_of(dreq, struct thread_deferred_req, handle);\n\tcomplete(&dr->completion);\n}\n\nstatic void cache_wait_req(struct cache_req *req, struct cache_head *item)\n{\n\tstruct thread_deferred_req sleeper;\n\tstruct cache_deferred_req *dreq = &sleeper.handle;\n\n\tsleeper.completion = COMPLETION_INITIALIZER_ONSTACK(sleeper.completion);\n\tdreq->revisit = cache_restart_thread;\n\n\tsetup_deferral(dreq, item, 0);\n\n\tif (!test_bit(CACHE_PENDING, &item->flags) ||\n\t    wait_for_completion_interruptible_timeout(\n\t\t    &sleeper.completion, req->thread_wait) <= 0) {\n\t\t/* The completion wasn't completed, so we need\n\t\t * to clean up\n\t\t */\n\t\tspin_lock(&cache_defer_lock);\n\t\tif (!hlist_unhashed(&sleeper.handle.hash)) {\n\t\t\t__unhash_deferred_req(&sleeper.handle);\n\t\t\tspin_unlock(&cache_defer_lock);\n\t\t} else {\n\t\t\t/* cache_revisit_request already removed\n\t\t\t * this from the hash table, but hasn't\n\t\t\t * called ->revisit yet.  It will very soon\n\t\t\t * and we need to wait for it.\n\t\t\t */\n\t\t\tspin_unlock(&cache_defer_lock);\n\t\t\twait_for_completion(&sleeper.completion);\n\t\t}\n\t}\n}\n\nstatic void cache_limit_defers(void)\n{\n\t/* Make sure we haven't exceed the limit of allowed deferred\n\t * requests.\n\t */\n\tstruct cache_deferred_req *discard = NULL;\n\n\tif (cache_defer_cnt <= DFR_MAX)\n\t\treturn;\n\n\tspin_lock(&cache_defer_lock);\n\n\t/* Consider removing either the first or the last */\n\tif (cache_defer_cnt > DFR_MAX) {\n\t\tif (prandom_u32() & 1)\n\t\t\tdiscard = list_entry(cache_defer_list.next,\n\t\t\t\t\t     struct cache_deferred_req, recent);\n\t\telse\n\t\t\tdiscard = list_entry(cache_defer_list.prev,\n\t\t\t\t\t     struct cache_deferred_req, recent);\n\t\t__unhash_deferred_req(discard);\n\t}\n\tspin_unlock(&cache_defer_lock);\n\tif (discard)\n\t\tdiscard->revisit(discard, 1);\n}\n\n/* Return true if and only if a deferred request is queued. */\nstatic bool cache_defer_req(struct cache_req *req, struct cache_head *item)\n{\n\tstruct cache_deferred_req *dreq;\n\n\tif (req->thread_wait) {\n\t\tcache_wait_req(req, item);\n\t\tif (!test_bit(CACHE_PENDING, &item->flags))\n\t\t\treturn false;\n\t}\n\tdreq = req->defer(req);\n\tif (dreq == NULL)\n\t\treturn false;\n\tsetup_deferral(dreq, item, 1);\n\tif (!test_bit(CACHE_PENDING, &item->flags))\n\t\t/* Bit could have been cleared before we managed to\n\t\t * set up the deferral, so need to revisit just in case\n\t\t */\n\t\tcache_revisit_request(item);\n\n\tcache_limit_defers();\n\treturn true;\n}\n\nstatic void cache_revisit_request(struct cache_head *item)\n{\n\tstruct cache_deferred_req *dreq;\n\tstruct list_head pending;\n\tstruct hlist_node *tmp;\n\tint hash = DFR_HASH(item);\n\n\tINIT_LIST_HEAD(&pending);\n\tspin_lock(&cache_defer_lock);\n\n\thlist_for_each_entry_safe(dreq, tmp, &cache_defer_hash[hash], hash)\n\t\tif (dreq->item == item) {\n\t\t\t__unhash_deferred_req(dreq);\n\t\t\tlist_add(&dreq->recent, &pending);\n\t\t}\n\n\tspin_unlock(&cache_defer_lock);\n\n\twhile (!list_empty(&pending)) {\n\t\tdreq = list_entry(pending.next, struct cache_deferred_req, recent);\n\t\tlist_del_init(&dreq->recent);\n\t\tdreq->revisit(dreq, 0);\n\t}\n}\n\nvoid cache_clean_deferred(void *owner)\n{\n\tstruct cache_deferred_req *dreq, *tmp;\n\tstruct list_head pending;\n\n\n\tINIT_LIST_HEAD(&pending);\n\tspin_lock(&cache_defer_lock);\n\n\tlist_for_each_entry_safe(dreq, tmp, &cache_defer_list, recent) {\n\t\tif (dreq->owner == owner) {\n\t\t\t__unhash_deferred_req(dreq);\n\t\t\tlist_add(&dreq->recent, &pending);\n\t\t}\n\t}\n\tspin_unlock(&cache_defer_lock);\n\n\twhile (!list_empty(&pending)) {\n\t\tdreq = list_entry(pending.next, struct cache_deferred_req, recent);\n\t\tlist_del_init(&dreq->recent);\n\t\tdreq->revisit(dreq, 1);\n\t}\n}\n\n/*\n * communicate with user-space\n *\n * We have a magic /proc file - /proc/net/rpc/<cachename>/channel.\n * On read, you get a full request, or block.\n * On write, an update request is processed.\n * Poll works if anything to read, and always allows write.\n *\n * Implemented by linked list of requests.  Each open file has\n * a ->private that also exists in this list.  New requests are added\n * to the end and may wakeup and preceding readers.\n * New readers are added to the head.  If, on read, an item is found with\n * CACHE_UPCALLING clear, we free it from the list.\n *\n */\n\nstatic DEFINE_SPINLOCK(queue_lock);\nstatic DEFINE_MUTEX(queue_io_mutex);\n\nstruct cache_queue {\n\tstruct list_head\tlist;\n\tint\t\t\treader;\t/* if 0, then request */\n};\nstruct cache_request {\n\tstruct cache_queue\tq;\n\tstruct cache_head\t*item;\n\tchar\t\t\t* buf;\n\tint\t\t\tlen;\n\tint\t\t\treaders;\n};\nstruct cache_reader {\n\tstruct cache_queue\tq;\n\tint\t\t\toffset;\t/* if non-0, we have a refcnt on next request */\n};\n\nstatic int cache_request(struct cache_detail *detail,\n\t\t\t       struct cache_request *crq)\n{\n\tchar *bp = crq->buf;\n\tint len = PAGE_SIZE;\n\n\tdetail->cache_request(detail, crq->item, &bp, &len);\n\tif (len < 0)\n\t\treturn -EAGAIN;\n\treturn PAGE_SIZE - len;\n}\n\nstatic ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n\t\t\t  loff_t *ppos, struct cache_detail *cd)\n{\n\tstruct cache_reader *rp = filp->private_data;\n\tstruct cache_request *rq;\n\tstruct inode *inode = file_inode(filp);\n\tint err;\n\n\tif (count == 0)\n\t\treturn 0;\n\n\tinode_lock(inode); /* protect against multiple concurrent\n\t\t\t      * readers on this file */\n again:\n\tspin_lock(&queue_lock);\n\t/* need to find next request */\n\twhile (rp->q.list.next != &cd->queue &&\n\t       list_entry(rp->q.list.next, struct cache_queue, list)\n\t       ->reader) {\n\t\tstruct list_head *next = rp->q.list.next;\n\t\tlist_move(&rp->q.list, next);\n\t}\n\tif (rp->q.list.next == &cd->queue) {\n\t\tspin_unlock(&queue_lock);\n\t\tinode_unlock(inode);\n\t\tWARN_ON_ONCE(rp->offset);\n\t\treturn 0;\n\t}\n\trq = container_of(rp->q.list.next, struct cache_request, q.list);\n\tWARN_ON_ONCE(rq->q.reader);\n\tif (rp->offset == 0)\n\t\trq->readers++;\n\tspin_unlock(&queue_lock);\n\n\tif (rq->len == 0) {\n\t\terr = cache_request(cd, rq);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t\trq->len = err;\n\t}\n\n\tif (rp->offset == 0 && !test_bit(CACHE_PENDING, &rq->item->flags)) {\n\t\terr = -EAGAIN;\n\t\tspin_lock(&queue_lock);\n\t\tlist_move(&rp->q.list, &rq->q.list);\n\t\tspin_unlock(&queue_lock);\n\t} else {\n\t\tif (rp->offset + count > rq->len)\n\t\t\tcount = rq->len - rp->offset;\n\t\terr = -EFAULT;\n\t\tif (copy_to_user(buf, rq->buf + rp->offset, count))\n\t\t\tgoto out;\n\t\trp->offset += count;\n\t\tif (rp->offset >= rq->len) {\n\t\t\trp->offset = 0;\n\t\t\tspin_lock(&queue_lock);\n\t\t\tlist_move(&rp->q.list, &rq->q.list);\n\t\t\tspin_unlock(&queue_lock);\n\t\t}\n\t\terr = 0;\n\t}\n out:\n\tif (rp->offset == 0) {\n\t\t/* need to release rq */\n\t\tspin_lock(&queue_lock);\n\t\trq->readers--;\n\t\tif (rq->readers == 0 &&\n\t\t    !test_bit(CACHE_PENDING, &rq->item->flags)) {\n\t\t\tlist_del(&rq->q.list);\n\t\t\tspin_unlock(&queue_lock);\n\t\t\tcache_put(rq->item, cd);\n\t\t\tkfree(rq->buf);\n\t\t\tkfree(rq);\n\t\t} else\n\t\t\tspin_unlock(&queue_lock);\n\t}\n\tif (err == -EAGAIN)\n\t\tgoto again;\n\tinode_unlock(inode);\n\treturn err ? err :  count;\n}\n\nstatic ssize_t cache_do_downcall(char *kaddr, const char __user *buf,\n\t\t\t\t size_t count, struct cache_detail *cd)\n{\n\tssize_t ret;\n\n\tif (count == 0)\n\t\treturn -EINVAL;\n\tif (copy_from_user(kaddr, buf, count))\n\t\treturn -EFAULT;\n\tkaddr[count] = '\\0';\n\tret = cd->cache_parse(cd, kaddr, count);\n\tif (!ret)\n\t\tret = count;\n\treturn ret;\n}\n\nstatic ssize_t cache_slow_downcall(const char __user *buf,\n\t\t\t\t   size_t count, struct cache_detail *cd)\n{\n\tstatic char write_buf[32768]; /* protected by queue_io_mutex */\n\tssize_t ret = -EINVAL;\n\n\tif (count >= sizeof(write_buf))\n\t\tgoto out;\n\tmutex_lock(&queue_io_mutex);\n\tret = cache_do_downcall(write_buf, buf, count, cd);\n\tmutex_unlock(&queue_io_mutex);\nout:\n\treturn ret;\n}\n\nstatic ssize_t cache_downcall(struct address_space *mapping,\n\t\t\t      const char __user *buf,\n\t\t\t      size_t count, struct cache_detail *cd)\n{\n\tstruct page *page;\n\tchar *kaddr;\n\tssize_t ret = -ENOMEM;\n\n\tif (count >= PAGE_SIZE)\n\t\tgoto out_slow;\n\n\tpage = find_or_create_page(mapping, 0, GFP_KERNEL);\n\tif (!page)\n\t\tgoto out_slow;\n\n\tkaddr = kmap(page);\n\tret = cache_do_downcall(kaddr, buf, count, cd);\n\tkunmap(page);\n\tunlock_page(page);\n\tput_page(page);\n\treturn ret;\nout_slow:\n\treturn cache_slow_downcall(buf, count, cd);\n}\n\nstatic ssize_t cache_write(struct file *filp, const char __user *buf,\n\t\t\t   size_t count, loff_t *ppos,\n\t\t\t   struct cache_detail *cd)\n{\n\tstruct address_space *mapping = filp->f_mapping;\n\tstruct inode *inode = file_inode(filp);\n\tssize_t ret = -EINVAL;\n\n\tif (!cd->cache_parse)\n\t\tgoto out;\n\n\tinode_lock(inode);\n\tret = cache_downcall(mapping, buf, count, cd);\n\tinode_unlock(inode);\nout:\n\treturn ret;\n}\n\nstatic DECLARE_WAIT_QUEUE_HEAD(queue_wait);\n\nstatic __poll_t cache_poll(struct file *filp, poll_table *wait,\n\t\t\t       struct cache_detail *cd)\n{\n\t__poll_t mask;\n\tstruct cache_reader *rp = filp->private_data;\n\tstruct cache_queue *cq;\n\n\tpoll_wait(filp, &queue_wait, wait);\n\n\t/* alway allow write */\n\tmask = EPOLLOUT | EPOLLWRNORM;\n\n\tif (!rp)\n\t\treturn mask;\n\n\tspin_lock(&queue_lock);\n\n\tfor (cq= &rp->q; &cq->list != &cd->queue;\n\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n\t\tif (!cq->reader) {\n\t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\t\t\tbreak;\n\t\t}\n\tspin_unlock(&queue_lock);\n\treturn mask;\n}\n\nstatic int cache_ioctl(struct inode *ino, struct file *filp,\n\t\t       unsigned int cmd, unsigned long arg,\n\t\t       struct cache_detail *cd)\n{\n\tint len = 0;\n\tstruct cache_reader *rp = filp->private_data;\n\tstruct cache_queue *cq;\n\n\tif (cmd != FIONREAD || !rp)\n\t\treturn -EINVAL;\n\n\tspin_lock(&queue_lock);\n\n\t/* only find the length remaining in current request,\n\t * or the length of the next request\n\t */\n\tfor (cq= &rp->q; &cq->list != &cd->queue;\n\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n\t\tif (!cq->reader) {\n\t\t\tstruct cache_request *cr =\n\t\t\t\tcontainer_of(cq, struct cache_request, q);\n\t\t\tlen = cr->len - rp->offset;\n\t\t\tbreak;\n\t\t}\n\tspin_unlock(&queue_lock);\n\n\treturn put_user(len, (int __user *)arg);\n}\n\nstatic int cache_open(struct inode *inode, struct file *filp,\n\t\t      struct cache_detail *cd)\n{\n\tstruct cache_reader *rp = NULL;\n\n\tif (!cd || !try_module_get(cd->owner))\n\t\treturn -EACCES;\n\tnonseekable_open(inode, filp);\n\tif (filp->f_mode & FMODE_READ) {\n\t\trp = kmalloc(sizeof(*rp), GFP_KERNEL);\n\t\tif (!rp) {\n\t\t\tmodule_put(cd->owner);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\trp->offset = 0;\n\t\trp->q.reader = 1;\n\n\t\tspin_lock(&queue_lock);\n\t\tlist_add(&rp->q.list, &cd->queue);\n\t\tspin_unlock(&queue_lock);\n\t}\n\tif (filp->f_mode & FMODE_WRITE)\n\t\tatomic_inc(&cd->writers);\n\tfilp->private_data = rp;\n\treturn 0;\n}\n\nstatic int cache_release(struct inode *inode, struct file *filp,\n\t\t\t struct cache_detail *cd)\n{\n\tstruct cache_reader *rp = filp->private_data;\n\n\tif (rp) {\n\t\tspin_lock(&queue_lock);\n\t\tif (rp->offset) {\n\t\t\tstruct cache_queue *cq;\n\t\t\tfor (cq= &rp->q; &cq->list != &cd->queue;\n\t\t\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n\t\t\t\tif (!cq->reader) {\n\t\t\t\t\tcontainer_of(cq, struct cache_request, q)\n\t\t\t\t\t\t->readers--;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\trp->offset = 0;\n\t\t}\n\t\tlist_del(&rp->q.list);\n\t\tspin_unlock(&queue_lock);\n\n\t\tfilp->private_data = NULL;\n\t\tkfree(rp);\n\n\t}\n\tif (filp->f_mode & FMODE_WRITE) {\n\t\tatomic_dec(&cd->writers);\n\t\tcd->last_close = seconds_since_boot();\n\t}\n\tmodule_put(cd->owner);\n\treturn 0;\n}\n\n\n\nstatic void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)\n{\n\tstruct cache_queue *cq, *tmp;\n\tstruct cache_request *cr;\n\tstruct list_head dequeued;\n\n\tINIT_LIST_HEAD(&dequeued);\n\tspin_lock(&queue_lock);\n\tlist_for_each_entry_safe(cq, tmp, &detail->queue, list)\n\t\tif (!cq->reader) {\n\t\t\tcr = container_of(cq, struct cache_request, q);\n\t\t\tif (cr->item != ch)\n\t\t\t\tcontinue;\n\t\t\tif (test_bit(CACHE_PENDING, &ch->flags))\n\t\t\t\t/* Lost a race and it is pending again */\n\t\t\t\tbreak;\n\t\t\tif (cr->readers != 0)\n\t\t\t\tcontinue;\n\t\t\tlist_move(&cr->q.list, &dequeued);\n\t\t}\n\tspin_unlock(&queue_lock);\n\twhile (!list_empty(&dequeued)) {\n\t\tcr = list_entry(dequeued.next, struct cache_request, q.list);\n\t\tlist_del(&cr->q.list);\n\t\tcache_put(cr->item, detail);\n\t\tkfree(cr->buf);\n\t\tkfree(cr);\n\t}\n}\n\n/*\n * Support routines for text-based upcalls.\n * Fields are separated by spaces.\n * Fields are either mangled to quote space tab newline slosh with slosh\n * or a hexified with a leading \\x\n * Record is terminated with newline.\n *\n */\n\nvoid qword_add(char **bpp, int *lp, char *str)\n{\n\tchar *bp = *bpp;\n\tint len = *lp;\n\tint ret;\n\n\tif (len < 0) return;\n\n\tret = string_escape_str(str, bp, len, ESCAPE_OCTAL, \"\\\\ \\n\\t\");\n\tif (ret >= len) {\n\t\tbp += len;\n\t\tlen = -1;\n\t} else {\n\t\tbp += ret;\n\t\tlen -= ret;\n\t\t*bp++ = ' ';\n\t\tlen--;\n\t}\n\t*bpp = bp;\n\t*lp = len;\n}\nEXPORT_SYMBOL_GPL(qword_add);\n\nvoid qword_addhex(char **bpp, int *lp, char *buf, int blen)\n{\n\tchar *bp = *bpp;\n\tint len = *lp;\n\n\tif (len < 0) return;\n\n\tif (len > 2) {\n\t\t*bp++ = '\\\\';\n\t\t*bp++ = 'x';\n\t\tlen -= 2;\n\t\twhile (blen && len >= 2) {\n\t\t\tbp = hex_byte_pack(bp, *buf++);\n\t\t\tlen -= 2;\n\t\t\tblen--;\n\t\t}\n\t}\n\tif (blen || len<1) len = -1;\n\telse {\n\t\t*bp++ = ' ';\n\t\tlen--;\n\t}\n\t*bpp = bp;\n\t*lp = len;\n}\nEXPORT_SYMBOL_GPL(qword_addhex);\n\nstatic void warn_no_listener(struct cache_detail *detail)\n{\n\tif (detail->last_warn != detail->last_close) {\n\t\tdetail->last_warn = detail->last_close;\n\t\tif (detail->warn_no_listener)\n\t\t\tdetail->warn_no_listener(detail, detail->last_close != 0);\n\t}\n}\n\nstatic bool cache_listeners_exist(struct cache_detail *detail)\n{\n\tif (atomic_read(&detail->writers))\n\t\treturn true;\n\tif (detail->last_close == 0)\n\t\t/* This cache was never opened */\n\t\treturn false;\n\tif (detail->last_close < seconds_since_boot() - 30)\n\t\t/*\n\t\t * We allow for the possibility that someone might\n\t\t * restart a userspace daemon without restarting the\n\t\t * server; but after 30 seconds, we give up.\n\t\t */\n\t\t return false;\n\treturn true;\n}\n\n/*\n * register an upcall request to user-space and queue it up for read() by the\n * upcall daemon.\n *\n * Each request is at most one page long.\n */\nstatic int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n{\n\tchar *buf;\n\tstruct cache_request *crq;\n\tint ret = 0;\n\n\tif (test_bit(CACHE_CLEANED, &h->flags))\n\t\t/* Too late to make an upcall */\n\t\treturn -EAGAIN;\n\n\tbuf = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -EAGAIN;\n\n\tcrq = kmalloc(sizeof (*crq), GFP_KERNEL);\n\tif (!crq) {\n\t\tkfree(buf);\n\t\treturn -EAGAIN;\n\t}\n\n\tcrq->q.reader = 0;\n\tcrq->buf = buf;\n\tcrq->len = 0;\n\tcrq->readers = 0;\n\tspin_lock(&queue_lock);\n\tif (test_bit(CACHE_PENDING, &h->flags)) {\n\t\tcrq->item = cache_get(h);\n\t\tlist_add_tail(&crq->q.list, &detail->queue);\n\t\ttrace_cache_entry_upcall(detail, h);\n\t} else\n\t\t/* Lost a race, no longer PENDING, so don't enqueue */\n\t\tret = -EAGAIN;\n\tspin_unlock(&queue_lock);\n\twake_up(&queue_wait);\n\tif (ret == -EAGAIN) {\n\t\tkfree(buf);\n\t\tkfree(crq);\n\t}\n\treturn ret;\n}\n\nint sunrpc_cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n{\n\tif (test_and_set_bit(CACHE_PENDING, &h->flags))\n\t\treturn 0;\n\treturn cache_pipe_upcall(detail, h);\n}\nEXPORT_SYMBOL_GPL(sunrpc_cache_pipe_upcall);\n\nint sunrpc_cache_pipe_upcall_timeout(struct cache_detail *detail,\n\t\t\t\t     struct cache_head *h)\n{\n\tif (!cache_listeners_exist(detail)) {\n\t\twarn_no_listener(detail);\n\t\ttrace_cache_entry_no_listener(detail, h);\n\t\treturn -EINVAL;\n\t}\n\treturn sunrpc_cache_pipe_upcall(detail, h);\n}\nEXPORT_SYMBOL_GPL(sunrpc_cache_pipe_upcall_timeout);\n\n/*\n * parse a message from user-space and pass it\n * to an appropriate cache\n * Messages are, like requests, separated into fields by\n * spaces and dequotes as \\xHEXSTRING or embedded \\nnn octal\n *\n * Message is\n *   reply cachename expiry key ... content....\n *\n * key and content are both parsed by cache\n */\n\nint qword_get(char **bpp, char *dest, int bufsize)\n{\n\t/* return bytes copied, or -1 on error */\n\tchar *bp = *bpp;\n\tint len = 0;\n\n\twhile (*bp == ' ') bp++;\n\n\tif (bp[0] == '\\\\' && bp[1] == 'x') {\n\t\t/* HEX STRING */\n\t\tbp += 2;\n\t\twhile (len < bufsize - 1) {\n\t\t\tint h, l;\n\n\t\t\th = hex_to_bin(bp[0]);\n\t\t\tif (h < 0)\n\t\t\t\tbreak;\n\n\t\t\tl = hex_to_bin(bp[1]);\n\t\t\tif (l < 0)\n\t\t\t\tbreak;\n\n\t\t\t*dest++ = (h << 4) | l;\n\t\t\tbp += 2;\n\t\t\tlen++;\n\t\t}\n\t} else {\n\t\t/* text with \\nnn octal quoting */\n\t\twhile (*bp != ' ' && *bp != '\\n' && *bp && len < bufsize-1) {\n\t\t\tif (*bp == '\\\\' &&\n\t\t\t    isodigit(bp[1]) && (bp[1] <= '3') &&\n\t\t\t    isodigit(bp[2]) &&\n\t\t\t    isodigit(bp[3])) {\n\t\t\t\tint byte = (*++bp -'0');\n\t\t\t\tbp++;\n\t\t\t\tbyte = (byte << 3) | (*bp++ - '0');\n\t\t\t\tbyte = (byte << 3) | (*bp++ - '0');\n\t\t\t\t*dest++ = byte;\n\t\t\t\tlen++;\n\t\t\t} else {\n\t\t\t\t*dest++ = *bp++;\n\t\t\t\tlen++;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (*bp != ' ' && *bp != '\\n' && *bp != '\\0')\n\t\treturn -1;\n\twhile (*bp == ' ') bp++;\n\t*bpp = bp;\n\t*dest = '\\0';\n\treturn len;\n}\nEXPORT_SYMBOL_GPL(qword_get);\n\n\n/*\n * support /proc/net/rpc/$CACHENAME/content\n * as a seqfile.\n * We call ->cache_show passing NULL for the item to\n * get a header, then pass each real item in the cache\n */\n\nstatic void *__cache_seq_start(struct seq_file *m, loff_t *pos)\n{\n\tloff_t n = *pos;\n\tunsigned int hash, entry;\n\tstruct cache_head *ch;\n\tstruct cache_detail *cd = m->private;\n\n\tif (!n--)\n\t\treturn SEQ_START_TOKEN;\n\thash = n >> 32;\n\tentry = n & ((1LL<<32) - 1);\n\n\thlist_for_each_entry_rcu(ch, &cd->hash_table[hash], cache_list)\n\t\tif (!entry--)\n\t\t\treturn ch;\n\tn &= ~((1LL<<32) - 1);\n\tdo {\n\t\thash++;\n\t\tn += 1LL<<32;\n\t} while(hash < cd->hash_size &&\n\t\thlist_empty(&cd->hash_table[hash]));\n\tif (hash >= cd->hash_size)\n\t\treturn NULL;\n\t*pos = n+1;\n\treturn hlist_entry_safe(rcu_dereference_raw(\n\t\t\t\thlist_first_rcu(&cd->hash_table[hash])),\n\t\t\t\tstruct cache_head, cache_list);\n}\n\nstatic void *cache_seq_next(struct seq_file *m, void *p, loff_t *pos)\n{\n\tstruct cache_head *ch = p;\n\tint hash = (*pos >> 32);\n\tstruct cache_detail *cd = m->private;\n\n\tif (p == SEQ_START_TOKEN)\n\t\thash = 0;\n\telse if (ch->cache_list.next == NULL) {\n\t\thash++;\n\t\t*pos += 1LL<<32;\n\t} else {\n\t\t++*pos;\n\t\treturn hlist_entry_safe(rcu_dereference_raw(\n\t\t\t\t\thlist_next_rcu(&ch->cache_list)),\n\t\t\t\t\tstruct cache_head, cache_list);\n\t}\n\t*pos &= ~((1LL<<32) - 1);\n\twhile (hash < cd->hash_size &&\n\t       hlist_empty(&cd->hash_table[hash])) {\n\t\thash++;\n\t\t*pos += 1LL<<32;\n\t}\n\tif (hash >= cd->hash_size)\n\t\treturn NULL;\n\t++*pos;\n\treturn hlist_entry_safe(rcu_dereference_raw(\n\t\t\t\thlist_first_rcu(&cd->hash_table[hash])),\n\t\t\t\tstruct cache_head, cache_list);\n}\n\nvoid *cache_seq_start_rcu(struct seq_file *m, loff_t *pos)\n\t__acquires(RCU)\n{\n\trcu_read_lock();\n\treturn __cache_seq_start(m, pos);\n}\nEXPORT_SYMBOL_GPL(cache_seq_start_rcu);\n\nvoid *cache_seq_next_rcu(struct seq_file *file, void *p, loff_t *pos)\n{\n\treturn cache_seq_next(file, p, pos);\n}\nEXPORT_SYMBOL_GPL(cache_seq_next_rcu);\n\nvoid cache_seq_stop_rcu(struct seq_file *m, void *p)\n\t__releases(RCU)\n{\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(cache_seq_stop_rcu);\n\nstatic int c_show(struct seq_file *m, void *p)\n{\n\tstruct cache_head *cp = p;\n\tstruct cache_detail *cd = m->private;\n\n\tif (p == SEQ_START_TOKEN)\n\t\treturn cd->cache_show(m, cd, NULL);\n\n\tifdebug(CACHE)\n\t\tseq_printf(m, \"# expiry=%lld refcnt=%d flags=%lx\\n\",\n\t\t\t   convert_to_wallclock(cp->expiry_time),\n\t\t\t   kref_read(&cp->ref), cp->flags);\n\tcache_get(cp);\n\tif (cache_check(cd, cp, NULL))\n\t\t/* cache_check does a cache_put on failure */\n\t\tseq_puts(m, \"# \");\n\telse {\n\t\tif (cache_is_expired(cd, cp))\n\t\t\tseq_puts(m, \"# \");\n\t\tcache_put(cp, cd);\n\t}\n\n\treturn cd->cache_show(m, cd, cp);\n}\n\nstatic const struct seq_operations cache_content_op = {\n\t.start\t= cache_seq_start_rcu,\n\t.next\t= cache_seq_next_rcu,\n\t.stop\t= cache_seq_stop_rcu,\n\t.show\t= c_show,\n};\n\nstatic int content_open(struct inode *inode, struct file *file,\n\t\t\tstruct cache_detail *cd)\n{\n\tstruct seq_file *seq;\n\tint err;\n\n\tif (!cd || !try_module_get(cd->owner))\n\t\treturn -EACCES;\n\n\terr = seq_open(file, &cache_content_op);\n\tif (err) {\n\t\tmodule_put(cd->owner);\n\t\treturn err;\n\t}\n\n\tseq = file->private_data;\n\tseq->private = cd;\n\treturn 0;\n}\n\nstatic int content_release(struct inode *inode, struct file *file,\n\t\tstruct cache_detail *cd)\n{\n\tint ret = seq_release(inode, file);\n\tmodule_put(cd->owner);\n\treturn ret;\n}\n\nstatic int open_flush(struct inode *inode, struct file *file,\n\t\t\tstruct cache_detail *cd)\n{\n\tif (!cd || !try_module_get(cd->owner))\n\t\treturn -EACCES;\n\treturn nonseekable_open(inode, file);\n}\n\nstatic int release_flush(struct inode *inode, struct file *file,\n\t\t\tstruct cache_detail *cd)\n{\n\tmodule_put(cd->owner);\n\treturn 0;\n}\n\nstatic ssize_t read_flush(struct file *file, char __user *buf,\n\t\t\t  size_t count, loff_t *ppos,\n\t\t\t  struct cache_detail *cd)\n{\n\tchar tbuf[22];\n\tsize_t len;\n\n\tlen = snprintf(tbuf, sizeof(tbuf), \"%llu\\n\",\n\t\t\tconvert_to_wallclock(cd->flush_time));\n\treturn simple_read_from_buffer(buf, count, ppos, tbuf, len);\n}\n\nstatic ssize_t write_flush(struct file *file, const char __user *buf,\n\t\t\t   size_t count, loff_t *ppos,\n\t\t\t   struct cache_detail *cd)\n{\n\tchar tbuf[20];\n\tchar *ep;\n\ttime64_t now;\n\n\tif (*ppos || count > sizeof(tbuf)-1)\n\t\treturn -EINVAL;\n\tif (copy_from_user(tbuf, buf, count))\n\t\treturn -EFAULT;\n\ttbuf[count] = 0;\n\tsimple_strtoul(tbuf, &ep, 0);\n\tif (*ep && *ep != '\\n')\n\t\treturn -EINVAL;\n\t/* Note that while we check that 'buf' holds a valid number,\n\t * we always ignore the value and just flush everything.\n\t * Making use of the number leads to races.\n\t */\n\n\tnow = seconds_since_boot();\n\t/* Always flush everything, so behave like cache_purge()\n\t * Do this by advancing flush_time to the current time,\n\t * or by one second if it has already reached the current time.\n\t * Newly added cache entries will always have ->last_refresh greater\n\t * that ->flush_time, so they don't get flushed prematurely.\n\t */\n\n\tif (cd->flush_time >= now)\n\t\tnow = cd->flush_time + 1;\n\n\tcd->flush_time = now;\n\tcd->nextcheck = now;\n\tcache_flush();\n\n\tif (cd->flush)\n\t\tcd->flush();\n\n\t*ppos += count;\n\treturn count;\n}\n\nstatic ssize_t cache_read_procfs(struct file *filp, char __user *buf,\n\t\t\t\t size_t count, loff_t *ppos)\n{\n\tstruct cache_detail *cd = PDE_DATA(file_inode(filp));\n\n\treturn cache_read(filp, buf, count, ppos, cd);\n}\n\nstatic ssize_t cache_write_procfs(struct file *filp, const char __user *buf,\n\t\t\t\t  size_t count, loff_t *ppos)\n{\n\tstruct cache_detail *cd = PDE_DATA(file_inode(filp));\n\n\treturn cache_write(filp, buf, count, ppos, cd);\n}\n\nstatic __poll_t cache_poll_procfs(struct file *filp, poll_table *wait)\n{\n\tstruct cache_detail *cd = PDE_DATA(file_inode(filp));\n\n\treturn cache_poll(filp, wait, cd);\n}\n\nstatic long cache_ioctl_procfs(struct file *filp,\n\t\t\t       unsigned int cmd, unsigned long arg)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct cache_detail *cd = PDE_DATA(inode);\n\n\treturn cache_ioctl(inode, filp, cmd, arg, cd);\n}\n\nstatic int cache_open_procfs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = PDE_DATA(inode);\n\n\treturn cache_open(inode, filp, cd);\n}\n\nstatic int cache_release_procfs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = PDE_DATA(inode);\n\n\treturn cache_release(inode, filp, cd);\n}\n\nstatic const struct proc_ops cache_channel_proc_ops = {\n\t.proc_lseek\t= no_llseek,\n\t.proc_read\t= cache_read_procfs,\n\t.proc_write\t= cache_write_procfs,\n\t.proc_poll\t= cache_poll_procfs,\n\t.proc_ioctl\t= cache_ioctl_procfs, /* for FIONREAD */\n\t.proc_open\t= cache_open_procfs,\n\t.proc_release\t= cache_release_procfs,\n};\n\nstatic int content_open_procfs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = PDE_DATA(inode);\n\n\treturn content_open(inode, filp, cd);\n}\n\nstatic int content_release_procfs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = PDE_DATA(inode);\n\n\treturn content_release(inode, filp, cd);\n}\n\nstatic const struct proc_ops content_proc_ops = {\n\t.proc_open\t= content_open_procfs,\n\t.proc_read\t= seq_read,\n\t.proc_lseek\t= seq_lseek,\n\t.proc_release\t= content_release_procfs,\n};\n\nstatic int open_flush_procfs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = PDE_DATA(inode);\n\n\treturn open_flush(inode, filp, cd);\n}\n\nstatic int release_flush_procfs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = PDE_DATA(inode);\n\n\treturn release_flush(inode, filp, cd);\n}\n\nstatic ssize_t read_flush_procfs(struct file *filp, char __user *buf,\n\t\t\t    size_t count, loff_t *ppos)\n{\n\tstruct cache_detail *cd = PDE_DATA(file_inode(filp));\n\n\treturn read_flush(filp, buf, count, ppos, cd);\n}\n\nstatic ssize_t write_flush_procfs(struct file *filp,\n\t\t\t\t  const char __user *buf,\n\t\t\t\t  size_t count, loff_t *ppos)\n{\n\tstruct cache_detail *cd = PDE_DATA(file_inode(filp));\n\n\treturn write_flush(filp, buf, count, ppos, cd);\n}\n\nstatic const struct proc_ops cache_flush_proc_ops = {\n\t.proc_open\t= open_flush_procfs,\n\t.proc_read\t= read_flush_procfs,\n\t.proc_write\t= write_flush_procfs,\n\t.proc_release\t= release_flush_procfs,\n\t.proc_lseek\t= no_llseek,\n};\n\nstatic void remove_cache_proc_entries(struct cache_detail *cd)\n{\n\tif (cd->procfs) {\n\t\tproc_remove(cd->procfs);\n\t\tcd->procfs = NULL;\n\t}\n}\n\n#ifdef CONFIG_PROC_FS\nstatic int create_cache_proc_entries(struct cache_detail *cd, struct net *net)\n{\n\tstruct proc_dir_entry *p;\n\tstruct sunrpc_net *sn;\n\n\tsn = net_generic(net, sunrpc_net_id);\n\tcd->procfs = proc_mkdir(cd->name, sn->proc_net_rpc);\n\tif (cd->procfs == NULL)\n\t\tgoto out_nomem;\n\n\tp = proc_create_data(\"flush\", S_IFREG | 0600,\n\t\t\t     cd->procfs, &cache_flush_proc_ops, cd);\n\tif (p == NULL)\n\t\tgoto out_nomem;\n\n\tif (cd->cache_request || cd->cache_parse) {\n\t\tp = proc_create_data(\"channel\", S_IFREG | 0600, cd->procfs,\n\t\t\t\t     &cache_channel_proc_ops, cd);\n\t\tif (p == NULL)\n\t\t\tgoto out_nomem;\n\t}\n\tif (cd->cache_show) {\n\t\tp = proc_create_data(\"content\", S_IFREG | 0400, cd->procfs,\n\t\t\t\t     &content_proc_ops, cd);\n\t\tif (p == NULL)\n\t\t\tgoto out_nomem;\n\t}\n\treturn 0;\nout_nomem:\n\tremove_cache_proc_entries(cd);\n\treturn -ENOMEM;\n}\n#else /* CONFIG_PROC_FS */\nstatic int create_cache_proc_entries(struct cache_detail *cd, struct net *net)\n{\n\treturn 0;\n}\n#endif\n\nvoid __init cache_initialize(void)\n{\n\tINIT_DEFERRABLE_WORK(&cache_cleaner, do_cache_clean);\n}\n\nint cache_register_net(struct cache_detail *cd, struct net *net)\n{\n\tint ret;\n\n\tsunrpc_init_cache_detail(cd);\n\tret = create_cache_proc_entries(cd, net);\n\tif (ret)\n\t\tsunrpc_destroy_cache_detail(cd);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(cache_register_net);\n\nvoid cache_unregister_net(struct cache_detail *cd, struct net *net)\n{\n\tremove_cache_proc_entries(cd);\n\tsunrpc_destroy_cache_detail(cd);\n}\nEXPORT_SYMBOL_GPL(cache_unregister_net);\n\nstruct cache_detail *cache_create_net(const struct cache_detail *tmpl, struct net *net)\n{\n\tstruct cache_detail *cd;\n\tint i;\n\n\tcd = kmemdup(tmpl, sizeof(struct cache_detail), GFP_KERNEL);\n\tif (cd == NULL)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcd->hash_table = kcalloc(cd->hash_size, sizeof(struct hlist_head),\n\t\t\t\t GFP_KERNEL);\n\tif (cd->hash_table == NULL) {\n\t\tkfree(cd);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tfor (i = 0; i < cd->hash_size; i++)\n\t\tINIT_HLIST_HEAD(&cd->hash_table[i]);\n\tcd->net = net;\n\treturn cd;\n}\nEXPORT_SYMBOL_GPL(cache_create_net);\n\nvoid cache_destroy_net(struct cache_detail *cd, struct net *net)\n{\n\tkfree(cd->hash_table);\n\tkfree(cd);\n}\nEXPORT_SYMBOL_GPL(cache_destroy_net);\n\nstatic ssize_t cache_read_pipefs(struct file *filp, char __user *buf,\n\t\t\t\t size_t count, loff_t *ppos)\n{\n\tstruct cache_detail *cd = RPC_I(file_inode(filp))->private;\n\n\treturn cache_read(filp, buf, count, ppos, cd);\n}\n\nstatic ssize_t cache_write_pipefs(struct file *filp, const char __user *buf,\n\t\t\t\t  size_t count, loff_t *ppos)\n{\n\tstruct cache_detail *cd = RPC_I(file_inode(filp))->private;\n\n\treturn cache_write(filp, buf, count, ppos, cd);\n}\n\nstatic __poll_t cache_poll_pipefs(struct file *filp, poll_table *wait)\n{\n\tstruct cache_detail *cd = RPC_I(file_inode(filp))->private;\n\n\treturn cache_poll(filp, wait, cd);\n}\n\nstatic long cache_ioctl_pipefs(struct file *filp,\n\t\t\t      unsigned int cmd, unsigned long arg)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct cache_detail *cd = RPC_I(inode)->private;\n\n\treturn cache_ioctl(inode, filp, cmd, arg, cd);\n}\n\nstatic int cache_open_pipefs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = RPC_I(inode)->private;\n\n\treturn cache_open(inode, filp, cd);\n}\n\nstatic int cache_release_pipefs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = RPC_I(inode)->private;\n\n\treturn cache_release(inode, filp, cd);\n}\n\nconst struct file_operations cache_file_operations_pipefs = {\n\t.owner\t\t= THIS_MODULE,\n\t.llseek\t\t= no_llseek,\n\t.read\t\t= cache_read_pipefs,\n\t.write\t\t= cache_write_pipefs,\n\t.poll\t\t= cache_poll_pipefs,\n\t.unlocked_ioctl\t= cache_ioctl_pipefs, /* for FIONREAD */\n\t.open\t\t= cache_open_pipefs,\n\t.release\t= cache_release_pipefs,\n};\n\nstatic int content_open_pipefs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = RPC_I(inode)->private;\n\n\treturn content_open(inode, filp, cd);\n}\n\nstatic int content_release_pipefs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = RPC_I(inode)->private;\n\n\treturn content_release(inode, filp, cd);\n}\n\nconst struct file_operations content_file_operations_pipefs = {\n\t.open\t\t= content_open_pipefs,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= content_release_pipefs,\n};\n\nstatic int open_flush_pipefs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = RPC_I(inode)->private;\n\n\treturn open_flush(inode, filp, cd);\n}\n\nstatic int release_flush_pipefs(struct inode *inode, struct file *filp)\n{\n\tstruct cache_detail *cd = RPC_I(inode)->private;\n\n\treturn release_flush(inode, filp, cd);\n}\n\nstatic ssize_t read_flush_pipefs(struct file *filp, char __user *buf,\n\t\t\t    size_t count, loff_t *ppos)\n{\n\tstruct cache_detail *cd = RPC_I(file_inode(filp))->private;\n\n\treturn read_flush(filp, buf, count, ppos, cd);\n}\n\nstatic ssize_t write_flush_pipefs(struct file *filp,\n\t\t\t\t  const char __user *buf,\n\t\t\t\t  size_t count, loff_t *ppos)\n{\n\tstruct cache_detail *cd = RPC_I(file_inode(filp))->private;\n\n\treturn write_flush(filp, buf, count, ppos, cd);\n}\n\nconst struct file_operations cache_flush_operations_pipefs = {\n\t.open\t\t= open_flush_pipefs,\n\t.read\t\t= read_flush_pipefs,\n\t.write\t\t= write_flush_pipefs,\n\t.release\t= release_flush_pipefs,\n\t.llseek\t\t= no_llseek,\n};\n\nint sunrpc_cache_register_pipefs(struct dentry *parent,\n\t\t\t\t const char *name, umode_t umode,\n\t\t\t\t struct cache_detail *cd)\n{\n\tstruct dentry *dir = rpc_create_cache_dir(parent, name, umode, cd);\n\tif (IS_ERR(dir))\n\t\treturn PTR_ERR(dir);\n\tcd->pipefs = dir;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(sunrpc_cache_register_pipefs);\n\nvoid sunrpc_cache_unregister_pipefs(struct cache_detail *cd)\n{\n\tif (cd->pipefs) {\n\t\trpc_remove_cache_dir(cd->pipefs);\n\t\tcd->pipefs = NULL;\n\t}\n}\nEXPORT_SYMBOL_GPL(sunrpc_cache_unregister_pipefs);\n\nvoid sunrpc_cache_unhash(struct cache_detail *cd, struct cache_head *h)\n{\n\tspin_lock(&cd->hash_lock);\n\tif (!hlist_unhashed(&h->cache_list)){\n\t\tsunrpc_begin_cache_remove_entry(h, cd);\n\t\tspin_unlock(&cd->hash_lock);\n\t\tsunrpc_end_cache_remove_entry(h, cd);\n\t} else\n\t\tspin_unlock(&cd->hash_lock);\n}\nEXPORT_SYMBOL_GPL(sunrpc_cache_unhash);\n"}}, "reports": [{"events": [{"location": {"col": 11, "file": 1, "line": 473}, "message": "Left side of '&&' is false"}, {"location": {"col": 61, "file": 2, "line": 687}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 11, "file": 1, "line": 473}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 2, "line": 687}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 37, "file": 3, "line": 39}, "message": "expanded from macro 'BUILD_BUG_ON_MSG'"}, {"location": {"col": 2, "file": 4, "line": 326}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 4, "line": 314}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 4, "line": 306}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 11, "file": 1, "line": 473}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 2, "line": 687}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 37, "file": 3, "line": 39}, "message": "expanded from macro 'BUILD_BUG_ON_MSG'"}, {"location": {"col": 2, "file": 4, "line": 326}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 4, "line": 314}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 4, "line": 304}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 19, "file": 1, "line": 475}, "message": "Assuming 'audit_watch_group' is equal to field 'group'"}, {"location": {"col": 25, "file": 5, "line": 102}, "message": "expanded from macro 'WARN_ON_ONCE'"}, {"location": {"col": 6, "file": 1, "line": 475}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 5, "line": 103}, "message": "expanded from macro 'WARN_ON_ONCE'"}, {"location": {"col": 6, "file": 1, "line": 475}, "message": "Left side of '||' is false"}, {"location": {"col": 33, "file": 5, "line": 101}, "message": "expanded from macro 'WARN_ON_ONCE'"}, {"location": {"col": 19, "file": 1, "line": 476}, "message": "Assuming 'inode' is non-null"}, {"location": {"col": 25, "file": 5, "line": 102}, "message": "expanded from macro 'WARN_ON_ONCE'"}, {"location": {"col": 6, "file": 1, "line": 476}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 5, "line": 103}, "message": "expanded from macro 'WARN_ON_ONCE'"}, {"location": {"col": 2, "file": 1, "line": 475}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 1, "line": 479}, "message": "Assuming the condition is true"}, {"location": {"col": 6, "file": 1, "line": 479}, "message": "Left side of '&&' is true"}, {"location": {"col": 40, "file": 1, "line": 479}, "message": "'inode' is non-null"}, {"location": {"col": 2, "file": 1, "line": 479}, "message": "Taking true branch"}, {"location": {"col": 3, "file": 1, "line": 480}, "message": "Calling 'audit_update_watch'"}, {"location": {"col": 2, "file": 1, "line": 255}, "message": "Left side of '&&' is false"}, {"location": {"col": 13, "file": 0, "line": 715}, "message": "expanded from macro 'list_for_each_entry_safe'"}, {"location": {"col": 2, "file": 0, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 0, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 61, "file": 2, "line": 687}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 2, "file": 1, "line": 255}, "message": "Taking false branch"}, {"location": {"col": 13, "file": 0, "line": 715}, "message": "expanded from macro 'list_for_each_entry_safe'"}, {"location": {"col": 2, "file": 0, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 0, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 2, "file": 0, "line": 86}, "message": "Use of memory after it is freed"}], "macros": [], "notes": [], "path": "/src/include/linux/list.h", "reportHash": "21b1aeef8bbf6ac342b157d5b324c793", "checkerName": "clang-analyzer-unix.Malloc", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 2, "file": 6, "line": 8507}, "message": "Calling '__io_remove_buffers'"}, {"location": {"col": 7, "file": 6, "line": 4146}, "message": "'nbufs' is 4294967295"}, {"location": {"col": 2, "file": 6, "line": 4146}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 6, "line": 4150}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 9, "file": 6, "line": 4153}, "message": "Left side of '&&' is false"}, {"location": {"col": 2, "file": 0, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 0, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 61, "file": 2, "line": 687}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 9, "file": 6, "line": 4153}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 0, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 0, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 2, "file": 2, "line": 687}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 13, "file": 0, "line": 135}, "message": "Use of memory after it is freed"}], "macros": [], "notes": [], "path": "/src/include/linux/list.h", "reportHash": "b3c01708b0cee9cdd4f2eac7192b166e", "checkerName": "clang-analyzer-unix.Malloc", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 25, "file": 6, "line": 2149}, "message": "Left side of '&&' is false"}, {"location": {"col": 61, "file": 2, "line": 687}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 25, "file": 6, "line": 2149}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 2, "line": 687}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 37, "file": 3, "line": 39}, "message": "expanded from macro 'BUILD_BUG_ON_MSG'"}, {"location": {"col": 2, "file": 4, "line": 326}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 4, "line": 314}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 4, "line": 306}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 25, "file": 6, "line": 2149}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 2, "line": 687}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 37, "file": 3, "line": 39}, "message": "expanded from macro 'BUILD_BUG_ON_MSG'"}, {"location": {"col": 2, "file": 4, "line": 326}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 4, "line": 314}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 4, "line": 304}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 6, "line": 2152}, "message": "Calling '__io_req_task_submit'"}, {"location": {"col": 6, "file": 6, "line": 2137}, "message": "Left side of '&&' is true"}, {"location": {"col": 2, "file": 6, "line": 2137}, "message": "Taking true branch"}, {"location": {"col": 3, "file": 6, "line": 2140}, "message": "Calling '__io_queue_sqe'"}, {"location": {"col": 7, "file": 6, "line": 6446}, "message": "Assuming the condition is false"}, {"location": {"col": 44, "file": 6, "line": 6446}, "message": "Left side of '&&' is false"}, {"location": {"col": 8, "file": 6, "line": 6457}, "message": "Calling 'io_issue_sqe'"}, {"location": {"col": 2, "file": 6, "line": 6162}, "message": "Control jumps to 'case IORING_OP_REMOVE_BUFFERS:'  at line 6248"}, {"location": {"col": 9, "file": 6, "line": 6249}, "message": "Calling 'io_remove_buffers'"}, {"location": {"col": 2, "file": 6, "line": 4176}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 34, "file": 7, "line": 386}, "message": "expanded from macro 'lockdep_assert_held'"}, {"location": {"col": 6, "file": 6, "line": 4180}, "message": "Assuming 'head' is non-null"}, {"location": {"col": 2, "file": 6, "line": 4180}, "message": "Taking true branch"}, {"location": {"col": 9, "file": 6, "line": 4181}, "message": "Calling '__io_remove_buffers'"}, {"location": {"col": 6, "file": 6, "line": 4146}, "message": "Assuming 'nbufs' is not equal to 0"}, {"location": {"col": 2, "file": 6, "line": 4146}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 6, "line": 4150}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 9, "file": 6, "line": 4153}, "message": "Left side of '&&' is false"}, {"location": {"col": 2, "file": 0, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 0, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 61, "file": 2, "line": 687}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 9, "file": 6, "line": 4153}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 0, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 0, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 2, "file": 2, "line": 687}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 14, "file": 0, "line": 147}, "message": "Use of memory after it is freed"}], "macros": [], "notes": [], "path": "/src/include/linux/list.h", "reportHash": "874993d918e218cfb0bc7249989dc39a", "checkerName": "clang-analyzer-unix.Malloc", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 6, "file": 8, "line": 1623}, "message": "Assuming the condition is true"}, {"location": {"col": 2, "file": 8, "line": 1623}, "message": "Taking true branch"}, {"location": {"col": 3, "file": 8, "line": 1624}, "message": "Calling 'isolate_freepages'"}, {"location": {"col": 22, "file": 8, "line": 1512}, "message": "Calling 'fast_isolate_freepages'"}, {"location": {"col": 23, "file": 8, "line": 1343}, "message": "Assuming '__UNIQUE_ID___x448' is >= '__UNIQUE_ID___y449'"}, {"location": {"col": 19, "file": 9, "line": 51}, "message": "expanded from macro 'min'"}, {"location": {"col": 3, "file": 9, "line": 44}, "message": "expanded from macro '__careful_cmp'"}, {"location": {"col": 3, "file": 9, "line": 39}, "message": "expanded from macro '__cmp_once'"}, {"location": {"col": 26, "file": 9, "line": 34}, "message": "expanded from macro '__cmp'"}, {"location": {"col": 23, "file": 8, "line": 1343}, "message": "'?' condition is false"}, {"location": {"col": 19, "file": 9, "line": 51}, "message": "expanded from macro 'min'"}, {"location": {"col": 3, "file": 9, "line": 44}, "message": "expanded from macro '__careful_cmp'"}, {"location": {"col": 3, "file": 9, "line": 39}, "message": "expanded from macro '__cmp_once'"}, {"location": {"col": 26, "file": 9, "line": 34}, "message": "expanded from macro '__cmp'"}, {"location": {"col": 6, "file": 8, "line": 1353}, "message": "Assuming field 'order' is > 0"}, {"location": {"col": 2, "file": 8, "line": 1353}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 8, "line": 1360}, "message": "Assuming field 'free_pfn' is < field 'compact_init_free_pfn'"}, {"location": {"col": 2, "file": 8, "line": 1360}, "message": "Taking false branch"}, {"location": {"col": 19, "file": 8, "line": 1373}, "message": "Assuming 'min_pfn' is <= 'low_pfn'"}, {"location": {"col": 25, "file": 5, "line": 102}, "message": "expanded from macro 'WARN_ON_ONCE'"}, {"location": {"col": 6, "file": 8, "line": 1373}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 5, "line": 103}, "message": "expanded from macro 'WARN_ON_ONCE'"}, {"location": {"col": 2, "file": 8, "line": 1373}, "message": "Taking false branch"}, {"location": {"col": 21, "file": 8, "line": 1380}, "message": "Assuming '__UNIQUE_ID___x451' is < '__UNIQUE_ID___y452'"}, {"location": {"col": 27, "file": 9, "line": 110}, "message": "expanded from macro 'min_t'"}, {"location": {"col": 3, "file": 9, "line": 44}, "message": "expanded from macro '__careful_cmp'"}, {"location": {"col": 3, "file": 9, "line": 39}, "message": "expanded from macro '__cmp_once'"}, {"location": {"col": 26, "file": 9, "line": 34}, "message": "expanded from macro '__cmp'"}, {"location": {"col": 21, "file": 8, "line": 1380}, "message": "'?' condition is true"}, {"location": {"col": 27, "file": 9, "line": 110}, "message": "expanded from macro 'min_t'"}, {"location": {"col": 3, "file": 9, "line": 44}, "message": "expanded from macro '__careful_cmp'"}, {"location": {"col": 3, "file": 9, "line": 39}, "message": "expanded from macro '__cmp_once'"}, {"location": {"col": 26, "file": 9, "line": 34}, "message": "expanded from macro '__cmp'"}, {"location": {"col": 8, "file": 8, "line": 1383}, "message": "'page' is null"}, {"location": {"col": 7, "file": 8, "line": 1383}, "message": "Left side of '&&' is true"}, {"location": {"col": 16, "file": 8, "line": 1383}, "message": "'order' is >= 0"}, {"location": {"col": 2, "file": 8, "line": 1382}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 7, "file": 8, "line": 1391}, "message": "Assuming field 'nr_free' is not equal to 0"}, {"location": {"col": 3, "file": 8, "line": 1391}, "message": "Taking false branch"}, {"location": {"col": 3, "file": 8, "line": 1394}, "message": "Value assigned to field 'prev'"}, {"location": {"col": 2, "file": 10, "line": 384}, "message": "expanded from macro 'spin_lock_irqsave'"}, {"location": {"col": 11, "file": 10, "line": 252}, "message": "expanded from macro 'raw_spin_lock_irqsave'"}, {"location": {"col": 3, "file": 8, "line": 1394}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 10, "line": 384}, "message": "expanded from macro 'spin_lock_irqsave'"}, {"location": {"col": 2, "file": 10, "line": 250}, "message": "expanded from macro 'raw_spin_lock_irqsave'"}, {"location": {"col": 3, "file": 8, "line": 1394}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 43, "file": 10, "line": 382}, "message": "expanded from macro 'spin_lock_irqsave'"}, {"location": {"col": 3, "file": 8, "line": 1396}, "message": "Left side of '&&' is false"}, {"location": {"col": 13, "file": 0, "line": 639}, "message": "expanded from macro 'list_for_each_entry_reverse'"}, {"location": {"col": 2, "file": 0, "line": 533}, "message": "expanded from macro 'list_last_entry'"}, {"location": {"col": 2, "file": 0, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 61, "file": 2, "line": 687}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 3, "file": 8, "line": 1396}, "message": "Taking false branch"}, {"location": {"col": 13, "file": 0, "line": 639}, "message": "expanded from macro 'list_for_each_entry_reverse'"}, {"location": {"col": 2, "file": 0, "line": 533}, "message": "expanded from macro 'list_last_entry'"}, {"location": {"col": 2, "file": 0, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 13, "file": 0, "line": 441}, "message": "Access to field 'next' results in a dereference of a null pointer (loaded from variable 'prev')"}], "macros": [], "notes": [], "path": "/src/include/linux/list.h", "reportHash": "3435eb22b6026eaa5ba595dbdced8f52", "checkerName": "clang-analyzer-core.NullDereference", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 6, "file": 11, "line": 5020}, "message": "Assuming 'from' is <= 'to'"}, {"location": {"col": 2, "file": 11, "line": 5020}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 11, "line": 5030}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 11, "line": 5030}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 11, "line": 5039}, "message": "Assuming 'vma' is non-null"}, {"location": {"col": 6, "file": 11, "line": 5039}, "message": "Left side of '||' is false"}, {"location": {"col": 14, "file": 11, "line": 5039}, "message": "Assuming the condition is true"}, {"location": {"col": 2, "file": 11, "line": 5039}, "message": "Taking true branch"}, {"location": {"col": 6, "file": 11, "line": 5061}, "message": "'chg' is >= 0"}, {"location": {"col": 2, "file": 11, "line": 5061}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 11, "line": 5069}, "message": "'ret' is >= 0"}, {"location": {"col": 2, "file": 11, "line": 5069}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 11, "line": 5074}, "message": "'vma' is non-null"}, {"location": {"col": 6, "file": 11, "line": 5074}, "message": "Left side of '&&' is true"}, {"location": {"col": 44, "file": 11, "line": 5074}, "message": "Left side of '&&' is false"}, {"location": {"col": 6, "file": 11, "line": 5087}, "message": "'gbl_reserve' is >= 0"}, {"location": {"col": 2, "file": 11, "line": 5087}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 11, "line": 5097}, "message": "'ret' is >= 0"}, {"location": {"col": 2, "file": 11, "line": 5097}, "message": "Taking false branch"}, {"location": {"col": 7, "file": 11, "line": 5112}, "message": "'vma' is non-null"}, {"location": {"col": 6, "file": 11, "line": 5112}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 11, "line": 5112}, "message": "Taking true branch"}, {"location": {"col": 9, "file": 11, "line": 5113}, "message": "Calling 'region_add'"}, {"location": {"col": 6, "file": 11, "line": 500}, "message": "'actual_regions_needed' is <= 'in_regions_needed'"}, {"location": {"col": 48, "file": 11, "line": 500}, "message": "Left side of '&&' is false"}, {"location": {"col": 8, "file": 11, "line": 517}, "message": "Calling 'add_reservation_in_range'"}, {"location": {"col": 6, "file": 11, "line": 341}, "message": "'regions_needed' is null"}, {"location": {"col": 2, "file": 11, "line": 341}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 11, "line": 348}, "message": "Left side of '&&' is false"}, {"location": {"col": 13, "file": 0, "line": 715}, "message": "expanded from macro 'list_for_each_entry_safe'"}, {"location": {"col": 2, "file": 0, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 0, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 61, "file": 2, "line": 687}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 2, "file": 11, "line": 348}, "message": "Taking false branch"}, {"location": {"col": 13, "file": 0, "line": 715}, "message": "expanded from macro 'list_for_each_entry_safe'"}, {"location": {"col": 2, "file": 0, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 0, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 13, "file": 0, "line": 135}, "message": "Use of memory after it is freed"}], "macros": [], "notes": [], "path": "/src/include/linux/list.h", "reportHash": "b3c01708b0cee9cdd4f2eac7192b166e", "checkerName": "clang-analyzer-unix.Malloc", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 2, "file": 12, "line": 973}, "message": "'new_xattr' initialized to a null pointer value"}, {"location": {"col": 6, "file": 12, "line": 976}, "message": "Assuming 'removed_size' is null"}, {"location": {"col": 2, "file": 12, "line": 976}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 12, "line": 980}, "message": "Assuming 'value' is null"}, {"location": {"col": 2, "file": 12, "line": 980}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 12, "line": 993}, "message": "Left side of '&&' is false"}, {"location": {"col": 13, "file": 0, "line": 628}, "message": "expanded from macro 'list_for_each_entry'"}, {"location": {"col": 2, "file": 0, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 0, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 61, "file": 2, "line": 687}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 2, "file": 12, "line": 993}, "message": "Taking false branch"}, {"location": {"col": 13, "file": 0, "line": 628}, "message": "expanded from macro 'list_for_each_entry'"}, {"location": {"col": 2, "file": 0, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 0, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 12, "file": 0, "line": 71}, "message": "Access to field 'next' results in a dereference of a null pointer (loaded from variable 'new')"}], "macros": [], "notes": [], "path": "/src/include/linux/list.h", "reportHash": "4a7b11d557672931017ea284e013b5fb", "checkerName": "clang-analyzer-core.NullDereference", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 6, "file": 13, "line": 545}, "message": "Assuming 'buffer_mapping' is not equal to NULL"}, {"location": {"col": 6, "file": 13, "line": 545}, "message": "Left side of '||' is false"}, {"location": {"col": 32, "file": 13, "line": 545}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 13, "line": 545}, "message": "Taking false branch"}, {"location": {"col": 9, "file": 13, "line": 548}, "message": "Calling 'fsync_buffers_list'"}, {"location": {"col": 2, "file": 13, "line": 710}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 8, "file": 13, "line": 711}, "message": "Left side of '&&' is false"}, {"location": {"col": 24, "file": 13, "line": 58}, "message": "expanded from macro 'BH_ENTRY'"}, {"location": {"col": 2, "file": 0, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 61, "file": 2, "line": 687}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 8, "file": 13, "line": 711}, "message": "Taking false branch"}, {"location": {"col": 24, "file": 13, "line": 58}, "message": "expanded from macro 'BH_ENTRY'"}, {"location": {"col": 2, "file": 0, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 2, "file": 2, "line": 687}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 24, "file": 0, "line": 86}, "message": "Access to field 'next' results in a dereference of a null pointer (loaded from variable 'head')"}], "macros": [], "notes": [], "path": "/src/include/linux/list.h", "reportHash": "bd2942a4fc1760f947b63a2699365ee0", "checkerName": "clang-analyzer-core.NullDereference", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 6, "file": 14, "line": 896}, "message": "Assuming field 'init_info' is null"}, {"location": {"col": 21, "file": 14, "line": 896}, "message": "Left side of '&&' is false"}, {"location": {"col": 6, "file": 14, "line": 898}, "message": "Assuming field 'prepare_resources' is null"}, {"location": {"col": 2, "file": 14, "line": 898}, "message": "Taking false branch"}, {"location": {"col": 9, "file": 14, "line": 901}, "message": "Calling 'acpi_pci_probe_root_resources'"}, {"location": {"col": 6, "file": 14, "line": 785}, "message": "Assuming 'ret' is >= 0"}, {"location": {"col": 2, "file": 14, "line": 785}, "message": "Taking false branch"}, {"location": {"col": 11, "file": 14, "line": 788}, "message": "Assuming 'ret' is not equal to 0"}, {"location": {"col": 7, "file": 14, "line": 788}, "message": "Taking false branch"}, {"location": {"col": 3, "file": 14, "line": 792}, "message": "Left side of '&&' is false"}, {"location": {"col": 2, "file": 15, "line": 67}, "message": "expanded from macro 'resource_list_for_each_entry_safe'"}, {"location": {"col": 13, "file": 0, "line": 715}, "message": "expanded from macro 'list_for_each_entry_safe'"}, {"location": {"col": 2, "file": 0, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 0, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 61, "file": 2, "line": 687}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 3, "file": 14, "line": 792}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 15, "line": 67}, "message": "expanded from macro 'resource_list_for_each_entry_safe'"}, {"location": {"col": 13, "file": 0, "line": 715}, "message": "expanded from macro 'list_for_each_entry_safe'"}, {"location": {"col": 2, "file": 0, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 0, "line": 135}, "message": "Use of memory after it is freed"}], "macros": [], "notes": [], "path": "/src/include/linux/list.h", "reportHash": "3d829579b19ca8fb303f26597456353c", "checkerName": "clang-analyzer-unix.Malloc", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 6, "file": 14, "line": 896}, "message": "Assuming field 'init_info' is null"}, {"location": {"col": 21, "file": 14, "line": 896}, "message": "Left side of '&&' is false"}, {"location": {"col": 6, "file": 14, "line": 898}, "message": "Assuming field 'prepare_resources' is null"}, {"location": {"col": 2, "file": 14, "line": 898}, "message": "Taking false branch"}, {"location": {"col": 9, "file": 14, "line": 901}, "message": "Calling 'acpi_pci_probe_root_resources'"}, {"location": {"col": 6, "file": 14, "line": 785}, "message": "Assuming 'ret' is >= 0"}, {"location": {"col": 2, "file": 14, "line": 785}, "message": "Taking false branch"}, {"location": {"col": 11, "file": 14, "line": 788}, "message": "Assuming 'ret' is not equal to 0"}, {"location": {"col": 7, "file": 14, "line": 788}, "message": "Taking false branch"}, {"location": {"col": 3, "file": 14, "line": 792}, "message": "Left side of '&&' is false"}, {"location": {"col": 2, "file": 15, "line": 67}, "message": "expanded from macro 'resource_list_for_each_entry_safe'"}, {"location": {"col": 13, "file": 0, "line": 715}, "message": "expanded from macro 'list_for_each_entry_safe'"}, {"location": {"col": 2, "file": 0, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 0, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 61, "file": 2, "line": 687}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 3, "file": 14, "line": 792}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 15, "line": 67}, "message": "expanded from macro 'resource_list_for_each_entry_safe'"}, {"location": {"col": 13, "file": 0, "line": 715}, "message": "expanded from macro 'list_for_each_entry_safe'"}, {"location": {"col": 2, "file": 0, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 14, "file": 0, "line": 440}, "message": "Use of memory after it is freed"}], "macros": [], "notes": [], "path": "/src/include/linux/list.h", "reportHash": "cb0b19d7331dfbd9e7ad3dd1f2198f16", "checkerName": "clang-analyzer-unix.Malloc", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 2, "file": 16, "line": 1351}, "message": "'nh' initialized to a null pointer value"}, {"location": {"col": 6, "file": 16, "line": 1356}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 16, "line": 1356}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 16, "line": 1360}, "message": "Assuming field 'scope' is <= field 'fc_scope'"}, {"location": {"col": 2, "file": 16, "line": 1360}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 16, "line": 1365}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 16, "line": 1365}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 16, "line": 1371}, "message": "Assuming field 'fc_nh_id' is 0"}, {"location": {"col": 2, "file": 16, "line": 1371}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 16, "line": 1389}, "message": "Assuming field 'fc_mp' is null"}, {"location": {"col": 2, "file": 16, "line": 1389}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 16, "line": 1397}, "message": "Assuming 'fib_info_cnt' is < 'fib_info_hash_size'"}, {"location": {"col": 2, "file": 16, "line": 1397}, "message": "Taking false branch"}, {"location": {"col": 7, "file": 16, "line": 1418}, "message": "Calling 'kzalloc'"}, {"location": {"col": 9, "file": 17, "line": 682}, "message": "Calling 'kmalloc'"}, {"location": {"col": 2, "file": 17, "line": 540}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 17, "line": 557}, "message": "Returning pointer, which participates in a condition later"}, {"location": {"col": 9, "file": 17, "line": 682}, "message": "Returning from 'kmalloc'"}, {"location": {"col": 2, "file": 17, "line": 682}, "message": "Returning pointer, which participates in a condition later"}, {"location": {"col": 7, "file": 16, "line": 1418}, "message": "Returning from 'kzalloc'"}, {"location": {"col": 6, "file": 16, "line": 1419}, "message": "Assuming 'fi' is non-null"}, {"location": {"col": 2, "file": 16, "line": 1419}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 16, "line": 1423}, "message": "Calling 'IS_ERR'"}, {"location": {"col": 9, "file": 18, "line": 36}, "message": "Assuming the condition is false"}, {"location": {"col": 34, "file": 18, "line": 22}, "message": "expanded from macro 'IS_ERR_VALUE'"}, {"location": {"col": 42, "file": 19, "line": 78}, "message": "expanded from macro 'unlikely'"}, {"location": {"col": 2, "file": 18, "line": 36}, "message": "Returning zero, which participates in a condition later"}, {"location": {"col": 6, "file": 16, "line": 1423}, "message": "Returning from 'IS_ERR'"}, {"location": {"col": 2, "file": 16, "line": 1423}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 16, "line": 1440}, "message": "'nh' is null"}, {"location": {"col": 2, "file": 16, "line": 1440}, "message": "Taking false branch"}, {"location": {"col": 3, "file": 16, "line": 1449}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 2, "file": 16, "line": 73}, "message": "expanded from macro 'change_nexthops'"}, {"location": {"col": 3, "file": 16, "line": 1449}, "message": "Loop condition is false. Execution continues on line 1453"}, {"location": {"col": 2, "file": 16, "line": 73}, "message": "expanded from macro 'change_nexthops'"}, {"location": {"col": 12, "file": 16, "line": 1453}, "message": "Field 'fc_mp' is null"}, {"location": {"col": 3, "file": 16, "line": 1453}, "message": "Taking false branch"}, {"location": {"col": 10, "file": 16, "line": 1457}, "message": "Calling 'fib_nh_init'"}, {"location": {"col": 8, "file": 16, "line": 619}, "message": "Calling 'fib_nh_common_init'"}, {"location": {"col": 29, "file": 16, "line": 580}, "message": "Value assigned to field 'error', which participates in a condition later"}, {"location": {"col": 27, "file": 20, "line": 140}, "message": "expanded from macro 'alloc_percpu_gfp'"}, {"location": {"col": 6, "file": 16, "line": 582}, "message": "Assuming field 'nhc_pcpu_rth_output' is non-null"}, {"location": {"col": 2, "file": 16, "line": 582}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 16, "line": 585}, "message": "Assuming 'encap' is null"}, {"location": {"col": 2, "file": 16, "line": 585}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 16, "line": 602}, "message": "Returning without writing to 'cfg->fc_gw_family', which participates in a condition later"}, {"location": {"col": 2, "file": 16, "line": 602}, "message": "Returning without writing to 'cfg->fc_oif', which participates in a condition later"}, {"location": {"col": 2, "file": 16, "line": 602}, "message": "Returning without writing to 'cfg->fc_mp', which participates in a condition later"}, {"location": {"col": 8, "file": 16, "line": 619}, "message": "Returning from 'fib_nh_common_init'"}, {"location": {"col": 6, "file": 16, "line": 621}, "message": "'err' is 0"}, {"location": {"col": 2, "file": 16, "line": 621}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 16, "line": 626}, "message": "Assuming field 'fc_gw_family' is not equal to AF_INET"}, {"location": {"col": 2, "file": 16, "line": 626}, "message": "Taking false branch"}, {"location": {"col": 11, "file": 16, "line": 628}, "message": "Assuming field 'fc_gw_family' is not equal to AF_INET6"}, {"location": {"col": 7, "file": 16, "line": 628}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 16, "line": 641}, "message": "Returning without writing to 'cfg->fc_gw_family', which participates in a condition later"}, {"location": {"col": 2, "file": 16, "line": 641}, "message": "Returning without writing to 'cfg->fc_oif', which participates in a condition later"}, {"location": {"col": 2, "file": 16, "line": 641}, "message": "Returning without writing to 'cfg->fc_mp', which participates in a condition later"}, {"location": {"col": 10, "file": 16, "line": 1457}, "message": "Returning from 'fib_nh_init'"}, {"location": {"col": 6, "file": 16, "line": 1460}, "message": "'err' is equal to 0"}, {"location": {"col": 2, "file": 16, "line": 1460}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 16, "line": 1463}, "message": "Assuming field 'error' is not equal to 0"}, {"location": {"col": 2, "file": 16, "line": 1463}, "message": "Taking true branch"}, {"location": {"col": 7, "file": 16, "line": 1464}, "message": "Assuming field 'fc_gw_family' is 0"}, {"location": {"col": 7, "file": 16, "line": 1464}, "message": "Left side of '||' is false"}, {"location": {"col": 28, "file": 16, "line": 1464}, "message": "Assuming field 'fc_oif' is 0"}, {"location": {"col": 7, "file": 16, "line": 1464}, "message": "Left side of '||' is false"}, {"location": {"col": 48, "file": 16, "line": 1464}, "message": "Field 'fc_mp' is null"}, {"location": {"col": 3, "file": 16, "line": 1464}, "message": "Taking false branch"}, {"location": {"col": 3, "file": 16, "line": 1469}, "message": "Control jumps to line 1545"}, {"location": {"col": 6, "file": 16, "line": 1546}, "message": "Assuming 'ofi' is null"}, {"location": {"col": 2, "file": 16, "line": 1546}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 16, "line": 1558}, "message": "Assuming field 'fib_prefsrc' is 0"}, {"location": {"col": 2, "file": 16, "line": 1558}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 16, "line": 1564}, "message": "Assuming field 'nh' is non-null"}, {"location": {"col": 2, "file": 16, "line": 1564}, "message": "Taking true branch"}, {"location": {"col": 26, "file": 16, "line": 1565}, "message": "Passing null pointer value via 2nd parameter 'head'"}, {"location": {"col": 3, "file": 16, "line": 1565}, "message": "Calling 'list_add'"}, {"location": {"col": 24, "file": 0, "line": 86}, "message": "Access to field 'next' results in a dereference of a null pointer (loaded from variable 'head')"}, {"location": {"col": 24, "file": 0, "line": 86}, "message": "Access to field 'next' results in a dereference of a null pointer (loaded from variable 'head')"}], "macros": [], "notes": [], "path": "/src/include/linux/list.h", "reportHash": "bd2942a4fc1760f947b63a2699365ee0", "checkerName": "clang-analyzer-core.NullDereference", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 6, "file": 21, "line": 1668}, "message": "Assuming 'xprt' is not equal to NULL"}, {"location": {"col": 2, "file": 21, "line": 1668}, "message": "Taking false branch"}, {"location": {"col": 14, "file": 21, "line": 1673}, "message": "Assuming 'i' is < 'num_prealloc'"}, {"location": {"col": 2, "file": 21, "line": 1673}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 7, "file": 21, "line": 1675}, "message": "Assuming 'req' is null"}, {"location": {"col": 3, "file": 21, "line": 1675}, "message": "Taking true branch"}, {"location": {"col": 4, "file": 21, "line": 1676}, "message": "Control jumps to line 1689"}, {"location": {"col": 2, "file": 21, "line": 1689}, "message": "Calling 'xprt_free'"}, {"location": {"col": 2, "file": 21, "line": 1698}, "message": "Calling 'xprt_free_all_slots'"}, {"location": {"col": 2, "file": 21, "line": 1652}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 9, "file": 21, "line": 1653}, "message": "Left side of '&&' is false"}, {"location": {"col": 2, "file": 0, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 0, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 61, "file": 2, "line": 687}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 9, "file": 21, "line": 1653}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 0, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 0, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 2, "file": 2, "line": 687}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 13, "file": 0, "line": 135}, "message": "Use of memory after it is freed"}], "macros": [], "notes": [], "path": "/src/include/linux/list.h", "reportHash": "b3c01708b0cee9cdd4f2eac7192b166e", "checkerName": "clang-analyzer-unix.Malloc", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 2, "file": 22, "line": 1914}, "message": "Taking true branch"}, {"location": {"col": 3, "file": 22, "line": 1917}, "message": "Calling 'sunrpc_end_cache_remove_entry'"}, {"location": {"col": 2, "file": 22, "line": 93}, "message": "Calling 'cache_fresh_unlocked'"}, {"location": {"col": 6, "file": 22, "line": 175}, "message": "Assuming the condition is true"}, {"location": {"col": 2, "file": 22, "line": 175}, "message": "Taking true branch"}, {"location": {"col": 3, "file": 22, "line": 177}, "message": "Calling 'cache_dequeue'"}, {"location": {"col": 2, "file": 22, "line": 1095}, "message": "Left side of '&&' is false"}, {"location": {"col": 13, "file": 0, "line": 715}, "message": "expanded from macro 'list_for_each_entry_safe'"}, {"location": {"col": 2, "file": 0, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 0, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 61, "file": 2, "line": 687}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 2, "file": 22, "line": 1095}, "message": "Taking false branch"}, {"location": {"col": 13, "file": 0, "line": 715}, "message": "expanded from macro 'list_for_each_entry_safe'"}, {"location": {"col": 2, "file": 0, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 0, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 13, "file": 0, "line": 135}, "message": "Use of memory after it is freed"}], "macros": [], "notes": [], "path": "/src/include/linux/list.h", "reportHash": "b3c01708b0cee9cdd4f2eac7192b166e", "checkerName": "clang-analyzer-unix.Malloc", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
