<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"1": {"id": 1, "path": "/src/net/core/dev.c", "content": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n *      NET3    Protocol independent device support routines.\n *\n *\tDerived from the non IP parts of dev.c 1.0.19\n *              Authors:\tRoss Biro\n *\t\t\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\t\t\tMark Evans, <evansmp@uhura.aston.ac.uk>\n *\n *\tAdditional Authors:\n *\t\tFlorian la Roche <rzsfl@rz.uni-sb.de>\n *\t\tAlan Cox <gw4pts@gw4pts.ampr.org>\n *\t\tDavid Hinds <dahinds@users.sourceforge.net>\n *\t\tAlexey Kuznetsov <kuznet@ms2.inr.ac.ru>\n *\t\tAdam Sulmicki <adam@cfar.umd.edu>\n *              Pekka Riikonen <priikone@poesidon.pspt.fi>\n *\n *\tChanges:\n *              D.J. Barrow     :       Fixed bug where dev->refcnt gets set\n *                                      to 2 if register_netdev gets called\n *                                      before net_dev_init & also removed a\n *                                      few lines of code in the process.\n *\t\tAlan Cox\t:\tdevice private ioctl copies fields back.\n *\t\tAlan Cox\t:\tTransmit queue code does relevant\n *\t\t\t\t\tstunts to keep the queue safe.\n *\t\tAlan Cox\t:\tFixed double lock.\n *\t\tAlan Cox\t:\tFixed promisc NULL pointer trap\n *\t\t????????\t:\tSupport the full private ioctl range\n *\t\tAlan Cox\t:\tMoved ioctl permission check into\n *\t\t\t\t\tdrivers\n *\t\tTim Kordas\t:\tSIOCADDMULTI/SIOCDELMULTI\n *\t\tAlan Cox\t:\t100 backlog just doesn't cut it when\n *\t\t\t\t\tyou start doing multicast video 8)\n *\t\tAlan Cox\t:\tRewrote net_bh and list manager.\n *              Alan Cox        :       Fix ETH_P_ALL echoback lengths.\n *\t\tAlan Cox\t:\tTook out transmit every packet pass\n *\t\t\t\t\tSaved a few bytes in the ioctl handler\n *\t\tAlan Cox\t:\tNetwork driver sets packet type before\n *\t\t\t\t\tcalling netif_rx. Saves a function\n *\t\t\t\t\tcall a packet.\n *\t\tAlan Cox\t:\tHashed net_bh()\n *\t\tRichard Kooijman:\tTimestamp fixes.\n *\t\tAlan Cox\t:\tWrong field in SIOCGIFDSTADDR\n *\t\tAlan Cox\t:\tDevice lock protection.\n *              Alan Cox        :       Fixed nasty side effect of device close\n *\t\t\t\t\tchanges.\n *\t\tRudi Cilibrasi\t:\tPass the right thing to\n *\t\t\t\t\tset_mac_address()\n *\t\tDave Miller\t:\t32bit quantity for the device lock to\n *\t\t\t\t\tmake it work out on a Sparc.\n *\t\tBjorn Ekwall\t:\tAdded KERNELD hack.\n *\t\tAlan Cox\t:\tCleaned up the backlog initialise.\n *\t\tCraig Metz\t:\tSIOCGIFCONF fix if space for under\n *\t\t\t\t\t1 device.\n *\t    Thomas Bogendoerfer :\tReturn ENODEV for dev_open, if there\n *\t\t\t\t\tis no device open function.\n *\t\tAndi Kleen\t:\tFix error reporting for SIOCGIFCONF\n *\t    Michael Chastain\t:\tFix signed/unsigned for SIOCGIFCONF\n *\t\tCyrus Durgin\t:\tCleaned for KMOD\n *\t\tAdam Sulmicki   :\tBug Fix : Network Device Unload\n *\t\t\t\t\tA network device unload needs to purge\n *\t\t\t\t\tthe backlog queue.\n *\tPaul Rusty Russell\t:\tSIOCSIFNAME\n *              Pekka Riikonen  :\tNetdev boot-time settings code\n *              Andrew Morton   :       Make unregister_netdevice wait\n *                                      indefinitely on dev->refcnt\n *              J Hadi Salim    :       - Backlog queue sampling\n *\t\t\t\t        - netif_rx() feedback\n */\n\n#include <linux/uaccess.h>\n#include <linux/bitops.h>\n#include <linux/capability.h>\n#include <linux/cpu.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/hash.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/mutex.h>\n#include <linux/rwsem.h>\n#include <linux/string.h>\n#include <linux/mm.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/errno.h>\n#include <linux/interrupt.h>\n#include <linux/if_ether.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/ethtool.h>\n#include <linux/skbuff.h>\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n#include <net/net_namespace.h>\n#include <net/sock.h>\n#include <net/busy_poll.h>\n#include <linux/rtnetlink.h>\n#include <linux/stat.h>\n#include <net/dsa.h>\n#include <net/dst.h>\n#include <net/dst_metadata.h>\n#include <net/pkt_sched.h>\n#include <net/pkt_cls.h>\n#include <net/checksum.h>\n#include <net/xfrm.h>\n#include <linux/highmem.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/netpoll.h>\n#include <linux/rcupdate.h>\n#include <linux/delay.h>\n#include <net/iw_handler.h>\n#include <asm/current.h>\n#include <linux/audit.h>\n#include <linux/dmaengine.h>\n#include <linux/err.h>\n#include <linux/ctype.h>\n#include <linux/if_arp.h>\n#include <linux/if_vlan.h>\n#include <linux/ip.h>\n#include <net/ip.h>\n#include <net/mpls.h>\n#include <linux/ipv6.h>\n#include <linux/in.h>\n#include <linux/jhash.h>\n#include <linux/random.h>\n#include <trace/events/napi.h>\n#include <trace/events/net.h>\n#include <trace/events/skb.h>\n#include <linux/inetdevice.h>\n#include <linux/cpu_rmap.h>\n#include <linux/static_key.h>\n#include <linux/hashtable.h>\n#include <linux/vmalloc.h>\n#include <linux/if_macvlan.h>\n#include <linux/errqueue.h>\n#include <linux/hrtimer.h>\n#include <linux/netfilter_ingress.h>\n#include <linux/crash_dump.h>\n#include <linux/sctp.h>\n#include <net/udp_tunnel.h>\n#include <linux/net_namespace.h>\n#include <linux/indirect_call_wrapper.h>\n#include <net/devlink.h>\n#include <linux/pm_runtime.h>\n#include <linux/prandom.h>\n\n#include \"net-sysfs.h\"\n\n#define MAX_GRO_SKBS 8\n\n/* This should be increased if a protocol with a bigger head is added. */\n#define GRO_MAX_HEAD (MAX_HEADER + 128)\n\nstatic DEFINE_SPINLOCK(ptype_lock);\nstatic DEFINE_SPINLOCK(offload_lock);\nstruct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;\nstruct list_head ptype_all __read_mostly;\t/* Taps */\nstatic struct list_head offload_base __read_mostly;\n\nstatic int netif_rx_internal(struct sk_buff *skb);\nstatic int call_netdevice_notifiers_info(unsigned long val,\n\t\t\t\t\t struct netdev_notifier_info *info);\nstatic int call_netdevice_notifiers_extack(unsigned long val,\n\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t   struct netlink_ext_ack *extack);\nstatic struct napi_struct *napi_by_id(unsigned int napi_id);\n\n/*\n * The @dev_base_head list is protected by @dev_base_lock and the rtnl\n * semaphore.\n *\n * Pure readers hold dev_base_lock for reading, or rcu_read_lock()\n *\n * Writers must hold the rtnl semaphore while they loop through the\n * dev_base_head list, and hold dev_base_lock for writing when they do the\n * actual updates.  This allows pure readers to access the list even\n * while a writer is preparing to update it.\n *\n * To put it another way, dev_base_lock is held for writing only to\n * protect against pure readers; the rtnl semaphore provides the\n * protection against other writers.\n *\n * See, for example usages, register_netdevice() and\n * unregister_netdevice(), which must be called with the rtnl\n * semaphore held.\n */\nDEFINE_RWLOCK(dev_base_lock);\nEXPORT_SYMBOL(dev_base_lock);\n\nstatic DEFINE_MUTEX(ifalias_mutex);\n\n/* protects napi_hash addition/deletion and napi_gen_id */\nstatic DEFINE_SPINLOCK(napi_hash_lock);\n\nstatic unsigned int napi_gen_id = NR_CPUS;\nstatic DEFINE_READ_MOSTLY_HASHTABLE(napi_hash, 8);\n\nstatic DECLARE_RWSEM(devnet_rename_sem);\n\nstatic inline void dev_base_seq_inc(struct net *net)\n{\n\twhile (++net->dev_base_seq == 0)\n\t\t;\n}\n\nstatic inline struct hlist_head *dev_name_hash(struct net *net, const char *name)\n{\n\tunsigned int hash = full_name_hash(net, name, strnlen(name, IFNAMSIZ));\n\n\treturn &net->dev_name_head[hash_32(hash, NETDEV_HASHBITS)];\n}\n\nstatic inline struct hlist_head *dev_index_hash(struct net *net, int ifindex)\n{\n\treturn &net->dev_index_head[ifindex & (NETDEV_HASHENTRIES - 1)];\n}\n\nstatic inline void rps_lock(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tspin_lock(&sd->input_pkt_queue.lock);\n#endif\n}\n\nstatic inline void rps_unlock(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tspin_unlock(&sd->input_pkt_queue.lock);\n#endif\n}\n\nstatic struct netdev_name_node *netdev_name_node_alloc(struct net_device *dev,\n\t\t\t\t\t\t       const char *name)\n{\n\tstruct netdev_name_node *name_node;\n\n\tname_node = kmalloc(sizeof(*name_node), GFP_KERNEL);\n\tif (!name_node)\n\t\treturn NULL;\n\tINIT_HLIST_NODE(&name_node->hlist);\n\tname_node->dev = dev;\n\tname_node->name = name;\n\treturn name_node;\n}\n\nstatic struct netdev_name_node *\nnetdev_name_node_head_alloc(struct net_device *dev)\n{\n\tstruct netdev_name_node *name_node;\n\n\tname_node = netdev_name_node_alloc(dev, dev->name);\n\tif (!name_node)\n\t\treturn NULL;\n\tINIT_LIST_HEAD(&name_node->list);\n\treturn name_node;\n}\n\nstatic void netdev_name_node_free(struct netdev_name_node *name_node)\n{\n\tkfree(name_node);\n}\n\nstatic void netdev_name_node_add(struct net *net,\n\t\t\t\t struct netdev_name_node *name_node)\n{\n\thlist_add_head_rcu(&name_node->hlist,\n\t\t\t   dev_name_hash(net, name_node->name));\n}\n\nstatic void netdev_name_node_del(struct netdev_name_node *name_node)\n{\n\thlist_del_rcu(&name_node->hlist);\n}\n\nstatic struct netdev_name_node *netdev_name_node_lookup(struct net *net,\n\t\t\t\t\t\t\tconst char *name)\n{\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\tstruct netdev_name_node *name_node;\n\n\thlist_for_each_entry(name_node, head, hlist)\n\t\tif (!strcmp(name_node->name, name))\n\t\t\treturn name_node;\n\treturn NULL;\n}\n\nstatic struct netdev_name_node *netdev_name_node_lookup_rcu(struct net *net,\n\t\t\t\t\t\t\t    const char *name)\n{\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\tstruct netdev_name_node *name_node;\n\n\thlist_for_each_entry_rcu(name_node, head, hlist)\n\t\tif (!strcmp(name_node->name, name))\n\t\t\treturn name_node;\n\treturn NULL;\n}\n\nint netdev_name_node_alt_create(struct net_device *dev, const char *name)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net = dev_net(dev);\n\n\tname_node = netdev_name_node_lookup(net, name);\n\tif (name_node)\n\t\treturn -EEXIST;\n\tname_node = netdev_name_node_alloc(dev, name);\n\tif (!name_node)\n\t\treturn -ENOMEM;\n\tnetdev_name_node_add(net, name_node);\n\t/* The node that holds dev->name acts as a head of per-device list. */\n\tlist_add_tail(&name_node->list, &dev->name_node->list);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_name_node_alt_create);\n\nstatic void __netdev_name_node_alt_destroy(struct netdev_name_node *name_node)\n{\n\tlist_del(&name_node->list);\n\tnetdev_name_node_del(name_node);\n\tkfree(name_node->name);\n\tnetdev_name_node_free(name_node);\n}\n\nint netdev_name_node_alt_destroy(struct net_device *dev, const char *name)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net = dev_net(dev);\n\n\tname_node = netdev_name_node_lookup(net, name);\n\tif (!name_node)\n\t\treturn -ENOENT;\n\t/* lookup might have found our primary name or a name belonging\n\t * to another device.\n\t */\n\tif (name_node == dev->name_node || name_node->dev != dev)\n\t\treturn -EINVAL;\n\n\t__netdev_name_node_alt_destroy(name_node);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_name_node_alt_destroy);\n\nstatic void netdev_name_node_alt_flush(struct net_device *dev)\n{\n\tstruct netdev_name_node *name_node, *tmp;\n\n\tlist_for_each_entry_safe(name_node, tmp, &dev->name_node->list, list)\n\t\t__netdev_name_node_alt_destroy(name_node);\n}\n\n/* Device list insertion */\nstatic void list_netdevice(struct net_device *dev)\n{\n\tstruct net *net = dev_net(dev);\n\n\tASSERT_RTNL();\n\n\twrite_lock_bh(&dev_base_lock);\n\tlist_add_tail_rcu(&dev->dev_list, &net->dev_base_head);\n\tnetdev_name_node_add(net, dev->name_node);\n\thlist_add_head_rcu(&dev->index_hlist,\n\t\t\t   dev_index_hash(net, dev->ifindex));\n\twrite_unlock_bh(&dev_base_lock);\n\n\tdev_base_seq_inc(net);\n}\n\n/* Device list removal\n * caller must respect a RCU grace period before freeing/reusing dev\n */\nstatic void unlist_netdevice(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\t/* Unlink dev from the device chain */\n\twrite_lock_bh(&dev_base_lock);\n\tlist_del_rcu(&dev->dev_list);\n\tnetdev_name_node_del(dev->name_node);\n\thlist_del_rcu(&dev->index_hlist);\n\twrite_unlock_bh(&dev_base_lock);\n\n\tdev_base_seq_inc(dev_net(dev));\n}\n\n/*\n *\tOur notifier list\n */\n\nstatic RAW_NOTIFIER_HEAD(netdev_chain);\n\n/*\n *\tDevice drivers call our routines to queue packets here. We empty the\n *\tqueue in the local softnet handler.\n */\n\nDEFINE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);\nEXPORT_PER_CPU_SYMBOL(softnet_data);\n\n#ifdef CONFIG_LOCKDEP\n/*\n * register_netdevice() inits txq->_xmit_lock and sets lockdep class\n * according to dev->type\n */\nstatic const unsigned short netdev_lock_type[] = {\n\t ARPHRD_NETROM, ARPHRD_ETHER, ARPHRD_EETHER, ARPHRD_AX25,\n\t ARPHRD_PRONET, ARPHRD_CHAOS, ARPHRD_IEEE802, ARPHRD_ARCNET,\n\t ARPHRD_APPLETLK, ARPHRD_DLCI, ARPHRD_ATM, ARPHRD_METRICOM,\n\t ARPHRD_IEEE1394, ARPHRD_EUI64, ARPHRD_INFINIBAND, ARPHRD_SLIP,\n\t ARPHRD_CSLIP, ARPHRD_SLIP6, ARPHRD_CSLIP6, ARPHRD_RSRVD,\n\t ARPHRD_ADAPT, ARPHRD_ROSE, ARPHRD_X25, ARPHRD_HWX25,\n\t ARPHRD_PPP, ARPHRD_CISCO, ARPHRD_LAPB, ARPHRD_DDCMP,\n\t ARPHRD_RAWHDLC, ARPHRD_TUNNEL, ARPHRD_TUNNEL6, ARPHRD_FRAD,\n\t ARPHRD_SKIP, ARPHRD_LOOPBACK, ARPHRD_LOCALTLK, ARPHRD_FDDI,\n\t ARPHRD_BIF, ARPHRD_SIT, ARPHRD_IPDDP, ARPHRD_IPGRE,\n\t ARPHRD_PIMREG, ARPHRD_HIPPI, ARPHRD_ASH, ARPHRD_ECONET,\n\t ARPHRD_IRDA, ARPHRD_FCPP, ARPHRD_FCAL, ARPHRD_FCPL,\n\t ARPHRD_FCFABRIC, ARPHRD_IEEE80211, ARPHRD_IEEE80211_PRISM,\n\t ARPHRD_IEEE80211_RADIOTAP, ARPHRD_PHONET, ARPHRD_PHONET_PIPE,\n\t ARPHRD_IEEE802154, ARPHRD_VOID, ARPHRD_NONE};\n\nstatic const char *const netdev_lock_name[] = {\n\t\"_xmit_NETROM\", \"_xmit_ETHER\", \"_xmit_EETHER\", \"_xmit_AX25\",\n\t\"_xmit_PRONET\", \"_xmit_CHAOS\", \"_xmit_IEEE802\", \"_xmit_ARCNET\",\n\t\"_xmit_APPLETLK\", \"_xmit_DLCI\", \"_xmit_ATM\", \"_xmit_METRICOM\",\n\t\"_xmit_IEEE1394\", \"_xmit_EUI64\", \"_xmit_INFINIBAND\", \"_xmit_SLIP\",\n\t\"_xmit_CSLIP\", \"_xmit_SLIP6\", \"_xmit_CSLIP6\", \"_xmit_RSRVD\",\n\t\"_xmit_ADAPT\", \"_xmit_ROSE\", \"_xmit_X25\", \"_xmit_HWX25\",\n\t\"_xmit_PPP\", \"_xmit_CISCO\", \"_xmit_LAPB\", \"_xmit_DDCMP\",\n\t\"_xmit_RAWHDLC\", \"_xmit_TUNNEL\", \"_xmit_TUNNEL6\", \"_xmit_FRAD\",\n\t\"_xmit_SKIP\", \"_xmit_LOOPBACK\", \"_xmit_LOCALTLK\", \"_xmit_FDDI\",\n\t\"_xmit_BIF\", \"_xmit_SIT\", \"_xmit_IPDDP\", \"_xmit_IPGRE\",\n\t\"_xmit_PIMREG\", \"_xmit_HIPPI\", \"_xmit_ASH\", \"_xmit_ECONET\",\n\t\"_xmit_IRDA\", \"_xmit_FCPP\", \"_xmit_FCAL\", \"_xmit_FCPL\",\n\t\"_xmit_FCFABRIC\", \"_xmit_IEEE80211\", \"_xmit_IEEE80211_PRISM\",\n\t\"_xmit_IEEE80211_RADIOTAP\", \"_xmit_PHONET\", \"_xmit_PHONET_PIPE\",\n\t\"_xmit_IEEE802154\", \"_xmit_VOID\", \"_xmit_NONE\"};\n\nstatic struct lock_class_key netdev_xmit_lock_key[ARRAY_SIZE(netdev_lock_type)];\nstatic struct lock_class_key netdev_addr_lock_key[ARRAY_SIZE(netdev_lock_type)];\n\nstatic inline unsigned short netdev_lock_pos(unsigned short dev_type)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(netdev_lock_type); i++)\n\t\tif (netdev_lock_type[i] == dev_type)\n\t\t\treturn i;\n\t/* the last key is used by default */\n\treturn ARRAY_SIZE(netdev_lock_type) - 1;\n}\n\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev_type);\n\tlockdep_set_class_and_name(lock, &netdev_xmit_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev->type);\n\tlockdep_set_class_and_name(&dev->addr_list_lock,\n\t\t\t\t   &netdev_addr_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n#else\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n}\n\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n}\n#endif\n\n/*******************************************************************************\n *\n *\t\tProtocol management and registration routines\n *\n *******************************************************************************/\n\n\n/*\n *\tAdd a protocol ID to the list. Now that the input handler is\n *\tsmarter we can dispense with all the messy stuff that used to be\n *\there.\n *\n *\tBEWARE!!! Protocol handlers, mangling input packets,\n *\tMUST BE last in hash buckets and checking protocol handlers\n *\tMUST start from promiscuous ptype_all chain in net_bh.\n *\tIt is true now, do not change it.\n *\tExplanation follows: if protocol handler, mangling packet, will\n *\tbe the first on list, it is not able to sense, that packet\n *\tis cloned and should be copied-on-write, so that it will\n *\tchange it and subsequent readers will get broken packet.\n *\t\t\t\t\t\t\t--ANK (980803)\n */\n\nstatic inline struct list_head *ptype_head(const struct packet_type *pt)\n{\n\tif (pt->type == htons(ETH_P_ALL))\n\t\treturn pt->dev ? &pt->dev->ptype_all : &ptype_all;\n\telse\n\t\treturn pt->dev ? &pt->dev->ptype_specific :\n\t\t\t\t &ptype_base[ntohs(pt->type) & PTYPE_HASH_MASK];\n}\n\n/**\n *\tdev_add_pack - add packet handler\n *\t@pt: packet type declaration\n *\n *\tAdd a protocol handler to the networking stack. The passed &packet_type\n *\tis linked into kernel lists and may not be freed until it has been\n *\tremoved from the kernel lists.\n *\n *\tThis call does not sleep therefore it can not\n *\tguarantee all CPU's that are in middle of receiving packets\n *\twill see the new packet type (until the next received packet).\n */\n\nvoid dev_add_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\n\tspin_lock(&ptype_lock);\n\tlist_add_rcu(&pt->list, head);\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(dev_add_pack);\n\n/**\n *\t__dev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *      The packet type might still be in use by receivers\n *\tand must not be freed until after all the CPU's have gone\n *\tthrough a quiescent state.\n */\nvoid __dev_remove_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\tstruct packet_type *pt1;\n\n\tspin_lock(&ptype_lock);\n\n\tlist_for_each_entry(pt1, head, list) {\n\t\tif (pt == pt1) {\n\t\t\tlist_del_rcu(&pt->list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpr_warn(\"dev_remove_pack: %p not found\\n\", pt);\nout:\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(__dev_remove_pack);\n\n/**\n *\tdev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *\tThis call sleeps to guarantee that no CPU is looking at the packet\n *\ttype after return.\n */\nvoid dev_remove_pack(struct packet_type *pt)\n{\n\t__dev_remove_pack(pt);\n\n\tsynchronize_net();\n}\nEXPORT_SYMBOL(dev_remove_pack);\n\n\n/**\n *\tdev_add_offload - register offload handlers\n *\t@po: protocol offload declaration\n *\n *\tAdd protocol offload handlers to the networking stack. The passed\n *\t&proto_offload is linked into kernel lists and may not be freed until\n *\tit has been removed from the kernel lists.\n *\n *\tThis call does not sleep therefore it can not\n *\tguarantee all CPU's that are in middle of receiving packets\n *\twill see the new offload handlers (until the next received packet).\n */\nvoid dev_add_offload(struct packet_offload *po)\n{\n\tstruct packet_offload *elem;\n\n\tspin_lock(&offload_lock);\n\tlist_for_each_entry(elem, &offload_base, list) {\n\t\tif (po->priority < elem->priority)\n\t\t\tbreak;\n\t}\n\tlist_add_rcu(&po->list, elem->list.prev);\n\tspin_unlock(&offload_lock);\n}\nEXPORT_SYMBOL(dev_add_offload);\n\n/**\n *\t__dev_remove_offload\t - remove offload handler\n *\t@po: packet offload declaration\n *\n *\tRemove a protocol offload handler that was previously added to the\n *\tkernel offload handlers by dev_add_offload(). The passed &offload_type\n *\tis removed from the kernel lists and can be freed or reused once this\n *\tfunction returns.\n *\n *      The packet type might still be in use by receivers\n *\tand must not be freed until after all the CPU's have gone\n *\tthrough a quiescent state.\n */\nstatic void __dev_remove_offload(struct packet_offload *po)\n{\n\tstruct list_head *head = &offload_base;\n\tstruct packet_offload *po1;\n\n\tspin_lock(&offload_lock);\n\n\tlist_for_each_entry(po1, head, list) {\n\t\tif (po == po1) {\n\t\t\tlist_del_rcu(&po->list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpr_warn(\"dev_remove_offload: %p not found\\n\", po);\nout:\n\tspin_unlock(&offload_lock);\n}\n\n/**\n *\tdev_remove_offload\t - remove packet offload handler\n *\t@po: packet offload declaration\n *\n *\tRemove a packet offload handler that was previously added to the kernel\n *\toffload handlers by dev_add_offload(). The passed &offload_type is\n *\tremoved from the kernel lists and can be freed or reused once this\n *\tfunction returns.\n *\n *\tThis call sleeps to guarantee that no CPU is looking at the packet\n *\ttype after return.\n */\nvoid dev_remove_offload(struct packet_offload *po)\n{\n\t__dev_remove_offload(po);\n\n\tsynchronize_net();\n}\nEXPORT_SYMBOL(dev_remove_offload);\n\n/******************************************************************************\n *\n *\t\t      Device Boot-time Settings Routines\n *\n ******************************************************************************/\n\n/* Boot time configuration table */\nstatic struct netdev_boot_setup dev_boot_setup[NETDEV_BOOT_SETUP_MAX];\n\n/**\n *\tnetdev_boot_setup_add\t- add new setup entry\n *\t@name: name of the device\n *\t@map: configured settings for the device\n *\n *\tAdds new setup entry to the dev_boot_setup list.  The function\n *\treturns 0 on error and 1 on success.  This is a generic routine to\n *\tall netdevices.\n */\nstatic int netdev_boot_setup_add(char *name, struct ifmap *map)\n{\n\tstruct netdev_boot_setup *s;\n\tint i;\n\n\ts = dev_boot_setup;\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++) {\n\t\tif (s[i].name[0] == '\\0' || s[i].name[0] == ' ') {\n\t\t\tmemset(s[i].name, 0, sizeof(s[i].name));\n\t\t\tstrlcpy(s[i].name, name, IFNAMSIZ);\n\t\t\tmemcpy(&s[i].map, map, sizeof(s[i].map));\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn i >= NETDEV_BOOT_SETUP_MAX ? 0 : 1;\n}\n\n/**\n * netdev_boot_setup_check\t- check boot time settings\n * @dev: the netdevice\n *\n * Check boot time settings for the device.\n * The found settings are set for the device to be used\n * later in the device probing.\n * Returns 0 if no settings found, 1 if they are.\n */\nint netdev_boot_setup_check(struct net_device *dev)\n{\n\tstruct netdev_boot_setup *s = dev_boot_setup;\n\tint i;\n\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++) {\n\t\tif (s[i].name[0] != '\\0' && s[i].name[0] != ' ' &&\n\t\t    !strcmp(dev->name, s[i].name)) {\n\t\t\tdev->irq = s[i].map.irq;\n\t\t\tdev->base_addr = s[i].map.base_addr;\n\t\t\tdev->mem_start = s[i].map.mem_start;\n\t\t\tdev->mem_end = s[i].map.mem_end;\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_boot_setup_check);\n\n\n/**\n * netdev_boot_base\t- get address from boot time settings\n * @prefix: prefix for network device\n * @unit: id for network device\n *\n * Check boot time settings for the base address of device.\n * The found settings are set for the device to be used\n * later in the device probing.\n * Returns 0 if no settings found.\n */\nunsigned long netdev_boot_base(const char *prefix, int unit)\n{\n\tconst struct netdev_boot_setup *s = dev_boot_setup;\n\tchar name[IFNAMSIZ];\n\tint i;\n\n\tsprintf(name, \"%s%d\", prefix, unit);\n\n\t/*\n\t * If device already registered then return base of 1\n\t * to indicate not to probe for this interface\n\t */\n\tif (__dev_get_by_name(&init_net, name))\n\t\treturn 1;\n\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++)\n\t\tif (!strcmp(name, s[i].name))\n\t\t\treturn s[i].map.base_addr;\n\treturn 0;\n}\n\n/*\n * Saves at boot time configured settings for any netdevice.\n */\nint __init netdev_boot_setup(char *str)\n{\n\tint ints[5];\n\tstruct ifmap map;\n\n\tstr = get_options(str, ARRAY_SIZE(ints), ints);\n\tif (!str || !*str)\n\t\treturn 0;\n\n\t/* Save settings */\n\tmemset(&map, 0, sizeof(map));\n\tif (ints[0] > 0)\n\t\tmap.irq = ints[1];\n\tif (ints[0] > 1)\n\t\tmap.base_addr = ints[2];\n\tif (ints[0] > 2)\n\t\tmap.mem_start = ints[3];\n\tif (ints[0] > 3)\n\t\tmap.mem_end = ints[4];\n\n\t/* Add new entry to the list */\n\treturn netdev_boot_setup_add(str, &map);\n}\n\n__setup(\"netdev=\", netdev_boot_setup);\n\n/*******************************************************************************\n *\n *\t\t\t    Device Interface Subroutines\n *\n *******************************************************************************/\n\n/**\n *\tdev_get_iflink\t- get 'iflink' value of a interface\n *\t@dev: targeted interface\n *\n *\tIndicates the ifindex the interface is linked to.\n *\tPhysical interfaces have the same 'ifindex' and 'iflink' values.\n */\n\nint dev_get_iflink(const struct net_device *dev)\n{\n\tif (dev->netdev_ops && dev->netdev_ops->ndo_get_iflink)\n\t\treturn dev->netdev_ops->ndo_get_iflink(dev);\n\n\treturn dev->ifindex;\n}\nEXPORT_SYMBOL(dev_get_iflink);\n\n/**\n *\tdev_fill_metadata_dst - Retrieve tunnel egress information.\n *\t@dev: targeted interface\n *\t@skb: The packet.\n *\n *\tFor better visibility of tunnel traffic OVS needs to retrieve\n *\tegress tunnel information for a packet. Following API allows\n *\tuser to get this info.\n */\nint dev_fill_metadata_dst(struct net_device *dev, struct sk_buff *skb)\n{\n\tstruct ip_tunnel_info *info;\n\n\tif (!dev->netdev_ops  || !dev->netdev_ops->ndo_fill_metadata_dst)\n\t\treturn -EINVAL;\n\n\tinfo = skb_tunnel_info_unclone(skb);\n\tif (!info)\n\t\treturn -ENOMEM;\n\tif (unlikely(!(info->mode & IP_TUNNEL_INFO_TX)))\n\t\treturn -EINVAL;\n\n\treturn dev->netdev_ops->ndo_fill_metadata_dst(dev, skb);\n}\nEXPORT_SYMBOL_GPL(dev_fill_metadata_dst);\n\n/**\n *\t__dev_get_by_name\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name. Must be called under RTNL semaphore\n *\tor @dev_base_lock. If the name is found a pointer to the device\n *\tis returned. If the name is not found then %NULL is returned. The\n *\treference counters are not incremented so the caller must be\n *\tcareful with locks.\n */\n\nstruct net_device *__dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct netdev_name_node *node_name;\n\n\tnode_name = netdev_name_node_lookup(net, name);\n\treturn node_name ? node_name->dev : NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_name);\n\n/**\n * dev_get_by_name_rcu\t- find a device by its name\n * @net: the applicable net namespace\n * @name: name to find\n *\n * Find an interface by name.\n * If the name is found a pointer to the device is returned.\n * If the name is not found then %NULL is returned.\n * The reference counters are not incremented so the caller must be\n * careful with locks. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_name_rcu(struct net *net, const char *name)\n{\n\tstruct netdev_name_node *node_name;\n\n\tnode_name = netdev_name_node_lookup_rcu(net, name);\n\treturn node_name ? node_name->dev : NULL;\n}\nEXPORT_SYMBOL(dev_get_by_name_rcu);\n\n/**\n *\tdev_get_by_name\t\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name. This can be called from any\n *\tcontext and does its own locking. The returned handle has\n *\tthe usage count incremented and the caller must use dev_put() to\n *\trelease it when it is no longer needed. %NULL is returned if no\n *\tmatching device is found.\n */\n\nstruct net_device *dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_name_rcu(net, name);\n\tif (dev)\n\t\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_name);\n\n/**\n *\t__dev_get_by_index - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold either the RTNL semaphore\n *\tor @dev_base_lock.\n */\n\nstruct net_device *__dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry(dev, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_index);\n\n/**\n *\tdev_get_by_index_rcu - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_index_rcu(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry_rcu(dev, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_get_by_index_rcu);\n\n\n/**\n *\tdev_get_by_index - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns NULL if the device\n *\tis not found or a pointer to the device. The device returned has\n *\thad a reference added and the pointer is safe until the user calls\n *\tdev_put to indicate they have finished with it.\n */\n\nstruct net_device *dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tif (dev)\n\t\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_index);\n\n/**\n *\tdev_get_by_napi_id - find a device by napi_id\n *\t@napi_id: ID of the NAPI struct\n *\n *\tSearch for an interface by NAPI ID. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not had\n *\tits reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_napi_id(unsigned int napi_id)\n{\n\tstruct napi_struct *napi;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tif (napi_id < MIN_NAPI_ID)\n\t\treturn NULL;\n\n\tnapi = napi_by_id(napi_id);\n\n\treturn napi ? napi->dev : NULL;\n}\nEXPORT_SYMBOL(dev_get_by_napi_id);\n\n/**\n *\tnetdev_get_name - get a netdevice name, knowing its ifindex.\n *\t@net: network namespace\n *\t@name: a pointer to the buffer where the name will be stored.\n *\t@ifindex: the ifindex of the interface to get the name from.\n */\nint netdev_get_name(struct net *net, char *name, int ifindex)\n{\n\tstruct net_device *dev;\n\tint ret;\n\n\tdown_read(&devnet_rename_sem);\n\trcu_read_lock();\n\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tif (!dev) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tstrcpy(name, dev->name);\n\n\tret = 0;\nout:\n\trcu_read_unlock();\n\tup_read(&devnet_rename_sem);\n\treturn ret;\n}\n\n/**\n *\tdev_getbyhwaddr_rcu - find a device by its hardware address\n *\t@net: the applicable net namespace\n *\t@type: media type of device\n *\t@ha: hardware address\n *\n *\tSearch for an interface by MAC address. Returns NULL if the device\n *\tis not found or a pointer to the device.\n *\tThe caller must hold RCU or RTNL.\n *\tThe returned device has not had its ref count increased\n *\tand the caller must therefore be careful about locking\n *\n */\n\nstruct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,\n\t\t\t\t       const char *ha)\n{\n\tstruct net_device *dev;\n\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type &&\n\t\t    !memcmp(dev->dev_addr, ha, dev->addr_len))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_getbyhwaddr_rcu);\n\nstruct net_device *dev_getfirstbyhwtype(struct net *net, unsigned short type)\n{\n\tstruct net_device *dev, *ret = NULL;\n\n\trcu_read_lock();\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type) {\n\t\t\tdev_hold(dev);\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_getfirstbyhwtype);\n\n/**\n *\t__dev_get_by_flags - find any device with given flags\n *\t@net: the applicable net namespace\n *\t@if_flags: IFF_* values\n *\t@mask: bitmask of bits in if_flags to check\n *\n *\tSearch for any interface with the given flags. Returns NULL if a device\n *\tis not found or a pointer to the device. Must be called inside\n *\trtnl_lock(), and result refcount is unchanged.\n */\n\nstruct net_device *__dev_get_by_flags(struct net *net, unsigned short if_flags,\n\t\t\t\t      unsigned short mask)\n{\n\tstruct net_device *dev, *ret;\n\n\tASSERT_RTNL();\n\n\tret = NULL;\n\tfor_each_netdev(net, dev) {\n\t\tif (((dev->flags ^ if_flags) & mask) == 0) {\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(__dev_get_by_flags);\n\n/**\n *\tdev_valid_name - check if name is okay for network device\n *\t@name: name string\n *\n *\tNetwork device names need to be valid file names to\n *\tallow sysfs to work.  We also disallow any kind of\n *\twhitespace.\n */\nbool dev_valid_name(const char *name)\n{\n\tif (*name == '\\0')\n\t\treturn false;\n\tif (strnlen(name, IFNAMSIZ) == IFNAMSIZ)\n\t\treturn false;\n\tif (!strcmp(name, \".\") || !strcmp(name, \"..\"))\n\t\treturn false;\n\n\twhile (*name) {\n\t\tif (*name == '/' || *name == ':' || isspace(*name))\n\t\t\treturn false;\n\t\tname++;\n\t}\n\treturn true;\n}\nEXPORT_SYMBOL(dev_valid_name);\n\n/**\n *\t__dev_alloc_name - allocate a name for a device\n *\t@net: network namespace to allocate the device name in\n *\t@name: name format string\n *\t@buf:  scratch buffer and result name string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nstatic int __dev_alloc_name(struct net *net, const char *name, char *buf)\n{\n\tint i = 0;\n\tconst char *p;\n\tconst int max_netdevices = 8*PAGE_SIZE;\n\tunsigned long *inuse;\n\tstruct net_device *d;\n\n\tif (!dev_valid_name(name))\n\t\treturn -EINVAL;\n\n\tp = strchr(name, '%');\n\tif (p) {\n\t\t/*\n\t\t * Verify the string as this thing may have come from\n\t\t * the user.  There must be either one \"%d\" and no other \"%\"\n\t\t * characters.\n\t\t */\n\t\tif (p[1] != 'd' || strchr(p + 2, '%'))\n\t\t\treturn -EINVAL;\n\n\t\t/* Use one page as a bit array of possible slots */\n\t\tinuse = (unsigned long *) get_zeroed_page(GFP_ATOMIC);\n\t\tif (!inuse)\n\t\t\treturn -ENOMEM;\n\n\t\tfor_each_netdev(net, d) {\n\t\t\tif (!sscanf(d->name, name, &i))\n\t\t\t\tcontinue;\n\t\t\tif (i < 0 || i >= max_netdevices)\n\t\t\t\tcontinue;\n\n\t\t\t/*  avoid cases where sscanf is not exact inverse of printf */\n\t\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\t\t\tif (!strncmp(buf, d->name, IFNAMSIZ))\n\t\t\t\tset_bit(i, inuse);\n\t\t}\n\n\t\ti = find_first_zero_bit(inuse, max_netdevices);\n\t\tfree_page((unsigned long) inuse);\n\t}\n\n\tsnprintf(buf, IFNAMSIZ, name, i);\n\tif (!__dev_get_by_name(net, buf))\n\t\treturn i;\n\n\t/* It is possible to run out of possible slots\n\t * when the name is long and there isn't enough space left\n\t * for the digits, or if all bits are used.\n\t */\n\treturn -ENFILE;\n}\n\nstatic int dev_alloc_name_ns(struct net *net,\n\t\t\t     struct net_device *dev,\n\t\t\t     const char *name)\n{\n\tchar buf[IFNAMSIZ];\n\tint ret;\n\n\tBUG_ON(!net);\n\tret = __dev_alloc_name(net, name, buf);\n\tif (ret >= 0)\n\t\tstrlcpy(dev->name, buf, IFNAMSIZ);\n\treturn ret;\n}\n\n/**\n *\tdev_alloc_name - allocate a name for a device\n *\t@dev: device\n *\t@name: name format string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nint dev_alloc_name(struct net_device *dev, const char *name)\n{\n\treturn dev_alloc_name_ns(dev_net(dev), dev, name);\n}\nEXPORT_SYMBOL(dev_alloc_name);\n\nstatic int dev_get_valid_name(struct net *net, struct net_device *dev,\n\t\t\t      const char *name)\n{\n\tBUG_ON(!net);\n\n\tif (!dev_valid_name(name))\n\t\treturn -EINVAL;\n\n\tif (strchr(name, '%'))\n\t\treturn dev_alloc_name_ns(net, dev, name);\n\telse if (__dev_get_by_name(net, name))\n\t\treturn -EEXIST;\n\telse if (dev->name != name)\n\t\tstrlcpy(dev->name, name, IFNAMSIZ);\n\n\treturn 0;\n}\n\n/**\n *\tdev_change_name - change name of a device\n *\t@dev: device\n *\t@newname: name (or format string) must be at least IFNAMSIZ\n *\n *\tChange name of a device, can pass format strings \"eth%d\".\n *\tfor wildcarding.\n */\nint dev_change_name(struct net_device *dev, const char *newname)\n{\n\tunsigned char old_assign_type;\n\tchar oldname[IFNAMSIZ];\n\tint err = 0;\n\tint ret;\n\tstruct net *net;\n\n\tASSERT_RTNL();\n\tBUG_ON(!dev_net(dev));\n\n\tnet = dev_net(dev);\n\n\t/* Some auto-enslaved devices e.g. failover slaves are\n\t * special, as userspace might rename the device after\n\t * the interface had been brought up and running since\n\t * the point kernel initiated auto-enslavement. Allow\n\t * live name change even when these slave devices are\n\t * up and running.\n\t *\n\t * Typically, users of these auto-enslaving devices\n\t * don't actually care about slave name change, as\n\t * they are supposed to operate on master interface\n\t * directly.\n\t */\n\tif (dev->flags & IFF_UP &&\n\t    likely(!(dev->priv_flags & IFF_LIVE_RENAME_OK)))\n\t\treturn -EBUSY;\n\n\tdown_write(&devnet_rename_sem);\n\n\tif (strncmp(newname, dev->name, IFNAMSIZ) == 0) {\n\t\tup_write(&devnet_rename_sem);\n\t\treturn 0;\n\t}\n\n\tmemcpy(oldname, dev->name, IFNAMSIZ);\n\n\terr = dev_get_valid_name(net, dev, newname);\n\tif (err < 0) {\n\t\tup_write(&devnet_rename_sem);\n\t\treturn err;\n\t}\n\n\tif (oldname[0] && !strchr(oldname, '%'))\n\t\tnetdev_info(dev, \"renamed from %s\\n\", oldname);\n\n\told_assign_type = dev->name_assign_type;\n\tdev->name_assign_type = NET_NAME_RENAMED;\n\nrollback:\n\tret = device_rename(&dev->dev, dev->name);\n\tif (ret) {\n\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\tdev->name_assign_type = old_assign_type;\n\t\tup_write(&devnet_rename_sem);\n\t\treturn ret;\n\t}\n\n\tup_write(&devnet_rename_sem);\n\n\tnetdev_adjacent_rename_links(dev, oldname);\n\n\twrite_lock_bh(&dev_base_lock);\n\tnetdev_name_node_del(dev->name_node);\n\twrite_unlock_bh(&dev_base_lock);\n\n\tsynchronize_rcu();\n\n\twrite_lock_bh(&dev_base_lock);\n\tnetdev_name_node_add(net, dev->name_node);\n\twrite_unlock_bh(&dev_base_lock);\n\n\tret = call_netdevice_notifiers(NETDEV_CHANGENAME, dev);\n\tret = notifier_to_errno(ret);\n\n\tif (ret) {\n\t\t/* err >= 0 after dev_alloc_name() or stores the first errno */\n\t\tif (err >= 0) {\n\t\t\terr = ret;\n\t\t\tdown_write(&devnet_rename_sem);\n\t\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\t\tmemcpy(oldname, newname, IFNAMSIZ);\n\t\t\tdev->name_assign_type = old_assign_type;\n\t\t\told_assign_type = NET_NAME_RENAMED;\n\t\t\tgoto rollback;\n\t\t} else {\n\t\t\tpr_err(\"%s: name change rollback failed: %d\\n\",\n\t\t\t       dev->name, ret);\n\t\t}\n\t}\n\n\treturn err;\n}\n\n/**\n *\tdev_set_alias - change ifalias of a device\n *\t@dev: device\n *\t@alias: name up to IFALIASZ\n *\t@len: limit of bytes to copy from info\n *\n *\tSet ifalias for a device,\n */\nint dev_set_alias(struct net_device *dev, const char *alias, size_t len)\n{\n\tstruct dev_ifalias *new_alias = NULL;\n\n\tif (len >= IFALIASZ)\n\t\treturn -EINVAL;\n\n\tif (len) {\n\t\tnew_alias = kmalloc(sizeof(*new_alias) + len + 1, GFP_KERNEL);\n\t\tif (!new_alias)\n\t\t\treturn -ENOMEM;\n\n\t\tmemcpy(new_alias->ifalias, alias, len);\n\t\tnew_alias->ifalias[len] = 0;\n\t}\n\n\tmutex_lock(&ifalias_mutex);\n\tnew_alias = rcu_replace_pointer(dev->ifalias, new_alias,\n\t\t\t\t\tmutex_is_locked(&ifalias_mutex));\n\tmutex_unlock(&ifalias_mutex);\n\n\tif (new_alias)\n\t\tkfree_rcu(new_alias, rcuhead);\n\n\treturn len;\n}\nEXPORT_SYMBOL(dev_set_alias);\n\n/**\n *\tdev_get_alias - get ifalias of a device\n *\t@dev: device\n *\t@name: buffer to store name of ifalias\n *\t@len: size of buffer\n *\n *\tget ifalias for a device.  Caller must make sure dev cannot go\n *\taway,  e.g. rcu read lock or own a reference count to device.\n */\nint dev_get_alias(const struct net_device *dev, char *name, size_t len)\n{\n\tconst struct dev_ifalias *alias;\n\tint ret = 0;\n\n\trcu_read_lock();\n\talias = rcu_dereference(dev->ifalias);\n\tif (alias)\n\t\tret = snprintf(name, len, \"%s\", alias->ifalias);\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n/**\n *\tnetdev_features_change - device changes features\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed features.\n */\nvoid netdev_features_change(struct net_device *dev)\n{\n\tcall_netdevice_notifiers(NETDEV_FEAT_CHANGE, dev);\n}\nEXPORT_SYMBOL(netdev_features_change);\n\n/**\n *\tnetdev_state_change - device changes state\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed state. This function calls\n *\tthe notifier chains for netdev_chain and sends a NEWLINK message\n *\tto the routing socket.\n */\nvoid netdev_state_change(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tstruct netdev_notifier_change_info change_info = {\n\t\t\t.info.dev = dev,\n\t\t};\n\n\t\tcall_netdevice_notifiers_info(NETDEV_CHANGE,\n\t\t\t\t\t      &change_info.info);\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, 0, GFP_KERNEL);\n\t}\n}\nEXPORT_SYMBOL(netdev_state_change);\n\n/**\n * __netdev_notify_peers - notify network peers about existence of @dev,\n * to be called when rtnl lock is already held.\n * @dev: network device\n *\n * Generate traffic such that interested network peers are aware of\n * @dev, such as by generating a gratuitous ARP. This may be used when\n * a device wants to inform the rest of the network about some sort of\n * reconfiguration such as a failover event or virtual machine\n * migration.\n */\nvoid __netdev_notify_peers(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\tcall_netdevice_notifiers(NETDEV_NOTIFY_PEERS, dev);\n\tcall_netdevice_notifiers(NETDEV_RESEND_IGMP, dev);\n}\nEXPORT_SYMBOL(__netdev_notify_peers);\n\n/**\n * netdev_notify_peers - notify network peers about existence of @dev\n * @dev: network device\n *\n * Generate traffic such that interested network peers are aware of\n * @dev, such as by generating a gratuitous ARP. This may be used when\n * a device wants to inform the rest of the network about some sort of\n * reconfiguration such as a failover event or virtual machine\n * migration.\n */\nvoid netdev_notify_peers(struct net_device *dev)\n{\n\trtnl_lock();\n\t__netdev_notify_peers(dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(netdev_notify_peers);\n\nstatic int __dev_open(struct net_device *dev, struct netlink_ext_ack *extack)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint ret;\n\n\tASSERT_RTNL();\n\n\tif (!netif_device_present(dev)) {\n\t\t/* may be detached because parent is runtime-suspended */\n\t\tif (dev->dev.parent)\n\t\t\tpm_runtime_resume(dev->dev.parent);\n\t\tif (!netif_device_present(dev))\n\t\t\treturn -ENODEV;\n\t}\n\n\t/* Block netpoll from trying to do any rx path servicing.\n\t * If we don't do this there is a chance ndo_poll_controller\n\t * or ndo_poll may be running while we open the device\n\t */\n\tnetpoll_poll_disable(dev);\n\n\tret = call_netdevice_notifiers_extack(NETDEV_PRE_UP, dev, extack);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\tif (ops->ndo_validate_addr)\n\t\tret = ops->ndo_validate_addr(dev);\n\n\tif (!ret && ops->ndo_open)\n\t\tret = ops->ndo_open(dev);\n\n\tnetpoll_poll_enable(dev);\n\n\tif (ret)\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\telse {\n\t\tdev->flags |= IFF_UP;\n\t\tdev_set_rx_mode(dev);\n\t\tdev_activate(dev);\n\t\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\t}\n\n\treturn ret;\n}\n\n/**\n *\tdev_open\t- prepare an interface for use.\n *\t@dev: device to open\n *\t@extack: netlink extended ack\n *\n *\tTakes a device from down to up state. The device's private open\n *\tfunction is invoked and then the multicast lists are loaded. Finally\n *\tthe device is moved into the up state and a %NETDEV_UP message is\n *\tsent to the netdev notifier chain.\n *\n *\tCalling this function on an active interface is a nop. On a failure\n *\ta negative errno code is returned.\n */\nint dev_open(struct net_device *dev, struct netlink_ext_ack *extack)\n{\n\tint ret;\n\n\tif (dev->flags & IFF_UP)\n\t\treturn 0;\n\n\tret = __dev_open(dev, extack);\n\tif (ret < 0)\n\t\treturn ret;\n\n\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP|IFF_RUNNING, GFP_KERNEL);\n\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_open);\n\nstatic void __dev_close_many(struct list_head *head)\n{\n\tstruct net_device *dev;\n\n\tASSERT_RTNL();\n\tmight_sleep();\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\t/* Temporarily disable netpoll until the interface is down */\n\t\tnetpoll_poll_disable(dev);\n\n\t\tcall_netdevice_notifiers(NETDEV_GOING_DOWN, dev);\n\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\n\t\t/* Synchronize to scheduled poll. We cannot touch poll list, it\n\t\t * can be even on different cpu. So just clear netif_running().\n\t\t *\n\t\t * dev->stop() will invoke napi_disable() on all of it's\n\t\t * napi_struct instances on this device.\n\t\t */\n\t\tsmp_mb__after_atomic(); /* Commit netif_running(). */\n\t}\n\n\tdev_deactivate_many(head);\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t\t/*\n\t\t *\tCall the device specific close. This cannot fail.\n\t\t *\tOnly if device is UP\n\t\t *\n\t\t *\tWe allow it to be called even after a DETACH hot-plug\n\t\t *\tevent.\n\t\t */\n\t\tif (ops->ndo_stop)\n\t\t\tops->ndo_stop(dev);\n\n\t\tdev->flags &= ~IFF_UP;\n\t\tnetpoll_poll_enable(dev);\n\t}\n}\n\nstatic void __dev_close(struct net_device *dev)\n{\n\tLIST_HEAD(single);\n\n\tlist_add(&dev->close_list, &single);\n\t__dev_close_many(&single);\n\tlist_del(&single);\n}\n\nvoid dev_close_many(struct list_head *head, bool unlink)\n{\n\tstruct net_device *dev, *tmp;\n\n\t/* Remove the devices that don't need to be closed */\n\tlist_for_each_entry_safe(dev, tmp, head, close_list)\n\t\tif (!(dev->flags & IFF_UP))\n\t\t\tlist_del_init(&dev->close_list);\n\n\t__dev_close_many(head);\n\n\tlist_for_each_entry_safe(dev, tmp, head, close_list) {\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP|IFF_RUNNING, GFP_KERNEL);\n\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t\tif (unlink)\n\t\t\tlist_del_init(&dev->close_list);\n\t}\n}\nEXPORT_SYMBOL(dev_close_many);\n\n/**\n *\tdev_close - shutdown an interface.\n *\t@dev: device to shutdown\n *\n *\tThis function moves an active device into down state. A\n *\t%NETDEV_GOING_DOWN is sent to the netdev notifier chain. The device\n *\tis then deactivated and finally a %NETDEV_DOWN is sent to the notifier\n *\tchain.\n */\nvoid dev_close(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tLIST_HEAD(single);\n\n\t\tlist_add(&dev->close_list, &single);\n\t\tdev_close_many(&single, true);\n\t\tlist_del(&single);\n\t}\n}\nEXPORT_SYMBOL(dev_close);\n\n\n/**\n *\tdev_disable_lro - disable Large Receive Offload on a device\n *\t@dev: device\n *\n *\tDisable Large Receive Offload (LRO) on a net device.  Must be\n *\tcalled under RTNL.  This is needed if received packets may be\n *\tforwarded to another interface.\n */\nvoid dev_disable_lro(struct net_device *dev)\n{\n\tstruct net_device *lower_dev;\n\tstruct list_head *iter;\n\n\tdev->wanted_features &= ~NETIF_F_LRO;\n\tnetdev_update_features(dev);\n\n\tif (unlikely(dev->features & NETIF_F_LRO))\n\t\tnetdev_WARN(dev, \"failed to disable LRO!\\n\");\n\n\tnetdev_for_each_lower_dev(dev, lower_dev, iter)\n\t\tdev_disable_lro(lower_dev);\n}\nEXPORT_SYMBOL(dev_disable_lro);\n\n/**\n *\tdev_disable_gro_hw - disable HW Generic Receive Offload on a device\n *\t@dev: device\n *\n *\tDisable HW Generic Receive Offload (GRO_HW) on a net device.  Must be\n *\tcalled under RTNL.  This is needed if Generic XDP is installed on\n *\tthe device.\n */\nstatic void dev_disable_gro_hw(struct net_device *dev)\n{\n\tdev->wanted_features &= ~NETIF_F_GRO_HW;\n\tnetdev_update_features(dev);\n\n\tif (unlikely(dev->features & NETIF_F_GRO_HW))\n\t\tnetdev_WARN(dev, \"failed to disable GRO_HW!\\n\");\n}\n\nconst char *netdev_cmd_to_name(enum netdev_cmd cmd)\n{\n#define N(val) \t\t\t\t\t\t\\\n\tcase NETDEV_##val:\t\t\t\t\\\n\t\treturn \"NETDEV_\" __stringify(val);\n\tswitch (cmd) {\n\tN(UP) N(DOWN) N(REBOOT) N(CHANGE) N(REGISTER) N(UNREGISTER)\n\tN(CHANGEMTU) N(CHANGEADDR) N(GOING_DOWN) N(CHANGENAME) N(FEAT_CHANGE)\n\tN(BONDING_FAILOVER) N(PRE_UP) N(PRE_TYPE_CHANGE) N(POST_TYPE_CHANGE)\n\tN(POST_INIT) N(RELEASE) N(NOTIFY_PEERS) N(JOIN) N(CHANGEUPPER)\n\tN(RESEND_IGMP) N(PRECHANGEMTU) N(CHANGEINFODATA) N(BONDING_INFO)\n\tN(PRECHANGEUPPER) N(CHANGELOWERSTATE) N(UDP_TUNNEL_PUSH_INFO)\n\tN(UDP_TUNNEL_DROP_INFO) N(CHANGE_TX_QUEUE_LEN)\n\tN(CVLAN_FILTER_PUSH_INFO) N(CVLAN_FILTER_DROP_INFO)\n\tN(SVLAN_FILTER_PUSH_INFO) N(SVLAN_FILTER_DROP_INFO)\n\tN(PRE_CHANGEADDR)\n\t}\n#undef N\n\treturn \"UNKNOWN_NETDEV_EVENT\";\n}\nEXPORT_SYMBOL_GPL(netdev_cmd_to_name);\n\nstatic int call_netdevice_notifier(struct notifier_block *nb, unsigned long val,\n\t\t\t\t   struct net_device *dev)\n{\n\tstruct netdev_notifier_info info = {\n\t\t.dev = dev,\n\t};\n\n\treturn nb->notifier_call(nb, val, &info);\n}\n\nstatic int call_netdevice_register_notifiers(struct notifier_block *nb,\n\t\t\t\t\t     struct net_device *dev)\n{\n\tint err;\n\n\terr = call_netdevice_notifier(nb, NETDEV_REGISTER, dev);\n\terr = notifier_to_errno(err);\n\tif (err)\n\t\treturn err;\n\n\tif (!(dev->flags & IFF_UP))\n\t\treturn 0;\n\n\tcall_netdevice_notifier(nb, NETDEV_UP, dev);\n\treturn 0;\n}\n\nstatic void call_netdevice_unregister_notifiers(struct notifier_block *nb,\n\t\t\t\t\t\tstruct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tcall_netdevice_notifier(nb, NETDEV_GOING_DOWN,\n\t\t\t\t\tdev);\n\t\tcall_netdevice_notifier(nb, NETDEV_DOWN, dev);\n\t}\n\tcall_netdevice_notifier(nb, NETDEV_UNREGISTER, dev);\n}\n\nstatic int call_netdevice_register_net_notifiers(struct notifier_block *nb,\n\t\t\t\t\t\t struct net *net)\n{\n\tstruct net_device *dev;\n\tint err;\n\n\tfor_each_netdev(net, dev) {\n\t\terr = call_netdevice_register_notifiers(nb, dev);\n\t\tif (err)\n\t\t\tgoto rollback;\n\t}\n\treturn 0;\n\nrollback:\n\tfor_each_netdev_continue_reverse(net, dev)\n\t\tcall_netdevice_unregister_notifiers(nb, dev);\n\treturn err;\n}\n\nstatic void call_netdevice_unregister_net_notifiers(struct notifier_block *nb,\n\t\t\t\t\t\t    struct net *net)\n{\n\tstruct net_device *dev;\n\n\tfor_each_netdev(net, dev)\n\t\tcall_netdevice_unregister_notifiers(nb, dev);\n}\n\nstatic int dev_boot_phase = 1;\n\n/**\n * register_netdevice_notifier - register a network notifier block\n * @nb: notifier\n *\n * Register a notifier to be called when network device events occur.\n * The notifier passed is linked into the kernel structures and must\n * not be reused until it has been unregistered. A negative errno code\n * is returned on a failure.\n *\n * When registered all registration and up events are replayed\n * to the new notifier to allow device to have a race free\n * view of the network device list.\n */\n\nint register_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net *net;\n\tint err;\n\n\t/* Close race with setup_net() and cleanup_net() */\n\tdown_write(&pernet_ops_rwsem);\n\trtnl_lock();\n\terr = raw_notifier_chain_register(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\tif (dev_boot_phase)\n\t\tgoto unlock;\n\tfor_each_net(net) {\n\t\terr = call_netdevice_register_net_notifiers(nb, net);\n\t\tif (err)\n\t\t\tgoto rollback;\n\t}\n\nunlock:\n\trtnl_unlock();\n\tup_write(&pernet_ops_rwsem);\n\treturn err;\n\nrollback:\n\tfor_each_net_continue_reverse(net)\n\t\tcall_netdevice_unregister_net_notifiers(nb, net);\n\n\traw_notifier_chain_unregister(&netdev_chain, nb);\n\tgoto unlock;\n}\nEXPORT_SYMBOL(register_netdevice_notifier);\n\n/**\n * unregister_netdevice_notifier - unregister a network notifier block\n * @nb: notifier\n *\n * Unregister a notifier previously registered by\n * register_netdevice_notifier(). The notifier is unlinked into the\n * kernel structures and may then be reused. A negative errno code\n * is returned on a failure.\n *\n * After unregistering unregister and down device events are synthesized\n * for all devices on the device list to the removed notifier to remove\n * the need for special case cleanup code.\n */\n\nint unregister_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net *net;\n\tint err;\n\n\t/* Close race with setup_net() and cleanup_net() */\n\tdown_write(&pernet_ops_rwsem);\n\trtnl_lock();\n\terr = raw_notifier_chain_unregister(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\n\tfor_each_net(net)\n\t\tcall_netdevice_unregister_net_notifiers(nb, net);\n\nunlock:\n\trtnl_unlock();\n\tup_write(&pernet_ops_rwsem);\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier);\n\nstatic int __register_netdevice_notifier_net(struct net *net,\n\t\t\t\t\t     struct notifier_block *nb,\n\t\t\t\t\t     bool ignore_call_fail)\n{\n\tint err;\n\n\terr = raw_notifier_chain_register(&net->netdev_chain, nb);\n\tif (err)\n\t\treturn err;\n\tif (dev_boot_phase)\n\t\treturn 0;\n\n\terr = call_netdevice_register_net_notifiers(nb, net);\n\tif (err && !ignore_call_fail)\n\t\tgoto chain_unregister;\n\n\treturn 0;\n\nchain_unregister:\n\traw_notifier_chain_unregister(&net->netdev_chain, nb);\n\treturn err;\n}\n\nstatic int __unregister_netdevice_notifier_net(struct net *net,\n\t\t\t\t\t       struct notifier_block *nb)\n{\n\tint err;\n\n\terr = raw_notifier_chain_unregister(&net->netdev_chain, nb);\n\tif (err)\n\t\treturn err;\n\n\tcall_netdevice_unregister_net_notifiers(nb, net);\n\treturn 0;\n}\n\n/**\n * register_netdevice_notifier_net - register a per-netns network notifier block\n * @net: network namespace\n * @nb: notifier\n *\n * Register a notifier to be called when network device events occur.\n * The notifier passed is linked into the kernel structures and must\n * not be reused until it has been unregistered. A negative errno code\n * is returned on a failure.\n *\n * When registered all registration and up events are replayed\n * to the new notifier to allow device to have a race free\n * view of the network device list.\n */\n\nint register_netdevice_notifier_net(struct net *net, struct notifier_block *nb)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = __register_netdevice_notifier_net(net, nb, false);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdevice_notifier_net);\n\n/**\n * unregister_netdevice_notifier_net - unregister a per-netns\n *                                     network notifier block\n * @net: network namespace\n * @nb: notifier\n *\n * Unregister a notifier previously registered by\n * register_netdevice_notifier(). The notifier is unlinked into the\n * kernel structures and may then be reused. A negative errno code\n * is returned on a failure.\n *\n * After unregistering unregister and down device events are synthesized\n * for all devices on the device list to the removed notifier to remove\n * the need for special case cleanup code.\n */\n\nint unregister_netdevice_notifier_net(struct net *net,\n\t\t\t\t      struct notifier_block *nb)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = __unregister_netdevice_notifier_net(net, nb);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier_net);\n\nint register_netdevice_notifier_dev_net(struct net_device *dev,\n\t\t\t\t\tstruct notifier_block *nb,\n\t\t\t\t\tstruct netdev_net_notifier *nn)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = __register_netdevice_notifier_net(dev_net(dev), nb, false);\n\tif (!err) {\n\t\tnn->nb = nb;\n\t\tlist_add(&nn->list, &dev->net_notifier_list);\n\t}\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdevice_notifier_dev_net);\n\nint unregister_netdevice_notifier_dev_net(struct net_device *dev,\n\t\t\t\t\t  struct notifier_block *nb,\n\t\t\t\t\t  struct netdev_net_notifier *nn)\n{\n\tint err;\n\n\trtnl_lock();\n\tlist_del(&nn->list);\n\terr = __unregister_netdevice_notifier_net(dev_net(dev), nb);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier_dev_net);\n\nstatic void move_netdevice_notifiers_dev_net(struct net_device *dev,\n\t\t\t\t\t     struct net *net)\n{\n\tstruct netdev_net_notifier *nn;\n\n\tlist_for_each_entry(nn, &dev->net_notifier_list, list) {\n\t\t__unregister_netdevice_notifier_net(dev_net(dev), nn->nb);\n\t\t__register_netdevice_notifier_net(net, nn->nb, true);\n\t}\n}\n\n/**\n *\tcall_netdevice_notifiers_info - call all network notifier blocks\n *\t@val: value passed unmodified to notifier function\n *\t@info: notifier information data\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\n\nstatic int call_netdevice_notifiers_info(unsigned long val,\n\t\t\t\t\t struct netdev_notifier_info *info)\n{\n\tstruct net *net = dev_net(info->dev);\n\tint ret;\n\n\tASSERT_RTNL();\n\n\t/* Run per-netns notifier block chain first, then run the global one.\n\t * Hopefully, one day, the global one is going to be removed after\n\t * all notifier block registrators get converted to be per-netns.\n\t */\n\tret = raw_notifier_call_chain(&net->netdev_chain, val, info);\n\tif (ret & NOTIFY_STOP_MASK)\n\t\treturn ret;\n\treturn raw_notifier_call_chain(&netdev_chain, val, info);\n}\n\nstatic int call_netdevice_notifiers_extack(unsigned long val,\n\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_info info = {\n\t\t.dev = dev,\n\t\t.extack = extack,\n\t};\n\n\treturn call_netdevice_notifiers_info(val, &info);\n}\n\n/**\n *\tcall_netdevice_notifiers - call all network notifier blocks\n *      @val: value passed unmodified to notifier function\n *      @dev: net_device pointer passed unmodified to notifier function\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\n\nint call_netdevice_notifiers(unsigned long val, struct net_device *dev)\n{\n\treturn call_netdevice_notifiers_extack(val, dev, NULL);\n}\nEXPORT_SYMBOL(call_netdevice_notifiers);\n\n/**\n *\tcall_netdevice_notifiers_mtu - call all network notifier blocks\n *\t@val: value passed unmodified to notifier function\n *\t@dev: net_device pointer passed unmodified to notifier function\n *\t@arg: additional u32 argument passed to the notifier function\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\nstatic int call_netdevice_notifiers_mtu(unsigned long val,\n\t\t\t\t\tstruct net_device *dev, u32 arg)\n{\n\tstruct netdev_notifier_info_ext info = {\n\t\t.info.dev = dev,\n\t\t.ext.mtu = arg,\n\t};\n\n\tBUILD_BUG_ON(offsetof(struct netdev_notifier_info_ext, info) != 0);\n\n\treturn call_netdevice_notifiers_info(val, &info.info);\n}\n\n#ifdef CONFIG_NET_INGRESS\nstatic DEFINE_STATIC_KEY_FALSE(ingress_needed_key);\n\nvoid net_inc_ingress_queue(void)\n{\n\tstatic_branch_inc(&ingress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_inc_ingress_queue);\n\nvoid net_dec_ingress_queue(void)\n{\n\tstatic_branch_dec(&ingress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_dec_ingress_queue);\n#endif\n\n#ifdef CONFIG_NET_EGRESS\nstatic DEFINE_STATIC_KEY_FALSE(egress_needed_key);\n\nvoid net_inc_egress_queue(void)\n{\n\tstatic_branch_inc(&egress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_inc_egress_queue);\n\nvoid net_dec_egress_queue(void)\n{\n\tstatic_branch_dec(&egress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_dec_egress_queue);\n#endif\n\nstatic DEFINE_STATIC_KEY_FALSE(netstamp_needed_key);\n#ifdef CONFIG_JUMP_LABEL\nstatic atomic_t netstamp_needed_deferred;\nstatic atomic_t netstamp_wanted;\nstatic void netstamp_clear(struct work_struct *work)\n{\n\tint deferred = atomic_xchg(&netstamp_needed_deferred, 0);\n\tint wanted;\n\n\twanted = atomic_add_return(deferred, &netstamp_wanted);\n\tif (wanted > 0)\n\t\tstatic_branch_enable(&netstamp_needed_key);\n\telse\n\t\tstatic_branch_disable(&netstamp_needed_key);\n}\nstatic DECLARE_WORK(netstamp_work, netstamp_clear);\n#endif\n\nvoid net_enable_timestamp(void)\n{\n#ifdef CONFIG_JUMP_LABEL\n\tint wanted;\n\n\twhile (1) {\n\t\twanted = atomic_read(&netstamp_wanted);\n\t\tif (wanted <= 0)\n\t\t\tbreak;\n\t\tif (atomic_cmpxchg(&netstamp_wanted, wanted, wanted + 1) == wanted)\n\t\t\treturn;\n\t}\n\tatomic_inc(&netstamp_needed_deferred);\n\tschedule_work(&netstamp_work);\n#else\n\tstatic_branch_inc(&netstamp_needed_key);\n#endif\n}\nEXPORT_SYMBOL(net_enable_timestamp);\n\nvoid net_disable_timestamp(void)\n{\n#ifdef CONFIG_JUMP_LABEL\n\tint wanted;\n\n\twhile (1) {\n\t\twanted = atomic_read(&netstamp_wanted);\n\t\tif (wanted <= 1)\n\t\t\tbreak;\n\t\tif (atomic_cmpxchg(&netstamp_wanted, wanted, wanted - 1) == wanted)\n\t\t\treturn;\n\t}\n\tatomic_dec(&netstamp_needed_deferred);\n\tschedule_work(&netstamp_work);\n#else\n\tstatic_branch_dec(&netstamp_needed_key);\n#endif\n}\nEXPORT_SYMBOL(net_disable_timestamp);\n\nstatic inline void net_timestamp_set(struct sk_buff *skb)\n{\n\tskb->tstamp = 0;\n\tif (static_branch_unlikely(&netstamp_needed_key))\n\t\t__net_timestamp(skb);\n}\n\n#define net_timestamp_check(COND, SKB)\t\t\t\t\\\n\tif (static_branch_unlikely(&netstamp_needed_key)) {\t\\\n\t\tif ((COND) && !(SKB)->tstamp)\t\t\t\\\n\t\t\t__net_timestamp(SKB);\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\nbool is_skb_forwardable(const struct net_device *dev, const struct sk_buff *skb)\n{\n\tunsigned int len;\n\n\tif (!(dev->flags & IFF_UP))\n\t\treturn false;\n\n\tlen = dev->mtu + dev->hard_header_len + VLAN_HLEN;\n\tif (skb->len <= len)\n\t\treturn true;\n\n\t/* if TSO is enabled, we don't care about the length as the packet\n\t * could be forwarded without being segmented before\n\t */\n\tif (skb_is_gso(skb))\n\t\treturn true;\n\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(is_skb_forwardable);\n\nint __dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\tint ret = ____dev_forward_skb(dev, skb);\n\n\tif (likely(!ret)) {\n\t\tskb->protocol = eth_type_trans(skb, dev);\n\t\tskb_postpull_rcsum(skb, eth_hdr(skb), ETH_HLEN);\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(__dev_forward_skb);\n\n/**\n * dev_forward_skb - loopback an skb to another netif\n *\n * @dev: destination network device\n * @skb: buffer to forward\n *\n * return values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped, but freed)\n *\n * dev_forward_skb can be used for injecting an skb from the\n * start_xmit function of one device into the receive queue\n * of another device.\n *\n * The receiving device may be in another namespace, so\n * we have to clear all information in the skb that could\n * impact namespace isolation.\n */\nint dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn __dev_forward_skb(dev, skb) ?: netif_rx_internal(skb);\n}\nEXPORT_SYMBOL_GPL(dev_forward_skb);\n\nstatic inline int deliver_skb(struct sk_buff *skb,\n\t\t\t      struct packet_type *pt_prev,\n\t\t\t      struct net_device *orig_dev)\n{\n\tif (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))\n\t\treturn -ENOMEM;\n\trefcount_inc(&skb->users);\n\treturn pt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n}\n\nstatic inline void deliver_ptype_list_skb(struct sk_buff *skb,\n\t\t\t\t\t  struct packet_type **pt,\n\t\t\t\t\t  struct net_device *orig_dev,\n\t\t\t\t\t  __be16 type,\n\t\t\t\t\t  struct list_head *ptype_list)\n{\n\tstruct packet_type *ptype, *pt_prev = *pt;\n\n\tlist_for_each_entry_rcu(ptype, ptype_list, list) {\n\t\tif (ptype->type != type)\n\t\t\tcontinue;\n\t\tif (pt_prev)\n\t\t\tdeliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\t*pt = pt_prev;\n}\n\nstatic inline bool skb_loop_sk(struct packet_type *ptype, struct sk_buff *skb)\n{\n\tif (!ptype->af_packet_priv || !skb->sk)\n\t\treturn false;\n\n\tif (ptype->id_match)\n\t\treturn ptype->id_match(ptype, skb->sk);\n\telse if ((struct sock *)ptype->af_packet_priv == skb->sk)\n\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * dev_nit_active - return true if any network interface taps are in use\n *\n * @dev: network device to check for the presence of taps\n */\nbool dev_nit_active(struct net_device *dev)\n{\n\treturn !list_empty(&ptype_all) || !list_empty(&dev->ptype_all);\n}\nEXPORT_SYMBOL_GPL(dev_nit_active);\n\n/*\n *\tSupport routine. Sends outgoing frames to any network\n *\ttaps currently in use.\n */\n\nvoid dev_queue_xmit_nit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct packet_type *ptype;\n\tstruct sk_buff *skb2 = NULL;\n\tstruct packet_type *pt_prev = NULL;\n\tstruct list_head *ptype_list = &ptype_all;\n\n\trcu_read_lock();\nagain:\n\tlist_for_each_entry_rcu(ptype, ptype_list, list) {\n\t\tif (ptype->ignore_outgoing)\n\t\t\tcontinue;\n\n\t\t/* Never send packets back to the socket\n\t\t * they originated from - MvS (miquels@drinkel.ow.org)\n\t\t */\n\t\tif (skb_loop_sk(ptype, skb))\n\t\t\tcontinue;\n\n\t\tif (pt_prev) {\n\t\t\tdeliver_skb(skb2, pt_prev, skb->dev);\n\t\t\tpt_prev = ptype;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* need to clone skb, done only once */\n\t\tskb2 = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!skb2)\n\t\t\tgoto out_unlock;\n\n\t\tnet_timestamp_set(skb2);\n\n\t\t/* skb->nh should be correctly\n\t\t * set by sender, so that the second statement is\n\t\t * just protection against buggy protocols.\n\t\t */\n\t\tskb_reset_mac_header(skb2);\n\n\t\tif (skb_network_header(skb2) < skb2->data ||\n\t\t    skb_network_header(skb2) > skb_tail_pointer(skb2)) {\n\t\t\tnet_crit_ratelimited(\"protocol %04x is buggy, dev %s\\n\",\n\t\t\t\t\t     ntohs(skb2->protocol),\n\t\t\t\t\t     dev->name);\n\t\t\tskb_reset_network_header(skb2);\n\t\t}\n\n\t\tskb2->transport_header = skb2->network_header;\n\t\tskb2->pkt_type = PACKET_OUTGOING;\n\t\tpt_prev = ptype;\n\t}\n\n\tif (ptype_list == &ptype_all) {\n\t\tptype_list = &dev->ptype_all;\n\t\tgoto again;\n\t}\nout_unlock:\n\tif (pt_prev) {\n\t\tif (!skb_orphan_frags_rx(skb2, GFP_ATOMIC))\n\t\t\tpt_prev->func(skb2, skb->dev, pt_prev, skb->dev);\n\t\telse\n\t\t\tkfree_skb(skb2);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(dev_queue_xmit_nit);\n\n/**\n * netif_setup_tc - Handle tc mappings on real_num_tx_queues change\n * @dev: Network device\n * @txq: number of queues available\n *\n * If real_num_tx_queues is changed the tc mappings may no longer be\n * valid. To resolve this verify the tc mapping remains valid and if\n * not NULL the mapping. With no priorities mapping to this\n * offset/count pair it will no longer be used. In the worst case TC0\n * is invalid nothing can be done so disable priority mappings. If is\n * expected that drivers will fix this mapping if they can before\n * calling netif_set_real_num_tx_queues.\n */\nstatic void netif_setup_tc(struct net_device *dev, unsigned int txq)\n{\n\tint i;\n\tstruct netdev_tc_txq *tc = &dev->tc_to_txq[0];\n\n\t/* If TC0 is invalidated disable TC mapping */\n\tif (tc->offset + tc->count > txq) {\n\t\tpr_warn(\"Number of in use tx queues changed invalidating tc mappings. Priority traffic classification disabled!\\n\");\n\t\tdev->num_tc = 0;\n\t\treturn;\n\t}\n\n\t/* Invalidated prio to tc mappings set to TC0 */\n\tfor (i = 1; i < TC_BITMASK + 1; i++) {\n\t\tint q = netdev_get_prio_tc_map(dev, i);\n\n\t\ttc = &dev->tc_to_txq[q];\n\t\tif (tc->offset + tc->count > txq) {\n\t\t\tpr_warn(\"Number of in use tx queues changed. Priority %i to tc mapping %i is no longer valid. Setting map to 0\\n\",\n\t\t\t\ti, q);\n\t\t\tnetdev_set_prio_tc_map(dev, i, 0);\n\t\t}\n\t}\n}\n\nint netdev_txq_to_tc(struct net_device *dev, unsigned int txq)\n{\n\tif (dev->num_tc) {\n\t\tstruct netdev_tc_txq *tc = &dev->tc_to_txq[0];\n\t\tint i;\n\n\t\t/* walk through the TCs and see if it falls into any of them */\n\t\tfor (i = 0; i < TC_MAX_QUEUE; i++, tc++) {\n\t\t\tif ((txq - tc->offset) < tc->count)\n\t\t\t\treturn i;\n\t\t}\n\n\t\t/* didn't find it, just return -1 to indicate no match */\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_txq_to_tc);\n\n#ifdef CONFIG_XPS\nstruct static_key xps_needed __read_mostly;\nEXPORT_SYMBOL(xps_needed);\nstruct static_key xps_rxqs_needed __read_mostly;\nEXPORT_SYMBOL(xps_rxqs_needed);\nstatic DEFINE_MUTEX(xps_map_mutex);\n#define xmap_dereference(P)\t\t\\\n\trcu_dereference_protected((P), lockdep_is_held(&xps_map_mutex))\n\nstatic bool remove_xps_queue(struct xps_dev_maps *dev_maps,\n\t\t\t     int tci, u16 index)\n{\n\tstruct xps_map *map = NULL;\n\tint pos;\n\n\tif (dev_maps)\n\t\tmap = xmap_dereference(dev_maps->attr_map[tci]);\n\tif (!map)\n\t\treturn false;\n\n\tfor (pos = map->len; pos--;) {\n\t\tif (map->queues[pos] != index)\n\t\t\tcontinue;\n\n\t\tif (map->len > 1) {\n\t\t\tmap->queues[pos] = map->queues[--map->len];\n\t\t\tbreak;\n\t\t}\n\n\t\tRCU_INIT_POINTER(dev_maps->attr_map[tci], NULL);\n\t\tkfree_rcu(map, rcu);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool remove_xps_queue_cpu(struct net_device *dev,\n\t\t\t\t struct xps_dev_maps *dev_maps,\n\t\t\t\t int cpu, u16 offset, u16 count)\n{\n\tint num_tc = dev->num_tc ? : 1;\n\tbool active = false;\n\tint tci;\n\n\tfor (tci = cpu * num_tc; num_tc--; tci++) {\n\t\tint i, j;\n\n\t\tfor (i = count, j = offset; i--; j++) {\n\t\t\tif (!remove_xps_queue(dev_maps, tci, j))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tactive |= i < 0;\n\t}\n\n\treturn active;\n}\n\nstatic void reset_xps_maps(struct net_device *dev,\n\t\t\t   struct xps_dev_maps *dev_maps,\n\t\t\t   bool is_rxqs_map)\n{\n\tif (is_rxqs_map) {\n\t\tstatic_key_slow_dec_cpuslocked(&xps_rxqs_needed);\n\t\tRCU_INIT_POINTER(dev->xps_rxqs_map, NULL);\n\t} else {\n\t\tRCU_INIT_POINTER(dev->xps_cpus_map, NULL);\n\t}\n\tstatic_key_slow_dec_cpuslocked(&xps_needed);\n\tkfree_rcu(dev_maps, rcu);\n}\n\nstatic void clean_xps_maps(struct net_device *dev, const unsigned long *mask,\n\t\t\t   struct xps_dev_maps *dev_maps, unsigned int nr_ids,\n\t\t\t   u16 offset, u16 count, bool is_rxqs_map)\n{\n\tbool active = false;\n\tint i, j;\n\n\tfor (j = -1; j = netif_attrmask_next(j, mask, nr_ids),\n\t     j < nr_ids;)\n\t\tactive |= remove_xps_queue_cpu(dev, dev_maps, j, offset,\n\t\t\t\t\t       count);\n\tif (!active)\n\t\treset_xps_maps(dev, dev_maps, is_rxqs_map);\n\n\tif (!is_rxqs_map) {\n\t\tfor (i = offset + (count - 1); count--; i--) {\n\t\t\tnetdev_queue_numa_node_write(\n\t\t\t\tnetdev_get_tx_queue(dev, i),\n\t\t\t\tNUMA_NO_NODE);\n\t\t}\n\t}\n}\n\nstatic void netif_reset_xps_queues(struct net_device *dev, u16 offset,\n\t\t\t\t   u16 count)\n{\n\tconst unsigned long *possible_mask = NULL;\n\tstruct xps_dev_maps *dev_maps;\n\tunsigned int nr_ids;\n\n\tif (!static_key_false(&xps_needed))\n\t\treturn;\n\n\tcpus_read_lock();\n\tmutex_lock(&xps_map_mutex);\n\n\tif (static_key_false(&xps_rxqs_needed)) {\n\t\tdev_maps = xmap_dereference(dev->xps_rxqs_map);\n\t\tif (dev_maps) {\n\t\t\tnr_ids = dev->num_rx_queues;\n\t\t\tclean_xps_maps(dev, possible_mask, dev_maps, nr_ids,\n\t\t\t\t       offset, count, true);\n\t\t}\n\t}\n\n\tdev_maps = xmap_dereference(dev->xps_cpus_map);\n\tif (!dev_maps)\n\t\tgoto out_no_maps;\n\n\tif (num_possible_cpus() > 1)\n\t\tpossible_mask = cpumask_bits(cpu_possible_mask);\n\tnr_ids = nr_cpu_ids;\n\tclean_xps_maps(dev, possible_mask, dev_maps, nr_ids, offset, count,\n\t\t       false);\n\nout_no_maps:\n\tmutex_unlock(&xps_map_mutex);\n\tcpus_read_unlock();\n}\n\nstatic void netif_reset_xps_queues_gt(struct net_device *dev, u16 index)\n{\n\tnetif_reset_xps_queues(dev, index, dev->num_tx_queues - index);\n}\n\nstatic struct xps_map *expand_xps_map(struct xps_map *map, int attr_index,\n\t\t\t\t      u16 index, bool is_rxqs_map)\n{\n\tstruct xps_map *new_map;\n\tint alloc_len = XPS_MIN_MAP_ALLOC;\n\tint i, pos;\n\n\tfor (pos = 0; map && pos < map->len; pos++) {\n\t\tif (map->queues[pos] != index)\n\t\t\tcontinue;\n\t\treturn map;\n\t}\n\n\t/* Need to add tx-queue to this CPU's/rx-queue's existing map */\n\tif (map) {\n\t\tif (pos < map->alloc_len)\n\t\t\treturn map;\n\n\t\talloc_len = map->alloc_len * 2;\n\t}\n\n\t/* Need to allocate new map to store tx-queue on this CPU's/rx-queue's\n\t *  map\n\t */\n\tif (is_rxqs_map)\n\t\tnew_map = kzalloc(XPS_MAP_SIZE(alloc_len), GFP_KERNEL);\n\telse\n\t\tnew_map = kzalloc_node(XPS_MAP_SIZE(alloc_len), GFP_KERNEL,\n\t\t\t\t       cpu_to_node(attr_index));\n\tif (!new_map)\n\t\treturn NULL;\n\n\tfor (i = 0; i < pos; i++)\n\t\tnew_map->queues[i] = map->queues[i];\n\tnew_map->alloc_len = alloc_len;\n\tnew_map->len = pos;\n\n\treturn new_map;\n}\n\n/* Must be called under cpus_read_lock */\nint __netif_set_xps_queue(struct net_device *dev, const unsigned long *mask,\n\t\t\t  u16 index, bool is_rxqs_map)\n{\n\tconst unsigned long *online_mask = NULL, *possible_mask = NULL;\n\tstruct xps_dev_maps *dev_maps, *new_dev_maps = NULL;\n\tint i, j, tci, numa_node_id = -2;\n\tint maps_sz, num_tc = 1, tc = 0;\n\tstruct xps_map *map, *new_map;\n\tbool active = false;\n\tunsigned int nr_ids;\n\n\tif (dev->num_tc) {\n\t\t/* Do not allow XPS on subordinate device directly */\n\t\tnum_tc = dev->num_tc;\n\t\tif (num_tc < 0)\n\t\t\treturn -EINVAL;\n\n\t\t/* If queue belongs to subordinate dev use its map */\n\t\tdev = netdev_get_tx_queue(dev, index)->sb_dev ? : dev;\n\n\t\ttc = netdev_txq_to_tc(dev, index);\n\t\tif (tc < 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&xps_map_mutex);\n\tif (is_rxqs_map) {\n\t\tmaps_sz = XPS_RXQ_DEV_MAPS_SIZE(num_tc, dev->num_rx_queues);\n\t\tdev_maps = xmap_dereference(dev->xps_rxqs_map);\n\t\tnr_ids = dev->num_rx_queues;\n\t} else {\n\t\tmaps_sz = XPS_CPU_DEV_MAPS_SIZE(num_tc);\n\t\tif (num_possible_cpus() > 1) {\n\t\t\tonline_mask = cpumask_bits(cpu_online_mask);\n\t\t\tpossible_mask = cpumask_bits(cpu_possible_mask);\n\t\t}\n\t\tdev_maps = xmap_dereference(dev->xps_cpus_map);\n\t\tnr_ids = nr_cpu_ids;\n\t}\n\n\tif (maps_sz < L1_CACHE_BYTES)\n\t\tmaps_sz = L1_CACHE_BYTES;\n\n\t/* allocate memory for queue storage */\n\tfor (j = -1; j = netif_attrmask_next_and(j, online_mask, mask, nr_ids),\n\t     j < nr_ids;) {\n\t\tif (!new_dev_maps)\n\t\t\tnew_dev_maps = kzalloc(maps_sz, GFP_KERNEL);\n\t\tif (!new_dev_maps) {\n\t\t\tmutex_unlock(&xps_map_mutex);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\ttci = j * num_tc + tc;\n\t\tmap = dev_maps ? xmap_dereference(dev_maps->attr_map[tci]) :\n\t\t\t\t NULL;\n\n\t\tmap = expand_xps_map(map, j, index, is_rxqs_map);\n\t\tif (!map)\n\t\t\tgoto error;\n\n\t\tRCU_INIT_POINTER(new_dev_maps->attr_map[tci], map);\n\t}\n\n\tif (!new_dev_maps)\n\t\tgoto out_no_new_maps;\n\n\tif (!dev_maps) {\n\t\t/* Increment static keys at most once per type */\n\t\tstatic_key_slow_inc_cpuslocked(&xps_needed);\n\t\tif (is_rxqs_map)\n\t\t\tstatic_key_slow_inc_cpuslocked(&xps_rxqs_needed);\n\t}\n\n\tfor (j = -1; j = netif_attrmask_next(j, possible_mask, nr_ids),\n\t     j < nr_ids;) {\n\t\t/* copy maps belonging to foreign traffic classes */\n\t\tfor (i = tc, tci = j * num_tc; dev_maps && i--; tci++) {\n\t\t\t/* fill in the new device map from the old device map */\n\t\t\tmap = xmap_dereference(dev_maps->attr_map[tci]);\n\t\t\tRCU_INIT_POINTER(new_dev_maps->attr_map[tci], map);\n\t\t}\n\n\t\t/* We need to explicitly update tci as prevous loop\n\t\t * could break out early if dev_maps is NULL.\n\t\t */\n\t\ttci = j * num_tc + tc;\n\n\t\tif (netif_attr_test_mask(j, mask, nr_ids) &&\n\t\t    netif_attr_test_online(j, online_mask, nr_ids)) {\n\t\t\t/* add tx-queue to CPU/rx-queue maps */\n\t\t\tint pos = 0;\n\n\t\t\tmap = xmap_dereference(new_dev_maps->attr_map[tci]);\n\t\t\twhile ((pos < map->len) && (map->queues[pos] != index))\n\t\t\t\tpos++;\n\n\t\t\tif (pos == map->len)\n\t\t\t\tmap->queues[map->len++] = index;\n#ifdef CONFIG_NUMA\n\t\t\tif (!is_rxqs_map) {\n\t\t\t\tif (numa_node_id == -2)\n\t\t\t\t\tnuma_node_id = cpu_to_node(j);\n\t\t\t\telse if (numa_node_id != cpu_to_node(j))\n\t\t\t\t\tnuma_node_id = -1;\n\t\t\t}\n#endif\n\t\t} else if (dev_maps) {\n\t\t\t/* fill in the new device map from the old device map */\n\t\t\tmap = xmap_dereference(dev_maps->attr_map[tci]);\n\t\t\tRCU_INIT_POINTER(new_dev_maps->attr_map[tci], map);\n\t\t}\n\n\t\t/* copy maps belonging to foreign traffic classes */\n\t\tfor (i = num_tc - tc, tci++; dev_maps && --i; tci++) {\n\t\t\t/* fill in the new device map from the old device map */\n\t\t\tmap = xmap_dereference(dev_maps->attr_map[tci]);\n\t\t\tRCU_INIT_POINTER(new_dev_maps->attr_map[tci], map);\n\t\t}\n\t}\n\n\tif (is_rxqs_map)\n\t\trcu_assign_pointer(dev->xps_rxqs_map, new_dev_maps);\n\telse\n\t\trcu_assign_pointer(dev->xps_cpus_map, new_dev_maps);\n\n\t/* Cleanup old maps */\n\tif (!dev_maps)\n\t\tgoto out_no_old_maps;\n\n\tfor (j = -1; j = netif_attrmask_next(j, possible_mask, nr_ids),\n\t     j < nr_ids;) {\n\t\tfor (i = num_tc, tci = j * num_tc; i--; tci++) {\n\t\t\tnew_map = xmap_dereference(new_dev_maps->attr_map[tci]);\n\t\t\tmap = xmap_dereference(dev_maps->attr_map[tci]);\n\t\t\tif (map && map != new_map)\n\t\t\t\tkfree_rcu(map, rcu);\n\t\t}\n\t}\n\n\tkfree_rcu(dev_maps, rcu);\n\nout_no_old_maps:\n\tdev_maps = new_dev_maps;\n\tactive = true;\n\nout_no_new_maps:\n\tif (!is_rxqs_map) {\n\t\t/* update Tx queue numa node */\n\t\tnetdev_queue_numa_node_write(netdev_get_tx_queue(dev, index),\n\t\t\t\t\t     (numa_node_id >= 0) ?\n\t\t\t\t\t     numa_node_id : NUMA_NO_NODE);\n\t}\n\n\tif (!dev_maps)\n\t\tgoto out_no_maps;\n\n\t/* removes tx-queue from unused CPUs/rx-queues */\n\tfor (j = -1; j = netif_attrmask_next(j, possible_mask, nr_ids),\n\t     j < nr_ids;) {\n\t\tfor (i = tc, tci = j * num_tc; i--; tci++)\n\t\t\tactive |= remove_xps_queue(dev_maps, tci, index);\n\t\tif (!netif_attr_test_mask(j, mask, nr_ids) ||\n\t\t    !netif_attr_test_online(j, online_mask, nr_ids))\n\t\t\tactive |= remove_xps_queue(dev_maps, tci, index);\n\t\tfor (i = num_tc - tc, tci++; --i; tci++)\n\t\t\tactive |= remove_xps_queue(dev_maps, tci, index);\n\t}\n\n\t/* free map if not active */\n\tif (!active)\n\t\treset_xps_maps(dev, dev_maps, is_rxqs_map);\n\nout_no_maps:\n\tmutex_unlock(&xps_map_mutex);\n\n\treturn 0;\nerror:\n\t/* remove any maps that we added */\n\tfor (j = -1; j = netif_attrmask_next(j, possible_mask, nr_ids),\n\t     j < nr_ids;) {\n\t\tfor (i = num_tc, tci = j * num_tc; i--; tci++) {\n\t\t\tnew_map = xmap_dereference(new_dev_maps->attr_map[tci]);\n\t\t\tmap = dev_maps ?\n\t\t\t      xmap_dereference(dev_maps->attr_map[tci]) :\n\t\t\t      NULL;\n\t\t\tif (new_map && new_map != map)\n\t\t\t\tkfree(new_map);\n\t\t}\n\t}\n\n\tmutex_unlock(&xps_map_mutex);\n\n\tkfree(new_dev_maps);\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL_GPL(__netif_set_xps_queue);\n\nint netif_set_xps_queue(struct net_device *dev, const struct cpumask *mask,\n\t\t\tu16 index)\n{\n\tint ret;\n\n\tcpus_read_lock();\n\tret =  __netif_set_xps_queue(dev, cpumask_bits(mask), index, false);\n\tcpus_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_set_xps_queue);\n\n#endif\nstatic void netdev_unbind_all_sb_channels(struct net_device *dev)\n{\n\tstruct netdev_queue *txq = &dev->_tx[dev->num_tx_queues];\n\n\t/* Unbind any subordinate channels */\n\twhile (txq-- != &dev->_tx[0]) {\n\t\tif (txq->sb_dev)\n\t\t\tnetdev_unbind_sb_channel(dev, txq->sb_dev);\n\t}\n}\n\nvoid netdev_reset_tc(struct net_device *dev)\n{\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\tnetdev_unbind_all_sb_channels(dev);\n\n\t/* Reset TC configuration of device */\n\tdev->num_tc = 0;\n\tmemset(dev->tc_to_txq, 0, sizeof(dev->tc_to_txq));\n\tmemset(dev->prio_tc_map, 0, sizeof(dev->prio_tc_map));\n}\nEXPORT_SYMBOL(netdev_reset_tc);\n\nint netdev_set_tc_queue(struct net_device *dev, u8 tc, u16 count, u16 offset)\n{\n\tif (tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues(dev, offset, count);\n#endif\n\tdev->tc_to_txq[tc].count = count;\n\tdev->tc_to_txq[tc].offset = offset;\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_tc_queue);\n\nint netdev_set_num_tc(struct net_device *dev, u8 num_tc)\n{\n\tif (num_tc > TC_MAX_QUEUE)\n\t\treturn -EINVAL;\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\tnetdev_unbind_all_sb_channels(dev);\n\n\tdev->num_tc = num_tc;\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_num_tc);\n\nvoid netdev_unbind_sb_channel(struct net_device *dev,\n\t\t\t      struct net_device *sb_dev)\n{\n\tstruct netdev_queue *txq = &dev->_tx[dev->num_tx_queues];\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(sb_dev, 0);\n#endif\n\tmemset(sb_dev->tc_to_txq, 0, sizeof(sb_dev->tc_to_txq));\n\tmemset(sb_dev->prio_tc_map, 0, sizeof(sb_dev->prio_tc_map));\n\n\twhile (txq-- != &dev->_tx[0]) {\n\t\tif (txq->sb_dev == sb_dev)\n\t\t\ttxq->sb_dev = NULL;\n\t}\n}\nEXPORT_SYMBOL(netdev_unbind_sb_channel);\n\nint netdev_bind_sb_channel_queue(struct net_device *dev,\n\t\t\t\t struct net_device *sb_dev,\n\t\t\t\t u8 tc, u16 count, u16 offset)\n{\n\t/* Make certain the sb_dev and dev are already configured */\n\tif (sb_dev->num_tc >= 0 || tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n\t/* We cannot hand out queues we don't have */\n\tif ((offset + count) > dev->real_num_tx_queues)\n\t\treturn -EINVAL;\n\n\t/* Record the mapping */\n\tsb_dev->tc_to_txq[tc].count = count;\n\tsb_dev->tc_to_txq[tc].offset = offset;\n\n\t/* Provide a way for Tx queue to find the tc_to_txq map or\n\t * XPS map for itself.\n\t */\n\twhile (count--)\n\t\tnetdev_get_tx_queue(dev, count + offset)->sb_dev = sb_dev;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_bind_sb_channel_queue);\n\nint netdev_set_sb_channel(struct net_device *dev, u16 channel)\n{\n\t/* Do not use a multiqueue device to represent a subordinate channel */\n\tif (netif_is_multiqueue(dev))\n\t\treturn -ENODEV;\n\n\t/* We allow channels 1 - 32767 to be used for subordinate channels.\n\t * Channel 0 is meant to be \"native\" mode and used only to represent\n\t * the main root device. We allow writing 0 to reset the device back\n\t * to normal mode after being used as a subordinate channel.\n\t */\n\tif (channel > S16_MAX)\n\t\treturn -EINVAL;\n\n\tdev->num_tc = -channel;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_sb_channel);\n\n/*\n * Routine to help set real_num_tx_queues. To avoid skbs mapped to queues\n * greater than real_num_tx_queues stale skbs on the qdisc must be flushed.\n */\nint netif_set_real_num_tx_queues(struct net_device *dev, unsigned int txq)\n{\n\tbool disabling;\n\tint rc;\n\n\tdisabling = txq < dev->real_num_tx_queues;\n\n\tif (txq < 1 || txq > dev->num_tx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED ||\n\t    dev->reg_state == NETREG_UNREGISTERING) {\n\t\tASSERT_RTNL();\n\n\t\trc = netdev_queue_update_kobjects(dev, dev->real_num_tx_queues,\n\t\t\t\t\t\t  txq);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tif (dev->num_tc)\n\t\t\tnetif_setup_tc(dev, txq);\n\n\t\tdev->real_num_tx_queues = txq;\n\n\t\tif (disabling) {\n\t\t\tsynchronize_net();\n\t\t\tqdisc_reset_all_tx_gt(dev, txq);\n#ifdef CONFIG_XPS\n\t\t\tnetif_reset_xps_queues_gt(dev, txq);\n#endif\n\t\t}\n\t} else {\n\t\tdev->real_num_tx_queues = txq;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_tx_queues);\n\n#ifdef CONFIG_SYSFS\n/**\n *\tnetif_set_real_num_rx_queues - set actual number of RX queues used\n *\t@dev: Network device\n *\t@rxq: Actual number of RX queues\n *\n *\tThis must be called either with the rtnl_lock held or before\n *\tregistration of the net device.  Returns 0 on success, or a\n *\tnegative error code.  If called before registration, it always\n *\tsucceeds.\n */\nint netif_set_real_num_rx_queues(struct net_device *dev, unsigned int rxq)\n{\n\tint rc;\n\n\tif (rxq < 1 || rxq > dev->num_rx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED) {\n\t\tASSERT_RTNL();\n\n\t\trc = net_rx_queue_update_kobjects(dev, dev->real_num_rx_queues,\n\t\t\t\t\t\t  rxq);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tdev->real_num_rx_queues = rxq;\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_rx_queues);\n#endif\n\n/**\n * netif_get_num_default_rss_queues - default number of RSS queues\n *\n * This routine should set an upper limit on the number of RSS queues\n * used by default by multiqueue devices.\n */\nint netif_get_num_default_rss_queues(void)\n{\n\treturn is_kdump_kernel() ?\n\t\t1 : min_t(int, DEFAULT_MAX_NUM_RSS_QUEUES, num_online_cpus());\n}\nEXPORT_SYMBOL(netif_get_num_default_rss_queues);\n\nstatic void __netif_reschedule(struct Qdisc *q)\n{\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tsd = this_cpu_ptr(&softnet_data);\n\tq->next_sched = NULL;\n\t*sd->output_queue_tailp = q;\n\tsd->output_queue_tailp = &q->next_sched;\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\n\nvoid __netif_schedule(struct Qdisc *q)\n{\n\tif (!test_and_set_bit(__QDISC_STATE_SCHED, &q->state))\n\t\t__netif_reschedule(q);\n}\nEXPORT_SYMBOL(__netif_schedule);\n\nstruct dev_kfree_skb_cb {\n\tenum skb_free_reason reason;\n};\n\nstatic struct dev_kfree_skb_cb *get_kfree_skb_cb(const struct sk_buff *skb)\n{\n\treturn (struct dev_kfree_skb_cb *)skb->cb;\n}\n\nvoid netif_schedule_queue(struct netdev_queue *txq)\n{\n\trcu_read_lock();\n\tif (!netif_xmit_stopped(txq)) {\n\t\tstruct Qdisc *q = rcu_dereference(txq->qdisc);\n\n\t\t__netif_schedule(q);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(netif_schedule_queue);\n\nvoid netif_tx_wake_queue(struct netdev_queue *dev_queue)\n{\n\tif (test_and_clear_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state)) {\n\t\tstruct Qdisc *q;\n\n\t\trcu_read_lock();\n\t\tq = rcu_dereference(dev_queue->qdisc);\n\t\t__netif_schedule(q);\n\t\trcu_read_unlock();\n\t}\n}\nEXPORT_SYMBOL(netif_tx_wake_queue);\n\nvoid __dev_kfree_skb_irq(struct sk_buff *skb, enum skb_free_reason reason)\n{\n\tunsigned long flags;\n\n\tif (unlikely(!skb))\n\t\treturn;\n\n\tif (likely(refcount_read(&skb->users) == 1)) {\n\t\tsmp_rmb();\n\t\trefcount_set(&skb->users, 0);\n\t} else if (likely(!refcount_dec_and_test(&skb->users))) {\n\t\treturn;\n\t}\n\tget_kfree_skb_cb(skb)->reason = reason;\n\tlocal_irq_save(flags);\n\tskb->next = __this_cpu_read(softnet_data.completion_queue);\n\t__this_cpu_write(softnet_data.completion_queue, skb);\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(__dev_kfree_skb_irq);\n\nvoid __dev_kfree_skb_any(struct sk_buff *skb, enum skb_free_reason reason)\n{\n\tif (in_irq() || irqs_disabled())\n\t\t__dev_kfree_skb_irq(skb, reason);\n\telse\n\t\tdev_kfree_skb(skb);\n}\nEXPORT_SYMBOL(__dev_kfree_skb_any);\n\n\n/**\n * netif_device_detach - mark device as removed\n * @dev: network device\n *\n * Mark device as removed from system and therefore no longer available.\n */\nvoid netif_device_detach(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_stop_all_queues(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_detach);\n\n/**\n * netif_device_attach - mark device as attached\n * @dev: network device\n *\n * Mark device as attached from system and restart if needed.\n */\nvoid netif_device_attach(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_wake_all_queues(dev);\n\t\t__netdev_watchdog_up(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_attach);\n\n/*\n * Returns a Tx hash based on the given packet descriptor a Tx queues' number\n * to be used as a distribution range.\n */\nstatic u16 skb_tx_hash(const struct net_device *dev,\n\t\t       const struct net_device *sb_dev,\n\t\t       struct sk_buff *skb)\n{\n\tu32 hash;\n\tu16 qoffset = 0;\n\tu16 qcount = dev->real_num_tx_queues;\n\n\tif (dev->num_tc) {\n\t\tu8 tc = netdev_get_prio_tc_map(dev, skb->priority);\n\n\t\tqoffset = sb_dev->tc_to_txq[tc].offset;\n\t\tqcount = sb_dev->tc_to_txq[tc].count;\n\t}\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\thash = skb_get_rx_queue(skb);\n\t\tif (hash >= qoffset)\n\t\t\thash -= qoffset;\n\t\twhile (unlikely(hash >= qcount))\n\t\t\thash -= qcount;\n\t\treturn hash + qoffset;\n\t}\n\n\treturn (u16) reciprocal_scale(skb_get_hash(skb), qcount) + qoffset;\n}\n\nstatic void skb_warn_bad_offload(const struct sk_buff *skb)\n{\n\tstatic const netdev_features_t null_features;\n\tstruct net_device *dev = skb->dev;\n\tconst char *name = \"\";\n\n\tif (!net_ratelimit())\n\t\treturn;\n\n\tif (dev) {\n\t\tif (dev->dev.parent)\n\t\t\tname = dev_driver_string(dev->dev.parent);\n\t\telse\n\t\t\tname = netdev_name(dev);\n\t}\n\tskb_dump(KERN_WARNING, skb, false);\n\tWARN(1, \"%s: caps=(%pNF, %pNF)\\n\",\n\t     name, dev ? &dev->features : &null_features,\n\t     skb->sk ? &skb->sk->sk_route_caps : &null_features);\n}\n\n/*\n * Invalidate hardware checksum when packet is to be mangled, and\n * complete checksum manually on outgoing path.\n */\nint skb_checksum_help(struct sk_buff *skb)\n{\n\t__wsum csum;\n\tint ret = 0, offset;\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tgoto out_set_summed;\n\n\tif (unlikely(skb_is_gso(skb))) {\n\t\tskb_warn_bad_offload(skb);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Before computing a checksum, we should make sure no frag could\n\t * be modified by an external entity : checksum could be wrong.\n\t */\n\tif (skb_has_shared_frag(skb)) {\n\t\tret = __skb_linearize(skb);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\toffset = skb_checksum_start_offset(skb);\n\tBUG_ON(offset >= skb_headlen(skb));\n\tcsum = skb_checksum(skb, offset, skb->len - offset, 0);\n\n\toffset += skb->csum_offset;\n\tBUG_ON(offset + sizeof(__sum16) > skb_headlen(skb));\n\n\tret = skb_ensure_writable(skb, offset + sizeof(__sum16));\n\tif (ret)\n\t\tgoto out;\n\n\t*(__sum16 *)(skb->data + offset) = csum_fold(csum) ?: CSUM_MANGLED_0;\nout_set_summed:\n\tskb->ip_summed = CHECKSUM_NONE;\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL(skb_checksum_help);\n\nint skb_crc32c_csum_help(struct sk_buff *skb)\n{\n\t__le32 crc32c_csum;\n\tint ret = 0, offset, start;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\tgoto out;\n\n\tif (unlikely(skb_is_gso(skb)))\n\t\tgoto out;\n\n\t/* Before computing a checksum, we should make sure no frag could\n\t * be modified by an external entity : checksum could be wrong.\n\t */\n\tif (unlikely(skb_has_shared_frag(skb))) {\n\t\tret = __skb_linearize(skb);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\tstart = skb_checksum_start_offset(skb);\n\toffset = start + offsetof(struct sctphdr, checksum);\n\tif (WARN_ON_ONCE(offset >= skb_headlen(skb))) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = skb_ensure_writable(skb, offset + sizeof(__le32));\n\tif (ret)\n\t\tgoto out;\n\n\tcrc32c_csum = cpu_to_le32(~__skb_checksum(skb, start,\n\t\t\t\t\t\t  skb->len - start, ~(__u32)0,\n\t\t\t\t\t\t  crc32c_csum_stub));\n\t*(__le32 *)(skb->data + offset) = crc32c_csum;\n\tskb->ip_summed = CHECKSUM_NONE;\n\tskb->csum_not_inet = 0;\nout:\n\treturn ret;\n}\n\n__be16 skb_network_protocol(struct sk_buff *skb, int *depth)\n{\n\t__be16 type = skb->protocol;\n\n\t/* Tunnel gso handlers can set protocol to ethernet. */\n\tif (type == htons(ETH_P_TEB)) {\n\t\tstruct ethhdr *eth;\n\n\t\tif (unlikely(!pskb_may_pull(skb, sizeof(struct ethhdr))))\n\t\t\treturn 0;\n\n\t\teth = (struct ethhdr *)skb->data;\n\t\ttype = eth->h_proto;\n\t}\n\n\treturn __vlan_get_protocol(skb, type, depth);\n}\n\n/**\n *\tskb_mac_gso_segment - mac layer segmentation handler.\n *\t@skb: buffer to segment\n *\t@features: features for the output path (see dev->features)\n */\nstruct sk_buff *skb_mac_gso_segment(struct sk_buff *skb,\n\t\t\t\t    netdev_features_t features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EPROTONOSUPPORT);\n\tstruct packet_offload *ptype;\n\tint vlan_depth = skb->mac_len;\n\t__be16 type = skb_network_protocol(skb, &vlan_depth);\n\n\tif (unlikely(!type))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t__skb_pull(skb, vlan_depth);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, &offload_base, list) {\n\t\tif (ptype->type == type && ptype->callbacks.gso_segment) {\n\t\t\tsegs = ptype->callbacks.gso_segment(skb, features);\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\t__skb_push(skb, skb->data - skb_mac_header(skb));\n\n\treturn segs;\n}\nEXPORT_SYMBOL(skb_mac_gso_segment);\n\n\n/* openvswitch calls this on rx path, so we need a different check.\n */\nstatic inline bool skb_needs_check(struct sk_buff *skb, bool tx_path)\n{\n\tif (tx_path)\n\t\treturn skb->ip_summed != CHECKSUM_PARTIAL &&\n\t\t       skb->ip_summed != CHECKSUM_UNNECESSARY;\n\n\treturn skb->ip_summed == CHECKSUM_NONE;\n}\n\n/**\n *\t__skb_gso_segment - Perform segmentation on skb.\n *\t@skb: buffer to segment\n *\t@features: features for the output path (see dev->features)\n *\t@tx_path: whether it is called in TX path\n *\n *\tThis function segments the given skb and returns a list of segments.\n *\n *\tIt may return NULL if the skb requires no segmentation.  This is\n *\tonly possible when GSO is used for verifying header integrity.\n *\n *\tSegmentation preserves SKB_GSO_CB_OFFSET bytes of previous skb cb.\n */\nstruct sk_buff *__skb_gso_segment(struct sk_buff *skb,\n\t\t\t\t  netdev_features_t features, bool tx_path)\n{\n\tstruct sk_buff *segs;\n\n\tif (unlikely(skb_needs_check(skb, tx_path))) {\n\t\tint err;\n\n\t\t/* We're going to init ->check field in TCP or UDP header */\n\t\terr = skb_cow_head(skb, 0);\n\t\tif (err < 0)\n\t\t\treturn ERR_PTR(err);\n\t}\n\n\t/* Only report GSO partial support if it will enable us to\n\t * support segmentation on this frame without needing additional\n\t * work.\n\t */\n\tif (features & NETIF_F_GSO_PARTIAL) {\n\t\tnetdev_features_t partial_features = NETIF_F_GSO_ROBUST;\n\t\tstruct net_device *dev = skb->dev;\n\n\t\tpartial_features |= dev->features & dev->gso_partial_features;\n\t\tif (!skb_gso_ok(skb, features | partial_features))\n\t\t\tfeatures &= ~NETIF_F_GSO_PARTIAL;\n\t}\n\n\tBUILD_BUG_ON(SKB_GSO_CB_OFFSET +\n\t\t     sizeof(*SKB_GSO_CB(skb)) > sizeof(skb->cb));\n\n\tSKB_GSO_CB(skb)->mac_offset = skb_headroom(skb);\n\tSKB_GSO_CB(skb)->encap_level = 0;\n\n\tskb_reset_mac_header(skb);\n\tskb_reset_mac_len(skb);\n\n\tsegs = skb_mac_gso_segment(skb, features);\n\n\tif (segs != skb && unlikely(skb_needs_check(skb, tx_path) && !IS_ERR(segs)))\n\t\tskb_warn_bad_offload(skb);\n\n\treturn segs;\n}\nEXPORT_SYMBOL(__skb_gso_segment);\n\n/* Take action when hardware reception checksum errors are detected. */\n#ifdef CONFIG_BUG\nvoid netdev_rx_csum_fault(struct net_device *dev, struct sk_buff *skb)\n{\n\tif (net_ratelimit()) {\n\t\tpr_err(\"%s: hw csum failure\\n\", dev ? dev->name : \"<unknown>\");\n\t\tskb_dump(KERN_ERR, skb, true);\n\t\tdump_stack();\n\t}\n}\nEXPORT_SYMBOL(netdev_rx_csum_fault);\n#endif\n\n/* XXX: check that highmem exists at all on the given machine. */\nstatic int illegal_highdma(struct net_device *dev, struct sk_buff *skb)\n{\n#ifdef CONFIG_HIGHMEM\n\tint i;\n\n\tif (!(dev->features & NETIF_F_HIGHDMA)) {\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\t\tif (PageHighMem(skb_frag_page(frag)))\n\t\t\t\treturn 1;\n\t\t}\n\t}\n#endif\n\treturn 0;\n}\n\n/* If MPLS offload request, verify we are testing hardware MPLS features\n * instead of standard features for the netdev.\n */\n#if IS_ENABLED(CONFIG_NET_MPLS_GSO)\nstatic netdev_features_t net_mpls_features(struct sk_buff *skb,\n\t\t\t\t\t   netdev_features_t features,\n\t\t\t\t\t   __be16 type)\n{\n\tif (eth_p_mpls(type))\n\t\tfeatures &= skb->dev->mpls_features;\n\n\treturn features;\n}\n#else\nstatic netdev_features_t net_mpls_features(struct sk_buff *skb,\n\t\t\t\t\t   netdev_features_t features,\n\t\t\t\t\t   __be16 type)\n{\n\treturn features;\n}\n#endif\n\nstatic netdev_features_t harmonize_features(struct sk_buff *skb,\n\tnetdev_features_t features)\n{\n\t__be16 type;\n\n\ttype = skb_network_protocol(skb, NULL);\n\tfeatures = net_mpls_features(skb, features, type);\n\n\tif (skb->ip_summed != CHECKSUM_NONE &&\n\t    !can_checksum_protocol(features, type)) {\n\t\tfeatures &= ~(NETIF_F_CSUM_MASK | NETIF_F_GSO_MASK);\n\t}\n\tif (illegal_highdma(skb->dev, skb))\n\t\tfeatures &= ~NETIF_F_SG;\n\n\treturn features;\n}\n\nnetdev_features_t passthru_features_check(struct sk_buff *skb,\n\t\t\t\t\t  struct net_device *dev,\n\t\t\t\t\t  netdev_features_t features)\n{\n\treturn features;\n}\nEXPORT_SYMBOL(passthru_features_check);\n\nstatic netdev_features_t dflt_features_check(struct sk_buff *skb,\n\t\t\t\t\t     struct net_device *dev,\n\t\t\t\t\t     netdev_features_t features)\n{\n\treturn vlan_features_check(skb, features);\n}\n\nstatic netdev_features_t gso_features_check(const struct sk_buff *skb,\n\t\t\t\t\t    struct net_device *dev,\n\t\t\t\t\t    netdev_features_t features)\n{\n\tu16 gso_segs = skb_shinfo(skb)->gso_segs;\n\n\tif (gso_segs > dev->gso_max_segs)\n\t\treturn features & ~NETIF_F_GSO_MASK;\n\n\tif (!skb_shinfo(skb)->gso_type) {\n\t\tskb_warn_bad_offload(skb);\n\t\treturn features & ~NETIF_F_GSO_MASK;\n\t}\n\n\t/* Support for GSO partial features requires software\n\t * intervention before we can actually process the packets\n\t * so we need to strip support for any partial features now\n\t * and we can pull them back in after we have partially\n\t * segmented the frame.\n\t */\n\tif (!(skb_shinfo(skb)->gso_type & SKB_GSO_PARTIAL))\n\t\tfeatures &= ~dev->gso_partial_features;\n\n\t/* Make sure to clear the IPv4 ID mangling feature if the\n\t * IPv4 header has the potential to be fragmented.\n\t */\n\tif (skb_shinfo(skb)->gso_type & SKB_GSO_TCPV4) {\n\t\tstruct iphdr *iph = skb->encapsulation ?\n\t\t\t\t    inner_ip_hdr(skb) : ip_hdr(skb);\n\n\t\tif (!(iph->frag_off & htons(IP_DF)))\n\t\t\tfeatures &= ~NETIF_F_TSO_MANGLEID;\n\t}\n\n\treturn features;\n}\n\nnetdev_features_t netif_skb_features(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tnetdev_features_t features = dev->features;\n\n\tif (skb_is_gso(skb))\n\t\tfeatures = gso_features_check(skb, dev, features);\n\n\t/* If encapsulation offload request, verify we are testing\n\t * hardware encapsulation features instead of standard\n\t * features for the netdev\n\t */\n\tif (skb->encapsulation)\n\t\tfeatures &= dev->hw_enc_features;\n\n\tif (skb_vlan_tagged(skb))\n\t\tfeatures = netdev_intersect_features(features,\n\t\t\t\t\t\t     dev->vlan_features |\n\t\t\t\t\t\t     NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t\t\t     NETIF_F_HW_VLAN_STAG_TX);\n\n\tif (dev->netdev_ops->ndo_features_check)\n\t\tfeatures &= dev->netdev_ops->ndo_features_check(skb, dev,\n\t\t\t\t\t\t\t\tfeatures);\n\telse\n\t\tfeatures &= dflt_features_check(skb, dev, features);\n\n\treturn harmonize_features(skb, features);\n}\nEXPORT_SYMBOL(netif_skb_features);\n\nstatic int xmit_one(struct sk_buff *skb, struct net_device *dev,\n\t\t    struct netdev_queue *txq, bool more)\n{\n\tunsigned int len;\n\tint rc;\n\n\tif (dev_nit_active(dev))\n\t\tdev_queue_xmit_nit(skb, dev);\n\n\tlen = skb->len;\n\tPRANDOM_ADD_NOISE(skb, dev, txq, len + jiffies);\n\ttrace_net_dev_start_xmit(skb, dev);\n\trc = netdev_start_xmit(skb, dev, txq, more);\n\ttrace_net_dev_xmit(skb, rc, dev, len);\n\n\treturn rc;\n}\n\nstruct sk_buff *dev_hard_start_xmit(struct sk_buff *first, struct net_device *dev,\n\t\t\t\t    struct netdev_queue *txq, int *ret)\n{\n\tstruct sk_buff *skb = first;\n\tint rc = NETDEV_TX_OK;\n\n\twhile (skb) {\n\t\tstruct sk_buff *next = skb->next;\n\n\t\tskb_mark_not_on_list(skb);\n\t\trc = xmit_one(skb, dev, txq, next != NULL);\n\t\tif (unlikely(!dev_xmit_complete(rc))) {\n\t\t\tskb->next = next;\n\t\t\tgoto out;\n\t\t}\n\n\t\tskb = next;\n\t\tif (netif_tx_queue_stopped(txq) && skb) {\n\t\t\trc = NETDEV_TX_BUSY;\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\t*ret = rc;\n\treturn skb;\n}\n\nstatic struct sk_buff *validate_xmit_vlan(struct sk_buff *skb,\n\t\t\t\t\t  netdev_features_t features)\n{\n\tif (skb_vlan_tag_present(skb) &&\n\t    !vlan_hw_offload_capable(features, skb->vlan_proto))\n\t\tskb = __vlan_hwaccel_push_inside(skb);\n\treturn skb;\n}\n\nint skb_csum_hwoffload_help(struct sk_buff *skb,\n\t\t\t    const netdev_features_t features)\n{\n\tif (unlikely(skb_csum_is_sctp(skb)))\n\t\treturn !!(features & NETIF_F_SCTP_CRC) ? 0 :\n\t\t\tskb_crc32c_csum_help(skb);\n\n\tif (features & NETIF_F_HW_CSUM)\n\t\treturn 0;\n\n\tif (features & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)) {\n\t\tswitch (skb->csum_offset) {\n\t\tcase offsetof(struct tcphdr, check):\n\t\tcase offsetof(struct udphdr, check):\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn skb_checksum_help(skb);\n}\nEXPORT_SYMBOL(skb_csum_hwoffload_help);\n\nstatic struct sk_buff *validate_xmit_skb(struct sk_buff *skb, struct net_device *dev, bool *again)\n{\n\tnetdev_features_t features;\n\n\tfeatures = netif_skb_features(skb);\n\tskb = validate_xmit_vlan(skb, features);\n\tif (unlikely(!skb))\n\t\tgoto out_null;\n\n\tskb = sk_validate_xmit_skb(skb, dev);\n\tif (unlikely(!skb))\n\t\tgoto out_null;\n\n\tif (netif_needs_gso(skb, features)) {\n\t\tstruct sk_buff *segs;\n\n\t\tsegs = skb_gso_segment(skb, features);\n\t\tif (IS_ERR(segs)) {\n\t\t\tgoto out_kfree_skb;\n\t\t} else if (segs) {\n\t\t\tconsume_skb(skb);\n\t\t\tskb = segs;\n\t\t}\n\t} else {\n\t\tif (skb_needs_linearize(skb, features) &&\n\t\t    __skb_linearize(skb))\n\t\t\tgoto out_kfree_skb;\n\n\t\t/* If packet is not checksummed and device does not\n\t\t * support checksumming for this protocol, complete\n\t\t * checksumming here.\n\t\t */\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tif (skb->encapsulation)\n\t\t\t\tskb_set_inner_transport_header(skb,\n\t\t\t\t\t\t\t       skb_checksum_start_offset(skb));\n\t\t\telse\n\t\t\t\tskb_set_transport_header(skb,\n\t\t\t\t\t\t\t skb_checksum_start_offset(skb));\n\t\t\tif (skb_csum_hwoffload_help(skb, features))\n\t\t\t\tgoto out_kfree_skb;\n\t\t}\n\t}\n\n\tskb = validate_xmit_xfrm(skb, features, again);\n\n\treturn skb;\n\nout_kfree_skb:\n\tkfree_skb(skb);\nout_null:\n\tatomic_long_inc(&dev->tx_dropped);\n\treturn NULL;\n}\n\nstruct sk_buff *validate_xmit_skb_list(struct sk_buff *skb, struct net_device *dev, bool *again)\n{\n\tstruct sk_buff *next, *head = NULL, *tail;\n\n\tfor (; skb != NULL; skb = next) {\n\t\tnext = skb->next;\n\t\tskb_mark_not_on_list(skb);\n\n\t\t/* in case skb wont be segmented, point to itself */\n\t\tskb->prev = skb;\n\n\t\tskb = validate_xmit_skb(skb, dev, again);\n\t\tif (!skb)\n\t\t\tcontinue;\n\n\t\tif (!head)\n\t\t\thead = skb;\n\t\telse\n\t\t\ttail->next = skb;\n\t\t/* If skb was segmented, skb->prev points to\n\t\t * the last segment. If not, it still contains skb.\n\t\t */\n\t\ttail = skb->prev;\n\t}\n\treturn head;\n}\nEXPORT_SYMBOL_GPL(validate_xmit_skb_list);\n\nstatic void qdisc_pkt_len_init(struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\n\t/* To get more precise estimation of bytes sent on wire,\n\t * we add to pkt_len the headers size of all segments\n\t */\n\tif (shinfo->gso_size && skb_transport_header_was_set(skb)) {\n\t\tunsigned int hdr_len;\n\t\tu16 gso_segs = shinfo->gso_segs;\n\n\t\t/* mac layer + network layer */\n\t\thdr_len = skb_transport_header(skb) - skb_mac_header(skb);\n\n\t\t/* + transport layer */\n\t\tif (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6))) {\n\t\t\tconst struct tcphdr *th;\n\t\t\tstruct tcphdr _tcphdr;\n\n\t\t\tth = skb_header_pointer(skb, skb_transport_offset(skb),\n\t\t\t\t\t\tsizeof(_tcphdr), &_tcphdr);\n\t\t\tif (likely(th))\n\t\t\t\thdr_len += __tcp_hdrlen(th);\n\t\t} else {\n\t\t\tstruct udphdr _udphdr;\n\n\t\t\tif (skb_header_pointer(skb, skb_transport_offset(skb),\n\t\t\t\t\t       sizeof(_udphdr), &_udphdr))\n\t\t\t\thdr_len += sizeof(struct udphdr);\n\t\t}\n\n\t\tif (shinfo->gso_type & SKB_GSO_DODGY)\n\t\t\tgso_segs = DIV_ROUND_UP(skb->len - hdr_len,\n\t\t\t\t\t\tshinfo->gso_size);\n\n\t\tqdisc_skb_cb(skb)->pkt_len += (gso_segs - 1) * hdr_len;\n\t}\n}\n\nstatic inline int __dev_xmit_skb(struct sk_buff *skb, struct Qdisc *q,\n\t\t\t\t struct net_device *dev,\n\t\t\t\t struct netdev_queue *txq)\n{\n\tspinlock_t *root_lock = qdisc_lock(q);\n\tstruct sk_buff *to_free = NULL;\n\tbool contended;\n\tint rc;\n\n\tqdisc_calculate_pkt_len(skb, q);\n\n\tif (q->flags & TCQ_F_NOLOCK) {\n\t\trc = q->enqueue(skb, q, &to_free) & NET_XMIT_MASK;\n\t\tqdisc_run(q);\n\n\t\tif (unlikely(to_free))\n\t\t\tkfree_skb_list(to_free);\n\t\treturn rc;\n\t}\n\n\t/*\n\t * Heuristic to force contended enqueues to serialize on a\n\t * separate lock before trying to get qdisc main lock.\n\t * This permits qdisc->running owner to get the lock more\n\t * often and dequeue packets faster.\n\t */\n\tcontended = qdisc_is_running(q);\n\tif (unlikely(contended))\n\t\tspin_lock(&q->busylock);\n\n\tspin_lock(root_lock);\n\tif (unlikely(test_bit(__QDISC_STATE_DEACTIVATED, &q->state))) {\n\t\t__qdisc_drop(skb, &to_free);\n\t\trc = NET_XMIT_DROP;\n\t} else if ((q->flags & TCQ_F_CAN_BYPASS) && !qdisc_qlen(q) &&\n\t\t   qdisc_run_begin(q)) {\n\t\t/*\n\t\t * This is a work-conserving queue; there are no old skbs\n\t\t * waiting to be sent out; and the qdisc is not running -\n\t\t * xmit the skb directly.\n\t\t */\n\n\t\tqdisc_bstats_update(q, skb);\n\n\t\tif (sch_direct_xmit(skb, q, dev, txq, root_lock, true)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t}\n\n\t\tqdisc_run_end(q);\n\t\trc = NET_XMIT_SUCCESS;\n\t} else {\n\t\trc = q->enqueue(skb, q, &to_free) & NET_XMIT_MASK;\n\t\tif (qdisc_run_begin(q)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t\tqdisc_run_end(q);\n\t\t}\n\t}\n\tspin_unlock(root_lock);\n\tif (unlikely(to_free))\n\t\tkfree_skb_list(to_free);\n\tif (unlikely(contended))\n\t\tspin_unlock(&q->busylock);\n\treturn rc;\n}\n\n#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)\nstatic void skb_update_prio(struct sk_buff *skb)\n{\n\tconst struct netprio_map *map;\n\tconst struct sock *sk;\n\tunsigned int prioidx;\n\n\tif (skb->priority)\n\t\treturn;\n\tmap = rcu_dereference_bh(skb->dev->priomap);\n\tif (!map)\n\t\treturn;\n\tsk = skb_to_full_sk(skb);\n\tif (!sk)\n\t\treturn;\n\n\tprioidx = sock_cgroup_prioidx(&sk->sk_cgrp_data);\n\n\tif (prioidx < map->priomap_len)\n\t\tskb->priority = map->priomap[prioidx];\n}\n#else\n#define skb_update_prio(skb)\n#endif\n\n/**\n *\tdev_loopback_xmit - loop back @skb\n *\t@net: network namespace this loopback is happening in\n *\t@sk:  sk needed to be a netfilter okfn\n *\t@skb: buffer to transmit\n */\nint dev_loopback_xmit(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tskb_reset_mac_header(skb);\n\t__skb_pull(skb, skb_network_offset(skb));\n\tskb->pkt_type = PACKET_LOOPBACK;\n\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\tWARN_ON(!skb_dst(skb));\n\tskb_dst_force(skb);\n\tnetif_rx_ni(skb);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_loopback_xmit);\n\n#ifdef CONFIG_NET_EGRESS\nstatic struct sk_buff *\nsch_handle_egress(struct sk_buff *skb, int *ret, struct net_device *dev)\n{\n\tstruct mini_Qdisc *miniq = rcu_dereference_bh(dev->miniq_egress);\n\tstruct tcf_result cl_res;\n\n\tif (!miniq)\n\t\treturn skb;\n\n\t/* qdisc_skb_cb(skb)->pkt_len was already set by the caller. */\n\tqdisc_skb_cb(skb)->mru = 0;\n\tqdisc_skb_cb(skb)->post_ct = false;\n\tmini_qdisc_bstats_cpu_update(miniq, skb);\n\n\tswitch (tcf_classify(skb, miniq->filter_list, &cl_res, false)) {\n\tcase TC_ACT_OK:\n\tcase TC_ACT_RECLASSIFY:\n\t\tskb->tc_index = TC_H_MIN(cl_res.classid);\n\t\tbreak;\n\tcase TC_ACT_SHOT:\n\t\tmini_qdisc_qstats_cpu_drop(miniq);\n\t\t*ret = NET_XMIT_DROP;\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\tcase TC_ACT_STOLEN:\n\tcase TC_ACT_QUEUED:\n\tcase TC_ACT_TRAP:\n\t\t*ret = NET_XMIT_SUCCESS;\n\t\tconsume_skb(skb);\n\t\treturn NULL;\n\tcase TC_ACT_REDIRECT:\n\t\t/* No need to push/pop skb's mac_header here on egress! */\n\t\tskb_do_redirect(skb);\n\t\t*ret = NET_XMIT_SUCCESS;\n\t\treturn NULL;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn skb;\n}\n#endif /* CONFIG_NET_EGRESS */\n\n#ifdef CONFIG_XPS\nstatic int __get_xps_queue_idx(struct net_device *dev, struct sk_buff *skb,\n\t\t\t       struct xps_dev_maps *dev_maps, unsigned int tci)\n{\n\tstruct xps_map *map;\n\tint queue_index = -1;\n\n\tif (dev->num_tc) {\n\t\ttci *= dev->num_tc;\n\t\ttci += netdev_get_prio_tc_map(dev, skb->priority);\n\t}\n\n\tmap = rcu_dereference(dev_maps->attr_map[tci]);\n\tif (map) {\n\t\tif (map->len == 1)\n\t\t\tqueue_index = map->queues[0];\n\t\telse\n\t\t\tqueue_index = map->queues[reciprocal_scale(\n\t\t\t\t\t\tskb_get_hash(skb), map->len)];\n\t\tif (unlikely(queue_index >= dev->real_num_tx_queues))\n\t\t\tqueue_index = -1;\n\t}\n\treturn queue_index;\n}\n#endif\n\nstatic int get_xps_queue(struct net_device *dev, struct net_device *sb_dev,\n\t\t\t struct sk_buff *skb)\n{\n#ifdef CONFIG_XPS\n\tstruct xps_dev_maps *dev_maps;\n\tstruct sock *sk = skb->sk;\n\tint queue_index = -1;\n\n\tif (!static_key_false(&xps_needed))\n\t\treturn -1;\n\n\trcu_read_lock();\n\tif (!static_key_false(&xps_rxqs_needed))\n\t\tgoto get_cpus_map;\n\n\tdev_maps = rcu_dereference(sb_dev->xps_rxqs_map);\n\tif (dev_maps) {\n\t\tint tci = sk_rx_queue_get(sk);\n\n\t\tif (tci >= 0 && tci < dev->num_rx_queues)\n\t\t\tqueue_index = __get_xps_queue_idx(dev, skb, dev_maps,\n\t\t\t\t\t\t\t  tci);\n\t}\n\nget_cpus_map:\n\tif (queue_index < 0) {\n\t\tdev_maps = rcu_dereference(sb_dev->xps_cpus_map);\n\t\tif (dev_maps) {\n\t\t\tunsigned int tci = skb->sender_cpu - 1;\n\n\t\t\tqueue_index = __get_xps_queue_idx(dev, skb, dev_maps,\n\t\t\t\t\t\t\t  tci);\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn queue_index;\n#else\n\treturn -1;\n#endif\n}\n\nu16 dev_pick_tx_zero(struct net_device *dev, struct sk_buff *skb,\n\t\t     struct net_device *sb_dev)\n{\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_pick_tx_zero);\n\nu16 dev_pick_tx_cpu_id(struct net_device *dev, struct sk_buff *skb,\n\t\t       struct net_device *sb_dev)\n{\n\treturn (u16)raw_smp_processor_id() % dev->real_num_tx_queues;\n}\nEXPORT_SYMBOL(dev_pick_tx_cpu_id);\n\nu16 netdev_pick_tx(struct net_device *dev, struct sk_buff *skb,\n\t\t     struct net_device *sb_dev)\n{\n\tstruct sock *sk = skb->sk;\n\tint queue_index = sk_tx_queue_get(sk);\n\n\tsb_dev = sb_dev ? : dev;\n\n\tif (queue_index < 0 || skb->ooo_okay ||\n\t    queue_index >= dev->real_num_tx_queues) {\n\t\tint new_index = get_xps_queue(dev, sb_dev, skb);\n\n\t\tif (new_index < 0)\n\t\t\tnew_index = skb_tx_hash(dev, sb_dev, skb);\n\n\t\tif (queue_index != new_index && sk &&\n\t\t    sk_fullsock(sk) &&\n\t\t    rcu_access_pointer(sk->sk_dst_cache))\n\t\t\tsk_tx_queue_set(sk, new_index);\n\n\t\tqueue_index = new_index;\n\t}\n\n\treturn queue_index;\n}\nEXPORT_SYMBOL(netdev_pick_tx);\n\nstruct netdev_queue *netdev_core_pick_tx(struct net_device *dev,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t struct net_device *sb_dev)\n{\n\tint queue_index = 0;\n\n#ifdef CONFIG_XPS\n\tu32 sender_cpu = skb->sender_cpu - 1;\n\n\tif (sender_cpu >= (u32)NR_CPUS)\n\t\tskb->sender_cpu = raw_smp_processor_id() + 1;\n#endif\n\n\tif (dev->real_num_tx_queues != 1) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t\tif (ops->ndo_select_queue)\n\t\t\tqueue_index = ops->ndo_select_queue(dev, skb, sb_dev);\n\t\telse\n\t\t\tqueue_index = netdev_pick_tx(dev, skb, sb_dev);\n\n\t\tqueue_index = netdev_cap_txqueue(dev, queue_index);\n\t}\n\n\tskb_set_queue_mapping(skb, queue_index);\n\treturn netdev_get_tx_queue(dev, queue_index);\n}\n\n/**\n *\t__dev_queue_xmit - transmit a buffer\n *\t@skb: buffer to transmit\n *\t@sb_dev: suboordinate device used for L2 forwarding offload\n *\n *\tQueue a buffer for transmission to a network device. The caller must\n *\thave set the device and priority and built the buffer before calling\n *\tthis function. The function can be called from an interrupt.\n *\n *\tA negative errno code is returned on a failure. A success does not\n *\tguarantee the frame will be transmitted as it may be dropped due\n *\tto congestion or traffic shaping.\n *\n * -----------------------------------------------------------------------------------\n *      I notice this method can also return errors from the queue disciplines,\n *      including NET_XMIT_DROP, which is a positive value.  So, errors can also\n *      be positive.\n *\n *      Regardless of the return value, the skb is consumed, so it is currently\n *      difficult to retry a send to this method.  (You can bump the ref count\n *      before sending to hold a reference for retry if you are careful.)\n *\n *      When calling this method, interrupts MUST be enabled.  This is because\n *      the BH enable code must have IRQs enabled so that it will not deadlock.\n *          --BLG\n */\nstatic int __dev_queue_xmit(struct sk_buff *skb, struct net_device *sb_dev)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_queue *txq;\n\tstruct Qdisc *q;\n\tint rc = -ENOMEM;\n\tbool again = false;\n\n\tskb_reset_mac_header(skb);\n\n\tif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_SCHED_TSTAMP))\n\t\t__skb_tstamp_tx(skb, NULL, NULL, skb->sk, SCM_TSTAMP_SCHED);\n\n\t/* Disable soft irqs for various locks below. Also\n\t * stops preemption for RCU.\n\t */\n\trcu_read_lock_bh();\n\n\tskb_update_prio(skb);\n\n\tqdisc_pkt_len_init(skb);\n#ifdef CONFIG_NET_CLS_ACT\n\tskb->tc_at_ingress = 0;\n# ifdef CONFIG_NET_EGRESS\n\tif (static_branch_unlikely(&egress_needed_key)) {\n\t\tskb = sch_handle_egress(skb, &rc, dev);\n\t\tif (!skb)\n\t\t\tgoto out;\n\t}\n# endif\n#endif\n\t/* If device/qdisc don't need skb->dst, release it right now while\n\t * its hot in this cpu cache.\n\t */\n\tif (dev->priv_flags & IFF_XMIT_DST_RELEASE)\n\t\tskb_dst_drop(skb);\n\telse\n\t\tskb_dst_force(skb);\n\n\ttxq = netdev_core_pick_tx(dev, skb, sb_dev);\n\tq = rcu_dereference_bh(txq->qdisc);\n\n\ttrace_net_dev_queue(skb);\n\tif (q->enqueue) {\n\t\trc = __dev_xmit_skb(skb, q, dev, txq);\n\t\tgoto out;\n\t}\n\n\t/* The device has no queue. Common case for software devices:\n\t * loopback, all the sorts of tunnels...\n\n\t * Really, it is unlikely that netif_tx_lock protection is necessary\n\t * here.  (f.e. loopback and IP tunnels are clean ignoring statistics\n\t * counters.)\n\t * However, it is possible, that they rely on protection\n\t * made by us here.\n\n\t * Check this and shot the lock. It is not prone from deadlocks.\n\t *Either shot noqueue qdisc, it is even simpler 8)\n\t */\n\tif (dev->flags & IFF_UP) {\n\t\tint cpu = smp_processor_id(); /* ok because BHs are off */\n\n\t\tif (txq->xmit_lock_owner != cpu) {\n\t\t\tif (dev_xmit_recursion())\n\t\t\t\tgoto recursion_alert;\n\n\t\t\tskb = validate_xmit_skb(skb, dev, &again);\n\t\t\tif (!skb)\n\t\t\t\tgoto out;\n\n\t\t\tPRANDOM_ADD_NOISE(skb, dev, txq, jiffies);\n\t\t\tHARD_TX_LOCK(dev, txq, cpu);\n\n\t\t\tif (!netif_xmit_stopped(txq)) {\n\t\t\t\tdev_xmit_recursion_inc();\n\t\t\t\tskb = dev_hard_start_xmit(skb, dev, txq, &rc);\n\t\t\t\tdev_xmit_recursion_dec();\n\t\t\t\tif (dev_xmit_complete(rc)) {\n\t\t\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\tnet_crit_ratelimited(\"Virtual device %s asks to queue packet!\\n\",\n\t\t\t\t\t     dev->name);\n\t\t} else {\n\t\t\t/* Recursion is detected! It is possible,\n\t\t\t * unfortunately\n\t\t\t */\nrecursion_alert:\n\t\t\tnet_crit_ratelimited(\"Dead loop on virtual device %s, fix it urgently!\\n\",\n\t\t\t\t\t     dev->name);\n\t\t}\n\t}\n\n\trc = -ENETDOWN;\n\trcu_read_unlock_bh();\n\n\tatomic_long_inc(&dev->tx_dropped);\n\tkfree_skb_list(skb);\n\treturn rc;\nout:\n\trcu_read_unlock_bh();\n\treturn rc;\n}\n\nint dev_queue_xmit(struct sk_buff *skb)\n{\n\treturn __dev_queue_xmit(skb, NULL);\n}\nEXPORT_SYMBOL(dev_queue_xmit);\n\nint dev_queue_xmit_accel(struct sk_buff *skb, struct net_device *sb_dev)\n{\n\treturn __dev_queue_xmit(skb, sb_dev);\n}\nEXPORT_SYMBOL(dev_queue_xmit_accel);\n\nint __dev_direct_xmit(struct sk_buff *skb, u16 queue_id)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct sk_buff *orig_skb = skb;\n\tstruct netdev_queue *txq;\n\tint ret = NETDEV_TX_BUSY;\n\tbool again = false;\n\n\tif (unlikely(!netif_running(dev) ||\n\t\t     !netif_carrier_ok(dev)))\n\t\tgoto drop;\n\n\tskb = validate_xmit_skb_list(skb, dev, &again);\n\tif (skb != orig_skb)\n\t\tgoto drop;\n\n\tskb_set_queue_mapping(skb, queue_id);\n\ttxq = skb_get_tx_queue(dev, skb);\n\tPRANDOM_ADD_NOISE(skb, dev, txq, jiffies);\n\n\tlocal_bh_disable();\n\n\tdev_xmit_recursion_inc();\n\tHARD_TX_LOCK(dev, txq, smp_processor_id());\n\tif (!netif_xmit_frozen_or_drv_stopped(txq))\n\t\tret = netdev_start_xmit(skb, dev, txq, false);\n\tHARD_TX_UNLOCK(dev, txq);\n\tdev_xmit_recursion_dec();\n\n\tlocal_bh_enable();\n\treturn ret;\ndrop:\n\tatomic_long_inc(&dev->tx_dropped);\n\tkfree_skb_list(skb);\n\treturn NET_XMIT_DROP;\n}\nEXPORT_SYMBOL(__dev_direct_xmit);\n\n/*************************************************************************\n *\t\t\tReceiver routines\n *************************************************************************/\n\nint netdev_max_backlog __read_mostly = 1000;\nEXPORT_SYMBOL(netdev_max_backlog);\n\nint netdev_tstamp_prequeue __read_mostly = 1;\nint netdev_budget __read_mostly = 300;\n/* Must be at least 2 jiffes to guarantee 1 jiffy timeout */\nunsigned int __read_mostly netdev_budget_usecs = 2 * USEC_PER_SEC / HZ;\nint weight_p __read_mostly = 64;           /* old backlog weight */\nint dev_weight_rx_bias __read_mostly = 1;  /* bias for backlog weight */\nint dev_weight_tx_bias __read_mostly = 1;  /* bias for output_queue quota */\nint dev_rx_weight __read_mostly = 64;\nint dev_tx_weight __read_mostly = 64;\n/* Maximum number of GRO_NORMAL skbs to batch up for list-RX */\nint gro_normal_batch __read_mostly = 8;\n\n/* Called with irq disabled */\nstatic inline void ____napi_schedule(struct softnet_data *sd,\n\t\t\t\t     struct napi_struct *napi)\n{\n\tlist_add_tail(&napi->poll_list, &sd->poll_list);\n\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n}\n\n#ifdef CONFIG_RPS\n\n/* One global table that all flow-based protocols share. */\nstruct rps_sock_flow_table __rcu *rps_sock_flow_table __read_mostly;\nEXPORT_SYMBOL(rps_sock_flow_table);\nu32 rps_cpu_mask __read_mostly;\nEXPORT_SYMBOL(rps_cpu_mask);\n\nstruct static_key_false rps_needed __read_mostly;\nEXPORT_SYMBOL(rps_needed);\nstruct static_key_false rfs_needed __read_mostly;\nEXPORT_SYMBOL(rfs_needed);\n\nstatic struct rps_dev_flow *\nset_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t    struct rps_dev_flow *rflow, u16 next_cpu)\n{\n\tif (next_cpu < nr_cpu_ids) {\n#ifdef CONFIG_RFS_ACCEL\n\t\tstruct netdev_rx_queue *rxqueue;\n\t\tstruct rps_dev_flow_table *flow_table;\n\t\tstruct rps_dev_flow *old_rflow;\n\t\tu32 flow_id;\n\t\tu16 rxq_index;\n\t\tint rc;\n\n\t\t/* Should we steer this flow to a different hardware queue? */\n\t\tif (!skb_rx_queue_recorded(skb) || !dev->rx_cpu_rmap ||\n\t\t    !(dev->features & NETIF_F_NTUPLE))\n\t\t\tgoto out;\n\t\trxq_index = cpu_rmap_lookup_index(dev->rx_cpu_rmap, next_cpu);\n\t\tif (rxq_index == skb_get_rx_queue(skb))\n\t\t\tgoto out;\n\n\t\trxqueue = dev->_rx + rxq_index;\n\t\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\t\tif (!flow_table)\n\t\t\tgoto out;\n\t\tflow_id = skb_get_hash(skb) & flow_table->mask;\n\t\trc = dev->netdev_ops->ndo_rx_flow_steer(dev, skb,\n\t\t\t\t\t\t\trxq_index, flow_id);\n\t\tif (rc < 0)\n\t\t\tgoto out;\n\t\told_rflow = rflow;\n\t\trflow = &flow_table->flows[flow_id];\n\t\trflow->filter = rc;\n\t\tif (old_rflow->filter == rflow->filter)\n\t\t\told_rflow->filter = RPS_NO_FILTER;\n\tout:\n#endif\n\t\trflow->last_qtail =\n\t\t\tper_cpu(softnet_data, next_cpu).input_queue_head;\n\t}\n\n\trflow->cpu = next_cpu;\n\treturn rflow;\n}\n\n/*\n * get_rps_cpu is called from netif_receive_skb and returns the target\n * CPU from the RPS map of the receiving queue for a given skb.\n * rcu_read_lock must be held on entry.\n */\nstatic int get_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t\t       struct rps_dev_flow **rflowp)\n{\n\tconst struct rps_sock_flow_table *sock_flow_table;\n\tstruct netdev_rx_queue *rxqueue = dev->_rx;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_map *map;\n\tint cpu = -1;\n\tu32 tcpu;\n\tu32 hash;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tu16 index = skb_get_rx_queue(skb);\n\n\t\tif (unlikely(index >= dev->real_num_rx_queues)) {\n\t\t\tWARN_ONCE(dev->real_num_rx_queues > 1,\n\t\t\t\t  \"%s received packet on queue %u, but number \"\n\t\t\t\t  \"of RX queues is %u\\n\",\n\t\t\t\t  dev->name, index, dev->real_num_rx_queues);\n\t\t\tgoto done;\n\t\t}\n\t\trxqueue += index;\n\t}\n\n\t/* Avoid computing hash if RFS/RPS is not active for this rxqueue */\n\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tmap = rcu_dereference(rxqueue->rps_map);\n\tif (!flow_table && !map)\n\t\tgoto done;\n\n\tskb_reset_network_header(skb);\n\thash = skb_get_hash(skb);\n\tif (!hash)\n\t\tgoto done;\n\n\tsock_flow_table = rcu_dereference(rps_sock_flow_table);\n\tif (flow_table && sock_flow_table) {\n\t\tstruct rps_dev_flow *rflow;\n\t\tu32 next_cpu;\n\t\tu32 ident;\n\n\t\t/* First check into global flow table if there is a match */\n\t\tident = sock_flow_table->ents[hash & sock_flow_table->mask];\n\t\tif ((ident ^ hash) & ~rps_cpu_mask)\n\t\t\tgoto try_rps;\n\n\t\tnext_cpu = ident & rps_cpu_mask;\n\n\t\t/* OK, now we know there is a match,\n\t\t * we can look at the local (per receive queue) flow table\n\t\t */\n\t\trflow = &flow_table->flows[hash & flow_table->mask];\n\t\ttcpu = rflow->cpu;\n\n\t\t/*\n\t\t * If the desired CPU (where last recvmsg was done) is\n\t\t * different from current CPU (one in the rx-queue flow\n\t\t * table entry), switch if one of the following holds:\n\t\t *   - Current CPU is unset (>= nr_cpu_ids).\n\t\t *   - Current CPU is offline.\n\t\t *   - The current CPU's queue tail has advanced beyond the\n\t\t *     last packet that was enqueued using this table entry.\n\t\t *     This guarantees that all previous packets for the flow\n\t\t *     have been dequeued, thus preserving in order delivery.\n\t\t */\n\t\tif (unlikely(tcpu != next_cpu) &&\n\t\t    (tcpu >= nr_cpu_ids || !cpu_online(tcpu) ||\n\t\t     ((int)(per_cpu(softnet_data, tcpu).input_queue_head -\n\t\t      rflow->last_qtail)) >= 0)) {\n\t\t\ttcpu = next_cpu;\n\t\t\trflow = set_rps_cpu(dev, skb, rflow, next_cpu);\n\t\t}\n\n\t\tif (tcpu < nr_cpu_ids && cpu_online(tcpu)) {\n\t\t\t*rflowp = rflow;\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ntry_rps:\n\n\tif (map) {\n\t\ttcpu = map->cpus[reciprocal_scale(hash, map->len)];\n\t\tif (cpu_online(tcpu)) {\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ndone:\n\treturn cpu;\n}\n\n#ifdef CONFIG_RFS_ACCEL\n\n/**\n * rps_may_expire_flow - check whether an RFS hardware filter may be removed\n * @dev: Device on which the filter was set\n * @rxq_index: RX queue index\n * @flow_id: Flow ID passed to ndo_rx_flow_steer()\n * @filter_id: Filter ID returned by ndo_rx_flow_steer()\n *\n * Drivers that implement ndo_rx_flow_steer() should periodically call\n * this function for each installed filter and remove the filters for\n * which it returns %true.\n */\nbool rps_may_expire_flow(struct net_device *dev, u16 rxq_index,\n\t\t\t u32 flow_id, u16 filter_id)\n{\n\tstruct netdev_rx_queue *rxqueue = dev->_rx + rxq_index;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_dev_flow *rflow;\n\tbool expire = true;\n\tunsigned int cpu;\n\n\trcu_read_lock();\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tif (flow_table && flow_id <= flow_table->mask) {\n\t\trflow = &flow_table->flows[flow_id];\n\t\tcpu = READ_ONCE(rflow->cpu);\n\t\tif (rflow->filter == filter_id && cpu < nr_cpu_ids &&\n\t\t    ((int)(per_cpu(softnet_data, cpu).input_queue_head -\n\t\t\t   rflow->last_qtail) <\n\t\t     (int)(10 * flow_table->mask)))\n\t\t\texpire = false;\n\t}\n\trcu_read_unlock();\n\treturn expire;\n}\nEXPORT_SYMBOL(rps_may_expire_flow);\n\n#endif /* CONFIG_RFS_ACCEL */\n\n/* Called from hardirq (IPI) context */\nstatic void rps_trigger_softirq(void *data)\n{\n\tstruct softnet_data *sd = data;\n\n\t____napi_schedule(sd, &sd->backlog);\n\tsd->received_rps++;\n}\n\n#endif /* CONFIG_RPS */\n\n/*\n * Check if this softnet_data structure is another cpu one\n * If yes, queue it to our IPI list and return 1\n * If no, return 0\n */\nstatic int rps_ipi_queued(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tstruct softnet_data *mysd = this_cpu_ptr(&softnet_data);\n\n\tif (sd != mysd) {\n\t\tsd->rps_ipi_next = mysd->rps_ipi_list;\n\t\tmysd->rps_ipi_list = sd;\n\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\t\treturn 1;\n\t}\n#endif /* CONFIG_RPS */\n\treturn 0;\n}\n\n#ifdef CONFIG_NET_FLOW_LIMIT\nint netdev_flow_limit_table_len __read_mostly = (1 << 12);\n#endif\n\nstatic bool skb_flow_limit(struct sk_buff *skb, unsigned int qlen)\n{\n#ifdef CONFIG_NET_FLOW_LIMIT\n\tstruct sd_flow_limit *fl;\n\tstruct softnet_data *sd;\n\tunsigned int old_flow, new_flow;\n\n\tif (qlen < (netdev_max_backlog >> 1))\n\t\treturn false;\n\n\tsd = this_cpu_ptr(&softnet_data);\n\n\trcu_read_lock();\n\tfl = rcu_dereference(sd->flow_limit);\n\tif (fl) {\n\t\tnew_flow = skb_get_hash(skb) & (fl->num_buckets - 1);\n\t\told_flow = fl->history[fl->history_head];\n\t\tfl->history[fl->history_head] = new_flow;\n\n\t\tfl->history_head++;\n\t\tfl->history_head &= FLOW_LIMIT_HISTORY - 1;\n\n\t\tif (likely(fl->buckets[old_flow]))\n\t\t\tfl->buckets[old_flow]--;\n\n\t\tif (++fl->buckets[new_flow] > (FLOW_LIMIT_HISTORY >> 1)) {\n\t\t\tfl->count++;\n\t\t\trcu_read_unlock();\n\t\t\treturn true;\n\t\t}\n\t}\n\trcu_read_unlock();\n#endif\n\treturn false;\n}\n\n/*\n * enqueue_to_backlog is called to queue an skb to a per CPU backlog\n * queue (may be a remote CPU queue).\n */\nstatic int enqueue_to_backlog(struct sk_buff *skb, int cpu,\n\t\t\t      unsigned int *qtail)\n{\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\tunsigned int qlen;\n\n\tsd = &per_cpu(softnet_data, cpu);\n\n\tlocal_irq_save(flags);\n\n\trps_lock(sd);\n\tif (!netif_running(skb->dev))\n\t\tgoto drop;\n\tqlen = skb_queue_len(&sd->input_pkt_queue);\n\tif (qlen <= netdev_max_backlog && !skb_flow_limit(skb, qlen)) {\n\t\tif (qlen) {\nenqueue:\n\t\t\t__skb_queue_tail(&sd->input_pkt_queue, skb);\n\t\t\tinput_queue_tail_incr_save(sd, qtail);\n\t\t\trps_unlock(sd);\n\t\t\tlocal_irq_restore(flags);\n\t\t\treturn NET_RX_SUCCESS;\n\t\t}\n\n\t\t/* Schedule NAPI for backlog device\n\t\t * We can use non atomic operation since we own the queue lock\n\t\t */\n\t\tif (!__test_and_set_bit(NAPI_STATE_SCHED, &sd->backlog.state)) {\n\t\t\tif (!rps_ipi_queued(sd))\n\t\t\t\t____napi_schedule(sd, &sd->backlog);\n\t\t}\n\t\tgoto enqueue;\n\t}\n\ndrop:\n\tsd->dropped++;\n\trps_unlock(sd);\n\n\tlocal_irq_restore(flags);\n\n\tatomic_long_inc(&skb->dev->rx_dropped);\n\tkfree_skb(skb);\n\treturn NET_RX_DROP;\n}\n\nstatic struct netdev_rx_queue *netif_get_rxqueue(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_rx_queue *rxqueue;\n\n\trxqueue = dev->_rx;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tu16 index = skb_get_rx_queue(skb);\n\n\t\tif (unlikely(index >= dev->real_num_rx_queues)) {\n\t\t\tWARN_ONCE(dev->real_num_rx_queues > 1,\n\t\t\t\t  \"%s received packet on queue %u, but number \"\n\t\t\t\t  \"of RX queues is %u\\n\",\n\t\t\t\t  dev->name, index, dev->real_num_rx_queues);\n\n\t\t\treturn rxqueue; /* Return first rxqueue */\n\t\t}\n\t\trxqueue += index;\n\t}\n\treturn rxqueue;\n}\n\nstatic u32 netif_receive_generic_xdp(struct sk_buff *skb,\n\t\t\t\t     struct xdp_buff *xdp,\n\t\t\t\t     struct bpf_prog *xdp_prog)\n{\n\tvoid *orig_data, *orig_data_end, *hard_start;\n\tstruct netdev_rx_queue *rxqueue;\n\tu32 metalen, act = XDP_DROP;\n\tu32 mac_len, frame_sz;\n\t__be16 orig_eth_type;\n\tstruct ethhdr *eth;\n\tbool orig_bcast;\n\tint off;\n\n\t/* Reinjected packets coming from act_mirred or similar should\n\t * not get XDP generic processing.\n\t */\n\tif (skb_is_redirected(skb))\n\t\treturn XDP_PASS;\n\n\t/* XDP packets must be linear and must have sufficient headroom\n\t * of XDP_PACKET_HEADROOM bytes. This is the guarantee that also\n\t * native XDP provides, thus we need to do it here as well.\n\t */\n\tif (skb_cloned(skb) || skb_is_nonlinear(skb) ||\n\t    skb_headroom(skb) < XDP_PACKET_HEADROOM) {\n\t\tint hroom = XDP_PACKET_HEADROOM - skb_headroom(skb);\n\t\tint troom = skb->tail + skb->data_len - skb->end;\n\n\t\t/* In case we have to go down the path and also linearize,\n\t\t * then lets do the pskb_expand_head() work just once here.\n\t\t */\n\t\tif (pskb_expand_head(skb,\n\t\t\t\t     hroom > 0 ? ALIGN(hroom, NET_SKB_PAD) : 0,\n\t\t\t\t     troom > 0 ? troom + 128 : 0, GFP_ATOMIC))\n\t\t\tgoto do_drop;\n\t\tif (skb_linearize(skb))\n\t\t\tgoto do_drop;\n\t}\n\n\t/* The XDP program wants to see the packet starting at the MAC\n\t * header.\n\t */\n\tmac_len = skb->data - skb_mac_header(skb);\n\thard_start = skb->data - skb_headroom(skb);\n\n\t/* SKB \"head\" area always have tailroom for skb_shared_info */\n\tframe_sz = (void *)skb_end_pointer(skb) - hard_start;\n\tframe_sz += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\trxqueue = netif_get_rxqueue(skb);\n\txdp_init_buff(xdp, frame_sz, &rxqueue->xdp_rxq);\n\txdp_prepare_buff(xdp, hard_start, skb_headroom(skb) - mac_len,\n\t\t\t skb_headlen(skb) + mac_len, true);\n\n\torig_data_end = xdp->data_end;\n\torig_data = xdp->data;\n\teth = (struct ethhdr *)xdp->data;\n\torig_bcast = is_multicast_ether_addr_64bits(eth->h_dest);\n\torig_eth_type = eth->h_proto;\n\n\tact = bpf_prog_run_xdp(xdp_prog, xdp);\n\n\t/* check if bpf_xdp_adjust_head was used */\n\toff = xdp->data - orig_data;\n\tif (off) {\n\t\tif (off > 0)\n\t\t\t__skb_pull(skb, off);\n\t\telse if (off < 0)\n\t\t\t__skb_push(skb, -off);\n\n\t\tskb->mac_header += off;\n\t\tskb_reset_network_header(skb);\n\t}\n\n\t/* check if bpf_xdp_adjust_tail was used */\n\toff = xdp->data_end - orig_data_end;\n\tif (off != 0) {\n\t\tskb_set_tail_pointer(skb, xdp->data_end - xdp->data);\n\t\tskb->len += off; /* positive on grow, negative on shrink */\n\t}\n\n\t/* check if XDP changed eth hdr such SKB needs update */\n\teth = (struct ethhdr *)xdp->data;\n\tif ((orig_eth_type != eth->h_proto) ||\n\t    (orig_bcast != is_multicast_ether_addr_64bits(eth->h_dest))) {\n\t\t__skb_push(skb, ETH_HLEN);\n\t\tskb->protocol = eth_type_trans(skb, skb->dev);\n\t}\n\n\tswitch (act) {\n\tcase XDP_REDIRECT:\n\tcase XDP_TX:\n\t\t__skb_push(skb, mac_len);\n\t\tbreak;\n\tcase XDP_PASS:\n\t\tmetalen = xdp->data - xdp->data_meta;\n\t\tif (metalen)\n\t\t\tskb_metadata_set(skb, metalen);\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(skb->dev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\tdo_drop:\n\t\tkfree_skb(skb);\n\t\tbreak;\n\t}\n\n\treturn act;\n}\n\n/* When doing generic XDP we have to bypass the qdisc layer and the\n * network taps in order to match in-driver-XDP behavior.\n */\nvoid generic_xdp_tx(struct sk_buff *skb, struct bpf_prog *xdp_prog)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_queue *txq;\n\tbool free_skb = true;\n\tint cpu, rc;\n\n\ttxq = netdev_core_pick_tx(dev, skb, NULL);\n\tcpu = smp_processor_id();\n\tHARD_TX_LOCK(dev, txq, cpu);\n\tif (!netif_xmit_stopped(txq)) {\n\t\trc = netdev_start_xmit(skb, dev, txq, 0);\n\t\tif (dev_xmit_complete(rc))\n\t\t\tfree_skb = false;\n\t}\n\tHARD_TX_UNLOCK(dev, txq);\n\tif (free_skb) {\n\t\ttrace_xdp_exception(dev, xdp_prog, XDP_TX);\n\t\tkfree_skb(skb);\n\t}\n}\n\nstatic DEFINE_STATIC_KEY_FALSE(generic_xdp_needed_key);\n\nint do_xdp_generic(struct bpf_prog *xdp_prog, struct sk_buff *skb)\n{\n\tif (xdp_prog) {\n\t\tstruct xdp_buff xdp;\n\t\tu32 act;\n\t\tint err;\n\n\t\tact = netif_receive_generic_xdp(skb, &xdp, xdp_prog);\n\t\tif (act != XDP_PASS) {\n\t\t\tswitch (act) {\n\t\t\tcase XDP_REDIRECT:\n\t\t\t\terr = xdp_do_generic_redirect(skb->dev, skb,\n\t\t\t\t\t\t\t      &xdp, xdp_prog);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out_redir;\n\t\t\t\tbreak;\n\t\t\tcase XDP_TX:\n\t\t\t\tgeneric_xdp_tx(skb, xdp_prog);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\treturn XDP_DROP;\n\t\t}\n\t}\n\treturn XDP_PASS;\nout_redir:\n\tkfree_skb(skb);\n\treturn XDP_DROP;\n}\nEXPORT_SYMBOL_GPL(do_xdp_generic);\n\nstatic int netif_rx_internal(struct sk_buff *skb)\n{\n\tint ret;\n\n\tnet_timestamp_check(netdev_tstamp_prequeue, skb);\n\n\ttrace_netif_rx(skb);\n\n#ifdef CONFIG_RPS\n\tif (static_branch_unlikely(&rps_needed)) {\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu;\n\n\t\tpreempt_disable();\n\t\trcu_read_lock();\n\n\t\tcpu = get_rps_cpu(skb->dev, skb, &rflow);\n\t\tif (cpu < 0)\n\t\t\tcpu = smp_processor_id();\n\n\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\n\t\trcu_read_unlock();\n\t\tpreempt_enable();\n\t} else\n#endif\n\t{\n\t\tunsigned int qtail;\n\n\t\tret = enqueue_to_backlog(skb, get_cpu(), &qtail);\n\t\tput_cpu();\n\t}\n\treturn ret;\n}\n\n/**\n *\tnetif_rx\t-\tpost buffer to the network code\n *\t@skb: buffer to post\n *\n *\tThis function receives a packet from a device driver and queues it for\n *\tthe upper (protocol) levels to process.  It always succeeds. The buffer\n *\tmay be dropped during processing for congestion control or by the\n *\tprotocol layers.\n *\n *\treturn values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped)\n *\n */\n\nint netif_rx(struct sk_buff *skb)\n{\n\tint ret;\n\n\ttrace_netif_rx_entry(skb);\n\n\tret = netif_rx_internal(skb);\n\ttrace_netif_rx_exit(ret);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_rx);\n\nint netif_rx_ni(struct sk_buff *skb)\n{\n\tint err;\n\n\ttrace_netif_rx_ni_entry(skb);\n\n\tpreempt_disable();\n\terr = netif_rx_internal(skb);\n\tif (local_softirq_pending())\n\t\tdo_softirq();\n\tpreempt_enable();\n\ttrace_netif_rx_ni_exit(err);\n\n\treturn err;\n}\nEXPORT_SYMBOL(netif_rx_ni);\n\nint netif_rx_any_context(struct sk_buff *skb)\n{\n\t/*\n\t * If invoked from contexts which do not invoke bottom half\n\t * processing either at return from interrupt or when softrqs are\n\t * reenabled, use netif_rx_ni() which invokes bottomhalf processing\n\t * directly.\n\t */\n\tif (in_interrupt())\n\t\treturn netif_rx(skb);\n\telse\n\t\treturn netif_rx_ni(skb);\n}\nEXPORT_SYMBOL(netif_rx_any_context);\n\nstatic __latent_entropy void net_tx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\n\tif (sd->completion_queue) {\n\t\tstruct sk_buff *clist;\n\n\t\tlocal_irq_disable();\n\t\tclist = sd->completion_queue;\n\t\tsd->completion_queue = NULL;\n\t\tlocal_irq_enable();\n\n\t\twhile (clist) {\n\t\t\tstruct sk_buff *skb = clist;\n\n\t\t\tclist = clist->next;\n\n\t\t\tWARN_ON(refcount_read(&skb->users));\n\t\t\tif (likely(get_kfree_skb_cb(skb)->reason == SKB_REASON_CONSUMED))\n\t\t\t\ttrace_consume_skb(skb);\n\t\t\telse\n\t\t\t\ttrace_kfree_skb(skb, net_tx_action);\n\n\t\t\tif (skb->fclone != SKB_FCLONE_UNAVAILABLE)\n\t\t\t\t__kfree_skb(skb);\n\t\t\telse\n\t\t\t\t__kfree_skb_defer(skb);\n\t\t}\n\n\t\t__kfree_skb_flush();\n\t}\n\n\tif (sd->output_queue) {\n\t\tstruct Qdisc *head;\n\n\t\tlocal_irq_disable();\n\t\thead = sd->output_queue;\n\t\tsd->output_queue = NULL;\n\t\tsd->output_queue_tailp = &sd->output_queue;\n\t\tlocal_irq_enable();\n\n\t\twhile (head) {\n\t\t\tstruct Qdisc *q = head;\n\t\t\tspinlock_t *root_lock = NULL;\n\n\t\t\thead = head->next_sched;\n\n\t\t\tif (!(q->flags & TCQ_F_NOLOCK)) {\n\t\t\t\troot_lock = qdisc_lock(q);\n\t\t\t\tspin_lock(root_lock);\n\t\t\t}\n\t\t\t/* We need to make sure head->next_sched is read\n\t\t\t * before clearing __QDISC_STATE_SCHED\n\t\t\t */\n\t\t\tsmp_mb__before_atomic();\n\t\t\tclear_bit(__QDISC_STATE_SCHED, &q->state);\n\t\t\tqdisc_run(q);\n\t\t\tif (root_lock)\n\t\t\t\tspin_unlock(root_lock);\n\t\t}\n\t}\n\n\txfrm_dev_backlog(sd);\n}\n\n#if IS_ENABLED(CONFIG_BRIDGE) && IS_ENABLED(CONFIG_ATM_LANE)\n/* This hook is defined here for ATM LANE */\nint (*br_fdb_test_addr_hook)(struct net_device *dev,\n\t\t\t     unsigned char *addr) __read_mostly;\nEXPORT_SYMBOL_GPL(br_fdb_test_addr_hook);\n#endif\n\nstatic inline struct sk_buff *\nsch_handle_ingress(struct sk_buff *skb, struct packet_type **pt_prev, int *ret,\n\t\t   struct net_device *orig_dev, bool *another)\n{\n#ifdef CONFIG_NET_CLS_ACT\n\tstruct mini_Qdisc *miniq = rcu_dereference_bh(skb->dev->miniq_ingress);\n\tstruct tcf_result cl_res;\n\n\t/* If there's at least one ingress present somewhere (so\n\t * we get here via enabled static key), remaining devices\n\t * that are not configured with an ingress qdisc will bail\n\t * out here.\n\t */\n\tif (!miniq)\n\t\treturn skb;\n\n\tif (*pt_prev) {\n\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t*pt_prev = NULL;\n\t}\n\n\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\tqdisc_skb_cb(skb)->mru = 0;\n\tqdisc_skb_cb(skb)->post_ct = false;\n\tskb->tc_at_ingress = 1;\n\tmini_qdisc_bstats_cpu_update(miniq, skb);\n\n\tswitch (tcf_classify_ingress(skb, miniq->block, miniq->filter_list,\n\t\t\t\t     &cl_res, false)) {\n\tcase TC_ACT_OK:\n\tcase TC_ACT_RECLASSIFY:\n\t\tskb->tc_index = TC_H_MIN(cl_res.classid);\n\t\tbreak;\n\tcase TC_ACT_SHOT:\n\t\tmini_qdisc_qstats_cpu_drop(miniq);\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\tcase TC_ACT_STOLEN:\n\tcase TC_ACT_QUEUED:\n\tcase TC_ACT_TRAP:\n\t\tconsume_skb(skb);\n\t\treturn NULL;\n\tcase TC_ACT_REDIRECT:\n\t\t/* skb_mac_header check was done by cls/act_bpf, so\n\t\t * we can safely push the L2 header back before\n\t\t * redirecting to another netdev\n\t\t */\n\t\t__skb_push(skb, skb->mac_len);\n\t\tif (skb_do_redirect(skb) == -EAGAIN) {\n\t\t\t__skb_pull(skb, skb->mac_len);\n\t\t\t*another = true;\n\t\t\tbreak;\n\t\t}\n\t\treturn NULL;\n\tcase TC_ACT_CONSUMED:\n\t\treturn NULL;\n\tdefault:\n\t\tbreak;\n\t}\n#endif /* CONFIG_NET_CLS_ACT */\n\treturn skb;\n}\n\n/**\n *\tnetdev_is_rx_handler_busy - check if receive handler is registered\n *\t@dev: device to check\n *\n *\tCheck if a receive handler is already registered for a given device.\n *\tReturn true if there one.\n *\n *\tThe caller must hold the rtnl_mutex.\n */\nbool netdev_is_rx_handler_busy(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\treturn dev && rtnl_dereference(dev->rx_handler);\n}\nEXPORT_SYMBOL_GPL(netdev_is_rx_handler_busy);\n\n/**\n *\tnetdev_rx_handler_register - register receive handler\n *\t@dev: device to register a handler for\n *\t@rx_handler: receive handler to register\n *\t@rx_handler_data: data pointer that is used by rx handler\n *\n *\tRegister a receive handler for a device. This handler will then be\n *\tcalled from __netif_receive_skb. A negative errno code is returned\n *\ton a failure.\n *\n *\tThe caller must hold the rtnl_mutex.\n *\n *\tFor a general description of rx_handler, see enum rx_handler_result.\n */\nint netdev_rx_handler_register(struct net_device *dev,\n\t\t\t       rx_handler_func_t *rx_handler,\n\t\t\t       void *rx_handler_data)\n{\n\tif (netdev_is_rx_handler_busy(dev))\n\t\treturn -EBUSY;\n\n\tif (dev->priv_flags & IFF_NO_RX_HANDLER)\n\t\treturn -EINVAL;\n\n\t/* Note: rx_handler_data must be set before rx_handler */\n\trcu_assign_pointer(dev->rx_handler_data, rx_handler_data);\n\trcu_assign_pointer(dev->rx_handler, rx_handler);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_register);\n\n/**\n *\tnetdev_rx_handler_unregister - unregister receive handler\n *\t@dev: device to unregister a handler from\n *\n *\tUnregister a receive handler from a device.\n *\n *\tThe caller must hold the rtnl_mutex.\n */\nvoid netdev_rx_handler_unregister(struct net_device *dev)\n{\n\n\tASSERT_RTNL();\n\tRCU_INIT_POINTER(dev->rx_handler, NULL);\n\t/* a reader seeing a non NULL rx_handler in a rcu_read_lock()\n\t * section has a guarantee to see a non NULL rx_handler_data\n\t * as well.\n\t */\n\tsynchronize_net();\n\tRCU_INIT_POINTER(dev->rx_handler_data, NULL);\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_unregister);\n\n/*\n * Limit the use of PFMEMALLOC reserves to those protocols that implement\n * the special handling of PFMEMALLOC skbs.\n */\nstatic bool skb_pfmemalloc_protocol(struct sk_buff *skb)\n{\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_ARP):\n\tcase htons(ETH_P_IP):\n\tcase htons(ETH_P_IPV6):\n\tcase htons(ETH_P_8021Q):\n\tcase htons(ETH_P_8021AD):\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic inline int nf_ingress(struct sk_buff *skb, struct packet_type **pt_prev,\n\t\t\t     int *ret, struct net_device *orig_dev)\n{\n\tif (nf_hook_ingress_active(skb)) {\n\t\tint ingress_retval;\n\n\t\tif (*pt_prev) {\n\t\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t\t*pt_prev = NULL;\n\t\t}\n\n\t\trcu_read_lock();\n\t\tingress_retval = nf_hook_ingress(skb);\n\t\trcu_read_unlock();\n\t\treturn ingress_retval;\n\t}\n\treturn 0;\n}\n\nstatic int __netif_receive_skb_core(struct sk_buff **pskb, bool pfmemalloc,\n\t\t\t\t    struct packet_type **ppt_prev)\n{\n\tstruct packet_type *ptype, *pt_prev;\n\trx_handler_func_t *rx_handler;\n\tstruct sk_buff *skb = *pskb;\n\tstruct net_device *orig_dev;\n\tbool deliver_exact = false;\n\tint ret = NET_RX_DROP;\n\t__be16 type;\n\n\tnet_timestamp_check(!netdev_tstamp_prequeue, skb);\n\n\ttrace_netif_receive_skb(skb);\n\n\torig_dev = skb->dev;\n\n\tskb_reset_network_header(skb);\n\tif (!skb_transport_header_was_set(skb))\n\t\tskb_reset_transport_header(skb);\n\tskb_reset_mac_len(skb);\n\n\tpt_prev = NULL;\n\nanother_round:\n\tskb->skb_iif = skb->dev->ifindex;\n\n\t__this_cpu_inc(softnet_data.processed);\n\n\tif (static_branch_unlikely(&generic_xdp_needed_key)) {\n\t\tint ret2;\n\n\t\tpreempt_disable();\n\t\tret2 = do_xdp_generic(rcu_dereference(skb->dev->xdp_prog), skb);\n\t\tpreempt_enable();\n\n\t\tif (ret2 != XDP_PASS) {\n\t\t\tret = NET_RX_DROP;\n\t\t\tgoto out;\n\t\t}\n\t\tskb_reset_mac_len(skb);\n\t}\n\n\tif (eth_type_vlan(skb->protocol)) {\n\t\tskb = skb_vlan_untag(skb);\n\t\tif (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n\tif (skb_skip_tc_classify(skb))\n\t\tgoto skip_classify;\n\n\tif (pfmemalloc)\n\t\tgoto skip_taps;\n\n\tlist_for_each_entry_rcu(ptype, &ptype_all, list) {\n\t\tif (pt_prev)\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\n\tlist_for_each_entry_rcu(ptype, &skb->dev->ptype_all, list) {\n\t\tif (pt_prev)\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\nskip_taps:\n#ifdef CONFIG_NET_INGRESS\n\tif (static_branch_unlikely(&ingress_needed_key)) {\n\t\tbool another = false;\n\n\t\tskb = sch_handle_ingress(skb, &pt_prev, &ret, orig_dev,\n\t\t\t\t\t &another);\n\t\tif (another)\n\t\t\tgoto another_round;\n\t\tif (!skb)\n\t\t\tgoto out;\n\n\t\tif (nf_ingress(skb, &pt_prev, &ret, orig_dev) < 0)\n\t\t\tgoto out;\n\t}\n#endif\n\tskb_reset_redirect(skb);\nskip_classify:\n\tif (pfmemalloc && !skb_pfmemalloc_protocol(skb))\n\t\tgoto drop;\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tif (vlan_do_receive(&skb))\n\t\t\tgoto another_round;\n\t\telse if (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n\trx_handler = rcu_dereference(skb->dev->rx_handler);\n\tif (rx_handler) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tswitch (rx_handler(&skb)) {\n\t\tcase RX_HANDLER_CONSUMED:\n\t\t\tret = NET_RX_SUCCESS;\n\t\t\tgoto out;\n\t\tcase RX_HANDLER_ANOTHER:\n\t\t\tgoto another_round;\n\t\tcase RX_HANDLER_EXACT:\n\t\t\tdeliver_exact = true;\n\t\tcase RX_HANDLER_PASS:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t}\n\n\tif (unlikely(skb_vlan_tag_present(skb)) && !netdev_uses_dsa(skb->dev)) {\ncheck_vlan_id:\n\t\tif (skb_vlan_tag_get_id(skb)) {\n\t\t\t/* Vlan id is non 0 and vlan_do_receive() above couldn't\n\t\t\t * find vlan device.\n\t\t\t */\n\t\t\tskb->pkt_type = PACKET_OTHERHOST;\n\t\t} else if (eth_type_vlan(skb->protocol)) {\n\t\t\t/* Outer header is 802.1P with vlan 0, inner header is\n\t\t\t * 802.1Q or 802.1AD and vlan_do_receive() above could\n\t\t\t * not find vlan dev for vlan id 0.\n\t\t\t */\n\t\t\t__vlan_hwaccel_clear_tag(skb);\n\t\t\tskb = skb_vlan_untag(skb);\n\t\t\tif (unlikely(!skb))\n\t\t\t\tgoto out;\n\t\t\tif (vlan_do_receive(&skb))\n\t\t\t\t/* After stripping off 802.1P header with vlan 0\n\t\t\t\t * vlan dev is found for inner header.\n\t\t\t\t */\n\t\t\t\tgoto another_round;\n\t\t\telse if (unlikely(!skb))\n\t\t\t\tgoto out;\n\t\t\telse\n\t\t\t\t/* We have stripped outer 802.1P vlan 0 header.\n\t\t\t\t * But could not find vlan dev.\n\t\t\t\t * check again for vlan id to set OTHERHOST.\n\t\t\t\t */\n\t\t\t\tgoto check_vlan_id;\n\t\t}\n\t\t/* Note: we might in the future use prio bits\n\t\t * and set skb->priority like in vlan_do_receive()\n\t\t * For the time being, just ignore Priority Code Point\n\t\t */\n\t\t__vlan_hwaccel_clear_tag(skb);\n\t}\n\n\ttype = skb->protocol;\n\n\t/* deliver only exact match when indicated */\n\tif (likely(!deliver_exact)) {\n\t\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t\t       &ptype_base[ntohs(type) &\n\t\t\t\t\t\t   PTYPE_HASH_MASK]);\n\t}\n\n\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t       &orig_dev->ptype_specific);\n\n\tif (unlikely(skb->dev != orig_dev)) {\n\t\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t\t       &skb->dev->ptype_specific);\n\t}\n\n\tif (pt_prev) {\n\t\tif (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))\n\t\t\tgoto drop;\n\t\t*ppt_prev = pt_prev;\n\t} else {\ndrop:\n\t\tif (!deliver_exact)\n\t\t\tatomic_long_inc(&skb->dev->rx_dropped);\n\t\telse\n\t\t\tatomic_long_inc(&skb->dev->rx_nohandler);\n\t\tkfree_skb(skb);\n\t\t/* Jamal, now you will not able to escape explaining\n\t\t * me how you were going to use this. :-)\n\t\t */\n\t\tret = NET_RX_DROP;\n\t}\n\nout:\n\t/* The invariant here is that if *ppt_prev is not NULL\n\t * then skb should also be non-NULL.\n\t *\n\t * Apparently *ppt_prev assignment above holds this invariant due to\n\t * skb dereferencing near it.\n\t */\n\t*pskb = skb;\n\treturn ret;\n}\n\nstatic int __netif_receive_skb_one_core(struct sk_buff *skb, bool pfmemalloc)\n{\n\tstruct net_device *orig_dev = skb->dev;\n\tstruct packet_type *pt_prev = NULL;\n\tint ret;\n\n\tret = __netif_receive_skb_core(&skb, pfmemalloc, &pt_prev);\n\tif (pt_prev)\n\t\tret = INDIRECT_CALL_INET(pt_prev->func, ipv6_rcv, ip_rcv, skb,\n\t\t\t\t\t skb->dev, pt_prev, orig_dev);\n\treturn ret;\n}\n\n/**\n *\tnetif_receive_skb_core - special purpose version of netif_receive_skb\n *\t@skb: buffer to process\n *\n *\tMore direct receive version of netif_receive_skb().  It should\n *\tonly be used by callers that have a need to skip RPS and Generic XDP.\n *\tCaller must also take care of handling if ``(page_is_)pfmemalloc``.\n *\n *\tThis function may only be called from softirq context and interrupts\n *\tshould be enabled.\n *\n *\tReturn values (usually ignored):\n *\tNET_RX_SUCCESS: no congestion\n *\tNET_RX_DROP: packet was dropped\n */\nint netif_receive_skb_core(struct sk_buff *skb)\n{\n\tint ret;\n\n\trcu_read_lock();\n\tret = __netif_receive_skb_one_core(skb, false);\n\trcu_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_receive_skb_core);\n\nstatic inline void __netif_receive_skb_list_ptype(struct list_head *head,\n\t\t\t\t\t\t  struct packet_type *pt_prev,\n\t\t\t\t\t\t  struct net_device *orig_dev)\n{\n\tstruct sk_buff *skb, *next;\n\n\tif (!pt_prev)\n\t\treturn;\n\tif (list_empty(head))\n\t\treturn;\n\tif (pt_prev->list_func != NULL)\n\t\tINDIRECT_CALL_INET(pt_prev->list_func, ipv6_list_rcv,\n\t\t\t\t   ip_list_rcv, head, pt_prev, orig_dev);\n\telse\n\t\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\t\tskb_list_del_init(skb);\n\t\t\tpt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n\t\t}\n}\n\nstatic void __netif_receive_skb_list_core(struct list_head *head, bool pfmemalloc)\n{\n\t/* Fast-path assumptions:\n\t * - There is no RX handler.\n\t * - Only one packet_type matches.\n\t * If either of these fails, we will end up doing some per-packet\n\t * processing in-line, then handling the 'last ptype' for the whole\n\t * sublist.  This can't cause out-of-order delivery to any single ptype,\n\t * because the 'last ptype' must be constant across the sublist, and all\n\t * other ptypes are handled per-packet.\n\t */\n\t/* Current (common) ptype of sublist */\n\tstruct packet_type *pt_curr = NULL;\n\t/* Current (common) orig_dev of sublist */\n\tstruct net_device *od_curr = NULL;\n\tstruct list_head sublist;\n\tstruct sk_buff *skb, *next;\n\n\tINIT_LIST_HEAD(&sublist);\n\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\tstruct net_device *orig_dev = skb->dev;\n\t\tstruct packet_type *pt_prev = NULL;\n\n\t\tskb_list_del_init(skb);\n\t\t__netif_receive_skb_core(&skb, pfmemalloc, &pt_prev);\n\t\tif (!pt_prev)\n\t\t\tcontinue;\n\t\tif (pt_curr != pt_prev || od_curr != orig_dev) {\n\t\t\t/* dispatch old sublist */\n\t\t\t__netif_receive_skb_list_ptype(&sublist, pt_curr, od_curr);\n\t\t\t/* start new sublist */\n\t\t\tINIT_LIST_HEAD(&sublist);\n\t\t\tpt_curr = pt_prev;\n\t\t\tod_curr = orig_dev;\n\t\t}\n\t\tlist_add_tail(&skb->list, &sublist);\n\t}\n\n\t/* dispatch final sublist */\n\t__netif_receive_skb_list_ptype(&sublist, pt_curr, od_curr);\n}\n\nstatic int __netif_receive_skb(struct sk_buff *skb)\n{\n\tint ret;\n\n\tif (sk_memalloc_socks() && skb_pfmemalloc(skb)) {\n\t\tunsigned int noreclaim_flag;\n\n\t\t/*\n\t\t * PFMEMALLOC skbs are special, they should\n\t\t * - be delivered to SOCK_MEMALLOC sockets only\n\t\t * - stay away from userspace\n\t\t * - have bounded memory usage\n\t\t *\n\t\t * Use PF_MEMALLOC as this saves us from propagating the allocation\n\t\t * context down to all allocation sites.\n\t\t */\n\t\tnoreclaim_flag = memalloc_noreclaim_save();\n\t\tret = __netif_receive_skb_one_core(skb, true);\n\t\tmemalloc_noreclaim_restore(noreclaim_flag);\n\t} else\n\t\tret = __netif_receive_skb_one_core(skb, false);\n\n\treturn ret;\n}\n\nstatic void __netif_receive_skb_list(struct list_head *head)\n{\n\tunsigned long noreclaim_flag = 0;\n\tstruct sk_buff *skb, *next;\n\tbool pfmemalloc = false; /* Is current sublist PF_MEMALLOC? */\n\n\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\tif ((sk_memalloc_socks() && skb_pfmemalloc(skb)) != pfmemalloc) {\n\t\t\tstruct list_head sublist;\n\n\t\t\t/* Handle the previous sublist */\n\t\t\tlist_cut_before(&sublist, head, &skb->list);\n\t\t\tif (!list_empty(&sublist))\n\t\t\t\t__netif_receive_skb_list_core(&sublist, pfmemalloc);\n\t\t\tpfmemalloc = !pfmemalloc;\n\t\t\t/* See comments in __netif_receive_skb */\n\t\t\tif (pfmemalloc)\n\t\t\t\tnoreclaim_flag = memalloc_noreclaim_save();\n\t\t\telse\n\t\t\t\tmemalloc_noreclaim_restore(noreclaim_flag);\n\t\t}\n\t}\n\t/* Handle the remaining sublist */\n\tif (!list_empty(head))\n\t\t__netif_receive_skb_list_core(head, pfmemalloc);\n\t/* Restore pflags */\n\tif (pfmemalloc)\n\t\tmemalloc_noreclaim_restore(noreclaim_flag);\n}\n\nstatic int generic_xdp_install(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tstruct bpf_prog *old = rtnl_dereference(dev->xdp_prog);\n\tstruct bpf_prog *new = xdp->prog;\n\tint ret = 0;\n\n\tif (new) {\n\t\tu32 i;\n\n\t\tmutex_lock(&new->aux->used_maps_mutex);\n\n\t\t/* generic XDP does not work with DEVMAPs that can\n\t\t * have a bpf_prog installed on an entry\n\t\t */\n\t\tfor (i = 0; i < new->aux->used_map_cnt; i++) {\n\t\t\tif (dev_map_can_have_prog(new->aux->used_maps[i]) ||\n\t\t\t    cpu_map_prog_allowed(new->aux->used_maps[i])) {\n\t\t\t\tmutex_unlock(&new->aux->used_maps_mutex);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\tmutex_unlock(&new->aux->used_maps_mutex);\n\t}\n\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\trcu_assign_pointer(dev->xdp_prog, new);\n\t\tif (old)\n\t\t\tbpf_prog_put(old);\n\n\t\tif (old && !new) {\n\t\t\tstatic_branch_dec(&generic_xdp_needed_key);\n\t\t} else if (new && !old) {\n\t\t\tstatic_branch_inc(&generic_xdp_needed_key);\n\t\t\tdev_disable_lro(dev);\n\t\t\tdev_disable_gro_hw(dev);\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int netif_receive_skb_internal(struct sk_buff *skb)\n{\n\tint ret;\n\n\tnet_timestamp_check(netdev_tstamp_prequeue, skb);\n\n\tif (skb_defer_rx_timestamp(skb))\n\t\treturn NET_RX_SUCCESS;\n\n\trcu_read_lock();\n#ifdef CONFIG_RPS\n\tif (static_branch_unlikely(&rps_needed)) {\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu = get_rps_cpu(skb->dev, skb, &rflow);\n\n\t\tif (cpu >= 0) {\n\t\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\t\t\trcu_read_unlock();\n\t\t\treturn ret;\n\t\t}\n\t}\n#endif\n\tret = __netif_receive_skb(skb);\n\trcu_read_unlock();\n\treturn ret;\n}\n\nstatic void netif_receive_skb_list_internal(struct list_head *head)\n{\n\tstruct sk_buff *skb, *next;\n\tstruct list_head sublist;\n\n\tINIT_LIST_HEAD(&sublist);\n\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\tnet_timestamp_check(netdev_tstamp_prequeue, skb);\n\t\tskb_list_del_init(skb);\n\t\tif (!skb_defer_rx_timestamp(skb))\n\t\t\tlist_add_tail(&skb->list, &sublist);\n\t}\n\tlist_splice_init(&sublist, head);\n\n\trcu_read_lock();\n#ifdef CONFIG_RPS\n\tif (static_branch_unlikely(&rps_needed)) {\n\t\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\t\tint cpu = get_rps_cpu(skb->dev, skb, &rflow);\n\n\t\t\tif (cpu >= 0) {\n\t\t\t\t/* Will be handled, remove from list */\n\t\t\t\tskb_list_del_init(skb);\n\t\t\t\tenqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\t\t\t}\n\t\t}\n\t}\n#endif\n\t__netif_receive_skb_list(head);\n\trcu_read_unlock();\n}\n\n/**\n *\tnetif_receive_skb - process receive buffer from network\n *\t@skb: buffer to process\n *\n *\tnetif_receive_skb() is the main receive data processing function.\n *\tIt always succeeds. The buffer may be dropped during processing\n *\tfor congestion control or by the protocol layers.\n *\n *\tThis function may only be called from softirq context and interrupts\n *\tshould be enabled.\n *\n *\tReturn values (usually ignored):\n *\tNET_RX_SUCCESS: no congestion\n *\tNET_RX_DROP: packet was dropped\n */\nint netif_receive_skb(struct sk_buff *skb)\n{\n\tint ret;\n\n\ttrace_netif_receive_skb_entry(skb);\n\n\tret = netif_receive_skb_internal(skb);\n\ttrace_netif_receive_skb_exit(ret);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_receive_skb);\n\n/**\n *\tnetif_receive_skb_list - process many receive buffers from network\n *\t@head: list of skbs to process.\n *\n *\tSince return value of netif_receive_skb() is normally ignored, and\n *\twouldn't be meaningful for a list, this function returns void.\n *\n *\tThis function may only be called from softirq context and interrupts\n *\tshould be enabled.\n */\nvoid netif_receive_skb_list(struct list_head *head)\n{\n\tstruct sk_buff *skb;\n\n\tif (list_empty(head))\n\t\treturn;\n\tif (trace_netif_receive_skb_list_entry_enabled()) {\n\t\tlist_for_each_entry(skb, head, list)\n\t\t\ttrace_netif_receive_skb_list_entry(skb);\n\t}\n\tnetif_receive_skb_list_internal(head);\n\ttrace_netif_receive_skb_list_exit(0);\n}\nEXPORT_SYMBOL(netif_receive_skb_list);\n\nstatic DEFINE_PER_CPU(struct work_struct, flush_works);\n\n/* Network device is going away, flush any packets still pending */\nstatic void flush_backlog(struct work_struct *work)\n{\n\tstruct sk_buff *skb, *tmp;\n\tstruct softnet_data *sd;\n\n\tlocal_bh_disable();\n\tsd = this_cpu_ptr(&softnet_data);\n\n\tlocal_irq_disable();\n\trps_lock(sd);\n\tskb_queue_walk_safe(&sd->input_pkt_queue, skb, tmp) {\n\t\tif (skb->dev->reg_state == NETREG_UNREGISTERING) {\n\t\t\t__skb_unlink(skb, &sd->input_pkt_queue);\n\t\t\tdev_kfree_skb_irq(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n\trps_unlock(sd);\n\tlocal_irq_enable();\n\n\tskb_queue_walk_safe(&sd->process_queue, skb, tmp) {\n\t\tif (skb->dev->reg_state == NETREG_UNREGISTERING) {\n\t\t\t__skb_unlink(skb, &sd->process_queue);\n\t\t\tkfree_skb(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n\tlocal_bh_enable();\n}\n\nstatic bool flush_required(int cpu)\n{\n#if IS_ENABLED(CONFIG_RPS)\n\tstruct softnet_data *sd = &per_cpu(softnet_data, cpu);\n\tbool do_flush;\n\n\tlocal_irq_disable();\n\trps_lock(sd);\n\n\t/* as insertion into process_queue happens with the rps lock held,\n\t * process_queue access may race only with dequeue\n\t */\n\tdo_flush = !skb_queue_empty(&sd->input_pkt_queue) ||\n\t\t   !skb_queue_empty_lockless(&sd->process_queue);\n\trps_unlock(sd);\n\tlocal_irq_enable();\n\n\treturn do_flush;\n#endif\n\t/* without RPS we can't safely check input_pkt_queue: during a\n\t * concurrent remote skb_queue_splice() we can detect as empty both\n\t * input_pkt_queue and process_queue even if the latter could end-up\n\t * containing a lot of packets.\n\t */\n\treturn true;\n}\n\nstatic void flush_all_backlogs(void)\n{\n\tstatic cpumask_t flush_cpus;\n\tunsigned int cpu;\n\n\t/* since we are under rtnl lock protection we can use static data\n\t * for the cpumask and avoid allocating on stack the possibly\n\t * large mask\n\t */\n\tASSERT_RTNL();\n\n\tget_online_cpus();\n\n\tcpumask_clear(&flush_cpus);\n\tfor_each_online_cpu(cpu) {\n\t\tif (flush_required(cpu)) {\n\t\t\tqueue_work_on(cpu, system_highpri_wq,\n\t\t\t\t      per_cpu_ptr(&flush_works, cpu));\n\t\t\tcpumask_set_cpu(cpu, &flush_cpus);\n\t\t}\n\t}\n\n\t/* we can have in flight packet[s] on the cpus we are not flushing,\n\t * synchronize_net() in unregister_netdevice_many() will take care of\n\t * them\n\t */\n\tfor_each_cpu(cpu, &flush_cpus)\n\t\tflush_work(per_cpu_ptr(&flush_works, cpu));\n\n\tput_online_cpus();\n}\n\n/* Pass the currently batched GRO_NORMAL SKBs up to the stack. */\nstatic void gro_normal_list(struct napi_struct *napi)\n{\n\tif (!napi->rx_count)\n\t\treturn;\n\tnetif_receive_skb_list_internal(&napi->rx_list);\n\tINIT_LIST_HEAD(&napi->rx_list);\n\tnapi->rx_count = 0;\n}\n\n/* Queue one GRO_NORMAL SKB up for list processing. If batch size exceeded,\n * pass the whole batch up to the stack.\n */\nstatic void gro_normal_one(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tlist_add_tail(&skb->list, &napi->rx_list);\n\tif (++napi->rx_count >= gro_normal_batch)\n\t\tgro_normal_list(napi);\n}\n\nINDIRECT_CALLABLE_DECLARE(int inet_gro_complete(struct sk_buff *, int));\nINDIRECT_CALLABLE_DECLARE(int ipv6_gro_complete(struct sk_buff *, int));\nstatic int napi_gro_complete(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tstruct packet_offload *ptype;\n\t__be16 type = skb->protocol;\n\tstruct list_head *head = &offload_base;\n\tint err = -ENOENT;\n\n\tBUILD_BUG_ON(sizeof(struct napi_gro_cb) > sizeof(skb->cb));\n\n\tif (NAPI_GRO_CB(skb)->count == 1) {\n\t\tskb_shinfo(skb)->gso_size = 0;\n\t\tgoto out;\n\t}\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_complete)\n\t\t\tcontinue;\n\n\t\terr = INDIRECT_CALL_INET(ptype->callbacks.gro_complete,\n\t\t\t\t\t ipv6_gro_complete, inet_gro_complete,\n\t\t\t\t\t skb, 0);\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\tif (err) {\n\t\tWARN_ON(&ptype->list == head);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_SUCCESS;\n\t}\n\nout:\n\tgro_normal_one(napi, skb);\n\treturn NET_RX_SUCCESS;\n}\n\nstatic void __napi_gro_flush_chain(struct napi_struct *napi, u32 index,\n\t\t\t\t   bool flush_old)\n{\n\tstruct list_head *head = &napi->gro_hash[index].list;\n\tstruct sk_buff *skb, *p;\n\n\tlist_for_each_entry_safe_reverse(skb, p, head, list) {\n\t\tif (flush_old && NAPI_GRO_CB(skb)->age == jiffies)\n\t\t\treturn;\n\t\tskb_list_del_init(skb);\n\t\tnapi_gro_complete(napi, skb);\n\t\tnapi->gro_hash[index].count--;\n\t}\n\n\tif (!napi->gro_hash[index].count)\n\t\t__clear_bit(index, &napi->gro_bitmask);\n}\n\n/* napi->gro_hash[].list contains packets ordered by age.\n * youngest packets at the head of it.\n * Complete skbs in reverse order to reduce latencies.\n */\nvoid napi_gro_flush(struct napi_struct *napi, bool flush_old)\n{\n\tunsigned long bitmask = napi->gro_bitmask;\n\tunsigned int i, base = ~0U;\n\n\twhile ((i = ffs(bitmask)) != 0) {\n\t\tbitmask >>= i;\n\t\tbase += i;\n\t\t__napi_gro_flush_chain(napi, base, flush_old);\n\t}\n}\nEXPORT_SYMBOL(napi_gro_flush);\n\nstatic struct list_head *gro_list_prepare(struct napi_struct *napi,\n\t\t\t\t\t  struct sk_buff *skb)\n{\n\tunsigned int maclen = skb->dev->hard_header_len;\n\tu32 hash = skb_get_hash_raw(skb);\n\tstruct list_head *head;\n\tstruct sk_buff *p;\n\n\thead = &napi->gro_hash[hash & (GRO_HASH_BUCKETS - 1)].list;\n\tlist_for_each_entry(p, head, list) {\n\t\tunsigned long diffs;\n\n\t\tNAPI_GRO_CB(p)->flush = 0;\n\n\t\tif (hash != skb_get_hash_raw(p)) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tdiffs = (unsigned long)p->dev ^ (unsigned long)skb->dev;\n\t\tdiffs |= skb_vlan_tag_present(p) ^ skb_vlan_tag_present(skb);\n\t\tif (skb_vlan_tag_present(p))\n\t\t\tdiffs |= skb_vlan_tag_get(p) ^ skb_vlan_tag_get(skb);\n\t\tdiffs |= skb_metadata_dst_cmp(p, skb);\n\t\tdiffs |= skb_metadata_differs(p, skb);\n\t\tif (maclen == ETH_HLEN)\n\t\t\tdiffs |= compare_ether_header(skb_mac_header(p),\n\t\t\t\t\t\t      skb_mac_header(skb));\n\t\telse if (!diffs)\n\t\t\tdiffs = memcmp(skb_mac_header(p),\n\t\t\t\t       skb_mac_header(skb),\n\t\t\t\t       maclen);\n\t\tNAPI_GRO_CB(p)->same_flow = !diffs;\n\t}\n\n\treturn head;\n}\n\nstatic void skb_gro_reset_offset(struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *pinfo = skb_shinfo(skb);\n\tconst skb_frag_t *frag0 = &pinfo->frags[0];\n\n\tNAPI_GRO_CB(skb)->data_offset = 0;\n\tNAPI_GRO_CB(skb)->frag0 = NULL;\n\tNAPI_GRO_CB(skb)->frag0_len = 0;\n\n\tif (!skb_headlen(skb) && pinfo->nr_frags &&\n\t    !PageHighMem(skb_frag_page(frag0))) {\n\t\tNAPI_GRO_CB(skb)->frag0 = skb_frag_address(frag0);\n\t\tNAPI_GRO_CB(skb)->frag0_len = min_t(unsigned int,\n\t\t\t\t\t\t    skb_frag_size(frag0),\n\t\t\t\t\t\t    skb->end - skb->tail);\n\t}\n}\n\nstatic void gro_pull_from_frag0(struct sk_buff *skb, int grow)\n{\n\tstruct skb_shared_info *pinfo = skb_shinfo(skb);\n\n\tBUG_ON(skb->end - skb->tail < grow);\n\n\tmemcpy(skb_tail_pointer(skb), NAPI_GRO_CB(skb)->frag0, grow);\n\n\tskb->data_len -= grow;\n\tskb->tail += grow;\n\n\tskb_frag_off_add(&pinfo->frags[0], grow);\n\tskb_frag_size_sub(&pinfo->frags[0], grow);\n\n\tif (unlikely(!skb_frag_size(&pinfo->frags[0]))) {\n\t\tskb_frag_unref(skb, 0);\n\t\tmemmove(pinfo->frags, pinfo->frags + 1,\n\t\t\t--pinfo->nr_frags * sizeof(pinfo->frags[0]));\n\t}\n}\n\nstatic void gro_flush_oldest(struct napi_struct *napi, struct list_head *head)\n{\n\tstruct sk_buff *oldest;\n\n\toldest = list_last_entry(head, struct sk_buff, list);\n\n\t/* We are called with head length >= MAX_GRO_SKBS, so this is\n\t * impossible.\n\t */\n\tif (WARN_ON_ONCE(!oldest))\n\t\treturn;\n\n\t/* Do not adjust napi->gro_hash[].count, caller is adding a new\n\t * SKB to the chain.\n\t */\n\tskb_list_del_init(oldest);\n\tnapi_gro_complete(napi, oldest);\n}\n\nINDIRECT_CALLABLE_DECLARE(struct sk_buff *inet_gro_receive(struct list_head *,\n\t\t\t\t\t\t\t   struct sk_buff *));\nINDIRECT_CALLABLE_DECLARE(struct sk_buff *ipv6_gro_receive(struct list_head *,\n\t\t\t\t\t\t\t   struct sk_buff *));\nstatic enum gro_result dev_gro_receive(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tu32 hash = skb_get_hash_raw(skb) & (GRO_HASH_BUCKETS - 1);\n\tstruct list_head *head = &offload_base;\n\tstruct packet_offload *ptype;\n\t__be16 type = skb->protocol;\n\tstruct list_head *gro_head;\n\tstruct sk_buff *pp = NULL;\n\tenum gro_result ret;\n\tint same_flow;\n\tint grow;\n\n\tif (netif_elide_gro(skb->dev))\n\t\tgoto normal;\n\n\tgro_head = gro_list_prepare(napi, skb);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_receive)\n\t\t\tcontinue;\n\n\t\tskb_set_network_header(skb, skb_gro_offset(skb));\n\t\tskb_reset_mac_len(skb);\n\t\tNAPI_GRO_CB(skb)->same_flow = 0;\n\t\tNAPI_GRO_CB(skb)->flush = skb_is_gso(skb) || skb_has_frag_list(skb);\n\t\tNAPI_GRO_CB(skb)->free = 0;\n\t\tNAPI_GRO_CB(skb)->encap_mark = 0;\n\t\tNAPI_GRO_CB(skb)->recursion_counter = 0;\n\t\tNAPI_GRO_CB(skb)->is_fou = 0;\n\t\tNAPI_GRO_CB(skb)->is_atomic = 1;\n\t\tNAPI_GRO_CB(skb)->gro_remcsum_start = 0;\n\n\t\t/* Setup for GRO checksum validation */\n\t\tswitch (skb->ip_summed) {\n\t\tcase CHECKSUM_COMPLETE:\n\t\t\tNAPI_GRO_CB(skb)->csum = skb->csum;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 1;\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = 0;\n\t\t\tbreak;\n\t\tcase CHECKSUM_UNNECESSARY:\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = skb->csum_level + 1;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = 0;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 0;\n\t\t}\n\n\t\tpp = INDIRECT_CALL_INET(ptype->callbacks.gro_receive,\n\t\t\t\t\tipv6_gro_receive, inet_gro_receive,\n\t\t\t\t\tgro_head, skb);\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\tif (&ptype->list == head)\n\t\tgoto normal;\n\n\tif (PTR_ERR(pp) == -EINPROGRESS) {\n\t\tret = GRO_CONSUMED;\n\t\tgoto ok;\n\t}\n\n\tsame_flow = NAPI_GRO_CB(skb)->same_flow;\n\tret = NAPI_GRO_CB(skb)->free ? GRO_MERGED_FREE : GRO_MERGED;\n\n\tif (pp) {\n\t\tskb_list_del_init(pp);\n\t\tnapi_gro_complete(napi, pp);\n\t\tnapi->gro_hash[hash].count--;\n\t}\n\n\tif (same_flow)\n\t\tgoto ok;\n\n\tif (NAPI_GRO_CB(skb)->flush)\n\t\tgoto normal;\n\n\tif (unlikely(napi->gro_hash[hash].count >= MAX_GRO_SKBS)) {\n\t\tgro_flush_oldest(napi, gro_head);\n\t} else {\n\t\tnapi->gro_hash[hash].count++;\n\t}\n\tNAPI_GRO_CB(skb)->count = 1;\n\tNAPI_GRO_CB(skb)->age = jiffies;\n\tNAPI_GRO_CB(skb)->last = skb;\n\tskb_shinfo(skb)->gso_size = skb_gro_len(skb);\n\tlist_add(&skb->list, gro_head);\n\tret = GRO_HELD;\n\npull:\n\tgrow = skb_gro_offset(skb) - skb_headlen(skb);\n\tif (grow > 0)\n\t\tgro_pull_from_frag0(skb, grow);\nok:\n\tif (napi->gro_hash[hash].count) {\n\t\tif (!test_bit(hash, &napi->gro_bitmask))\n\t\t\t__set_bit(hash, &napi->gro_bitmask);\n\t} else if (test_bit(hash, &napi->gro_bitmask)) {\n\t\t__clear_bit(hash, &napi->gro_bitmask);\n\t}\n\n\treturn ret;\n\nnormal:\n\tret = GRO_NORMAL;\n\tgoto pull;\n}\n\nstruct packet_offload *gro_find_receive_by_type(__be16 type)\n{\n\tstruct list_head *offload_head = &offload_base;\n\tstruct packet_offload *ptype;\n\n\tlist_for_each_entry_rcu(ptype, offload_head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_receive)\n\t\t\tcontinue;\n\t\treturn ptype;\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(gro_find_receive_by_type);\n\nstruct packet_offload *gro_find_complete_by_type(__be16 type)\n{\n\tstruct list_head *offload_head = &offload_base;\n\tstruct packet_offload *ptype;\n\n\tlist_for_each_entry_rcu(ptype, offload_head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_complete)\n\t\t\tcontinue;\n\t\treturn ptype;\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(gro_find_complete_by_type);\n\nstatic void napi_skb_free_stolen_head(struct sk_buff *skb)\n{\n\tskb_dst_drop(skb);\n\tskb_ext_put(skb);\n\tkmem_cache_free(skbuff_head_cache, skb);\n}\n\nstatic gro_result_t napi_skb_finish(struct napi_struct *napi,\n\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t    gro_result_t ret)\n{\n\tswitch (ret) {\n\tcase GRO_NORMAL:\n\t\tgro_normal_one(napi, skb);\n\t\tbreak;\n\n\tcase GRO_MERGED_FREE:\n\t\tif (NAPI_GRO_CB(skb)->free == NAPI_GRO_FREE_STOLEN_HEAD)\n\t\t\tnapi_skb_free_stolen_head(skb);\n\t\telse\n\t\t\t__kfree_skb(skb);\n\t\tbreak;\n\n\tcase GRO_HELD:\n\tcase GRO_MERGED:\n\tcase GRO_CONSUMED:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\ngro_result_t napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tgro_result_t ret;\n\n\tskb_mark_napi_id(skb, napi);\n\ttrace_napi_gro_receive_entry(skb);\n\n\tskb_gro_reset_offset(skb);\n\n\tret = napi_skb_finish(napi, skb, dev_gro_receive(napi, skb));\n\ttrace_napi_gro_receive_exit(ret);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(napi_gro_receive);\n\nstatic void napi_reuse_skb(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tif (unlikely(skb->pfmemalloc)) {\n\t\tconsume_skb(skb);\n\t\treturn;\n\t}\n\t__skb_pull(skb, skb_headlen(skb));\n\t/* restore the reserve we had after netdev_alloc_skb_ip_align() */\n\tskb_reserve(skb, NET_SKB_PAD + NET_IP_ALIGN - skb_headroom(skb));\n\t__vlan_hwaccel_clear_tag(skb);\n\tskb->dev = napi->dev;\n\tskb->skb_iif = 0;\n\n\t/* eth_type_trans() assumes pkt_type is PACKET_HOST */\n\tskb->pkt_type = PACKET_HOST;\n\n\tskb->encapsulation = 0;\n\tskb_shinfo(skb)->gso_type = 0;\n\tskb->truesize = SKB_TRUESIZE(skb_end_offset(skb));\n\tskb_ext_reset(skb);\n\n\tnapi->skb = skb;\n}\n\nstruct sk_buff *napi_get_frags(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi->skb;\n\n\tif (!skb) {\n\t\tskb = napi_alloc_skb(napi, GRO_MAX_HEAD);\n\t\tif (skb) {\n\t\t\tnapi->skb = skb;\n\t\t\tskb_mark_napi_id(skb, napi);\n\t\t}\n\t}\n\treturn skb;\n}\nEXPORT_SYMBOL(napi_get_frags);\n\nstatic gro_result_t napi_frags_finish(struct napi_struct *napi,\n\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t      gro_result_t ret)\n{\n\tswitch (ret) {\n\tcase GRO_NORMAL:\n\tcase GRO_HELD:\n\t\t__skb_push(skb, ETH_HLEN);\n\t\tskb->protocol = eth_type_trans(skb, skb->dev);\n\t\tif (ret == GRO_NORMAL)\n\t\t\tgro_normal_one(napi, skb);\n\t\tbreak;\n\n\tcase GRO_MERGED_FREE:\n\t\tif (NAPI_GRO_CB(skb)->free == NAPI_GRO_FREE_STOLEN_HEAD)\n\t\t\tnapi_skb_free_stolen_head(skb);\n\t\telse\n\t\t\tnapi_reuse_skb(napi, skb);\n\t\tbreak;\n\n\tcase GRO_MERGED:\n\tcase GRO_CONSUMED:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\n/* Upper GRO stack assumes network header starts at gro_offset=0\n * Drivers could call both napi_gro_frags() and napi_gro_receive()\n * We copy ethernet header into skb->data to have a common layout.\n */\nstatic struct sk_buff *napi_frags_skb(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi->skb;\n\tconst struct ethhdr *eth;\n\tunsigned int hlen = sizeof(*eth);\n\n\tnapi->skb = NULL;\n\n\tskb_reset_mac_header(skb);\n\tskb_gro_reset_offset(skb);\n\n\tif (unlikely(skb_gro_header_hard(skb, hlen))) {\n\t\teth = skb_gro_header_slow(skb, hlen, 0);\n\t\tif (unlikely(!eth)) {\n\t\t\tnet_warn_ratelimited(\"%s: dropping impossible skb from %s\\n\",\n\t\t\t\t\t     __func__, napi->dev->name);\n\t\t\tnapi_reuse_skb(napi, skb);\n\t\t\treturn NULL;\n\t\t}\n\t} else {\n\t\teth = (const struct ethhdr *)skb->data;\n\t\tgro_pull_from_frag0(skb, hlen);\n\t\tNAPI_GRO_CB(skb)->frag0 += hlen;\n\t\tNAPI_GRO_CB(skb)->frag0_len -= hlen;\n\t}\n\t__skb_pull(skb, hlen);\n\n\t/*\n\t * This works because the only protocols we care about don't require\n\t * special handling.\n\t * We'll fix it up properly in napi_frags_finish()\n\t */\n\tskb->protocol = eth->h_proto;\n\n\treturn skb;\n}\n\ngro_result_t napi_gro_frags(struct napi_struct *napi)\n{\n\tgro_result_t ret;\n\tstruct sk_buff *skb = napi_frags_skb(napi);\n\n\ttrace_napi_gro_frags_entry(skb);\n\n\tret = napi_frags_finish(napi, skb, dev_gro_receive(napi, skb));\n\ttrace_napi_gro_frags_exit(ret);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(napi_gro_frags);\n\n/* Compute the checksum from gro_offset and return the folded value\n * after adding in any pseudo checksum.\n */\n__sum16 __skb_gro_checksum_complete(struct sk_buff *skb)\n{\n\t__wsum wsum;\n\t__sum16 sum;\n\n\twsum = skb_checksum(skb, skb_gro_offset(skb), skb_gro_len(skb), 0);\n\n\t/* NAPI_GRO_CB(skb)->csum holds pseudo checksum */\n\tsum = csum_fold(csum_add(NAPI_GRO_CB(skb)->csum, wsum));\n\t/* See comments in __skb_checksum_complete(). */\n\tif (likely(!sum)) {\n\t\tif (unlikely(skb->ip_summed == CHECKSUM_COMPLETE) &&\n\t\t    !skb->csum_complete_sw)\n\t\t\tnetdev_rx_csum_fault(skb->dev, skb);\n\t}\n\n\tNAPI_GRO_CB(skb)->csum = wsum;\n\tNAPI_GRO_CB(skb)->csum_valid = 1;\n\n\treturn sum;\n}\nEXPORT_SYMBOL(__skb_gro_checksum_complete);\n\nstatic void net_rps_send_ipi(struct softnet_data *remsd)\n{\n#ifdef CONFIG_RPS\n\twhile (remsd) {\n\t\tstruct softnet_data *next = remsd->rps_ipi_next;\n\n\t\tif (cpu_online(remsd->cpu))\n\t\t\tsmp_call_function_single_async(remsd->cpu, &remsd->csd);\n\t\tremsd = next;\n\t}\n#endif\n}\n\n/*\n * net_rps_action_and_irq_enable sends any pending IPI's for rps.\n * Note: called with local irq disabled, but exits with local irq enabled.\n */\nstatic void net_rps_action_and_irq_enable(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tstruct softnet_data *remsd = sd->rps_ipi_list;\n\n\tif (remsd) {\n\t\tsd->rps_ipi_list = NULL;\n\n\t\tlocal_irq_enable();\n\n\t\t/* Send pending IPI's to kick RPS processing on remote cpus. */\n\t\tnet_rps_send_ipi(remsd);\n\t} else\n#endif\n\t\tlocal_irq_enable();\n}\n\nstatic bool sd_has_rps_ipi_waiting(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\treturn sd->rps_ipi_list != NULL;\n#else\n\treturn false;\n#endif\n}\n\nstatic int process_backlog(struct napi_struct *napi, int quota)\n{\n\tstruct softnet_data *sd = container_of(napi, struct softnet_data, backlog);\n\tbool again = true;\n\tint work = 0;\n\n\t/* Check if we have pending ipi, its better to send them now,\n\t * not waiting net_rx_action() end.\n\t */\n\tif (sd_has_rps_ipi_waiting(sd)) {\n\t\tlocal_irq_disable();\n\t\tnet_rps_action_and_irq_enable(sd);\n\t}\n\n\tnapi->weight = dev_rx_weight;\n\twhile (again) {\n\t\tstruct sk_buff *skb;\n\n\t\twhile ((skb = __skb_dequeue(&sd->process_queue))) {\n\t\t\trcu_read_lock();\n\t\t\t__netif_receive_skb(skb);\n\t\t\trcu_read_unlock();\n\t\t\tinput_queue_head_incr(sd);\n\t\t\tif (++work >= quota)\n\t\t\t\treturn work;\n\n\t\t}\n\n\t\tlocal_irq_disable();\n\t\trps_lock(sd);\n\t\tif (skb_queue_empty(&sd->input_pkt_queue)) {\n\t\t\t/*\n\t\t\t * Inline a custom version of __napi_complete().\n\t\t\t * only current cpu owns and manipulates this napi,\n\t\t\t * and NAPI_STATE_SCHED is the only possible flag set\n\t\t\t * on backlog.\n\t\t\t * We can use a plain write instead of clear_bit(),\n\t\t\t * and we dont need an smp_mb() memory barrier.\n\t\t\t */\n\t\t\tnapi->state = 0;\n\t\t\tagain = false;\n\t\t} else {\n\t\t\tskb_queue_splice_tail_init(&sd->input_pkt_queue,\n\t\t\t\t\t\t   &sd->process_queue);\n\t\t}\n\t\trps_unlock(sd);\n\t\tlocal_irq_enable();\n\t}\n\n\treturn work;\n}\n\n/**\n * __napi_schedule - schedule for receive\n * @n: entry to schedule\n *\n * The entry's receive function will be scheduled to run.\n * Consider using __napi_schedule_irqoff() if hard irqs are masked.\n */\nvoid __napi_schedule(struct napi_struct *n)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t____napi_schedule(this_cpu_ptr(&softnet_data), n);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(__napi_schedule);\n\n/**\n *\tnapi_schedule_prep - check if napi can be scheduled\n *\t@n: napi context\n *\n * Test if NAPI routine is already running, and if not mark\n * it as running.  This is used as a condition variable to\n * insure only one NAPI poll instance runs.  We also make\n * sure there is no pending NAPI disable.\n */\nbool napi_schedule_prep(struct napi_struct *n)\n{\n\tunsigned long val, new;\n\n\tdo {\n\t\tval = READ_ONCE(n->state);\n\t\tif (unlikely(val & NAPIF_STATE_DISABLE))\n\t\t\treturn false;\n\t\tnew = val | NAPIF_STATE_SCHED;\n\n\t\t/* Sets STATE_MISSED bit if STATE_SCHED was already set\n\t\t * This was suggested by Alexander Duyck, as compiler\n\t\t * emits better code than :\n\t\t * if (val & NAPIF_STATE_SCHED)\n\t\t *     new |= NAPIF_STATE_MISSED;\n\t\t */\n\t\tnew |= (val & NAPIF_STATE_SCHED) / NAPIF_STATE_SCHED *\n\t\t\t\t\t\t   NAPIF_STATE_MISSED;\n\t} while (cmpxchg(&n->state, val, new) != val);\n\n\treturn !(val & NAPIF_STATE_SCHED);\n}\nEXPORT_SYMBOL(napi_schedule_prep);\n\n/**\n * __napi_schedule_irqoff - schedule for receive\n * @n: entry to schedule\n *\n * Variant of __napi_schedule() assuming hard irqs are masked\n */\nvoid __napi_schedule_irqoff(struct napi_struct *n)\n{\n\t____napi_schedule(this_cpu_ptr(&softnet_data), n);\n}\nEXPORT_SYMBOL(__napi_schedule_irqoff);\n\nbool napi_complete_done(struct napi_struct *n, int work_done)\n{\n\tunsigned long flags, val, new, timeout = 0;\n\tbool ret = true;\n\n\t/*\n\t * 1) Don't let napi dequeue from the cpu poll list\n\t *    just in case its running on a different cpu.\n\t * 2) If we are busy polling, do nothing here, we have\n\t *    the guarantee we will be called later.\n\t */\n\tif (unlikely(n->state & (NAPIF_STATE_NPSVC |\n\t\t\t\t NAPIF_STATE_IN_BUSY_POLL)))\n\t\treturn false;\n\n\tif (work_done) {\n\t\tif (n->gro_bitmask)\n\t\t\ttimeout = READ_ONCE(n->dev->gro_flush_timeout);\n\t\tn->defer_hard_irqs_count = READ_ONCE(n->dev->napi_defer_hard_irqs);\n\t}\n\tif (n->defer_hard_irqs_count > 0) {\n\t\tn->defer_hard_irqs_count--;\n\t\ttimeout = READ_ONCE(n->dev->gro_flush_timeout);\n\t\tif (timeout)\n\t\t\tret = false;\n\t}\n\tif (n->gro_bitmask) {\n\t\t/* When the NAPI instance uses a timeout and keeps postponing\n\t\t * it, we need to bound somehow the time packets are kept in\n\t\t * the GRO layer\n\t\t */\n\t\tnapi_gro_flush(n, !!timeout);\n\t}\n\n\tgro_normal_list(n);\n\n\tif (unlikely(!list_empty(&n->poll_list))) {\n\t\t/* If n->poll_list is not empty, we need to mask irqs */\n\t\tlocal_irq_save(flags);\n\t\tlist_del_init(&n->poll_list);\n\t\tlocal_irq_restore(flags);\n\t}\n\n\tdo {\n\t\tval = READ_ONCE(n->state);\n\n\t\tWARN_ON_ONCE(!(val & NAPIF_STATE_SCHED));\n\n\t\tnew = val & ~(NAPIF_STATE_MISSED | NAPIF_STATE_SCHED |\n\t\t\t      NAPIF_STATE_PREFER_BUSY_POLL);\n\n\t\t/* If STATE_MISSED was set, leave STATE_SCHED set,\n\t\t * because we will call napi->poll() one more time.\n\t\t * This C code was suggested by Alexander Duyck to help gcc.\n\t\t */\n\t\tnew |= (val & NAPIF_STATE_MISSED) / NAPIF_STATE_MISSED *\n\t\t\t\t\t\t    NAPIF_STATE_SCHED;\n\t} while (cmpxchg(&n->state, val, new) != val);\n\n\tif (unlikely(val & NAPIF_STATE_MISSED)) {\n\t\t__napi_schedule(n);\n\t\treturn false;\n\t}\n\n\tif (timeout)\n\t\thrtimer_start(&n->timer, ns_to_ktime(timeout),\n\t\t\t      HRTIMER_MODE_REL_PINNED);\n\treturn ret;\n}\nEXPORT_SYMBOL(napi_complete_done);\n\n/* must be called under rcu_read_lock(), as we dont take a reference */\nstatic struct napi_struct *napi_by_id(unsigned int napi_id)\n{\n\tunsigned int hash = napi_id % HASH_SIZE(napi_hash);\n\tstruct napi_struct *napi;\n\n\thlist_for_each_entry_rcu(napi, &napi_hash[hash], napi_hash_node)\n\t\tif (napi->napi_id == napi_id)\n\t\t\treturn napi;\n\n\treturn NULL;\n}\n\n#if defined(CONFIG_NET_RX_BUSY_POLL)\n\nstatic void __busy_poll_stop(struct napi_struct *napi, bool skip_schedule)\n{\n\tif (!skip_schedule) {\n\t\tgro_normal_list(napi);\n\t\t__napi_schedule(napi);\n\t\treturn;\n\t}\n\n\tif (napi->gro_bitmask) {\n\t\t/* flush too old packets\n\t\t * If HZ < 1000, flush all packets.\n\t\t */\n\t\tnapi_gro_flush(napi, HZ >= 1000);\n\t}\n\n\tgro_normal_list(napi);\n\tclear_bit(NAPI_STATE_SCHED, &napi->state);\n}\n\nstatic void busy_poll_stop(struct napi_struct *napi, void *have_poll_lock, bool prefer_busy_poll,\n\t\t\t   u16 budget)\n{\n\tbool skip_schedule = false;\n\tunsigned long timeout;\n\tint rc;\n\n\t/* Busy polling means there is a high chance device driver hard irq\n\t * could not grab NAPI_STATE_SCHED, and that NAPI_STATE_MISSED was\n\t * set in napi_schedule_prep().\n\t * Since we are about to call napi->poll() once more, we can safely\n\t * clear NAPI_STATE_MISSED.\n\t *\n\t * Note: x86 could use a single \"lock and ...\" instruction\n\t * to perform these two clear_bit()\n\t */\n\tclear_bit(NAPI_STATE_MISSED, &napi->state);\n\tclear_bit(NAPI_STATE_IN_BUSY_POLL, &napi->state);\n\n\tlocal_bh_disable();\n\n\tif (prefer_busy_poll) {\n\t\tnapi->defer_hard_irqs_count = READ_ONCE(napi->dev->napi_defer_hard_irqs);\n\t\ttimeout = READ_ONCE(napi->dev->gro_flush_timeout);\n\t\tif (napi->defer_hard_irqs_count && timeout) {\n\t\t\thrtimer_start(&napi->timer, ns_to_ktime(timeout), HRTIMER_MODE_REL_PINNED);\n\t\t\tskip_schedule = true;\n\t\t}\n\t}\n\n\t/* All we really want here is to re-enable device interrupts.\n\t * Ideally, a new ndo_busy_poll_stop() could avoid another round.\n\t */\n\trc = napi->poll(napi, budget);\n\t/* We can't gro_normal_list() here, because napi->poll() might have\n\t * rearmed the napi (napi_complete_done()) in which case it could\n\t * already be running on another CPU.\n\t */\n\ttrace_napi_poll(napi, rc, budget);\n\tnetpoll_poll_unlock(have_poll_lock);\n\tif (rc == budget)\n\t\t__busy_poll_stop(napi, skip_schedule);\n\tlocal_bh_enable();\n}\n\nvoid napi_busy_loop(unsigned int napi_id,\n\t\t    bool (*loop_end)(void *, unsigned long),\n\t\t    void *loop_end_arg, bool prefer_busy_poll, u16 budget)\n{\n\tunsigned long start_time = loop_end ? busy_loop_current_time() : 0;\n\tint (*napi_poll)(struct napi_struct *napi, int budget);\n\tvoid *have_poll_lock = NULL;\n\tstruct napi_struct *napi;\n\nrestart:\n\tnapi_poll = NULL;\n\n\trcu_read_lock();\n\n\tnapi = napi_by_id(napi_id);\n\tif (!napi)\n\t\tgoto out;\n\n\tpreempt_disable();\n\tfor (;;) {\n\t\tint work = 0;\n\n\t\tlocal_bh_disable();\n\t\tif (!napi_poll) {\n\t\t\tunsigned long val = READ_ONCE(napi->state);\n\n\t\t\t/* If multiple threads are competing for this napi,\n\t\t\t * we avoid dirtying napi->state as much as we can.\n\t\t\t */\n\t\t\tif (val & (NAPIF_STATE_DISABLE | NAPIF_STATE_SCHED |\n\t\t\t\t   NAPIF_STATE_IN_BUSY_POLL)) {\n\t\t\t\tif (prefer_busy_poll)\n\t\t\t\t\tset_bit(NAPI_STATE_PREFER_BUSY_POLL, &napi->state);\n\t\t\t\tgoto count;\n\t\t\t}\n\t\t\tif (cmpxchg(&napi->state, val,\n\t\t\t\t    val | NAPIF_STATE_IN_BUSY_POLL |\n\t\t\t\t\t  NAPIF_STATE_SCHED) != val) {\n\t\t\t\tif (prefer_busy_poll)\n\t\t\t\t\tset_bit(NAPI_STATE_PREFER_BUSY_POLL, &napi->state);\n\t\t\t\tgoto count;\n\t\t\t}\n\t\t\thave_poll_lock = netpoll_poll_lock(napi);\n\t\t\tnapi_poll = napi->poll;\n\t\t}\n\t\twork = napi_poll(napi, budget);\n\t\ttrace_napi_poll(napi, work, budget);\n\t\tgro_normal_list(napi);\ncount:\n\t\tif (work > 0)\n\t\t\t__NET_ADD_STATS(dev_net(napi->dev),\n\t\t\t\t\tLINUX_MIB_BUSYPOLLRXPACKETS, work);\n\t\tlocal_bh_enable();\n\n\t\tif (!loop_end || loop_end(loop_end_arg, start_time))\n\t\t\tbreak;\n\n\t\tif (unlikely(need_resched())) {\n\t\t\tif (napi_poll)\n\t\t\t\tbusy_poll_stop(napi, have_poll_lock, prefer_busy_poll, budget);\n\t\t\tpreempt_enable();\n\t\t\trcu_read_unlock();\n\t\t\tcond_resched();\n\t\t\tif (loop_end(loop_end_arg, start_time))\n\t\t\t\treturn;\n\t\t\tgoto restart;\n\t\t}\n\t\tcpu_relax();\n\t}\n\tif (napi_poll)\n\t\tbusy_poll_stop(napi, have_poll_lock, prefer_busy_poll, budget);\n\tpreempt_enable();\nout:\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(napi_busy_loop);\n\n#endif /* CONFIG_NET_RX_BUSY_POLL */\n\nstatic void napi_hash_add(struct napi_struct *napi)\n{\n\tif (test_bit(NAPI_STATE_NO_BUSY_POLL, &napi->state))\n\t\treturn;\n\n\tspin_lock(&napi_hash_lock);\n\n\t/* 0..NR_CPUS range is reserved for sender_cpu use */\n\tdo {\n\t\tif (unlikely(++napi_gen_id < MIN_NAPI_ID))\n\t\t\tnapi_gen_id = MIN_NAPI_ID;\n\t} while (napi_by_id(napi_gen_id));\n\tnapi->napi_id = napi_gen_id;\n\n\thlist_add_head_rcu(&napi->napi_hash_node,\n\t\t\t   &napi_hash[napi->napi_id % HASH_SIZE(napi_hash)]);\n\n\tspin_unlock(&napi_hash_lock);\n}\n\n/* Warning : caller is responsible to make sure rcu grace period\n * is respected before freeing memory containing @napi\n */\nstatic void napi_hash_del(struct napi_struct *napi)\n{\n\tspin_lock(&napi_hash_lock);\n\n\thlist_del_init_rcu(&napi->napi_hash_node);\n\n\tspin_unlock(&napi_hash_lock);\n}\n\nstatic enum hrtimer_restart napi_watchdog(struct hrtimer *timer)\n{\n\tstruct napi_struct *napi;\n\n\tnapi = container_of(timer, struct napi_struct, timer);\n\n\t/* Note : we use a relaxed variant of napi_schedule_prep() not setting\n\t * NAPI_STATE_MISSED, since we do not react to a device IRQ.\n\t */\n\tif (!napi_disable_pending(napi) &&\n\t    !test_and_set_bit(NAPI_STATE_SCHED, &napi->state)) {\n\t\tclear_bit(NAPI_STATE_PREFER_BUSY_POLL, &napi->state);\n\t\t__napi_schedule_irqoff(napi);\n\t}\n\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void init_gro_hash(struct napi_struct *napi)\n{\n\tint i;\n\n\tfor (i = 0; i < GRO_HASH_BUCKETS; i++) {\n\t\tINIT_LIST_HEAD(&napi->gro_hash[i].list);\n\t\tnapi->gro_hash[i].count = 0;\n\t}\n\tnapi->gro_bitmask = 0;\n}\n\nvoid netif_napi_add(struct net_device *dev, struct napi_struct *napi,\n\t\t    int (*poll)(struct napi_struct *, int), int weight)\n{\n\tif (WARN_ON(test_and_set_bit(NAPI_STATE_LISTED, &napi->state)))\n\t\treturn;\n\n\tINIT_LIST_HEAD(&napi->poll_list);\n\tINIT_HLIST_NODE(&napi->napi_hash_node);\n\thrtimer_init(&napi->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_PINNED);\n\tnapi->timer.function = napi_watchdog;\n\tinit_gro_hash(napi);\n\tnapi->skb = NULL;\n\tINIT_LIST_HEAD(&napi->rx_list);\n\tnapi->rx_count = 0;\n\tnapi->poll = poll;\n\tif (weight > NAPI_POLL_WEIGHT)\n\t\tnetdev_err_once(dev, \"%s() called with weight %d\\n\", __func__,\n\t\t\t\tweight);\n\tnapi->weight = weight;\n\tnapi->dev = dev;\n#ifdef CONFIG_NETPOLL\n\tnapi->poll_owner = -1;\n#endif\n\tset_bit(NAPI_STATE_SCHED, &napi->state);\n\tset_bit(NAPI_STATE_NPSVC, &napi->state);\n\tlist_add_rcu(&napi->dev_list, &dev->napi_list);\n\tnapi_hash_add(napi);\n}\nEXPORT_SYMBOL(netif_napi_add);\n\nvoid napi_disable(struct napi_struct *n)\n{\n\tmight_sleep();\n\tset_bit(NAPI_STATE_DISABLE, &n->state);\n\n\twhile (test_and_set_bit(NAPI_STATE_SCHED, &n->state))\n\t\tmsleep(1);\n\twhile (test_and_set_bit(NAPI_STATE_NPSVC, &n->state))\n\t\tmsleep(1);\n\n\thrtimer_cancel(&n->timer);\n\n\tclear_bit(NAPI_STATE_PREFER_BUSY_POLL, &n->state);\n\tclear_bit(NAPI_STATE_DISABLE, &n->state);\n}\nEXPORT_SYMBOL(napi_disable);\n\nstatic void flush_gro_hash(struct napi_struct *napi)\n{\n\tint i;\n\n\tfor (i = 0; i < GRO_HASH_BUCKETS; i++) {\n\t\tstruct sk_buff *skb, *n;\n\n\t\tlist_for_each_entry_safe(skb, n, &napi->gro_hash[i].list, list)\n\t\t\tkfree_skb(skb);\n\t\tnapi->gro_hash[i].count = 0;\n\t}\n}\n\n/* Must be called in process context */\nvoid __netif_napi_del(struct napi_struct *napi)\n{\n\tif (!test_and_clear_bit(NAPI_STATE_LISTED, &napi->state))\n\t\treturn;\n\n\tnapi_hash_del(napi);\n\tlist_del_rcu(&napi->dev_list);\n\tnapi_free_frags(napi);\n\n\tflush_gro_hash(napi);\n\tnapi->gro_bitmask = 0;\n}\nEXPORT_SYMBOL(__netif_napi_del);\n\nstatic int napi_poll(struct napi_struct *n, struct list_head *repoll)\n{\n\tvoid *have;\n\tint work, weight;\n\n\tlist_del_init(&n->poll_list);\n\n\thave = netpoll_poll_lock(n);\n\n\tweight = n->weight;\n\n\t/* This NAPI_STATE_SCHED test is for avoiding a race\n\t * with netpoll's poll_napi().  Only the entity which\n\t * obtains the lock and sees NAPI_STATE_SCHED set will\n\t * actually make the ->poll() call.  Therefore we avoid\n\t * accidentally calling ->poll() when NAPI is not scheduled.\n\t */\n\twork = 0;\n\tif (test_bit(NAPI_STATE_SCHED, &n->state)) {\n\t\twork = n->poll(n, weight);\n\t\ttrace_napi_poll(n, work, weight);\n\t}\n\n\tif (unlikely(work > weight))\n\t\tpr_err_once(\"NAPI poll function %pS returned %d, exceeding its budget of %d.\\n\",\n\t\t\t    n->poll, work, weight);\n\n\tif (likely(work < weight))\n\t\tgoto out_unlock;\n\n\t/* Drivers must not modify the NAPI state if they\n\t * consume the entire weight.  In such cases this code\n\t * still \"owns\" the NAPI instance and therefore can\n\t * move the instance around on the list at-will.\n\t */\n\tif (unlikely(napi_disable_pending(n))) {\n\t\tnapi_complete(n);\n\t\tgoto out_unlock;\n\t}\n\n\t/* The NAPI context has more processing work, but busy-polling\n\t * is preferred. Exit early.\n\t */\n\tif (napi_prefer_busy_poll(n)) {\n\t\tif (napi_complete_done(n, work)) {\n\t\t\t/* If timeout is not set, we need to make sure\n\t\t\t * that the NAPI is re-scheduled.\n\t\t\t */\n\t\t\tnapi_schedule(n);\n\t\t}\n\t\tgoto out_unlock;\n\t}\n\n\tif (n->gro_bitmask) {\n\t\t/* flush too old packets\n\t\t * If HZ < 1000, flush all packets.\n\t\t */\n\t\tnapi_gro_flush(n, HZ >= 1000);\n\t}\n\n\tgro_normal_list(n);\n\n\t/* Some drivers may have called napi_schedule\n\t * prior to exhausting their budget.\n\t */\n\tif (unlikely(!list_empty(&n->poll_list))) {\n\t\tpr_warn_once(\"%s: Budget exhausted after napi rescheduled\\n\",\n\t\t\t     n->dev ? n->dev->name : \"backlog\");\n\t\tgoto out_unlock;\n\t}\n\n\tlist_add_tail(&n->poll_list, repoll);\n\nout_unlock:\n\tnetpoll_poll_unlock(have);\n\n\treturn work;\n}\n\nstatic __latent_entropy void net_rx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\tunsigned long time_limit = jiffies +\n\t\tusecs_to_jiffies(netdev_budget_usecs);\n\tint budget = netdev_budget;\n\tLIST_HEAD(list);\n\tLIST_HEAD(repoll);\n\n\tlocal_irq_disable();\n\tlist_splice_init(&sd->poll_list, &list);\n\tlocal_irq_enable();\n\n\tfor (;;) {\n\t\tstruct napi_struct *n;\n\n\t\tif (list_empty(&list)) {\n\t\t\tif (!sd_has_rps_ipi_waiting(sd) && list_empty(&repoll))\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\t}\n\n\t\tn = list_first_entry(&list, struct napi_struct, poll_list);\n\t\tbudget -= napi_poll(n, &repoll);\n\n\t\t/* If softirq window is exhausted then punt.\n\t\t * Allow this to run for 2 jiffies since which will allow\n\t\t * an average latency of 1.5/HZ.\n\t\t */\n\t\tif (unlikely(budget <= 0 ||\n\t\t\t     time_after_eq(jiffies, time_limit))) {\n\t\t\tsd->time_squeeze++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tlocal_irq_disable();\n\n\tlist_splice_tail_init(&sd->poll_list, &list);\n\tlist_splice_tail(&repoll, &list);\n\tlist_splice(&list, &sd->poll_list);\n\tif (!list_empty(&sd->poll_list))\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\n\tnet_rps_action_and_irq_enable(sd);\nout:\n\t__kfree_skb_flush();\n}\n\nstruct netdev_adjacent {\n\tstruct net_device *dev;\n\n\t/* upper master flag, there can only be one master device per list */\n\tbool master;\n\n\t/* lookup ignore flag */\n\tbool ignore;\n\n\t/* counter for the number of times this device was added to us */\n\tu16 ref_nr;\n\n\t/* private field for the users */\n\tvoid *private;\n\n\tstruct list_head list;\n\tstruct rcu_head rcu;\n};\n\nstatic struct netdev_adjacent *__netdev_find_adj(struct net_device *adj_dev,\n\t\t\t\t\t\t struct list_head *adj_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tlist_for_each_entry(adj, adj_list, list) {\n\t\tif (adj->dev == adj_dev)\n\t\t\treturn adj;\n\t}\n\treturn NULL;\n}\n\nstatic int ____netdev_has_upper_dev(struct net_device *upper_dev,\n\t\t\t\t    struct netdev_nested_priv *priv)\n{\n\tstruct net_device *dev = (struct net_device *)priv->data;\n\n\treturn upper_dev == dev;\n}\n\n/**\n * netdev_has_upper_dev - Check if device is linked to an upper device\n * @dev: device\n * @upper_dev: upper device to check\n *\n * Find out if a device is linked to specified upper device and return true\n * in case it is. Note that this checks only immediate upper device,\n * not through a complete stack of devices. The caller must hold the RTNL lock.\n */\nbool netdev_has_upper_dev(struct net_device *dev,\n\t\t\t  struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.data = (void *)upper_dev,\n\t};\n\n\tASSERT_RTNL();\n\n\treturn netdev_walk_all_upper_dev_rcu(dev, ____netdev_has_upper_dev,\n\t\t\t\t\t     &priv);\n}\nEXPORT_SYMBOL(netdev_has_upper_dev);\n\n/**\n * netdev_has_upper_dev_all_rcu - Check if device is linked to an upper device\n * @dev: device\n * @upper_dev: upper device to check\n *\n * Find out if a device is linked to specified upper device and return true\n * in case it is. Note that this checks the entire upper device chain.\n * The caller must hold rcu lock.\n */\n\nbool netdev_has_upper_dev_all_rcu(struct net_device *dev,\n\t\t\t\t  struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.data = (void *)upper_dev,\n\t};\n\n\treturn !!netdev_walk_all_upper_dev_rcu(dev, ____netdev_has_upper_dev,\n\t\t\t\t\t       &priv);\n}\nEXPORT_SYMBOL(netdev_has_upper_dev_all_rcu);\n\n/**\n * netdev_has_any_upper_dev - Check if device is linked to some device\n * @dev: device\n *\n * Find out if a device is linked to an upper device and return true in case\n * it is. The caller must hold the RTNL lock.\n */\nbool netdev_has_any_upper_dev(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\treturn !list_empty(&dev->adj_list.upper);\n}\nEXPORT_SYMBOL(netdev_has_any_upper_dev);\n\n/**\n * netdev_master_upper_dev_get - Get master upper device\n * @dev: device\n *\n * Find a master upper device and return pointer to it or NULL in case\n * it's not there. The caller must hold the RTNL lock.\n */\nstruct net_device *netdev_master_upper_dev_get(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tASSERT_RTNL();\n\n\tif (list_empty(&dev->adj_list.upper))\n\t\treturn NULL;\n\n\tupper = list_first_entry(&dev->adj_list.upper,\n\t\t\t\t struct netdev_adjacent, list);\n\tif (likely(upper->master))\n\t\treturn upper->dev;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_get);\n\nstatic struct net_device *__netdev_master_upper_dev_get(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tASSERT_RTNL();\n\n\tif (list_empty(&dev->adj_list.upper))\n\t\treturn NULL;\n\n\tupper = list_first_entry(&dev->adj_list.upper,\n\t\t\t\t struct netdev_adjacent, list);\n\tif (likely(upper->master) && !upper->ignore)\n\t\treturn upper->dev;\n\treturn NULL;\n}\n\n/**\n * netdev_has_any_lower_dev - Check if device is linked to some device\n * @dev: device\n *\n * Find out if a device is linked to a lower device and return true in case\n * it is. The caller must hold the RTNL lock.\n */\nstatic bool netdev_has_any_lower_dev(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\treturn !list_empty(&dev->adj_list.lower);\n}\n\nvoid *netdev_adjacent_get_private(struct list_head *adj_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tadj = list_entry(adj_list, struct netdev_adjacent, list);\n\n\treturn adj->private;\n}\nEXPORT_SYMBOL(netdev_adjacent_get_private);\n\n/**\n * netdev_upper_get_next_dev_rcu - Get the next dev from upper list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next device from the dev's upper list, starting from iter\n * position. The caller must hold RCU read lock.\n */\nstruct net_device *netdev_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t struct list_head **iter)\n{\n\tstruct netdev_adjacent *upper;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !lockdep_rtnl_is_held());\n\n\tupper = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\n\treturn upper->dev;\n}\nEXPORT_SYMBOL(netdev_upper_get_next_dev_rcu);\n\nstatic struct net_device *__netdev_next_upper_dev(struct net_device *dev,\n\t\t\t\t\t\t  struct list_head **iter,\n\t\t\t\t\t\t  bool *ignore)\n{\n\tstruct netdev_adjacent *upper;\n\n\tupper = list_entry((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\t*ignore = upper->ignore;\n\n\treturn upper->dev;\n}\n\nstatic struct net_device *netdev_next_upper_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t    struct list_head **iter)\n{\n\tstruct netdev_adjacent *upper;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !lockdep_rtnl_is_held());\n\n\tupper = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\n\treturn upper->dev;\n}\n\nstatic int __netdev_walk_all_upper_dev(struct net_device *dev,\n\t\t\t\t       int (*fn)(struct net_device *dev,\n\t\t\t\t\t struct netdev_nested_priv *priv),\n\t\t\t\t       struct netdev_nested_priv *priv)\n{\n\tstruct net_device *udev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\tbool ignore;\n\n\tnow = dev;\n\titer = &dev->adj_list.upper;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tudev = __netdev_next_upper_dev(now, &iter, &ignore);\n\t\t\tif (!udev)\n\t\t\t\tbreak;\n\t\t\tif (ignore)\n\t\t\t\tcontinue;\n\n\t\t\tnext = udev;\n\t\t\tniter = &udev->adj_list.upper;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\n\nint netdev_walk_all_upper_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *dev,\n\t\t\t\t\t    struct netdev_nested_priv *priv),\n\t\t\t\t  struct netdev_nested_priv *priv)\n{\n\tstruct net_device *udev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\n\tnow = dev;\n\titer = &dev->adj_list.upper;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tudev = netdev_next_upper_dev_rcu(now, &iter);\n\t\t\tif (!udev)\n\t\t\t\tbreak;\n\n\t\t\tnext = udev;\n\t\t\tniter = &udev->adj_list.upper;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_upper_dev_rcu);\n\nstatic bool __netdev_has_upper_dev(struct net_device *dev,\n\t\t\t\t   struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = 0,\n\t\t.data = (void *)upper_dev,\n\t};\n\n\tASSERT_RTNL();\n\n\treturn __netdev_walk_all_upper_dev(dev, ____netdev_has_upper_dev,\n\t\t\t\t\t   &priv);\n}\n\n/**\n * netdev_lower_get_next_private - Get the next ->private from the\n *\t\t\t\t   lower neighbour list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent->private from the dev's lower neighbour\n * list, starting from iter position. The caller must hold either hold the\n * RTNL lock or its own locking that guarantees that the neighbour lower\n * list will remain unchanged.\n */\nvoid *netdev_lower_get_next_private(struct net_device *dev,\n\t\t\t\t    struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry(*iter, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = lower->list.next;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_get_next_private);\n\n/**\n * netdev_lower_get_next_private_rcu - Get the next ->private from the\n *\t\t\t\t       lower neighbour list, RCU\n *\t\t\t\t       variant\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent->private from the dev's lower neighbour\n * list, starting from iter position. The caller must hold RCU read lock.\n */\nvoid *netdev_lower_get_next_private_rcu(struct net_device *dev,\n\t\t\t\t\tstruct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tlower = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_get_next_private_rcu);\n\n/**\n * netdev_lower_get_next - Get the next device from the lower neighbour\n *                         list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent from the dev's lower neighbour\n * list, starting from iter position. The caller must hold RTNL lock or\n * its own locking that guarantees that the neighbour lower\n * list will remain unchanged.\n */\nvoid *netdev_lower_get_next(struct net_device *dev, struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry(*iter, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = lower->list.next;\n\n\treturn lower->dev;\n}\nEXPORT_SYMBOL(netdev_lower_get_next);\n\nstatic struct net_device *netdev_next_lower_dev(struct net_device *dev,\n\t\t\t\t\t\tstruct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->dev;\n}\n\nstatic struct net_device *__netdev_next_lower_dev(struct net_device *dev,\n\t\t\t\t\t\t  struct list_head **iter,\n\t\t\t\t\t\t  bool *ignore)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\t*ignore = lower->ignore;\n\n\treturn lower->dev;\n}\n\nint netdev_walk_all_lower_dev(struct net_device *dev,\n\t\t\t      int (*fn)(struct net_device *dev,\n\t\t\t\t\tstruct netdev_nested_priv *priv),\n\t\t\t      struct netdev_nested_priv *priv)\n{\n\tstruct net_device *ldev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\n\tnow = dev;\n\titer = &dev->adj_list.lower;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tldev = netdev_next_lower_dev(now, &iter);\n\t\t\tif (!ldev)\n\t\t\t\tbreak;\n\n\t\t\tnext = ldev;\n\t\t\tniter = &ldev->adj_list.lower;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_lower_dev);\n\nstatic int __netdev_walk_all_lower_dev(struct net_device *dev,\n\t\t\t\t       int (*fn)(struct net_device *dev,\n\t\t\t\t\t struct netdev_nested_priv *priv),\n\t\t\t\t       struct netdev_nested_priv *priv)\n{\n\tstruct net_device *ldev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\tbool ignore;\n\n\tnow = dev;\n\titer = &dev->adj_list.lower;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tldev = __netdev_next_lower_dev(now, &iter, &ignore);\n\t\t\tif (!ldev)\n\t\t\t\tbreak;\n\t\t\tif (ignore)\n\t\t\t\tcontinue;\n\n\t\t\tnext = ldev;\n\t\t\tniter = &ldev->adj_list.lower;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\n\nstruct net_device *netdev_next_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t\t     struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->dev;\n}\nEXPORT_SYMBOL(netdev_next_lower_dev_rcu);\n\nstatic u8 __netdev_upper_depth(struct net_device *dev)\n{\n\tstruct net_device *udev;\n\tstruct list_head *iter;\n\tu8 max_depth = 0;\n\tbool ignore;\n\n\tfor (iter = &dev->adj_list.upper,\n\t     udev = __netdev_next_upper_dev(dev, &iter, &ignore);\n\t     udev;\n\t     udev = __netdev_next_upper_dev(dev, &iter, &ignore)) {\n\t\tif (ignore)\n\t\t\tcontinue;\n\t\tif (max_depth < udev->upper_level)\n\t\t\tmax_depth = udev->upper_level;\n\t}\n\n\treturn max_depth;\n}\n\nstatic u8 __netdev_lower_depth(struct net_device *dev)\n{\n\tstruct net_device *ldev;\n\tstruct list_head *iter;\n\tu8 max_depth = 0;\n\tbool ignore;\n\n\tfor (iter = &dev->adj_list.lower,\n\t     ldev = __netdev_next_lower_dev(dev, &iter, &ignore);\n\t     ldev;\n\t     ldev = __netdev_next_lower_dev(dev, &iter, &ignore)) {\n\t\tif (ignore)\n\t\t\tcontinue;\n\t\tif (max_depth < ldev->lower_level)\n\t\t\tmax_depth = ldev->lower_level;\n\t}\n\n\treturn max_depth;\n}\n\nstatic int __netdev_update_upper_level(struct net_device *dev,\n\t\t\t\t       struct netdev_nested_priv *__unused)\n{\n\tdev->upper_level = __netdev_upper_depth(dev) + 1;\n\treturn 0;\n}\n\nstatic int __netdev_update_lower_level(struct net_device *dev,\n\t\t\t\t       struct netdev_nested_priv *priv)\n{\n\tdev->lower_level = __netdev_lower_depth(dev) + 1;\n\n#ifdef CONFIG_LOCKDEP\n\tif (!priv)\n\t\treturn 0;\n\n\tif (priv->flags & NESTED_SYNC_IMM)\n\t\tdev->nested_level = dev->lower_level - 1;\n\tif (priv->flags & NESTED_SYNC_TODO)\n\t\tnet_unlink_todo(dev);\n#endif\n\treturn 0;\n}\n\nint netdev_walk_all_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *dev,\n\t\t\t\t\t    struct netdev_nested_priv *priv),\n\t\t\t\t  struct netdev_nested_priv *priv)\n{\n\tstruct net_device *ldev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\n\tnow = dev;\n\titer = &dev->adj_list.lower;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tldev = netdev_next_lower_dev_rcu(now, &iter);\n\t\t\tif (!ldev)\n\t\t\t\tbreak;\n\n\t\t\tnext = ldev;\n\t\t\tniter = &ldev->adj_list.lower;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_lower_dev_rcu);\n\n/**\n * netdev_lower_get_first_private_rcu - Get the first ->private from the\n *\t\t\t\t       lower neighbour list, RCU\n *\t\t\t\t       variant\n * @dev: device\n *\n * Gets the first netdev_adjacent->private from the dev's lower neighbour\n * list. The caller must hold RCU read lock.\n */\nvoid *netdev_lower_get_first_private_rcu(struct net_device *dev)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_first_or_null_rcu(&dev->adj_list.lower,\n\t\t\tstruct netdev_adjacent, list);\n\tif (lower)\n\t\treturn lower->private;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_lower_get_first_private_rcu);\n\n/**\n * netdev_master_upper_dev_get_rcu - Get master upper device\n * @dev: device\n *\n * Find a master upper device and return pointer to it or NULL in case\n * it's not there. The caller must hold the RCU read lock.\n */\nstruct net_device *netdev_master_upper_dev_get_rcu(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tupper = list_first_or_null_rcu(&dev->adj_list.upper,\n\t\t\t\t       struct netdev_adjacent, list);\n\tif (upper && likely(upper->master))\n\t\treturn upper->dev;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_get_rcu);\n\nstatic int netdev_adjacent_sysfs_add(struct net_device *dev,\n\t\t\t      struct net_device *adj_dev,\n\t\t\t      struct list_head *dev_list)\n{\n\tchar linkname[IFNAMSIZ+7];\n\n\tsprintf(linkname, dev_list == &dev->adj_list.upper ?\n\t\t\"upper_%s\" : \"lower_%s\", adj_dev->name);\n\treturn sysfs_create_link(&(dev->dev.kobj), &(adj_dev->dev.kobj),\n\t\t\t\t linkname);\n}\nstatic void netdev_adjacent_sysfs_del(struct net_device *dev,\n\t\t\t       char *name,\n\t\t\t       struct list_head *dev_list)\n{\n\tchar linkname[IFNAMSIZ+7];\n\n\tsprintf(linkname, dev_list == &dev->adj_list.upper ?\n\t\t\"upper_%s\" : \"lower_%s\", name);\n\tsysfs_remove_link(&(dev->dev.kobj), linkname);\n}\n\nstatic inline bool netdev_adjacent_is_neigh_list(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *adj_dev,\n\t\t\t\t\t\t struct list_head *dev_list)\n{\n\treturn (dev_list == &dev->adj_list.upper ||\n\t\tdev_list == &dev->adj_list.lower) &&\n\t\tnet_eq(dev_net(dev), dev_net(adj_dev));\n}\n\nstatic int __netdev_adjacent_dev_insert(struct net_device *dev,\n\t\t\t\t\tstruct net_device *adj_dev,\n\t\t\t\t\tstruct list_head *dev_list,\n\t\t\t\t\tvoid *private, bool master)\n{\n\tstruct netdev_adjacent *adj;\n\tint ret;\n\n\tadj = __netdev_find_adj(adj_dev, dev_list);\n\n\tif (adj) {\n\t\tadj->ref_nr += 1;\n\t\tpr_debug(\"Insert adjacency: dev %s adj_dev %s adj->ref_nr %d\\n\",\n\t\t\t dev->name, adj_dev->name, adj->ref_nr);\n\n\t\treturn 0;\n\t}\n\n\tadj = kmalloc(sizeof(*adj), GFP_KERNEL);\n\tif (!adj)\n\t\treturn -ENOMEM;\n\n\tadj->dev = adj_dev;\n\tadj->master = master;\n\tadj->ref_nr = 1;\n\tadj->private = private;\n\tadj->ignore = false;\n\tdev_hold(adj_dev);\n\n\tpr_debug(\"Insert adjacency: dev %s adj_dev %s adj->ref_nr %d; dev_hold on %s\\n\",\n\t\t dev->name, adj_dev->name, adj->ref_nr, adj_dev->name);\n\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list)) {\n\t\tret = netdev_adjacent_sysfs_add(dev, adj_dev, dev_list);\n\t\tif (ret)\n\t\t\tgoto free_adj;\n\t}\n\n\t/* Ensure that master link is always the first item in list. */\n\tif (master) {\n\t\tret = sysfs_create_link(&(dev->dev.kobj),\n\t\t\t\t\t&(adj_dev->dev.kobj), \"master\");\n\t\tif (ret)\n\t\t\tgoto remove_symlinks;\n\n\t\tlist_add_rcu(&adj->list, dev_list);\n\t} else {\n\t\tlist_add_tail_rcu(&adj->list, dev_list);\n\t}\n\n\treturn 0;\n\nremove_symlinks:\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list))\n\t\tnetdev_adjacent_sysfs_del(dev, adj_dev->name, dev_list);\nfree_adj:\n\tkfree(adj);\n\tdev_put(adj_dev);\n\n\treturn ret;\n}\n\nstatic void __netdev_adjacent_dev_remove(struct net_device *dev,\n\t\t\t\t\t struct net_device *adj_dev,\n\t\t\t\t\t u16 ref_nr,\n\t\t\t\t\t struct list_head *dev_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tpr_debug(\"Remove adjacency: dev %s adj_dev %s ref_nr %d\\n\",\n\t\t dev->name, adj_dev->name, ref_nr);\n\n\tadj = __netdev_find_adj(adj_dev, dev_list);\n\n\tif (!adj) {\n\t\tpr_err(\"Adjacency does not exist for device %s from %s\\n\",\n\t\t       dev->name, adj_dev->name);\n\t\tWARN_ON(1);\n\t\treturn;\n\t}\n\n\tif (adj->ref_nr > ref_nr) {\n\t\tpr_debug(\"adjacency: %s to %s ref_nr - %d = %d\\n\",\n\t\t\t dev->name, adj_dev->name, ref_nr,\n\t\t\t adj->ref_nr - ref_nr);\n\t\tadj->ref_nr -= ref_nr;\n\t\treturn;\n\t}\n\n\tif (adj->master)\n\t\tsysfs_remove_link(&(dev->dev.kobj), \"master\");\n\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list))\n\t\tnetdev_adjacent_sysfs_del(dev, adj_dev->name, dev_list);\n\n\tlist_del_rcu(&adj->list);\n\tpr_debug(\"adjacency: dev_put for %s, because link removed from %s to %s\\n\",\n\t\t adj_dev->name, dev->name, adj_dev->name);\n\tdev_put(adj_dev);\n\tkfree_rcu(adj, rcu);\n}\n\nstatic int __netdev_adjacent_dev_link_lists(struct net_device *dev,\n\t\t\t\t\t    struct net_device *upper_dev,\n\t\t\t\t\t    struct list_head *up_list,\n\t\t\t\t\t    struct list_head *down_list,\n\t\t\t\t\t    void *private, bool master)\n{\n\tint ret;\n\n\tret = __netdev_adjacent_dev_insert(dev, upper_dev, up_list,\n\t\t\t\t\t   private, master);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_insert(upper_dev, dev, down_list,\n\t\t\t\t\t   private, false);\n\tif (ret) {\n\t\t__netdev_adjacent_dev_remove(dev, upper_dev, 1, up_list);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void __netdev_adjacent_dev_unlink_lists(struct net_device *dev,\n\t\t\t\t\t       struct net_device *upper_dev,\n\t\t\t\t\t       u16 ref_nr,\n\t\t\t\t\t       struct list_head *up_list,\n\t\t\t\t\t       struct list_head *down_list)\n{\n\t__netdev_adjacent_dev_remove(dev, upper_dev, ref_nr, up_list);\n\t__netdev_adjacent_dev_remove(upper_dev, dev, ref_nr, down_list);\n}\n\nstatic int __netdev_adjacent_dev_link_neighbour(struct net_device *dev,\n\t\t\t\t\t\tstruct net_device *upper_dev,\n\t\t\t\t\t\tvoid *private, bool master)\n{\n\treturn __netdev_adjacent_dev_link_lists(dev, upper_dev,\n\t\t\t\t\t\t&dev->adj_list.upper,\n\t\t\t\t\t\t&upper_dev->adj_list.lower,\n\t\t\t\t\t\tprivate, master);\n}\n\nstatic void __netdev_adjacent_dev_unlink_neighbour(struct net_device *dev,\n\t\t\t\t\t\t   struct net_device *upper_dev)\n{\n\t__netdev_adjacent_dev_unlink_lists(dev, upper_dev, 1,\n\t\t\t\t\t   &dev->adj_list.upper,\n\t\t\t\t\t   &upper_dev->adj_list.lower);\n}\n\nstatic int __netdev_upper_dev_link(struct net_device *dev,\n\t\t\t\t   struct net_device *upper_dev, bool master,\n\t\t\t\t   void *upper_priv, void *upper_info,\n\t\t\t\t   struct netdev_nested_priv *priv,\n\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_changeupper_info changeupper_info = {\n\t\t.info = {\n\t\t\t.dev = dev,\n\t\t\t.extack = extack,\n\t\t},\n\t\t.upper_dev = upper_dev,\n\t\t.master = master,\n\t\t.linking = true,\n\t\t.upper_info = upper_info,\n\t};\n\tstruct net_device *master_dev;\n\tint ret = 0;\n\n\tASSERT_RTNL();\n\n\tif (dev == upper_dev)\n\t\treturn -EBUSY;\n\n\t/* To prevent loops, check if dev is not upper device to upper_dev. */\n\tif (__netdev_has_upper_dev(upper_dev, dev))\n\t\treturn -EBUSY;\n\n\tif ((dev->lower_level + upper_dev->upper_level) > MAX_NEST_DEV)\n\t\treturn -EMLINK;\n\n\tif (!master) {\n\t\tif (__netdev_has_upper_dev(dev, upper_dev))\n\t\t\treturn -EEXIST;\n\t} else {\n\t\tmaster_dev = __netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\treturn master_dev == upper_dev ? -EEXIST : -EBUSY;\n\t}\n\n\tret = call_netdevice_notifiers_info(NETDEV_PRECHANGEUPPER,\n\t\t\t\t\t    &changeupper_info.info);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_link_neighbour(dev, upper_dev, upper_priv,\n\t\t\t\t\t\t   master);\n\tif (ret)\n\t\treturn ret;\n\n\tret = call_netdevice_notifiers_info(NETDEV_CHANGEUPPER,\n\t\t\t\t\t    &changeupper_info.info);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto rollback;\n\n\t__netdev_update_upper_level(dev, NULL);\n\t__netdev_walk_all_lower_dev(dev, __netdev_update_upper_level, NULL);\n\n\t__netdev_update_lower_level(upper_dev, priv);\n\t__netdev_walk_all_upper_dev(upper_dev, __netdev_update_lower_level,\n\t\t\t\t    priv);\n\n\treturn 0;\n\nrollback:\n\t__netdev_adjacent_dev_unlink_neighbour(dev, upper_dev);\n\n\treturn ret;\n}\n\n/**\n * netdev_upper_dev_link - Add a link to the upper device\n * @dev: device\n * @upper_dev: new upper device\n * @extack: netlink extended ack\n *\n * Adds a link to device which is upper to this one. The caller must hold\n * the RTNL lock. On a failure a negative errno code is returned.\n * On success the reference counts are adjusted and the function\n * returns zero.\n */\nint netdev_upper_dev_link(struct net_device *dev,\n\t\t\t  struct net_device *upper_dev,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_IMM | NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\treturn __netdev_upper_dev_link(dev, upper_dev, false,\n\t\t\t\t       NULL, NULL, &priv, extack);\n}\nEXPORT_SYMBOL(netdev_upper_dev_link);\n\n/**\n * netdev_master_upper_dev_link - Add a master link to the upper device\n * @dev: device\n * @upper_dev: new upper device\n * @upper_priv: upper device private\n * @upper_info: upper info to be passed down via notifier\n * @extack: netlink extended ack\n *\n * Adds a link to device which is upper to this one. In this case, only\n * one master upper device can be linked, although other non-master devices\n * might be linked as well. The caller must hold the RTNL lock.\n * On a failure a negative errno code is returned. On success the reference\n * counts are adjusted and the function returns zero.\n */\nint netdev_master_upper_dev_link(struct net_device *dev,\n\t\t\t\t struct net_device *upper_dev,\n\t\t\t\t void *upper_priv, void *upper_info,\n\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_IMM | NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\treturn __netdev_upper_dev_link(dev, upper_dev, true,\n\t\t\t\t       upper_priv, upper_info, &priv, extack);\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_link);\n\nstatic void __netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t\t      struct net_device *upper_dev,\n\t\t\t\t      struct netdev_nested_priv *priv)\n{\n\tstruct netdev_notifier_changeupper_info changeupper_info = {\n\t\t.info = {\n\t\t\t.dev = dev,\n\t\t},\n\t\t.upper_dev = upper_dev,\n\t\t.linking = false,\n\t};\n\n\tASSERT_RTNL();\n\n\tchangeupper_info.master = netdev_master_upper_dev_get(dev) == upper_dev;\n\n\tcall_netdevice_notifiers_info(NETDEV_PRECHANGEUPPER,\n\t\t\t\t      &changeupper_info.info);\n\n\t__netdev_adjacent_dev_unlink_neighbour(dev, upper_dev);\n\n\tcall_netdevice_notifiers_info(NETDEV_CHANGEUPPER,\n\t\t\t\t      &changeupper_info.info);\n\n\t__netdev_update_upper_level(dev, NULL);\n\t__netdev_walk_all_lower_dev(dev, __netdev_update_upper_level, NULL);\n\n\t__netdev_update_lower_level(upper_dev, priv);\n\t__netdev_walk_all_upper_dev(upper_dev, __netdev_update_lower_level,\n\t\t\t\t    priv);\n}\n\n/**\n * netdev_upper_dev_unlink - Removes a link to upper device\n * @dev: device\n * @upper_dev: new upper device\n *\n * Removes a link to device which is upper to this one. The caller must hold\n * the RTNL lock.\n */\nvoid netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t     struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\t__netdev_upper_dev_unlink(dev, upper_dev, &priv);\n}\nEXPORT_SYMBOL(netdev_upper_dev_unlink);\n\nstatic void __netdev_adjacent_dev_set(struct net_device *upper_dev,\n\t\t\t\t      struct net_device *lower_dev,\n\t\t\t\t      bool val)\n{\n\tstruct netdev_adjacent *adj;\n\n\tadj = __netdev_find_adj(lower_dev, &upper_dev->adj_list.lower);\n\tif (adj)\n\t\tadj->ignore = val;\n\n\tadj = __netdev_find_adj(upper_dev, &lower_dev->adj_list.upper);\n\tif (adj)\n\t\tadj->ignore = val;\n}\n\nstatic void netdev_adjacent_dev_disable(struct net_device *upper_dev,\n\t\t\t\t\tstruct net_device *lower_dev)\n{\n\t__netdev_adjacent_dev_set(upper_dev, lower_dev, true);\n}\n\nstatic void netdev_adjacent_dev_enable(struct net_device *upper_dev,\n\t\t\t\t       struct net_device *lower_dev)\n{\n\t__netdev_adjacent_dev_set(upper_dev, lower_dev, false);\n}\n\nint netdev_adjacent_change_prepare(struct net_device *old_dev,\n\t\t\t\t   struct net_device *new_dev,\n\t\t\t\t   struct net_device *dev,\n\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = 0,\n\t\t.data = NULL,\n\t};\n\tint err;\n\n\tif (!new_dev)\n\t\treturn 0;\n\n\tif (old_dev && new_dev != old_dev)\n\t\tnetdev_adjacent_dev_disable(dev, old_dev);\n\terr = __netdev_upper_dev_link(new_dev, dev, false, NULL, NULL, &priv,\n\t\t\t\t      extack);\n\tif (err) {\n\t\tif (old_dev && new_dev != old_dev)\n\t\t\tnetdev_adjacent_dev_enable(dev, old_dev);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_adjacent_change_prepare);\n\nvoid netdev_adjacent_change_commit(struct net_device *old_dev,\n\t\t\t\t   struct net_device *new_dev,\n\t\t\t\t   struct net_device *dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_IMM | NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\tif (!new_dev || !old_dev)\n\t\treturn;\n\n\tif (new_dev == old_dev)\n\t\treturn;\n\n\tnetdev_adjacent_dev_enable(dev, old_dev);\n\t__netdev_upper_dev_unlink(old_dev, dev, &priv);\n}\nEXPORT_SYMBOL(netdev_adjacent_change_commit);\n\nvoid netdev_adjacent_change_abort(struct net_device *old_dev,\n\t\t\t\t  struct net_device *new_dev,\n\t\t\t\t  struct net_device *dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = 0,\n\t\t.data = NULL,\n\t};\n\n\tif (!new_dev)\n\t\treturn;\n\n\tif (old_dev && new_dev != old_dev)\n\t\tnetdev_adjacent_dev_enable(dev, old_dev);\n\n\t__netdev_upper_dev_unlink(new_dev, dev, &priv);\n}\nEXPORT_SYMBOL(netdev_adjacent_change_abort);\n\n/**\n * netdev_bonding_info_change - Dispatch event about slave change\n * @dev: device\n * @bonding_info: info to dispatch\n *\n * Send NETDEV_BONDING_INFO to netdev notifiers with info.\n * The caller must hold the RTNL lock.\n */\nvoid netdev_bonding_info_change(struct net_device *dev,\n\t\t\t\tstruct netdev_bonding_info *bonding_info)\n{\n\tstruct netdev_notifier_bonding_info info = {\n\t\t.info.dev = dev,\n\t};\n\n\tmemcpy(&info.bonding_info, bonding_info,\n\t       sizeof(struct netdev_bonding_info));\n\tcall_netdevice_notifiers_info(NETDEV_BONDING_INFO,\n\t\t\t\t      &info.info);\n}\nEXPORT_SYMBOL(netdev_bonding_info_change);\n\n/**\n * netdev_get_xmit_slave - Get the xmit slave of master device\n * @dev: device\n * @skb: The packet\n * @all_slaves: assume all the slaves are active\n *\n * The reference counters are not incremented so the caller must be\n * careful with locks. The caller must hold RCU lock.\n * %NULL is returned if no slave is found.\n */\n\nstruct net_device *netdev_get_xmit_slave(struct net_device *dev,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t bool all_slaves)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_get_xmit_slave)\n\t\treturn NULL;\n\treturn ops->ndo_get_xmit_slave(dev, skb, all_slaves);\n}\nEXPORT_SYMBOL(netdev_get_xmit_slave);\n\nstatic struct net_device *netdev_sk_get_lower_dev(struct net_device *dev,\n\t\t\t\t\t\t  struct sock *sk)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_sk_get_lower_dev)\n\t\treturn NULL;\n\treturn ops->ndo_sk_get_lower_dev(dev, sk);\n}\n\n/**\n * netdev_sk_get_lowest_dev - Get the lowest device in chain given device and socket\n * @dev: device\n * @sk: the socket\n *\n * %NULL is returned if no lower device is found.\n */\n\nstruct net_device *netdev_sk_get_lowest_dev(struct net_device *dev,\n\t\t\t\t\t    struct sock *sk)\n{\n\tstruct net_device *lower;\n\n\tlower = netdev_sk_get_lower_dev(dev, sk);\n\twhile (lower) {\n\t\tdev = lower;\n\t\tlower = netdev_sk_get_lower_dev(dev, sk);\n\t}\n\n\treturn dev;\n}\nEXPORT_SYMBOL(netdev_sk_get_lowest_dev);\n\nstatic void netdev_adjacent_add_links(struct net_device *dev)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_add(dev, iter->dev,\n\t\t\t\t\t  &dev->adj_list.upper);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_add(dev, iter->dev,\n\t\t\t\t\t  &dev->adj_list.lower);\n\t}\n}\n\nstatic void netdev_adjacent_del_links(struct net_device *dev)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, dev->name,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_del(dev, iter->dev->name,\n\t\t\t\t\t  &dev->adj_list.upper);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, dev->name,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_del(dev, iter->dev->name,\n\t\t\t\t\t  &dev->adj_list.lower);\n\t}\n}\n\nvoid netdev_adjacent_rename_links(struct net_device *dev, char *oldname)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, oldname,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, oldname,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t}\n}\n\nvoid *netdev_lower_dev_get_private(struct net_device *dev,\n\t\t\t\t   struct net_device *lower_dev)\n{\n\tstruct netdev_adjacent *lower;\n\n\tif (!lower_dev)\n\t\treturn NULL;\n\tlower = __netdev_find_adj(lower_dev, &dev->adj_list.lower);\n\tif (!lower)\n\t\treturn NULL;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_dev_get_private);\n\n\n/**\n * netdev_lower_state_changed - Dispatch event about lower device state change\n * @lower_dev: device\n * @lower_state_info: state to dispatch\n *\n * Send NETDEV_CHANGELOWERSTATE to netdev notifiers with info.\n * The caller must hold the RTNL lock.\n */\nvoid netdev_lower_state_changed(struct net_device *lower_dev,\n\t\t\t\tvoid *lower_state_info)\n{\n\tstruct netdev_notifier_changelowerstate_info changelowerstate_info = {\n\t\t.info.dev = lower_dev,\n\t};\n\n\tASSERT_RTNL();\n\tchangelowerstate_info.lower_state_info = lower_state_info;\n\tcall_netdevice_notifiers_info(NETDEV_CHANGELOWERSTATE,\n\t\t\t\t      &changelowerstate_info.info);\n}\nEXPORT_SYMBOL(netdev_lower_state_changed);\n\nstatic void dev_change_rx_flags(struct net_device *dev, int flags)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_change_rx_flags)\n\t\tops->ndo_change_rx_flags(dev, flags);\n}\n\nstatic int __dev_set_promiscuity(struct net_device *dev, int inc, bool notify)\n{\n\tunsigned int old_flags = dev->flags;\n\tkuid_t uid;\n\tkgid_t gid;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_PROMISC;\n\tdev->promiscuity += inc;\n\tif (dev->promiscuity == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch promisc and return error.\n\t\t */\n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_PROMISC;\n\t\telse {\n\t\t\tdev->promiscuity -= inc;\n\t\t\tpr_warn(\"%s: promiscuity touches roof, set promiscuity failed. promiscuity feature of device might be broken.\\n\",\n\t\t\t\tdev->name);\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags != old_flags) {\n\t\tpr_info(\"device %s %s promiscuous mode\\n\",\n\t\t\tdev->name,\n\t\t\tdev->flags & IFF_PROMISC ? \"entered\" : \"left\");\n\t\tif (audit_enabled) {\n\t\t\tcurrent_uid_gid(&uid, &gid);\n\t\t\taudit_log(audit_context(), GFP_ATOMIC,\n\t\t\t\t  AUDIT_ANOM_PROMISCUOUS,\n\t\t\t\t  \"dev=%s prom=%d old_prom=%d auid=%u uid=%u gid=%u ses=%u\",\n\t\t\t\t  dev->name, (dev->flags & IFF_PROMISC),\n\t\t\t\t  (old_flags & IFF_PROMISC),\n\t\t\t\t  from_kuid(&init_user_ns, audit_get_loginuid(current)),\n\t\t\t\t  from_kuid(&init_user_ns, uid),\n\t\t\t\t  from_kgid(&init_user_ns, gid),\n\t\t\t\t  audit_get_sessionid(current));\n\t\t}\n\n\t\tdev_change_rx_flags(dev, IFF_PROMISC);\n\t}\n\tif (notify)\n\t\t__dev_notify_flags(dev, old_flags, IFF_PROMISC);\n\treturn 0;\n}\n\n/**\n *\tdev_set_promiscuity\t- update promiscuity count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove promiscuity from a device. While the count in the device\n *\tremains above zero the interface remains promiscuous. Once it hits zero\n *\tthe device reverts back to normal filtering operation. A negative inc\n *\tvalue is used to drop promiscuity on the device.\n *\tReturn 0 if successful or a negative errno code on error.\n */\nint dev_set_promiscuity(struct net_device *dev, int inc)\n{\n\tunsigned int old_flags = dev->flags;\n\tint err;\n\n\terr = __dev_set_promiscuity(dev, inc, true);\n\tif (err < 0)\n\t\treturn err;\n\tif (dev->flags != old_flags)\n\t\tdev_set_rx_mode(dev);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_promiscuity);\n\nstatic int __dev_set_allmulti(struct net_device *dev, int inc, bool notify)\n{\n\tunsigned int old_flags = dev->flags, old_gflags = dev->gflags;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_ALLMULTI;\n\tdev->allmulti += inc;\n\tif (dev->allmulti == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch allmulti and return error.\n\t\t */\n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_ALLMULTI;\n\t\telse {\n\t\t\tdev->allmulti -= inc;\n\t\t\tpr_warn(\"%s: allmulti touches roof, set allmulti failed. allmulti feature of device might be broken.\\n\",\n\t\t\t\tdev->name);\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags ^ old_flags) {\n\t\tdev_change_rx_flags(dev, IFF_ALLMULTI);\n\t\tdev_set_rx_mode(dev);\n\t\tif (notify)\n\t\t\t__dev_notify_flags(dev, old_flags,\n\t\t\t\t\t   dev->gflags ^ old_gflags);\n\t}\n\treturn 0;\n}\n\n/**\n *\tdev_set_allmulti\t- update allmulti count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove reception of all multicast frames to a device. While the\n *\tcount in the device remains above zero the interface remains listening\n *\tto all interfaces. Once it hits zero the device reverts back to normal\n *\tfiltering operation. A negative @inc value is used to drop the counter\n *\twhen releasing a resource needing all multicasts.\n *\tReturn 0 if successful or a negative errno code on error.\n */\n\nint dev_set_allmulti(struct net_device *dev, int inc)\n{\n\treturn __dev_set_allmulti(dev, inc, true);\n}\nEXPORT_SYMBOL(dev_set_allmulti);\n\n/*\n *\tUpload unicast and multicast address lists to device and\n *\tconfigure RX filtering. When the device doesn't support unicast\n *\tfiltering it is put in promiscuous mode while unicast addresses\n *\tare present.\n */\nvoid __dev_set_rx_mode(struct net_device *dev)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t/* dev_open will call this function so the list will stay sane. */\n\tif (!(dev->flags&IFF_UP))\n\t\treturn;\n\n\tif (!netif_device_present(dev))\n\t\treturn;\n\n\tif (!(dev->priv_flags & IFF_UNICAST_FLT)) {\n\t\t/* Unicast addresses changes may only happen under the rtnl,\n\t\t * therefore calling __dev_set_promiscuity here is safe.\n\t\t */\n\t\tif (!netdev_uc_empty(dev) && !dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, 1, false);\n\t\t\tdev->uc_promisc = true;\n\t\t} else if (netdev_uc_empty(dev) && dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, -1, false);\n\t\t\tdev->uc_promisc = false;\n\t\t}\n\t}\n\n\tif (ops->ndo_set_rx_mode)\n\t\tops->ndo_set_rx_mode(dev);\n}\n\nvoid dev_set_rx_mode(struct net_device *dev)\n{\n\tnetif_addr_lock_bh(dev);\n\t__dev_set_rx_mode(dev);\n\tnetif_addr_unlock_bh(dev);\n}\n\n/**\n *\tdev_get_flags - get flags reported to userspace\n *\t@dev: device\n *\n *\tGet the combination of flag bits exported through APIs to userspace.\n */\nunsigned int dev_get_flags(const struct net_device *dev)\n{\n\tunsigned int flags;\n\n\tflags = (dev->flags & ~(IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI |\n\t\t\t\tIFF_RUNNING |\n\t\t\t\tIFF_LOWER_UP |\n\t\t\t\tIFF_DORMANT)) |\n\t\t(dev->gflags & (IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI));\n\n\tif (netif_running(dev)) {\n\t\tif (netif_oper_up(dev))\n\t\t\tflags |= IFF_RUNNING;\n\t\tif (netif_carrier_ok(dev))\n\t\t\tflags |= IFF_LOWER_UP;\n\t\tif (netif_dormant(dev))\n\t\t\tflags |= IFF_DORMANT;\n\t}\n\n\treturn flags;\n}\nEXPORT_SYMBOL(dev_get_flags);\n\nint __dev_change_flags(struct net_device *dev, unsigned int flags,\n\t\t       struct netlink_ext_ack *extack)\n{\n\tunsigned int old_flags = dev->flags;\n\tint ret;\n\n\tASSERT_RTNL();\n\n\t/*\n\t *\tSet the flags on our device.\n\t */\n\n\tdev->flags = (flags & (IFF_DEBUG | IFF_NOTRAILERS | IFF_NOARP |\n\t\t\t       IFF_DYNAMIC | IFF_MULTICAST | IFF_PORTSEL |\n\t\t\t       IFF_AUTOMEDIA)) |\n\t\t     (dev->flags & (IFF_UP | IFF_VOLATILE | IFF_PROMISC |\n\t\t\t\t    IFF_ALLMULTI));\n\n\t/*\n\t *\tLoad in the correct multicast list now the flags have changed.\n\t */\n\n\tif ((old_flags ^ flags) & IFF_MULTICAST)\n\t\tdev_change_rx_flags(dev, IFF_MULTICAST);\n\n\tdev_set_rx_mode(dev);\n\n\t/*\n\t *\tHave we downed the interface. We handle IFF_UP ourselves\n\t *\taccording to user attempts to set it, rather than blindly\n\t *\tsetting it.\n\t */\n\n\tret = 0;\n\tif ((old_flags ^ flags) & IFF_UP) {\n\t\tif (old_flags & IFF_UP)\n\t\t\t__dev_close(dev);\n\t\telse\n\t\t\tret = __dev_open(dev, extack);\n\t}\n\n\tif ((flags ^ dev->gflags) & IFF_PROMISC) {\n\t\tint inc = (flags & IFF_PROMISC) ? 1 : -1;\n\t\tunsigned int old_flags = dev->flags;\n\n\t\tdev->gflags ^= IFF_PROMISC;\n\n\t\tif (__dev_set_promiscuity(dev, inc, false) >= 0)\n\t\t\tif (dev->flags != old_flags)\n\t\t\t\tdev_set_rx_mode(dev);\n\t}\n\n\t/* NOTE: order of synchronization of IFF_PROMISC and IFF_ALLMULTI\n\t * is important. Some (broken) drivers set IFF_PROMISC, when\n\t * IFF_ALLMULTI is requested not asking us and not reporting.\n\t */\n\tif ((flags ^ dev->gflags) & IFF_ALLMULTI) {\n\t\tint inc = (flags & IFF_ALLMULTI) ? 1 : -1;\n\n\t\tdev->gflags ^= IFF_ALLMULTI;\n\t\t__dev_set_allmulti(dev, inc, false);\n\t}\n\n\treturn ret;\n}\n\nvoid __dev_notify_flags(struct net_device *dev, unsigned int old_flags,\n\t\t\tunsigned int gchanges)\n{\n\tunsigned int changes = dev->flags ^ old_flags;\n\n\tif (gchanges)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, gchanges, GFP_ATOMIC);\n\n\tif (changes & IFF_UP) {\n\t\tif (dev->flags & IFF_UP)\n\t\t\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\t\telse\n\t\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t}\n\n\tif (dev->flags & IFF_UP &&\n\t    (changes & ~(IFF_UP | IFF_PROMISC | IFF_ALLMULTI | IFF_VOLATILE))) {\n\t\tstruct netdev_notifier_change_info change_info = {\n\t\t\t.info = {\n\t\t\t\t.dev = dev,\n\t\t\t},\n\t\t\t.flags_changed = changes,\n\t\t};\n\n\t\tcall_netdevice_notifiers_info(NETDEV_CHANGE, &change_info.info);\n\t}\n}\n\n/**\n *\tdev_change_flags - change device settings\n *\t@dev: device\n *\t@flags: device state flags\n *\t@extack: netlink extended ack\n *\n *\tChange settings on device based state flags. The flags are\n *\tin the userspace exported format.\n */\nint dev_change_flags(struct net_device *dev, unsigned int flags,\n\t\t     struct netlink_ext_ack *extack)\n{\n\tint ret;\n\tunsigned int changes, old_flags = dev->flags, old_gflags = dev->gflags;\n\n\tret = __dev_change_flags(dev, flags, extack);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tchanges = (old_flags ^ dev->flags) | (old_gflags ^ dev->gflags);\n\t__dev_notify_flags(dev, old_flags, changes);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_change_flags);\n\nint __dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_change_mtu)\n\t\treturn ops->ndo_change_mtu(dev, new_mtu);\n\n\t/* Pairs with all the lockless reads of dev->mtu in the stack */\n\tWRITE_ONCE(dev->mtu, new_mtu);\n\treturn 0;\n}\nEXPORT_SYMBOL(__dev_set_mtu);\n\nint dev_validate_mtu(struct net_device *dev, int new_mtu,\n\t\t     struct netlink_ext_ack *extack)\n{\n\t/* MTU must be positive, and in range */\n\tif (new_mtu < 0 || new_mtu < dev->min_mtu) {\n\t\tNL_SET_ERR_MSG(extack, \"mtu less than device minimum\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (dev->max_mtu > 0 && new_mtu > dev->max_mtu) {\n\t\tNL_SET_ERR_MSG(extack, \"mtu greater than device maximum\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n/**\n *\tdev_set_mtu_ext - Change maximum transfer unit\n *\t@dev: device\n *\t@new_mtu: new transfer unit\n *\t@extack: netlink extended ack\n *\n *\tChange the maximum transfer size of the network device.\n */\nint dev_set_mtu_ext(struct net_device *dev, int new_mtu,\n\t\t    struct netlink_ext_ack *extack)\n{\n\tint err, orig_mtu;\n\n\tif (new_mtu == dev->mtu)\n\t\treturn 0;\n\n\terr = dev_validate_mtu(dev, new_mtu, extack);\n\tif (err)\n\t\treturn err;\n\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\n\terr = call_netdevice_notifiers(NETDEV_PRECHANGEMTU, dev);\n\terr = notifier_to_errno(err);\n\tif (err)\n\t\treturn err;\n\n\torig_mtu = dev->mtu;\n\terr = __dev_set_mtu(dev, new_mtu);\n\n\tif (!err) {\n\t\terr = call_netdevice_notifiers_mtu(NETDEV_CHANGEMTU, dev,\n\t\t\t\t\t\t   orig_mtu);\n\t\terr = notifier_to_errno(err);\n\t\tif (err) {\n\t\t\t/* setting mtu back and notifying everyone again,\n\t\t\t * so that they have a chance to revert changes.\n\t\t\t */\n\t\t\t__dev_set_mtu(dev, orig_mtu);\n\t\t\tcall_netdevice_notifiers_mtu(NETDEV_CHANGEMTU, dev,\n\t\t\t\t\t\t     new_mtu);\n\t\t}\n\t}\n\treturn err;\n}\n\nint dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tstruct netlink_ext_ack extack;\n\tint err;\n\n\tmemset(&extack, 0, sizeof(extack));\n\terr = dev_set_mtu_ext(dev, new_mtu, &extack);\n\tif (err && extack._msg)\n\t\tnet_err_ratelimited(\"%s: %s\\n\", dev->name, extack._msg);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_mtu);\n\n/**\n *\tdev_change_tx_queue_len - Change TX queue length of a netdevice\n *\t@dev: device\n *\t@new_len: new tx queue length\n */\nint dev_change_tx_queue_len(struct net_device *dev, unsigned long new_len)\n{\n\tunsigned int orig_len = dev->tx_queue_len;\n\tint res;\n\n\tif (new_len != (unsigned int)new_len)\n\t\treturn -ERANGE;\n\n\tif (new_len != orig_len) {\n\t\tdev->tx_queue_len = new_len;\n\t\tres = call_netdevice_notifiers(NETDEV_CHANGE_TX_QUEUE_LEN, dev);\n\t\tres = notifier_to_errno(res);\n\t\tif (res)\n\t\t\tgoto err_rollback;\n\t\tres = dev_qdisc_change_tx_queue_len(dev);\n\t\tif (res)\n\t\t\tgoto err_rollback;\n\t}\n\n\treturn 0;\n\nerr_rollback:\n\tnetdev_err(dev, \"refused to change device tx_queue_len\\n\");\n\tdev->tx_queue_len = orig_len;\n\treturn res;\n}\n\n/**\n *\tdev_set_group - Change group this device belongs to\n *\t@dev: device\n *\t@new_group: group this device should belong to\n */\nvoid dev_set_group(struct net_device *dev, int new_group)\n{\n\tdev->group = new_group;\n}\nEXPORT_SYMBOL(dev_set_group);\n\n/**\n *\tdev_pre_changeaddr_notify - Call NETDEV_PRE_CHANGEADDR.\n *\t@dev: device\n *\t@addr: new address\n *\t@extack: netlink extended ack\n */\nint dev_pre_changeaddr_notify(struct net_device *dev, const char *addr,\n\t\t\t      struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_pre_changeaddr_info info = {\n\t\t.info.dev = dev,\n\t\t.info.extack = extack,\n\t\t.dev_addr = addr,\n\t};\n\tint rc;\n\n\trc = call_netdevice_notifiers_info(NETDEV_PRE_CHANGEADDR, &info.info);\n\treturn notifier_to_errno(rc);\n}\nEXPORT_SYMBOL(dev_pre_changeaddr_notify);\n\n/**\n *\tdev_set_mac_address - Change Media Access Control Address\n *\t@dev: device\n *\t@sa: new address\n *\t@extack: netlink extended ack\n *\n *\tChange the hardware (MAC) address of the device\n */\nint dev_set_mac_address(struct net_device *dev, struct sockaddr *sa,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint err;\n\n\tif (!ops->ndo_set_mac_address)\n\t\treturn -EOPNOTSUPP;\n\tif (sa->sa_family != dev->type)\n\t\treturn -EINVAL;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\terr = dev_pre_changeaddr_notify(dev, sa->sa_data, extack);\n\tif (err)\n\t\treturn err;\n\terr = ops->ndo_set_mac_address(dev, sa);\n\tif (err)\n\t\treturn err;\n\tdev->addr_assign_type = NET_ADDR_SET;\n\tcall_netdevice_notifiers(NETDEV_CHANGEADDR, dev);\n\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_set_mac_address);\n\n/**\n *\tdev_change_carrier - Change device carrier\n *\t@dev: device\n *\t@new_carrier: new value\n *\n *\tChange device carrier\n */\nint dev_change_carrier(struct net_device *dev, bool new_carrier)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_change_carrier)\n\t\treturn -EOPNOTSUPP;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\treturn ops->ndo_change_carrier(dev, new_carrier);\n}\nEXPORT_SYMBOL(dev_change_carrier);\n\n/**\n *\tdev_get_phys_port_id - Get device physical port ID\n *\t@dev: device\n *\t@ppid: port ID\n *\n *\tGet device physical port ID\n */\nint dev_get_phys_port_id(struct net_device *dev,\n\t\t\t struct netdev_phys_item_id *ppid)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_get_phys_port_id)\n\t\treturn -EOPNOTSUPP;\n\treturn ops->ndo_get_phys_port_id(dev, ppid);\n}\nEXPORT_SYMBOL(dev_get_phys_port_id);\n\n/**\n *\tdev_get_phys_port_name - Get device physical port name\n *\t@dev: device\n *\t@name: port name\n *\t@len: limit of bytes to copy to name\n *\n *\tGet device physical port name\n */\nint dev_get_phys_port_name(struct net_device *dev,\n\t\t\t   char *name, size_t len)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint err;\n\n\tif (ops->ndo_get_phys_port_name) {\n\t\terr = ops->ndo_get_phys_port_name(dev, name, len);\n\t\tif (err != -EOPNOTSUPP)\n\t\t\treturn err;\n\t}\n\treturn devlink_compat_phys_port_name_get(dev, name, len);\n}\nEXPORT_SYMBOL(dev_get_phys_port_name);\n\n/**\n *\tdev_get_port_parent_id - Get the device's port parent identifier\n *\t@dev: network device\n *\t@ppid: pointer to a storage for the port's parent identifier\n *\t@recurse: allow/disallow recursion to lower devices\n *\n *\tGet the devices's port parent identifier\n */\nint dev_get_port_parent_id(struct net_device *dev,\n\t\t\t   struct netdev_phys_item_id *ppid,\n\t\t\t   bool recurse)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tstruct netdev_phys_item_id first = { };\n\tstruct net_device *lower_dev;\n\tstruct list_head *iter;\n\tint err;\n\n\tif (ops->ndo_get_port_parent_id) {\n\t\terr = ops->ndo_get_port_parent_id(dev, ppid);\n\t\tif (err != -EOPNOTSUPP)\n\t\t\treturn err;\n\t}\n\n\terr = devlink_compat_switch_id_get(dev, ppid);\n\tif (!err || err != -EOPNOTSUPP)\n\t\treturn err;\n\n\tif (!recurse)\n\t\treturn -EOPNOTSUPP;\n\n\tnetdev_for_each_lower_dev(dev, lower_dev, iter) {\n\t\terr = dev_get_port_parent_id(lower_dev, ppid, recurse);\n\t\tif (err)\n\t\t\tbreak;\n\t\tif (!first.id_len)\n\t\t\tfirst = *ppid;\n\t\telse if (memcmp(&first, ppid, sizeof(*ppid)))\n\t\t\treturn -EOPNOTSUPP;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(dev_get_port_parent_id);\n\n/**\n *\tnetdev_port_same_parent_id - Indicate if two network devices have\n *\tthe same port parent identifier\n *\t@a: first network device\n *\t@b: second network device\n */\nbool netdev_port_same_parent_id(struct net_device *a, struct net_device *b)\n{\n\tstruct netdev_phys_item_id a_id = { };\n\tstruct netdev_phys_item_id b_id = { };\n\n\tif (dev_get_port_parent_id(a, &a_id, true) ||\n\t    dev_get_port_parent_id(b, &b_id, true))\n\t\treturn false;\n\n\treturn netdev_phys_item_id_same(&a_id, &b_id);\n}\nEXPORT_SYMBOL(netdev_port_same_parent_id);\n\n/**\n *\tdev_change_proto_down - update protocol port state information\n *\t@dev: device\n *\t@proto_down: new value\n *\n *\tThis info can be used by switch drivers to set the phys state of the\n *\tport.\n */\nint dev_change_proto_down(struct net_device *dev, bool proto_down)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_change_proto_down)\n\t\treturn -EOPNOTSUPP;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\treturn ops->ndo_change_proto_down(dev, proto_down);\n}\nEXPORT_SYMBOL(dev_change_proto_down);\n\n/**\n *\tdev_change_proto_down_generic - generic implementation for\n * \tndo_change_proto_down that sets carrier according to\n * \tproto_down.\n *\n *\t@dev: device\n *\t@proto_down: new value\n */\nint dev_change_proto_down_generic(struct net_device *dev, bool proto_down)\n{\n\tif (proto_down)\n\t\tnetif_carrier_off(dev);\n\telse\n\t\tnetif_carrier_on(dev);\n\tdev->proto_down = proto_down;\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_change_proto_down_generic);\n\n/**\n *\tdev_change_proto_down_reason - proto down reason\n *\n *\t@dev: device\n *\t@mask: proto down mask\n *\t@value: proto down value\n */\nvoid dev_change_proto_down_reason(struct net_device *dev, unsigned long mask,\n\t\t\t\t  u32 value)\n{\n\tint b;\n\n\tif (!mask) {\n\t\tdev->proto_down_reason = value;\n\t} else {\n\t\tfor_each_set_bit(b, &mask, 32) {\n\t\t\tif (value & (1 << b))\n\t\t\t\tdev->proto_down_reason |= BIT(b);\n\t\t\telse\n\t\t\t\tdev->proto_down_reason &= ~BIT(b);\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(dev_change_proto_down_reason);\n\nstruct bpf_xdp_link {\n\tstruct bpf_link link;\n\tstruct net_device *dev; /* protected by rtnl_lock, no refcnt held */\n\tint flags;\n};\n\nstatic enum bpf_xdp_mode dev_xdp_mode(struct net_device *dev, u32 flags)\n{\n\tif (flags & XDP_FLAGS_HW_MODE)\n\t\treturn XDP_MODE_HW;\n\tif (flags & XDP_FLAGS_DRV_MODE)\n\t\treturn XDP_MODE_DRV;\n\tif (flags & XDP_FLAGS_SKB_MODE)\n\t\treturn XDP_MODE_SKB;\n\treturn dev->netdev_ops->ndo_bpf ? XDP_MODE_DRV : XDP_MODE_SKB;\n}\n\nstatic bpf_op_t dev_xdp_bpf_op(struct net_device *dev, enum bpf_xdp_mode mode)\n{\n\tswitch (mode) {\n\tcase XDP_MODE_SKB:\n\t\treturn generic_xdp_install;\n\tcase XDP_MODE_DRV:\n\tcase XDP_MODE_HW:\n\t\treturn dev->netdev_ops->ndo_bpf;\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic struct bpf_xdp_link *dev_xdp_link(struct net_device *dev,\n\t\t\t\t\t enum bpf_xdp_mode mode)\n{\n\treturn dev->xdp_state[mode].link;\n}\n\nstatic struct bpf_prog *dev_xdp_prog(struct net_device *dev,\n\t\t\t\t     enum bpf_xdp_mode mode)\n{\n\tstruct bpf_xdp_link *link = dev_xdp_link(dev, mode);\n\n\tif (link)\n\t\treturn link->link.prog;\n\treturn dev->xdp_state[mode].prog;\n}\n\nstatic u8 dev_xdp_prog_count(struct net_device *dev)\n{\n\tu8 count = 0;\n\tint i;\n\n\tfor (i = 0; i < __MAX_XDP_MODE; i++)\n\t\tif (dev->xdp_state[i].prog || dev->xdp_state[i].link)\n\t\t\tcount++;\n\treturn count;\n}\n\nu32 dev_xdp_prog_id(struct net_device *dev, enum bpf_xdp_mode mode)\n{\n\tstruct bpf_prog *prog = dev_xdp_prog(dev, mode);\n\n\treturn prog ? prog->aux->id : 0;\n}\n\nstatic void dev_xdp_set_link(struct net_device *dev, enum bpf_xdp_mode mode,\n\t\t\t     struct bpf_xdp_link *link)\n{\n\tdev->xdp_state[mode].link = link;\n\tdev->xdp_state[mode].prog = NULL;\n}\n\nstatic void dev_xdp_set_prog(struct net_device *dev, enum bpf_xdp_mode mode,\n\t\t\t     struct bpf_prog *prog)\n{\n\tdev->xdp_state[mode].link = NULL;\n\tdev->xdp_state[mode].prog = prog;\n}\n\nstatic int dev_xdp_install(struct net_device *dev, enum bpf_xdp_mode mode,\n\t\t\t   bpf_op_t bpf_op, struct netlink_ext_ack *extack,\n\t\t\t   u32 flags, struct bpf_prog *prog)\n{\n\tstruct netdev_bpf xdp;\n\tint err;\n\n\tmemset(&xdp, 0, sizeof(xdp));\n\txdp.command = mode == XDP_MODE_HW ? XDP_SETUP_PROG_HW : XDP_SETUP_PROG;\n\txdp.extack = extack;\n\txdp.flags = flags;\n\txdp.prog = prog;\n\n\t/* Drivers assume refcnt is already incremented (i.e, prog pointer is\n\t * \"moved\" into driver), so they don't increment it on their own, but\n\t * they do decrement refcnt when program is detached or replaced.\n\t * Given net_device also owns link/prog, we need to bump refcnt here\n\t * to prevent drivers from underflowing it.\n\t */\n\tif (prog)\n\t\tbpf_prog_inc(prog);\n\terr = bpf_op(dev, &xdp);\n\tif (err) {\n\t\tif (prog)\n\t\t\tbpf_prog_put(prog);\n\t\treturn err;\n\t}\n\n\tif (mode != XDP_MODE_HW)\n\t\tbpf_prog_change_xdp(dev_xdp_prog(dev, mode), prog);\n\n\treturn 0;\n}\n\nstatic void dev_xdp_uninstall(struct net_device *dev)\n{\n\tstruct bpf_xdp_link *link;\n\tstruct bpf_prog *prog;\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\n\tASSERT_RTNL();\n\n\tfor (mode = XDP_MODE_SKB; mode < __MAX_XDP_MODE; mode++) {\n\t\tprog = dev_xdp_prog(dev, mode);\n\t\tif (!prog)\n\t\t\tcontinue;\n\n\t\tbpf_op = dev_xdp_bpf_op(dev, mode);\n\t\tif (!bpf_op)\n\t\t\tcontinue;\n\n\t\tWARN_ON(dev_xdp_install(dev, mode, bpf_op, NULL, 0, NULL));\n\n\t\t/* auto-detach link from net device */\n\t\tlink = dev_xdp_link(dev, mode);\n\t\tif (link)\n\t\t\tlink->dev = NULL;\n\t\telse\n\t\t\tbpf_prog_put(prog);\n\n\t\tdev_xdp_set_link(dev, mode, NULL);\n\t}\n}\n\nstatic int dev_xdp_attach(struct net_device *dev, struct netlink_ext_ack *extack,\n\t\t\t  struct bpf_xdp_link *link, struct bpf_prog *new_prog,\n\t\t\t  struct bpf_prog *old_prog, u32 flags)\n{\n\tunsigned int num_modes = hweight32(flags & XDP_FLAGS_MODES);\n\tstruct bpf_prog *cur_prog;\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\tint err;\n\n\tASSERT_RTNL();\n\n\t/* either link or prog attachment, never both */\n\tif (link && (new_prog || old_prog))\n\t\treturn -EINVAL;\n\t/* link supports only XDP mode flags */\n\tif (link && (flags & ~XDP_FLAGS_MODES)) {\n\t\tNL_SET_ERR_MSG(extack, \"Invalid XDP flags for BPF link attachment\");\n\t\treturn -EINVAL;\n\t}\n\t/* just one XDP mode bit should be set, zero defaults to drv/skb mode */\n\tif (num_modes > 1) {\n\t\tNL_SET_ERR_MSG(extack, \"Only one XDP mode flag can be set\");\n\t\treturn -EINVAL;\n\t}\n\t/* avoid ambiguity if offload + drv/skb mode progs are both loaded */\n\tif (!num_modes && dev_xdp_prog_count(dev) > 1) {\n\t\tNL_SET_ERR_MSG(extack,\n\t\t\t       \"More than one program loaded, unset mode is ambiguous\");\n\t\treturn -EINVAL;\n\t}\n\t/* old_prog != NULL implies XDP_FLAGS_REPLACE is set */\n\tif (old_prog && !(flags & XDP_FLAGS_REPLACE)) {\n\t\tNL_SET_ERR_MSG(extack, \"XDP_FLAGS_REPLACE is not specified\");\n\t\treturn -EINVAL;\n\t}\n\n\tmode = dev_xdp_mode(dev, flags);\n\t/* can't replace attached link */\n\tif (dev_xdp_link(dev, mode)) {\n\t\tNL_SET_ERR_MSG(extack, \"Can't replace active BPF XDP link\");\n\t\treturn -EBUSY;\n\t}\n\n\tcur_prog = dev_xdp_prog(dev, mode);\n\t/* can't replace attached prog with link */\n\tif (link && cur_prog) {\n\t\tNL_SET_ERR_MSG(extack, \"Can't replace active XDP program with BPF link\");\n\t\treturn -EBUSY;\n\t}\n\tif ((flags & XDP_FLAGS_REPLACE) && cur_prog != old_prog) {\n\t\tNL_SET_ERR_MSG(extack, \"Active program does not match expected\");\n\t\treturn -EEXIST;\n\t}\n\n\t/* put effective new program into new_prog */\n\tif (link)\n\t\tnew_prog = link->link.prog;\n\n\tif (new_prog) {\n\t\tbool offload = mode == XDP_MODE_HW;\n\t\tenum bpf_xdp_mode other_mode = mode == XDP_MODE_SKB\n\t\t\t\t\t       ? XDP_MODE_DRV : XDP_MODE_SKB;\n\n\t\tif ((flags & XDP_FLAGS_UPDATE_IF_NOEXIST) && cur_prog) {\n\t\t\tNL_SET_ERR_MSG(extack, \"XDP program already attached\");\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tif (!offload && dev_xdp_prog(dev, other_mode)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Native and generic XDP can't be active at the same time\");\n\t\t\treturn -EEXIST;\n\t\t}\n\t\tif (!offload && bpf_prog_is_dev_bound(new_prog->aux)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Using device-bound program without HW_MODE flag is not supported\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (new_prog->expected_attach_type == BPF_XDP_DEVMAP) {\n\t\t\tNL_SET_ERR_MSG(extack, \"BPF_XDP_DEVMAP programs can not be attached to a device\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (new_prog->expected_attach_type == BPF_XDP_CPUMAP) {\n\t\t\tNL_SET_ERR_MSG(extack, \"BPF_XDP_CPUMAP programs can not be attached to a device\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* don't call drivers if the effective program didn't change */\n\tif (new_prog != cur_prog) {\n\t\tbpf_op = dev_xdp_bpf_op(dev, mode);\n\t\tif (!bpf_op) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Underlying driver does not support XDP in native mode\");\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\terr = dev_xdp_install(dev, mode, bpf_op, extack, flags, new_prog);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (link)\n\t\tdev_xdp_set_link(dev, mode, link);\n\telse\n\t\tdev_xdp_set_prog(dev, mode, new_prog);\n\tif (cur_prog)\n\t\tbpf_prog_put(cur_prog);\n\n\treturn 0;\n}\n\nstatic int dev_xdp_attach_link(struct net_device *dev,\n\t\t\t       struct netlink_ext_ack *extack,\n\t\t\t       struct bpf_xdp_link *link)\n{\n\treturn dev_xdp_attach(dev, extack, link, NULL, NULL, link->flags);\n}\n\nstatic int dev_xdp_detach_link(struct net_device *dev,\n\t\t\t       struct netlink_ext_ack *extack,\n\t\t\t       struct bpf_xdp_link *link)\n{\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\n\tASSERT_RTNL();\n\n\tmode = dev_xdp_mode(dev, link->flags);\n\tif (dev_xdp_link(dev, mode) != link)\n\t\treturn -EINVAL;\n\n\tbpf_op = dev_xdp_bpf_op(dev, mode);\n\tWARN_ON(dev_xdp_install(dev, mode, bpf_op, NULL, 0, NULL));\n\tdev_xdp_set_link(dev, mode, NULL);\n\treturn 0;\n}\n\nstatic void bpf_xdp_link_release(struct bpf_link *link)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\n\trtnl_lock();\n\n\t/* if racing with net_device's tear down, xdp_link->dev might be\n\t * already NULL, in which case link was already auto-detached\n\t */\n\tif (xdp_link->dev) {\n\t\tWARN_ON(dev_xdp_detach_link(xdp_link->dev, NULL, xdp_link));\n\t\txdp_link->dev = NULL;\n\t}\n\n\trtnl_unlock();\n}\n\nstatic int bpf_xdp_link_detach(struct bpf_link *link)\n{\n\tbpf_xdp_link_release(link);\n\treturn 0;\n}\n\nstatic void bpf_xdp_link_dealloc(struct bpf_link *link)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\n\tkfree(xdp_link);\n}\n\nstatic void bpf_xdp_link_show_fdinfo(const struct bpf_link *link,\n\t\t\t\t     struct seq_file *seq)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\tu32 ifindex = 0;\n\n\trtnl_lock();\n\tif (xdp_link->dev)\n\t\tifindex = xdp_link->dev->ifindex;\n\trtnl_unlock();\n\n\tseq_printf(seq, \"ifindex:\\t%u\\n\", ifindex);\n}\n\nstatic int bpf_xdp_link_fill_link_info(const struct bpf_link *link,\n\t\t\t\t       struct bpf_link_info *info)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\tu32 ifindex = 0;\n\n\trtnl_lock();\n\tif (xdp_link->dev)\n\t\tifindex = xdp_link->dev->ifindex;\n\trtnl_unlock();\n\n\tinfo->xdp.ifindex = ifindex;\n\treturn 0;\n}\n\nstatic int bpf_xdp_link_update(struct bpf_link *link, struct bpf_prog *new_prog,\n\t\t\t       struct bpf_prog *old_prog)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\tint err = 0;\n\n\trtnl_lock();\n\n\t/* link might have been auto-released already, so fail */\n\tif (!xdp_link->dev) {\n\t\terr = -ENOLINK;\n\t\tgoto out_unlock;\n\t}\n\n\tif (old_prog && link->prog != old_prog) {\n\t\terr = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\told_prog = link->prog;\n\tif (old_prog == new_prog) {\n\t\t/* no-op, don't disturb drivers */\n\t\tbpf_prog_put(new_prog);\n\t\tgoto out_unlock;\n\t}\n\n\tmode = dev_xdp_mode(xdp_link->dev, xdp_link->flags);\n\tbpf_op = dev_xdp_bpf_op(xdp_link->dev, mode);\n\terr = dev_xdp_install(xdp_link->dev, mode, bpf_op, NULL,\n\t\t\t      xdp_link->flags, new_prog);\n\tif (err)\n\t\tgoto out_unlock;\n\n\told_prog = xchg(&link->prog, new_prog);\n\tbpf_prog_put(old_prog);\n\nout_unlock:\n\trtnl_unlock();\n\treturn err;\n}\n\nstatic const struct bpf_link_ops bpf_xdp_link_lops = {\n\t.release = bpf_xdp_link_release,\n\t.dealloc = bpf_xdp_link_dealloc,\n\t.detach = bpf_xdp_link_detach,\n\t.show_fdinfo = bpf_xdp_link_show_fdinfo,\n\t.fill_link_info = bpf_xdp_link_fill_link_info,\n\t.update_prog = bpf_xdp_link_update,\n};\n\nint bpf_xdp_link_attach(const union bpf_attr *attr, struct bpf_prog *prog)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct bpf_link_primer link_primer;\n\tstruct bpf_xdp_link *link;\n\tstruct net_device *dev;\n\tint err, fd;\n\n\tdev = dev_get_by_index(net, attr->link_create.target_ifindex);\n\tif (!dev)\n\t\treturn -EINVAL;\n\n\tlink = kzalloc(sizeof(*link), GFP_USER);\n\tif (!link) {\n\t\terr = -ENOMEM;\n\t\tgoto out_put_dev;\n\t}\n\n\tbpf_link_init(&link->link, BPF_LINK_TYPE_XDP, &bpf_xdp_link_lops, prog);\n\tlink->dev = dev;\n\tlink->flags = attr->link_create.flags;\n\n\terr = bpf_link_prime(&link->link, &link_primer);\n\tif (err) {\n\t\tkfree(link);\n\t\tgoto out_put_dev;\n\t}\n\n\trtnl_lock();\n\terr = dev_xdp_attach_link(dev, NULL, link);\n\trtnl_unlock();\n\n\tif (err) {\n\t\tbpf_link_cleanup(&link_primer);\n\t\tgoto out_put_dev;\n\t}\n\n\tfd = bpf_link_settle(&link_primer);\n\t/* link itself doesn't hold dev's refcnt to not complicate shutdown */\n\tdev_put(dev);\n\treturn fd;\n\nout_put_dev:\n\tdev_put(dev);\n\treturn err;\n}\n\n/**\n *\tdev_change_xdp_fd - set or clear a bpf program for a device rx path\n *\t@dev: device\n *\t@extack: netlink extended ack\n *\t@fd: new program fd or negative value to clear\n *\t@expected_fd: old program fd that userspace expects to replace or clear\n *\t@flags: xdp-related flags\n *\n *\tSet or clear a bpf program for a device\n */\nint dev_change_xdp_fd(struct net_device *dev, struct netlink_ext_ack *extack,\n\t\t      int fd, int expected_fd, u32 flags)\n{\n\tenum bpf_xdp_mode mode = dev_xdp_mode(dev, flags);\n\tstruct bpf_prog *new_prog = NULL, *old_prog = NULL;\n\tint err;\n\n\tASSERT_RTNL();\n\n\tif (fd >= 0) {\n\t\tnew_prog = bpf_prog_get_type_dev(fd, BPF_PROG_TYPE_XDP,\n\t\t\t\t\t\t mode != XDP_MODE_SKB);\n\t\tif (IS_ERR(new_prog))\n\t\t\treturn PTR_ERR(new_prog);\n\t}\n\n\tif (expected_fd >= 0) {\n\t\told_prog = bpf_prog_get_type_dev(expected_fd, BPF_PROG_TYPE_XDP,\n\t\t\t\t\t\t mode != XDP_MODE_SKB);\n\t\tif (IS_ERR(old_prog)) {\n\t\t\terr = PTR_ERR(old_prog);\n\t\t\told_prog = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\terr = dev_xdp_attach(dev, extack, NULL, new_prog, old_prog, flags);\n\nerr_out:\n\tif (err && new_prog)\n\t\tbpf_prog_put(new_prog);\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\treturn err;\n}\n\n/**\n *\tdev_new_index\t-\tallocate an ifindex\n *\t@net: the applicable net namespace\n *\n *\tReturns a suitable unique value for a new device interface\n *\tnumber.  The caller must hold the rtnl semaphore or the\n *\tdev_base_lock to be sure it remains unique.\n */\nstatic int dev_new_index(struct net *net)\n{\n\tint ifindex = net->ifindex;\n\n\tfor (;;) {\n\t\tif (++ifindex <= 0)\n\t\t\tifindex = 1;\n\t\tif (!__dev_get_by_index(net, ifindex))\n\t\t\treturn net->ifindex = ifindex;\n\t}\n}\n\n/* Delayed registration/unregisteration */\nstatic LIST_HEAD(net_todo_list);\nDECLARE_WAIT_QUEUE_HEAD(netdev_unregistering_wq);\n\nstatic void net_set_todo(struct net_device *dev)\n{\n\tlist_add_tail(&dev->todo_list, &net_todo_list);\n\tdev_net(dev)->dev_unreg_count++;\n}\n\nstatic netdev_features_t netdev_sync_upper_features(struct net_device *lower,\n\tstruct net_device *upper, netdev_features_t features)\n{\n\tnetdev_features_t upper_disables = NETIF_F_UPPER_DISABLES;\n\tnetdev_features_t feature;\n\tint feature_bit;\n\n\tfor_each_netdev_feature(upper_disables, feature_bit) {\n\t\tfeature = __NETIF_F_BIT(feature_bit);\n\t\tif (!(upper->wanted_features & feature)\n\t\t    && (features & feature)) {\n\t\t\tnetdev_dbg(lower, \"Dropping feature %pNF, upper dev %s has it off.\\n\",\n\t\t\t\t   &feature, upper->name);\n\t\t\tfeatures &= ~feature;\n\t\t}\n\t}\n\n\treturn features;\n}\n\nstatic void netdev_sync_lower_features(struct net_device *upper,\n\tstruct net_device *lower, netdev_features_t features)\n{\n\tnetdev_features_t upper_disables = NETIF_F_UPPER_DISABLES;\n\tnetdev_features_t feature;\n\tint feature_bit;\n\n\tfor_each_netdev_feature(upper_disables, feature_bit) {\n\t\tfeature = __NETIF_F_BIT(feature_bit);\n\t\tif (!(features & feature) && (lower->features & feature)) {\n\t\t\tnetdev_dbg(upper, \"Disabling feature %pNF on lower dev %s.\\n\",\n\t\t\t\t   &feature, lower->name);\n\t\t\tlower->wanted_features &= ~feature;\n\t\t\t__netdev_update_features(lower);\n\n\t\t\tif (unlikely(lower->features & feature))\n\t\t\t\tnetdev_WARN(upper, \"failed to disable %pNF on %s!\\n\",\n\t\t\t\t\t    &feature, lower->name);\n\t\t\telse\n\t\t\t\tnetdev_features_change(lower);\n\t\t}\n\t}\n}\n\nstatic netdev_features_t netdev_fix_features(struct net_device *dev,\n\tnetdev_features_t features)\n{\n\t/* Fix illegal checksum combinations */\n\tif ((features & NETIF_F_HW_CSUM) &&\n\t    (features & (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM))) {\n\t\tnetdev_warn(dev, \"mixed HW and IP checksum settings.\\n\");\n\t\tfeatures &= ~(NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t}\n\n\t/* TSO requires that SG is present as well. */\n\tif ((features & NETIF_F_ALL_TSO) && !(features & NETIF_F_SG)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO features since no SG feature.\\n\");\n\t\tfeatures &= ~NETIF_F_ALL_TSO;\n\t}\n\n\tif ((features & NETIF_F_TSO) && !(features & NETIF_F_HW_CSUM) &&\n\t\t\t\t\t!(features & NETIF_F_IP_CSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO features since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_TSO;\n\t\tfeatures &= ~NETIF_F_TSO_ECN;\n\t}\n\n\tif ((features & NETIF_F_TSO6) && !(features & NETIF_F_HW_CSUM) &&\n\t\t\t\t\t !(features & NETIF_F_IPV6_CSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO6 features since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_TSO6;\n\t}\n\n\t/* TSO with IPv4 ID mangling requires IPv4 TSO be enabled */\n\tif ((features & NETIF_F_TSO_MANGLEID) && !(features & NETIF_F_TSO))\n\t\tfeatures &= ~NETIF_F_TSO_MANGLEID;\n\n\t/* TSO ECN requires that TSO is present as well. */\n\tif ((features & NETIF_F_ALL_TSO) == NETIF_F_TSO_ECN)\n\t\tfeatures &= ~NETIF_F_TSO_ECN;\n\n\t/* Software GSO depends on SG. */\n\tif ((features & NETIF_F_GSO) && !(features & NETIF_F_SG)) {\n\t\tnetdev_dbg(dev, \"Dropping NETIF_F_GSO since no SG feature.\\n\");\n\t\tfeatures &= ~NETIF_F_GSO;\n\t}\n\n\t/* GSO partial features require GSO partial be set */\n\tif ((features & dev->gso_partial_features) &&\n\t    !(features & NETIF_F_GSO_PARTIAL)) {\n\t\tnetdev_dbg(dev,\n\t\t\t   \"Dropping partially supported GSO features since no GSO partial.\\n\");\n\t\tfeatures &= ~dev->gso_partial_features;\n\t}\n\n\tif (!(features & NETIF_F_RXCSUM)) {\n\t\t/* NETIF_F_GRO_HW implies doing RXCSUM since every packet\n\t\t * successfully merged by hardware must also have the\n\t\t * checksum verified by hardware.  If the user does not\n\t\t * want to enable RXCSUM, logically, we should disable GRO_HW.\n\t\t */\n\t\tif (features & NETIF_F_GRO_HW) {\n\t\t\tnetdev_dbg(dev, \"Dropping NETIF_F_GRO_HW since no RXCSUM feature.\\n\");\n\t\t\tfeatures &= ~NETIF_F_GRO_HW;\n\t\t}\n\t}\n\n\t/* LRO/HW-GRO features cannot be combined with RX-FCS */\n\tif (features & NETIF_F_RXFCS) {\n\t\tif (features & NETIF_F_LRO) {\n\t\t\tnetdev_dbg(dev, \"Dropping LRO feature since RX-FCS is requested.\\n\");\n\t\t\tfeatures &= ~NETIF_F_LRO;\n\t\t}\n\n\t\tif (features & NETIF_F_GRO_HW) {\n\t\t\tnetdev_dbg(dev, \"Dropping HW-GRO feature since RX-FCS is requested.\\n\");\n\t\t\tfeatures &= ~NETIF_F_GRO_HW;\n\t\t}\n\t}\n\n\tif (features & NETIF_F_HW_TLS_TX) {\n\t\tbool ip_csum = (features & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)) ==\n\t\t\t(NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM);\n\t\tbool hw_csum = features & NETIF_F_HW_CSUM;\n\n\t\tif (!ip_csum && !hw_csum) {\n\t\t\tnetdev_dbg(dev, \"Dropping TLS TX HW offload feature since no CSUM feature.\\n\");\n\t\t\tfeatures &= ~NETIF_F_HW_TLS_TX;\n\t\t}\n\t}\n\n\tif ((features & NETIF_F_HW_TLS_RX) && !(features & NETIF_F_RXCSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TLS RX HW offload feature since no RXCSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_HW_TLS_RX;\n\t}\n\n\treturn features;\n}\n\nint __netdev_update_features(struct net_device *dev)\n{\n\tstruct net_device *upper, *lower;\n\tnetdev_features_t features;\n\tstruct list_head *iter;\n\tint err = -1;\n\n\tASSERT_RTNL();\n\n\tfeatures = netdev_get_wanted_features(dev);\n\n\tif (dev->netdev_ops->ndo_fix_features)\n\t\tfeatures = dev->netdev_ops->ndo_fix_features(dev, features);\n\n\t/* driver might be less strict about feature dependencies */\n\tfeatures = netdev_fix_features(dev, features);\n\n\t/* some features can't be enabled if they're off on an upper device */\n\tnetdev_for_each_upper_dev_rcu(dev, upper, iter)\n\t\tfeatures = netdev_sync_upper_features(dev, upper, features);\n\n\tif (dev->features == features)\n\t\tgoto sync_lower;\n\n\tnetdev_dbg(dev, \"Features changed: %pNF -> %pNF\\n\",\n\t\t&dev->features, &features);\n\n\tif (dev->netdev_ops->ndo_set_features)\n\t\terr = dev->netdev_ops->ndo_set_features(dev, features);\n\telse\n\t\terr = 0;\n\n\tif (unlikely(err < 0)) {\n\t\tnetdev_err(dev,\n\t\t\t\"set_features() failed (%d); wanted %pNF, left %pNF\\n\",\n\t\t\terr, &features, &dev->features);\n\t\t/* return non-0 since some features might have changed and\n\t\t * it's better to fire a spurious notification than miss it\n\t\t */\n\t\treturn -1;\n\t}\n\nsync_lower:\n\t/* some features must be disabled on lower devices when disabled\n\t * on an upper device (think: bonding master or bridge)\n\t */\n\tnetdev_for_each_lower_dev(dev, lower, iter)\n\t\tnetdev_sync_lower_features(dev, lower, features);\n\n\tif (!err) {\n\t\tnetdev_features_t diff = features ^ dev->features;\n\n\t\tif (diff & NETIF_F_RX_UDP_TUNNEL_PORT) {\n\t\t\t/* udp_tunnel_{get,drop}_rx_info both need\n\t\t\t * NETIF_F_RX_UDP_TUNNEL_PORT enabled on the\n\t\t\t * device, or they won't do anything.\n\t\t\t * Thus we need to update dev->features\n\t\t\t * *before* calling udp_tunnel_get_rx_info,\n\t\t\t * but *after* calling udp_tunnel_drop_rx_info.\n\t\t\t */\n\t\t\tif (features & NETIF_F_RX_UDP_TUNNEL_PORT) {\n\t\t\t\tdev->features = features;\n\t\t\t\tudp_tunnel_get_rx_info(dev);\n\t\t\t} else {\n\t\t\t\tudp_tunnel_drop_rx_info(dev);\n\t\t\t}\n\t\t}\n\n\t\tif (diff & NETIF_F_HW_VLAN_CTAG_FILTER) {\n\t\t\tif (features & NETIF_F_HW_VLAN_CTAG_FILTER) {\n\t\t\t\tdev->features = features;\n\t\t\t\terr |= vlan_get_rx_ctag_filter_info(dev);\n\t\t\t} else {\n\t\t\t\tvlan_drop_rx_ctag_filter_info(dev);\n\t\t\t}\n\t\t}\n\n\t\tif (diff & NETIF_F_HW_VLAN_STAG_FILTER) {\n\t\t\tif (features & NETIF_F_HW_VLAN_STAG_FILTER) {\n\t\t\t\tdev->features = features;\n\t\t\t\terr |= vlan_get_rx_stag_filter_info(dev);\n\t\t\t} else {\n\t\t\t\tvlan_drop_rx_stag_filter_info(dev);\n\t\t\t}\n\t\t}\n\n\t\tdev->features = features;\n\t}\n\n\treturn err < 0 ? 0 : 1;\n}\n\n/**\n *\tnetdev_update_features - recalculate device features\n *\t@dev: the device to check\n *\n *\tRecalculate dev->features set and send notifications if it\n *\thas changed. Should be called after driver or hardware dependent\n *\tconditions might have changed that influence the features.\n */\nvoid netdev_update_features(struct net_device *dev)\n{\n\tif (__netdev_update_features(dev))\n\t\tnetdev_features_change(dev);\n}\nEXPORT_SYMBOL(netdev_update_features);\n\n/**\n *\tnetdev_change_features - recalculate device features\n *\t@dev: the device to check\n *\n *\tRecalculate dev->features set and send notifications even\n *\tif they have not changed. Should be called instead of\n *\tnetdev_update_features() if also dev->vlan_features might\n *\thave changed to allow the changes to be propagated to stacked\n *\tVLAN devices.\n */\nvoid netdev_change_features(struct net_device *dev)\n{\n\t__netdev_update_features(dev);\n\tnetdev_features_change(dev);\n}\nEXPORT_SYMBOL(netdev_change_features);\n\n/**\n *\tnetif_stacked_transfer_operstate -\ttransfer operstate\n *\t@rootdev: the root or lower level device to transfer state from\n *\t@dev: the device to transfer operstate to\n *\n *\tTransfer operational state from root to device. This is normally\n *\tcalled when a stacking relationship exists between the root\n *\tdevice and the device(a leaf device).\n */\nvoid netif_stacked_transfer_operstate(const struct net_device *rootdev,\n\t\t\t\t\tstruct net_device *dev)\n{\n\tif (rootdev->operstate == IF_OPER_DORMANT)\n\t\tnetif_dormant_on(dev);\n\telse\n\t\tnetif_dormant_off(dev);\n\n\tif (rootdev->operstate == IF_OPER_TESTING)\n\t\tnetif_testing_on(dev);\n\telse\n\t\tnetif_testing_off(dev);\n\n\tif (netif_carrier_ok(rootdev))\n\t\tnetif_carrier_on(dev);\n\telse\n\t\tnetif_carrier_off(dev);\n}\nEXPORT_SYMBOL(netif_stacked_transfer_operstate);\n\nstatic int netif_alloc_rx_queues(struct net_device *dev)\n{\n\tunsigned int i, count = dev->num_rx_queues;\n\tstruct netdev_rx_queue *rx;\n\tsize_t sz = count * sizeof(*rx);\n\tint err = 0;\n\n\tBUG_ON(count < 1);\n\n\trx = kvzalloc(sz, GFP_KERNEL | __GFP_RETRY_MAYFAIL);\n\tif (!rx)\n\t\treturn -ENOMEM;\n\n\tdev->_rx = rx;\n\n\tfor (i = 0; i < count; i++) {\n\t\trx[i].dev = dev;\n\n\t\t/* XDP RX-queue setup */\n\t\terr = xdp_rxq_info_reg(&rx[i].xdp_rxq, dev, i, 0);\n\t\tif (err < 0)\n\t\t\tgoto err_rxq_info;\n\t}\n\treturn 0;\n\nerr_rxq_info:\n\t/* Rollback successful reg's and free other resources */\n\twhile (i--)\n\t\txdp_rxq_info_unreg(&rx[i].xdp_rxq);\n\tkvfree(dev->_rx);\n\tdev->_rx = NULL;\n\treturn err;\n}\n\nstatic void netif_free_rx_queues(struct net_device *dev)\n{\n\tunsigned int i, count = dev->num_rx_queues;\n\n\t/* netif_alloc_rx_queues alloc failed, resources have been unreg'ed */\n\tif (!dev->_rx)\n\t\treturn;\n\n\tfor (i = 0; i < count; i++)\n\t\txdp_rxq_info_unreg(&dev->_rx[i].xdp_rxq);\n\n\tkvfree(dev->_rx);\n}\n\nstatic void netdev_init_one_queue(struct net_device *dev,\n\t\t\t\t  struct netdev_queue *queue, void *_unused)\n{\n\t/* Initialize queue lock */\n\tspin_lock_init(&queue->_xmit_lock);\n\tnetdev_set_xmit_lockdep_class(&queue->_xmit_lock, dev->type);\n\tqueue->xmit_lock_owner = -1;\n\tnetdev_queue_numa_node_write(queue, NUMA_NO_NODE);\n\tqueue->dev = dev;\n#ifdef CONFIG_BQL\n\tdql_init(&queue->dql, HZ);\n#endif\n}\n\nstatic void netif_free_tx_queues(struct net_device *dev)\n{\n\tkvfree(dev->_tx);\n}\n\nstatic int netif_alloc_netdev_queues(struct net_device *dev)\n{\n\tunsigned int count = dev->num_tx_queues;\n\tstruct netdev_queue *tx;\n\tsize_t sz = count * sizeof(*tx);\n\n\tif (count < 1 || count > 0xffff)\n\t\treturn -EINVAL;\n\n\ttx = kvzalloc(sz, GFP_KERNEL | __GFP_RETRY_MAYFAIL);\n\tif (!tx)\n\t\treturn -ENOMEM;\n\n\tdev->_tx = tx;\n\n\tnetdev_for_each_tx_queue(dev, netdev_init_one_queue, NULL);\n\tspin_lock_init(&dev->tx_global_lock);\n\n\treturn 0;\n}\n\nvoid netif_tx_stop_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\tnetif_tx_stop_queue(txq);\n\t}\n}\nEXPORT_SYMBOL(netif_tx_stop_all_queues);\n\n/**\n *\tregister_netdevice\t- register a network device\n *\t@dev: device to register\n *\n *\tTake a completed network device structure and add it to the kernel\n *\tinterfaces. A %NETDEV_REGISTER message is sent to the netdev notifier\n *\tchain. 0 is returned on success. A negative errno code is returned\n *\ton a failure to set up the device, or if the name is a duplicate.\n *\n *\tCallers must hold the rtnl semaphore. You may want\n *\tregister_netdev() instead of this.\n *\n *\tBUGS:\n *\tThe locking appears insufficient to guarantee two parallel registers\n *\twill not get the same name.\n */\n\nint register_netdevice(struct net_device *dev)\n{\n\tint ret;\n\tstruct net *net = dev_net(dev);\n\n\tBUILD_BUG_ON(sizeof(netdev_features_t) * BITS_PER_BYTE <\n\t\t     NETDEV_FEATURE_COUNT);\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tmight_sleep();\n\n\t/* When net_device's are persistent, this will be fatal. */\n\tBUG_ON(dev->reg_state != NETREG_UNINITIALIZED);\n\tBUG_ON(!net);\n\n\tret = ethtool_check_ops(dev->ethtool_ops);\n\tif (ret)\n\t\treturn ret;\n\n\tspin_lock_init(&dev->addr_list_lock);\n\tnetdev_set_addr_lockdep_class(dev);\n\n\tret = dev_get_valid_name(net, dev, dev->name);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = -ENOMEM;\n\tdev->name_node = netdev_name_node_head_alloc(dev);\n\tif (!dev->name_node)\n\t\tgoto out;\n\n\t/* Init, if this function is available */\n\tif (dev->netdev_ops->ndo_init) {\n\t\tret = dev->netdev_ops->ndo_init(dev);\n\t\tif (ret) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = -EIO;\n\t\t\tgoto err_free_name;\n\t\t}\n\t}\n\n\tif (((dev->hw_features | dev->features) &\n\t     NETIF_F_HW_VLAN_CTAG_FILTER) &&\n\t    (!dev->netdev_ops->ndo_vlan_rx_add_vid ||\n\t     !dev->netdev_ops->ndo_vlan_rx_kill_vid)) {\n\t\tnetdev_WARN(dev, \"Buggy VLAN acceleration in driver!\\n\");\n\t\tret = -EINVAL;\n\t\tgoto err_uninit;\n\t}\n\n\tret = -EBUSY;\n\tif (!dev->ifindex)\n\t\tdev->ifindex = dev_new_index(net);\n\telse if (__dev_get_by_index(net, dev->ifindex))\n\t\tgoto err_uninit;\n\n\t/* Transfer changeable features to wanted_features and enable\n\t * software offloads (GSO and GRO).\n\t */\n\tdev->hw_features |= (NETIF_F_SOFT_FEATURES | NETIF_F_SOFT_FEATURES_OFF);\n\tdev->features |= NETIF_F_SOFT_FEATURES;\n\n\tif (dev->udp_tunnel_nic_info) {\n\t\tdev->features |= NETIF_F_RX_UDP_TUNNEL_PORT;\n\t\tdev->hw_features |= NETIF_F_RX_UDP_TUNNEL_PORT;\n\t}\n\n\tdev->wanted_features = dev->features & dev->hw_features;\n\n\tif (!(dev->flags & IFF_LOOPBACK))\n\t\tdev->hw_features |= NETIF_F_NOCACHE_COPY;\n\n\t/* If IPv4 TCP segmentation offload is supported we should also\n\t * allow the device to enable segmenting the frame with the option\n\t * of ignoring a static IP ID value.  This doesn't enable the\n\t * feature itself but allows the user to enable it later.\n\t */\n\tif (dev->hw_features & NETIF_F_TSO)\n\t\tdev->hw_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->vlan_features & NETIF_F_TSO)\n\t\tdev->vlan_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->mpls_features & NETIF_F_TSO)\n\t\tdev->mpls_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->hw_enc_features & NETIF_F_TSO)\n\t\tdev->hw_enc_features |= NETIF_F_TSO_MANGLEID;\n\n\t/* Make NETIF_F_HIGHDMA inheritable to VLAN devices.\n\t */\n\tdev->vlan_features |= NETIF_F_HIGHDMA;\n\n\t/* Make NETIF_F_SG inheritable to tunnel devices.\n\t */\n\tdev->hw_enc_features |= NETIF_F_SG | NETIF_F_GSO_PARTIAL;\n\n\t/* Make NETIF_F_SG inheritable to MPLS.\n\t */\n\tdev->mpls_features |= NETIF_F_SG;\n\n\tret = call_netdevice_notifiers(NETDEV_POST_INIT, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto err_uninit;\n\n\tret = netdev_register_kobject(dev);\n\tif (ret) {\n\t\tdev->reg_state = NETREG_UNREGISTERED;\n\t\tgoto err_uninit;\n\t}\n\tdev->reg_state = NETREG_REGISTERED;\n\n\t__netdev_update_features(dev);\n\n\t/*\n\t *\tDefault initial state at registry is that the\n\t *\tdevice is present.\n\t */\n\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\n\tlinkwatch_init_dev(dev);\n\n\tdev_init_scheduler(dev);\n\tdev_hold(dev);\n\tlist_netdevice(dev);\n\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\n\t/* If the device has permanent device address, driver should\n\t * set dev_addr and also addr_assign_type should be set to\n\t * NET_ADDR_PERM (default value).\n\t */\n\tif (dev->addr_assign_type == NET_ADDR_PERM)\n\t\tmemcpy(dev->perm_addr, dev->dev_addr, dev->addr_len);\n\n\t/* Notify protocols, that a new device appeared. */\n\tret = call_netdevice_notifiers(NETDEV_REGISTER, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret) {\n\t\t/* Expect explicit free_netdev() on failure */\n\t\tdev->needs_free_netdev = false;\n\t\tunregister_netdevice_queue(dev, NULL);\n\t\tgoto out;\n\t}\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\tif (!dev->rtnl_link_ops ||\n\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U, GFP_KERNEL);\n\nout:\n\treturn ret;\n\nerr_uninit:\n\tif (dev->netdev_ops->ndo_uninit)\n\t\tdev->netdev_ops->ndo_uninit(dev);\n\tif (dev->priv_destructor)\n\t\tdev->priv_destructor(dev);\nerr_free_name:\n\tnetdev_name_node_free(dev->name_node);\n\tgoto out;\n}\nEXPORT_SYMBOL(register_netdevice);\n\n/**\n *\tinit_dummy_netdev\t- init a dummy network device for NAPI\n *\t@dev: device to init\n *\n *\tThis takes a network device structure and initialize the minimum\n *\tamount of fields so it can be used to schedule NAPI polls without\n *\tregistering a full blown interface. This is to be used by drivers\n *\tthat need to tie several hardware interfaces to a single NAPI\n *\tpoll scheduler due to HW limitations.\n */\nint init_dummy_netdev(struct net_device *dev)\n{\n\t/* Clear everything. Note we don't initialize spinlocks\n\t * are they aren't supposed to be taken by any of the\n\t * NAPI code and this dummy netdev is supposed to be\n\t * only ever used for NAPI polls\n\t */\n\tmemset(dev, 0, sizeof(struct net_device));\n\n\t/* make sure we BUG if trying to hit standard\n\t * register/unregister code path\n\t */\n\tdev->reg_state = NETREG_DUMMY;\n\n\t/* NAPI wants this */\n\tINIT_LIST_HEAD(&dev->napi_list);\n\n\t/* a dummy interface is started by default */\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\t/* napi_busy_loop stats accounting wants this */\n\tdev_net_set(dev, &init_net);\n\n\t/* Note : We dont allocate pcpu_refcnt for dummy devices,\n\t * because users of this 'device' dont need to change\n\t * its refcount.\n\t */\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(init_dummy_netdev);\n\n\n/**\n *\tregister_netdev\t- register a network device\n *\t@dev: device to register\n *\n *\tTake a completed network device structure and add it to the kernel\n *\tinterfaces. A %NETDEV_REGISTER message is sent to the netdev notifier\n *\tchain. 0 is returned on success. A negative errno code is returned\n *\ton a failure to set up the device, or if the name is a duplicate.\n *\n *\tThis is a wrapper around register_netdevice that takes the rtnl semaphore\n *\tand expands the device name if you passed a format string to\n *\talloc_netdev.\n */\nint register_netdev(struct net_device *dev)\n{\n\tint err;\n\n\tif (rtnl_lock_killable())\n\t\treturn -EINTR;\n\terr = register_netdevice(dev);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdev);\n\nint netdev_refcnt_read(const struct net_device *dev)\n{\n\tint i, refcnt = 0;\n\n\tfor_each_possible_cpu(i)\n\t\trefcnt += *per_cpu_ptr(dev->pcpu_refcnt, i);\n\treturn refcnt;\n}\nEXPORT_SYMBOL(netdev_refcnt_read);\n\n#define WAIT_REFS_MIN_MSECS 1\n#define WAIT_REFS_MAX_MSECS 250\n/**\n * netdev_wait_allrefs - wait until all references are gone.\n * @dev: target net_device\n *\n * This is called when unregistering network devices.\n *\n * Any protocol or device that holds a reference should register\n * for netdevice notification, and cleanup and put back the\n * reference if they receive an UNREGISTER event.\n * We can get stuck here if buggy protocols don't correctly\n * call dev_put.\n */\nstatic void netdev_wait_allrefs(struct net_device *dev)\n{\n\tunsigned long rebroadcast_time, warning_time;\n\tint wait = 0, refcnt;\n\n\tlinkwatch_forget_dev(dev);\n\n\trebroadcast_time = warning_time = jiffies;\n\trefcnt = netdev_refcnt_read(dev);\n\n\twhile (refcnt != 0) {\n\t\tif (time_after(jiffies, rebroadcast_time + 1 * HZ)) {\n\t\t\trtnl_lock();\n\n\t\t\t/* Rebroadcast unregister notification */\n\t\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\t\t__rtnl_unlock();\n\t\t\trcu_barrier();\n\t\t\trtnl_lock();\n\n\t\t\tif (test_bit(__LINK_STATE_LINKWATCH_PENDING,\n\t\t\t\t     &dev->state)) {\n\t\t\t\t/* We must not have linkwatch events\n\t\t\t\t * pending on unregister. If this\n\t\t\t\t * happens, we simply run the queue\n\t\t\t\t * unscheduled, resulting in a noop\n\t\t\t\t * for this device.\n\t\t\t\t */\n\t\t\t\tlinkwatch_run_queue();\n\t\t\t}\n\n\t\t\t__rtnl_unlock();\n\n\t\t\trebroadcast_time = jiffies;\n\t\t}\n\n\t\tif (!wait) {\n\t\t\trcu_barrier();\n\t\t\twait = WAIT_REFS_MIN_MSECS;\n\t\t} else {\n\t\t\tmsleep(wait);\n\t\t\twait = min(wait << 1, WAIT_REFS_MAX_MSECS);\n\t\t}\n\n\t\trefcnt = netdev_refcnt_read(dev);\n\n\t\tif (refcnt && time_after(jiffies, warning_time + 10 * HZ)) {\n\t\t\tpr_emerg(\"unregister_netdevice: waiting for %s to become free. Usage count = %d\\n\",\n\t\t\t\t dev->name, refcnt);\n\t\t\twarning_time = jiffies;\n\t\t}\n\t}\n}\n\n/* The sequence is:\n *\n *\trtnl_lock();\n *\t...\n *\tregister_netdevice(x1);\n *\tregister_netdevice(x2);\n *\t...\n *\tunregister_netdevice(y1);\n *\tunregister_netdevice(y2);\n *      ...\n *\trtnl_unlock();\n *\tfree_netdev(y1);\n *\tfree_netdev(y2);\n *\n * We are invoked by rtnl_unlock().\n * This allows us to deal with problems:\n * 1) We can delete sysfs objects which invoke hotplug\n *    without deadlocking with linkwatch via keventd.\n * 2) Since we run with the RTNL semaphore not held, we can sleep\n *    safely in order to wait for the netdev refcnt to drop to zero.\n *\n * We must not return until all unregister events added during\n * the interval the lock was held have been completed.\n */\nvoid netdev_run_todo(void)\n{\n\tstruct list_head list;\n#ifdef CONFIG_LOCKDEP\n\tstruct list_head unlink_list;\n\n\tlist_replace_init(&net_unlink_list, &unlink_list);\n\n\twhile (!list_empty(&unlink_list)) {\n\t\tstruct net_device *dev = list_first_entry(&unlink_list,\n\t\t\t\t\t\t\t  struct net_device,\n\t\t\t\t\t\t\t  unlink_list);\n\t\tlist_del_init(&dev->unlink_list);\n\t\tdev->nested_level = dev->lower_level - 1;\n\t}\n#endif\n\n\t/* Snapshot list, allow later requests */\n\tlist_replace_init(&net_todo_list, &list);\n\n\t__rtnl_unlock();\n\n\n\t/* Wait for rcu callbacks to finish before next phase */\n\tif (!list_empty(&list))\n\t\trcu_barrier();\n\n\twhile (!list_empty(&list)) {\n\t\tstruct net_device *dev\n\t\t\t= list_first_entry(&list, struct net_device, todo_list);\n\t\tlist_del(&dev->todo_list);\n\n\t\tif (unlikely(dev->reg_state != NETREG_UNREGISTERING)) {\n\t\t\tpr_err(\"network todo '%s' but state %d\\n\",\n\t\t\t       dev->name, dev->reg_state);\n\t\t\tdump_stack();\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev->reg_state = NETREG_UNREGISTERED;\n\n\t\tnetdev_wait_allrefs(dev);\n\n\t\t/* paranoia */\n\t\tBUG_ON(netdev_refcnt_read(dev));\n\t\tBUG_ON(!list_empty(&dev->ptype_all));\n\t\tBUG_ON(!list_empty(&dev->ptype_specific));\n\t\tWARN_ON(rcu_access_pointer(dev->ip_ptr));\n\t\tWARN_ON(rcu_access_pointer(dev->ip6_ptr));\n#if IS_ENABLED(CONFIG_DECNET)\n\t\tWARN_ON(dev->dn_ptr);\n#endif\n\t\tif (dev->priv_destructor)\n\t\t\tdev->priv_destructor(dev);\n\t\tif (dev->needs_free_netdev)\n\t\t\tfree_netdev(dev);\n\n\t\t/* Report a network device has been unregistered */\n\t\trtnl_lock();\n\t\tdev_net(dev)->dev_unreg_count--;\n\t\t__rtnl_unlock();\n\t\twake_up(&netdev_unregistering_wq);\n\n\t\t/* Free network device */\n\t\tkobject_put(&dev->dev.kobj);\n\t}\n}\n\n/* Convert net_device_stats to rtnl_link_stats64. rtnl_link_stats64 has\n * all the same fields in the same order as net_device_stats, with only\n * the type differing, but rtnl_link_stats64 may have additional fields\n * at the end for newer counters.\n */\nvoid netdev_stats_to_stats64(struct rtnl_link_stats64 *stats64,\n\t\t\t     const struct net_device_stats *netdev_stats)\n{\n#if BITS_PER_LONG == 64\n\tBUILD_BUG_ON(sizeof(*stats64) < sizeof(*netdev_stats));\n\tmemcpy(stats64, netdev_stats, sizeof(*netdev_stats));\n\t/* zero out counters that only exist in rtnl_link_stats64 */\n\tmemset((char *)stats64 + sizeof(*netdev_stats), 0,\n\t       sizeof(*stats64) - sizeof(*netdev_stats));\n#else\n\tsize_t i, n = sizeof(*netdev_stats) / sizeof(unsigned long);\n\tconst unsigned long *src = (const unsigned long *)netdev_stats;\n\tu64 *dst = (u64 *)stats64;\n\n\tBUILD_BUG_ON(n > sizeof(*stats64) / sizeof(u64));\n\tfor (i = 0; i < n; i++)\n\t\tdst[i] = src[i];\n\t/* zero out counters that only exist in rtnl_link_stats64 */\n\tmemset((char *)stats64 + n * sizeof(u64), 0,\n\t       sizeof(*stats64) - n * sizeof(u64));\n#endif\n}\nEXPORT_SYMBOL(netdev_stats_to_stats64);\n\n/**\n *\tdev_get_stats\t- get network device statistics\n *\t@dev: device to get statistics from\n *\t@storage: place to store stats\n *\n *\tGet network statistics from device. Return @storage.\n *\tThe device driver may provide its own method by setting\n *\tdev->netdev_ops->get_stats64 or dev->netdev_ops->get_stats;\n *\totherwise the internal statistics structure is used.\n */\nstruct rtnl_link_stats64 *dev_get_stats(struct net_device *dev,\n\t\t\t\t\tstruct rtnl_link_stats64 *storage)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_get_stats64) {\n\t\tmemset(storage, 0, sizeof(*storage));\n\t\tops->ndo_get_stats64(dev, storage);\n\t} else if (ops->ndo_get_stats) {\n\t\tnetdev_stats_to_stats64(storage, ops->ndo_get_stats(dev));\n\t} else {\n\t\tnetdev_stats_to_stats64(storage, &dev->stats);\n\t}\n\tstorage->rx_dropped += (unsigned long)atomic_long_read(&dev->rx_dropped);\n\tstorage->tx_dropped += (unsigned long)atomic_long_read(&dev->tx_dropped);\n\tstorage->rx_nohandler += (unsigned long)atomic_long_read(&dev->rx_nohandler);\n\treturn storage;\n}\nEXPORT_SYMBOL(dev_get_stats);\n\n/**\n *\tdev_fetch_sw_netstats - get per-cpu network device statistics\n *\t@s: place to store stats\n *\t@netstats: per-cpu network stats to read from\n *\n *\tRead per-cpu network statistics and populate the related fields in @s.\n */\nvoid dev_fetch_sw_netstats(struct rtnl_link_stats64 *s,\n\t\t\t   const struct pcpu_sw_netstats __percpu *netstats)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tconst struct pcpu_sw_netstats *stats;\n\t\tstruct pcpu_sw_netstats tmp;\n\t\tunsigned int start;\n\n\t\tstats = per_cpu_ptr(netstats, cpu);\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin_irq(&stats->syncp);\n\t\t\ttmp.rx_packets = stats->rx_packets;\n\t\t\ttmp.rx_bytes   = stats->rx_bytes;\n\t\t\ttmp.tx_packets = stats->tx_packets;\n\t\t\ttmp.tx_bytes   = stats->tx_bytes;\n\t\t} while (u64_stats_fetch_retry_irq(&stats->syncp, start));\n\n\t\ts->rx_packets += tmp.rx_packets;\n\t\ts->rx_bytes   += tmp.rx_bytes;\n\t\ts->tx_packets += tmp.tx_packets;\n\t\ts->tx_bytes   += tmp.tx_bytes;\n\t}\n}\nEXPORT_SYMBOL_GPL(dev_fetch_sw_netstats);\n\n/**\n *\tdev_get_tstats64 - ndo_get_stats64 implementation\n *\t@dev: device to get statistics from\n *\t@s: place to store stats\n *\n *\tPopulate @s from dev->stats and dev->tstats. Can be used as\n *\tndo_get_stats64() callback.\n */\nvoid dev_get_tstats64(struct net_device *dev, struct rtnl_link_stats64 *s)\n{\n\tnetdev_stats_to_stats64(s, &dev->stats);\n\tdev_fetch_sw_netstats(s, dev->tstats);\n}\nEXPORT_SYMBOL_GPL(dev_get_tstats64);\n\nstruct netdev_queue *dev_ingress_queue_create(struct net_device *dev)\n{\n\tstruct netdev_queue *queue = dev_ingress_queue(dev);\n\n#ifdef CONFIG_NET_CLS_ACT\n\tif (queue)\n\t\treturn queue;\n\tqueue = kzalloc(sizeof(*queue), GFP_KERNEL);\n\tif (!queue)\n\t\treturn NULL;\n\tnetdev_init_one_queue(dev, queue, NULL);\n\tRCU_INIT_POINTER(queue->qdisc, &noop_qdisc);\n\tqueue->qdisc_sleeping = &noop_qdisc;\n\trcu_assign_pointer(dev->ingress_queue, queue);\n#endif\n\treturn queue;\n}\n\nstatic const struct ethtool_ops default_ethtool_ops;\n\nvoid netdev_set_default_ethtool_ops(struct net_device *dev,\n\t\t\t\t    const struct ethtool_ops *ops)\n{\n\tif (dev->ethtool_ops == &default_ethtool_ops)\n\t\tdev->ethtool_ops = ops;\n}\nEXPORT_SYMBOL_GPL(netdev_set_default_ethtool_ops);\n\nvoid netdev_freemem(struct net_device *dev)\n{\n\tchar *addr = (char *)dev - dev->padded;\n\n\tkvfree(addr);\n}\n\n/**\n * alloc_netdev_mqs - allocate network device\n * @sizeof_priv: size of private data to allocate space for\n * @name: device name format string\n * @name_assign_type: origin of device name\n * @setup: callback to initialize device\n * @txqs: the number of TX subqueues to allocate\n * @rxqs: the number of RX subqueues to allocate\n *\n * Allocates a struct net_device with private data area for driver use\n * and performs basic initialization.  Also allocates subqueue structs\n * for each queue on the device.\n */\nstruct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,\n\t\tunsigned char name_assign_type,\n\t\tvoid (*setup)(struct net_device *),\n\t\tunsigned int txqs, unsigned int rxqs)\n{\n\tstruct net_device *dev;\n\tunsigned int alloc_size;\n\tstruct net_device *p;\n\n\tBUG_ON(strlen(name) >= sizeof(dev->name));\n\n\tif (txqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device with zero queues\\n\");\n\t\treturn NULL;\n\t}\n\n\tif (rxqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device with zero RX queues\\n\");\n\t\treturn NULL;\n\t}\n\n\talloc_size = sizeof(struct net_device);\n\tif (sizeof_priv) {\n\t\t/* ensure 32-byte alignment of private area */\n\t\talloc_size = ALIGN(alloc_size, NETDEV_ALIGN);\n\t\talloc_size += sizeof_priv;\n\t}\n\t/* ensure 32-byte alignment of whole construct */\n\talloc_size += NETDEV_ALIGN - 1;\n\n\tp = kvzalloc(alloc_size, GFP_KERNEL | __GFP_RETRY_MAYFAIL);\n\tif (!p)\n\t\treturn NULL;\n\n\tdev = PTR_ALIGN(p, NETDEV_ALIGN);\n\tdev->padded = (char *)dev - (char *)p;\n\n\tdev->pcpu_refcnt = alloc_percpu(int);\n\tif (!dev->pcpu_refcnt)\n\t\tgoto free_dev;\n\n\tif (dev_addr_init(dev))\n\t\tgoto free_pcpu;\n\n\tdev_mc_init(dev);\n\tdev_uc_init(dev);\n\n\tdev_net_set(dev, &init_net);\n\n\tdev->gso_max_size = GSO_MAX_SIZE;\n\tdev->gso_max_segs = GSO_MAX_SEGS;\n\tdev->upper_level = 1;\n\tdev->lower_level = 1;\n#ifdef CONFIG_LOCKDEP\n\tdev->nested_level = 0;\n\tINIT_LIST_HEAD(&dev->unlink_list);\n#endif\n\n\tINIT_LIST_HEAD(&dev->napi_list);\n\tINIT_LIST_HEAD(&dev->unreg_list);\n\tINIT_LIST_HEAD(&dev->close_list);\n\tINIT_LIST_HEAD(&dev->link_watch_list);\n\tINIT_LIST_HEAD(&dev->adj_list.upper);\n\tINIT_LIST_HEAD(&dev->adj_list.lower);\n\tINIT_LIST_HEAD(&dev->ptype_all);\n\tINIT_LIST_HEAD(&dev->ptype_specific);\n\tINIT_LIST_HEAD(&dev->net_notifier_list);\n#ifdef CONFIG_NET_SCHED\n\thash_init(dev->qdisc_hash);\n#endif\n\tdev->priv_flags = IFF_XMIT_DST_RELEASE | IFF_XMIT_DST_RELEASE_PERM;\n\tsetup(dev);\n\n\tif (!dev->tx_queue_len) {\n\t\tdev->priv_flags |= IFF_NO_QUEUE;\n\t\tdev->tx_queue_len = DEFAULT_TX_QUEUE_LEN;\n\t}\n\n\tdev->num_tx_queues = txqs;\n\tdev->real_num_tx_queues = txqs;\n\tif (netif_alloc_netdev_queues(dev))\n\t\tgoto free_all;\n\n\tdev->num_rx_queues = rxqs;\n\tdev->real_num_rx_queues = rxqs;\n\tif (netif_alloc_rx_queues(dev))\n\t\tgoto free_all;\n\n\tstrcpy(dev->name, name);\n\tdev->name_assign_type = name_assign_type;\n\tdev->group = INIT_NETDEV_GROUP;\n\tif (!dev->ethtool_ops)\n\t\tdev->ethtool_ops = &default_ethtool_ops;\n\n\tnf_hook_ingress_init(dev);\n\n\treturn dev;\n\nfree_all:\n\tfree_netdev(dev);\n\treturn NULL;\n\nfree_pcpu:\n\tfree_percpu(dev->pcpu_refcnt);\nfree_dev:\n\tnetdev_freemem(dev);\n\treturn NULL;\n}\nEXPORT_SYMBOL(alloc_netdev_mqs);\n\n/**\n * free_netdev - free network device\n * @dev: device\n *\n * This function does the last stage of destroying an allocated device\n * interface. The reference to the device object is released. If this\n * is the last reference then it will be freed.Must be called in process\n * context.\n */\nvoid free_netdev(struct net_device *dev)\n{\n\tstruct napi_struct *p, *n;\n\n\tmight_sleep();\n\n\t/* When called immediately after register_netdevice() failed the unwind\n\t * handling may still be dismantling the device. Handle that case by\n\t * deferring the free.\n\t */\n\tif (dev->reg_state == NETREG_UNREGISTERING) {\n\t\tASSERT_RTNL();\n\t\tdev->needs_free_netdev = true;\n\t\treturn;\n\t}\n\n\tnetif_free_tx_queues(dev);\n\tnetif_free_rx_queues(dev);\n\n\tkfree(rcu_dereference_protected(dev->ingress_queue, 1));\n\n\t/* Flush device addresses */\n\tdev_addr_flush(dev);\n\n\tlist_for_each_entry_safe(p, n, &dev->napi_list, dev_list)\n\t\tnetif_napi_del(p);\n\n\tfree_percpu(dev->pcpu_refcnt);\n\tdev->pcpu_refcnt = NULL;\n\tfree_percpu(dev->xdp_bulkq);\n\tdev->xdp_bulkq = NULL;\n\n\t/*  Compatibility with error handling in drivers */\n\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\tnetdev_freemem(dev);\n\t\treturn;\n\t}\n\n\tBUG_ON(dev->reg_state != NETREG_UNREGISTERED);\n\tdev->reg_state = NETREG_RELEASED;\n\n\t/* will free via device release */\n\tput_device(&dev->dev);\n}\nEXPORT_SYMBOL(free_netdev);\n\n/**\n *\tsynchronize_net -  Synchronize with packet receive processing\n *\n *\tWait for packets currently being received to be done.\n *\tDoes not block later packets from starting.\n */\nvoid synchronize_net(void)\n{\n\tmight_sleep();\n\tif (rtnl_is_locked())\n\t\tsynchronize_rcu_expedited();\n\telse\n\t\tsynchronize_rcu();\n}\nEXPORT_SYMBOL(synchronize_net);\n\n/**\n *\tunregister_netdevice_queue - remove device from the kernel\n *\t@dev: device\n *\t@head: list\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\tIf head not NULL, device is queued to be unregistered later.\n *\n *\tCallers must hold the rtnl semaphore.  You may want\n *\tunregister_netdev() instead of this.\n */\n\nvoid unregister_netdevice_queue(struct net_device *dev, struct list_head *head)\n{\n\tASSERT_RTNL();\n\n\tif (head) {\n\t\tlist_move_tail(&dev->unreg_list, head);\n\t} else {\n\t\tLIST_HEAD(single);\n\n\t\tlist_add(&dev->unreg_list, &single);\n\t\tunregister_netdevice_many(&single);\n\t}\n}\nEXPORT_SYMBOL(unregister_netdevice_queue);\n\n/**\n *\tunregister_netdevice_many - unregister many devices\n *\t@head: list of devices\n *\n *  Note: As most callers use a stack allocated list_head,\n *  we force a list_del() to make sure stack wont be corrupted later.\n */\nvoid unregister_netdevice_many(struct list_head *head)\n{\n\tstruct net_device *dev, *tmp;\n\tLIST_HEAD(close_head);\n\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tif (list_empty(head))\n\t\treturn;\n\n\tlist_for_each_entry_safe(dev, tmp, head, unreg_list) {\n\t\t/* Some devices call without registering\n\t\t * for initialization unwind. Remove those\n\t\t * devices and proceed with the remaining.\n\t\t */\n\t\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\t\tpr_debug(\"unregister_netdevice: device %s/%p never was registered\\n\",\n\t\t\t\t dev->name, dev);\n\n\t\t\tWARN_ON(1);\n\t\t\tlist_del(&dev->unreg_list);\n\t\t\tcontinue;\n\t\t}\n\t\tdev->dismantle = true;\n\t\tBUG_ON(dev->reg_state != NETREG_REGISTERED);\n\t}\n\n\t/* If device is running, close it first. */\n\tlist_for_each_entry(dev, head, unreg_list)\n\t\tlist_add_tail(&dev->close_list, &close_head);\n\tdev_close_many(&close_head, true);\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\t/* And unlink it from device chain. */\n\t\tunlist_netdevice(dev);\n\n\t\tdev->reg_state = NETREG_UNREGISTERING;\n\t}\n\tflush_all_backlogs();\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\tstruct sk_buff *skb = NULL;\n\n\t\t/* Shutdown queueing discipline. */\n\t\tdev_shutdown(dev);\n\n\t\tdev_xdp_uninstall(dev);\n\n\t\t/* Notify protocols, that we are about to destroy\n\t\t * this device. They should clean all the things.\n\t\t */\n\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\tif (!dev->rtnl_link_ops ||\n\t\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\t\tskb = rtmsg_ifinfo_build_skb(RTM_DELLINK, dev, ~0U, 0,\n\t\t\t\t\t\t     GFP_KERNEL, NULL, 0);\n\n\t\t/*\n\t\t *\tFlush the unicast and multicast chains\n\t\t */\n\t\tdev_uc_flush(dev);\n\t\tdev_mc_flush(dev);\n\n\t\tnetdev_name_node_alt_flush(dev);\n\t\tnetdev_name_node_free(dev->name_node);\n\n\t\tif (dev->netdev_ops->ndo_uninit)\n\t\t\tdev->netdev_ops->ndo_uninit(dev);\n\n\t\tif (skb)\n\t\t\trtmsg_ifinfo_send(skb, dev, GFP_KERNEL);\n\n\t\t/* Notifier chain MUST detach us all upper devices. */\n\t\tWARN_ON(netdev_has_any_upper_dev(dev));\n\t\tWARN_ON(netdev_has_any_lower_dev(dev));\n\n\t\t/* Remove entries from kobject tree */\n\t\tnetdev_unregister_kobject(dev);\n#ifdef CONFIG_XPS\n\t\t/* Remove XPS queueing entries */\n\t\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\t}\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\tdev_put(dev);\n\t\tnet_set_todo(dev);\n\t}\n\n\tlist_del(head);\n}\nEXPORT_SYMBOL(unregister_netdevice_many);\n\n/**\n *\tunregister_netdev - remove device from the kernel\n *\t@dev: device\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\n *\tThis is just a wrapper for unregister_netdevice that takes\n *\tthe rtnl semaphore.  In general you want to use this and not\n *\tunregister_netdevice.\n */\nvoid unregister_netdev(struct net_device *dev)\n{\n\trtnl_lock();\n\tunregister_netdevice(dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(unregister_netdev);\n\n/**\n *\tdev_change_net_namespace - move device to different nethost namespace\n *\t@dev: device\n *\t@net: network namespace\n *\t@pat: If not NULL name pattern to try if the current device name\n *\t      is already taken in the destination network namespace.\n *\n *\tThis function shuts down a device interface and moves it\n *\tto a new network namespace. On success 0 is returned, on\n *\ta failure a netagive errno code is returned.\n *\n *\tCallers must hold the rtnl semaphore.\n */\n\nint dev_change_net_namespace(struct net_device *dev, struct net *net, const char *pat)\n{\n\tstruct net *net_old = dev_net(dev);\n\tint err, new_nsid, new_ifindex;\n\n\tASSERT_RTNL();\n\n\t/* Don't allow namespace local devices to be moved. */\n\terr = -EINVAL;\n\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\tgoto out;\n\n\t/* Ensure the device has been registrered */\n\tif (dev->reg_state != NETREG_REGISTERED)\n\t\tgoto out;\n\n\t/* Get out if there is nothing todo */\n\terr = 0;\n\tif (net_eq(net_old, net))\n\t\tgoto out;\n\n\t/* Pick the destination device name, and ensure\n\t * we can use it in the destination network namespace.\n\t */\n\terr = -EEXIST;\n\tif (__dev_get_by_name(net, dev->name)) {\n\t\t/* We get here if we can't use the current device name */\n\t\tif (!pat)\n\t\t\tgoto out;\n\t\terr = dev_get_valid_name(net, dev, pat);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\t/*\n\t * And now a mini version of register_netdevice unregister_netdevice.\n\t */\n\n\t/* If device is running close it first. */\n\tdev_close(dev);\n\n\t/* And unlink it from device chain */\n\tunlist_netdevice(dev);\n\n\tsynchronize_net();\n\n\t/* Shutdown queueing discipline. */\n\tdev_shutdown(dev);\n\n\t/* Notify protocols, that we are about to destroy\n\t * this device. They should clean all the things.\n\t *\n\t * Note that dev->reg_state stays at NETREG_REGISTERED.\n\t * This is wanted because this way 8021q and macvlan know\n\t * the device is just moving and can keep their slaves up.\n\t */\n\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\trcu_barrier();\n\n\tnew_nsid = peernet2id_alloc(dev_net(dev), net, GFP_KERNEL);\n\t/* If there is an ifindex conflict assign a new one */\n\tif (__dev_get_by_index(net, dev->ifindex))\n\t\tnew_ifindex = dev_new_index(net);\n\telse\n\t\tnew_ifindex = dev->ifindex;\n\n\trtmsg_ifinfo_newnet(RTM_DELLINK, dev, ~0U, GFP_KERNEL, &new_nsid,\n\t\t\t    new_ifindex);\n\n\t/*\n\t *\tFlush the unicast and multicast chains\n\t */\n\tdev_uc_flush(dev);\n\tdev_mc_flush(dev);\n\n\t/* Send a netdev-removed uevent to the old namespace */\n\tkobject_uevent(&dev->dev.kobj, KOBJ_REMOVE);\n\tnetdev_adjacent_del_links(dev);\n\n\t/* Move per-net netdevice notifiers that are following the netdevice */\n\tmove_netdevice_notifiers_dev_net(dev, net);\n\n\t/* Actually switch the network namespace */\n\tdev_net_set(dev, net);\n\tdev->ifindex = new_ifindex;\n\n\t/* Send a netdev-add uevent to the new namespace */\n\tkobject_uevent(&dev->dev.kobj, KOBJ_ADD);\n\tnetdev_adjacent_add_links(dev);\n\n\t/* Fixup kobjects */\n\terr = device_rename(&dev->dev, dev->name);\n\tWARN_ON(err);\n\n\t/* Adapt owner in case owning user namespace of target network\n\t * namespace is different from the original one.\n\t */\n\terr = netdev_change_owner(dev, net_old, net);\n\tWARN_ON(err);\n\n\t/* Add the device back in the hashes */\n\tlist_netdevice(dev);\n\n\t/* Notify protocols, that a new device appeared. */\n\tcall_netdevice_notifiers(NETDEV_REGISTER, dev);\n\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U, GFP_KERNEL);\n\n\tsynchronize_net();\n\terr = 0;\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(dev_change_net_namespace);\n\nstatic int dev_cpu_dead(unsigned int oldcpu)\n{\n\tstruct sk_buff **list_skb;\n\tstruct sk_buff *skb;\n\tunsigned int cpu;\n\tstruct softnet_data *sd, *oldsd, *remsd = NULL;\n\n\tlocal_irq_disable();\n\tcpu = smp_processor_id();\n\tsd = &per_cpu(softnet_data, cpu);\n\toldsd = &per_cpu(softnet_data, oldcpu);\n\n\t/* Find end of our completion_queue. */\n\tlist_skb = &sd->completion_queue;\n\twhile (*list_skb)\n\t\tlist_skb = &(*list_skb)->next;\n\t/* Append completion queue from offline CPU. */\n\t*list_skb = oldsd->completion_queue;\n\toldsd->completion_queue = NULL;\n\n\t/* Append output queue from offline CPU. */\n\tif (oldsd->output_queue) {\n\t\t*sd->output_queue_tailp = oldsd->output_queue;\n\t\tsd->output_queue_tailp = oldsd->output_queue_tailp;\n\t\toldsd->output_queue = NULL;\n\t\toldsd->output_queue_tailp = &oldsd->output_queue;\n\t}\n\t/* Append NAPI poll list from offline CPU, with one exception :\n\t * process_backlog() must be called by cpu owning percpu backlog.\n\t * We properly handle process_queue & input_pkt_queue later.\n\t */\n\twhile (!list_empty(&oldsd->poll_list)) {\n\t\tstruct napi_struct *napi = list_first_entry(&oldsd->poll_list,\n\t\t\t\t\t\t\t    struct napi_struct,\n\t\t\t\t\t\t\t    poll_list);\n\n\t\tlist_del_init(&napi->poll_list);\n\t\tif (napi->poll == process_backlog)\n\t\t\tnapi->state = 0;\n\t\telse\n\t\t\t____napi_schedule(sd, napi);\n\t}\n\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_enable();\n\n#ifdef CONFIG_RPS\n\tremsd = oldsd->rps_ipi_list;\n\toldsd->rps_ipi_list = NULL;\n#endif\n\t/* send out pending IPI's on offline CPU */\n\tnet_rps_send_ipi(remsd);\n\n\t/* Process offline CPU's input_pkt_queue */\n\twhile ((skb = __skb_dequeue(&oldsd->process_queue))) {\n\t\tnetif_rx_ni(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\twhile ((skb = skb_dequeue(&oldsd->input_pkt_queue))) {\n\t\tnetif_rx_ni(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\n\treturn 0;\n}\n\n/**\n *\tnetdev_increment_features - increment feature set by one\n *\t@all: current feature set\n *\t@one: new feature set\n *\t@mask: mask feature set\n *\n *\tComputes a new feature set after adding a device with feature set\n *\t@one to the master device with current feature set @all.  Will not\n *\tenable anything that is off in @mask. Returns the new feature set.\n */\nnetdev_features_t netdev_increment_features(netdev_features_t all,\n\tnetdev_features_t one, netdev_features_t mask)\n{\n\tif (mask & NETIF_F_HW_CSUM)\n\t\tmask |= NETIF_F_CSUM_MASK;\n\tmask |= NETIF_F_VLAN_CHALLENGED;\n\n\tall |= one & (NETIF_F_ONE_FOR_ALL | NETIF_F_CSUM_MASK) & mask;\n\tall &= one | ~NETIF_F_ALL_FOR_ALL;\n\n\t/* If one device supports hw checksumming, set for all. */\n\tif (all & NETIF_F_HW_CSUM)\n\t\tall &= ~(NETIF_F_CSUM_MASK & ~NETIF_F_HW_CSUM);\n\n\treturn all;\n}\nEXPORT_SYMBOL(netdev_increment_features);\n\nstatic struct hlist_head * __net_init netdev_create_hash(void)\n{\n\tint i;\n\tstruct hlist_head *hash;\n\n\thash = kmalloc_array(NETDEV_HASHENTRIES, sizeof(*hash), GFP_KERNEL);\n\tif (hash != NULL)\n\t\tfor (i = 0; i < NETDEV_HASHENTRIES; i++)\n\t\t\tINIT_HLIST_HEAD(&hash[i]);\n\n\treturn hash;\n}\n\n/* Initialize per network namespace state */\nstatic int __net_init netdev_init(struct net *net)\n{\n\tBUILD_BUG_ON(GRO_HASH_BUCKETS >\n\t\t     8 * sizeof_field(struct napi_struct, gro_bitmask));\n\n\tif (net != &init_net)\n\t\tINIT_LIST_HEAD(&net->dev_base_head);\n\n\tnet->dev_name_head = netdev_create_hash();\n\tif (net->dev_name_head == NULL)\n\t\tgoto err_name;\n\n\tnet->dev_index_head = netdev_create_hash();\n\tif (net->dev_index_head == NULL)\n\t\tgoto err_idx;\n\n\tRAW_INIT_NOTIFIER_HEAD(&net->netdev_chain);\n\n\treturn 0;\n\nerr_idx:\n\tkfree(net->dev_name_head);\nerr_name:\n\treturn -ENOMEM;\n}\n\n/**\n *\tnetdev_drivername - network driver for the device\n *\t@dev: network device\n *\n *\tDetermine network driver for device.\n */\nconst char *netdev_drivername(const struct net_device *dev)\n{\n\tconst struct device_driver *driver;\n\tconst struct device *parent;\n\tconst char *empty = \"\";\n\n\tparent = dev->dev.parent;\n\tif (!parent)\n\t\treturn empty;\n\n\tdriver = parent->driver;\n\tif (driver && driver->name)\n\t\treturn driver->name;\n\treturn empty;\n}\n\nstatic void __netdev_printk(const char *level, const struct net_device *dev,\n\t\t\t    struct va_format *vaf)\n{\n\tif (dev && dev->dev.parent) {\n\t\tdev_printk_emit(level[1] - '0',\n\t\t\t\tdev->dev.parent,\n\t\t\t\t\"%s %s %s%s: %pV\",\n\t\t\t\tdev_driver_string(dev->dev.parent),\n\t\t\t\tdev_name(dev->dev.parent),\n\t\t\t\tnetdev_name(dev), netdev_reg_state(dev),\n\t\t\t\tvaf);\n\t} else if (dev) {\n\t\tprintk(\"%s%s%s: %pV\",\n\t\t       level, netdev_name(dev), netdev_reg_state(dev), vaf);\n\t} else {\n\t\tprintk(\"%s(NULL net_device): %pV\", level, vaf);\n\t}\n}\n\nvoid netdev_printk(const char *level, const struct net_device *dev,\n\t\t   const char *format, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tva_start(args, format);\n\n\tvaf.fmt = format;\n\tvaf.va = &args;\n\n\t__netdev_printk(level, dev, &vaf);\n\n\tva_end(args);\n}\nEXPORT_SYMBOL(netdev_printk);\n\n#define define_netdev_printk_level(func, level)\t\t\t\\\nvoid func(const struct net_device *dev, const char *fmt, ...)\t\\\n{\t\t\t\t\t\t\t\t\\\n\tstruct va_format vaf;\t\t\t\t\t\\\n\tva_list args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_start(args, fmt);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tvaf.fmt = fmt;\t\t\t\t\t\t\\\n\tvaf.va = &args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\t__netdev_printk(level, dev, &vaf);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_end(args);\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\\\nEXPORT_SYMBOL(func);\n\ndefine_netdev_printk_level(netdev_emerg, KERN_EMERG);\ndefine_netdev_printk_level(netdev_alert, KERN_ALERT);\ndefine_netdev_printk_level(netdev_crit, KERN_CRIT);\ndefine_netdev_printk_level(netdev_err, KERN_ERR);\ndefine_netdev_printk_level(netdev_warn, KERN_WARNING);\ndefine_netdev_printk_level(netdev_notice, KERN_NOTICE);\ndefine_netdev_printk_level(netdev_info, KERN_INFO);\n\nstatic void __net_exit netdev_exit(struct net *net)\n{\n\tkfree(net->dev_name_head);\n\tkfree(net->dev_index_head);\n\tif (net != &init_net)\n\t\tWARN_ON_ONCE(!list_empty(&net->dev_base_head));\n}\n\nstatic struct pernet_operations __net_initdata netdev_net_ops = {\n\t.init = netdev_init,\n\t.exit = netdev_exit,\n};\n\nstatic void __net_exit default_device_exit(struct net *net)\n{\n\tstruct net_device *dev, *aux;\n\t/*\n\t * Push all migratable network devices back to the\n\t * initial network namespace\n\t */\n\trtnl_lock();\n\tfor_each_netdev_safe(net, dev, aux) {\n\t\tint err;\n\t\tchar fb_name[IFNAMSIZ];\n\n\t\t/* Ignore unmoveable devices (i.e. loopback) */\n\t\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\t\tcontinue;\n\n\t\t/* Leave virtual devices for the generic cleanup */\n\t\tif (dev->rtnl_link_ops)\n\t\t\tcontinue;\n\n\t\t/* Push remaining network devices to init_net */\n\t\tsnprintf(fb_name, IFNAMSIZ, \"dev%d\", dev->ifindex);\n\t\tif (__dev_get_by_name(&init_net, fb_name))\n\t\t\tsnprintf(fb_name, IFNAMSIZ, \"dev%%d\");\n\t\terr = dev_change_net_namespace(dev, &init_net, fb_name);\n\t\tif (err) {\n\t\t\tpr_emerg(\"%s: failed to move %s to init_net: %d\\n\",\n\t\t\t\t __func__, dev->name, err);\n\t\t\tBUG();\n\t\t}\n\t}\n\trtnl_unlock();\n}\n\nstatic void __net_exit rtnl_lock_unregistering(struct list_head *net_list)\n{\n\t/* Return with the rtnl_lock held when there are no network\n\t * devices unregistering in any network namespace in net_list.\n\t */\n\tstruct net *net;\n\tbool unregistering;\n\tDEFINE_WAIT_FUNC(wait, woken_wake_function);\n\n\tadd_wait_queue(&netdev_unregistering_wq, &wait);\n\tfor (;;) {\n\t\tunregistering = false;\n\t\trtnl_lock();\n\t\tlist_for_each_entry(net, net_list, exit_list) {\n\t\t\tif (net->dev_unreg_count > 0) {\n\t\t\t\tunregistering = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!unregistering)\n\t\t\tbreak;\n\t\t__rtnl_unlock();\n\n\t\twait_woken(&wait, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);\n\t}\n\tremove_wait_queue(&netdev_unregistering_wq, &wait);\n}\n\nstatic void __net_exit default_device_exit_batch(struct list_head *net_list)\n{\n\t/* At exit all network devices most be removed from a network\n\t * namespace.  Do this in the reverse order of registration.\n\t * Do this across as many network namespaces as possible to\n\t * improve batching efficiency.\n\t */\n\tstruct net_device *dev;\n\tstruct net *net;\n\tLIST_HEAD(dev_kill_list);\n\n\t/* To prevent network device cleanup code from dereferencing\n\t * loopback devices or network devices that have been freed\n\t * wait here for all pending unregistrations to complete,\n\t * before unregistring the loopback device and allowing the\n\t * network namespace be freed.\n\t *\n\t * The netdev todo list containing all network devices\n\t * unregistrations that happen in default_device_exit_batch\n\t * will run in the rtnl_unlock() at the end of\n\t * default_device_exit_batch.\n\t */\n\trtnl_lock_unregistering(net_list);\n\tlist_for_each_entry(net, net_list, exit_list) {\n\t\tfor_each_netdev_reverse(net, dev) {\n\t\t\tif (dev->rtnl_link_ops && dev->rtnl_link_ops->dellink)\n\t\t\t\tdev->rtnl_link_ops->dellink(dev, &dev_kill_list);\n\t\t\telse\n\t\t\t\tunregister_netdevice_queue(dev, &dev_kill_list);\n\t\t}\n\t}\n\tunregister_netdevice_many(&dev_kill_list);\n\trtnl_unlock();\n}\n\nstatic struct pernet_operations __net_initdata default_device_ops = {\n\t.exit = default_device_exit,\n\t.exit_batch = default_device_exit_batch,\n};\n\n/*\n *\tInitialize the DEV module. At boot time this walks the device list and\n *\tunhooks any devices that fail to initialise (normally hardware not\n *\tpresent) and leaves us with a valid list of present and active devices.\n *\n */\n\n/*\n *       This is called single threaded during boot, so no need\n *       to take the rtnl semaphore.\n */\nstatic int __init net_dev_init(void)\n{\n\tint i, rc = -ENOMEM;\n\n\tBUG_ON(!dev_boot_phase);\n\n\tif (dev_proc_init())\n\t\tgoto out;\n\n\tif (netdev_kobject_init())\n\t\tgoto out;\n\n\tINIT_LIST_HEAD(&ptype_all);\n\tfor (i = 0; i < PTYPE_HASH_SIZE; i++)\n\t\tINIT_LIST_HEAD(&ptype_base[i]);\n\n\tINIT_LIST_HEAD(&offload_base);\n\n\tif (register_pernet_subsys(&netdev_net_ops))\n\t\tgoto out;\n\n\t/*\n\t *\tInitialise the packet receive queues.\n\t */\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct work_struct *flush = per_cpu_ptr(&flush_works, i);\n\t\tstruct softnet_data *sd = &per_cpu(softnet_data, i);\n\n\t\tINIT_WORK(flush, flush_backlog);\n\n\t\tskb_queue_head_init(&sd->input_pkt_queue);\n\t\tskb_queue_head_init(&sd->process_queue);\n#ifdef CONFIG_XFRM_OFFLOAD\n\t\tskb_queue_head_init(&sd->xfrm_backlog);\n#endif\n\t\tINIT_LIST_HEAD(&sd->poll_list);\n\t\tsd->output_queue_tailp = &sd->output_queue;\n#ifdef CONFIG_RPS\n\t\tINIT_CSD(&sd->csd, rps_trigger_softirq, sd);\n\t\tsd->cpu = i;\n#endif\n\n\t\tinit_gro_hash(&sd->backlog);\n\t\tsd->backlog.poll = process_backlog;\n\t\tsd->backlog.weight = weight_p;\n\t}\n\n\tdev_boot_phase = 0;\n\n\t/* The loopback device is special if any other network devices\n\t * is present in a network namespace the loopback device must\n\t * be present. Since we now dynamically allocate and free the\n\t * loopback device ensure this invariant is maintained by\n\t * keeping the loopback device as the first device on the\n\t * list of network devices.  Ensuring the loopback devices\n\t * is the first device that appears and the last network device\n\t * that disappears.\n\t */\n\tif (register_pernet_device(&loopback_net_ops))\n\t\tgoto out;\n\n\tif (register_pernet_device(&default_device_ops))\n\t\tgoto out;\n\n\topen_softirq(NET_TX_SOFTIRQ, net_tx_action);\n\topen_softirq(NET_RX_SOFTIRQ, net_rx_action);\n\n\trc = cpuhp_setup_state_nocalls(CPUHP_NET_DEV_DEAD, \"net/dev:dead\",\n\t\t\t\t       NULL, dev_cpu_dead);\n\tWARN_ON(rc < 0);\n\trc = 0;\nout:\n\treturn rc;\n}\n\nsubsys_initcall(net_dev_init);\n"}, "2": {"id": 2, "path": "/src/include/linux/list.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_LIST_H\n#define _LINUX_LIST_H\n\n#include <linux/types.h>\n#include <linux/stddef.h>\n#include <linux/poison.h>\n#include <linux/const.h>\n#include <linux/kernel.h>\n\n/*\n * Circular doubly linked list implementation.\n *\n * Some of the internal functions (\"__xxx\") are useful when\n * manipulating whole lists rather than single entries, as\n * sometimes we already know the next/prev entries and we can\n * generate better code by using them directly rather than\n * using the generic single-entry routines.\n */\n\n#define LIST_HEAD_INIT(name) { &(name), &(name) }\n\n#define LIST_HEAD(name) \\\n\tstruct list_head name = LIST_HEAD_INIT(name)\n\n/**\n * INIT_LIST_HEAD - Initialize a list_head structure\n * @list: list_head structure to be initialized.\n *\n * Initializes the list_head to point to itself.  If it is a list header,\n * the result is an empty list.\n */\nstatic inline void INIT_LIST_HEAD(struct list_head *list)\n{\n\tWRITE_ONCE(list->next, list);\n\tlist->prev = list;\n}\n\n#ifdef CONFIG_DEBUG_LIST\nextern bool __list_add_valid(struct list_head *new,\n\t\t\t      struct list_head *prev,\n\t\t\t      struct list_head *next);\nextern bool __list_del_entry_valid(struct list_head *entry);\n#else\nstatic inline bool __list_add_valid(struct list_head *new,\n\t\t\t\tstruct list_head *prev,\n\t\t\t\tstruct list_head *next)\n{\n\treturn true;\n}\nstatic inline bool __list_del_entry_valid(struct list_head *entry)\n{\n\treturn true;\n}\n#endif\n\n/*\n * Insert a new entry between two known consecutive entries.\n *\n * This is only for internal list manipulation where we know\n * the prev/next entries already!\n */\nstatic inline void __list_add(struct list_head *new,\n\t\t\t      struct list_head *prev,\n\t\t\t      struct list_head *next)\n{\n\tif (!__list_add_valid(new, prev, next))\n\t\treturn;\n\n\tnext->prev = new;\n\tnew->next = next;\n\tnew->prev = prev;\n\tWRITE_ONCE(prev->next, new);\n}\n\n/**\n * list_add - add a new entry\n * @new: new entry to be added\n * @head: list head to add it after\n *\n * Insert a new entry after the specified head.\n * This is good for implementing stacks.\n */\nstatic inline void list_add(struct list_head *new, struct list_head *head)\n{\n\t__list_add(new, head, head->next);\n}\n\n\n/**\n * list_add_tail - add a new entry\n * @new: new entry to be added\n * @head: list head to add it before\n *\n * Insert a new entry before the specified head.\n * This is useful for implementing queues.\n */\nstatic inline void list_add_tail(struct list_head *new, struct list_head *head)\n{\n\t__list_add(new, head->prev, head);\n}\n\n/*\n * Delete a list entry by making the prev/next entries\n * point to each other.\n *\n * This is only for internal list manipulation where we know\n * the prev/next entries already!\n */\nstatic inline void __list_del(struct list_head * prev, struct list_head * next)\n{\n\tnext->prev = prev;\n\tWRITE_ONCE(prev->next, next);\n}\n\n/*\n * Delete a list entry and clear the 'prev' pointer.\n *\n * This is a special-purpose list clearing method used in the networking code\n * for lists allocated as per-cpu, where we don't want to incur the extra\n * WRITE_ONCE() overhead of a regular list_del_init(). The code that uses this\n * needs to check the node 'prev' pointer instead of calling list_empty().\n */\nstatic inline void __list_del_clearprev(struct list_head *entry)\n{\n\t__list_del(entry->prev, entry->next);\n\tentry->prev = NULL;\n}\n\nstatic inline void __list_del_entry(struct list_head *entry)\n{\n\tif (!__list_del_entry_valid(entry))\n\t\treturn;\n\n\t__list_del(entry->prev, entry->next);\n}\n\n/**\n * list_del - deletes entry from list.\n * @entry: the element to delete from the list.\n * Note: list_empty() on entry does not return true after this, the entry is\n * in an undefined state.\n */\nstatic inline void list_del(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tentry->next = LIST_POISON1;\n\tentry->prev = LIST_POISON2;\n}\n\n/**\n * list_replace - replace old entry by new one\n * @old : the element to be replaced\n * @new : the new element to insert\n *\n * If @old was empty, it will be overwritten.\n */\nstatic inline void list_replace(struct list_head *old,\n\t\t\t\tstruct list_head *new)\n{\n\tnew->next = old->next;\n\tnew->next->prev = new;\n\tnew->prev = old->prev;\n\tnew->prev->next = new;\n}\n\n/**\n * list_replace_init - replace old entry by new one and initialize the old one\n * @old : the element to be replaced\n * @new : the new element to insert\n *\n * If @old was empty, it will be overwritten.\n */\nstatic inline void list_replace_init(struct list_head *old,\n\t\t\t\t     struct list_head *new)\n{\n\tlist_replace(old, new);\n\tINIT_LIST_HEAD(old);\n}\n\n/**\n * list_swap - replace entry1 with entry2 and re-add entry1 at entry2's position\n * @entry1: the location to place entry2\n * @entry2: the location to place entry1\n */\nstatic inline void list_swap(struct list_head *entry1,\n\t\t\t     struct list_head *entry2)\n{\n\tstruct list_head *pos = entry2->prev;\n\n\tlist_del(entry2);\n\tlist_replace(entry1, entry2);\n\tif (pos == entry1)\n\t\tpos = entry2;\n\tlist_add(entry1, pos);\n}\n\n/**\n * list_del_init - deletes entry from list and reinitialize it.\n * @entry: the element to delete from the list.\n */\nstatic inline void list_del_init(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tINIT_LIST_HEAD(entry);\n}\n\n/**\n * list_move - delete from one list and add as another's head\n * @list: the entry to move\n * @head: the head that will precede our entry\n */\nstatic inline void list_move(struct list_head *list, struct list_head *head)\n{\n\t__list_del_entry(list);\n\tlist_add(list, head);\n}\n\n/**\n * list_move_tail - delete from one list and add as another's tail\n * @list: the entry to move\n * @head: the head that will follow our entry\n */\nstatic inline void list_move_tail(struct list_head *list,\n\t\t\t\t  struct list_head *head)\n{\n\t__list_del_entry(list);\n\tlist_add_tail(list, head);\n}\n\n/**\n * list_bulk_move_tail - move a subsection of a list to its tail\n * @head: the head that will follow our entry\n * @first: first entry to move\n * @last: last entry to move, can be the same as first\n *\n * Move all entries between @first and including @last before @head.\n * All three entries must belong to the same linked list.\n */\nstatic inline void list_bulk_move_tail(struct list_head *head,\n\t\t\t\t       struct list_head *first,\n\t\t\t\t       struct list_head *last)\n{\n\tfirst->prev->next = last->next;\n\tlast->next->prev = first->prev;\n\n\thead->prev->next = first;\n\tfirst->prev = head->prev;\n\n\tlast->next = head;\n\thead->prev = last;\n}\n\n/**\n * list_is_first -- tests whether @list is the first entry in list @head\n * @list: the entry to test\n * @head: the head of the list\n */\nstatic inline int list_is_first(const struct list_head *list,\n\t\t\t\t\tconst struct list_head *head)\n{\n\treturn list->prev == head;\n}\n\n/**\n * list_is_last - tests whether @list is the last entry in list @head\n * @list: the entry to test\n * @head: the head of the list\n */\nstatic inline int list_is_last(const struct list_head *list,\n\t\t\t\tconst struct list_head *head)\n{\n\treturn list->next == head;\n}\n\n/**\n * list_empty - tests whether a list is empty\n * @head: the list to test.\n */\nstatic inline int list_empty(const struct list_head *head)\n{\n\treturn READ_ONCE(head->next) == head;\n}\n\n/**\n * list_del_init_careful - deletes entry from list and reinitialize it.\n * @entry: the element to delete from the list.\n *\n * This is the same as list_del_init(), except designed to be used\n * together with list_empty_careful() in a way to guarantee ordering\n * of other memory operations.\n *\n * Any memory operations done before a list_del_init_careful() are\n * guaranteed to be visible after a list_empty_careful() test.\n */\nstatic inline void list_del_init_careful(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tentry->prev = entry;\n\tsmp_store_release(&entry->next, entry);\n}\n\n/**\n * list_empty_careful - tests whether a list is empty and not being modified\n * @head: the list to test\n *\n * Description:\n * tests whether a list is empty _and_ checks that no other CPU might be\n * in the process of modifying either member (next or prev)\n *\n * NOTE: using list_empty_careful() without synchronization\n * can only be safe if the only activity that can happen\n * to the list entry is list_del_init(). Eg. it cannot be used\n * if another CPU could re-list_add() it.\n */\nstatic inline int list_empty_careful(const struct list_head *head)\n{\n\tstruct list_head *next = smp_load_acquire(&head->next);\n\treturn (next == head) && (next == head->prev);\n}\n\n/**\n * list_rotate_left - rotate the list to the left\n * @head: the head of the list\n */\nstatic inline void list_rotate_left(struct list_head *head)\n{\n\tstruct list_head *first;\n\n\tif (!list_empty(head)) {\n\t\tfirst = head->next;\n\t\tlist_move_tail(first, head);\n\t}\n}\n\n/**\n * list_rotate_to_front() - Rotate list to specific item.\n * @list: The desired new front of the list.\n * @head: The head of the list.\n *\n * Rotates list so that @list becomes the new front of the list.\n */\nstatic inline void list_rotate_to_front(struct list_head *list,\n\t\t\t\t\tstruct list_head *head)\n{\n\t/*\n\t * Deletes the list head from the list denoted by @head and\n\t * places it as the tail of @list, this effectively rotates the\n\t * list so that @list is at the front.\n\t */\n\tlist_move_tail(head, list);\n}\n\n/**\n * list_is_singular - tests whether a list has just one entry.\n * @head: the list to test.\n */\nstatic inline int list_is_singular(const struct list_head *head)\n{\n\treturn !list_empty(head) && (head->next == head->prev);\n}\n\nstatic inline void __list_cut_position(struct list_head *list,\n\t\tstruct list_head *head, struct list_head *entry)\n{\n\tstruct list_head *new_first = entry->next;\n\tlist->next = head->next;\n\tlist->next->prev = list;\n\tlist->prev = entry;\n\tentry->next = list;\n\thead->next = new_first;\n\tnew_first->prev = head;\n}\n\n/**\n * list_cut_position - cut a list into two\n * @list: a new list to add all removed entries\n * @head: a list with entries\n * @entry: an entry within head, could be the head itself\n *\tand if so we won't cut the list\n *\n * This helper moves the initial part of @head, up to and\n * including @entry, from @head to @list. You should\n * pass on @entry an element you know is on @head. @list\n * should be an empty list or a list you do not care about\n * losing its data.\n *\n */\nstatic inline void list_cut_position(struct list_head *list,\n\t\tstruct list_head *head, struct list_head *entry)\n{\n\tif (list_empty(head))\n\t\treturn;\n\tif (list_is_singular(head) &&\n\t\t(head->next != entry && head != entry))\n\t\treturn;\n\tif (entry == head)\n\t\tINIT_LIST_HEAD(list);\n\telse\n\t\t__list_cut_position(list, head, entry);\n}\n\n/**\n * list_cut_before - cut a list into two, before given entry\n * @list: a new list to add all removed entries\n * @head: a list with entries\n * @entry: an entry within head, could be the head itself\n *\n * This helper moves the initial part of @head, up to but\n * excluding @entry, from @head to @list.  You should pass\n * in @entry an element you know is on @head.  @list should\n * be an empty list or a list you do not care about losing\n * its data.\n * If @entry == @head, all entries on @head are moved to\n * @list.\n */\nstatic inline void list_cut_before(struct list_head *list,\n\t\t\t\t   struct list_head *head,\n\t\t\t\t   struct list_head *entry)\n{\n\tif (head->next == entry) {\n\t\tINIT_LIST_HEAD(list);\n\t\treturn;\n\t}\n\tlist->next = head->next;\n\tlist->next->prev = list;\n\tlist->prev = entry->prev;\n\tlist->prev->next = list;\n\thead->next = entry;\n\tentry->prev = head;\n}\n\nstatic inline void __list_splice(const struct list_head *list,\n\t\t\t\t struct list_head *prev,\n\t\t\t\t struct list_head *next)\n{\n\tstruct list_head *first = list->next;\n\tstruct list_head *last = list->prev;\n\n\tfirst->prev = prev;\n\tprev->next = first;\n\n\tlast->next = next;\n\tnext->prev = last;\n}\n\n/**\n * list_splice - join two lists, this is designed for stacks\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n */\nstatic inline void list_splice(const struct list_head *list,\n\t\t\t\tstruct list_head *head)\n{\n\tif (!list_empty(list))\n\t\t__list_splice(list, head, head->next);\n}\n\n/**\n * list_splice_tail - join two lists, each list being a queue\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n */\nstatic inline void list_splice_tail(struct list_head *list,\n\t\t\t\tstruct list_head *head)\n{\n\tif (!list_empty(list))\n\t\t__list_splice(list, head->prev, head);\n}\n\n/**\n * list_splice_init - join two lists and reinitialise the emptied list.\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n *\n * The list at @list is reinitialised\n */\nstatic inline void list_splice_init(struct list_head *list,\n\t\t\t\t    struct list_head *head)\n{\n\tif (!list_empty(list)) {\n\t\t__list_splice(list, head, head->next);\n\t\tINIT_LIST_HEAD(list);\n\t}\n}\n\n/**\n * list_splice_tail_init - join two lists and reinitialise the emptied list\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n *\n * Each of the lists is a queue.\n * The list at @list is reinitialised\n */\nstatic inline void list_splice_tail_init(struct list_head *list,\n\t\t\t\t\t struct list_head *head)\n{\n\tif (!list_empty(list)) {\n\t\t__list_splice(list, head->prev, head);\n\t\tINIT_LIST_HEAD(list);\n\t}\n}\n\n/**\n * list_entry - get the struct for this entry\n * @ptr:\tthe &struct list_head pointer.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_entry(ptr, type, member) \\\n\tcontainer_of(ptr, type, member)\n\n/**\n * list_first_entry - get the first element from a list\n * @ptr:\tthe list head to take the element from.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n *\n * Note, that list is expected to be not empty.\n */\n#define list_first_entry(ptr, type, member) \\\n\tlist_entry((ptr)->next, type, member)\n\n/**\n * list_last_entry - get the last element from a list\n * @ptr:\tthe list head to take the element from.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n *\n * Note, that list is expected to be not empty.\n */\n#define list_last_entry(ptr, type, member) \\\n\tlist_entry((ptr)->prev, type, member)\n\n/**\n * list_first_entry_or_null - get the first element from a list\n * @ptr:\tthe list head to take the element from.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n *\n * Note that if the list is empty, it returns NULL.\n */\n#define list_first_entry_or_null(ptr, type, member) ({ \\\n\tstruct list_head *head__ = (ptr); \\\n\tstruct list_head *pos__ = READ_ONCE(head__->next); \\\n\tpos__ != head__ ? list_entry(pos__, type, member) : NULL; \\\n})\n\n/**\n * list_next_entry - get the next element in list\n * @pos:\tthe type * to cursor\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_next_entry(pos, member) \\\n\tlist_entry((pos)->member.next, typeof(*(pos)), member)\n\n/**\n * list_prev_entry - get the prev element in list\n * @pos:\tthe type * to cursor\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_prev_entry(pos, member) \\\n\tlist_entry((pos)->member.prev, typeof(*(pos)), member)\n\n/**\n * list_for_each\t-\titerate over a list\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @head:\tthe head for your list.\n */\n#define list_for_each(pos, head) \\\n\tfor (pos = (head)->next; pos != (head); pos = pos->next)\n\n/**\n * list_for_each_continue - continue iteration over a list\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @head:\tthe head for your list.\n *\n * Continue to iterate over a list, continuing after the current position.\n */\n#define list_for_each_continue(pos, head) \\\n\tfor (pos = pos->next; pos != (head); pos = pos->next)\n\n/**\n * list_for_each_prev\t-\titerate over a list backwards\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @head:\tthe head for your list.\n */\n#define list_for_each_prev(pos, head) \\\n\tfor (pos = (head)->prev; pos != (head); pos = pos->prev)\n\n/**\n * list_for_each_safe - iterate over a list safe against removal of list entry\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @n:\t\tanother &struct list_head to use as temporary storage\n * @head:\tthe head for your list.\n */\n#define list_for_each_safe(pos, n, head) \\\n\tfor (pos = (head)->next, n = pos->next; pos != (head); \\\n\t\tpos = n, n = pos->next)\n\n/**\n * list_for_each_prev_safe - iterate over a list backwards safe against removal of list entry\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @n:\t\tanother &struct list_head to use as temporary storage\n * @head:\tthe head for your list.\n */\n#define list_for_each_prev_safe(pos, n, head) \\\n\tfor (pos = (head)->prev, n = pos->prev; \\\n\t     pos != (head); \\\n\t     pos = n, n = pos->prev)\n\n/**\n * list_entry_is_head - test if the entry points to the head of the list\n * @pos:\tthe type * to cursor\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_entry_is_head(pos, head, member)\t\t\t\t\\\n\t(&pos->member == (head))\n\n/**\n * list_for_each_entry\t-\titerate over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_for_each_entry(pos, head, member)\t\t\t\t\\\n\tfor (pos = list_first_entry(head, typeof(*pos), member);\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_next_entry(pos, member))\n\n/**\n * list_for_each_entry_reverse - iterate backwards over list of given type.\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_for_each_entry_reverse(pos, head, member)\t\t\t\\\n\tfor (pos = list_last_entry(head, typeof(*pos), member);\t\t\\\n\t     !list_entry_is_head(pos, head, member); \t\t\t\\\n\t     pos = list_prev_entry(pos, member))\n\n/**\n * list_prepare_entry - prepare a pos entry for use in list_for_each_entry_continue()\n * @pos:\tthe type * to use as a start point\n * @head:\tthe head of the list\n * @member:\tthe name of the list_head within the struct.\n *\n * Prepares a pos entry for use as a start point in list_for_each_entry_continue().\n */\n#define list_prepare_entry(pos, head, member) \\\n\t((pos) ? : list_entry(head, typeof(*pos), member))\n\n/**\n * list_for_each_entry_continue - continue iteration over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Continue to iterate over list of given type, continuing after\n * the current position.\n */\n#define list_for_each_entry_continue(pos, head, member) \t\t\\\n\tfor (pos = list_next_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_next_entry(pos, member))\n\n/**\n * list_for_each_entry_continue_reverse - iterate backwards from the given point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Start to iterate over list of given type backwards, continuing after\n * the current position.\n */\n#define list_for_each_entry_continue_reverse(pos, head, member)\t\t\\\n\tfor (pos = list_prev_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_prev_entry(pos, member))\n\n/**\n * list_for_each_entry_from - iterate over list of given type from the current point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate over list of given type, continuing from current position.\n */\n#define list_for_each_entry_from(pos, head, member) \t\t\t\\\n\tfor (; !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_next_entry(pos, member))\n\n/**\n * list_for_each_entry_from_reverse - iterate backwards over list of given type\n *                                    from the current point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate backwards over list of given type, continuing from current position.\n */\n#define list_for_each_entry_from_reverse(pos, head, member)\t\t\\\n\tfor (; !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_prev_entry(pos, member))\n\n/**\n * list_for_each_entry_safe - iterate over list of given type safe against removal of list entry\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_for_each_entry_safe(pos, n, head, member)\t\t\t\\\n\tfor (pos = list_first_entry(head, typeof(*pos), member),\t\\\n\t\tn = list_next_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member); \t\t\t\\\n\t     pos = n, n = list_next_entry(n, member))\n\n/**\n * list_for_each_entry_safe_continue - continue list iteration safe against removal\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate over list of given type, continuing after current point,\n * safe against removal of list entry.\n */\n#define list_for_each_entry_safe_continue(pos, n, head, member) \t\t\\\n\tfor (pos = list_next_entry(pos, member), \t\t\t\t\\\n\t\tn = list_next_entry(pos, member);\t\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\t\\\n\t     pos = n, n = list_next_entry(n, member))\n\n/**\n * list_for_each_entry_safe_from - iterate over list from current point safe against removal\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate over list of given type from current point, safe against\n * removal of list entry.\n */\n#define list_for_each_entry_safe_from(pos, n, head, member) \t\t\t\\\n\tfor (n = list_next_entry(pos, member);\t\t\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\t\\\n\t     pos = n, n = list_next_entry(n, member))\n\n/**\n * list_for_each_entry_safe_reverse - iterate backwards over list safe against removal\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate backwards over list of given type, safe against removal\n * of list entry.\n */\n#define list_for_each_entry_safe_reverse(pos, n, head, member)\t\t\\\n\tfor (pos = list_last_entry(head, typeof(*pos), member),\t\t\\\n\t\tn = list_prev_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member); \t\t\t\\\n\t     pos = n, n = list_prev_entry(n, member))\n\n/**\n * list_safe_reset_next - reset a stale list_for_each_entry_safe loop\n * @pos:\tthe loop cursor used in the list_for_each_entry_safe loop\n * @n:\t\ttemporary storage used in list_for_each_entry_safe\n * @member:\tthe name of the list_head within the struct.\n *\n * list_safe_reset_next is not safe to use in general if the list may be\n * modified concurrently (eg. the lock is dropped in the loop body). An\n * exception to this is if the cursor element (pos) is pinned in the list,\n * and list_safe_reset_next is called after re-taking the lock and before\n * completing the current iteration of the loop body.\n */\n#define list_safe_reset_next(pos, n, member)\t\t\t\t\\\n\tn = list_next_entry(pos, member)\n\n/*\n * Double linked lists with a single pointer list head.\n * Mostly useful for hash tables where the two pointer list head is\n * too wasteful.\n * You lose the ability to access the tail in O(1).\n */\n\n#define HLIST_HEAD_INIT { .first = NULL }\n#define HLIST_HEAD(name) struct hlist_head name = {  .first = NULL }\n#define INIT_HLIST_HEAD(ptr) ((ptr)->first = NULL)\nstatic inline void INIT_HLIST_NODE(struct hlist_node *h)\n{\n\th->next = NULL;\n\th->pprev = NULL;\n}\n\n/**\n * hlist_unhashed - Has node been removed from list and reinitialized?\n * @h: Node to be checked\n *\n * Not that not all removal functions will leave a node in unhashed\n * state.  For example, hlist_nulls_del_init_rcu() does leave the\n * node in unhashed state, but hlist_nulls_del() does not.\n */\nstatic inline int hlist_unhashed(const struct hlist_node *h)\n{\n\treturn !h->pprev;\n}\n\n/**\n * hlist_unhashed_lockless - Version of hlist_unhashed for lockless use\n * @h: Node to be checked\n *\n * This variant of hlist_unhashed() must be used in lockless contexts\n * to avoid potential load-tearing.  The READ_ONCE() is paired with the\n * various WRITE_ONCE() in hlist helpers that are defined below.\n */\nstatic inline int hlist_unhashed_lockless(const struct hlist_node *h)\n{\n\treturn !READ_ONCE(h->pprev);\n}\n\n/**\n * hlist_empty - Is the specified hlist_head structure an empty hlist?\n * @h: Structure to check.\n */\nstatic inline int hlist_empty(const struct hlist_head *h)\n{\n\treturn !READ_ONCE(h->first);\n}\n\nstatic inline void __hlist_del(struct hlist_node *n)\n{\n\tstruct hlist_node *next = n->next;\n\tstruct hlist_node **pprev = n->pprev;\n\n\tWRITE_ONCE(*pprev, next);\n\tif (next)\n\t\tWRITE_ONCE(next->pprev, pprev);\n}\n\n/**\n * hlist_del - Delete the specified hlist_node from its list\n * @n: Node to delete.\n *\n * Note that this function leaves the node in hashed state.  Use\n * hlist_del_init() or similar instead to unhash @n.\n */\nstatic inline void hlist_del(struct hlist_node *n)\n{\n\t__hlist_del(n);\n\tn->next = LIST_POISON1;\n\tn->pprev = LIST_POISON2;\n}\n\n/**\n * hlist_del_init - Delete the specified hlist_node from its list and initialize\n * @n: Node to delete.\n *\n * Note that this function leaves the node in unhashed state.\n */\nstatic inline void hlist_del_init(struct hlist_node *n)\n{\n\tif (!hlist_unhashed(n)) {\n\t\t__hlist_del(n);\n\t\tINIT_HLIST_NODE(n);\n\t}\n}\n\n/**\n * hlist_add_head - add a new entry at the beginning of the hlist\n * @n: new entry to be added\n * @h: hlist head to add it after\n *\n * Insert a new entry after the specified head.\n * This is good for implementing stacks.\n */\nstatic inline void hlist_add_head(struct hlist_node *n, struct hlist_head *h)\n{\n\tstruct hlist_node *first = h->first;\n\tWRITE_ONCE(n->next, first);\n\tif (first)\n\t\tWRITE_ONCE(first->pprev, &n->next);\n\tWRITE_ONCE(h->first, n);\n\tWRITE_ONCE(n->pprev, &h->first);\n}\n\n/**\n * hlist_add_before - add a new entry before the one specified\n * @n: new entry to be added\n * @next: hlist node to add it before, which must be non-NULL\n */\nstatic inline void hlist_add_before(struct hlist_node *n,\n\t\t\t\t    struct hlist_node *next)\n{\n\tWRITE_ONCE(n->pprev, next->pprev);\n\tWRITE_ONCE(n->next, next);\n\tWRITE_ONCE(next->pprev, &n->next);\n\tWRITE_ONCE(*(n->pprev), n);\n}\n\n/**\n * hlist_add_behind - add a new entry after the one specified\n * @n: new entry to be added\n * @prev: hlist node to add it after, which must be non-NULL\n */\nstatic inline void hlist_add_behind(struct hlist_node *n,\n\t\t\t\t    struct hlist_node *prev)\n{\n\tWRITE_ONCE(n->next, prev->next);\n\tWRITE_ONCE(prev->next, n);\n\tWRITE_ONCE(n->pprev, &prev->next);\n\n\tif (n->next)\n\t\tWRITE_ONCE(n->next->pprev, &n->next);\n}\n\n/**\n * hlist_add_fake - create a fake hlist consisting of a single headless node\n * @n: Node to make a fake list out of\n *\n * This makes @n appear to be its own predecessor on a headless hlist.\n * The point of this is to allow things like hlist_del() to work correctly\n * in cases where there is no list.\n */\nstatic inline void hlist_add_fake(struct hlist_node *n)\n{\n\tn->pprev = &n->next;\n}\n\n/**\n * hlist_fake: Is this node a fake hlist?\n * @h: Node to check for being a self-referential fake hlist.\n */\nstatic inline bool hlist_fake(struct hlist_node *h)\n{\n\treturn h->pprev == &h->next;\n}\n\n/**\n * hlist_is_singular_node - is node the only element of the specified hlist?\n * @n: Node to check for singularity.\n * @h: Header for potentially singular list.\n *\n * Check whether the node is the only node of the head without\n * accessing head, thus avoiding unnecessary cache misses.\n */\nstatic inline bool\nhlist_is_singular_node(struct hlist_node *n, struct hlist_head *h)\n{\n\treturn !n->next && n->pprev == &h->first;\n}\n\n/**\n * hlist_move_list - Move an hlist\n * @old: hlist_head for old list.\n * @new: hlist_head for new list.\n *\n * Move a list from one list head to another. Fixup the pprev\n * reference of the first entry if it exists.\n */\nstatic inline void hlist_move_list(struct hlist_head *old,\n\t\t\t\t   struct hlist_head *new)\n{\n\tnew->first = old->first;\n\tif (new->first)\n\t\tnew->first->pprev = &new->first;\n\told->first = NULL;\n}\n\n#define hlist_entry(ptr, type, member) container_of(ptr,type,member)\n\n#define hlist_for_each(pos, head) \\\n\tfor (pos = (head)->first; pos ; pos = pos->next)\n\n#define hlist_for_each_safe(pos, n, head) \\\n\tfor (pos = (head)->first; pos && ({ n = pos->next; 1; }); \\\n\t     pos = n)\n\n#define hlist_entry_safe(ptr, type, member) \\\n\t({ typeof(ptr) ____ptr = (ptr); \\\n\t   ____ptr ? hlist_entry(____ptr, type, member) : NULL; \\\n\t})\n\n/**\n * hlist_for_each_entry\t- iterate over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry(pos, head, member)\t\t\t\t\\\n\tfor (pos = hlist_entry_safe((head)->first, typeof(*(pos)), member);\\\n\t     pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_continue - iterate over a hlist continuing after current point\n * @pos:\tthe type * to use as a loop cursor.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_continue(pos, member)\t\t\t\\\n\tfor (pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member);\\\n\t     pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_from - iterate over a hlist continuing from current point\n * @pos:\tthe type * to use as a loop cursor.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_from(pos, member)\t\t\t\t\\\n\tfor (; pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_safe - iterate over list of given type safe against removal of list entry\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\ta &struct hlist_node to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_safe(pos, n, head, member) \t\t\\\n\tfor (pos = hlist_entry_safe((head)->first, typeof(*pos), member);\\\n\t     pos && ({ n = pos->member.next; 1; });\t\t\t\\\n\t     pos = hlist_entry_safe(n, typeof(*pos), member))\n\n#endif\n"}, "3": {"id": 3, "path": "/src/include/linux/kernel.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_KERNEL_H\n#define _LINUX_KERNEL_H\n\n#include <stdarg.h>\n#include <linux/limits.h>\n#include <linux/linkage.h>\n#include <linux/stddef.h>\n#include <linux/types.h>\n#include <linux/compiler.h>\n#include <linux/bitops.h>\n#include <linux/log2.h>\n#include <linux/math.h>\n#include <linux/minmax.h>\n#include <linux/typecheck.h>\n#include <linux/printk.h>\n#include <linux/build_bug.h>\n\n#include <asm/byteorder.h>\n\n#include <uapi/linux/kernel.h>\n\n#define STACK_MAGIC\t0xdeadbeef\n\n/**\n * REPEAT_BYTE - repeat the value @x multiple times as an unsigned long value\n * @x: value to repeat\n *\n * NOTE: @x is not checked for > 0xff; larger values produce odd results.\n */\n#define REPEAT_BYTE(x)\t((~0ul / 0xff) * (x))\n\n/* @a is a power of 2 value */\n#define ALIGN(x, a)\t\t__ALIGN_KERNEL((x), (a))\n#define ALIGN_DOWN(x, a)\t__ALIGN_KERNEL((x) - ((a) - 1), (a))\n#define __ALIGN_MASK(x, mask)\t__ALIGN_KERNEL_MASK((x), (mask))\n#define PTR_ALIGN(p, a)\t\t((typeof(p))ALIGN((unsigned long)(p), (a)))\n#define PTR_ALIGN_DOWN(p, a)\t((typeof(p))ALIGN_DOWN((unsigned long)(p), (a)))\n#define IS_ALIGNED(x, a)\t\t(((x) & ((typeof(x))(a) - 1)) == 0)\n\n/* generic data direction definitions */\n#define READ\t\t\t0\n#define WRITE\t\t\t1\n\n/**\n * ARRAY_SIZE - get the number of elements in array @arr\n * @arr: array to be sized\n */\n#define ARRAY_SIZE(arr) (sizeof(arr) / sizeof((arr)[0]) + __must_be_array(arr))\n\n#define u64_to_user_ptr(x) (\t\t\\\n{\t\t\t\t\t\\\n\ttypecheck(u64, (x));\t\t\\\n\t(void __user *)(uintptr_t)(x);\t\\\n}\t\t\t\t\t\\\n)\n\n#define typeof_member(T, m)\ttypeof(((T*)0)->m)\n\n#define _RET_IP_\t\t(unsigned long)__builtin_return_address(0)\n#define _THIS_IP_  ({ __label__ __here; __here: (unsigned long)&&__here; })\n\n/**\n * upper_32_bits - return bits 32-63 of a number\n * @n: the number we're accessing\n *\n * A basic shift-right of a 64- or 32-bit quantity.  Use this to suppress\n * the \"right shift count >= width of type\" warning when that quantity is\n * 32-bits.\n */\n#define upper_32_bits(n) ((u32)(((n) >> 16) >> 16))\n\n/**\n * lower_32_bits - return bits 0-31 of a number\n * @n: the number we're accessing\n */\n#define lower_32_bits(n) ((u32)((n) & 0xffffffff))\n\nstruct completion;\nstruct pt_regs;\nstruct user;\n\n#ifdef CONFIG_PREEMPT_VOLUNTARY\nextern int _cond_resched(void);\n# define might_resched() _cond_resched()\n#else\n# define might_resched() do { } while (0)\n#endif\n\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\nextern void ___might_sleep(const char *file, int line, int preempt_offset);\nextern void __might_sleep(const char *file, int line, int preempt_offset);\nextern void __cant_sleep(const char *file, int line, int preempt_offset);\nextern void __cant_migrate(const char *file, int line);\n\n/**\n * might_sleep - annotation for functions that can sleep\n *\n * this macro will print a stack trace if it is executed in an atomic\n * context (spinlock, irq-handler, ...). Additional sections where blocking is\n * not allowed can be annotated with non_block_start() and non_block_end()\n * pairs.\n *\n * This is a useful debugging help to be able to catch problems early and not\n * be bitten later when the calling function happens to sleep when it is not\n * supposed to.\n */\n# define might_sleep() \\\n\tdo { __might_sleep(__FILE__, __LINE__, 0); might_resched(); } while (0)\n/**\n * cant_sleep - annotation for functions that cannot sleep\n *\n * this macro will print a stack trace if it is executed with preemption enabled\n */\n# define cant_sleep() \\\n\tdo { __cant_sleep(__FILE__, __LINE__, 0); } while (0)\n# define sched_annotate_sleep()\t(current->task_state_change = 0)\n\n/**\n * cant_migrate - annotation for functions that cannot migrate\n *\n * Will print a stack trace if executed in code which is migratable\n */\n# define cant_migrate()\t\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (IS_ENABLED(CONFIG_SMP))\t\t\t\t\\\n\t\t\t__cant_migrate(__FILE__, __LINE__);\t\t\\\n\t} while (0)\n\n/**\n * non_block_start - annotate the start of section where sleeping is prohibited\n *\n * This is on behalf of the oom reaper, specifically when it is calling the mmu\n * notifiers. The problem is that if the notifier were to block on, for example,\n * mutex_lock() and if the process which holds that mutex were to perform a\n * sleeping memory allocation, the oom reaper is now blocked on completion of\n * that memory allocation. Other blocking calls like wait_event() pose similar\n * issues.\n */\n# define non_block_start() (current->non_block_count++)\n/**\n * non_block_end - annotate the end of section where sleeping is prohibited\n *\n * Closes a section opened by non_block_start().\n */\n# define non_block_end() WARN_ON(current->non_block_count-- == 0)\n#else\n  static inline void ___might_sleep(const char *file, int line,\n\t\t\t\t   int preempt_offset) { }\n  static inline void __might_sleep(const char *file, int line,\n\t\t\t\t   int preempt_offset) { }\n# define might_sleep() do { might_resched(); } while (0)\n# define cant_sleep() do { } while (0)\n# define cant_migrate()\t\tdo { } while (0)\n# define sched_annotate_sleep() do { } while (0)\n# define non_block_start() do { } while (0)\n# define non_block_end() do { } while (0)\n#endif\n\n#define might_sleep_if(cond) do { if (cond) might_sleep(); } while (0)\n\n#if defined(CONFIG_MMU) && \\\n\t(defined(CONFIG_PROVE_LOCKING) || defined(CONFIG_DEBUG_ATOMIC_SLEEP))\n#define might_fault() __might_fault(__FILE__, __LINE__)\nvoid __might_fault(const char *file, int line);\n#else\nstatic inline void might_fault(void) { }\n#endif\n\nextern struct atomic_notifier_head panic_notifier_list;\nextern long (*panic_blink)(int state);\n__printf(1, 2)\nvoid panic(const char *fmt, ...) __noreturn __cold;\nvoid nmi_panic(struct pt_regs *regs, const char *msg);\nextern void oops_enter(void);\nextern void oops_exit(void);\nextern bool oops_may_print(void);\nvoid do_exit(long error_code) __noreturn;\nvoid complete_and_exit(struct completion *, long) __noreturn;\n\n/* Internal, do not use. */\nint __must_check _kstrtoul(const char *s, unsigned int base, unsigned long *res);\nint __must_check _kstrtol(const char *s, unsigned int base, long *res);\n\nint __must_check kstrtoull(const char *s, unsigned int base, unsigned long long *res);\nint __must_check kstrtoll(const char *s, unsigned int base, long long *res);\n\n/**\n * kstrtoul - convert a string to an unsigned long\n * @s: The start of the string. The string must be null-terminated, and may also\n *  include a single newline before its terminating null. The first character\n *  may also be a plus sign, but not a minus sign.\n * @base: The number base to use. The maximum supported base is 16. If base is\n *  given as 0, then the base of the string is automatically detected with the\n *  conventional semantics - If it begins with 0x the number will be parsed as a\n *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be\n *  parsed as an octal number. Otherwise it will be parsed as a decimal.\n * @res: Where to write the result of the conversion on success.\n *\n * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.\n * Preferred over simple_strtoul(). Return code must be checked.\n*/\nstatic inline int __must_check kstrtoul(const char *s, unsigned int base, unsigned long *res)\n{\n\t/*\n\t * We want to shortcut function call, but\n\t * __builtin_types_compatible_p(unsigned long, unsigned long long) = 0.\n\t */\n\tif (sizeof(unsigned long) == sizeof(unsigned long long) &&\n\t    __alignof__(unsigned long) == __alignof__(unsigned long long))\n\t\treturn kstrtoull(s, base, (unsigned long long *)res);\n\telse\n\t\treturn _kstrtoul(s, base, res);\n}\n\n/**\n * kstrtol - convert a string to a long\n * @s: The start of the string. The string must be null-terminated, and may also\n *  include a single newline before its terminating null. The first character\n *  may also be a plus sign or a minus sign.\n * @base: The number base to use. The maximum supported base is 16. If base is\n *  given as 0, then the base of the string is automatically detected with the\n *  conventional semantics - If it begins with 0x the number will be parsed as a\n *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be\n *  parsed as an octal number. Otherwise it will be parsed as a decimal.\n * @res: Where to write the result of the conversion on success.\n *\n * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.\n * Preferred over simple_strtol(). Return code must be checked.\n */\nstatic inline int __must_check kstrtol(const char *s, unsigned int base, long *res)\n{\n\t/*\n\t * We want to shortcut function call, but\n\t * __builtin_types_compatible_p(long, long long) = 0.\n\t */\n\tif (sizeof(long) == sizeof(long long) &&\n\t    __alignof__(long) == __alignof__(long long))\n\t\treturn kstrtoll(s, base, (long long *)res);\n\telse\n\t\treturn _kstrtol(s, base, res);\n}\n\nint __must_check kstrtouint(const char *s, unsigned int base, unsigned int *res);\nint __must_check kstrtoint(const char *s, unsigned int base, int *res);\n\nstatic inline int __must_check kstrtou64(const char *s, unsigned int base, u64 *res)\n{\n\treturn kstrtoull(s, base, res);\n}\n\nstatic inline int __must_check kstrtos64(const char *s, unsigned int base, s64 *res)\n{\n\treturn kstrtoll(s, base, res);\n}\n\nstatic inline int __must_check kstrtou32(const char *s, unsigned int base, u32 *res)\n{\n\treturn kstrtouint(s, base, res);\n}\n\nstatic inline int __must_check kstrtos32(const char *s, unsigned int base, s32 *res)\n{\n\treturn kstrtoint(s, base, res);\n}\n\nint __must_check kstrtou16(const char *s, unsigned int base, u16 *res);\nint __must_check kstrtos16(const char *s, unsigned int base, s16 *res);\nint __must_check kstrtou8(const char *s, unsigned int base, u8 *res);\nint __must_check kstrtos8(const char *s, unsigned int base, s8 *res);\nint __must_check kstrtobool(const char *s, bool *res);\n\nint __must_check kstrtoull_from_user(const char __user *s, size_t count, unsigned int base, unsigned long long *res);\nint __must_check kstrtoll_from_user(const char __user *s, size_t count, unsigned int base, long long *res);\nint __must_check kstrtoul_from_user(const char __user *s, size_t count, unsigned int base, unsigned long *res);\nint __must_check kstrtol_from_user(const char __user *s, size_t count, unsigned int base, long *res);\nint __must_check kstrtouint_from_user(const char __user *s, size_t count, unsigned int base, unsigned int *res);\nint __must_check kstrtoint_from_user(const char __user *s, size_t count, unsigned int base, int *res);\nint __must_check kstrtou16_from_user(const char __user *s, size_t count, unsigned int base, u16 *res);\nint __must_check kstrtos16_from_user(const char __user *s, size_t count, unsigned int base, s16 *res);\nint __must_check kstrtou8_from_user(const char __user *s, size_t count, unsigned int base, u8 *res);\nint __must_check kstrtos8_from_user(const char __user *s, size_t count, unsigned int base, s8 *res);\nint __must_check kstrtobool_from_user(const char __user *s, size_t count, bool *res);\n\nstatic inline int __must_check kstrtou64_from_user(const char __user *s, size_t count, unsigned int base, u64 *res)\n{\n\treturn kstrtoull_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtos64_from_user(const char __user *s, size_t count, unsigned int base, s64 *res)\n{\n\treturn kstrtoll_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtou32_from_user(const char __user *s, size_t count, unsigned int base, u32 *res)\n{\n\treturn kstrtouint_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtos32_from_user(const char __user *s, size_t count, unsigned int base, s32 *res)\n{\n\treturn kstrtoint_from_user(s, count, base, res);\n}\n\n/*\n * Use kstrto<foo> instead.\n *\n * NOTE: simple_strto<foo> does not check for the range overflow and,\n *\t depending on the input, may give interesting results.\n *\n * Use these functions if and only if you cannot use kstrto<foo>, because\n * the conversion ends on the first non-digit character, which may be far\n * beyond the supported range. It might be useful to parse the strings like\n * 10x50 or 12:21 without altering original string or temporary buffer in use.\n * Keep in mind above caveat.\n */\n\nextern unsigned long simple_strtoul(const char *,char **,unsigned int);\nextern long simple_strtol(const char *,char **,unsigned int);\nextern unsigned long long simple_strtoull(const char *,char **,unsigned int);\nextern long long simple_strtoll(const char *,char **,unsigned int);\n\nextern int num_to_str(char *buf, int size,\n\t\t      unsigned long long num, unsigned int width);\n\n/* lib/printf utilities */\n\nextern __printf(2, 3) int sprintf(char *buf, const char * fmt, ...);\nextern __printf(2, 0) int vsprintf(char *buf, const char *, va_list);\nextern __printf(3, 4)\nint snprintf(char *buf, size_t size, const char *fmt, ...);\nextern __printf(3, 0)\nint vsnprintf(char *buf, size_t size, const char *fmt, va_list args);\nextern __printf(3, 4)\nint scnprintf(char *buf, size_t size, const char *fmt, ...);\nextern __printf(3, 0)\nint vscnprintf(char *buf, size_t size, const char *fmt, va_list args);\nextern __printf(2, 3) __malloc\nchar *kasprintf(gfp_t gfp, const char *fmt, ...);\nextern __printf(2, 0) __malloc\nchar *kvasprintf(gfp_t gfp, const char *fmt, va_list args);\nextern __printf(2, 0)\nconst char *kvasprintf_const(gfp_t gfp, const char *fmt, va_list args);\n\nextern __scanf(2, 3)\nint sscanf(const char *, const char *, ...);\nextern __scanf(2, 0)\nint vsscanf(const char *, const char *, va_list);\n\nextern int get_option(char **str, int *pint);\nextern char *get_options(const char *str, int nints, int *ints);\nextern unsigned long long memparse(const char *ptr, char **retptr);\nextern bool parse_option_str(const char *str, const char *option);\nextern char *next_arg(char *args, char **param, char **val);\n\nextern int core_kernel_text(unsigned long addr);\nextern int init_kernel_text(unsigned long addr);\nextern int core_kernel_data(unsigned long addr);\nextern int __kernel_text_address(unsigned long addr);\nextern int kernel_text_address(unsigned long addr);\nextern int func_ptr_is_kernel_text(void *ptr);\n\n#ifdef CONFIG_SMP\nextern unsigned int sysctl_oops_all_cpu_backtrace;\n#else\n#define sysctl_oops_all_cpu_backtrace 0\n#endif /* CONFIG_SMP */\n\nextern void bust_spinlocks(int yes);\nextern int panic_timeout;\nextern unsigned long panic_print;\nextern int panic_on_oops;\nextern int panic_on_unrecovered_nmi;\nextern int panic_on_io_nmi;\nextern int panic_on_warn;\nextern unsigned long panic_on_taint;\nextern bool panic_on_taint_nousertaint;\nextern int sysctl_panic_on_rcu_stall;\nextern int sysctl_max_rcu_stall_to_panic;\nextern int sysctl_panic_on_stackoverflow;\n\nextern bool crash_kexec_post_notifiers;\n\n/*\n * panic_cpu is used for synchronizing panic() and crash_kexec() execution. It\n * holds a CPU number which is executing panic() currently. A value of\n * PANIC_CPU_INVALID means no CPU has entered panic() or crash_kexec().\n */\nextern atomic_t panic_cpu;\n#define PANIC_CPU_INVALID\t-1\n\n/*\n * Only to be used by arch init code. If the user over-wrote the default\n * CONFIG_PANIC_TIMEOUT, honor it.\n */\nstatic inline void set_arch_panic_timeout(int timeout, int arch_default_timeout)\n{\n\tif (panic_timeout == arch_default_timeout)\n\t\tpanic_timeout = timeout;\n}\nextern const char *print_tainted(void);\nenum lockdep_ok {\n\tLOCKDEP_STILL_OK,\n\tLOCKDEP_NOW_UNRELIABLE\n};\nextern void add_taint(unsigned flag, enum lockdep_ok);\nextern int test_taint(unsigned flag);\nextern unsigned long get_taint(void);\nextern int root_mountflags;\n\nextern bool early_boot_irqs_disabled;\n\n/*\n * Values used for system_state. Ordering of the states must not be changed\n * as code checks for <, <=, >, >= STATE.\n */\nextern enum system_states {\n\tSYSTEM_BOOTING,\n\tSYSTEM_SCHEDULING,\n\tSYSTEM_RUNNING,\n\tSYSTEM_HALT,\n\tSYSTEM_POWER_OFF,\n\tSYSTEM_RESTART,\n\tSYSTEM_SUSPEND,\n} system_state;\n\n/* This cannot be an enum because some may be used in assembly source. */\n#define TAINT_PROPRIETARY_MODULE\t0\n#define TAINT_FORCED_MODULE\t\t1\n#define TAINT_CPU_OUT_OF_SPEC\t\t2\n#define TAINT_FORCED_RMMOD\t\t3\n#define TAINT_MACHINE_CHECK\t\t4\n#define TAINT_BAD_PAGE\t\t\t5\n#define TAINT_USER\t\t\t6\n#define TAINT_DIE\t\t\t7\n#define TAINT_OVERRIDDEN_ACPI_TABLE\t8\n#define TAINT_WARN\t\t\t9\n#define TAINT_CRAP\t\t\t10\n#define TAINT_FIRMWARE_WORKAROUND\t11\n#define TAINT_OOT_MODULE\t\t12\n#define TAINT_UNSIGNED_MODULE\t\t13\n#define TAINT_SOFTLOCKUP\t\t14\n#define TAINT_LIVEPATCH\t\t\t15\n#define TAINT_AUX\t\t\t16\n#define TAINT_RANDSTRUCT\t\t17\n#define TAINT_FLAGS_COUNT\t\t18\n#define TAINT_FLAGS_MAX\t\t\t((1UL << TAINT_FLAGS_COUNT) - 1)\n\nstruct taint_flag {\n\tchar c_true;\t/* character printed when tainted */\n\tchar c_false;\t/* character printed when not tainted */\n\tbool module;\t/* also show as a per-module taint flag */\n};\n\nextern const struct taint_flag taint_flags[TAINT_FLAGS_COUNT];\n\nextern const char hex_asc[];\n#define hex_asc_lo(x)\thex_asc[((x) & 0x0f)]\n#define hex_asc_hi(x)\thex_asc[((x) & 0xf0) >> 4]\n\nstatic inline char *hex_byte_pack(char *buf, u8 byte)\n{\n\t*buf++ = hex_asc_hi(byte);\n\t*buf++ = hex_asc_lo(byte);\n\treturn buf;\n}\n\nextern const char hex_asc_upper[];\n#define hex_asc_upper_lo(x)\thex_asc_upper[((x) & 0x0f)]\n#define hex_asc_upper_hi(x)\thex_asc_upper[((x) & 0xf0) >> 4]\n\nstatic inline char *hex_byte_pack_upper(char *buf, u8 byte)\n{\n\t*buf++ = hex_asc_upper_hi(byte);\n\t*buf++ = hex_asc_upper_lo(byte);\n\treturn buf;\n}\n\nextern int hex_to_bin(char ch);\nextern int __must_check hex2bin(u8 *dst, const char *src, size_t count);\nextern char *bin2hex(char *dst, const void *src, size_t count);\n\nbool mac_pton(const char *s, u8 *mac);\n\n/*\n * General tracing related utility functions - trace_printk(),\n * tracing_on/tracing_off and tracing_start()/tracing_stop\n *\n * Use tracing_on/tracing_off when you want to quickly turn on or off\n * tracing. It simply enables or disables the recording of the trace events.\n * This also corresponds to the user space /sys/kernel/debug/tracing/tracing_on\n * file, which gives a means for the kernel and userspace to interact.\n * Place a tracing_off() in the kernel where you want tracing to end.\n * From user space, examine the trace, and then echo 1 > tracing_on\n * to continue tracing.\n *\n * tracing_stop/tracing_start has slightly more overhead. It is used\n * by things like suspend to ram where disabling the recording of the\n * trace is not enough, but tracing must actually stop because things\n * like calling smp_processor_id() may crash the system.\n *\n * Most likely, you want to use tracing_on/tracing_off.\n */\n\nenum ftrace_dump_mode {\n\tDUMP_NONE,\n\tDUMP_ALL,\n\tDUMP_ORIG,\n};\n\n#ifdef CONFIG_TRACING\nvoid tracing_on(void);\nvoid tracing_off(void);\nint tracing_is_on(void);\nvoid tracing_snapshot(void);\nvoid tracing_snapshot_alloc(void);\n\nextern void tracing_start(void);\nextern void tracing_stop(void);\n\nstatic inline __printf(1, 2)\nvoid ____trace_printk_check_format(const char *fmt, ...)\n{\n}\n#define __trace_printk_check_format(fmt, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\t\\\n\t\t____trace_printk_check_format(fmt, ##args);\t\t\\\n} while (0)\n\n/**\n * trace_printk - printf formatting in the ftrace buffer\n * @fmt: the printf format for printing\n *\n * Note: __trace_printk is an internal function for trace_printk() and\n *       the @ip is passed in via the trace_printk() macro.\n *\n * This function allows a kernel developer to debug fast path sections\n * that printk is not appropriate for. By scattering in various\n * printk like tracing in the code, a developer can quickly see\n * where problems are occurring.\n *\n * This is intended as a debugging tool for the developer only.\n * Please refrain from leaving trace_printks scattered around in\n * your code. (Extra memory is used for special buffers that are\n * allocated when trace_printk() is used.)\n *\n * A little optimization trick is done here. If there's only one\n * argument, there's no need to scan the string for printf formats.\n * The trace_puts() will suffice. But how can we take advantage of\n * using trace_puts() when trace_printk() has only one argument?\n * By stringifying the args and checking the size we can tell\n * whether or not there are args. __stringify((__VA_ARGS__)) will\n * turn into \"()\\0\" with a size of 3 when there are no args, anything\n * else will be bigger. All we need to do is define a string to this,\n * and then take its size and compare to 3. If it's bigger, use\n * do_trace_printk() otherwise, optimize it to trace_puts(). Then just\n * let gcc optimize the rest.\n */\n\n#define trace_printk(fmt, ...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\\\n\tchar _______STR[] = __stringify((__VA_ARGS__));\t\\\n\tif (sizeof(_______STR) > 3)\t\t\t\\\n\t\tdo_trace_printk(fmt, ##__VA_ARGS__);\t\\\n\telse\t\t\t\t\t\t\\\n\t\ttrace_puts(fmt);\t\t\t\\\n} while (0)\n\n#define do_trace_printk(fmt, args...)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tstatic const char *trace_printk_fmt __used\t\t\t\\\n\t\t__section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t__builtin_constant_p(fmt) ? fmt : NULL;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t__trace_printk_check_format(fmt, ##args);\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(fmt))\t\t\t\t\t\\\n\t\t__trace_bprintk(_THIS_IP_, trace_printk_fmt, ##args);\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t__trace_printk(_THIS_IP_, fmt, ##args);\t\t\t\\\n} while (0)\n\nextern __printf(2, 3)\nint __trace_bprintk(unsigned long ip, const char *fmt, ...);\n\nextern __printf(2, 3)\nint __trace_printk(unsigned long ip, const char *fmt, ...);\n\n/**\n * trace_puts - write a string into the ftrace buffer\n * @str: the string to record\n *\n * Note: __trace_bputs is an internal function for trace_puts and\n *       the @ip is passed in via the trace_puts macro.\n *\n * This is similar to trace_printk() but is made for those really fast\n * paths that a developer wants the least amount of \"Heisenbug\" effects,\n * where the processing of the print format is still too much.\n *\n * This function allows a kernel developer to debug fast path sections\n * that printk is not appropriate for. By scattering in various\n * printk like tracing in the code, a developer can quickly see\n * where problems are occurring.\n *\n * This is intended as a debugging tool for the developer only.\n * Please refrain from leaving trace_puts scattered around in\n * your code. (Extra memory is used for special buffers that are\n * allocated when trace_puts() is used.)\n *\n * Returns: 0 if nothing was written, positive # if string was.\n *  (1 when __trace_bputs is used, strlen(str) when __trace_puts is used)\n */\n\n#define trace_puts(str) ({\t\t\t\t\t\t\\\n\tstatic const char *trace_printk_fmt __used\t\t\t\\\n\t\t__section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t__builtin_constant_p(str) ? str : NULL;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(str))\t\t\t\t\t\\\n\t\t__trace_bputs(_THIS_IP_, trace_printk_fmt);\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t__trace_puts(_THIS_IP_, str, strlen(str));\t\t\\\n})\nextern int __trace_bputs(unsigned long ip, const char *str);\nextern int __trace_puts(unsigned long ip, const char *str, int size);\n\nextern void trace_dump_stack(int skip);\n\n/*\n * The double __builtin_constant_p is because gcc will give us an error\n * if we try to allocate the static variable to fmt if it is not a\n * constant. Even with the outer if statement.\n */\n#define ftrace_vprintk(fmt, vargs)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(fmt)) {\t\t\t\t\\\n\t\tstatic const char *trace_printk_fmt __used\t\t\\\n\t\t  __section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t\t__builtin_constant_p(fmt) ? fmt : NULL;\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t__ftrace_vbprintk(_THIS_IP_, trace_printk_fmt, vargs);\t\\\n\t} else\t\t\t\t\t\t\t\t\\\n\t\t__ftrace_vprintk(_THIS_IP_, fmt, vargs);\t\t\\\n} while (0)\n\nextern __printf(2, 0) int\n__ftrace_vbprintk(unsigned long ip, const char *fmt, va_list ap);\n\nextern __printf(2, 0) int\n__ftrace_vprintk(unsigned long ip, const char *fmt, va_list ap);\n\nextern void ftrace_dump(enum ftrace_dump_mode oops_dump_mode);\n#else\nstatic inline void tracing_start(void) { }\nstatic inline void tracing_stop(void) { }\nstatic inline void trace_dump_stack(int skip) { }\n\nstatic inline void tracing_on(void) { }\nstatic inline void tracing_off(void) { }\nstatic inline int tracing_is_on(void) { return 0; }\nstatic inline void tracing_snapshot(void) { }\nstatic inline void tracing_snapshot_alloc(void) { }\n\nstatic inline __printf(1, 2)\nint trace_printk(const char *fmt, ...)\n{\n\treturn 0;\n}\nstatic __printf(1, 0) inline int\nftrace_vprintk(const char *fmt, va_list ap)\n{\n\treturn 0;\n}\nstatic inline void ftrace_dump(enum ftrace_dump_mode oops_dump_mode) { }\n#endif /* CONFIG_TRACING */\n\n/* This counts to 12. Any more, it will return 13th argument. */\n#define __COUNT_ARGS(_0, _1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _n, X...) _n\n#define COUNT_ARGS(X...) __COUNT_ARGS(, ##X, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0)\n\n#define __CONCAT(a, b) a ## b\n#define CONCATENATE(a, b) __CONCAT(a, b)\n\n/**\n * container_of - cast a member of a structure out to the containing structure\n * @ptr:\tthe pointer to the member.\n * @type:\tthe type of the container struct this is embedded in.\n * @member:\tthe name of the member within the struct.\n *\n */\n#define container_of(ptr, type, member) ({\t\t\t\t\\\n\tvoid *__mptr = (void *)(ptr);\t\t\t\t\t\\\n\tBUILD_BUG_ON_MSG(!__same_type(*(ptr), ((type *)0)->member) &&\t\\\n\t\t\t !__same_type(*(ptr), void),\t\t\t\\\n\t\t\t \"pointer type mismatch in container_of()\");\t\\\n\t((type *)(__mptr - offsetof(type, member))); })\n\n/**\n * container_of_safe - cast a member of a structure out to the containing structure\n * @ptr:\tthe pointer to the member.\n * @type:\tthe type of the container struct this is embedded in.\n * @member:\tthe name of the member within the struct.\n *\n * If IS_ERR_OR_NULL(ptr), ptr is returned unchanged.\n */\n#define container_of_safe(ptr, type, member) ({\t\t\t\t\\\n\tvoid *__mptr = (void *)(ptr);\t\t\t\t\t\\\n\tBUILD_BUG_ON_MSG(!__same_type(*(ptr), ((type *)0)->member) &&\t\\\n\t\t\t !__same_type(*(ptr), void),\t\t\t\\\n\t\t\t \"pointer type mismatch in container_of()\");\t\\\n\tIS_ERR_OR_NULL(__mptr) ? ERR_CAST(__mptr) :\t\t\t\\\n\t\t((type *)(__mptr - offsetof(type, member))); })\n\n/* Rebuild everything on CONFIG_FTRACE_MCOUNT_RECORD */\n#ifdef CONFIG_FTRACE_MCOUNT_RECORD\n# define REBUILD_DUE_TO_FTRACE_MCOUNT_RECORD\n#endif\n\n/* Permissions on a sysfs file: you didn't miss the 0 prefix did you? */\n#define VERIFY_OCTAL_PERMISSIONS(perms)\t\t\t\t\t\t\\\n\t(BUILD_BUG_ON_ZERO((perms) < 0) +\t\t\t\t\t\\\n\t BUILD_BUG_ON_ZERO((perms) > 0777) +\t\t\t\t\t\\\n\t /* USER_READABLE >= GROUP_READABLE >= OTHER_READABLE */\t\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 6) & 4) < (((perms) >> 3) & 4)) +\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 3) & 4) < ((perms) & 4)) +\t\t\\\n\t /* USER_WRITABLE >= GROUP_WRITABLE */\t\t\t\t\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 6) & 2) < (((perms) >> 3) & 2)) +\t\\\n\t /* OTHER_WRITABLE?  Generally considered a bad idea. */\t\t\\\n\t BUILD_BUG_ON_ZERO((perms) & 2) +\t\t\t\t\t\\\n\t (perms))\n#endif\n"}, "0": {"id": 0, "path": "/src/include/net/dst_metadata.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __NET_DST_METADATA_H\n#define __NET_DST_METADATA_H 1\n\n#include <linux/skbuff.h>\n#include <net/ip_tunnels.h>\n#include <net/dst.h>\n\nenum metadata_type {\n\tMETADATA_IP_TUNNEL,\n\tMETADATA_HW_PORT_MUX,\n};\n\nstruct hw_port_info {\n\tstruct net_device *lower_dev;\n\tu32 port_id;\n};\n\nstruct metadata_dst {\n\tstruct dst_entry\t\tdst;\n\tenum metadata_type\t\ttype;\n\tunion {\n\t\tstruct ip_tunnel_info\ttun_info;\n\t\tstruct hw_port_info\tport_info;\n\t} u;\n};\n\nstatic inline struct metadata_dst *skb_metadata_dst(const struct sk_buff *skb)\n{\n\tstruct metadata_dst *md_dst = (struct metadata_dst *) skb_dst(skb);\n\n\tif (md_dst && md_dst->dst.flags & DST_METADATA)\n\t\treturn md_dst;\n\n\treturn NULL;\n}\n\nstatic inline struct ip_tunnel_info *\nskb_tunnel_info(const struct sk_buff *skb)\n{\n\tstruct metadata_dst *md_dst = skb_metadata_dst(skb);\n\tstruct dst_entry *dst;\n\n\tif (md_dst && md_dst->type == METADATA_IP_TUNNEL)\n\t\treturn &md_dst->u.tun_info;\n\n\tdst = skb_dst(skb);\n\tif (dst && dst->lwtstate)\n\t\treturn lwt_tun_info(dst->lwtstate);\n\n\treturn NULL;\n}\n\nstatic inline bool skb_valid_dst(const struct sk_buff *skb)\n{\n\tstruct dst_entry *dst = skb_dst(skb);\n\n\treturn dst && !(dst->flags & DST_METADATA);\n}\n\nstatic inline int skb_metadata_dst_cmp(const struct sk_buff *skb_a,\n\t\t\t\t       const struct sk_buff *skb_b)\n{\n\tconst struct metadata_dst *a, *b;\n\n\tif (!(skb_a->_skb_refdst | skb_b->_skb_refdst))\n\t\treturn 0;\n\n\ta = (const struct metadata_dst *) skb_dst(skb_a);\n\tb = (const struct metadata_dst *) skb_dst(skb_b);\n\n\tif (!a != !b || a->type != b->type)\n\t\treturn 1;\n\n\tswitch (a->type) {\n\tcase METADATA_HW_PORT_MUX:\n\t\treturn memcmp(&a->u.port_info, &b->u.port_info,\n\t\t\t      sizeof(a->u.port_info));\n\tcase METADATA_IP_TUNNEL:\n\t\treturn memcmp(&a->u.tun_info, &b->u.tun_info,\n\t\t\t      sizeof(a->u.tun_info) +\n\t\t\t\t\t a->u.tun_info.options_len);\n\tdefault:\n\t\treturn 1;\n\t}\n}\n\nvoid metadata_dst_free(struct metadata_dst *);\nstruct metadata_dst *metadata_dst_alloc(u8 optslen, enum metadata_type type,\n\t\t\t\t\tgfp_t flags);\nvoid metadata_dst_free_percpu(struct metadata_dst __percpu *md_dst);\nstruct metadata_dst __percpu *\nmetadata_dst_alloc_percpu(u8 optslen, enum metadata_type type, gfp_t flags);\n\nstatic inline struct metadata_dst *tun_rx_dst(int md_size)\n{\n\tstruct metadata_dst *tun_dst;\n\n\ttun_dst = metadata_dst_alloc(md_size, METADATA_IP_TUNNEL, GFP_ATOMIC);\n\tif (!tun_dst)\n\t\treturn NULL;\n\n\ttun_dst->u.tun_info.options_len = 0;\n\ttun_dst->u.tun_info.mode = 0;\n\treturn tun_dst;\n}\n\nstatic inline struct metadata_dst *tun_dst_unclone(struct sk_buff *skb)\n{\n\tstruct metadata_dst *md_dst = skb_metadata_dst(skb);\n\tint md_size;\n\tstruct metadata_dst *new_md;\n\n\tif (!md_dst || md_dst->type != METADATA_IP_TUNNEL)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tmd_size = md_dst->u.tun_info.options_len;\n\tnew_md = metadata_dst_alloc(md_size, METADATA_IP_TUNNEL, GFP_ATOMIC);\n\tif (!new_md)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmemcpy(&new_md->u.tun_info, &md_dst->u.tun_info,\n\t       sizeof(struct ip_tunnel_info) + md_size);\n\tskb_dst_drop(skb);\n\tdst_hold(&new_md->dst);\n\tskb_dst_set(skb, &new_md->dst);\n\treturn new_md;\n}\n\nstatic inline struct ip_tunnel_info *skb_tunnel_info_unclone(struct sk_buff *skb)\n{\n\tstruct metadata_dst *dst;\n\n\tdst = tun_dst_unclone(skb);\n\tif (IS_ERR(dst))\n\t\treturn NULL;\n\n\treturn &dst->u.tun_info;\n}\n\nstatic inline struct metadata_dst *__ip_tun_set_dst(__be32 saddr,\n\t\t\t\t\t\t    __be32 daddr,\n\t\t\t\t\t\t    __u8 tos, __u8 ttl,\n\t\t\t\t\t\t    __be16 tp_dst,\n\t\t\t\t\t\t    __be16 flags,\n\t\t\t\t\t\t    __be64 tunnel_id,\n\t\t\t\t\t\t    int md_size)\n{\n\tstruct metadata_dst *tun_dst;\n\n\ttun_dst = tun_rx_dst(md_size);\n\tif (!tun_dst)\n\t\treturn NULL;\n\n\tip_tunnel_key_init(&tun_dst->u.tun_info.key,\n\t\t\t   saddr, daddr, tos, ttl,\n\t\t\t   0, 0, tp_dst, tunnel_id, flags);\n\treturn tun_dst;\n}\n\nstatic inline struct metadata_dst *ip_tun_rx_dst(struct sk_buff *skb,\n\t\t\t\t\t\t __be16 flags,\n\t\t\t\t\t\t __be64 tunnel_id,\n\t\t\t\t\t\t int md_size)\n{\n\tconst struct iphdr *iph = ip_hdr(skb);\n\n\treturn __ip_tun_set_dst(iph->saddr, iph->daddr, iph->tos, iph->ttl,\n\t\t\t\t0, flags, tunnel_id, md_size);\n}\n\nstatic inline struct metadata_dst *__ipv6_tun_set_dst(const struct in6_addr *saddr,\n\t\t\t\t\t\t      const struct in6_addr *daddr,\n\t\t\t\t\t\t      __u8 tos, __u8 ttl,\n\t\t\t\t\t\t      __be16 tp_dst,\n\t\t\t\t\t\t      __be32 label,\n\t\t\t\t\t\t      __be16 flags,\n\t\t\t\t\t\t      __be64 tunnel_id,\n\t\t\t\t\t\t      int md_size)\n{\n\tstruct metadata_dst *tun_dst;\n\tstruct ip_tunnel_info *info;\n\n\ttun_dst = tun_rx_dst(md_size);\n\tif (!tun_dst)\n\t\treturn NULL;\n\n\tinfo = &tun_dst->u.tun_info;\n\tinfo->mode = IP_TUNNEL_INFO_IPV6;\n\tinfo->key.tun_flags = flags;\n\tinfo->key.tun_id = tunnel_id;\n\tinfo->key.tp_src = 0;\n\tinfo->key.tp_dst = tp_dst;\n\n\tinfo->key.u.ipv6.src = *saddr;\n\tinfo->key.u.ipv6.dst = *daddr;\n\n\tinfo->key.tos = tos;\n\tinfo->key.ttl = ttl;\n\tinfo->key.label = label;\n\n\treturn tun_dst;\n}\n\nstatic inline struct metadata_dst *ipv6_tun_rx_dst(struct sk_buff *skb,\n\t\t\t\t\t\t   __be16 flags,\n\t\t\t\t\t\t   __be64 tunnel_id,\n\t\t\t\t\t\t   int md_size)\n{\n\tconst struct ipv6hdr *ip6h = ipv6_hdr(skb);\n\n\treturn __ipv6_tun_set_dst(&ip6h->saddr, &ip6h->daddr,\n\t\t\t\t  ipv6_get_dsfield(ip6h), ip6h->hop_limit,\n\t\t\t\t  0, ip6_flowlabel(ip6h), flags, tunnel_id,\n\t\t\t\t  md_size);\n}\n#endif /* __NET_DST_METADATA_H */\n"}}, "reports": [{"events": [{"location": {"col": 2, "file": 1, "line": 5837}, "message": "Left side of '&&' is false"}, {"location": {"col": 13, "file": 2, "line": 628}, "message": "expanded from macro 'list_for_each_entry'"}, {"location": {"col": 2, "file": 2, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 2, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 61, "file": 3, "line": 694}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 2, "file": 1, "line": 5837}, "message": "Taking false branch"}, {"location": {"col": 13, "file": 2, "line": 628}, "message": "expanded from macro 'list_for_each_entry'"}, {"location": {"col": 2, "file": 2, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 2, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 18, "file": 0, "line": 72}, "message": "Access to field 'type' results in a dereference of a null pointer (loaded from variable 'a')"}], "macros": [], "notes": [], "path": "/src/include/net/dst_metadata.h", "reportHash": "3f8bce81f2794021dc516235a1a4d5eb", "checkerName": "clang-analyzer-core.NullDereference", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
