<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"1": {"id": 1, "path": "/src/include/linux/list.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_LIST_H\n#define _LINUX_LIST_H\n\n#include <linux/types.h>\n#include <linux/stddef.h>\n#include <linux/poison.h>\n#include <linux/const.h>\n#include <linux/kernel.h>\n\n/*\n * Circular doubly linked list implementation.\n *\n * Some of the internal functions (\"__xxx\") are useful when\n * manipulating whole lists rather than single entries, as\n * sometimes we already know the next/prev entries and we can\n * generate better code by using them directly rather than\n * using the generic single-entry routines.\n */\n\n#define LIST_HEAD_INIT(name) { &(name), &(name) }\n\n#define LIST_HEAD(name) \\\n\tstruct list_head name = LIST_HEAD_INIT(name)\n\n/**\n * INIT_LIST_HEAD - Initialize a list_head structure\n * @list: list_head structure to be initialized.\n *\n * Initializes the list_head to point to itself.  If it is a list header,\n * the result is an empty list.\n */\nstatic inline void INIT_LIST_HEAD(struct list_head *list)\n{\n\tWRITE_ONCE(list->next, list);\n\tlist->prev = list;\n}\n\n#ifdef CONFIG_DEBUG_LIST\nextern bool __list_add_valid(struct list_head *new,\n\t\t\t      struct list_head *prev,\n\t\t\t      struct list_head *next);\nextern bool __list_del_entry_valid(struct list_head *entry);\n#else\nstatic inline bool __list_add_valid(struct list_head *new,\n\t\t\t\tstruct list_head *prev,\n\t\t\t\tstruct list_head *next)\n{\n\treturn true;\n}\nstatic inline bool __list_del_entry_valid(struct list_head *entry)\n{\n\treturn true;\n}\n#endif\n\n/*\n * Insert a new entry between two known consecutive entries.\n *\n * This is only for internal list manipulation where we know\n * the prev/next entries already!\n */\nstatic inline void __list_add(struct list_head *new,\n\t\t\t      struct list_head *prev,\n\t\t\t      struct list_head *next)\n{\n\tif (!__list_add_valid(new, prev, next))\n\t\treturn;\n\n\tnext->prev = new;\n\tnew->next = next;\n\tnew->prev = prev;\n\tWRITE_ONCE(prev->next, new);\n}\n\n/**\n * list_add - add a new entry\n * @new: new entry to be added\n * @head: list head to add it after\n *\n * Insert a new entry after the specified head.\n * This is good for implementing stacks.\n */\nstatic inline void list_add(struct list_head *new, struct list_head *head)\n{\n\t__list_add(new, head, head->next);\n}\n\n\n/**\n * list_add_tail - add a new entry\n * @new: new entry to be added\n * @head: list head to add it before\n *\n * Insert a new entry before the specified head.\n * This is useful for implementing queues.\n */\nstatic inline void list_add_tail(struct list_head *new, struct list_head *head)\n{\n\t__list_add(new, head->prev, head);\n}\n\n/*\n * Delete a list entry by making the prev/next entries\n * point to each other.\n *\n * This is only for internal list manipulation where we know\n * the prev/next entries already!\n */\nstatic inline void __list_del(struct list_head * prev, struct list_head * next)\n{\n\tnext->prev = prev;\n\tWRITE_ONCE(prev->next, next);\n}\n\n/*\n * Delete a list entry and clear the 'prev' pointer.\n *\n * This is a special-purpose list clearing method used in the networking code\n * for lists allocated as per-cpu, where we don't want to incur the extra\n * WRITE_ONCE() overhead of a regular list_del_init(). The code that uses this\n * needs to check the node 'prev' pointer instead of calling list_empty().\n */\nstatic inline void __list_del_clearprev(struct list_head *entry)\n{\n\t__list_del(entry->prev, entry->next);\n\tentry->prev = NULL;\n}\n\nstatic inline void __list_del_entry(struct list_head *entry)\n{\n\tif (!__list_del_entry_valid(entry))\n\t\treturn;\n\n\t__list_del(entry->prev, entry->next);\n}\n\n/**\n * list_del - deletes entry from list.\n * @entry: the element to delete from the list.\n * Note: list_empty() on entry does not return true after this, the entry is\n * in an undefined state.\n */\nstatic inline void list_del(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tentry->next = LIST_POISON1;\n\tentry->prev = LIST_POISON2;\n}\n\n/**\n * list_replace - replace old entry by new one\n * @old : the element to be replaced\n * @new : the new element to insert\n *\n * If @old was empty, it will be overwritten.\n */\nstatic inline void list_replace(struct list_head *old,\n\t\t\t\tstruct list_head *new)\n{\n\tnew->next = old->next;\n\tnew->next->prev = new;\n\tnew->prev = old->prev;\n\tnew->prev->next = new;\n}\n\n/**\n * list_replace_init - replace old entry by new one and initialize the old one\n * @old : the element to be replaced\n * @new : the new element to insert\n *\n * If @old was empty, it will be overwritten.\n */\nstatic inline void list_replace_init(struct list_head *old,\n\t\t\t\t     struct list_head *new)\n{\n\tlist_replace(old, new);\n\tINIT_LIST_HEAD(old);\n}\n\n/**\n * list_swap - replace entry1 with entry2 and re-add entry1 at entry2's position\n * @entry1: the location to place entry2\n * @entry2: the location to place entry1\n */\nstatic inline void list_swap(struct list_head *entry1,\n\t\t\t     struct list_head *entry2)\n{\n\tstruct list_head *pos = entry2->prev;\n\n\tlist_del(entry2);\n\tlist_replace(entry1, entry2);\n\tif (pos == entry1)\n\t\tpos = entry2;\n\tlist_add(entry1, pos);\n}\n\n/**\n * list_del_init - deletes entry from list and reinitialize it.\n * @entry: the element to delete from the list.\n */\nstatic inline void list_del_init(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tINIT_LIST_HEAD(entry);\n}\n\n/**\n * list_move - delete from one list and add as another's head\n * @list: the entry to move\n * @head: the head that will precede our entry\n */\nstatic inline void list_move(struct list_head *list, struct list_head *head)\n{\n\t__list_del_entry(list);\n\tlist_add(list, head);\n}\n\n/**\n * list_move_tail - delete from one list and add as another's tail\n * @list: the entry to move\n * @head: the head that will follow our entry\n */\nstatic inline void list_move_tail(struct list_head *list,\n\t\t\t\t  struct list_head *head)\n{\n\t__list_del_entry(list);\n\tlist_add_tail(list, head);\n}\n\n/**\n * list_bulk_move_tail - move a subsection of a list to its tail\n * @head: the head that will follow our entry\n * @first: first entry to move\n * @last: last entry to move, can be the same as first\n *\n * Move all entries between @first and including @last before @head.\n * All three entries must belong to the same linked list.\n */\nstatic inline void list_bulk_move_tail(struct list_head *head,\n\t\t\t\t       struct list_head *first,\n\t\t\t\t       struct list_head *last)\n{\n\tfirst->prev->next = last->next;\n\tlast->next->prev = first->prev;\n\n\thead->prev->next = first;\n\tfirst->prev = head->prev;\n\n\tlast->next = head;\n\thead->prev = last;\n}\n\n/**\n * list_is_first -- tests whether @list is the first entry in list @head\n * @list: the entry to test\n * @head: the head of the list\n */\nstatic inline int list_is_first(const struct list_head *list,\n\t\t\t\t\tconst struct list_head *head)\n{\n\treturn list->prev == head;\n}\n\n/**\n * list_is_last - tests whether @list is the last entry in list @head\n * @list: the entry to test\n * @head: the head of the list\n */\nstatic inline int list_is_last(const struct list_head *list,\n\t\t\t\tconst struct list_head *head)\n{\n\treturn list->next == head;\n}\n\n/**\n * list_empty - tests whether a list is empty\n * @head: the list to test.\n */\nstatic inline int list_empty(const struct list_head *head)\n{\n\treturn READ_ONCE(head->next) == head;\n}\n\n/**\n * list_del_init_careful - deletes entry from list and reinitialize it.\n * @entry: the element to delete from the list.\n *\n * This is the same as list_del_init(), except designed to be used\n * together with list_empty_careful() in a way to guarantee ordering\n * of other memory operations.\n *\n * Any memory operations done before a list_del_init_careful() are\n * guaranteed to be visible after a list_empty_careful() test.\n */\nstatic inline void list_del_init_careful(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tentry->prev = entry;\n\tsmp_store_release(&entry->next, entry);\n}\n\n/**\n * list_empty_careful - tests whether a list is empty and not being modified\n * @head: the list to test\n *\n * Description:\n * tests whether a list is empty _and_ checks that no other CPU might be\n * in the process of modifying either member (next or prev)\n *\n * NOTE: using list_empty_careful() without synchronization\n * can only be safe if the only activity that can happen\n * to the list entry is list_del_init(). Eg. it cannot be used\n * if another CPU could re-list_add() it.\n */\nstatic inline int list_empty_careful(const struct list_head *head)\n{\n\tstruct list_head *next = smp_load_acquire(&head->next);\n\treturn (next == head) && (next == head->prev);\n}\n\n/**\n * list_rotate_left - rotate the list to the left\n * @head: the head of the list\n */\nstatic inline void list_rotate_left(struct list_head *head)\n{\n\tstruct list_head *first;\n\n\tif (!list_empty(head)) {\n\t\tfirst = head->next;\n\t\tlist_move_tail(first, head);\n\t}\n}\n\n/**\n * list_rotate_to_front() - Rotate list to specific item.\n * @list: The desired new front of the list.\n * @head: The head of the list.\n *\n * Rotates list so that @list becomes the new front of the list.\n */\nstatic inline void list_rotate_to_front(struct list_head *list,\n\t\t\t\t\tstruct list_head *head)\n{\n\t/*\n\t * Deletes the list head from the list denoted by @head and\n\t * places it as the tail of @list, this effectively rotates the\n\t * list so that @list is at the front.\n\t */\n\tlist_move_tail(head, list);\n}\n\n/**\n * list_is_singular - tests whether a list has just one entry.\n * @head: the list to test.\n */\nstatic inline int list_is_singular(const struct list_head *head)\n{\n\treturn !list_empty(head) && (head->next == head->prev);\n}\n\nstatic inline void __list_cut_position(struct list_head *list,\n\t\tstruct list_head *head, struct list_head *entry)\n{\n\tstruct list_head *new_first = entry->next;\n\tlist->next = head->next;\n\tlist->next->prev = list;\n\tlist->prev = entry;\n\tentry->next = list;\n\thead->next = new_first;\n\tnew_first->prev = head;\n}\n\n/**\n * list_cut_position - cut a list into two\n * @list: a new list to add all removed entries\n * @head: a list with entries\n * @entry: an entry within head, could be the head itself\n *\tand if so we won't cut the list\n *\n * This helper moves the initial part of @head, up to and\n * including @entry, from @head to @list. You should\n * pass on @entry an element you know is on @head. @list\n * should be an empty list or a list you do not care about\n * losing its data.\n *\n */\nstatic inline void list_cut_position(struct list_head *list,\n\t\tstruct list_head *head, struct list_head *entry)\n{\n\tif (list_empty(head))\n\t\treturn;\n\tif (list_is_singular(head) &&\n\t\t(head->next != entry && head != entry))\n\t\treturn;\n\tif (entry == head)\n\t\tINIT_LIST_HEAD(list);\n\telse\n\t\t__list_cut_position(list, head, entry);\n}\n\n/**\n * list_cut_before - cut a list into two, before given entry\n * @list: a new list to add all removed entries\n * @head: a list with entries\n * @entry: an entry within head, could be the head itself\n *\n * This helper moves the initial part of @head, up to but\n * excluding @entry, from @head to @list.  You should pass\n * in @entry an element you know is on @head.  @list should\n * be an empty list or a list you do not care about losing\n * its data.\n * If @entry == @head, all entries on @head are moved to\n * @list.\n */\nstatic inline void list_cut_before(struct list_head *list,\n\t\t\t\t   struct list_head *head,\n\t\t\t\t   struct list_head *entry)\n{\n\tif (head->next == entry) {\n\t\tINIT_LIST_HEAD(list);\n\t\treturn;\n\t}\n\tlist->next = head->next;\n\tlist->next->prev = list;\n\tlist->prev = entry->prev;\n\tlist->prev->next = list;\n\thead->next = entry;\n\tentry->prev = head;\n}\n\nstatic inline void __list_splice(const struct list_head *list,\n\t\t\t\t struct list_head *prev,\n\t\t\t\t struct list_head *next)\n{\n\tstruct list_head *first = list->next;\n\tstruct list_head *last = list->prev;\n\n\tfirst->prev = prev;\n\tprev->next = first;\n\n\tlast->next = next;\n\tnext->prev = last;\n}\n\n/**\n * list_splice - join two lists, this is designed for stacks\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n */\nstatic inline void list_splice(const struct list_head *list,\n\t\t\t\tstruct list_head *head)\n{\n\tif (!list_empty(list))\n\t\t__list_splice(list, head, head->next);\n}\n\n/**\n * list_splice_tail - join two lists, each list being a queue\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n */\nstatic inline void list_splice_tail(struct list_head *list,\n\t\t\t\tstruct list_head *head)\n{\n\tif (!list_empty(list))\n\t\t__list_splice(list, head->prev, head);\n}\n\n/**\n * list_splice_init - join two lists and reinitialise the emptied list.\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n *\n * The list at @list is reinitialised\n */\nstatic inline void list_splice_init(struct list_head *list,\n\t\t\t\t    struct list_head *head)\n{\n\tif (!list_empty(list)) {\n\t\t__list_splice(list, head, head->next);\n\t\tINIT_LIST_HEAD(list);\n\t}\n}\n\n/**\n * list_splice_tail_init - join two lists and reinitialise the emptied list\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n *\n * Each of the lists is a queue.\n * The list at @list is reinitialised\n */\nstatic inline void list_splice_tail_init(struct list_head *list,\n\t\t\t\t\t struct list_head *head)\n{\n\tif (!list_empty(list)) {\n\t\t__list_splice(list, head->prev, head);\n\t\tINIT_LIST_HEAD(list);\n\t}\n}\n\n/**\n * list_entry - get the struct for this entry\n * @ptr:\tthe &struct list_head pointer.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_entry(ptr, type, member) \\\n\tcontainer_of(ptr, type, member)\n\n/**\n * list_first_entry - get the first element from a list\n * @ptr:\tthe list head to take the element from.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n *\n * Note, that list is expected to be not empty.\n */\n#define list_first_entry(ptr, type, member) \\\n\tlist_entry((ptr)->next, type, member)\n\n/**\n * list_last_entry - get the last element from a list\n * @ptr:\tthe list head to take the element from.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n *\n * Note, that list is expected to be not empty.\n */\n#define list_last_entry(ptr, type, member) \\\n\tlist_entry((ptr)->prev, type, member)\n\n/**\n * list_first_entry_or_null - get the first element from a list\n * @ptr:\tthe list head to take the element from.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n *\n * Note that if the list is empty, it returns NULL.\n */\n#define list_first_entry_or_null(ptr, type, member) ({ \\\n\tstruct list_head *head__ = (ptr); \\\n\tstruct list_head *pos__ = READ_ONCE(head__->next); \\\n\tpos__ != head__ ? list_entry(pos__, type, member) : NULL; \\\n})\n\n/**\n * list_next_entry - get the next element in list\n * @pos:\tthe type * to cursor\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_next_entry(pos, member) \\\n\tlist_entry((pos)->member.next, typeof(*(pos)), member)\n\n/**\n * list_prev_entry - get the prev element in list\n * @pos:\tthe type * to cursor\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_prev_entry(pos, member) \\\n\tlist_entry((pos)->member.prev, typeof(*(pos)), member)\n\n/**\n * list_for_each\t-\titerate over a list\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @head:\tthe head for your list.\n */\n#define list_for_each(pos, head) \\\n\tfor (pos = (head)->next; pos != (head); pos = pos->next)\n\n/**\n * list_for_each_continue - continue iteration over a list\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @head:\tthe head for your list.\n *\n * Continue to iterate over a list, continuing after the current position.\n */\n#define list_for_each_continue(pos, head) \\\n\tfor (pos = pos->next; pos != (head); pos = pos->next)\n\n/**\n * list_for_each_prev\t-\titerate over a list backwards\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @head:\tthe head for your list.\n */\n#define list_for_each_prev(pos, head) \\\n\tfor (pos = (head)->prev; pos != (head); pos = pos->prev)\n\n/**\n * list_for_each_safe - iterate over a list safe against removal of list entry\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @n:\t\tanother &struct list_head to use as temporary storage\n * @head:\tthe head for your list.\n */\n#define list_for_each_safe(pos, n, head) \\\n\tfor (pos = (head)->next, n = pos->next; pos != (head); \\\n\t\tpos = n, n = pos->next)\n\n/**\n * list_for_each_prev_safe - iterate over a list backwards safe against removal of list entry\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @n:\t\tanother &struct list_head to use as temporary storage\n * @head:\tthe head for your list.\n */\n#define list_for_each_prev_safe(pos, n, head) \\\n\tfor (pos = (head)->prev, n = pos->prev; \\\n\t     pos != (head); \\\n\t     pos = n, n = pos->prev)\n\n/**\n * list_entry_is_head - test if the entry points to the head of the list\n * @pos:\tthe type * to cursor\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_entry_is_head(pos, head, member)\t\t\t\t\\\n\t(&pos->member == (head))\n\n/**\n * list_for_each_entry\t-\titerate over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_for_each_entry(pos, head, member)\t\t\t\t\\\n\tfor (pos = list_first_entry(head, typeof(*pos), member);\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_next_entry(pos, member))\n\n/**\n * list_for_each_entry_reverse - iterate backwards over list of given type.\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_for_each_entry_reverse(pos, head, member)\t\t\t\\\n\tfor (pos = list_last_entry(head, typeof(*pos), member);\t\t\\\n\t     !list_entry_is_head(pos, head, member); \t\t\t\\\n\t     pos = list_prev_entry(pos, member))\n\n/**\n * list_prepare_entry - prepare a pos entry for use in list_for_each_entry_continue()\n * @pos:\tthe type * to use as a start point\n * @head:\tthe head of the list\n * @member:\tthe name of the list_head within the struct.\n *\n * Prepares a pos entry for use as a start point in list_for_each_entry_continue().\n */\n#define list_prepare_entry(pos, head, member) \\\n\t((pos) ? : list_entry(head, typeof(*pos), member))\n\n/**\n * list_for_each_entry_continue - continue iteration over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Continue to iterate over list of given type, continuing after\n * the current position.\n */\n#define list_for_each_entry_continue(pos, head, member) \t\t\\\n\tfor (pos = list_next_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_next_entry(pos, member))\n\n/**\n * list_for_each_entry_continue_reverse - iterate backwards from the given point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Start to iterate over list of given type backwards, continuing after\n * the current position.\n */\n#define list_for_each_entry_continue_reverse(pos, head, member)\t\t\\\n\tfor (pos = list_prev_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_prev_entry(pos, member))\n\n/**\n * list_for_each_entry_from - iterate over list of given type from the current point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate over list of given type, continuing from current position.\n */\n#define list_for_each_entry_from(pos, head, member) \t\t\t\\\n\tfor (; !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_next_entry(pos, member))\n\n/**\n * list_for_each_entry_from_reverse - iterate backwards over list of given type\n *                                    from the current point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate backwards over list of given type, continuing from current position.\n */\n#define list_for_each_entry_from_reverse(pos, head, member)\t\t\\\n\tfor (; !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_prev_entry(pos, member))\n\n/**\n * list_for_each_entry_safe - iterate over list of given type safe against removal of list entry\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_for_each_entry_safe(pos, n, head, member)\t\t\t\\\n\tfor (pos = list_first_entry(head, typeof(*pos), member),\t\\\n\t\tn = list_next_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member); \t\t\t\\\n\t     pos = n, n = list_next_entry(n, member))\n\n/**\n * list_for_each_entry_safe_continue - continue list iteration safe against removal\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate over list of given type, continuing after current point,\n * safe against removal of list entry.\n */\n#define list_for_each_entry_safe_continue(pos, n, head, member) \t\t\\\n\tfor (pos = list_next_entry(pos, member), \t\t\t\t\\\n\t\tn = list_next_entry(pos, member);\t\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\t\\\n\t     pos = n, n = list_next_entry(n, member))\n\n/**\n * list_for_each_entry_safe_from - iterate over list from current point safe against removal\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate over list of given type from current point, safe against\n * removal of list entry.\n */\n#define list_for_each_entry_safe_from(pos, n, head, member) \t\t\t\\\n\tfor (n = list_next_entry(pos, member);\t\t\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\t\\\n\t     pos = n, n = list_next_entry(n, member))\n\n/**\n * list_for_each_entry_safe_reverse - iterate backwards over list safe against removal\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate backwards over list of given type, safe against removal\n * of list entry.\n */\n#define list_for_each_entry_safe_reverse(pos, n, head, member)\t\t\\\n\tfor (pos = list_last_entry(head, typeof(*pos), member),\t\t\\\n\t\tn = list_prev_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member); \t\t\t\\\n\t     pos = n, n = list_prev_entry(n, member))\n\n/**\n * list_safe_reset_next - reset a stale list_for_each_entry_safe loop\n * @pos:\tthe loop cursor used in the list_for_each_entry_safe loop\n * @n:\t\ttemporary storage used in list_for_each_entry_safe\n * @member:\tthe name of the list_head within the struct.\n *\n * list_safe_reset_next is not safe to use in general if the list may be\n * modified concurrently (eg. the lock is dropped in the loop body). An\n * exception to this is if the cursor element (pos) is pinned in the list,\n * and list_safe_reset_next is called after re-taking the lock and before\n * completing the current iteration of the loop body.\n */\n#define list_safe_reset_next(pos, n, member)\t\t\t\t\\\n\tn = list_next_entry(pos, member)\n\n/*\n * Double linked lists with a single pointer list head.\n * Mostly useful for hash tables where the two pointer list head is\n * too wasteful.\n * You lose the ability to access the tail in O(1).\n */\n\n#define HLIST_HEAD_INIT { .first = NULL }\n#define HLIST_HEAD(name) struct hlist_head name = {  .first = NULL }\n#define INIT_HLIST_HEAD(ptr) ((ptr)->first = NULL)\nstatic inline void INIT_HLIST_NODE(struct hlist_node *h)\n{\n\th->next = NULL;\n\th->pprev = NULL;\n}\n\n/**\n * hlist_unhashed - Has node been removed from list and reinitialized?\n * @h: Node to be checked\n *\n * Not that not all removal functions will leave a node in unhashed\n * state.  For example, hlist_nulls_del_init_rcu() does leave the\n * node in unhashed state, but hlist_nulls_del() does not.\n */\nstatic inline int hlist_unhashed(const struct hlist_node *h)\n{\n\treturn !h->pprev;\n}\n\n/**\n * hlist_unhashed_lockless - Version of hlist_unhashed for lockless use\n * @h: Node to be checked\n *\n * This variant of hlist_unhashed() must be used in lockless contexts\n * to avoid potential load-tearing.  The READ_ONCE() is paired with the\n * various WRITE_ONCE() in hlist helpers that are defined below.\n */\nstatic inline int hlist_unhashed_lockless(const struct hlist_node *h)\n{\n\treturn !READ_ONCE(h->pprev);\n}\n\n/**\n * hlist_empty - Is the specified hlist_head structure an empty hlist?\n * @h: Structure to check.\n */\nstatic inline int hlist_empty(const struct hlist_head *h)\n{\n\treturn !READ_ONCE(h->first);\n}\n\nstatic inline void __hlist_del(struct hlist_node *n)\n{\n\tstruct hlist_node *next = n->next;\n\tstruct hlist_node **pprev = n->pprev;\n\n\tWRITE_ONCE(*pprev, next);\n\tif (next)\n\t\tWRITE_ONCE(next->pprev, pprev);\n}\n\n/**\n * hlist_del - Delete the specified hlist_node from its list\n * @n: Node to delete.\n *\n * Note that this function leaves the node in hashed state.  Use\n * hlist_del_init() or similar instead to unhash @n.\n */\nstatic inline void hlist_del(struct hlist_node *n)\n{\n\t__hlist_del(n);\n\tn->next = LIST_POISON1;\n\tn->pprev = LIST_POISON2;\n}\n\n/**\n * hlist_del_init - Delete the specified hlist_node from its list and initialize\n * @n: Node to delete.\n *\n * Note that this function leaves the node in unhashed state.\n */\nstatic inline void hlist_del_init(struct hlist_node *n)\n{\n\tif (!hlist_unhashed(n)) {\n\t\t__hlist_del(n);\n\t\tINIT_HLIST_NODE(n);\n\t}\n}\n\n/**\n * hlist_add_head - add a new entry at the beginning of the hlist\n * @n: new entry to be added\n * @h: hlist head to add it after\n *\n * Insert a new entry after the specified head.\n * This is good for implementing stacks.\n */\nstatic inline void hlist_add_head(struct hlist_node *n, struct hlist_head *h)\n{\n\tstruct hlist_node *first = h->first;\n\tWRITE_ONCE(n->next, first);\n\tif (first)\n\t\tWRITE_ONCE(first->pprev, &n->next);\n\tWRITE_ONCE(h->first, n);\n\tWRITE_ONCE(n->pprev, &h->first);\n}\n\n/**\n * hlist_add_before - add a new entry before the one specified\n * @n: new entry to be added\n * @next: hlist node to add it before, which must be non-NULL\n */\nstatic inline void hlist_add_before(struct hlist_node *n,\n\t\t\t\t    struct hlist_node *next)\n{\n\tWRITE_ONCE(n->pprev, next->pprev);\n\tWRITE_ONCE(n->next, next);\n\tWRITE_ONCE(next->pprev, &n->next);\n\tWRITE_ONCE(*(n->pprev), n);\n}\n\n/**\n * hlist_add_behind - add a new entry after the one specified\n * @n: new entry to be added\n * @prev: hlist node to add it after, which must be non-NULL\n */\nstatic inline void hlist_add_behind(struct hlist_node *n,\n\t\t\t\t    struct hlist_node *prev)\n{\n\tWRITE_ONCE(n->next, prev->next);\n\tWRITE_ONCE(prev->next, n);\n\tWRITE_ONCE(n->pprev, &prev->next);\n\n\tif (n->next)\n\t\tWRITE_ONCE(n->next->pprev, &n->next);\n}\n\n/**\n * hlist_add_fake - create a fake hlist consisting of a single headless node\n * @n: Node to make a fake list out of\n *\n * This makes @n appear to be its own predecessor on a headless hlist.\n * The point of this is to allow things like hlist_del() to work correctly\n * in cases where there is no list.\n */\nstatic inline void hlist_add_fake(struct hlist_node *n)\n{\n\tn->pprev = &n->next;\n}\n\n/**\n * hlist_fake: Is this node a fake hlist?\n * @h: Node to check for being a self-referential fake hlist.\n */\nstatic inline bool hlist_fake(struct hlist_node *h)\n{\n\treturn h->pprev == &h->next;\n}\n\n/**\n * hlist_is_singular_node - is node the only element of the specified hlist?\n * @n: Node to check for singularity.\n * @h: Header for potentially singular list.\n *\n * Check whether the node is the only node of the head without\n * accessing head, thus avoiding unnecessary cache misses.\n */\nstatic inline bool\nhlist_is_singular_node(struct hlist_node *n, struct hlist_head *h)\n{\n\treturn !n->next && n->pprev == &h->first;\n}\n\n/**\n * hlist_move_list - Move an hlist\n * @old: hlist_head for old list.\n * @new: hlist_head for new list.\n *\n * Move a list from one list head to another. Fixup the pprev\n * reference of the first entry if it exists.\n */\nstatic inline void hlist_move_list(struct hlist_head *old,\n\t\t\t\t   struct hlist_head *new)\n{\n\tnew->first = old->first;\n\tif (new->first)\n\t\tnew->first->pprev = &new->first;\n\told->first = NULL;\n}\n\n#define hlist_entry(ptr, type, member) container_of(ptr,type,member)\n\n#define hlist_for_each(pos, head) \\\n\tfor (pos = (head)->first; pos ; pos = pos->next)\n\n#define hlist_for_each_safe(pos, n, head) \\\n\tfor (pos = (head)->first; pos && ({ n = pos->next; 1; }); \\\n\t     pos = n)\n\n#define hlist_entry_safe(ptr, type, member) \\\n\t({ typeof(ptr) ____ptr = (ptr); \\\n\t   ____ptr ? hlist_entry(____ptr, type, member) : NULL; \\\n\t})\n\n/**\n * hlist_for_each_entry\t- iterate over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry(pos, head, member)\t\t\t\t\\\n\tfor (pos = hlist_entry_safe((head)->first, typeof(*(pos)), member);\\\n\t     pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_continue - iterate over a hlist continuing after current point\n * @pos:\tthe type * to use as a loop cursor.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_continue(pos, member)\t\t\t\\\n\tfor (pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member);\\\n\t     pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_from - iterate over a hlist continuing from current point\n * @pos:\tthe type * to use as a loop cursor.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_from(pos, member)\t\t\t\t\\\n\tfor (; pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_safe - iterate over list of given type safe against removal of list entry\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\ta &struct hlist_node to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_safe(pos, n, head, member) \t\t\\\n\tfor (pos = hlist_entry_safe((head)->first, typeof(*pos), member);\\\n\t     pos && ({ n = pos->member.next; 1; });\t\t\t\\\n\t     pos = hlist_entry_safe(n, typeof(*pos), member))\n\n#endif\n"}, "2": {"id": 2, "path": "/src/include/linux/kernel.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_KERNEL_H\n#define _LINUX_KERNEL_H\n\n#include <stdarg.h>\n#include <linux/limits.h>\n#include <linux/linkage.h>\n#include <linux/stddef.h>\n#include <linux/types.h>\n#include <linux/compiler.h>\n#include <linux/bitops.h>\n#include <linux/log2.h>\n#include <linux/math.h>\n#include <linux/minmax.h>\n#include <linux/typecheck.h>\n#include <linux/printk.h>\n#include <linux/build_bug.h>\n#include <linux/static_call_types.h>\n#include <asm/byteorder.h>\n\n#include <uapi/linux/kernel.h>\n\n#define STACK_MAGIC\t0xdeadbeef\n\n/**\n * REPEAT_BYTE - repeat the value @x multiple times as an unsigned long value\n * @x: value to repeat\n *\n * NOTE: @x is not checked for > 0xff; larger values produce odd results.\n */\n#define REPEAT_BYTE(x)\t((~0ul / 0xff) * (x))\n\n/* @a is a power of 2 value */\n#define ALIGN(x, a)\t\t__ALIGN_KERNEL((x), (a))\n#define ALIGN_DOWN(x, a)\t__ALIGN_KERNEL((x) - ((a) - 1), (a))\n#define __ALIGN_MASK(x, mask)\t__ALIGN_KERNEL_MASK((x), (mask))\n#define PTR_ALIGN(p, a)\t\t((typeof(p))ALIGN((unsigned long)(p), (a)))\n#define PTR_ALIGN_DOWN(p, a)\t((typeof(p))ALIGN_DOWN((unsigned long)(p), (a)))\n#define IS_ALIGNED(x, a)\t\t(((x) & ((typeof(x))(a) - 1)) == 0)\n\n/* generic data direction definitions */\n#define READ\t\t\t0\n#define WRITE\t\t\t1\n\n/**\n * ARRAY_SIZE - get the number of elements in array @arr\n * @arr: array to be sized\n */\n#define ARRAY_SIZE(arr) (sizeof(arr) / sizeof((arr)[0]) + __must_be_array(arr))\n\n#define u64_to_user_ptr(x) (\t\t\\\n{\t\t\t\t\t\\\n\ttypecheck(u64, (x));\t\t\\\n\t(void __user *)(uintptr_t)(x);\t\\\n}\t\t\t\t\t\\\n)\n\n#define typeof_member(T, m)\ttypeof(((T*)0)->m)\n\n#define _RET_IP_\t\t(unsigned long)__builtin_return_address(0)\n#define _THIS_IP_  ({ __label__ __here; __here: (unsigned long)&&__here; })\n\n/**\n * upper_32_bits - return bits 32-63 of a number\n * @n: the number we're accessing\n *\n * A basic shift-right of a 64- or 32-bit quantity.  Use this to suppress\n * the \"right shift count >= width of type\" warning when that quantity is\n * 32-bits.\n */\n#define upper_32_bits(n) ((u32)(((n) >> 16) >> 16))\n\n/**\n * lower_32_bits - return bits 0-31 of a number\n * @n: the number we're accessing\n */\n#define lower_32_bits(n) ((u32)((n) & 0xffffffff))\n\nstruct completion;\nstruct pt_regs;\nstruct user;\n\n#ifdef CONFIG_PREEMPT_VOLUNTARY\n\nextern int __cond_resched(void);\n# define might_resched() __cond_resched()\n\n#elif defined(CONFIG_PREEMPT_DYNAMIC)\n\nextern int __cond_resched(void);\n\nDECLARE_STATIC_CALL(might_resched, __cond_resched);\n\nstatic __always_inline void might_resched(void)\n{\n\tstatic_call_mod(might_resched)();\n}\n\n#else\n\n# define might_resched() do { } while (0)\n\n#endif /* CONFIG_PREEMPT_* */\n\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\nextern void ___might_sleep(const char *file, int line, int preempt_offset);\nextern void __might_sleep(const char *file, int line, int preempt_offset);\nextern void __cant_sleep(const char *file, int line, int preempt_offset);\nextern void __cant_migrate(const char *file, int line);\n\n/**\n * might_sleep - annotation for functions that can sleep\n *\n * this macro will print a stack trace if it is executed in an atomic\n * context (spinlock, irq-handler, ...). Additional sections where blocking is\n * not allowed can be annotated with non_block_start() and non_block_end()\n * pairs.\n *\n * This is a useful debugging help to be able to catch problems early and not\n * be bitten later when the calling function happens to sleep when it is not\n * supposed to.\n */\n# define might_sleep() \\\n\tdo { __might_sleep(__FILE__, __LINE__, 0); might_resched(); } while (0)\n/**\n * cant_sleep - annotation for functions that cannot sleep\n *\n * this macro will print a stack trace if it is executed with preemption enabled\n */\n# define cant_sleep() \\\n\tdo { __cant_sleep(__FILE__, __LINE__, 0); } while (0)\n# define sched_annotate_sleep()\t(current->task_state_change = 0)\n\n/**\n * cant_migrate - annotation for functions that cannot migrate\n *\n * Will print a stack trace if executed in code which is migratable\n */\n# define cant_migrate()\t\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (IS_ENABLED(CONFIG_SMP))\t\t\t\t\\\n\t\t\t__cant_migrate(__FILE__, __LINE__);\t\t\\\n\t} while (0)\n\n/**\n * non_block_start - annotate the start of section where sleeping is prohibited\n *\n * This is on behalf of the oom reaper, specifically when it is calling the mmu\n * notifiers. The problem is that if the notifier were to block on, for example,\n * mutex_lock() and if the process which holds that mutex were to perform a\n * sleeping memory allocation, the oom reaper is now blocked on completion of\n * that memory allocation. Other blocking calls like wait_event() pose similar\n * issues.\n */\n# define non_block_start() (current->non_block_count++)\n/**\n * non_block_end - annotate the end of section where sleeping is prohibited\n *\n * Closes a section opened by non_block_start().\n */\n# define non_block_end() WARN_ON(current->non_block_count-- == 0)\n#else\n  static inline void ___might_sleep(const char *file, int line,\n\t\t\t\t   int preempt_offset) { }\n  static inline void __might_sleep(const char *file, int line,\n\t\t\t\t   int preempt_offset) { }\n# define might_sleep() do { might_resched(); } while (0)\n# define cant_sleep() do { } while (0)\n# define cant_migrate()\t\tdo { } while (0)\n# define sched_annotate_sleep() do { } while (0)\n# define non_block_start() do { } while (0)\n# define non_block_end() do { } while (0)\n#endif\n\n#define might_sleep_if(cond) do { if (cond) might_sleep(); } while (0)\n\n#if defined(CONFIG_MMU) && \\\n\t(defined(CONFIG_PROVE_LOCKING) || defined(CONFIG_DEBUG_ATOMIC_SLEEP))\n#define might_fault() __might_fault(__FILE__, __LINE__)\nvoid __might_fault(const char *file, int line);\n#else\nstatic inline void might_fault(void) { }\n#endif\n\nextern struct atomic_notifier_head panic_notifier_list;\nextern long (*panic_blink)(int state);\n__printf(1, 2)\nvoid panic(const char *fmt, ...) __noreturn __cold;\nvoid nmi_panic(struct pt_regs *regs, const char *msg);\nextern void oops_enter(void);\nextern void oops_exit(void);\nextern bool oops_may_print(void);\nvoid do_exit(long error_code) __noreturn;\nvoid complete_and_exit(struct completion *, long) __noreturn;\n\n/* Internal, do not use. */\nint __must_check _kstrtoul(const char *s, unsigned int base, unsigned long *res);\nint __must_check _kstrtol(const char *s, unsigned int base, long *res);\n\nint __must_check kstrtoull(const char *s, unsigned int base, unsigned long long *res);\nint __must_check kstrtoll(const char *s, unsigned int base, long long *res);\n\n/**\n * kstrtoul - convert a string to an unsigned long\n * @s: The start of the string. The string must be null-terminated, and may also\n *  include a single newline before its terminating null. The first character\n *  may also be a plus sign, but not a minus sign.\n * @base: The number base to use. The maximum supported base is 16. If base is\n *  given as 0, then the base of the string is automatically detected with the\n *  conventional semantics - If it begins with 0x the number will be parsed as a\n *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be\n *  parsed as an octal number. Otherwise it will be parsed as a decimal.\n * @res: Where to write the result of the conversion on success.\n *\n * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.\n * Preferred over simple_strtoul(). Return code must be checked.\n*/\nstatic inline int __must_check kstrtoul(const char *s, unsigned int base, unsigned long *res)\n{\n\t/*\n\t * We want to shortcut function call, but\n\t * __builtin_types_compatible_p(unsigned long, unsigned long long) = 0.\n\t */\n\tif (sizeof(unsigned long) == sizeof(unsigned long long) &&\n\t    __alignof__(unsigned long) == __alignof__(unsigned long long))\n\t\treturn kstrtoull(s, base, (unsigned long long *)res);\n\telse\n\t\treturn _kstrtoul(s, base, res);\n}\n\n/**\n * kstrtol - convert a string to a long\n * @s: The start of the string. The string must be null-terminated, and may also\n *  include a single newline before its terminating null. The first character\n *  may also be a plus sign or a minus sign.\n * @base: The number base to use. The maximum supported base is 16. If base is\n *  given as 0, then the base of the string is automatically detected with the\n *  conventional semantics - If it begins with 0x the number will be parsed as a\n *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be\n *  parsed as an octal number. Otherwise it will be parsed as a decimal.\n * @res: Where to write the result of the conversion on success.\n *\n * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.\n * Preferred over simple_strtol(). Return code must be checked.\n */\nstatic inline int __must_check kstrtol(const char *s, unsigned int base, long *res)\n{\n\t/*\n\t * We want to shortcut function call, but\n\t * __builtin_types_compatible_p(long, long long) = 0.\n\t */\n\tif (sizeof(long) == sizeof(long long) &&\n\t    __alignof__(long) == __alignof__(long long))\n\t\treturn kstrtoll(s, base, (long long *)res);\n\telse\n\t\treturn _kstrtol(s, base, res);\n}\n\nint __must_check kstrtouint(const char *s, unsigned int base, unsigned int *res);\nint __must_check kstrtoint(const char *s, unsigned int base, int *res);\n\nstatic inline int __must_check kstrtou64(const char *s, unsigned int base, u64 *res)\n{\n\treturn kstrtoull(s, base, res);\n}\n\nstatic inline int __must_check kstrtos64(const char *s, unsigned int base, s64 *res)\n{\n\treturn kstrtoll(s, base, res);\n}\n\nstatic inline int __must_check kstrtou32(const char *s, unsigned int base, u32 *res)\n{\n\treturn kstrtouint(s, base, res);\n}\n\nstatic inline int __must_check kstrtos32(const char *s, unsigned int base, s32 *res)\n{\n\treturn kstrtoint(s, base, res);\n}\n\nint __must_check kstrtou16(const char *s, unsigned int base, u16 *res);\nint __must_check kstrtos16(const char *s, unsigned int base, s16 *res);\nint __must_check kstrtou8(const char *s, unsigned int base, u8 *res);\nint __must_check kstrtos8(const char *s, unsigned int base, s8 *res);\nint __must_check kstrtobool(const char *s, bool *res);\n\nint __must_check kstrtoull_from_user(const char __user *s, size_t count, unsigned int base, unsigned long long *res);\nint __must_check kstrtoll_from_user(const char __user *s, size_t count, unsigned int base, long long *res);\nint __must_check kstrtoul_from_user(const char __user *s, size_t count, unsigned int base, unsigned long *res);\nint __must_check kstrtol_from_user(const char __user *s, size_t count, unsigned int base, long *res);\nint __must_check kstrtouint_from_user(const char __user *s, size_t count, unsigned int base, unsigned int *res);\nint __must_check kstrtoint_from_user(const char __user *s, size_t count, unsigned int base, int *res);\nint __must_check kstrtou16_from_user(const char __user *s, size_t count, unsigned int base, u16 *res);\nint __must_check kstrtos16_from_user(const char __user *s, size_t count, unsigned int base, s16 *res);\nint __must_check kstrtou8_from_user(const char __user *s, size_t count, unsigned int base, u8 *res);\nint __must_check kstrtos8_from_user(const char __user *s, size_t count, unsigned int base, s8 *res);\nint __must_check kstrtobool_from_user(const char __user *s, size_t count, bool *res);\n\nstatic inline int __must_check kstrtou64_from_user(const char __user *s, size_t count, unsigned int base, u64 *res)\n{\n\treturn kstrtoull_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtos64_from_user(const char __user *s, size_t count, unsigned int base, s64 *res)\n{\n\treturn kstrtoll_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtou32_from_user(const char __user *s, size_t count, unsigned int base, u32 *res)\n{\n\treturn kstrtouint_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtos32_from_user(const char __user *s, size_t count, unsigned int base, s32 *res)\n{\n\treturn kstrtoint_from_user(s, count, base, res);\n}\n\n/*\n * Use kstrto<foo> instead.\n *\n * NOTE: simple_strto<foo> does not check for the range overflow and,\n *\t depending on the input, may give interesting results.\n *\n * Use these functions if and only if you cannot use kstrto<foo>, because\n * the conversion ends on the first non-digit character, which may be far\n * beyond the supported range. It might be useful to parse the strings like\n * 10x50 or 12:21 without altering original string or temporary buffer in use.\n * Keep in mind above caveat.\n */\n\nextern unsigned long simple_strtoul(const char *,char **,unsigned int);\nextern long simple_strtol(const char *,char **,unsigned int);\nextern unsigned long long simple_strtoull(const char *,char **,unsigned int);\nextern long long simple_strtoll(const char *,char **,unsigned int);\n\nextern int num_to_str(char *buf, int size,\n\t\t      unsigned long long num, unsigned int width);\n\n/* lib/printf utilities */\n\nextern __printf(2, 3) int sprintf(char *buf, const char * fmt, ...);\nextern __printf(2, 0) int vsprintf(char *buf, const char *, va_list);\nextern __printf(3, 4)\nint snprintf(char *buf, size_t size, const char *fmt, ...);\nextern __printf(3, 0)\nint vsnprintf(char *buf, size_t size, const char *fmt, va_list args);\nextern __printf(3, 4)\nint scnprintf(char *buf, size_t size, const char *fmt, ...);\nextern __printf(3, 0)\nint vscnprintf(char *buf, size_t size, const char *fmt, va_list args);\nextern __printf(2, 3) __malloc\nchar *kasprintf(gfp_t gfp, const char *fmt, ...);\nextern __printf(2, 0) __malloc\nchar *kvasprintf(gfp_t gfp, const char *fmt, va_list args);\nextern __printf(2, 0)\nconst char *kvasprintf_const(gfp_t gfp, const char *fmt, va_list args);\n\nextern __scanf(2, 3)\nint sscanf(const char *, const char *, ...);\nextern __scanf(2, 0)\nint vsscanf(const char *, const char *, va_list);\n\nextern int get_option(char **str, int *pint);\nextern char *get_options(const char *str, int nints, int *ints);\nextern unsigned long long memparse(const char *ptr, char **retptr);\nextern bool parse_option_str(const char *str, const char *option);\nextern char *next_arg(char *args, char **param, char **val);\n\nextern int core_kernel_text(unsigned long addr);\nextern int init_kernel_text(unsigned long addr);\nextern int core_kernel_data(unsigned long addr);\nextern int __kernel_text_address(unsigned long addr);\nextern int kernel_text_address(unsigned long addr);\nextern int func_ptr_is_kernel_text(void *ptr);\n\n#ifdef CONFIG_SMP\nextern unsigned int sysctl_oops_all_cpu_backtrace;\n#else\n#define sysctl_oops_all_cpu_backtrace 0\n#endif /* CONFIG_SMP */\n\nextern void bust_spinlocks(int yes);\nextern int panic_timeout;\nextern unsigned long panic_print;\nextern int panic_on_oops;\nextern int panic_on_unrecovered_nmi;\nextern int panic_on_io_nmi;\nextern int panic_on_warn;\nextern unsigned long panic_on_taint;\nextern bool panic_on_taint_nousertaint;\nextern int sysctl_panic_on_rcu_stall;\nextern int sysctl_max_rcu_stall_to_panic;\nextern int sysctl_panic_on_stackoverflow;\n\nextern bool crash_kexec_post_notifiers;\n\n/*\n * panic_cpu is used for synchronizing panic() and crash_kexec() execution. It\n * holds a CPU number which is executing panic() currently. A value of\n * PANIC_CPU_INVALID means no CPU has entered panic() or crash_kexec().\n */\nextern atomic_t panic_cpu;\n#define PANIC_CPU_INVALID\t-1\n\n/*\n * Only to be used by arch init code. If the user over-wrote the default\n * CONFIG_PANIC_TIMEOUT, honor it.\n */\nstatic inline void set_arch_panic_timeout(int timeout, int arch_default_timeout)\n{\n\tif (panic_timeout == arch_default_timeout)\n\t\tpanic_timeout = timeout;\n}\nextern const char *print_tainted(void);\nenum lockdep_ok {\n\tLOCKDEP_STILL_OK,\n\tLOCKDEP_NOW_UNRELIABLE\n};\nextern void add_taint(unsigned flag, enum lockdep_ok);\nextern int test_taint(unsigned flag);\nextern unsigned long get_taint(void);\nextern int root_mountflags;\n\nextern bool early_boot_irqs_disabled;\n\n/*\n * Values used for system_state. Ordering of the states must not be changed\n * as code checks for <, <=, >, >= STATE.\n */\nextern enum system_states {\n\tSYSTEM_BOOTING,\n\tSYSTEM_SCHEDULING,\n\tSYSTEM_RUNNING,\n\tSYSTEM_HALT,\n\tSYSTEM_POWER_OFF,\n\tSYSTEM_RESTART,\n\tSYSTEM_SUSPEND,\n} system_state;\n\n/* This cannot be an enum because some may be used in assembly source. */\n#define TAINT_PROPRIETARY_MODULE\t0\n#define TAINT_FORCED_MODULE\t\t1\n#define TAINT_CPU_OUT_OF_SPEC\t\t2\n#define TAINT_FORCED_RMMOD\t\t3\n#define TAINT_MACHINE_CHECK\t\t4\n#define TAINT_BAD_PAGE\t\t\t5\n#define TAINT_USER\t\t\t6\n#define TAINT_DIE\t\t\t7\n#define TAINT_OVERRIDDEN_ACPI_TABLE\t8\n#define TAINT_WARN\t\t\t9\n#define TAINT_CRAP\t\t\t10\n#define TAINT_FIRMWARE_WORKAROUND\t11\n#define TAINT_OOT_MODULE\t\t12\n#define TAINT_UNSIGNED_MODULE\t\t13\n#define TAINT_SOFTLOCKUP\t\t14\n#define TAINT_LIVEPATCH\t\t\t15\n#define TAINT_AUX\t\t\t16\n#define TAINT_RANDSTRUCT\t\t17\n#define TAINT_FLAGS_COUNT\t\t18\n#define TAINT_FLAGS_MAX\t\t\t((1UL << TAINT_FLAGS_COUNT) - 1)\n\nstruct taint_flag {\n\tchar c_true;\t/* character printed when tainted */\n\tchar c_false;\t/* character printed when not tainted */\n\tbool module;\t/* also show as a per-module taint flag */\n};\n\nextern const struct taint_flag taint_flags[TAINT_FLAGS_COUNT];\n\nextern const char hex_asc[];\n#define hex_asc_lo(x)\thex_asc[((x) & 0x0f)]\n#define hex_asc_hi(x)\thex_asc[((x) & 0xf0) >> 4]\n\nstatic inline char *hex_byte_pack(char *buf, u8 byte)\n{\n\t*buf++ = hex_asc_hi(byte);\n\t*buf++ = hex_asc_lo(byte);\n\treturn buf;\n}\n\nextern const char hex_asc_upper[];\n#define hex_asc_upper_lo(x)\thex_asc_upper[((x) & 0x0f)]\n#define hex_asc_upper_hi(x)\thex_asc_upper[((x) & 0xf0) >> 4]\n\nstatic inline char *hex_byte_pack_upper(char *buf, u8 byte)\n{\n\t*buf++ = hex_asc_upper_hi(byte);\n\t*buf++ = hex_asc_upper_lo(byte);\n\treturn buf;\n}\n\nextern int hex_to_bin(char ch);\nextern int __must_check hex2bin(u8 *dst, const char *src, size_t count);\nextern char *bin2hex(char *dst, const void *src, size_t count);\n\nbool mac_pton(const char *s, u8 *mac);\n\n/*\n * General tracing related utility functions - trace_printk(),\n * tracing_on/tracing_off and tracing_start()/tracing_stop\n *\n * Use tracing_on/tracing_off when you want to quickly turn on or off\n * tracing. It simply enables or disables the recording of the trace events.\n * This also corresponds to the user space /sys/kernel/debug/tracing/tracing_on\n * file, which gives a means for the kernel and userspace to interact.\n * Place a tracing_off() in the kernel where you want tracing to end.\n * From user space, examine the trace, and then echo 1 > tracing_on\n * to continue tracing.\n *\n * tracing_stop/tracing_start has slightly more overhead. It is used\n * by things like suspend to ram where disabling the recording of the\n * trace is not enough, but tracing must actually stop because things\n * like calling smp_processor_id() may crash the system.\n *\n * Most likely, you want to use tracing_on/tracing_off.\n */\n\nenum ftrace_dump_mode {\n\tDUMP_NONE,\n\tDUMP_ALL,\n\tDUMP_ORIG,\n};\n\n#ifdef CONFIG_TRACING\nvoid tracing_on(void);\nvoid tracing_off(void);\nint tracing_is_on(void);\nvoid tracing_snapshot(void);\nvoid tracing_snapshot_alloc(void);\n\nextern void tracing_start(void);\nextern void tracing_stop(void);\n\nstatic inline __printf(1, 2)\nvoid ____trace_printk_check_format(const char *fmt, ...)\n{\n}\n#define __trace_printk_check_format(fmt, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\t\\\n\t\t____trace_printk_check_format(fmt, ##args);\t\t\\\n} while (0)\n\n/**\n * trace_printk - printf formatting in the ftrace buffer\n * @fmt: the printf format for printing\n *\n * Note: __trace_printk is an internal function for trace_printk() and\n *       the @ip is passed in via the trace_printk() macro.\n *\n * This function allows a kernel developer to debug fast path sections\n * that printk is not appropriate for. By scattering in various\n * printk like tracing in the code, a developer can quickly see\n * where problems are occurring.\n *\n * This is intended as a debugging tool for the developer only.\n * Please refrain from leaving trace_printks scattered around in\n * your code. (Extra memory is used for special buffers that are\n * allocated when trace_printk() is used.)\n *\n * A little optimization trick is done here. If there's only one\n * argument, there's no need to scan the string for printf formats.\n * The trace_puts() will suffice. But how can we take advantage of\n * using trace_puts() when trace_printk() has only one argument?\n * By stringifying the args and checking the size we can tell\n * whether or not there are args. __stringify((__VA_ARGS__)) will\n * turn into \"()\\0\" with a size of 3 when there are no args, anything\n * else will be bigger. All we need to do is define a string to this,\n * and then take its size and compare to 3. If it's bigger, use\n * do_trace_printk() otherwise, optimize it to trace_puts(). Then just\n * let gcc optimize the rest.\n */\n\n#define trace_printk(fmt, ...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\\\n\tchar _______STR[] = __stringify((__VA_ARGS__));\t\\\n\tif (sizeof(_______STR) > 3)\t\t\t\\\n\t\tdo_trace_printk(fmt, ##__VA_ARGS__);\t\\\n\telse\t\t\t\t\t\t\\\n\t\ttrace_puts(fmt);\t\t\t\\\n} while (0)\n\n#define do_trace_printk(fmt, args...)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tstatic const char *trace_printk_fmt __used\t\t\t\\\n\t\t__section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t__builtin_constant_p(fmt) ? fmt : NULL;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t__trace_printk_check_format(fmt, ##args);\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(fmt))\t\t\t\t\t\\\n\t\t__trace_bprintk(_THIS_IP_, trace_printk_fmt, ##args);\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t__trace_printk(_THIS_IP_, fmt, ##args);\t\t\t\\\n} while (0)\n\nextern __printf(2, 3)\nint __trace_bprintk(unsigned long ip, const char *fmt, ...);\n\nextern __printf(2, 3)\nint __trace_printk(unsigned long ip, const char *fmt, ...);\n\n/**\n * trace_puts - write a string into the ftrace buffer\n * @str: the string to record\n *\n * Note: __trace_bputs is an internal function for trace_puts and\n *       the @ip is passed in via the trace_puts macro.\n *\n * This is similar to trace_printk() but is made for those really fast\n * paths that a developer wants the least amount of \"Heisenbug\" effects,\n * where the processing of the print format is still too much.\n *\n * This function allows a kernel developer to debug fast path sections\n * that printk is not appropriate for. By scattering in various\n * printk like tracing in the code, a developer can quickly see\n * where problems are occurring.\n *\n * This is intended as a debugging tool for the developer only.\n * Please refrain from leaving trace_puts scattered around in\n * your code. (Extra memory is used for special buffers that are\n * allocated when trace_puts() is used.)\n *\n * Returns: 0 if nothing was written, positive # if string was.\n *  (1 when __trace_bputs is used, strlen(str) when __trace_puts is used)\n */\n\n#define trace_puts(str) ({\t\t\t\t\t\t\\\n\tstatic const char *trace_printk_fmt __used\t\t\t\\\n\t\t__section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t__builtin_constant_p(str) ? str : NULL;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(str))\t\t\t\t\t\\\n\t\t__trace_bputs(_THIS_IP_, trace_printk_fmt);\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t__trace_puts(_THIS_IP_, str, strlen(str));\t\t\\\n})\nextern int __trace_bputs(unsigned long ip, const char *str);\nextern int __trace_puts(unsigned long ip, const char *str, int size);\n\nextern void trace_dump_stack(int skip);\n\n/*\n * The double __builtin_constant_p is because gcc will give us an error\n * if we try to allocate the static variable to fmt if it is not a\n * constant. Even with the outer if statement.\n */\n#define ftrace_vprintk(fmt, vargs)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(fmt)) {\t\t\t\t\\\n\t\tstatic const char *trace_printk_fmt __used\t\t\\\n\t\t  __section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t\t__builtin_constant_p(fmt) ? fmt : NULL;\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t__ftrace_vbprintk(_THIS_IP_, trace_printk_fmt, vargs);\t\\\n\t} else\t\t\t\t\t\t\t\t\\\n\t\t__ftrace_vprintk(_THIS_IP_, fmt, vargs);\t\t\\\n} while (0)\n\nextern __printf(2, 0) int\n__ftrace_vbprintk(unsigned long ip, const char *fmt, va_list ap);\n\nextern __printf(2, 0) int\n__ftrace_vprintk(unsigned long ip, const char *fmt, va_list ap);\n\nextern void ftrace_dump(enum ftrace_dump_mode oops_dump_mode);\n#else\nstatic inline void tracing_start(void) { }\nstatic inline void tracing_stop(void) { }\nstatic inline void trace_dump_stack(int skip) { }\n\nstatic inline void tracing_on(void) { }\nstatic inline void tracing_off(void) { }\nstatic inline int tracing_is_on(void) { return 0; }\nstatic inline void tracing_snapshot(void) { }\nstatic inline void tracing_snapshot_alloc(void) { }\n\nstatic inline __printf(1, 2)\nint trace_printk(const char *fmt, ...)\n{\n\treturn 0;\n}\nstatic __printf(1, 0) inline int\nftrace_vprintk(const char *fmt, va_list ap)\n{\n\treturn 0;\n}\nstatic inline void ftrace_dump(enum ftrace_dump_mode oops_dump_mode) { }\n#endif /* CONFIG_TRACING */\n\n/* This counts to 12. Any more, it will return 13th argument. */\n#define __COUNT_ARGS(_0, _1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _n, X...) _n\n#define COUNT_ARGS(X...) __COUNT_ARGS(, ##X, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0)\n\n#define __CONCAT(a, b) a ## b\n#define CONCATENATE(a, b) __CONCAT(a, b)\n\n/**\n * container_of - cast a member of a structure out to the containing structure\n * @ptr:\tthe pointer to the member.\n * @type:\tthe type of the container struct this is embedded in.\n * @member:\tthe name of the member within the struct.\n *\n */\n#define container_of(ptr, type, member) ({\t\t\t\t\\\n\tvoid *__mptr = (void *)(ptr);\t\t\t\t\t\\\n\tBUILD_BUG_ON_MSG(!__same_type(*(ptr), ((type *)0)->member) &&\t\\\n\t\t\t !__same_type(*(ptr), void),\t\t\t\\\n\t\t\t \"pointer type mismatch in container_of()\");\t\\\n\t((type *)(__mptr - offsetof(type, member))); })\n\n/**\n * container_of_safe - cast a member of a structure out to the containing structure\n * @ptr:\tthe pointer to the member.\n * @type:\tthe type of the container struct this is embedded in.\n * @member:\tthe name of the member within the struct.\n *\n * If IS_ERR_OR_NULL(ptr), ptr is returned unchanged.\n */\n#define container_of_safe(ptr, type, member) ({\t\t\t\t\\\n\tvoid *__mptr = (void *)(ptr);\t\t\t\t\t\\\n\tBUILD_BUG_ON_MSG(!__same_type(*(ptr), ((type *)0)->member) &&\t\\\n\t\t\t !__same_type(*(ptr), void),\t\t\t\\\n\t\t\t \"pointer type mismatch in container_of()\");\t\\\n\tIS_ERR_OR_NULL(__mptr) ? ERR_CAST(__mptr) :\t\t\t\\\n\t\t((type *)(__mptr - offsetof(type, member))); })\n\n/* Rebuild everything on CONFIG_FTRACE_MCOUNT_RECORD */\n#ifdef CONFIG_FTRACE_MCOUNT_RECORD\n# define REBUILD_DUE_TO_FTRACE_MCOUNT_RECORD\n#endif\n\n/* Permissions on a sysfs file: you didn't miss the 0 prefix did you? */\n#define VERIFY_OCTAL_PERMISSIONS(perms)\t\t\t\t\t\t\\\n\t(BUILD_BUG_ON_ZERO((perms) < 0) +\t\t\t\t\t\\\n\t BUILD_BUG_ON_ZERO((perms) > 0777) +\t\t\t\t\t\\\n\t /* USER_READABLE >= GROUP_READABLE >= OTHER_READABLE */\t\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 6) & 4) < (((perms) >> 3) & 4)) +\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 3) & 4) < ((perms) & 4)) +\t\t\\\n\t /* USER_WRITABLE >= GROUP_WRITABLE */\t\t\t\t\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 6) & 2) < (((perms) >> 3) & 2)) +\t\\\n\t /* OTHER_WRITABLE?  Generally considered a bad idea. */\t\t\\\n\t BUILD_BUG_ON_ZERO((perms) & 2) +\t\t\t\t\t\\\n\t (perms))\n#endif\n"}, "0": {"id": 0, "path": "/src/mm/page_alloc.c", "content": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n *  linux/mm/page_alloc.c\n *\n *  Manages the free list, the system allocates free pages here.\n *  Note that kmalloc() lives in slab.c\n *\n *  Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds\n *  Swap reorganised 29.12.95, Stephen Tweedie\n *  Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999\n *  Reshaped it to be a zoned allocator, Ingo Molnar, Red Hat, 1999\n *  Discontiguous memory support, Kanoj Sarcar, SGI, Nov 1999\n *  Zone balancing, Kanoj Sarcar, SGI, Jan 2000\n *  Per cpu hot/cold page lists, bulk allocation, Martin J. Bligh, Sept 2002\n *          (lots of bits borrowed from Ingo Molnar & Andrew Morton)\n */\n\n#include <linux/stddef.h>\n#include <linux/mm.h>\n#include <linux/highmem.h>\n#include <linux/swap.h>\n#include <linux/interrupt.h>\n#include <linux/pagemap.h>\n#include <linux/jiffies.h>\n#include <linux/memblock.h>\n#include <linux/compiler.h>\n#include <linux/kernel.h>\n#include <linux/kasan.h>\n#include <linux/module.h>\n#include <linux/suspend.h>\n#include <linux/pagevec.h>\n#include <linux/blkdev.h>\n#include <linux/slab.h>\n#include <linux/ratelimit.h>\n#include <linux/oom.h>\n#include <linux/topology.h>\n#include <linux/sysctl.h>\n#include <linux/cpu.h>\n#include <linux/cpuset.h>\n#include <linux/memory_hotplug.h>\n#include <linux/nodemask.h>\n#include <linux/vmalloc.h>\n#include <linux/vmstat.h>\n#include <linux/mempolicy.h>\n#include <linux/memremap.h>\n#include <linux/stop_machine.h>\n#include <linux/random.h>\n#include <linux/sort.h>\n#include <linux/pfn.h>\n#include <linux/backing-dev.h>\n#include <linux/fault-inject.h>\n#include <linux/page-isolation.h>\n#include <linux/debugobjects.h>\n#include <linux/kmemleak.h>\n#include <linux/compaction.h>\n#include <trace/events/kmem.h>\n#include <trace/events/oom.h>\n#include <linux/prefetch.h>\n#include <linux/mm_inline.h>\n#include <linux/mmu_notifier.h>\n#include <linux/migrate.h>\n#include <linux/hugetlb.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/mm.h>\n#include <linux/page_owner.h>\n#include <linux/kthread.h>\n#include <linux/memcontrol.h>\n#include <linux/ftrace.h>\n#include <linux/lockdep.h>\n#include <linux/nmi.h>\n#include <linux/psi.h>\n#include <linux/padata.h>\n#include <linux/khugepaged.h>\n#include <linux/buffer_head.h>\n\n#include <asm/sections.h>\n#include <asm/tlbflush.h>\n#include <asm/div64.h>\n#include \"internal.h\"\n#include \"shuffle.h\"\n#include \"page_reporting.h\"\n\n/* Free Page Internal flags: for internal, non-pcp variants of free_pages(). */\ntypedef int __bitwise fpi_t;\n\n/* No special request */\n#define FPI_NONE\t\t((__force fpi_t)0)\n\n/*\n * Skip free page reporting notification for the (possibly merged) page.\n * This does not hinder free page reporting from grabbing the page,\n * reporting it and marking it \"reported\" -  it only skips notifying\n * the free page reporting infrastructure about a newly freed page. For\n * example, used when temporarily pulling a page from a freelist and\n * putting it back unmodified.\n */\n#define FPI_SKIP_REPORT_NOTIFY\t((__force fpi_t)BIT(0))\n\n/*\n * Place the (possibly merged) page to the tail of the freelist. Will ignore\n * page shuffling (relevant code - e.g., memory onlining - is expected to\n * shuffle the whole zone).\n *\n * Note: No code should rely on this flag for correctness - it's purely\n *       to allow for optimizations when handing back either fresh pages\n *       (memory onlining) or untouched pages (page isolation, free page\n *       reporting).\n */\n#define FPI_TO_TAIL\t\t((__force fpi_t)BIT(1))\n\n/*\n * Don't poison memory with KASAN (only for the tag-based modes).\n * During boot, all non-reserved memblock memory is exposed to page_alloc.\n * Poisoning all that memory lengthens boot time, especially on systems with\n * large amount of RAM. This flag is used to skip that poisoning.\n * This is only done for the tag-based KASAN modes, as those are able to\n * detect memory corruptions with the memory tags assigned by default.\n * All memory allocated normally after boot gets poisoned as usual.\n */\n#define FPI_SKIP_KASAN_POISON\t((__force fpi_t)BIT(2))\n\n/* prevent >1 _updater_ of zone percpu pageset ->high and ->batch fields */\nstatic DEFINE_MUTEX(pcp_batch_high_lock);\n#define MIN_PERCPU_PAGELIST_FRACTION\t(8)\n\n#ifdef CONFIG_USE_PERCPU_NUMA_NODE_ID\nDEFINE_PER_CPU(int, numa_node);\nEXPORT_PER_CPU_SYMBOL(numa_node);\n#endif\n\nDEFINE_STATIC_KEY_TRUE(vm_numa_stat_key);\n\n#ifdef CONFIG_HAVE_MEMORYLESS_NODES\n/*\n * N.B., Do NOT reference the '_numa_mem_' per cpu variable directly.\n * It will not be defined when CONFIG_HAVE_MEMORYLESS_NODES is not defined.\n * Use the accessor functions set_numa_mem(), numa_mem_id() and cpu_to_mem()\n * defined in <linux/topology.h>.\n */\nDEFINE_PER_CPU(int, _numa_mem_);\t\t/* Kernel \"local memory\" node */\nEXPORT_PER_CPU_SYMBOL(_numa_mem_);\n#endif\n\n/* work_structs for global per-cpu drains */\nstruct pcpu_drain {\n\tstruct zone *zone;\n\tstruct work_struct work;\n};\nstatic DEFINE_MUTEX(pcpu_drain_mutex);\nstatic DEFINE_PER_CPU(struct pcpu_drain, pcpu_drain);\n\n#ifdef CONFIG_GCC_PLUGIN_LATENT_ENTROPY\nvolatile unsigned long latent_entropy __latent_entropy;\nEXPORT_SYMBOL(latent_entropy);\n#endif\n\n/*\n * Array of node states.\n */\nnodemask_t node_states[NR_NODE_STATES] __read_mostly = {\n\t[N_POSSIBLE] = NODE_MASK_ALL,\n\t[N_ONLINE] = { { [0] = 1UL } },\n#ifndef CONFIG_NUMA\n\t[N_NORMAL_MEMORY] = { { [0] = 1UL } },\n#ifdef CONFIG_HIGHMEM\n\t[N_HIGH_MEMORY] = { { [0] = 1UL } },\n#endif\n\t[N_MEMORY] = { { [0] = 1UL } },\n\t[N_CPU] = { { [0] = 1UL } },\n#endif\t/* NUMA */\n};\nEXPORT_SYMBOL(node_states);\n\natomic_long_t _totalram_pages __read_mostly;\nEXPORT_SYMBOL(_totalram_pages);\nunsigned long totalreserve_pages __read_mostly;\nunsigned long totalcma_pages __read_mostly;\n\nint percpu_pagelist_fraction;\ngfp_t gfp_allowed_mask __read_mostly = GFP_BOOT_MASK;\nDEFINE_STATIC_KEY_FALSE(init_on_alloc);\nEXPORT_SYMBOL(init_on_alloc);\n\nDEFINE_STATIC_KEY_FALSE(init_on_free);\nEXPORT_SYMBOL(init_on_free);\n\nstatic bool _init_on_alloc_enabled_early __read_mostly\n\t\t\t\t= IS_ENABLED(CONFIG_INIT_ON_ALLOC_DEFAULT_ON);\nstatic int __init early_init_on_alloc(char *buf)\n{\n\n\treturn kstrtobool(buf, &_init_on_alloc_enabled_early);\n}\nearly_param(\"init_on_alloc\", early_init_on_alloc);\n\nstatic bool _init_on_free_enabled_early __read_mostly\n\t\t\t\t= IS_ENABLED(CONFIG_INIT_ON_FREE_DEFAULT_ON);\nstatic int __init early_init_on_free(char *buf)\n{\n\treturn kstrtobool(buf, &_init_on_free_enabled_early);\n}\nearly_param(\"init_on_free\", early_init_on_free);\n\n/*\n * A cached value of the page's pageblock's migratetype, used when the page is\n * put on a pcplist. Used to avoid the pageblock migratetype lookup when\n * freeing from pcplists in most cases, at the cost of possibly becoming stale.\n * Also the migratetype set in the page does not necessarily match the pcplist\n * index, e.g. page might have MIGRATE_CMA set but be on a pcplist with any\n * other index - this ensures that it will be put on the correct CMA freelist.\n */\nstatic inline int get_pcppage_migratetype(struct page *page)\n{\n\treturn page->index;\n}\n\nstatic inline void set_pcppage_migratetype(struct page *page, int migratetype)\n{\n\tpage->index = migratetype;\n}\n\n#ifdef CONFIG_PM_SLEEP\n/*\n * The following functions are used by the suspend/hibernate code to temporarily\n * change gfp_allowed_mask in order to avoid using I/O during memory allocations\n * while devices are suspended.  To avoid races with the suspend/hibernate code,\n * they should always be called with system_transition_mutex held\n * (gfp_allowed_mask also should only be modified with system_transition_mutex\n * held, unless the suspend/hibernate code is guaranteed not to run in parallel\n * with that modification).\n */\n\nstatic gfp_t saved_gfp_mask;\n\nvoid pm_restore_gfp_mask(void)\n{\n\tWARN_ON(!mutex_is_locked(&system_transition_mutex));\n\tif (saved_gfp_mask) {\n\t\tgfp_allowed_mask = saved_gfp_mask;\n\t\tsaved_gfp_mask = 0;\n\t}\n}\n\nvoid pm_restrict_gfp_mask(void)\n{\n\tWARN_ON(!mutex_is_locked(&system_transition_mutex));\n\tWARN_ON(saved_gfp_mask);\n\tsaved_gfp_mask = gfp_allowed_mask;\n\tgfp_allowed_mask &= ~(__GFP_IO | __GFP_FS);\n}\n\nbool pm_suspended_storage(void)\n{\n\tif ((gfp_allowed_mask & (__GFP_IO | __GFP_FS)) == (__GFP_IO | __GFP_FS))\n\t\treturn false;\n\treturn true;\n}\n#endif /* CONFIG_PM_SLEEP */\n\n#ifdef CONFIG_HUGETLB_PAGE_SIZE_VARIABLE\nunsigned int pageblock_order __read_mostly;\n#endif\n\nstatic void __free_pages_ok(struct page *page, unsigned int order,\n\t\t\t    fpi_t fpi_flags);\n\n/*\n * results with 256, 32 in the lowmem_reserve sysctl:\n *\t1G machine -> (16M dma, 800M-16M normal, 1G-800M high)\n *\t1G machine -> (16M dma, 784M normal, 224M high)\n *\tNORMAL allocation will leave 784M/256 of ram reserved in the ZONE_DMA\n *\tHIGHMEM allocation will leave 224M/32 of ram reserved in ZONE_NORMAL\n *\tHIGHMEM allocation will leave (224M+784M)/256 of ram reserved in ZONE_DMA\n *\n * TBD: should special case ZONE_DMA32 machines here - in those we normally\n * don't need any ZONE_NORMAL reservation\n */\nint sysctl_lowmem_reserve_ratio[MAX_NR_ZONES] = {\n#ifdef CONFIG_ZONE_DMA\n\t[ZONE_DMA] = 256,\n#endif\n#ifdef CONFIG_ZONE_DMA32\n\t[ZONE_DMA32] = 256,\n#endif\n\t[ZONE_NORMAL] = 32,\n#ifdef CONFIG_HIGHMEM\n\t[ZONE_HIGHMEM] = 0,\n#endif\n\t[ZONE_MOVABLE] = 0,\n};\n\nstatic char * const zone_names[MAX_NR_ZONES] = {\n#ifdef CONFIG_ZONE_DMA\n\t \"DMA\",\n#endif\n#ifdef CONFIG_ZONE_DMA32\n\t \"DMA32\",\n#endif\n\t \"Normal\",\n#ifdef CONFIG_HIGHMEM\n\t \"HighMem\",\n#endif\n\t \"Movable\",\n#ifdef CONFIG_ZONE_DEVICE\n\t \"Device\",\n#endif\n};\n\nconst char * const migratetype_names[MIGRATE_TYPES] = {\n\t\"Unmovable\",\n\t\"Movable\",\n\t\"Reclaimable\",\n\t\"HighAtomic\",\n#ifdef CONFIG_CMA\n\t\"CMA\",\n#endif\n#ifdef CONFIG_MEMORY_ISOLATION\n\t\"Isolate\",\n#endif\n};\n\ncompound_page_dtor * const compound_page_dtors[NR_COMPOUND_DTORS] = {\n\t[NULL_COMPOUND_DTOR] = NULL,\n\t[COMPOUND_PAGE_DTOR] = free_compound_page,\n#ifdef CONFIG_HUGETLB_PAGE\n\t[HUGETLB_PAGE_DTOR] = free_huge_page,\n#endif\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t[TRANSHUGE_PAGE_DTOR] = free_transhuge_page,\n#endif\n};\n\nint min_free_kbytes = 1024;\nint user_min_free_kbytes = -1;\n#ifdef CONFIG_DISCONTIGMEM\n/*\n * DiscontigMem defines memory ranges as separate pg_data_t even if the ranges\n * are not on separate NUMA nodes. Functionally this works but with\n * watermark_boost_factor, it can reclaim prematurely as the ranges can be\n * quite small. By default, do not boost watermarks on discontigmem as in\n * many cases very high-order allocations like THP are likely to be\n * unsupported and the premature reclaim offsets the advantage of long-term\n * fragmentation avoidance.\n */\nint watermark_boost_factor __read_mostly;\n#else\nint watermark_boost_factor __read_mostly = 15000;\n#endif\nint watermark_scale_factor = 10;\n\nstatic unsigned long nr_kernel_pages __initdata;\nstatic unsigned long nr_all_pages __initdata;\nstatic unsigned long dma_reserve __initdata;\n\nstatic unsigned long arch_zone_lowest_possible_pfn[MAX_NR_ZONES] __initdata;\nstatic unsigned long arch_zone_highest_possible_pfn[MAX_NR_ZONES] __initdata;\nstatic unsigned long required_kernelcore __initdata;\nstatic unsigned long required_kernelcore_percent __initdata;\nstatic unsigned long required_movablecore __initdata;\nstatic unsigned long required_movablecore_percent __initdata;\nstatic unsigned long zone_movable_pfn[MAX_NUMNODES] __initdata;\nstatic bool mirrored_kernelcore __meminitdata;\n\n/* movable_zone is the \"real\" zone pages in ZONE_MOVABLE are taken from */\nint movable_zone;\nEXPORT_SYMBOL(movable_zone);\n\n#if MAX_NUMNODES > 1\nunsigned int nr_node_ids __read_mostly = MAX_NUMNODES;\nunsigned int nr_online_nodes __read_mostly = 1;\nEXPORT_SYMBOL(nr_node_ids);\nEXPORT_SYMBOL(nr_online_nodes);\n#endif\n\nint page_group_by_mobility_disabled __read_mostly;\n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\n/*\n * During boot we initialize deferred pages on-demand, as needed, but once\n * page_alloc_init_late() has finished, the deferred pages are all initialized,\n * and we can permanently disable that path.\n */\nstatic DEFINE_STATIC_KEY_TRUE(deferred_pages);\n\n/*\n * Calling kasan_free_pages() only after deferred memory initialization\n * has completed. Poisoning pages during deferred memory init will greatly\n * lengthen the process and cause problem in large memory systems as the\n * deferred pages initialization is done with interrupt disabled.\n *\n * Assuming that there will be no reference to those newly initialized\n * pages before they are ever allocated, this should have no effect on\n * KASAN memory tracking as the poison will be properly inserted at page\n * allocation time. The only corner case is when pages are allocated by\n * on-demand allocation and then freed again before the deferred pages\n * initialization is done, but this is not likely to happen.\n */\nstatic inline void kasan_free_nondeferred_pages(struct page *page, int order,\n\t\t\t\t\t\tbool init, fpi_t fpi_flags)\n{\n\tif (static_branch_unlikely(&deferred_pages))\n\t\treturn;\n\tif (!IS_ENABLED(CONFIG_KASAN_GENERIC) &&\n\t\t\t(fpi_flags & FPI_SKIP_KASAN_POISON))\n\t\treturn;\n\tkasan_free_pages(page, order, init);\n}\n\n/* Returns true if the struct page for the pfn is uninitialised */\nstatic inline bool __meminit early_page_uninitialised(unsigned long pfn)\n{\n\tint nid = early_pfn_to_nid(pfn);\n\n\tif (node_online(nid) && pfn >= NODE_DATA(nid)->first_deferred_pfn)\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * Returns true when the remaining initialisation should be deferred until\n * later in the boot cycle when it can be parallelised.\n */\nstatic bool __meminit\ndefer_init(int nid, unsigned long pfn, unsigned long end_pfn)\n{\n\tstatic unsigned long prev_end_pfn, nr_initialised;\n\n\t/*\n\t * prev_end_pfn static that contains the end of previous zone\n\t * No need to protect because called very early in boot before smp_init.\n\t */\n\tif (prev_end_pfn != end_pfn) {\n\t\tprev_end_pfn = end_pfn;\n\t\tnr_initialised = 0;\n\t}\n\n\t/* Always populate low zones for address-constrained allocations */\n\tif (end_pfn < pgdat_end_pfn(NODE_DATA(nid)))\n\t\treturn false;\n\n\tif (NODE_DATA(nid)->first_deferred_pfn != ULONG_MAX)\n\t\treturn true;\n\t/*\n\t * We start only with one section of pages, more pages are added as\n\t * needed until the rest of deferred pages are initialized.\n\t */\n\tnr_initialised++;\n\tif ((nr_initialised > PAGES_PER_SECTION) &&\n\t    (pfn & (PAGES_PER_SECTION - 1)) == 0) {\n\t\tNODE_DATA(nid)->first_deferred_pfn = pfn;\n\t\treturn true;\n\t}\n\treturn false;\n}\n#else\nstatic inline void kasan_free_nondeferred_pages(struct page *page, int order,\n\t\t\t\t\t\tbool init, fpi_t fpi_flags)\n{\n\tif (!IS_ENABLED(CONFIG_KASAN_GENERIC) &&\n\t\t\t(fpi_flags & FPI_SKIP_KASAN_POISON))\n\t\treturn;\n\tkasan_free_pages(page, order, init);\n}\n\nstatic inline bool early_page_uninitialised(unsigned long pfn)\n{\n\treturn false;\n}\n\nstatic inline bool defer_init(int nid, unsigned long pfn, unsigned long end_pfn)\n{\n\treturn false;\n}\n#endif\n\n/* Return a pointer to the bitmap storing bits affecting a block of pages */\nstatic inline unsigned long *get_pageblock_bitmap(struct page *page,\n\t\t\t\t\t\t\tunsigned long pfn)\n{\n#ifdef CONFIG_SPARSEMEM\n\treturn section_to_usemap(__pfn_to_section(pfn));\n#else\n\treturn page_zone(page)->pageblock_flags;\n#endif /* CONFIG_SPARSEMEM */\n}\n\nstatic inline int pfn_to_bitidx(struct page *page, unsigned long pfn)\n{\n#ifdef CONFIG_SPARSEMEM\n\tpfn &= (PAGES_PER_SECTION-1);\n#else\n\tpfn = pfn - round_down(page_zone(page)->zone_start_pfn, pageblock_nr_pages);\n#endif /* CONFIG_SPARSEMEM */\n\treturn (pfn >> pageblock_order) * NR_PAGEBLOCK_BITS;\n}\n\nstatic __always_inline\nunsigned long __get_pfnblock_flags_mask(struct page *page,\n\t\t\t\t\tunsigned long pfn,\n\t\t\t\t\tunsigned long mask)\n{\n\tunsigned long *bitmap;\n\tunsigned long bitidx, word_bitidx;\n\tunsigned long word;\n\n\tbitmap = get_pageblock_bitmap(page, pfn);\n\tbitidx = pfn_to_bitidx(page, pfn);\n\tword_bitidx = bitidx / BITS_PER_LONG;\n\tbitidx &= (BITS_PER_LONG-1);\n\n\tword = bitmap[word_bitidx];\n\treturn (word >> bitidx) & mask;\n}\n\n/**\n * get_pfnblock_flags_mask - Return the requested group of flags for the pageblock_nr_pages block of pages\n * @page: The page within the block of interest\n * @pfn: The target page frame number\n * @mask: mask of bits that the caller is interested in\n *\n * Return: pageblock_bits flags\n */\nunsigned long get_pfnblock_flags_mask(struct page *page, unsigned long pfn,\n\t\t\t\t\tunsigned long mask)\n{\n\treturn __get_pfnblock_flags_mask(page, pfn, mask);\n}\n\nstatic __always_inline int get_pfnblock_migratetype(struct page *page, unsigned long pfn)\n{\n\treturn __get_pfnblock_flags_mask(page, pfn, MIGRATETYPE_MASK);\n}\n\n/**\n * set_pfnblock_flags_mask - Set the requested group of flags for a pageblock_nr_pages block of pages\n * @page: The page within the block of interest\n * @flags: The flags to set\n * @pfn: The target page frame number\n * @mask: mask of bits that the caller is interested in\n */\nvoid set_pfnblock_flags_mask(struct page *page, unsigned long flags,\n\t\t\t\t\tunsigned long pfn,\n\t\t\t\t\tunsigned long mask)\n{\n\tunsigned long *bitmap;\n\tunsigned long bitidx, word_bitidx;\n\tunsigned long old_word, word;\n\n\tBUILD_BUG_ON(NR_PAGEBLOCK_BITS != 4);\n\tBUILD_BUG_ON(MIGRATE_TYPES > (1 << PB_migratetype_bits));\n\n\tbitmap = get_pageblock_bitmap(page, pfn);\n\tbitidx = pfn_to_bitidx(page, pfn);\n\tword_bitidx = bitidx / BITS_PER_LONG;\n\tbitidx &= (BITS_PER_LONG-1);\n\n\tVM_BUG_ON_PAGE(!zone_spans_pfn(page_zone(page), pfn), page);\n\n\tmask <<= bitidx;\n\tflags <<= bitidx;\n\n\tword = READ_ONCE(bitmap[word_bitidx]);\n\tfor (;;) {\n\t\told_word = cmpxchg(&bitmap[word_bitidx], word, (word & ~mask) | flags);\n\t\tif (word == old_word)\n\t\t\tbreak;\n\t\tword = old_word;\n\t}\n}\n\nvoid set_pageblock_migratetype(struct page *page, int migratetype)\n{\n\tif (unlikely(page_group_by_mobility_disabled &&\n\t\t     migratetype < MIGRATE_PCPTYPES))\n\t\tmigratetype = MIGRATE_UNMOVABLE;\n\n\tset_pfnblock_flags_mask(page, (unsigned long)migratetype,\n\t\t\t\tpage_to_pfn(page), MIGRATETYPE_MASK);\n}\n\n#ifdef CONFIG_DEBUG_VM\nstatic int page_outside_zone_boundaries(struct zone *zone, struct page *page)\n{\n\tint ret = 0;\n\tunsigned seq;\n\tunsigned long pfn = page_to_pfn(page);\n\tunsigned long sp, start_pfn;\n\n\tdo {\n\t\tseq = zone_span_seqbegin(zone);\n\t\tstart_pfn = zone->zone_start_pfn;\n\t\tsp = zone->spanned_pages;\n\t\tif (!zone_spans_pfn(zone, pfn))\n\t\t\tret = 1;\n\t} while (zone_span_seqretry(zone, seq));\n\n\tif (ret)\n\t\tpr_err(\"page 0x%lx outside node %d zone %s [ 0x%lx - 0x%lx ]\\n\",\n\t\t\tpfn, zone_to_nid(zone), zone->name,\n\t\t\tstart_pfn, start_pfn + sp);\n\n\treturn ret;\n}\n\nstatic int page_is_consistent(struct zone *zone, struct page *page)\n{\n\tif (!pfn_valid_within(page_to_pfn(page)))\n\t\treturn 0;\n\tif (zone != page_zone(page))\n\t\treturn 0;\n\n\treturn 1;\n}\n/*\n * Temporary debugging check for pages not lying within a given zone.\n */\nstatic int __maybe_unused bad_range(struct zone *zone, struct page *page)\n{\n\tif (page_outside_zone_boundaries(zone, page))\n\t\treturn 1;\n\tif (!page_is_consistent(zone, page))\n\t\treturn 1;\n\n\treturn 0;\n}\n#else\nstatic inline int __maybe_unused bad_range(struct zone *zone, struct page *page)\n{\n\treturn 0;\n}\n#endif\n\nstatic void bad_page(struct page *page, const char *reason)\n{\n\tstatic unsigned long resume;\n\tstatic unsigned long nr_shown;\n\tstatic unsigned long nr_unshown;\n\n\t/*\n\t * Allow a burst of 60 reports, then keep quiet for that minute;\n\t * or allow a steady drip of one report per second.\n\t */\n\tif (nr_shown == 60) {\n\t\tif (time_before(jiffies, resume)) {\n\t\t\tnr_unshown++;\n\t\t\tgoto out;\n\t\t}\n\t\tif (nr_unshown) {\n\t\t\tpr_alert(\n\t\t\t      \"BUG: Bad page state: %lu messages suppressed\\n\",\n\t\t\t\tnr_unshown);\n\t\t\tnr_unshown = 0;\n\t\t}\n\t\tnr_shown = 0;\n\t}\n\tif (nr_shown++ == 0)\n\t\tresume = jiffies + 60 * HZ;\n\n\tpr_alert(\"BUG: Bad page state in process %s  pfn:%05lx\\n\",\n\t\tcurrent->comm, page_to_pfn(page));\n\t__dump_page(page, reason);\n\tdump_page_owner(page);\n\n\tprint_modules();\n\tdump_stack();\nout:\n\t/* Leave bad fields for debug, except PageBuddy could make trouble */\n\tpage_mapcount_reset(page); /* remove PageBuddy */\n\tadd_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);\n}\n\n/*\n * Higher-order pages are called \"compound pages\".  They are structured thusly:\n *\n * The first PAGE_SIZE page is called the \"head page\" and have PG_head set.\n *\n * The remaining PAGE_SIZE pages are called \"tail pages\". PageTail() is encoded\n * in bit 0 of page->compound_head. The rest of bits is pointer to head page.\n *\n * The first tail page's ->compound_dtor holds the offset in array of compound\n * page destructors. See compound_page_dtors.\n *\n * The first tail page's ->compound_order holds the order of allocation.\n * This usage means that zero-order pages may not be compound.\n */\n\nvoid free_compound_page(struct page *page)\n{\n\tmem_cgroup_uncharge(page);\n\t__free_pages_ok(page, compound_order(page), FPI_NONE);\n}\n\nvoid prep_compound_page(struct page *page, unsigned int order)\n{\n\tint i;\n\tint nr_pages = 1 << order;\n\n\t__SetPageHead(page);\n\tfor (i = 1; i < nr_pages; i++) {\n\t\tstruct page *p = page + i;\n\t\tset_page_count(p, 0);\n\t\tp->mapping = TAIL_MAPPING;\n\t\tset_compound_head(p, page);\n\t}\n\n\tset_compound_page_dtor(page, COMPOUND_PAGE_DTOR);\n\tset_compound_order(page, order);\n\tatomic_set(compound_mapcount_ptr(page), -1);\n\tif (hpage_pincount_available(page))\n\t\tatomic_set(compound_pincount_ptr(page), 0);\n}\n\n#ifdef CONFIG_DEBUG_PAGEALLOC\nunsigned int _debug_guardpage_minorder;\n\nbool _debug_pagealloc_enabled_early __read_mostly\n\t\t\t= IS_ENABLED(CONFIG_DEBUG_PAGEALLOC_ENABLE_DEFAULT);\nEXPORT_SYMBOL(_debug_pagealloc_enabled_early);\nDEFINE_STATIC_KEY_FALSE(_debug_pagealloc_enabled);\nEXPORT_SYMBOL(_debug_pagealloc_enabled);\n\nDEFINE_STATIC_KEY_FALSE(_debug_guardpage_enabled);\n\nstatic int __init early_debug_pagealloc(char *buf)\n{\n\treturn kstrtobool(buf, &_debug_pagealloc_enabled_early);\n}\nearly_param(\"debug_pagealloc\", early_debug_pagealloc);\n\nstatic int __init debug_guardpage_minorder_setup(char *buf)\n{\n\tunsigned long res;\n\n\tif (kstrtoul(buf, 10, &res) < 0 ||  res > MAX_ORDER / 2) {\n\t\tpr_err(\"Bad debug_guardpage_minorder value\\n\");\n\t\treturn 0;\n\t}\n\t_debug_guardpage_minorder = res;\n\tpr_info(\"Setting debug_guardpage_minorder to %lu\\n\", res);\n\treturn 0;\n}\nearly_param(\"debug_guardpage_minorder\", debug_guardpage_minorder_setup);\n\nstatic inline bool set_page_guard(struct zone *zone, struct page *page,\n\t\t\t\tunsigned int order, int migratetype)\n{\n\tif (!debug_guardpage_enabled())\n\t\treturn false;\n\n\tif (order >= debug_guardpage_minorder())\n\t\treturn false;\n\n\t__SetPageGuard(page);\n\tINIT_LIST_HEAD(&page->lru);\n\tset_page_private(page, order);\n\t/* Guard pages are not available for any usage */\n\t__mod_zone_freepage_state(zone, -(1 << order), migratetype);\n\n\treturn true;\n}\n\nstatic inline void clear_page_guard(struct zone *zone, struct page *page,\n\t\t\t\tunsigned int order, int migratetype)\n{\n\tif (!debug_guardpage_enabled())\n\t\treturn;\n\n\t__ClearPageGuard(page);\n\n\tset_page_private(page, 0);\n\tif (!is_migrate_isolate(migratetype))\n\t\t__mod_zone_freepage_state(zone, (1 << order), migratetype);\n}\n#else\nstatic inline bool set_page_guard(struct zone *zone, struct page *page,\n\t\t\tunsigned int order, int migratetype) { return false; }\nstatic inline void clear_page_guard(struct zone *zone, struct page *page,\n\t\t\t\tunsigned int order, int migratetype) {}\n#endif\n\n/*\n * Enable static keys related to various memory debugging and hardening options.\n * Some override others, and depend on early params that are evaluated in the\n * order of appearance. So we need to first gather the full picture of what was\n * enabled, and then make decisions.\n */\nvoid init_mem_debugging_and_hardening(void)\n{\n\tif (_init_on_alloc_enabled_early) {\n\t\tif (page_poisoning_enabled())\n\t\t\tpr_info(\"mem auto-init: CONFIG_PAGE_POISONING is on, \"\n\t\t\t\t\"will take precedence over init_on_alloc\\n\");\n\t\telse\n\t\t\tstatic_branch_enable(&init_on_alloc);\n\t}\n\tif (_init_on_free_enabled_early) {\n\t\tif (page_poisoning_enabled())\n\t\t\tpr_info(\"mem auto-init: CONFIG_PAGE_POISONING is on, \"\n\t\t\t\t\"will take precedence over init_on_free\\n\");\n\t\telse\n\t\t\tstatic_branch_enable(&init_on_free);\n\t}\n\n#ifdef CONFIG_PAGE_POISONING\n\t/*\n\t * Page poisoning is debug page alloc for some arches. If\n\t * either of those options are enabled, enable poisoning.\n\t */\n\tif (page_poisoning_enabled() ||\n\t     (!IS_ENABLED(CONFIG_ARCH_SUPPORTS_DEBUG_PAGEALLOC) &&\n\t      debug_pagealloc_enabled()))\n\t\tstatic_branch_enable(&_page_poisoning_enabled);\n#endif\n\n#ifdef CONFIG_DEBUG_PAGEALLOC\n\tif (!debug_pagealloc_enabled())\n\t\treturn;\n\n\tstatic_branch_enable(&_debug_pagealloc_enabled);\n\n\tif (!debug_guardpage_minorder())\n\t\treturn;\n\n\tstatic_branch_enable(&_debug_guardpage_enabled);\n#endif\n}\n\nstatic inline void set_buddy_order(struct page *page, unsigned int order)\n{\n\tset_page_private(page, order);\n\t__SetPageBuddy(page);\n}\n\n/*\n * This function checks whether a page is free && is the buddy\n * we can coalesce a page and its buddy if\n * (a) the buddy is not in a hole (check before calling!) &&\n * (b) the buddy is in the buddy system &&\n * (c) a page and its buddy have the same order &&\n * (d) a page and its buddy are in the same zone.\n *\n * For recording whether a page is in the buddy system, we set PageBuddy.\n * Setting, clearing, and testing PageBuddy is serialized by zone->lock.\n *\n * For recording page's order, we use page_private(page).\n */\nstatic inline bool page_is_buddy(struct page *page, struct page *buddy,\n\t\t\t\t\t\t\tunsigned int order)\n{\n\tif (!page_is_guard(buddy) && !PageBuddy(buddy))\n\t\treturn false;\n\n\tif (buddy_order(buddy) != order)\n\t\treturn false;\n\n\t/*\n\t * zone check is done late to avoid uselessly calculating\n\t * zone/node ids for pages that could never merge.\n\t */\n\tif (page_zone_id(page) != page_zone_id(buddy))\n\t\treturn false;\n\n\tVM_BUG_ON_PAGE(page_count(buddy) != 0, buddy);\n\n\treturn true;\n}\n\n#ifdef CONFIG_COMPACTION\nstatic inline struct capture_control *task_capc(struct zone *zone)\n{\n\tstruct capture_control *capc = current->capture_control;\n\n\treturn unlikely(capc) &&\n\t\t!(current->flags & PF_KTHREAD) &&\n\t\t!capc->page &&\n\t\tcapc->cc->zone == zone ? capc : NULL;\n}\n\nstatic inline bool\ncompaction_capture(struct capture_control *capc, struct page *page,\n\t\t   int order, int migratetype)\n{\n\tif (!capc || order != capc->cc->order)\n\t\treturn false;\n\n\t/* Do not accidentally pollute CMA or isolated regions*/\n\tif (is_migrate_cma(migratetype) ||\n\t    is_migrate_isolate(migratetype))\n\t\treturn false;\n\n\t/*\n\t * Do not let lower order allocations polluate a movable pageblock.\n\t * This might let an unmovable request use a reclaimable pageblock\n\t * and vice-versa but no more than normal fallback logic which can\n\t * have trouble finding a high-order free page.\n\t */\n\tif (order < pageblock_order && migratetype == MIGRATE_MOVABLE)\n\t\treturn false;\n\n\tcapc->page = page;\n\treturn true;\n}\n\n#else\nstatic inline struct capture_control *task_capc(struct zone *zone)\n{\n\treturn NULL;\n}\n\nstatic inline bool\ncompaction_capture(struct capture_control *capc, struct page *page,\n\t\t   int order, int migratetype)\n{\n\treturn false;\n}\n#endif /* CONFIG_COMPACTION */\n\n/* Used for pages not on another list */\nstatic inline void add_to_free_list(struct page *page, struct zone *zone,\n\t\t\t\t    unsigned int order, int migratetype)\n{\n\tstruct free_area *area = &zone->free_area[order];\n\n\tlist_add(&page->lru, &area->free_list[migratetype]);\n\tarea->nr_free++;\n}\n\n/* Used for pages not on another list */\nstatic inline void add_to_free_list_tail(struct page *page, struct zone *zone,\n\t\t\t\t\t unsigned int order, int migratetype)\n{\n\tstruct free_area *area = &zone->free_area[order];\n\n\tlist_add_tail(&page->lru, &area->free_list[migratetype]);\n\tarea->nr_free++;\n}\n\n/*\n * Used for pages which are on another list. Move the pages to the tail\n * of the list - so the moved pages won't immediately be considered for\n * allocation again (e.g., optimization for memory onlining).\n */\nstatic inline void move_to_free_list(struct page *page, struct zone *zone,\n\t\t\t\t     unsigned int order, int migratetype)\n{\n\tstruct free_area *area = &zone->free_area[order];\n\n\tlist_move_tail(&page->lru, &area->free_list[migratetype]);\n}\n\nstatic inline void del_page_from_free_list(struct page *page, struct zone *zone,\n\t\t\t\t\t   unsigned int order)\n{\n\t/* clear reported state and update reported page count */\n\tif (page_reported(page))\n\t\t__ClearPageReported(page);\n\n\tlist_del(&page->lru);\n\t__ClearPageBuddy(page);\n\tset_page_private(page, 0);\n\tzone->free_area[order].nr_free--;\n}\n\n/*\n * If this is not the largest possible page, check if the buddy\n * of the next-highest order is free. If it is, it's possible\n * that pages are being freed that will coalesce soon. In case,\n * that is happening, add the free page to the tail of the list\n * so it's less likely to be used soon and more likely to be merged\n * as a higher order page\n */\nstatic inline bool\nbuddy_merge_likely(unsigned long pfn, unsigned long buddy_pfn,\n\t\t   struct page *page, unsigned int order)\n{\n\tstruct page *higher_page, *higher_buddy;\n\tunsigned long combined_pfn;\n\n\tif (order >= MAX_ORDER - 2)\n\t\treturn false;\n\n\tif (!pfn_valid_within(buddy_pfn))\n\t\treturn false;\n\n\tcombined_pfn = buddy_pfn & pfn;\n\thigher_page = page + (combined_pfn - pfn);\n\tbuddy_pfn = __find_buddy_pfn(combined_pfn, order + 1);\n\thigher_buddy = higher_page + (buddy_pfn - combined_pfn);\n\n\treturn pfn_valid_within(buddy_pfn) &&\n\t       page_is_buddy(higher_page, higher_buddy, order + 1);\n}\n\n/*\n * Freeing function for a buddy system allocator.\n *\n * The concept of a buddy system is to maintain direct-mapped table\n * (containing bit values) for memory blocks of various \"orders\".\n * The bottom level table contains the map for the smallest allocatable\n * units of memory (here, pages), and each level above it describes\n * pairs of units from the levels below, hence, \"buddies\".\n * At a high level, all that happens here is marking the table entry\n * at the bottom level available, and propagating the changes upward\n * as necessary, plus some accounting needed to play nicely with other\n * parts of the VM system.\n * At each level, we keep a list of pages, which are heads of continuous\n * free pages of length of (1 << order) and marked with PageBuddy.\n * Page's order is recorded in page_private(page) field.\n * So when we are allocating or freeing one, we can derive the state of the\n * other.  That is, if we allocate a small block, and both were\n * free, the remainder of the region must be split into blocks.\n * If a block is freed, and its buddy is also free, then this\n * triggers coalescing into a block of larger size.\n *\n * -- nyc\n */\n\nstatic inline void __free_one_page(struct page *page,\n\t\tunsigned long pfn,\n\t\tstruct zone *zone, unsigned int order,\n\t\tint migratetype, fpi_t fpi_flags)\n{\n\tstruct capture_control *capc = task_capc(zone);\n\tunsigned long buddy_pfn;\n\tunsigned long combined_pfn;\n\tunsigned int max_order;\n\tstruct page *buddy;\n\tbool to_tail;\n\n\tmax_order = min_t(unsigned int, MAX_ORDER - 1, pageblock_order);\n\n\tVM_BUG_ON(!zone_is_initialized(zone));\n\tVM_BUG_ON_PAGE(page->flags & PAGE_FLAGS_CHECK_AT_PREP, page);\n\n\tVM_BUG_ON(migratetype == -1);\n\tif (likely(!is_migrate_isolate(migratetype)))\n\t\t__mod_zone_freepage_state(zone, 1 << order, migratetype);\n\n\tVM_BUG_ON_PAGE(pfn & ((1 << order) - 1), page);\n\tVM_BUG_ON_PAGE(bad_range(zone, page), page);\n\ncontinue_merging:\n\twhile (order < max_order) {\n\t\tif (compaction_capture(capc, page, order, migratetype)) {\n\t\t\t__mod_zone_freepage_state(zone, -(1 << order),\n\t\t\t\t\t\t\t\tmigratetype);\n\t\t\treturn;\n\t\t}\n\t\tbuddy_pfn = __find_buddy_pfn(pfn, order);\n\t\tbuddy = page + (buddy_pfn - pfn);\n\n\t\tif (!pfn_valid_within(buddy_pfn))\n\t\t\tgoto done_merging;\n\t\tif (!page_is_buddy(page, buddy, order))\n\t\t\tgoto done_merging;\n\t\t/*\n\t\t * Our buddy is free or it is CONFIG_DEBUG_PAGEALLOC guard page,\n\t\t * merge with it and move up one order.\n\t\t */\n\t\tif (page_is_guard(buddy))\n\t\t\tclear_page_guard(zone, buddy, order, migratetype);\n\t\telse\n\t\t\tdel_page_from_free_list(buddy, zone, order);\n\t\tcombined_pfn = buddy_pfn & pfn;\n\t\tpage = page + (combined_pfn - pfn);\n\t\tpfn = combined_pfn;\n\t\torder++;\n\t}\n\tif (order < MAX_ORDER - 1) {\n\t\t/* If we are here, it means order is >= pageblock_order.\n\t\t * We want to prevent merge between freepages on isolate\n\t\t * pageblock and normal pageblock. Without this, pageblock\n\t\t * isolation could cause incorrect freepage or CMA accounting.\n\t\t *\n\t\t * We don't want to hit this code for the more frequent\n\t\t * low-order merging.\n\t\t */\n\t\tif (unlikely(has_isolate_pageblock(zone))) {\n\t\t\tint buddy_mt;\n\n\t\t\tbuddy_pfn = __find_buddy_pfn(pfn, order);\n\t\t\tbuddy = page + (buddy_pfn - pfn);\n\t\t\tbuddy_mt = get_pageblock_migratetype(buddy);\n\n\t\t\tif (migratetype != buddy_mt\n\t\t\t\t\t&& (is_migrate_isolate(migratetype) ||\n\t\t\t\t\t\tis_migrate_isolate(buddy_mt)))\n\t\t\t\tgoto done_merging;\n\t\t}\n\t\tmax_order = order + 1;\n\t\tgoto continue_merging;\n\t}\n\ndone_merging:\n\tset_buddy_order(page, order);\n\n\tif (fpi_flags & FPI_TO_TAIL)\n\t\tto_tail = true;\n\telse if (is_shuffle_order(order))\n\t\tto_tail = shuffle_pick_tail();\n\telse\n\t\tto_tail = buddy_merge_likely(pfn, buddy_pfn, page, order);\n\n\tif (to_tail)\n\t\tadd_to_free_list_tail(page, zone, order, migratetype);\n\telse\n\t\tadd_to_free_list(page, zone, order, migratetype);\n\n\t/* Notify page reporting subsystem of freed page */\n\tif (!(fpi_flags & FPI_SKIP_REPORT_NOTIFY))\n\t\tpage_reporting_notify_free(order);\n}\n\n/*\n * A bad page could be due to a number of fields. Instead of multiple branches,\n * try and check multiple fields with one check. The caller must do a detailed\n * check if necessary.\n */\nstatic inline bool page_expected_state(struct page *page,\n\t\t\t\t\tunsigned long check_flags)\n{\n\tif (unlikely(atomic_read(&page->_mapcount) != -1))\n\t\treturn false;\n\n\tif (unlikely((unsigned long)page->mapping |\n\t\t\tpage_ref_count(page) |\n#ifdef CONFIG_MEMCG\n\t\t\t(unsigned long)page_memcg(page) |\n#endif\n\t\t\t(page->flags & check_flags)))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic const char *page_bad_reason(struct page *page, unsigned long flags)\n{\n\tconst char *bad_reason = NULL;\n\n\tif (unlikely(atomic_read(&page->_mapcount) != -1))\n\t\tbad_reason = \"nonzero mapcount\";\n\tif (unlikely(page->mapping != NULL))\n\t\tbad_reason = \"non-NULL mapping\";\n\tif (unlikely(page_ref_count(page) != 0))\n\t\tbad_reason = \"nonzero _refcount\";\n\tif (unlikely(page->flags & flags)) {\n\t\tif (flags == PAGE_FLAGS_CHECK_AT_PREP)\n\t\t\tbad_reason = \"PAGE_FLAGS_CHECK_AT_PREP flag(s) set\";\n\t\telse\n\t\t\tbad_reason = \"PAGE_FLAGS_CHECK_AT_FREE flag(s) set\";\n\t}\n#ifdef CONFIG_MEMCG\n\tif (unlikely(page_memcg(page)))\n\t\tbad_reason = \"page still charged to cgroup\";\n#endif\n\treturn bad_reason;\n}\n\nstatic void check_free_page_bad(struct page *page)\n{\n\tbad_page(page,\n\t\t page_bad_reason(page, PAGE_FLAGS_CHECK_AT_FREE));\n}\n\nstatic inline int check_free_page(struct page *page)\n{\n\tif (likely(page_expected_state(page, PAGE_FLAGS_CHECK_AT_FREE)))\n\t\treturn 0;\n\n\t/* Something has gone sideways, find it */\n\tcheck_free_page_bad(page);\n\treturn 1;\n}\n\nstatic int free_tail_pages_check(struct page *head_page, struct page *page)\n{\n\tint ret = 1;\n\n\t/*\n\t * We rely page->lru.next never has bit 0 set, unless the page\n\t * is PageTail(). Let's make sure that's true even for poisoned ->lru.\n\t */\n\tBUILD_BUG_ON((unsigned long)LIST_POISON1 & 1);\n\n\tif (!IS_ENABLED(CONFIG_DEBUG_VM)) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\tswitch (page - head_page) {\n\tcase 1:\n\t\t/* the first tail page: ->mapping may be compound_mapcount() */\n\t\tif (unlikely(compound_mapcount(page))) {\n\t\t\tbad_page(page, \"nonzero compound_mapcount\");\n\t\t\tgoto out;\n\t\t}\n\t\tbreak;\n\tcase 2:\n\t\t/*\n\t\t * the second tail page: ->mapping is\n\t\t * deferred_list.next -- ignore value.\n\t\t */\n\t\tbreak;\n\tdefault:\n\t\tif (page->mapping != TAIL_MAPPING) {\n\t\t\tbad_page(page, \"corrupted mapping in tail page\");\n\t\t\tgoto out;\n\t\t}\n\t\tbreak;\n\t}\n\tif (unlikely(!PageTail(page))) {\n\t\tbad_page(page, \"PageTail not set\");\n\t\tgoto out;\n\t}\n\tif (unlikely(compound_head(page) != head_page)) {\n\t\tbad_page(page, \"compound_head not consistent\");\n\t\tgoto out;\n\t}\n\tret = 0;\nout:\n\tpage->mapping = NULL;\n\tclear_compound_head(page);\n\treturn ret;\n}\n\nstatic void kernel_init_free_pages(struct page *page, int numpages)\n{\n\tint i;\n\n\t/* s390's use of memset() could override KASAN redzones. */\n\tkasan_disable_current();\n\tfor (i = 0; i < numpages; i++) {\n\t\tu8 tag = page_kasan_tag(page + i);\n\t\tpage_kasan_tag_reset(page + i);\n\t\tclear_highpage(page + i);\n\t\tpage_kasan_tag_set(page + i, tag);\n\t}\n\tkasan_enable_current();\n}\n\nstatic __always_inline bool free_pages_prepare(struct page *page,\n\t\t\tunsigned int order, bool check_free, fpi_t fpi_flags)\n{\n\tint bad = 0;\n\tbool init;\n\n\tVM_BUG_ON_PAGE(PageTail(page), page);\n\n\ttrace_mm_page_free(page, order);\n\n\tif (unlikely(PageHWPoison(page)) && !order) {\n\t\t/*\n\t\t * Do not let hwpoison pages hit pcplists/buddy\n\t\t * Untie memcg state and reset page's owner\n\t\t */\n\t\tif (memcg_kmem_enabled() && PageMemcgKmem(page))\n\t\t\t__memcg_kmem_uncharge_page(page, order);\n\t\treset_page_owner(page, order);\n\t\treturn false;\n\t}\n\n\t/*\n\t * Check tail pages before head page information is cleared to\n\t * avoid checking PageCompound for order-0 pages.\n\t */\n\tif (unlikely(order)) {\n\t\tbool compound = PageCompound(page);\n\t\tint i;\n\n\t\tVM_BUG_ON_PAGE(compound && compound_order(page) != order, page);\n\n\t\tif (compound)\n\t\t\tClearPageDoubleMap(page);\n\t\tfor (i = 1; i < (1 << order); i++) {\n\t\t\tif (compound)\n\t\t\t\tbad += free_tail_pages_check(page, page + i);\n\t\t\tif (unlikely(check_free_page(page + i))) {\n\t\t\t\tbad++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t(page + i)->flags &= ~PAGE_FLAGS_CHECK_AT_PREP;\n\t\t}\n\t}\n\tif (PageMappingFlags(page))\n\t\tpage->mapping = NULL;\n\tif (memcg_kmem_enabled() && PageMemcgKmem(page))\n\t\t__memcg_kmem_uncharge_page(page, order);\n\tif (check_free)\n\t\tbad += check_free_page(page);\n\tif (bad)\n\t\treturn false;\n\n\tpage_cpupid_reset_last(page);\n\tpage->flags &= ~PAGE_FLAGS_CHECK_AT_PREP;\n\treset_page_owner(page, order);\n\n\tif (!PageHighMem(page)) {\n\t\tdebug_check_no_locks_freed(page_address(page),\n\t\t\t\t\t   PAGE_SIZE << order);\n\t\tdebug_check_no_obj_freed(page_address(page),\n\t\t\t\t\t   PAGE_SIZE << order);\n\t}\n\n\tkernel_poison_pages(page, 1 << order);\n\n\t/*\n\t * As memory initialization might be integrated into KASAN,\n\t * kasan_free_pages and kernel_init_free_pages must be\n\t * kept together to avoid discrepancies in behavior.\n\t *\n\t * With hardware tag-based KASAN, memory tags must be set before the\n\t * page becomes unavailable via debug_pagealloc or arch_free_page.\n\t */\n\tinit = want_init_on_free();\n\tif (init && !kasan_has_integrated_init())\n\t\tkernel_init_free_pages(page, 1 << order);\n\tkasan_free_nondeferred_pages(page, order, init, fpi_flags);\n\n\t/*\n\t * arch_free_page() can make the page's contents inaccessible.  s390\n\t * does this.  So nothing which can access the page's contents should\n\t * happen after this.\n\t */\n\tarch_free_page(page, order);\n\n\tdebug_pagealloc_unmap_pages(page, 1 << order);\n\n\treturn true;\n}\n\n#ifdef CONFIG_DEBUG_VM\n/*\n * With DEBUG_VM enabled, order-0 pages are checked immediately when being freed\n * to pcp lists. With debug_pagealloc also enabled, they are also rechecked when\n * moved from pcp lists to free lists.\n */\nstatic bool free_pcp_prepare(struct page *page)\n{\n\treturn free_pages_prepare(page, 0, true, FPI_NONE);\n}\n\nstatic bool bulkfree_pcp_prepare(struct page *page)\n{\n\tif (debug_pagealloc_enabled_static())\n\t\treturn check_free_page(page);\n\telse\n\t\treturn false;\n}\n#else\n/*\n * With DEBUG_VM disabled, order-0 pages being freed are checked only when\n * moving from pcp lists to free list in order to reduce overhead. With\n * debug_pagealloc enabled, they are checked also immediately when being freed\n * to the pcp lists.\n */\nstatic bool free_pcp_prepare(struct page *page)\n{\n\tif (debug_pagealloc_enabled_static())\n\t\treturn free_pages_prepare(page, 0, true, FPI_NONE);\n\telse\n\t\treturn free_pages_prepare(page, 0, false, FPI_NONE);\n}\n\nstatic bool bulkfree_pcp_prepare(struct page *page)\n{\n\treturn check_free_page(page);\n}\n#endif /* CONFIG_DEBUG_VM */\n\nstatic inline void prefetch_buddy(struct page *page)\n{\n\tunsigned long pfn = page_to_pfn(page);\n\tunsigned long buddy_pfn = __find_buddy_pfn(pfn, 0);\n\tstruct page *buddy = page + (buddy_pfn - pfn);\n\n\tprefetch(buddy);\n}\n\n/*\n * Frees a number of pages from the PCP lists\n * Assumes all pages on list are in same zone, and of same order.\n * count is the number of pages to free.\n *\n * If the zone was previously in an \"all pages pinned\" state then look to\n * see if this freeing clears that state.\n *\n * And clear the zone's pages_scanned counter, to hold off the \"all pages are\n * pinned\" detection logic.\n */\nstatic void free_pcppages_bulk(struct zone *zone, int count,\n\t\t\t\t\tstruct per_cpu_pages *pcp)\n{\n\tint migratetype = 0;\n\tint batch_free = 0;\n\tint prefetch_nr = READ_ONCE(pcp->batch);\n\tbool isolated_pageblocks;\n\tstruct page *page, *tmp;\n\tLIST_HEAD(head);\n\n\t/*\n\t * Ensure proper count is passed which otherwise would stuck in the\n\t * below while (list_empty(list)) loop.\n\t */\n\tcount = min(pcp->count, count);\n\twhile (count) {\n\t\tstruct list_head *list;\n\n\t\t/*\n\t\t * Remove pages from lists in a round-robin fashion. A\n\t\t * batch_free count is maintained that is incremented when an\n\t\t * empty list is encountered.  This is so more pages are freed\n\t\t * off fuller lists instead of spinning excessively around empty\n\t\t * lists\n\t\t */\n\t\tdo {\n\t\t\tbatch_free++;\n\t\t\tif (++migratetype == MIGRATE_PCPTYPES)\n\t\t\t\tmigratetype = 0;\n\t\t\tlist = &pcp->lists[migratetype];\n\t\t} while (list_empty(list));\n\n\t\t/* This is the only non-empty list. Free them all. */\n\t\tif (batch_free == MIGRATE_PCPTYPES)\n\t\t\tbatch_free = count;\n\n\t\tdo {\n\t\t\tpage = list_last_entry(list, struct page, lru);\n\t\t\t/* must delete to avoid corrupting pcp list */\n\t\t\tlist_del(&page->lru);\n\t\t\tpcp->count--;\n\n\t\t\tif (bulkfree_pcp_prepare(page))\n\t\t\t\tcontinue;\n\n\t\t\tlist_add_tail(&page->lru, &head);\n\n\t\t\t/*\n\t\t\t * We are going to put the page back to the global\n\t\t\t * pool, prefetch its buddy to speed up later access\n\t\t\t * under zone->lock. It is believed the overhead of\n\t\t\t * an additional test and calculating buddy_pfn here\n\t\t\t * can be offset by reduced memory latency later. To\n\t\t\t * avoid excessive prefetching due to large count, only\n\t\t\t * prefetch buddy for the first pcp->batch nr of pages.\n\t\t\t */\n\t\t\tif (prefetch_nr) {\n\t\t\t\tprefetch_buddy(page);\n\t\t\t\tprefetch_nr--;\n\t\t\t}\n\t\t} while (--count && --batch_free && !list_empty(list));\n\t}\n\n\tspin_lock(&zone->lock);\n\tisolated_pageblocks = has_isolate_pageblock(zone);\n\n\t/*\n\t * Use safe version since after __free_one_page(),\n\t * page->lru.next will not point to original list.\n\t */\n\tlist_for_each_entry_safe(page, tmp, &head, lru) {\n\t\tint mt = get_pcppage_migratetype(page);\n\t\t/* MIGRATE_ISOLATE page should not go to pcplists */\n\t\tVM_BUG_ON_PAGE(is_migrate_isolate(mt), page);\n\t\t/* Pageblock could have been isolated meanwhile */\n\t\tif (unlikely(isolated_pageblocks))\n\t\t\tmt = get_pageblock_migratetype(page);\n\n\t\t__free_one_page(page, page_to_pfn(page), zone, 0, mt, FPI_NONE);\n\t\ttrace_mm_page_pcpu_drain(page, 0, mt);\n\t}\n\tspin_unlock(&zone->lock);\n}\n\nstatic void free_one_page(struct zone *zone,\n\t\t\t\tstruct page *page, unsigned long pfn,\n\t\t\t\tunsigned int order,\n\t\t\t\tint migratetype, fpi_t fpi_flags)\n{\n\tspin_lock(&zone->lock);\n\tif (unlikely(has_isolate_pageblock(zone) ||\n\t\tis_migrate_isolate(migratetype))) {\n\t\tmigratetype = get_pfnblock_migratetype(page, pfn);\n\t}\n\t__free_one_page(page, pfn, zone, order, migratetype, fpi_flags);\n\tspin_unlock(&zone->lock);\n}\n\nstatic void __meminit __init_single_page(struct page *page, unsigned long pfn,\n\t\t\t\tunsigned long zone, int nid)\n{\n\tmm_zero_struct_page(page);\n\tset_page_links(page, zone, nid, pfn);\n\tinit_page_count(page);\n\tpage_mapcount_reset(page);\n\tpage_cpupid_reset_last(page);\n\tpage_kasan_tag_reset(page);\n\n\tINIT_LIST_HEAD(&page->lru);\n#ifdef WANT_PAGE_VIRTUAL\n\t/* The shift won't overflow because ZONE_NORMAL is below 4G. */\n\tif (!is_highmem_idx(zone))\n\t\tset_page_address(page, __va(pfn << PAGE_SHIFT));\n#endif\n}\n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\nstatic void __meminit init_reserved_page(unsigned long pfn)\n{\n\tpg_data_t *pgdat;\n\tint nid, zid;\n\n\tif (!early_page_uninitialised(pfn))\n\t\treturn;\n\n\tnid = early_pfn_to_nid(pfn);\n\tpgdat = NODE_DATA(nid);\n\n\tfor (zid = 0; zid < MAX_NR_ZONES; zid++) {\n\t\tstruct zone *zone = &pgdat->node_zones[zid];\n\n\t\tif (pfn >= zone->zone_start_pfn && pfn < zone_end_pfn(zone))\n\t\t\tbreak;\n\t}\n\t__init_single_page(pfn_to_page(pfn), pfn, zid, nid);\n}\n#else\nstatic inline void init_reserved_page(unsigned long pfn)\n{\n}\n#endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */\n\n/*\n * Initialised pages do not have PageReserved set. This function is\n * called for each range allocated by the bootmem allocator and\n * marks the pages PageReserved. The remaining valid pages are later\n * sent to the buddy page allocator.\n */\nvoid __meminit reserve_bootmem_region(phys_addr_t start, phys_addr_t end)\n{\n\tunsigned long start_pfn = PFN_DOWN(start);\n\tunsigned long end_pfn = PFN_UP(end);\n\n\tfor (; start_pfn < end_pfn; start_pfn++) {\n\t\tif (pfn_valid(start_pfn)) {\n\t\t\tstruct page *page = pfn_to_page(start_pfn);\n\n\t\t\tinit_reserved_page(start_pfn);\n\n\t\t\t/* Avoid false-positive PageTail() */\n\t\t\tINIT_LIST_HEAD(&page->lru);\n\n\t\t\t/*\n\t\t\t * no need for atomic set_bit because the struct\n\t\t\t * page is not visible yet so nobody should\n\t\t\t * access it yet.\n\t\t\t */\n\t\t\t__SetPageReserved(page);\n\t\t}\n\t}\n}\n\nstatic void __free_pages_ok(struct page *page, unsigned int order,\n\t\t\t    fpi_t fpi_flags)\n{\n\tunsigned long flags;\n\tint migratetype;\n\tunsigned long pfn = page_to_pfn(page);\n\n\tif (!free_pages_prepare(page, order, true, fpi_flags))\n\t\treturn;\n\n\tmigratetype = get_pfnblock_migratetype(page, pfn);\n\tlocal_irq_save(flags);\n\t__count_vm_events(PGFREE, 1 << order);\n\tfree_one_page(page_zone(page), page, pfn, order, migratetype,\n\t\t      fpi_flags);\n\tlocal_irq_restore(flags);\n}\n\nvoid __free_pages_core(struct page *page, unsigned int order)\n{\n\tunsigned int nr_pages = 1 << order;\n\tstruct page *p = page;\n\tunsigned int loop;\n\n\t/*\n\t * When initializing the memmap, __init_single_page() sets the refcount\n\t * of all pages to 1 (\"allocated\"/\"not free\"). We have to set the\n\t * refcount of all involved pages to 0.\n\t */\n\tprefetchw(p);\n\tfor (loop = 0; loop < (nr_pages - 1); loop++, p++) {\n\t\tprefetchw(p + 1);\n\t\t__ClearPageReserved(p);\n\t\tset_page_count(p, 0);\n\t}\n\t__ClearPageReserved(p);\n\tset_page_count(p, 0);\n\n\tatomic_long_add(nr_pages, &page_zone(page)->managed_pages);\n\n\t/*\n\t * Bypass PCP and place fresh pages right to the tail, primarily\n\t * relevant for memory onlining.\n\t */\n\t__free_pages_ok(page, order, FPI_TO_TAIL | FPI_SKIP_KASAN_POISON);\n}\n\n#ifdef CONFIG_NEED_MULTIPLE_NODES\n\n/*\n * During memory init memblocks map pfns to nids. The search is expensive and\n * this caches recent lookups. The implementation of __early_pfn_to_nid\n * treats start/end as pfns.\n */\nstruct mminit_pfnnid_cache {\n\tunsigned long last_start;\n\tunsigned long last_end;\n\tint last_nid;\n};\n\nstatic struct mminit_pfnnid_cache early_pfnnid_cache __meminitdata;\n\n/*\n * Required by SPARSEMEM. Given a PFN, return what node the PFN is on.\n */\nstatic int __meminit __early_pfn_to_nid(unsigned long pfn,\n\t\t\t\t\tstruct mminit_pfnnid_cache *state)\n{\n\tunsigned long start_pfn, end_pfn;\n\tint nid;\n\n\tif (state->last_start <= pfn && pfn < state->last_end)\n\t\treturn state->last_nid;\n\n\tnid = memblock_search_pfn_nid(pfn, &start_pfn, &end_pfn);\n\tif (nid != NUMA_NO_NODE) {\n\t\tstate->last_start = start_pfn;\n\t\tstate->last_end = end_pfn;\n\t\tstate->last_nid = nid;\n\t}\n\n\treturn nid;\n}\n\nint __meminit early_pfn_to_nid(unsigned long pfn)\n{\n\tstatic DEFINE_SPINLOCK(early_pfn_lock);\n\tint nid;\n\n\tspin_lock(&early_pfn_lock);\n\tnid = __early_pfn_to_nid(pfn, &early_pfnnid_cache);\n\tif (nid < 0)\n\t\tnid = first_online_node;\n\tspin_unlock(&early_pfn_lock);\n\n\treturn nid;\n}\n#endif /* CONFIG_NEED_MULTIPLE_NODES */\n\nvoid __init memblock_free_pages(struct page *page, unsigned long pfn,\n\t\t\t\t\t\t\tunsigned int order)\n{\n\tif (early_page_uninitialised(pfn))\n\t\treturn;\n\t__free_pages_core(page, order);\n}\n\n/*\n * Check that the whole (or subset of) a pageblock given by the interval of\n * [start_pfn, end_pfn) is valid and within the same zone, before scanning it\n * with the migration of free compaction scanner. The scanners then need to\n * use only pfn_valid_within() check for arches that allow holes within\n * pageblocks.\n *\n * Return struct page pointer of start_pfn, or NULL if checks were not passed.\n *\n * It's possible on some configurations to have a setup like node0 node1 node0\n * i.e. it's possible that all pages within a zones range of pages do not\n * belong to a single zone. We assume that a border between node0 and node1\n * can occur within a single pageblock, but not a node0 node1 node0\n * interleaving within a single pageblock. It is therefore sufficient to check\n * the first and last page of a pageblock and avoid checking each individual\n * page in a pageblock.\n */\nstruct page *__pageblock_pfn_to_page(unsigned long start_pfn,\n\t\t\t\t     unsigned long end_pfn, struct zone *zone)\n{\n\tstruct page *start_page;\n\tstruct page *end_page;\n\n\t/* end_pfn is one past the range we are checking */\n\tend_pfn--;\n\n\tif (!pfn_valid(start_pfn) || !pfn_valid(end_pfn))\n\t\treturn NULL;\n\n\tstart_page = pfn_to_online_page(start_pfn);\n\tif (!start_page)\n\t\treturn NULL;\n\n\tif (page_zone(start_page) != zone)\n\t\treturn NULL;\n\n\tend_page = pfn_to_page(end_pfn);\n\n\t/* This gives a shorter code than deriving page_zone(end_page) */\n\tif (page_zone_id(start_page) != page_zone_id(end_page))\n\t\treturn NULL;\n\n\treturn start_page;\n}\n\nvoid set_zone_contiguous(struct zone *zone)\n{\n\tunsigned long block_start_pfn = zone->zone_start_pfn;\n\tunsigned long block_end_pfn;\n\n\tblock_end_pfn = ALIGN(block_start_pfn + 1, pageblock_nr_pages);\n\tfor (; block_start_pfn < zone_end_pfn(zone);\n\t\t\tblock_start_pfn = block_end_pfn,\n\t\t\t block_end_pfn += pageblock_nr_pages) {\n\n\t\tblock_end_pfn = min(block_end_pfn, zone_end_pfn(zone));\n\n\t\tif (!__pageblock_pfn_to_page(block_start_pfn,\n\t\t\t\t\t     block_end_pfn, zone))\n\t\t\treturn;\n\t\tcond_resched();\n\t}\n\n\t/* We confirm that there is no hole */\n\tzone->contiguous = true;\n}\n\nvoid clear_zone_contiguous(struct zone *zone)\n{\n\tzone->contiguous = false;\n}\n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\nstatic void __init deferred_free_range(unsigned long pfn,\n\t\t\t\t       unsigned long nr_pages)\n{\n\tstruct page *page;\n\tunsigned long i;\n\n\tif (!nr_pages)\n\t\treturn;\n\n\tpage = pfn_to_page(pfn);\n\n\t/* Free a large naturally-aligned chunk if possible */\n\tif (nr_pages == pageblock_nr_pages &&\n\t    (pfn & (pageblock_nr_pages - 1)) == 0) {\n\t\tset_pageblock_migratetype(page, MIGRATE_MOVABLE);\n\t\t__free_pages_core(page, pageblock_order);\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < nr_pages; i++, page++, pfn++) {\n\t\tif ((pfn & (pageblock_nr_pages - 1)) == 0)\n\t\t\tset_pageblock_migratetype(page, MIGRATE_MOVABLE);\n\t\t__free_pages_core(page, 0);\n\t}\n}\n\n/* Completion tracking for deferred_init_memmap() threads */\nstatic atomic_t pgdat_init_n_undone __initdata;\nstatic __initdata DECLARE_COMPLETION(pgdat_init_all_done_comp);\n\nstatic inline void __init pgdat_init_report_one_done(void)\n{\n\tif (atomic_dec_and_test(&pgdat_init_n_undone))\n\t\tcomplete(&pgdat_init_all_done_comp);\n}\n\n/*\n * Returns true if page needs to be initialized or freed to buddy allocator.\n *\n * First we check if pfn is valid on architectures where it is possible to have\n * holes within pageblock_nr_pages. On systems where it is not possible, this\n * function is optimized out.\n *\n * Then, we check if a current large page is valid by only checking the validity\n * of the head pfn.\n */\nstatic inline bool __init deferred_pfn_valid(unsigned long pfn)\n{\n\tif (!pfn_valid_within(pfn))\n\t\treturn false;\n\tif (!(pfn & (pageblock_nr_pages - 1)) && !pfn_valid(pfn))\n\t\treturn false;\n\treturn true;\n}\n\n/*\n * Free pages to buddy allocator. Try to free aligned pages in\n * pageblock_nr_pages sizes.\n */\nstatic void __init deferred_free_pages(unsigned long pfn,\n\t\t\t\t       unsigned long end_pfn)\n{\n\tunsigned long nr_pgmask = pageblock_nr_pages - 1;\n\tunsigned long nr_free = 0;\n\n\tfor (; pfn < end_pfn; pfn++) {\n\t\tif (!deferred_pfn_valid(pfn)) {\n\t\t\tdeferred_free_range(pfn - nr_free, nr_free);\n\t\t\tnr_free = 0;\n\t\t} else if (!(pfn & nr_pgmask)) {\n\t\t\tdeferred_free_range(pfn - nr_free, nr_free);\n\t\t\tnr_free = 1;\n\t\t} else {\n\t\t\tnr_free++;\n\t\t}\n\t}\n\t/* Free the last block of pages to allocator */\n\tdeferred_free_range(pfn - nr_free, nr_free);\n}\n\n/*\n * Initialize struct pages.  We minimize pfn page lookups and scheduler checks\n * by performing it only once every pageblock_nr_pages.\n * Return number of pages initialized.\n */\nstatic unsigned long  __init deferred_init_pages(struct zone *zone,\n\t\t\t\t\t\t unsigned long pfn,\n\t\t\t\t\t\t unsigned long end_pfn)\n{\n\tunsigned long nr_pgmask = pageblock_nr_pages - 1;\n\tint nid = zone_to_nid(zone);\n\tunsigned long nr_pages = 0;\n\tint zid = zone_idx(zone);\n\tstruct page *page = NULL;\n\n\tfor (; pfn < end_pfn; pfn++) {\n\t\tif (!deferred_pfn_valid(pfn)) {\n\t\t\tpage = NULL;\n\t\t\tcontinue;\n\t\t} else if (!page || !(pfn & nr_pgmask)) {\n\t\t\tpage = pfn_to_page(pfn);\n\t\t} else {\n\t\t\tpage++;\n\t\t}\n\t\t__init_single_page(page, pfn, zid, nid);\n\t\tnr_pages++;\n\t}\n\treturn (nr_pages);\n}\n\n/*\n * This function is meant to pre-load the iterator for the zone init.\n * Specifically it walks through the ranges until we are caught up to the\n * first_init_pfn value and exits there. If we never encounter the value we\n * return false indicating there are no valid ranges left.\n */\nstatic bool __init\ndeferred_init_mem_pfn_range_in_zone(u64 *i, struct zone *zone,\n\t\t\t\t    unsigned long *spfn, unsigned long *epfn,\n\t\t\t\t    unsigned long first_init_pfn)\n{\n\tu64 j;\n\n\t/*\n\t * Start out by walking through the ranges in this zone that have\n\t * already been initialized. We don't need to do anything with them\n\t * so we just need to flush them out of the system.\n\t */\n\tfor_each_free_mem_pfn_range_in_zone(j, zone, spfn, epfn) {\n\t\tif (*epfn <= first_init_pfn)\n\t\t\tcontinue;\n\t\tif (*spfn < first_init_pfn)\n\t\t\t*spfn = first_init_pfn;\n\t\t*i = j;\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * Initialize and free pages. We do it in two loops: first we initialize\n * struct page, then free to buddy allocator, because while we are\n * freeing pages we can access pages that are ahead (computing buddy\n * page in __free_one_page()).\n *\n * In order to try and keep some memory in the cache we have the loop\n * broken along max page order boundaries. This way we will not cause\n * any issues with the buddy page computation.\n */\nstatic unsigned long __init\ndeferred_init_maxorder(u64 *i, struct zone *zone, unsigned long *start_pfn,\n\t\t       unsigned long *end_pfn)\n{\n\tunsigned long mo_pfn = ALIGN(*start_pfn + 1, MAX_ORDER_NR_PAGES);\n\tunsigned long spfn = *start_pfn, epfn = *end_pfn;\n\tunsigned long nr_pages = 0;\n\tu64 j = *i;\n\n\t/* First we loop through and initialize the page values */\n\tfor_each_free_mem_pfn_range_in_zone_from(j, zone, start_pfn, end_pfn) {\n\t\tunsigned long t;\n\n\t\tif (mo_pfn <= *start_pfn)\n\t\t\tbreak;\n\n\t\tt = min(mo_pfn, *end_pfn);\n\t\tnr_pages += deferred_init_pages(zone, *start_pfn, t);\n\n\t\tif (mo_pfn < *end_pfn) {\n\t\t\t*start_pfn = mo_pfn;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Reset values and now loop through freeing pages as needed */\n\tswap(j, *i);\n\n\tfor_each_free_mem_pfn_range_in_zone_from(j, zone, &spfn, &epfn) {\n\t\tunsigned long t;\n\n\t\tif (mo_pfn <= spfn)\n\t\t\tbreak;\n\n\t\tt = min(mo_pfn, epfn);\n\t\tdeferred_free_pages(spfn, t);\n\n\t\tif (mo_pfn <= epfn)\n\t\t\tbreak;\n\t}\n\n\treturn nr_pages;\n}\n\nstatic void __init\ndeferred_init_memmap_chunk(unsigned long start_pfn, unsigned long end_pfn,\n\t\t\t   void *arg)\n{\n\tunsigned long spfn, epfn;\n\tstruct zone *zone = arg;\n\tu64 i;\n\n\tdeferred_init_mem_pfn_range_in_zone(&i, zone, &spfn, &epfn, start_pfn);\n\n\t/*\n\t * Initialize and free pages in MAX_ORDER sized increments so that we\n\t * can avoid introducing any issues with the buddy allocator.\n\t */\n\twhile (spfn < end_pfn) {\n\t\tdeferred_init_maxorder(&i, zone, &spfn, &epfn);\n\t\tcond_resched();\n\t}\n}\n\n/* An arch may override for more concurrency. */\n__weak int __init\ndeferred_page_init_max_threads(const struct cpumask *node_cpumask)\n{\n\treturn 1;\n}\n\n/* Initialise remaining memory on a node */\nstatic int __init deferred_init_memmap(void *data)\n{\n\tpg_data_t *pgdat = data;\n\tconst struct cpumask *cpumask = cpumask_of_node(pgdat->node_id);\n\tunsigned long spfn = 0, epfn = 0;\n\tunsigned long first_init_pfn, flags;\n\tunsigned long start = jiffies;\n\tstruct zone *zone;\n\tint zid, max_threads;\n\tu64 i;\n\n\t/* Bind memory initialisation thread to a local node if possible */\n\tif (!cpumask_empty(cpumask))\n\t\tset_cpus_allowed_ptr(current, cpumask);\n\n\tpgdat_resize_lock(pgdat, &flags);\n\tfirst_init_pfn = pgdat->first_deferred_pfn;\n\tif (first_init_pfn == ULONG_MAX) {\n\t\tpgdat_resize_unlock(pgdat, &flags);\n\t\tpgdat_init_report_one_done();\n\t\treturn 0;\n\t}\n\n\t/* Sanity check boundaries */\n\tBUG_ON(pgdat->first_deferred_pfn < pgdat->node_start_pfn);\n\tBUG_ON(pgdat->first_deferred_pfn > pgdat_end_pfn(pgdat));\n\tpgdat->first_deferred_pfn = ULONG_MAX;\n\n\t/*\n\t * Once we unlock here, the zone cannot be grown anymore, thus if an\n\t * interrupt thread must allocate this early in boot, zone must be\n\t * pre-grown prior to start of deferred page initialization.\n\t */\n\tpgdat_resize_unlock(pgdat, &flags);\n\n\t/* Only the highest zone is deferred so find it */\n\tfor (zid = 0; zid < MAX_NR_ZONES; zid++) {\n\t\tzone = pgdat->node_zones + zid;\n\t\tif (first_init_pfn < zone_end_pfn(zone))\n\t\t\tbreak;\n\t}\n\n\t/* If the zone is empty somebody else may have cleared out the zone */\n\tif (!deferred_init_mem_pfn_range_in_zone(&i, zone, &spfn, &epfn,\n\t\t\t\t\t\t first_init_pfn))\n\t\tgoto zone_empty;\n\n\tmax_threads = deferred_page_init_max_threads(cpumask);\n\n\twhile (spfn < epfn) {\n\t\tunsigned long epfn_align = ALIGN(epfn, PAGES_PER_SECTION);\n\t\tstruct padata_mt_job job = {\n\t\t\t.thread_fn   = deferred_init_memmap_chunk,\n\t\t\t.fn_arg      = zone,\n\t\t\t.start       = spfn,\n\t\t\t.size        = epfn_align - spfn,\n\t\t\t.align       = PAGES_PER_SECTION,\n\t\t\t.min_chunk   = PAGES_PER_SECTION,\n\t\t\t.max_threads = max_threads,\n\t\t};\n\n\t\tpadata_do_multithreaded(&job);\n\t\tdeferred_init_mem_pfn_range_in_zone(&i, zone, &spfn, &epfn,\n\t\t\t\t\t\t    epfn_align);\n\t}\nzone_empty:\n\t/* Sanity check that the next zone really is unpopulated */\n\tWARN_ON(++zid < MAX_NR_ZONES && populated_zone(++zone));\n\n\tpr_info(\"node %d deferred pages initialised in %ums\\n\",\n\t\tpgdat->node_id, jiffies_to_msecs(jiffies - start));\n\n\tpgdat_init_report_one_done();\n\treturn 0;\n}\n\n/*\n * If this zone has deferred pages, try to grow it by initializing enough\n * deferred pages to satisfy the allocation specified by order, rounded up to\n * the nearest PAGES_PER_SECTION boundary.  So we're adding memory in increments\n * of SECTION_SIZE bytes by initializing struct pages in increments of\n * PAGES_PER_SECTION * sizeof(struct page) bytes.\n *\n * Return true when zone was grown, otherwise return false. We return true even\n * when we grow less than requested, to let the caller decide if there are\n * enough pages to satisfy the allocation.\n *\n * Note: We use noinline because this function is needed only during boot, and\n * it is called from a __ref function _deferred_grow_zone. This way we are\n * making sure that it is not inlined into permanent text section.\n */\nstatic noinline bool __init\ndeferred_grow_zone(struct zone *zone, unsigned int order)\n{\n\tunsigned long nr_pages_needed = ALIGN(1 << order, PAGES_PER_SECTION);\n\tpg_data_t *pgdat = zone->zone_pgdat;\n\tunsigned long first_deferred_pfn = pgdat->first_deferred_pfn;\n\tunsigned long spfn, epfn, flags;\n\tunsigned long nr_pages = 0;\n\tu64 i;\n\n\t/* Only the last zone may have deferred pages */\n\tif (zone_end_pfn(zone) != pgdat_end_pfn(pgdat))\n\t\treturn false;\n\n\tpgdat_resize_lock(pgdat, &flags);\n\n\t/*\n\t * If someone grew this zone while we were waiting for spinlock, return\n\t * true, as there might be enough pages already.\n\t */\n\tif (first_deferred_pfn != pgdat->first_deferred_pfn) {\n\t\tpgdat_resize_unlock(pgdat, &flags);\n\t\treturn true;\n\t}\n\n\t/* If the zone is empty somebody else may have cleared out the zone */\n\tif (!deferred_init_mem_pfn_range_in_zone(&i, zone, &spfn, &epfn,\n\t\t\t\t\t\t first_deferred_pfn)) {\n\t\tpgdat->first_deferred_pfn = ULONG_MAX;\n\t\tpgdat_resize_unlock(pgdat, &flags);\n\t\t/* Retry only once. */\n\t\treturn first_deferred_pfn != ULONG_MAX;\n\t}\n\n\t/*\n\t * Initialize and free pages in MAX_ORDER sized increments so\n\t * that we can avoid introducing any issues with the buddy\n\t * allocator.\n\t */\n\twhile (spfn < epfn) {\n\t\t/* update our first deferred PFN for this section */\n\t\tfirst_deferred_pfn = spfn;\n\n\t\tnr_pages += deferred_init_maxorder(&i, zone, &spfn, &epfn);\n\t\ttouch_nmi_watchdog();\n\n\t\t/* We should only stop along section boundaries */\n\t\tif ((first_deferred_pfn ^ spfn) < PAGES_PER_SECTION)\n\t\t\tcontinue;\n\n\t\t/* If our quota has been met we can stop here */\n\t\tif (nr_pages >= nr_pages_needed)\n\t\t\tbreak;\n\t}\n\n\tpgdat->first_deferred_pfn = spfn;\n\tpgdat_resize_unlock(pgdat, &flags);\n\n\treturn nr_pages > 0;\n}\n\n/*\n * deferred_grow_zone() is __init, but it is called from\n * get_page_from_freelist() during early boot until deferred_pages permanently\n * disables this call. This is why we have refdata wrapper to avoid warning,\n * and to ensure that the function body gets unloaded.\n */\nstatic bool __ref\n_deferred_grow_zone(struct zone *zone, unsigned int order)\n{\n\treturn deferred_grow_zone(zone, order);\n}\n\n#endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */\n\nvoid __init page_alloc_init_late(void)\n{\n\tstruct zone *zone;\n\tint nid;\n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\n\n\t/* There will be num_node_state(N_MEMORY) threads */\n\tatomic_set(&pgdat_init_n_undone, num_node_state(N_MEMORY));\n\tfor_each_node_state(nid, N_MEMORY) {\n\t\tkthread_run(deferred_init_memmap, NODE_DATA(nid), \"pgdatinit%d\", nid);\n\t}\n\n\t/* Block until all are initialised */\n\twait_for_completion(&pgdat_init_all_done_comp);\n\n\t/*\n\t * The number of managed pages has changed due to the initialisation\n\t * so the pcpu batch and high limits needs to be updated or the limits\n\t * will be artificially small.\n\t */\n\tfor_each_populated_zone(zone)\n\t\tzone_pcp_update(zone);\n\n\t/*\n\t * We initialized the rest of the deferred pages.  Permanently disable\n\t * on-demand struct page initialization.\n\t */\n\tstatic_branch_disable(&deferred_pages);\n\n\t/* Reinit limits that are based on free pages after the kernel is up */\n\tfiles_maxfiles_init();\n#endif\n\n\tbuffer_init();\n\n\t/* Discard memblock private memory */\n\tmemblock_discard();\n\n\tfor_each_node_state(nid, N_MEMORY)\n\t\tshuffle_free_memory(NODE_DATA(nid));\n\n\tfor_each_populated_zone(zone)\n\t\tset_zone_contiguous(zone);\n}\n\n#ifdef CONFIG_CMA\n/* Free whole pageblock and set its migration type to MIGRATE_CMA. */\nvoid __init init_cma_reserved_pageblock(struct page *page)\n{\n\tunsigned i = pageblock_nr_pages;\n\tstruct page *p = page;\n\n\tdo {\n\t\t__ClearPageReserved(p);\n\t\tset_page_count(p, 0);\n\t} while (++p, --i);\n\n\tset_pageblock_migratetype(page, MIGRATE_CMA);\n\n\tif (pageblock_order >= MAX_ORDER) {\n\t\ti = pageblock_nr_pages;\n\t\tp = page;\n\t\tdo {\n\t\t\tset_page_refcounted(p);\n\t\t\t__free_pages(p, MAX_ORDER - 1);\n\t\t\tp += MAX_ORDER_NR_PAGES;\n\t\t} while (i -= MAX_ORDER_NR_PAGES);\n\t} else {\n\t\tset_page_refcounted(page);\n\t\t__free_pages(page, pageblock_order);\n\t}\n\n\tadjust_managed_page_count(page, pageblock_nr_pages);\n\tpage_zone(page)->cma_pages += pageblock_nr_pages;\n}\n#endif\n\n/*\n * The order of subdivision here is critical for the IO subsystem.\n * Please do not alter this order without good reasons and regression\n * testing. Specifically, as large blocks of memory are subdivided,\n * the order in which smaller blocks are delivered depends on the order\n * they're subdivided in this function. This is the primary factor\n * influencing the order in which pages are delivered to the IO\n * subsystem according to empirical testing, and this is also justified\n * by considering the behavior of a buddy system containing a single\n * large block of memory acted on by a series of small allocations.\n * This behavior is a critical factor in sglist merging's success.\n *\n * -- nyc\n */\nstatic inline void expand(struct zone *zone, struct page *page,\n\tint low, int high, int migratetype)\n{\n\tunsigned long size = 1 << high;\n\n\twhile (high > low) {\n\t\thigh--;\n\t\tsize >>= 1;\n\t\tVM_BUG_ON_PAGE(bad_range(zone, &page[size]), &page[size]);\n\n\t\t/*\n\t\t * Mark as guard pages (or page), that will allow to\n\t\t * merge back to allocator when buddy will be freed.\n\t\t * Corresponding page table entries will not be touched,\n\t\t * pages will stay not present in virtual address space\n\t\t */\n\t\tif (set_page_guard(zone, &page[size], high, migratetype))\n\t\t\tcontinue;\n\n\t\tadd_to_free_list(&page[size], zone, high, migratetype);\n\t\tset_buddy_order(&page[size], high);\n\t}\n}\n\nstatic void check_new_page_bad(struct page *page)\n{\n\tif (unlikely(page->flags & __PG_HWPOISON)) {\n\t\t/* Don't complain about hwpoisoned pages */\n\t\tpage_mapcount_reset(page); /* remove PageBuddy */\n\t\treturn;\n\t}\n\n\tbad_page(page,\n\t\t page_bad_reason(page, PAGE_FLAGS_CHECK_AT_PREP));\n}\n\n/*\n * This page is about to be returned from the page allocator\n */\nstatic inline int check_new_page(struct page *page)\n{\n\tif (likely(page_expected_state(page,\n\t\t\t\tPAGE_FLAGS_CHECK_AT_PREP|__PG_HWPOISON)))\n\t\treturn 0;\n\n\tcheck_new_page_bad(page);\n\treturn 1;\n}\n\n#ifdef CONFIG_DEBUG_VM\n/*\n * With DEBUG_VM enabled, order-0 pages are checked for expected state when\n * being allocated from pcp lists. With debug_pagealloc also enabled, they are\n * also checked when pcp lists are refilled from the free lists.\n */\nstatic inline bool check_pcp_refill(struct page *page)\n{\n\tif (debug_pagealloc_enabled_static())\n\t\treturn check_new_page(page);\n\telse\n\t\treturn false;\n}\n\nstatic inline bool check_new_pcp(struct page *page)\n{\n\treturn check_new_page(page);\n}\n#else\n/*\n * With DEBUG_VM disabled, free order-0 pages are checked for expected state\n * when pcp lists are being refilled from the free lists. With debug_pagealloc\n * enabled, they are also checked when being allocated from the pcp lists.\n */\nstatic inline bool check_pcp_refill(struct page *page)\n{\n\treturn check_new_page(page);\n}\nstatic inline bool check_new_pcp(struct page *page)\n{\n\tif (debug_pagealloc_enabled_static())\n\t\treturn check_new_page(page);\n\telse\n\t\treturn false;\n}\n#endif /* CONFIG_DEBUG_VM */\n\nstatic bool check_new_pages(struct page *page, unsigned int order)\n{\n\tint i;\n\tfor (i = 0; i < (1 << order); i++) {\n\t\tstruct page *p = page + i;\n\n\t\tif (unlikely(check_new_page(p)))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\ninline void post_alloc_hook(struct page *page, unsigned int order,\n\t\t\t\tgfp_t gfp_flags)\n{\n\tbool init;\n\n\tset_page_private(page, 0);\n\tset_page_refcounted(page);\n\n\tarch_alloc_page(page, order);\n\tdebug_pagealloc_map_pages(page, 1 << order);\n\n\t/*\n\t * As memory initialization might be integrated into KASAN,\n\t * kasan_alloc_pages and kernel_init_free_pages must be\n\t * kept together to avoid discrepancies in behavior.\n\t */\n\tinit = !want_init_on_free() && want_init_on_alloc(gfp_flags);\n\tkasan_alloc_pages(page, order, init);\n\tif (init && !kasan_has_integrated_init())\n\t\tkernel_init_free_pages(page, 1 << order);\n\n\tkernel_unpoison_pages(page, 1 << order);\n\tset_page_owner(page, order, gfp_flags);\n}\n\nstatic void prep_new_page(struct page *page, unsigned int order, gfp_t gfp_flags,\n\t\t\t\t\t\t\tunsigned int alloc_flags)\n{\n\tpost_alloc_hook(page, order, gfp_flags);\n\n\tif (order && (gfp_flags & __GFP_COMP))\n\t\tprep_compound_page(page, order);\n\n\t/*\n\t * page is set pfmemalloc when ALLOC_NO_WATERMARKS was necessary to\n\t * allocate the page. The expectation is that the caller is taking\n\t * steps that will free more memory. The caller should avoid the page\n\t * being used for !PFMEMALLOC purposes.\n\t */\n\tif (alloc_flags & ALLOC_NO_WATERMARKS)\n\t\tset_page_pfmemalloc(page);\n\telse\n\t\tclear_page_pfmemalloc(page);\n}\n\n/*\n * Go through the free lists for the given migratetype and remove\n * the smallest available page from the freelists\n */\nstatic __always_inline\nstruct page *__rmqueue_smallest(struct zone *zone, unsigned int order,\n\t\t\t\t\t\tint migratetype)\n{\n\tunsigned int current_order;\n\tstruct free_area *area;\n\tstruct page *page;\n\n\t/* Find a page of the appropriate size in the preferred list */\n\tfor (current_order = order; current_order < MAX_ORDER; ++current_order) {\n\t\tarea = &(zone->free_area[current_order]);\n\t\tpage = get_page_from_free_area(area, migratetype);\n\t\tif (!page)\n\t\t\tcontinue;\n\t\tdel_page_from_free_list(page, zone, current_order);\n\t\texpand(zone, page, order, current_order, migratetype);\n\t\tset_pcppage_migratetype(page, migratetype);\n\t\treturn page;\n\t}\n\n\treturn NULL;\n}\n\n\n/*\n * This array describes the order lists are fallen back to when\n * the free lists for the desirable migrate type are depleted\n */\nstatic int fallbacks[MIGRATE_TYPES][3] = {\n\t[MIGRATE_UNMOVABLE]   = { MIGRATE_RECLAIMABLE, MIGRATE_MOVABLE,   MIGRATE_TYPES },\n\t[MIGRATE_MOVABLE]     = { MIGRATE_RECLAIMABLE, MIGRATE_UNMOVABLE, MIGRATE_TYPES },\n\t[MIGRATE_RECLAIMABLE] = { MIGRATE_UNMOVABLE,   MIGRATE_MOVABLE,   MIGRATE_TYPES },\n#ifdef CONFIG_CMA\n\t[MIGRATE_CMA]         = { MIGRATE_TYPES }, /* Never used */\n#endif\n#ifdef CONFIG_MEMORY_ISOLATION\n\t[MIGRATE_ISOLATE]     = { MIGRATE_TYPES }, /* Never used */\n#endif\n};\n\n#ifdef CONFIG_CMA\nstatic __always_inline struct page *__rmqueue_cma_fallback(struct zone *zone,\n\t\t\t\t\tunsigned int order)\n{\n\treturn __rmqueue_smallest(zone, order, MIGRATE_CMA);\n}\n#else\nstatic inline struct page *__rmqueue_cma_fallback(struct zone *zone,\n\t\t\t\t\tunsigned int order) { return NULL; }\n#endif\n\n/*\n * Move the free pages in a range to the freelist tail of the requested type.\n * Note that start_page and end_pages are not aligned on a pageblock\n * boundary. If alignment is required, use move_freepages_block()\n */\nstatic int move_freepages(struct zone *zone,\n\t\t\t  struct page *start_page, struct page *end_page,\n\t\t\t  int migratetype, int *num_movable)\n{\n\tstruct page *page;\n\tunsigned int order;\n\tint pages_moved = 0;\n\n\tfor (page = start_page; page <= end_page;) {\n\t\tif (!pfn_valid_within(page_to_pfn(page))) {\n\t\t\tpage++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!PageBuddy(page)) {\n\t\t\t/*\n\t\t\t * We assume that pages that could be isolated for\n\t\t\t * migration are movable. But we don't actually try\n\t\t\t * isolating, as that would be expensive.\n\t\t\t */\n\t\t\tif (num_movable &&\n\t\t\t\t\t(PageLRU(page) || __PageMovable(page)))\n\t\t\t\t(*num_movable)++;\n\n\t\t\tpage++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Make sure we are not inadvertently changing nodes */\n\t\tVM_BUG_ON_PAGE(page_to_nid(page) != zone_to_nid(zone), page);\n\t\tVM_BUG_ON_PAGE(page_zone(page) != zone, page);\n\n\t\torder = buddy_order(page);\n\t\tmove_to_free_list(page, zone, order, migratetype);\n\t\tpage += 1 << order;\n\t\tpages_moved += 1 << order;\n\t}\n\n\treturn pages_moved;\n}\n\nint move_freepages_block(struct zone *zone, struct page *page,\n\t\t\t\tint migratetype, int *num_movable)\n{\n\tunsigned long start_pfn, end_pfn;\n\tstruct page *start_page, *end_page;\n\n\tif (num_movable)\n\t\t*num_movable = 0;\n\n\tstart_pfn = page_to_pfn(page);\n\tstart_pfn = start_pfn & ~(pageblock_nr_pages-1);\n\tstart_page = pfn_to_page(start_pfn);\n\tend_page = start_page + pageblock_nr_pages - 1;\n\tend_pfn = start_pfn + pageblock_nr_pages - 1;\n\n\t/* Do not cross zone boundaries */\n\tif (!zone_spans_pfn(zone, start_pfn))\n\t\tstart_page = page;\n\tif (!zone_spans_pfn(zone, end_pfn))\n\t\treturn 0;\n\n\treturn move_freepages(zone, start_page, end_page, migratetype,\n\t\t\t\t\t\t\t\tnum_movable);\n}\n\nstatic void change_pageblock_range(struct page *pageblock_page,\n\t\t\t\t\tint start_order, int migratetype)\n{\n\tint nr_pageblocks = 1 << (start_order - pageblock_order);\n\n\twhile (nr_pageblocks--) {\n\t\tset_pageblock_migratetype(pageblock_page, migratetype);\n\t\tpageblock_page += pageblock_nr_pages;\n\t}\n}\n\n/*\n * When we are falling back to another migratetype during allocation, try to\n * steal extra free pages from the same pageblocks to satisfy further\n * allocations, instead of polluting multiple pageblocks.\n *\n * If we are stealing a relatively large buddy page, it is likely there will\n * be more free pages in the pageblock, so try to steal them all. For\n * reclaimable and unmovable allocations, we steal regardless of page size,\n * as fragmentation caused by those allocations polluting movable pageblocks\n * is worse than movable allocations stealing from unmovable and reclaimable\n * pageblocks.\n */\nstatic bool can_steal_fallback(unsigned int order, int start_mt)\n{\n\t/*\n\t * Leaving this order check is intended, although there is\n\t * relaxed order check in next check. The reason is that\n\t * we can actually steal whole pageblock if this condition met,\n\t * but, below check doesn't guarantee it and that is just heuristic\n\t * so could be changed anytime.\n\t */\n\tif (order >= pageblock_order)\n\t\treturn true;\n\n\tif (order >= pageblock_order / 2 ||\n\t\tstart_mt == MIGRATE_RECLAIMABLE ||\n\t\tstart_mt == MIGRATE_UNMOVABLE ||\n\t\tpage_group_by_mobility_disabled)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool boost_watermark(struct zone *zone)\n{\n\tunsigned long max_boost;\n\n\tif (!watermark_boost_factor)\n\t\treturn false;\n\t/*\n\t * Don't bother in zones that are unlikely to produce results.\n\t * On small machines, including kdump capture kernels running\n\t * in a small area, boosting the watermark can cause an out of\n\t * memory situation immediately.\n\t */\n\tif ((pageblock_nr_pages * 4) > zone_managed_pages(zone))\n\t\treturn false;\n\n\tmax_boost = mult_frac(zone->_watermark[WMARK_HIGH],\n\t\t\twatermark_boost_factor, 10000);\n\n\t/*\n\t * high watermark may be uninitialised if fragmentation occurs\n\t * very early in boot so do not boost. We do not fall\n\t * through and boost by pageblock_nr_pages as failing\n\t * allocations that early means that reclaim is not going\n\t * to help and it may even be impossible to reclaim the\n\t * boosted watermark resulting in a hang.\n\t */\n\tif (!max_boost)\n\t\treturn false;\n\n\tmax_boost = max(pageblock_nr_pages, max_boost);\n\n\tzone->watermark_boost = min(zone->watermark_boost + pageblock_nr_pages,\n\t\tmax_boost);\n\n\treturn true;\n}\n\n/*\n * This function implements actual steal behaviour. If order is large enough,\n * we can steal whole pageblock. If not, we first move freepages in this\n * pageblock to our migratetype and determine how many already-allocated pages\n * are there in the pageblock with a compatible migratetype. If at least half\n * of pages are free or compatible, we can change migratetype of the pageblock\n * itself, so pages freed in the future will be put on the correct free list.\n */\nstatic void steal_suitable_fallback(struct zone *zone, struct page *page,\n\t\tunsigned int alloc_flags, int start_type, bool whole_block)\n{\n\tunsigned int current_order = buddy_order(page);\n\tint free_pages, movable_pages, alike_pages;\n\tint old_block_type;\n\n\told_block_type = get_pageblock_migratetype(page);\n\n\t/*\n\t * This can happen due to races and we want to prevent broken\n\t * highatomic accounting.\n\t */\n\tif (is_migrate_highatomic(old_block_type))\n\t\tgoto single_page;\n\n\t/* Take ownership for orders >= pageblock_order */\n\tif (current_order >= pageblock_order) {\n\t\tchange_pageblock_range(page, current_order, start_type);\n\t\tgoto single_page;\n\t}\n\n\t/*\n\t * Boost watermarks to increase reclaim pressure to reduce the\n\t * likelihood of future fallbacks. Wake kswapd now as the node\n\t * may be balanced overall and kswapd will not wake naturally.\n\t */\n\tif (boost_watermark(zone) && (alloc_flags & ALLOC_KSWAPD))\n\t\tset_bit(ZONE_BOOSTED_WATERMARK, &zone->flags);\n\n\t/* We are not allowed to try stealing from the whole block */\n\tif (!whole_block)\n\t\tgoto single_page;\n\n\tfree_pages = move_freepages_block(zone, page, start_type,\n\t\t\t\t\t\t&movable_pages);\n\t/*\n\t * Determine how many pages are compatible with our allocation.\n\t * For movable allocation, it's the number of movable pages which\n\t * we just obtained. For other types it's a bit more tricky.\n\t */\n\tif (start_type == MIGRATE_MOVABLE) {\n\t\talike_pages = movable_pages;\n\t} else {\n\t\t/*\n\t\t * If we are falling back a RECLAIMABLE or UNMOVABLE allocation\n\t\t * to MOVABLE pageblock, consider all non-movable pages as\n\t\t * compatible. If it's UNMOVABLE falling back to RECLAIMABLE or\n\t\t * vice versa, be conservative since we can't distinguish the\n\t\t * exact migratetype of non-movable pages.\n\t\t */\n\t\tif (old_block_type == MIGRATE_MOVABLE)\n\t\t\talike_pages = pageblock_nr_pages\n\t\t\t\t\t\t- (free_pages + movable_pages);\n\t\telse\n\t\t\talike_pages = 0;\n\t}\n\n\t/* moving whole block can fail due to zone boundary conditions */\n\tif (!free_pages)\n\t\tgoto single_page;\n\n\t/*\n\t * If a sufficient number of pages in the block are either free or of\n\t * comparable migratability as our allocation, claim the whole block.\n\t */\n\tif (free_pages + alike_pages >= (1 << (pageblock_order-1)) ||\n\t\t\tpage_group_by_mobility_disabled)\n\t\tset_pageblock_migratetype(page, start_type);\n\n\treturn;\n\nsingle_page:\n\tmove_to_free_list(page, zone, current_order, start_type);\n}\n\n/*\n * Check whether there is a suitable fallback freepage with requested order.\n * If only_stealable is true, this function returns fallback_mt only if\n * we can steal other freepages all together. This would help to reduce\n * fragmentation due to mixed migratetype pages in one pageblock.\n */\nint find_suitable_fallback(struct free_area *area, unsigned int order,\n\t\t\tint migratetype, bool only_stealable, bool *can_steal)\n{\n\tint i;\n\tint fallback_mt;\n\n\tif (area->nr_free == 0)\n\t\treturn -1;\n\n\t*can_steal = false;\n\tfor (i = 0;; i++) {\n\t\tfallback_mt = fallbacks[migratetype][i];\n\t\tif (fallback_mt == MIGRATE_TYPES)\n\t\t\tbreak;\n\n\t\tif (free_area_empty(area, fallback_mt))\n\t\t\tcontinue;\n\n\t\tif (can_steal_fallback(order, migratetype))\n\t\t\t*can_steal = true;\n\n\t\tif (!only_stealable)\n\t\t\treturn fallback_mt;\n\n\t\tif (*can_steal)\n\t\t\treturn fallback_mt;\n\t}\n\n\treturn -1;\n}\n\n/*\n * Reserve a pageblock for exclusive use of high-order atomic allocations if\n * there are no empty page blocks that contain a page with a suitable order\n */\nstatic void reserve_highatomic_pageblock(struct page *page, struct zone *zone,\n\t\t\t\tunsigned int alloc_order)\n{\n\tint mt;\n\tunsigned long max_managed, flags;\n\n\t/*\n\t * Limit the number reserved to 1 pageblock or roughly 1% of a zone.\n\t * Check is race-prone but harmless.\n\t */\n\tmax_managed = (zone_managed_pages(zone) / 100) + pageblock_nr_pages;\n\tif (zone->nr_reserved_highatomic >= max_managed)\n\t\treturn;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\n\t/* Recheck the nr_reserved_highatomic limit under the lock */\n\tif (zone->nr_reserved_highatomic >= max_managed)\n\t\tgoto out_unlock;\n\n\t/* Yoink! */\n\tmt = get_pageblock_migratetype(page);\n\tif (!is_migrate_highatomic(mt) && !is_migrate_isolate(mt)\n\t    && !is_migrate_cma(mt)) {\n\t\tzone->nr_reserved_highatomic += pageblock_nr_pages;\n\t\tset_pageblock_migratetype(page, MIGRATE_HIGHATOMIC);\n\t\tmove_freepages_block(zone, page, MIGRATE_HIGHATOMIC, NULL);\n\t}\n\nout_unlock:\n\tspin_unlock_irqrestore(&zone->lock, flags);\n}\n\n/*\n * Used when an allocation is about to fail under memory pressure. This\n * potentially hurts the reliability of high-order allocations when under\n * intense memory pressure but failed atomic allocations should be easier\n * to recover from than an OOM.\n *\n * If @force is true, try to unreserve a pageblock even though highatomic\n * pageblock is exhausted.\n */\nstatic bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n\t\t\t\t\t\tbool force)\n{\n\tstruct zonelist *zonelist = ac->zonelist;\n\tunsigned long flags;\n\tstruct zoneref *z;\n\tstruct zone *zone;\n\tstruct page *page;\n\tint order;\n\tbool ret;\n\n\tfor_each_zone_zonelist_nodemask(zone, z, zonelist, ac->highest_zoneidx,\n\t\t\t\t\t\t\t\tac->nodemask) {\n\t\t/*\n\t\t * Preserve at least one pageblock unless memory pressure\n\t\t * is really high.\n\t\t */\n\t\tif (!force && zone->nr_reserved_highatomic <=\n\t\t\t\t\tpageblock_nr_pages)\n\t\t\tcontinue;\n\n\t\tspin_lock_irqsave(&zone->lock, flags);\n\t\tfor (order = 0; order < MAX_ORDER; order++) {\n\t\t\tstruct free_area *area = &(zone->free_area[order]);\n\n\t\t\tpage = get_page_from_free_area(area, MIGRATE_HIGHATOMIC);\n\t\t\tif (!page)\n\t\t\t\tcontinue;\n\n\t\t\t/*\n\t\t\t * In page freeing path, migratetype change is racy so\n\t\t\t * we can counter several free pages in a pageblock\n\t\t\t * in this loop althoug we changed the pageblock type\n\t\t\t * from highatomic to ac->migratetype. So we should\n\t\t\t * adjust the count once.\n\t\t\t */\n\t\t\tif (is_migrate_highatomic_page(page)) {\n\t\t\t\t/*\n\t\t\t\t * It should never happen but changes to\n\t\t\t\t * locking could inadvertently allow a per-cpu\n\t\t\t\t * drain to add pages to MIGRATE_HIGHATOMIC\n\t\t\t\t * while unreserving so be safe and watch for\n\t\t\t\t * underflows.\n\t\t\t\t */\n\t\t\t\tzone->nr_reserved_highatomic -= min(\n\t\t\t\t\t\tpageblock_nr_pages,\n\t\t\t\t\t\tzone->nr_reserved_highatomic);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Convert to ac->migratetype and avoid the normal\n\t\t\t * pageblock stealing heuristics. Minimally, the caller\n\t\t\t * is doing the work and needs the pages. More\n\t\t\t * importantly, if the block was always converted to\n\t\t\t * MIGRATE_UNMOVABLE or another type then the number\n\t\t\t * of pageblocks that cannot be completely freed\n\t\t\t * may increase.\n\t\t\t */\n\t\t\tset_pageblock_migratetype(page, ac->migratetype);\n\t\t\tret = move_freepages_block(zone, page, ac->migratetype,\n\t\t\t\t\t\t\t\t\tNULL);\n\t\t\tif (ret) {\n\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t}\n\n\treturn false;\n}\n\n/*\n * Try finding a free buddy page on the fallback list and put it on the free\n * list of requested migratetype, possibly along with other pages from the same\n * block, depending on fragmentation avoidance heuristics. Returns true if\n * fallback was found so that __rmqueue_smallest() can grab it.\n *\n * The use of signed ints for order and current_order is a deliberate\n * deviation from the rest of this file, to make the for loop\n * condition simpler.\n */\nstatic __always_inline bool\n__rmqueue_fallback(struct zone *zone, int order, int start_migratetype,\n\t\t\t\t\t\tunsigned int alloc_flags)\n{\n\tstruct free_area *area;\n\tint current_order;\n\tint min_order = order;\n\tstruct page *page;\n\tint fallback_mt;\n\tbool can_steal;\n\n\t/*\n\t * Do not steal pages from freelists belonging to other pageblocks\n\t * i.e. orders < pageblock_order. If there are no local zones free,\n\t * the zonelists will be reiterated without ALLOC_NOFRAGMENT.\n\t */\n\tif (alloc_flags & ALLOC_NOFRAGMENT)\n\t\tmin_order = pageblock_order;\n\n\t/*\n\t * Find the largest available free page in the other list. This roughly\n\t * approximates finding the pageblock with the most free pages, which\n\t * would be too costly to do exactly.\n\t */\n\tfor (current_order = MAX_ORDER - 1; current_order >= min_order;\n\t\t\t\t--current_order) {\n\t\tarea = &(zone->free_area[current_order]);\n\t\tfallback_mt = find_suitable_fallback(area, current_order,\n\t\t\t\tstart_migratetype, false, &can_steal);\n\t\tif (fallback_mt == -1)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * We cannot steal all free pages from the pageblock and the\n\t\t * requested migratetype is movable. In that case it's better to\n\t\t * steal and split the smallest available page instead of the\n\t\t * largest available page, because even if the next movable\n\t\t * allocation falls back into a different pageblock than this\n\t\t * one, it won't cause permanent fragmentation.\n\t\t */\n\t\tif (!can_steal && start_migratetype == MIGRATE_MOVABLE\n\t\t\t\t\t&& current_order > order)\n\t\t\tgoto find_smallest;\n\n\t\tgoto do_steal;\n\t}\n\n\treturn false;\n\nfind_smallest:\n\tfor (current_order = order; current_order < MAX_ORDER;\n\t\t\t\t\t\t\tcurrent_order++) {\n\t\tarea = &(zone->free_area[current_order]);\n\t\tfallback_mt = find_suitable_fallback(area, current_order,\n\t\t\t\tstart_migratetype, false, &can_steal);\n\t\tif (fallback_mt != -1)\n\t\t\tbreak;\n\t}\n\n\t/*\n\t * This should not happen - we already found a suitable fallback\n\t * when looking for the largest page.\n\t */\n\tVM_BUG_ON(current_order == MAX_ORDER);\n\ndo_steal:\n\tpage = get_page_from_free_area(area, fallback_mt);\n\n\tsteal_suitable_fallback(zone, page, alloc_flags, start_migratetype,\n\t\t\t\t\t\t\t\tcan_steal);\n\n\ttrace_mm_page_alloc_extfrag(page, order, current_order,\n\t\tstart_migratetype, fallback_mt);\n\n\treturn true;\n\n}\n\n/*\n * Do the hard work of removing an element from the buddy allocator.\n * Call me with the zone->lock already held.\n */\nstatic __always_inline struct page *\n__rmqueue(struct zone *zone, unsigned int order, int migratetype,\n\t\t\t\t\t\tunsigned int alloc_flags)\n{\n\tstruct page *page;\n\n\tif (IS_ENABLED(CONFIG_CMA)) {\n\t\t/*\n\t\t * Balance movable allocations between regular and CMA areas by\n\t\t * allocating from CMA when over half of the zone's free memory\n\t\t * is in the CMA area.\n\t\t */\n\t\tif (alloc_flags & ALLOC_CMA &&\n\t\t    zone_page_state(zone, NR_FREE_CMA_PAGES) >\n\t\t    zone_page_state(zone, NR_FREE_PAGES) / 2) {\n\t\t\tpage = __rmqueue_cma_fallback(zone, order);\n\t\t\tif (page)\n\t\t\t\tgoto out;\n\t\t}\n\t}\nretry:\n\tpage = __rmqueue_smallest(zone, order, migratetype);\n\tif (unlikely(!page)) {\n\t\tif (alloc_flags & ALLOC_CMA)\n\t\t\tpage = __rmqueue_cma_fallback(zone, order);\n\n\t\tif (!page && __rmqueue_fallback(zone, order, migratetype,\n\t\t\t\t\t\t\t\talloc_flags))\n\t\t\tgoto retry;\n\t}\nout:\n\tif (page)\n\t\ttrace_mm_page_alloc_zone_locked(page, order, migratetype);\n\treturn page;\n}\n\n/*\n * Obtain a specified number of elements from the buddy allocator, all under\n * a single hold of the lock, for efficiency.  Add them to the supplied list.\n * Returns the number of new pages which were placed at *list.\n */\nstatic int rmqueue_bulk(struct zone *zone, unsigned int order,\n\t\t\tunsigned long count, struct list_head *list,\n\t\t\tint migratetype, unsigned int alloc_flags)\n{\n\tint i, alloced = 0;\n\n\tspin_lock(&zone->lock);\n\tfor (i = 0; i < count; ++i) {\n\t\tstruct page *page = __rmqueue(zone, order, migratetype,\n\t\t\t\t\t\t\t\talloc_flags);\n\t\tif (unlikely(page == NULL))\n\t\t\tbreak;\n\n\t\tif (unlikely(check_pcp_refill(page)))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Split buddy pages returned by expand() are received here in\n\t\t * physical page order. The page is added to the tail of\n\t\t * caller's list. From the callers perspective, the linked list\n\t\t * is ordered by page number under some conditions. This is\n\t\t * useful for IO devices that can forward direction from the\n\t\t * head, thus also in the physical page order. This is useful\n\t\t * for IO devices that can merge IO requests if the physical\n\t\t * pages are ordered properly.\n\t\t */\n\t\tlist_add_tail(&page->lru, list);\n\t\talloced++;\n\t\tif (is_migrate_cma(get_pcppage_migratetype(page)))\n\t\t\t__mod_zone_page_state(zone, NR_FREE_CMA_PAGES,\n\t\t\t\t\t      -(1 << order));\n\t}\n\n\t/*\n\t * i pages were removed from the buddy list even if some leak due\n\t * to check_pcp_refill failing so adjust NR_FREE_PAGES based\n\t * on i. Do not confuse with 'alloced' which is the number of\n\t * pages added to the pcp list.\n\t */\n\t__mod_zone_page_state(zone, NR_FREE_PAGES, -(i << order));\n\tspin_unlock(&zone->lock);\n\treturn alloced;\n}\n\n#ifdef CONFIG_NUMA\n/*\n * Called from the vmstat counter updater to drain pagesets of this\n * currently executing processor on remote nodes after they have\n * expired.\n *\n * Note that this function must be called with the thread pinned to\n * a single processor.\n */\nvoid drain_zone_pages(struct zone *zone, struct per_cpu_pages *pcp)\n{\n\tunsigned long flags;\n\tint to_drain, batch;\n\n\tlocal_irq_save(flags);\n\tbatch = READ_ONCE(pcp->batch);\n\tto_drain = min(pcp->count, batch);\n\tif (to_drain > 0)\n\t\tfree_pcppages_bulk(zone, to_drain, pcp);\n\tlocal_irq_restore(flags);\n}\n#endif\n\n/*\n * Drain pcplists of the indicated processor and zone.\n *\n * The processor must either be the current processor and the\n * thread pinned to the current processor or a processor that\n * is not online.\n */\nstatic void drain_pages_zone(unsigned int cpu, struct zone *zone)\n{\n\tunsigned long flags;\n\tstruct per_cpu_pageset *pset;\n\tstruct per_cpu_pages *pcp;\n\n\tlocal_irq_save(flags);\n\tpset = per_cpu_ptr(zone->pageset, cpu);\n\n\tpcp = &pset->pcp;\n\tif (pcp->count)\n\t\tfree_pcppages_bulk(zone, pcp->count, pcp);\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Drain pcplists of all zones on the indicated processor.\n *\n * The processor must either be the current processor and the\n * thread pinned to the current processor or a processor that\n * is not online.\n */\nstatic void drain_pages(unsigned int cpu)\n{\n\tstruct zone *zone;\n\n\tfor_each_populated_zone(zone) {\n\t\tdrain_pages_zone(cpu, zone);\n\t}\n}\n\n/*\n * Spill all of this CPU's per-cpu pages back into the buddy allocator.\n *\n * The CPU has to be pinned. When zone parameter is non-NULL, spill just\n * the single zone's pages.\n */\nvoid drain_local_pages(struct zone *zone)\n{\n\tint cpu = smp_processor_id();\n\n\tif (zone)\n\t\tdrain_pages_zone(cpu, zone);\n\telse\n\t\tdrain_pages(cpu);\n}\n\nstatic void drain_local_pages_wq(struct work_struct *work)\n{\n\tstruct pcpu_drain *drain;\n\n\tdrain = container_of(work, struct pcpu_drain, work);\n\n\t/*\n\t * drain_all_pages doesn't use proper cpu hotplug protection so\n\t * we can race with cpu offline when the WQ can move this from\n\t * a cpu pinned worker to an unbound one. We can operate on a different\n\t * cpu which is allright but we also have to make sure to not move to\n\t * a different one.\n\t */\n\tpreempt_disable();\n\tdrain_local_pages(drain->zone);\n\tpreempt_enable();\n}\n\n/*\n * The implementation of drain_all_pages(), exposing an extra parameter to\n * drain on all cpus.\n *\n * drain_all_pages() is optimized to only execute on cpus where pcplists are\n * not empty. The check for non-emptiness can however race with a free to\n * pcplist that has not yet increased the pcp->count from 0 to 1. Callers\n * that need the guarantee that every CPU has drained can disable the\n * optimizing racy check.\n */\nstatic void __drain_all_pages(struct zone *zone, bool force_all_cpus)\n{\n\tint cpu;\n\n\t/*\n\t * Allocate in the BSS so we wont require allocation in\n\t * direct reclaim path for CONFIG_CPUMASK_OFFSTACK=y\n\t */\n\tstatic cpumask_t cpus_with_pcps;\n\n\t/*\n\t * Make sure nobody triggers this path before mm_percpu_wq is fully\n\t * initialized.\n\t */\n\tif (WARN_ON_ONCE(!mm_percpu_wq))\n\t\treturn;\n\n\t/*\n\t * Do not drain if one is already in progress unless it's specific to\n\t * a zone. Such callers are primarily CMA and memory hotplug and need\n\t * the drain to be complete when the call returns.\n\t */\n\tif (unlikely(!mutex_trylock(&pcpu_drain_mutex))) {\n\t\tif (!zone)\n\t\t\treturn;\n\t\tmutex_lock(&pcpu_drain_mutex);\n\t}\n\n\t/*\n\t * We don't care about racing with CPU hotplug event\n\t * as offline notification will cause the notified\n\t * cpu to drain that CPU pcps and on_each_cpu_mask\n\t * disables preemption as part of its processing\n\t */\n\tfor_each_online_cpu(cpu) {\n\t\tstruct per_cpu_pageset *pcp;\n\t\tstruct zone *z;\n\t\tbool has_pcps = false;\n\n\t\tif (force_all_cpus) {\n\t\t\t/*\n\t\t\t * The pcp.count check is racy, some callers need a\n\t\t\t * guarantee that no cpu is missed.\n\t\t\t */\n\t\t\thas_pcps = true;\n\t\t} else if (zone) {\n\t\t\tpcp = per_cpu_ptr(zone->pageset, cpu);\n\t\t\tif (pcp->pcp.count)\n\t\t\t\thas_pcps = true;\n\t\t} else {\n\t\t\tfor_each_populated_zone(z) {\n\t\t\t\tpcp = per_cpu_ptr(z->pageset, cpu);\n\t\t\t\tif (pcp->pcp.count) {\n\t\t\t\t\thas_pcps = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (has_pcps)\n\t\t\tcpumask_set_cpu(cpu, &cpus_with_pcps);\n\t\telse\n\t\t\tcpumask_clear_cpu(cpu, &cpus_with_pcps);\n\t}\n\n\tfor_each_cpu(cpu, &cpus_with_pcps) {\n\t\tstruct pcpu_drain *drain = per_cpu_ptr(&pcpu_drain, cpu);\n\n\t\tdrain->zone = zone;\n\t\tINIT_WORK(&drain->work, drain_local_pages_wq);\n\t\tqueue_work_on(cpu, mm_percpu_wq, &drain->work);\n\t}\n\tfor_each_cpu(cpu, &cpus_with_pcps)\n\t\tflush_work(&per_cpu_ptr(&pcpu_drain, cpu)->work);\n\n\tmutex_unlock(&pcpu_drain_mutex);\n}\n\n/*\n * Spill all the per-cpu pages from all CPUs back into the buddy allocator.\n *\n * When zone parameter is non-NULL, spill just the single zone's pages.\n *\n * Note that this can be extremely slow as the draining happens in a workqueue.\n */\nvoid drain_all_pages(struct zone *zone)\n{\n\t__drain_all_pages(zone, false);\n}\n\n#ifdef CONFIG_HIBERNATION\n\n/*\n * Touch the watchdog for every WD_PAGE_COUNT pages.\n */\n#define WD_PAGE_COUNT\t(128*1024)\n\nvoid mark_free_pages(struct zone *zone)\n{\n\tunsigned long pfn, max_zone_pfn, page_count = WD_PAGE_COUNT;\n\tunsigned long flags;\n\tunsigned int order, t;\n\tstruct page *page;\n\n\tif (zone_is_empty(zone))\n\t\treturn;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\n\tmax_zone_pfn = zone_end_pfn(zone);\n\tfor (pfn = zone->zone_start_pfn; pfn < max_zone_pfn; pfn++)\n\t\tif (pfn_valid(pfn)) {\n\t\t\tpage = pfn_to_page(pfn);\n\n\t\t\tif (!--page_count) {\n\t\t\t\ttouch_nmi_watchdog();\n\t\t\t\tpage_count = WD_PAGE_COUNT;\n\t\t\t}\n\n\t\t\tif (page_zone(page) != zone)\n\t\t\t\tcontinue;\n\n\t\t\tif (!swsusp_page_is_forbidden(page))\n\t\t\t\tswsusp_unset_page_free(page);\n\t\t}\n\n\tfor_each_migratetype_order(order, t) {\n\t\tlist_for_each_entry(page,\n\t\t\t\t&zone->free_area[order].free_list[t], lru) {\n\t\t\tunsigned long i;\n\n\t\t\tpfn = page_to_pfn(page);\n\t\t\tfor (i = 0; i < (1UL << order); i++) {\n\t\t\t\tif (!--page_count) {\n\t\t\t\t\ttouch_nmi_watchdog();\n\t\t\t\t\tpage_count = WD_PAGE_COUNT;\n\t\t\t\t}\n\t\t\t\tswsusp_set_page_free(pfn_to_page(pfn + i));\n\t\t\t}\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&zone->lock, flags);\n}\n#endif /* CONFIG_PM */\n\nstatic bool free_unref_page_prepare(struct page *page, unsigned long pfn)\n{\n\tint migratetype;\n\n\tif (!free_pcp_prepare(page))\n\t\treturn false;\n\n\tmigratetype = get_pfnblock_migratetype(page, pfn);\n\tset_pcppage_migratetype(page, migratetype);\n\treturn true;\n}\n\nstatic void free_unref_page_commit(struct page *page, unsigned long pfn)\n{\n\tstruct zone *zone = page_zone(page);\n\tstruct per_cpu_pages *pcp;\n\tint migratetype;\n\n\tmigratetype = get_pcppage_migratetype(page);\n\t__count_vm_event(PGFREE);\n\n\t/*\n\t * We only track unmovable, reclaimable and movable on pcp lists.\n\t * Free ISOLATE pages back to the allocator because they are being\n\t * offlined but treat HIGHATOMIC as movable pages so we can get those\n\t * areas back if necessary. Otherwise, we may have to free\n\t * excessively into the page allocator\n\t */\n\tif (migratetype >= MIGRATE_PCPTYPES) {\n\t\tif (unlikely(is_migrate_isolate(migratetype))) {\n\t\t\tfree_one_page(zone, page, pfn, 0, migratetype,\n\t\t\t\t      FPI_NONE);\n\t\t\treturn;\n\t\t}\n\t\tmigratetype = MIGRATE_MOVABLE;\n\t}\n\n\tpcp = &this_cpu_ptr(zone->pageset)->pcp;\n\tlist_add(&page->lru, &pcp->lists[migratetype]);\n\tpcp->count++;\n\tif (pcp->count >= READ_ONCE(pcp->high))\n\t\tfree_pcppages_bulk(zone, READ_ONCE(pcp->batch), pcp);\n}\n\n/*\n * Free a 0-order page\n */\nvoid free_unref_page(struct page *page)\n{\n\tunsigned long flags;\n\tunsigned long pfn = page_to_pfn(page);\n\n\tif (!free_unref_page_prepare(page, pfn))\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tfree_unref_page_commit(page, pfn);\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Free a list of 0-order pages\n */\nvoid free_unref_page_list(struct list_head *list)\n{\n\tstruct page *page, *next;\n\tunsigned long flags, pfn;\n\tint batch_count = 0;\n\n\t/* Prepare pages for freeing */\n\tlist_for_each_entry_safe(page, next, list, lru) {\n\t\tpfn = page_to_pfn(page);\n\t\tif (!free_unref_page_prepare(page, pfn))\n\t\t\tlist_del(&page->lru);\n\t\tset_page_private(page, pfn);\n\t}\n\n\tlocal_irq_save(flags);\n\tlist_for_each_entry_safe(page, next, list, lru) {\n\t\tunsigned long pfn = page_private(page);\n\n\t\tset_page_private(page, 0);\n\t\ttrace_mm_page_free_batched(page);\n\t\tfree_unref_page_commit(page, pfn);\n\n\t\t/*\n\t\t * Guard against excessive IRQ disabled times when we get\n\t\t * a large list of pages to free.\n\t\t */\n\t\tif (++batch_count == SWAP_CLUSTER_MAX) {\n\t\t\tlocal_irq_restore(flags);\n\t\t\tbatch_count = 0;\n\t\t\tlocal_irq_save(flags);\n\t\t}\n\t}\n\tlocal_irq_restore(flags);\n}\n\n/*\n * split_page takes a non-compound higher-order page, and splits it into\n * n (1<<order) sub-pages: page[0..n]\n * Each sub-page must be freed individually.\n *\n * Note: this is probably too low level an operation for use in drivers.\n * Please consult with lkml before using this in your driver.\n */\nvoid split_page(struct page *page, unsigned int order)\n{\n\tint i;\n\n\tVM_BUG_ON_PAGE(PageCompound(page), page);\n\tVM_BUG_ON_PAGE(!page_count(page), page);\n\n\tfor (i = 1; i < (1 << order); i++)\n\t\tset_page_refcounted(page + i);\n\tsplit_page_owner(page, 1 << order);\n\tsplit_page_memcg(page, 1 << order);\n}\nEXPORT_SYMBOL_GPL(split_page);\n\nint __isolate_free_page(struct page *page, unsigned int order)\n{\n\tunsigned long watermark;\n\tstruct zone *zone;\n\tint mt;\n\n\tBUG_ON(!PageBuddy(page));\n\n\tzone = page_zone(page);\n\tmt = get_pageblock_migratetype(page);\n\n\tif (!is_migrate_isolate(mt)) {\n\t\t/*\n\t\t * Obey watermarks as if the page was being allocated. We can\n\t\t * emulate a high-order watermark check with a raised order-0\n\t\t * watermark, because we already know our high-order page\n\t\t * exists.\n\t\t */\n\t\twatermark = zone->_watermark[WMARK_MIN] + (1UL << order);\n\t\tif (!zone_watermark_ok(zone, 0, watermark, 0, ALLOC_CMA))\n\t\t\treturn 0;\n\n\t\t__mod_zone_freepage_state(zone, -(1UL << order), mt);\n\t}\n\n\t/* Remove page from free list */\n\n\tdel_page_from_free_list(page, zone, order);\n\n\t/*\n\t * Set the pageblock if the isolated page is at least half of a\n\t * pageblock\n\t */\n\tif (order >= pageblock_order - 1) {\n\t\tstruct page *endpage = page + (1 << order) - 1;\n\t\tfor (; page < endpage; page += pageblock_nr_pages) {\n\t\t\tint mt = get_pageblock_migratetype(page);\n\t\t\tif (!is_migrate_isolate(mt) && !is_migrate_cma(mt)\n\t\t\t    && !is_migrate_highatomic(mt))\n\t\t\t\tset_pageblock_migratetype(page,\n\t\t\t\t\t\t\t  MIGRATE_MOVABLE);\n\t\t}\n\t}\n\n\n\treturn 1UL << order;\n}\n\n/**\n * __putback_isolated_page - Return a now-isolated page back where we got it\n * @page: Page that was isolated\n * @order: Order of the isolated page\n * @mt: The page's pageblock's migratetype\n *\n * This function is meant to return a page pulled from the free lists via\n * __isolate_free_page back to the free lists they were pulled from.\n */\nvoid __putback_isolated_page(struct page *page, unsigned int order, int mt)\n{\n\tstruct zone *zone = page_zone(page);\n\n\t/* zone lock should be held when this function is called */\n\tlockdep_assert_held(&zone->lock);\n\n\t/* Return isolated page to tail of freelist. */\n\t__free_one_page(page, page_to_pfn(page), zone, order, mt,\n\t\t\tFPI_SKIP_REPORT_NOTIFY | FPI_TO_TAIL);\n}\n\n/*\n * Update NUMA hit/miss statistics\n *\n * Must be called with interrupts disabled.\n */\nstatic inline void zone_statistics(struct zone *preferred_zone, struct zone *z)\n{\n#ifdef CONFIG_NUMA\n\tenum numa_stat_item local_stat = NUMA_LOCAL;\n\n\t/* skip numa counters update if numa stats is disabled */\n\tif (!static_branch_likely(&vm_numa_stat_key))\n\t\treturn;\n\n\tif (zone_to_nid(z) != numa_node_id())\n\t\tlocal_stat = NUMA_OTHER;\n\n\tif (zone_to_nid(z) == zone_to_nid(preferred_zone))\n\t\t__inc_numa_state(z, NUMA_HIT);\n\telse {\n\t\t__inc_numa_state(z, NUMA_MISS);\n\t\t__inc_numa_state(preferred_zone, NUMA_FOREIGN);\n\t}\n\t__inc_numa_state(z, local_stat);\n#endif\n}\n\n/* Remove page from the per-cpu list, caller must protect the list */\nstatic struct page *__rmqueue_pcplist(struct zone *zone, int migratetype,\n\t\t\tunsigned int alloc_flags,\n\t\t\tstruct per_cpu_pages *pcp,\n\t\t\tstruct list_head *list)\n{\n\tstruct page *page;\n\n\tdo {\n\t\tif (list_empty(list)) {\n\t\t\tpcp->count += rmqueue_bulk(zone, 0,\n\t\t\t\t\tREAD_ONCE(pcp->batch), list,\n\t\t\t\t\tmigratetype, alloc_flags);\n\t\t\tif (unlikely(list_empty(list)))\n\t\t\t\treturn NULL;\n\t\t}\n\n\t\tpage = list_first_entry(list, struct page, lru);\n\t\tlist_del(&page->lru);\n\t\tpcp->count--;\n\t} while (check_new_pcp(page));\n\n\treturn page;\n}\n\n/* Lock and remove page from the per-cpu list */\nstatic struct page *rmqueue_pcplist(struct zone *preferred_zone,\n\t\t\tstruct zone *zone, gfp_t gfp_flags,\n\t\t\tint migratetype, unsigned int alloc_flags)\n{\n\tstruct per_cpu_pages *pcp;\n\tstruct list_head *list;\n\tstruct page *page;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpcp = &this_cpu_ptr(zone->pageset)->pcp;\n\tlist = &pcp->lists[migratetype];\n\tpage = __rmqueue_pcplist(zone,  migratetype, alloc_flags, pcp, list);\n\tif (page) {\n\t\t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1);\n\t\tzone_statistics(preferred_zone, zone);\n\t}\n\tlocal_irq_restore(flags);\n\treturn page;\n}\n\n/*\n * Allocate a page from the given zone. Use pcplists for order-0 allocations.\n */\nstatic inline\nstruct page *rmqueue(struct zone *preferred_zone,\n\t\t\tstruct zone *zone, unsigned int order,\n\t\t\tgfp_t gfp_flags, unsigned int alloc_flags,\n\t\t\tint migratetype)\n{\n\tunsigned long flags;\n\tstruct page *page;\n\n\tif (likely(order == 0)) {\n\t\t/*\n\t\t * MIGRATE_MOVABLE pcplist could have the pages on CMA area and\n\t\t * we need to skip it when CMA area isn't allowed.\n\t\t */\n\t\tif (!IS_ENABLED(CONFIG_CMA) || alloc_flags & ALLOC_CMA ||\n\t\t\t\tmigratetype != MIGRATE_MOVABLE) {\n\t\t\tpage = rmqueue_pcplist(preferred_zone, zone, gfp_flags,\n\t\t\t\t\tmigratetype, alloc_flags);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/*\n\t * We most definitely don't want callers attempting to\n\t * allocate greater than order-1 page units with __GFP_NOFAIL.\n\t */\n\tWARN_ON_ONCE((gfp_flags & __GFP_NOFAIL) && (order > 1));\n\tspin_lock_irqsave(&zone->lock, flags);\n\n\tdo {\n\t\tpage = NULL;\n\t\t/*\n\t\t * order-0 request can reach here when the pcplist is skipped\n\t\t * due to non-CMA allocation context. HIGHATOMIC area is\n\t\t * reserved for high-order atomic allocation, so order-0\n\t\t * request should skip it.\n\t\t */\n\t\tif (order > 0 && alloc_flags & ALLOC_HARDER) {\n\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n\t\t\tif (page)\n\t\t\t\ttrace_mm_page_alloc_zone_locked(page, order, migratetype);\n\t\t}\n\t\tif (!page)\n\t\t\tpage = __rmqueue(zone, order, migratetype, alloc_flags);\n\t} while (page && check_new_pages(page, order));\n\tspin_unlock(&zone->lock);\n\tif (!page)\n\t\tgoto failed;\n\t__mod_zone_freepage_state(zone, -(1 << order),\n\t\t\t\t  get_pcppage_migratetype(page));\n\n\t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n\tzone_statistics(preferred_zone, zone);\n\tlocal_irq_restore(flags);\n\nout:\n\t/* Separate test+clear to avoid unnecessary atomics */\n\tif (test_bit(ZONE_BOOSTED_WATERMARK, &zone->flags)) {\n\t\tclear_bit(ZONE_BOOSTED_WATERMARK, &zone->flags);\n\t\twakeup_kswapd(zone, 0, 0, zone_idx(zone));\n\t}\n\n\tVM_BUG_ON_PAGE(page && bad_range(zone, page), page);\n\treturn page;\n\nfailed:\n\tlocal_irq_restore(flags);\n\treturn NULL;\n}\n\n#ifdef CONFIG_FAIL_PAGE_ALLOC\n\nstatic struct {\n\tstruct fault_attr attr;\n\n\tbool ignore_gfp_highmem;\n\tbool ignore_gfp_reclaim;\n\tu32 min_order;\n} fail_page_alloc = {\n\t.attr = FAULT_ATTR_INITIALIZER,\n\t.ignore_gfp_reclaim = true,\n\t.ignore_gfp_highmem = true,\n\t.min_order = 1,\n};\n\nstatic int __init setup_fail_page_alloc(char *str)\n{\n\treturn setup_fault_attr(&fail_page_alloc.attr, str);\n}\n__setup(\"fail_page_alloc=\", setup_fail_page_alloc);\n\nstatic bool __should_fail_alloc_page(gfp_t gfp_mask, unsigned int order)\n{\n\tif (order < fail_page_alloc.min_order)\n\t\treturn false;\n\tif (gfp_mask & __GFP_NOFAIL)\n\t\treturn false;\n\tif (fail_page_alloc.ignore_gfp_highmem && (gfp_mask & __GFP_HIGHMEM))\n\t\treturn false;\n\tif (fail_page_alloc.ignore_gfp_reclaim &&\n\t\t\t(gfp_mask & __GFP_DIRECT_RECLAIM))\n\t\treturn false;\n\n\treturn should_fail(&fail_page_alloc.attr, 1 << order);\n}\n\n#ifdef CONFIG_FAULT_INJECTION_DEBUG_FS\n\nstatic int __init fail_page_alloc_debugfs(void)\n{\n\tumode_t mode = S_IFREG | 0600;\n\tstruct dentry *dir;\n\n\tdir = fault_create_debugfs_attr(\"fail_page_alloc\", NULL,\n\t\t\t\t\t&fail_page_alloc.attr);\n\n\tdebugfs_create_bool(\"ignore-gfp-wait\", mode, dir,\n\t\t\t    &fail_page_alloc.ignore_gfp_reclaim);\n\tdebugfs_create_bool(\"ignore-gfp-highmem\", mode, dir,\n\t\t\t    &fail_page_alloc.ignore_gfp_highmem);\n\tdebugfs_create_u32(\"min-order\", mode, dir, &fail_page_alloc.min_order);\n\n\treturn 0;\n}\n\nlate_initcall(fail_page_alloc_debugfs);\n\n#endif /* CONFIG_FAULT_INJECTION_DEBUG_FS */\n\n#else /* CONFIG_FAIL_PAGE_ALLOC */\n\nstatic inline bool __should_fail_alloc_page(gfp_t gfp_mask, unsigned int order)\n{\n\treturn false;\n}\n\n#endif /* CONFIG_FAIL_PAGE_ALLOC */\n\nnoinline bool should_fail_alloc_page(gfp_t gfp_mask, unsigned int order)\n{\n\treturn __should_fail_alloc_page(gfp_mask, order);\n}\nALLOW_ERROR_INJECTION(should_fail_alloc_page, TRUE);\n\nstatic inline long __zone_watermark_unusable_free(struct zone *z,\n\t\t\t\tunsigned int order, unsigned int alloc_flags)\n{\n\tconst bool alloc_harder = (alloc_flags & (ALLOC_HARDER|ALLOC_OOM));\n\tlong unusable_free = (1 << order) - 1;\n\n\t/*\n\t * If the caller does not have rights to ALLOC_HARDER then subtract\n\t * the high-atomic reserves. This will over-estimate the size of the\n\t * atomic reserve but it avoids a search.\n\t */\n\tif (likely(!alloc_harder))\n\t\tunusable_free += z->nr_reserved_highatomic;\n\n#ifdef CONFIG_CMA\n\t/* If allocation can't use CMA areas don't use free CMA pages */\n\tif (!(alloc_flags & ALLOC_CMA))\n\t\tunusable_free += zone_page_state(z, NR_FREE_CMA_PAGES);\n#endif\n\n\treturn unusable_free;\n}\n\n/*\n * Return true if free base pages are above 'mark'. For high-order checks it\n * will return true of the order-0 watermark is reached and there is at least\n * one free page of a suitable size. Checking now avoids taking the zone lock\n * to check in the allocation paths if no pages are free.\n */\nbool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,\n\t\t\t int highest_zoneidx, unsigned int alloc_flags,\n\t\t\t long free_pages)\n{\n\tlong min = mark;\n\tint o;\n\tconst bool alloc_harder = (alloc_flags & (ALLOC_HARDER|ALLOC_OOM));\n\n\t/* free_pages may go negative - that's OK */\n\tfree_pages -= __zone_watermark_unusable_free(z, order, alloc_flags);\n\n\tif (alloc_flags & ALLOC_HIGH)\n\t\tmin -= min / 2;\n\n\tif (unlikely(alloc_harder)) {\n\t\t/*\n\t\t * OOM victims can try even harder than normal ALLOC_HARDER\n\t\t * users on the grounds that it's definitely going to be in\n\t\t * the exit path shortly and free memory. Any allocation it\n\t\t * makes during the free path will be small and short-lived.\n\t\t */\n\t\tif (alloc_flags & ALLOC_OOM)\n\t\t\tmin -= min / 2;\n\t\telse\n\t\t\tmin -= min / 4;\n\t}\n\n\t/*\n\t * Check watermarks for an order-0 allocation request. If these\n\t * are not met, then a high-order request also cannot go ahead\n\t * even if a suitable page happened to be free.\n\t */\n\tif (free_pages <= min + z->lowmem_reserve[highest_zoneidx])\n\t\treturn false;\n\n\t/* If this is an order-0 request then the watermark is fine */\n\tif (!order)\n\t\treturn true;\n\n\t/* For a high-order request, check at least one suitable page is free */\n\tfor (o = order; o < MAX_ORDER; o++) {\n\t\tstruct free_area *area = &z->free_area[o];\n\t\tint mt;\n\n\t\tif (!area->nr_free)\n\t\t\tcontinue;\n\n\t\tfor (mt = 0; mt < MIGRATE_PCPTYPES; mt++) {\n\t\t\tif (!free_area_empty(area, mt))\n\t\t\t\treturn true;\n\t\t}\n\n#ifdef CONFIG_CMA\n\t\tif ((alloc_flags & ALLOC_CMA) &&\n\t\t    !free_area_empty(area, MIGRATE_CMA)) {\n\t\t\treturn true;\n\t\t}\n#endif\n\t\tif (alloc_harder && !free_area_empty(area, MIGRATE_HIGHATOMIC))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nbool zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,\n\t\t      int highest_zoneidx, unsigned int alloc_flags)\n{\n\treturn __zone_watermark_ok(z, order, mark, highest_zoneidx, alloc_flags,\n\t\t\t\t\tzone_page_state(z, NR_FREE_PAGES));\n}\n\nstatic inline bool zone_watermark_fast(struct zone *z, unsigned int order,\n\t\t\t\tunsigned long mark, int highest_zoneidx,\n\t\t\t\tunsigned int alloc_flags, gfp_t gfp_mask)\n{\n\tlong free_pages;\n\n\tfree_pages = zone_page_state(z, NR_FREE_PAGES);\n\n\t/*\n\t * Fast check for order-0 only. If this fails then the reserves\n\t * need to be calculated.\n\t */\n\tif (!order) {\n\t\tlong fast_free;\n\n\t\tfast_free = free_pages;\n\t\tfast_free -= __zone_watermark_unusable_free(z, 0, alloc_flags);\n\t\tif (fast_free > mark + z->lowmem_reserve[highest_zoneidx])\n\t\t\treturn true;\n\t}\n\n\tif (__zone_watermark_ok(z, order, mark, highest_zoneidx, alloc_flags,\n\t\t\t\t\tfree_pages))\n\t\treturn true;\n\t/*\n\t * Ignore watermark boosting for GFP_ATOMIC order-0 allocations\n\t * when checking the min watermark. The min watermark is the\n\t * point where boosting is ignored so that kswapd is woken up\n\t * when below the low watermark.\n\t */\n\tif (unlikely(!order && (gfp_mask & __GFP_ATOMIC) && z->watermark_boost\n\t\t&& ((alloc_flags & ALLOC_WMARK_MASK) == WMARK_MIN))) {\n\t\tmark = z->_watermark[WMARK_MIN];\n\t\treturn __zone_watermark_ok(z, order, mark, highest_zoneidx,\n\t\t\t\t\talloc_flags, free_pages);\n\t}\n\n\treturn false;\n}\n\nbool zone_watermark_ok_safe(struct zone *z, unsigned int order,\n\t\t\tunsigned long mark, int highest_zoneidx)\n{\n\tlong free_pages = zone_page_state(z, NR_FREE_PAGES);\n\n\tif (z->percpu_drift_mark && free_pages < z->percpu_drift_mark)\n\t\tfree_pages = zone_page_state_snapshot(z, NR_FREE_PAGES);\n\n\treturn __zone_watermark_ok(z, order, mark, highest_zoneidx, 0,\n\t\t\t\t\t\t\t\tfree_pages);\n}\n\n#ifdef CONFIG_NUMA\nstatic bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)\n{\n\treturn node_distance(zone_to_nid(local_zone), zone_to_nid(zone)) <=\n\t\t\t\tnode_reclaim_distance;\n}\n#else\t/* CONFIG_NUMA */\nstatic bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)\n{\n\treturn true;\n}\n#endif\t/* CONFIG_NUMA */\n\n/*\n * The restriction on ZONE_DMA32 as being a suitable zone to use to avoid\n * fragmentation is subtle. If the preferred zone was HIGHMEM then\n * premature use of a lower zone may cause lowmem pressure problems that\n * are worse than fragmentation. If the next zone is ZONE_DMA then it is\n * probably too small. It only makes sense to spread allocations to avoid\n * fragmentation between the Normal and DMA32 zones.\n */\nstatic inline unsigned int\nalloc_flags_nofragment(struct zone *zone, gfp_t gfp_mask)\n{\n\tunsigned int alloc_flags;\n\n\t/*\n\t * __GFP_KSWAPD_RECLAIM is assumed to be the same as ALLOC_KSWAPD\n\t * to save a branch.\n\t */\n\talloc_flags = (__force int) (gfp_mask & __GFP_KSWAPD_RECLAIM);\n\n#ifdef CONFIG_ZONE_DMA32\n\tif (!zone)\n\t\treturn alloc_flags;\n\n\tif (zone_idx(zone) != ZONE_NORMAL)\n\t\treturn alloc_flags;\n\n\t/*\n\t * If ZONE_DMA32 exists, assume it is the one after ZONE_NORMAL and\n\t * the pointer is within zone->zone_pgdat->node_zones[]. Also assume\n\t * on UMA that if Normal is populated then so is DMA32.\n\t */\n\tBUILD_BUG_ON(ZONE_NORMAL - ZONE_DMA32 != 1);\n\tif (nr_online_nodes > 1 && !populated_zone(--zone))\n\t\treturn alloc_flags;\n\n\talloc_flags |= ALLOC_NOFRAGMENT;\n#endif /* CONFIG_ZONE_DMA32 */\n\treturn alloc_flags;\n}\n\n/* Must be called after current_gfp_context() which can change gfp_mask */\nstatic inline unsigned int gfp_to_alloc_flags_cma(gfp_t gfp_mask,\n\t\t\t\t\t\t  unsigned int alloc_flags)\n{\n#ifdef CONFIG_CMA\n\tif (gfp_migratetype(gfp_mask) == MIGRATE_MOVABLE)\n\t\talloc_flags |= ALLOC_CMA;\n#endif\n\treturn alloc_flags;\n}\n\n/*\n * get_page_from_freelist goes through the zonelist trying to allocate\n * a page.\n */\nstatic struct page *\nget_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,\n\t\t\t\t\t\tconst struct alloc_context *ac)\n{\n\tstruct zoneref *z;\n\tstruct zone *zone;\n\tstruct pglist_data *last_pgdat_dirty_limit = NULL;\n\tbool no_fallback;\n\nretry:\n\t/*\n\t * Scan zonelist, looking for a zone with enough free.\n\t * See also __cpuset_node_allowed() comment in kernel/cpuset.c.\n\t */\n\tno_fallback = alloc_flags & ALLOC_NOFRAGMENT;\n\tz = ac->preferred_zoneref;\n\tfor_next_zone_zonelist_nodemask(zone, z, ac->highest_zoneidx,\n\t\t\t\t\tac->nodemask) {\n\t\tstruct page *page;\n\t\tunsigned long mark;\n\n\t\tif (cpusets_enabled() &&\n\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n\t\t\t\tcontinue;\n\t\t/*\n\t\t * When allocating a page cache page for writing, we\n\t\t * want to get it from a node that is within its dirty\n\t\t * limit, such that no single node holds more than its\n\t\t * proportional share of globally allowed dirty pages.\n\t\t * The dirty limits take into account the node's\n\t\t * lowmem reserves and high watermark so that kswapd\n\t\t * should be able to balance it without having to\n\t\t * write pages from its LRU list.\n\t\t *\n\t\t * XXX: For now, allow allocations to potentially\n\t\t * exceed the per-node dirty limit in the slowpath\n\t\t * (spread_dirty_pages unset) before going into reclaim,\n\t\t * which is important when on a NUMA setup the allowed\n\t\t * nodes are together not big enough to reach the\n\t\t * global limit.  The proper fix for these situations\n\t\t * will require awareness of nodes in the\n\t\t * dirty-throttling and the flusher threads.\n\t\t */\n\t\tif (ac->spread_dirty_pages) {\n\t\t\tif (last_pgdat_dirty_limit == zone->zone_pgdat)\n\t\t\t\tcontinue;\n\n\t\t\tif (!node_dirty_ok(zone->zone_pgdat)) {\n\t\t\t\tlast_pgdat_dirty_limit = zone->zone_pgdat;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (no_fallback && nr_online_nodes > 1 &&\n\t\t    zone != ac->preferred_zoneref->zone) {\n\t\t\tint local_nid;\n\n\t\t\t/*\n\t\t\t * If moving to a remote node, retry but allow\n\t\t\t * fragmenting fallbacks. Locality is more important\n\t\t\t * than fragmentation avoidance.\n\t\t\t */\n\t\t\tlocal_nid = zone_to_nid(ac->preferred_zoneref->zone);\n\t\t\tif (zone_to_nid(zone) != local_nid) {\n\t\t\t\talloc_flags &= ~ALLOC_NOFRAGMENT;\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t}\n\n\t\tmark = wmark_pages(zone, alloc_flags & ALLOC_WMARK_MASK);\n\t\tif (!zone_watermark_fast(zone, order, mark,\n\t\t\t\t       ac->highest_zoneidx, alloc_flags,\n\t\t\t\t       gfp_mask)) {\n\t\t\tint ret;\n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\n\t\t\t/*\n\t\t\t * Watermark failed for this zone, but see if we can\n\t\t\t * grow this zone if it contains deferred pages.\n\t\t\t */\n\t\t\tif (static_branch_unlikely(&deferred_pages)) {\n\t\t\t\tif (_deferred_grow_zone(zone, order))\n\t\t\t\t\tgoto try_this_zone;\n\t\t\t}\n#endif\n\t\t\t/* Checked here to keep the fast path fast */\n\t\t\tBUILD_BUG_ON(ALLOC_NO_WATERMARKS < NR_WMARK);\n\t\t\tif (alloc_flags & ALLOC_NO_WATERMARKS)\n\t\t\t\tgoto try_this_zone;\n\n\t\t\tif (!node_reclaim_enabled() ||\n\t\t\t    !zone_allows_reclaim(ac->preferred_zoneref->zone, zone))\n\t\t\t\tcontinue;\n\n\t\t\tret = node_reclaim(zone->zone_pgdat, gfp_mask, order);\n\t\t\tswitch (ret) {\n\t\t\tcase NODE_RECLAIM_NOSCAN:\n\t\t\t\t/* did not scan */\n\t\t\t\tcontinue;\n\t\t\tcase NODE_RECLAIM_FULL:\n\t\t\t\t/* scanned but unreclaimable */\n\t\t\t\tcontinue;\n\t\t\tdefault:\n\t\t\t\t/* did we reclaim enough */\n\t\t\t\tif (zone_watermark_ok(zone, order, mark,\n\t\t\t\t\tac->highest_zoneidx, alloc_flags))\n\t\t\t\t\tgoto try_this_zone;\n\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\ntry_this_zone:\n\t\tpage = rmqueue(ac->preferred_zoneref->zone, zone, order,\n\t\t\t\tgfp_mask, alloc_flags, ac->migratetype);\n\t\tif (page) {\n\t\t\tprep_new_page(page, order, gfp_mask, alloc_flags);\n\n\t\t\t/*\n\t\t\t * If this is a high-order atomic allocation then check\n\t\t\t * if the pageblock should be reserved for the future\n\t\t\t */\n\t\t\tif (unlikely(order && (alloc_flags & ALLOC_HARDER)))\n\t\t\t\treserve_highatomic_pageblock(page, zone, order);\n\n\t\t\treturn page;\n\t\t} else {\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\n\t\t\t/* Try again if zone has deferred pages */\n\t\t\tif (static_branch_unlikely(&deferred_pages)) {\n\t\t\t\tif (_deferred_grow_zone(zone, order))\n\t\t\t\t\tgoto try_this_zone;\n\t\t\t}\n#endif\n\t\t}\n\t}\n\n\t/*\n\t * It's possible on a UMA machine to get through all zones that are\n\t * fragmented. If avoiding fragmentation, reset and try again.\n\t */\n\tif (no_fallback) {\n\t\talloc_flags &= ~ALLOC_NOFRAGMENT;\n\t\tgoto retry;\n\t}\n\n\treturn NULL;\n}\n\nstatic void warn_alloc_show_mem(gfp_t gfp_mask, nodemask_t *nodemask)\n{\n\tunsigned int filter = SHOW_MEM_FILTER_NODES;\n\n\t/*\n\t * This documents exceptions given to allocations in certain\n\t * contexts that are allowed to allocate outside current's set\n\t * of allowed nodes.\n\t */\n\tif (!(gfp_mask & __GFP_NOMEMALLOC))\n\t\tif (tsk_is_oom_victim(current) ||\n\t\t    (current->flags & (PF_MEMALLOC | PF_EXITING)))\n\t\t\tfilter &= ~SHOW_MEM_FILTER_NODES;\n\tif (in_interrupt() || !(gfp_mask & __GFP_DIRECT_RECLAIM))\n\t\tfilter &= ~SHOW_MEM_FILTER_NODES;\n\n\tshow_mem(filter, nodemask);\n}\n\nvoid warn_alloc(gfp_t gfp_mask, nodemask_t *nodemask, const char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\tstatic DEFINE_RATELIMIT_STATE(nopage_rs, 10*HZ, 1);\n\n\tif ((gfp_mask & __GFP_NOWARN) || !__ratelimit(&nopage_rs))\n\t\treturn;\n\n\tva_start(args, fmt);\n\tvaf.fmt = fmt;\n\tvaf.va = &args;\n\tpr_warn(\"%s: %pV, mode:%#x(%pGg), nodemask=%*pbl\",\n\t\t\tcurrent->comm, &vaf, gfp_mask, &gfp_mask,\n\t\t\tnodemask_pr_args(nodemask));\n\tva_end(args);\n\n\tcpuset_print_current_mems_allowed();\n\tpr_cont(\"\\n\");\n\tdump_stack();\n\twarn_alloc_show_mem(gfp_mask, nodemask);\n}\n\nstatic inline struct page *\n__alloc_pages_cpuset_fallback(gfp_t gfp_mask, unsigned int order,\n\t\t\t      unsigned int alloc_flags,\n\t\t\t      const struct alloc_context *ac)\n{\n\tstruct page *page;\n\n\tpage = get_page_from_freelist(gfp_mask, order,\n\t\t\talloc_flags|ALLOC_CPUSET, ac);\n\t/*\n\t * fallback to ignore cpuset restriction if our nodes\n\t * are depleted\n\t */\n\tif (!page)\n\t\tpage = get_page_from_freelist(gfp_mask, order,\n\t\t\t\talloc_flags, ac);\n\n\treturn page;\n}\n\nstatic inline struct page *\n__alloc_pages_may_oom(gfp_t gfp_mask, unsigned int order,\n\tconst struct alloc_context *ac, unsigned long *did_some_progress)\n{\n\tstruct oom_control oc = {\n\t\t.zonelist = ac->zonelist,\n\t\t.nodemask = ac->nodemask,\n\t\t.memcg = NULL,\n\t\t.gfp_mask = gfp_mask,\n\t\t.order = order,\n\t};\n\tstruct page *page;\n\n\t*did_some_progress = 0;\n\n\t/*\n\t * Acquire the oom lock.  If that fails, somebody else is\n\t * making progress for us.\n\t */\n\tif (!mutex_trylock(&oom_lock)) {\n\t\t*did_some_progress = 1;\n\t\tschedule_timeout_uninterruptible(1);\n\t\treturn NULL;\n\t}\n\n\t/*\n\t * Go through the zonelist yet one more time, keep very high watermark\n\t * here, this is only to catch a parallel oom killing, we must fail if\n\t * we're still under heavy pressure. But make sure that this reclaim\n\t * attempt shall not depend on __GFP_DIRECT_RECLAIM && !__GFP_NORETRY\n\t * allocation which will never fail due to oom_lock already held.\n\t */\n\tpage = get_page_from_freelist((gfp_mask | __GFP_HARDWALL) &\n\t\t\t\t      ~__GFP_DIRECT_RECLAIM, order,\n\t\t\t\t      ALLOC_WMARK_HIGH|ALLOC_CPUSET, ac);\n\tif (page)\n\t\tgoto out;\n\n\t/* Coredumps can quickly deplete all memory reserves */\n\tif (current->flags & PF_DUMPCORE)\n\t\tgoto out;\n\t/* The OOM killer will not help higher order allocs */\n\tif (order > PAGE_ALLOC_COSTLY_ORDER)\n\t\tgoto out;\n\t/*\n\t * We have already exhausted all our reclaim opportunities without any\n\t * success so it is time to admit defeat. We will skip the OOM killer\n\t * because it is very likely that the caller has a more reasonable\n\t * fallback than shooting a random task.\n\t *\n\t * The OOM killer may not free memory on a specific node.\n\t */\n\tif (gfp_mask & (__GFP_RETRY_MAYFAIL | __GFP_THISNODE))\n\t\tgoto out;\n\t/* The OOM killer does not needlessly kill tasks for lowmem */\n\tif (ac->highest_zoneidx < ZONE_NORMAL)\n\t\tgoto out;\n\tif (pm_suspended_storage())\n\t\tgoto out;\n\t/*\n\t * XXX: GFP_NOFS allocations should rather fail than rely on\n\t * other request to make a forward progress.\n\t * We are in an unfortunate situation where out_of_memory cannot\n\t * do much for this context but let's try it to at least get\n\t * access to memory reserved if the current task is killed (see\n\t * out_of_memory). Once filesystems are ready to handle allocation\n\t * failures more gracefully we should just bail out here.\n\t */\n\n\t/* Exhausted what can be done so it's blame time */\n\tif (out_of_memory(&oc) || WARN_ON_ONCE(gfp_mask & __GFP_NOFAIL)) {\n\t\t*did_some_progress = 1;\n\n\t\t/*\n\t\t * Help non-failing allocations by giving them access to memory\n\t\t * reserves\n\t\t */\n\t\tif (gfp_mask & __GFP_NOFAIL)\n\t\t\tpage = __alloc_pages_cpuset_fallback(gfp_mask, order,\n\t\t\t\t\tALLOC_NO_WATERMARKS, ac);\n\t}\nout:\n\tmutex_unlock(&oom_lock);\n\treturn page;\n}\n\n/*\n * Maximum number of compaction retries wit a progress before OOM\n * killer is consider as the only way to move forward.\n */\n#define MAX_COMPACT_RETRIES 16\n\n#ifdef CONFIG_COMPACTION\n/* Try memory compaction for high-order allocations before reclaim */\nstatic struct page *\n__alloc_pages_direct_compact(gfp_t gfp_mask, unsigned int order,\n\t\tunsigned int alloc_flags, const struct alloc_context *ac,\n\t\tenum compact_priority prio, enum compact_result *compact_result)\n{\n\tstruct page *page = NULL;\n\tunsigned long pflags;\n\tunsigned int noreclaim_flag;\n\n\tif (!order)\n\t\treturn NULL;\n\n\tpsi_memstall_enter(&pflags);\n\tnoreclaim_flag = memalloc_noreclaim_save();\n\n\t*compact_result = try_to_compact_pages(gfp_mask, order, alloc_flags, ac,\n\t\t\t\t\t\t\t\tprio, &page);\n\n\tmemalloc_noreclaim_restore(noreclaim_flag);\n\tpsi_memstall_leave(&pflags);\n\n\tif (*compact_result == COMPACT_SKIPPED)\n\t\treturn NULL;\n\t/*\n\t * At least in one zone compaction wasn't deferred or skipped, so let's\n\t * count a compaction stall\n\t */\n\tcount_vm_event(COMPACTSTALL);\n\n\t/* Prep a captured page if available */\n\tif (page)\n\t\tprep_new_page(page, order, gfp_mask, alloc_flags);\n\n\t/* Try get a page from the freelist if available */\n\tif (!page)\n\t\tpage = get_page_from_freelist(gfp_mask, order, alloc_flags, ac);\n\n\tif (page) {\n\t\tstruct zone *zone = page_zone(page);\n\n\t\tzone->compact_blockskip_flush = false;\n\t\tcompaction_defer_reset(zone, order, true);\n\t\tcount_vm_event(COMPACTSUCCESS);\n\t\treturn page;\n\t}\n\n\t/*\n\t * It's bad if compaction run occurs and fails. The most likely reason\n\t * is that pages exist, but not enough to satisfy watermarks.\n\t */\n\tcount_vm_event(COMPACTFAIL);\n\n\tcond_resched();\n\n\treturn NULL;\n}\n\nstatic inline bool\nshould_compact_retry(struct alloc_context *ac, int order, int alloc_flags,\n\t\t     enum compact_result compact_result,\n\t\t     enum compact_priority *compact_priority,\n\t\t     int *compaction_retries)\n{\n\tint max_retries = MAX_COMPACT_RETRIES;\n\tint min_priority;\n\tbool ret = false;\n\tint retries = *compaction_retries;\n\tenum compact_priority priority = *compact_priority;\n\n\tif (!order)\n\t\treturn false;\n\n\tif (compaction_made_progress(compact_result))\n\t\t(*compaction_retries)++;\n\n\t/*\n\t * compaction considers all the zone as desperately out of memory\n\t * so it doesn't really make much sense to retry except when the\n\t * failure could be caused by insufficient priority\n\t */\n\tif (compaction_failed(compact_result))\n\t\tgoto check_priority;\n\n\t/*\n\t * compaction was skipped because there are not enough order-0 pages\n\t * to work with, so we retry only if it looks like reclaim can help.\n\t */\n\tif (compaction_needs_reclaim(compact_result)) {\n\t\tret = compaction_zonelist_suitable(ac, order, alloc_flags);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * make sure the compaction wasn't deferred or didn't bail out early\n\t * due to locks contention before we declare that we should give up.\n\t * But the next retry should use a higher priority if allowed, so\n\t * we don't just keep bailing out endlessly.\n\t */\n\tif (compaction_withdrawn(compact_result)) {\n\t\tgoto check_priority;\n\t}\n\n\t/*\n\t * !costly requests are much more important than __GFP_RETRY_MAYFAIL\n\t * costly ones because they are de facto nofail and invoke OOM\n\t * killer to move on while costly can fail and users are ready\n\t * to cope with that. 1/4 retries is rather arbitrary but we\n\t * would need much more detailed feedback from compaction to\n\t * make a better decision.\n\t */\n\tif (order > PAGE_ALLOC_COSTLY_ORDER)\n\t\tmax_retries /= 4;\n\tif (*compaction_retries <= max_retries) {\n\t\tret = true;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Make sure there are attempts at the highest priority if we exhausted\n\t * all retries or failed at the lower priorities.\n\t */\ncheck_priority:\n\tmin_priority = (order > PAGE_ALLOC_COSTLY_ORDER) ?\n\t\t\tMIN_COMPACT_COSTLY_PRIORITY : MIN_COMPACT_PRIORITY;\n\n\tif (*compact_priority > min_priority) {\n\t\t(*compact_priority)--;\n\t\t*compaction_retries = 0;\n\t\tret = true;\n\t}\nout:\n\ttrace_compact_retry(order, priority, compact_result, retries, max_retries, ret);\n\treturn ret;\n}\n#else\nstatic inline struct page *\n__alloc_pages_direct_compact(gfp_t gfp_mask, unsigned int order,\n\t\tunsigned int alloc_flags, const struct alloc_context *ac,\n\t\tenum compact_priority prio, enum compact_result *compact_result)\n{\n\t*compact_result = COMPACT_SKIPPED;\n\treturn NULL;\n}\n\nstatic inline bool\nshould_compact_retry(struct alloc_context *ac, unsigned int order, int alloc_flags,\n\t\t     enum compact_result compact_result,\n\t\t     enum compact_priority *compact_priority,\n\t\t     int *compaction_retries)\n{\n\tstruct zone *zone;\n\tstruct zoneref *z;\n\n\tif (!order || order > PAGE_ALLOC_COSTLY_ORDER)\n\t\treturn false;\n\n\t/*\n\t * There are setups with compaction disabled which would prefer to loop\n\t * inside the allocator rather than hit the oom killer prematurely.\n\t * Let's give them a good hope and keep retrying while the order-0\n\t * watermarks are OK.\n\t */\n\tfor_each_zone_zonelist_nodemask(zone, z, ac->zonelist,\n\t\t\t\tac->highest_zoneidx, ac->nodemask) {\n\t\tif (zone_watermark_ok(zone, 0, min_wmark_pages(zone),\n\t\t\t\t\tac->highest_zoneidx, alloc_flags))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n#endif /* CONFIG_COMPACTION */\n\n#ifdef CONFIG_LOCKDEP\nstatic struct lockdep_map __fs_reclaim_map =\n\tSTATIC_LOCKDEP_MAP_INIT(\"fs_reclaim\", &__fs_reclaim_map);\n\nstatic bool __need_reclaim(gfp_t gfp_mask)\n{\n\t/* no reclaim without waiting on it */\n\tif (!(gfp_mask & __GFP_DIRECT_RECLAIM))\n\t\treturn false;\n\n\t/* this guy won't enter reclaim */\n\tif (current->flags & PF_MEMALLOC)\n\t\treturn false;\n\n\tif (gfp_mask & __GFP_NOLOCKDEP)\n\t\treturn false;\n\n\treturn true;\n}\n\nvoid __fs_reclaim_acquire(void)\n{\n\tlock_map_acquire(&__fs_reclaim_map);\n}\n\nvoid __fs_reclaim_release(void)\n{\n\tlock_map_release(&__fs_reclaim_map);\n}\n\nvoid fs_reclaim_acquire(gfp_t gfp_mask)\n{\n\tgfp_mask = current_gfp_context(gfp_mask);\n\n\tif (__need_reclaim(gfp_mask)) {\n\t\tif (gfp_mask & __GFP_FS)\n\t\t\t__fs_reclaim_acquire();\n\n#ifdef CONFIG_MMU_NOTIFIER\n\t\tlock_map_acquire(&__mmu_notifier_invalidate_range_start_map);\n\t\tlock_map_release(&__mmu_notifier_invalidate_range_start_map);\n#endif\n\n\t}\n}\nEXPORT_SYMBOL_GPL(fs_reclaim_acquire);\n\nvoid fs_reclaim_release(gfp_t gfp_mask)\n{\n\tgfp_mask = current_gfp_context(gfp_mask);\n\n\tif (__need_reclaim(gfp_mask)) {\n\t\tif (gfp_mask & __GFP_FS)\n\t\t\t__fs_reclaim_release();\n\t}\n}\nEXPORT_SYMBOL_GPL(fs_reclaim_release);\n#endif\n\n/* Perform direct synchronous page reclaim */\nstatic unsigned long\n__perform_reclaim(gfp_t gfp_mask, unsigned int order,\n\t\t\t\t\tconst struct alloc_context *ac)\n{\n\tunsigned int noreclaim_flag;\n\tunsigned long pflags, progress;\n\n\tcond_resched();\n\n\t/* We now go into synchronous reclaim */\n\tcpuset_memory_pressure_bump();\n\tpsi_memstall_enter(&pflags);\n\tfs_reclaim_acquire(gfp_mask);\n\tnoreclaim_flag = memalloc_noreclaim_save();\n\n\tprogress = try_to_free_pages(ac->zonelist, order, gfp_mask,\n\t\t\t\t\t\t\t\tac->nodemask);\n\n\tmemalloc_noreclaim_restore(noreclaim_flag);\n\tfs_reclaim_release(gfp_mask);\n\tpsi_memstall_leave(&pflags);\n\n\tcond_resched();\n\n\treturn progress;\n}\n\n/* The really slow allocator path where we enter direct reclaim */\nstatic inline struct page *\n__alloc_pages_direct_reclaim(gfp_t gfp_mask, unsigned int order,\n\t\tunsigned int alloc_flags, const struct alloc_context *ac,\n\t\tunsigned long *did_some_progress)\n{\n\tstruct page *page = NULL;\n\tbool drained = false;\n\n\t*did_some_progress = __perform_reclaim(gfp_mask, order, ac);\n\tif (unlikely(!(*did_some_progress)))\n\t\treturn NULL;\n\nretry:\n\tpage = get_page_from_freelist(gfp_mask, order, alloc_flags, ac);\n\n\t/*\n\t * If an allocation failed after direct reclaim, it could be because\n\t * pages are pinned on the per-cpu lists or in high alloc reserves.\n\t * Shrink them and try again\n\t */\n\tif (!page && !drained) {\n\t\tunreserve_highatomic_pageblock(ac, false);\n\t\tdrain_all_pages(NULL);\n\t\tdrained = true;\n\t\tgoto retry;\n\t}\n\n\treturn page;\n}\n\nstatic void wake_all_kswapds(unsigned int order, gfp_t gfp_mask,\n\t\t\t     const struct alloc_context *ac)\n{\n\tstruct zoneref *z;\n\tstruct zone *zone;\n\tpg_data_t *last_pgdat = NULL;\n\tenum zone_type highest_zoneidx = ac->highest_zoneidx;\n\n\tfor_each_zone_zonelist_nodemask(zone, z, ac->zonelist, highest_zoneidx,\n\t\t\t\t\tac->nodemask) {\n\t\tif (last_pgdat != zone->zone_pgdat)\n\t\t\twakeup_kswapd(zone, gfp_mask, order, highest_zoneidx);\n\t\tlast_pgdat = zone->zone_pgdat;\n\t}\n}\n\nstatic inline unsigned int\ngfp_to_alloc_flags(gfp_t gfp_mask)\n{\n\tunsigned int alloc_flags = ALLOC_WMARK_MIN | ALLOC_CPUSET;\n\n\t/*\n\t * __GFP_HIGH is assumed to be the same as ALLOC_HIGH\n\t * and __GFP_KSWAPD_RECLAIM is assumed to be the same as ALLOC_KSWAPD\n\t * to save two branches.\n\t */\n\tBUILD_BUG_ON(__GFP_HIGH != (__force gfp_t) ALLOC_HIGH);\n\tBUILD_BUG_ON(__GFP_KSWAPD_RECLAIM != (__force gfp_t) ALLOC_KSWAPD);\n\n\t/*\n\t * The caller may dip into page reserves a bit more if the caller\n\t * cannot run direct reclaim, or if the caller has realtime scheduling\n\t * policy or is asking for __GFP_HIGH memory.  GFP_ATOMIC requests will\n\t * set both ALLOC_HARDER (__GFP_ATOMIC) and ALLOC_HIGH (__GFP_HIGH).\n\t */\n\talloc_flags |= (__force int)\n\t\t(gfp_mask & (__GFP_HIGH | __GFP_KSWAPD_RECLAIM));\n\n\tif (gfp_mask & __GFP_ATOMIC) {\n\t\t/*\n\t\t * Not worth trying to allocate harder for __GFP_NOMEMALLOC even\n\t\t * if it can't schedule.\n\t\t */\n\t\tif (!(gfp_mask & __GFP_NOMEMALLOC))\n\t\t\talloc_flags |= ALLOC_HARDER;\n\t\t/*\n\t\t * Ignore cpuset mems for GFP_ATOMIC rather than fail, see the\n\t\t * comment for __cpuset_node_allowed().\n\t\t */\n\t\talloc_flags &= ~ALLOC_CPUSET;\n\t} else if (unlikely(rt_task(current)) && !in_interrupt())\n\t\talloc_flags |= ALLOC_HARDER;\n\n\talloc_flags = gfp_to_alloc_flags_cma(gfp_mask, alloc_flags);\n\n\treturn alloc_flags;\n}\n\nstatic bool oom_reserves_allowed(struct task_struct *tsk)\n{\n\tif (!tsk_is_oom_victim(tsk))\n\t\treturn false;\n\n\t/*\n\t * !MMU doesn't have oom reaper so give access to memory reserves\n\t * only to the thread with TIF_MEMDIE set\n\t */\n\tif (!IS_ENABLED(CONFIG_MMU) && !test_thread_flag(TIF_MEMDIE))\n\t\treturn false;\n\n\treturn true;\n}\n\n/*\n * Distinguish requests which really need access to full memory\n * reserves from oom victims which can live with a portion of it\n */\nstatic inline int __gfp_pfmemalloc_flags(gfp_t gfp_mask)\n{\n\tif (unlikely(gfp_mask & __GFP_NOMEMALLOC))\n\t\treturn 0;\n\tif (gfp_mask & __GFP_MEMALLOC)\n\t\treturn ALLOC_NO_WATERMARKS;\n\tif (in_serving_softirq() && (current->flags & PF_MEMALLOC))\n\t\treturn ALLOC_NO_WATERMARKS;\n\tif (!in_interrupt()) {\n\t\tif (current->flags & PF_MEMALLOC)\n\t\t\treturn ALLOC_NO_WATERMARKS;\n\t\telse if (oom_reserves_allowed(current))\n\t\t\treturn ALLOC_OOM;\n\t}\n\n\treturn 0;\n}\n\nbool gfp_pfmemalloc_allowed(gfp_t gfp_mask)\n{\n\treturn !!__gfp_pfmemalloc_flags(gfp_mask);\n}\n\n/*\n * Checks whether it makes sense to retry the reclaim to make a forward progress\n * for the given allocation request.\n *\n * We give up when we either have tried MAX_RECLAIM_RETRIES in a row\n * without success, or when we couldn't even meet the watermark if we\n * reclaimed all remaining pages on the LRU lists.\n *\n * Returns true if a retry is viable or false to enter the oom path.\n */\nstatic inline bool\nshould_reclaim_retry(gfp_t gfp_mask, unsigned order,\n\t\t     struct alloc_context *ac, int alloc_flags,\n\t\t     bool did_some_progress, int *no_progress_loops)\n{\n\tstruct zone *zone;\n\tstruct zoneref *z;\n\tbool ret = false;\n\n\t/*\n\t * Costly allocations might have made a progress but this doesn't mean\n\t * their order will become available due to high fragmentation so\n\t * always increment the no progress counter for them\n\t */\n\tif (did_some_progress && order <= PAGE_ALLOC_COSTLY_ORDER)\n\t\t*no_progress_loops = 0;\n\telse\n\t\t(*no_progress_loops)++;\n\n\t/*\n\t * Make sure we converge to OOM if we cannot make any progress\n\t * several times in the row.\n\t */\n\tif (*no_progress_loops > MAX_RECLAIM_RETRIES) {\n\t\t/* Before OOM, exhaust highatomic_reserve */\n\t\treturn unreserve_highatomic_pageblock(ac, true);\n\t}\n\n\t/*\n\t * Keep reclaiming pages while there is a chance this will lead\n\t * somewhere.  If none of the target zones can satisfy our allocation\n\t * request even if all reclaimable pages are considered then we are\n\t * screwed and have to go OOM.\n\t */\n\tfor_each_zone_zonelist_nodemask(zone, z, ac->zonelist,\n\t\t\t\tac->highest_zoneidx, ac->nodemask) {\n\t\tunsigned long available;\n\t\tunsigned long reclaimable;\n\t\tunsigned long min_wmark = min_wmark_pages(zone);\n\t\tbool wmark;\n\n\t\tavailable = reclaimable = zone_reclaimable_pages(zone);\n\t\tavailable += zone_page_state_snapshot(zone, NR_FREE_PAGES);\n\n\t\t/*\n\t\t * Would the allocation succeed if we reclaimed all\n\t\t * reclaimable pages?\n\t\t */\n\t\twmark = __zone_watermark_ok(zone, order, min_wmark,\n\t\t\t\tac->highest_zoneidx, alloc_flags, available);\n\t\ttrace_reclaim_retry_zone(z, order, reclaimable,\n\t\t\t\tavailable, min_wmark, *no_progress_loops, wmark);\n\t\tif (wmark) {\n\t\t\t/*\n\t\t\t * If we didn't make any progress and have a lot of\n\t\t\t * dirty + writeback pages then we should wait for\n\t\t\t * an IO to complete to slow down the reclaim and\n\t\t\t * prevent from pre mature OOM\n\t\t\t */\n\t\t\tif (!did_some_progress) {\n\t\t\t\tunsigned long write_pending;\n\n\t\t\t\twrite_pending = zone_page_state_snapshot(zone,\n\t\t\t\t\t\t\tNR_ZONE_WRITE_PENDING);\n\n\t\t\t\tif (2 * write_pending > reclaimable) {\n\t\t\t\t\tcongestion_wait(BLK_RW_ASYNC, HZ/10);\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tret = true;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\t/*\n\t * Memory allocation/reclaim might be called from a WQ context and the\n\t * current implementation of the WQ concurrency control doesn't\n\t * recognize that a particular WQ is congested if the worker thread is\n\t * looping without ever sleeping. Therefore we have to do a short sleep\n\t * here rather than calling cond_resched().\n\t */\n\tif (current->flags & PF_WQ_WORKER)\n\t\tschedule_timeout_uninterruptible(1);\n\telse\n\t\tcond_resched();\n\treturn ret;\n}\n\nstatic inline bool\ncheck_retry_cpuset(int cpuset_mems_cookie, struct alloc_context *ac)\n{\n\t/*\n\t * It's possible that cpuset's mems_allowed and the nodemask from\n\t * mempolicy don't intersect. This should be normally dealt with by\n\t * policy_nodemask(), but it's possible to race with cpuset update in\n\t * such a way the check therein was true, and then it became false\n\t * before we got our cpuset_mems_cookie here.\n\t * This assumes that for all allocations, ac->nodemask can come only\n\t * from MPOL_BIND mempolicy (whose documented semantics is to be ignored\n\t * when it does not intersect with the cpuset restrictions) or the\n\t * caller can deal with a violated nodemask.\n\t */\n\tif (cpusets_enabled() && ac->nodemask &&\n\t\t\t!cpuset_nodemask_valid_mems_allowed(ac->nodemask)) {\n\t\tac->nodemask = NULL;\n\t\treturn true;\n\t}\n\n\t/*\n\t * When updating a task's mems_allowed or mempolicy nodemask, it is\n\t * possible to race with parallel threads in such a way that our\n\t * allocation can fail while the mask is being updated. If we are about\n\t * to fail, check if the cpuset changed during allocation and if so,\n\t * retry.\n\t */\n\tif (read_mems_allowed_retry(cpuset_mems_cookie))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline struct page *\n__alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,\n\t\t\t\t\t\tstruct alloc_context *ac)\n{\n\tbool can_direct_reclaim = gfp_mask & __GFP_DIRECT_RECLAIM;\n\tconst bool costly_order = order > PAGE_ALLOC_COSTLY_ORDER;\n\tstruct page *page = NULL;\n\tunsigned int alloc_flags;\n\tunsigned long did_some_progress;\n\tenum compact_priority compact_priority;\n\tenum compact_result compact_result;\n\tint compaction_retries;\n\tint no_progress_loops;\n\tunsigned int cpuset_mems_cookie;\n\tint reserve_flags;\n\n\t/*\n\t * We also sanity check to catch abuse of atomic reserves being used by\n\t * callers that are not in atomic context.\n\t */\n\tif (WARN_ON_ONCE((gfp_mask & (__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)) ==\n\t\t\t\t(__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)))\n\t\tgfp_mask &= ~__GFP_ATOMIC;\n\nretry_cpuset:\n\tcompaction_retries = 0;\n\tno_progress_loops = 0;\n\tcompact_priority = DEF_COMPACT_PRIORITY;\n\tcpuset_mems_cookie = read_mems_allowed_begin();\n\n\t/*\n\t * The fast path uses conservative alloc_flags to succeed only until\n\t * kswapd needs to be woken up, and to avoid the cost of setting up\n\t * alloc_flags precisely. So we do that now.\n\t */\n\talloc_flags = gfp_to_alloc_flags(gfp_mask);\n\n\t/*\n\t * We need to recalculate the starting point for the zonelist iterator\n\t * because we might have used different nodemask in the fast path, or\n\t * there was a cpuset modification and we are retrying - otherwise we\n\t * could end up iterating over non-eligible zones endlessly.\n\t */\n\tac->preferred_zoneref = first_zones_zonelist(ac->zonelist,\n\t\t\t\t\tac->highest_zoneidx, ac->nodemask);\n\tif (!ac->preferred_zoneref->zone)\n\t\tgoto nopage;\n\n\tif (alloc_flags & ALLOC_KSWAPD)\n\t\twake_all_kswapds(order, gfp_mask, ac);\n\n\t/*\n\t * The adjusted alloc_flags might result in immediate success, so try\n\t * that first\n\t */\n\tpage = get_page_from_freelist(gfp_mask, order, alloc_flags, ac);\n\tif (page)\n\t\tgoto got_pg;\n\n\t/*\n\t * For costly allocations, try direct compaction first, as it's likely\n\t * that we have enough base pages and don't need to reclaim. For non-\n\t * movable high-order allocations, do that as well, as compaction will\n\t * try prevent permanent fragmentation by migrating from blocks of the\n\t * same migratetype.\n\t * Don't try this for allocations that are allowed to ignore\n\t * watermarks, as the ALLOC_NO_WATERMARKS attempt didn't yet happen.\n\t */\n\tif (can_direct_reclaim &&\n\t\t\t(costly_order ||\n\t\t\t   (order > 0 && ac->migratetype != MIGRATE_MOVABLE))\n\t\t\t&& !gfp_pfmemalloc_allowed(gfp_mask)) {\n\t\tpage = __alloc_pages_direct_compact(gfp_mask, order,\n\t\t\t\t\t\talloc_flags, ac,\n\t\t\t\t\t\tINIT_COMPACT_PRIORITY,\n\t\t\t\t\t\t&compact_result);\n\t\tif (page)\n\t\t\tgoto got_pg;\n\n\t\t/*\n\t\t * Checks for costly allocations with __GFP_NORETRY, which\n\t\t * includes some THP page fault allocations\n\t\t */\n\t\tif (costly_order && (gfp_mask & __GFP_NORETRY)) {\n\t\t\t/*\n\t\t\t * If allocating entire pageblock(s) and compaction\n\t\t\t * failed because all zones are below low watermarks\n\t\t\t * or is prohibited because it recently failed at this\n\t\t\t * order, fail immediately unless the allocator has\n\t\t\t * requested compaction and reclaim retry.\n\t\t\t *\n\t\t\t * Reclaim is\n\t\t\t *  - potentially very expensive because zones are far\n\t\t\t *    below their low watermarks or this is part of very\n\t\t\t *    bursty high order allocations,\n\t\t\t *  - not guaranteed to help because isolate_freepages()\n\t\t\t *    may not iterate over freed pages as part of its\n\t\t\t *    linear scan, and\n\t\t\t *  - unlikely to make entire pageblocks free on its\n\t\t\t *    own.\n\t\t\t */\n\t\t\tif (compact_result == COMPACT_SKIPPED ||\n\t\t\t    compact_result == COMPACT_DEFERRED)\n\t\t\t\tgoto nopage;\n\n\t\t\t/*\n\t\t\t * Looks like reclaim/compaction is worth trying, but\n\t\t\t * sync compaction could be very expensive, so keep\n\t\t\t * using async compaction.\n\t\t\t */\n\t\t\tcompact_priority = INIT_COMPACT_PRIORITY;\n\t\t}\n\t}\n\nretry:\n\t/* Ensure kswapd doesn't accidentally go to sleep as long as we loop */\n\tif (alloc_flags & ALLOC_KSWAPD)\n\t\twake_all_kswapds(order, gfp_mask, ac);\n\n\treserve_flags = __gfp_pfmemalloc_flags(gfp_mask);\n\tif (reserve_flags)\n\t\talloc_flags = gfp_to_alloc_flags_cma(gfp_mask, reserve_flags);\n\n\t/*\n\t * Reset the nodemask and zonelist iterators if memory policies can be\n\t * ignored. These allocations are high priority and system rather than\n\t * user oriented.\n\t */\n\tif (!(alloc_flags & ALLOC_CPUSET) || reserve_flags) {\n\t\tac->nodemask = NULL;\n\t\tac->preferred_zoneref = first_zones_zonelist(ac->zonelist,\n\t\t\t\t\tac->highest_zoneidx, ac->nodemask);\n\t}\n\n\t/* Attempt with potentially adjusted zonelist and alloc_flags */\n\tpage = get_page_from_freelist(gfp_mask, order, alloc_flags, ac);\n\tif (page)\n\t\tgoto got_pg;\n\n\t/* Caller is not willing to reclaim, we can't balance anything */\n\tif (!can_direct_reclaim)\n\t\tgoto nopage;\n\n\t/* Avoid recursion of direct reclaim */\n\tif (current->flags & PF_MEMALLOC)\n\t\tgoto nopage;\n\n\t/* Try direct reclaim and then allocating */\n\tpage = __alloc_pages_direct_reclaim(gfp_mask, order, alloc_flags, ac,\n\t\t\t\t\t\t\t&did_some_progress);\n\tif (page)\n\t\tgoto got_pg;\n\n\t/* Try direct compaction and then allocating */\n\tpage = __alloc_pages_direct_compact(gfp_mask, order, alloc_flags, ac,\n\t\t\t\t\tcompact_priority, &compact_result);\n\tif (page)\n\t\tgoto got_pg;\n\n\t/* Do not loop if specifically requested */\n\tif (gfp_mask & __GFP_NORETRY)\n\t\tgoto nopage;\n\n\t/*\n\t * Do not retry costly high order allocations unless they are\n\t * __GFP_RETRY_MAYFAIL\n\t */\n\tif (costly_order && !(gfp_mask & __GFP_RETRY_MAYFAIL))\n\t\tgoto nopage;\n\n\tif (should_reclaim_retry(gfp_mask, order, ac, alloc_flags,\n\t\t\t\t did_some_progress > 0, &no_progress_loops))\n\t\tgoto retry;\n\n\t/*\n\t * It doesn't make any sense to retry for the compaction if the order-0\n\t * reclaim is not able to make any progress because the current\n\t * implementation of the compaction depends on the sufficient amount\n\t * of free memory (see __compaction_suitable)\n\t */\n\tif (did_some_progress > 0 &&\n\t\t\tshould_compact_retry(ac, order, alloc_flags,\n\t\t\t\tcompact_result, &compact_priority,\n\t\t\t\t&compaction_retries))\n\t\tgoto retry;\n\n\n\t/* Deal with possible cpuset update races before we start OOM killing */\n\tif (check_retry_cpuset(cpuset_mems_cookie, ac))\n\t\tgoto retry_cpuset;\n\n\t/* Reclaim has failed us, start killing things */\n\tpage = __alloc_pages_may_oom(gfp_mask, order, ac, &did_some_progress);\n\tif (page)\n\t\tgoto got_pg;\n\n\t/* Avoid allocations with no watermarks from looping endlessly */\n\tif (tsk_is_oom_victim(current) &&\n\t    (alloc_flags & ALLOC_OOM ||\n\t     (gfp_mask & __GFP_NOMEMALLOC)))\n\t\tgoto nopage;\n\n\t/* Retry as long as the OOM killer is making progress */\n\tif (did_some_progress) {\n\t\tno_progress_loops = 0;\n\t\tgoto retry;\n\t}\n\nnopage:\n\t/* Deal with possible cpuset update races before we fail */\n\tif (check_retry_cpuset(cpuset_mems_cookie, ac))\n\t\tgoto retry_cpuset;\n\n\t/*\n\t * Make sure that __GFP_NOFAIL request doesn't leak out and make sure\n\t * we always retry\n\t */\n\tif (gfp_mask & __GFP_NOFAIL) {\n\t\t/*\n\t\t * All existing users of the __GFP_NOFAIL are blockable, so warn\n\t\t * of any new users that actually require GFP_NOWAIT\n\t\t */\n\t\tif (WARN_ON_ONCE(!can_direct_reclaim))\n\t\t\tgoto fail;\n\n\t\t/*\n\t\t * PF_MEMALLOC request from this context is rather bizarre\n\t\t * because we cannot reclaim anything and only can loop waiting\n\t\t * for somebody to do a work for us\n\t\t */\n\t\tWARN_ON_ONCE(current->flags & PF_MEMALLOC);\n\n\t\t/*\n\t\t * non failing costly orders are a hard requirement which we\n\t\t * are not prepared for much so let's warn about these users\n\t\t * so that we can identify them and convert them to something\n\t\t * else.\n\t\t */\n\t\tWARN_ON_ONCE(order > PAGE_ALLOC_COSTLY_ORDER);\n\n\t\t/*\n\t\t * Help non-failing allocations by giving them access to memory\n\t\t * reserves but do not use ALLOC_NO_WATERMARKS because this\n\t\t * could deplete whole memory reserves which would just make\n\t\t * the situation worse\n\t\t */\n\t\tpage = __alloc_pages_cpuset_fallback(gfp_mask, order, ALLOC_HARDER, ac);\n\t\tif (page)\n\t\t\tgoto got_pg;\n\n\t\tcond_resched();\n\t\tgoto retry;\n\t}\nfail:\n\twarn_alloc(gfp_mask, ac->nodemask,\n\t\t\t\"page allocation failure: order:%u\", order);\ngot_pg:\n\treturn page;\n}\n\nstatic inline bool prepare_alloc_pages(gfp_t gfp_mask, unsigned int order,\n\t\tint preferred_nid, nodemask_t *nodemask,\n\t\tstruct alloc_context *ac, gfp_t *alloc_gfp,\n\t\tunsigned int *alloc_flags)\n{\n\tac->highest_zoneidx = gfp_zone(gfp_mask);\n\tac->zonelist = node_zonelist(preferred_nid, gfp_mask);\n\tac->nodemask = nodemask;\n\tac->migratetype = gfp_migratetype(gfp_mask);\n\n\tif (cpusets_enabled()) {\n\t\t*alloc_gfp |= __GFP_HARDWALL;\n\t\t/*\n\t\t * When we are in the interrupt context, it is irrelevant\n\t\t * to the current task context. It means that any node ok.\n\t\t */\n\t\tif (!in_interrupt() && !ac->nodemask)\n\t\t\tac->nodemask = &cpuset_current_mems_allowed;\n\t\telse\n\t\t\t*alloc_flags |= ALLOC_CPUSET;\n\t}\n\n\tfs_reclaim_acquire(gfp_mask);\n\tfs_reclaim_release(gfp_mask);\n\n\tmight_sleep_if(gfp_mask & __GFP_DIRECT_RECLAIM);\n\n\tif (should_fail_alloc_page(gfp_mask, order))\n\t\treturn false;\n\n\t*alloc_flags = gfp_to_alloc_flags_cma(gfp_mask, *alloc_flags);\n\n\t/* Dirty zone balancing only done in the fast path */\n\tac->spread_dirty_pages = (gfp_mask & __GFP_WRITE);\n\n\t/*\n\t * The preferred zone is used for statistics but crucially it is\n\t * also used as the starting point for the zonelist iterator. It\n\t * may get reset for allocations that ignore memory policies.\n\t */\n\tac->preferred_zoneref = first_zones_zonelist(ac->zonelist,\n\t\t\t\t\tac->highest_zoneidx, ac->nodemask);\n\n\treturn true;\n}\n\n/*\n * This is the 'heart' of the zoned buddy allocator.\n */\nstruct page *__alloc_pages(gfp_t gfp, unsigned int order, int preferred_nid,\n\t\t\t\t\t\t\tnodemask_t *nodemask)\n{\n\tstruct page *page;\n\tunsigned int alloc_flags = ALLOC_WMARK_LOW;\n\tgfp_t alloc_gfp; /* The gfp_t that was actually used for allocation */\n\tstruct alloc_context ac = { };\n\n\t/*\n\t * There are several places where we assume that the order value is sane\n\t * so bail out early if the request is out of bound.\n\t */\n\tif (unlikely(order >= MAX_ORDER)) {\n\t\tWARN_ON_ONCE(!(gfp & __GFP_NOWARN));\n\t\treturn NULL;\n\t}\n\n\tgfp &= gfp_allowed_mask;\n\t/*\n\t * Apply scoped allocation constraints. This is mainly about GFP_NOFS\n\t * resp. GFP_NOIO which has to be inherited for all allocation requests\n\t * from a particular context which has been marked by\n\t * memalloc_no{fs,io}_{save,restore}. And PF_MEMALLOC_PIN which ensures\n\t * movable zones are not used during allocation.\n\t */\n\tgfp = current_gfp_context(gfp);\n\talloc_gfp = gfp;\n\tif (!prepare_alloc_pages(gfp, order, preferred_nid, nodemask, &ac,\n\t\t\t&alloc_gfp, &alloc_flags))\n\t\treturn NULL;\n\n\t/*\n\t * Forbid the first pass from falling back to types that fragment\n\t * memory until all local zones are considered.\n\t */\n\talloc_flags |= alloc_flags_nofragment(ac.preferred_zoneref->zone, gfp);\n\n\t/* First allocation attempt */\n\tpage = get_page_from_freelist(alloc_gfp, order, alloc_flags, &ac);\n\tif (likely(page))\n\t\tgoto out;\n\n\talloc_gfp = gfp;\n\tac.spread_dirty_pages = false;\n\n\t/*\n\t * Restore the original nodemask if it was potentially replaced with\n\t * &cpuset_current_mems_allowed to optimize the fast-path attempt.\n\t */\n\tac.nodemask = nodemask;\n\n\tpage = __alloc_pages_slowpath(alloc_gfp, order, &ac);\n\nout:\n\tif (memcg_kmem_enabled() && (gfp & __GFP_ACCOUNT) && page &&\n\t    unlikely(__memcg_kmem_charge_page(page, gfp, order) != 0)) {\n\t\t__free_pages(page, order);\n\t\tpage = NULL;\n\t}\n\n\ttrace_mm_page_alloc(page, order, alloc_gfp, ac.migratetype);\n\n\treturn page;\n}\nEXPORT_SYMBOL(__alloc_pages);\n\n/*\n * Common helper functions. Never use with __GFP_HIGHMEM because the returned\n * address cannot represent highmem pages. Use alloc_pages and then kmap if\n * you need to access high mem.\n */\nunsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order)\n{\n\tstruct page *page;\n\n\tpage = alloc_pages(gfp_mask & ~__GFP_HIGHMEM, order);\n\tif (!page)\n\t\treturn 0;\n\treturn (unsigned long) page_address(page);\n}\nEXPORT_SYMBOL(__get_free_pages);\n\nunsigned long get_zeroed_page(gfp_t gfp_mask)\n{\n\treturn __get_free_pages(gfp_mask | __GFP_ZERO, 0);\n}\nEXPORT_SYMBOL(get_zeroed_page);\n\nstatic inline void free_the_page(struct page *page, unsigned int order)\n{\n\tif (order == 0)\t\t/* Via pcp? */\n\t\tfree_unref_page(page);\n\telse\n\t\t__free_pages_ok(page, order, FPI_NONE);\n}\n\n/**\n * __free_pages - Free pages allocated with alloc_pages().\n * @page: The page pointer returned from alloc_pages().\n * @order: The order of the allocation.\n *\n * This function can free multi-page allocations that are not compound\n * pages.  It does not check that the @order passed in matches that of\n * the allocation, so it is easy to leak memory.  Freeing more memory\n * than was allocated will probably emit a warning.\n *\n * If the last reference to this page is speculative, it will be released\n * by put_page() which only frees the first page of a non-compound\n * allocation.  To prevent the remaining pages from being leaked, we free\n * the subsequent pages here.  If you want to use the page's reference\n * count to decide when to free the allocation, you should allocate a\n * compound page, and use put_page() instead of __free_pages().\n *\n * Context: May be called in interrupt context or while holding a normal\n * spinlock, but not in NMI context or while holding a raw spinlock.\n */\nvoid __free_pages(struct page *page, unsigned int order)\n{\n\tif (put_page_testzero(page))\n\t\tfree_the_page(page, order);\n\telse if (!PageHead(page))\n\t\twhile (order-- > 0)\n\t\t\tfree_the_page(page + (1 << order), order);\n}\nEXPORT_SYMBOL(__free_pages);\n\nvoid free_pages(unsigned long addr, unsigned int order)\n{\n\tif (addr != 0) {\n\t\tVM_BUG_ON(!virt_addr_valid((void *)addr));\n\t\t__free_pages(virt_to_page((void *)addr), order);\n\t}\n}\n\nEXPORT_SYMBOL(free_pages);\n\n/*\n * Page Fragment:\n *  An arbitrary-length arbitrary-offset area of memory which resides\n *  within a 0 or higher order page.  Multiple fragments within that page\n *  are individually refcounted, in the page's reference counter.\n *\n * The page_frag functions below provide a simple allocation framework for\n * page fragments.  This is used by the network stack and network device\n * drivers to provide a backing region of memory for use as either an\n * sk_buff->head, or to be used in the \"frags\" portion of skb_shared_info.\n */\nstatic struct page *__page_frag_cache_refill(struct page_frag_cache *nc,\n\t\t\t\t\t     gfp_t gfp_mask)\n{\n\tstruct page *page = NULL;\n\tgfp_t gfp = gfp_mask;\n\n#if (PAGE_SIZE < PAGE_FRAG_CACHE_MAX_SIZE)\n\tgfp_mask |= __GFP_COMP | __GFP_NOWARN | __GFP_NORETRY |\n\t\t    __GFP_NOMEMALLOC;\n\tpage = alloc_pages_node(NUMA_NO_NODE, gfp_mask,\n\t\t\t\tPAGE_FRAG_CACHE_MAX_ORDER);\n\tnc->size = page ? PAGE_FRAG_CACHE_MAX_SIZE : PAGE_SIZE;\n#endif\n\tif (unlikely(!page))\n\t\tpage = alloc_pages_node(NUMA_NO_NODE, gfp, 0);\n\n\tnc->va = page ? page_address(page) : NULL;\n\n\treturn page;\n}\n\nvoid __page_frag_cache_drain(struct page *page, unsigned int count)\n{\n\tVM_BUG_ON_PAGE(page_ref_count(page) == 0, page);\n\n\tif (page_ref_sub_and_test(page, count))\n\t\tfree_the_page(page, compound_order(page));\n}\nEXPORT_SYMBOL(__page_frag_cache_drain);\n\nvoid *page_frag_alloc_align(struct page_frag_cache *nc,\n\t\t      unsigned int fragsz, gfp_t gfp_mask,\n\t\t      unsigned int align_mask)\n{\n\tunsigned int size = PAGE_SIZE;\n\tstruct page *page;\n\tint offset;\n\n\tif (unlikely(!nc->va)) {\nrefill:\n\t\tpage = __page_frag_cache_refill(nc, gfp_mask);\n\t\tif (!page)\n\t\t\treturn NULL;\n\n#if (PAGE_SIZE < PAGE_FRAG_CACHE_MAX_SIZE)\n\t\t/* if size can vary use size else just use PAGE_SIZE */\n\t\tsize = nc->size;\n#endif\n\t\t/* Even if we own the page, we do not use atomic_set().\n\t\t * This would break get_page_unless_zero() users.\n\t\t */\n\t\tpage_ref_add(page, PAGE_FRAG_CACHE_MAX_SIZE);\n\n\t\t/* reset page count bias and offset to start of new frag */\n\t\tnc->pfmemalloc = page_is_pfmemalloc(page);\n\t\tnc->pagecnt_bias = PAGE_FRAG_CACHE_MAX_SIZE + 1;\n\t\tnc->offset = size;\n\t}\n\n\toffset = nc->offset - fragsz;\n\tif (unlikely(offset < 0)) {\n\t\tpage = virt_to_page(nc->va);\n\n\t\tif (!page_ref_sub_and_test(page, nc->pagecnt_bias))\n\t\t\tgoto refill;\n\n\t\tif (unlikely(nc->pfmemalloc)) {\n\t\t\tfree_the_page(page, compound_order(page));\n\t\t\tgoto refill;\n\t\t}\n\n#if (PAGE_SIZE < PAGE_FRAG_CACHE_MAX_SIZE)\n\t\t/* if size can vary use size else just use PAGE_SIZE */\n\t\tsize = nc->size;\n#endif\n\t\t/* OK, page count is 0, we can safely set it */\n\t\tset_page_count(page, PAGE_FRAG_CACHE_MAX_SIZE + 1);\n\n\t\t/* reset page count bias and offset to start of new frag */\n\t\tnc->pagecnt_bias = PAGE_FRAG_CACHE_MAX_SIZE + 1;\n\t\toffset = size - fragsz;\n\t}\n\n\tnc->pagecnt_bias--;\n\toffset &= align_mask;\n\tnc->offset = offset;\n\n\treturn nc->va + offset;\n}\nEXPORT_SYMBOL(page_frag_alloc_align);\n\n/*\n * Frees a page fragment allocated out of either a compound or order 0 page.\n */\nvoid page_frag_free(void *addr)\n{\n\tstruct page *page = virt_to_head_page(addr);\n\n\tif (unlikely(put_page_testzero(page)))\n\t\tfree_the_page(page, compound_order(page));\n}\nEXPORT_SYMBOL(page_frag_free);\n\nstatic void *make_alloc_exact(unsigned long addr, unsigned int order,\n\t\tsize_t size)\n{\n\tif (addr) {\n\t\tunsigned long alloc_end = addr + (PAGE_SIZE << order);\n\t\tunsigned long used = addr + PAGE_ALIGN(size);\n\n\t\tsplit_page(virt_to_page((void *)addr), order);\n\t\twhile (used < alloc_end) {\n\t\t\tfree_page(used);\n\t\t\tused += PAGE_SIZE;\n\t\t}\n\t}\n\treturn (void *)addr;\n}\n\n/**\n * alloc_pages_exact - allocate an exact number physically-contiguous pages.\n * @size: the number of bytes to allocate\n * @gfp_mask: GFP flags for the allocation, must not contain __GFP_COMP\n *\n * This function is similar to alloc_pages(), except that it allocates the\n * minimum number of pages to satisfy the request.  alloc_pages() can only\n * allocate memory in power-of-two pages.\n *\n * This function is also limited by MAX_ORDER.\n *\n * Memory allocated by this function must be released by free_pages_exact().\n *\n * Return: pointer to the allocated area or %NULL in case of error.\n */\nvoid *alloc_pages_exact(size_t size, gfp_t gfp_mask)\n{\n\tunsigned int order = get_order(size);\n\tunsigned long addr;\n\n\tif (WARN_ON_ONCE(gfp_mask & __GFP_COMP))\n\t\tgfp_mask &= ~__GFP_COMP;\n\n\taddr = __get_free_pages(gfp_mask, order);\n\treturn make_alloc_exact(addr, order, size);\n}\nEXPORT_SYMBOL(alloc_pages_exact);\n\n/**\n * alloc_pages_exact_nid - allocate an exact number of physically-contiguous\n *\t\t\t   pages on a node.\n * @nid: the preferred node ID where memory should be allocated\n * @size: the number of bytes to allocate\n * @gfp_mask: GFP flags for the allocation, must not contain __GFP_COMP\n *\n * Like alloc_pages_exact(), but try to allocate on node nid first before falling\n * back.\n *\n * Return: pointer to the allocated area or %NULL in case of error.\n */\nvoid * __meminit alloc_pages_exact_nid(int nid, size_t size, gfp_t gfp_mask)\n{\n\tunsigned int order = get_order(size);\n\tstruct page *p;\n\n\tif (WARN_ON_ONCE(gfp_mask & __GFP_COMP))\n\t\tgfp_mask &= ~__GFP_COMP;\n\n\tp = alloc_pages_node(nid, gfp_mask, order);\n\tif (!p)\n\t\treturn NULL;\n\treturn make_alloc_exact((unsigned long)page_address(p), order, size);\n}\n\n/**\n * free_pages_exact - release memory allocated via alloc_pages_exact()\n * @virt: the value returned by alloc_pages_exact.\n * @size: size of allocation, same value as passed to alloc_pages_exact().\n *\n * Release the memory allocated by a previous call to alloc_pages_exact.\n */\nvoid free_pages_exact(void *virt, size_t size)\n{\n\tunsigned long addr = (unsigned long)virt;\n\tunsigned long end = addr + PAGE_ALIGN(size);\n\n\twhile (addr < end) {\n\t\tfree_page(addr);\n\t\taddr += PAGE_SIZE;\n\t}\n}\nEXPORT_SYMBOL(free_pages_exact);\n\n/**\n * nr_free_zone_pages - count number of pages beyond high watermark\n * @offset: The zone index of the highest zone\n *\n * nr_free_zone_pages() counts the number of pages which are beyond the\n * high watermark within all zones at or below a given zone index.  For each\n * zone, the number of pages is calculated as:\n *\n *     nr_free_zone_pages = managed_pages - high_pages\n *\n * Return: number of pages beyond high watermark.\n */\nstatic unsigned long nr_free_zone_pages(int offset)\n{\n\tstruct zoneref *z;\n\tstruct zone *zone;\n\n\t/* Just pick one node, since fallback list is circular */\n\tunsigned long sum = 0;\n\n\tstruct zonelist *zonelist = node_zonelist(numa_node_id(), GFP_KERNEL);\n\n\tfor_each_zone_zonelist(zone, z, zonelist, offset) {\n\t\tunsigned long size = zone_managed_pages(zone);\n\t\tunsigned long high = high_wmark_pages(zone);\n\t\tif (size > high)\n\t\t\tsum += size - high;\n\t}\n\n\treturn sum;\n}\n\n/**\n * nr_free_buffer_pages - count number of pages beyond high watermark\n *\n * nr_free_buffer_pages() counts the number of pages which are beyond the high\n * watermark within ZONE_DMA and ZONE_NORMAL.\n *\n * Return: number of pages beyond high watermark within ZONE_DMA and\n * ZONE_NORMAL.\n */\nunsigned long nr_free_buffer_pages(void)\n{\n\treturn nr_free_zone_pages(gfp_zone(GFP_USER));\n}\nEXPORT_SYMBOL_GPL(nr_free_buffer_pages);\n\nstatic inline void show_node(struct zone *zone)\n{\n\tif (IS_ENABLED(CONFIG_NUMA))\n\t\tprintk(\"Node %d \", zone_to_nid(zone));\n}\n\nlong si_mem_available(void)\n{\n\tlong available;\n\tunsigned long pagecache;\n\tunsigned long wmark_low = 0;\n\tunsigned long pages[NR_LRU_LISTS];\n\tunsigned long reclaimable;\n\tstruct zone *zone;\n\tint lru;\n\n\tfor (lru = LRU_BASE; lru < NR_LRU_LISTS; lru++)\n\t\tpages[lru] = global_node_page_state(NR_LRU_BASE + lru);\n\n\tfor_each_zone(zone)\n\t\twmark_low += low_wmark_pages(zone);\n\n\t/*\n\t * Estimate the amount of memory available for userspace allocations,\n\t * without causing swapping.\n\t */\n\tavailable = global_zone_page_state(NR_FREE_PAGES) - totalreserve_pages;\n\n\t/*\n\t * Not all the page cache can be freed, otherwise the system will\n\t * start swapping. Assume at least half of the page cache, or the\n\t * low watermark worth of cache, needs to stay.\n\t */\n\tpagecache = pages[LRU_ACTIVE_FILE] + pages[LRU_INACTIVE_FILE];\n\tpagecache -= min(pagecache / 2, wmark_low);\n\tavailable += pagecache;\n\n\t/*\n\t * Part of the reclaimable slab and other kernel memory consists of\n\t * items that are in use, and cannot be freed. Cap this estimate at the\n\t * low watermark.\n\t */\n\treclaimable = global_node_page_state_pages(NR_SLAB_RECLAIMABLE_B) +\n\t\tglobal_node_page_state(NR_KERNEL_MISC_RECLAIMABLE);\n\tavailable += reclaimable - min(reclaimable / 2, wmark_low);\n\n\tif (available < 0)\n\t\tavailable = 0;\n\treturn available;\n}\nEXPORT_SYMBOL_GPL(si_mem_available);\n\nvoid si_meminfo(struct sysinfo *val)\n{\n\tval->totalram = totalram_pages();\n\tval->sharedram = global_node_page_state(NR_SHMEM);\n\tval->freeram = global_zone_page_state(NR_FREE_PAGES);\n\tval->bufferram = nr_blockdev_pages();\n\tval->totalhigh = totalhigh_pages();\n\tval->freehigh = nr_free_highpages();\n\tval->mem_unit = PAGE_SIZE;\n}\n\nEXPORT_SYMBOL(si_meminfo);\n\n#ifdef CONFIG_NUMA\nvoid si_meminfo_node(struct sysinfo *val, int nid)\n{\n\tint zone_type;\t\t/* needs to be signed */\n\tunsigned long managed_pages = 0;\n\tunsigned long managed_highpages = 0;\n\tunsigned long free_highpages = 0;\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\tfor (zone_type = 0; zone_type < MAX_NR_ZONES; zone_type++)\n\t\tmanaged_pages += zone_managed_pages(&pgdat->node_zones[zone_type]);\n\tval->totalram = managed_pages;\n\tval->sharedram = node_page_state(pgdat, NR_SHMEM);\n\tval->freeram = sum_zone_node_page_state(nid, NR_FREE_PAGES);\n#ifdef CONFIG_HIGHMEM\n\tfor (zone_type = 0; zone_type < MAX_NR_ZONES; zone_type++) {\n\t\tstruct zone *zone = &pgdat->node_zones[zone_type];\n\n\t\tif (is_highmem(zone)) {\n\t\t\tmanaged_highpages += zone_managed_pages(zone);\n\t\t\tfree_highpages += zone_page_state(zone, NR_FREE_PAGES);\n\t\t}\n\t}\n\tval->totalhigh = managed_highpages;\n\tval->freehigh = free_highpages;\n#else\n\tval->totalhigh = managed_highpages;\n\tval->freehigh = free_highpages;\n#endif\n\tval->mem_unit = PAGE_SIZE;\n}\n#endif\n\n/*\n * Determine whether the node should be displayed or not, depending on whether\n * SHOW_MEM_FILTER_NODES was passed to show_free_areas().\n */\nstatic bool show_mem_node_skip(unsigned int flags, int nid, nodemask_t *nodemask)\n{\n\tif (!(flags & SHOW_MEM_FILTER_NODES))\n\t\treturn false;\n\n\t/*\n\t * no node mask - aka implicit memory numa policy. Do not bother with\n\t * the synchronization - read_mems_allowed_begin - because we do not\n\t * have to be precise here.\n\t */\n\tif (!nodemask)\n\t\tnodemask = &cpuset_current_mems_allowed;\n\n\treturn !node_isset(nid, *nodemask);\n}\n\n#define K(x) ((x) << (PAGE_SHIFT-10))\n\nstatic void show_migration_types(unsigned char type)\n{\n\tstatic const char types[MIGRATE_TYPES] = {\n\t\t[MIGRATE_UNMOVABLE]\t= 'U',\n\t\t[MIGRATE_MOVABLE]\t= 'M',\n\t\t[MIGRATE_RECLAIMABLE]\t= 'E',\n\t\t[MIGRATE_HIGHATOMIC]\t= 'H',\n#ifdef CONFIG_CMA\n\t\t[MIGRATE_CMA]\t\t= 'C',\n#endif\n#ifdef CONFIG_MEMORY_ISOLATION\n\t\t[MIGRATE_ISOLATE]\t= 'I',\n#endif\n\t};\n\tchar tmp[MIGRATE_TYPES + 1];\n\tchar *p = tmp;\n\tint i;\n\n\tfor (i = 0; i < MIGRATE_TYPES; i++) {\n\t\tif (type & (1 << i))\n\t\t\t*p++ = types[i];\n\t}\n\n\t*p = '\\0';\n\tprintk(KERN_CONT \"(%s) \", tmp);\n}\n\n/*\n * Show free area list (used inside shift_scroll-lock stuff)\n * We also calculate the percentage fragmentation. We do this by counting the\n * memory on each free list with the exception of the first item on the list.\n *\n * Bits in @filter:\n * SHOW_MEM_FILTER_NODES: suppress nodes that are not allowed by current's\n *   cpuset.\n */\nvoid show_free_areas(unsigned int filter, nodemask_t *nodemask)\n{\n\tunsigned long free_pcp = 0;\n\tint cpu;\n\tstruct zone *zone;\n\tpg_data_t *pgdat;\n\n\tfor_each_populated_zone(zone) {\n\t\tif (show_mem_node_skip(filter, zone_to_nid(zone), nodemask))\n\t\t\tcontinue;\n\n\t\tfor_each_online_cpu(cpu)\n\t\t\tfree_pcp += per_cpu_ptr(zone->pageset, cpu)->pcp.count;\n\t}\n\n\tprintk(\"active_anon:%lu inactive_anon:%lu isolated_anon:%lu\\n\"\n\t\t\" active_file:%lu inactive_file:%lu isolated_file:%lu\\n\"\n\t\t\" unevictable:%lu dirty:%lu writeback:%lu\\n\"\n\t\t\" slab_reclaimable:%lu slab_unreclaimable:%lu\\n\"\n\t\t\" mapped:%lu shmem:%lu pagetables:%lu bounce:%lu\\n\"\n\t\t\" free:%lu free_pcp:%lu free_cma:%lu\\n\",\n\t\tglobal_node_page_state(NR_ACTIVE_ANON),\n\t\tglobal_node_page_state(NR_INACTIVE_ANON),\n\t\tglobal_node_page_state(NR_ISOLATED_ANON),\n\t\tglobal_node_page_state(NR_ACTIVE_FILE),\n\t\tglobal_node_page_state(NR_INACTIVE_FILE),\n\t\tglobal_node_page_state(NR_ISOLATED_FILE),\n\t\tglobal_node_page_state(NR_UNEVICTABLE),\n\t\tglobal_node_page_state(NR_FILE_DIRTY),\n\t\tglobal_node_page_state(NR_WRITEBACK),\n\t\tglobal_node_page_state_pages(NR_SLAB_RECLAIMABLE_B),\n\t\tglobal_node_page_state_pages(NR_SLAB_UNRECLAIMABLE_B),\n\t\tglobal_node_page_state(NR_FILE_MAPPED),\n\t\tglobal_node_page_state(NR_SHMEM),\n\t\tglobal_node_page_state(NR_PAGETABLE),\n\t\tglobal_zone_page_state(NR_BOUNCE),\n\t\tglobal_zone_page_state(NR_FREE_PAGES),\n\t\tfree_pcp,\n\t\tglobal_zone_page_state(NR_FREE_CMA_PAGES));\n\n\tfor_each_online_pgdat(pgdat) {\n\t\tif (show_mem_node_skip(filter, pgdat->node_id, nodemask))\n\t\t\tcontinue;\n\n\t\tprintk(\"Node %d\"\n\t\t\t\" active_anon:%lukB\"\n\t\t\t\" inactive_anon:%lukB\"\n\t\t\t\" active_file:%lukB\"\n\t\t\t\" inactive_file:%lukB\"\n\t\t\t\" unevictable:%lukB\"\n\t\t\t\" isolated(anon):%lukB\"\n\t\t\t\" isolated(file):%lukB\"\n\t\t\t\" mapped:%lukB\"\n\t\t\t\" dirty:%lukB\"\n\t\t\t\" writeback:%lukB\"\n\t\t\t\" shmem:%lukB\"\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\t\t\" shmem_thp: %lukB\"\n\t\t\t\" shmem_pmdmapped: %lukB\"\n\t\t\t\" anon_thp: %lukB\"\n#endif\n\t\t\t\" writeback_tmp:%lukB\"\n\t\t\t\" kernel_stack:%lukB\"\n#ifdef CONFIG_SHADOW_CALL_STACK\n\t\t\t\" shadow_call_stack:%lukB\"\n#endif\n\t\t\t\" pagetables:%lukB\"\n\t\t\t\" all_unreclaimable? %s\"\n\t\t\t\"\\n\",\n\t\t\tpgdat->node_id,\n\t\t\tK(node_page_state(pgdat, NR_ACTIVE_ANON)),\n\t\t\tK(node_page_state(pgdat, NR_INACTIVE_ANON)),\n\t\t\tK(node_page_state(pgdat, NR_ACTIVE_FILE)),\n\t\t\tK(node_page_state(pgdat, NR_INACTIVE_FILE)),\n\t\t\tK(node_page_state(pgdat, NR_UNEVICTABLE)),\n\t\t\tK(node_page_state(pgdat, NR_ISOLATED_ANON)),\n\t\t\tK(node_page_state(pgdat, NR_ISOLATED_FILE)),\n\t\t\tK(node_page_state(pgdat, NR_FILE_MAPPED)),\n\t\t\tK(node_page_state(pgdat, NR_FILE_DIRTY)),\n\t\t\tK(node_page_state(pgdat, NR_WRITEBACK)),\n\t\t\tK(node_page_state(pgdat, NR_SHMEM)),\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\t\tK(node_page_state(pgdat, NR_SHMEM_THPS)),\n\t\t\tK(node_page_state(pgdat, NR_SHMEM_PMDMAPPED)),\n\t\t\tK(node_page_state(pgdat, NR_ANON_THPS)),\n#endif\n\t\t\tK(node_page_state(pgdat, NR_WRITEBACK_TEMP)),\n\t\t\tnode_page_state(pgdat, NR_KERNEL_STACK_KB),\n#ifdef CONFIG_SHADOW_CALL_STACK\n\t\t\tnode_page_state(pgdat, NR_KERNEL_SCS_KB),\n#endif\n\t\t\tK(node_page_state(pgdat, NR_PAGETABLE)),\n\t\t\tpgdat->kswapd_failures >= MAX_RECLAIM_RETRIES ?\n\t\t\t\t\"yes\" : \"no\");\n\t}\n\n\tfor_each_populated_zone(zone) {\n\t\tint i;\n\n\t\tif (show_mem_node_skip(filter, zone_to_nid(zone), nodemask))\n\t\t\tcontinue;\n\n\t\tfree_pcp = 0;\n\t\tfor_each_online_cpu(cpu)\n\t\t\tfree_pcp += per_cpu_ptr(zone->pageset, cpu)->pcp.count;\n\n\t\tshow_node(zone);\n\t\tprintk(KERN_CONT\n\t\t\t\"%s\"\n\t\t\t\" free:%lukB\"\n\t\t\t\" min:%lukB\"\n\t\t\t\" low:%lukB\"\n\t\t\t\" high:%lukB\"\n\t\t\t\" reserved_highatomic:%luKB\"\n\t\t\t\" active_anon:%lukB\"\n\t\t\t\" inactive_anon:%lukB\"\n\t\t\t\" active_file:%lukB\"\n\t\t\t\" inactive_file:%lukB\"\n\t\t\t\" unevictable:%lukB\"\n\t\t\t\" writepending:%lukB\"\n\t\t\t\" present:%lukB\"\n\t\t\t\" managed:%lukB\"\n\t\t\t\" mlocked:%lukB\"\n\t\t\t\" bounce:%lukB\"\n\t\t\t\" free_pcp:%lukB\"\n\t\t\t\" local_pcp:%ukB\"\n\t\t\t\" free_cma:%lukB\"\n\t\t\t\"\\n\",\n\t\t\tzone->name,\n\t\t\tK(zone_page_state(zone, NR_FREE_PAGES)),\n\t\t\tK(min_wmark_pages(zone)),\n\t\t\tK(low_wmark_pages(zone)),\n\t\t\tK(high_wmark_pages(zone)),\n\t\t\tK(zone->nr_reserved_highatomic),\n\t\t\tK(zone_page_state(zone, NR_ZONE_ACTIVE_ANON)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_INACTIVE_ANON)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_ACTIVE_FILE)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_INACTIVE_FILE)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_UNEVICTABLE)),\n\t\t\tK(zone_page_state(zone, NR_ZONE_WRITE_PENDING)),\n\t\t\tK(zone->present_pages),\n\t\t\tK(zone_managed_pages(zone)),\n\t\t\tK(zone_page_state(zone, NR_MLOCK)),\n\t\t\tK(zone_page_state(zone, NR_BOUNCE)),\n\t\t\tK(free_pcp),\n\t\t\tK(this_cpu_read(zone->pageset->pcp.count)),\n\t\t\tK(zone_page_state(zone, NR_FREE_CMA_PAGES)));\n\t\tprintk(\"lowmem_reserve[]:\");\n\t\tfor (i = 0; i < MAX_NR_ZONES; i++)\n\t\t\tprintk(KERN_CONT \" %ld\", zone->lowmem_reserve[i]);\n\t\tprintk(KERN_CONT \"\\n\");\n\t}\n\n\tfor_each_populated_zone(zone) {\n\t\tunsigned int order;\n\t\tunsigned long nr[MAX_ORDER], flags, total = 0;\n\t\tunsigned char types[MAX_ORDER];\n\n\t\tif (show_mem_node_skip(filter, zone_to_nid(zone), nodemask))\n\t\t\tcontinue;\n\t\tshow_node(zone);\n\t\tprintk(KERN_CONT \"%s: \", zone->name);\n\n\t\tspin_lock_irqsave(&zone->lock, flags);\n\t\tfor (order = 0; order < MAX_ORDER; order++) {\n\t\t\tstruct free_area *area = &zone->free_area[order];\n\t\t\tint type;\n\n\t\t\tnr[order] = area->nr_free;\n\t\t\ttotal += nr[order] << order;\n\n\t\t\ttypes[order] = 0;\n\t\t\tfor (type = 0; type < MIGRATE_TYPES; type++) {\n\t\t\t\tif (!free_area_empty(area, type))\n\t\t\t\t\ttypes[order] |= 1 << type;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t\tfor (order = 0; order < MAX_ORDER; order++) {\n\t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n\t\t\t       nr[order], K(1UL) << order);\n\t\t\tif (nr[order])\n\t\t\t\tshow_migration_types(types[order]);\n\t\t}\n\t\tprintk(KERN_CONT \"= %lukB\\n\", K(total));\n\t}\n\n\thugetlb_show_meminfo();\n\n\tprintk(\"%ld total pagecache pages\\n\", global_node_page_state(NR_FILE_PAGES));\n\n\tshow_swap_cache_info();\n}\n\nstatic void zoneref_set_zone(struct zone *zone, struct zoneref *zoneref)\n{\n\tzoneref->zone = zone;\n\tzoneref->zone_idx = zone_idx(zone);\n}\n\n/*\n * Builds allocation fallback zone lists.\n *\n * Add all populated zones of a node to the zonelist.\n */\nstatic int build_zonerefs_node(pg_data_t *pgdat, struct zoneref *zonerefs)\n{\n\tstruct zone *zone;\n\tenum zone_type zone_type = MAX_NR_ZONES;\n\tint nr_zones = 0;\n\n\tdo {\n\t\tzone_type--;\n\t\tzone = pgdat->node_zones + zone_type;\n\t\tif (managed_zone(zone)) {\n\t\t\tzoneref_set_zone(zone, &zonerefs[nr_zones++]);\n\t\t\tcheck_highest_zone(zone_type);\n\t\t}\n\t} while (zone_type);\n\n\treturn nr_zones;\n}\n\n#ifdef CONFIG_NUMA\n\nstatic int __parse_numa_zonelist_order(char *s)\n{\n\t/*\n\t * We used to support different zonlists modes but they turned\n\t * out to be just not useful. Let's keep the warning in place\n\t * if somebody still use the cmd line parameter so that we do\n\t * not fail it silently\n\t */\n\tif (!(*s == 'd' || *s == 'D' || *s == 'n' || *s == 'N')) {\n\t\tpr_warn(\"Ignoring unsupported numa_zonelist_order value:  %s\\n\", s);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nchar numa_zonelist_order[] = \"Node\";\n\n/*\n * sysctl handler for numa_zonelist_order\n */\nint numa_zonelist_order_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tif (write)\n\t\treturn __parse_numa_zonelist_order(buffer);\n\treturn proc_dostring(table, write, buffer, length, ppos);\n}\n\n\n#define MAX_NODE_LOAD (nr_online_nodes)\nstatic int node_load[MAX_NUMNODES];\n\n/**\n * find_next_best_node - find the next node that should appear in a given node's fallback list\n * @node: node whose fallback list we're appending\n * @used_node_mask: nodemask_t of already used nodes\n *\n * We use a number of factors to determine which is the next node that should\n * appear on a given node's fallback list.  The node should not have appeared\n * already in @node's fallback list, and it should be the next closest node\n * according to the distance array (which contains arbitrary distance values\n * from each node to each node in the system), and should also prefer nodes\n * with no CPUs, since presumably they'll have very little allocation pressure\n * on them otherwise.\n *\n * Return: node id of the found node or %NUMA_NO_NODE if no node is found.\n */\nstatic int find_next_best_node(int node, nodemask_t *used_node_mask)\n{\n\tint n, val;\n\tint min_val = INT_MAX;\n\tint best_node = NUMA_NO_NODE;\n\n\t/* Use the local node if we haven't already */\n\tif (!node_isset(node, *used_node_mask)) {\n\t\tnode_set(node, *used_node_mask);\n\t\treturn node;\n\t}\n\n\tfor_each_node_state(n, N_MEMORY) {\n\n\t\t/* Don't want a node to appear more than once */\n\t\tif (node_isset(n, *used_node_mask))\n\t\t\tcontinue;\n\n\t\t/* Use the distance array to find the distance */\n\t\tval = node_distance(node, n);\n\n\t\t/* Penalize nodes under us (\"prefer the next node\") */\n\t\tval += (n < node);\n\n\t\t/* Give preference to headless and unused nodes */\n\t\tif (!cpumask_empty(cpumask_of_node(n)))\n\t\t\tval += PENALTY_FOR_NODE_WITH_CPUS;\n\n\t\t/* Slight preference for less loaded node */\n\t\tval *= (MAX_NODE_LOAD*MAX_NUMNODES);\n\t\tval += node_load[n];\n\n\t\tif (val < min_val) {\n\t\t\tmin_val = val;\n\t\t\tbest_node = n;\n\t\t}\n\t}\n\n\tif (best_node >= 0)\n\t\tnode_set(best_node, *used_node_mask);\n\n\treturn best_node;\n}\n\n\n/*\n * Build zonelists ordered by node and zones within node.\n * This results in maximum locality--normal zone overflows into local\n * DMA zone, if any--but risks exhausting DMA zone.\n */\nstatic void build_zonelists_in_node_order(pg_data_t *pgdat, int *node_order,\n\t\tunsigned nr_nodes)\n{\n\tstruct zoneref *zonerefs;\n\tint i;\n\n\tzonerefs = pgdat->node_zonelists[ZONELIST_FALLBACK]._zonerefs;\n\n\tfor (i = 0; i < nr_nodes; i++) {\n\t\tint nr_zones;\n\n\t\tpg_data_t *node = NODE_DATA(node_order[i]);\n\n\t\tnr_zones = build_zonerefs_node(node, zonerefs);\n\t\tzonerefs += nr_zones;\n\t}\n\tzonerefs->zone = NULL;\n\tzonerefs->zone_idx = 0;\n}\n\n/*\n * Build gfp_thisnode zonelists\n */\nstatic void build_thisnode_zonelists(pg_data_t *pgdat)\n{\n\tstruct zoneref *zonerefs;\n\tint nr_zones;\n\n\tzonerefs = pgdat->node_zonelists[ZONELIST_NOFALLBACK]._zonerefs;\n\tnr_zones = build_zonerefs_node(pgdat, zonerefs);\n\tzonerefs += nr_zones;\n\tzonerefs->zone = NULL;\n\tzonerefs->zone_idx = 0;\n}\n\n/*\n * Build zonelists ordered by zone and nodes within zones.\n * This results in conserving DMA zone[s] until all Normal memory is\n * exhausted, but results in overflowing to remote node while memory\n * may still exist in local DMA zone.\n */\n\nstatic void build_zonelists(pg_data_t *pgdat)\n{\n\tstatic int node_order[MAX_NUMNODES];\n\tint node, load, nr_nodes = 0;\n\tnodemask_t used_mask = NODE_MASK_NONE;\n\tint local_node, prev_node;\n\n\t/* NUMA-aware ordering of nodes */\n\tlocal_node = pgdat->node_id;\n\tload = nr_online_nodes;\n\tprev_node = local_node;\n\n\tmemset(node_order, 0, sizeof(node_order));\n\twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0) {\n\t\t/*\n\t\t * We don't want to pressure a particular node.\n\t\t * So adding penalty to the first node in same\n\t\t * distance group to make it round-robin.\n\t\t */\n\t\tif (node_distance(local_node, node) !=\n\t\t    node_distance(local_node, prev_node))\n\t\t\tnode_load[node] = load;\n\n\t\tnode_order[nr_nodes++] = node;\n\t\tprev_node = node;\n\t\tload--;\n\t}\n\n\tbuild_zonelists_in_node_order(pgdat, node_order, nr_nodes);\n\tbuild_thisnode_zonelists(pgdat);\n}\n\n#ifdef CONFIG_HAVE_MEMORYLESS_NODES\n/*\n * Return node id of node used for \"local\" allocations.\n * I.e., first node id of first zone in arg node's generic zonelist.\n * Used for initializing percpu 'numa_mem', which is used primarily\n * for kernel allocations, so use GFP_KERNEL flags to locate zonelist.\n */\nint local_memory_node(int node)\n{\n\tstruct zoneref *z;\n\n\tz = first_zones_zonelist(node_zonelist(node, GFP_KERNEL),\n\t\t\t\t   gfp_zone(GFP_KERNEL),\n\t\t\t\t   NULL);\n\treturn zone_to_nid(z->zone);\n}\n#endif\n\nstatic void setup_min_unmapped_ratio(void);\nstatic void setup_min_slab_ratio(void);\n#else\t/* CONFIG_NUMA */\n\nstatic void build_zonelists(pg_data_t *pgdat)\n{\n\tint node, local_node;\n\tstruct zoneref *zonerefs;\n\tint nr_zones;\n\n\tlocal_node = pgdat->node_id;\n\n\tzonerefs = pgdat->node_zonelists[ZONELIST_FALLBACK]._zonerefs;\n\tnr_zones = build_zonerefs_node(pgdat, zonerefs);\n\tzonerefs += nr_zones;\n\n\t/*\n\t * Now we build the zonelist so that it contains the zones\n\t * of all the other nodes.\n\t * We don't want to pressure a particular node, so when\n\t * building the zones for node N, we make sure that the\n\t * zones coming right after the local ones are those from\n\t * node N+1 (modulo N)\n\t */\n\tfor (node = local_node + 1; node < MAX_NUMNODES; node++) {\n\t\tif (!node_online(node))\n\t\t\tcontinue;\n\t\tnr_zones = build_zonerefs_node(NODE_DATA(node), zonerefs);\n\t\tzonerefs += nr_zones;\n\t}\n\tfor (node = 0; node < local_node; node++) {\n\t\tif (!node_online(node))\n\t\t\tcontinue;\n\t\tnr_zones = build_zonerefs_node(NODE_DATA(node), zonerefs);\n\t\tzonerefs += nr_zones;\n\t}\n\n\tzonerefs->zone = NULL;\n\tzonerefs->zone_idx = 0;\n}\n\n#endif\t/* CONFIG_NUMA */\n\n/*\n * Boot pageset table. One per cpu which is going to be used for all\n * zones and all nodes. The parameters will be set in such a way\n * that an item put on a list will immediately be handed over to\n * the buddy list. This is safe since pageset manipulation is done\n * with interrupts disabled.\n *\n * The boot_pagesets must be kept even after bootup is complete for\n * unused processors and/or zones. They do play a role for bootstrapping\n * hotplugged processors.\n *\n * zoneinfo_show() and maybe other functions do\n * not check if the processor is online before following the pageset pointer.\n * Other parts of the kernel may not check if the zone is available.\n */\nstatic void pageset_init(struct per_cpu_pageset *p);\n/* These effectively disable the pcplists in the boot pageset completely */\n#define BOOT_PAGESET_HIGH\t0\n#define BOOT_PAGESET_BATCH\t1\nstatic DEFINE_PER_CPU(struct per_cpu_pageset, boot_pageset);\nstatic DEFINE_PER_CPU(struct per_cpu_nodestat, boot_nodestats);\n\nstatic void __build_all_zonelists(void *data)\n{\n\tint nid;\n\tint __maybe_unused cpu;\n\tpg_data_t *self = data;\n\tstatic DEFINE_SPINLOCK(lock);\n\n\tspin_lock(&lock);\n\n#ifdef CONFIG_NUMA\n\tmemset(node_load, 0, sizeof(node_load));\n#endif\n\n\t/*\n\t * This node is hotadded and no memory is yet present.   So just\n\t * building zonelists is fine - no need to touch other nodes.\n\t */\n\tif (self && !node_online(self->node_id)) {\n\t\tbuild_zonelists(self);\n\t} else {\n\t\tfor_each_online_node(nid) {\n\t\t\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\t\t\tbuild_zonelists(pgdat);\n\t\t}\n\n#ifdef CONFIG_HAVE_MEMORYLESS_NODES\n\t\t/*\n\t\t * We now know the \"local memory node\" for each node--\n\t\t * i.e., the node of the first zone in the generic zonelist.\n\t\t * Set up numa_mem percpu variable for on-line cpus.  During\n\t\t * boot, only the boot cpu should be on-line;  we'll init the\n\t\t * secondary cpus' numa_mem as they come on-line.  During\n\t\t * node/memory hotplug, we'll fixup all on-line cpus.\n\t\t */\n\t\tfor_each_online_cpu(cpu)\n\t\t\tset_cpu_numa_mem(cpu, local_memory_node(cpu_to_node(cpu)));\n#endif\n\t}\n\n\tspin_unlock(&lock);\n}\n\nstatic noinline void __init\nbuild_all_zonelists_init(void)\n{\n\tint cpu;\n\n\t__build_all_zonelists(NULL);\n\n\t/*\n\t * Initialize the boot_pagesets that are going to be used\n\t * for bootstrapping processors. The real pagesets for\n\t * each zone will be allocated later when the per cpu\n\t * allocator is available.\n\t *\n\t * boot_pagesets are used also for bootstrapping offline\n\t * cpus if the system is already booted because the pagesets\n\t * are needed to initialize allocators on a specific cpu too.\n\t * F.e. the percpu allocator needs the page allocator which\n\t * needs the percpu allocator in order to allocate its pagesets\n\t * (a chicken-egg dilemma).\n\t */\n\tfor_each_possible_cpu(cpu)\n\t\tpageset_init(&per_cpu(boot_pageset, cpu));\n\n\tmminit_verify_zonelist();\n\tcpuset_init_current_mems_allowed();\n}\n\n/*\n * unless system_state == SYSTEM_BOOTING.\n *\n * __ref due to call of __init annotated helper build_all_zonelists_init\n * [protected by SYSTEM_BOOTING].\n */\nvoid __ref build_all_zonelists(pg_data_t *pgdat)\n{\n\tunsigned long vm_total_pages;\n\n\tif (system_state == SYSTEM_BOOTING) {\n\t\tbuild_all_zonelists_init();\n\t} else {\n\t\t__build_all_zonelists(pgdat);\n\t\t/* cpuset refresh routine should be here */\n\t}\n\t/* Get the number of free pages beyond high watermark in all zones. */\n\tvm_total_pages = nr_free_zone_pages(gfp_zone(GFP_HIGHUSER_MOVABLE));\n\t/*\n\t * Disable grouping by mobility if the number of pages in the\n\t * system is too low to allow the mechanism to work. It would be\n\t * more accurate, but expensive to check per-zone. This check is\n\t * made on memory-hotadd so a system can start with mobility\n\t * disabled and enable it later\n\t */\n\tif (vm_total_pages < (pageblock_nr_pages * MIGRATE_TYPES))\n\t\tpage_group_by_mobility_disabled = 1;\n\telse\n\t\tpage_group_by_mobility_disabled = 0;\n\n\tpr_info(\"Built %u zonelists, mobility grouping %s.  Total pages: %ld\\n\",\n\t\tnr_online_nodes,\n\t\tpage_group_by_mobility_disabled ? \"off\" : \"on\",\n\t\tvm_total_pages);\n#ifdef CONFIG_NUMA\n\tpr_info(\"Policy zone: %s\\n\", zone_names[policy_zone]);\n#endif\n}\n\n/* If zone is ZONE_MOVABLE but memory is mirrored, it is an overlapped init */\nstatic bool __meminit\noverlap_memmap_init(unsigned long zone, unsigned long *pfn)\n{\n\tstatic struct memblock_region *r;\n\n\tif (mirrored_kernelcore && zone == ZONE_MOVABLE) {\n\t\tif (!r || *pfn >= memblock_region_memory_end_pfn(r)) {\n\t\t\tfor_each_mem_region(r) {\n\t\t\t\tif (*pfn < memblock_region_memory_end_pfn(r))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (*pfn >= memblock_region_memory_base_pfn(r) &&\n\t\t    memblock_is_mirror(r)) {\n\t\t\t*pfn = memblock_region_memory_end_pfn(r);\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\n/*\n * Initially all pages are reserved - free ones are freed\n * up by memblock_free_all() once the early boot process is\n * done. Non-atomic initialization, single-pass.\n *\n * All aligned pageblocks are initialized to the specified migratetype\n * (usually MIGRATE_MOVABLE). Besides setting the migratetype, no related\n * zone stats (e.g., nr_isolate_pageblock) are touched.\n */\nvoid __meminit memmap_init_range(unsigned long size, int nid, unsigned long zone,\n\t\tunsigned long start_pfn, unsigned long zone_end_pfn,\n\t\tenum meminit_context context,\n\t\tstruct vmem_altmap *altmap, int migratetype)\n{\n\tunsigned long pfn, end_pfn = start_pfn + size;\n\tstruct page *page;\n\n\tif (highest_memmap_pfn < end_pfn - 1)\n\t\thighest_memmap_pfn = end_pfn - 1;\n\n#ifdef CONFIG_ZONE_DEVICE\n\t/*\n\t * Honor reservation requested by the driver for this ZONE_DEVICE\n\t * memory. We limit the total number of pages to initialize to just\n\t * those that might contain the memory mapping. We will defer the\n\t * ZONE_DEVICE page initialization until after we have released\n\t * the hotplug lock.\n\t */\n\tif (zone == ZONE_DEVICE) {\n\t\tif (!altmap)\n\t\t\treturn;\n\n\t\tif (start_pfn == altmap->base_pfn)\n\t\t\tstart_pfn += altmap->reserve;\n\t\tend_pfn = altmap->base_pfn + vmem_altmap_offset(altmap);\n\t}\n#endif\n\n\tfor (pfn = start_pfn; pfn < end_pfn; ) {\n\t\t/*\n\t\t * There can be holes in boot-time mem_map[]s handed to this\n\t\t * function.  They do not exist on hotplugged memory.\n\t\t */\n\t\tif (context == MEMINIT_EARLY) {\n\t\t\tif (overlap_memmap_init(zone, &pfn))\n\t\t\t\tcontinue;\n\t\t\tif (defer_init(nid, pfn, zone_end_pfn))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tpage = pfn_to_page(pfn);\n\t\t__init_single_page(page, pfn, zone, nid);\n\t\tif (context == MEMINIT_HOTPLUG)\n\t\t\t__SetPageReserved(page);\n\n\t\t/*\n\t\t * Usually, we want to mark the pageblock MIGRATE_MOVABLE,\n\t\t * such that unmovable allocations won't be scattered all\n\t\t * over the place during system boot.\n\t\t */\n\t\tif (IS_ALIGNED(pfn, pageblock_nr_pages)) {\n\t\t\tset_pageblock_migratetype(page, migratetype);\n\t\t\tcond_resched();\n\t\t}\n\t\tpfn++;\n\t}\n}\n\n#ifdef CONFIG_ZONE_DEVICE\nvoid __ref memmap_init_zone_device(struct zone *zone,\n\t\t\t\t   unsigned long start_pfn,\n\t\t\t\t   unsigned long nr_pages,\n\t\t\t\t   struct dev_pagemap *pgmap)\n{\n\tunsigned long pfn, end_pfn = start_pfn + nr_pages;\n\tstruct pglist_data *pgdat = zone->zone_pgdat;\n\tstruct vmem_altmap *altmap = pgmap_altmap(pgmap);\n\tunsigned long zone_idx = zone_idx(zone);\n\tunsigned long start = jiffies;\n\tint nid = pgdat->node_id;\n\n\tif (WARN_ON_ONCE(!pgmap || zone_idx(zone) != ZONE_DEVICE))\n\t\treturn;\n\n\t/*\n\t * The call to memmap_init_zone should have already taken care\n\t * of the pages reserved for the memmap, so we can just jump to\n\t * the end of that region and start processing the device pages.\n\t */\n\tif (altmap) {\n\t\tstart_pfn = altmap->base_pfn + vmem_altmap_offset(altmap);\n\t\tnr_pages = end_pfn - start_pfn;\n\t}\n\n\tfor (pfn = start_pfn; pfn < end_pfn; pfn++) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\t__init_single_page(page, pfn, zone_idx, nid);\n\n\t\t/*\n\t\t * Mark page reserved as it will need to wait for onlining\n\t\t * phase for it to be fully associated with a zone.\n\t\t *\n\t\t * We can use the non-atomic __set_bit operation for setting\n\t\t * the flag as we are still initializing the pages.\n\t\t */\n\t\t__SetPageReserved(page);\n\n\t\t/*\n\t\t * ZONE_DEVICE pages union ->lru with a ->pgmap back pointer\n\t\t * and zone_device_data.  It is a bug if a ZONE_DEVICE page is\n\t\t * ever freed or placed on a driver-private list.\n\t\t */\n\t\tpage->pgmap = pgmap;\n\t\tpage->zone_device_data = NULL;\n\n\t\t/*\n\t\t * Mark the block movable so that blocks are reserved for\n\t\t * movable at startup. This will force kernel allocations\n\t\t * to reserve their blocks rather than leaking throughout\n\t\t * the address space during boot when many long-lived\n\t\t * kernel allocations are made.\n\t\t *\n\t\t * Please note that MEMINIT_HOTPLUG path doesn't clear memmap\n\t\t * because this is done early in section_activate()\n\t\t */\n\t\tif (IS_ALIGNED(pfn, pageblock_nr_pages)) {\n\t\t\tset_pageblock_migratetype(page, MIGRATE_MOVABLE);\n\t\t\tcond_resched();\n\t\t}\n\t}\n\n\tpr_info(\"%s initialised %lu pages in %ums\\n\", __func__,\n\t\tnr_pages, jiffies_to_msecs(jiffies - start));\n}\n\n#endif\nstatic void __meminit zone_init_free_lists(struct zone *zone)\n{\n\tunsigned int order, t;\n\tfor_each_migratetype_order(order, t) {\n\t\tINIT_LIST_HEAD(&zone->free_area[order].free_list[t]);\n\t\tzone->free_area[order].nr_free = 0;\n\t}\n}\n\n#if !defined(CONFIG_FLAT_NODE_MEM_MAP)\n/*\n * Only struct pages that correspond to ranges defined by memblock.memory\n * are zeroed and initialized by going through __init_single_page() during\n * memmap_init_zone().\n *\n * But, there could be struct pages that correspond to holes in\n * memblock.memory. This can happen because of the following reasons:\n * - physical memory bank size is not necessarily the exact multiple of the\n *   arbitrary section size\n * - early reserved memory may not be listed in memblock.memory\n * - memory layouts defined with memmap= kernel parameter may not align\n *   nicely with memmap sections\n *\n * Explicitly initialize those struct pages so that:\n * - PG_Reserved is set\n * - zone and node links point to zone and node that span the page if the\n *   hole is in the middle of a zone\n * - zone and node links point to adjacent zone/node if the hole falls on\n *   the zone boundary; the pages in such holes will be prepended to the\n *   zone/node above the hole except for the trailing pages in the last\n *   section that will be appended to the zone/node below.\n */\nstatic u64 __meminit init_unavailable_range(unsigned long spfn,\n\t\t\t\t\t    unsigned long epfn,\n\t\t\t\t\t    int zone, int node)\n{\n\tunsigned long pfn;\n\tu64 pgcnt = 0;\n\n\tfor (pfn = spfn; pfn < epfn; pfn++) {\n\t\tif (!pfn_valid(ALIGN_DOWN(pfn, pageblock_nr_pages))) {\n\t\t\tpfn = ALIGN_DOWN(pfn, pageblock_nr_pages)\n\t\t\t\t+ pageblock_nr_pages - 1;\n\t\t\tcontinue;\n\t\t}\n\t\t__init_single_page(pfn_to_page(pfn), pfn, zone, node);\n\t\t__SetPageReserved(pfn_to_page(pfn));\n\t\tpgcnt++;\n\t}\n\n\treturn pgcnt;\n}\n#else\nstatic inline u64 init_unavailable_range(unsigned long spfn, unsigned long epfn,\n\t\t\t\t\t int zone, int node)\n{\n\treturn 0;\n}\n#endif\n\nvoid __meminit __weak memmap_init_zone(struct zone *zone)\n{\n\tunsigned long zone_start_pfn = zone->zone_start_pfn;\n\tunsigned long zone_end_pfn = zone_start_pfn + zone->spanned_pages;\n\tint i, nid = zone_to_nid(zone), zone_id = zone_idx(zone);\n\tstatic unsigned long hole_pfn;\n\tunsigned long start_pfn, end_pfn;\n\tu64 pgcnt = 0;\n\n\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, NULL) {\n\t\tstart_pfn = clamp(start_pfn, zone_start_pfn, zone_end_pfn);\n\t\tend_pfn = clamp(end_pfn, zone_start_pfn, zone_end_pfn);\n\n\t\tif (end_pfn > start_pfn)\n\t\t\tmemmap_init_range(end_pfn - start_pfn, nid,\n\t\t\t\t\tzone_id, start_pfn, zone_end_pfn,\n\t\t\t\t\tMEMINIT_EARLY, NULL, MIGRATE_MOVABLE);\n\n\t\tif (hole_pfn < start_pfn)\n\t\t\tpgcnt += init_unavailable_range(hole_pfn, start_pfn,\n\t\t\t\t\t\t\tzone_id, nid);\n\t\thole_pfn = end_pfn;\n\t}\n\n#ifdef CONFIG_SPARSEMEM\n\t/*\n\t * Initialize the hole in the range [zone_end_pfn, section_end].\n\t * If zone boundary falls in the middle of a section, this hole\n\t * will be re-initialized during the call to this function for the\n\t * higher zone.\n\t */\n\tend_pfn = round_up(zone_end_pfn, PAGES_PER_SECTION);\n\tif (hole_pfn < end_pfn)\n\t\tpgcnt += init_unavailable_range(hole_pfn, end_pfn,\n\t\t\t\t\t\tzone_id, nid);\n#endif\n\n\tif (pgcnt)\n\t\tpr_info(\"  %s zone: %llu pages in unavailable ranges\\n\",\n\t\t\tzone->name, pgcnt);\n}\n\nstatic int zone_batchsize(struct zone *zone)\n{\n#ifdef CONFIG_MMU\n\tint batch;\n\n\t/*\n\t * The per-cpu-pages pools are set to around 1000th of the\n\t * size of the zone.\n\t */\n\tbatch = zone_managed_pages(zone) / 1024;\n\t/* But no more than a meg. */\n\tif (batch * PAGE_SIZE > 1024 * 1024)\n\t\tbatch = (1024 * 1024) / PAGE_SIZE;\n\tbatch /= 4;\t\t/* We effectively *= 4 below */\n\tif (batch < 1)\n\t\tbatch = 1;\n\n\t/*\n\t * Clamp the batch to a 2^n - 1 value. Having a power\n\t * of 2 value was found to be more likely to have\n\t * suboptimal cache aliasing properties in some cases.\n\t *\n\t * For example if 2 tasks are alternately allocating\n\t * batches of pages, one task can end up with a lot\n\t * of pages of one half of the possible page colors\n\t * and the other with pages of the other colors.\n\t */\n\tbatch = rounddown_pow_of_two(batch + batch/2) - 1;\n\n\treturn batch;\n\n#else\n\t/* The deferral and batching of frees should be suppressed under NOMMU\n\t * conditions.\n\t *\n\t * The problem is that NOMMU needs to be able to allocate large chunks\n\t * of contiguous memory as there's no hardware page translation to\n\t * assemble apparent contiguous memory from discontiguous pages.\n\t *\n\t * Queueing large contiguous runs of pages for batching, however,\n\t * causes the pages to actually be freed in smaller chunks.  As there\n\t * can be a significant delay between the individual batches being\n\t * recycled, this leads to the once large chunks of space being\n\t * fragmented and becoming unavailable for high-order allocations.\n\t */\n\treturn 0;\n#endif\n}\n\n/*\n * pcp->high and pcp->batch values are related and generally batch is lower\n * than high. They are also related to pcp->count such that count is lower\n * than high, and as soon as it reaches high, the pcplist is flushed.\n *\n * However, guaranteeing these relations at all times would require e.g. write\n * barriers here but also careful usage of read barriers at the read side, and\n * thus be prone to error and bad for performance. Thus the update only prevents\n * store tearing. Any new users of pcp->batch and pcp->high should ensure they\n * can cope with those fields changing asynchronously, and fully trust only the\n * pcp->count field on the local CPU with interrupts disabled.\n *\n * mutex_is_locked(&pcp_batch_high_lock) required when calling this function\n * outside of boot time (or some other assurance that no concurrent updaters\n * exist).\n */\nstatic void pageset_update(struct per_cpu_pages *pcp, unsigned long high,\n\t\tunsigned long batch)\n{\n\tWRITE_ONCE(pcp->batch, batch);\n\tWRITE_ONCE(pcp->high, high);\n}\n\nstatic void pageset_init(struct per_cpu_pageset *p)\n{\n\tstruct per_cpu_pages *pcp;\n\tint migratetype;\n\n\tmemset(p, 0, sizeof(*p));\n\n\tpcp = &p->pcp;\n\tfor (migratetype = 0; migratetype < MIGRATE_PCPTYPES; migratetype++)\n\t\tINIT_LIST_HEAD(&pcp->lists[migratetype]);\n\n\t/*\n\t * Set batch and high values safe for a boot pageset. A true percpu\n\t * pageset's initialization will update them subsequently. Here we don't\n\t * need to be as careful as pageset_update() as nobody can access the\n\t * pageset yet.\n\t */\n\tpcp->high = BOOT_PAGESET_HIGH;\n\tpcp->batch = BOOT_PAGESET_BATCH;\n}\n\nstatic void __zone_set_pageset_high_and_batch(struct zone *zone, unsigned long high,\n\t\tunsigned long batch)\n{\n\tstruct per_cpu_pageset *p;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tp = per_cpu_ptr(zone->pageset, cpu);\n\t\tpageset_update(&p->pcp, high, batch);\n\t}\n}\n\n/*\n * Calculate and set new high and batch values for all per-cpu pagesets of a\n * zone, based on the zone's size and the percpu_pagelist_fraction sysctl.\n */\nstatic void zone_set_pageset_high_and_batch(struct zone *zone)\n{\n\tunsigned long new_high, new_batch;\n\n\tif (percpu_pagelist_fraction) {\n\t\tnew_high = zone_managed_pages(zone) / percpu_pagelist_fraction;\n\t\tnew_batch = max(1UL, new_high / 4);\n\t\tif ((new_high / 4) > (PAGE_SHIFT * 8))\n\t\t\tnew_batch = PAGE_SHIFT * 8;\n\t} else {\n\t\tnew_batch = zone_batchsize(zone);\n\t\tnew_high = 6 * new_batch;\n\t\tnew_batch = max(1UL, 1 * new_batch);\n\t}\n\n\tif (zone->pageset_high == new_high &&\n\t    zone->pageset_batch == new_batch)\n\t\treturn;\n\n\tzone->pageset_high = new_high;\n\tzone->pageset_batch = new_batch;\n\n\t__zone_set_pageset_high_and_batch(zone, new_high, new_batch);\n}\n\nvoid __meminit setup_zone_pageset(struct zone *zone)\n{\n\tstruct per_cpu_pageset *p;\n\tint cpu;\n\n\tzone->pageset = alloc_percpu(struct per_cpu_pageset);\n\tfor_each_possible_cpu(cpu) {\n\t\tp = per_cpu_ptr(zone->pageset, cpu);\n\t\tpageset_init(p);\n\t}\n\n\tzone_set_pageset_high_and_batch(zone);\n}\n\n/*\n * Allocate per cpu pagesets and initialize them.\n * Before this call only boot pagesets were available.\n */\nvoid __init setup_per_cpu_pageset(void)\n{\n\tstruct pglist_data *pgdat;\n\tstruct zone *zone;\n\tint __maybe_unused cpu;\n\n\tfor_each_populated_zone(zone)\n\t\tsetup_zone_pageset(zone);\n\n#ifdef CONFIG_NUMA\n\t/*\n\t * Unpopulated zones continue using the boot pagesets.\n\t * The numa stats for these pagesets need to be reset.\n\t * Otherwise, they will end up skewing the stats of\n\t * the nodes these zones are associated with.\n\t */\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct per_cpu_pageset *pcp = &per_cpu(boot_pageset, cpu);\n\t\tmemset(pcp->vm_numa_stat_diff, 0,\n\t\t       sizeof(pcp->vm_numa_stat_diff));\n\t}\n#endif\n\n\tfor_each_online_pgdat(pgdat)\n\t\tpgdat->per_cpu_nodestats =\n\t\t\talloc_percpu(struct per_cpu_nodestat);\n}\n\nstatic __meminit void zone_pcp_init(struct zone *zone)\n{\n\t/*\n\t * per cpu subsystem is not up at this point. The following code\n\t * relies on the ability of the linker to provide the\n\t * offset of a (static) per cpu variable into the per cpu area.\n\t */\n\tzone->pageset = &boot_pageset;\n\tzone->pageset_high = BOOT_PAGESET_HIGH;\n\tzone->pageset_batch = BOOT_PAGESET_BATCH;\n\n\tif (populated_zone(zone))\n\t\tprintk(KERN_DEBUG \"  %s zone: %lu pages, LIFO batch:%u\\n\",\n\t\t\tzone->name, zone->present_pages,\n\t\t\t\t\t zone_batchsize(zone));\n}\n\nvoid __meminit init_currently_empty_zone(struct zone *zone,\n\t\t\t\t\tunsigned long zone_start_pfn,\n\t\t\t\t\tunsigned long size)\n{\n\tstruct pglist_data *pgdat = zone->zone_pgdat;\n\tint zone_idx = zone_idx(zone) + 1;\n\n\tif (zone_idx > pgdat->nr_zones)\n\t\tpgdat->nr_zones = zone_idx;\n\n\tzone->zone_start_pfn = zone_start_pfn;\n\n\tmminit_dprintk(MMINIT_TRACE, \"memmap_init\",\n\t\t\t\"Initialising map node %d zone %lu pfns %lu -> %lu\\n\",\n\t\t\tpgdat->node_id,\n\t\t\t(unsigned long)zone_idx(zone),\n\t\t\tzone_start_pfn, (zone_start_pfn + size));\n\n\tzone_init_free_lists(zone);\n\tzone->initialized = 1;\n}\n\n/**\n * get_pfn_range_for_nid - Return the start and end page frames for a node\n * @nid: The nid to return the range for. If MAX_NUMNODES, the min and max PFN are returned.\n * @start_pfn: Passed by reference. On return, it will have the node start_pfn.\n * @end_pfn: Passed by reference. On return, it will have the node end_pfn.\n *\n * It returns the start and end page frame of a node based on information\n * provided by memblock_set_node(). If called for a node\n * with no available memory, a warning is printed and the start and end\n * PFNs will be 0.\n */\nvoid __init get_pfn_range_for_nid(unsigned int nid,\n\t\t\tunsigned long *start_pfn, unsigned long *end_pfn)\n{\n\tunsigned long this_start_pfn, this_end_pfn;\n\tint i;\n\n\t*start_pfn = -1UL;\n\t*end_pfn = 0;\n\n\tfor_each_mem_pfn_range(i, nid, &this_start_pfn, &this_end_pfn, NULL) {\n\t\t*start_pfn = min(*start_pfn, this_start_pfn);\n\t\t*end_pfn = max(*end_pfn, this_end_pfn);\n\t}\n\n\tif (*start_pfn == -1UL)\n\t\t*start_pfn = 0;\n}\n\n/*\n * This finds a zone that can be used for ZONE_MOVABLE pages. The\n * assumption is made that zones within a node are ordered in monotonic\n * increasing memory addresses so that the \"highest\" populated zone is used\n */\nstatic void __init find_usable_zone_for_movable(void)\n{\n\tint zone_index;\n\tfor (zone_index = MAX_NR_ZONES - 1; zone_index >= 0; zone_index--) {\n\t\tif (zone_index == ZONE_MOVABLE)\n\t\t\tcontinue;\n\n\t\tif (arch_zone_highest_possible_pfn[zone_index] >\n\t\t\t\tarch_zone_lowest_possible_pfn[zone_index])\n\t\t\tbreak;\n\t}\n\n\tVM_BUG_ON(zone_index == -1);\n\tmovable_zone = zone_index;\n}\n\n/*\n * The zone ranges provided by the architecture do not include ZONE_MOVABLE\n * because it is sized independent of architecture. Unlike the other zones,\n * the starting point for ZONE_MOVABLE is not fixed. It may be different\n * in each node depending on the size of each node and how evenly kernelcore\n * is distributed. This helper function adjusts the zone ranges\n * provided by the architecture for a given node by using the end of the\n * highest usable zone for ZONE_MOVABLE. This preserves the assumption that\n * zones within a node are in order of monotonic increases memory addresses\n */\nstatic void __init adjust_zone_range_for_zone_movable(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\tunsigned long *zone_start_pfn,\n\t\t\t\t\tunsigned long *zone_end_pfn)\n{\n\t/* Only adjust if ZONE_MOVABLE is on this node */\n\tif (zone_movable_pfn[nid]) {\n\t\t/* Size ZONE_MOVABLE */\n\t\tif (zone_type == ZONE_MOVABLE) {\n\t\t\t*zone_start_pfn = zone_movable_pfn[nid];\n\t\t\t*zone_end_pfn = min(node_end_pfn,\n\t\t\t\tarch_zone_highest_possible_pfn[movable_zone]);\n\n\t\t/* Adjust for ZONE_MOVABLE starting within this range */\n\t\t} else if (!mirrored_kernelcore &&\n\t\t\t*zone_start_pfn < zone_movable_pfn[nid] &&\n\t\t\t*zone_end_pfn > zone_movable_pfn[nid]) {\n\t\t\t*zone_end_pfn = zone_movable_pfn[nid];\n\n\t\t/* Check if this whole range is within ZONE_MOVABLE */\n\t\t} else if (*zone_start_pfn >= zone_movable_pfn[nid])\n\t\t\t*zone_start_pfn = *zone_end_pfn;\n\t}\n}\n\n/*\n * Return the number of pages a zone spans in a node, including holes\n * present_pages = zone_spanned_pages_in_node() - zone_absent_pages_in_node()\n */\nstatic unsigned long __init zone_spanned_pages_in_node(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\tunsigned long node_end_pfn,\n\t\t\t\t\tunsigned long *zone_start_pfn,\n\t\t\t\t\tunsigned long *zone_end_pfn)\n{\n\tunsigned long zone_low = arch_zone_lowest_possible_pfn[zone_type];\n\tunsigned long zone_high = arch_zone_highest_possible_pfn[zone_type];\n\t/* When hotadd a new node from cpu_up(), the node should be empty */\n\tif (!node_start_pfn && !node_end_pfn)\n\t\treturn 0;\n\n\t/* Get the start and end of the zone */\n\t*zone_start_pfn = clamp(node_start_pfn, zone_low, zone_high);\n\t*zone_end_pfn = clamp(node_end_pfn, zone_low, zone_high);\n\tadjust_zone_range_for_zone_movable(nid, zone_type,\n\t\t\t\tnode_start_pfn, node_end_pfn,\n\t\t\t\tzone_start_pfn, zone_end_pfn);\n\n\t/* Check that this node has pages within the zone's required range */\n\tif (*zone_end_pfn < node_start_pfn || *zone_start_pfn > node_end_pfn)\n\t\treturn 0;\n\n\t/* Move the zone boundaries inside the node if necessary */\n\t*zone_end_pfn = min(*zone_end_pfn, node_end_pfn);\n\t*zone_start_pfn = max(*zone_start_pfn, node_start_pfn);\n\n\t/* Return the spanned pages */\n\treturn *zone_end_pfn - *zone_start_pfn;\n}\n\n/*\n * Return the number of holes in a range on a node. If nid is MAX_NUMNODES,\n * then all holes in the requested range will be accounted for.\n */\nunsigned long __init __absent_pages_in_range(int nid,\n\t\t\t\tunsigned long range_start_pfn,\n\t\t\t\tunsigned long range_end_pfn)\n{\n\tunsigned long nr_absent = range_end_pfn - range_start_pfn;\n\tunsigned long start_pfn, end_pfn;\n\tint i;\n\n\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, NULL) {\n\t\tstart_pfn = clamp(start_pfn, range_start_pfn, range_end_pfn);\n\t\tend_pfn = clamp(end_pfn, range_start_pfn, range_end_pfn);\n\t\tnr_absent -= end_pfn - start_pfn;\n\t}\n\treturn nr_absent;\n}\n\n/**\n * absent_pages_in_range - Return number of page frames in holes within a range\n * @start_pfn: The start PFN to start searching for holes\n * @end_pfn: The end PFN to stop searching for holes\n *\n * Return: the number of pages frames in memory holes within a range.\n */\nunsigned long __init absent_pages_in_range(unsigned long start_pfn,\n\t\t\t\t\t\t\tunsigned long end_pfn)\n{\n\treturn __absent_pages_in_range(MAX_NUMNODES, start_pfn, end_pfn);\n}\n\n/* Return the number of page frames in holes in a zone on a node */\nstatic unsigned long __init zone_absent_pages_in_node(int nid,\n\t\t\t\t\tunsigned long zone_type,\n\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\tunsigned long node_end_pfn)\n{\n\tunsigned long zone_low = arch_zone_lowest_possible_pfn[zone_type];\n\tunsigned long zone_high = arch_zone_highest_possible_pfn[zone_type];\n\tunsigned long zone_start_pfn, zone_end_pfn;\n\tunsigned long nr_absent;\n\n\t/* When hotadd a new node from cpu_up(), the node should be empty */\n\tif (!node_start_pfn && !node_end_pfn)\n\t\treturn 0;\n\n\tzone_start_pfn = clamp(node_start_pfn, zone_low, zone_high);\n\tzone_end_pfn = clamp(node_end_pfn, zone_low, zone_high);\n\n\tadjust_zone_range_for_zone_movable(nid, zone_type,\n\t\t\tnode_start_pfn, node_end_pfn,\n\t\t\t&zone_start_pfn, &zone_end_pfn);\n\tnr_absent = __absent_pages_in_range(nid, zone_start_pfn, zone_end_pfn);\n\n\t/*\n\t * ZONE_MOVABLE handling.\n\t * Treat pages to be ZONE_MOVABLE in ZONE_NORMAL as absent pages\n\t * and vice versa.\n\t */\n\tif (mirrored_kernelcore && zone_movable_pfn[nid]) {\n\t\tunsigned long start_pfn, end_pfn;\n\t\tstruct memblock_region *r;\n\n\t\tfor_each_mem_region(r) {\n\t\t\tstart_pfn = clamp(memblock_region_memory_base_pfn(r),\n\t\t\t\t\t  zone_start_pfn, zone_end_pfn);\n\t\t\tend_pfn = clamp(memblock_region_memory_end_pfn(r),\n\t\t\t\t\tzone_start_pfn, zone_end_pfn);\n\n\t\t\tif (zone_type == ZONE_MOVABLE &&\n\t\t\t    memblock_is_mirror(r))\n\t\t\t\tnr_absent += end_pfn - start_pfn;\n\n\t\t\tif (zone_type == ZONE_NORMAL &&\n\t\t\t    !memblock_is_mirror(r))\n\t\t\t\tnr_absent += end_pfn - start_pfn;\n\t\t}\n\t}\n\n\treturn nr_absent;\n}\n\nstatic void __init calculate_node_totalpages(struct pglist_data *pgdat,\n\t\t\t\t\t\tunsigned long node_start_pfn,\n\t\t\t\t\t\tunsigned long node_end_pfn)\n{\n\tunsigned long realtotalpages = 0, totalpages = 0;\n\tenum zone_type i;\n\n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tstruct zone *zone = pgdat->node_zones + i;\n\t\tunsigned long zone_start_pfn, zone_end_pfn;\n\t\tunsigned long spanned, absent;\n\t\tunsigned long size, real_size;\n\n\t\tspanned = zone_spanned_pages_in_node(pgdat->node_id, i,\n\t\t\t\t\t\t     node_start_pfn,\n\t\t\t\t\t\t     node_end_pfn,\n\t\t\t\t\t\t     &zone_start_pfn,\n\t\t\t\t\t\t     &zone_end_pfn);\n\t\tabsent = zone_absent_pages_in_node(pgdat->node_id, i,\n\t\t\t\t\t\t   node_start_pfn,\n\t\t\t\t\t\t   node_end_pfn);\n\n\t\tsize = spanned;\n\t\treal_size = size - absent;\n\n\t\tif (size)\n\t\t\tzone->zone_start_pfn = zone_start_pfn;\n\t\telse\n\t\t\tzone->zone_start_pfn = 0;\n\t\tzone->spanned_pages = size;\n\t\tzone->present_pages = real_size;\n\n\t\ttotalpages += size;\n\t\trealtotalpages += real_size;\n\t}\n\n\tpgdat->node_spanned_pages = totalpages;\n\tpgdat->node_present_pages = realtotalpages;\n\tprintk(KERN_DEBUG \"On node %d totalpages: %lu\\n\", pgdat->node_id,\n\t\t\t\t\t\t\trealtotalpages);\n}\n\n#ifndef CONFIG_SPARSEMEM\n/*\n * Calculate the size of the zone->blockflags rounded to an unsigned long\n * Start by making sure zonesize is a multiple of pageblock_order by rounding\n * up. Then use 1 NR_PAGEBLOCK_BITS worth of bits per pageblock, finally\n * round what is now in bits to nearest long in bits, then return it in\n * bytes.\n */\nstatic unsigned long __init usemap_size(unsigned long zone_start_pfn, unsigned long zonesize)\n{\n\tunsigned long usemapsize;\n\n\tzonesize += zone_start_pfn & (pageblock_nr_pages-1);\n\tusemapsize = roundup(zonesize, pageblock_nr_pages);\n\tusemapsize = usemapsize >> pageblock_order;\n\tusemapsize *= NR_PAGEBLOCK_BITS;\n\tusemapsize = roundup(usemapsize, 8 * sizeof(unsigned long));\n\n\treturn usemapsize / 8;\n}\n\nstatic void __ref setup_usemap(struct zone *zone)\n{\n\tunsigned long usemapsize = usemap_size(zone->zone_start_pfn,\n\t\t\t\t\t       zone->spanned_pages);\n\tzone->pageblock_flags = NULL;\n\tif (usemapsize) {\n\t\tzone->pageblock_flags =\n\t\t\tmemblock_alloc_node(usemapsize, SMP_CACHE_BYTES,\n\t\t\t\t\t    zone_to_nid(zone));\n\t\tif (!zone->pageblock_flags)\n\t\t\tpanic(\"Failed to allocate %ld bytes for zone %s pageblock flags on node %d\\n\",\n\t\t\t      usemapsize, zone->name, zone_to_nid(zone));\n\t}\n}\n#else\nstatic inline void setup_usemap(struct zone *zone) {}\n#endif /* CONFIG_SPARSEMEM */\n\n#ifdef CONFIG_HUGETLB_PAGE_SIZE_VARIABLE\n\n/* Initialise the number of pages represented by NR_PAGEBLOCK_BITS */\nvoid __init set_pageblock_order(void)\n{\n\tunsigned int order;\n\n\t/* Check that pageblock_nr_pages has not already been setup */\n\tif (pageblock_order)\n\t\treturn;\n\n\tif (HPAGE_SHIFT > PAGE_SHIFT)\n\t\torder = HUGETLB_PAGE_ORDER;\n\telse\n\t\torder = MAX_ORDER - 1;\n\n\t/*\n\t * Assume the largest contiguous order of interest is a huge page.\n\t * This value may be variable depending on boot parameters on IA64 and\n\t * powerpc.\n\t */\n\tpageblock_order = order;\n}\n#else /* CONFIG_HUGETLB_PAGE_SIZE_VARIABLE */\n\n/*\n * When CONFIG_HUGETLB_PAGE_SIZE_VARIABLE is not set, set_pageblock_order()\n * is unused as pageblock_order is set at compile-time. See\n * include/linux/pageblock-flags.h for the values of pageblock_order based on\n * the kernel config\n */\nvoid __init set_pageblock_order(void)\n{\n}\n\n#endif /* CONFIG_HUGETLB_PAGE_SIZE_VARIABLE */\n\nstatic unsigned long __init calc_memmap_size(unsigned long spanned_pages,\n\t\t\t\t\t\tunsigned long present_pages)\n{\n\tunsigned long pages = spanned_pages;\n\n\t/*\n\t * Provide a more accurate estimation if there are holes within\n\t * the zone and SPARSEMEM is in use. If there are holes within the\n\t * zone, each populated memory region may cost us one or two extra\n\t * memmap pages due to alignment because memmap pages for each\n\t * populated regions may not be naturally aligned on page boundary.\n\t * So the (present_pages >> 4) heuristic is a tradeoff for that.\n\t */\n\tif (spanned_pages > present_pages + (present_pages >> 4) &&\n\t    IS_ENABLED(CONFIG_SPARSEMEM))\n\t\tpages = present_pages;\n\n\treturn PAGE_ALIGN(pages * sizeof(struct page)) >> PAGE_SHIFT;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstatic void pgdat_init_split_queue(struct pglist_data *pgdat)\n{\n\tstruct deferred_split *ds_queue = &pgdat->deferred_split_queue;\n\n\tspin_lock_init(&ds_queue->split_queue_lock);\n\tINIT_LIST_HEAD(&ds_queue->split_queue);\n\tds_queue->split_queue_len = 0;\n}\n#else\nstatic void pgdat_init_split_queue(struct pglist_data *pgdat) {}\n#endif\n\n#ifdef CONFIG_COMPACTION\nstatic void pgdat_init_kcompactd(struct pglist_data *pgdat)\n{\n\tinit_waitqueue_head(&pgdat->kcompactd_wait);\n}\n#else\nstatic void pgdat_init_kcompactd(struct pglist_data *pgdat) {}\n#endif\n\nstatic void __meminit pgdat_init_internals(struct pglist_data *pgdat)\n{\n\tpgdat_resize_init(pgdat);\n\n\tpgdat_init_split_queue(pgdat);\n\tpgdat_init_kcompactd(pgdat);\n\n\tinit_waitqueue_head(&pgdat->kswapd_wait);\n\tinit_waitqueue_head(&pgdat->pfmemalloc_wait);\n\n\tpgdat_page_ext_init(pgdat);\n\tlruvec_init(&pgdat->__lruvec);\n}\n\nstatic void __meminit zone_init_internals(struct zone *zone, enum zone_type idx, int nid,\n\t\t\t\t\t\t\tunsigned long remaining_pages)\n{\n\tatomic_long_set(&zone->managed_pages, remaining_pages);\n\tzone_set_nid(zone, nid);\n\tzone->name = zone_names[idx];\n\tzone->zone_pgdat = NODE_DATA(nid);\n\tspin_lock_init(&zone->lock);\n\tzone_seqlock_init(zone);\n\tzone_pcp_init(zone);\n}\n\n/*\n * Set up the zone data structures\n * - init pgdat internals\n * - init all zones belonging to this node\n *\n * NOTE: this function is only called during memory hotplug\n */\n#ifdef CONFIG_MEMORY_HOTPLUG\nvoid __ref free_area_init_core_hotplug(int nid)\n{\n\tenum zone_type z;\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\tpgdat_init_internals(pgdat);\n\tfor (z = 0; z < MAX_NR_ZONES; z++)\n\t\tzone_init_internals(&pgdat->node_zones[z], z, nid, 0);\n}\n#endif\n\n/*\n * Set up the zone data structures:\n *   - mark all pages reserved\n *   - mark all memory queues empty\n *   - clear the memory bitmaps\n *\n * NOTE: pgdat should get zeroed by caller.\n * NOTE: this function is only called during early init.\n */\nstatic void __init free_area_init_core(struct pglist_data *pgdat)\n{\n\tenum zone_type j;\n\tint nid = pgdat->node_id;\n\n\tpgdat_init_internals(pgdat);\n\tpgdat->per_cpu_nodestats = &boot_nodestats;\n\n\tfor (j = 0; j < MAX_NR_ZONES; j++) {\n\t\tstruct zone *zone = pgdat->node_zones + j;\n\t\tunsigned long size, freesize, memmap_pages;\n\n\t\tsize = zone->spanned_pages;\n\t\tfreesize = zone->present_pages;\n\n\t\t/*\n\t\t * Adjust freesize so that it accounts for how much memory\n\t\t * is used by this zone for memmap. This affects the watermark\n\t\t * and per-cpu initialisations\n\t\t */\n\t\tmemmap_pages = calc_memmap_size(size, freesize);\n\t\tif (!is_highmem_idx(j)) {\n\t\t\tif (freesize >= memmap_pages) {\n\t\t\t\tfreesize -= memmap_pages;\n\t\t\t\tif (memmap_pages)\n\t\t\t\t\tprintk(KERN_DEBUG\n\t\t\t\t\t       \"  %s zone: %lu pages used for memmap\\n\",\n\t\t\t\t\t       zone_names[j], memmap_pages);\n\t\t\t} else\n\t\t\t\tpr_warn(\"  %s zone: %lu pages exceeds freesize %lu\\n\",\n\t\t\t\t\tzone_names[j], memmap_pages, freesize);\n\t\t}\n\n\t\t/* Account for reserved pages */\n\t\tif (j == 0 && freesize > dma_reserve) {\n\t\t\tfreesize -= dma_reserve;\n\t\t\tprintk(KERN_DEBUG \"  %s zone: %lu pages reserved\\n\",\n\t\t\t\t\tzone_names[0], dma_reserve);\n\t\t}\n\n\t\tif (!is_highmem_idx(j))\n\t\t\tnr_kernel_pages += freesize;\n\t\t/* Charge for highmem memmap if there are enough kernel pages */\n\t\telse if (nr_kernel_pages > memmap_pages * 2)\n\t\t\tnr_kernel_pages -= memmap_pages;\n\t\tnr_all_pages += freesize;\n\n\t\t/*\n\t\t * Set an approximate value for lowmem here, it will be adjusted\n\t\t * when the bootmem allocator frees pages into the buddy system.\n\t\t * And all highmem pages will be managed by the buddy system.\n\t\t */\n\t\tzone_init_internals(zone, j, nid, freesize);\n\n\t\tif (!size)\n\t\t\tcontinue;\n\n\t\tset_pageblock_order();\n\t\tsetup_usemap(zone);\n\t\tinit_currently_empty_zone(zone, zone->zone_start_pfn, size);\n\t\tmemmap_init_zone(zone);\n\t}\n}\n\n#ifdef CONFIG_FLAT_NODE_MEM_MAP\nstatic void __ref alloc_node_mem_map(struct pglist_data *pgdat)\n{\n\tunsigned long __maybe_unused start = 0;\n\tunsigned long __maybe_unused offset = 0;\n\n\t/* Skip empty nodes */\n\tif (!pgdat->node_spanned_pages)\n\t\treturn;\n\n\tstart = pgdat->node_start_pfn & ~(MAX_ORDER_NR_PAGES - 1);\n\toffset = pgdat->node_start_pfn - start;\n\t/* ia64 gets its own node_mem_map, before this, without bootmem */\n\tif (!pgdat->node_mem_map) {\n\t\tunsigned long size, end;\n\t\tstruct page *map;\n\n\t\t/*\n\t\t * The zone's endpoints aren't required to be MAX_ORDER\n\t\t * aligned but the node_mem_map endpoints must be in order\n\t\t * for the buddy allocator to function correctly.\n\t\t */\n\t\tend = pgdat_end_pfn(pgdat);\n\t\tend = ALIGN(end, MAX_ORDER_NR_PAGES);\n\t\tsize =  (end - start) * sizeof(struct page);\n\t\tmap = memblock_alloc_node(size, SMP_CACHE_BYTES,\n\t\t\t\t\t  pgdat->node_id);\n\t\tif (!map)\n\t\t\tpanic(\"Failed to allocate %ld bytes for node %d memory map\\n\",\n\t\t\t      size, pgdat->node_id);\n\t\tpgdat->node_mem_map = map + offset;\n\t}\n\tpr_debug(\"%s: node %d, pgdat %08lx, node_mem_map %08lx\\n\",\n\t\t\t\t__func__, pgdat->node_id, (unsigned long)pgdat,\n\t\t\t\t(unsigned long)pgdat->node_mem_map);\n#ifndef CONFIG_NEED_MULTIPLE_NODES\n\t/*\n\t * With no DISCONTIG, the global mem_map is just set as node 0's\n\t */\n\tif (pgdat == NODE_DATA(0)) {\n\t\tmem_map = NODE_DATA(0)->node_mem_map;\n\t\tif (page_to_pfn(mem_map) != pgdat->node_start_pfn)\n\t\t\tmem_map -= offset;\n\t}\n#endif\n}\n#else\nstatic void __ref alloc_node_mem_map(struct pglist_data *pgdat) { }\n#endif /* CONFIG_FLAT_NODE_MEM_MAP */\n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\nstatic inline void pgdat_set_deferred_range(pg_data_t *pgdat)\n{\n\tpgdat->first_deferred_pfn = ULONG_MAX;\n}\n#else\nstatic inline void pgdat_set_deferred_range(pg_data_t *pgdat) {}\n#endif\n\nstatic void __init free_area_init_node(int nid)\n{\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\tunsigned long start_pfn = 0;\n\tunsigned long end_pfn = 0;\n\n\t/* pg_data_t should be reset to zero when it's allocated */\n\tWARN_ON(pgdat->nr_zones || pgdat->kswapd_highest_zoneidx);\n\n\tget_pfn_range_for_nid(nid, &start_pfn, &end_pfn);\n\n\tpgdat->node_id = nid;\n\tpgdat->node_start_pfn = start_pfn;\n\tpgdat->per_cpu_nodestats = NULL;\n\n\tpr_info(\"Initmem setup node %d [mem %#018Lx-%#018Lx]\\n\", nid,\n\t\t(u64)start_pfn << PAGE_SHIFT,\n\t\tend_pfn ? ((u64)end_pfn << PAGE_SHIFT) - 1 : 0);\n\tcalculate_node_totalpages(pgdat, start_pfn, end_pfn);\n\n\talloc_node_mem_map(pgdat);\n\tpgdat_set_deferred_range(pgdat);\n\n\tfree_area_init_core(pgdat);\n}\n\nvoid __init free_area_init_memoryless_node(int nid)\n{\n\tfree_area_init_node(nid);\n}\n\n#if MAX_NUMNODES > 1\n/*\n * Figure out the number of possible node ids.\n */\nvoid __init setup_nr_node_ids(void)\n{\n\tunsigned int highest;\n\n\thighest = find_last_bit(node_possible_map.bits, MAX_NUMNODES);\n\tnr_node_ids = highest + 1;\n}\n#endif\n\n/**\n * node_map_pfn_alignment - determine the maximum internode alignment\n *\n * This function should be called after node map is populated and sorted.\n * It calculates the maximum power of two alignment which can distinguish\n * all the nodes.\n *\n * For example, if all nodes are 1GiB and aligned to 1GiB, the return value\n * would indicate 1GiB alignment with (1 << (30 - PAGE_SHIFT)).  If the\n * nodes are shifted by 256MiB, 256MiB.  Note that if only the last node is\n * shifted, 1GiB is enough and this function will indicate so.\n *\n * This is used to test whether pfn -> nid mapping of the chosen memory\n * model has fine enough granularity to avoid incorrect mapping for the\n * populated node map.\n *\n * Return: the determined alignment in pfn's.  0 if there is no alignment\n * requirement (single node).\n */\nunsigned long __init node_map_pfn_alignment(void)\n{\n\tunsigned long accl_mask = 0, last_end = 0;\n\tunsigned long start, end, mask;\n\tint last_nid = NUMA_NO_NODE;\n\tint i, nid;\n\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start, &end, &nid) {\n\t\tif (!start || last_nid < 0 || last_nid == nid) {\n\t\t\tlast_nid = nid;\n\t\t\tlast_end = end;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * Start with a mask granular enough to pin-point to the\n\t\t * start pfn and tick off bits one-by-one until it becomes\n\t\t * too coarse to separate the current node from the last.\n\t\t */\n\t\tmask = ~((1 << __ffs(start)) - 1);\n\t\twhile (mask && last_end <= (start & (mask << 1)))\n\t\t\tmask <<= 1;\n\n\t\t/* accumulate all internode masks */\n\t\taccl_mask |= mask;\n\t}\n\n\t/* convert mask to number of pages */\n\treturn ~accl_mask + 1;\n}\n\n/**\n * find_min_pfn_with_active_regions - Find the minimum PFN registered\n *\n * Return: the minimum PFN based on information provided via\n * memblock_set_node().\n */\nunsigned long __init find_min_pfn_with_active_regions(void)\n{\n\treturn PHYS_PFN(memblock_start_of_DRAM());\n}\n\n/*\n * early_calculate_totalpages()\n * Sum pages in active regions for movable zone.\n * Populate N_MEMORY for calculating usable_nodes.\n */\nstatic unsigned long __init early_calculate_totalpages(void)\n{\n\tunsigned long totalpages = 0;\n\tunsigned long start_pfn, end_pfn;\n\tint i, nid;\n\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid) {\n\t\tunsigned long pages = end_pfn - start_pfn;\n\n\t\ttotalpages += pages;\n\t\tif (pages)\n\t\t\tnode_set_state(nid, N_MEMORY);\n\t}\n\treturn totalpages;\n}\n\n/*\n * Find the PFN the Movable zone begins in each node. Kernel memory\n * is spread evenly between nodes as long as the nodes have enough\n * memory. When they don't, some nodes will have more kernelcore than\n * others\n */\nstatic void __init find_zone_movable_pfns_for_nodes(void)\n{\n\tint i, nid;\n\tunsigned long usable_startpfn;\n\tunsigned long kernelcore_node, kernelcore_remaining;\n\t/* save the state before borrow the nodemask */\n\tnodemask_t saved_node_state = node_states[N_MEMORY];\n\tunsigned long totalpages = early_calculate_totalpages();\n\tint usable_nodes = nodes_weight(node_states[N_MEMORY]);\n\tstruct memblock_region *r;\n\n\t/* Need to find movable_zone earlier when movable_node is specified. */\n\tfind_usable_zone_for_movable();\n\n\t/*\n\t * If movable_node is specified, ignore kernelcore and movablecore\n\t * options.\n\t */\n\tif (movable_node_is_enabled()) {\n\t\tfor_each_mem_region(r) {\n\t\t\tif (!memblock_is_hotpluggable(r))\n\t\t\t\tcontinue;\n\n\t\t\tnid = memblock_get_region_node(r);\n\n\t\t\tusable_startpfn = PFN_DOWN(r->base);\n\t\t\tzone_movable_pfn[nid] = zone_movable_pfn[nid] ?\n\t\t\t\tmin(usable_startpfn, zone_movable_pfn[nid]) :\n\t\t\t\tusable_startpfn;\n\t\t}\n\n\t\tgoto out2;\n\t}\n\n\t/*\n\t * If kernelcore=mirror is specified, ignore movablecore option\n\t */\n\tif (mirrored_kernelcore) {\n\t\tbool mem_below_4gb_not_mirrored = false;\n\n\t\tfor_each_mem_region(r) {\n\t\t\tif (memblock_is_mirror(r))\n\t\t\t\tcontinue;\n\n\t\t\tnid = memblock_get_region_node(r);\n\n\t\t\tusable_startpfn = memblock_region_memory_base_pfn(r);\n\n\t\t\tif (usable_startpfn < 0x100000) {\n\t\t\t\tmem_below_4gb_not_mirrored = true;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tzone_movable_pfn[nid] = zone_movable_pfn[nid] ?\n\t\t\t\tmin(usable_startpfn, zone_movable_pfn[nid]) :\n\t\t\t\tusable_startpfn;\n\t\t}\n\n\t\tif (mem_below_4gb_not_mirrored)\n\t\t\tpr_warn(\"This configuration results in unmirrored kernel memory.\\n\");\n\n\t\tgoto out2;\n\t}\n\n\t/*\n\t * If kernelcore=nn% or movablecore=nn% was specified, calculate the\n\t * amount of necessary memory.\n\t */\n\tif (required_kernelcore_percent)\n\t\trequired_kernelcore = (totalpages * 100 * required_kernelcore_percent) /\n\t\t\t\t       10000UL;\n\tif (required_movablecore_percent)\n\t\trequired_movablecore = (totalpages * 100 * required_movablecore_percent) /\n\t\t\t\t\t10000UL;\n\n\t/*\n\t * If movablecore= was specified, calculate what size of\n\t * kernelcore that corresponds so that memory usable for\n\t * any allocation type is evenly spread. If both kernelcore\n\t * and movablecore are specified, then the value of kernelcore\n\t * will be used for required_kernelcore if it's greater than\n\t * what movablecore would have allowed.\n\t */\n\tif (required_movablecore) {\n\t\tunsigned long corepages;\n\n\t\t/*\n\t\t * Round-up so that ZONE_MOVABLE is at least as large as what\n\t\t * was requested by the user\n\t\t */\n\t\trequired_movablecore =\n\t\t\troundup(required_movablecore, MAX_ORDER_NR_PAGES);\n\t\trequired_movablecore = min(totalpages, required_movablecore);\n\t\tcorepages = totalpages - required_movablecore;\n\n\t\trequired_kernelcore = max(required_kernelcore, corepages);\n\t}\n\n\t/*\n\t * If kernelcore was not specified or kernelcore size is larger\n\t * than totalpages, there is no ZONE_MOVABLE.\n\t */\n\tif (!required_kernelcore || required_kernelcore >= totalpages)\n\t\tgoto out;\n\n\t/* usable_startpfn is the lowest possible pfn ZONE_MOVABLE can be at */\n\tusable_startpfn = arch_zone_lowest_possible_pfn[movable_zone];\n\nrestart:\n\t/* Spread kernelcore memory as evenly as possible throughout nodes */\n\tkernelcore_node = required_kernelcore / usable_nodes;\n\tfor_each_node_state(nid, N_MEMORY) {\n\t\tunsigned long start_pfn, end_pfn;\n\n\t\t/*\n\t\t * Recalculate kernelcore_node if the division per node\n\t\t * now exceeds what is necessary to satisfy the requested\n\t\t * amount of memory for the kernel\n\t\t */\n\t\tif (required_kernelcore < kernelcore_node)\n\t\t\tkernelcore_node = required_kernelcore / usable_nodes;\n\n\t\t/*\n\t\t * As the map is walked, we track how much memory is usable\n\t\t * by the kernel using kernelcore_remaining. When it is\n\t\t * 0, the rest of the node is usable by ZONE_MOVABLE\n\t\t */\n\t\tkernelcore_remaining = kernelcore_node;\n\n\t\t/* Go through each range of PFNs within this node */\n\t\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, NULL) {\n\t\t\tunsigned long size_pages;\n\n\t\t\tstart_pfn = max(start_pfn, zone_movable_pfn[nid]);\n\t\t\tif (start_pfn >= end_pfn)\n\t\t\t\tcontinue;\n\n\t\t\t/* Account for what is only usable for kernelcore */\n\t\t\tif (start_pfn < usable_startpfn) {\n\t\t\t\tunsigned long kernel_pages;\n\t\t\t\tkernel_pages = min(end_pfn, usable_startpfn)\n\t\t\t\t\t\t\t\t- start_pfn;\n\n\t\t\t\tkernelcore_remaining -= min(kernel_pages,\n\t\t\t\t\t\t\tkernelcore_remaining);\n\t\t\t\trequired_kernelcore -= min(kernel_pages,\n\t\t\t\t\t\t\trequired_kernelcore);\n\n\t\t\t\t/* Continue if range is now fully accounted */\n\t\t\t\tif (end_pfn <= usable_startpfn) {\n\n\t\t\t\t\t/*\n\t\t\t\t\t * Push zone_movable_pfn to the end so\n\t\t\t\t\t * that if we have to rebalance\n\t\t\t\t\t * kernelcore across nodes, we will\n\t\t\t\t\t * not double account here\n\t\t\t\t\t */\n\t\t\t\t\tzone_movable_pfn[nid] = end_pfn;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tstart_pfn = usable_startpfn;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * The usable PFN range for ZONE_MOVABLE is from\n\t\t\t * start_pfn->end_pfn. Calculate size_pages as the\n\t\t\t * number of pages used as kernelcore\n\t\t\t */\n\t\t\tsize_pages = end_pfn - start_pfn;\n\t\t\tif (size_pages > kernelcore_remaining)\n\t\t\t\tsize_pages = kernelcore_remaining;\n\t\t\tzone_movable_pfn[nid] = start_pfn + size_pages;\n\n\t\t\t/*\n\t\t\t * Some kernelcore has been met, update counts and\n\t\t\t * break if the kernelcore for this node has been\n\t\t\t * satisfied\n\t\t\t */\n\t\t\trequired_kernelcore -= min(required_kernelcore,\n\t\t\t\t\t\t\t\tsize_pages);\n\t\t\tkernelcore_remaining -= size_pages;\n\t\t\tif (!kernelcore_remaining)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\t/*\n\t * If there is still required_kernelcore, we do another pass with one\n\t * less node in the count. This will push zone_movable_pfn[nid] further\n\t * along on the nodes that still have memory until kernelcore is\n\t * satisfied\n\t */\n\tusable_nodes--;\n\tif (usable_nodes && required_kernelcore > usable_nodes)\n\t\tgoto restart;\n\nout2:\n\t/* Align start of ZONE_MOVABLE on all nids to MAX_ORDER_NR_PAGES */\n\tfor (nid = 0; nid < MAX_NUMNODES; nid++)\n\t\tzone_movable_pfn[nid] =\n\t\t\troundup(zone_movable_pfn[nid], MAX_ORDER_NR_PAGES);\n\nout:\n\t/* restore the node_state */\n\tnode_states[N_MEMORY] = saved_node_state;\n}\n\n/* Any regular or high memory on that node ? */\nstatic void check_for_memory(pg_data_t *pgdat, int nid)\n{\n\tenum zone_type zone_type;\n\n\tfor (zone_type = 0; zone_type <= ZONE_MOVABLE - 1; zone_type++) {\n\t\tstruct zone *zone = &pgdat->node_zones[zone_type];\n\t\tif (populated_zone(zone)) {\n\t\t\tif (IS_ENABLED(CONFIG_HIGHMEM))\n\t\t\t\tnode_set_state(nid, N_HIGH_MEMORY);\n\t\t\tif (zone_type <= ZONE_NORMAL)\n\t\t\t\tnode_set_state(nid, N_NORMAL_MEMORY);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n/*\n * Some architecturs, e.g. ARC may have ZONE_HIGHMEM below ZONE_NORMAL. For\n * such cases we allow max_zone_pfn sorted in the descending order\n */\nbool __weak arch_has_descending_max_zone_pfns(void)\n{\n\treturn false;\n}\n\n/**\n * free_area_init - Initialise all pg_data_t and zone data\n * @max_zone_pfn: an array of max PFNs for each zone\n *\n * This will call free_area_init_node() for each active node in the system.\n * Using the page ranges provided by memblock_set_node(), the size of each\n * zone in each node and their holes is calculated. If the maximum PFN\n * between two adjacent zones match, it is assumed that the zone is empty.\n * For example, if arch_max_dma_pfn == arch_max_dma32_pfn, it is assumed\n * that arch_max_dma32_pfn has no pages. It is also assumed that a zone\n * starts where the previous one ended. For example, ZONE_DMA32 starts\n * at arch_max_dma_pfn.\n */\nvoid __init free_area_init(unsigned long *max_zone_pfn)\n{\n\tunsigned long start_pfn, end_pfn;\n\tint i, nid, zone;\n\tbool descending;\n\n\t/* Record where the zone boundaries are */\n\tmemset(arch_zone_lowest_possible_pfn, 0,\n\t\t\t\tsizeof(arch_zone_lowest_possible_pfn));\n\tmemset(arch_zone_highest_possible_pfn, 0,\n\t\t\t\tsizeof(arch_zone_highest_possible_pfn));\n\n\tstart_pfn = find_min_pfn_with_active_regions();\n\tdescending = arch_has_descending_max_zone_pfns();\n\n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tif (descending)\n\t\t\tzone = MAX_NR_ZONES - i - 1;\n\t\telse\n\t\t\tzone = i;\n\n\t\tif (zone == ZONE_MOVABLE)\n\t\t\tcontinue;\n\n\t\tend_pfn = max(max_zone_pfn[zone], start_pfn);\n\t\tarch_zone_lowest_possible_pfn[zone] = start_pfn;\n\t\tarch_zone_highest_possible_pfn[zone] = end_pfn;\n\n\t\tstart_pfn = end_pfn;\n\t}\n\n\t/* Find the PFNs that ZONE_MOVABLE begins at in each node */\n\tmemset(zone_movable_pfn, 0, sizeof(zone_movable_pfn));\n\tfind_zone_movable_pfns_for_nodes();\n\n\t/* Print out the zone ranges */\n\tpr_info(\"Zone ranges:\\n\");\n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tif (i == ZONE_MOVABLE)\n\t\t\tcontinue;\n\t\tpr_info(\"  %-8s \", zone_names[i]);\n\t\tif (arch_zone_lowest_possible_pfn[i] ==\n\t\t\t\tarch_zone_highest_possible_pfn[i])\n\t\t\tpr_cont(\"empty\\n\");\n\t\telse\n\t\t\tpr_cont(\"[mem %#018Lx-%#018Lx]\\n\",\n\t\t\t\t(u64)arch_zone_lowest_possible_pfn[i]\n\t\t\t\t\t<< PAGE_SHIFT,\n\t\t\t\t((u64)arch_zone_highest_possible_pfn[i]\n\t\t\t\t\t<< PAGE_SHIFT) - 1);\n\t}\n\n\t/* Print out the PFNs ZONE_MOVABLE begins at in each node */\n\tpr_info(\"Movable zone start for each node\\n\");\n\tfor (i = 0; i < MAX_NUMNODES; i++) {\n\t\tif (zone_movable_pfn[i])\n\t\t\tpr_info(\"  Node %d: %#018Lx\\n\", i,\n\t\t\t       (u64)zone_movable_pfn[i] << PAGE_SHIFT);\n\t}\n\n\t/*\n\t * Print out the early node map, and initialize the\n\t * subsection-map relative to active online memory ranges to\n\t * enable future \"sub-section\" extensions of the memory map.\n\t */\n\tpr_info(\"Early memory node ranges\\n\");\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid) {\n\t\tpr_info(\"  node %3d: [mem %#018Lx-%#018Lx]\\n\", nid,\n\t\t\t(u64)start_pfn << PAGE_SHIFT,\n\t\t\t((u64)end_pfn << PAGE_SHIFT) - 1);\n\t\tsubsection_map_init(start_pfn, end_pfn - start_pfn);\n\t}\n\n\t/* Initialise every node */\n\tmminit_verify_pageflags_layout();\n\tsetup_nr_node_ids();\n\tfor_each_online_node(nid) {\n\t\tpg_data_t *pgdat = NODE_DATA(nid);\n\t\tfree_area_init_node(nid);\n\n\t\t/* Any memory on that node */\n\t\tif (pgdat->node_present_pages)\n\t\t\tnode_set_state(nid, N_MEMORY);\n\t\tcheck_for_memory(pgdat, nid);\n\t}\n}\n\nstatic int __init cmdline_parse_core(char *p, unsigned long *core,\n\t\t\t\t     unsigned long *percent)\n{\n\tunsigned long long coremem;\n\tchar *endptr;\n\n\tif (!p)\n\t\treturn -EINVAL;\n\n\t/* Value may be a percentage of total memory, otherwise bytes */\n\tcoremem = simple_strtoull(p, &endptr, 0);\n\tif (*endptr == '%') {\n\t\t/* Paranoid check for percent values greater than 100 */\n\t\tWARN_ON(coremem > 100);\n\n\t\t*percent = coremem;\n\t} else {\n\t\tcoremem = memparse(p, &p);\n\t\t/* Paranoid check that UL is enough for the coremem value */\n\t\tWARN_ON((coremem >> PAGE_SHIFT) > ULONG_MAX);\n\n\t\t*core = coremem >> PAGE_SHIFT;\n\t\t*percent = 0UL;\n\t}\n\treturn 0;\n}\n\n/*\n * kernelcore=size sets the amount of memory for use for allocations that\n * cannot be reclaimed or migrated.\n */\nstatic int __init cmdline_parse_kernelcore(char *p)\n{\n\t/* parse kernelcore=mirror */\n\tif (parse_option_str(p, \"mirror\")) {\n\t\tmirrored_kernelcore = true;\n\t\treturn 0;\n\t}\n\n\treturn cmdline_parse_core(p, &required_kernelcore,\n\t\t\t\t  &required_kernelcore_percent);\n}\n\n/*\n * movablecore=size sets the amount of memory for use for allocations that\n * can be reclaimed or migrated.\n */\nstatic int __init cmdline_parse_movablecore(char *p)\n{\n\treturn cmdline_parse_core(p, &required_movablecore,\n\t\t\t\t  &required_movablecore_percent);\n}\n\nearly_param(\"kernelcore\", cmdline_parse_kernelcore);\nearly_param(\"movablecore\", cmdline_parse_movablecore);\n\nvoid adjust_managed_page_count(struct page *page, long count)\n{\n\tatomic_long_add(count, &page_zone(page)->managed_pages);\n\ttotalram_pages_add(count);\n#ifdef CONFIG_HIGHMEM\n\tif (PageHighMem(page))\n\t\ttotalhigh_pages_add(count);\n#endif\n}\nEXPORT_SYMBOL(adjust_managed_page_count);\n\nunsigned long free_reserved_area(void *start, void *end, int poison, const char *s)\n{\n\tvoid *pos;\n\tunsigned long pages = 0;\n\n\tstart = (void *)PAGE_ALIGN((unsigned long)start);\n\tend = (void *)((unsigned long)end & PAGE_MASK);\n\tfor (pos = start; pos < end; pos += PAGE_SIZE, pages++) {\n\t\tstruct page *page = virt_to_page(pos);\n\t\tvoid *direct_map_addr;\n\n\t\t/*\n\t\t * 'direct_map_addr' might be different from 'pos'\n\t\t * because some architectures' virt_to_page()\n\t\t * work with aliases.  Getting the direct map\n\t\t * address ensures that we get a _writeable_\n\t\t * alias for the memset().\n\t\t */\n\t\tdirect_map_addr = page_address(page);\n\t\t/*\n\t\t * Perform a kasan-unchecked memset() since this memory\n\t\t * has not been initialized.\n\t\t */\n\t\tdirect_map_addr = kasan_reset_tag(direct_map_addr);\n\t\tif ((unsigned int)poison <= 0xFF)\n\t\t\tmemset(direct_map_addr, poison, PAGE_SIZE);\n\n\t\tfree_reserved_page(page);\n\t}\n\n\tif (pages && s)\n\t\tpr_info(\"Freeing %s memory: %ldK\\n\",\n\t\t\ts, pages << (PAGE_SHIFT - 10));\n\n\treturn pages;\n}\n\nvoid __init mem_init_print_info(const char *str)\n{\n\tunsigned long physpages, codesize, datasize, rosize, bss_size;\n\tunsigned long init_code_size, init_data_size;\n\n\tphyspages = get_num_physpages();\n\tcodesize = _etext - _stext;\n\tdatasize = _edata - _sdata;\n\trosize = __end_rodata - __start_rodata;\n\tbss_size = __bss_stop - __bss_start;\n\tinit_data_size = __init_end - __init_begin;\n\tinit_code_size = _einittext - _sinittext;\n\n\t/*\n\t * Detect special cases and adjust section sizes accordingly:\n\t * 1) .init.* may be embedded into .data sections\n\t * 2) .init.text.* may be out of [__init_begin, __init_end],\n\t *    please refer to arch/tile/kernel/vmlinux.lds.S.\n\t * 3) .rodata.* may be embedded into .text or .data sections.\n\t */\n#define adj_init_size(start, end, size, pos, adj) \\\n\tdo { \\\n\t\tif (start <= pos && pos < end && size > adj) \\\n\t\t\tsize -= adj; \\\n\t} while (0)\n\n\tadj_init_size(__init_begin, __init_end, init_data_size,\n\t\t     _sinittext, init_code_size);\n\tadj_init_size(_stext, _etext, codesize, _sinittext, init_code_size);\n\tadj_init_size(_sdata, _edata, datasize, __init_begin, init_data_size);\n\tadj_init_size(_stext, _etext, codesize, __start_rodata, rosize);\n\tadj_init_size(_sdata, _edata, datasize, __start_rodata, rosize);\n\n#undef\tadj_init_size\n\n\tpr_info(\"Memory: %luK/%luK available (%luK kernel code, %luK rwdata, %luK rodata, %luK init, %luK bss, %luK reserved, %luK cma-reserved\"\n#ifdef\tCONFIG_HIGHMEM\n\t\t\", %luK highmem\"\n#endif\n\t\t\"%s%s)\\n\",\n\t\tnr_free_pages() << (PAGE_SHIFT - 10),\n\t\tphyspages << (PAGE_SHIFT - 10),\n\t\tcodesize >> 10, datasize >> 10, rosize >> 10,\n\t\t(init_data_size + init_code_size) >> 10, bss_size >> 10,\n\t\t(physpages - totalram_pages() - totalcma_pages) << (PAGE_SHIFT - 10),\n\t\ttotalcma_pages << (PAGE_SHIFT - 10),\n#ifdef\tCONFIG_HIGHMEM\n\t\ttotalhigh_pages() << (PAGE_SHIFT - 10),\n#endif\n\t\tstr ? \", \" : \"\", str ? str : \"\");\n}\n\n/**\n * set_dma_reserve - set the specified number of pages reserved in the first zone\n * @new_dma_reserve: The number of pages to mark reserved\n *\n * The per-cpu batchsize and zone watermarks are determined by managed_pages.\n * In the DMA zone, a significant percentage may be consumed by kernel image\n * and other unfreeable allocations which can skew the watermarks badly. This\n * function may optionally be used to account for unfreeable pages in the\n * first zone (e.g., ZONE_DMA). The effect will be lower watermarks and\n * smaller per-cpu batchsize.\n */\nvoid __init set_dma_reserve(unsigned long new_dma_reserve)\n{\n\tdma_reserve = new_dma_reserve;\n}\n\nstatic int page_alloc_cpu_dead(unsigned int cpu)\n{\n\n\tlru_add_drain_cpu(cpu);\n\tdrain_pages(cpu);\n\n\t/*\n\t * Spill the event counters of the dead processor\n\t * into the current processors event counters.\n\t * This artificially elevates the count of the current\n\t * processor.\n\t */\n\tvm_events_fold_cpu(cpu);\n\n\t/*\n\t * Zero the differential counters of the dead processor\n\t * so that the vm statistics are consistent.\n\t *\n\t * This is only okay since the processor is dead and cannot\n\t * race with what we are doing.\n\t */\n\tcpu_vm_stats_fold(cpu);\n\treturn 0;\n}\n\n#ifdef CONFIG_NUMA\nint hashdist = HASHDIST_DEFAULT;\n\nstatic int __init set_hashdist(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\thashdist = simple_strtoul(str, &str, 0);\n\treturn 1;\n}\n__setup(\"hashdist=\", set_hashdist);\n#endif\n\nvoid __init page_alloc_init(void)\n{\n\tint ret;\n\n#ifdef CONFIG_NUMA\n\tif (num_node_state(N_MEMORY) == 1)\n\t\thashdist = 0;\n#endif\n\n\tret = cpuhp_setup_state_nocalls(CPUHP_PAGE_ALLOC_DEAD,\n\t\t\t\t\t\"mm/page_alloc:dead\", NULL,\n\t\t\t\t\tpage_alloc_cpu_dead);\n\tWARN_ON(ret < 0);\n}\n\n/*\n * calculate_totalreserve_pages - called when sysctl_lowmem_reserve_ratio\n *\tor min_free_kbytes changes.\n */\nstatic void calculate_totalreserve_pages(void)\n{\n\tstruct pglist_data *pgdat;\n\tunsigned long reserve_pages = 0;\n\tenum zone_type i, j;\n\n\tfor_each_online_pgdat(pgdat) {\n\n\t\tpgdat->totalreserve_pages = 0;\n\n\t\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\t\tstruct zone *zone = pgdat->node_zones + i;\n\t\t\tlong max = 0;\n\t\t\tunsigned long managed_pages = zone_managed_pages(zone);\n\n\t\t\t/* Find valid and maximum lowmem_reserve in the zone */\n\t\t\tfor (j = i; j < MAX_NR_ZONES; j++) {\n\t\t\t\tif (zone->lowmem_reserve[j] > max)\n\t\t\t\t\tmax = zone->lowmem_reserve[j];\n\t\t\t}\n\n\t\t\t/* we treat the high watermark as reserved pages. */\n\t\t\tmax += high_wmark_pages(zone);\n\n\t\t\tif (max > managed_pages)\n\t\t\t\tmax = managed_pages;\n\n\t\t\tpgdat->totalreserve_pages += max;\n\n\t\t\treserve_pages += max;\n\t\t}\n\t}\n\ttotalreserve_pages = reserve_pages;\n}\n\n/*\n * setup_per_zone_lowmem_reserve - called whenever\n *\tsysctl_lowmem_reserve_ratio changes.  Ensures that each zone\n *\thas a correct pages reserved value, so an adequate number of\n *\tpages are left in the zone after a successful __alloc_pages().\n */\nstatic void setup_per_zone_lowmem_reserve(void)\n{\n\tstruct pglist_data *pgdat;\n\tenum zone_type i, j;\n\n\tfor_each_online_pgdat(pgdat) {\n\t\tfor (i = 0; i < MAX_NR_ZONES - 1; i++) {\n\t\t\tstruct zone *zone = &pgdat->node_zones[i];\n\t\t\tint ratio = sysctl_lowmem_reserve_ratio[i];\n\t\t\tbool clear = !ratio || !zone_managed_pages(zone);\n\t\t\tunsigned long managed_pages = 0;\n\n\t\t\tfor (j = i + 1; j < MAX_NR_ZONES; j++) {\n\t\t\t\tif (clear) {\n\t\t\t\t\tzone->lowmem_reserve[j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\tstruct zone *upper_zone = &pgdat->node_zones[j];\n\n\t\t\t\t\tmanaged_pages += zone_managed_pages(upper_zone);\n\t\t\t\t\tzone->lowmem_reserve[j] = managed_pages / ratio;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t/* update totalreserve_pages */\n\tcalculate_totalreserve_pages();\n}\n\nstatic void __setup_per_zone_wmarks(void)\n{\n\tunsigned long pages_min = min_free_kbytes >> (PAGE_SHIFT - 10);\n\tunsigned long lowmem_pages = 0;\n\tstruct zone *zone;\n\tunsigned long flags;\n\n\t/* Calculate total number of !ZONE_HIGHMEM pages */\n\tfor_each_zone(zone) {\n\t\tif (!is_highmem(zone))\n\t\t\tlowmem_pages += zone_managed_pages(zone);\n\t}\n\n\tfor_each_zone(zone) {\n\t\tu64 tmp;\n\n\t\tspin_lock_irqsave(&zone->lock, flags);\n\t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n\t\tdo_div(tmp, lowmem_pages);\n\t\tif (is_highmem(zone)) {\n\t\t\t/*\n\t\t\t * __GFP_HIGH and PF_MEMALLOC allocations usually don't\n\t\t\t * need highmem pages, so cap pages_min to a small\n\t\t\t * value here.\n\t\t\t *\n\t\t\t * The WMARK_HIGH-WMARK_LOW and (WMARK_LOW-WMARK_MIN)\n\t\t\t * deltas control async page reclaim, and so should\n\t\t\t * not be capped for highmem.\n\t\t\t */\n\t\t\tunsigned long min_pages;\n\n\t\t\tmin_pages = zone_managed_pages(zone) / 1024;\n\t\t\tmin_pages = clamp(min_pages, SWAP_CLUSTER_MAX, 128UL);\n\t\t\tzone->_watermark[WMARK_MIN] = min_pages;\n\t\t} else {\n\t\t\t/*\n\t\t\t * If it's a lowmem zone, reserve a number of pages\n\t\t\t * proportionate to the zone's size.\n\t\t\t */\n\t\t\tzone->_watermark[WMARK_MIN] = tmp;\n\t\t}\n\n\t\t/*\n\t\t * Set the kswapd watermarks distance according to the\n\t\t * scale factor in proportion to available memory, but\n\t\t * ensure a minimum size on small systems.\n\t\t */\n\t\ttmp = max_t(u64, tmp >> 2,\n\t\t\t    mult_frac(zone_managed_pages(zone),\n\t\t\t\t      watermark_scale_factor, 10000));\n\n\t\tzone->watermark_boost = 0;\n\t\tzone->_watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;\n\t\tzone->_watermark[WMARK_HIGH] = min_wmark_pages(zone) + tmp * 2;\n\n\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t}\n\n\t/* update totalreserve_pages */\n\tcalculate_totalreserve_pages();\n}\n\n/**\n * setup_per_zone_wmarks - called when min_free_kbytes changes\n * or when memory is hot-{added|removed}\n *\n * Ensures that the watermark[min,low,high] values for each zone are set\n * correctly with respect to min_free_kbytes.\n */\nvoid setup_per_zone_wmarks(void)\n{\n\tstatic DEFINE_SPINLOCK(lock);\n\n\tspin_lock(&lock);\n\t__setup_per_zone_wmarks();\n\tspin_unlock(&lock);\n}\n\n/*\n * Initialise min_free_kbytes.\n *\n * For small machines we want it small (128k min).  For large machines\n * we want it large (256MB max).  But it is not linear, because network\n * bandwidth does not increase linearly with machine size.  We use\n *\n *\tmin_free_kbytes = 4 * sqrt(lowmem_kbytes), for better accuracy:\n *\tmin_free_kbytes = sqrt(lowmem_kbytes * 16)\n *\n * which yields\n *\n * 16MB:\t512k\n * 32MB:\t724k\n * 64MB:\t1024k\n * 128MB:\t1448k\n * 256MB:\t2048k\n * 512MB:\t2896k\n * 1024MB:\t4096k\n * 2048MB:\t5792k\n * 4096MB:\t8192k\n * 8192MB:\t11584k\n * 16384MB:\t16384k\n */\nint __meminit init_per_zone_wmark_min(void)\n{\n\tunsigned long lowmem_kbytes;\n\tint new_min_free_kbytes;\n\n\tlowmem_kbytes = nr_free_buffer_pages() * (PAGE_SIZE >> 10);\n\tnew_min_free_kbytes = int_sqrt(lowmem_kbytes * 16);\n\n\tif (new_min_free_kbytes > user_min_free_kbytes) {\n\t\tmin_free_kbytes = new_min_free_kbytes;\n\t\tif (min_free_kbytes < 128)\n\t\t\tmin_free_kbytes = 128;\n\t\tif (min_free_kbytes > 262144)\n\t\t\tmin_free_kbytes = 262144;\n\t} else {\n\t\tpr_warn(\"min_free_kbytes is not updated to %d because user defined value %d is preferred\\n\",\n\t\t\t\tnew_min_free_kbytes, user_min_free_kbytes);\n\t}\n\tsetup_per_zone_wmarks();\n\trefresh_zone_stat_thresholds();\n\tsetup_per_zone_lowmem_reserve();\n\n#ifdef CONFIG_NUMA\n\tsetup_min_unmapped_ratio();\n\tsetup_min_slab_ratio();\n#endif\n\n\tkhugepaged_min_free_kbytes_update();\n\n\treturn 0;\n}\npostcore_initcall(init_per_zone_wmark_min)\n\n/*\n * min_free_kbytes_sysctl_handler - just a wrapper around proc_dointvec() so\n *\tthat we can call two helper functions whenever min_free_kbytes\n *\tchanges.\n */\nint min_free_kbytes_sysctl_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tif (write) {\n\t\tuser_min_free_kbytes = min_free_kbytes;\n\t\tsetup_per_zone_wmarks();\n\t}\n\treturn 0;\n}\n\nint watermark_scale_factor_sysctl_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tif (write)\n\t\tsetup_per_zone_wmarks();\n\n\treturn 0;\n}\n\n#ifdef CONFIG_NUMA\nstatic void setup_min_unmapped_ratio(void)\n{\n\tpg_data_t *pgdat;\n\tstruct zone *zone;\n\n\tfor_each_online_pgdat(pgdat)\n\t\tpgdat->min_unmapped_pages = 0;\n\n\tfor_each_zone(zone)\n\t\tzone->zone_pgdat->min_unmapped_pages += (zone_managed_pages(zone) *\n\t\t\t\t\t\t         sysctl_min_unmapped_ratio) / 100;\n}\n\n\nint sysctl_min_unmapped_ratio_sysctl_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tsetup_min_unmapped_ratio();\n\n\treturn 0;\n}\n\nstatic void setup_min_slab_ratio(void)\n{\n\tpg_data_t *pgdat;\n\tstruct zone *zone;\n\n\tfor_each_online_pgdat(pgdat)\n\t\tpgdat->min_slab_pages = 0;\n\n\tfor_each_zone(zone)\n\t\tzone->zone_pgdat->min_slab_pages += (zone_managed_pages(zone) *\n\t\t\t\t\t\t     sysctl_min_slab_ratio) / 100;\n}\n\nint sysctl_min_slab_ratio_sysctl_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tint rc;\n\n\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (rc)\n\t\treturn rc;\n\n\tsetup_min_slab_ratio();\n\n\treturn 0;\n}\n#endif\n\n/*\n * lowmem_reserve_ratio_sysctl_handler - just a wrapper around\n *\tproc_dointvec() so that we can call setup_per_zone_lowmem_reserve()\n *\twhenever sysctl_lowmem_reserve_ratio changes.\n *\n * The reserve ratio obviously has absolutely no relation with the\n * minimum watermarks. The lowmem reserve ratio can only make sense\n * if in function of the boot time zone sizes.\n */\nint lowmem_reserve_ratio_sysctl_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tint i;\n\n\tproc_dointvec_minmax(table, write, buffer, length, ppos);\n\n\tfor (i = 0; i < MAX_NR_ZONES; i++) {\n\t\tif (sysctl_lowmem_reserve_ratio[i] < 1)\n\t\t\tsysctl_lowmem_reserve_ratio[i] = 0;\n\t}\n\n\tsetup_per_zone_lowmem_reserve();\n\treturn 0;\n}\n\n/*\n * percpu_pagelist_fraction - changes the pcp->high for each zone on each\n * cpu.  It is the fraction of total pages in each zone that a hot per cpu\n * pagelist can have before it gets flushed back to buddy allocator.\n */\nint percpu_pagelist_fraction_sysctl_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct zone *zone;\n\tint old_percpu_pagelist_fraction;\n\tint ret;\n\n\tmutex_lock(&pcp_batch_high_lock);\n\told_percpu_pagelist_fraction = percpu_pagelist_fraction;\n\n\tret = proc_dointvec_minmax(table, write, buffer, length, ppos);\n\tif (!write || ret < 0)\n\t\tgoto out;\n\n\t/* Sanity checking to avoid pcp imbalance */\n\tif (percpu_pagelist_fraction &&\n\t    percpu_pagelist_fraction < MIN_PERCPU_PAGELIST_FRACTION) {\n\t\tpercpu_pagelist_fraction = old_percpu_pagelist_fraction;\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* No change? */\n\tif (percpu_pagelist_fraction == old_percpu_pagelist_fraction)\n\t\tgoto out;\n\n\tfor_each_populated_zone(zone)\n\t\tzone_set_pageset_high_and_batch(zone);\nout:\n\tmutex_unlock(&pcp_batch_high_lock);\n\treturn ret;\n}\n\n#ifndef __HAVE_ARCH_RESERVED_KERNEL_PAGES\n/*\n * Returns the number of pages that arch has reserved but\n * is not known to alloc_large_system_hash().\n */\nstatic unsigned long __init arch_reserved_kernel_pages(void)\n{\n\treturn 0;\n}\n#endif\n\n/*\n * Adaptive scale is meant to reduce sizes of hash tables on large memory\n * machines. As memory size is increased the scale is also increased but at\n * slower pace.  Starting from ADAPT_SCALE_BASE (64G), every time memory\n * quadruples the scale is increased by one, which means the size of hash table\n * only doubles, instead of quadrupling as well.\n * Because 32-bit systems cannot have large physical memory, where this scaling\n * makes sense, it is disabled on such platforms.\n */\n#if __BITS_PER_LONG > 32\n#define ADAPT_SCALE_BASE\t(64ul << 30)\n#define ADAPT_SCALE_SHIFT\t2\n#define ADAPT_SCALE_NPAGES\t(ADAPT_SCALE_BASE >> PAGE_SHIFT)\n#endif\n\n/*\n * allocate a large system hash table from bootmem\n * - it is assumed that the hash table must contain an exact power-of-2\n *   quantity of entries\n * - limit is the number of hash buckets, not the total allocation size\n */\nvoid *__init alloc_large_system_hash(const char *tablename,\n\t\t\t\t     unsigned long bucketsize,\n\t\t\t\t     unsigned long numentries,\n\t\t\t\t     int scale,\n\t\t\t\t     int flags,\n\t\t\t\t     unsigned int *_hash_shift,\n\t\t\t\t     unsigned int *_hash_mask,\n\t\t\t\t     unsigned long low_limit,\n\t\t\t\t     unsigned long high_limit)\n{\n\tunsigned long long max = high_limit;\n\tunsigned long log2qty, size;\n\tvoid *table = NULL;\n\tgfp_t gfp_flags;\n\tbool virt;\n\n\t/* allow the kernel cmdline to have a say */\n\tif (!numentries) {\n\t\t/* round applicable memory size up to nearest megabyte */\n\t\tnumentries = nr_kernel_pages;\n\t\tnumentries -= arch_reserved_kernel_pages();\n\n\t\t/* It isn't necessary when PAGE_SIZE >= 1MB */\n\t\tif (PAGE_SHIFT < 20)\n\t\t\tnumentries = round_up(numentries, (1<<20)/PAGE_SIZE);\n\n#if __BITS_PER_LONG > 32\n\t\tif (!high_limit) {\n\t\t\tunsigned long adapt;\n\n\t\t\tfor (adapt = ADAPT_SCALE_NPAGES; adapt < numentries;\n\t\t\t     adapt <<= ADAPT_SCALE_SHIFT)\n\t\t\t\tscale++;\n\t\t}\n#endif\n\n\t\t/* limit to 1 bucket per 2^scale bytes of low memory */\n\t\tif (scale > PAGE_SHIFT)\n\t\t\tnumentries >>= (scale - PAGE_SHIFT);\n\t\telse\n\t\t\tnumentries <<= (PAGE_SHIFT - scale);\n\n\t\t/* Make sure we've got at least a 0-order allocation.. */\n\t\tif (unlikely(flags & HASH_SMALL)) {\n\t\t\t/* Makes no sense without HASH_EARLY */\n\t\t\tWARN_ON(!(flags & HASH_EARLY));\n\t\t\tif (!(numentries >> *_hash_shift)) {\n\t\t\t\tnumentries = 1UL << *_hash_shift;\n\t\t\t\tBUG_ON(!numentries);\n\t\t\t}\n\t\t} else if (unlikely((numentries * bucketsize) < PAGE_SIZE))\n\t\t\tnumentries = PAGE_SIZE / bucketsize;\n\t}\n\tnumentries = roundup_pow_of_two(numentries);\n\n\t/* limit allocation size to 1/16 total memory by default */\n\tif (max == 0) {\n\t\tmax = ((unsigned long long)nr_all_pages << PAGE_SHIFT) >> 4;\n\t\tdo_div(max, bucketsize);\n\t}\n\tmax = min(max, 0x80000000ULL);\n\n\tif (numentries < low_limit)\n\t\tnumentries = low_limit;\n\tif (numentries > max)\n\t\tnumentries = max;\n\n\tlog2qty = ilog2(numentries);\n\n\tgfp_flags = (flags & HASH_ZERO) ? GFP_ATOMIC | __GFP_ZERO : GFP_ATOMIC;\n\tdo {\n\t\tvirt = false;\n\t\tsize = bucketsize << log2qty;\n\t\tif (flags & HASH_EARLY) {\n\t\t\tif (flags & HASH_ZERO)\n\t\t\t\ttable = memblock_alloc(size, SMP_CACHE_BYTES);\n\t\t\telse\n\t\t\t\ttable = memblock_alloc_raw(size,\n\t\t\t\t\t\t\t   SMP_CACHE_BYTES);\n\t\t} else if (get_order(size) >= MAX_ORDER || hashdist) {\n\t\t\ttable = __vmalloc(size, gfp_flags);\n\t\t\tvirt = true;\n\t\t} else {\n\t\t\t/*\n\t\t\t * If bucketsize is not a power-of-two, we may free\n\t\t\t * some pages at the end of hash table which\n\t\t\t * alloc_pages_exact() automatically does\n\t\t\t */\n\t\t\ttable = alloc_pages_exact(size, gfp_flags);\n\t\t\tkmemleak_alloc(table, size, 1, gfp_flags);\n\t\t}\n\t} while (!table && size > PAGE_SIZE && --log2qty);\n\n\tif (!table)\n\t\tpanic(\"Failed to allocate %s hash table\\n\", tablename);\n\n\tpr_info(\"%s hash table entries: %ld (order: %d, %lu bytes, %s)\\n\",\n\t\ttablename, 1UL << log2qty, ilog2(size) - PAGE_SHIFT, size,\n\t\tvirt ? \"vmalloc\" : \"linear\");\n\n\tif (_hash_shift)\n\t\t*_hash_shift = log2qty;\n\tif (_hash_mask)\n\t\t*_hash_mask = (1 << log2qty) - 1;\n\n\treturn table;\n}\n\n/*\n * This function checks whether pageblock includes unmovable pages or not.\n *\n * PageLRU check without isolation or lru_lock could race so that\n * MIGRATE_MOVABLE block might include unmovable pages. And __PageMovable\n * check without lock_page also may miss some movable non-lru pages at\n * race condition. So you can't expect this function should be exact.\n *\n * Returns a page without holding a reference. If the caller wants to\n * dereference that page (e.g., dumping), it has to make sure that it\n * cannot get removed (e.g., via memory unplug) concurrently.\n *\n */\nstruct page *has_unmovable_pages(struct zone *zone, struct page *page,\n\t\t\t\t int migratetype, int flags)\n{\n\tunsigned long iter = 0;\n\tunsigned long pfn = page_to_pfn(page);\n\tunsigned long offset = pfn % pageblock_nr_pages;\n\n\tif (is_migrate_cma_page(page)) {\n\t\t/*\n\t\t * CMA allocations (alloc_contig_range) really need to mark\n\t\t * isolate CMA pageblocks even when they are not movable in fact\n\t\t * so consider them movable here.\n\t\t */\n\t\tif (is_migrate_cma(migratetype))\n\t\t\treturn NULL;\n\n\t\treturn page;\n\t}\n\n\tfor (; iter < pageblock_nr_pages - offset; iter++) {\n\t\tif (!pfn_valid_within(pfn + iter))\n\t\t\tcontinue;\n\n\t\tpage = pfn_to_page(pfn + iter);\n\n\t\t/*\n\t\t * Both, bootmem allocations and memory holes are marked\n\t\t * PG_reserved and are unmovable. We can even have unmovable\n\t\t * allocations inside ZONE_MOVABLE, for example when\n\t\t * specifying \"movablecore\".\n\t\t */\n\t\tif (PageReserved(page))\n\t\t\treturn page;\n\n\t\t/*\n\t\t * If the zone is movable and we have ruled out all reserved\n\t\t * pages then it should be reasonably safe to assume the rest\n\t\t * is movable.\n\t\t */\n\t\tif (zone_idx(zone) == ZONE_MOVABLE)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Hugepages are not in LRU lists, but they're movable.\n\t\t * THPs are on the LRU, but need to be counted as #small pages.\n\t\t * We need not scan over tail pages because we don't\n\t\t * handle each tail page individually in migration.\n\t\t */\n\t\tif (PageHuge(page) || PageTransCompound(page)) {\n\t\t\tstruct page *head = compound_head(page);\n\t\t\tunsigned int skip_pages;\n\n\t\t\tif (PageHuge(page)) {\n\t\t\t\tif (!hugepage_migration_supported(page_hstate(head)))\n\t\t\t\t\treturn page;\n\t\t\t} else if (!PageLRU(head) && !__PageMovable(head)) {\n\t\t\t\treturn page;\n\t\t\t}\n\n\t\t\tskip_pages = compound_nr(head) - (page - head);\n\t\t\titer += skip_pages - 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * We can't use page_count without pin a page\n\t\t * because another CPU can free compound page.\n\t\t * This check already skips compound tails of THP\n\t\t * because their page->_refcount is zero at all time.\n\t\t */\n\t\tif (!page_ref_count(page)) {\n\t\t\tif (PageBuddy(page))\n\t\t\t\titer += (1 << buddy_order(page)) - 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * The HWPoisoned page may be not in buddy system, and\n\t\t * page_count() is not 0.\n\t\t */\n\t\tif ((flags & MEMORY_OFFLINE) && PageHWPoison(page))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * We treat all PageOffline() pages as movable when offlining\n\t\t * to give drivers a chance to decrement their reference count\n\t\t * in MEM_GOING_OFFLINE in order to indicate that these pages\n\t\t * can be offlined as there are no direct references anymore.\n\t\t * For actually unmovable PageOffline() where the driver does\n\t\t * not support this, we will fail later when trying to actually\n\t\t * move these pages that still have a reference count > 0.\n\t\t * (false negatives in this function only)\n\t\t */\n\t\tif ((flags & MEMORY_OFFLINE) && PageOffline(page))\n\t\t\tcontinue;\n\n\t\tif (__PageMovable(page) || PageLRU(page))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * If there are RECLAIMABLE pages, we need to check\n\t\t * it.  But now, memory offline itself doesn't call\n\t\t * shrink_node_slabs() and it still to be fixed.\n\t\t */\n\t\treturn page;\n\t}\n\treturn NULL;\n}\n\n#ifdef CONFIG_CONTIG_ALLOC\nstatic unsigned long pfn_max_align_down(unsigned long pfn)\n{\n\treturn pfn & ~(max_t(unsigned long, MAX_ORDER_NR_PAGES,\n\t\t\t     pageblock_nr_pages) - 1);\n}\n\nstatic unsigned long pfn_max_align_up(unsigned long pfn)\n{\n\treturn ALIGN(pfn, max_t(unsigned long, MAX_ORDER_NR_PAGES,\n\t\t\t\tpageblock_nr_pages));\n}\n\n#if defined(CONFIG_DYNAMIC_DEBUG) || \\\n\t(defined(CONFIG_DYNAMIC_DEBUG_CORE) && defined(DYNAMIC_DEBUG_MODULE))\n/* Usage: See admin-guide/dynamic-debug-howto.rst */\nstatic void alloc_contig_dump_pages(struct list_head *page_list)\n{\n\tDEFINE_DYNAMIC_DEBUG_METADATA(descriptor, \"migrate failure\");\n\n\tif (DYNAMIC_DEBUG_BRANCH(descriptor)) {\n\t\tstruct page *page;\n\n\t\tdump_stack();\n\t\tlist_for_each_entry(page, page_list, lru)\n\t\t\tdump_page(page, \"migration failure\");\n\t}\n}\n#else\nstatic inline void alloc_contig_dump_pages(struct list_head *page_list)\n{\n}\n#endif\n\n/* [start, end) must belong to a single zone. */\nstatic int __alloc_contig_migrate_range(struct compact_control *cc,\n\t\t\t\t\tunsigned long start, unsigned long end)\n{\n\t/* This function is based on compact_zone() from compaction.c. */\n\tunsigned int nr_reclaimed;\n\tunsigned long pfn = start;\n\tunsigned int tries = 0;\n\tint ret = 0;\n\tstruct migration_target_control mtc = {\n\t\t.nid = zone_to_nid(cc->zone),\n\t\t.gfp_mask = GFP_USER | __GFP_MOVABLE | __GFP_RETRY_MAYFAIL,\n\t};\n\n\tmigrate_prep();\n\n\twhile (pfn < end || !list_empty(&cc->migratepages)) {\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (list_empty(&cc->migratepages)) {\n\t\t\tcc->nr_migratepages = 0;\n\t\t\tpfn = isolate_migratepages_range(cc, pfn, end);\n\t\t\tif (!pfn) {\n\t\t\t\tret = -EINTR;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttries = 0;\n\t\t} else if (++tries == 5) {\n\t\t\tret = ret < 0 ? ret : -EBUSY;\n\t\t\tbreak;\n\t\t}\n\n\t\tnr_reclaimed = reclaim_clean_pages_from_list(cc->zone,\n\t\t\t\t\t\t\t&cc->migratepages);\n\t\tcc->nr_migratepages -= nr_reclaimed;\n\n\t\tret = migrate_pages(&cc->migratepages, alloc_migration_target,\n\t\t\t\tNULL, (unsigned long)&mtc, cc->mode, MR_CONTIG_RANGE);\n\t}\n\tif (ret < 0) {\n\t\talloc_contig_dump_pages(&cc->migratepages);\n\t\tputback_movable_pages(&cc->migratepages);\n\t\treturn ret;\n\t}\n\treturn 0;\n}\n\n/**\n * alloc_contig_range() -- tries to allocate given range of pages\n * @start:\tstart PFN to allocate\n * @end:\tone-past-the-last PFN to allocate\n * @migratetype:\tmigratetype of the underlaying pageblocks (either\n *\t\t\t#MIGRATE_MOVABLE or #MIGRATE_CMA).  All pageblocks\n *\t\t\tin range must have the same migratetype and it must\n *\t\t\tbe either of the two.\n * @gfp_mask:\tGFP mask to use during compaction\n *\n * The PFN range does not have to be pageblock or MAX_ORDER_NR_PAGES\n * aligned.  The PFN range must belong to a single zone.\n *\n * The first thing this routine does is attempt to MIGRATE_ISOLATE all\n * pageblocks in the range.  Once isolated, the pageblocks should not\n * be modified by others.\n *\n * Return: zero on success or negative error code.  On success all\n * pages which PFN is in [start, end) are allocated for the caller and\n * need to be freed with free_contig_range().\n */\nint alloc_contig_range(unsigned long start, unsigned long end,\n\t\t       unsigned migratetype, gfp_t gfp_mask)\n{\n\tunsigned long outer_start, outer_end;\n\tunsigned int order;\n\tint ret = 0;\n\n\tstruct compact_control cc = {\n\t\t.nr_migratepages = 0,\n\t\t.order = -1,\n\t\t.zone = page_zone(pfn_to_page(start)),\n\t\t.mode = MIGRATE_SYNC,\n\t\t.ignore_skip_hint = true,\n\t\t.no_set_skip_hint = true,\n\t\t.gfp_mask = current_gfp_context(gfp_mask),\n\t\t.alloc_contig = true,\n\t};\n\tINIT_LIST_HEAD(&cc.migratepages);\n\n\t/*\n\t * What we do here is we mark all pageblocks in range as\n\t * MIGRATE_ISOLATE.  Because pageblock and max order pages may\n\t * have different sizes, and due to the way page allocator\n\t * work, we align the range to biggest of the two pages so\n\t * that page allocator won't try to merge buddies from\n\t * different pageblocks and change MIGRATE_ISOLATE to some\n\t * other migration type.\n\t *\n\t * Once the pageblocks are marked as MIGRATE_ISOLATE, we\n\t * migrate the pages from an unaligned range (ie. pages that\n\t * we are interested in).  This will put all the pages in\n\t * range back to page allocator as MIGRATE_ISOLATE.\n\t *\n\t * When this is done, we take the pages in range from page\n\t * allocator removing them from the buddy system.  This way\n\t * page allocator will never consider using them.\n\t *\n\t * This lets us mark the pageblocks back as\n\t * MIGRATE_CMA/MIGRATE_MOVABLE so that free pages in the\n\t * aligned range but not in the unaligned, original range are\n\t * put back to page allocator so that buddy can use them.\n\t */\n\n\tret = start_isolate_page_range(pfn_max_align_down(start),\n\t\t\t\t       pfn_max_align_up(end), migratetype, 0);\n\tif (ret)\n\t\treturn ret;\n\n\tdrain_all_pages(cc.zone);\n\n\t/*\n\t * In case of -EBUSY, we'd like to know which page causes problem.\n\t * So, just fall through. test_pages_isolated() has a tracepoint\n\t * which will report the busy page.\n\t *\n\t * It is possible that busy pages could become available before\n\t * the call to test_pages_isolated, and the range will actually be\n\t * allocated.  So, if we fall through be sure to clear ret so that\n\t * -EBUSY is not accidentally used or returned to caller.\n\t */\n\tret = __alloc_contig_migrate_range(&cc, start, end);\n\tif (ret && ret != -EBUSY)\n\t\tgoto done;\n\tret = 0;\n\n\t/*\n\t * Pages from [start, end) are within a MAX_ORDER_NR_PAGES\n\t * aligned blocks that are marked as MIGRATE_ISOLATE.  What's\n\t * more, all pages in [start, end) are free in page allocator.\n\t * What we are going to do is to allocate all pages from\n\t * [start, end) (that is remove them from page allocator).\n\t *\n\t * The only problem is that pages at the beginning and at the\n\t * end of interesting range may be not aligned with pages that\n\t * page allocator holds, ie. they can be part of higher order\n\t * pages.  Because of this, we reserve the bigger range and\n\t * once this is done free the pages we are not interested in.\n\t *\n\t * We don't have to hold zone->lock here because the pages are\n\t * isolated thus they won't get removed from buddy.\n\t */\n\n\torder = 0;\n\touter_start = start;\n\twhile (!PageBuddy(pfn_to_page(outer_start))) {\n\t\tif (++order >= MAX_ORDER) {\n\t\t\touter_start = start;\n\t\t\tbreak;\n\t\t}\n\t\touter_start &= ~0UL << order;\n\t}\n\n\tif (outer_start != start) {\n\t\torder = buddy_order(pfn_to_page(outer_start));\n\n\t\t/*\n\t\t * outer_start page could be small order buddy page and\n\t\t * it doesn't include start page. Adjust outer_start\n\t\t * in this case to report failed page properly\n\t\t * on tracepoint in test_pages_isolated()\n\t\t */\n\t\tif (outer_start + (1UL << order) <= start)\n\t\t\touter_start = start;\n\t}\n\n\t/* Make sure the range is really isolated. */\n\tif (test_pages_isolated(outer_start, end, 0)) {\n\t\tret = -EBUSY;\n\t\tgoto done;\n\t}\n\n\t/* Grab isolated pages from freelists. */\n\touter_end = isolate_freepages_range(&cc, outer_start, end);\n\tif (!outer_end) {\n\t\tret = -EBUSY;\n\t\tgoto done;\n\t}\n\n\t/* Free head and tail (if any) */\n\tif (start != outer_start)\n\t\tfree_contig_range(outer_start, start - outer_start);\n\tif (end != outer_end)\n\t\tfree_contig_range(end, outer_end - end);\n\ndone:\n\tundo_isolate_page_range(pfn_max_align_down(start),\n\t\t\t\tpfn_max_align_up(end), migratetype);\n\treturn ret;\n}\nEXPORT_SYMBOL(alloc_contig_range);\n\nstatic int __alloc_contig_pages(unsigned long start_pfn,\n\t\t\t\tunsigned long nr_pages, gfp_t gfp_mask)\n{\n\tunsigned long end_pfn = start_pfn + nr_pages;\n\n\treturn alloc_contig_range(start_pfn, end_pfn, MIGRATE_MOVABLE,\n\t\t\t\t  gfp_mask);\n}\n\nstatic bool pfn_range_valid_contig(struct zone *z, unsigned long start_pfn,\n\t\t\t\t   unsigned long nr_pages)\n{\n\tunsigned long i, end_pfn = start_pfn + nr_pages;\n\tstruct page *page;\n\n\tfor (i = start_pfn; i < end_pfn; i++) {\n\t\tpage = pfn_to_online_page(i);\n\t\tif (!page)\n\t\t\treturn false;\n\n\t\tif (page_zone(page) != z)\n\t\t\treturn false;\n\n\t\tif (PageReserved(page))\n\t\t\treturn false;\n\n\t\tif (page_count(page) > 0)\n\t\t\treturn false;\n\n\t\tif (PageHuge(page))\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic bool zone_spans_last_pfn(const struct zone *zone,\n\t\t\t\tunsigned long start_pfn, unsigned long nr_pages)\n{\n\tunsigned long last_pfn = start_pfn + nr_pages - 1;\n\n\treturn zone_spans_pfn(zone, last_pfn);\n}\n\n/**\n * alloc_contig_pages() -- tries to find and allocate contiguous range of pages\n * @nr_pages:\tNumber of contiguous pages to allocate\n * @gfp_mask:\tGFP mask to limit search and used during compaction\n * @nid:\tTarget node\n * @nodemask:\tMask for other possible nodes\n *\n * This routine is a wrapper around alloc_contig_range(). It scans over zones\n * on an applicable zonelist to find a contiguous pfn range which can then be\n * tried for allocation with alloc_contig_range(). This routine is intended\n * for allocation requests which can not be fulfilled with the buddy allocator.\n *\n * The allocated memory is always aligned to a page boundary. If nr_pages is a\n * power of two then the alignment is guaranteed to be to the given nr_pages\n * (e.g. 1GB request would be aligned to 1GB).\n *\n * Allocated pages can be freed with free_contig_range() or by manually calling\n * __free_page() on each allocated page.\n *\n * Return: pointer to contiguous pages on success, or NULL if not successful.\n */\nstruct page *alloc_contig_pages(unsigned long nr_pages, gfp_t gfp_mask,\n\t\t\t\tint nid, nodemask_t *nodemask)\n{\n\tunsigned long ret, pfn, flags;\n\tstruct zonelist *zonelist;\n\tstruct zone *zone;\n\tstruct zoneref *z;\n\n\tzonelist = node_zonelist(nid, gfp_mask);\n\tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n\t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n\t\tspin_lock_irqsave(&zone->lock, flags);\n\n\t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n\t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n\t\t\tif (pfn_range_valid_contig(zone, pfn, nr_pages)) {\n\t\t\t\t/*\n\t\t\t\t * We release the zone lock here because\n\t\t\t\t * alloc_contig_range() will also lock the zone\n\t\t\t\t * at some point. If there's an allocation\n\t\t\t\t * spinning on this lock, it may win the race\n\t\t\t\t * and cause alloc_contig_range() to fail...\n\t\t\t\t */\n\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t\t\t\tret = __alloc_contig_pages(pfn, nr_pages,\n\t\t\t\t\t\t\tgfp_mask);\n\t\t\t\tif (!ret)\n\t\t\t\t\treturn pfn_to_page(pfn);\n\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n\t\t\t}\n\t\t\tpfn += nr_pages;\n\t\t}\n\t\tspin_unlock_irqrestore(&zone->lock, flags);\n\t}\n\treturn NULL;\n}\n#endif /* CONFIG_CONTIG_ALLOC */\n\nvoid free_contig_range(unsigned long pfn, unsigned int nr_pages)\n{\n\tunsigned int count = 0;\n\n\tfor (; nr_pages--; pfn++) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\tcount += page_count(page) != 1;\n\t\t__free_page(page);\n\t}\n\tWARN(count != 0, \"%d pages are still in use!\\n\", count);\n}\nEXPORT_SYMBOL(free_contig_range);\n\n/*\n * The zone indicated has a new number of managed_pages; batch sizes and percpu\n * page high values need to be recalulated.\n */\nvoid __meminit zone_pcp_update(struct zone *zone)\n{\n\tmutex_lock(&pcp_batch_high_lock);\n\tzone_set_pageset_high_and_batch(zone);\n\tmutex_unlock(&pcp_batch_high_lock);\n}\n\n/*\n * Effectively disable pcplists for the zone by setting the high limit to 0\n * and draining all cpus. A concurrent page freeing on another CPU that's about\n * to put the page on pcplist will either finish before the drain and the page\n * will be drained, or observe the new high limit and skip the pcplist.\n *\n * Must be paired with a call to zone_pcp_enable().\n */\nvoid zone_pcp_disable(struct zone *zone)\n{\n\tmutex_lock(&pcp_batch_high_lock);\n\t__zone_set_pageset_high_and_batch(zone, 0, 1);\n\t__drain_all_pages(zone, true);\n}\n\nvoid zone_pcp_enable(struct zone *zone)\n{\n\t__zone_set_pageset_high_and_batch(zone, zone->pageset_high, zone->pageset_batch);\n\tmutex_unlock(&pcp_batch_high_lock);\n}\n\nvoid zone_pcp_reset(struct zone *zone)\n{\n\tunsigned long flags;\n\tint cpu;\n\tstruct per_cpu_pageset *pset;\n\n\t/* avoid races with drain_pages()  */\n\tlocal_irq_save(flags);\n\tif (zone->pageset != &boot_pageset) {\n\t\tfor_each_online_cpu(cpu) {\n\t\t\tpset = per_cpu_ptr(zone->pageset, cpu);\n\t\t\tdrain_zonestat(zone, pset);\n\t\t}\n\t\tfree_percpu(zone->pageset);\n\t\tzone->pageset = &boot_pageset;\n\t}\n\tlocal_irq_restore(flags);\n}\n\n#ifdef CONFIG_MEMORY_HOTREMOVE\n/*\n * All pages in the range must be in a single zone, must not contain holes,\n * must span full sections, and must be isolated before calling this function.\n */\nvoid __offline_isolated_pages(unsigned long start_pfn, unsigned long end_pfn,\n\t\t\t      unsigned long buddy_start_pfn)\n{\n\tunsigned long pfn = start_pfn;\n\tstruct page *page;\n\tstruct zone *zone;\n\tunsigned int order;\n\tunsigned long flags;\n\n\toffline_mem_sections(pfn, end_pfn);\n\tzone = page_zone(pfn_to_page(pfn));\n\tspin_lock_irqsave(&zone->lock, flags);\n\tpfn = buddy_start_pfn;\n\twhile (pfn < end_pfn) {\n\t\tpage = pfn_to_page(pfn);\n\t\t/*\n\t\t * The HWPoisoned page may be not in buddy system, and\n\t\t * page_count() is not 0.\n\t\t */\n\t\tif (unlikely(!PageBuddy(page) && PageHWPoison(page))) {\n\t\t\tpfn++;\n\t\t\tcontinue;\n\t\t}\n\t\t/*\n\t\t * At this point all remaining PageOffline() pages have a\n\t\t * reference count of 0 and can simply be skipped.\n\t\t */\n\t\tif (PageOffline(page)) {\n\t\t\tBUG_ON(page_count(page));\n\t\t\tBUG_ON(PageBuddy(page));\n\t\t\tpfn++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tBUG_ON(page_count(page));\n\t\tBUG_ON(!PageBuddy(page));\n\t\torder = buddy_order(page);\n\t\tdel_page_from_free_list(page, zone, order);\n\t\tpfn += (1 << order);\n\t}\n\tspin_unlock_irqrestore(&zone->lock, flags);\n}\n#endif\n\nbool is_free_buddy_page(struct page *page)\n{\n\tstruct zone *zone = page_zone(page);\n\tunsigned long pfn = page_to_pfn(page);\n\tunsigned long flags;\n\tunsigned int order;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\tfor (order = 0; order < MAX_ORDER; order++) {\n\t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n\n\t\tif (PageBuddy(page_head) && buddy_order(page_head) >= order)\n\t\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&zone->lock, flags);\n\n\treturn order < MAX_ORDER;\n}\n\n#ifdef CONFIG_MEMORY_FAILURE\n/*\n * Break down a higher-order page in sub-pages, and keep our target out of\n * buddy allocator.\n */\nstatic void break_down_buddy_pages(struct zone *zone, struct page *page,\n\t\t\t\t   struct page *target, int low, int high,\n\t\t\t\t   int migratetype)\n{\n\tunsigned long size = 1 << high;\n\tstruct page *current_buddy, *next_page;\n\n\twhile (high > low) {\n\t\thigh--;\n\t\tsize >>= 1;\n\n\t\tif (target >= &page[size]) {\n\t\t\tnext_page = page + size;\n\t\t\tcurrent_buddy = page;\n\t\t} else {\n\t\t\tnext_page = page;\n\t\t\tcurrent_buddy = page + size;\n\t\t}\n\n\t\tif (set_page_guard(zone, current_buddy, high, migratetype))\n\t\t\tcontinue;\n\n\t\tif (current_buddy != target) {\n\t\t\tadd_to_free_list(current_buddy, zone, high, migratetype);\n\t\t\tset_buddy_order(current_buddy, high);\n\t\t\tpage = next_page;\n\t\t}\n\t}\n}\n\n/*\n * Take a page that will be marked as poisoned off the buddy allocator.\n */\nbool take_page_off_buddy(struct page *page)\n{\n\tstruct zone *zone = page_zone(page);\n\tunsigned long pfn = page_to_pfn(page);\n\tunsigned long flags;\n\tunsigned int order;\n\tbool ret = false;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\tfor (order = 0; order < MAX_ORDER; order++) {\n\t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n\t\tint page_order = buddy_order(page_head);\n\n\t\tif (PageBuddy(page_head) && page_order >= order) {\n\t\t\tunsigned long pfn_head = page_to_pfn(page_head);\n\t\t\tint migratetype = get_pfnblock_migratetype(page_head,\n\t\t\t\t\t\t\t\t   pfn_head);\n\n\t\t\tdel_page_from_free_list(page_head, zone, page_order);\n\t\t\tbreak_down_buddy_pages(zone, page_head, page, 0,\n\t\t\t\t\t\tpage_order, migratetype);\n\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\t\tif (page_count(page_head) > 0)\n\t\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&zone->lock, flags);\n\treturn ret;\n}\n#endif\n"}, "3": {"id": 3, "path": "/src/include/linux/mmzone.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_MMZONE_H\n#define _LINUX_MMZONE_H\n\n#ifndef __ASSEMBLY__\n#ifndef __GENERATING_BOUNDS_H\n\n#include <linux/spinlock.h>\n#include <linux/list.h>\n#include <linux/wait.h>\n#include <linux/bitops.h>\n#include <linux/cache.h>\n#include <linux/threads.h>\n#include <linux/numa.h>\n#include <linux/init.h>\n#include <linux/seqlock.h>\n#include <linux/nodemask.h>\n#include <linux/pageblock-flags.h>\n#include <linux/page-flags-layout.h>\n#include <linux/atomic.h>\n#include <linux/mm_types.h>\n#include <linux/page-flags.h>\n#include <asm/page.h>\n\n/* Free memory management - zoned buddy allocator.  */\n#ifndef CONFIG_FORCE_MAX_ZONEORDER\n#define MAX_ORDER 11\n#else\n#define MAX_ORDER CONFIG_FORCE_MAX_ZONEORDER\n#endif\n#define MAX_ORDER_NR_PAGES (1 << (MAX_ORDER - 1))\n\n/*\n * PAGE_ALLOC_COSTLY_ORDER is the order at which allocations are deemed\n * costly to service.  That is between allocation orders which should\n * coalesce naturally under reasonable reclaim pressure and those which\n * will not.\n */\n#define PAGE_ALLOC_COSTLY_ORDER 3\n\nenum migratetype {\n\tMIGRATE_UNMOVABLE,\n\tMIGRATE_MOVABLE,\n\tMIGRATE_RECLAIMABLE,\n\tMIGRATE_PCPTYPES,\t/* the number of types on the pcp lists */\n\tMIGRATE_HIGHATOMIC = MIGRATE_PCPTYPES,\n#ifdef CONFIG_CMA\n\t/*\n\t * MIGRATE_CMA migration type is designed to mimic the way\n\t * ZONE_MOVABLE works.  Only movable pages can be allocated\n\t * from MIGRATE_CMA pageblocks and page allocator never\n\t * implicitly change migration type of MIGRATE_CMA pageblock.\n\t *\n\t * The way to use it is to change migratetype of a range of\n\t * pageblocks to MIGRATE_CMA which can be done by\n\t * __free_pageblock_cma() function.  What is important though\n\t * is that a range of pageblocks must be aligned to\n\t * MAX_ORDER_NR_PAGES should biggest page be bigger then\n\t * a single pageblock.\n\t */\n\tMIGRATE_CMA,\n#endif\n#ifdef CONFIG_MEMORY_ISOLATION\n\tMIGRATE_ISOLATE,\t/* can't allocate from here */\n#endif\n\tMIGRATE_TYPES\n};\n\n/* In mm/page_alloc.c; keep in sync also with show_migration_types() there */\nextern const char * const migratetype_names[MIGRATE_TYPES];\n\n#ifdef CONFIG_CMA\n#  define is_migrate_cma(migratetype) unlikely((migratetype) == MIGRATE_CMA)\n#  define is_migrate_cma_page(_page) (get_pageblock_migratetype(_page) == MIGRATE_CMA)\n#else\n#  define is_migrate_cma(migratetype) false\n#  define is_migrate_cma_page(_page) false\n#endif\n\nstatic inline bool is_migrate_movable(int mt)\n{\n\treturn is_migrate_cma(mt) || mt == MIGRATE_MOVABLE;\n}\n\n#define for_each_migratetype_order(order, type) \\\n\tfor (order = 0; order < MAX_ORDER; order++) \\\n\t\tfor (type = 0; type < MIGRATE_TYPES; type++)\n\nextern int page_group_by_mobility_disabled;\n\n#define MIGRATETYPE_MASK ((1UL << PB_migratetype_bits) - 1)\n\n#define get_pageblock_migratetype(page)\t\t\t\t\t\\\n\tget_pfnblock_flags_mask(page, page_to_pfn(page), MIGRATETYPE_MASK)\n\nstruct free_area {\n\tstruct list_head\tfree_list[MIGRATE_TYPES];\n\tunsigned long\t\tnr_free;\n};\n\nstatic inline struct page *get_page_from_free_area(struct free_area *area,\n\t\t\t\t\t    int migratetype)\n{\n\treturn list_first_entry_or_null(&area->free_list[migratetype],\n\t\t\t\t\tstruct page, lru);\n}\n\nstatic inline bool free_area_empty(struct free_area *area, int migratetype)\n{\n\treturn list_empty(&area->free_list[migratetype]);\n}\n\nstruct pglist_data;\n\n/*\n * Add a wild amount of padding here to ensure datas fall into separate\n * cachelines.  There are very few zone structures in the machine, so space\n * consumption is not a concern here.\n */\n#if defined(CONFIG_SMP)\nstruct zone_padding {\n\tchar x[0];\n} ____cacheline_internodealigned_in_smp;\n#define ZONE_PADDING(name)\tstruct zone_padding name;\n#else\n#define ZONE_PADDING(name)\n#endif\n\n#ifdef CONFIG_NUMA\nenum numa_stat_item {\n\tNUMA_HIT,\t\t/* allocated in intended node */\n\tNUMA_MISS,\t\t/* allocated in non intended node */\n\tNUMA_FOREIGN,\t\t/* was intended here, hit elsewhere */\n\tNUMA_INTERLEAVE_HIT,\t/* interleaver preferred this zone */\n\tNUMA_LOCAL,\t\t/* allocation from local node */\n\tNUMA_OTHER,\t\t/* allocation from other node */\n\tNR_VM_NUMA_STAT_ITEMS\n};\n#else\n#define NR_VM_NUMA_STAT_ITEMS 0\n#endif\n\nenum zone_stat_item {\n\t/* First 128 byte cacheline (assuming 64 bit words) */\n\tNR_FREE_PAGES,\n\tNR_ZONE_LRU_BASE, /* Used only for compaction and reclaim retry */\n\tNR_ZONE_INACTIVE_ANON = NR_ZONE_LRU_BASE,\n\tNR_ZONE_ACTIVE_ANON,\n\tNR_ZONE_INACTIVE_FILE,\n\tNR_ZONE_ACTIVE_FILE,\n\tNR_ZONE_UNEVICTABLE,\n\tNR_ZONE_WRITE_PENDING,\t/* Count of dirty, writeback and unstable pages */\n\tNR_MLOCK,\t\t/* mlock()ed pages found and moved off LRU */\n\t/* Second 128 byte cacheline */\n\tNR_BOUNCE,\n#if IS_ENABLED(CONFIG_ZSMALLOC)\n\tNR_ZSPAGES,\t\t/* allocated in zsmalloc */\n#endif\n\tNR_FREE_CMA_PAGES,\n\tNR_VM_ZONE_STAT_ITEMS };\n\nenum node_stat_item {\n\tNR_LRU_BASE,\n\tNR_INACTIVE_ANON = NR_LRU_BASE, /* must match order of LRU_[IN]ACTIVE */\n\tNR_ACTIVE_ANON,\t\t/*  \"     \"     \"   \"       \"         */\n\tNR_INACTIVE_FILE,\t/*  \"     \"     \"   \"       \"         */\n\tNR_ACTIVE_FILE,\t\t/*  \"     \"     \"   \"       \"         */\n\tNR_UNEVICTABLE,\t\t/*  \"     \"     \"   \"       \"         */\n\tNR_SLAB_RECLAIMABLE_B,\n\tNR_SLAB_UNRECLAIMABLE_B,\n\tNR_ISOLATED_ANON,\t/* Temporary isolated pages from anon lru */\n\tNR_ISOLATED_FILE,\t/* Temporary isolated pages from file lru */\n\tWORKINGSET_NODES,\n\tWORKINGSET_REFAULT_BASE,\n\tWORKINGSET_REFAULT_ANON = WORKINGSET_REFAULT_BASE,\n\tWORKINGSET_REFAULT_FILE,\n\tWORKINGSET_ACTIVATE_BASE,\n\tWORKINGSET_ACTIVATE_ANON = WORKINGSET_ACTIVATE_BASE,\n\tWORKINGSET_ACTIVATE_FILE,\n\tWORKINGSET_RESTORE_BASE,\n\tWORKINGSET_RESTORE_ANON = WORKINGSET_RESTORE_BASE,\n\tWORKINGSET_RESTORE_FILE,\n\tWORKINGSET_NODERECLAIM,\n\tNR_ANON_MAPPED,\t/* Mapped anonymous pages */\n\tNR_FILE_MAPPED,\t/* pagecache pages mapped into pagetables.\n\t\t\t   only modified from process context */\n\tNR_FILE_PAGES,\n\tNR_FILE_DIRTY,\n\tNR_WRITEBACK,\n\tNR_WRITEBACK_TEMP,\t/* Writeback using temporary buffers */\n\tNR_SHMEM,\t\t/* shmem pages (included tmpfs/GEM pages) */\n\tNR_SHMEM_THPS,\n\tNR_SHMEM_PMDMAPPED,\n\tNR_FILE_THPS,\n\tNR_FILE_PMDMAPPED,\n\tNR_ANON_THPS,\n\tNR_VMSCAN_WRITE,\n\tNR_VMSCAN_IMMEDIATE,\t/* Prioritise for reclaim when writeback ends */\n\tNR_DIRTIED,\t\t/* page dirtyings since bootup */\n\tNR_WRITTEN,\t\t/* page writings since bootup */\n\tNR_KERNEL_MISC_RECLAIMABLE,\t/* reclaimable non-slab kernel pages */\n\tNR_FOLL_PIN_ACQUIRED,\t/* via: pin_user_page(), gup flag: FOLL_PIN */\n\tNR_FOLL_PIN_RELEASED,\t/* pages returned via unpin_user_page() */\n\tNR_KERNEL_STACK_KB,\t/* measured in KiB */\n#if IS_ENABLED(CONFIG_SHADOW_CALL_STACK)\n\tNR_KERNEL_SCS_KB,\t/* measured in KiB */\n#endif\n\tNR_PAGETABLE,\t\t/* used for pagetables */\n#ifdef CONFIG_SWAP\n\tNR_SWAPCACHE,\n#endif\n\tNR_VM_NODE_STAT_ITEMS\n};\n\n/*\n * Returns true if the item should be printed in THPs (/proc/vmstat\n * currently prints number of anon, file and shmem THPs. But the item\n * is charged in pages).\n */\nstatic __always_inline bool vmstat_item_print_in_thp(enum node_stat_item item)\n{\n\tif (!IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE))\n\t\treturn false;\n\n\treturn item == NR_ANON_THPS ||\n\t       item == NR_FILE_THPS ||\n\t       item == NR_SHMEM_THPS ||\n\t       item == NR_SHMEM_PMDMAPPED ||\n\t       item == NR_FILE_PMDMAPPED;\n}\n\n/*\n * Returns true if the value is measured in bytes (most vmstat values are\n * measured in pages). This defines the API part, the internal representation\n * might be different.\n */\nstatic __always_inline bool vmstat_item_in_bytes(int idx)\n{\n\t/*\n\t * Global and per-node slab counters track slab pages.\n\t * It's expected that changes are multiples of PAGE_SIZE.\n\t * Internally values are stored in pages.\n\t *\n\t * Per-memcg and per-lruvec counters track memory, consumed\n\t * by individual slab objects. These counters are actually\n\t * byte-precise.\n\t */\n\treturn (idx == NR_SLAB_RECLAIMABLE_B ||\n\t\tidx == NR_SLAB_UNRECLAIMABLE_B);\n}\n\n/*\n * We do arithmetic on the LRU lists in various places in the code,\n * so it is important to keep the active lists LRU_ACTIVE higher in\n * the array than the corresponding inactive lists, and to keep\n * the *_FILE lists LRU_FILE higher than the corresponding _ANON lists.\n *\n * This has to be kept in sync with the statistics in zone_stat_item\n * above and the descriptions in vmstat_text in mm/vmstat.c\n */\n#define LRU_BASE 0\n#define LRU_ACTIVE 1\n#define LRU_FILE 2\n\nenum lru_list {\n\tLRU_INACTIVE_ANON = LRU_BASE,\n\tLRU_ACTIVE_ANON = LRU_BASE + LRU_ACTIVE,\n\tLRU_INACTIVE_FILE = LRU_BASE + LRU_FILE,\n\tLRU_ACTIVE_FILE = LRU_BASE + LRU_FILE + LRU_ACTIVE,\n\tLRU_UNEVICTABLE,\n\tNR_LRU_LISTS\n};\n\n#define for_each_lru(lru) for (lru = 0; lru < NR_LRU_LISTS; lru++)\n\n#define for_each_evictable_lru(lru) for (lru = 0; lru <= LRU_ACTIVE_FILE; lru++)\n\nstatic inline bool is_file_lru(enum lru_list lru)\n{\n\treturn (lru == LRU_INACTIVE_FILE || lru == LRU_ACTIVE_FILE);\n}\n\nstatic inline bool is_active_lru(enum lru_list lru)\n{\n\treturn (lru == LRU_ACTIVE_ANON || lru == LRU_ACTIVE_FILE);\n}\n\n#define ANON_AND_FILE 2\n\nenum lruvec_flags {\n\tLRUVEC_CONGESTED,\t\t/* lruvec has many dirty pages\n\t\t\t\t\t * backed by a congested BDI\n\t\t\t\t\t */\n};\n\nstruct lruvec {\n\tstruct list_head\t\tlists[NR_LRU_LISTS];\n\t/* per lruvec lru_lock for memcg */\n\tspinlock_t\t\t\tlru_lock;\n\t/*\n\t * These track the cost of reclaiming one LRU - file or anon -\n\t * over the other. As the observed cost of reclaiming one LRU\n\t * increases, the reclaim scan balance tips toward the other.\n\t */\n\tunsigned long\t\t\tanon_cost;\n\tunsigned long\t\t\tfile_cost;\n\t/* Non-resident age, driven by LRU movement */\n\tatomic_long_t\t\t\tnonresident_age;\n\t/* Refaults at the time of last reclaim cycle */\n\tunsigned long\t\t\trefaults[ANON_AND_FILE];\n\t/* Various lruvec state flags (enum lruvec_flags) */\n\tunsigned long\t\t\tflags;\n#ifdef CONFIG_MEMCG\n\tstruct pglist_data *pgdat;\n#endif\n};\n\n/* Isolate unmapped pages */\n#define ISOLATE_UNMAPPED\t((__force isolate_mode_t)0x2)\n/* Isolate for asynchronous migration */\n#define ISOLATE_ASYNC_MIGRATE\t((__force isolate_mode_t)0x4)\n/* Isolate unevictable pages */\n#define ISOLATE_UNEVICTABLE\t((__force isolate_mode_t)0x8)\n\n/* LRU Isolation modes. */\ntypedef unsigned __bitwise isolate_mode_t;\n\nenum zone_watermarks {\n\tWMARK_MIN,\n\tWMARK_LOW,\n\tWMARK_HIGH,\n\tNR_WMARK\n};\n\n#define min_wmark_pages(z) (z->_watermark[WMARK_MIN] + z->watermark_boost)\n#define low_wmark_pages(z) (z->_watermark[WMARK_LOW] + z->watermark_boost)\n#define high_wmark_pages(z) (z->_watermark[WMARK_HIGH] + z->watermark_boost)\n#define wmark_pages(z, i) (z->_watermark[i] + z->watermark_boost)\n\nstruct per_cpu_pages {\n\tint count;\t\t/* number of pages in the list */\n\tint high;\t\t/* high watermark, emptying needed */\n\tint batch;\t\t/* chunk size for buddy add/remove */\n\n\t/* Lists of pages, one per migrate type stored on the pcp-lists */\n\tstruct list_head lists[MIGRATE_PCPTYPES];\n};\n\nstruct per_cpu_pageset {\n\tstruct per_cpu_pages pcp;\n#ifdef CONFIG_NUMA\n\ts8 expire;\n\tu16 vm_numa_stat_diff[NR_VM_NUMA_STAT_ITEMS];\n#endif\n#ifdef CONFIG_SMP\n\ts8 stat_threshold;\n\ts8 vm_stat_diff[NR_VM_ZONE_STAT_ITEMS];\n#endif\n};\n\nstruct per_cpu_nodestat {\n\ts8 stat_threshold;\n\ts8 vm_node_stat_diff[NR_VM_NODE_STAT_ITEMS];\n};\n\n#endif /* !__GENERATING_BOUNDS.H */\n\nenum zone_type {\n\t/*\n\t * ZONE_DMA and ZONE_DMA32 are used when there are peripherals not able\n\t * to DMA to all of the addressable memory (ZONE_NORMAL).\n\t * On architectures where this area covers the whole 32 bit address\n\t * space ZONE_DMA32 is used. ZONE_DMA is left for the ones with smaller\n\t * DMA addressing constraints. This distinction is important as a 32bit\n\t * DMA mask is assumed when ZONE_DMA32 is defined. Some 64-bit\n\t * platforms may need both zones as they support peripherals with\n\t * different DMA addressing limitations.\n\t */\n#ifdef CONFIG_ZONE_DMA\n\tZONE_DMA,\n#endif\n#ifdef CONFIG_ZONE_DMA32\n\tZONE_DMA32,\n#endif\n\t/*\n\t * Normal addressable memory is in ZONE_NORMAL. DMA operations can be\n\t * performed on pages in ZONE_NORMAL if the DMA devices support\n\t * transfers to all addressable memory.\n\t */\n\tZONE_NORMAL,\n#ifdef CONFIG_HIGHMEM\n\t/*\n\t * A memory area that is only addressable by the kernel through\n\t * mapping portions into its own address space. This is for example\n\t * used by i386 to allow the kernel to address the memory beyond\n\t * 900MB. The kernel will set up special mappings (page\n\t * table entries on i386) for each page that the kernel needs to\n\t * access.\n\t */\n\tZONE_HIGHMEM,\n#endif\n\t/*\n\t * ZONE_MOVABLE is similar to ZONE_NORMAL, except that it contains\n\t * movable pages with few exceptional cases described below. Main use\n\t * cases for ZONE_MOVABLE are to make memory offlining/unplug more\n\t * likely to succeed, and to locally limit unmovable allocations - e.g.,\n\t * to increase the number of THP/huge pages. Notable special cases are:\n\t *\n\t * 1. Pinned pages: (long-term) pinning of movable pages might\n\t *    essentially turn such pages unmovable. Therefore, we do not allow\n\t *    pinning long-term pages in ZONE_MOVABLE. When pages are pinned and\n\t *    faulted, they come from the right zone right away. However, it is\n\t *    still possible that address space already has pages in\n\t *    ZONE_MOVABLE at the time when pages are pinned (i.e. user has\n\t *    touches that memory before pinning). In such case we migrate them\n\t *    to a different zone. When migration fails - pinning fails.\n\t * 2. memblock allocations: kernelcore/movablecore setups might create\n\t *    situations where ZONE_MOVABLE contains unmovable allocations\n\t *    after boot. Memory offlining and allocations fail early.\n\t * 3. Memory holes: kernelcore/movablecore setups might create very rare\n\t *    situations where ZONE_MOVABLE contains memory holes after boot,\n\t *    for example, if we have sections that are only partially\n\t *    populated. Memory offlining and allocations fail early.\n\t * 4. PG_hwpoison pages: while poisoned pages can be skipped during\n\t *    memory offlining, such pages cannot be allocated.\n\t * 5. Unmovable PG_offline pages: in paravirtualized environments,\n\t *    hotplugged memory blocks might only partially be managed by the\n\t *    buddy (e.g., via XEN-balloon, Hyper-V balloon, virtio-mem). The\n\t *    parts not manged by the buddy are unmovable PG_offline pages. In\n\t *    some cases (virtio-mem), such pages can be skipped during\n\t *    memory offlining, however, cannot be moved/allocated. These\n\t *    techniques might use alloc_contig_range() to hide previously\n\t *    exposed pages from the buddy again (e.g., to implement some sort\n\t *    of memory unplug in virtio-mem).\n\t * 6. ZERO_PAGE(0), kernelcore/movablecore setups might create\n\t *    situations where ZERO_PAGE(0) which is allocated differently\n\t *    on different platforms may end up in a movable zone. ZERO_PAGE(0)\n\t *    cannot be migrated.\n\t * 7. Memory-hotplug: when using memmap_on_memory and onlining the\n\t *    memory to the MOVABLE zone, the vmemmap pages are also placed in\n\t *    such zone. Such pages cannot be really moved around as they are\n\t *    self-stored in the range, but they are treated as movable when\n\t *    the range they describe is about to be offlined.\n\t *\n\t * In general, no unmovable allocations that degrade memory offlining\n\t * should end up in ZONE_MOVABLE. Allocators (like alloc_contig_range())\n\t * have to expect that migrating pages in ZONE_MOVABLE can fail (even\n\t * if has_unmovable_pages() states that there are no unmovable pages,\n\t * there can be false negatives).\n\t */\n\tZONE_MOVABLE,\n#ifdef CONFIG_ZONE_DEVICE\n\tZONE_DEVICE,\n#endif\n\t__MAX_NR_ZONES\n\n};\n\n#ifndef __GENERATING_BOUNDS_H\n\n#define ASYNC_AND_SYNC 2\n\nstruct zone {\n\t/* Read-mostly fields */\n\n\t/* zone watermarks, access with *_wmark_pages(zone) macros */\n\tunsigned long _watermark[NR_WMARK];\n\tunsigned long watermark_boost;\n\n\tunsigned long nr_reserved_highatomic;\n\n\t/*\n\t * We don't know if the memory that we're going to allocate will be\n\t * freeable or/and it will be released eventually, so to avoid totally\n\t * wasting several GB of ram we must reserve some of the lower zone\n\t * memory (otherwise we risk to run OOM on the lower zones despite\n\t * there being tons of freeable ram on the higher zones).  This array is\n\t * recalculated at runtime if the sysctl_lowmem_reserve_ratio sysctl\n\t * changes.\n\t */\n\tlong lowmem_reserve[MAX_NR_ZONES];\n\n#ifdef CONFIG_NUMA\n\tint node;\n#endif\n\tstruct pglist_data\t*zone_pgdat;\n\tstruct per_cpu_pageset __percpu *pageset;\n\t/*\n\t * the high and batch values are copied to individual pagesets for\n\t * faster access\n\t */\n\tint pageset_high;\n\tint pageset_batch;\n\n#ifndef CONFIG_SPARSEMEM\n\t/*\n\t * Flags for a pageblock_nr_pages block. See pageblock-flags.h.\n\t * In SPARSEMEM, this map is stored in struct mem_section\n\t */\n\tunsigned long\t\t*pageblock_flags;\n#endif /* CONFIG_SPARSEMEM */\n\n\t/* zone_start_pfn == zone_start_paddr >> PAGE_SHIFT */\n\tunsigned long\t\tzone_start_pfn;\n\n\t/*\n\t * spanned_pages is the total pages spanned by the zone, including\n\t * holes, which is calculated as:\n\t * \tspanned_pages = zone_end_pfn - zone_start_pfn;\n\t *\n\t * present_pages is physical pages existing within the zone, which\n\t * is calculated as:\n\t *\tpresent_pages = spanned_pages - absent_pages(pages in holes);\n\t *\n\t * managed_pages is present pages managed by the buddy system, which\n\t * is calculated as (reserved_pages includes pages allocated by the\n\t * bootmem allocator):\n\t *\tmanaged_pages = present_pages - reserved_pages;\n\t *\n\t * cma pages is present pages that are assigned for CMA use\n\t * (MIGRATE_CMA).\n\t *\n\t * So present_pages may be used by memory hotplug or memory power\n\t * management logic to figure out unmanaged pages by checking\n\t * (present_pages - managed_pages). And managed_pages should be used\n\t * by page allocator and vm scanner to calculate all kinds of watermarks\n\t * and thresholds.\n\t *\n\t * Locking rules:\n\t *\n\t * zone_start_pfn and spanned_pages are protected by span_seqlock.\n\t * It is a seqlock because it has to be read outside of zone->lock,\n\t * and it is done in the main allocator path.  But, it is written\n\t * quite infrequently.\n\t *\n\t * The span_seq lock is declared along with zone->lock because it is\n\t * frequently read in proximity to zone->lock.  It's good to\n\t * give them a chance of being in the same cacheline.\n\t *\n\t * Write access to present_pages at runtime should be protected by\n\t * mem_hotplug_begin/end(). Any reader who can't tolerant drift of\n\t * present_pages should get_online_mems() to get a stable value.\n\t */\n\tatomic_long_t\t\tmanaged_pages;\n\tunsigned long\t\tspanned_pages;\n\tunsigned long\t\tpresent_pages;\n#ifdef CONFIG_CMA\n\tunsigned long\t\tcma_pages;\n#endif\n\n\tconst char\t\t*name;\n\n#ifdef CONFIG_MEMORY_ISOLATION\n\t/*\n\t * Number of isolated pageblock. It is used to solve incorrect\n\t * freepage counting problem due to racy retrieving migratetype\n\t * of pageblock. Protected by zone->lock.\n\t */\n\tunsigned long\t\tnr_isolate_pageblock;\n#endif\n\n#ifdef CONFIG_MEMORY_HOTPLUG\n\t/* see spanned/present_pages for more description */\n\tseqlock_t\t\tspan_seqlock;\n#endif\n\n\tint initialized;\n\n\t/* Write-intensive fields used from the page allocator */\n\tZONE_PADDING(_pad1_)\n\n\t/* free areas of different sizes */\n\tstruct free_area\tfree_area[MAX_ORDER];\n\n\t/* zone flags, see below */\n\tunsigned long\t\tflags;\n\n\t/* Primarily protects free_area */\n\tspinlock_t\t\tlock;\n\n\t/* Write-intensive fields used by compaction and vmstats. */\n\tZONE_PADDING(_pad2_)\n\n\t/*\n\t * When free pages are below this point, additional steps are taken\n\t * when reading the number of free pages to avoid per-cpu counter\n\t * drift allowing watermarks to be breached\n\t */\n\tunsigned long percpu_drift_mark;\n\n#if defined CONFIG_COMPACTION || defined CONFIG_CMA\n\t/* pfn where compaction free scanner should start */\n\tunsigned long\t\tcompact_cached_free_pfn;\n\t/* pfn where compaction migration scanner should start */\n\tunsigned long\t\tcompact_cached_migrate_pfn[ASYNC_AND_SYNC];\n\tunsigned long\t\tcompact_init_migrate_pfn;\n\tunsigned long\t\tcompact_init_free_pfn;\n#endif\n\n#ifdef CONFIG_COMPACTION\n\t/*\n\t * On compaction failure, 1<<compact_defer_shift compactions\n\t * are skipped before trying again. The number attempted since\n\t * last failure is tracked with compact_considered.\n\t * compact_order_failed is the minimum compaction failed order.\n\t */\n\tunsigned int\t\tcompact_considered;\n\tunsigned int\t\tcompact_defer_shift;\n\tint\t\t\tcompact_order_failed;\n#endif\n\n#if defined CONFIG_COMPACTION || defined CONFIG_CMA\n\t/* Set to true when the PG_migrate_skip bits should be cleared */\n\tbool\t\t\tcompact_blockskip_flush;\n#endif\n\n\tbool\t\t\tcontiguous;\n\n\tZONE_PADDING(_pad3_)\n\t/* Zone statistics */\n\tatomic_long_t\t\tvm_stat[NR_VM_ZONE_STAT_ITEMS];\n\tatomic_long_t\t\tvm_numa_stat[NR_VM_NUMA_STAT_ITEMS];\n} ____cacheline_internodealigned_in_smp;\n\nenum pgdat_flags {\n\tPGDAT_DIRTY,\t\t\t/* reclaim scanning has recently found\n\t\t\t\t\t * many dirty file pages at the tail\n\t\t\t\t\t * of the LRU.\n\t\t\t\t\t */\n\tPGDAT_WRITEBACK,\t\t/* reclaim scanning has recently found\n\t\t\t\t\t * many pages under writeback\n\t\t\t\t\t */\n\tPGDAT_RECLAIM_LOCKED,\t\t/* prevents concurrent reclaim */\n};\n\nenum zone_flags {\n\tZONE_BOOSTED_WATERMARK,\t\t/* zone recently boosted watermarks.\n\t\t\t\t\t * Cleared when kswapd is woken.\n\t\t\t\t\t */\n};\n\nstatic inline unsigned long zone_managed_pages(struct zone *zone)\n{\n\treturn (unsigned long)atomic_long_read(&zone->managed_pages);\n}\n\nstatic inline unsigned long zone_cma_pages(struct zone *zone)\n{\n#ifdef CONFIG_CMA\n\treturn zone->cma_pages;\n#else\n\treturn 0;\n#endif\n}\n\nstatic inline unsigned long zone_end_pfn(const struct zone *zone)\n{\n\treturn zone->zone_start_pfn + zone->spanned_pages;\n}\n\nstatic inline bool zone_spans_pfn(const struct zone *zone, unsigned long pfn)\n{\n\treturn zone->zone_start_pfn <= pfn && pfn < zone_end_pfn(zone);\n}\n\nstatic inline bool zone_is_initialized(struct zone *zone)\n{\n\treturn zone->initialized;\n}\n\nstatic inline bool zone_is_empty(struct zone *zone)\n{\n\treturn zone->spanned_pages == 0;\n}\n\n/*\n * Return true if [start_pfn, start_pfn + nr_pages) range has a non-empty\n * intersection with the given zone\n */\nstatic inline bool zone_intersects(struct zone *zone,\n\t\tunsigned long start_pfn, unsigned long nr_pages)\n{\n\tif (zone_is_empty(zone))\n\t\treturn false;\n\tif (start_pfn >= zone_end_pfn(zone) ||\n\t    start_pfn + nr_pages <= zone->zone_start_pfn)\n\t\treturn false;\n\n\treturn true;\n}\n\n/*\n * The \"priority\" of VM scanning is how much of the queues we will scan in one\n * go. A value of 12 for DEF_PRIORITY implies that we will scan 1/4096th of the\n * queues (\"queue_length >> 12\") during an aging round.\n */\n#define DEF_PRIORITY 12\n\n/* Maximum number of zones on a zonelist */\n#define MAX_ZONES_PER_ZONELIST (MAX_NUMNODES * MAX_NR_ZONES)\n\nenum {\n\tZONELIST_FALLBACK,\t/* zonelist with fallback */\n#ifdef CONFIG_NUMA\n\t/*\n\t * The NUMA zonelists are doubled because we need zonelists that\n\t * restrict the allocations to a single node for __GFP_THISNODE.\n\t */\n\tZONELIST_NOFALLBACK,\t/* zonelist without fallback (__GFP_THISNODE) */\n#endif\n\tMAX_ZONELISTS\n};\n\n/*\n * This struct contains information about a zone in a zonelist. It is stored\n * here to avoid dereferences into large structures and lookups of tables\n */\nstruct zoneref {\n\tstruct zone *zone;\t/* Pointer to actual zone */\n\tint zone_idx;\t\t/* zone_idx(zoneref->zone) */\n};\n\n/*\n * One allocation request operates on a zonelist. A zonelist\n * is a list of zones, the first one is the 'goal' of the\n * allocation, the other zones are fallback zones, in decreasing\n * priority.\n *\n * To speed the reading of the zonelist, the zonerefs contain the zone index\n * of the entry being read. Helper functions to access information given\n * a struct zoneref are\n *\n * zonelist_zone()\t- Return the struct zone * for an entry in _zonerefs\n * zonelist_zone_idx()\t- Return the index of the zone for an entry\n * zonelist_node_idx()\t- Return the index of the node for an entry\n */\nstruct zonelist {\n\tstruct zoneref _zonerefs[MAX_ZONES_PER_ZONELIST + 1];\n};\n\n#ifndef CONFIG_DISCONTIGMEM\n/* The array of struct pages - for discontigmem use pgdat->lmem_map */\nextern struct page *mem_map;\n#endif\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstruct deferred_split {\n\tspinlock_t split_queue_lock;\n\tstruct list_head split_queue;\n\tunsigned long split_queue_len;\n};\n#endif\n\n/*\n * On NUMA machines, each NUMA node would have a pg_data_t to describe\n * it's memory layout. On UMA machines there is a single pglist_data which\n * describes the whole memory.\n *\n * Memory statistics and page replacement data structures are maintained on a\n * per-zone basis.\n */\ntypedef struct pglist_data {\n\t/*\n\t * node_zones contains just the zones for THIS node. Not all of the\n\t * zones may be populated, but it is the full list. It is referenced by\n\t * this node's node_zonelists as well as other node's node_zonelists.\n\t */\n\tstruct zone node_zones[MAX_NR_ZONES];\n\n\t/*\n\t * node_zonelists contains references to all zones in all nodes.\n\t * Generally the first zones will be references to this node's\n\t * node_zones.\n\t */\n\tstruct zonelist node_zonelists[MAX_ZONELISTS];\n\n\tint nr_zones; /* number of populated zones in this node */\n#ifdef CONFIG_FLAT_NODE_MEM_MAP\t/* means !SPARSEMEM */\n\tstruct page *node_mem_map;\n#ifdef CONFIG_PAGE_EXTENSION\n\tstruct page_ext *node_page_ext;\n#endif\n#endif\n#if defined(CONFIG_MEMORY_HOTPLUG) || defined(CONFIG_DEFERRED_STRUCT_PAGE_INIT)\n\t/*\n\t * Must be held any time you expect node_start_pfn,\n\t * node_present_pages, node_spanned_pages or nr_zones to stay constant.\n\t * Also synchronizes pgdat->first_deferred_pfn during deferred page\n\t * init.\n\t *\n\t * pgdat_resize_lock() and pgdat_resize_unlock() are provided to\n\t * manipulate node_size_lock without checking for CONFIG_MEMORY_HOTPLUG\n\t * or CONFIG_DEFERRED_STRUCT_PAGE_INIT.\n\t *\n\t * Nests above zone->lock and zone->span_seqlock\n\t */\n\tspinlock_t node_size_lock;\n#endif\n\tunsigned long node_start_pfn;\n\tunsigned long node_present_pages; /* total number of physical pages */\n\tunsigned long node_spanned_pages; /* total size of physical page\n\t\t\t\t\t     range, including holes */\n\tint node_id;\n\twait_queue_head_t kswapd_wait;\n\twait_queue_head_t pfmemalloc_wait;\n\tstruct task_struct *kswapd;\t/* Protected by\n\t\t\t\t\t   mem_hotplug_begin/end() */\n\tint kswapd_order;\n\tenum zone_type kswapd_highest_zoneidx;\n\n\tint kswapd_failures;\t\t/* Number of 'reclaimed == 0' runs */\n\n#ifdef CONFIG_COMPACTION\n\tint kcompactd_max_order;\n\tenum zone_type kcompactd_highest_zoneidx;\n\twait_queue_head_t kcompactd_wait;\n\tstruct task_struct *kcompactd;\n#endif\n\t/*\n\t * This is a per-node reserve of pages that are not available\n\t * to userspace allocations.\n\t */\n\tunsigned long\t\ttotalreserve_pages;\n\n#ifdef CONFIG_NUMA\n\t/*\n\t * node reclaim becomes active if more unmapped pages exist.\n\t */\n\tunsigned long\t\tmin_unmapped_pages;\n\tunsigned long\t\tmin_slab_pages;\n#endif /* CONFIG_NUMA */\n\n\t/* Write-intensive fields used by page reclaim */\n\tZONE_PADDING(_pad1_)\n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\n\t/*\n\t * If memory initialisation on large machines is deferred then this\n\t * is the first PFN that needs to be initialised.\n\t */\n\tunsigned long first_deferred_pfn;\n#endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tstruct deferred_split deferred_split_queue;\n#endif\n\n\t/* Fields commonly accessed by the page reclaim scanner */\n\n\t/*\n\t * NOTE: THIS IS UNUSED IF MEMCG IS ENABLED.\n\t *\n\t * Use mem_cgroup_lruvec() to look up lruvecs.\n\t */\n\tstruct lruvec\t\t__lruvec;\n\n\tunsigned long\t\tflags;\n\n\tZONE_PADDING(_pad2_)\n\n\t/* Per-node vmstats */\n\tstruct per_cpu_nodestat __percpu *per_cpu_nodestats;\n\tatomic_long_t\t\tvm_stat[NR_VM_NODE_STAT_ITEMS];\n} pg_data_t;\n\n#define node_present_pages(nid)\t(NODE_DATA(nid)->node_present_pages)\n#define node_spanned_pages(nid)\t(NODE_DATA(nid)->node_spanned_pages)\n#ifdef CONFIG_FLAT_NODE_MEM_MAP\n#define pgdat_page_nr(pgdat, pagenr)\t((pgdat)->node_mem_map + (pagenr))\n#else\n#define pgdat_page_nr(pgdat, pagenr)\tpfn_to_page((pgdat)->node_start_pfn + (pagenr))\n#endif\n#define nid_page_nr(nid, pagenr) \tpgdat_page_nr(NODE_DATA(nid),(pagenr))\n\n#define node_start_pfn(nid)\t(NODE_DATA(nid)->node_start_pfn)\n#define node_end_pfn(nid) pgdat_end_pfn(NODE_DATA(nid))\n\nstatic inline unsigned long pgdat_end_pfn(pg_data_t *pgdat)\n{\n\treturn pgdat->node_start_pfn + pgdat->node_spanned_pages;\n}\n\nstatic inline bool pgdat_is_empty(pg_data_t *pgdat)\n{\n\treturn !pgdat->node_start_pfn && !pgdat->node_spanned_pages;\n}\n\n#include <linux/memory_hotplug.h>\n\nvoid build_all_zonelists(pg_data_t *pgdat);\nvoid wakeup_kswapd(struct zone *zone, gfp_t gfp_mask, int order,\n\t\t   enum zone_type highest_zoneidx);\nbool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,\n\t\t\t int highest_zoneidx, unsigned int alloc_flags,\n\t\t\t long free_pages);\nbool zone_watermark_ok(struct zone *z, unsigned int order,\n\t\tunsigned long mark, int highest_zoneidx,\n\t\tunsigned int alloc_flags);\nbool zone_watermark_ok_safe(struct zone *z, unsigned int order,\n\t\tunsigned long mark, int highest_zoneidx);\n/*\n * Memory initialization context, use to differentiate memory added by\n * the platform statically or via memory hotplug interface.\n */\nenum meminit_context {\n\tMEMINIT_EARLY,\n\tMEMINIT_HOTPLUG,\n};\n\nextern void init_currently_empty_zone(struct zone *zone, unsigned long start_pfn,\n\t\t\t\t     unsigned long size);\n\nextern void lruvec_init(struct lruvec *lruvec);\n\nstatic inline struct pglist_data *lruvec_pgdat(struct lruvec *lruvec)\n{\n#ifdef CONFIG_MEMCG\n\treturn lruvec->pgdat;\n#else\n\treturn container_of(lruvec, struct pglist_data, __lruvec);\n#endif\n}\n\n#ifdef CONFIG_HAVE_MEMORYLESS_NODES\nint local_memory_node(int node_id);\n#else\nstatic inline int local_memory_node(int node_id) { return node_id; };\n#endif\n\n/*\n * zone_idx() returns 0 for the ZONE_DMA zone, 1 for the ZONE_NORMAL zone, etc.\n */\n#define zone_idx(zone)\t\t((zone) - (zone)->zone_pgdat->node_zones)\n\n#ifdef CONFIG_ZONE_DEVICE\nstatic inline bool zone_is_zone_device(struct zone *zone)\n{\n\treturn zone_idx(zone) == ZONE_DEVICE;\n}\n#else\nstatic inline bool zone_is_zone_device(struct zone *zone)\n{\n\treturn false;\n}\n#endif\n\n/*\n * Returns true if a zone has pages managed by the buddy allocator.\n * All the reclaim decisions have to use this function rather than\n * populated_zone(). If the whole zone is reserved then we can easily\n * end up with populated_zone() && !managed_zone().\n */\nstatic inline bool managed_zone(struct zone *zone)\n{\n\treturn zone_managed_pages(zone);\n}\n\n/* Returns true if a zone has memory */\nstatic inline bool populated_zone(struct zone *zone)\n{\n\treturn zone->present_pages;\n}\n\n#ifdef CONFIG_NUMA\nstatic inline int zone_to_nid(struct zone *zone)\n{\n\treturn zone->node;\n}\n\nstatic inline void zone_set_nid(struct zone *zone, int nid)\n{\n\tzone->node = nid;\n}\n#else\nstatic inline int zone_to_nid(struct zone *zone)\n{\n\treturn 0;\n}\n\nstatic inline void zone_set_nid(struct zone *zone, int nid) {}\n#endif\n\nextern int movable_zone;\n\n#ifdef CONFIG_HIGHMEM\nstatic inline int zone_movable_is_highmem(void)\n{\n#ifdef CONFIG_NEED_MULTIPLE_NODES\n\treturn movable_zone == ZONE_HIGHMEM;\n#else\n\treturn (ZONE_MOVABLE - 1) == ZONE_HIGHMEM;\n#endif\n}\n#endif\n\nstatic inline int is_highmem_idx(enum zone_type idx)\n{\n#ifdef CONFIG_HIGHMEM\n\treturn (idx == ZONE_HIGHMEM ||\n\t\t(idx == ZONE_MOVABLE && zone_movable_is_highmem()));\n#else\n\treturn 0;\n#endif\n}\n\n/**\n * is_highmem - helper function to quickly check if a struct zone is a\n *              highmem zone or not.  This is an attempt to keep references\n *              to ZONE_{DMA/NORMAL/HIGHMEM/etc} in general code to a minimum.\n * @zone - pointer to struct zone variable\n */\nstatic inline int is_highmem(struct zone *zone)\n{\n#ifdef CONFIG_HIGHMEM\n\treturn is_highmem_idx(zone_idx(zone));\n#else\n\treturn 0;\n#endif\n}\n\n/* These two functions are used to setup the per zone pages min values */\nstruct ctl_table;\n\nint min_free_kbytes_sysctl_handler(struct ctl_table *, int, void *, size_t *,\n\t\tloff_t *);\nint watermark_scale_factor_sysctl_handler(struct ctl_table *, int, void *,\n\t\tsize_t *, loff_t *);\nextern int sysctl_lowmem_reserve_ratio[MAX_NR_ZONES];\nint lowmem_reserve_ratio_sysctl_handler(struct ctl_table *, int, void *,\n\t\tsize_t *, loff_t *);\nint percpu_pagelist_fraction_sysctl_handler(struct ctl_table *, int,\n\t\tvoid *, size_t *, loff_t *);\nint sysctl_min_unmapped_ratio_sysctl_handler(struct ctl_table *, int,\n\t\tvoid *, size_t *, loff_t *);\nint sysctl_min_slab_ratio_sysctl_handler(struct ctl_table *, int,\n\t\tvoid *, size_t *, loff_t *);\nint numa_zonelist_order_handler(struct ctl_table *, int,\n\t\tvoid *, size_t *, loff_t *);\nextern int percpu_pagelist_fraction;\nextern char numa_zonelist_order[];\n#define NUMA_ZONELIST_ORDER_LEN\t16\n\n#ifndef CONFIG_NEED_MULTIPLE_NODES\n\nextern struct pglist_data contig_page_data;\n#define NODE_DATA(nid)\t\t(&contig_page_data)\n#define NODE_MEM_MAP(nid)\tmem_map\n\n#else /* CONFIG_NEED_MULTIPLE_NODES */\n\n#include <asm/mmzone.h>\n\n#endif /* !CONFIG_NEED_MULTIPLE_NODES */\n\nextern struct pglist_data *first_online_pgdat(void);\nextern struct pglist_data *next_online_pgdat(struct pglist_data *pgdat);\nextern struct zone *next_zone(struct zone *zone);\n\n/**\n * for_each_online_pgdat - helper macro to iterate over all online nodes\n * @pgdat - pointer to a pg_data_t variable\n */\n#define for_each_online_pgdat(pgdat)\t\t\t\\\n\tfor (pgdat = first_online_pgdat();\t\t\\\n\t     pgdat;\t\t\t\t\t\\\n\t     pgdat = next_online_pgdat(pgdat))\n/**\n * for_each_zone - helper macro to iterate over all memory zones\n * @zone - pointer to struct zone variable\n *\n * The user only needs to declare the zone variable, for_each_zone\n * fills it in.\n */\n#define for_each_zone(zone)\t\t\t        \\\n\tfor (zone = (first_online_pgdat())->node_zones; \\\n\t     zone;\t\t\t\t\t\\\n\t     zone = next_zone(zone))\n\n#define for_each_populated_zone(zone)\t\t        \\\n\tfor (zone = (first_online_pgdat())->node_zones; \\\n\t     zone;\t\t\t\t\t\\\n\t     zone = next_zone(zone))\t\t\t\\\n\t\tif (!populated_zone(zone))\t\t\\\n\t\t\t; /* do nothing */\t\t\\\n\t\telse\n\nstatic inline struct zone *zonelist_zone(struct zoneref *zoneref)\n{\n\treturn zoneref->zone;\n}\n\nstatic inline int zonelist_zone_idx(struct zoneref *zoneref)\n{\n\treturn zoneref->zone_idx;\n}\n\nstatic inline int zonelist_node_idx(struct zoneref *zoneref)\n{\n\treturn zone_to_nid(zoneref->zone);\n}\n\nstruct zoneref *__next_zones_zonelist(struct zoneref *z,\n\t\t\t\t\tenum zone_type highest_zoneidx,\n\t\t\t\t\tnodemask_t *nodes);\n\n/**\n * next_zones_zonelist - Returns the next zone at or below highest_zoneidx within the allowed nodemask using a cursor within a zonelist as a starting point\n * @z - The cursor used as a starting point for the search\n * @highest_zoneidx - The zone index of the highest zone to return\n * @nodes - An optional nodemask to filter the zonelist with\n *\n * This function returns the next zone at or below a given zone index that is\n * within the allowed nodemask using a cursor as the starting point for the\n * search. The zoneref returned is a cursor that represents the current zone\n * being examined. It should be advanced by one before calling\n * next_zones_zonelist again.\n */\nstatic __always_inline struct zoneref *next_zones_zonelist(struct zoneref *z,\n\t\t\t\t\tenum zone_type highest_zoneidx,\n\t\t\t\t\tnodemask_t *nodes)\n{\n\tif (likely(!nodes && zonelist_zone_idx(z) <= highest_zoneidx))\n\t\treturn z;\n\treturn __next_zones_zonelist(z, highest_zoneidx, nodes);\n}\n\n/**\n * first_zones_zonelist - Returns the first zone at or below highest_zoneidx within the allowed nodemask in a zonelist\n * @zonelist - The zonelist to search for a suitable zone\n * @highest_zoneidx - The zone index of the highest zone to return\n * @nodes - An optional nodemask to filter the zonelist with\n * @return - Zoneref pointer for the first suitable zone found (see below)\n *\n * This function returns the first zone at or below a given zone index that is\n * within the allowed nodemask. The zoneref returned is a cursor that can be\n * used to iterate the zonelist with next_zones_zonelist by advancing it by\n * one before calling.\n *\n * When no eligible zone is found, zoneref->zone is NULL (zoneref itself is\n * never NULL). This may happen either genuinely, or due to concurrent nodemask\n * update due to cpuset modification.\n */\nstatic inline struct zoneref *first_zones_zonelist(struct zonelist *zonelist,\n\t\t\t\t\tenum zone_type highest_zoneidx,\n\t\t\t\t\tnodemask_t *nodes)\n{\n\treturn next_zones_zonelist(zonelist->_zonerefs,\n\t\t\t\t\t\t\thighest_zoneidx, nodes);\n}\n\n/**\n * for_each_zone_zonelist_nodemask - helper macro to iterate over valid zones in a zonelist at or below a given zone index and within a nodemask\n * @zone - The current zone in the iterator\n * @z - The current pointer within zonelist->_zonerefs being iterated\n * @zlist - The zonelist being iterated\n * @highidx - The zone index of the highest zone to return\n * @nodemask - Nodemask allowed by the allocator\n *\n * This iterator iterates though all zones at or below a given zone index and\n * within a given nodemask\n */\n#define for_each_zone_zonelist_nodemask(zone, z, zlist, highidx, nodemask) \\\n\tfor (z = first_zones_zonelist(zlist, highidx, nodemask), zone = zonelist_zone(z);\t\\\n\t\tzone;\t\t\t\t\t\t\t\\\n\t\tz = next_zones_zonelist(++z, highidx, nodemask),\t\\\n\t\t\tzone = zonelist_zone(z))\n\n#define for_next_zone_zonelist_nodemask(zone, z, highidx, nodemask) \\\n\tfor (zone = z->zone;\t\\\n\t\tzone;\t\t\t\t\t\t\t\\\n\t\tz = next_zones_zonelist(++z, highidx, nodemask),\t\\\n\t\t\tzone = zonelist_zone(z))\n\n\n/**\n * for_each_zone_zonelist - helper macro to iterate over valid zones in a zonelist at or below a given zone index\n * @zone - The current zone in the iterator\n * @z - The current pointer within zonelist->zones being iterated\n * @zlist - The zonelist being iterated\n * @highidx - The zone index of the highest zone to return\n *\n * This iterator iterates though all zones at or below a given zone index.\n */\n#define for_each_zone_zonelist(zone, z, zlist, highidx) \\\n\tfor_each_zone_zonelist_nodemask(zone, z, zlist, highidx, NULL)\n\n#ifdef CONFIG_SPARSEMEM\n#include <asm/sparsemem.h>\n#endif\n\n#ifdef CONFIG_FLATMEM\n#define pfn_to_nid(pfn)\t\t(0)\n#endif\n\n#ifdef CONFIG_SPARSEMEM\n\n/*\n * SECTION_SHIFT    \t\t#bits space required to store a section #\n *\n * PA_SECTION_SHIFT\t\tphysical address to/from section number\n * PFN_SECTION_SHIFT\t\tpfn to/from section number\n */\n#define PA_SECTION_SHIFT\t(SECTION_SIZE_BITS)\n#define PFN_SECTION_SHIFT\t(SECTION_SIZE_BITS - PAGE_SHIFT)\n\n#define NR_MEM_SECTIONS\t\t(1UL << SECTIONS_SHIFT)\n\n#define PAGES_PER_SECTION       (1UL << PFN_SECTION_SHIFT)\n#define PAGE_SECTION_MASK\t(~(PAGES_PER_SECTION-1))\n\n#define SECTION_BLOCKFLAGS_BITS \\\n\t((1UL << (PFN_SECTION_SHIFT - pageblock_order)) * NR_PAGEBLOCK_BITS)\n\n#if (MAX_ORDER - 1 + PAGE_SHIFT) > SECTION_SIZE_BITS\n#error Allocator MAX_ORDER exceeds SECTION_SIZE\n#endif\n\nstatic inline unsigned long pfn_to_section_nr(unsigned long pfn)\n{\n\treturn pfn >> PFN_SECTION_SHIFT;\n}\nstatic inline unsigned long section_nr_to_pfn(unsigned long sec)\n{\n\treturn sec << PFN_SECTION_SHIFT;\n}\n\n#define SECTION_ALIGN_UP(pfn)\t(((pfn) + PAGES_PER_SECTION - 1) & PAGE_SECTION_MASK)\n#define SECTION_ALIGN_DOWN(pfn)\t((pfn) & PAGE_SECTION_MASK)\n\n#define SUBSECTION_SHIFT 21\n#define SUBSECTION_SIZE (1UL << SUBSECTION_SHIFT)\n\n#define PFN_SUBSECTION_SHIFT (SUBSECTION_SHIFT - PAGE_SHIFT)\n#define PAGES_PER_SUBSECTION (1UL << PFN_SUBSECTION_SHIFT)\n#define PAGE_SUBSECTION_MASK (~(PAGES_PER_SUBSECTION-1))\n\n#if SUBSECTION_SHIFT > SECTION_SIZE_BITS\n#error Subsection size exceeds section size\n#else\n#define SUBSECTIONS_PER_SECTION (1UL << (SECTION_SIZE_BITS - SUBSECTION_SHIFT))\n#endif\n\n#define SUBSECTION_ALIGN_UP(pfn) ALIGN((pfn), PAGES_PER_SUBSECTION)\n#define SUBSECTION_ALIGN_DOWN(pfn) ((pfn) & PAGE_SUBSECTION_MASK)\n\nstruct mem_section_usage {\n#ifdef CONFIG_SPARSEMEM_VMEMMAP\n\tDECLARE_BITMAP(subsection_map, SUBSECTIONS_PER_SECTION);\n#endif\n\t/* See declaration of similar field in struct zone */\n\tunsigned long pageblock_flags[0];\n};\n\nvoid subsection_map_init(unsigned long pfn, unsigned long nr_pages);\n\nstruct page;\nstruct page_ext;\nstruct mem_section {\n\t/*\n\t * This is, logically, a pointer to an array of struct\n\t * pages.  However, it is stored with some other magic.\n\t * (see sparse.c::sparse_init_one_section())\n\t *\n\t * Additionally during early boot we encode node id of\n\t * the location of the section here to guide allocation.\n\t * (see sparse.c::memory_present())\n\t *\n\t * Making it a UL at least makes someone do a cast\n\t * before using it wrong.\n\t */\n\tunsigned long section_mem_map;\n\n\tstruct mem_section_usage *usage;\n#ifdef CONFIG_PAGE_EXTENSION\n\t/*\n\t * If SPARSEMEM, pgdat doesn't have page_ext pointer. We use\n\t * section. (see page_ext.h about this.)\n\t */\n\tstruct page_ext *page_ext;\n\tunsigned long pad;\n#endif\n\t/*\n\t * WARNING: mem_section must be a power-of-2 in size for the\n\t * calculation and use of SECTION_ROOT_MASK to make sense.\n\t */\n};\n\n#ifdef CONFIG_SPARSEMEM_EXTREME\n#define SECTIONS_PER_ROOT       (PAGE_SIZE / sizeof (struct mem_section))\n#else\n#define SECTIONS_PER_ROOT\t1\n#endif\n\n#define SECTION_NR_TO_ROOT(sec)\t((sec) / SECTIONS_PER_ROOT)\n#define NR_SECTION_ROOTS\tDIV_ROUND_UP(NR_MEM_SECTIONS, SECTIONS_PER_ROOT)\n#define SECTION_ROOT_MASK\t(SECTIONS_PER_ROOT - 1)\n\n#ifdef CONFIG_SPARSEMEM_EXTREME\nextern struct mem_section **mem_section;\n#else\nextern struct mem_section mem_section[NR_SECTION_ROOTS][SECTIONS_PER_ROOT];\n#endif\n\nstatic inline unsigned long *section_to_usemap(struct mem_section *ms)\n{\n\treturn ms->usage->pageblock_flags;\n}\n\nstatic inline struct mem_section *__nr_to_section(unsigned long nr)\n{\n#ifdef CONFIG_SPARSEMEM_EXTREME\n\tif (!mem_section)\n\t\treturn NULL;\n#endif\n\tif (!mem_section[SECTION_NR_TO_ROOT(nr)])\n\t\treturn NULL;\n\treturn &mem_section[SECTION_NR_TO_ROOT(nr)][nr & SECTION_ROOT_MASK];\n}\nextern unsigned long __section_nr(struct mem_section *ms);\nextern size_t mem_section_usage_size(void);\n\n/*\n * We use the lower bits of the mem_map pointer to store\n * a little bit of information.  The pointer is calculated\n * as mem_map - section_nr_to_pfn(pnum).  The result is\n * aligned to the minimum alignment of the two values:\n *   1. All mem_map arrays are page-aligned.\n *   2. section_nr_to_pfn() always clears PFN_SECTION_SHIFT\n *      lowest bits.  PFN_SECTION_SHIFT is arch-specific\n *      (equal SECTION_SIZE_BITS - PAGE_SHIFT), and the\n *      worst combination is powerpc with 256k pages,\n *      which results in PFN_SECTION_SHIFT equal 6.\n * To sum it up, at least 6 bits are available.\n */\n#define SECTION_MARKED_PRESENT\t\t(1UL<<0)\n#define SECTION_HAS_MEM_MAP\t\t(1UL<<1)\n#define SECTION_IS_ONLINE\t\t(1UL<<2)\n#define SECTION_IS_EARLY\t\t(1UL<<3)\n#define SECTION_TAINT_ZONE_DEVICE\t(1UL<<4)\n#define SECTION_MAP_LAST_BIT\t\t(1UL<<5)\n#define SECTION_MAP_MASK\t\t(~(SECTION_MAP_LAST_BIT-1))\n#define SECTION_NID_SHIFT\t\t3\n\nstatic inline struct page *__section_mem_map_addr(struct mem_section *section)\n{\n\tunsigned long map = section->section_mem_map;\n\tmap &= SECTION_MAP_MASK;\n\treturn (struct page *)map;\n}\n\nstatic inline int present_section(struct mem_section *section)\n{\n\treturn (section && (section->section_mem_map & SECTION_MARKED_PRESENT));\n}\n\nstatic inline int present_section_nr(unsigned long nr)\n{\n\treturn present_section(__nr_to_section(nr));\n}\n\nstatic inline int valid_section(struct mem_section *section)\n{\n\treturn (section && (section->section_mem_map & SECTION_HAS_MEM_MAP));\n}\n\nstatic inline int early_section(struct mem_section *section)\n{\n\treturn (section && (section->section_mem_map & SECTION_IS_EARLY));\n}\n\nstatic inline int valid_section_nr(unsigned long nr)\n{\n\treturn valid_section(__nr_to_section(nr));\n}\n\nstatic inline int online_section(struct mem_section *section)\n{\n\treturn (section && (section->section_mem_map & SECTION_IS_ONLINE));\n}\n\nstatic inline int online_device_section(struct mem_section *section)\n{\n\tunsigned long flags = SECTION_IS_ONLINE | SECTION_TAINT_ZONE_DEVICE;\n\n\treturn section && ((section->section_mem_map & flags) == flags);\n}\n\nstatic inline int online_section_nr(unsigned long nr)\n{\n\treturn online_section(__nr_to_section(nr));\n}\n\n#ifdef CONFIG_MEMORY_HOTPLUG\nvoid online_mem_sections(unsigned long start_pfn, unsigned long end_pfn);\n#ifdef CONFIG_MEMORY_HOTREMOVE\nvoid offline_mem_sections(unsigned long start_pfn, unsigned long end_pfn);\n#endif\n#endif\n\nstatic inline struct mem_section *__pfn_to_section(unsigned long pfn)\n{\n\treturn __nr_to_section(pfn_to_section_nr(pfn));\n}\n\nextern unsigned long __highest_present_section_nr;\n\nstatic inline int subsection_map_index(unsigned long pfn)\n{\n\treturn (pfn & ~(PAGE_SECTION_MASK)) / PAGES_PER_SUBSECTION;\n}\n\n#ifdef CONFIG_SPARSEMEM_VMEMMAP\nstatic inline int pfn_section_valid(struct mem_section *ms, unsigned long pfn)\n{\n\tint idx = subsection_map_index(pfn);\n\n\treturn test_bit(idx, ms->usage->subsection_map);\n}\n#else\nstatic inline int pfn_section_valid(struct mem_section *ms, unsigned long pfn)\n{\n\treturn 1;\n}\n#endif\n\n#ifndef CONFIG_HAVE_ARCH_PFN_VALID\nstatic inline int pfn_valid(unsigned long pfn)\n{\n\tstruct mem_section *ms;\n\n\tif (pfn_to_section_nr(pfn) >= NR_MEM_SECTIONS)\n\t\treturn 0;\n\tms = __nr_to_section(pfn_to_section_nr(pfn));\n\tif (!valid_section(ms))\n\t\treturn 0;\n\t/*\n\t * Traditionally early sections always returned pfn_valid() for\n\t * the entire section-sized span.\n\t */\n\treturn early_section(ms) || pfn_section_valid(ms, pfn);\n}\n#endif\n\nstatic inline int pfn_in_present_section(unsigned long pfn)\n{\n\tif (pfn_to_section_nr(pfn) >= NR_MEM_SECTIONS)\n\t\treturn 0;\n\treturn present_section(__nr_to_section(pfn_to_section_nr(pfn)));\n}\n\nstatic inline unsigned long next_present_section_nr(unsigned long section_nr)\n{\n\twhile (++section_nr <= __highest_present_section_nr) {\n\t\tif (present_section_nr(section_nr))\n\t\t\treturn section_nr;\n\t}\n\n\treturn -1;\n}\n\n/*\n * These are _only_ used during initialisation, therefore they\n * can use __initdata ...  They could have names to indicate\n * this restriction.\n */\n#ifdef CONFIG_NUMA\n#define pfn_to_nid(pfn)\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tunsigned long __pfn_to_nid_pfn = (pfn);\t\t\t\t\\\n\tpage_to_nid(pfn_to_page(__pfn_to_nid_pfn));\t\t\t\\\n})\n#else\n#define pfn_to_nid(pfn)\t\t(0)\n#endif\n\nvoid sparse_init(void);\n#else\n#define sparse_init()\tdo {} while (0)\n#define sparse_index_init(_sec, _nid)  do {} while (0)\n#define pfn_in_present_section pfn_valid\n#define subsection_map_init(_pfn, _nr_pages) do {} while (0)\n#endif /* CONFIG_SPARSEMEM */\n\n/*\n * If it is possible to have holes within a MAX_ORDER_NR_PAGES, then we\n * need to check pfn validity within that MAX_ORDER_NR_PAGES block.\n * pfn_valid_within() should be used in this case; we optimise this away\n * when we have no holes within a MAX_ORDER_NR_PAGES block.\n */\n#ifdef CONFIG_HOLES_IN_ZONE\n#define pfn_valid_within(pfn) pfn_valid(pfn)\n#else\n#define pfn_valid_within(pfn) (1)\n#endif\n\n#endif /* !__GENERATING_BOUNDS.H */\n#endif /* !__ASSEMBLY__ */\n#endif /* _LINUX_MMZONE_H */\n"}, "4": {"id": 4, "path": "/src/include/linux/irqflags.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n/*\n * include/linux/irqflags.h\n *\n * IRQ flags tracing: follow the state of the hardirq and softirq flags and\n * provide callbacks for transitions between ON and OFF states.\n *\n * This file gets included from lowlevel asm headers too, to provide\n * wrapped versions of the local_irq_*() APIs, based on the\n * raw_local_irq_*() macros from the lowlevel headers.\n */\n#ifndef _LINUX_TRACE_IRQFLAGS_H\n#define _LINUX_TRACE_IRQFLAGS_H\n\n#include <linux/typecheck.h>\n#include <asm/irqflags.h>\n#include <asm/percpu.h>\n\n/* Currently lockdep_softirqs_on/off is used only by lockdep */\n#ifdef CONFIG_PROVE_LOCKING\n  extern void lockdep_softirqs_on(unsigned long ip);\n  extern void lockdep_softirqs_off(unsigned long ip);\n  extern void lockdep_hardirqs_on_prepare(unsigned long ip);\n  extern void lockdep_hardirqs_on(unsigned long ip);\n  extern void lockdep_hardirqs_off(unsigned long ip);\n#else\n  static inline void lockdep_softirqs_on(unsigned long ip) { }\n  static inline void lockdep_softirqs_off(unsigned long ip) { }\n  static inline void lockdep_hardirqs_on_prepare(unsigned long ip) { }\n  static inline void lockdep_hardirqs_on(unsigned long ip) { }\n  static inline void lockdep_hardirqs_off(unsigned long ip) { }\n#endif\n\n#ifdef CONFIG_TRACE_IRQFLAGS\n\n/* Per-task IRQ trace events information. */\nstruct irqtrace_events {\n\tunsigned int\tirq_events;\n\tunsigned long\thardirq_enable_ip;\n\tunsigned long\thardirq_disable_ip;\n\tunsigned int\thardirq_enable_event;\n\tunsigned int\thardirq_disable_event;\n\tunsigned long\tsoftirq_disable_ip;\n\tunsigned long\tsoftirq_enable_ip;\n\tunsigned int\tsoftirq_disable_event;\n\tunsigned int\tsoftirq_enable_event;\n};\n\nDECLARE_PER_CPU(int, hardirqs_enabled);\nDECLARE_PER_CPU(int, hardirq_context);\n\nextern void trace_hardirqs_on_prepare(void);\nextern void trace_hardirqs_off_finish(void);\nextern void trace_hardirqs_on(void);\nextern void trace_hardirqs_off(void);\n\n# define lockdep_hardirq_context()\t(raw_cpu_read(hardirq_context))\n# define lockdep_softirq_context(p)\t((p)->softirq_context)\n# define lockdep_hardirqs_enabled()\t(this_cpu_read(hardirqs_enabled))\n# define lockdep_softirqs_enabled(p)\t((p)->softirqs_enabled)\n# define lockdep_hardirq_enter()\t\t\t\\\ndo {\t\t\t\t\t\t\t\\\n\tif (__this_cpu_inc_return(hardirq_context) == 1)\\\n\t\tcurrent->hardirq_threaded = 0;\t\t\\\n} while (0)\n# define lockdep_hardirq_threaded()\t\t\\\ndo {\t\t\t\t\t\t\\\n\tcurrent->hardirq_threaded = 1;\t\t\\\n} while (0)\n# define lockdep_hardirq_exit()\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\t__this_cpu_dec(hardirq_context);\t\\\n} while (0)\n# define lockdep_softirq_enter()\t\t\\\ndo {\t\t\t\t\t\t\\\n\tcurrent->softirq_context++;\t\t\\\n} while (0)\n# define lockdep_softirq_exit()\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tcurrent->softirq_context--;\t\t\\\n} while (0)\n\n# define lockdep_hrtimer_enter(__hrtimer)\t\t\\\n({\t\t\t\t\t\t\t\\\n\tbool __expires_hardirq = true;\t\t\t\\\n\t\t\t\t\t\t\t\\\n\tif (!__hrtimer->is_hard) {\t\t\t\\\n\t\tcurrent->irq_config = 1;\t\t\\\n\t\t__expires_hardirq = false;\t\t\\\n\t}\t\t\t\t\t\t\\\n\t__expires_hardirq;\t\t\t\t\\\n})\n\n# define lockdep_hrtimer_exit(__expires_hardirq)\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\tif (!__expires_hardirq)\t\t\t\\\n\t\t\tcurrent->irq_config = 0;\t\\\n\t} while (0)\n\n# define lockdep_posixtimer_enter()\t\t\t\t\\\n\t  do {\t\t\t\t\t\t\t\\\n\t\t  current->irq_config = 1;\t\t\t\\\n\t  } while (0)\n\n# define lockdep_posixtimer_exit()\t\t\t\t\\\n\t  do {\t\t\t\t\t\t\t\\\n\t\t  current->irq_config = 0;\t\t\t\\\n\t  } while (0)\n\n# define lockdep_irq_work_enter(_flags)\t\t\t\t\t\\\n\t  do {\t\t\t\t\t\t\t\t\\\n\t\t  if (!((_flags) & IRQ_WORK_HARD_IRQ))\t\t\t\\\n\t\t\tcurrent->irq_config = 1;\t\t\t\\\n\t  } while (0)\n# define lockdep_irq_work_exit(_flags)\t\t\t\t\t\\\n\t  do {\t\t\t\t\t\t\t\t\\\n\t\t  if (!((_flags) & IRQ_WORK_HARD_IRQ))\t\t\t\\\n\t\t\tcurrent->irq_config = 0;\t\t\t\\\n\t  } while (0)\n\n#else\n# define trace_hardirqs_on_prepare()\t\tdo { } while (0)\n# define trace_hardirqs_off_finish()\t\tdo { } while (0)\n# define trace_hardirqs_on()\t\t\tdo { } while (0)\n# define trace_hardirqs_off()\t\t\tdo { } while (0)\n# define lockdep_hardirq_context()\t\t0\n# define lockdep_softirq_context(p)\t\t0\n# define lockdep_hardirqs_enabled()\t\t0\n# define lockdep_softirqs_enabled(p)\t\t0\n# define lockdep_hardirq_enter()\t\tdo { } while (0)\n# define lockdep_hardirq_threaded()\t\tdo { } while (0)\n# define lockdep_hardirq_exit()\t\t\tdo { } while (0)\n# define lockdep_softirq_enter()\t\tdo { } while (0)\n# define lockdep_softirq_exit()\t\t\tdo { } while (0)\n# define lockdep_hrtimer_enter(__hrtimer)\tfalse\n# define lockdep_hrtimer_exit(__context)\tdo { } while (0)\n# define lockdep_posixtimer_enter()\t\tdo { } while (0)\n# define lockdep_posixtimer_exit()\t\tdo { } while (0)\n# define lockdep_irq_work_enter(__work)\t\tdo { } while (0)\n# define lockdep_irq_work_exit(__work)\t\tdo { } while (0)\n#endif\n\n#if defined(CONFIG_IRQSOFF_TRACER) || \\\n\tdefined(CONFIG_PREEMPT_TRACER)\n extern void stop_critical_timings(void);\n extern void start_critical_timings(void);\n#else\n# define stop_critical_timings() do { } while (0)\n# define start_critical_timings() do { } while (0)\n#endif\n\n#ifdef CONFIG_DEBUG_IRQFLAGS\nextern void warn_bogus_irq_restore(void);\n#define raw_check_bogus_irq_restore()\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\tif (unlikely(!arch_irqs_disabled()))\t\\\n\t\t\twarn_bogus_irq_restore();\t\\\n\t} while (0)\n#else\n#define raw_check_bogus_irq_restore() do { } while (0)\n#endif\n\n/*\n * Wrap the arch provided IRQ routines to provide appropriate checks.\n */\n#define raw_local_irq_disable()\t\tarch_local_irq_disable()\n#define raw_local_irq_enable()\t\tarch_local_irq_enable()\n#define raw_local_irq_save(flags)\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\\\n\t\tflags = arch_local_irq_save();\t\t\\\n\t} while (0)\n#define raw_local_irq_restore(flags)\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\\\n\t\traw_check_bogus_irq_restore();\t\t\\\n\t\tarch_local_irq_restore(flags);\t\t\\\n\t} while (0)\n#define raw_local_save_flags(flags)\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\\\n\t\tflags = arch_local_save_flags();\t\\\n\t} while (0)\n#define raw_irqs_disabled_flags(flags)\t\t\t\\\n\t({\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\\\n\t\tarch_irqs_disabled_flags(flags);\t\\\n\t})\n#define raw_irqs_disabled()\t\t(arch_irqs_disabled())\n#define raw_safe_halt()\t\t\tarch_safe_halt()\n\n/*\n * The local_irq_*() APIs are equal to the raw_local_irq*()\n * if !TRACE_IRQFLAGS.\n */\n#ifdef CONFIG_TRACE_IRQFLAGS\n\n#define local_irq_enable()\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\ttrace_hardirqs_on();\t\t\t\\\n\t\traw_local_irq_enable();\t\t\t\\\n\t} while (0)\n\n#define local_irq_disable()\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\tbool was_disabled = raw_irqs_disabled();\\\n\t\traw_local_irq_disable();\t\t\\\n\t\tif (!was_disabled)\t\t\t\\\n\t\t\ttrace_hardirqs_off();\t\t\\\n\t} while (0)\n\n#define local_irq_save(flags)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\traw_local_irq_save(flags);\t\t\\\n\t\tif (!raw_irqs_disabled_flags(flags))\t\\\n\t\t\ttrace_hardirqs_off();\t\t\\\n\t} while (0)\n\n#define local_irq_restore(flags)\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\tif (!raw_irqs_disabled_flags(flags))\t\\\n\t\t\ttrace_hardirqs_on();\t\t\\\n\t\traw_local_irq_restore(flags);\t\t\\\n\t} while (0)\n\n#define safe_halt()\t\t\t\t\\\n\tdo {\t\t\t\t\t\\\n\t\ttrace_hardirqs_on();\t\t\\\n\t\traw_safe_halt();\t\t\\\n\t} while (0)\n\n\n#else /* !CONFIG_TRACE_IRQFLAGS */\n\n#define local_irq_enable()\tdo { raw_local_irq_enable(); } while (0)\n#define local_irq_disable()\tdo { raw_local_irq_disable(); } while (0)\n#define local_irq_save(flags)\tdo { raw_local_irq_save(flags); } while (0)\n#define local_irq_restore(flags) do { raw_local_irq_restore(flags); } while (0)\n#define safe_halt()\t\tdo { raw_safe_halt(); } while (0)\n\n#endif /* CONFIG_TRACE_IRQFLAGS */\n\n#define local_save_flags(flags)\traw_local_save_flags(flags)\n\n/*\n * Some architectures don't define arch_irqs_disabled(), so even if either\n * definition would be fine we need to use different ones for the time being\n * to avoid build issues.\n */\n#ifdef CONFIG_TRACE_IRQFLAGS_SUPPORT\n#define irqs_disabled()\t\t\t\t\t\\\n\t({\t\t\t\t\t\t\\\n\t\tunsigned long _flags;\t\t\t\\\n\t\traw_local_save_flags(_flags);\t\t\\\n\t\traw_irqs_disabled_flags(_flags);\t\\\n\t})\n#else /* !CONFIG_TRACE_IRQFLAGS_SUPPORT */\n#define irqs_disabled()\traw_irqs_disabled()\n#endif /* CONFIG_TRACE_IRQFLAGS_SUPPORT */\n\n#define irqs_disabled_flags(flags) raw_irqs_disabled_flags(flags)\n\n#endif\n"}, "5": {"id": 5, "path": "/src/include/linux/percpu-defs.h", "content": "/* SPDX-License-Identifier: GPL-2.0-only */\n/*\n * linux/percpu-defs.h - basic definitions for percpu areas\n *\n * DO NOT INCLUDE DIRECTLY OUTSIDE PERCPU IMPLEMENTATION PROPER.\n *\n * This file is separate from linux/percpu.h to avoid cyclic inclusion\n * dependency from arch header files.  Only to be included from\n * asm/percpu.h.\n *\n * This file includes macros necessary to declare percpu sections and\n * variables, and definitions of percpu accessors and operations.  It\n * should provide enough percpu features to arch header files even when\n * they can only include asm/percpu.h to avoid cyclic inclusion dependency.\n */\n\n#ifndef _LINUX_PERCPU_DEFS_H\n#define _LINUX_PERCPU_DEFS_H\n\n#ifdef CONFIG_SMP\n\n#ifdef MODULE\n#define PER_CPU_SHARED_ALIGNED_SECTION \"\"\n#define PER_CPU_ALIGNED_SECTION \"\"\n#else\n#define PER_CPU_SHARED_ALIGNED_SECTION \"..shared_aligned\"\n#define PER_CPU_ALIGNED_SECTION \"..shared_aligned\"\n#endif\n#define PER_CPU_FIRST_SECTION \"..first\"\n\n#else\n\n#define PER_CPU_SHARED_ALIGNED_SECTION \"\"\n#define PER_CPU_ALIGNED_SECTION \"..shared_aligned\"\n#define PER_CPU_FIRST_SECTION \"\"\n\n#endif\n\n/*\n * Base implementations of per-CPU variable declarations and definitions, where\n * the section in which the variable is to be placed is provided by the\n * 'sec' argument.  This may be used to affect the parameters governing the\n * variable's storage.\n *\n * NOTE!  The sections for the DECLARE and for the DEFINE must match, lest\n * linkage errors occur due the compiler generating the wrong code to access\n * that section.\n */\n#define __PCPU_ATTRS(sec)\t\t\t\t\t\t\\\n\t__percpu __attribute__((section(PER_CPU_BASE_SECTION sec)))\t\\\n\tPER_CPU_ATTRIBUTES\n\n#define __PCPU_DUMMY_ATTRS\t\t\t\t\t\t\\\n\t__section(\".discard\") __attribute__((unused))\n\n/*\n * s390 and alpha modules require percpu variables to be defined as\n * weak to force the compiler to generate GOT based external\n * references for them.  This is necessary because percpu sections\n * will be located outside of the usually addressable area.\n *\n * This definition puts the following two extra restrictions when\n * defining percpu variables.\n *\n * 1. The symbol must be globally unique, even the static ones.\n * 2. Static percpu variables cannot be defined inside a function.\n *\n * Archs which need weak percpu definitions should define\n * ARCH_NEEDS_WEAK_PER_CPU in asm/percpu.h when necessary.\n *\n * To ensure that the generic code observes the above two\n * restrictions, if CONFIG_DEBUG_FORCE_WEAK_PER_CPU is set weak\n * definition is used for all cases.\n */\n#if defined(ARCH_NEEDS_WEAK_PER_CPU) || defined(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)\n/*\n * __pcpu_scope_* dummy variable is used to enforce scope.  It\n * receives the static modifier when it's used in front of\n * DEFINE_PER_CPU() and will trigger build failure if\n * DECLARE_PER_CPU() is used for the same variable.\n *\n * __pcpu_unique_* dummy variable is used to enforce symbol uniqueness\n * such that hidden weak symbol collision, which will cause unrelated\n * variables to share the same address, can be detected during build.\n */\n#define DECLARE_PER_CPU_SECTION(type, name, sec)\t\t\t\\\n\textern __PCPU_DUMMY_ATTRS char __pcpu_scope_##name;\t\t\\\n\textern __PCPU_ATTRS(sec) __typeof__(type) name\n\n#define DEFINE_PER_CPU_SECTION(type, name, sec)\t\t\t\t\\\n\t__PCPU_DUMMY_ATTRS char __pcpu_scope_##name;\t\t\t\\\n\textern __PCPU_DUMMY_ATTRS char __pcpu_unique_##name;\t\t\\\n\t__PCPU_DUMMY_ATTRS char __pcpu_unique_##name;\t\t\t\\\n\textern __PCPU_ATTRS(sec) __typeof__(type) name;\t\t\t\\\n\t__PCPU_ATTRS(sec) __weak __typeof__(type) name\n#else\n/*\n * Normal declaration and definition macros.\n */\n#define DECLARE_PER_CPU_SECTION(type, name, sec)\t\t\t\\\n\textern __PCPU_ATTRS(sec) __typeof__(type) name\n\n#define DEFINE_PER_CPU_SECTION(type, name, sec)\t\t\t\t\\\n\t__PCPU_ATTRS(sec) __typeof__(type) name\n#endif\n\n/*\n * Variant on the per-CPU variable declaration/definition theme used for\n * ordinary per-CPU variables.\n */\n#define DECLARE_PER_CPU(type, name)\t\t\t\t\t\\\n\tDECLARE_PER_CPU_SECTION(type, name, \"\")\n\n#define DEFINE_PER_CPU(type, name)\t\t\t\t\t\\\n\tDEFINE_PER_CPU_SECTION(type, name, \"\")\n\n/*\n * Declaration/definition used for per-CPU variables that must come first in\n * the set of variables.\n */\n#define DECLARE_PER_CPU_FIRST(type, name)\t\t\t\t\\\n\tDECLARE_PER_CPU_SECTION(type, name, PER_CPU_FIRST_SECTION)\n\n#define DEFINE_PER_CPU_FIRST(type, name)\t\t\t\t\\\n\tDEFINE_PER_CPU_SECTION(type, name, PER_CPU_FIRST_SECTION)\n\n/*\n * Declaration/definition used for per-CPU variables that must be cacheline\n * aligned under SMP conditions so that, whilst a particular instance of the\n * data corresponds to a particular CPU, inefficiencies due to direct access by\n * other CPUs are reduced by preventing the data from unnecessarily spanning\n * cachelines.\n *\n * An example of this would be statistical data, where each CPU's set of data\n * is updated by that CPU alone, but the data from across all CPUs is collated\n * by a CPU processing a read from a proc file.\n */\n#define DECLARE_PER_CPU_SHARED_ALIGNED(type, name)\t\t\t\\\n\tDECLARE_PER_CPU_SECTION(type, name, PER_CPU_SHARED_ALIGNED_SECTION) \\\n\t____cacheline_aligned_in_smp\n\n#define DEFINE_PER_CPU_SHARED_ALIGNED(type, name)\t\t\t\\\n\tDEFINE_PER_CPU_SECTION(type, name, PER_CPU_SHARED_ALIGNED_SECTION) \\\n\t____cacheline_aligned_in_smp\n\n#define DECLARE_PER_CPU_ALIGNED(type, name)\t\t\t\t\\\n\tDECLARE_PER_CPU_SECTION(type, name, PER_CPU_ALIGNED_SECTION)\t\\\n\t____cacheline_aligned\n\n#define DEFINE_PER_CPU_ALIGNED(type, name)\t\t\t\t\\\n\tDEFINE_PER_CPU_SECTION(type, name, PER_CPU_ALIGNED_SECTION)\t\\\n\t____cacheline_aligned\n\n/*\n * Declaration/definition used for per-CPU variables that must be page aligned.\n */\n#define DECLARE_PER_CPU_PAGE_ALIGNED(type, name)\t\t\t\\\n\tDECLARE_PER_CPU_SECTION(type, name, \"..page_aligned\")\t\t\\\n\t__aligned(PAGE_SIZE)\n\n#define DEFINE_PER_CPU_PAGE_ALIGNED(type, name)\t\t\t\t\\\n\tDEFINE_PER_CPU_SECTION(type, name, \"..page_aligned\")\t\t\\\n\t__aligned(PAGE_SIZE)\n\n/*\n * Declaration/definition used for per-CPU variables that must be read mostly.\n */\n#define DECLARE_PER_CPU_READ_MOSTLY(type, name)\t\t\t\\\n\tDECLARE_PER_CPU_SECTION(type, name, \"..read_mostly\")\n\n#define DEFINE_PER_CPU_READ_MOSTLY(type, name)\t\t\t\t\\\n\tDEFINE_PER_CPU_SECTION(type, name, \"..read_mostly\")\n\n/*\n * Declaration/definition used for per-CPU variables that should be accessed\n * as decrypted when memory encryption is enabled in the guest.\n */\n#ifdef CONFIG_AMD_MEM_ENCRYPT\n#define DECLARE_PER_CPU_DECRYPTED(type, name)\t\t\t\t\\\n\tDECLARE_PER_CPU_SECTION(type, name, \"..decrypted\")\n\n#define DEFINE_PER_CPU_DECRYPTED(type, name)\t\t\t\t\\\n\tDEFINE_PER_CPU_SECTION(type, name, \"..decrypted\")\n#else\n#define DEFINE_PER_CPU_DECRYPTED(type, name)\tDEFINE_PER_CPU(type, name)\n#endif\n\n/*\n * Intermodule exports for per-CPU variables.  sparse forgets about\n * address space across EXPORT_SYMBOL(), change EXPORT_SYMBOL() to\n * noop if __CHECKER__.\n */\n#ifndef __CHECKER__\n#define EXPORT_PER_CPU_SYMBOL(var) EXPORT_SYMBOL(var)\n#define EXPORT_PER_CPU_SYMBOL_GPL(var) EXPORT_SYMBOL_GPL(var)\n#else\n#define EXPORT_PER_CPU_SYMBOL(var)\n#define EXPORT_PER_CPU_SYMBOL_GPL(var)\n#endif\n\n/*\n * Accessors and operations.\n */\n#ifndef __ASSEMBLY__\n\n/*\n * __verify_pcpu_ptr() verifies @ptr is a percpu pointer without evaluating\n * @ptr and is invoked once before a percpu area is accessed by all\n * accessors and operations.  This is performed in the generic part of\n * percpu and arch overrides don't need to worry about it; however, if an\n * arch wants to implement an arch-specific percpu accessor or operation,\n * it may use __verify_pcpu_ptr() to verify the parameters.\n *\n * + 0 is required in order to convert the pointer type from a\n * potential array type to a pointer to a single item of the array.\n */\n#define __verify_pcpu_ptr(ptr)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tconst void __percpu *__vpp_verify = (typeof((ptr) + 0))NULL;\t\\\n\t(void)__vpp_verify;\t\t\t\t\t\t\\\n} while (0)\n\n#ifdef CONFIG_SMP\n\n/*\n * Add an offset to a pointer but keep the pointer as-is.  Use RELOC_HIDE()\n * to prevent the compiler from making incorrect assumptions about the\n * pointer value.  The weird cast keeps both GCC and sparse happy.\n */\n#define SHIFT_PERCPU_PTR(__p, __offset)\t\t\t\t\t\\\n\tRELOC_HIDE((typeof(*(__p)) __kernel __force *)(__p), (__offset))\n\n#define per_cpu_ptr(ptr, cpu)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__verify_pcpu_ptr(ptr);\t\t\t\t\t\t\\\n\tSHIFT_PERCPU_PTR((ptr), per_cpu_offset((cpu)));\t\t\t\\\n})\n\n#define raw_cpu_ptr(ptr)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__verify_pcpu_ptr(ptr);\t\t\t\t\t\t\\\n\tarch_raw_cpu_ptr(ptr);\t\t\t\t\t\t\\\n})\n\n#ifdef CONFIG_DEBUG_PREEMPT\n#define this_cpu_ptr(ptr)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__verify_pcpu_ptr(ptr);\t\t\t\t\t\t\\\n\tSHIFT_PERCPU_PTR(ptr, my_cpu_offset);\t\t\t\t\\\n})\n#else\n#define this_cpu_ptr(ptr) raw_cpu_ptr(ptr)\n#endif\n\n#else\t/* CONFIG_SMP */\n\n#define VERIFY_PERCPU_PTR(__p)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__verify_pcpu_ptr(__p);\t\t\t\t\t\t\\\n\t(typeof(*(__p)) __kernel __force *)(__p);\t\t\t\\\n})\n\n#define per_cpu_ptr(ptr, cpu)\t({ (void)(cpu); VERIFY_PERCPU_PTR(ptr); })\n#define raw_cpu_ptr(ptr)\tper_cpu_ptr(ptr, 0)\n#define this_cpu_ptr(ptr)\traw_cpu_ptr(ptr)\n\n#endif\t/* CONFIG_SMP */\n\n#define per_cpu(var, cpu)\t(*per_cpu_ptr(&(var), cpu))\n\n/*\n * Must be an lvalue. Since @var must be a simple identifier,\n * we force a syntax error here if it isn't.\n */\n#define get_cpu_var(var)\t\t\t\t\t\t\\\n(*({\t\t\t\t\t\t\t\t\t\\\n\tpreempt_disable();\t\t\t\t\t\t\\\n\tthis_cpu_ptr(&var);\t\t\t\t\t\t\\\n}))\n\n/*\n * The weird & is necessary because sparse considers (void)(var) to be\n * a direct dereference of percpu variable (var).\n */\n#define put_cpu_var(var)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t(void)&(var);\t\t\t\t\t\t\t\\\n\tpreempt_enable();\t\t\t\t\t\t\\\n} while (0)\n\n#define get_cpu_ptr(var)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tpreempt_disable();\t\t\t\t\t\t\\\n\tthis_cpu_ptr(var);\t\t\t\t\t\t\\\n})\n\n#define put_cpu_ptr(var)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t(void)(var);\t\t\t\t\t\t\t\\\n\tpreempt_enable();\t\t\t\t\t\t\\\n} while (0)\n\n/*\n * Branching function to split up a function into a set of functions that\n * are called for different scalar sizes of the objects handled.\n */\n\nextern void __bad_size_call_parameter(void);\n\n#ifdef CONFIG_DEBUG_PREEMPT\nextern void __this_cpu_preempt_check(const char *op);\n#else\nstatic inline void __this_cpu_preempt_check(const char *op) { }\n#endif\n\n#define __pcpu_size_call_return(stem, variable)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\ttypeof(variable) pscr_ret__;\t\t\t\t\t\\\n\t__verify_pcpu_ptr(&(variable));\t\t\t\t\t\\\n\tswitch(sizeof(variable)) {\t\t\t\t\t\\\n\tcase 1: pscr_ret__ = stem##1(variable); break;\t\t\t\\\n\tcase 2: pscr_ret__ = stem##2(variable); break;\t\t\t\\\n\tcase 4: pscr_ret__ = stem##4(variable); break;\t\t\t\\\n\tcase 8: pscr_ret__ = stem##8(variable); break;\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t__bad_size_call_parameter(); break;\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tpscr_ret__;\t\t\t\t\t\t\t\\\n})\n\n#define __pcpu_size_call_return2(stem, variable, ...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\ttypeof(variable) pscr2_ret__;\t\t\t\t\t\\\n\t__verify_pcpu_ptr(&(variable));\t\t\t\t\t\\\n\tswitch(sizeof(variable)) {\t\t\t\t\t\\\n\tcase 1: pscr2_ret__ = stem##1(variable, __VA_ARGS__); break;\t\\\n\tcase 2: pscr2_ret__ = stem##2(variable, __VA_ARGS__); break;\t\\\n\tcase 4: pscr2_ret__ = stem##4(variable, __VA_ARGS__); break;\t\\\n\tcase 8: pscr2_ret__ = stem##8(variable, __VA_ARGS__); break;\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t__bad_size_call_parameter(); break;\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tpscr2_ret__;\t\t\t\t\t\t\t\\\n})\n\n/*\n * Special handling for cmpxchg_double.  cmpxchg_double is passed two\n * percpu variables.  The first has to be aligned to a double word\n * boundary and the second has to follow directly thereafter.\n * We enforce this on all architectures even if they don't support\n * a double cmpxchg instruction, since it's a cheap requirement, and it\n * avoids breaking the requirement for architectures with the instruction.\n */\n#define __pcpu_double_call_return_bool(stem, pcp1, pcp2, ...)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tbool pdcrb_ret__;\t\t\t\t\t\t\\\n\t__verify_pcpu_ptr(&(pcp1));\t\t\t\t\t\\\n\tBUILD_BUG_ON(sizeof(pcp1) != sizeof(pcp2));\t\t\t\\\n\tVM_BUG_ON((unsigned long)(&(pcp1)) % (2 * sizeof(pcp1)));\t\\\n\tVM_BUG_ON((unsigned long)(&(pcp2)) !=\t\t\t\t\\\n\t\t  (unsigned long)(&(pcp1)) + sizeof(pcp1));\t\t\\\n\tswitch(sizeof(pcp1)) {\t\t\t\t\t\t\\\n\tcase 1: pdcrb_ret__ = stem##1(pcp1, pcp2, __VA_ARGS__); break;\t\\\n\tcase 2: pdcrb_ret__ = stem##2(pcp1, pcp2, __VA_ARGS__); break;\t\\\n\tcase 4: pdcrb_ret__ = stem##4(pcp1, pcp2, __VA_ARGS__); break;\t\\\n\tcase 8: pdcrb_ret__ = stem##8(pcp1, pcp2, __VA_ARGS__); break;\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t__bad_size_call_parameter(); break;\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tpdcrb_ret__;\t\t\t\t\t\t\t\\\n})\n\n#define __pcpu_size_call(stem, variable, ...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t__verify_pcpu_ptr(&(variable));\t\t\t\t\t\\\n\tswitch(sizeof(variable)) {\t\t\t\t\t\\\n\t\tcase 1: stem##1(variable, __VA_ARGS__);break;\t\t\\\n\t\tcase 2: stem##2(variable, __VA_ARGS__);break;\t\t\\\n\t\tcase 4: stem##4(variable, __VA_ARGS__);break;\t\t\\\n\t\tcase 8: stem##8(variable, __VA_ARGS__);break;\t\t\\\n\t\tdefault: \t\t\t\t\t\t\\\n\t\t\t__bad_size_call_parameter();break;\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n/*\n * this_cpu operations (C) 2008-2013 Christoph Lameter <cl@linux.com>\n *\n * Optimized manipulation for memory allocated through the per cpu\n * allocator or for addresses of per cpu variables.\n *\n * These operation guarantee exclusivity of access for other operations\n * on the *same* processor. The assumption is that per cpu data is only\n * accessed by a single processor instance (the current one).\n *\n * The arch code can provide optimized implementation by defining macros\n * for certain scalar sizes. F.e. provide this_cpu_add_2() to provide per\n * cpu atomic operations for 2 byte sized RMW actions. If arch code does\n * not provide operations for a scalar size then the fallback in the\n * generic code will be used.\n *\n * cmpxchg_double replaces two adjacent scalars at once.  The first two\n * parameters are per cpu variables which have to be of the same size.  A\n * truth value is returned to indicate success or failure (since a double\n * register result is difficult to handle).  There is very limited hardware\n * support for these operations, so only certain sizes may work.\n */\n\n/*\n * Operations for contexts where we do not want to do any checks for\n * preemptions.  Unless strictly necessary, always use [__]this_cpu_*()\n * instead.\n *\n * If there is no other protection through preempt disable and/or disabling\n * interupts then one of these RMW operations can show unexpected behavior\n * because the execution thread was rescheduled on another processor or an\n * interrupt occurred and the same percpu variable was modified from the\n * interrupt context.\n */\n#define raw_cpu_read(pcp)\t\t__pcpu_size_call_return(raw_cpu_read_, pcp)\n#define raw_cpu_write(pcp, val)\t\t__pcpu_size_call(raw_cpu_write_, pcp, val)\n#define raw_cpu_add(pcp, val)\t\t__pcpu_size_call(raw_cpu_add_, pcp, val)\n#define raw_cpu_and(pcp, val)\t\t__pcpu_size_call(raw_cpu_and_, pcp, val)\n#define raw_cpu_or(pcp, val)\t\t__pcpu_size_call(raw_cpu_or_, pcp, val)\n#define raw_cpu_add_return(pcp, val)\t__pcpu_size_call_return2(raw_cpu_add_return_, pcp, val)\n#define raw_cpu_xchg(pcp, nval)\t\t__pcpu_size_call_return2(raw_cpu_xchg_, pcp, nval)\n#define raw_cpu_cmpxchg(pcp, oval, nval) \\\n\t__pcpu_size_call_return2(raw_cpu_cmpxchg_, pcp, oval, nval)\n#define raw_cpu_cmpxchg_double(pcp1, pcp2, oval1, oval2, nval1, nval2) \\\n\t__pcpu_double_call_return_bool(raw_cpu_cmpxchg_double_, pcp1, pcp2, oval1, oval2, nval1, nval2)\n\n#define raw_cpu_sub(pcp, val)\t\traw_cpu_add(pcp, -(val))\n#define raw_cpu_inc(pcp)\t\traw_cpu_add(pcp, 1)\n#define raw_cpu_dec(pcp)\t\traw_cpu_sub(pcp, 1)\n#define raw_cpu_sub_return(pcp, val)\traw_cpu_add_return(pcp, -(typeof(pcp))(val))\n#define raw_cpu_inc_return(pcp)\t\traw_cpu_add_return(pcp, 1)\n#define raw_cpu_dec_return(pcp)\t\traw_cpu_add_return(pcp, -1)\n\n/*\n * Operations for contexts that are safe from preemption/interrupts.  These\n * operations verify that preemption is disabled.\n */\n#define __this_cpu_read(pcp)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"read\");\t\t\t\t\\\n\traw_cpu_read(pcp);\t\t\t\t\t\t\\\n})\n\n#define __this_cpu_write(pcp, val)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"write\");\t\t\t\t\\\n\traw_cpu_write(pcp, val);\t\t\t\t\t\\\n})\n\n#define __this_cpu_add(pcp, val)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"add\");\t\t\t\t\\\n\traw_cpu_add(pcp, val);\t\t\t\t\t\t\\\n})\n\n#define __this_cpu_and(pcp, val)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"and\");\t\t\t\t\\\n\traw_cpu_and(pcp, val);\t\t\t\t\t\t\\\n})\n\n#define __this_cpu_or(pcp, val)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"or\");\t\t\t\t\t\\\n\traw_cpu_or(pcp, val);\t\t\t\t\t\t\\\n})\n\n#define __this_cpu_add_return(pcp, val)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"add_return\");\t\t\t\t\\\n\traw_cpu_add_return(pcp, val);\t\t\t\t\t\\\n})\n\n#define __this_cpu_xchg(pcp, nval)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"xchg\");\t\t\t\t\\\n\traw_cpu_xchg(pcp, nval);\t\t\t\t\t\\\n})\n\n#define __this_cpu_cmpxchg(pcp, oval, nval)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"cmpxchg\");\t\t\t\t\\\n\traw_cpu_cmpxchg(pcp, oval, nval);\t\t\t\t\\\n})\n\n#define __this_cpu_cmpxchg_double(pcp1, pcp2, oval1, oval2, nval1, nval2) \\\n({\t__this_cpu_preempt_check(\"cmpxchg_double\");\t\t\t\\\n\traw_cpu_cmpxchg_double(pcp1, pcp2, oval1, oval2, nval1, nval2);\t\\\n})\n\n#define __this_cpu_sub(pcp, val)\t__this_cpu_add(pcp, -(typeof(pcp))(val))\n#define __this_cpu_inc(pcp)\t\t__this_cpu_add(pcp, 1)\n#define __this_cpu_dec(pcp)\t\t__this_cpu_sub(pcp, 1)\n#define __this_cpu_sub_return(pcp, val)\t__this_cpu_add_return(pcp, -(typeof(pcp))(val))\n#define __this_cpu_inc_return(pcp)\t__this_cpu_add_return(pcp, 1)\n#define __this_cpu_dec_return(pcp)\t__this_cpu_add_return(pcp, -1)\n\n/*\n * Operations with implied preemption/interrupt protection.  These\n * operations can be used without worrying about preemption or interrupt.\n */\n#define this_cpu_read(pcp)\t\t__pcpu_size_call_return(this_cpu_read_, pcp)\n#define this_cpu_write(pcp, val)\t__pcpu_size_call(this_cpu_write_, pcp, val)\n#define this_cpu_add(pcp, val)\t\t__pcpu_size_call(this_cpu_add_, pcp, val)\n#define this_cpu_and(pcp, val)\t\t__pcpu_size_call(this_cpu_and_, pcp, val)\n#define this_cpu_or(pcp, val)\t\t__pcpu_size_call(this_cpu_or_, pcp, val)\n#define this_cpu_add_return(pcp, val)\t__pcpu_size_call_return2(this_cpu_add_return_, pcp, val)\n#define this_cpu_xchg(pcp, nval)\t__pcpu_size_call_return2(this_cpu_xchg_, pcp, nval)\n#define this_cpu_cmpxchg(pcp, oval, nval) \\\n\t__pcpu_size_call_return2(this_cpu_cmpxchg_, pcp, oval, nval)\n#define this_cpu_cmpxchg_double(pcp1, pcp2, oval1, oval2, nval1, nval2) \\\n\t__pcpu_double_call_return_bool(this_cpu_cmpxchg_double_, pcp1, pcp2, oval1, oval2, nval1, nval2)\n\n#define this_cpu_sub(pcp, val)\t\tthis_cpu_add(pcp, -(typeof(pcp))(val))\n#define this_cpu_inc(pcp)\t\tthis_cpu_add(pcp, 1)\n#define this_cpu_dec(pcp)\t\tthis_cpu_sub(pcp, 1)\n#define this_cpu_sub_return(pcp, val)\tthis_cpu_add_return(pcp, -(typeof(pcp))(val))\n#define this_cpu_inc_return(pcp)\tthis_cpu_add_return(pcp, 1)\n#define this_cpu_dec_return(pcp)\tthis_cpu_add_return(pcp, -1)\n\n#endif /* __ASSEMBLY__ */\n#endif /* _LINUX_PERCPU_DEFS_H */\n"}, "6": {"id": 6, "path": "/src/include/asm-generic/rwonce.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n/*\n * Prevent the compiler from merging or refetching reads or writes. The\n * compiler is also forbidden from reordering successive instances of\n * READ_ONCE and WRITE_ONCE, but only when the compiler is aware of some\n * particular ordering. One way to make the compiler aware of ordering is to\n * put the two invocations of READ_ONCE or WRITE_ONCE in different C\n * statements.\n *\n * These two macros will also work on aggregate data types like structs or\n * unions.\n *\n * Their two major use cases are: (1) Mediating communication between\n * process-level code and irq/NMI handlers, all running on the same CPU,\n * and (2) Ensuring that the compiler does not fold, spindle, or otherwise\n * mutilate accesses that either do not require ordering or that interact\n * with an explicit memory barrier or atomic instruction that provides the\n * required ordering.\n */\n#ifndef __ASM_GENERIC_RWONCE_H\n#define __ASM_GENERIC_RWONCE_H\n\n#ifndef __ASSEMBLY__\n\n#include <linux/compiler_types.h>\n#include <linux/kasan-checks.h>\n#include <linux/kcsan-checks.h>\n\n/*\n * Yes, this permits 64-bit accesses on 32-bit architectures. These will\n * actually be atomic in some cases (namely Armv7 + LPAE), but for others we\n * rely on the access being split into 2x32-bit accesses for a 32-bit quantity\n * (e.g. a virtual address) and a strong prevailing wind.\n */\n#define compiletime_assert_rwonce_type(t)\t\t\t\t\t\\\n\tcompiletime_assert(__native_word(t) || sizeof(t) == sizeof(long long),\t\\\n\t\t\"Unsupported access size for {READ,WRITE}_ONCE().\")\n\n/*\n * Use __READ_ONCE() instead of READ_ONCE() if you do not require any\n * atomicity. Note that this may result in tears!\n */\n#ifndef __READ_ONCE\n#define __READ_ONCE(x)\t(*(const volatile __unqual_scalar_typeof(x) *)&(x))\n#endif\n\n#define READ_ONCE(x)\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tcompiletime_assert_rwonce_type(x);\t\t\t\t\\\n\t__READ_ONCE(x);\t\t\t\t\t\t\t\\\n})\n\n#define __WRITE_ONCE(x, val)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t*(volatile typeof(x) *)&(x) = (val);\t\t\t\t\\\n} while (0)\n\n#define WRITE_ONCE(x, val)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tcompiletime_assert_rwonce_type(x);\t\t\t\t\\\n\t__WRITE_ONCE(x, val);\t\t\t\t\t\t\\\n} while (0)\n\nstatic __no_sanitize_or_inline\nunsigned long __read_once_word_nocheck(const void *addr)\n{\n\treturn __READ_ONCE(*(unsigned long *)addr);\n}\n\n/*\n * Use READ_ONCE_NOCHECK() instead of READ_ONCE() if you need to load a\n * word from memory atomically but without telling KASAN/KCSAN. This is\n * usually used by unwinding code when walking the stack of a running process.\n */\n#define READ_ONCE_NOCHECK(x)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tcompiletime_assert(sizeof(x) == sizeof(unsigned long),\t\t\\\n\t\t\"Unsupported access size for READ_ONCE_NOCHECK().\");\t\\\n\t(typeof(x))__read_once_word_nocheck(&(x));\t\t\t\\\n})\n\nstatic __no_kasan_or_inline\nunsigned long read_word_at_a_time(const void *addr)\n{\n\tkasan_check_read(addr, 1);\n\treturn *(unsigned long *)addr;\n}\n\n#endif /* __ASSEMBLY__ */\n#endif\t/* __ASM_GENERIC_RWONCE_H */\n"}, "7": {"id": 7, "path": "/src/include/linux/compiler_types.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_COMPILER_TYPES_H\n#define __LINUX_COMPILER_TYPES_H\n\n#ifndef __ASSEMBLY__\n\n#ifdef __CHECKER__\n/* address spaces */\n# define __kernel\t__attribute__((address_space(0)))\n# define __user\t\t__attribute__((noderef, address_space(__user)))\n# define __iomem\t__attribute__((noderef, address_space(__iomem)))\n# define __percpu\t__attribute__((noderef, address_space(__percpu)))\n# define __rcu\t\t__attribute__((noderef, address_space(__rcu)))\nstatic inline void __chk_user_ptr(const volatile void __user *ptr) { }\nstatic inline void __chk_io_ptr(const volatile void __iomem *ptr) { }\n/* context/locking */\n# define __must_hold(x)\t__attribute__((context(x,1,1)))\n# define __acquires(x)\t__attribute__((context(x,0,1)))\n# define __releases(x)\t__attribute__((context(x,1,0)))\n# define __acquire(x)\t__context__(x,1)\n# define __release(x)\t__context__(x,-1)\n# define __cond_lock(x,c)\t((c) ? ({ __acquire(x); 1; }) : 0)\n/* other */\n# define __force\t__attribute__((force))\n# define __nocast\t__attribute__((nocast))\n# define __safe\t\t__attribute__((safe))\n# define __private\t__attribute__((noderef))\n# define ACCESS_PRIVATE(p, member) (*((typeof((p)->member) __force *) &(p)->member))\n#else /* __CHECKER__ */\n/* address spaces */\n# define __kernel\n# ifdef STRUCTLEAK_PLUGIN\n#  define __user\t__attribute__((user))\n# else\n#  define __user\n# endif\n# define __iomem\n# define __percpu\n# define __rcu\n# define __chk_user_ptr(x)\t(void)0\n# define __chk_io_ptr(x)\t(void)0\n/* context/locking */\n# define __must_hold(x)\n# define __acquires(x)\n# define __releases(x)\n# define __acquire(x)\t(void)0\n# define __release(x)\t(void)0\n# define __cond_lock(x,c) (c)\n/* other */\n# define __force\n# define __nocast\n# define __safe\n# define __private\n# define ACCESS_PRIVATE(p, member) ((p)->member)\n# define __builtin_warning(x, y...) (1)\n#endif /* __CHECKER__ */\n\n/* Indirect macros required for expanded argument pasting, eg. __LINE__. */\n#define ___PASTE(a,b) a##b\n#define __PASTE(a,b) ___PASTE(a,b)\n\n#ifdef __KERNEL__\n\n/* Attributes */\n#include <linux/compiler_attributes.h>\n\n/* Builtins */\n\n/*\n * __has_builtin is supported on gcc >= 10, clang >= 3 and icc >= 21.\n * In the meantime, to support gcc < 10, we implement __has_builtin\n * by hand.\n */\n#ifndef __has_builtin\n#define __has_builtin(x) (0)\n#endif\n\n/* Compiler specific macros. */\n#ifdef __clang__\n#include <linux/compiler-clang.h>\n#elif defined(__INTEL_COMPILER)\n#include <linux/compiler-intel.h>\n#elif defined(__GNUC__)\n/* The above compilers also define __GNUC__, so order is important here. */\n#include <linux/compiler-gcc.h>\n#else\n#error \"Unknown compiler\"\n#endif\n\n/*\n * Some architectures need to provide custom definitions of macros provided\n * by linux/compiler-*.h, and can do so using asm/compiler.h. We include that\n * conditionally rather than using an asm-generic wrapper in order to avoid\n * build failures if any C compilation, which will include this file via an\n * -include argument in c_flags, occurs prior to the asm-generic wrappers being\n * generated.\n */\n#ifdef CONFIG_HAVE_ARCH_COMPILER_H\n#include <asm/compiler.h>\n#endif\n\nstruct ftrace_branch_data {\n\tconst char *func;\n\tconst char *file;\n\tunsigned line;\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long correct;\n\t\t\tunsigned long incorrect;\n\t\t};\n\t\tstruct {\n\t\t\tunsigned long miss;\n\t\t\tunsigned long hit;\n\t\t};\n\t\tunsigned long miss_hit[2];\n\t};\n};\n\nstruct ftrace_likely_data {\n\tstruct ftrace_branch_data\tdata;\n\tunsigned long\t\t\tconstant;\n};\n\n#if defined(CC_USING_HOTPATCH)\n#define notrace\t\t\t__attribute__((hotpatch(0, 0)))\n#elif defined(CC_USING_PATCHABLE_FUNCTION_ENTRY)\n#define notrace\t\t\t__attribute__((patchable_function_entry(0, 0)))\n#else\n#define notrace\t\t\t__attribute__((__no_instrument_function__))\n#endif\n\n/*\n * it doesn't make sense on ARM (currently the only user of __naked)\n * to trace naked functions because then mcount is called without\n * stack and frame pointer being set up and there is no chance to\n * restore the lr register to the value before mcount was called.\n */\n#define __naked\t\t\t__attribute__((__naked__)) notrace\n\n#define __compiler_offsetof(a, b)\t__builtin_offsetof(a, b)\n\n/*\n * Prefer gnu_inline, so that extern inline functions do not emit an\n * externally visible function. This makes extern inline behave as per gnu89\n * semantics rather than c99. This prevents multiple symbol definition errors\n * of extern inline functions at link time.\n * A lot of inline functions can cause havoc with function tracing.\n */\n#define inline inline __gnu_inline __inline_maybe_unused notrace\n\n/*\n * gcc provides both __inline__ and __inline as alternate spellings of\n * the inline keyword, though the latter is undocumented. New kernel\n * code should only use the inline spelling, but some existing code\n * uses __inline__. Since we #define inline above, to ensure\n * __inline__ has the same semantics, we need this #define.\n *\n * However, the spelling __inline is strictly reserved for referring\n * to the bare keyword.\n */\n#define __inline__ inline\n\n/*\n * GCC does not warn about unused static inline functions for -Wunused-function.\n * Suppress the warning in clang as well by using __maybe_unused, but enable it\n * for W=1 build. This will allow clang to find unused functions. Remove the\n * __inline_maybe_unused entirely after fixing most of -Wunused-function warnings.\n */\n#ifdef KBUILD_EXTRA_WARN1\n#define __inline_maybe_unused\n#else\n#define __inline_maybe_unused __maybe_unused\n#endif\n\n/*\n * Rather then using noinline to prevent stack consumption, use\n * noinline_for_stack instead.  For documentation reasons.\n */\n#define noinline_for_stack noinline\n\n/*\n * Sanitizer helper attributes: Because using __always_inline and\n * __no_sanitize_* conflict, provide helper attributes that will either expand\n * to __no_sanitize_* in compilation units where instrumentation is enabled\n * (__SANITIZE_*__), or __always_inline in compilation units without\n * instrumentation (__SANITIZE_*__ undefined).\n */\n#ifdef __SANITIZE_ADDRESS__\n/*\n * We can't declare function 'inline' because __no_sanitize_address conflicts\n * with inlining. Attempt to inline it may cause a build failure.\n *     https://gcc.gnu.org/bugzilla/show_bug.cgi?id=67368\n * '__maybe_unused' allows us to avoid defined-but-not-used warnings.\n */\n# define __no_kasan_or_inline __no_sanitize_address notrace __maybe_unused\n# define __no_sanitize_or_inline __no_kasan_or_inline\n#else\n# define __no_kasan_or_inline __always_inline\n#endif\n\n#define __no_kcsan __no_sanitize_thread\n#ifdef __SANITIZE_THREAD__\n# define __no_sanitize_or_inline __no_kcsan notrace __maybe_unused\n#endif\n\n#ifndef __no_sanitize_or_inline\n#define __no_sanitize_or_inline __always_inline\n#endif\n\n/* Section for code which can't be instrumented at all */\n#define noinstr\t\t\t\t\t\t\t\t\\\n\tnoinline notrace __attribute((__section__(\".noinstr.text\")))\t\\\n\t__no_kcsan __no_sanitize_address\n\n#endif /* __KERNEL__ */\n\n#endif /* __ASSEMBLY__ */\n\n/*\n * The below symbols may be defined for one or more, but not ALL, of the above\n * compilers. We don't consider that to be an error, so set them to nothing.\n * For example, some of them are for compiler specific plugins.\n */\n#ifndef __latent_entropy\n# define __latent_entropy\n#endif\n\n#ifndef __randomize_layout\n# define __randomize_layout __designated_init\n#endif\n\n#ifndef __no_randomize_layout\n# define __no_randomize_layout\n#endif\n\n#ifndef randomized_struct_fields_start\n# define randomized_struct_fields_start\n# define randomized_struct_fields_end\n#endif\n\n#ifndef __noscs\n# define __noscs\n#endif\n\n#ifndef asm_volatile_goto\n#define asm_volatile_goto(x...) asm goto(x)\n#endif\n\n#ifdef CONFIG_CC_HAS_ASM_INLINE\n#define asm_inline asm __inline\n#else\n#define asm_inline asm\n#endif\n\n/* Are two types/vars the same type (ignoring qualifiers)? */\n#define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b))\n\n/*\n * __unqual_scalar_typeof(x) - Declare an unqualified scalar type, leaving\n *\t\t\t       non-scalar types unchanged.\n */\n/*\n * Prefer C11 _Generic for better compile-times and simpler code. Note: 'char'\n * is not type-compatible with 'signed char', and we define a separate case.\n */\n#define __scalar_type_to_expr_cases(type)\t\t\t\t\\\n\t\tunsigned type:\t(unsigned type)0,\t\t\t\\\n\t\tsigned type:\t(signed type)0\n\n#define __unqual_scalar_typeof(x) typeof(\t\t\t\t\\\n\t\t_Generic((x),\t\t\t\t\t\t\\\n\t\t\t char:\t(char)0,\t\t\t\t\\\n\t\t\t __scalar_type_to_expr_cases(char),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(short),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(int),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(long),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(long long),\t\\\n\t\t\t default: (x)))\n\n/* Is this type a native word size -- useful for atomic operations */\n#define __native_word(t) \\\n\t(sizeof(t) == sizeof(char) || sizeof(t) == sizeof(short) || \\\n\t sizeof(t) == sizeof(int) || sizeof(t) == sizeof(long))\n\n/* Compile time object size, -1 for unknown */\n#ifndef __compiletime_object_size\n# define __compiletime_object_size(obj) -1\n#endif\n#ifndef __compiletime_warning\n# define __compiletime_warning(message)\n#endif\n#ifndef __compiletime_error\n# define __compiletime_error(message)\n#endif\n\n#ifdef __OPTIMIZE__\n# define __compiletime_assert(condition, msg, prefix, suffix)\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\textern void prefix ## suffix(void) __compiletime_error(msg); \\\n\t\tif (!(condition))\t\t\t\t\t\\\n\t\t\tprefix ## suffix();\t\t\t\t\\\n\t} while (0)\n#else\n# define __compiletime_assert(condition, msg, prefix, suffix) do { } while (0)\n#endif\n\n#define _compiletime_assert(condition, msg, prefix, suffix) \\\n\t__compiletime_assert(condition, msg, prefix, suffix)\n\n/**\n * compiletime_assert - break build and emit msg if condition is false\n * @condition: a compile-time constant condition to check\n * @msg:       a message to emit if condition is false\n *\n * In tradition of POSIX assert, this macro will break the build if the\n * supplied condition is *false*, emitting the supplied error message if the\n * compiler has support to do so.\n */\n#define compiletime_assert(condition, msg) \\\n\t_compiletime_assert(condition, msg, __compiletime_assert_, __COUNTER__)\n\n#define compiletime_assert_atomic_type(t)\t\t\t\t\\\n\tcompiletime_assert(__native_word(t),\t\t\t\t\\\n\t\t\"Need native word sized stores/loads for atomicity.\")\n\n/* Helpers for emitting diagnostics in pragmas. */\n#ifndef __diag\n#define __diag(string)\n#endif\n\n#ifndef __diag_GCC\n#define __diag_GCC(version, severity, string)\n#endif\n\n#define __diag_push()\t__diag(push)\n#define __diag_pop()\t__diag(pop)\n\n#define __diag_ignore(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, ignore, option)\n#define __diag_warn(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, warn, option)\n#define __diag_error(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, error, option)\n\n#endif /* __LINUX_COMPILER_TYPES_H */\n"}, "8": {"id": 8, "path": "/src/include/linux/minmax.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_MINMAX_H\n#define _LINUX_MINMAX_H\n\n/*\n * min()/max()/clamp() macros must accomplish three things:\n *\n * - avoid multiple evaluations of the arguments (so side-effects like\n *   \"x++\" happen only once) when non-constant.\n * - perform strict type-checking (to generate warnings instead of\n *   nasty runtime surprises). See the \"unnecessary\" pointer comparison\n *   in __typecheck().\n * - retain result as a constant expressions when called with only\n *   constant expressions (to avoid tripping VLA warnings in stack\n *   allocation usage).\n */\n#define __typecheck(x, y) \\\n\t(!!(sizeof((typeof(x) *)1 == (typeof(y) *)1)))\n\n/*\n * This returns a constant expression while determining if an argument is\n * a constant expression, most importantly without evaluating the argument.\n * Glory to Martin Uecker <Martin.Uecker@med.uni-goettingen.de>\n */\n#define __is_constexpr(x) \\\n\t(sizeof(int) == sizeof(*(8 ? ((void *)((long)(x) * 0l)) : (int *)8)))\n\n#define __no_side_effects(x, y) \\\n\t\t(__is_constexpr(x) && __is_constexpr(y))\n\n#define __safe_cmp(x, y) \\\n\t\t(__typecheck(x, y) && __no_side_effects(x, y))\n\n#define __cmp(x, y, op)\t((x) op (y) ? (x) : (y))\n\n#define __cmp_once(x, y, unique_x, unique_y, op) ({\t\\\n\t\ttypeof(x) unique_x = (x);\t\t\\\n\t\ttypeof(y) unique_y = (y);\t\t\\\n\t\t__cmp(unique_x, unique_y, op); })\n\n#define __careful_cmp(x, y, op) \\\n\t__builtin_choose_expr(__safe_cmp(x, y), \\\n\t\t__cmp(x, y, op), \\\n\t\t__cmp_once(x, y, __UNIQUE_ID(__x), __UNIQUE_ID(__y), op))\n\n/**\n * min - return minimum of two values of the same or compatible types\n * @x: first value\n * @y: second value\n */\n#define min(x, y)\t__careful_cmp(x, y, <)\n\n/**\n * max - return maximum of two values of the same or compatible types\n * @x: first value\n * @y: second value\n */\n#define max(x, y)\t__careful_cmp(x, y, >)\n\n/**\n * min3 - return minimum of three values\n * @x: first value\n * @y: second value\n * @z: third value\n */\n#define min3(x, y, z) min((typeof(x))min(x, y), z)\n\n/**\n * max3 - return maximum of three values\n * @x: first value\n * @y: second value\n * @z: third value\n */\n#define max3(x, y, z) max((typeof(x))max(x, y), z)\n\n/**\n * min_not_zero - return the minimum that is _not_ zero, unless both are zero\n * @x: value1\n * @y: value2\n */\n#define min_not_zero(x, y) ({\t\t\t\\\n\ttypeof(x) __x = (x);\t\t\t\\\n\ttypeof(y) __y = (y);\t\t\t\\\n\t__x == 0 ? __y : ((__y == 0) ? __x : min(__x, __y)); })\n\n/**\n * clamp - return a value clamped to a given range with strict typechecking\n * @val: current value\n * @lo: lowest allowable value\n * @hi: highest allowable value\n *\n * This macro does strict typechecking of @lo/@hi to make sure they are of the\n * same type as @val.  See the unnecessary pointer comparisons.\n */\n#define clamp(val, lo, hi) min((typeof(val))max(val, lo), hi)\n\n/*\n * ..and if you can't take the strict\n * types, you can specify one yourself.\n *\n * Or not use min/max/clamp at all, of course.\n */\n\n/**\n * min_t - return minimum of two values, using the specified type\n * @type: data type to use\n * @x: first value\n * @y: second value\n */\n#define min_t(type, x, y)\t__careful_cmp((type)(x), (type)(y), <)\n\n/**\n * max_t - return maximum of two values, using the specified type\n * @type: data type to use\n * @x: first value\n * @y: second value\n */\n#define max_t(type, x, y)\t__careful_cmp((type)(x), (type)(y), >)\n\n/**\n * clamp_t - return a value clamped to a given range using a given type\n * @type: the type of variable to use\n * @val: current value\n * @lo: minimum allowable value\n * @hi: maximum allowable value\n *\n * This macro does no typechecking and uses temporary variables of type\n * @type to make all the comparisons.\n */\n#define clamp_t(type, val, lo, hi) min_t(type, max_t(type, val, lo), hi)\n\n/**\n * clamp_val - return a value clamped to a given range using val's type\n * @val: current value\n * @lo: minimum allowable value\n * @hi: maximum allowable value\n *\n * This macro does no typechecking and uses temporary variables of whatever\n * type the input argument @val is.  This is useful when @val is an unsigned\n * type and @lo and @hi are literals that will otherwise be assigned a signed\n * integer type.\n */\n#define clamp_val(val, lo, hi) clamp_t(typeof(val), val, lo, hi)\n\n/**\n * swap - swap values of @a and @b\n * @a: first value\n * @b: second value\n */\n#define swap(a, b) \\\n\tdo { typeof(a) __tmp = (a); (a) = (b); (b) = __tmp; } while (0)\n\n#endif\t/* _LINUX_MINMAX_H */\n"}, "9": {"id": 9, "path": "/src/include/asm-generic/div64.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_GENERIC_DIV64_H\n#define _ASM_GENERIC_DIV64_H\n/*\n * Copyright (C) 2003 Bernardo Innocenti <bernie@develer.com>\n * Based on former asm-ppc/div64.h and asm-m68knommu/div64.h\n *\n * Optimization for constant divisors on 32-bit machines:\n * Copyright (C) 2006-2015 Nicolas Pitre\n *\n * The semantics of do_div() are:\n *\n * uint32_t do_div(uint64_t *n, uint32_t base)\n * {\n * \tuint32_t remainder = *n % base;\n * \t*n = *n / base;\n * \treturn remainder;\n * }\n *\n * NOTE: macro parameter n is evaluated multiple times,\n *       beware of side effects!\n */\n\n#include <linux/types.h>\n#include <linux/compiler.h>\n\n#if BITS_PER_LONG == 64\n\n/**\n * do_div - returns 2 values: calculate remainder and update new dividend\n * @n: uint64_t dividend (will be updated)\n * @base: uint32_t divisor\n *\n * Summary:\n * ``uint32_t remainder = n % base;``\n * ``n = n / base;``\n *\n * Return: (uint32_t)remainder\n *\n * NOTE: macro parameter @n is evaluated multiple times,\n * beware of side effects!\n */\n# define do_div(n,base) ({\t\t\t\t\t\\\n\tuint32_t __base = (base);\t\t\t\t\\\n\tuint32_t __rem;\t\t\t\t\t\t\\\n\t__rem = ((uint64_t)(n)) % __base;\t\t\t\\\n\t(n) = ((uint64_t)(n)) / __base;\t\t\t\t\\\n\t__rem;\t\t\t\t\t\t\t\\\n })\n\n#elif BITS_PER_LONG == 32\n\n#include <linux/log2.h>\n\n/*\n * If the divisor happens to be constant, we determine the appropriate\n * inverse at compile time to turn the division into a few inline\n * multiplications which ought to be much faster. And yet only if compiling\n * with a sufficiently recent gcc version to perform proper 64-bit constant\n * propagation.\n *\n * (It is unfortunate that gcc doesn't perform all this internally.)\n */\n\n#ifndef __div64_const32_is_OK\n#define __div64_const32_is_OK (__GNUC__ >= 4)\n#endif\n\n#define __div64_const32(n, ___b)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t/*\t\t\t\t\t\t\t\t\\\n\t * Multiplication by reciprocal of b: n / b = n * (p / b) / p\t\\\n\t *\t\t\t\t\t\t\t\t\\\n\t * We rely on the fact that most of this code gets optimized\t\\\n\t * away at compile time due to constant propagation and only\t\\\n\t * a few multiplication instructions should remain.\t\t\\\n\t * Hence this monstrous macro (static inline doesn't always\t\\\n\t * do the trick here).\t\t\t\t\t\t\\\n\t */\t\t\t\t\t\t\t\t\\\n\tuint64_t ___res, ___x, ___t, ___m, ___n = (n);\t\t\t\\\n\tuint32_t ___p, ___bias;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t/* determine MSB of b */\t\t\t\t\t\\\n\t___p = 1 << ilog2(___b);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t/* compute m = ((p << 64) + b - 1) / b */\t\t\t\\\n\t___m = (~0ULL / ___b) * ___p;\t\t\t\t\t\\\n\t___m += (((~0ULL % ___b + 1) * ___p) + ___b - 1) / ___b;\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t/* one less than the dividend with highest result */\t\t\\\n\t___x = ~0ULL / ___b * ___b - 1;\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t/* test our ___m with res = m * x / (p << 64) */\t\t\\\n\t___res = ((___m & 0xffffffff) * (___x & 0xffffffff)) >> 32;\t\\\n\t___t = ___res += (___m & 0xffffffff) * (___x >> 32);\t\t\\\n\t___res += (___x & 0xffffffff) * (___m >> 32);\t\t\t\\\n\t___t = (___res < ___t) ? (1ULL << 32) : 0;\t\t\t\\\n\t___res = (___res >> 32) + ___t;\t\t\t\t\t\\\n\t___res += (___m >> 32) * (___x >> 32);\t\t\t\t\\\n\t___res /= ___p;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t/* Now sanitize and optimize what we've got. */\t\t\t\\\n\tif (~0ULL % (___b / (___b & -___b)) == 0) {\t\t\t\\\n\t\t/* special case, can be simplified to ... */\t\t\\\n\t\t___n /= (___b & -___b);\t\t\t\t\t\\\n\t\t___m = ~0ULL / (___b / (___b & -___b));\t\t\t\\\n\t\t___p = 1;\t\t\t\t\t\t\\\n\t\t___bias = 1;\t\t\t\t\t\t\\\n\t} else if (___res != ___x / ___b) {\t\t\t\t\\\n\t\t/*\t\t\t\t\t\t\t\\\n\t\t * We can't get away without a bias to compensate\t\\\n\t\t * for bit truncation errors.  To avoid it we'd need an\t\\\n\t\t * additional bit to represent m which would overflow\t\\\n\t\t * a 64-bit variable.\t\t\t\t\t\\\n\t\t *\t\t\t\t\t\t\t\\\n\t\t * Instead we do m = p / b and n / b = (n * m + m) / p.\t\\\n\t\t */\t\t\t\t\t\t\t\\\n\t\t___bias = 1;\t\t\t\t\t\t\\\n\t\t/* Compute m = (p << 64) / b */\t\t\t\t\\\n\t\t___m = (~0ULL / ___b) * ___p;\t\t\t\t\\\n\t\t___m += ((~0ULL % ___b + 1) * ___p) / ___b;\t\t\\\n\t} else {\t\t\t\t\t\t\t\\\n\t\t/*\t\t\t\t\t\t\t\\\n\t\t * Reduce m / p, and try to clear bit 31 of m when\t\\\n\t\t * possible, otherwise that'll need extra overflow\t\\\n\t\t * handling later.\t\t\t\t\t\\\n\t\t */\t\t\t\t\t\t\t\\\n\t\tuint32_t ___bits = -(___m & -___m);\t\t\t\\\n\t\t___bits |= ___m >> 32;\t\t\t\t\t\\\n\t\t___bits = (~___bits) << 1;\t\t\t\t\\\n\t\t/*\t\t\t\t\t\t\t\\\n\t\t * If ___bits == 0 then setting bit 31 is  unavoidable.\t\\\n\t\t * Simply apply the maximum possible reduction in that\t\\\n\t\t * case. Otherwise the MSB of ___bits indicates the\t\\\n\t\t * best reduction we should apply.\t\t\t\\\n\t\t */\t\t\t\t\t\t\t\\\n\t\tif (!___bits) {\t\t\t\t\t\t\\\n\t\t\t___p /= (___m & -___m);\t\t\t\t\\\n\t\t\t___m /= (___m & -___m);\t\t\t\t\\\n\t\t} else {\t\t\t\t\t\t\\\n\t\t\t___p >>= ilog2(___bits);\t\t\t\\\n\t\t\t___m >>= ilog2(___bits);\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\t/* No bias needed. */\t\t\t\t\t\\\n\t\t___bias = 0;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t/*\t\t\t\t\t\t\t\t\\\n\t * Now we have a combination of 2 conditions:\t\t\t\\\n\t *\t\t\t\t\t\t\t\t\\\n\t * 1) whether or not we need to apply a bias, and\t\t\\\n\t *\t\t\t\t\t\t\t\t\\\n\t * 2) whether or not there might be an overflow in the cross\t\\\n\t *    product determined by (___m & ((1 << 63) | (1 << 31))).\t\\\n\t *\t\t\t\t\t\t\t\t\\\n\t * Select the best way to do (m_bias + m * n) / (1 << 64).\t\\\n\t * From now on there will be actual runtime code generated.\t\\\n\t */\t\t\t\t\t\t\t\t\\\n\t___res = __arch_xprod_64(___m, ___n, ___bias);\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t___res /= ___p;\t\t\t\t\t\t\t\\\n})\n\n#ifndef __arch_xprod_64\n/*\n * Default C implementation for __arch_xprod_64()\n *\n * Prototype: uint64_t __arch_xprod_64(const uint64_t m, uint64_t n, bool bias)\n * Semantic:  retval = ((bias ? m : 0) + m * n) >> 64\n *\n * The product is a 128-bit value, scaled down to 64 bits.\n * Assuming constant propagation to optimize away unused conditional code.\n * Architectures may provide their own optimized assembly implementation.\n */\nstatic inline uint64_t __arch_xprod_64(const uint64_t m, uint64_t n, bool bias)\n{\n\tuint32_t m_lo = m;\n\tuint32_t m_hi = m >> 32;\n\tuint32_t n_lo = n;\n\tuint32_t n_hi = n >> 32;\n\tuint64_t res;\n\tuint32_t res_lo, res_hi, tmp;\n\n\tif (!bias) {\n\t\tres = ((uint64_t)m_lo * n_lo) >> 32;\n\t} else if (!(m & ((1ULL << 63) | (1ULL << 31)))) {\n\t\t/* there can't be any overflow here */\n\t\tres = (m + (uint64_t)m_lo * n_lo) >> 32;\n\t} else {\n\t\tres = m + (uint64_t)m_lo * n_lo;\n\t\tres_lo = res >> 32;\n\t\tres_hi = (res_lo < m_hi);\n\t\tres = res_lo | ((uint64_t)res_hi << 32);\n\t}\n\n\tif (!(m & ((1ULL << 63) | (1ULL << 31)))) {\n\t\t/* there can't be any overflow here */\n\t\tres += (uint64_t)m_lo * n_hi;\n\t\tres += (uint64_t)m_hi * n_lo;\n\t\tres >>= 32;\n\t} else {\n\t\tres += (uint64_t)m_lo * n_hi;\n\t\ttmp = res >> 32;\n\t\tres += (uint64_t)m_hi * n_lo;\n\t\tres_lo = res >> 32;\n\t\tres_hi = (res_lo < tmp);\n\t\tres = res_lo | ((uint64_t)res_hi << 32);\n\t}\n\n\tres += (uint64_t)m_hi * n_hi;\n\n\treturn res;\n}\n#endif\n\n#ifndef __div64_32\nextern uint32_t __div64_32(uint64_t *dividend, uint32_t divisor);\n#endif\n\n/* The unnecessary pointer compare is there\n * to check for type safety (n must be 64bit)\n */\n# define do_div(n,base) ({\t\t\t\t\\\n\tuint32_t __base = (base);\t\t\t\\\n\tuint32_t __rem;\t\t\t\t\t\\\n\t(void)(((typeof((n)) *)0) == ((uint64_t *)0));\t\\\n\tif (__builtin_constant_p(__base) &&\t\t\\\n\t    is_power_of_2(__base)) {\t\t\t\\\n\t\t__rem = (n) & (__base - 1);\t\t\\\n\t\t(n) >>= ilog2(__base);\t\t\t\\\n\t} else if (__div64_const32_is_OK &&\t\t\\\n\t\t   __builtin_constant_p(__base) &&\t\\\n\t\t   __base != 0) {\t\t\t\\\n\t\tuint32_t __res_lo, __n_lo = (n);\t\\\n\t\t(n) = __div64_const32(n, __base);\t\\\n\t\t/* the remainder can be computed with 32-bit regs */ \\\n\t\t__res_lo = (n);\t\t\t\t\\\n\t\t__rem = __n_lo - __res_lo * __base;\t\\\n\t} else if (likely(((n) >> 32) == 0)) {\t\t\\\n\t\t__rem = (uint32_t)(n) % __base;\t\t\\\n\t\t(n) = (uint32_t)(n) / __base;\t\t\\\n\t} else \t\t\t\t\t\t\\\n\t\t__rem = __div64_32(&(n), __base);\t\\\n\t__rem;\t\t\t\t\t\t\\\n })\n\n#else /* BITS_PER_LONG == ?? */\n\n# error do_div() does not yet support the C64\n\n#endif /* BITS_PER_LONG */\n\n#endif /* _ASM_GENERIC_DIV64_H */\n"}, "10": {"id": 10, "path": "/src/include/linux/spinlock.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_SPINLOCK_H\n#define __LINUX_SPINLOCK_H\n\n/*\n * include/linux/spinlock.h - generic spinlock/rwlock declarations\n *\n * here's the role of the various spinlock/rwlock related include files:\n *\n * on SMP builds:\n *\n *  asm/spinlock_types.h: contains the arch_spinlock_t/arch_rwlock_t and the\n *                        initializers\n *\n *  linux/spinlock_types.h:\n *                        defines the generic type and initializers\n *\n *  asm/spinlock.h:       contains the arch_spin_*()/etc. lowlevel\n *                        implementations, mostly inline assembly code\n *\n *   (also included on UP-debug builds:)\n *\n *  linux/spinlock_api_smp.h:\n *                        contains the prototypes for the _spin_*() APIs.\n *\n *  linux/spinlock.h:     builds the final spin_*() APIs.\n *\n * on UP builds:\n *\n *  linux/spinlock_type_up.h:\n *                        contains the generic, simplified UP spinlock type.\n *                        (which is an empty structure on non-debug builds)\n *\n *  linux/spinlock_types.h:\n *                        defines the generic type and initializers\n *\n *  linux/spinlock_up.h:\n *                        contains the arch_spin_*()/etc. version of UP\n *                        builds. (which are NOPs on non-debug, non-preempt\n *                        builds)\n *\n *   (included on UP-non-debug builds:)\n *\n *  linux/spinlock_api_up.h:\n *                        builds the _spin_*() APIs.\n *\n *  linux/spinlock.h:     builds the final spin_*() APIs.\n */\n\n#include <linux/typecheck.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n#include <linux/compiler.h>\n#include <linux/irqflags.h>\n#include <linux/thread_info.h>\n#include <linux/kernel.h>\n#include <linux/stringify.h>\n#include <linux/bottom_half.h>\n#include <linux/lockdep.h>\n#include <asm/barrier.h>\n#include <asm/mmiowb.h>\n\n\n/*\n * Must define these before including other files, inline functions need them\n */\n#define LOCK_SECTION_NAME \".text..lock.\"KBUILD_BASENAME\n\n#define LOCK_SECTION_START(extra)               \\\n        \".subsection 1\\n\\t\"                     \\\n        extra                                   \\\n        \".ifndef \" LOCK_SECTION_NAME \"\\n\\t\"     \\\n        LOCK_SECTION_NAME \":\\n\\t\"               \\\n        \".endif\\n\"\n\n#define LOCK_SECTION_END                        \\\n        \".previous\\n\\t\"\n\n#define __lockfunc __section(\".spinlock.text\")\n\n/*\n * Pull the arch_spinlock_t and arch_rwlock_t definitions:\n */\n#include <linux/spinlock_types.h>\n\n/*\n * Pull the arch_spin*() functions/declarations (UP-nondebug doesn't need them):\n */\n#ifdef CONFIG_SMP\n# include <asm/spinlock.h>\n#else\n# include <linux/spinlock_up.h>\n#endif\n\n#ifdef CONFIG_DEBUG_SPINLOCK\n  extern void __raw_spin_lock_init(raw_spinlock_t *lock, const char *name,\n\t\t\t\t   struct lock_class_key *key, short inner);\n\n# define raw_spin_lock_init(lock)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tstatic struct lock_class_key __key;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t__raw_spin_lock_init((lock), #lock, &__key, LD_WAIT_SPIN);\t\\\n} while (0)\n\n#else\n# define raw_spin_lock_init(lock)\t\t\t\t\\\n\tdo { *(lock) = __RAW_SPIN_LOCK_UNLOCKED(lock); } while (0)\n#endif\n\n#define raw_spin_is_locked(lock)\tarch_spin_is_locked(&(lock)->raw_lock)\n\n#ifdef arch_spin_is_contended\n#define raw_spin_is_contended(lock)\tarch_spin_is_contended(&(lock)->raw_lock)\n#else\n#define raw_spin_is_contended(lock)\t(((void)(lock), 0))\n#endif /*arch_spin_is_contended*/\n\n/*\n * smp_mb__after_spinlock() provides the equivalent of a full memory barrier\n * between program-order earlier lock acquisitions and program-order later\n * memory accesses.\n *\n * This guarantees that the following two properties hold:\n *\n *   1) Given the snippet:\n *\n *\t  { X = 0;  Y = 0; }\n *\n *\t  CPU0\t\t\t\tCPU1\n *\n *\t  WRITE_ONCE(X, 1);\t\tWRITE_ONCE(Y, 1);\n *\t  spin_lock(S);\t\t\tsmp_mb();\n *\t  smp_mb__after_spinlock();\tr1 = READ_ONCE(X);\n *\t  r0 = READ_ONCE(Y);\n *\t  spin_unlock(S);\n *\n *      it is forbidden that CPU0 does not observe CPU1's store to Y (r0 = 0)\n *      and CPU1 does not observe CPU0's store to X (r1 = 0); see the comments\n *      preceding the call to smp_mb__after_spinlock() in __schedule() and in\n *      try_to_wake_up().\n *\n *   2) Given the snippet:\n *\n *  { X = 0;  Y = 0; }\n *\n *  CPU0\t\tCPU1\t\t\t\tCPU2\n *\n *  spin_lock(S);\tspin_lock(S);\t\t\tr1 = READ_ONCE(Y);\n *  WRITE_ONCE(X, 1);\tsmp_mb__after_spinlock();\tsmp_rmb();\n *  spin_unlock(S);\tr0 = READ_ONCE(X);\t\tr2 = READ_ONCE(X);\n *\t\t\tWRITE_ONCE(Y, 1);\n *\t\t\tspin_unlock(S);\n *\n *      it is forbidden that CPU0's critical section executes before CPU1's\n *      critical section (r0 = 1), CPU2 observes CPU1's store to Y (r1 = 1)\n *      and CPU2 does not observe CPU0's store to X (r2 = 0); see the comments\n *      preceding the calls to smp_rmb() in try_to_wake_up() for similar\n *      snippets but \"projected\" onto two CPUs.\n *\n * Property (2) upgrades the lock to an RCsc lock.\n *\n * Since most load-store architectures implement ACQUIRE with an smp_mb() after\n * the LL/SC loop, they need no further barriers. Similarly all our TSO\n * architectures imply an smp_mb() for each atomic instruction and equally don't\n * need more.\n *\n * Architectures that can implement ACQUIRE better need to take care.\n */\n#ifndef smp_mb__after_spinlock\n#define smp_mb__after_spinlock()\tdo { } while (0)\n#endif\n\n#ifdef CONFIG_DEBUG_SPINLOCK\n extern void do_raw_spin_lock(raw_spinlock_t *lock) __acquires(lock);\n#define do_raw_spin_lock_flags(lock, flags) do_raw_spin_lock(lock)\n extern int do_raw_spin_trylock(raw_spinlock_t *lock);\n extern void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock);\n#else\nstatic inline void do_raw_spin_lock(raw_spinlock_t *lock) __acquires(lock)\n{\n\t__acquire(lock);\n\tarch_spin_lock(&lock->raw_lock);\n\tmmiowb_spin_lock();\n}\n\n#ifndef arch_spin_lock_flags\n#define arch_spin_lock_flags(lock, flags)\tarch_spin_lock(lock)\n#endif\n\nstatic inline void\ndo_raw_spin_lock_flags(raw_spinlock_t *lock, unsigned long *flags) __acquires(lock)\n{\n\t__acquire(lock);\n\tarch_spin_lock_flags(&lock->raw_lock, *flags);\n\tmmiowb_spin_lock();\n}\n\nstatic inline int do_raw_spin_trylock(raw_spinlock_t *lock)\n{\n\tint ret = arch_spin_trylock(&(lock)->raw_lock);\n\n\tif (ret)\n\t\tmmiowb_spin_lock();\n\n\treturn ret;\n}\n\nstatic inline void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock)\n{\n\tmmiowb_spin_unlock();\n\tarch_spin_unlock(&lock->raw_lock);\n\t__release(lock);\n}\n#endif\n\n/*\n * Define the various spin_lock methods.  Note we define these\n * regardless of whether CONFIG_SMP or CONFIG_PREEMPTION are set. The\n * various methods are defined as nops in the case they are not\n * required.\n */\n#define raw_spin_trylock(lock)\t__cond_lock(lock, _raw_spin_trylock(lock))\n\n#define raw_spin_lock(lock)\t_raw_spin_lock(lock)\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n# define raw_spin_lock_nested(lock, subclass) \\\n\t_raw_spin_lock_nested(lock, subclass)\n\n# define raw_spin_lock_nest_lock(lock, nest_lock)\t\t\t\\\n\t do {\t\t\t\t\t\t\t\t\\\n\t\t typecheck(struct lockdep_map *, &(nest_lock)->dep_map);\\\n\t\t _raw_spin_lock_nest_lock(lock, &(nest_lock)->dep_map);\t\\\n\t } while (0)\n#else\n/*\n * Always evaluate the 'subclass' argument to avoid that the compiler\n * warns about set-but-not-used variables when building with\n * CONFIG_DEBUG_LOCK_ALLOC=n and with W=1.\n */\n# define raw_spin_lock_nested(lock, subclass)\t\t\\\n\t_raw_spin_lock(((void)(subclass), (lock)))\n# define raw_spin_lock_nest_lock(lock, nest_lock)\t_raw_spin_lock(lock)\n#endif\n\n#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)\n\n#define raw_spin_lock_irqsave(lock, flags)\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\\\n\t\tflags = _raw_spin_lock_irqsave(lock);\t\\\n\t} while (0)\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n#define raw_spin_lock_irqsave_nested(lock, flags, subclass)\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\t\t\\\n\t\tflags = _raw_spin_lock_irqsave_nested(lock, subclass);\t\\\n\t} while (0)\n#else\n#define raw_spin_lock_irqsave_nested(lock, flags, subclass)\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\t\t\\\n\t\tflags = _raw_spin_lock_irqsave(lock);\t\t\t\\\n\t} while (0)\n#endif\n\n#else\n\n#define raw_spin_lock_irqsave(lock, flags)\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\\\n\t\t_raw_spin_lock_irqsave(lock, flags);\t\\\n\t} while (0)\n\n#define raw_spin_lock_irqsave_nested(lock, flags, subclass)\t\\\n\traw_spin_lock_irqsave(lock, flags)\n\n#endif\n\n#define raw_spin_lock_irq(lock)\t\t_raw_spin_lock_irq(lock)\n#define raw_spin_lock_bh(lock)\t\t_raw_spin_lock_bh(lock)\n#define raw_spin_unlock(lock)\t\t_raw_spin_unlock(lock)\n#define raw_spin_unlock_irq(lock)\t_raw_spin_unlock_irq(lock)\n\n#define raw_spin_unlock_irqrestore(lock, flags)\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\t\\\n\t\t_raw_spin_unlock_irqrestore(lock, flags);\t\\\n\t} while (0)\n#define raw_spin_unlock_bh(lock)\t_raw_spin_unlock_bh(lock)\n\n#define raw_spin_trylock_bh(lock) \\\n\t__cond_lock(lock, _raw_spin_trylock_bh(lock))\n\n#define raw_spin_trylock_irq(lock) \\\n({ \\\n\tlocal_irq_disable(); \\\n\traw_spin_trylock(lock) ? \\\n\t1 : ({ local_irq_enable(); 0;  }); \\\n})\n\n#define raw_spin_trylock_irqsave(lock, flags) \\\n({ \\\n\tlocal_irq_save(flags); \\\n\traw_spin_trylock(lock) ? \\\n\t1 : ({ local_irq_restore(flags); 0; }); \\\n})\n\n/* Include rwlock functions */\n#include <linux/rwlock.h>\n\n/*\n * Pull the _spin_*()/_read_*()/_write_*() functions/declarations:\n */\n#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)\n# include <linux/spinlock_api_smp.h>\n#else\n# include <linux/spinlock_api_up.h>\n#endif\n\n/*\n * Map the spin_lock functions to the raw variants for PREEMPT_RT=n\n */\n\nstatic __always_inline raw_spinlock_t *spinlock_check(spinlock_t *lock)\n{\n\treturn &lock->rlock;\n}\n\n#ifdef CONFIG_DEBUG_SPINLOCK\n\n# define spin_lock_init(lock)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tstatic struct lock_class_key __key;\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\t__raw_spin_lock_init(spinlock_check(lock),\t\t\\\n\t\t\t     #lock, &__key, LD_WAIT_CONFIG);\t\\\n} while (0)\n\n#else\n\n# define spin_lock_init(_lock)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tspinlock_check(_lock);\t\t\t\\\n\t*(_lock) = __SPIN_LOCK_UNLOCKED(_lock);\t\\\n} while (0)\n\n#endif\n\nstatic __always_inline void spin_lock(spinlock_t *lock)\n{\n\traw_spin_lock(&lock->rlock);\n}\n\nstatic __always_inline void spin_lock_bh(spinlock_t *lock)\n{\n\traw_spin_lock_bh(&lock->rlock);\n}\n\nstatic __always_inline int spin_trylock(spinlock_t *lock)\n{\n\treturn raw_spin_trylock(&lock->rlock);\n}\n\n#define spin_lock_nested(lock, subclass)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_nested(spinlock_check(lock), subclass);\t\\\n} while (0)\n\n#define spin_lock_nest_lock(lock, nest_lock)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_nest_lock(spinlock_check(lock), nest_lock);\t\\\n} while (0)\n\nstatic __always_inline void spin_lock_irq(spinlock_t *lock)\n{\n\traw_spin_lock_irq(&lock->rlock);\n}\n\n#define spin_lock_irqsave(lock, flags)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_irqsave(spinlock_check(lock), flags);\t\\\n} while (0)\n\n#define spin_lock_irqsave_nested(lock, flags, subclass)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \\\n} while (0)\n\nstatic __always_inline void spin_unlock(spinlock_t *lock)\n{\n\traw_spin_unlock(&lock->rlock);\n}\n\nstatic __always_inline void spin_unlock_bh(spinlock_t *lock)\n{\n\traw_spin_unlock_bh(&lock->rlock);\n}\n\nstatic __always_inline void spin_unlock_irq(spinlock_t *lock)\n{\n\traw_spin_unlock_irq(&lock->rlock);\n}\n\nstatic __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)\n{\n\traw_spin_unlock_irqrestore(&lock->rlock, flags);\n}\n\nstatic __always_inline int spin_trylock_bh(spinlock_t *lock)\n{\n\treturn raw_spin_trylock_bh(&lock->rlock);\n}\n\nstatic __always_inline int spin_trylock_irq(spinlock_t *lock)\n{\n\treturn raw_spin_trylock_irq(&lock->rlock);\n}\n\n#define spin_trylock_irqsave(lock, flags)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\traw_spin_trylock_irqsave(spinlock_check(lock), flags); \\\n})\n\n/**\n * spin_is_locked() - Check whether a spinlock is locked.\n * @lock: Pointer to the spinlock.\n *\n * This function is NOT required to provide any memory ordering\n * guarantees; it could be used for debugging purposes or, when\n * additional synchronization is needed, accompanied with other\n * constructs (memory barriers) enforcing the synchronization.\n *\n * Returns: 1 if @lock is locked, 0 otherwise.\n *\n * Note that the function only tells you that the spinlock is\n * seen to be locked, not that it is locked on your CPU.\n *\n * Further, on CONFIG_SMP=n builds with CONFIG_DEBUG_SPINLOCK=n,\n * the return value is always 0 (see include/linux/spinlock_up.h).\n * Therefore you should not rely heavily on the return value.\n */\nstatic __always_inline int spin_is_locked(spinlock_t *lock)\n{\n\treturn raw_spin_is_locked(&lock->rlock);\n}\n\nstatic __always_inline int spin_is_contended(spinlock_t *lock)\n{\n\treturn raw_spin_is_contended(&lock->rlock);\n}\n\n#define assert_spin_locked(lock)\tassert_raw_spin_locked(&(lock)->rlock)\n\n/*\n * Pull the atomic_t declaration:\n * (asm-mips/atomic.h needs above definitions)\n */\n#include <linux/atomic.h>\n/**\n * atomic_dec_and_lock - lock on reaching reference count zero\n * @atomic: the atomic counter\n * @lock: the spinlock in question\n *\n * Decrements @atomic by 1.  If the result is 0, returns true and locks\n * @lock.  Returns false for all other cases.\n */\nextern int _atomic_dec_and_lock(atomic_t *atomic, spinlock_t *lock);\n#define atomic_dec_and_lock(atomic, lock) \\\n\t\t__cond_lock(lock, _atomic_dec_and_lock(atomic, lock))\n\nextern int _atomic_dec_and_lock_irqsave(atomic_t *atomic, spinlock_t *lock,\n\t\t\t\t\tunsigned long *flags);\n#define atomic_dec_and_lock_irqsave(atomic, lock, flags) \\\n\t\t__cond_lock(lock, _atomic_dec_and_lock_irqsave(atomic, lock, &(flags)))\n\nint __alloc_bucket_spinlocks(spinlock_t **locks, unsigned int *lock_mask,\n\t\t\t     size_t max_size, unsigned int cpu_mult,\n\t\t\t     gfp_t gfp, const char *name,\n\t\t\t     struct lock_class_key *key);\n\n#define alloc_bucket_spinlocks(locks, lock_mask, max_size, cpu_mult, gfp)    \\\n\t({\t\t\t\t\t\t\t\t     \\\n\t\tstatic struct lock_class_key key;\t\t\t     \\\n\t\tint ret;\t\t\t\t\t\t     \\\n\t\t\t\t\t\t\t\t\t     \\\n\t\tret = __alloc_bucket_spinlocks(locks, lock_mask, max_size,   \\\n\t\t\t\t\t       cpu_mult, gfp, #locks, &key); \\\n\t\tret;\t\t\t\t\t\t\t     \\\n\t})\n\nvoid free_bucket_spinlocks(spinlock_t *locks);\n\n#endif /* __LINUX_SPINLOCK_H */\n"}}, "reports": [{"events": [{"location": {"col": 7, "file": 1, "line": 716}, "message": "expanded from macro 'list_for_each_entry_safe'"}, {"location": {"col": 2, "file": 1, "line": 555}, "message": "expanded from macro 'list_next_entry'"}, {"location": {"col": 2, "file": 1, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 2, "file": 2, "line": 708}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 2, "file": 0, "line": 7803}, "message": "Calling 'drain_pages'"}, {"location": {"col": 2, "file": 0, "line": 3045}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 2, "file": 3, "line": 1080}, "message": "expanded from macro 'for_each_populated_zone'"}, {"location": {"col": 2, "file": 0, "line": 3045}, "message": "Assuming the condition is false"}, {"location": {"col": 7, "file": 3, "line": 1083}, "message": "expanded from macro 'for_each_populated_zone'"}, {"location": {"col": 2, "file": 0, "line": 3045}, "message": "Taking false branch"}, {"location": {"col": 3, "file": 3, "line": 1083}, "message": "expanded from macro 'for_each_populated_zone'"}, {"location": {"col": 3, "file": 0, "line": 3046}, "message": "Calling 'drain_pages_zone'"}, {"location": {"col": 2, "file": 0, "line": 3025}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 36, "file": 4, "line": 237}, "message": "expanded from macro 'local_irq_save'"}, {"location": {"col": 2, "file": 4, "line": 169}, "message": "expanded from macro 'raw_local_irq_save'"}, {"location": {"col": 2, "file": 0, "line": 3025}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 31, "file": 4, "line": 237}, "message": "expanded from macro 'local_irq_save'"}, {"location": {"col": 9, "file": 0, "line": 3026}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 5, "line": 235}, "message": "expanded from macro 'per_cpu_ptr'"}, {"location": {"col": 37, "file": 5, "line": 217}, "message": "expanded from macro '__verify_pcpu_ptr'"}, {"location": {"col": 6, "file": 0, "line": 3029}, "message": "Assuming field 'count' is not equal to 0"}, {"location": {"col": 2, "file": 0, "line": 3029}, "message": "Taking true branch"}, {"location": {"col": 3, "file": 0, "line": 3030}, "message": "Calling 'free_pcppages_bulk'"}, {"location": {"col": 20, "file": 0, "line": 1395}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 6, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 21, "file": 6, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 7, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 20, "file": 0, "line": 1395}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 6, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 21, "file": 6, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 7, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 20, "file": 0, "line": 1395}, "message": "Left side of '||' is true"}, {"location": {"col": 2, "file": 6, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 21, "file": 6, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 28, "file": 7, "line": 283}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 20, "file": 0, "line": 1395}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 6, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 2, "file": 6, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 7, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 7, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 7, "line": 300}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 20, "file": 0, "line": 1395}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 6, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 2, "file": 6, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 7, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 7, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 7, "line": 298}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 10, "file": 0, "line": 1404}, "message": "'__UNIQUE_ID___x493' is >= '__UNIQUE_ID___y494'"}, {"location": {"col": 19, "file": 8, "line": 51}, "message": "expanded from macro 'min'"}, {"location": {"col": 3, "file": 8, "line": 44}, "message": "expanded from macro '__careful_cmp'"}, {"location": {"col": 3, "file": 8, "line": 39}, "message": "expanded from macro '__cmp_once'"}, {"location": {"col": 26, "file": 8, "line": 34}, "message": "expanded from macro '__cmp'"}, {"location": {"col": 10, "file": 0, "line": 1404}, "message": "'?' condition is false"}, {"location": {"col": 19, "file": 8, "line": 51}, "message": "expanded from macro 'min'"}, {"location": {"col": 3, "file": 8, "line": 44}, "message": "expanded from macro '__careful_cmp'"}, {"location": {"col": 3, "file": 8, "line": 39}, "message": "expanded from macro '__cmp_once'"}, {"location": {"col": 26, "file": 8, "line": 34}, "message": "expanded from macro '__cmp'"}, {"location": {"col": 2, "file": 0, "line": 1405}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 4, "file": 0, "line": 1417}, "message": "Taking false branch"}, {"location": {"col": 3, "file": 0, "line": 1415}, "message": "Loop condition is true. Execution continues on line 1416"}, {"location": {"col": 4, "file": 0, "line": 1417}, "message": "Taking false branch"}, {"location": {"col": 3, "file": 0, "line": 1415}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 7, "file": 0, "line": 1423}, "message": "'batch_free' is not equal to MIGRATE_PCPTYPES"}, {"location": {"col": 3, "file": 0, "line": 1423}, "message": "Taking false branch"}, {"location": {"col": 11, "file": 0, "line": 1427}, "message": "Left side of '&&' is false"}, {"location": {"col": 2, "file": 1, "line": 533}, "message": "expanded from macro 'list_last_entry'"}, {"location": {"col": 2, "file": 1, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 61, "file": 2, "line": 709}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 11, "file": 0, "line": 1427}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 1, "line": 533}, "message": "expanded from macro 'list_last_entry'"}, {"location": {"col": 2, "file": 1, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 2, "file": 2, "line": 709}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 2, "file": 0, "line": 1460}, "message": "Assigned value is garbage or undefined"}], "macros": [], "notes": [], "path": "/src/mm/page_alloc.c", "reportHash": "484fa28aabef9680a8b51c21ecc8c517", "checkerName": "clang-analyzer-core.uninitialized.Assign", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 26, "file": 9, "line": 46}, "message": "expanded from macro 'do_div'"}, {"location": {"col": 6, "file": 0, "line": 8088}, "message": "Assuming 'rc' is 0"}, {"location": {"col": 2, "file": 0, "line": 8088}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 0, "line": 8091}, "message": "Assuming 'write' is not equal to 0"}, {"location": {"col": 2, "file": 0, "line": 8091}, "message": "Taking true branch"}, {"location": {"col": 3, "file": 0, "line": 8092}, "message": "Calling 'setup_per_zone_wmarks'"}, {"location": {"col": 2, "file": 0, "line": 8000}, "message": "Calling '__setup_per_zone_wmarks'"}, {"location": {"col": 2, "file": 0, "line": 7929}, "message": "'lowmem_pages' initialized to 0"}, {"location": {"col": 2, "file": 0, "line": 7934}, "message": "Loop condition is false. Execution continues on line 7939"}, {"location": {"col": 2, "file": 3, "line": 1075}, "message": "expanded from macro 'for_each_zone'"}, {"location": {"col": 2, "file": 0, "line": 7939}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 2, "file": 3, "line": 1075}, "message": "expanded from macro 'for_each_zone'"}, {"location": {"col": 3, "file": 0, "line": 7942}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 10, "line": 384}, "message": "expanded from macro 'spin_lock_irqsave'"}, {"location": {"col": 2, "file": 10, "line": 250}, "message": "expanded from macro 'raw_spin_lock_irqsave'"}, {"location": {"col": 3, "file": 0, "line": 7942}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 43, "file": 10, "line": 382}, "message": "expanded from macro 'spin_lock_irqsave'"}, {"location": {"col": 3, "file": 0, "line": 7944}, "message": "'__base' initialized to 0"}, {"location": {"col": 2, "file": 9, "line": 44}, "message": "expanded from macro 'do_div'"}, {"location": {"col": 3, "file": 0, "line": 7944}, "message": "Division by zero"}, {"location": {"col": 26, "file": 9, "line": 46}, "message": "expanded from macro 'do_div'"}, {"location": {"col": 3, "file": 0, "line": 7944}, "message": "Division by zero"}], "macros": [], "notes": [], "path": "/src/mm/page_alloc.c", "reportHash": "7e20e5d534e44e2d4b9414673bedb765", "checkerName": "clang-analyzer-core.DivideZero", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
