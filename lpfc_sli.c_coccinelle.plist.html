<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"0": {"id": 0, "path": "/src/drivers/scsi/lpfc/lpfc_sli.c", "content": "/*******************************************************************\n * This file is part of the Emulex Linux Device Driver for         *\n * Fibre Channel Host Bus Adapters.                                *\n * Copyright (C) 2017-2020 Broadcom. All Rights Reserved. The term *\n * \u201cBroadcom\u201d refers to Broadcom Inc. and/or its subsidiaries.  *\n * Copyright (C) 2004-2016 Emulex.  All rights reserved.           *\n * EMULEX and SLI are trademarks of Emulex.                        *\n * www.broadcom.com                                                *\n * Portions Copyright (C) 2004-2005 Christoph Hellwig              *\n *                                                                 *\n * This program is free software; you can redistribute it and/or   *\n * modify it under the terms of version 2 of the GNU General       *\n * Public License as published by the Free Software Foundation.    *\n * This program is distributed in the hope that it will be useful. *\n * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND          *\n * WARRANTIES, INCLUDING ANY IMPLIED WARRANTY OF MERCHANTABILITY,  *\n * FITNESS FOR A PARTICULAR PURPOSE, OR NON-INFRINGEMENT, ARE      *\n * DISCLAIMED, EXCEPT TO THE EXTENT THAT SUCH DISCLAIMERS ARE HELD *\n * TO BE LEGALLY INVALID.  See the GNU General Public License for  *\n * more details, a copy of which can be found in the file COPYING  *\n * included with this package.                                     *\n *******************************************************************/\n\n#include <linux/blkdev.h>\n#include <linux/pci.h>\n#include <linux/interrupt.h>\n#include <linux/delay.h>\n#include <linux/slab.h>\n#include <linux/lockdep.h>\n\n#include <scsi/scsi.h>\n#include <scsi/scsi_cmnd.h>\n#include <scsi/scsi_device.h>\n#include <scsi/scsi_host.h>\n#include <scsi/scsi_transport_fc.h>\n#include <scsi/fc/fc_fs.h>\n#include <linux/aer.h>\n#include <linux/crash_dump.h>\n#ifdef CONFIG_X86\n#include <asm/set_memory.h>\n#endif\n\n#include \"lpfc_hw4.h\"\n#include \"lpfc_hw.h\"\n#include \"lpfc_sli.h\"\n#include \"lpfc_sli4.h\"\n#include \"lpfc_nl.h\"\n#include \"lpfc_disc.h\"\n#include \"lpfc.h\"\n#include \"lpfc_scsi.h\"\n#include \"lpfc_nvme.h\"\n#include \"lpfc_crtn.h\"\n#include \"lpfc_logmsg.h\"\n#include \"lpfc_compat.h\"\n#include \"lpfc_debugfs.h\"\n#include \"lpfc_vport.h\"\n#include \"lpfc_version.h\"\n\n/* There are only four IOCB completion types. */\ntypedef enum _lpfc_iocb_type {\n\tLPFC_UNKNOWN_IOCB,\n\tLPFC_UNSOL_IOCB,\n\tLPFC_SOL_IOCB,\n\tLPFC_ABORT_IOCB\n} lpfc_iocb_type;\n\n\n/* Provide function prototypes local to this module. */\nstatic int lpfc_sli_issue_mbox_s4(struct lpfc_hba *, LPFC_MBOXQ_t *,\n\t\t\t\t  uint32_t);\nstatic int lpfc_sli4_read_rev(struct lpfc_hba *, LPFC_MBOXQ_t *,\n\t\t\t      uint8_t *, uint32_t *);\nstatic struct lpfc_iocbq *lpfc_sli4_els_wcqe_to_rspiocbq(struct lpfc_hba *,\n\t\t\t\t\t\t\t struct lpfc_iocbq *);\nstatic void lpfc_sli4_send_seq_to_ulp(struct lpfc_vport *,\n\t\t\t\t      struct hbq_dmabuf *);\nstatic void lpfc_sli4_handle_mds_loopback(struct lpfc_vport *vport,\n\t\t\t\t\t  struct hbq_dmabuf *dmabuf);\nstatic bool lpfc_sli4_fp_handle_cqe(struct lpfc_hba *phba,\n\t\t\t\t   struct lpfc_queue *cq, struct lpfc_cqe *cqe);\nstatic int lpfc_sli4_post_sgl_list(struct lpfc_hba *, struct list_head *,\n\t\t\t\t       int);\nstatic void lpfc_sli4_hba_handle_eqe(struct lpfc_hba *phba,\n\t\t\t\t     struct lpfc_queue *eq,\n\t\t\t\t     struct lpfc_eqe *eqe);\nstatic bool lpfc_sli4_mbox_completions_pending(struct lpfc_hba *phba);\nstatic bool lpfc_sli4_process_missed_mbox_completions(struct lpfc_hba *phba);\nstatic struct lpfc_cqe *lpfc_sli4_cq_get(struct lpfc_queue *q);\nstatic void __lpfc_sli4_consume_cqe(struct lpfc_hba *phba,\n\t\t\t\t    struct lpfc_queue *cq,\n\t\t\t\t    struct lpfc_cqe *cqe);\n\nstatic IOCB_t *\nlpfc_get_iocb_from_iocbq(struct lpfc_iocbq *iocbq)\n{\n\treturn &iocbq->iocb;\n}\n\n#if defined(CONFIG_64BIT) && defined(__LITTLE_ENDIAN)\n/**\n * lpfc_sli4_pcimem_bcopy - SLI4 memory copy function\n * @srcp: Source memory pointer.\n * @destp: Destination memory pointer.\n * @cnt: Number of words required to be copied.\n *       Must be a multiple of sizeof(uint64_t)\n *\n * This function is used for copying data between driver memory\n * and the SLI WQ. This function also changes the endianness\n * of each word if native endianness is different from SLI\n * endianness. This function can be called with or without\n * lock.\n **/\nstatic void\nlpfc_sli4_pcimem_bcopy(void *srcp, void *destp, uint32_t cnt)\n{\n\tuint64_t *src = srcp;\n\tuint64_t *dest = destp;\n\tint i;\n\n\tfor (i = 0; i < (int)cnt; i += sizeof(uint64_t))\n\t\t*dest++ = *src++;\n}\n#else\n#define lpfc_sli4_pcimem_bcopy(a, b, c) lpfc_sli_pcimem_bcopy(a, b, c)\n#endif\n\n/**\n * lpfc_sli4_wq_put - Put a Work Queue Entry on an Work Queue\n * @q: The Work Queue to operate on.\n * @wqe: The work Queue Entry to put on the Work queue.\n *\n * This routine will copy the contents of @wqe to the next available entry on\n * the @q. This function will then ring the Work Queue Doorbell to signal the\n * HBA to start processing the Work Queue Entry. This function returns 0 if\n * successful. If no entries are available on @q then this function will return\n * -ENOMEM.\n * The caller is expected to hold the hbalock when calling this routine.\n **/\nstatic int\nlpfc_sli4_wq_put(struct lpfc_queue *q, union lpfc_wqe128 *wqe)\n{\n\tunion lpfc_wqe *temp_wqe;\n\tstruct lpfc_register doorbell;\n\tuint32_t host_index;\n\tuint32_t idx;\n\tuint32_t i = 0;\n\tuint8_t *tmp;\n\tu32 if_type;\n\n\t/* sanity check on queue memory */\n\tif (unlikely(!q))\n\t\treturn -ENOMEM;\n\ttemp_wqe = lpfc_sli4_qe(q, q->host_index);\n\n\t/* If the host has not yet processed the next entry then we are done */\n\tidx = ((q->host_index + 1) % q->entry_count);\n\tif (idx == q->hba_index) {\n\t\tq->WQ_overflow++;\n\t\treturn -EBUSY;\n\t}\n\tq->WQ_posted++;\n\t/* set consumption flag every once in a while */\n\tif (!((q->host_index + 1) % q->notify_interval))\n\t\tbf_set(wqe_wqec, &wqe->generic.wqe_com, 1);\n\telse\n\t\tbf_set(wqe_wqec, &wqe->generic.wqe_com, 0);\n\tif (q->phba->sli3_options & LPFC_SLI4_PHWQ_ENABLED)\n\t\tbf_set(wqe_wqid, &wqe->generic.wqe_com, q->queue_id);\n\tlpfc_sli4_pcimem_bcopy(wqe, temp_wqe, q->entry_size);\n\tif (q->dpp_enable && q->phba->cfg_enable_dpp) {\n\t\t/* write to DPP aperture taking advatage of Combined Writes */\n\t\ttmp = (uint8_t *)temp_wqe;\n#ifdef __raw_writeq\n\t\tfor (i = 0; i < q->entry_size; i += sizeof(uint64_t))\n\t\t\t__raw_writeq(*((uint64_t *)(tmp + i)),\n\t\t\t\t\tq->dpp_regaddr + i);\n#else\n\t\tfor (i = 0; i < q->entry_size; i += sizeof(uint32_t))\n\t\t\t__raw_writel(*((uint32_t *)(tmp + i)),\n\t\t\t\t\tq->dpp_regaddr + i);\n#endif\n\t}\n\t/* ensure WQE bcopy and DPP flushed before doorbell write */\n\twmb();\n\n\t/* Update the host index before invoking device */\n\thost_index = q->host_index;\n\n\tq->host_index = idx;\n\n\t/* Ring Doorbell */\n\tdoorbell.word0 = 0;\n\tif (q->db_format == LPFC_DB_LIST_FORMAT) {\n\t\tif (q->dpp_enable && q->phba->cfg_enable_dpp) {\n\t\t\tbf_set(lpfc_if6_wq_db_list_fm_num_posted, &doorbell, 1);\n\t\t\tbf_set(lpfc_if6_wq_db_list_fm_dpp, &doorbell, 1);\n\t\t\tbf_set(lpfc_if6_wq_db_list_fm_dpp_id, &doorbell,\n\t\t\t    q->dpp_id);\n\t\t\tbf_set(lpfc_if6_wq_db_list_fm_id, &doorbell,\n\t\t\t    q->queue_id);\n\t\t} else {\n\t\t\tbf_set(lpfc_wq_db_list_fm_num_posted, &doorbell, 1);\n\t\t\tbf_set(lpfc_wq_db_list_fm_id, &doorbell, q->queue_id);\n\n\t\t\t/* Leave bits <23:16> clear for if_type 6 dpp */\n\t\t\tif_type = bf_get(lpfc_sli_intf_if_type,\n\t\t\t\t\t &q->phba->sli4_hba.sli_intf);\n\t\t\tif (if_type != LPFC_SLI_INTF_IF_TYPE_6)\n\t\t\t\tbf_set(lpfc_wq_db_list_fm_index, &doorbell,\n\t\t\t\t       host_index);\n\t\t}\n\t} else if (q->db_format == LPFC_DB_RING_FORMAT) {\n\t\tbf_set(lpfc_wq_db_ring_fm_num_posted, &doorbell, 1);\n\t\tbf_set(lpfc_wq_db_ring_fm_id, &doorbell, q->queue_id);\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\twritel(doorbell.word0, q->db_regaddr);\n\n\treturn 0;\n}\n\n/**\n * lpfc_sli4_wq_release - Updates internal hba index for WQ\n * @q: The Work Queue to operate on.\n * @index: The index to advance the hba index to.\n *\n * This routine will update the HBA index of a queue to reflect consumption of\n * Work Queue Entries by the HBA. When the HBA indicates that it has consumed\n * an entry the host calls this function to update the queue's internal\n * pointers.\n **/\nstatic void\nlpfc_sli4_wq_release(struct lpfc_queue *q, uint32_t index)\n{\n\t/* sanity check on queue memory */\n\tif (unlikely(!q))\n\t\treturn;\n\n\tq->hba_index = index;\n}\n\n/**\n * lpfc_sli4_mq_put - Put a Mailbox Queue Entry on an Mailbox Queue\n * @q: The Mailbox Queue to operate on.\n * @mqe: The Mailbox Queue Entry to put on the Work queue.\n *\n * This routine will copy the contents of @mqe to the next available entry on\n * the @q. This function will then ring the Work Queue Doorbell to signal the\n * HBA to start processing the Work Queue Entry. This function returns 0 if\n * successful. If no entries are available on @q then this function will return\n * -ENOMEM.\n * The caller is expected to hold the hbalock when calling this routine.\n **/\nstatic uint32_t\nlpfc_sli4_mq_put(struct lpfc_queue *q, struct lpfc_mqe *mqe)\n{\n\tstruct lpfc_mqe *temp_mqe;\n\tstruct lpfc_register doorbell;\n\n\t/* sanity check on queue memory */\n\tif (unlikely(!q))\n\t\treturn -ENOMEM;\n\ttemp_mqe = lpfc_sli4_qe(q, q->host_index);\n\n\t/* If the host has not yet processed the next entry then we are done */\n\tif (((q->host_index + 1) % q->entry_count) == q->hba_index)\n\t\treturn -ENOMEM;\n\tlpfc_sli4_pcimem_bcopy(mqe, temp_mqe, q->entry_size);\n\t/* Save off the mailbox pointer for completion */\n\tq->phba->mbox = (MAILBOX_t *)temp_mqe;\n\n\t/* Update the host index before invoking device */\n\tq->host_index = ((q->host_index + 1) % q->entry_count);\n\n\t/* Ring Doorbell */\n\tdoorbell.word0 = 0;\n\tbf_set(lpfc_mq_doorbell_num_posted, &doorbell, 1);\n\tbf_set(lpfc_mq_doorbell_id, &doorbell, q->queue_id);\n\twritel(doorbell.word0, q->phba->sli4_hba.MQDBregaddr);\n\treturn 0;\n}\n\n/**\n * lpfc_sli4_mq_release - Updates internal hba index for MQ\n * @q: The Mailbox Queue to operate on.\n *\n * This routine will update the HBA index of a queue to reflect consumption of\n * a Mailbox Queue Entry by the HBA. When the HBA indicates that it has consumed\n * an entry the host calls this function to update the queue's internal\n * pointers. This routine returns the number of entries that were consumed by\n * the HBA.\n **/\nstatic uint32_t\nlpfc_sli4_mq_release(struct lpfc_queue *q)\n{\n\t/* sanity check on queue memory */\n\tif (unlikely(!q))\n\t\treturn 0;\n\n\t/* Clear the mailbox pointer for completion */\n\tq->phba->mbox = NULL;\n\tq->hba_index = ((q->hba_index + 1) % q->entry_count);\n\treturn 1;\n}\n\n/**\n * lpfc_sli4_eq_get - Gets the next valid EQE from a EQ\n * @q: The Event Queue to get the first valid EQE from\n *\n * This routine will get the first valid Event Queue Entry from @q, update\n * the queue's internal hba index, and return the EQE. If no valid EQEs are in\n * the Queue (no more work to do), or the Queue is full of EQEs that have been\n * processed, but not popped back to the HBA then this routine will return NULL.\n **/\nstatic struct lpfc_eqe *\nlpfc_sli4_eq_get(struct lpfc_queue *q)\n{\n\tstruct lpfc_eqe *eqe;\n\n\t/* sanity check on queue memory */\n\tif (unlikely(!q))\n\t\treturn NULL;\n\teqe = lpfc_sli4_qe(q, q->host_index);\n\n\t/* If the next EQE is not valid then we are done */\n\tif (bf_get_le32(lpfc_eqe_valid, eqe) != q->qe_valid)\n\t\treturn NULL;\n\n\t/*\n\t * insert barrier for instruction interlock : data from the hardware\n\t * must have the valid bit checked before it can be copied and acted\n\t * upon. Speculative instructions were allowing a bcopy at the start\n\t * of lpfc_sli4_fp_handle_wcqe(), which is called immediately\n\t * after our return, to copy data before the valid bit check above\n\t * was done. As such, some of the copied data was stale. The barrier\n\t * ensures the check is before any data is copied.\n\t */\n\tmb();\n\treturn eqe;\n}\n\n/**\n * lpfc_sli4_eq_clr_intr - Turn off interrupts from this EQ\n * @q: The Event Queue to disable interrupts\n *\n **/\nvoid\nlpfc_sli4_eq_clr_intr(struct lpfc_queue *q)\n{\n\tstruct lpfc_register doorbell;\n\n\tdoorbell.word0 = 0;\n\tbf_set(lpfc_eqcq_doorbell_eqci, &doorbell, 1);\n\tbf_set(lpfc_eqcq_doorbell_qt, &doorbell, LPFC_QUEUE_TYPE_EVENT);\n\tbf_set(lpfc_eqcq_doorbell_eqid_hi, &doorbell,\n\t\t(q->queue_id >> LPFC_EQID_HI_FIELD_SHIFT));\n\tbf_set(lpfc_eqcq_doorbell_eqid_lo, &doorbell, q->queue_id);\n\twritel(doorbell.word0, q->phba->sli4_hba.EQDBregaddr);\n}\n\n/**\n * lpfc_sli4_if6_eq_clr_intr - Turn off interrupts from this EQ\n * @q: The Event Queue to disable interrupts\n *\n **/\nvoid\nlpfc_sli4_if6_eq_clr_intr(struct lpfc_queue *q)\n{\n\tstruct lpfc_register doorbell;\n\n\tdoorbell.word0 = 0;\n\tbf_set(lpfc_if6_eq_doorbell_eqid, &doorbell, q->queue_id);\n\twritel(doorbell.word0, q->phba->sli4_hba.EQDBregaddr);\n}\n\n/**\n * lpfc_sli4_write_eq_db - write EQ DB for eqe's consumed or arm state\n * @phba: adapter with EQ\n * @q: The Event Queue that the host has completed processing for.\n * @count: Number of elements that have been consumed\n * @arm: Indicates whether the host wants to arms this CQ.\n *\n * This routine will notify the HBA, by ringing the doorbell, that count\n * number of EQEs have been processed. The @arm parameter indicates whether\n * the queue should be rearmed when ringing the doorbell.\n **/\nvoid\nlpfc_sli4_write_eq_db(struct lpfc_hba *phba, struct lpfc_queue *q,\n\t\t     uint32_t count, bool arm)\n{\n\tstruct lpfc_register doorbell;\n\n\t/* sanity check on queue memory */\n\tif (unlikely(!q || (count == 0 && !arm)))\n\t\treturn;\n\n\t/* ring doorbell for number popped */\n\tdoorbell.word0 = 0;\n\tif (arm) {\n\t\tbf_set(lpfc_eqcq_doorbell_arm, &doorbell, 1);\n\t\tbf_set(lpfc_eqcq_doorbell_eqci, &doorbell, 1);\n\t}\n\tbf_set(lpfc_eqcq_doorbell_num_released, &doorbell, count);\n\tbf_set(lpfc_eqcq_doorbell_qt, &doorbell, LPFC_QUEUE_TYPE_EVENT);\n\tbf_set(lpfc_eqcq_doorbell_eqid_hi, &doorbell,\n\t\t\t(q->queue_id >> LPFC_EQID_HI_FIELD_SHIFT));\n\tbf_set(lpfc_eqcq_doorbell_eqid_lo, &doorbell, q->queue_id);\n\twritel(doorbell.word0, q->phba->sli4_hba.EQDBregaddr);\n\t/* PCI read to flush PCI pipeline on re-arming for INTx mode */\n\tif ((q->phba->intr_type == INTx) && (arm == LPFC_QUEUE_REARM))\n\t\treadl(q->phba->sli4_hba.EQDBregaddr);\n}\n\n/**\n * lpfc_sli4_if6_write_eq_db - write EQ DB for eqe's consumed or arm state\n * @phba: adapter with EQ\n * @q: The Event Queue that the host has completed processing for.\n * @count: Number of elements that have been consumed\n * @arm: Indicates whether the host wants to arms this CQ.\n *\n * This routine will notify the HBA, by ringing the doorbell, that count\n * number of EQEs have been processed. The @arm parameter indicates whether\n * the queue should be rearmed when ringing the doorbell.\n **/\nvoid\nlpfc_sli4_if6_write_eq_db(struct lpfc_hba *phba, struct lpfc_queue *q,\n\t\t\t  uint32_t count, bool arm)\n{\n\tstruct lpfc_register doorbell;\n\n\t/* sanity check on queue memory */\n\tif (unlikely(!q || (count == 0 && !arm)))\n\t\treturn;\n\n\t/* ring doorbell for number popped */\n\tdoorbell.word0 = 0;\n\tif (arm)\n\t\tbf_set(lpfc_if6_eq_doorbell_arm, &doorbell, 1);\n\tbf_set(lpfc_if6_eq_doorbell_num_released, &doorbell, count);\n\tbf_set(lpfc_if6_eq_doorbell_eqid, &doorbell, q->queue_id);\n\twritel(doorbell.word0, q->phba->sli4_hba.EQDBregaddr);\n\t/* PCI read to flush PCI pipeline on re-arming for INTx mode */\n\tif ((q->phba->intr_type == INTx) && (arm == LPFC_QUEUE_REARM))\n\t\treadl(q->phba->sli4_hba.EQDBregaddr);\n}\n\nstatic void\n__lpfc_sli4_consume_eqe(struct lpfc_hba *phba, struct lpfc_queue *eq,\n\t\t\tstruct lpfc_eqe *eqe)\n{\n\tif (!phba->sli4_hba.pc_sli4_params.eqav)\n\t\tbf_set_le32(lpfc_eqe_valid, eqe, 0);\n\n\teq->host_index = ((eq->host_index + 1) % eq->entry_count);\n\n\t/* if the index wrapped around, toggle the valid bit */\n\tif (phba->sli4_hba.pc_sli4_params.eqav && !eq->host_index)\n\t\teq->qe_valid = (eq->qe_valid) ? 0 : 1;\n}\n\nstatic void\nlpfc_sli4_eqcq_flush(struct lpfc_hba *phba, struct lpfc_queue *eq)\n{\n\tstruct lpfc_eqe *eqe = NULL;\n\tu32 eq_count = 0, cq_count = 0;\n\tstruct lpfc_cqe *cqe = NULL;\n\tstruct lpfc_queue *cq = NULL, *childq = NULL;\n\tint cqid = 0;\n\n\t/* walk all the EQ entries and drop on the floor */\n\teqe = lpfc_sli4_eq_get(eq);\n\twhile (eqe) {\n\t\t/* Get the reference to the corresponding CQ */\n\t\tcqid = bf_get_le32(lpfc_eqe_resource_id, eqe);\n\t\tcq = NULL;\n\n\t\tlist_for_each_entry(childq, &eq->child_list, list) {\n\t\t\tif (childq->queue_id == cqid) {\n\t\t\t\tcq = childq;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\t/* If CQ is valid, iterate through it and drop all the CQEs */\n\t\tif (cq) {\n\t\t\tcqe = lpfc_sli4_cq_get(cq);\n\t\t\twhile (cqe) {\n\t\t\t\t__lpfc_sli4_consume_cqe(phba, cq, cqe);\n\t\t\t\tcq_count++;\n\t\t\t\tcqe = lpfc_sli4_cq_get(cq);\n\t\t\t}\n\t\t\t/* Clear and re-arm the CQ */\n\t\t\tphba->sli4_hba.sli4_write_cq_db(phba, cq, cq_count,\n\t\t\t    LPFC_QUEUE_REARM);\n\t\t\tcq_count = 0;\n\t\t}\n\t\t__lpfc_sli4_consume_eqe(phba, eq, eqe);\n\t\teq_count++;\n\t\teqe = lpfc_sli4_eq_get(eq);\n\t}\n\n\t/* Clear and re-arm the EQ */\n\tphba->sli4_hba.sli4_write_eq_db(phba, eq, eq_count, LPFC_QUEUE_REARM);\n}\n\nstatic int\nlpfc_sli4_process_eq(struct lpfc_hba *phba, struct lpfc_queue *eq,\n\t\t     uint8_t rearm)\n{\n\tstruct lpfc_eqe *eqe;\n\tint count = 0, consumed = 0;\n\n\tif (cmpxchg(&eq->queue_claimed, 0, 1) != 0)\n\t\tgoto rearm_and_exit;\n\n\teqe = lpfc_sli4_eq_get(eq);\n\twhile (eqe) {\n\t\tlpfc_sli4_hba_handle_eqe(phba, eq, eqe);\n\t\t__lpfc_sli4_consume_eqe(phba, eq, eqe);\n\n\t\tconsumed++;\n\t\tif (!(++count % eq->max_proc_limit))\n\t\t\tbreak;\n\n\t\tif (!(count % eq->notify_interval)) {\n\t\t\tphba->sli4_hba.sli4_write_eq_db(phba, eq, consumed,\n\t\t\t\t\t\t\tLPFC_QUEUE_NOARM);\n\t\t\tconsumed = 0;\n\t\t}\n\n\t\teqe = lpfc_sli4_eq_get(eq);\n\t}\n\teq->EQ_processed += count;\n\n\t/* Track the max number of EQEs processed in 1 intr */\n\tif (count > eq->EQ_max_eqe)\n\t\teq->EQ_max_eqe = count;\n\n\txchg(&eq->queue_claimed, 0);\n\nrearm_and_exit:\n\t/* Always clear the EQ. */\n\tphba->sli4_hba.sli4_write_eq_db(phba, eq, consumed, rearm);\n\n\treturn count;\n}\n\n/**\n * lpfc_sli4_cq_get - Gets the next valid CQE from a CQ\n * @q: The Completion Queue to get the first valid CQE from\n *\n * This routine will get the first valid Completion Queue Entry from @q, update\n * the queue's internal hba index, and return the CQE. If no valid CQEs are in\n * the Queue (no more work to do), or the Queue is full of CQEs that have been\n * processed, but not popped back to the HBA then this routine will return NULL.\n **/\nstatic struct lpfc_cqe *\nlpfc_sli4_cq_get(struct lpfc_queue *q)\n{\n\tstruct lpfc_cqe *cqe;\n\n\t/* sanity check on queue memory */\n\tif (unlikely(!q))\n\t\treturn NULL;\n\tcqe = lpfc_sli4_qe(q, q->host_index);\n\n\t/* If the next CQE is not valid then we are done */\n\tif (bf_get_le32(lpfc_cqe_valid, cqe) != q->qe_valid)\n\t\treturn NULL;\n\n\t/*\n\t * insert barrier for instruction interlock : data from the hardware\n\t * must have the valid bit checked before it can be copied and acted\n\t * upon. Given what was seen in lpfc_sli4_cq_get() of speculative\n\t * instructions allowing action on content before valid bit checked,\n\t * add barrier here as well. May not be needed as \"content\" is a\n\t * single 32-bit entity here (vs multi word structure for cq's).\n\t */\n\tmb();\n\treturn cqe;\n}\n\nstatic void\n__lpfc_sli4_consume_cqe(struct lpfc_hba *phba, struct lpfc_queue *cq,\n\t\t\tstruct lpfc_cqe *cqe)\n{\n\tif (!phba->sli4_hba.pc_sli4_params.cqav)\n\t\tbf_set_le32(lpfc_cqe_valid, cqe, 0);\n\n\tcq->host_index = ((cq->host_index + 1) % cq->entry_count);\n\n\t/* if the index wrapped around, toggle the valid bit */\n\tif (phba->sli4_hba.pc_sli4_params.cqav && !cq->host_index)\n\t\tcq->qe_valid = (cq->qe_valid) ? 0 : 1;\n}\n\n/**\n * lpfc_sli4_write_cq_db - write cq DB for entries consumed or arm state.\n * @phba: the adapter with the CQ\n * @q: The Completion Queue that the host has completed processing for.\n * @count: the number of elements that were consumed\n * @arm: Indicates whether the host wants to arms this CQ.\n *\n * This routine will notify the HBA, by ringing the doorbell, that the\n * CQEs have been processed. The @arm parameter specifies whether the\n * queue should be rearmed when ringing the doorbell.\n **/\nvoid\nlpfc_sli4_write_cq_db(struct lpfc_hba *phba, struct lpfc_queue *q,\n\t\t     uint32_t count, bool arm)\n{\n\tstruct lpfc_register doorbell;\n\n\t/* sanity check on queue memory */\n\tif (unlikely(!q || (count == 0 && !arm)))\n\t\treturn;\n\n\t/* ring doorbell for number popped */\n\tdoorbell.word0 = 0;\n\tif (arm)\n\t\tbf_set(lpfc_eqcq_doorbell_arm, &doorbell, 1);\n\tbf_set(lpfc_eqcq_doorbell_num_released, &doorbell, count);\n\tbf_set(lpfc_eqcq_doorbell_qt, &doorbell, LPFC_QUEUE_TYPE_COMPLETION);\n\tbf_set(lpfc_eqcq_doorbell_cqid_hi, &doorbell,\n\t\t\t(q->queue_id >> LPFC_CQID_HI_FIELD_SHIFT));\n\tbf_set(lpfc_eqcq_doorbell_cqid_lo, &doorbell, q->queue_id);\n\twritel(doorbell.word0, q->phba->sli4_hba.CQDBregaddr);\n}\n\n/**\n * lpfc_sli4_if6_write_cq_db - write cq DB for entries consumed or arm state.\n * @phba: the adapter with the CQ\n * @q: The Completion Queue that the host has completed processing for.\n * @count: the number of elements that were consumed\n * @arm: Indicates whether the host wants to arms this CQ.\n *\n * This routine will notify the HBA, by ringing the doorbell, that the\n * CQEs have been processed. The @arm parameter specifies whether the\n * queue should be rearmed when ringing the doorbell.\n **/\nvoid\nlpfc_sli4_if6_write_cq_db(struct lpfc_hba *phba, struct lpfc_queue *q,\n\t\t\t uint32_t count, bool arm)\n{\n\tstruct lpfc_register doorbell;\n\n\t/* sanity check on queue memory */\n\tif (unlikely(!q || (count == 0 && !arm)))\n\t\treturn;\n\n\t/* ring doorbell for number popped */\n\tdoorbell.word0 = 0;\n\tif (arm)\n\t\tbf_set(lpfc_if6_cq_doorbell_arm, &doorbell, 1);\n\tbf_set(lpfc_if6_cq_doorbell_num_released, &doorbell, count);\n\tbf_set(lpfc_if6_cq_doorbell_cqid, &doorbell, q->queue_id);\n\twritel(doorbell.word0, q->phba->sli4_hba.CQDBregaddr);\n}\n\n/*\n * lpfc_sli4_rq_put - Put a Receive Buffer Queue Entry on a Receive Queue\n *\n * This routine will copy the contents of @wqe to the next available entry on\n * the @q. This function will then ring the Receive Queue Doorbell to signal the\n * HBA to start processing the Receive Queue Entry. This function returns the\n * index that the rqe was copied to if successful. If no entries are available\n * on @q then this function will return -ENOMEM.\n * The caller is expected to hold the hbalock when calling this routine.\n **/\nint\nlpfc_sli4_rq_put(struct lpfc_queue *hq, struct lpfc_queue *dq,\n\t\t struct lpfc_rqe *hrqe, struct lpfc_rqe *drqe)\n{\n\tstruct lpfc_rqe *temp_hrqe;\n\tstruct lpfc_rqe *temp_drqe;\n\tstruct lpfc_register doorbell;\n\tint hq_put_index;\n\tint dq_put_index;\n\n\t/* sanity check on queue memory */\n\tif (unlikely(!hq) || unlikely(!dq))\n\t\treturn -ENOMEM;\n\thq_put_index = hq->host_index;\n\tdq_put_index = dq->host_index;\n\ttemp_hrqe = lpfc_sli4_qe(hq, hq_put_index);\n\ttemp_drqe = lpfc_sli4_qe(dq, dq_put_index);\n\n\tif (hq->type != LPFC_HRQ || dq->type != LPFC_DRQ)\n\t\treturn -EINVAL;\n\tif (hq_put_index != dq_put_index)\n\t\treturn -EINVAL;\n\t/* If the host has not yet processed the next entry then we are done */\n\tif (((hq_put_index + 1) % hq->entry_count) == hq->hba_index)\n\t\treturn -EBUSY;\n\tlpfc_sli4_pcimem_bcopy(hrqe, temp_hrqe, hq->entry_size);\n\tlpfc_sli4_pcimem_bcopy(drqe, temp_drqe, dq->entry_size);\n\n\t/* Update the host index to point to the next slot */\n\thq->host_index = ((hq_put_index + 1) % hq->entry_count);\n\tdq->host_index = ((dq_put_index + 1) % dq->entry_count);\n\thq->RQ_buf_posted++;\n\n\t/* Ring The Header Receive Queue Doorbell */\n\tif (!(hq->host_index % hq->notify_interval)) {\n\t\tdoorbell.word0 = 0;\n\t\tif (hq->db_format == LPFC_DB_RING_FORMAT) {\n\t\t\tbf_set(lpfc_rq_db_ring_fm_num_posted, &doorbell,\n\t\t\t       hq->notify_interval);\n\t\t\tbf_set(lpfc_rq_db_ring_fm_id, &doorbell, hq->queue_id);\n\t\t} else if (hq->db_format == LPFC_DB_LIST_FORMAT) {\n\t\t\tbf_set(lpfc_rq_db_list_fm_num_posted, &doorbell,\n\t\t\t       hq->notify_interval);\n\t\t\tbf_set(lpfc_rq_db_list_fm_index, &doorbell,\n\t\t\t       hq->host_index);\n\t\t\tbf_set(lpfc_rq_db_list_fm_id, &doorbell, hq->queue_id);\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n\t\twritel(doorbell.word0, hq->db_regaddr);\n\t}\n\treturn hq_put_index;\n}\n\n/*\n * lpfc_sli4_rq_release - Updates internal hba index for RQ\n *\n * This routine will update the HBA index of a queue to reflect consumption of\n * one Receive Queue Entry by the HBA. When the HBA indicates that it has\n * consumed an entry the host calls this function to update the queue's\n * internal pointers. This routine returns the number of entries that were\n * consumed by the HBA.\n **/\nstatic uint32_t\nlpfc_sli4_rq_release(struct lpfc_queue *hq, struct lpfc_queue *dq)\n{\n\t/* sanity check on queue memory */\n\tif (unlikely(!hq) || unlikely(!dq))\n\t\treturn 0;\n\n\tif ((hq->type != LPFC_HRQ) || (dq->type != LPFC_DRQ))\n\t\treturn 0;\n\thq->hba_index = ((hq->hba_index + 1) % hq->entry_count);\n\tdq->hba_index = ((dq->hba_index + 1) % dq->entry_count);\n\treturn 1;\n}\n\n/**\n * lpfc_cmd_iocb - Get next command iocb entry in the ring\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n *\n * This function returns pointer to next command iocb entry\n * in the command ring. The caller must hold hbalock to prevent\n * other threads consume the next command iocb.\n * SLI-2/SLI-3 provide different sized iocbs.\n **/\nstatic inline IOCB_t *\nlpfc_cmd_iocb(struct lpfc_hba *phba, struct lpfc_sli_ring *pring)\n{\n\treturn (IOCB_t *) (((char *) pring->sli.sli3.cmdringaddr) +\n\t\t\t   pring->sli.sli3.cmdidx * phba->iocb_cmd_size);\n}\n\n/**\n * lpfc_resp_iocb - Get next response iocb entry in the ring\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n *\n * This function returns pointer to next response iocb entry\n * in the response ring. The caller must hold hbalock to make sure\n * that no other thread consume the next response iocb.\n * SLI-2/SLI-3 provide different sized iocbs.\n **/\nstatic inline IOCB_t *\nlpfc_resp_iocb(struct lpfc_hba *phba, struct lpfc_sli_ring *pring)\n{\n\treturn (IOCB_t *) (((char *) pring->sli.sli3.rspringaddr) +\n\t\t\t   pring->sli.sli3.rspidx * phba->iocb_rsp_size);\n}\n\n/**\n * __lpfc_sli_get_iocbq - Allocates an iocb object from iocb pool\n * @phba: Pointer to HBA context object.\n *\n * This function is called with hbalock held. This function\n * allocates a new driver iocb object from the iocb pool. If the\n * allocation is successful, it returns pointer to the newly\n * allocated iocb object else it returns NULL.\n **/\nstruct lpfc_iocbq *\n__lpfc_sli_get_iocbq(struct lpfc_hba *phba)\n{\n\tstruct list_head *lpfc_iocb_list = &phba->lpfc_iocb_list;\n\tstruct lpfc_iocbq * iocbq = NULL;\n\n\tlockdep_assert_held(&phba->hbalock);\n\n\tlist_remove_head(lpfc_iocb_list, iocbq, struct lpfc_iocbq, list);\n\tif (iocbq)\n\t\tphba->iocb_cnt++;\n\tif (phba->iocb_cnt > phba->iocb_max)\n\t\tphba->iocb_max = phba->iocb_cnt;\n\treturn iocbq;\n}\n\n/**\n * __lpfc_clear_active_sglq - Remove the active sglq for this XRI.\n * @phba: Pointer to HBA context object.\n * @xritag: XRI value.\n *\n * This function clears the sglq pointer from the array of acive\n * sglq's. The xritag that is passed in is used to index into the\n * array. Before the xritag can be used it needs to be adjusted\n * by subtracting the xribase.\n *\n * Returns sglq ponter = success, NULL = Failure.\n **/\nstruct lpfc_sglq *\n__lpfc_clear_active_sglq(struct lpfc_hba *phba, uint16_t xritag)\n{\n\tstruct lpfc_sglq *sglq;\n\n\tsglq = phba->sli4_hba.lpfc_sglq_active_list[xritag];\n\tphba->sli4_hba.lpfc_sglq_active_list[xritag] = NULL;\n\treturn sglq;\n}\n\n/**\n * __lpfc_get_active_sglq - Get the active sglq for this XRI.\n * @phba: Pointer to HBA context object.\n * @xritag: XRI value.\n *\n * This function returns the sglq pointer from the array of acive\n * sglq's. The xritag that is passed in is used to index into the\n * array. Before the xritag can be used it needs to be adjusted\n * by subtracting the xribase.\n *\n * Returns sglq ponter = success, NULL = Failure.\n **/\nstruct lpfc_sglq *\n__lpfc_get_active_sglq(struct lpfc_hba *phba, uint16_t xritag)\n{\n\tstruct lpfc_sglq *sglq;\n\n\tsglq =  phba->sli4_hba.lpfc_sglq_active_list[xritag];\n\treturn sglq;\n}\n\n/**\n * lpfc_clr_rrq_active - Clears RRQ active bit in xri_bitmap.\n * @phba: Pointer to HBA context object.\n * @xritag: xri used in this exchange.\n * @rrq: The RRQ to be cleared.\n *\n **/\nvoid\nlpfc_clr_rrq_active(struct lpfc_hba *phba,\n\t\t    uint16_t xritag,\n\t\t    struct lpfc_node_rrq *rrq)\n{\n\tstruct lpfc_nodelist *ndlp = NULL;\n\n\tif ((rrq->vport) && NLP_CHK_NODE_ACT(rrq->ndlp))\n\t\tndlp = lpfc_findnode_did(rrq->vport, rrq->nlp_DID);\n\n\t/* The target DID could have been swapped (cable swap)\n\t * we should use the ndlp from the findnode if it is\n\t * available.\n\t */\n\tif ((!ndlp) && rrq->ndlp)\n\t\tndlp = rrq->ndlp;\n\n\tif (!ndlp)\n\t\tgoto out;\n\n\tif (test_and_clear_bit(xritag, ndlp->active_rrqs_xri_bitmap)) {\n\t\trrq->send_rrq = 0;\n\t\trrq->xritag = 0;\n\t\trrq->rrq_stop_time = 0;\n\t}\nout:\n\tmempool_free(rrq, phba->rrq_pool);\n}\n\n/**\n * lpfc_handle_rrq_active - Checks if RRQ has waithed RATOV.\n * @phba: Pointer to HBA context object.\n *\n * This function is called with hbalock held. This function\n * Checks if stop_time (ratov from setting rrq active) has\n * been reached, if it has and the send_rrq flag is set then\n * it will call lpfc_send_rrq. If the send_rrq flag is not set\n * then it will just call the routine to clear the rrq and\n * free the rrq resource.\n * The timer is set to the next rrq that is going to expire before\n * leaving the routine.\n *\n **/\nvoid\nlpfc_handle_rrq_active(struct lpfc_hba *phba)\n{\n\tstruct lpfc_node_rrq *rrq;\n\tstruct lpfc_node_rrq *nextrrq;\n\tunsigned long next_time;\n\tunsigned long iflags;\n\tLIST_HEAD(send_rrq);\n\n\tspin_lock_irqsave(&phba->hbalock, iflags);\n\tphba->hba_flag &= ~HBA_RRQ_ACTIVE;\n\tnext_time = jiffies + msecs_to_jiffies(1000 * (phba->fc_ratov + 1));\n\tlist_for_each_entry_safe(rrq, nextrrq,\n\t\t\t\t &phba->active_rrq_list, list) {\n\t\tif (time_after(jiffies, rrq->rrq_stop_time))\n\t\t\tlist_move(&rrq->list, &send_rrq);\n\t\telse if (time_before(rrq->rrq_stop_time, next_time))\n\t\t\tnext_time = rrq->rrq_stop_time;\n\t}\n\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\tif ((!list_empty(&phba->active_rrq_list)) &&\n\t    (!(phba->pport->load_flag & FC_UNLOADING)))\n\t\tmod_timer(&phba->rrq_tmr, next_time);\n\tlist_for_each_entry_safe(rrq, nextrrq, &send_rrq, list) {\n\t\tlist_del(&rrq->list);\n\t\tif (!rrq->send_rrq) {\n\t\t\t/* this call will free the rrq */\n\t\t\tlpfc_clr_rrq_active(phba, rrq->xritag, rrq);\n\t\t} else if (lpfc_send_rrq(phba, rrq)) {\n\t\t\t/* if we send the rrq then the completion handler\n\t\t\t*  will clear the bit in the xribitmap.\n\t\t\t*/\n\t\t\tlpfc_clr_rrq_active(phba, rrq->xritag,\n\t\t\t\t\t    rrq);\n\t\t}\n\t}\n}\n\n/**\n * lpfc_get_active_rrq - Get the active RRQ for this exchange.\n * @vport: Pointer to vport context object.\n * @xri: The xri used in the exchange.\n * @did: The targets DID for this exchange.\n *\n * returns NULL = rrq not found in the phba->active_rrq_list.\n *         rrq = rrq for this xri and target.\n **/\nstruct lpfc_node_rrq *\nlpfc_get_active_rrq(struct lpfc_vport *vport, uint16_t xri, uint32_t did)\n{\n\tstruct lpfc_hba *phba = vport->phba;\n\tstruct lpfc_node_rrq *rrq;\n\tstruct lpfc_node_rrq *nextrrq;\n\tunsigned long iflags;\n\n\tif (phba->sli_rev != LPFC_SLI_REV4)\n\t\treturn NULL;\n\tspin_lock_irqsave(&phba->hbalock, iflags);\n\tlist_for_each_entry_safe(rrq, nextrrq, &phba->active_rrq_list, list) {\n\t\tif (rrq->vport == vport && rrq->xritag == xri &&\n\t\t\t\trrq->nlp_DID == did){\n\t\t\tlist_del(&rrq->list);\n\t\t\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t\t\treturn rrq;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\treturn NULL;\n}\n\n/**\n * lpfc_cleanup_vports_rrqs - Remove and clear the active RRQ for this vport.\n * @vport: Pointer to vport context object.\n * @ndlp: Pointer to the lpfc_node_list structure.\n * If ndlp is NULL Remove all active RRQs for this vport from the\n * phba->active_rrq_list and clear the rrq.\n * If ndlp is not NULL then only remove rrqs for this vport & this ndlp.\n **/\nvoid\nlpfc_cleanup_vports_rrqs(struct lpfc_vport *vport, struct lpfc_nodelist *ndlp)\n\n{\n\tstruct lpfc_hba *phba = vport->phba;\n\tstruct lpfc_node_rrq *rrq;\n\tstruct lpfc_node_rrq *nextrrq;\n\tunsigned long iflags;\n\tLIST_HEAD(rrq_list);\n\n\tif (phba->sli_rev != LPFC_SLI_REV4)\n\t\treturn;\n\tif (!ndlp) {\n\t\tlpfc_sli4_vport_delete_els_xri_aborted(vport);\n\t\tlpfc_sli4_vport_delete_fcp_xri_aborted(vport);\n\t}\n\tspin_lock_irqsave(&phba->hbalock, iflags);\n\tlist_for_each_entry_safe(rrq, nextrrq, &phba->active_rrq_list, list)\n\t\tif ((rrq->vport == vport) && (!ndlp  || rrq->ndlp == ndlp))\n\t\t\tlist_move(&rrq->list, &rrq_list);\n\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\n\tlist_for_each_entry_safe(rrq, nextrrq, &rrq_list, list) {\n\t\tlist_del(&rrq->list);\n\t\tlpfc_clr_rrq_active(phba, rrq->xritag, rrq);\n\t}\n}\n\n/**\n * lpfc_test_rrq_active - Test RRQ bit in xri_bitmap.\n * @phba: Pointer to HBA context object.\n * @ndlp: Targets nodelist pointer for this exchange.\n * @xritag: the xri in the bitmap to test.\n *\n * This function returns:\n * 0 = rrq not active for this xri\n * 1 = rrq is valid for this xri.\n **/\nint\nlpfc_test_rrq_active(struct lpfc_hba *phba, struct lpfc_nodelist *ndlp,\n\t\t\tuint16_t  xritag)\n{\n\tif (!ndlp)\n\t\treturn 0;\n\tif (!ndlp->active_rrqs_xri_bitmap)\n\t\treturn 0;\n\tif (test_bit(xritag, ndlp->active_rrqs_xri_bitmap))\n\t\treturn 1;\n\telse\n\t\treturn 0;\n}\n\n/**\n * lpfc_set_rrq_active - set RRQ active bit in xri_bitmap.\n * @phba: Pointer to HBA context object.\n * @ndlp: nodelist pointer for this target.\n * @xritag: xri used in this exchange.\n * @rxid: Remote Exchange ID.\n * @send_rrq: Flag used to determine if we should send rrq els cmd.\n *\n * This function takes the hbalock.\n * The active bit is always set in the active rrq xri_bitmap even\n * if there is no slot avaiable for the other rrq information.\n *\n * returns 0 rrq actived for this xri\n *         < 0 No memory or invalid ndlp.\n **/\nint\nlpfc_set_rrq_active(struct lpfc_hba *phba, struct lpfc_nodelist *ndlp,\n\t\t    uint16_t xritag, uint16_t rxid, uint16_t send_rrq)\n{\n\tunsigned long iflags;\n\tstruct lpfc_node_rrq *rrq;\n\tint empty;\n\n\tif (!ndlp)\n\t\treturn -EINVAL;\n\n\tif (!phba->cfg_enable_rrq)\n\t\treturn -EINVAL;\n\n\tspin_lock_irqsave(&phba->hbalock, iflags);\n\tif (phba->pport->load_flag & FC_UNLOADING) {\n\t\tphba->hba_flag &= ~HBA_RRQ_ACTIVE;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * set the active bit even if there is no mem available.\n\t */\n\tif (NLP_CHK_FREE_REQ(ndlp))\n\t\tgoto out;\n\n\tif (ndlp->vport && (ndlp->vport->load_flag & FC_UNLOADING))\n\t\tgoto out;\n\n\tif (!ndlp->active_rrqs_xri_bitmap)\n\t\tgoto out;\n\n\tif (test_and_set_bit(xritag, ndlp->active_rrqs_xri_bitmap))\n\t\tgoto out;\n\n\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\trrq = mempool_alloc(phba->rrq_pool, GFP_ATOMIC);\n\tif (!rrq) {\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\t\"3155 Unable to allocate RRQ xri:0x%x rxid:0x%x\"\n\t\t\t\t\" DID:0x%x Send:%d\\n\",\n\t\t\t\txritag, rxid, ndlp->nlp_DID, send_rrq);\n\t\treturn -EINVAL;\n\t}\n\tif (phba->cfg_enable_rrq == 1)\n\t\trrq->send_rrq = send_rrq;\n\telse\n\t\trrq->send_rrq = 0;\n\trrq->xritag = xritag;\n\trrq->rrq_stop_time = jiffies +\n\t\t\t\tmsecs_to_jiffies(1000 * (phba->fc_ratov + 1));\n\trrq->ndlp = ndlp;\n\trrq->nlp_DID = ndlp->nlp_DID;\n\trrq->vport = ndlp->vport;\n\trrq->rxid = rxid;\n\tspin_lock_irqsave(&phba->hbalock, iflags);\n\tempty = list_empty(&phba->active_rrq_list);\n\tlist_add_tail(&rrq->list, &phba->active_rrq_list);\n\tphba->hba_flag |= HBA_RRQ_ACTIVE;\n\tif (empty)\n\t\tlpfc_worker_wake_up(phba);\n\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\treturn 0;\nout:\n\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\"2921 Can't set rrq active xri:0x%x rxid:0x%x\"\n\t\t\t\" DID:0x%x Send:%d\\n\",\n\t\t\txritag, rxid, ndlp->nlp_DID, send_rrq);\n\treturn -EINVAL;\n}\n\n/**\n * __lpfc_sli_get_els_sglq - Allocates an iocb object from sgl pool\n * @phba: Pointer to HBA context object.\n * @piocbq: Pointer to the iocbq.\n *\n * The driver calls this function with either the nvme ls ring lock\n * or the fc els ring lock held depending on the iocb usage.  This function\n * gets a new driver sglq object from the sglq list. If the list is not empty\n * then it is successful, it returns pointer to the newly allocated sglq\n * object else it returns NULL.\n **/\nstatic struct lpfc_sglq *\n__lpfc_sli_get_els_sglq(struct lpfc_hba *phba, struct lpfc_iocbq *piocbq)\n{\n\tstruct list_head *lpfc_els_sgl_list = &phba->sli4_hba.lpfc_els_sgl_list;\n\tstruct lpfc_sglq *sglq = NULL;\n\tstruct lpfc_sglq *start_sglq = NULL;\n\tstruct lpfc_io_buf *lpfc_cmd;\n\tstruct lpfc_nodelist *ndlp;\n\tstruct lpfc_sli_ring *pring = NULL;\n\tint found = 0;\n\n\tif (piocbq->iocb_flag & LPFC_IO_NVME_LS)\n\t\tpring =  phba->sli4_hba.nvmels_wq->pring;\n\telse\n\t\tpring = lpfc_phba_elsring(phba);\n\n\tlockdep_assert_held(&pring->ring_lock);\n\n\tif (piocbq->iocb_flag &  LPFC_IO_FCP) {\n\t\tlpfc_cmd = (struct lpfc_io_buf *) piocbq->context1;\n\t\tndlp = lpfc_cmd->rdata->pnode;\n\t} else  if ((piocbq->iocb.ulpCommand == CMD_GEN_REQUEST64_CR) &&\n\t\t\t!(piocbq->iocb_flag & LPFC_IO_LIBDFC)) {\n\t\tndlp = piocbq->context_un.ndlp;\n\t} else  if (piocbq->iocb_flag & LPFC_IO_LIBDFC) {\n\t\tif (piocbq->iocb_flag & LPFC_IO_LOOPBACK)\n\t\t\tndlp = NULL;\n\t\telse\n\t\t\tndlp = piocbq->context_un.ndlp;\n\t} else {\n\t\tndlp = piocbq->context1;\n\t}\n\n\tspin_lock(&phba->sli4_hba.sgl_list_lock);\n\tlist_remove_head(lpfc_els_sgl_list, sglq, struct lpfc_sglq, list);\n\tstart_sglq = sglq;\n\twhile (!found) {\n\t\tif (!sglq)\n\t\t\tbreak;\n\t\tif (ndlp && ndlp->active_rrqs_xri_bitmap &&\n\t\t    test_bit(sglq->sli4_lxritag,\n\t\t    ndlp->active_rrqs_xri_bitmap)) {\n\t\t\t/* This xri has an rrq outstanding for this DID.\n\t\t\t * put it back in the list and get another xri.\n\t\t\t */\n\t\t\tlist_add_tail(&sglq->list, lpfc_els_sgl_list);\n\t\t\tsglq = NULL;\n\t\t\tlist_remove_head(lpfc_els_sgl_list, sglq,\n\t\t\t\t\t\tstruct lpfc_sglq, list);\n\t\t\tif (sglq == start_sglq) {\n\t\t\t\tlist_add_tail(&sglq->list, lpfc_els_sgl_list);\n\t\t\t\tsglq = NULL;\n\t\t\t\tbreak;\n\t\t\t} else\n\t\t\t\tcontinue;\n\t\t}\n\t\tsglq->ndlp = ndlp;\n\t\tfound = 1;\n\t\tphba->sli4_hba.lpfc_sglq_active_list[sglq->sli4_lxritag] = sglq;\n\t\tsglq->state = SGL_ALLOCATED;\n\t}\n\tspin_unlock(&phba->sli4_hba.sgl_list_lock);\n\treturn sglq;\n}\n\n/**\n * __lpfc_sli_get_nvmet_sglq - Allocates an iocb object from sgl pool\n * @phba: Pointer to HBA context object.\n * @piocbq: Pointer to the iocbq.\n *\n * This function is called with the sgl_list lock held. This function\n * gets a new driver sglq object from the sglq list. If the\n * list is not empty then it is successful, it returns pointer to the newly\n * allocated sglq object else it returns NULL.\n **/\nstruct lpfc_sglq *\n__lpfc_sli_get_nvmet_sglq(struct lpfc_hba *phba, struct lpfc_iocbq *piocbq)\n{\n\tstruct list_head *lpfc_nvmet_sgl_list;\n\tstruct lpfc_sglq *sglq = NULL;\n\n\tlpfc_nvmet_sgl_list = &phba->sli4_hba.lpfc_nvmet_sgl_list;\n\n\tlockdep_assert_held(&phba->sli4_hba.sgl_list_lock);\n\n\tlist_remove_head(lpfc_nvmet_sgl_list, sglq, struct lpfc_sglq, list);\n\tif (!sglq)\n\t\treturn NULL;\n\tphba->sli4_hba.lpfc_sglq_active_list[sglq->sli4_lxritag] = sglq;\n\tsglq->state = SGL_ALLOCATED;\n\treturn sglq;\n}\n\n/**\n * lpfc_sli_get_iocbq - Allocates an iocb object from iocb pool\n * @phba: Pointer to HBA context object.\n *\n * This function is called with no lock held. This function\n * allocates a new driver iocb object from the iocb pool. If the\n * allocation is successful, it returns pointer to the newly\n * allocated iocb object else it returns NULL.\n **/\nstruct lpfc_iocbq *\nlpfc_sli_get_iocbq(struct lpfc_hba *phba)\n{\n\tstruct lpfc_iocbq * iocbq = NULL;\n\tunsigned long iflags;\n\n\tspin_lock_irqsave(&phba->hbalock, iflags);\n\tiocbq = __lpfc_sli_get_iocbq(phba);\n\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\treturn iocbq;\n}\n\n/**\n * __lpfc_sli_release_iocbq_s4 - Release iocb to the iocb pool\n * @phba: Pointer to HBA context object.\n * @iocbq: Pointer to driver iocb object.\n *\n * This function is called to release the driver iocb object\n * to the iocb pool. The iotag in the iocb object\n * does not change for each use of the iocb object. This function\n * clears all other fields of the iocb object when it is freed.\n * The sqlq structure that holds the xritag and phys and virtual\n * mappings for the scatter gather list is retrieved from the\n * active array of sglq. The get of the sglq pointer also clears\n * the entry in the array. If the status of the IO indiactes that\n * this IO was aborted then the sglq entry it put on the\n * lpfc_abts_els_sgl_list until the CQ_ABORTED_XRI is received. If the\n * IO has good status or fails for any other reason then the sglq\n * entry is added to the free list (lpfc_els_sgl_list). The hbalock is\n *  asserted held in the code path calling this routine.\n **/\nstatic void\n__lpfc_sli_release_iocbq_s4(struct lpfc_hba *phba, struct lpfc_iocbq *iocbq)\n{\n\tstruct lpfc_sglq *sglq;\n\tsize_t start_clean = offsetof(struct lpfc_iocbq, iocb);\n\tunsigned long iflag = 0;\n\tstruct lpfc_sli_ring *pring;\n\n\tif (iocbq->sli4_xritag == NO_XRI)\n\t\tsglq = NULL;\n\telse\n\t\tsglq = __lpfc_clear_active_sglq(phba, iocbq->sli4_lxritag);\n\n\n\tif (sglq)  {\n\t\tif (iocbq->iocb_flag & LPFC_IO_NVMET) {\n\t\t\tspin_lock_irqsave(&phba->sli4_hba.sgl_list_lock,\n\t\t\t\t\t  iflag);\n\t\t\tsglq->state = SGL_FREED;\n\t\t\tsglq->ndlp = NULL;\n\t\t\tlist_add_tail(&sglq->list,\n\t\t\t\t      &phba->sli4_hba.lpfc_nvmet_sgl_list);\n\t\t\tspin_unlock_irqrestore(\n\t\t\t\t&phba->sli4_hba.sgl_list_lock, iflag);\n\t\t\tgoto out;\n\t\t}\n\n\t\tpring = phba->sli4_hba.els_wq->pring;\n\t\tif ((iocbq->iocb_flag & LPFC_EXCHANGE_BUSY) &&\n\t\t\t(sglq->state != SGL_XRI_ABORTED)) {\n\t\t\tspin_lock_irqsave(&phba->sli4_hba.sgl_list_lock,\n\t\t\t\t\t  iflag);\n\t\t\tlist_add(&sglq->list,\n\t\t\t\t &phba->sli4_hba.lpfc_abts_els_sgl_list);\n\t\t\tspin_unlock_irqrestore(\n\t\t\t\t&phba->sli4_hba.sgl_list_lock, iflag);\n\t\t} else {\n\t\t\tspin_lock_irqsave(&phba->sli4_hba.sgl_list_lock,\n\t\t\t\t\t  iflag);\n\t\t\tsglq->state = SGL_FREED;\n\t\t\tsglq->ndlp = NULL;\n\t\t\tlist_add_tail(&sglq->list,\n\t\t\t\t      &phba->sli4_hba.lpfc_els_sgl_list);\n\t\t\tspin_unlock_irqrestore(\n\t\t\t\t&phba->sli4_hba.sgl_list_lock, iflag);\n\n\t\t\t/* Check if TXQ queue needs to be serviced */\n\t\t\tif (!list_empty(&pring->txq))\n\t\t\t\tlpfc_worker_wake_up(phba);\n\t\t}\n\t}\n\nout:\n\t/*\n\t * Clean all volatile data fields, preserve iotag and node struct.\n\t */\n\tmemset((char *)iocbq + start_clean, 0, sizeof(*iocbq) - start_clean);\n\tiocbq->sli4_lxritag = NO_XRI;\n\tiocbq->sli4_xritag = NO_XRI;\n\tiocbq->iocb_flag &= ~(LPFC_IO_NVME | LPFC_IO_NVMET |\n\t\t\t      LPFC_IO_NVME_LS);\n\tlist_add_tail(&iocbq->list, &phba->lpfc_iocb_list);\n}\n\n\n/**\n * __lpfc_sli_release_iocbq_s3 - Release iocb to the iocb pool\n * @phba: Pointer to HBA context object.\n * @iocbq: Pointer to driver iocb object.\n *\n * This function is called to release the driver iocb object to the\n * iocb pool. The iotag in the iocb object does not change for each\n * use of the iocb object. This function clears all other fields of\n * the iocb object when it is freed. The hbalock is asserted held in\n * the code path calling this routine.\n **/\nstatic void\n__lpfc_sli_release_iocbq_s3(struct lpfc_hba *phba, struct lpfc_iocbq *iocbq)\n{\n\tsize_t start_clean = offsetof(struct lpfc_iocbq, iocb);\n\n\t/*\n\t * Clean all volatile data fields, preserve iotag and node struct.\n\t */\n\tmemset((char*)iocbq + start_clean, 0, sizeof(*iocbq) - start_clean);\n\tiocbq->sli4_xritag = NO_XRI;\n\tlist_add_tail(&iocbq->list, &phba->lpfc_iocb_list);\n}\n\n/**\n * __lpfc_sli_release_iocbq - Release iocb to the iocb pool\n * @phba: Pointer to HBA context object.\n * @iocbq: Pointer to driver iocb object.\n *\n * This function is called with hbalock held to release driver\n * iocb object to the iocb pool. The iotag in the iocb object\n * does not change for each use of the iocb object. This function\n * clears all other fields of the iocb object when it is freed.\n **/\nstatic void\n__lpfc_sli_release_iocbq(struct lpfc_hba *phba, struct lpfc_iocbq *iocbq)\n{\n\tlockdep_assert_held(&phba->hbalock);\n\n\tphba->__lpfc_sli_release_iocbq(phba, iocbq);\n\tphba->iocb_cnt--;\n}\n\n/**\n * lpfc_sli_release_iocbq - Release iocb to the iocb pool\n * @phba: Pointer to HBA context object.\n * @iocbq: Pointer to driver iocb object.\n *\n * This function is called with no lock held to release the iocb to\n * iocb pool.\n **/\nvoid\nlpfc_sli_release_iocbq(struct lpfc_hba *phba, struct lpfc_iocbq *iocbq)\n{\n\tunsigned long iflags;\n\n\t/*\n\t * Clean all volatile data fields, preserve iotag and node struct.\n\t */\n\tspin_lock_irqsave(&phba->hbalock, iflags);\n\t__lpfc_sli_release_iocbq(phba, iocbq);\n\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n}\n\n/**\n * lpfc_sli_cancel_iocbs - Cancel all iocbs from a list.\n * @phba: Pointer to HBA context object.\n * @iocblist: List of IOCBs.\n * @ulpstatus: ULP status in IOCB command field.\n * @ulpWord4: ULP word-4 in IOCB command field.\n *\n * This function is called with a list of IOCBs to cancel. It cancels the IOCB\n * on the list by invoking the complete callback function associated with the\n * IOCB with the provided @ulpstatus and @ulpword4 set to the IOCB commond\n * fields.\n **/\nvoid\nlpfc_sli_cancel_iocbs(struct lpfc_hba *phba, struct list_head *iocblist,\n\t\t      uint32_t ulpstatus, uint32_t ulpWord4)\n{\n\tstruct lpfc_iocbq *piocb;\n\n\twhile (!list_empty(iocblist)) {\n\t\tlist_remove_head(iocblist, piocb, struct lpfc_iocbq, list);\n\t\tif (!piocb->iocb_cmpl) {\n\t\t\tif (piocb->iocb_flag & LPFC_IO_NVME)\n\t\t\t\tlpfc_nvme_cancel_iocb(phba, piocb);\n\t\t\telse\n\t\t\t\tlpfc_sli_release_iocbq(phba, piocb);\n\t\t} else {\n\t\t\tpiocb->iocb.ulpStatus = ulpstatus;\n\t\t\tpiocb->iocb.un.ulpWord[4] = ulpWord4;\n\t\t\t(piocb->iocb_cmpl) (phba, piocb, piocb);\n\t\t}\n\t}\n\treturn;\n}\n\n/**\n * lpfc_sli_iocb_cmd_type - Get the iocb type\n * @iocb_cmnd: iocb command code.\n *\n * This function is called by ring event handler function to get the iocb type.\n * This function translates the iocb command to an iocb command type used to\n * decide the final disposition of each completed IOCB.\n * The function returns\n * LPFC_UNKNOWN_IOCB if it is an unsupported iocb\n * LPFC_SOL_IOCB     if it is a solicited iocb completion\n * LPFC_ABORT_IOCB   if it is an abort iocb\n * LPFC_UNSOL_IOCB   if it is an unsolicited iocb\n *\n * The caller is not required to hold any lock.\n **/\nstatic lpfc_iocb_type\nlpfc_sli_iocb_cmd_type(uint8_t iocb_cmnd)\n{\n\tlpfc_iocb_type type = LPFC_UNKNOWN_IOCB;\n\n\tif (iocb_cmnd > CMD_MAX_IOCB_CMD)\n\t\treturn 0;\n\n\tswitch (iocb_cmnd) {\n\tcase CMD_XMIT_SEQUENCE_CR:\n\tcase CMD_XMIT_SEQUENCE_CX:\n\tcase CMD_XMIT_BCAST_CN:\n\tcase CMD_XMIT_BCAST_CX:\n\tcase CMD_ELS_REQUEST_CR:\n\tcase CMD_ELS_REQUEST_CX:\n\tcase CMD_CREATE_XRI_CR:\n\tcase CMD_CREATE_XRI_CX:\n\tcase CMD_GET_RPI_CN:\n\tcase CMD_XMIT_ELS_RSP_CX:\n\tcase CMD_GET_RPI_CR:\n\tcase CMD_FCP_IWRITE_CR:\n\tcase CMD_FCP_IWRITE_CX:\n\tcase CMD_FCP_IREAD_CR:\n\tcase CMD_FCP_IREAD_CX:\n\tcase CMD_FCP_ICMND_CR:\n\tcase CMD_FCP_ICMND_CX:\n\tcase CMD_FCP_TSEND_CX:\n\tcase CMD_FCP_TRSP_CX:\n\tcase CMD_FCP_TRECEIVE_CX:\n\tcase CMD_FCP_AUTO_TRSP_CX:\n\tcase CMD_ADAPTER_MSG:\n\tcase CMD_ADAPTER_DUMP:\n\tcase CMD_XMIT_SEQUENCE64_CR:\n\tcase CMD_XMIT_SEQUENCE64_CX:\n\tcase CMD_XMIT_BCAST64_CN:\n\tcase CMD_XMIT_BCAST64_CX:\n\tcase CMD_ELS_REQUEST64_CR:\n\tcase CMD_ELS_REQUEST64_CX:\n\tcase CMD_FCP_IWRITE64_CR:\n\tcase CMD_FCP_IWRITE64_CX:\n\tcase CMD_FCP_IREAD64_CR:\n\tcase CMD_FCP_IREAD64_CX:\n\tcase CMD_FCP_ICMND64_CR:\n\tcase CMD_FCP_ICMND64_CX:\n\tcase CMD_FCP_TSEND64_CX:\n\tcase CMD_FCP_TRSP64_CX:\n\tcase CMD_FCP_TRECEIVE64_CX:\n\tcase CMD_GEN_REQUEST64_CR:\n\tcase CMD_GEN_REQUEST64_CX:\n\tcase CMD_XMIT_ELS_RSP64_CX:\n\tcase DSSCMD_IWRITE64_CR:\n\tcase DSSCMD_IWRITE64_CX:\n\tcase DSSCMD_IREAD64_CR:\n\tcase DSSCMD_IREAD64_CX:\n\tcase CMD_SEND_FRAME:\n\t\ttype = LPFC_SOL_IOCB;\n\t\tbreak;\n\tcase CMD_ABORT_XRI_CN:\n\tcase CMD_ABORT_XRI_CX:\n\tcase CMD_CLOSE_XRI_CN:\n\tcase CMD_CLOSE_XRI_CX:\n\tcase CMD_XRI_ABORTED_CX:\n\tcase CMD_ABORT_MXRI64_CN:\n\tcase CMD_XMIT_BLS_RSP64_CX:\n\t\ttype = LPFC_ABORT_IOCB;\n\t\tbreak;\n\tcase CMD_RCV_SEQUENCE_CX:\n\tcase CMD_RCV_ELS_REQ_CX:\n\tcase CMD_RCV_SEQUENCE64_CX:\n\tcase CMD_RCV_ELS_REQ64_CX:\n\tcase CMD_ASYNC_STATUS:\n\tcase CMD_IOCB_RCV_SEQ64_CX:\n\tcase CMD_IOCB_RCV_ELS64_CX:\n\tcase CMD_IOCB_RCV_CONT64_CX:\n\tcase CMD_IOCB_RET_XRI64_CX:\n\t\ttype = LPFC_UNSOL_IOCB;\n\t\tbreak;\n\tcase CMD_IOCB_XMIT_MSEQ64_CR:\n\tcase CMD_IOCB_XMIT_MSEQ64_CX:\n\tcase CMD_IOCB_RCV_SEQ_LIST64_CX:\n\tcase CMD_IOCB_RCV_ELS_LIST64_CX:\n\tcase CMD_IOCB_CLOSE_EXTENDED_CN:\n\tcase CMD_IOCB_ABORT_EXTENDED_CN:\n\tcase CMD_IOCB_RET_HBQE64_CN:\n\tcase CMD_IOCB_FCP_IBIDIR64_CR:\n\tcase CMD_IOCB_FCP_IBIDIR64_CX:\n\tcase CMD_IOCB_FCP_ITASKMGT64_CX:\n\tcase CMD_IOCB_LOGENTRY_CN:\n\tcase CMD_IOCB_LOGENTRY_ASYNC_CN:\n\t\tprintk(\"%s - Unhandled SLI-3 Command x%x\\n\",\n\t\t\t\t__func__, iocb_cmnd);\n\t\ttype = LPFC_UNKNOWN_IOCB;\n\t\tbreak;\n\tdefault:\n\t\ttype = LPFC_UNKNOWN_IOCB;\n\t\tbreak;\n\t}\n\n\treturn type;\n}\n\n/**\n * lpfc_sli_ring_map - Issue config_ring mbox for all rings\n * @phba: Pointer to HBA context object.\n *\n * This function is called from SLI initialization code\n * to configure every ring of the HBA's SLI interface. The\n * caller is not required to hold any lock. This function issues\n * a config_ring mailbox command for each ring.\n * This function returns zero if successful else returns a negative\n * error code.\n **/\nstatic int\nlpfc_sli_ring_map(struct lpfc_hba *phba)\n{\n\tstruct lpfc_sli *psli = &phba->sli;\n\tLPFC_MBOXQ_t *pmb;\n\tMAILBOX_t *pmbox;\n\tint i, rc, ret = 0;\n\n\tpmb = (LPFC_MBOXQ_t *) mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!pmb)\n\t\treturn -ENOMEM;\n\tpmbox = &pmb->u.mb;\n\tphba->link_state = LPFC_INIT_MBX_CMDS;\n\tfor (i = 0; i < psli->num_rings; i++) {\n\t\tlpfc_config_ring(phba, i, pmb);\n\t\trc = lpfc_sli_issue_mbox(phba, pmb, MBX_POLL);\n\t\tif (rc != MBX_SUCCESS) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"0446 Adapter failed to init (%d), \"\n\t\t\t\t\t\"mbxCmd x%x CFG_RING, mbxStatus x%x, \"\n\t\t\t\t\t\"ring %d\\n\",\n\t\t\t\t\trc, pmbox->mbxCommand,\n\t\t\t\t\tpmbox->mbxStatus, i);\n\t\t\tphba->link_state = LPFC_HBA_ERROR;\n\t\t\tret = -ENXIO;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmempool_free(pmb, phba->mbox_mem_pool);\n\treturn ret;\n}\n\n/**\n * lpfc_sli_ringtxcmpl_put - Adds new iocb to the txcmplq\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n * @piocb: Pointer to the driver iocb object.\n *\n * The driver calls this function with the hbalock held for SLI3 ports or\n * the ring lock held for SLI4 ports. The function adds the\n * new iocb to txcmplq of the given ring. This function always returns\n * 0. If this function is called for ELS ring, this function checks if\n * there is a vport associated with the ELS command. This function also\n * starts els_tmofunc timer if this is an ELS command.\n **/\nstatic int\nlpfc_sli_ringtxcmpl_put(struct lpfc_hba *phba, struct lpfc_sli_ring *pring,\n\t\t\tstruct lpfc_iocbq *piocb)\n{\n\tif (phba->sli_rev == LPFC_SLI_REV4)\n\t\tlockdep_assert_held(&pring->ring_lock);\n\telse\n\t\tlockdep_assert_held(&phba->hbalock);\n\n\tBUG_ON(!piocb);\n\n\tlist_add_tail(&piocb->list, &pring->txcmplq);\n\tpiocb->iocb_flag |= LPFC_IO_ON_TXCMPLQ;\n\tpring->txcmplq_cnt++;\n\n\tif ((unlikely(pring->ringno == LPFC_ELS_RING)) &&\n\t   (piocb->iocb.ulpCommand != CMD_ABORT_XRI_CN) &&\n\t   (piocb->iocb.ulpCommand != CMD_CLOSE_XRI_CN)) {\n\t\tBUG_ON(!piocb->vport);\n\t\tif (!(piocb->vport->load_flag & FC_UNLOADING))\n\t\t\tmod_timer(&piocb->vport->els_tmofunc,\n\t\t\t\t  jiffies +\n\t\t\t\t  msecs_to_jiffies(1000 * (phba->fc_ratov << 1)));\n\t}\n\n\treturn 0;\n}\n\n/**\n * lpfc_sli_ringtx_get - Get first element of the txq\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n *\n * This function is called with hbalock held to get next\n * iocb in txq of the given ring. If there is any iocb in\n * the txq, the function returns first iocb in the list after\n * removing the iocb from the list, else it returns NULL.\n **/\nstruct lpfc_iocbq *\nlpfc_sli_ringtx_get(struct lpfc_hba *phba, struct lpfc_sli_ring *pring)\n{\n\tstruct lpfc_iocbq *cmd_iocb;\n\n\tlockdep_assert_held(&phba->hbalock);\n\n\tlist_remove_head((&pring->txq), cmd_iocb, struct lpfc_iocbq, list);\n\treturn cmd_iocb;\n}\n\n/**\n * lpfc_sli_next_iocb_slot - Get next iocb slot in the ring\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n *\n * This function is called with hbalock held and the caller must post the\n * iocb without releasing the lock. If the caller releases the lock,\n * iocb slot returned by the function is not guaranteed to be available.\n * The function returns pointer to the next available iocb slot if there\n * is available slot in the ring, else it returns NULL.\n * If the get index of the ring is ahead of the put index, the function\n * will post an error attention event to the worker thread to take the\n * HBA to offline state.\n **/\nstatic IOCB_t *\nlpfc_sli_next_iocb_slot (struct lpfc_hba *phba, struct lpfc_sli_ring *pring)\n{\n\tstruct lpfc_pgp *pgp = &phba->port_gp[pring->ringno];\n\tuint32_t  max_cmd_idx = pring->sli.sli3.numCiocb;\n\n\tlockdep_assert_held(&phba->hbalock);\n\n\tif ((pring->sli.sli3.next_cmdidx == pring->sli.sli3.cmdidx) &&\n\t   (++pring->sli.sli3.next_cmdidx >= max_cmd_idx))\n\t\tpring->sli.sli3.next_cmdidx = 0;\n\n\tif (unlikely(pring->sli.sli3.local_getidx ==\n\t\tpring->sli.sli3.next_cmdidx)) {\n\n\t\tpring->sli.sli3.local_getidx = le32_to_cpu(pgp->cmdGetInx);\n\n\t\tif (unlikely(pring->sli.sli3.local_getidx >= max_cmd_idx)) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"0315 Ring %d issue: portCmdGet %d \"\n\t\t\t\t\t\"is bigger than cmd ring %d\\n\",\n\t\t\t\t\tpring->ringno,\n\t\t\t\t\tpring->sli.sli3.local_getidx,\n\t\t\t\t\tmax_cmd_idx);\n\n\t\t\tphba->link_state = LPFC_HBA_ERROR;\n\t\t\t/*\n\t\t\t * All error attention handlers are posted to\n\t\t\t * worker thread\n\t\t\t */\n\t\t\tphba->work_ha |= HA_ERATT;\n\t\t\tphba->work_hs = HS_FFER3;\n\n\t\t\tlpfc_worker_wake_up(phba);\n\n\t\t\treturn NULL;\n\t\t}\n\n\t\tif (pring->sli.sli3.local_getidx == pring->sli.sli3.next_cmdidx)\n\t\t\treturn NULL;\n\t}\n\n\treturn lpfc_cmd_iocb(phba, pring);\n}\n\n/**\n * lpfc_sli_next_iotag - Get an iotag for the iocb\n * @phba: Pointer to HBA context object.\n * @iocbq: Pointer to driver iocb object.\n *\n * This function gets an iotag for the iocb. If there is no unused iotag and\n * the iocbq_lookup_len < 0xffff, this function allocates a bigger iotag_lookup\n * array and assigns a new iotag.\n * The function returns the allocated iotag if successful, else returns zero.\n * Zero is not a valid iotag.\n * The caller is not required to hold any lock.\n **/\nuint16_t\nlpfc_sli_next_iotag(struct lpfc_hba *phba, struct lpfc_iocbq *iocbq)\n{\n\tstruct lpfc_iocbq **new_arr;\n\tstruct lpfc_iocbq **old_arr;\n\tsize_t new_len;\n\tstruct lpfc_sli *psli = &phba->sli;\n\tuint16_t iotag;\n\n\tspin_lock_irq(&phba->hbalock);\n\tiotag = psli->last_iotag;\n\tif(++iotag < psli->iocbq_lookup_len) {\n\t\tpsli->last_iotag = iotag;\n\t\tpsli->iocbq_lookup[iotag] = iocbq;\n\t\tspin_unlock_irq(&phba->hbalock);\n\t\tiocbq->iotag = iotag;\n\t\treturn iotag;\n\t} else if (psli->iocbq_lookup_len < (0xffff\n\t\t\t\t\t   - LPFC_IOCBQ_LOOKUP_INCREMENT)) {\n\t\tnew_len = psli->iocbq_lookup_len + LPFC_IOCBQ_LOOKUP_INCREMENT;\n\t\tspin_unlock_irq(&phba->hbalock);\n\t\tnew_arr = kcalloc(new_len, sizeof(struct lpfc_iocbq *),\n\t\t\t\t  GFP_KERNEL);\n\t\tif (new_arr) {\n\t\t\tspin_lock_irq(&phba->hbalock);\n\t\t\told_arr = psli->iocbq_lookup;\n\t\t\tif (new_len <= psli->iocbq_lookup_len) {\n\t\t\t\t/* highly unprobable case */\n\t\t\t\tkfree(new_arr);\n\t\t\t\tiotag = psli->last_iotag;\n\t\t\t\tif(++iotag < psli->iocbq_lookup_len) {\n\t\t\t\t\tpsli->last_iotag = iotag;\n\t\t\t\t\tpsli->iocbq_lookup[iotag] = iocbq;\n\t\t\t\t\tspin_unlock_irq(&phba->hbalock);\n\t\t\t\t\tiocbq->iotag = iotag;\n\t\t\t\t\treturn iotag;\n\t\t\t\t}\n\t\t\t\tspin_unlock_irq(&phba->hbalock);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tif (psli->iocbq_lookup)\n\t\t\t\tmemcpy(new_arr, old_arr,\n\t\t\t\t       ((psli->last_iotag  + 1) *\n\t\t\t\t\tsizeof (struct lpfc_iocbq *)));\n\t\t\tpsli->iocbq_lookup = new_arr;\n\t\t\tpsli->iocbq_lookup_len = new_len;\n\t\t\tpsli->last_iotag = iotag;\n\t\t\tpsli->iocbq_lookup[iotag] = iocbq;\n\t\t\tspin_unlock_irq(&phba->hbalock);\n\t\t\tiocbq->iotag = iotag;\n\t\t\tkfree(old_arr);\n\t\t\treturn iotag;\n\t\t}\n\t} else\n\t\tspin_unlock_irq(&phba->hbalock);\n\n\tlpfc_printf_log(phba, KERN_WARNING, LOG_SLI,\n\t\t\t\"0318 Failed to allocate IOTAG.last IOTAG is %d\\n\",\n\t\t\tpsli->last_iotag);\n\n\treturn 0;\n}\n\n/**\n * lpfc_sli_submit_iocb - Submit an iocb to the firmware\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n * @iocb: Pointer to iocb slot in the ring.\n * @nextiocb: Pointer to driver iocb object which need to be\n *            posted to firmware.\n *\n * This function is called to post a new iocb to the firmware. This\n * function copies the new iocb to ring iocb slot and updates the\n * ring pointers. It adds the new iocb to txcmplq if there is\n * a completion call back for this iocb else the function will free the\n * iocb object.  The hbalock is asserted held in the code path calling\n * this routine.\n **/\nstatic void\nlpfc_sli_submit_iocb(struct lpfc_hba *phba, struct lpfc_sli_ring *pring,\n\t\tIOCB_t *iocb, struct lpfc_iocbq *nextiocb)\n{\n\t/*\n\t * Set up an iotag\n\t */\n\tnextiocb->iocb.ulpIoTag = (nextiocb->iocb_cmpl) ? nextiocb->iotag : 0;\n\n\n\tif (pring->ringno == LPFC_ELS_RING) {\n\t\tlpfc_debugfs_slow_ring_trc(phba,\n\t\t\t\"IOCB cmd ring:   wd4:x%08x wd6:x%08x wd7:x%08x\",\n\t\t\t*(((uint32_t *) &nextiocb->iocb) + 4),\n\t\t\t*(((uint32_t *) &nextiocb->iocb) + 6),\n\t\t\t*(((uint32_t *) &nextiocb->iocb) + 7));\n\t}\n\n\t/*\n\t * Issue iocb command to adapter\n\t */\n\tlpfc_sli_pcimem_bcopy(&nextiocb->iocb, iocb, phba->iocb_cmd_size);\n\twmb();\n\tpring->stats.iocb_cmd++;\n\n\t/*\n\t * If there is no completion routine to call, we can release the\n\t * IOCB buffer back right now. For IOCBs, like QUE_RING_BUF,\n\t * that have no rsp ring completion, iocb_cmpl MUST be NULL.\n\t */\n\tif (nextiocb->iocb_cmpl)\n\t\tlpfc_sli_ringtxcmpl_put(phba, pring, nextiocb);\n\telse\n\t\t__lpfc_sli_release_iocbq(phba, nextiocb);\n\n\t/*\n\t * Let the HBA know what IOCB slot will be the next one the\n\t * driver will put a command into.\n\t */\n\tpring->sli.sli3.cmdidx = pring->sli.sli3.next_cmdidx;\n\twritel(pring->sli.sli3.cmdidx, &phba->host_gp[pring->ringno].cmdPutInx);\n}\n\n/**\n * lpfc_sli_update_full_ring - Update the chip attention register\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n *\n * The caller is not required to hold any lock for calling this function.\n * This function updates the chip attention bits for the ring to inform firmware\n * that there are pending work to be done for this ring and requests an\n * interrupt when there is space available in the ring. This function is\n * called when the driver is unable to post more iocbs to the ring due\n * to unavailability of space in the ring.\n **/\nstatic void\nlpfc_sli_update_full_ring(struct lpfc_hba *phba, struct lpfc_sli_ring *pring)\n{\n\tint ringno = pring->ringno;\n\n\tpring->flag |= LPFC_CALL_RING_AVAILABLE;\n\n\twmb();\n\n\t/*\n\t * Set ring 'ringno' to SET R0CE_REQ in Chip Att register.\n\t * The HBA will tell us when an IOCB entry is available.\n\t */\n\twritel((CA_R0ATT|CA_R0CE_REQ) << (ringno*4), phba->CAregaddr);\n\treadl(phba->CAregaddr); /* flush */\n\n\tpring->stats.iocb_cmd_full++;\n}\n\n/**\n * lpfc_sli_update_ring - Update chip attention register\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n *\n * This function updates the chip attention register bit for the\n * given ring to inform HBA that there is more work to be done\n * in this ring. The caller is not required to hold any lock.\n **/\nstatic void\nlpfc_sli_update_ring(struct lpfc_hba *phba, struct lpfc_sli_ring *pring)\n{\n\tint ringno = pring->ringno;\n\n\t/*\n\t * Tell the HBA that there is work to do in this ring.\n\t */\n\tif (!(phba->sli3_options & LPFC_SLI3_CRP_ENABLED)) {\n\t\twmb();\n\t\twritel(CA_R0ATT << (ringno * 4), phba->CAregaddr);\n\t\treadl(phba->CAregaddr); /* flush */\n\t}\n}\n\n/**\n * lpfc_sli_resume_iocb - Process iocbs in the txq\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n *\n * This function is called with hbalock held to post pending iocbs\n * in the txq to the firmware. This function is called when driver\n * detects space available in the ring.\n **/\nstatic void\nlpfc_sli_resume_iocb(struct lpfc_hba *phba, struct lpfc_sli_ring *pring)\n{\n\tIOCB_t *iocb;\n\tstruct lpfc_iocbq *nextiocb;\n\n\tlockdep_assert_held(&phba->hbalock);\n\n\t/*\n\t * Check to see if:\n\t *  (a) there is anything on the txq to send\n\t *  (b) link is up\n\t *  (c) link attention events can be processed (fcp ring only)\n\t *  (d) IOCB processing is not blocked by the outstanding mbox command.\n\t */\n\n\tif (lpfc_is_link_up(phba) &&\n\t    (!list_empty(&pring->txq)) &&\n\t    (pring->ringno != LPFC_FCP_RING ||\n\t     phba->sli.sli_flag & LPFC_PROCESS_LA)) {\n\n\t\twhile ((iocb = lpfc_sli_next_iocb_slot(phba, pring)) &&\n\t\t       (nextiocb = lpfc_sli_ringtx_get(phba, pring)))\n\t\t\tlpfc_sli_submit_iocb(phba, pring, iocb, nextiocb);\n\n\t\tif (iocb)\n\t\t\tlpfc_sli_update_ring(phba, pring);\n\t\telse\n\t\t\tlpfc_sli_update_full_ring(phba, pring);\n\t}\n\n\treturn;\n}\n\n/**\n * lpfc_sli_next_hbq_slot - Get next hbq entry for the HBQ\n * @phba: Pointer to HBA context object.\n * @hbqno: HBQ number.\n *\n * This function is called with hbalock held to get the next\n * available slot for the given HBQ. If there is free slot\n * available for the HBQ it will return pointer to the next available\n * HBQ entry else it will return NULL.\n **/\nstatic struct lpfc_hbq_entry *\nlpfc_sli_next_hbq_slot(struct lpfc_hba *phba, uint32_t hbqno)\n{\n\tstruct hbq_s *hbqp = &phba->hbqs[hbqno];\n\n\tlockdep_assert_held(&phba->hbalock);\n\n\tif (hbqp->next_hbqPutIdx == hbqp->hbqPutIdx &&\n\t    ++hbqp->next_hbqPutIdx >= hbqp->entry_count)\n\t\thbqp->next_hbqPutIdx = 0;\n\n\tif (unlikely(hbqp->local_hbqGetIdx == hbqp->next_hbqPutIdx)) {\n\t\tuint32_t raw_index = phba->hbq_get[hbqno];\n\t\tuint32_t getidx = le32_to_cpu(raw_index);\n\n\t\thbqp->local_hbqGetIdx = getidx;\n\n\t\tif (unlikely(hbqp->local_hbqGetIdx >= hbqp->entry_count)) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"1802 HBQ %d: local_hbqGetIdx \"\n\t\t\t\t\t\"%u is > than hbqp->entry_count %u\\n\",\n\t\t\t\t\thbqno, hbqp->local_hbqGetIdx,\n\t\t\t\t\thbqp->entry_count);\n\n\t\t\tphba->link_state = LPFC_HBA_ERROR;\n\t\t\treturn NULL;\n\t\t}\n\n\t\tif (hbqp->local_hbqGetIdx == hbqp->next_hbqPutIdx)\n\t\t\treturn NULL;\n\t}\n\n\treturn (struct lpfc_hbq_entry *) phba->hbqs[hbqno].hbq_virt +\n\t\t\thbqp->hbqPutIdx;\n}\n\n/**\n * lpfc_sli_hbqbuf_free_all - Free all the hbq buffers\n * @phba: Pointer to HBA context object.\n *\n * This function is called with no lock held to free all the\n * hbq buffers while uninitializing the SLI interface. It also\n * frees the HBQ buffers returned by the firmware but not yet\n * processed by the upper layers.\n **/\nvoid\nlpfc_sli_hbqbuf_free_all(struct lpfc_hba *phba)\n{\n\tstruct lpfc_dmabuf *dmabuf, *next_dmabuf;\n\tstruct hbq_dmabuf *hbq_buf;\n\tunsigned long flags;\n\tint i, hbq_count;\n\n\thbq_count = lpfc_sli_hbq_count();\n\t/* Return all memory used by all HBQs */\n\tspin_lock_irqsave(&phba->hbalock, flags);\n\tfor (i = 0; i < hbq_count; ++i) {\n\t\tlist_for_each_entry_safe(dmabuf, next_dmabuf,\n\t\t\t\t&phba->hbqs[i].hbq_buffer_list, list) {\n\t\t\thbq_buf = container_of(dmabuf, struct hbq_dmabuf, dbuf);\n\t\t\tlist_del(&hbq_buf->dbuf.list);\n\t\t\t(phba->hbqs[i].hbq_free_buffer)(phba, hbq_buf);\n\t\t}\n\t\tphba->hbqs[i].buffer_count = 0;\n\t}\n\n\t/* Mark the HBQs not in use */\n\tphba->hbq_in_use = 0;\n\tspin_unlock_irqrestore(&phba->hbalock, flags);\n}\n\n/**\n * lpfc_sli_hbq_to_firmware - Post the hbq buffer to firmware\n * @phba: Pointer to HBA context object.\n * @hbqno: HBQ number.\n * @hbq_buf: Pointer to HBQ buffer.\n *\n * This function is called with the hbalock held to post a\n * hbq buffer to the firmware. If the function finds an empty\n * slot in the HBQ, it will post the buffer. The function will return\n * pointer to the hbq entry if it successfully post the buffer\n * else it will return NULL.\n **/\nstatic int\nlpfc_sli_hbq_to_firmware(struct lpfc_hba *phba, uint32_t hbqno,\n\t\t\t struct hbq_dmabuf *hbq_buf)\n{\n\tlockdep_assert_held(&phba->hbalock);\n\treturn phba->lpfc_sli_hbq_to_firmware(phba, hbqno, hbq_buf);\n}\n\n/**\n * lpfc_sli_hbq_to_firmware_s3 - Post the hbq buffer to SLI3 firmware\n * @phba: Pointer to HBA context object.\n * @hbqno: HBQ number.\n * @hbq_buf: Pointer to HBQ buffer.\n *\n * This function is called with the hbalock held to post a hbq buffer to the\n * firmware. If the function finds an empty slot in the HBQ, it will post the\n * buffer and place it on the hbq_buffer_list. The function will return zero if\n * it successfully post the buffer else it will return an error.\n **/\nstatic int\nlpfc_sli_hbq_to_firmware_s3(struct lpfc_hba *phba, uint32_t hbqno,\n\t\t\t    struct hbq_dmabuf *hbq_buf)\n{\n\tstruct lpfc_hbq_entry *hbqe;\n\tdma_addr_t physaddr = hbq_buf->dbuf.phys;\n\n\tlockdep_assert_held(&phba->hbalock);\n\t/* Get next HBQ entry slot to use */\n\thbqe = lpfc_sli_next_hbq_slot(phba, hbqno);\n\tif (hbqe) {\n\t\tstruct hbq_s *hbqp = &phba->hbqs[hbqno];\n\n\t\thbqe->bde.addrHigh = le32_to_cpu(putPaddrHigh(physaddr));\n\t\thbqe->bde.addrLow  = le32_to_cpu(putPaddrLow(physaddr));\n\t\thbqe->bde.tus.f.bdeSize = hbq_buf->total_size;\n\t\thbqe->bde.tus.f.bdeFlags = 0;\n\t\thbqe->bde.tus.w = le32_to_cpu(hbqe->bde.tus.w);\n\t\thbqe->buffer_tag = le32_to_cpu(hbq_buf->tag);\n\t\t\t\t/* Sync SLIM */\n\t\thbqp->hbqPutIdx = hbqp->next_hbqPutIdx;\n\t\twritel(hbqp->hbqPutIdx, phba->hbq_put + hbqno);\n\t\t\t\t/* flush */\n\t\treadl(phba->hbq_put + hbqno);\n\t\tlist_add_tail(&hbq_buf->dbuf.list, &hbqp->hbq_buffer_list);\n\t\treturn 0;\n\t} else\n\t\treturn -ENOMEM;\n}\n\n/**\n * lpfc_sli_hbq_to_firmware_s4 - Post the hbq buffer to SLI4 firmware\n * @phba: Pointer to HBA context object.\n * @hbqno: HBQ number.\n * @hbq_buf: Pointer to HBQ buffer.\n *\n * This function is called with the hbalock held to post an RQE to the SLI4\n * firmware. If able to post the RQE to the RQ it will queue the hbq entry to\n * the hbq_buffer_list and return zero, otherwise it will return an error.\n **/\nstatic int\nlpfc_sli_hbq_to_firmware_s4(struct lpfc_hba *phba, uint32_t hbqno,\n\t\t\t    struct hbq_dmabuf *hbq_buf)\n{\n\tint rc;\n\tstruct lpfc_rqe hrqe;\n\tstruct lpfc_rqe drqe;\n\tstruct lpfc_queue *hrq;\n\tstruct lpfc_queue *drq;\n\n\tif (hbqno != LPFC_ELS_HBQ)\n\t\treturn 1;\n\thrq = phba->sli4_hba.hdr_rq;\n\tdrq = phba->sli4_hba.dat_rq;\n\n\tlockdep_assert_held(&phba->hbalock);\n\thrqe.address_lo = putPaddrLow(hbq_buf->hbuf.phys);\n\thrqe.address_hi = putPaddrHigh(hbq_buf->hbuf.phys);\n\tdrqe.address_lo = putPaddrLow(hbq_buf->dbuf.phys);\n\tdrqe.address_hi = putPaddrHigh(hbq_buf->dbuf.phys);\n\trc = lpfc_sli4_rq_put(hrq, drq, &hrqe, &drqe);\n\tif (rc < 0)\n\t\treturn rc;\n\thbq_buf->tag = (rc | (hbqno << 16));\n\tlist_add_tail(&hbq_buf->dbuf.list, &phba->hbqs[hbqno].hbq_buffer_list);\n\treturn 0;\n}\n\n/* HBQ for ELS and CT traffic. */\nstatic struct lpfc_hbq_init lpfc_els_hbq = {\n\t.rn = 1,\n\t.entry_count = 256,\n\t.mask_count = 0,\n\t.profile = 0,\n\t.ring_mask = (1 << LPFC_ELS_RING),\n\t.buffer_count = 0,\n\t.init_count = 40,\n\t.add_count = 40,\n};\n\n/* Array of HBQs */\nstruct lpfc_hbq_init *lpfc_hbq_defs[] = {\n\t&lpfc_els_hbq,\n};\n\n/**\n * lpfc_sli_hbqbuf_fill_hbqs - Post more hbq buffers to HBQ\n * @phba: Pointer to HBA context object.\n * @hbqno: HBQ number.\n * @count: Number of HBQ buffers to be posted.\n *\n * This function is called with no lock held to post more hbq buffers to the\n * given HBQ. The function returns the number of HBQ buffers successfully\n * posted.\n **/\nstatic int\nlpfc_sli_hbqbuf_fill_hbqs(struct lpfc_hba *phba, uint32_t hbqno, uint32_t count)\n{\n\tuint32_t i, posted = 0;\n\tunsigned long flags;\n\tstruct hbq_dmabuf *hbq_buffer;\n\tLIST_HEAD(hbq_buf_list);\n\tif (!phba->hbqs[hbqno].hbq_alloc_buffer)\n\t\treturn 0;\n\n\tif ((phba->hbqs[hbqno].buffer_count + count) >\n\t    lpfc_hbq_defs[hbqno]->entry_count)\n\t\tcount = lpfc_hbq_defs[hbqno]->entry_count -\n\t\t\t\t\tphba->hbqs[hbqno].buffer_count;\n\tif (!count)\n\t\treturn 0;\n\t/* Allocate HBQ entries */\n\tfor (i = 0; i < count; i++) {\n\t\thbq_buffer = (phba->hbqs[hbqno].hbq_alloc_buffer)(phba);\n\t\tif (!hbq_buffer)\n\t\t\tbreak;\n\t\tlist_add_tail(&hbq_buffer->dbuf.list, &hbq_buf_list);\n\t}\n\t/* Check whether HBQ is still in use */\n\tspin_lock_irqsave(&phba->hbalock, flags);\n\tif (!phba->hbq_in_use)\n\t\tgoto err;\n\twhile (!list_empty(&hbq_buf_list)) {\n\t\tlist_remove_head(&hbq_buf_list, hbq_buffer, struct hbq_dmabuf,\n\t\t\t\t dbuf.list);\n\t\thbq_buffer->tag = (phba->hbqs[hbqno].buffer_count |\n\t\t\t\t      (hbqno << 16));\n\t\tif (!lpfc_sli_hbq_to_firmware(phba, hbqno, hbq_buffer)) {\n\t\t\tphba->hbqs[hbqno].buffer_count++;\n\t\t\tposted++;\n\t\t} else\n\t\t\t(phba->hbqs[hbqno].hbq_free_buffer)(phba, hbq_buffer);\n\t}\n\tspin_unlock_irqrestore(&phba->hbalock, flags);\n\treturn posted;\nerr:\n\tspin_unlock_irqrestore(&phba->hbalock, flags);\n\twhile (!list_empty(&hbq_buf_list)) {\n\t\tlist_remove_head(&hbq_buf_list, hbq_buffer, struct hbq_dmabuf,\n\t\t\t\t dbuf.list);\n\t\t(phba->hbqs[hbqno].hbq_free_buffer)(phba, hbq_buffer);\n\t}\n\treturn 0;\n}\n\n/**\n * lpfc_sli_hbqbuf_add_hbqs - Post more HBQ buffers to firmware\n * @phba: Pointer to HBA context object.\n * @qno: HBQ number.\n *\n * This function posts more buffers to the HBQ. This function\n * is called with no lock held. The function returns the number of HBQ entries\n * successfully allocated.\n **/\nint\nlpfc_sli_hbqbuf_add_hbqs(struct lpfc_hba *phba, uint32_t qno)\n{\n\tif (phba->sli_rev == LPFC_SLI_REV4)\n\t\treturn 0;\n\telse\n\t\treturn lpfc_sli_hbqbuf_fill_hbqs(phba, qno,\n\t\t\t\t\t lpfc_hbq_defs[qno]->add_count);\n}\n\n/**\n * lpfc_sli_hbqbuf_init_hbqs - Post initial buffers to the HBQ\n * @phba: Pointer to HBA context object.\n * @qno:  HBQ queue number.\n *\n * This function is called from SLI initialization code path with\n * no lock held to post initial HBQ buffers to firmware. The\n * function returns the number of HBQ entries successfully allocated.\n **/\nstatic int\nlpfc_sli_hbqbuf_init_hbqs(struct lpfc_hba *phba, uint32_t qno)\n{\n\tif (phba->sli_rev == LPFC_SLI_REV4)\n\t\treturn lpfc_sli_hbqbuf_fill_hbqs(phba, qno,\n\t\t\t\t\tlpfc_hbq_defs[qno]->entry_count);\n\telse\n\t\treturn lpfc_sli_hbqbuf_fill_hbqs(phba, qno,\n\t\t\t\t\t lpfc_hbq_defs[qno]->init_count);\n}\n\n/*\n * lpfc_sli_hbqbuf_get - Remove the first hbq off of an hbq list\n *\n * This function removes the first hbq buffer on an hbq list and returns a\n * pointer to that buffer. If it finds no buffers on the list it returns NULL.\n **/\nstatic struct hbq_dmabuf *\nlpfc_sli_hbqbuf_get(struct list_head *rb_list)\n{\n\tstruct lpfc_dmabuf *d_buf;\n\n\tlist_remove_head(rb_list, d_buf, struct lpfc_dmabuf, list);\n\tif (!d_buf)\n\t\treturn NULL;\n\treturn container_of(d_buf, struct hbq_dmabuf, dbuf);\n}\n\n/**\n * lpfc_sli_rqbuf_get - Remove the first dma buffer off of an RQ list\n * @phba: Pointer to HBA context object.\n * @hrq: HBQ number.\n *\n * This function removes the first RQ buffer on an RQ buffer list and returns a\n * pointer to that buffer. If it finds no buffers on the list it returns NULL.\n **/\nstatic struct rqb_dmabuf *\nlpfc_sli_rqbuf_get(struct lpfc_hba *phba, struct lpfc_queue *hrq)\n{\n\tstruct lpfc_dmabuf *h_buf;\n\tstruct lpfc_rqb *rqbp;\n\n\trqbp = hrq->rqbp;\n\tlist_remove_head(&rqbp->rqb_buffer_list, h_buf,\n\t\t\t struct lpfc_dmabuf, list);\n\tif (!h_buf)\n\t\treturn NULL;\n\trqbp->buffer_count--;\n\treturn container_of(h_buf, struct rqb_dmabuf, hbuf);\n}\n\n/**\n * lpfc_sli_hbqbuf_find - Find the hbq buffer associated with a tag\n * @phba: Pointer to HBA context object.\n * @tag: Tag of the hbq buffer.\n *\n * This function searches for the hbq buffer associated with the given tag in\n * the hbq buffer list. If it finds the hbq buffer, it returns the hbq_buffer\n * otherwise it returns NULL.\n **/\nstatic struct hbq_dmabuf *\nlpfc_sli_hbqbuf_find(struct lpfc_hba *phba, uint32_t tag)\n{\n\tstruct lpfc_dmabuf *d_buf;\n\tstruct hbq_dmabuf *hbq_buf;\n\tuint32_t hbqno;\n\n\thbqno = tag >> 16;\n\tif (hbqno >= LPFC_MAX_HBQS)\n\t\treturn NULL;\n\n\tspin_lock_irq(&phba->hbalock);\n\tlist_for_each_entry(d_buf, &phba->hbqs[hbqno].hbq_buffer_list, list) {\n\t\thbq_buf = container_of(d_buf, struct hbq_dmabuf, dbuf);\n\t\tif (hbq_buf->tag == tag) {\n\t\t\tspin_unlock_irq(&phba->hbalock);\n\t\t\treturn hbq_buf;\n\t\t}\n\t}\n\tspin_unlock_irq(&phba->hbalock);\n\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"1803 Bad hbq tag. Data: x%x x%x\\n\",\n\t\t\ttag, phba->hbqs[tag >> 16].buffer_count);\n\treturn NULL;\n}\n\n/**\n * lpfc_sli_free_hbq - Give back the hbq buffer to firmware\n * @phba: Pointer to HBA context object.\n * @hbq_buffer: Pointer to HBQ buffer.\n *\n * This function is called with hbalock. This function gives back\n * the hbq buffer to firmware. If the HBQ does not have space to\n * post the buffer, it will free the buffer.\n **/\nvoid\nlpfc_sli_free_hbq(struct lpfc_hba *phba, struct hbq_dmabuf *hbq_buffer)\n{\n\tuint32_t hbqno;\n\n\tif (hbq_buffer) {\n\t\thbqno = hbq_buffer->tag >> 16;\n\t\tif (lpfc_sli_hbq_to_firmware(phba, hbqno, hbq_buffer))\n\t\t\t(phba->hbqs[hbqno].hbq_free_buffer)(phba, hbq_buffer);\n\t}\n}\n\n/**\n * lpfc_sli_chk_mbx_command - Check if the mailbox is a legitimate mailbox\n * @mbxCommand: mailbox command code.\n *\n * This function is called by the mailbox event handler function to verify\n * that the completed mailbox command is a legitimate mailbox command. If the\n * completed mailbox is not known to the function, it will return MBX_SHUTDOWN\n * and the mailbox event handler will take the HBA offline.\n **/\nstatic int\nlpfc_sli_chk_mbx_command(uint8_t mbxCommand)\n{\n\tuint8_t ret;\n\n\tswitch (mbxCommand) {\n\tcase MBX_LOAD_SM:\n\tcase MBX_READ_NV:\n\tcase MBX_WRITE_NV:\n\tcase MBX_WRITE_VPARMS:\n\tcase MBX_RUN_BIU_DIAG:\n\tcase MBX_INIT_LINK:\n\tcase MBX_DOWN_LINK:\n\tcase MBX_CONFIG_LINK:\n\tcase MBX_CONFIG_RING:\n\tcase MBX_RESET_RING:\n\tcase MBX_READ_CONFIG:\n\tcase MBX_READ_RCONFIG:\n\tcase MBX_READ_SPARM:\n\tcase MBX_READ_STATUS:\n\tcase MBX_READ_RPI:\n\tcase MBX_READ_XRI:\n\tcase MBX_READ_REV:\n\tcase MBX_READ_LNK_STAT:\n\tcase MBX_REG_LOGIN:\n\tcase MBX_UNREG_LOGIN:\n\tcase MBX_CLEAR_LA:\n\tcase MBX_DUMP_MEMORY:\n\tcase MBX_DUMP_CONTEXT:\n\tcase MBX_RUN_DIAGS:\n\tcase MBX_RESTART:\n\tcase MBX_UPDATE_CFG:\n\tcase MBX_DOWN_LOAD:\n\tcase MBX_DEL_LD_ENTRY:\n\tcase MBX_RUN_PROGRAM:\n\tcase MBX_SET_MASK:\n\tcase MBX_SET_VARIABLE:\n\tcase MBX_UNREG_D_ID:\n\tcase MBX_KILL_BOARD:\n\tcase MBX_CONFIG_FARP:\n\tcase MBX_BEACON:\n\tcase MBX_LOAD_AREA:\n\tcase MBX_RUN_BIU_DIAG64:\n\tcase MBX_CONFIG_PORT:\n\tcase MBX_READ_SPARM64:\n\tcase MBX_READ_RPI64:\n\tcase MBX_REG_LOGIN64:\n\tcase MBX_READ_TOPOLOGY:\n\tcase MBX_WRITE_WWN:\n\tcase MBX_SET_DEBUG:\n\tcase MBX_LOAD_EXP_ROM:\n\tcase MBX_ASYNCEVT_ENABLE:\n\tcase MBX_REG_VPI:\n\tcase MBX_UNREG_VPI:\n\tcase MBX_HEARTBEAT:\n\tcase MBX_PORT_CAPABILITIES:\n\tcase MBX_PORT_IOV_CONTROL:\n\tcase MBX_SLI4_CONFIG:\n\tcase MBX_SLI4_REQ_FTRS:\n\tcase MBX_REG_FCFI:\n\tcase MBX_UNREG_FCFI:\n\tcase MBX_REG_VFI:\n\tcase MBX_UNREG_VFI:\n\tcase MBX_INIT_VPI:\n\tcase MBX_INIT_VFI:\n\tcase MBX_RESUME_RPI:\n\tcase MBX_READ_EVENT_LOG_STATUS:\n\tcase MBX_READ_EVENT_LOG:\n\tcase MBX_SECURITY_MGMT:\n\tcase MBX_AUTH_PORT:\n\tcase MBX_ACCESS_VDATA:\n\t\tret = mbxCommand;\n\t\tbreak;\n\tdefault:\n\t\tret = MBX_SHUTDOWN;\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\n/**\n * lpfc_sli_wake_mbox_wait - lpfc_sli_issue_mbox_wait mbox completion handler\n * @phba: Pointer to HBA context object.\n * @pmboxq: Pointer to mailbox command.\n *\n * This is completion handler function for mailbox commands issued from\n * lpfc_sli_issue_mbox_wait function. This function is called by the\n * mailbox event handler function with no lock held. This function\n * will wake up thread waiting on the wait queue pointed by context1\n * of the mailbox.\n **/\nvoid\nlpfc_sli_wake_mbox_wait(struct lpfc_hba *phba, LPFC_MBOXQ_t *pmboxq)\n{\n\tunsigned long drvr_flag;\n\tstruct completion *pmbox_done;\n\n\t/*\n\t * If pmbox_done is empty, the driver thread gave up waiting and\n\t * continued running.\n\t */\n\tpmboxq->mbox_flag |= LPFC_MBX_WAKE;\n\tspin_lock_irqsave(&phba->hbalock, drvr_flag);\n\tpmbox_done = (struct completion *)pmboxq->context3;\n\tif (pmbox_done)\n\t\tcomplete(pmbox_done);\n\tspin_unlock_irqrestore(&phba->hbalock, drvr_flag);\n\treturn;\n}\n\nstatic void\n__lpfc_sli_rpi_release(struct lpfc_vport *vport, struct lpfc_nodelist *ndlp)\n{\n\tunsigned long iflags;\n\n\tif (ndlp->nlp_flag & NLP_RELEASE_RPI) {\n\t\tlpfc_sli4_free_rpi(vport->phba, ndlp->nlp_rpi);\n\t\tspin_lock_irqsave(&vport->phba->ndlp_lock, iflags);\n\t\tndlp->nlp_flag &= ~NLP_RELEASE_RPI;\n\t\tndlp->nlp_rpi = LPFC_RPI_ALLOC_ERROR;\n\t\tspin_unlock_irqrestore(&vport->phba->ndlp_lock, iflags);\n\t}\n\tndlp->nlp_flag &= ~NLP_UNREG_INP;\n}\n\n/**\n * lpfc_sli_def_mbox_cmpl - Default mailbox completion handler\n * @phba: Pointer to HBA context object.\n * @pmb: Pointer to mailbox object.\n *\n * This function is the default mailbox completion handler. It\n * frees the memory resources associated with the completed mailbox\n * command. If the completed command is a REG_LOGIN mailbox command,\n * this function will issue a UREG_LOGIN to re-claim the RPI.\n **/\nvoid\nlpfc_sli_def_mbox_cmpl(struct lpfc_hba *phba, LPFC_MBOXQ_t *pmb)\n{\n\tstruct lpfc_vport  *vport = pmb->vport;\n\tstruct lpfc_dmabuf *mp;\n\tstruct lpfc_nodelist *ndlp;\n\tstruct Scsi_Host *shost;\n\tuint16_t rpi, vpi;\n\tint rc;\n\n\tmp = (struct lpfc_dmabuf *)(pmb->ctx_buf);\n\n\tif (mp) {\n\t\tlpfc_mbuf_free(phba, mp->virt, mp->phys);\n\t\tkfree(mp);\n\t}\n\n\t/*\n\t * If a REG_LOGIN succeeded  after node is destroyed or node\n\t * is in re-discovery driver need to cleanup the RPI.\n\t */\n\tif (!(phba->pport->load_flag & FC_UNLOADING) &&\n\t    pmb->u.mb.mbxCommand == MBX_REG_LOGIN64 &&\n\t    !pmb->u.mb.mbxStatus) {\n\t\trpi = pmb->u.mb.un.varWords[0];\n\t\tvpi = pmb->u.mb.un.varRegLogin.vpi;\n\t\tif (phba->sli_rev == LPFC_SLI_REV4)\n\t\t\tvpi -= phba->sli4_hba.max_cfg_param.vpi_base;\n\t\tlpfc_unreg_login(phba, vpi, rpi, pmb);\n\t\tpmb->vport = vport;\n\t\tpmb->mbox_cmpl = lpfc_sli_def_mbox_cmpl;\n\t\trc = lpfc_sli_issue_mbox(phba, pmb, MBX_NOWAIT);\n\t\tif (rc != MBX_NOT_FINISHED)\n\t\t\treturn;\n\t}\n\n\tif ((pmb->u.mb.mbxCommand == MBX_REG_VPI) &&\n\t\t!(phba->pport->load_flag & FC_UNLOADING) &&\n\t\t!pmb->u.mb.mbxStatus) {\n\t\tshost = lpfc_shost_from_vport(vport);\n\t\tspin_lock_irq(shost->host_lock);\n\t\tvport->vpi_state |= LPFC_VPI_REGISTERED;\n\t\tvport->fc_flag &= ~FC_VPORT_NEEDS_REG_VPI;\n\t\tspin_unlock_irq(shost->host_lock);\n\t}\n\n\tif (pmb->u.mb.mbxCommand == MBX_REG_LOGIN64) {\n\t\tndlp = (struct lpfc_nodelist *)pmb->ctx_ndlp;\n\t\tlpfc_nlp_put(ndlp);\n\t\tpmb->ctx_buf = NULL;\n\t\tpmb->ctx_ndlp = NULL;\n\t}\n\n\tif (pmb->u.mb.mbxCommand == MBX_UNREG_LOGIN) {\n\t\tndlp = (struct lpfc_nodelist *)pmb->ctx_ndlp;\n\n\t\t/* Check to see if there are any deferred events to process */\n\t\tif (ndlp) {\n\t\t\tlpfc_printf_vlog(\n\t\t\t\tvport,\n\t\t\t\tKERN_INFO, LOG_MBOX | LOG_DISCOVERY,\n\t\t\t\t\"1438 UNREG cmpl deferred mbox x%x \"\n\t\t\t\t\"on NPort x%x Data: x%x x%x %px\\n\",\n\t\t\t\tndlp->nlp_rpi, ndlp->nlp_DID,\n\t\t\t\tndlp->nlp_flag, ndlp->nlp_defer_did, ndlp);\n\n\t\t\tif ((ndlp->nlp_flag & NLP_UNREG_INP) &&\n\t\t\t    (ndlp->nlp_defer_did != NLP_EVT_NOTHING_PENDING)) {\n\t\t\t\tndlp->nlp_flag &= ~NLP_UNREG_INP;\n\t\t\t\tndlp->nlp_defer_did = NLP_EVT_NOTHING_PENDING;\n\t\t\t\tlpfc_issue_els_plogi(vport, ndlp->nlp_DID, 0);\n\t\t\t} else {\n\t\t\t\t__lpfc_sli_rpi_release(vport, ndlp);\n\t\t\t}\n\t\t\tif (vport->load_flag & FC_UNLOADING)\n\t\t\t\tlpfc_nlp_put(ndlp);\n\t\t\tpmb->ctx_ndlp = NULL;\n\t\t}\n\t}\n\n\t/* Check security permission status on INIT_LINK mailbox command */\n\tif ((pmb->u.mb.mbxCommand == MBX_INIT_LINK) &&\n\t    (pmb->u.mb.mbxStatus == MBXERR_SEC_NO_PERMISSION))\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2860 SLI authentication is required \"\n\t\t\t\t\"for INIT_LINK but has not done yet\\n\");\n\n\tif (bf_get(lpfc_mqe_command, &pmb->u.mqe) == MBX_SLI4_CONFIG)\n\t\tlpfc_sli4_mbox_cmd_free(phba, pmb);\n\telse\n\t\tmempool_free(pmb, phba->mbox_mem_pool);\n}\n /**\n * lpfc_sli4_unreg_rpi_cmpl_clr - mailbox completion handler\n * @phba: Pointer to HBA context object.\n * @pmb: Pointer to mailbox object.\n *\n * This function is the unreg rpi mailbox completion handler. It\n * frees the memory resources associated with the completed mailbox\n * command. An additional refrenece is put on the ndlp to prevent\n * lpfc_nlp_release from freeing the rpi bit in the bitmask before\n * the unreg mailbox command completes, this routine puts the\n * reference back.\n *\n **/\nvoid\nlpfc_sli4_unreg_rpi_cmpl_clr(struct lpfc_hba *phba, LPFC_MBOXQ_t *pmb)\n{\n\tstruct lpfc_vport  *vport = pmb->vport;\n\tstruct lpfc_nodelist *ndlp;\n\n\tndlp = pmb->ctx_ndlp;\n\tif (pmb->u.mb.mbxCommand == MBX_UNREG_LOGIN) {\n\t\tif (phba->sli_rev == LPFC_SLI_REV4 &&\n\t\t    (bf_get(lpfc_sli_intf_if_type,\n\t\t     &phba->sli4_hba.sli_intf) >=\n\t\t     LPFC_SLI_INTF_IF_TYPE_2)) {\n\t\t\tif (ndlp) {\n\t\t\t\tlpfc_printf_vlog(\n\t\t\t\t\tvport, KERN_INFO, LOG_MBOX | LOG_SLI,\n\t\t\t\t\t \"0010 UNREG_LOGIN vpi:%x \"\n\t\t\t\t\t \"rpi:%x DID:%x defer x%x flg x%x \"\n\t\t\t\t\t \"map:%x %px\\n\",\n\t\t\t\t\t vport->vpi, ndlp->nlp_rpi,\n\t\t\t\t\t ndlp->nlp_DID, ndlp->nlp_defer_did,\n\t\t\t\t\t ndlp->nlp_flag,\n\t\t\t\t\t ndlp->nlp_usg_map, ndlp);\n\t\t\t\tndlp->nlp_flag &= ~NLP_LOGO_ACC;\n\t\t\t\tlpfc_nlp_put(ndlp);\n\n\t\t\t\t/* Check to see if there are any deferred\n\t\t\t\t * events to process\n\t\t\t\t */\n\t\t\t\tif ((ndlp->nlp_flag & NLP_UNREG_INP) &&\n\t\t\t\t    (ndlp->nlp_defer_did !=\n\t\t\t\t    NLP_EVT_NOTHING_PENDING)) {\n\t\t\t\t\tlpfc_printf_vlog(\n\t\t\t\t\t\tvport, KERN_INFO, LOG_DISCOVERY,\n\t\t\t\t\t\t\"4111 UNREG cmpl deferred \"\n\t\t\t\t\t\t\"clr x%x on \"\n\t\t\t\t\t\t\"NPort x%x Data: x%x x%px\\n\",\n\t\t\t\t\t\tndlp->nlp_rpi, ndlp->nlp_DID,\n\t\t\t\t\t\tndlp->nlp_defer_did, ndlp);\n\t\t\t\t\tndlp->nlp_flag &= ~NLP_UNREG_INP;\n\t\t\t\t\tndlp->nlp_defer_did =\n\t\t\t\t\t\tNLP_EVT_NOTHING_PENDING;\n\t\t\t\t\tlpfc_issue_els_plogi(\n\t\t\t\t\t\tvport, ndlp->nlp_DID, 0);\n\t\t\t\t} else {\n\t\t\t\t\t__lpfc_sli_rpi_release(vport, ndlp);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tmempool_free(pmb, phba->mbox_mem_pool);\n}\n\n/**\n * lpfc_sli_handle_mb_event - Handle mailbox completions from firmware\n * @phba: Pointer to HBA context object.\n *\n * This function is called with no lock held. This function processes all\n * the completed mailbox commands and gives it to upper layers. The interrupt\n * service routine processes mailbox completion interrupt and adds completed\n * mailbox commands to the mboxq_cmpl queue and signals the worker thread.\n * Worker thread call lpfc_sli_handle_mb_event, which will return the\n * completed mailbox commands in mboxq_cmpl queue to the upper layers. This\n * function returns the mailbox commands to the upper layer by calling the\n * completion handler function of each mailbox.\n **/\nint\nlpfc_sli_handle_mb_event(struct lpfc_hba *phba)\n{\n\tMAILBOX_t *pmbox;\n\tLPFC_MBOXQ_t *pmb;\n\tint rc;\n\tLIST_HEAD(cmplq);\n\n\tphba->sli.slistat.mbox_event++;\n\n\t/* Get all completed mailboxe buffers into the cmplq */\n\tspin_lock_irq(&phba->hbalock);\n\tlist_splice_init(&phba->sli.mboxq_cmpl, &cmplq);\n\tspin_unlock_irq(&phba->hbalock);\n\n\t/* Get a Mailbox buffer to setup mailbox commands for callback */\n\tdo {\n\t\tlist_remove_head(&cmplq, pmb, LPFC_MBOXQ_t, list);\n\t\tif (pmb == NULL)\n\t\t\tbreak;\n\n\t\tpmbox = &pmb->u.mb;\n\n\t\tif (pmbox->mbxCommand != MBX_HEARTBEAT) {\n\t\t\tif (pmb->vport) {\n\t\t\t\tlpfc_debugfs_disc_trc(pmb->vport,\n\t\t\t\t\tLPFC_DISC_TRC_MBOX_VPORT,\n\t\t\t\t\t\"MBOX cmpl vport: cmd:x%x mb:x%x x%x\",\n\t\t\t\t\t(uint32_t)pmbox->mbxCommand,\n\t\t\t\t\tpmbox->un.varWords[0],\n\t\t\t\t\tpmbox->un.varWords[1]);\n\t\t\t}\n\t\t\telse {\n\t\t\t\tlpfc_debugfs_disc_trc(phba->pport,\n\t\t\t\t\tLPFC_DISC_TRC_MBOX,\n\t\t\t\t\t\"MBOX cmpl:       cmd:x%x mb:x%x x%x\",\n\t\t\t\t\t(uint32_t)pmbox->mbxCommand,\n\t\t\t\t\tpmbox->un.varWords[0],\n\t\t\t\t\tpmbox->un.varWords[1]);\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * It is a fatal error if unknown mbox command completion.\n\t\t */\n\t\tif (lpfc_sli_chk_mbx_command(pmbox->mbxCommand) ==\n\t\t    MBX_SHUTDOWN) {\n\t\t\t/* Unknown mailbox command compl */\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"(%d):0323 Unknown Mailbox command \"\n\t\t\t\t\t\"x%x (x%x/x%x) Cmpl\\n\",\n\t\t\t\t\tpmb->vport ? pmb->vport->vpi :\n\t\t\t\t\tLPFC_VPORT_UNKNOWN,\n\t\t\t\t\tpmbox->mbxCommand,\n\t\t\t\t\tlpfc_sli_config_mbox_subsys_get(phba,\n\t\t\t\t\t\t\t\t\tpmb),\n\t\t\t\t\tlpfc_sli_config_mbox_opcode_get(phba,\n\t\t\t\t\t\t\t\t\tpmb));\n\t\t\tphba->link_state = LPFC_HBA_ERROR;\n\t\t\tphba->work_hs = HS_FFER3;\n\t\t\tlpfc_handle_eratt(phba);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (pmbox->mbxStatus) {\n\t\t\tphba->sli.slistat.mbox_stat_err++;\n\t\t\tif (pmbox->mbxStatus == MBXERR_NO_RESOURCES) {\n\t\t\t\t/* Mbox cmd cmpl error - RETRYing */\n\t\t\t\tlpfc_printf_log(phba, KERN_INFO,\n\t\t\t\t\tLOG_MBOX | LOG_SLI,\n\t\t\t\t\t\"(%d):0305 Mbox cmd cmpl \"\n\t\t\t\t\t\"error - RETRYing Data: x%x \"\n\t\t\t\t\t\"(x%x/x%x) x%x x%x x%x\\n\",\n\t\t\t\t\tpmb->vport ? pmb->vport->vpi :\n\t\t\t\t\tLPFC_VPORT_UNKNOWN,\n\t\t\t\t\tpmbox->mbxCommand,\n\t\t\t\t\tlpfc_sli_config_mbox_subsys_get(phba,\n\t\t\t\t\t\t\t\t\tpmb),\n\t\t\t\t\tlpfc_sli_config_mbox_opcode_get(phba,\n\t\t\t\t\t\t\t\t\tpmb),\n\t\t\t\t\tpmbox->mbxStatus,\n\t\t\t\t\tpmbox->un.varWords[0],\n\t\t\t\t\tpmb->vport ? pmb->vport->port_state :\n\t\t\t\t\tLPFC_VPORT_UNKNOWN);\n\t\t\t\tpmbox->mbxStatus = 0;\n\t\t\t\tpmbox->mbxOwner = OWN_HOST;\n\t\t\t\trc = lpfc_sli_issue_mbox(phba, pmb, MBX_NOWAIT);\n\t\t\t\tif (rc != MBX_NOT_FINISHED)\n\t\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\t/* Mailbox cmd <cmd> Cmpl <cmpl> */\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_MBOX | LOG_SLI,\n\t\t\t\t\"(%d):0307 Mailbox cmd x%x (x%x/x%x) Cmpl %ps \"\n\t\t\t\t\"Data: x%x x%x x%x x%x x%x x%x x%x x%x x%x \"\n\t\t\t\t\"x%x x%x x%x\\n\",\n\t\t\t\tpmb->vport ? pmb->vport->vpi : 0,\n\t\t\t\tpmbox->mbxCommand,\n\t\t\t\tlpfc_sli_config_mbox_subsys_get(phba, pmb),\n\t\t\t\tlpfc_sli_config_mbox_opcode_get(phba, pmb),\n\t\t\t\tpmb->mbox_cmpl,\n\t\t\t\t*((uint32_t *) pmbox),\n\t\t\t\tpmbox->un.varWords[0],\n\t\t\t\tpmbox->un.varWords[1],\n\t\t\t\tpmbox->un.varWords[2],\n\t\t\t\tpmbox->un.varWords[3],\n\t\t\t\tpmbox->un.varWords[4],\n\t\t\t\tpmbox->un.varWords[5],\n\t\t\t\tpmbox->un.varWords[6],\n\t\t\t\tpmbox->un.varWords[7],\n\t\t\t\tpmbox->un.varWords[8],\n\t\t\t\tpmbox->un.varWords[9],\n\t\t\t\tpmbox->un.varWords[10]);\n\n\t\tif (pmb->mbox_cmpl)\n\t\t\tpmb->mbox_cmpl(phba,pmb);\n\t} while (1);\n\treturn 0;\n}\n\n/**\n * lpfc_sli_get_buff - Get the buffer associated with the buffer tag\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n * @tag: buffer tag.\n *\n * This function is called with no lock held. When QUE_BUFTAG_BIT bit\n * is set in the tag the buffer is posted for a particular exchange,\n * the function will return the buffer without replacing the buffer.\n * If the buffer is for unsolicited ELS or CT traffic, this function\n * returns the buffer and also posts another buffer to the firmware.\n **/\nstatic struct lpfc_dmabuf *\nlpfc_sli_get_buff(struct lpfc_hba *phba,\n\t\t  struct lpfc_sli_ring *pring,\n\t\t  uint32_t tag)\n{\n\tstruct hbq_dmabuf *hbq_entry;\n\n\tif (tag & QUE_BUFTAG_BIT)\n\t\treturn lpfc_sli_ring_taggedbuf_get(phba, pring, tag);\n\thbq_entry = lpfc_sli_hbqbuf_find(phba, tag);\n\tif (!hbq_entry)\n\t\treturn NULL;\n\treturn &hbq_entry->dbuf;\n}\n\n/**\n * lpfc_nvme_unsol_ls_handler - Process an unsolicited event data buffer\n *                              containing a NVME LS request.\n * @phba: pointer to lpfc hba data structure.\n * @piocb: pointer to the iocbq struct representing the sequence starting\n *        frame.\n *\n * This routine initially validates the NVME LS, validates there is a login\n * with the port that sent the LS, and then calls the appropriate nvme host\n * or target LS request handler.\n **/\nstatic void\nlpfc_nvme_unsol_ls_handler(struct lpfc_hba *phba, struct lpfc_iocbq *piocb)\n{\n\tstruct lpfc_nodelist *ndlp;\n\tstruct lpfc_dmabuf *d_buf;\n\tstruct hbq_dmabuf *nvmebuf;\n\tstruct fc_frame_header *fc_hdr;\n\tstruct lpfc_async_xchg_ctx *axchg = NULL;\n\tchar *failwhy = NULL;\n\tuint32_t oxid, sid, did, fctl, size;\n\tint ret = 1;\n\n\td_buf = piocb->context2;\n\n\tnvmebuf = container_of(d_buf, struct hbq_dmabuf, dbuf);\n\tfc_hdr = nvmebuf->hbuf.virt;\n\toxid = be16_to_cpu(fc_hdr->fh_ox_id);\n\tsid = sli4_sid_from_fc_hdr(fc_hdr);\n\tdid = sli4_did_from_fc_hdr(fc_hdr);\n\tfctl = (fc_hdr->fh_f_ctl[0] << 16 |\n\t\tfc_hdr->fh_f_ctl[1] << 8 |\n\t\tfc_hdr->fh_f_ctl[2]);\n\tsize = bf_get(lpfc_rcqe_length, &nvmebuf->cq_event.cqe.rcqe_cmpl);\n\n\tlpfc_nvmeio_data(phba, \"NVME LS    RCV: xri x%x sz %d from %06x\\n\",\n\t\t\t oxid, size, sid);\n\n\tif (phba->pport->load_flag & FC_UNLOADING) {\n\t\tfailwhy = \"Driver Unloading\";\n\t} else if (!(phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME)) {\n\t\tfailwhy = \"NVME FC4 Disabled\";\n\t} else if (!phba->nvmet_support && !phba->pport->localport) {\n\t\tfailwhy = \"No Localport\";\n\t} else if (phba->nvmet_support && !phba->targetport) {\n\t\tfailwhy = \"No Targetport\";\n\t} else if (unlikely(fc_hdr->fh_r_ctl != FC_RCTL_ELS4_REQ)) {\n\t\tfailwhy = \"Bad NVME LS R_CTL\";\n\t} else if (unlikely((fctl & 0x00FF0000) !=\n\t\t\t(FC_FC_FIRST_SEQ | FC_FC_END_SEQ | FC_FC_SEQ_INIT))) {\n\t\tfailwhy = \"Bad NVME LS F_CTL\";\n\t} else {\n\t\taxchg = kzalloc(sizeof(*axchg), GFP_ATOMIC);\n\t\tif (!axchg)\n\t\t\tfailwhy = \"No CTX memory\";\n\t}\n\n\tif (unlikely(failwhy)) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6154 Drop NVME LS: SID %06X OXID x%X: %s\\n\",\n\t\t\t\tsid, oxid, failwhy);\n\t\tgoto out_fail;\n\t}\n\n\t/* validate the source of the LS is logged in */\n\tndlp = lpfc_findnode_did(phba->pport, sid);\n\tif (!ndlp || !NLP_CHK_NODE_ACT(ndlp) ||\n\t    ((ndlp->nlp_state != NLP_STE_UNMAPPED_NODE) &&\n\t     (ndlp->nlp_state != NLP_STE_MAPPED_NODE))) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_NVME_DISC,\n\t\t\t\t\"6216 NVME Unsol rcv: No ndlp: \"\n\t\t\t\t\"NPort_ID x%x oxid x%x\\n\",\n\t\t\t\tsid, oxid);\n\t\tgoto out_fail;\n\t}\n\n\taxchg->phba = phba;\n\taxchg->ndlp = ndlp;\n\taxchg->size = size;\n\taxchg->oxid = oxid;\n\taxchg->sid = sid;\n\taxchg->wqeq = NULL;\n\taxchg->state = LPFC_NVME_STE_LS_RCV;\n\taxchg->entry_cnt = 1;\n\taxchg->rqb_buffer = (void *)nvmebuf;\n\taxchg->hdwq = &phba->sli4_hba.hdwq[0];\n\taxchg->payload = nvmebuf->dbuf.virt;\n\tINIT_LIST_HEAD(&axchg->list);\n\n\tif (phba->nvmet_support)\n\t\tret = lpfc_nvmet_handle_lsreq(phba, axchg);\n\telse\n\t\tret = lpfc_nvme_handle_lsreq(phba, axchg);\n\n\t/* if zero, LS was successfully handled. If non-zero, LS not handled */\n\tif (!ret)\n\t\treturn;\n\n\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"6155 Drop NVME LS from DID %06X: SID %06X OXID x%X \"\n\t\t\t\"NVMe%s handler failed %d\\n\",\n\t\t\tdid, sid, oxid,\n\t\t\t(phba->nvmet_support) ? \"T\" : \"I\", ret);\n\nout_fail:\n\n\t/* recycle receive buffer */\n\tlpfc_in_buf_free(phba, &nvmebuf->dbuf);\n\n\t/* If start of new exchange, abort it */\n\tif (axchg && (fctl & FC_FC_FIRST_SEQ && !(fctl & FC_FC_EX_CTX)))\n\t\tret = lpfc_nvme_unsol_ls_issue_abort(phba, axchg, sid, oxid);\n\n\tif (ret)\n\t\tkfree(axchg);\n}\n\n/**\n * lpfc_complete_unsol_iocb - Complete an unsolicited sequence\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n * @saveq: Pointer to the iocbq struct representing the sequence starting frame.\n * @fch_r_ctl: the r_ctl for the first frame of the sequence.\n * @fch_type: the type for the first frame of the sequence.\n *\n * This function is called with no lock held. This function uses the r_ctl and\n * type of the received sequence to find the correct callback function to call\n * to process the sequence.\n **/\nstatic int\nlpfc_complete_unsol_iocb(struct lpfc_hba *phba, struct lpfc_sli_ring *pring,\n\t\t\t struct lpfc_iocbq *saveq, uint32_t fch_r_ctl,\n\t\t\t uint32_t fch_type)\n{\n\tint i;\n\n\tswitch (fch_type) {\n\tcase FC_TYPE_NVME:\n\t\tlpfc_nvme_unsol_ls_handler(phba, saveq);\n\t\treturn 1;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* unSolicited Responses */\n\tif (pring->prt[0].profile) {\n\t\tif (pring->prt[0].lpfc_sli_rcv_unsol_event)\n\t\t\t(pring->prt[0].lpfc_sli_rcv_unsol_event) (phba, pring,\n\t\t\t\t\t\t\t\t\tsaveq);\n\t\treturn 1;\n\t}\n\t/* We must search, based on rctl / type\n\t   for the right routine */\n\tfor (i = 0; i < pring->num_mask; i++) {\n\t\tif ((pring->prt[i].rctl == fch_r_ctl) &&\n\t\t    (pring->prt[i].type == fch_type)) {\n\t\t\tif (pring->prt[i].lpfc_sli_rcv_unsol_event)\n\t\t\t\t(pring->prt[i].lpfc_sli_rcv_unsol_event)\n\t\t\t\t\t\t(phba, pring, saveq);\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/**\n * lpfc_sli_process_unsol_iocb - Unsolicited iocb handler\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n * @saveq: Pointer to the unsolicited iocb.\n *\n * This function is called with no lock held by the ring event handler\n * when there is an unsolicited iocb posted to the response ring by the\n * firmware. This function gets the buffer associated with the iocbs\n * and calls the event handler for the ring. This function handles both\n * qring buffers and hbq buffers.\n * When the function returns 1 the caller can free the iocb object otherwise\n * upper layer functions will free the iocb objects.\n **/\nstatic int\nlpfc_sli_process_unsol_iocb(struct lpfc_hba *phba, struct lpfc_sli_ring *pring,\n\t\t\t    struct lpfc_iocbq *saveq)\n{\n\tIOCB_t           * irsp;\n\tWORD5            * w5p;\n\tuint32_t           Rctl, Type;\n\tstruct lpfc_iocbq *iocbq;\n\tstruct lpfc_dmabuf *dmzbuf;\n\n\tirsp = &(saveq->iocb);\n\n\tif (irsp->ulpCommand == CMD_ASYNC_STATUS) {\n\t\tif (pring->lpfc_sli_rcv_async_status)\n\t\t\tpring->lpfc_sli_rcv_async_status(phba, pring, saveq);\n\t\telse\n\t\t\tlpfc_printf_log(phba,\n\t\t\t\t\tKERN_WARNING,\n\t\t\t\t\tLOG_SLI,\n\t\t\t\t\t\"0316 Ring %d handler: unexpected \"\n\t\t\t\t\t\"ASYNC_STATUS iocb received evt_code \"\n\t\t\t\t\t\"0x%x\\n\",\n\t\t\t\t\tpring->ringno,\n\t\t\t\t\tirsp->un.asyncstat.evt_code);\n\t\treturn 1;\n\t}\n\n\tif ((irsp->ulpCommand == CMD_IOCB_RET_XRI64_CX) &&\n\t\t(phba->sli3_options & LPFC_SLI3_HBQ_ENABLED)) {\n\t\tif (irsp->ulpBdeCount > 0) {\n\t\t\tdmzbuf = lpfc_sli_get_buff(phba, pring,\n\t\t\t\t\tirsp->un.ulpWord[3]);\n\t\t\tlpfc_in_buf_free(phba, dmzbuf);\n\t\t}\n\n\t\tif (irsp->ulpBdeCount > 1) {\n\t\t\tdmzbuf = lpfc_sli_get_buff(phba, pring,\n\t\t\t\t\tirsp->unsli3.sli3Words[3]);\n\t\t\tlpfc_in_buf_free(phba, dmzbuf);\n\t\t}\n\n\t\tif (irsp->ulpBdeCount > 2) {\n\t\t\tdmzbuf = lpfc_sli_get_buff(phba, pring,\n\t\t\t\tirsp->unsli3.sli3Words[7]);\n\t\t\tlpfc_in_buf_free(phba, dmzbuf);\n\t\t}\n\n\t\treturn 1;\n\t}\n\n\tif (phba->sli3_options & LPFC_SLI3_HBQ_ENABLED) {\n\t\tif (irsp->ulpBdeCount != 0) {\n\t\t\tsaveq->context2 = lpfc_sli_get_buff(phba, pring,\n\t\t\t\t\t\tirsp->un.ulpWord[3]);\n\t\t\tif (!saveq->context2)\n\t\t\t\tlpfc_printf_log(phba,\n\t\t\t\t\tKERN_ERR,\n\t\t\t\t\tLOG_SLI,\n\t\t\t\t\t\"0341 Ring %d Cannot find buffer for \"\n\t\t\t\t\t\"an unsolicited iocb. tag 0x%x\\n\",\n\t\t\t\t\tpring->ringno,\n\t\t\t\t\tirsp->un.ulpWord[3]);\n\t\t}\n\t\tif (irsp->ulpBdeCount == 2) {\n\t\t\tsaveq->context3 = lpfc_sli_get_buff(phba, pring,\n\t\t\t\t\t\tirsp->unsli3.sli3Words[7]);\n\t\t\tif (!saveq->context3)\n\t\t\t\tlpfc_printf_log(phba,\n\t\t\t\t\tKERN_ERR,\n\t\t\t\t\tLOG_SLI,\n\t\t\t\t\t\"0342 Ring %d Cannot find buffer for an\"\n\t\t\t\t\t\" unsolicited iocb. tag 0x%x\\n\",\n\t\t\t\t\tpring->ringno,\n\t\t\t\t\tirsp->unsli3.sli3Words[7]);\n\t\t}\n\t\tlist_for_each_entry(iocbq, &saveq->list, list) {\n\t\t\tirsp = &(iocbq->iocb);\n\t\t\tif (irsp->ulpBdeCount != 0) {\n\t\t\t\tiocbq->context2 = lpfc_sli_get_buff(phba, pring,\n\t\t\t\t\t\t\tirsp->un.ulpWord[3]);\n\t\t\t\tif (!iocbq->context2)\n\t\t\t\t\tlpfc_printf_log(phba,\n\t\t\t\t\t\tKERN_ERR,\n\t\t\t\t\t\tLOG_SLI,\n\t\t\t\t\t\t\"0343 Ring %d Cannot find \"\n\t\t\t\t\t\t\"buffer for an unsolicited iocb\"\n\t\t\t\t\t\t\". tag 0x%x\\n\", pring->ringno,\n\t\t\t\t\t\tirsp->un.ulpWord[3]);\n\t\t\t}\n\t\t\tif (irsp->ulpBdeCount == 2) {\n\t\t\t\tiocbq->context3 = lpfc_sli_get_buff(phba, pring,\n\t\t\t\t\t\tirsp->unsli3.sli3Words[7]);\n\t\t\t\tif (!iocbq->context3)\n\t\t\t\t\tlpfc_printf_log(phba,\n\t\t\t\t\t\tKERN_ERR,\n\t\t\t\t\t\tLOG_SLI,\n\t\t\t\t\t\t\"0344 Ring %d Cannot find \"\n\t\t\t\t\t\t\"buffer for an unsolicited \"\n\t\t\t\t\t\t\"iocb. tag 0x%x\\n\",\n\t\t\t\t\t\tpring->ringno,\n\t\t\t\t\t\tirsp->unsli3.sli3Words[7]);\n\t\t\t}\n\t\t}\n\t}\n\tif (irsp->ulpBdeCount != 0 &&\n\t    (irsp->ulpCommand == CMD_IOCB_RCV_CONT64_CX ||\n\t     irsp->ulpStatus == IOSTAT_INTERMED_RSP)) {\n\t\tint found = 0;\n\n\t\t/* search continue save q for same XRI */\n\t\tlist_for_each_entry(iocbq, &pring->iocb_continue_saveq, clist) {\n\t\t\tif (iocbq->iocb.unsli3.rcvsli3.ox_id ==\n\t\t\t\tsaveq->iocb.unsli3.rcvsli3.ox_id) {\n\t\t\t\tlist_add_tail(&saveq->list, &iocbq->list);\n\t\t\t\tfound = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!found)\n\t\t\tlist_add_tail(&saveq->clist,\n\t\t\t\t      &pring->iocb_continue_saveq);\n\t\tif (saveq->iocb.ulpStatus != IOSTAT_INTERMED_RSP) {\n\t\t\tlist_del_init(&iocbq->clist);\n\t\t\tsaveq = iocbq;\n\t\t\tirsp = &(saveq->iocb);\n\t\t} else\n\t\t\treturn 0;\n\t}\n\tif ((irsp->ulpCommand == CMD_RCV_ELS_REQ64_CX) ||\n\t    (irsp->ulpCommand == CMD_RCV_ELS_REQ_CX) ||\n\t    (irsp->ulpCommand == CMD_IOCB_RCV_ELS64_CX)) {\n\t\tRctl = FC_RCTL_ELS_REQ;\n\t\tType = FC_TYPE_ELS;\n\t} else {\n\t\tw5p = (WORD5 *)&(saveq->iocb.un.ulpWord[5]);\n\t\tRctl = w5p->hcsw.Rctl;\n\t\tType = w5p->hcsw.Type;\n\n\t\t/* Firmware Workaround */\n\t\tif ((Rctl == 0) && (pring->ringno == LPFC_ELS_RING) &&\n\t\t\t(irsp->ulpCommand == CMD_RCV_SEQUENCE64_CX ||\n\t\t\t irsp->ulpCommand == CMD_IOCB_RCV_SEQ64_CX)) {\n\t\t\tRctl = FC_RCTL_ELS_REQ;\n\t\t\tType = FC_TYPE_ELS;\n\t\t\tw5p->hcsw.Rctl = Rctl;\n\t\t\tw5p->hcsw.Type = Type;\n\t\t}\n\t}\n\n\tif (!lpfc_complete_unsol_iocb(phba, pring, saveq, Rctl, Type))\n\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_SLI,\n\t\t\t\t\"0313 Ring %d handler: unexpected Rctl x%x \"\n\t\t\t\t\"Type x%x received\\n\",\n\t\t\t\tpring->ringno, Rctl, Type);\n\n\treturn 1;\n}\n\n/**\n * lpfc_sli_iocbq_lookup - Find command iocb for the given response iocb\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n * @prspiocb: Pointer to response iocb object.\n *\n * This function looks up the iocb_lookup table to get the command iocb\n * corresponding to the given response iocb using the iotag of the\n * response iocb. The driver calls this function with the hbalock held\n * for SLI3 ports or the ring lock held for SLI4 ports.\n * This function returns the command iocb object if it finds the command\n * iocb else returns NULL.\n **/\nstatic struct lpfc_iocbq *\nlpfc_sli_iocbq_lookup(struct lpfc_hba *phba,\n\t\t      struct lpfc_sli_ring *pring,\n\t\t      struct lpfc_iocbq *prspiocb)\n{\n\tstruct lpfc_iocbq *cmd_iocb = NULL;\n\tuint16_t iotag;\n\tspinlock_t *temp_lock = NULL;\n\tunsigned long iflag = 0;\n\n\tif (phba->sli_rev == LPFC_SLI_REV4)\n\t\ttemp_lock = &pring->ring_lock;\n\telse\n\t\ttemp_lock = &phba->hbalock;\n\n\tspin_lock_irqsave(temp_lock, iflag);\n\tiotag = prspiocb->iocb.ulpIoTag;\n\n\tif (iotag != 0 && iotag <= phba->sli.last_iotag) {\n\t\tcmd_iocb = phba->sli.iocbq_lookup[iotag];\n\t\tif (cmd_iocb->iocb_flag & LPFC_IO_ON_TXCMPLQ) {\n\t\t\t/* remove from txcmpl queue list */\n\t\t\tlist_del_init(&cmd_iocb->list);\n\t\t\tcmd_iocb->iocb_flag &= ~LPFC_IO_ON_TXCMPLQ;\n\t\t\tpring->txcmplq_cnt--;\n\t\t\tspin_unlock_irqrestore(temp_lock, iflag);\n\t\t\treturn cmd_iocb;\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(temp_lock, iflag);\n\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"0317 iotag x%x is out of \"\n\t\t\t\"range: max iotag x%x wd0 x%x\\n\",\n\t\t\tiotag, phba->sli.last_iotag,\n\t\t\t*(((uint32_t *) &prspiocb->iocb) + 7));\n\treturn NULL;\n}\n\n/**\n * lpfc_sli_iocbq_lookup_by_tag - Find command iocb for the iotag\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n * @iotag: IOCB tag.\n *\n * This function looks up the iocb_lookup table to get the command iocb\n * corresponding to the given iotag. The driver calls this function with\n * the ring lock held because this function is an SLI4 port only helper.\n * This function returns the command iocb object if it finds the command\n * iocb else returns NULL.\n **/\nstatic struct lpfc_iocbq *\nlpfc_sli_iocbq_lookup_by_tag(struct lpfc_hba *phba,\n\t\t\t     struct lpfc_sli_ring *pring, uint16_t iotag)\n{\n\tstruct lpfc_iocbq *cmd_iocb = NULL;\n\tspinlock_t *temp_lock = NULL;\n\tunsigned long iflag = 0;\n\n\tif (phba->sli_rev == LPFC_SLI_REV4)\n\t\ttemp_lock = &pring->ring_lock;\n\telse\n\t\ttemp_lock = &phba->hbalock;\n\n\tspin_lock_irqsave(temp_lock, iflag);\n\tif (iotag != 0 && iotag <= phba->sli.last_iotag) {\n\t\tcmd_iocb = phba->sli.iocbq_lookup[iotag];\n\t\tif (cmd_iocb->iocb_flag & LPFC_IO_ON_TXCMPLQ) {\n\t\t\t/* remove from txcmpl queue list */\n\t\t\tlist_del_init(&cmd_iocb->list);\n\t\t\tcmd_iocb->iocb_flag &= ~LPFC_IO_ON_TXCMPLQ;\n\t\t\tpring->txcmplq_cnt--;\n\t\t\tspin_unlock_irqrestore(temp_lock, iflag);\n\t\t\treturn cmd_iocb;\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(temp_lock, iflag);\n\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"0372 iotag x%x lookup error: max iotag (x%x) \"\n\t\t\t\"iocb_flag x%x\\n\",\n\t\t\tiotag, phba->sli.last_iotag,\n\t\t\tcmd_iocb ? cmd_iocb->iocb_flag : 0xffff);\n\treturn NULL;\n}\n\n/**\n * lpfc_sli_process_sol_iocb - process solicited iocb completion\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n * @saveq: Pointer to the response iocb to be processed.\n *\n * This function is called by the ring event handler for non-fcp\n * rings when there is a new response iocb in the response ring.\n * The caller is not required to hold any locks. This function\n * gets the command iocb associated with the response iocb and\n * calls the completion handler for the command iocb. If there\n * is no completion handler, the function will free the resources\n * associated with command iocb. If the response iocb is for\n * an already aborted command iocb, the status of the completion\n * is changed to IOSTAT_LOCAL_REJECT/IOERR_SLI_ABORTED.\n * This function always returns 1.\n **/\nstatic int\nlpfc_sli_process_sol_iocb(struct lpfc_hba *phba, struct lpfc_sli_ring *pring,\n\t\t\t  struct lpfc_iocbq *saveq)\n{\n\tstruct lpfc_iocbq *cmdiocbp;\n\tint rc = 1;\n\tunsigned long iflag;\n\n\tcmdiocbp = lpfc_sli_iocbq_lookup(phba, pring, saveq);\n\tif (cmdiocbp) {\n\t\tif (cmdiocbp->iocb_cmpl) {\n\t\t\t/*\n\t\t\t * If an ELS command failed send an event to mgmt\n\t\t\t * application.\n\t\t\t */\n\t\t\tif (saveq->iocb.ulpStatus &&\n\t\t\t     (pring->ringno == LPFC_ELS_RING) &&\n\t\t\t     (cmdiocbp->iocb.ulpCommand ==\n\t\t\t\tCMD_ELS_REQUEST64_CR))\n\t\t\t\tlpfc_send_els_failure_event(phba,\n\t\t\t\t\tcmdiocbp, saveq);\n\n\t\t\t/*\n\t\t\t * Post all ELS completions to the worker thread.\n\t\t\t * All other are passed to the completion callback.\n\t\t\t */\n\t\t\tif (pring->ringno == LPFC_ELS_RING) {\n\t\t\t\tif ((phba->sli_rev < LPFC_SLI_REV4) &&\n\t\t\t\t    (cmdiocbp->iocb_flag &\n\t\t\t\t\t\t\tLPFC_DRIVER_ABORTED)) {\n\t\t\t\t\tspin_lock_irqsave(&phba->hbalock,\n\t\t\t\t\t\t\t  iflag);\n\t\t\t\t\tcmdiocbp->iocb_flag &=\n\t\t\t\t\t\t~LPFC_DRIVER_ABORTED;\n\t\t\t\t\tspin_unlock_irqrestore(&phba->hbalock,\n\t\t\t\t\t\t\t       iflag);\n\t\t\t\t\tsaveq->iocb.ulpStatus =\n\t\t\t\t\t\tIOSTAT_LOCAL_REJECT;\n\t\t\t\t\tsaveq->iocb.un.ulpWord[4] =\n\t\t\t\t\t\tIOERR_SLI_ABORTED;\n\n\t\t\t\t\t/* Firmware could still be in progress\n\t\t\t\t\t * of DMAing payload, so don't free data\n\t\t\t\t\t * buffer till after a hbeat.\n\t\t\t\t\t */\n\t\t\t\t\tspin_lock_irqsave(&phba->hbalock,\n\t\t\t\t\t\t\t  iflag);\n\t\t\t\t\tsaveq->iocb_flag |= LPFC_DELAY_MEM_FREE;\n\t\t\t\t\tspin_unlock_irqrestore(&phba->hbalock,\n\t\t\t\t\t\t\t       iflag);\n\t\t\t\t}\n\t\t\t\tif (phba->sli_rev == LPFC_SLI_REV4) {\n\t\t\t\t\tif (saveq->iocb_flag &\n\t\t\t\t\t    LPFC_EXCHANGE_BUSY) {\n\t\t\t\t\t\t/* Set cmdiocb flag for the\n\t\t\t\t\t\t * exchange busy so sgl (xri)\n\t\t\t\t\t\t * will not be released until\n\t\t\t\t\t\t * the abort xri is received\n\t\t\t\t\t\t * from hba.\n\t\t\t\t\t\t */\n\t\t\t\t\t\tspin_lock_irqsave(\n\t\t\t\t\t\t\t&phba->hbalock, iflag);\n\t\t\t\t\t\tcmdiocbp->iocb_flag |=\n\t\t\t\t\t\t\tLPFC_EXCHANGE_BUSY;\n\t\t\t\t\t\tspin_unlock_irqrestore(\n\t\t\t\t\t\t\t&phba->hbalock, iflag);\n\t\t\t\t\t}\n\t\t\t\t\tif (cmdiocbp->iocb_flag &\n\t\t\t\t\t    LPFC_DRIVER_ABORTED) {\n\t\t\t\t\t\t/*\n\t\t\t\t\t\t * Clear LPFC_DRIVER_ABORTED\n\t\t\t\t\t\t * bit in case it was driver\n\t\t\t\t\t\t * initiated abort.\n\t\t\t\t\t\t */\n\t\t\t\t\t\tspin_lock_irqsave(\n\t\t\t\t\t\t\t&phba->hbalock, iflag);\n\t\t\t\t\t\tcmdiocbp->iocb_flag &=\n\t\t\t\t\t\t\t~LPFC_DRIVER_ABORTED;\n\t\t\t\t\t\tspin_unlock_irqrestore(\n\t\t\t\t\t\t\t&phba->hbalock, iflag);\n\t\t\t\t\t\tcmdiocbp->iocb.ulpStatus =\n\t\t\t\t\t\t\tIOSTAT_LOCAL_REJECT;\n\t\t\t\t\t\tcmdiocbp->iocb.un.ulpWord[4] =\n\t\t\t\t\t\t\tIOERR_ABORT_REQUESTED;\n\t\t\t\t\t\t/*\n\t\t\t\t\t\t * For SLI4, irsiocb contains\n\t\t\t\t\t\t * NO_XRI in sli_xritag, it\n\t\t\t\t\t\t * shall not affect releasing\n\t\t\t\t\t\t * sgl (xri) process.\n\t\t\t\t\t\t */\n\t\t\t\t\t\tsaveq->iocb.ulpStatus =\n\t\t\t\t\t\t\tIOSTAT_LOCAL_REJECT;\n\t\t\t\t\t\tsaveq->iocb.un.ulpWord[4] =\n\t\t\t\t\t\t\tIOERR_SLI_ABORTED;\n\t\t\t\t\t\tspin_lock_irqsave(\n\t\t\t\t\t\t\t&phba->hbalock, iflag);\n\t\t\t\t\t\tsaveq->iocb_flag |=\n\t\t\t\t\t\t\tLPFC_DELAY_MEM_FREE;\n\t\t\t\t\t\tspin_unlock_irqrestore(\n\t\t\t\t\t\t\t&phba->hbalock, iflag);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t(cmdiocbp->iocb_cmpl) (phba, cmdiocbp, saveq);\n\t\t} else\n\t\t\tlpfc_sli_release_iocbq(phba, cmdiocbp);\n\t} else {\n\t\t/*\n\t\t * Unknown initiating command based on the response iotag.\n\t\t * This could be the case on the ELS ring because of\n\t\t * lpfc_els_abort().\n\t\t */\n\t\tif (pring->ringno != LPFC_ELS_RING) {\n\t\t\t/*\n\t\t\t * Ring <ringno> handler: unexpected completion IoTag\n\t\t\t * <IoTag>\n\t\t\t */\n\t\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_SLI,\n\t\t\t\t\t \"0322 Ring %d handler: \"\n\t\t\t\t\t \"unexpected completion IoTag x%x \"\n\t\t\t\t\t \"Data: x%x x%x x%x x%x\\n\",\n\t\t\t\t\t pring->ringno,\n\t\t\t\t\t saveq->iocb.ulpIoTag,\n\t\t\t\t\t saveq->iocb.ulpStatus,\n\t\t\t\t\t saveq->iocb.un.ulpWord[4],\n\t\t\t\t\t saveq->iocb.ulpCommand,\n\t\t\t\t\t saveq->iocb.ulpContext);\n\t\t}\n\t}\n\n\treturn rc;\n}\n\n/**\n * lpfc_sli_rsp_pointers_error - Response ring pointer error handler\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n *\n * This function is called from the iocb ring event handlers when\n * put pointer is ahead of the get pointer for a ring. This function signal\n * an error attention condition to the worker thread and the worker\n * thread will transition the HBA to offline state.\n **/\nstatic void\nlpfc_sli_rsp_pointers_error(struct lpfc_hba *phba, struct lpfc_sli_ring *pring)\n{\n\tstruct lpfc_pgp *pgp = &phba->port_gp[pring->ringno];\n\t/*\n\t * Ring <ringno> handler: portRspPut <portRspPut> is bigger than\n\t * rsp ring <portRspMax>\n\t */\n\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"0312 Ring %d handler: portRspPut %d \"\n\t\t\t\"is bigger than rsp ring %d\\n\",\n\t\t\tpring->ringno, le32_to_cpu(pgp->rspPutInx),\n\t\t\tpring->sli.sli3.numRiocb);\n\n\tphba->link_state = LPFC_HBA_ERROR;\n\n\t/*\n\t * All error attention handlers are posted to\n\t * worker thread\n\t */\n\tphba->work_ha |= HA_ERATT;\n\tphba->work_hs = HS_FFER3;\n\n\tlpfc_worker_wake_up(phba);\n\n\treturn;\n}\n\n/**\n * lpfc_poll_eratt - Error attention polling timer timeout handler\n * @t: Context to fetch pointer to address of HBA context object from.\n *\n * This function is invoked by the Error Attention polling timer when the\n * timer times out. It will check the SLI Error Attention register for\n * possible attention events. If so, it will post an Error Attention event\n * and wake up worker thread to process it. Otherwise, it will set up the\n * Error Attention polling timer for the next poll.\n **/\nvoid lpfc_poll_eratt(struct timer_list *t)\n{\n\tstruct lpfc_hba *phba;\n\tuint32_t eratt = 0;\n\tuint64_t sli_intr, cnt;\n\n\tphba = from_timer(phba, t, eratt_poll);\n\n\t/* Here we will also keep track of interrupts per sec of the hba */\n\tsli_intr = phba->sli.slistat.sli_intr;\n\n\tif (phba->sli.slistat.sli_prev_intr > sli_intr)\n\t\tcnt = (((uint64_t)(-1) - phba->sli.slistat.sli_prev_intr) +\n\t\t\tsli_intr);\n\telse\n\t\tcnt = (sli_intr - phba->sli.slistat.sli_prev_intr);\n\n\t/* 64-bit integer division not supported on 32-bit x86 - use do_div */\n\tdo_div(cnt, phba->eratt_poll_interval);\n\tphba->sli.slistat.sli_ips = cnt;\n\n\tphba->sli.slistat.sli_prev_intr = sli_intr;\n\n\t/* Check chip HA register for error event */\n\teratt = lpfc_sli_check_eratt(phba);\n\n\tif (eratt)\n\t\t/* Tell the worker thread there is work to do */\n\t\tlpfc_worker_wake_up(phba);\n\telse\n\t\t/* Restart the timer for next eratt poll */\n\t\tmod_timer(&phba->eratt_poll,\n\t\t\t  jiffies +\n\t\t\t  msecs_to_jiffies(1000 * phba->eratt_poll_interval));\n\treturn;\n}\n\n\n/**\n * lpfc_sli_handle_fast_ring_event - Handle ring events on FCP ring\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n * @mask: Host attention register mask for this ring.\n *\n * This function is called from the interrupt context when there is a ring\n * event for the fcp ring. The caller does not hold any lock.\n * The function processes each response iocb in the response ring until it\n * finds an iocb with LE bit set and chains all the iocbs up to the iocb with\n * LE bit set. The function will call the completion handler of the command iocb\n * if the response iocb indicates a completion for a command iocb or it is\n * an abort completion. The function will call lpfc_sli_process_unsol_iocb\n * function if this is an unsolicited iocb.\n * This routine presumes LPFC_FCP_RING handling and doesn't bother\n * to check it explicitly.\n */\nint\nlpfc_sli_handle_fast_ring_event(struct lpfc_hba *phba,\n\t\t\t\tstruct lpfc_sli_ring *pring, uint32_t mask)\n{\n\tstruct lpfc_pgp *pgp = &phba->port_gp[pring->ringno];\n\tIOCB_t *irsp = NULL;\n\tIOCB_t *entry = NULL;\n\tstruct lpfc_iocbq *cmdiocbq = NULL;\n\tstruct lpfc_iocbq rspiocbq;\n\tuint32_t status;\n\tuint32_t portRspPut, portRspMax;\n\tint rc = 1;\n\tlpfc_iocb_type type;\n\tunsigned long iflag;\n\tuint32_t rsp_cmpl = 0;\n\n\tspin_lock_irqsave(&phba->hbalock, iflag);\n\tpring->stats.iocb_event++;\n\n\t/*\n\t * The next available response entry should never exceed the maximum\n\t * entries.  If it does, treat it as an adapter hardware error.\n\t */\n\tportRspMax = pring->sli.sli3.numRiocb;\n\tportRspPut = le32_to_cpu(pgp->rspPutInx);\n\tif (unlikely(portRspPut >= portRspMax)) {\n\t\tlpfc_sli_rsp_pointers_error(phba, pring);\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\treturn 1;\n\t}\n\tif (phba->fcp_ring_in_use) {\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\treturn 1;\n\t} else\n\t\tphba->fcp_ring_in_use = 1;\n\n\trmb();\n\twhile (pring->sli.sli3.rspidx != portRspPut) {\n\t\t/*\n\t\t * Fetch an entry off the ring and copy it into a local data\n\t\t * structure.  The copy involves a byte-swap since the\n\t\t * network byte order and pci byte orders are different.\n\t\t */\n\t\tentry = lpfc_resp_iocb(phba, pring);\n\t\tphba->last_completion_time = jiffies;\n\n\t\tif (++pring->sli.sli3.rspidx >= portRspMax)\n\t\t\tpring->sli.sli3.rspidx = 0;\n\n\t\tlpfc_sli_pcimem_bcopy((uint32_t *) entry,\n\t\t\t\t      (uint32_t *) &rspiocbq.iocb,\n\t\t\t\t      phba->iocb_rsp_size);\n\t\tINIT_LIST_HEAD(&(rspiocbq.list));\n\t\tirsp = &rspiocbq.iocb;\n\n\t\ttype = lpfc_sli_iocb_cmd_type(irsp->ulpCommand & CMD_IOCB_MASK);\n\t\tpring->stats.iocb_rsp++;\n\t\trsp_cmpl++;\n\n\t\tif (unlikely(irsp->ulpStatus)) {\n\t\t\t/*\n\t\t\t * If resource errors reported from HBA, reduce\n\t\t\t * queuedepths of the SCSI device.\n\t\t\t */\n\t\t\tif ((irsp->ulpStatus == IOSTAT_LOCAL_REJECT) &&\n\t\t\t    ((irsp->un.ulpWord[4] & IOERR_PARAM_MASK) ==\n\t\t\t     IOERR_NO_RESOURCES)) {\n\t\t\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\t\t\tphba->lpfc_rampdown_queue_depth(phba);\n\t\t\t\tspin_lock_irqsave(&phba->hbalock, iflag);\n\t\t\t}\n\n\t\t\t/* Rsp ring <ringno> error: IOCB */\n\t\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_SLI,\n\t\t\t\t\t\"0336 Rsp Ring %d error: IOCB Data: \"\n\t\t\t\t\t\"x%x x%x x%x x%x x%x x%x x%x x%x\\n\",\n\t\t\t\t\tpring->ringno,\n\t\t\t\t\tirsp->un.ulpWord[0],\n\t\t\t\t\tirsp->un.ulpWord[1],\n\t\t\t\t\tirsp->un.ulpWord[2],\n\t\t\t\t\tirsp->un.ulpWord[3],\n\t\t\t\t\tirsp->un.ulpWord[4],\n\t\t\t\t\tirsp->un.ulpWord[5],\n\t\t\t\t\t*(uint32_t *)&irsp->un1,\n\t\t\t\t\t*((uint32_t *)&irsp->un1 + 1));\n\t\t}\n\n\t\tswitch (type) {\n\t\tcase LPFC_ABORT_IOCB:\n\t\tcase LPFC_SOL_IOCB:\n\t\t\t/*\n\t\t\t * Idle exchange closed via ABTS from port.  No iocb\n\t\t\t * resources need to be recovered.\n\t\t\t */\n\t\t\tif (unlikely(irsp->ulpCommand == CMD_XRI_ABORTED_CX)) {\n\t\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\t\t\t\"0333 IOCB cmd 0x%x\"\n\t\t\t\t\t\t\" processed. Skipping\"\n\t\t\t\t\t\t\" completion\\n\",\n\t\t\t\t\t\tirsp->ulpCommand);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\t\tcmdiocbq = lpfc_sli_iocbq_lookup(phba, pring,\n\t\t\t\t\t\t\t &rspiocbq);\n\t\t\tspin_lock_irqsave(&phba->hbalock, iflag);\n\t\t\tif (unlikely(!cmdiocbq))\n\t\t\t\tbreak;\n\t\t\tif (cmdiocbq->iocb_flag & LPFC_DRIVER_ABORTED)\n\t\t\t\tcmdiocbq->iocb_flag &= ~LPFC_DRIVER_ABORTED;\n\t\t\tif (cmdiocbq->iocb_cmpl) {\n\t\t\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\t\t\t(cmdiocbq->iocb_cmpl)(phba, cmdiocbq,\n\t\t\t\t\t\t      &rspiocbq);\n\t\t\t\tspin_lock_irqsave(&phba->hbalock, iflag);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase LPFC_UNSOL_IOCB:\n\t\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\t\tlpfc_sli_process_unsol_iocb(phba, pring, &rspiocbq);\n\t\t\tspin_lock_irqsave(&phba->hbalock, iflag);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (irsp->ulpCommand == CMD_ADAPTER_MSG) {\n\t\t\t\tchar adaptermsg[LPFC_MAX_ADPTMSG];\n\t\t\t\tmemset(adaptermsg, 0, LPFC_MAX_ADPTMSG);\n\t\t\t\tmemcpy(&adaptermsg[0], (uint8_t *) irsp,\n\t\t\t\t       MAX_MSG_DATA);\n\t\t\t\tdev_warn(&((phba->pcidev)->dev),\n\t\t\t\t\t \"lpfc%d: %s\\n\",\n\t\t\t\t\t phba->brd_no, adaptermsg);\n\t\t\t} else {\n\t\t\t\t/* Unknown IOCB command */\n\t\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\t\"0334 Unknown IOCB command \"\n\t\t\t\t\t\t\"Data: x%x, x%x x%x x%x x%x\\n\",\n\t\t\t\t\t\ttype, irsp->ulpCommand,\n\t\t\t\t\t\tirsp->ulpStatus,\n\t\t\t\t\t\tirsp->ulpIoTag,\n\t\t\t\t\t\tirsp->ulpContext);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * The response IOCB has been processed.  Update the ring\n\t\t * pointer in SLIM.  If the port response put pointer has not\n\t\t * been updated, sync the pgp->rspPutInx and fetch the new port\n\t\t * response put pointer.\n\t\t */\n\t\twritel(pring->sli.sli3.rspidx,\n\t\t\t&phba->host_gp[pring->ringno].rspGetInx);\n\n\t\tif (pring->sli.sli3.rspidx == portRspPut)\n\t\t\tportRspPut = le32_to_cpu(pgp->rspPutInx);\n\t}\n\n\tif ((rsp_cmpl > 0) && (mask & HA_R0RE_REQ)) {\n\t\tpring->stats.iocb_rsp_full++;\n\t\tstatus = ((CA_R0ATT | CA_R0RE_RSP) << (pring->ringno * 4));\n\t\twritel(status, phba->CAregaddr);\n\t\treadl(phba->CAregaddr);\n\t}\n\tif ((mask & HA_R0CE_RSP) && (pring->flag & LPFC_CALL_RING_AVAILABLE)) {\n\t\tpring->flag &= ~LPFC_CALL_RING_AVAILABLE;\n\t\tpring->stats.iocb_cmd_empty++;\n\n\t\t/* Force update of the local copy of cmdGetInx */\n\t\tpring->sli.sli3.local_getidx = le32_to_cpu(pgp->cmdGetInx);\n\t\tlpfc_sli_resume_iocb(phba, pring);\n\n\t\tif ((pring->lpfc_sli_cmd_available))\n\t\t\t(pring->lpfc_sli_cmd_available) (phba, pring);\n\n\t}\n\n\tphba->fcp_ring_in_use = 0;\n\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\treturn rc;\n}\n\n/**\n * lpfc_sli_sp_handle_rspiocb - Handle slow-path response iocb\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n * @rspiocbp: Pointer to driver response IOCB object.\n *\n * This function is called from the worker thread when there is a slow-path\n * response IOCB to process. This function chains all the response iocbs until\n * seeing the iocb with the LE bit set. The function will call\n * lpfc_sli_process_sol_iocb function if the response iocb indicates a\n * completion of a command iocb. The function will call the\n * lpfc_sli_process_unsol_iocb function if this is an unsolicited iocb.\n * The function frees the resources or calls the completion handler if this\n * iocb is an abort completion. The function returns NULL when the response\n * iocb has the LE bit set and all the chained iocbs are processed, otherwise\n * this function shall chain the iocb on to the iocb_continueq and return the\n * response iocb passed in.\n **/\nstatic struct lpfc_iocbq *\nlpfc_sli_sp_handle_rspiocb(struct lpfc_hba *phba, struct lpfc_sli_ring *pring,\n\t\t\tstruct lpfc_iocbq *rspiocbp)\n{\n\tstruct lpfc_iocbq *saveq;\n\tstruct lpfc_iocbq *cmdiocbp;\n\tstruct lpfc_iocbq *next_iocb;\n\tIOCB_t *irsp = NULL;\n\tuint32_t free_saveq;\n\tuint8_t iocb_cmd_type;\n\tlpfc_iocb_type type;\n\tunsigned long iflag;\n\tint rc;\n\n\tspin_lock_irqsave(&phba->hbalock, iflag);\n\t/* First add the response iocb to the countinueq list */\n\tlist_add_tail(&rspiocbp->list, &(pring->iocb_continueq));\n\tpring->iocb_continueq_cnt++;\n\n\t/* Now, determine whether the list is completed for processing */\n\tirsp = &rspiocbp->iocb;\n\tif (irsp->ulpLe) {\n\t\t/*\n\t\t * By default, the driver expects to free all resources\n\t\t * associated with this iocb completion.\n\t\t */\n\t\tfree_saveq = 1;\n\t\tsaveq = list_get_first(&pring->iocb_continueq,\n\t\t\t\t       struct lpfc_iocbq, list);\n\t\tirsp = &(saveq->iocb);\n\t\tlist_del_init(&pring->iocb_continueq);\n\t\tpring->iocb_continueq_cnt = 0;\n\n\t\tpring->stats.iocb_rsp++;\n\n\t\t/*\n\t\t * If resource errors reported from HBA, reduce\n\t\t * queuedepths of the SCSI device.\n\t\t */\n\t\tif ((irsp->ulpStatus == IOSTAT_LOCAL_REJECT) &&\n\t\t    ((irsp->un.ulpWord[4] & IOERR_PARAM_MASK) ==\n\t\t     IOERR_NO_RESOURCES)) {\n\t\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\t\tphba->lpfc_rampdown_queue_depth(phba);\n\t\t\tspin_lock_irqsave(&phba->hbalock, iflag);\n\t\t}\n\n\t\tif (irsp->ulpStatus) {\n\t\t\t/* Rsp ring <ringno> error: IOCB */\n\t\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_SLI,\n\t\t\t\t\t\"0328 Rsp Ring %d error: \"\n\t\t\t\t\t\"IOCB Data: \"\n\t\t\t\t\t\"x%x x%x x%x x%x \"\n\t\t\t\t\t\"x%x x%x x%x x%x \"\n\t\t\t\t\t\"x%x x%x x%x x%x \"\n\t\t\t\t\t\"x%x x%x x%x x%x\\n\",\n\t\t\t\t\tpring->ringno,\n\t\t\t\t\tirsp->un.ulpWord[0],\n\t\t\t\t\tirsp->un.ulpWord[1],\n\t\t\t\t\tirsp->un.ulpWord[2],\n\t\t\t\t\tirsp->un.ulpWord[3],\n\t\t\t\t\tirsp->un.ulpWord[4],\n\t\t\t\t\tirsp->un.ulpWord[5],\n\t\t\t\t\t*(((uint32_t *) irsp) + 6),\n\t\t\t\t\t*(((uint32_t *) irsp) + 7),\n\t\t\t\t\t*(((uint32_t *) irsp) + 8),\n\t\t\t\t\t*(((uint32_t *) irsp) + 9),\n\t\t\t\t\t*(((uint32_t *) irsp) + 10),\n\t\t\t\t\t*(((uint32_t *) irsp) + 11),\n\t\t\t\t\t*(((uint32_t *) irsp) + 12),\n\t\t\t\t\t*(((uint32_t *) irsp) + 13),\n\t\t\t\t\t*(((uint32_t *) irsp) + 14),\n\t\t\t\t\t*(((uint32_t *) irsp) + 15));\n\t\t}\n\n\t\t/*\n\t\t * Fetch the IOCB command type and call the correct completion\n\t\t * routine. Solicited and Unsolicited IOCBs on the ELS ring\n\t\t * get freed back to the lpfc_iocb_list by the discovery\n\t\t * kernel thread.\n\t\t */\n\t\tiocb_cmd_type = irsp->ulpCommand & CMD_IOCB_MASK;\n\t\ttype = lpfc_sli_iocb_cmd_type(iocb_cmd_type);\n\t\tswitch (type) {\n\t\tcase LPFC_SOL_IOCB:\n\t\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\t\trc = lpfc_sli_process_sol_iocb(phba, pring, saveq);\n\t\t\tspin_lock_irqsave(&phba->hbalock, iflag);\n\t\t\tbreak;\n\n\t\tcase LPFC_UNSOL_IOCB:\n\t\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\t\trc = lpfc_sli_process_unsol_iocb(phba, pring, saveq);\n\t\t\tspin_lock_irqsave(&phba->hbalock, iflag);\n\t\t\tif (!rc)\n\t\t\t\tfree_saveq = 0;\n\t\t\tbreak;\n\n\t\tcase LPFC_ABORT_IOCB:\n\t\t\tcmdiocbp = NULL;\n\t\t\tif (irsp->ulpCommand != CMD_XRI_ABORTED_CX) {\n\t\t\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\t\t\tcmdiocbp = lpfc_sli_iocbq_lookup(phba, pring,\n\t\t\t\t\t\t\t\t saveq);\n\t\t\t\tspin_lock_irqsave(&phba->hbalock, iflag);\n\t\t\t}\n\t\t\tif (cmdiocbp) {\n\t\t\t\t/* Call the specified completion routine */\n\t\t\t\tif (cmdiocbp->iocb_cmpl) {\n\t\t\t\t\tspin_unlock_irqrestore(&phba->hbalock,\n\t\t\t\t\t\t\t       iflag);\n\t\t\t\t\t(cmdiocbp->iocb_cmpl)(phba, cmdiocbp,\n\t\t\t\t\t\t\t      saveq);\n\t\t\t\t\tspin_lock_irqsave(&phba->hbalock,\n\t\t\t\t\t\t\t  iflag);\n\t\t\t\t} else\n\t\t\t\t\t__lpfc_sli_release_iocbq(phba,\n\t\t\t\t\t\t\t\t cmdiocbp);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase LPFC_UNKNOWN_IOCB:\n\t\t\tif (irsp->ulpCommand == CMD_ADAPTER_MSG) {\n\t\t\t\tchar adaptermsg[LPFC_MAX_ADPTMSG];\n\t\t\t\tmemset(adaptermsg, 0, LPFC_MAX_ADPTMSG);\n\t\t\t\tmemcpy(&adaptermsg[0], (uint8_t *)irsp,\n\t\t\t\t       MAX_MSG_DATA);\n\t\t\t\tdev_warn(&((phba->pcidev)->dev),\n\t\t\t\t\t \"lpfc%d: %s\\n\",\n\t\t\t\t\t phba->brd_no, adaptermsg);\n\t\t\t} else {\n\t\t\t\t/* Unknown IOCB command */\n\t\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\t\"0335 Unknown IOCB \"\n\t\t\t\t\t\t\"command Data: x%x \"\n\t\t\t\t\t\t\"x%x x%x x%x\\n\",\n\t\t\t\t\t\tirsp->ulpCommand,\n\t\t\t\t\t\tirsp->ulpStatus,\n\t\t\t\t\t\tirsp->ulpIoTag,\n\t\t\t\t\t\tirsp->ulpContext);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tif (free_saveq) {\n\t\t\tlist_for_each_entry_safe(rspiocbp, next_iocb,\n\t\t\t\t\t\t &saveq->list, list) {\n\t\t\t\tlist_del_init(&rspiocbp->list);\n\t\t\t\t__lpfc_sli_release_iocbq(phba, rspiocbp);\n\t\t\t}\n\t\t\t__lpfc_sli_release_iocbq(phba, saveq);\n\t\t}\n\t\trspiocbp = NULL;\n\t}\n\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\treturn rspiocbp;\n}\n\n/**\n * lpfc_sli_handle_slow_ring_event - Wrapper func for handling slow-path iocbs\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n * @mask: Host attention register mask for this ring.\n *\n * This routine wraps the actual slow_ring event process routine from the\n * API jump table function pointer from the lpfc_hba struct.\n **/\nvoid\nlpfc_sli_handle_slow_ring_event(struct lpfc_hba *phba,\n\t\t\t\tstruct lpfc_sli_ring *pring, uint32_t mask)\n{\n\tphba->lpfc_sli_handle_slow_ring_event(phba, pring, mask);\n}\n\n/**\n * lpfc_sli_handle_slow_ring_event_s3 - Handle SLI3 ring event for non-FCP rings\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n * @mask: Host attention register mask for this ring.\n *\n * This function is called from the worker thread when there is a ring event\n * for non-fcp rings. The caller does not hold any lock. The function will\n * remove each response iocb in the response ring and calls the handle\n * response iocb routine (lpfc_sli_sp_handle_rspiocb) to process it.\n **/\nstatic void\nlpfc_sli_handle_slow_ring_event_s3(struct lpfc_hba *phba,\n\t\t\t\t   struct lpfc_sli_ring *pring, uint32_t mask)\n{\n\tstruct lpfc_pgp *pgp;\n\tIOCB_t *entry;\n\tIOCB_t *irsp = NULL;\n\tstruct lpfc_iocbq *rspiocbp = NULL;\n\tuint32_t portRspPut, portRspMax;\n\tunsigned long iflag;\n\tuint32_t status;\n\n\tpgp = &phba->port_gp[pring->ringno];\n\tspin_lock_irqsave(&phba->hbalock, iflag);\n\tpring->stats.iocb_event++;\n\n\t/*\n\t * The next available response entry should never exceed the maximum\n\t * entries.  If it does, treat it as an adapter hardware error.\n\t */\n\tportRspMax = pring->sli.sli3.numRiocb;\n\tportRspPut = le32_to_cpu(pgp->rspPutInx);\n\tif (portRspPut >= portRspMax) {\n\t\t/*\n\t\t * Ring <ringno> handler: portRspPut <portRspPut> is bigger than\n\t\t * rsp ring <portRspMax>\n\t\t */\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0303 Ring %d handler: portRspPut %d \"\n\t\t\t\t\"is bigger than rsp ring %d\\n\",\n\t\t\t\tpring->ringno, portRspPut, portRspMax);\n\n\t\tphba->link_state = LPFC_HBA_ERROR;\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\n\t\tphba->work_hs = HS_FFER3;\n\t\tlpfc_handle_eratt(phba);\n\n\t\treturn;\n\t}\n\n\trmb();\n\twhile (pring->sli.sli3.rspidx != portRspPut) {\n\t\t/*\n\t\t * Build a completion list and call the appropriate handler.\n\t\t * The process is to get the next available response iocb, get\n\t\t * a free iocb from the list, copy the response data into the\n\t\t * free iocb, insert to the continuation list, and update the\n\t\t * next response index to slim.  This process makes response\n\t\t * iocb's in the ring available to DMA as fast as possible but\n\t\t * pays a penalty for a copy operation.  Since the iocb is\n\t\t * only 32 bytes, this penalty is considered small relative to\n\t\t * the PCI reads for register values and a slim write.  When\n\t\t * the ulpLe field is set, the entire Command has been\n\t\t * received.\n\t\t */\n\t\tentry = lpfc_resp_iocb(phba, pring);\n\n\t\tphba->last_completion_time = jiffies;\n\t\trspiocbp = __lpfc_sli_get_iocbq(phba);\n\t\tif (rspiocbp == NULL) {\n\t\t\tprintk(KERN_ERR \"%s: out of buffers! Failing \"\n\t\t\t       \"completion.\\n\", __func__);\n\t\t\tbreak;\n\t\t}\n\n\t\tlpfc_sli_pcimem_bcopy(entry, &rspiocbp->iocb,\n\t\t\t\t      phba->iocb_rsp_size);\n\t\tirsp = &rspiocbp->iocb;\n\n\t\tif (++pring->sli.sli3.rspidx >= portRspMax)\n\t\t\tpring->sli.sli3.rspidx = 0;\n\n\t\tif (pring->ringno == LPFC_ELS_RING) {\n\t\t\tlpfc_debugfs_slow_ring_trc(phba,\n\t\t\t\"IOCB rsp ring:   wd4:x%08x wd6:x%08x wd7:x%08x\",\n\t\t\t\t*(((uint32_t *) irsp) + 4),\n\t\t\t\t*(((uint32_t *) irsp) + 6),\n\t\t\t\t*(((uint32_t *) irsp) + 7));\n\t\t}\n\n\t\twritel(pring->sli.sli3.rspidx,\n\t\t\t&phba->host_gp[pring->ringno].rspGetInx);\n\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\t/* Handle the response IOCB */\n\t\trspiocbp = lpfc_sli_sp_handle_rspiocb(phba, pring, rspiocbp);\n\t\tspin_lock_irqsave(&phba->hbalock, iflag);\n\n\t\t/*\n\t\t * If the port response put pointer has not been updated, sync\n\t\t * the pgp->rspPutInx in the MAILBOX_tand fetch the new port\n\t\t * response put pointer.\n\t\t */\n\t\tif (pring->sli.sli3.rspidx == portRspPut) {\n\t\t\tportRspPut = le32_to_cpu(pgp->rspPutInx);\n\t\t}\n\t} /* while (pring->sli.sli3.rspidx != portRspPut) */\n\n\tif ((rspiocbp != NULL) && (mask & HA_R0RE_REQ)) {\n\t\t/* At least one response entry has been freed */\n\t\tpring->stats.iocb_rsp_full++;\n\t\t/* SET RxRE_RSP in Chip Att register */\n\t\tstatus = ((CA_R0ATT | CA_R0RE_RSP) << (pring->ringno * 4));\n\t\twritel(status, phba->CAregaddr);\n\t\treadl(phba->CAregaddr); /* flush */\n\t}\n\tif ((mask & HA_R0CE_RSP) && (pring->flag & LPFC_CALL_RING_AVAILABLE)) {\n\t\tpring->flag &= ~LPFC_CALL_RING_AVAILABLE;\n\t\tpring->stats.iocb_cmd_empty++;\n\n\t\t/* Force update of the local copy of cmdGetInx */\n\t\tpring->sli.sli3.local_getidx = le32_to_cpu(pgp->cmdGetInx);\n\t\tlpfc_sli_resume_iocb(phba, pring);\n\n\t\tif ((pring->lpfc_sli_cmd_available))\n\t\t\t(pring->lpfc_sli_cmd_available) (phba, pring);\n\n\t}\n\n\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\treturn;\n}\n\n/**\n * lpfc_sli_handle_slow_ring_event_s4 - Handle SLI4 slow-path els events\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n * @mask: Host attention register mask for this ring.\n *\n * This function is called from the worker thread when there is a pending\n * ELS response iocb on the driver internal slow-path response iocb worker\n * queue. The caller does not hold any lock. The function will remove each\n * response iocb from the response worker queue and calls the handle\n * response iocb routine (lpfc_sli_sp_handle_rspiocb) to process it.\n **/\nstatic void\nlpfc_sli_handle_slow_ring_event_s4(struct lpfc_hba *phba,\n\t\t\t\t   struct lpfc_sli_ring *pring, uint32_t mask)\n{\n\tstruct lpfc_iocbq *irspiocbq;\n\tstruct hbq_dmabuf *dmabuf;\n\tstruct lpfc_cq_event *cq_event;\n\tunsigned long iflag;\n\tint count = 0;\n\n\tspin_lock_irqsave(&phba->hbalock, iflag);\n\tphba->hba_flag &= ~HBA_SP_QUEUE_EVT;\n\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\twhile (!list_empty(&phba->sli4_hba.sp_queue_event)) {\n\t\t/* Get the response iocb from the head of work queue */\n\t\tspin_lock_irqsave(&phba->hbalock, iflag);\n\t\tlist_remove_head(&phba->sli4_hba.sp_queue_event,\n\t\t\t\t cq_event, struct lpfc_cq_event, list);\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\n\t\tswitch (bf_get(lpfc_wcqe_c_code, &cq_event->cqe.wcqe_cmpl)) {\n\t\tcase CQE_CODE_COMPL_WQE:\n\t\t\tirspiocbq = container_of(cq_event, struct lpfc_iocbq,\n\t\t\t\t\t\t cq_event);\n\t\t\t/* Translate ELS WCQE to response IOCBQ */\n\t\t\tirspiocbq = lpfc_sli4_els_wcqe_to_rspiocbq(phba,\n\t\t\t\t\t\t\t\t   irspiocbq);\n\t\t\tif (irspiocbq)\n\t\t\t\tlpfc_sli_sp_handle_rspiocb(phba, pring,\n\t\t\t\t\t\t\t   irspiocbq);\n\t\t\tcount++;\n\t\t\tbreak;\n\t\tcase CQE_CODE_RECEIVE:\n\t\tcase CQE_CODE_RECEIVE_V1:\n\t\t\tdmabuf = container_of(cq_event, struct hbq_dmabuf,\n\t\t\t\t\t      cq_event);\n\t\t\tlpfc_sli4_handle_received_buffer(phba, dmabuf);\n\t\t\tcount++;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Limit the number of events to 64 to avoid soft lockups */\n\t\tif (count == 64)\n\t\t\tbreak;\n\t}\n}\n\n/**\n * lpfc_sli_abort_iocb_ring - Abort all iocbs in the ring\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n *\n * This function aborts all iocbs in the given ring and frees all the iocb\n * objects in txq. This function issues an abort iocb for all the iocb commands\n * in txcmplq. The iocbs in the txcmplq is not guaranteed to complete before\n * the return of this function. The caller is not required to hold any locks.\n **/\nvoid\nlpfc_sli_abort_iocb_ring(struct lpfc_hba *phba, struct lpfc_sli_ring *pring)\n{\n\tLIST_HEAD(completions);\n\tstruct lpfc_iocbq *iocb, *next_iocb;\n\n\tif (pring->ringno == LPFC_ELS_RING) {\n\t\tlpfc_fabric_abort_hba(phba);\n\t}\n\n\t/* Error everything on txq and txcmplq\n\t * First do the txq.\n\t */\n\tif (phba->sli_rev >= LPFC_SLI_REV4) {\n\t\tspin_lock_irq(&pring->ring_lock);\n\t\tlist_splice_init(&pring->txq, &completions);\n\t\tpring->txq_cnt = 0;\n\t\tspin_unlock_irq(&pring->ring_lock);\n\n\t\tspin_lock_irq(&phba->hbalock);\n\t\t/* Next issue ABTS for everything on the txcmplq */\n\t\tlist_for_each_entry_safe(iocb, next_iocb, &pring->txcmplq, list)\n\t\t\tlpfc_sli_issue_abort_iotag(phba, pring, iocb);\n\t\tspin_unlock_irq(&phba->hbalock);\n\t} else {\n\t\tspin_lock_irq(&phba->hbalock);\n\t\tlist_splice_init(&pring->txq, &completions);\n\t\tpring->txq_cnt = 0;\n\n\t\t/* Next issue ABTS for everything on the txcmplq */\n\t\tlist_for_each_entry_safe(iocb, next_iocb, &pring->txcmplq, list)\n\t\t\tlpfc_sli_issue_abort_iotag(phba, pring, iocb);\n\t\tspin_unlock_irq(&phba->hbalock);\n\t}\n\n\t/* Cancel all the IOCBs from the completions list */\n\tlpfc_sli_cancel_iocbs(phba, &completions, IOSTAT_LOCAL_REJECT,\n\t\t\t      IOERR_SLI_ABORTED);\n}\n\n/**\n * lpfc_sli_abort_fcp_rings - Abort all iocbs in all FCP rings\n * @phba: Pointer to HBA context object.\n *\n * This function aborts all iocbs in FCP rings and frees all the iocb\n * objects in txq. This function issues an abort iocb for all the iocb commands\n * in txcmplq. The iocbs in the txcmplq is not guaranteed to complete before\n * the return of this function. The caller is not required to hold any locks.\n **/\nvoid\nlpfc_sli_abort_fcp_rings(struct lpfc_hba *phba)\n{\n\tstruct lpfc_sli *psli = &phba->sli;\n\tstruct lpfc_sli_ring  *pring;\n\tuint32_t i;\n\n\t/* Look on all the FCP Rings for the iotag */\n\tif (phba->sli_rev >= LPFC_SLI_REV4) {\n\t\tfor (i = 0; i < phba->cfg_hdw_queue; i++) {\n\t\t\tpring = phba->sli4_hba.hdwq[i].io_wq->pring;\n\t\t\tlpfc_sli_abort_iocb_ring(phba, pring);\n\t\t}\n\t} else {\n\t\tpring = &psli->sli3_ring[LPFC_FCP_RING];\n\t\tlpfc_sli_abort_iocb_ring(phba, pring);\n\t}\n}\n\n/**\n * lpfc_sli_flush_io_rings - flush all iocbs in the IO ring\n * @phba: Pointer to HBA context object.\n *\n * This function flushes all iocbs in the IO ring and frees all the iocb\n * objects in txq and txcmplq. This function will not issue abort iocbs\n * for all the iocb commands in txcmplq, they will just be returned with\n * IOERR_SLI_DOWN. This function is invoked with EEH when device's PCI\n * slot has been permanently disabled.\n **/\nvoid\nlpfc_sli_flush_io_rings(struct lpfc_hba *phba)\n{\n\tLIST_HEAD(txq);\n\tLIST_HEAD(txcmplq);\n\tstruct lpfc_sli *psli = &phba->sli;\n\tstruct lpfc_sli_ring  *pring;\n\tuint32_t i;\n\tstruct lpfc_iocbq *piocb, *next_iocb;\n\n\tspin_lock_irq(&phba->hbalock);\n\tif (phba->hba_flag & HBA_IOQ_FLUSH ||\n\t    !phba->sli4_hba.hdwq) {\n\t\tspin_unlock_irq(&phba->hbalock);\n\t\treturn;\n\t}\n\t/* Indicate the I/O queues are flushed */\n\tphba->hba_flag |= HBA_IOQ_FLUSH;\n\tspin_unlock_irq(&phba->hbalock);\n\n\t/* Look on all the FCP Rings for the iotag */\n\tif (phba->sli_rev >= LPFC_SLI_REV4) {\n\t\tfor (i = 0; i < phba->cfg_hdw_queue; i++) {\n\t\t\tpring = phba->sli4_hba.hdwq[i].io_wq->pring;\n\n\t\t\tspin_lock_irq(&pring->ring_lock);\n\t\t\t/* Retrieve everything on txq */\n\t\t\tlist_splice_init(&pring->txq, &txq);\n\t\t\tlist_for_each_entry_safe(piocb, next_iocb,\n\t\t\t\t\t\t &pring->txcmplq, list)\n\t\t\t\tpiocb->iocb_flag &= ~LPFC_IO_ON_TXCMPLQ;\n\t\t\t/* Retrieve everything on the txcmplq */\n\t\t\tlist_splice_init(&pring->txcmplq, &txcmplq);\n\t\t\tpring->txq_cnt = 0;\n\t\t\tpring->txcmplq_cnt = 0;\n\t\t\tspin_unlock_irq(&pring->ring_lock);\n\n\t\t\t/* Flush the txq */\n\t\t\tlpfc_sli_cancel_iocbs(phba, &txq,\n\t\t\t\t\t      IOSTAT_LOCAL_REJECT,\n\t\t\t\t\t      IOERR_SLI_DOWN);\n\t\t\t/* Flush the txcmpq */\n\t\t\tlpfc_sli_cancel_iocbs(phba, &txcmplq,\n\t\t\t\t\t      IOSTAT_LOCAL_REJECT,\n\t\t\t\t\t      IOERR_SLI_DOWN);\n\t\t}\n\t} else {\n\t\tpring = &psli->sli3_ring[LPFC_FCP_RING];\n\n\t\tspin_lock_irq(&phba->hbalock);\n\t\t/* Retrieve everything on txq */\n\t\tlist_splice_init(&pring->txq, &txq);\n\t\tlist_for_each_entry_safe(piocb, next_iocb,\n\t\t\t\t\t &pring->txcmplq, list)\n\t\t\tpiocb->iocb_flag &= ~LPFC_IO_ON_TXCMPLQ;\n\t\t/* Retrieve everything on the txcmplq */\n\t\tlist_splice_init(&pring->txcmplq, &txcmplq);\n\t\tpring->txq_cnt = 0;\n\t\tpring->txcmplq_cnt = 0;\n\t\tspin_unlock_irq(&phba->hbalock);\n\n\t\t/* Flush the txq */\n\t\tlpfc_sli_cancel_iocbs(phba, &txq, IOSTAT_LOCAL_REJECT,\n\t\t\t\t      IOERR_SLI_DOWN);\n\t\t/* Flush the txcmpq */\n\t\tlpfc_sli_cancel_iocbs(phba, &txcmplq, IOSTAT_LOCAL_REJECT,\n\t\t\t\t      IOERR_SLI_DOWN);\n\t}\n}\n\n/**\n * lpfc_sli_brdready_s3 - Check for sli3 host ready status\n * @phba: Pointer to HBA context object.\n * @mask: Bit mask to be checked.\n *\n * This function reads the host status register and compares\n * with the provided bit mask to check if HBA completed\n * the restart. This function will wait in a loop for the\n * HBA to complete restart. If the HBA does not restart within\n * 15 iterations, the function will reset the HBA again. The\n * function returns 1 when HBA fail to restart otherwise returns\n * zero.\n **/\nstatic int\nlpfc_sli_brdready_s3(struct lpfc_hba *phba, uint32_t mask)\n{\n\tuint32_t status;\n\tint i = 0;\n\tint retval = 0;\n\n\t/* Read the HBA Host Status Register */\n\tif (lpfc_readl(phba->HSregaddr, &status))\n\t\treturn 1;\n\n\t/*\n\t * Check status register every 100ms for 5 retries, then every\n\t * 500ms for 5, then every 2.5 sec for 5, then reset board and\n\t * every 2.5 sec for 4.\n\t * Break our of the loop if errors occurred during init.\n\t */\n\twhile (((status & mask) != mask) &&\n\t       !(status & HS_FFERM) &&\n\t       i++ < 20) {\n\n\t\tif (i <= 5)\n\t\t\tmsleep(10);\n\t\telse if (i <= 10)\n\t\t\tmsleep(500);\n\t\telse\n\t\t\tmsleep(2500);\n\n\t\tif (i == 15) {\n\t\t\t\t/* Do post */\n\t\t\tphba->pport->port_state = LPFC_VPORT_UNKNOWN;\n\t\t\tlpfc_sli_brdrestart(phba);\n\t\t}\n\t\t/* Read the HBA Host Status Register */\n\t\tif (lpfc_readl(phba->HSregaddr, &status)) {\n\t\t\tretval = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Check to see if any errors occurred during init */\n\tif ((status & HS_FFERM) || (i >= 20)) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2751 Adapter failed to restart, \"\n\t\t\t\t\"status reg x%x, FW Data: A8 x%x AC x%x\\n\",\n\t\t\t\tstatus,\n\t\t\t\treadl(phba->MBslimaddr + 0xa8),\n\t\t\t\treadl(phba->MBslimaddr + 0xac));\n\t\tphba->link_state = LPFC_HBA_ERROR;\n\t\tretval = 1;\n\t}\n\n\treturn retval;\n}\n\n/**\n * lpfc_sli_brdready_s4 - Check for sli4 host ready status\n * @phba: Pointer to HBA context object.\n * @mask: Bit mask to be checked.\n *\n * This function checks the host status register to check if HBA is\n * ready. This function will wait in a loop for the HBA to be ready\n * If the HBA is not ready , the function will will reset the HBA PCI\n * function again. The function returns 1 when HBA fail to be ready\n * otherwise returns zero.\n **/\nstatic int\nlpfc_sli_brdready_s4(struct lpfc_hba *phba, uint32_t mask)\n{\n\tuint32_t status;\n\tint retval = 0;\n\n\t/* Read the HBA Host Status Register */\n\tstatus = lpfc_sli4_post_status_check(phba);\n\n\tif (status) {\n\t\tphba->pport->port_state = LPFC_VPORT_UNKNOWN;\n\t\tlpfc_sli_brdrestart(phba);\n\t\tstatus = lpfc_sli4_post_status_check(phba);\n\t}\n\n\t/* Check to see if any errors occurred during init */\n\tif (status) {\n\t\tphba->link_state = LPFC_HBA_ERROR;\n\t\tretval = 1;\n\t} else\n\t\tphba->sli4_hba.intr_enable = 0;\n\n\treturn retval;\n}\n\n/**\n * lpfc_sli_brdready - Wrapper func for checking the hba readyness\n * @phba: Pointer to HBA context object.\n * @mask: Bit mask to be checked.\n *\n * This routine wraps the actual SLI3 or SLI4 hba readyness check routine\n * from the API jump table function pointer from the lpfc_hba struct.\n **/\nint\nlpfc_sli_brdready(struct lpfc_hba *phba, uint32_t mask)\n{\n\treturn phba->lpfc_sli_brdready(phba, mask);\n}\n\n#define BARRIER_TEST_PATTERN (0xdeadbeef)\n\n/**\n * lpfc_reset_barrier - Make HBA ready for HBA reset\n * @phba: Pointer to HBA context object.\n *\n * This function is called before resetting an HBA. This function is called\n * with hbalock held and requests HBA to quiesce DMAs before a reset.\n **/\nvoid lpfc_reset_barrier(struct lpfc_hba *phba)\n{\n\tuint32_t __iomem *resp_buf;\n\tuint32_t __iomem *mbox_buf;\n\tvolatile uint32_t mbox;\n\tuint32_t hc_copy, ha_copy, resp_data;\n\tint  i;\n\tuint8_t hdrtype;\n\n\tlockdep_assert_held(&phba->hbalock);\n\n\tpci_read_config_byte(phba->pcidev, PCI_HEADER_TYPE, &hdrtype);\n\tif (hdrtype != 0x80 ||\n\t    (FC_JEDEC_ID(phba->vpd.rev.biuRev) != HELIOS_JEDEC_ID &&\n\t     FC_JEDEC_ID(phba->vpd.rev.biuRev) != THOR_JEDEC_ID))\n\t\treturn;\n\n\t/*\n\t * Tell the other part of the chip to suspend temporarily all\n\t * its DMA activity.\n\t */\n\tresp_buf = phba->MBslimaddr;\n\n\t/* Disable the error attention */\n\tif (lpfc_readl(phba->HCregaddr, &hc_copy))\n\t\treturn;\n\twritel((hc_copy & ~HC_ERINT_ENA), phba->HCregaddr);\n\treadl(phba->HCregaddr); /* flush */\n\tphba->link_flag |= LS_IGNORE_ERATT;\n\n\tif (lpfc_readl(phba->HAregaddr, &ha_copy))\n\t\treturn;\n\tif (ha_copy & HA_ERATT) {\n\t\t/* Clear Chip error bit */\n\t\twritel(HA_ERATT, phba->HAregaddr);\n\t\tphba->pport->stopped = 1;\n\t}\n\n\tmbox = 0;\n\t((MAILBOX_t *)&mbox)->mbxCommand = MBX_KILL_BOARD;\n\t((MAILBOX_t *)&mbox)->mbxOwner = OWN_CHIP;\n\n\twritel(BARRIER_TEST_PATTERN, (resp_buf + 1));\n\tmbox_buf = phba->MBslimaddr;\n\twritel(mbox, mbox_buf);\n\n\tfor (i = 0; i < 50; i++) {\n\t\tif (lpfc_readl((resp_buf + 1), &resp_data))\n\t\t\treturn;\n\t\tif (resp_data != ~(BARRIER_TEST_PATTERN))\n\t\t\tmdelay(1);\n\t\telse\n\t\t\tbreak;\n\t}\n\tresp_data = 0;\n\tif (lpfc_readl((resp_buf + 1), &resp_data))\n\t\treturn;\n\tif (resp_data  != ~(BARRIER_TEST_PATTERN)) {\n\t\tif (phba->sli.sli_flag & LPFC_SLI_ACTIVE ||\n\t\t    phba->pport->stopped)\n\t\t\tgoto restore_hc;\n\t\telse\n\t\t\tgoto clear_errat;\n\t}\n\n\t((MAILBOX_t *)&mbox)->mbxOwner = OWN_HOST;\n\tresp_data = 0;\n\tfor (i = 0; i < 500; i++) {\n\t\tif (lpfc_readl(resp_buf, &resp_data))\n\t\t\treturn;\n\t\tif (resp_data != mbox)\n\t\t\tmdelay(1);\n\t\telse\n\t\t\tbreak;\n\t}\n\nclear_errat:\n\n\twhile (++i < 500) {\n\t\tif (lpfc_readl(phba->HAregaddr, &ha_copy))\n\t\t\treturn;\n\t\tif (!(ha_copy & HA_ERATT))\n\t\t\tmdelay(1);\n\t\telse\n\t\t\tbreak;\n\t}\n\n\tif (readl(phba->HAregaddr) & HA_ERATT) {\n\t\twritel(HA_ERATT, phba->HAregaddr);\n\t\tphba->pport->stopped = 1;\n\t}\n\nrestore_hc:\n\tphba->link_flag &= ~LS_IGNORE_ERATT;\n\twritel(hc_copy, phba->HCregaddr);\n\treadl(phba->HCregaddr); /* flush */\n}\n\n/**\n * lpfc_sli_brdkill - Issue a kill_board mailbox command\n * @phba: Pointer to HBA context object.\n *\n * This function issues a kill_board mailbox command and waits for\n * the error attention interrupt. This function is called for stopping\n * the firmware processing. The caller is not required to hold any\n * locks. This function calls lpfc_hba_down_post function to free\n * any pending commands after the kill. The function will return 1 when it\n * fails to kill the board else will return 0.\n **/\nint\nlpfc_sli_brdkill(struct lpfc_hba *phba)\n{\n\tstruct lpfc_sli *psli;\n\tLPFC_MBOXQ_t *pmb;\n\tuint32_t status;\n\tuint32_t ha_copy;\n\tint retval;\n\tint i = 0;\n\n\tpsli = &phba->sli;\n\n\t/* Kill HBA */\n\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\"0329 Kill HBA Data: x%x x%x\\n\",\n\t\t\tphba->pport->port_state, psli->sli_flag);\n\n\tpmb = (LPFC_MBOXQ_t *) mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!pmb)\n\t\treturn 1;\n\n\t/* Disable the error attention */\n\tspin_lock_irq(&phba->hbalock);\n\tif (lpfc_readl(phba->HCregaddr, &status)) {\n\t\tspin_unlock_irq(&phba->hbalock);\n\t\tmempool_free(pmb, phba->mbox_mem_pool);\n\t\treturn 1;\n\t}\n\tstatus &= ~HC_ERINT_ENA;\n\twritel(status, phba->HCregaddr);\n\treadl(phba->HCregaddr); /* flush */\n\tphba->link_flag |= LS_IGNORE_ERATT;\n\tspin_unlock_irq(&phba->hbalock);\n\n\tlpfc_kill_board(phba, pmb);\n\tpmb->mbox_cmpl = lpfc_sli_def_mbox_cmpl;\n\tretval = lpfc_sli_issue_mbox(phba, pmb, MBX_NOWAIT);\n\n\tif (retval != MBX_SUCCESS) {\n\t\tif (retval != MBX_BUSY)\n\t\t\tmempool_free(pmb, phba->mbox_mem_pool);\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2752 KILL_BOARD command failed retval %d\\n\",\n\t\t\t\tretval);\n\t\tspin_lock_irq(&phba->hbalock);\n\t\tphba->link_flag &= ~LS_IGNORE_ERATT;\n\t\tspin_unlock_irq(&phba->hbalock);\n\t\treturn 1;\n\t}\n\n\tspin_lock_irq(&phba->hbalock);\n\tpsli->sli_flag &= ~LPFC_SLI_ACTIVE;\n\tspin_unlock_irq(&phba->hbalock);\n\n\tmempool_free(pmb, phba->mbox_mem_pool);\n\n\t/* There is no completion for a KILL_BOARD mbox cmd. Check for an error\n\t * attention every 100ms for 3 seconds. If we don't get ERATT after\n\t * 3 seconds we still set HBA_ERROR state because the status of the\n\t * board is now undefined.\n\t */\n\tif (lpfc_readl(phba->HAregaddr, &ha_copy))\n\t\treturn 1;\n\twhile ((i++ < 30) && !(ha_copy & HA_ERATT)) {\n\t\tmdelay(100);\n\t\tif (lpfc_readl(phba->HAregaddr, &ha_copy))\n\t\t\treturn 1;\n\t}\n\n\tdel_timer_sync(&psli->mbox_tmo);\n\tif (ha_copy & HA_ERATT) {\n\t\twritel(HA_ERATT, phba->HAregaddr);\n\t\tphba->pport->stopped = 1;\n\t}\n\tspin_lock_irq(&phba->hbalock);\n\tpsli->sli_flag &= ~LPFC_SLI_MBOX_ACTIVE;\n\tpsli->mbox_active = NULL;\n\tphba->link_flag &= ~LS_IGNORE_ERATT;\n\tspin_unlock_irq(&phba->hbalock);\n\n\tlpfc_hba_down_post(phba);\n\tphba->link_state = LPFC_HBA_ERROR;\n\n\treturn ha_copy & HA_ERATT ? 0 : 1;\n}\n\n/**\n * lpfc_sli_brdreset - Reset a sli-2 or sli-3 HBA\n * @phba: Pointer to HBA context object.\n *\n * This function resets the HBA by writing HC_INITFF to the control\n * register. After the HBA resets, this function resets all the iocb ring\n * indices. This function disables PCI layer parity checking during\n * the reset.\n * This function returns 0 always.\n * The caller is not required to hold any locks.\n **/\nint\nlpfc_sli_brdreset(struct lpfc_hba *phba)\n{\n\tstruct lpfc_sli *psli;\n\tstruct lpfc_sli_ring *pring;\n\tuint16_t cfg_value;\n\tint i;\n\n\tpsli = &phba->sli;\n\n\t/* Reset HBA */\n\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\"0325 Reset HBA Data: x%x x%x\\n\",\n\t\t\t(phba->pport) ? phba->pport->port_state : 0,\n\t\t\tpsli->sli_flag);\n\n\t/* perform board reset */\n\tphba->fc_eventTag = 0;\n\tphba->link_events = 0;\n\tif (phba->pport) {\n\t\tphba->pport->fc_myDID = 0;\n\t\tphba->pport->fc_prevDID = 0;\n\t}\n\n\t/* Turn off parity checking and serr during the physical reset */\n\tif (pci_read_config_word(phba->pcidev, PCI_COMMAND, &cfg_value))\n\t\treturn -EIO;\n\n\tpci_write_config_word(phba->pcidev, PCI_COMMAND,\n\t\t\t      (cfg_value &\n\t\t\t       ~(PCI_COMMAND_PARITY | PCI_COMMAND_SERR)));\n\n\tpsli->sli_flag &= ~(LPFC_SLI_ACTIVE | LPFC_PROCESS_LA);\n\n\t/* Now toggle INITFF bit in the Host Control Register */\n\twritel(HC_INITFF, phba->HCregaddr);\n\tmdelay(1);\n\treadl(phba->HCregaddr); /* flush */\n\twritel(0, phba->HCregaddr);\n\treadl(phba->HCregaddr); /* flush */\n\n\t/* Restore PCI cmd register */\n\tpci_write_config_word(phba->pcidev, PCI_COMMAND, cfg_value);\n\n\t/* Initialize relevant SLI info */\n\tfor (i = 0; i < psli->num_rings; i++) {\n\t\tpring = &psli->sli3_ring[i];\n\t\tpring->flag = 0;\n\t\tpring->sli.sli3.rspidx = 0;\n\t\tpring->sli.sli3.next_cmdidx  = 0;\n\t\tpring->sli.sli3.local_getidx = 0;\n\t\tpring->sli.sli3.cmdidx = 0;\n\t\tpring->missbufcnt = 0;\n\t}\n\n\tphba->link_state = LPFC_WARM_START;\n\treturn 0;\n}\n\n/**\n * lpfc_sli4_brdreset - Reset a sli-4 HBA\n * @phba: Pointer to HBA context object.\n *\n * This function resets a SLI4 HBA. This function disables PCI layer parity\n * checking during resets the device. The caller is not required to hold\n * any locks.\n *\n * This function returns 0 on success else returns negative error code.\n **/\nint\nlpfc_sli4_brdreset(struct lpfc_hba *phba)\n{\n\tstruct lpfc_sli *psli = &phba->sli;\n\tuint16_t cfg_value;\n\tint rc = 0;\n\n\t/* Reset HBA */\n\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\"0295 Reset HBA Data: x%x x%x x%x\\n\",\n\t\t\tphba->pport->port_state, psli->sli_flag,\n\t\t\tphba->hba_flag);\n\n\t/* perform board reset */\n\tphba->fc_eventTag = 0;\n\tphba->link_events = 0;\n\tphba->pport->fc_myDID = 0;\n\tphba->pport->fc_prevDID = 0;\n\n\tspin_lock_irq(&phba->hbalock);\n\tpsli->sli_flag &= ~(LPFC_PROCESS_LA);\n\tphba->fcf.fcf_flag = 0;\n\tspin_unlock_irq(&phba->hbalock);\n\n\t/* SLI4 INTF 2: if FW dump is being taken skip INIT_PORT */\n\tif (phba->hba_flag & HBA_FW_DUMP_OP) {\n\t\tphba->hba_flag &= ~HBA_FW_DUMP_OP;\n\t\treturn rc;\n\t}\n\n\t/* Now physically reset the device */\n\tlpfc_printf_log(phba, KERN_INFO, LOG_INIT,\n\t\t\t\"0389 Performing PCI function reset!\\n\");\n\n\t/* Turn off parity checking and serr during the physical reset */\n\tif (pci_read_config_word(phba->pcidev, PCI_COMMAND, &cfg_value)) {\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_INIT,\n\t\t\t\t\"3205 PCI read Config failed\\n\");\n\t\treturn -EIO;\n\t}\n\n\tpci_write_config_word(phba->pcidev, PCI_COMMAND, (cfg_value &\n\t\t\t      ~(PCI_COMMAND_PARITY | PCI_COMMAND_SERR)));\n\n\t/* Perform FCoE PCI function reset before freeing queue memory */\n\trc = lpfc_pci_function_reset(phba);\n\n\t/* Restore PCI cmd register */\n\tpci_write_config_word(phba->pcidev, PCI_COMMAND, cfg_value);\n\n\treturn rc;\n}\n\n/**\n * lpfc_sli_brdrestart_s3 - Restart a sli-3 hba\n * @phba: Pointer to HBA context object.\n *\n * This function is called in the SLI initialization code path to\n * restart the HBA. The caller is not required to hold any lock.\n * This function writes MBX_RESTART mailbox command to the SLIM and\n * resets the HBA. At the end of the function, it calls lpfc_hba_down_post\n * function to free any pending commands. The function enables\n * POST only during the first initialization. The function returns zero.\n * The function does not guarantee completion of MBX_RESTART mailbox\n * command before the return of this function.\n **/\nstatic int\nlpfc_sli_brdrestart_s3(struct lpfc_hba *phba)\n{\n\tMAILBOX_t *mb;\n\tstruct lpfc_sli *psli;\n\tvolatile uint32_t word0;\n\tvoid __iomem *to_slim;\n\tuint32_t hba_aer_enabled;\n\n\tspin_lock_irq(&phba->hbalock);\n\n\t/* Take PCIe device Advanced Error Reporting (AER) state */\n\thba_aer_enabled = phba->hba_flag & HBA_AER_ENABLED;\n\n\tpsli = &phba->sli;\n\n\t/* Restart HBA */\n\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\"0337 Restart HBA Data: x%x x%x\\n\",\n\t\t\t(phba->pport) ? phba->pport->port_state : 0,\n\t\t\tpsli->sli_flag);\n\n\tword0 = 0;\n\tmb = (MAILBOX_t *) &word0;\n\tmb->mbxCommand = MBX_RESTART;\n\tmb->mbxHc = 1;\n\n\tlpfc_reset_barrier(phba);\n\n\tto_slim = phba->MBslimaddr;\n\twritel(*(uint32_t *) mb, to_slim);\n\treadl(to_slim); /* flush */\n\n\t/* Only skip post after fc_ffinit is completed */\n\tif (phba->pport && phba->pport->port_state)\n\t\tword0 = 1;\t/* This is really setting up word1 */\n\telse\n\t\tword0 = 0;\t/* This is really setting up word1 */\n\tto_slim = phba->MBslimaddr + sizeof (uint32_t);\n\twritel(*(uint32_t *) mb, to_slim);\n\treadl(to_slim); /* flush */\n\n\tlpfc_sli_brdreset(phba);\n\tif (phba->pport)\n\t\tphba->pport->stopped = 0;\n\tphba->link_state = LPFC_INIT_START;\n\tphba->hba_flag = 0;\n\tspin_unlock_irq(&phba->hbalock);\n\n\tmemset(&psli->lnk_stat_offsets, 0, sizeof(psli->lnk_stat_offsets));\n\tpsli->stats_start = ktime_get_seconds();\n\n\t/* Give the INITFF and Post time to settle. */\n\tmdelay(100);\n\n\t/* Reset HBA AER if it was enabled, note hba_flag was reset above */\n\tif (hba_aer_enabled)\n\t\tpci_disable_pcie_error_reporting(phba->pcidev);\n\n\tlpfc_hba_down_post(phba);\n\n\treturn 0;\n}\n\n/**\n * lpfc_sli_brdrestart_s4 - Restart the sli-4 hba\n * @phba: Pointer to HBA context object.\n *\n * This function is called in the SLI initialization code path to restart\n * a SLI4 HBA. The caller is not required to hold any lock.\n * At the end of the function, it calls lpfc_hba_down_post function to\n * free any pending commands.\n **/\nstatic int\nlpfc_sli_brdrestart_s4(struct lpfc_hba *phba)\n{\n\tstruct lpfc_sli *psli = &phba->sli;\n\tuint32_t hba_aer_enabled;\n\tint rc;\n\n\t/* Restart HBA */\n\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\"0296 Restart HBA Data: x%x x%x\\n\",\n\t\t\tphba->pport->port_state, psli->sli_flag);\n\n\t/* Take PCIe device Advanced Error Reporting (AER) state */\n\thba_aer_enabled = phba->hba_flag & HBA_AER_ENABLED;\n\n\trc = lpfc_sli4_brdreset(phba);\n\tif (rc) {\n\t\tphba->link_state = LPFC_HBA_ERROR;\n\t\tgoto hba_down_queue;\n\t}\n\n\tspin_lock_irq(&phba->hbalock);\n\tphba->pport->stopped = 0;\n\tphba->link_state = LPFC_INIT_START;\n\tphba->hba_flag = 0;\n\tspin_unlock_irq(&phba->hbalock);\n\n\tmemset(&psli->lnk_stat_offsets, 0, sizeof(psli->lnk_stat_offsets));\n\tpsli->stats_start = ktime_get_seconds();\n\n\t/* Reset HBA AER if it was enabled, note hba_flag was reset above */\n\tif (hba_aer_enabled)\n\t\tpci_disable_pcie_error_reporting(phba->pcidev);\n\nhba_down_queue:\n\tlpfc_hba_down_post(phba);\n\tlpfc_sli4_queue_destroy(phba);\n\n\treturn rc;\n}\n\n/**\n * lpfc_sli_brdrestart - Wrapper func for restarting hba\n * @phba: Pointer to HBA context object.\n *\n * This routine wraps the actual SLI3 or SLI4 hba restart routine from the\n * API jump table function pointer from the lpfc_hba struct.\n**/\nint\nlpfc_sli_brdrestart(struct lpfc_hba *phba)\n{\n\treturn phba->lpfc_sli_brdrestart(phba);\n}\n\n/**\n * lpfc_sli_chipset_init - Wait for the restart of the HBA after a restart\n * @phba: Pointer to HBA context object.\n *\n * This function is called after a HBA restart to wait for successful\n * restart of the HBA. Successful restart of the HBA is indicated by\n * HS_FFRDY and HS_MBRDY bits. If the HBA fails to restart even after 15\n * iteration, the function will restart the HBA again. The function returns\n * zero if HBA successfully restarted else returns negative error code.\n **/\nint\nlpfc_sli_chipset_init(struct lpfc_hba *phba)\n{\n\tuint32_t status, i = 0;\n\n\t/* Read the HBA Host Status Register */\n\tif (lpfc_readl(phba->HSregaddr, &status))\n\t\treturn -EIO;\n\n\t/* Check status register to see what current state is */\n\ti = 0;\n\twhile ((status & (HS_FFRDY | HS_MBRDY)) != (HS_FFRDY | HS_MBRDY)) {\n\n\t\t/* Check every 10ms for 10 retries, then every 100ms for 90\n\t\t * retries, then every 1 sec for 50 retires for a total of\n\t\t * ~60 seconds before reset the board again and check every\n\t\t * 1 sec for 50 retries. The up to 60 seconds before the\n\t\t * board ready is required by the Falcon FIPS zeroization\n\t\t * complete, and any reset the board in between shall cause\n\t\t * restart of zeroization, further delay the board ready.\n\t\t */\n\t\tif (i++ >= 200) {\n\t\t\t/* Adapter failed to init, timeout, status reg\n\t\t\t   <status> */\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"0436 Adapter failed to init, \"\n\t\t\t\t\t\"timeout, status reg x%x, \"\n\t\t\t\t\t\"FW Data: A8 x%x AC x%x\\n\", status,\n\t\t\t\t\treadl(phba->MBslimaddr + 0xa8),\n\t\t\t\t\treadl(phba->MBslimaddr + 0xac));\n\t\t\tphba->link_state = LPFC_HBA_ERROR;\n\t\t\treturn -ETIMEDOUT;\n\t\t}\n\n\t\t/* Check to see if any errors occurred during init */\n\t\tif (status & HS_FFERM) {\n\t\t\t/* ERROR: During chipset initialization */\n\t\t\t/* Adapter failed to init, chipset, status reg\n\t\t\t   <status> */\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"0437 Adapter failed to init, \"\n\t\t\t\t\t\"chipset, status reg x%x, \"\n\t\t\t\t\t\"FW Data: A8 x%x AC x%x\\n\", status,\n\t\t\t\t\treadl(phba->MBslimaddr + 0xa8),\n\t\t\t\t\treadl(phba->MBslimaddr + 0xac));\n\t\t\tphba->link_state = LPFC_HBA_ERROR;\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tif (i <= 10)\n\t\t\tmsleep(10);\n\t\telse if (i <= 100)\n\t\t\tmsleep(100);\n\t\telse\n\t\t\tmsleep(1000);\n\n\t\tif (i == 150) {\n\t\t\t/* Do post */\n\t\t\tphba->pport->port_state = LPFC_VPORT_UNKNOWN;\n\t\t\tlpfc_sli_brdrestart(phba);\n\t\t}\n\t\t/* Read the HBA Host Status Register */\n\t\tif (lpfc_readl(phba->HSregaddr, &status))\n\t\t\treturn -EIO;\n\t}\n\n\t/* Check to see if any errors occurred during init */\n\tif (status & HS_FFERM) {\n\t\t/* ERROR: During chipset initialization */\n\t\t/* Adapter failed to init, chipset, status reg <status> */\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0438 Adapter failed to init, chipset, \"\n\t\t\t\t\"status reg x%x, \"\n\t\t\t\t\"FW Data: A8 x%x AC x%x\\n\", status,\n\t\t\t\treadl(phba->MBslimaddr + 0xa8),\n\t\t\t\treadl(phba->MBslimaddr + 0xac));\n\t\tphba->link_state = LPFC_HBA_ERROR;\n\t\treturn -EIO;\n\t}\n\n\t/* Clear all interrupt enable conditions */\n\twritel(0, phba->HCregaddr);\n\treadl(phba->HCregaddr); /* flush */\n\n\t/* setup host attn register */\n\twritel(0xffffffff, phba->HAregaddr);\n\treadl(phba->HAregaddr); /* flush */\n\treturn 0;\n}\n\n/**\n * lpfc_sli_hbq_count - Get the number of HBQs to be configured\n *\n * This function calculates and returns the number of HBQs required to be\n * configured.\n **/\nint\nlpfc_sli_hbq_count(void)\n{\n\treturn ARRAY_SIZE(lpfc_hbq_defs);\n}\n\n/**\n * lpfc_sli_hbq_entry_count - Calculate total number of hbq entries\n *\n * This function adds the number of hbq entries in every HBQ to get\n * the total number of hbq entries required for the HBA and returns\n * the total count.\n **/\nstatic int\nlpfc_sli_hbq_entry_count(void)\n{\n\tint  hbq_count = lpfc_sli_hbq_count();\n\tint  count = 0;\n\tint  i;\n\n\tfor (i = 0; i < hbq_count; ++i)\n\t\tcount += lpfc_hbq_defs[i]->entry_count;\n\treturn count;\n}\n\n/**\n * lpfc_sli_hbq_size - Calculate memory required for all hbq entries\n *\n * This function calculates amount of memory required for all hbq entries\n * to be configured and returns the total memory required.\n **/\nint\nlpfc_sli_hbq_size(void)\n{\n\treturn lpfc_sli_hbq_entry_count() * sizeof(struct lpfc_hbq_entry);\n}\n\n/**\n * lpfc_sli_hbq_setup - configure and initialize HBQs\n * @phba: Pointer to HBA context object.\n *\n * This function is called during the SLI initialization to configure\n * all the HBQs and post buffers to the HBQ. The caller is not\n * required to hold any locks. This function will return zero if successful\n * else it will return negative error code.\n **/\nstatic int\nlpfc_sli_hbq_setup(struct lpfc_hba *phba)\n{\n\tint  hbq_count = lpfc_sli_hbq_count();\n\tLPFC_MBOXQ_t *pmb;\n\tMAILBOX_t *pmbox;\n\tuint32_t hbqno;\n\tuint32_t hbq_entry_index;\n\n\t\t\t\t/* Get a Mailbox buffer to setup mailbox\n\t\t\t\t * commands for HBA initialization\n\t\t\t\t */\n\tpmb = (LPFC_MBOXQ_t *) mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\n\tif (!pmb)\n\t\treturn -ENOMEM;\n\n\tpmbox = &pmb->u.mb;\n\n\t/* Initialize the struct lpfc_sli_hbq structure for each hbq */\n\tphba->link_state = LPFC_INIT_MBX_CMDS;\n\tphba->hbq_in_use = 1;\n\n\thbq_entry_index = 0;\n\tfor (hbqno = 0; hbqno < hbq_count; ++hbqno) {\n\t\tphba->hbqs[hbqno].next_hbqPutIdx = 0;\n\t\tphba->hbqs[hbqno].hbqPutIdx      = 0;\n\t\tphba->hbqs[hbqno].local_hbqGetIdx   = 0;\n\t\tphba->hbqs[hbqno].entry_count =\n\t\t\tlpfc_hbq_defs[hbqno]->entry_count;\n\t\tlpfc_config_hbq(phba, hbqno, lpfc_hbq_defs[hbqno],\n\t\t\thbq_entry_index, pmb);\n\t\thbq_entry_index += phba->hbqs[hbqno].entry_count;\n\n\t\tif (lpfc_sli_issue_mbox(phba, pmb, MBX_POLL) != MBX_SUCCESS) {\n\t\t\t/* Adapter failed to init, mbxCmd <cmd> CFG_RING,\n\t\t\t   mbxStatus <status>, ring <num> */\n\n\t\t\tlpfc_printf_log(phba, KERN_ERR,\n\t\t\t\t\tLOG_SLI | LOG_VPORT,\n\t\t\t\t\t\"1805 Adapter failed to init. \"\n\t\t\t\t\t\"Data: x%x x%x x%x\\n\",\n\t\t\t\t\tpmbox->mbxCommand,\n\t\t\t\t\tpmbox->mbxStatus, hbqno);\n\n\t\t\tphba->link_state = LPFC_HBA_ERROR;\n\t\t\tmempool_free(pmb, phba->mbox_mem_pool);\n\t\t\treturn -ENXIO;\n\t\t}\n\t}\n\tphba->hbq_count = hbq_count;\n\n\tmempool_free(pmb, phba->mbox_mem_pool);\n\n\t/* Initially populate or replenish the HBQs */\n\tfor (hbqno = 0; hbqno < hbq_count; ++hbqno)\n\t\tlpfc_sli_hbqbuf_init_hbqs(phba, hbqno);\n\treturn 0;\n}\n\n/**\n * lpfc_sli4_rb_setup - Initialize and post RBs to HBA\n * @phba: Pointer to HBA context object.\n *\n * This function is called during the SLI initialization to configure\n * all the HBQs and post buffers to the HBQ. The caller is not\n * required to hold any locks. This function will return zero if successful\n * else it will return negative error code.\n **/\nstatic int\nlpfc_sli4_rb_setup(struct lpfc_hba *phba)\n{\n\tphba->hbq_in_use = 1;\n\t/**\n\t * Specific case when the MDS diagnostics is enabled and supported.\n\t * The receive buffer count is truncated to manage the incoming\n\t * traffic.\n\t **/\n\tif (phba->cfg_enable_mds_diags && phba->mds_diags_support)\n\t\tphba->hbqs[LPFC_ELS_HBQ].entry_count =\n\t\t\tlpfc_hbq_defs[LPFC_ELS_HBQ]->entry_count >> 1;\n\telse\n\t\tphba->hbqs[LPFC_ELS_HBQ].entry_count =\n\t\t\tlpfc_hbq_defs[LPFC_ELS_HBQ]->entry_count;\n\tphba->hbq_count = 1;\n\tlpfc_sli_hbqbuf_init_hbqs(phba, LPFC_ELS_HBQ);\n\t/* Initially populate or replenish the HBQs */\n\treturn 0;\n}\n\n/**\n * lpfc_sli_config_port - Issue config port mailbox command\n * @phba: Pointer to HBA context object.\n * @sli_mode: sli mode - 2/3\n *\n * This function is called by the sli initialization code path\n * to issue config_port mailbox command. This function restarts the\n * HBA firmware and issues a config_port mailbox command to configure\n * the SLI interface in the sli mode specified by sli_mode\n * variable. The caller is not required to hold any locks.\n * The function returns 0 if successful, else returns negative error\n * code.\n **/\nint\nlpfc_sli_config_port(struct lpfc_hba *phba, int sli_mode)\n{\n\tLPFC_MBOXQ_t *pmb;\n\tuint32_t resetcount = 0, rc = 0, done = 0;\n\n\tpmb = (LPFC_MBOXQ_t *) mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!pmb) {\n\t\tphba->link_state = LPFC_HBA_ERROR;\n\t\treturn -ENOMEM;\n\t}\n\n\tphba->sli_rev = sli_mode;\n\twhile (resetcount < 2 && !done) {\n\t\tspin_lock_irq(&phba->hbalock);\n\t\tphba->sli.sli_flag |= LPFC_SLI_MBOX_ACTIVE;\n\t\tspin_unlock_irq(&phba->hbalock);\n\t\tphba->pport->port_state = LPFC_VPORT_UNKNOWN;\n\t\tlpfc_sli_brdrestart(phba);\n\t\trc = lpfc_sli_chipset_init(phba);\n\t\tif (rc)\n\t\t\tbreak;\n\n\t\tspin_lock_irq(&phba->hbalock);\n\t\tphba->sli.sli_flag &= ~LPFC_SLI_MBOX_ACTIVE;\n\t\tspin_unlock_irq(&phba->hbalock);\n\t\tresetcount++;\n\n\t\t/* Call pre CONFIG_PORT mailbox command initialization.  A\n\t\t * value of 0 means the call was successful.  Any other\n\t\t * nonzero value is a failure, but if ERESTART is returned,\n\t\t * the driver may reset the HBA and try again.\n\t\t */\n\t\trc = lpfc_config_port_prep(phba);\n\t\tif (rc == -ERESTART) {\n\t\t\tphba->link_state = LPFC_LINK_UNKNOWN;\n\t\t\tcontinue;\n\t\t} else if (rc)\n\t\t\tbreak;\n\n\t\tphba->link_state = LPFC_INIT_MBX_CMDS;\n\t\tlpfc_config_port(phba, pmb);\n\t\trc = lpfc_sli_issue_mbox(phba, pmb, MBX_POLL);\n\t\tphba->sli3_options &= ~(LPFC_SLI3_NPIV_ENABLED |\n\t\t\t\t\tLPFC_SLI3_HBQ_ENABLED |\n\t\t\t\t\tLPFC_SLI3_CRP_ENABLED |\n\t\t\t\t\tLPFC_SLI3_DSS_ENABLED);\n\t\tif (rc != MBX_SUCCESS) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0442 Adapter failed to init, mbxCmd x%x \"\n\t\t\t\t\"CONFIG_PORT, mbxStatus x%x Data: x%x\\n\",\n\t\t\t\tpmb->u.mb.mbxCommand, pmb->u.mb.mbxStatus, 0);\n\t\t\tspin_lock_irq(&phba->hbalock);\n\t\t\tphba->sli.sli_flag &= ~LPFC_SLI_ACTIVE;\n\t\t\tspin_unlock_irq(&phba->hbalock);\n\t\t\trc = -ENXIO;\n\t\t} else {\n\t\t\t/* Allow asynchronous mailbox command to go through */\n\t\t\tspin_lock_irq(&phba->hbalock);\n\t\t\tphba->sli.sli_flag &= ~LPFC_SLI_ASYNC_MBX_BLK;\n\t\t\tspin_unlock_irq(&phba->hbalock);\n\t\t\tdone = 1;\n\n\t\t\tif ((pmb->u.mb.un.varCfgPort.casabt == 1) &&\n\t\t\t    (pmb->u.mb.un.varCfgPort.gasabt == 0))\n\t\t\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_INIT,\n\t\t\t\t\t\"3110 Port did not grant ASABT\\n\");\n\t\t}\n\t}\n\tif (!done) {\n\t\trc = -EINVAL;\n\t\tgoto do_prep_failed;\n\t}\n\tif (pmb->u.mb.un.varCfgPort.sli_mode == 3) {\n\t\tif (!pmb->u.mb.un.varCfgPort.cMA) {\n\t\t\trc = -ENXIO;\n\t\t\tgoto do_prep_failed;\n\t\t}\n\t\tif (phba->max_vpi && pmb->u.mb.un.varCfgPort.gmv) {\n\t\t\tphba->sli3_options |= LPFC_SLI3_NPIV_ENABLED;\n\t\t\tphba->max_vpi = pmb->u.mb.un.varCfgPort.max_vpi;\n\t\t\tphba->max_vports = (phba->max_vpi > phba->max_vports) ?\n\t\t\t\tphba->max_vpi : phba->max_vports;\n\n\t\t} else\n\t\t\tphba->max_vpi = 0;\n\t\tif (pmb->u.mb.un.varCfgPort.gerbm)\n\t\t\tphba->sli3_options |= LPFC_SLI3_HBQ_ENABLED;\n\t\tif (pmb->u.mb.un.varCfgPort.gcrp)\n\t\t\tphba->sli3_options |= LPFC_SLI3_CRP_ENABLED;\n\n\t\tphba->hbq_get = phba->mbox->us.s3_pgp.hbq_get;\n\t\tphba->port_gp = phba->mbox->us.s3_pgp.port;\n\n\t\tif (phba->sli3_options & LPFC_SLI3_BG_ENABLED) {\n\t\t\tif (pmb->u.mb.un.varCfgPort.gbg == 0) {\n\t\t\t\tphba->cfg_enable_bg = 0;\n\t\t\t\tphba->sli3_options &= ~LPFC_SLI3_BG_ENABLED;\n\t\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\t\"0443 Adapter did not grant \"\n\t\t\t\t\t\t\"BlockGuard\\n\");\n\t\t\t}\n\t\t}\n\t} else {\n\t\tphba->hbq_get = NULL;\n\t\tphba->port_gp = phba->mbox->us.s2.port;\n\t\tphba->max_vpi = 0;\n\t}\ndo_prep_failed:\n\tmempool_free(pmb, phba->mbox_mem_pool);\n\treturn rc;\n}\n\n\n/**\n * lpfc_sli_hba_setup - SLI initialization function\n * @phba: Pointer to HBA context object.\n *\n * This function is the main SLI initialization function. This function\n * is called by the HBA initialization code, HBA reset code and HBA\n * error attention handler code. Caller is not required to hold any\n * locks. This function issues config_port mailbox command to configure\n * the SLI, setup iocb rings and HBQ rings. In the end the function\n * calls the config_port_post function to issue init_link mailbox\n * command and to start the discovery. The function will return zero\n * if successful, else it will return negative error code.\n **/\nint\nlpfc_sli_hba_setup(struct lpfc_hba *phba)\n{\n\tuint32_t rc;\n\tint  mode = 3, i;\n\tint longs;\n\n\tswitch (phba->cfg_sli_mode) {\n\tcase 2:\n\t\tif (phba->cfg_enable_npiv) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"1824 NPIV enabled: Override sli_mode \"\n\t\t\t\t\"parameter (%d) to auto (0).\\n\",\n\t\t\t\tphba->cfg_sli_mode);\n\t\t\tbreak;\n\t\t}\n\t\tmode = 2;\n\t\tbreak;\n\tcase 0:\n\tcase 3:\n\t\tbreak;\n\tdefault:\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"1819 Unrecognized sli_mode parameter: %d.\\n\",\n\t\t\t\tphba->cfg_sli_mode);\n\n\t\tbreak;\n\t}\n\tphba->fcp_embed_io = 0;\t/* SLI4 FC support only */\n\n\trc = lpfc_sli_config_port(phba, mode);\n\n\tif (rc && phba->cfg_sli_mode == 3)\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"1820 Unable to select SLI-3.  \"\n\t\t\t\t\"Not supported by adapter.\\n\");\n\tif (rc && mode != 2)\n\t\trc = lpfc_sli_config_port(phba, 2);\n\telse if (rc && mode == 2)\n\t\trc = lpfc_sli_config_port(phba, 3);\n\tif (rc)\n\t\tgoto lpfc_sli_hba_setup_error;\n\n\t/* Enable PCIe device Advanced Error Reporting (AER) if configured */\n\tif (phba->cfg_aer_support == 1 && !(phba->hba_flag & HBA_AER_ENABLED)) {\n\t\trc = pci_enable_pcie_error_reporting(phba->pcidev);\n\t\tif (!rc) {\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_INIT,\n\t\t\t\t\t\"2709 This device supports \"\n\t\t\t\t\t\"Advanced Error Reporting (AER)\\n\");\n\t\t\tspin_lock_irq(&phba->hbalock);\n\t\t\tphba->hba_flag |= HBA_AER_ENABLED;\n\t\t\tspin_unlock_irq(&phba->hbalock);\n\t\t} else {\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_INIT,\n\t\t\t\t\t\"2708 This device does not support \"\n\t\t\t\t\t\"Advanced Error Reporting (AER): %d\\n\",\n\t\t\t\t\trc);\n\t\t\tphba->cfg_aer_support = 0;\n\t\t}\n\t}\n\n\tif (phba->sli_rev == 3) {\n\t\tphba->iocb_cmd_size = SLI3_IOCB_CMD_SIZE;\n\t\tphba->iocb_rsp_size = SLI3_IOCB_RSP_SIZE;\n\t} else {\n\t\tphba->iocb_cmd_size = SLI2_IOCB_CMD_SIZE;\n\t\tphba->iocb_rsp_size = SLI2_IOCB_RSP_SIZE;\n\t\tphba->sli3_options = 0;\n\t}\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_INIT,\n\t\t\t\"0444 Firmware in SLI %x mode. Max_vpi %d\\n\",\n\t\t\tphba->sli_rev, phba->max_vpi);\n\trc = lpfc_sli_ring_map(phba);\n\n\tif (rc)\n\t\tgoto lpfc_sli_hba_setup_error;\n\n\t/* Initialize VPIs. */\n\tif (phba->sli_rev == LPFC_SLI_REV3) {\n\t\t/*\n\t\t * The VPI bitmask and physical ID array are allocated\n\t\t * and initialized once only - at driver load.  A port\n\t\t * reset doesn't need to reinitialize this memory.\n\t\t */\n\t\tif ((phba->vpi_bmask == NULL) && (phba->vpi_ids == NULL)) {\n\t\t\tlongs = (phba->max_vpi + BITS_PER_LONG) / BITS_PER_LONG;\n\t\t\tphba->vpi_bmask = kcalloc(longs,\n\t\t\t\t\t\t  sizeof(unsigned long),\n\t\t\t\t\t\t  GFP_KERNEL);\n\t\t\tif (!phba->vpi_bmask) {\n\t\t\t\trc = -ENOMEM;\n\t\t\t\tgoto lpfc_sli_hba_setup_error;\n\t\t\t}\n\n\t\t\tphba->vpi_ids = kcalloc(phba->max_vpi + 1,\n\t\t\t\t\t\tsizeof(uint16_t),\n\t\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!phba->vpi_ids) {\n\t\t\t\tkfree(phba->vpi_bmask);\n\t\t\t\trc = -ENOMEM;\n\t\t\t\tgoto lpfc_sli_hba_setup_error;\n\t\t\t}\n\t\t\tfor (i = 0; i < phba->max_vpi; i++)\n\t\t\t\tphba->vpi_ids[i] = i;\n\t\t}\n\t}\n\n\t/* Init HBQs */\n\tif (phba->sli3_options & LPFC_SLI3_HBQ_ENABLED) {\n\t\trc = lpfc_sli_hbq_setup(phba);\n\t\tif (rc)\n\t\t\tgoto lpfc_sli_hba_setup_error;\n\t}\n\tspin_lock_irq(&phba->hbalock);\n\tphba->sli.sli_flag |= LPFC_PROCESS_LA;\n\tspin_unlock_irq(&phba->hbalock);\n\n\trc = lpfc_config_port_post(phba);\n\tif (rc)\n\t\tgoto lpfc_sli_hba_setup_error;\n\n\treturn rc;\n\nlpfc_sli_hba_setup_error:\n\tphba->link_state = LPFC_HBA_ERROR;\n\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"0445 Firmware initialization failed\\n\");\n\treturn rc;\n}\n\n/**\n * lpfc_sli4_read_fcoe_params - Read fcoe params from conf region\n * @phba: Pointer to HBA context object.\n *\n * This function issue a dump mailbox command to read config region\n * 23 and parse the records in the region and populate driver\n * data structure.\n **/\nstatic int\nlpfc_sli4_read_fcoe_params(struct lpfc_hba *phba)\n{\n\tLPFC_MBOXQ_t *mboxq;\n\tstruct lpfc_dmabuf *mp;\n\tstruct lpfc_mqe *mqe;\n\tuint32_t data_length;\n\tint rc;\n\n\t/* Program the default value of vlan_id and fc_map */\n\tphba->valid_vlan = 0;\n\tphba->fc_map[0] = LPFC_FCOE_FCF_MAP0;\n\tphba->fc_map[1] = LPFC_FCOE_FCF_MAP1;\n\tphba->fc_map[2] = LPFC_FCOE_FCF_MAP2;\n\n\tmboxq = (LPFC_MBOXQ_t *)mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mboxq)\n\t\treturn -ENOMEM;\n\n\tmqe = &mboxq->u.mqe;\n\tif (lpfc_sli4_dump_cfg_rg23(phba, mboxq)) {\n\t\trc = -ENOMEM;\n\t\tgoto out_free_mboxq;\n\t}\n\n\tmp = (struct lpfc_dmabuf *)mboxq->ctx_buf;\n\trc = lpfc_sli_issue_mbox(phba, mboxq, MBX_POLL);\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_MBOX | LOG_SLI,\n\t\t\t\"(%d):2571 Mailbox cmd x%x Status x%x \"\n\t\t\t\"Data: x%x x%x x%x x%x x%x x%x x%x x%x x%x \"\n\t\t\t\"x%x x%x x%x x%x x%x x%x x%x x%x x%x \"\n\t\t\t\"CQ: x%x x%x x%x x%x\\n\",\n\t\t\tmboxq->vport ? mboxq->vport->vpi : 0,\n\t\t\tbf_get(lpfc_mqe_command, mqe),\n\t\t\tbf_get(lpfc_mqe_status, mqe),\n\t\t\tmqe->un.mb_words[0], mqe->un.mb_words[1],\n\t\t\tmqe->un.mb_words[2], mqe->un.mb_words[3],\n\t\t\tmqe->un.mb_words[4], mqe->un.mb_words[5],\n\t\t\tmqe->un.mb_words[6], mqe->un.mb_words[7],\n\t\t\tmqe->un.mb_words[8], mqe->un.mb_words[9],\n\t\t\tmqe->un.mb_words[10], mqe->un.mb_words[11],\n\t\t\tmqe->un.mb_words[12], mqe->un.mb_words[13],\n\t\t\tmqe->un.mb_words[14], mqe->un.mb_words[15],\n\t\t\tmqe->un.mb_words[16], mqe->un.mb_words[50],\n\t\t\tmboxq->mcqe.word0,\n\t\t\tmboxq->mcqe.mcqe_tag0, \tmboxq->mcqe.mcqe_tag1,\n\t\t\tmboxq->mcqe.trailer);\n\n\tif (rc) {\n\t\tlpfc_mbuf_free(phba, mp->virt, mp->phys);\n\t\tkfree(mp);\n\t\trc = -EIO;\n\t\tgoto out_free_mboxq;\n\t}\n\tdata_length = mqe->un.mb_words[5];\n\tif (data_length > DMP_RGN23_SIZE) {\n\t\tlpfc_mbuf_free(phba, mp->virt, mp->phys);\n\t\tkfree(mp);\n\t\trc = -EIO;\n\t\tgoto out_free_mboxq;\n\t}\n\n\tlpfc_parse_fcoe_conf(phba, mp->virt, data_length);\n\tlpfc_mbuf_free(phba, mp->virt, mp->phys);\n\tkfree(mp);\n\trc = 0;\n\nout_free_mboxq:\n\tmempool_free(mboxq, phba->mbox_mem_pool);\n\treturn rc;\n}\n\n/**\n * lpfc_sli4_read_rev - Issue READ_REV and collect vpd data\n * @phba: pointer to lpfc hba data structure.\n * @mboxq: pointer to the LPFC_MBOXQ_t structure.\n * @vpd: pointer to the memory to hold resulting port vpd data.\n * @vpd_size: On input, the number of bytes allocated to @vpd.\n *\t      On output, the number of data bytes in @vpd.\n *\n * This routine executes a READ_REV SLI4 mailbox command.  In\n * addition, this routine gets the port vpd data.\n *\n * Return codes\n * \t0 - successful\n * \t-ENOMEM - could not allocated memory.\n **/\nstatic int\nlpfc_sli4_read_rev(struct lpfc_hba *phba, LPFC_MBOXQ_t *mboxq,\n\t\t    uint8_t *vpd, uint32_t *vpd_size)\n{\n\tint rc = 0;\n\tuint32_t dma_size;\n\tstruct lpfc_dmabuf *dmabuf;\n\tstruct lpfc_mqe *mqe;\n\n\tdmabuf = kzalloc(sizeof(struct lpfc_dmabuf), GFP_KERNEL);\n\tif (!dmabuf)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Get a DMA buffer for the vpd data resulting from the READ_REV\n\t * mailbox command.\n\t */\n\tdma_size = *vpd_size;\n\tdmabuf->virt = dma_alloc_coherent(&phba->pcidev->dev, dma_size,\n\t\t\t\t\t  &dmabuf->phys, GFP_KERNEL);\n\tif (!dmabuf->virt) {\n\t\tkfree(dmabuf);\n\t\treturn -ENOMEM;\n\t}\n\n\t/*\n\t * The SLI4 implementation of READ_REV conflicts at word1,\n\t * bits 31:16 and SLI4 adds vpd functionality not present\n\t * in SLI3.  This code corrects the conflicts.\n\t */\n\tlpfc_read_rev(phba, mboxq);\n\tmqe = &mboxq->u.mqe;\n\tmqe->un.read_rev.vpd_paddr_high = putPaddrHigh(dmabuf->phys);\n\tmqe->un.read_rev.vpd_paddr_low = putPaddrLow(dmabuf->phys);\n\tmqe->un.read_rev.word1 &= 0x0000FFFF;\n\tbf_set(lpfc_mbx_rd_rev_vpd, &mqe->un.read_rev, 1);\n\tbf_set(lpfc_mbx_rd_rev_avail_len, &mqe->un.read_rev, dma_size);\n\n\trc = lpfc_sli_issue_mbox(phba, mboxq, MBX_POLL);\n\tif (rc) {\n\t\tdma_free_coherent(&phba->pcidev->dev, dma_size,\n\t\t\t\t  dmabuf->virt, dmabuf->phys);\n\t\tkfree(dmabuf);\n\t\treturn -EIO;\n\t}\n\n\t/*\n\t * The available vpd length cannot be bigger than the\n\t * DMA buffer passed to the port.  Catch the less than\n\t * case and update the caller's size.\n\t */\n\tif (mqe->un.read_rev.avail_vpd_len < *vpd_size)\n\t\t*vpd_size = mqe->un.read_rev.avail_vpd_len;\n\n\tmemcpy(vpd, dmabuf->virt, *vpd_size);\n\n\tdma_free_coherent(&phba->pcidev->dev, dma_size,\n\t\t\t  dmabuf->virt, dmabuf->phys);\n\tkfree(dmabuf);\n\treturn 0;\n}\n\n/**\n * lpfc_sli4_get_ctl_attr - Retrieve SLI4 device controller attributes\n * @phba: pointer to lpfc hba data structure.\n *\n * This routine retrieves SLI4 device physical port name this PCI function\n * is attached to.\n *\n * Return codes\n *      0 - successful\n *      otherwise - failed to retrieve controller attributes\n **/\nstatic int\nlpfc_sli4_get_ctl_attr(struct lpfc_hba *phba)\n{\n\tLPFC_MBOXQ_t *mboxq;\n\tstruct lpfc_mbx_get_cntl_attributes *mbx_cntl_attr;\n\tstruct lpfc_controller_attribute *cntl_attr;\n\tvoid *virtaddr = NULL;\n\tuint32_t alloclen, reqlen;\n\tuint32_t shdr_status, shdr_add_status;\n\tunion lpfc_sli4_cfg_shdr *shdr;\n\tint rc;\n\n\tmboxq = (LPFC_MBOXQ_t *)mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mboxq)\n\t\treturn -ENOMEM;\n\n\t/* Send COMMON_GET_CNTL_ATTRIBUTES mbox cmd */\n\treqlen = sizeof(struct lpfc_mbx_get_cntl_attributes);\n\talloclen = lpfc_sli4_config(phba, mboxq, LPFC_MBOX_SUBSYSTEM_COMMON,\n\t\t\tLPFC_MBOX_OPCODE_GET_CNTL_ATTRIBUTES, reqlen,\n\t\t\tLPFC_SLI4_MBX_NEMBED);\n\n\tif (alloclen < reqlen) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"3084 Allocated DMA memory size (%d) is \"\n\t\t\t\t\"less than the requested DMA memory size \"\n\t\t\t\t\"(%d)\\n\", alloclen, reqlen);\n\t\trc = -ENOMEM;\n\t\tgoto out_free_mboxq;\n\t}\n\trc = lpfc_sli_issue_mbox(phba, mboxq, MBX_POLL);\n\tvirtaddr = mboxq->sge_array->addr[0];\n\tmbx_cntl_attr = (struct lpfc_mbx_get_cntl_attributes *)virtaddr;\n\tshdr = &mbx_cntl_attr->cfg_shdr;\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_SLI,\n\t\t\t\t\"3085 Mailbox x%x (x%x/x%x) failed, \"\n\t\t\t\t\"rc:x%x, status:x%x, add_status:x%x\\n\",\n\t\t\t\tbf_get(lpfc_mqe_command, &mboxq->u.mqe),\n\t\t\t\tlpfc_sli_config_mbox_subsys_get(phba, mboxq),\n\t\t\t\tlpfc_sli_config_mbox_opcode_get(phba, mboxq),\n\t\t\t\trc, shdr_status, shdr_add_status);\n\t\trc = -ENXIO;\n\t\tgoto out_free_mboxq;\n\t}\n\n\tcntl_attr = &mbx_cntl_attr->cntl_attr;\n\tphba->sli4_hba.lnk_info.lnk_dv = LPFC_LNK_DAT_VAL;\n\tphba->sli4_hba.lnk_info.lnk_tp =\n\t\tbf_get(lpfc_cntl_attr_lnk_type, cntl_attr);\n\tphba->sli4_hba.lnk_info.lnk_no =\n\t\tbf_get(lpfc_cntl_attr_lnk_numb, cntl_attr);\n\n\tmemset(phba->BIOSVersion, 0, sizeof(phba->BIOSVersion));\n\tstrlcat(phba->BIOSVersion, (char *)cntl_attr->bios_ver_str,\n\t\tsizeof(phba->BIOSVersion));\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\"3086 lnk_type:%d, lnk_numb:%d, bios_ver:%s\\n\",\n\t\t\tphba->sli4_hba.lnk_info.lnk_tp,\n\t\t\tphba->sli4_hba.lnk_info.lnk_no,\n\t\t\tphba->BIOSVersion);\nout_free_mboxq:\n\tif (rc != MBX_TIMEOUT) {\n\t\tif (bf_get(lpfc_mqe_command, &mboxq->u.mqe) == MBX_SLI4_CONFIG)\n\t\t\tlpfc_sli4_mbox_cmd_free(phba, mboxq);\n\t\telse\n\t\t\tmempool_free(mboxq, phba->mbox_mem_pool);\n\t}\n\treturn rc;\n}\n\n/**\n * lpfc_sli4_retrieve_pport_name - Retrieve SLI4 device physical port name\n * @phba: pointer to lpfc hba data structure.\n *\n * This routine retrieves SLI4 device physical port name this PCI function\n * is attached to.\n *\n * Return codes\n *      0 - successful\n *      otherwise - failed to retrieve physical port name\n **/\nstatic int\nlpfc_sli4_retrieve_pport_name(struct lpfc_hba *phba)\n{\n\tLPFC_MBOXQ_t *mboxq;\n\tstruct lpfc_mbx_get_port_name *get_port_name;\n\tuint32_t shdr_status, shdr_add_status;\n\tunion lpfc_sli4_cfg_shdr *shdr;\n\tchar cport_name = 0;\n\tint rc;\n\n\t/* We assume nothing at this point */\n\tphba->sli4_hba.lnk_info.lnk_dv = LPFC_LNK_DAT_INVAL;\n\tphba->sli4_hba.pport_name_sta = LPFC_SLI4_PPNAME_NON;\n\n\tmboxq = (LPFC_MBOXQ_t *)mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mboxq)\n\t\treturn -ENOMEM;\n\t/* obtain link type and link number via READ_CONFIG */\n\tphba->sli4_hba.lnk_info.lnk_dv = LPFC_LNK_DAT_INVAL;\n\tlpfc_sli4_read_config(phba);\n\tif (phba->sli4_hba.lnk_info.lnk_dv == LPFC_LNK_DAT_VAL)\n\t\tgoto retrieve_ppname;\n\n\t/* obtain link type and link number via COMMON_GET_CNTL_ATTRIBUTES */\n\trc = lpfc_sli4_get_ctl_attr(phba);\n\tif (rc)\n\t\tgoto out_free_mboxq;\n\nretrieve_ppname:\n\tlpfc_sli4_config(phba, mboxq, LPFC_MBOX_SUBSYSTEM_COMMON,\n\t\tLPFC_MBOX_OPCODE_GET_PORT_NAME,\n\t\tsizeof(struct lpfc_mbx_get_port_name) -\n\t\tsizeof(struct lpfc_sli4_cfg_mhdr),\n\t\tLPFC_SLI4_MBX_EMBED);\n\tget_port_name = &mboxq->u.mqe.un.get_port_name;\n\tshdr = (union lpfc_sli4_cfg_shdr *)&get_port_name->header.cfg_shdr;\n\tbf_set(lpfc_mbox_hdr_version, &shdr->request, LPFC_OPCODE_VERSION_1);\n\tbf_set(lpfc_mbx_get_port_name_lnk_type, &get_port_name->u.request,\n\t\tphba->sli4_hba.lnk_info.lnk_tp);\n\trc = lpfc_sli_issue_mbox(phba, mboxq, MBX_POLL);\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_SLI,\n\t\t\t\t\"3087 Mailbox x%x (x%x/x%x) failed: \"\n\t\t\t\t\"rc:x%x, status:x%x, add_status:x%x\\n\",\n\t\t\t\tbf_get(lpfc_mqe_command, &mboxq->u.mqe),\n\t\t\t\tlpfc_sli_config_mbox_subsys_get(phba, mboxq),\n\t\t\t\tlpfc_sli_config_mbox_opcode_get(phba, mboxq),\n\t\t\t\trc, shdr_status, shdr_add_status);\n\t\trc = -ENXIO;\n\t\tgoto out_free_mboxq;\n\t}\n\tswitch (phba->sli4_hba.lnk_info.lnk_no) {\n\tcase LPFC_LINK_NUMBER_0:\n\t\tcport_name = bf_get(lpfc_mbx_get_port_name_name0,\n\t\t\t\t&get_port_name->u.response);\n\t\tphba->sli4_hba.pport_name_sta = LPFC_SLI4_PPNAME_GET;\n\t\tbreak;\n\tcase LPFC_LINK_NUMBER_1:\n\t\tcport_name = bf_get(lpfc_mbx_get_port_name_name1,\n\t\t\t\t&get_port_name->u.response);\n\t\tphba->sli4_hba.pport_name_sta = LPFC_SLI4_PPNAME_GET;\n\t\tbreak;\n\tcase LPFC_LINK_NUMBER_2:\n\t\tcport_name = bf_get(lpfc_mbx_get_port_name_name2,\n\t\t\t\t&get_port_name->u.response);\n\t\tphba->sli4_hba.pport_name_sta = LPFC_SLI4_PPNAME_GET;\n\t\tbreak;\n\tcase LPFC_LINK_NUMBER_3:\n\t\tcport_name = bf_get(lpfc_mbx_get_port_name_name3,\n\t\t\t\t&get_port_name->u.response);\n\t\tphba->sli4_hba.pport_name_sta = LPFC_SLI4_PPNAME_GET;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (phba->sli4_hba.pport_name_sta == LPFC_SLI4_PPNAME_GET) {\n\t\tphba->Port[0] = cport_name;\n\t\tphba->Port[1] = '\\0';\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\t\"3091 SLI get port name: %s\\n\", phba->Port);\n\t}\n\nout_free_mboxq:\n\tif (rc != MBX_TIMEOUT) {\n\t\tif (bf_get(lpfc_mqe_command, &mboxq->u.mqe) == MBX_SLI4_CONFIG)\n\t\t\tlpfc_sli4_mbox_cmd_free(phba, mboxq);\n\t\telse\n\t\t\tmempool_free(mboxq, phba->mbox_mem_pool);\n\t}\n\treturn rc;\n}\n\n/**\n * lpfc_sli4_arm_cqeq_intr - Arm sli-4 device completion and event queues\n * @phba: pointer to lpfc hba data structure.\n *\n * This routine is called to explicitly arm the SLI4 device's completion and\n * event queues\n **/\nstatic void\nlpfc_sli4_arm_cqeq_intr(struct lpfc_hba *phba)\n{\n\tint qidx;\n\tstruct lpfc_sli4_hba *sli4_hba = &phba->sli4_hba;\n\tstruct lpfc_sli4_hdw_queue *qp;\n\tstruct lpfc_queue *eq;\n\n\tsli4_hba->sli4_write_cq_db(phba, sli4_hba->mbx_cq, 0, LPFC_QUEUE_REARM);\n\tsli4_hba->sli4_write_cq_db(phba, sli4_hba->els_cq, 0, LPFC_QUEUE_REARM);\n\tif (sli4_hba->nvmels_cq)\n\t\tsli4_hba->sli4_write_cq_db(phba, sli4_hba->nvmels_cq, 0,\n\t\t\t\t\t   LPFC_QUEUE_REARM);\n\n\tif (sli4_hba->hdwq) {\n\t\t/* Loop thru all Hardware Queues */\n\t\tfor (qidx = 0; qidx < phba->cfg_hdw_queue; qidx++) {\n\t\t\tqp = &sli4_hba->hdwq[qidx];\n\t\t\t/* ARM the corresponding CQ */\n\t\t\tsli4_hba->sli4_write_cq_db(phba, qp->io_cq, 0,\n\t\t\t\t\t\tLPFC_QUEUE_REARM);\n\t\t}\n\n\t\t/* Loop thru all IRQ vectors */\n\t\tfor (qidx = 0; qidx < phba->cfg_irq_chann; qidx++) {\n\t\t\teq = sli4_hba->hba_eq_hdl[qidx].eq;\n\t\t\t/* ARM the corresponding EQ */\n\t\t\tsli4_hba->sli4_write_eq_db(phba, eq,\n\t\t\t\t\t\t   0, LPFC_QUEUE_REARM);\n\t\t}\n\t}\n\n\tif (phba->nvmet_support) {\n\t\tfor (qidx = 0; qidx < phba->cfg_nvmet_mrq; qidx++) {\n\t\t\tsli4_hba->sli4_write_cq_db(phba,\n\t\t\t\tsli4_hba->nvmet_cqset[qidx], 0,\n\t\t\t\tLPFC_QUEUE_REARM);\n\t\t}\n\t}\n}\n\n/**\n * lpfc_sli4_get_avail_extnt_rsrc - Get available resource extent count.\n * @phba: Pointer to HBA context object.\n * @type: The resource extent type.\n * @extnt_count: buffer to hold port available extent count.\n * @extnt_size: buffer to hold element count per extent.\n *\n * This function calls the port and retrievs the number of available\n * extents and their size for a particular extent type.\n *\n * Returns: 0 if successful.  Nonzero otherwise.\n **/\nint\nlpfc_sli4_get_avail_extnt_rsrc(struct lpfc_hba *phba, uint16_t type,\n\t\t\t       uint16_t *extnt_count, uint16_t *extnt_size)\n{\n\tint rc = 0;\n\tuint32_t length;\n\tuint32_t mbox_tmo;\n\tstruct lpfc_mbx_get_rsrc_extent_info *rsrc_info;\n\tLPFC_MBOXQ_t *mbox;\n\n\tmbox = (LPFC_MBOXQ_t *) mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox)\n\t\treturn -ENOMEM;\n\n\t/* Find out how many extents are available for this resource type */\n\tlength = (sizeof(struct lpfc_mbx_get_rsrc_extent_info) -\n\t\t  sizeof(struct lpfc_sli4_cfg_mhdr));\n\tlpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_COMMON,\n\t\t\t LPFC_MBOX_OPCODE_GET_RSRC_EXTENT_INFO,\n\t\t\t length, LPFC_SLI4_MBX_EMBED);\n\n\t/* Send an extents count of 0 - the GET doesn't use it. */\n\trc = lpfc_sli4_mbox_rsrc_extent(phba, mbox, 0, type,\n\t\t\t\t\tLPFC_SLI4_MBX_EMBED);\n\tif (unlikely(rc)) {\n\t\trc = -EIO;\n\t\tgoto err_exit;\n\t}\n\n\tif (!phba->sli4_hba.intr_enable)\n\t\trc = lpfc_sli_issue_mbox(phba, mbox, MBX_POLL);\n\telse {\n\t\tmbox_tmo = lpfc_mbox_tmo_val(phba, mbox);\n\t\trc = lpfc_sli_issue_mbox_wait(phba, mbox, mbox_tmo);\n\t}\n\tif (unlikely(rc)) {\n\t\trc = -EIO;\n\t\tgoto err_exit;\n\t}\n\n\trsrc_info = &mbox->u.mqe.un.rsrc_extent_info;\n\tif (bf_get(lpfc_mbox_hdr_status,\n\t\t   &rsrc_info->header.cfg_shdr.response)) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2930 Failed to get resource extents \"\n\t\t\t\t\"Status 0x%x Add'l Status 0x%x\\n\",\n\t\t\t\tbf_get(lpfc_mbox_hdr_status,\n\t\t\t\t       &rsrc_info->header.cfg_shdr.response),\n\t\t\t\tbf_get(lpfc_mbox_hdr_add_status,\n\t\t\t\t       &rsrc_info->header.cfg_shdr.response));\n\t\trc = -EIO;\n\t\tgoto err_exit;\n\t}\n\n\t*extnt_count = bf_get(lpfc_mbx_get_rsrc_extent_info_cnt,\n\t\t\t      &rsrc_info->u.rsp);\n\t*extnt_size = bf_get(lpfc_mbx_get_rsrc_extent_info_size,\n\t\t\t     &rsrc_info->u.rsp);\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\"3162 Retrieved extents type-%d from port: count:%d, \"\n\t\t\t\"size:%d\\n\", type, *extnt_count, *extnt_size);\n\nerr_exit:\n\tmempool_free(mbox, phba->mbox_mem_pool);\n\treturn rc;\n}\n\n/**\n * lpfc_sli4_chk_avail_extnt_rsrc - Check for available SLI4 resource extents.\n * @phba: Pointer to HBA context object.\n * @type: The extent type to check.\n *\n * This function reads the current available extents from the port and checks\n * if the extent count or extent size has changed since the last access.\n * Callers use this routine post port reset to understand if there is a\n * extent reprovisioning requirement.\n *\n * Returns:\n *   -Error: error indicates problem.\n *   1: Extent count or size has changed.\n *   0: No changes.\n **/\nstatic int\nlpfc_sli4_chk_avail_extnt_rsrc(struct lpfc_hba *phba, uint16_t type)\n{\n\tuint16_t curr_ext_cnt, rsrc_ext_cnt;\n\tuint16_t size_diff, rsrc_ext_size;\n\tint rc = 0;\n\tstruct lpfc_rsrc_blks *rsrc_entry;\n\tstruct list_head *rsrc_blk_list = NULL;\n\n\tsize_diff = 0;\n\tcurr_ext_cnt = 0;\n\trc = lpfc_sli4_get_avail_extnt_rsrc(phba, type,\n\t\t\t\t\t    &rsrc_ext_cnt,\n\t\t\t\t\t    &rsrc_ext_size);\n\tif (unlikely(rc))\n\t\treturn -EIO;\n\n\tswitch (type) {\n\tcase LPFC_RSC_TYPE_FCOE_RPI:\n\t\trsrc_blk_list = &phba->sli4_hba.lpfc_rpi_blk_list;\n\t\tbreak;\n\tcase LPFC_RSC_TYPE_FCOE_VPI:\n\t\trsrc_blk_list = &phba->lpfc_vpi_blk_list;\n\t\tbreak;\n\tcase LPFC_RSC_TYPE_FCOE_XRI:\n\t\trsrc_blk_list = &phba->sli4_hba.lpfc_xri_blk_list;\n\t\tbreak;\n\tcase LPFC_RSC_TYPE_FCOE_VFI:\n\t\trsrc_blk_list = &phba->sli4_hba.lpfc_vfi_blk_list;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tlist_for_each_entry(rsrc_entry, rsrc_blk_list, list) {\n\t\tcurr_ext_cnt++;\n\t\tif (rsrc_entry->rsrc_size != rsrc_ext_size)\n\t\t\tsize_diff++;\n\t}\n\n\tif (curr_ext_cnt != rsrc_ext_cnt || size_diff != 0)\n\t\trc = 1;\n\n\treturn rc;\n}\n\n/**\n * lpfc_sli4_cfg_post_extnts -\n * @phba: Pointer to HBA context object.\n * @extnt_cnt: number of available extents.\n * @type: the extent type (rpi, xri, vfi, vpi).\n * @emb: buffer to hold either MBX_EMBED or MBX_NEMBED operation.\n * @mbox: pointer to the caller's allocated mailbox structure.\n *\n * This function executes the extents allocation request.  It also\n * takes care of the amount of memory needed to allocate or get the\n * allocated extents. It is the caller's responsibility to evaluate\n * the response.\n *\n * Returns:\n *   -Error:  Error value describes the condition found.\n *   0: if successful\n **/\nstatic int\nlpfc_sli4_cfg_post_extnts(struct lpfc_hba *phba, uint16_t extnt_cnt,\n\t\t\t  uint16_t type, bool *emb, LPFC_MBOXQ_t *mbox)\n{\n\tint rc = 0;\n\tuint32_t req_len;\n\tuint32_t emb_len;\n\tuint32_t alloc_len, mbox_tmo;\n\n\t/* Calculate the total requested length of the dma memory */\n\treq_len = extnt_cnt * sizeof(uint16_t);\n\n\t/*\n\t * Calculate the size of an embedded mailbox.  The uint32_t\n\t * accounts for extents-specific word.\n\t */\n\temb_len = sizeof(MAILBOX_t) - sizeof(struct mbox_header) -\n\t\tsizeof(uint32_t);\n\n\t/*\n\t * Presume the allocation and response will fit into an embedded\n\t * mailbox.  If not true, reconfigure to a non-embedded mailbox.\n\t */\n\t*emb = LPFC_SLI4_MBX_EMBED;\n\tif (req_len > emb_len) {\n\t\treq_len = extnt_cnt * sizeof(uint16_t) +\n\t\t\tsizeof(union lpfc_sli4_cfg_shdr) +\n\t\t\tsizeof(uint32_t);\n\t\t*emb = LPFC_SLI4_MBX_NEMBED;\n\t}\n\n\talloc_len = lpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_COMMON,\n\t\t\t\t     LPFC_MBOX_OPCODE_ALLOC_RSRC_EXTENT,\n\t\t\t\t     req_len, *emb);\n\tif (alloc_len < req_len) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"2982 Allocated DMA memory size (x%x) is \"\n\t\t\t\"less than the requested DMA memory \"\n\t\t\t\"size (x%x)\\n\", alloc_len, req_len);\n\t\treturn -ENOMEM;\n\t}\n\trc = lpfc_sli4_mbox_rsrc_extent(phba, mbox, extnt_cnt, type, *emb);\n\tif (unlikely(rc))\n\t\treturn -EIO;\n\n\tif (!phba->sli4_hba.intr_enable)\n\t\trc = lpfc_sli_issue_mbox(phba, mbox, MBX_POLL);\n\telse {\n\t\tmbox_tmo = lpfc_mbox_tmo_val(phba, mbox);\n\t\trc = lpfc_sli_issue_mbox_wait(phba, mbox, mbox_tmo);\n\t}\n\n\tif (unlikely(rc))\n\t\trc = -EIO;\n\treturn rc;\n}\n\n/**\n * lpfc_sli4_alloc_extent - Allocate an SLI4 resource extent.\n * @phba: Pointer to HBA context object.\n * @type:  The resource extent type to allocate.\n *\n * This function allocates the number of elements for the specified\n * resource type.\n **/\nstatic int\nlpfc_sli4_alloc_extent(struct lpfc_hba *phba, uint16_t type)\n{\n\tbool emb = false;\n\tuint16_t rsrc_id_cnt, rsrc_cnt, rsrc_size;\n\tuint16_t rsrc_id, rsrc_start, j, k;\n\tuint16_t *ids;\n\tint i, rc;\n\tunsigned long longs;\n\tunsigned long *bmask;\n\tstruct lpfc_rsrc_blks *rsrc_blks;\n\tLPFC_MBOXQ_t *mbox;\n\tuint32_t length;\n\tstruct lpfc_id_range *id_array = NULL;\n\tvoid *virtaddr = NULL;\n\tstruct lpfc_mbx_nembed_rsrc_extent *n_rsrc;\n\tstruct lpfc_mbx_alloc_rsrc_extents *rsrc_ext;\n\tstruct list_head *ext_blk_list;\n\n\trc = lpfc_sli4_get_avail_extnt_rsrc(phba, type,\n\t\t\t\t\t    &rsrc_cnt,\n\t\t\t\t\t    &rsrc_size);\n\tif (unlikely(rc))\n\t\treturn -EIO;\n\n\tif ((rsrc_cnt == 0) || (rsrc_size == 0)) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"3009 No available Resource Extents \"\n\t\t\t\"for resource type 0x%x: Count: 0x%x, \"\n\t\t\t\"Size 0x%x\\n\", type, rsrc_cnt,\n\t\t\trsrc_size);\n\t\treturn -ENOMEM;\n\t}\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_MBOX | LOG_INIT | LOG_SLI,\n\t\t\t\"2903 Post resource extents type-0x%x: \"\n\t\t\t\"count:%d, size %d\\n\", type, rsrc_cnt, rsrc_size);\n\n\tmbox = (LPFC_MBOXQ_t *) mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox)\n\t\treturn -ENOMEM;\n\n\trc = lpfc_sli4_cfg_post_extnts(phba, rsrc_cnt, type, &emb, mbox);\n\tif (unlikely(rc)) {\n\t\trc = -EIO;\n\t\tgoto err_exit;\n\t}\n\n\t/*\n\t * Figure out where the response is located.  Then get local pointers\n\t * to the response data.  The port does not guarantee to respond to\n\t * all extents counts request so update the local variable with the\n\t * allocated count from the port.\n\t */\n\tif (emb == LPFC_SLI4_MBX_EMBED) {\n\t\trsrc_ext = &mbox->u.mqe.un.alloc_rsrc_extents;\n\t\tid_array = &rsrc_ext->u.rsp.id[0];\n\t\trsrc_cnt = bf_get(lpfc_mbx_rsrc_cnt, &rsrc_ext->u.rsp);\n\t} else {\n\t\tvirtaddr = mbox->sge_array->addr[0];\n\t\tn_rsrc = (struct lpfc_mbx_nembed_rsrc_extent *) virtaddr;\n\t\trsrc_cnt = bf_get(lpfc_mbx_rsrc_cnt, n_rsrc);\n\t\tid_array = &n_rsrc->id;\n\t}\n\n\tlongs = ((rsrc_cnt * rsrc_size) + BITS_PER_LONG - 1) / BITS_PER_LONG;\n\trsrc_id_cnt = rsrc_cnt * rsrc_size;\n\n\t/*\n\t * Based on the resource size and count, correct the base and max\n\t * resource values.\n\t */\n\tlength = sizeof(struct lpfc_rsrc_blks);\n\tswitch (type) {\n\tcase LPFC_RSC_TYPE_FCOE_RPI:\n\t\tphba->sli4_hba.rpi_bmask = kcalloc(longs,\n\t\t\t\t\t\t   sizeof(unsigned long),\n\t\t\t\t\t\t   GFP_KERNEL);\n\t\tif (unlikely(!phba->sli4_hba.rpi_bmask)) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err_exit;\n\t\t}\n\t\tphba->sli4_hba.rpi_ids = kcalloc(rsrc_id_cnt,\n\t\t\t\t\t\t sizeof(uint16_t),\n\t\t\t\t\t\t GFP_KERNEL);\n\t\tif (unlikely(!phba->sli4_hba.rpi_ids)) {\n\t\t\tkfree(phba->sli4_hba.rpi_bmask);\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err_exit;\n\t\t}\n\n\t\t/*\n\t\t * The next_rpi was initialized with the maximum available\n\t\t * count but the port may allocate a smaller number.  Catch\n\t\t * that case and update the next_rpi.\n\t\t */\n\t\tphba->sli4_hba.next_rpi = rsrc_id_cnt;\n\n\t\t/* Initialize local ptrs for common extent processing later. */\n\t\tbmask = phba->sli4_hba.rpi_bmask;\n\t\tids = phba->sli4_hba.rpi_ids;\n\t\text_blk_list = &phba->sli4_hba.lpfc_rpi_blk_list;\n\t\tbreak;\n\tcase LPFC_RSC_TYPE_FCOE_VPI:\n\t\tphba->vpi_bmask = kcalloc(longs, sizeof(unsigned long),\n\t\t\t\t\t  GFP_KERNEL);\n\t\tif (unlikely(!phba->vpi_bmask)) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err_exit;\n\t\t}\n\t\tphba->vpi_ids = kcalloc(rsrc_id_cnt, sizeof(uint16_t),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif (unlikely(!phba->vpi_ids)) {\n\t\t\tkfree(phba->vpi_bmask);\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err_exit;\n\t\t}\n\n\t\t/* Initialize local ptrs for common extent processing later. */\n\t\tbmask = phba->vpi_bmask;\n\t\tids = phba->vpi_ids;\n\t\text_blk_list = &phba->lpfc_vpi_blk_list;\n\t\tbreak;\n\tcase LPFC_RSC_TYPE_FCOE_XRI:\n\t\tphba->sli4_hba.xri_bmask = kcalloc(longs,\n\t\t\t\t\t\t   sizeof(unsigned long),\n\t\t\t\t\t\t   GFP_KERNEL);\n\t\tif (unlikely(!phba->sli4_hba.xri_bmask)) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err_exit;\n\t\t}\n\t\tphba->sli4_hba.max_cfg_param.xri_used = 0;\n\t\tphba->sli4_hba.xri_ids = kcalloc(rsrc_id_cnt,\n\t\t\t\t\t\t sizeof(uint16_t),\n\t\t\t\t\t\t GFP_KERNEL);\n\t\tif (unlikely(!phba->sli4_hba.xri_ids)) {\n\t\t\tkfree(phba->sli4_hba.xri_bmask);\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err_exit;\n\t\t}\n\n\t\t/* Initialize local ptrs for common extent processing later. */\n\t\tbmask = phba->sli4_hba.xri_bmask;\n\t\tids = phba->sli4_hba.xri_ids;\n\t\text_blk_list = &phba->sli4_hba.lpfc_xri_blk_list;\n\t\tbreak;\n\tcase LPFC_RSC_TYPE_FCOE_VFI:\n\t\tphba->sli4_hba.vfi_bmask = kcalloc(longs,\n\t\t\t\t\t\t   sizeof(unsigned long),\n\t\t\t\t\t\t   GFP_KERNEL);\n\t\tif (unlikely(!phba->sli4_hba.vfi_bmask)) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err_exit;\n\t\t}\n\t\tphba->sli4_hba.vfi_ids = kcalloc(rsrc_id_cnt,\n\t\t\t\t\t\t sizeof(uint16_t),\n\t\t\t\t\t\t GFP_KERNEL);\n\t\tif (unlikely(!phba->sli4_hba.vfi_ids)) {\n\t\t\tkfree(phba->sli4_hba.vfi_bmask);\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err_exit;\n\t\t}\n\n\t\t/* Initialize local ptrs for common extent processing later. */\n\t\tbmask = phba->sli4_hba.vfi_bmask;\n\t\tids = phba->sli4_hba.vfi_ids;\n\t\text_blk_list = &phba->sli4_hba.lpfc_vfi_blk_list;\n\t\tbreak;\n\tdefault:\n\t\t/* Unsupported Opcode.  Fail call. */\n\t\tid_array = NULL;\n\t\tbmask = NULL;\n\t\tids = NULL;\n\t\text_blk_list = NULL;\n\t\tgoto err_exit;\n\t}\n\n\t/*\n\t * Complete initializing the extent configuration with the\n\t * allocated ids assigned to this function.  The bitmask serves\n\t * as an index into the array and manages the available ids.  The\n\t * array just stores the ids communicated to the port via the wqes.\n\t */\n\tfor (i = 0, j = 0, k = 0; i < rsrc_cnt; i++) {\n\t\tif ((i % 2) == 0)\n\t\t\trsrc_id = bf_get(lpfc_mbx_rsrc_id_word4_0,\n\t\t\t\t\t &id_array[k]);\n\t\telse\n\t\t\trsrc_id = bf_get(lpfc_mbx_rsrc_id_word4_1,\n\t\t\t\t\t &id_array[k]);\n\n\t\trsrc_blks = kzalloc(length, GFP_KERNEL);\n\t\tif (unlikely(!rsrc_blks)) {\n\t\t\trc = -ENOMEM;\n\t\t\tkfree(bmask);\n\t\t\tkfree(ids);\n\t\t\tgoto err_exit;\n\t\t}\n\t\trsrc_blks->rsrc_start = rsrc_id;\n\t\trsrc_blks->rsrc_size = rsrc_size;\n\t\tlist_add_tail(&rsrc_blks->list, ext_blk_list);\n\t\trsrc_start = rsrc_id;\n\t\tif ((type == LPFC_RSC_TYPE_FCOE_XRI) && (j == 0)) {\n\t\t\tphba->sli4_hba.io_xri_start = rsrc_start +\n\t\t\t\tlpfc_sli4_get_iocb_cnt(phba);\n\t\t}\n\n\t\twhile (rsrc_id < (rsrc_start + rsrc_size)) {\n\t\t\tids[j] = rsrc_id;\n\t\t\trsrc_id++;\n\t\t\tj++;\n\t\t}\n\t\t/* Entire word processed.  Get next word.*/\n\t\tif ((i % 2) == 1)\n\t\t\tk++;\n\t}\n err_exit:\n\tlpfc_sli4_mbox_cmd_free(phba, mbox);\n\treturn rc;\n}\n\n\n\n/**\n * lpfc_sli4_dealloc_extent - Deallocate an SLI4 resource extent.\n * @phba: Pointer to HBA context object.\n * @type: the extent's type.\n *\n * This function deallocates all extents of a particular resource type.\n * SLI4 does not allow for deallocating a particular extent range.  It\n * is the caller's responsibility to release all kernel memory resources.\n **/\nstatic int\nlpfc_sli4_dealloc_extent(struct lpfc_hba *phba, uint16_t type)\n{\n\tint rc;\n\tuint32_t length, mbox_tmo = 0;\n\tLPFC_MBOXQ_t *mbox;\n\tstruct lpfc_mbx_dealloc_rsrc_extents *dealloc_rsrc;\n\tstruct lpfc_rsrc_blks *rsrc_blk, *rsrc_blk_next;\n\n\tmbox = (LPFC_MBOXQ_t *) mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * This function sends an embedded mailbox because it only sends the\n\t * the resource type.  All extents of this type are released by the\n\t * port.\n\t */\n\tlength = (sizeof(struct lpfc_mbx_dealloc_rsrc_extents) -\n\t\t  sizeof(struct lpfc_sli4_cfg_mhdr));\n\tlpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_COMMON,\n\t\t\t LPFC_MBOX_OPCODE_DEALLOC_RSRC_EXTENT,\n\t\t\t length, LPFC_SLI4_MBX_EMBED);\n\n\t/* Send an extents count of 0 - the dealloc doesn't use it. */\n\trc = lpfc_sli4_mbox_rsrc_extent(phba, mbox, 0, type,\n\t\t\t\t\tLPFC_SLI4_MBX_EMBED);\n\tif (unlikely(rc)) {\n\t\trc = -EIO;\n\t\tgoto out_free_mbox;\n\t}\n\tif (!phba->sli4_hba.intr_enable)\n\t\trc = lpfc_sli_issue_mbox(phba, mbox, MBX_POLL);\n\telse {\n\t\tmbox_tmo = lpfc_mbox_tmo_val(phba, mbox);\n\t\trc = lpfc_sli_issue_mbox_wait(phba, mbox, mbox_tmo);\n\t}\n\tif (unlikely(rc)) {\n\t\trc = -EIO;\n\t\tgoto out_free_mbox;\n\t}\n\n\tdealloc_rsrc = &mbox->u.mqe.un.dealloc_rsrc_extents;\n\tif (bf_get(lpfc_mbox_hdr_status,\n\t\t   &dealloc_rsrc->header.cfg_shdr.response)) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2919 Failed to release resource extents \"\n\t\t\t\t\"for type %d - Status 0x%x Add'l Status 0x%x. \"\n\t\t\t\t\"Resource memory not released.\\n\",\n\t\t\t\ttype,\n\t\t\t\tbf_get(lpfc_mbox_hdr_status,\n\t\t\t\t    &dealloc_rsrc->header.cfg_shdr.response),\n\t\t\t\tbf_get(lpfc_mbox_hdr_add_status,\n\t\t\t\t    &dealloc_rsrc->header.cfg_shdr.response));\n\t\trc = -EIO;\n\t\tgoto out_free_mbox;\n\t}\n\n\t/* Release kernel memory resources for the specific type. */\n\tswitch (type) {\n\tcase LPFC_RSC_TYPE_FCOE_VPI:\n\t\tkfree(phba->vpi_bmask);\n\t\tkfree(phba->vpi_ids);\n\t\tbf_set(lpfc_vpi_rsrc_rdy, &phba->sli4_hba.sli4_flags, 0);\n\t\tlist_for_each_entry_safe(rsrc_blk, rsrc_blk_next,\n\t\t\t\t    &phba->lpfc_vpi_blk_list, list) {\n\t\t\tlist_del_init(&rsrc_blk->list);\n\t\t\tkfree(rsrc_blk);\n\t\t}\n\t\tphba->sli4_hba.max_cfg_param.vpi_used = 0;\n\t\tbreak;\n\tcase LPFC_RSC_TYPE_FCOE_XRI:\n\t\tkfree(phba->sli4_hba.xri_bmask);\n\t\tkfree(phba->sli4_hba.xri_ids);\n\t\tlist_for_each_entry_safe(rsrc_blk, rsrc_blk_next,\n\t\t\t\t    &phba->sli4_hba.lpfc_xri_blk_list, list) {\n\t\t\tlist_del_init(&rsrc_blk->list);\n\t\t\tkfree(rsrc_blk);\n\t\t}\n\t\tbreak;\n\tcase LPFC_RSC_TYPE_FCOE_VFI:\n\t\tkfree(phba->sli4_hba.vfi_bmask);\n\t\tkfree(phba->sli4_hba.vfi_ids);\n\t\tbf_set(lpfc_vfi_rsrc_rdy, &phba->sli4_hba.sli4_flags, 0);\n\t\tlist_for_each_entry_safe(rsrc_blk, rsrc_blk_next,\n\t\t\t\t    &phba->sli4_hba.lpfc_vfi_blk_list, list) {\n\t\t\tlist_del_init(&rsrc_blk->list);\n\t\t\tkfree(rsrc_blk);\n\t\t}\n\t\tbreak;\n\tcase LPFC_RSC_TYPE_FCOE_RPI:\n\t\t/* RPI bitmask and physical id array are cleaned up earlier. */\n\t\tlist_for_each_entry_safe(rsrc_blk, rsrc_blk_next,\n\t\t\t\t    &phba->sli4_hba.lpfc_rpi_blk_list, list) {\n\t\t\tlist_del_init(&rsrc_blk->list);\n\t\t\tkfree(rsrc_blk);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tbf_set(lpfc_idx_rsrc_rdy, &phba->sli4_hba.sli4_flags, 0);\n\n out_free_mbox:\n\tmempool_free(mbox, phba->mbox_mem_pool);\n\treturn rc;\n}\n\nstatic void\nlpfc_set_features(struct lpfc_hba *phba, LPFC_MBOXQ_t *mbox,\n\t\t  uint32_t feature)\n{\n\tuint32_t len;\n\n\tlen = sizeof(struct lpfc_mbx_set_feature) -\n\t\tsizeof(struct lpfc_sli4_cfg_mhdr);\n\tlpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_COMMON,\n\t\t\t LPFC_MBOX_OPCODE_SET_FEATURES, len,\n\t\t\t LPFC_SLI4_MBX_EMBED);\n\n\tswitch (feature) {\n\tcase LPFC_SET_UE_RECOVERY:\n\t\tbf_set(lpfc_mbx_set_feature_UER,\n\t\t       &mbox->u.mqe.un.set_feature, 1);\n\t\tmbox->u.mqe.un.set_feature.feature = LPFC_SET_UE_RECOVERY;\n\t\tmbox->u.mqe.un.set_feature.param_len = 8;\n\t\tbreak;\n\tcase LPFC_SET_MDS_DIAGS:\n\t\tbf_set(lpfc_mbx_set_feature_mds,\n\t\t       &mbox->u.mqe.un.set_feature, 1);\n\t\tbf_set(lpfc_mbx_set_feature_mds_deep_loopbk,\n\t\t       &mbox->u.mqe.un.set_feature, 1);\n\t\tmbox->u.mqe.un.set_feature.feature = LPFC_SET_MDS_DIAGS;\n\t\tmbox->u.mqe.un.set_feature.param_len = 8;\n\t\tbreak;\n\tcase LPFC_SET_DUAL_DUMP:\n\t\tbf_set(lpfc_mbx_set_feature_dd,\n\t\t       &mbox->u.mqe.un.set_feature, LPFC_ENABLE_DUAL_DUMP);\n\t\tbf_set(lpfc_mbx_set_feature_ddquery,\n\t\t       &mbox->u.mqe.un.set_feature, 0);\n\t\tmbox->u.mqe.un.set_feature.feature = LPFC_SET_DUAL_DUMP;\n\t\tmbox->u.mqe.un.set_feature.param_len = 4;\n\t\tbreak;\n\t}\n\n\treturn;\n}\n\n/**\n * lpfc_ras_stop_fwlog: Disable FW logging by the adapter\n * @phba: Pointer to HBA context object.\n *\n * Disable FW logging into host memory on the adapter. To\n * be done before reading logs from the host memory.\n **/\nvoid\nlpfc_ras_stop_fwlog(struct lpfc_hba *phba)\n{\n\tstruct lpfc_ras_fwlog *ras_fwlog = &phba->ras_fwlog;\n\n\tspin_lock_irq(&phba->hbalock);\n\tras_fwlog->state = INACTIVE;\n\tspin_unlock_irq(&phba->hbalock);\n\n\t/* Disable FW logging to host memory */\n\twritel(LPFC_CTL_PDEV_CTL_DDL_RAS,\n\t       phba->sli4_hba.conf_regs_memmap_p + LPFC_CTL_PDEV_CTL_OFFSET);\n\n\t/* Wait 10ms for firmware to stop using DMA buffer */\n\tusleep_range(10 * 1000, 20 * 1000);\n}\n\n/**\n * lpfc_sli4_ras_dma_free - Free memory allocated for FW logging.\n * @phba: Pointer to HBA context object.\n *\n * This function is called to free memory allocated for RAS FW logging\n * support in the driver.\n **/\nvoid\nlpfc_sli4_ras_dma_free(struct lpfc_hba *phba)\n{\n\tstruct lpfc_ras_fwlog *ras_fwlog = &phba->ras_fwlog;\n\tstruct lpfc_dmabuf *dmabuf, *next;\n\n\tif (!list_empty(&ras_fwlog->fwlog_buff_list)) {\n\t\tlist_for_each_entry_safe(dmabuf, next,\n\t\t\t\t    &ras_fwlog->fwlog_buff_list,\n\t\t\t\t    list) {\n\t\t\tlist_del(&dmabuf->list);\n\t\t\tdma_free_coherent(&phba->pcidev->dev,\n\t\t\t\t\t  LPFC_RAS_MAX_ENTRY_SIZE,\n\t\t\t\t\t  dmabuf->virt, dmabuf->phys);\n\t\t\tkfree(dmabuf);\n\t\t}\n\t}\n\n\tif (ras_fwlog->lwpd.virt) {\n\t\tdma_free_coherent(&phba->pcidev->dev,\n\t\t\t\t  sizeof(uint32_t) * 2,\n\t\t\t\t  ras_fwlog->lwpd.virt,\n\t\t\t\t  ras_fwlog->lwpd.phys);\n\t\tras_fwlog->lwpd.virt = NULL;\n\t}\n\n\tspin_lock_irq(&phba->hbalock);\n\tras_fwlog->state = INACTIVE;\n\tspin_unlock_irq(&phba->hbalock);\n}\n\n/**\n * lpfc_sli4_ras_dma_alloc: Allocate memory for FW support\n * @phba: Pointer to HBA context object.\n * @fwlog_buff_count: Count of buffers to be created.\n *\n * This routine DMA memory for Log Write Position Data[LPWD] and buffer\n * to update FW log is posted to the adapter.\n * Buffer count is calculated based on module param ras_fwlog_buffsize\n * Size of each buffer posted to FW is 64K.\n **/\n\nstatic int\nlpfc_sli4_ras_dma_alloc(struct lpfc_hba *phba,\n\t\t\tuint32_t fwlog_buff_count)\n{\n\tstruct lpfc_ras_fwlog *ras_fwlog = &phba->ras_fwlog;\n\tstruct lpfc_dmabuf *dmabuf;\n\tint rc = 0, i = 0;\n\n\t/* Initialize List */\n\tINIT_LIST_HEAD(&ras_fwlog->fwlog_buff_list);\n\n\t/* Allocate memory for the LWPD */\n\tras_fwlog->lwpd.virt = dma_alloc_coherent(&phba->pcidev->dev,\n\t\t\t\t\t    sizeof(uint32_t) * 2,\n\t\t\t\t\t    &ras_fwlog->lwpd.phys,\n\t\t\t\t\t    GFP_KERNEL);\n\tif (!ras_fwlog->lwpd.virt) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6185 LWPD Memory Alloc Failed\\n\");\n\n\t\treturn -ENOMEM;\n\t}\n\n\tras_fwlog->fw_buffcount = fwlog_buff_count;\n\tfor (i = 0; i < ras_fwlog->fw_buffcount; i++) {\n\t\tdmabuf = kzalloc(sizeof(struct lpfc_dmabuf),\n\t\t\t\t GFP_KERNEL);\n\t\tif (!dmabuf) {\n\t\t\trc = -ENOMEM;\n\t\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_INIT,\n\t\t\t\t\t\"6186 Memory Alloc failed FW logging\");\n\t\t\tgoto free_mem;\n\t\t}\n\n\t\tdmabuf->virt = dma_alloc_coherent(&phba->pcidev->dev,\n\t\t\t\t\t\t  LPFC_RAS_MAX_ENTRY_SIZE,\n\t\t\t\t\t\t  &dmabuf->phys, GFP_KERNEL);\n\t\tif (!dmabuf->virt) {\n\t\t\tkfree(dmabuf);\n\t\t\trc = -ENOMEM;\n\t\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_INIT,\n\t\t\t\t\t\"6187 DMA Alloc Failed FW logging\");\n\t\t\tgoto free_mem;\n\t\t}\n\t\tdmabuf->buffer_tag = i;\n\t\tlist_add_tail(&dmabuf->list, &ras_fwlog->fwlog_buff_list);\n\t}\n\nfree_mem:\n\tif (rc)\n\t\tlpfc_sli4_ras_dma_free(phba);\n\n\treturn rc;\n}\n\n/**\n * lpfc_sli4_ras_mbox_cmpl: Completion handler for RAS MBX command\n * @phba: pointer to lpfc hba data structure.\n * @pmb: pointer to the driver internal queue element for mailbox command.\n *\n * Completion handler for driver's RAS MBX command to the device.\n **/\nstatic void\nlpfc_sli4_ras_mbox_cmpl(struct lpfc_hba *phba, LPFC_MBOXQ_t *pmb)\n{\n\tMAILBOX_t *mb;\n\tunion lpfc_sli4_cfg_shdr *shdr;\n\tuint32_t shdr_status, shdr_add_status;\n\tstruct lpfc_ras_fwlog *ras_fwlog = &phba->ras_fwlog;\n\n\tmb = &pmb->u.mb;\n\n\tshdr = (union lpfc_sli4_cfg_shdr *)\n\t\t&pmb->u.mqe.un.ras_fwlog.header.cfg_shdr;\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\n\tif (mb->mbxStatus != MBX_SUCCESS || shdr_status) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6188 FW LOG mailbox \"\n\t\t\t\t\"completed with status x%x add_status x%x,\"\n\t\t\t\t\" mbx status x%x\\n\",\n\t\t\t\tshdr_status, shdr_add_status, mb->mbxStatus);\n\n\t\tras_fwlog->ras_hwsupport = false;\n\t\tgoto disable_ras;\n\t}\n\n\tspin_lock_irq(&phba->hbalock);\n\tras_fwlog->state = ACTIVE;\n\tspin_unlock_irq(&phba->hbalock);\n\tmempool_free(pmb, phba->mbox_mem_pool);\n\n\treturn;\n\ndisable_ras:\n\t/* Free RAS DMA memory */\n\tlpfc_sli4_ras_dma_free(phba);\n\tmempool_free(pmb, phba->mbox_mem_pool);\n}\n\n/**\n * lpfc_sli4_ras_fwlog_init: Initialize memory and post RAS MBX command\n * @phba: pointer to lpfc hba data structure.\n * @fwlog_level: Logging verbosity level.\n * @fwlog_enable: Enable/Disable logging.\n *\n * Initialize memory and post mailbox command to enable FW logging in host\n * memory.\n **/\nint\nlpfc_sli4_ras_fwlog_init(struct lpfc_hba *phba,\n\t\t\t uint32_t fwlog_level,\n\t\t\t uint32_t fwlog_enable)\n{\n\tstruct lpfc_ras_fwlog *ras_fwlog = &phba->ras_fwlog;\n\tstruct lpfc_mbx_set_ras_fwlog *mbx_fwlog = NULL;\n\tstruct lpfc_dmabuf *dmabuf;\n\tLPFC_MBOXQ_t *mbox;\n\tuint32_t len = 0, fwlog_buffsize, fwlog_entry_count;\n\tint rc = 0;\n\n\tspin_lock_irq(&phba->hbalock);\n\tras_fwlog->state = INACTIVE;\n\tspin_unlock_irq(&phba->hbalock);\n\n\tfwlog_buffsize = (LPFC_RAS_MIN_BUFF_POST_SIZE *\n\t\t\t  phba->cfg_ras_fwlog_buffsize);\n\tfwlog_entry_count = (fwlog_buffsize/LPFC_RAS_MAX_ENTRY_SIZE);\n\n\t/*\n\t * If re-enabling FW logging support use earlier allocated\n\t * DMA buffers while posting MBX command.\n\t **/\n\tif (!ras_fwlog->lwpd.virt) {\n\t\trc = lpfc_sli4_ras_dma_alloc(phba, fwlog_entry_count);\n\t\tif (rc) {\n\t\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_INIT,\n\t\t\t\t\t\"6189 FW Log Memory Allocation Failed\");\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\t/* Setup Mailbox command */\n\tmbox = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6190 RAS MBX Alloc Failed\");\n\t\trc = -ENOMEM;\n\t\tgoto mem_free;\n\t}\n\n\tras_fwlog->fw_loglevel = fwlog_level;\n\tlen = (sizeof(struct lpfc_mbx_set_ras_fwlog) -\n\t\tsizeof(struct lpfc_sli4_cfg_mhdr));\n\n\tlpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_LOWLEVEL,\n\t\t\t LPFC_MBOX_OPCODE_SET_DIAG_LOG_OPTION,\n\t\t\t len, LPFC_SLI4_MBX_EMBED);\n\n\tmbx_fwlog = (struct lpfc_mbx_set_ras_fwlog *)&mbox->u.mqe.un.ras_fwlog;\n\tbf_set(lpfc_fwlog_enable, &mbx_fwlog->u.request,\n\t       fwlog_enable);\n\tbf_set(lpfc_fwlog_loglvl, &mbx_fwlog->u.request,\n\t       ras_fwlog->fw_loglevel);\n\tbf_set(lpfc_fwlog_buffcnt, &mbx_fwlog->u.request,\n\t       ras_fwlog->fw_buffcount);\n\tbf_set(lpfc_fwlog_buffsz, &mbx_fwlog->u.request,\n\t       LPFC_RAS_MAX_ENTRY_SIZE/SLI4_PAGE_SIZE);\n\n\t/* Update DMA buffer address */\n\tlist_for_each_entry(dmabuf, &ras_fwlog->fwlog_buff_list, list) {\n\t\tmemset(dmabuf->virt, 0, LPFC_RAS_MAX_ENTRY_SIZE);\n\n\t\tmbx_fwlog->u.request.buff_fwlog[dmabuf->buffer_tag].addr_lo =\n\t\t\tputPaddrLow(dmabuf->phys);\n\n\t\tmbx_fwlog->u.request.buff_fwlog[dmabuf->buffer_tag].addr_hi =\n\t\t\tputPaddrHigh(dmabuf->phys);\n\t}\n\n\t/* Update LPWD address */\n\tmbx_fwlog->u.request.lwpd.addr_lo = putPaddrLow(ras_fwlog->lwpd.phys);\n\tmbx_fwlog->u.request.lwpd.addr_hi = putPaddrHigh(ras_fwlog->lwpd.phys);\n\n\tspin_lock_irq(&phba->hbalock);\n\tras_fwlog->state = REG_INPROGRESS;\n\tspin_unlock_irq(&phba->hbalock);\n\tmbox->vport = phba->pport;\n\tmbox->mbox_cmpl = lpfc_sli4_ras_mbox_cmpl;\n\n\trc = lpfc_sli_issue_mbox(phba, mbox, MBX_NOWAIT);\n\n\tif (rc == MBX_NOT_FINISHED) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6191 FW-Log Mailbox failed. \"\n\t\t\t\t\"status %d mbxStatus : x%x\", rc,\n\t\t\t\tbf_get(lpfc_mqe_status, &mbox->u.mqe));\n\t\tmempool_free(mbox, phba->mbox_mem_pool);\n\t\trc = -EIO;\n\t\tgoto mem_free;\n\t} else\n\t\trc = 0;\nmem_free:\n\tif (rc)\n\t\tlpfc_sli4_ras_dma_free(phba);\n\n\treturn rc;\n}\n\n/**\n * lpfc_sli4_ras_setup - Check if RAS supported on the adapter\n * @phba: Pointer to HBA context object.\n *\n * Check if RAS is supported on the adapter and initialize it.\n **/\nvoid\nlpfc_sli4_ras_setup(struct lpfc_hba *phba)\n{\n\t/* Check RAS FW Log needs to be enabled or not */\n\tif (lpfc_check_fwlog_support(phba))\n\t\treturn;\n\n\tlpfc_sli4_ras_fwlog_init(phba, phba->cfg_ras_fwlog_level,\n\t\t\t\t LPFC_RAS_ENABLE_LOGGING);\n}\n\n/**\n * lpfc_sli4_alloc_resource_identifiers - Allocate all SLI4 resource extents.\n * @phba: Pointer to HBA context object.\n *\n * This function allocates all SLI4 resource identifiers.\n **/\nint\nlpfc_sli4_alloc_resource_identifiers(struct lpfc_hba *phba)\n{\n\tint i, rc, error = 0;\n\tuint16_t count, base;\n\tunsigned long longs;\n\n\tif (!phba->sli4_hba.rpi_hdrs_in_use)\n\t\tphba->sli4_hba.next_rpi = phba->sli4_hba.max_cfg_param.max_rpi;\n\tif (phba->sli4_hba.extents_in_use) {\n\t\t/*\n\t\t * The port supports resource extents. The XRI, VPI, VFI, RPI\n\t\t * resource extent count must be read and allocated before\n\t\t * provisioning the resource id arrays.\n\t\t */\n\t\tif (bf_get(lpfc_idx_rsrc_rdy, &phba->sli4_hba.sli4_flags) ==\n\t\t    LPFC_IDX_RSRC_RDY) {\n\t\t\t/*\n\t\t\t * Extent-based resources are set - the driver could\n\t\t\t * be in a port reset. Figure out if any corrective\n\t\t\t * actions need to be taken.\n\t\t\t */\n\t\t\trc = lpfc_sli4_chk_avail_extnt_rsrc(phba,\n\t\t\t\t\t\t LPFC_RSC_TYPE_FCOE_VFI);\n\t\t\tif (rc != 0)\n\t\t\t\terror++;\n\t\t\trc = lpfc_sli4_chk_avail_extnt_rsrc(phba,\n\t\t\t\t\t\t LPFC_RSC_TYPE_FCOE_VPI);\n\t\t\tif (rc != 0)\n\t\t\t\terror++;\n\t\t\trc = lpfc_sli4_chk_avail_extnt_rsrc(phba,\n\t\t\t\t\t\t LPFC_RSC_TYPE_FCOE_XRI);\n\t\t\tif (rc != 0)\n\t\t\t\terror++;\n\t\t\trc = lpfc_sli4_chk_avail_extnt_rsrc(phba,\n\t\t\t\t\t\t LPFC_RSC_TYPE_FCOE_RPI);\n\t\t\tif (rc != 0)\n\t\t\t\terror++;\n\n\t\t\t/*\n\t\t\t * It's possible that the number of resources\n\t\t\t * provided to this port instance changed between\n\t\t\t * resets.  Detect this condition and reallocate\n\t\t\t * resources.  Otherwise, there is no action.\n\t\t\t */\n\t\t\tif (error) {\n\t\t\t\tlpfc_printf_log(phba, KERN_INFO,\n\t\t\t\t\t\tLOG_MBOX | LOG_INIT,\n\t\t\t\t\t\t\"2931 Detected extent resource \"\n\t\t\t\t\t\t\"change.  Reallocating all \"\n\t\t\t\t\t\t\"extents.\\n\");\n\t\t\t\trc = lpfc_sli4_dealloc_extent(phba,\n\t\t\t\t\t\t LPFC_RSC_TYPE_FCOE_VFI);\n\t\t\t\trc = lpfc_sli4_dealloc_extent(phba,\n\t\t\t\t\t\t LPFC_RSC_TYPE_FCOE_VPI);\n\t\t\t\trc = lpfc_sli4_dealloc_extent(phba,\n\t\t\t\t\t\t LPFC_RSC_TYPE_FCOE_XRI);\n\t\t\t\trc = lpfc_sli4_dealloc_extent(phba,\n\t\t\t\t\t\t LPFC_RSC_TYPE_FCOE_RPI);\n\t\t\t} else\n\t\t\t\treturn 0;\n\t\t}\n\n\t\trc = lpfc_sli4_alloc_extent(phba, LPFC_RSC_TYPE_FCOE_VFI);\n\t\tif (unlikely(rc))\n\t\t\tgoto err_exit;\n\n\t\trc = lpfc_sli4_alloc_extent(phba, LPFC_RSC_TYPE_FCOE_VPI);\n\t\tif (unlikely(rc))\n\t\t\tgoto err_exit;\n\n\t\trc = lpfc_sli4_alloc_extent(phba, LPFC_RSC_TYPE_FCOE_RPI);\n\t\tif (unlikely(rc))\n\t\t\tgoto err_exit;\n\n\t\trc = lpfc_sli4_alloc_extent(phba, LPFC_RSC_TYPE_FCOE_XRI);\n\t\tif (unlikely(rc))\n\t\t\tgoto err_exit;\n\t\tbf_set(lpfc_idx_rsrc_rdy, &phba->sli4_hba.sli4_flags,\n\t\t       LPFC_IDX_RSRC_RDY);\n\t\treturn rc;\n\t} else {\n\t\t/*\n\t\t * The port does not support resource extents.  The XRI, VPI,\n\t\t * VFI, RPI resource ids were determined from READ_CONFIG.\n\t\t * Just allocate the bitmasks and provision the resource id\n\t\t * arrays.  If a port reset is active, the resources don't\n\t\t * need any action - just exit.\n\t\t */\n\t\tif (bf_get(lpfc_idx_rsrc_rdy, &phba->sli4_hba.sli4_flags) ==\n\t\t    LPFC_IDX_RSRC_RDY) {\n\t\t\tlpfc_sli4_dealloc_resource_identifiers(phba);\n\t\t\tlpfc_sli4_remove_rpis(phba);\n\t\t}\n\t\t/* RPIs. */\n\t\tcount = phba->sli4_hba.max_cfg_param.max_rpi;\n\t\tif (count <= 0) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"3279 Invalid provisioning of \"\n\t\t\t\t\t\"rpi:%d\\n\", count);\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_exit;\n\t\t}\n\t\tbase = phba->sli4_hba.max_cfg_param.rpi_base;\n\t\tlongs = (count + BITS_PER_LONG - 1) / BITS_PER_LONG;\n\t\tphba->sli4_hba.rpi_bmask = kcalloc(longs,\n\t\t\t\t\t\t   sizeof(unsigned long),\n\t\t\t\t\t\t   GFP_KERNEL);\n\t\tif (unlikely(!phba->sli4_hba.rpi_bmask)) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err_exit;\n\t\t}\n\t\tphba->sli4_hba.rpi_ids = kcalloc(count, sizeof(uint16_t),\n\t\t\t\t\t\t GFP_KERNEL);\n\t\tif (unlikely(!phba->sli4_hba.rpi_ids)) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto free_rpi_bmask;\n\t\t}\n\n\t\tfor (i = 0; i < count; i++)\n\t\t\tphba->sli4_hba.rpi_ids[i] = base + i;\n\n\t\t/* VPIs. */\n\t\tcount = phba->sli4_hba.max_cfg_param.max_vpi;\n\t\tif (count <= 0) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"3280 Invalid provisioning of \"\n\t\t\t\t\t\"vpi:%d\\n\", count);\n\t\t\trc = -EINVAL;\n\t\t\tgoto free_rpi_ids;\n\t\t}\n\t\tbase = phba->sli4_hba.max_cfg_param.vpi_base;\n\t\tlongs = (count + BITS_PER_LONG - 1) / BITS_PER_LONG;\n\t\tphba->vpi_bmask = kcalloc(longs, sizeof(unsigned long),\n\t\t\t\t\t  GFP_KERNEL);\n\t\tif (unlikely(!phba->vpi_bmask)) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto free_rpi_ids;\n\t\t}\n\t\tphba->vpi_ids = kcalloc(count, sizeof(uint16_t),\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (unlikely(!phba->vpi_ids)) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto free_vpi_bmask;\n\t\t}\n\n\t\tfor (i = 0; i < count; i++)\n\t\t\tphba->vpi_ids[i] = base + i;\n\n\t\t/* XRIs. */\n\t\tcount = phba->sli4_hba.max_cfg_param.max_xri;\n\t\tif (count <= 0) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"3281 Invalid provisioning of \"\n\t\t\t\t\t\"xri:%d\\n\", count);\n\t\t\trc = -EINVAL;\n\t\t\tgoto free_vpi_ids;\n\t\t}\n\t\tbase = phba->sli4_hba.max_cfg_param.xri_base;\n\t\tlongs = (count + BITS_PER_LONG - 1) / BITS_PER_LONG;\n\t\tphba->sli4_hba.xri_bmask = kcalloc(longs,\n\t\t\t\t\t\t   sizeof(unsigned long),\n\t\t\t\t\t\t   GFP_KERNEL);\n\t\tif (unlikely(!phba->sli4_hba.xri_bmask)) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto free_vpi_ids;\n\t\t}\n\t\tphba->sli4_hba.max_cfg_param.xri_used = 0;\n\t\tphba->sli4_hba.xri_ids = kcalloc(count, sizeof(uint16_t),\n\t\t\t\t\t\t GFP_KERNEL);\n\t\tif (unlikely(!phba->sli4_hba.xri_ids)) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto free_xri_bmask;\n\t\t}\n\n\t\tfor (i = 0; i < count; i++)\n\t\t\tphba->sli4_hba.xri_ids[i] = base + i;\n\n\t\t/* VFIs. */\n\t\tcount = phba->sli4_hba.max_cfg_param.max_vfi;\n\t\tif (count <= 0) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"3282 Invalid provisioning of \"\n\t\t\t\t\t\"vfi:%d\\n\", count);\n\t\t\trc = -EINVAL;\n\t\t\tgoto free_xri_ids;\n\t\t}\n\t\tbase = phba->sli4_hba.max_cfg_param.vfi_base;\n\t\tlongs = (count + BITS_PER_LONG - 1) / BITS_PER_LONG;\n\t\tphba->sli4_hba.vfi_bmask = kcalloc(longs,\n\t\t\t\t\t\t   sizeof(unsigned long),\n\t\t\t\t\t\t   GFP_KERNEL);\n\t\tif (unlikely(!phba->sli4_hba.vfi_bmask)) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto free_xri_ids;\n\t\t}\n\t\tphba->sli4_hba.vfi_ids = kcalloc(count, sizeof(uint16_t),\n\t\t\t\t\t\t GFP_KERNEL);\n\t\tif (unlikely(!phba->sli4_hba.vfi_ids)) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto free_vfi_bmask;\n\t\t}\n\n\t\tfor (i = 0; i < count; i++)\n\t\t\tphba->sli4_hba.vfi_ids[i] = base + i;\n\n\t\t/*\n\t\t * Mark all resources ready.  An HBA reset doesn't need\n\t\t * to reset the initialization.\n\t\t */\n\t\tbf_set(lpfc_idx_rsrc_rdy, &phba->sli4_hba.sli4_flags,\n\t\t       LPFC_IDX_RSRC_RDY);\n\t\treturn 0;\n\t}\n\n free_vfi_bmask:\n\tkfree(phba->sli4_hba.vfi_bmask);\n\tphba->sli4_hba.vfi_bmask = NULL;\n free_xri_ids:\n\tkfree(phba->sli4_hba.xri_ids);\n\tphba->sli4_hba.xri_ids = NULL;\n free_xri_bmask:\n\tkfree(phba->sli4_hba.xri_bmask);\n\tphba->sli4_hba.xri_bmask = NULL;\n free_vpi_ids:\n\tkfree(phba->vpi_ids);\n\tphba->vpi_ids = NULL;\n free_vpi_bmask:\n\tkfree(phba->vpi_bmask);\n\tphba->vpi_bmask = NULL;\n free_rpi_ids:\n\tkfree(phba->sli4_hba.rpi_ids);\n\tphba->sli4_hba.rpi_ids = NULL;\n free_rpi_bmask:\n\tkfree(phba->sli4_hba.rpi_bmask);\n\tphba->sli4_hba.rpi_bmask = NULL;\n err_exit:\n\treturn rc;\n}\n\n/**\n * lpfc_sli4_dealloc_resource_identifiers - Deallocate all SLI4 resource extents.\n * @phba: Pointer to HBA context object.\n *\n * This function allocates the number of elements for the specified\n * resource type.\n **/\nint\nlpfc_sli4_dealloc_resource_identifiers(struct lpfc_hba *phba)\n{\n\tif (phba->sli4_hba.extents_in_use) {\n\t\tlpfc_sli4_dealloc_extent(phba, LPFC_RSC_TYPE_FCOE_VPI);\n\t\tlpfc_sli4_dealloc_extent(phba, LPFC_RSC_TYPE_FCOE_RPI);\n\t\tlpfc_sli4_dealloc_extent(phba, LPFC_RSC_TYPE_FCOE_XRI);\n\t\tlpfc_sli4_dealloc_extent(phba, LPFC_RSC_TYPE_FCOE_VFI);\n\t} else {\n\t\tkfree(phba->vpi_bmask);\n\t\tphba->sli4_hba.max_cfg_param.vpi_used = 0;\n\t\tkfree(phba->vpi_ids);\n\t\tbf_set(lpfc_vpi_rsrc_rdy, &phba->sli4_hba.sli4_flags, 0);\n\t\tkfree(phba->sli4_hba.xri_bmask);\n\t\tkfree(phba->sli4_hba.xri_ids);\n\t\tkfree(phba->sli4_hba.vfi_bmask);\n\t\tkfree(phba->sli4_hba.vfi_ids);\n\t\tbf_set(lpfc_vfi_rsrc_rdy, &phba->sli4_hba.sli4_flags, 0);\n\t\tbf_set(lpfc_idx_rsrc_rdy, &phba->sli4_hba.sli4_flags, 0);\n\t}\n\n\treturn 0;\n}\n\n/**\n * lpfc_sli4_get_allocated_extnts - Get the port's allocated extents.\n * @phba: Pointer to HBA context object.\n * @type: The resource extent type.\n * @extnt_cnt: buffer to hold port extent count response\n * @extnt_size: buffer to hold port extent size response.\n *\n * This function calls the port to read the host allocated extents\n * for a particular type.\n **/\nint\nlpfc_sli4_get_allocated_extnts(struct lpfc_hba *phba, uint16_t type,\n\t\t\t       uint16_t *extnt_cnt, uint16_t *extnt_size)\n{\n\tbool emb;\n\tint rc = 0;\n\tuint16_t curr_blks = 0;\n\tuint32_t req_len, emb_len;\n\tuint32_t alloc_len, mbox_tmo;\n\tstruct list_head *blk_list_head;\n\tstruct lpfc_rsrc_blks *rsrc_blk;\n\tLPFC_MBOXQ_t *mbox;\n\tvoid *virtaddr = NULL;\n\tstruct lpfc_mbx_nembed_rsrc_extent *n_rsrc;\n\tstruct lpfc_mbx_alloc_rsrc_extents *rsrc_ext;\n\tunion  lpfc_sli4_cfg_shdr *shdr;\n\n\tswitch (type) {\n\tcase LPFC_RSC_TYPE_FCOE_VPI:\n\t\tblk_list_head = &phba->lpfc_vpi_blk_list;\n\t\tbreak;\n\tcase LPFC_RSC_TYPE_FCOE_XRI:\n\t\tblk_list_head = &phba->sli4_hba.lpfc_xri_blk_list;\n\t\tbreak;\n\tcase LPFC_RSC_TYPE_FCOE_VFI:\n\t\tblk_list_head = &phba->sli4_hba.lpfc_vfi_blk_list;\n\t\tbreak;\n\tcase LPFC_RSC_TYPE_FCOE_RPI:\n\t\tblk_list_head = &phba->sli4_hba.lpfc_rpi_blk_list;\n\t\tbreak;\n\tdefault:\n\t\treturn -EIO;\n\t}\n\n\t/* Count the number of extents currently allocatd for this type. */\n\tlist_for_each_entry(rsrc_blk, blk_list_head, list) {\n\t\tif (curr_blks == 0) {\n\t\t\t/*\n\t\t\t * The GET_ALLOCATED mailbox does not return the size,\n\t\t\t * just the count.  The size should be just the size\n\t\t\t * stored in the current allocated block and all sizes\n\t\t\t * for an extent type are the same so set the return\n\t\t\t * value now.\n\t\t\t */\n\t\t\t*extnt_size = rsrc_blk->rsrc_size;\n\t\t}\n\t\tcurr_blks++;\n\t}\n\n\t/*\n\t * Calculate the size of an embedded mailbox.  The uint32_t\n\t * accounts for extents-specific word.\n\t */\n\temb_len = sizeof(MAILBOX_t) - sizeof(struct mbox_header) -\n\t\tsizeof(uint32_t);\n\n\t/*\n\t * Presume the allocation and response will fit into an embedded\n\t * mailbox.  If not true, reconfigure to a non-embedded mailbox.\n\t */\n\temb = LPFC_SLI4_MBX_EMBED;\n\treq_len = emb_len;\n\tif (req_len > emb_len) {\n\t\treq_len = curr_blks * sizeof(uint16_t) +\n\t\t\tsizeof(union lpfc_sli4_cfg_shdr) +\n\t\t\tsizeof(uint32_t);\n\t\temb = LPFC_SLI4_MBX_NEMBED;\n\t}\n\n\tmbox = (LPFC_MBOXQ_t *) mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox)\n\t\treturn -ENOMEM;\n\tmemset(mbox, 0, sizeof(LPFC_MBOXQ_t));\n\n\talloc_len = lpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_COMMON,\n\t\t\t\t     LPFC_MBOX_OPCODE_GET_ALLOC_RSRC_EXTENT,\n\t\t\t\t     req_len, emb);\n\tif (alloc_len < req_len) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"2983 Allocated DMA memory size (x%x) is \"\n\t\t\t\"less than the requested DMA memory \"\n\t\t\t\"size (x%x)\\n\", alloc_len, req_len);\n\t\trc = -ENOMEM;\n\t\tgoto err_exit;\n\t}\n\trc = lpfc_sli4_mbox_rsrc_extent(phba, mbox, curr_blks, type, emb);\n\tif (unlikely(rc)) {\n\t\trc = -EIO;\n\t\tgoto err_exit;\n\t}\n\n\tif (!phba->sli4_hba.intr_enable)\n\t\trc = lpfc_sli_issue_mbox(phba, mbox, MBX_POLL);\n\telse {\n\t\tmbox_tmo = lpfc_mbox_tmo_val(phba, mbox);\n\t\trc = lpfc_sli_issue_mbox_wait(phba, mbox, mbox_tmo);\n\t}\n\n\tif (unlikely(rc)) {\n\t\trc = -EIO;\n\t\tgoto err_exit;\n\t}\n\n\t/*\n\t * Figure out where the response is located.  Then get local pointers\n\t * to the response data.  The port does not guarantee to respond to\n\t * all extents counts request so update the local variable with the\n\t * allocated count from the port.\n\t */\n\tif (emb == LPFC_SLI4_MBX_EMBED) {\n\t\trsrc_ext = &mbox->u.mqe.un.alloc_rsrc_extents;\n\t\tshdr = &rsrc_ext->header.cfg_shdr;\n\t\t*extnt_cnt = bf_get(lpfc_mbx_rsrc_cnt, &rsrc_ext->u.rsp);\n\t} else {\n\t\tvirtaddr = mbox->sge_array->addr[0];\n\t\tn_rsrc = (struct lpfc_mbx_nembed_rsrc_extent *) virtaddr;\n\t\tshdr = &n_rsrc->cfg_shdr;\n\t\t*extnt_cnt = bf_get(lpfc_mbx_rsrc_cnt, n_rsrc);\n\t}\n\n\tif (bf_get(lpfc_mbox_hdr_status, &shdr->response)) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"2984 Failed to read allocated resources \"\n\t\t\t\"for type %d - Status 0x%x Add'l Status 0x%x.\\n\",\n\t\t\ttype,\n\t\t\tbf_get(lpfc_mbox_hdr_status, &shdr->response),\n\t\t\tbf_get(lpfc_mbox_hdr_add_status, &shdr->response));\n\t\trc = -EIO;\n\t\tgoto err_exit;\n\t}\n err_exit:\n\tlpfc_sli4_mbox_cmd_free(phba, mbox);\n\treturn rc;\n}\n\n/**\n * lpfc_sli4_repost_sgl_list - Repost the buffers sgl pages as block\n * @phba: pointer to lpfc hba data structure.\n * @sgl_list: linked link of sgl buffers to post\n * @cnt: number of linked list buffers\n *\n * This routine walks the list of buffers that have been allocated and\n * repost them to the port by using SGL block post. This is needed after a\n * pci_function_reset/warm_start or start. It attempts to construct blocks\n * of buffer sgls which contains contiguous xris and uses the non-embedded\n * SGL block post mailbox commands to post them to the port. For single\n * buffer sgl with non-contiguous xri, if any, it shall use embedded SGL post\n * mailbox command for posting.\n *\n * Returns: 0 = success, non-zero failure.\n **/\nstatic int\nlpfc_sli4_repost_sgl_list(struct lpfc_hba *phba,\n\t\t\t  struct list_head *sgl_list, int cnt)\n{\n\tstruct lpfc_sglq *sglq_entry = NULL;\n\tstruct lpfc_sglq *sglq_entry_next = NULL;\n\tstruct lpfc_sglq *sglq_entry_first = NULL;\n\tint status, total_cnt;\n\tint post_cnt = 0, num_posted = 0, block_cnt = 0;\n\tint last_xritag = NO_XRI;\n\tLIST_HEAD(prep_sgl_list);\n\tLIST_HEAD(blck_sgl_list);\n\tLIST_HEAD(allc_sgl_list);\n\tLIST_HEAD(post_sgl_list);\n\tLIST_HEAD(free_sgl_list);\n\n\tspin_lock_irq(&phba->hbalock);\n\tspin_lock(&phba->sli4_hba.sgl_list_lock);\n\tlist_splice_init(sgl_list, &allc_sgl_list);\n\tspin_unlock(&phba->sli4_hba.sgl_list_lock);\n\tspin_unlock_irq(&phba->hbalock);\n\n\ttotal_cnt = cnt;\n\tlist_for_each_entry_safe(sglq_entry, sglq_entry_next,\n\t\t\t\t &allc_sgl_list, list) {\n\t\tlist_del_init(&sglq_entry->list);\n\t\tblock_cnt++;\n\t\tif ((last_xritag != NO_XRI) &&\n\t\t    (sglq_entry->sli4_xritag != last_xritag + 1)) {\n\t\t\t/* a hole in xri block, form a sgl posting block */\n\t\t\tlist_splice_init(&prep_sgl_list, &blck_sgl_list);\n\t\t\tpost_cnt = block_cnt - 1;\n\t\t\t/* prepare list for next posting block */\n\t\t\tlist_add_tail(&sglq_entry->list, &prep_sgl_list);\n\t\t\tblock_cnt = 1;\n\t\t} else {\n\t\t\t/* prepare list for next posting block */\n\t\t\tlist_add_tail(&sglq_entry->list, &prep_sgl_list);\n\t\t\t/* enough sgls for non-embed sgl mbox command */\n\t\t\tif (block_cnt == LPFC_NEMBED_MBOX_SGL_CNT) {\n\t\t\t\tlist_splice_init(&prep_sgl_list,\n\t\t\t\t\t\t &blck_sgl_list);\n\t\t\t\tpost_cnt = block_cnt;\n\t\t\t\tblock_cnt = 0;\n\t\t\t}\n\t\t}\n\t\tnum_posted++;\n\n\t\t/* keep track of last sgl's xritag */\n\t\tlast_xritag = sglq_entry->sli4_xritag;\n\n\t\t/* end of repost sgl list condition for buffers */\n\t\tif (num_posted == total_cnt) {\n\t\t\tif (post_cnt == 0) {\n\t\t\t\tlist_splice_init(&prep_sgl_list,\n\t\t\t\t\t\t &blck_sgl_list);\n\t\t\t\tpost_cnt = block_cnt;\n\t\t\t} else if (block_cnt == 1) {\n\t\t\t\tstatus = lpfc_sli4_post_sgl(phba,\n\t\t\t\t\t\tsglq_entry->phys, 0,\n\t\t\t\t\t\tsglq_entry->sli4_xritag);\n\t\t\t\tif (!status) {\n\t\t\t\t\t/* successful, put sgl to posted list */\n\t\t\t\t\tlist_add_tail(&sglq_entry->list,\n\t\t\t\t\t\t      &post_sgl_list);\n\t\t\t\t} else {\n\t\t\t\t\t/* Failure, put sgl to free list */\n\t\t\t\t\tlpfc_printf_log(phba, KERN_WARNING,\n\t\t\t\t\t\tLOG_SLI,\n\t\t\t\t\t\t\"3159 Failed to post \"\n\t\t\t\t\t\t\"sgl, xritag:x%x\\n\",\n\t\t\t\t\t\tsglq_entry->sli4_xritag);\n\t\t\t\t\tlist_add_tail(&sglq_entry->list,\n\t\t\t\t\t\t      &free_sgl_list);\n\t\t\t\t\ttotal_cnt--;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t/* continue until a nembed page worth of sgls */\n\t\tif (post_cnt == 0)\n\t\t\tcontinue;\n\n\t\t/* post the buffer list sgls as a block */\n\t\tstatus = lpfc_sli4_post_sgl_list(phba, &blck_sgl_list,\n\t\t\t\t\t\t post_cnt);\n\n\t\tif (!status) {\n\t\t\t/* success, put sgl list to posted sgl list */\n\t\t\tlist_splice_init(&blck_sgl_list, &post_sgl_list);\n\t\t} else {\n\t\t\t/* Failure, put sgl list to free sgl list */\n\t\t\tsglq_entry_first = list_first_entry(&blck_sgl_list,\n\t\t\t\t\t\t\t    struct lpfc_sglq,\n\t\t\t\t\t\t\t    list);\n\t\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_SLI,\n\t\t\t\t\t\"3160 Failed to post sgl-list, \"\n\t\t\t\t\t\"xritag:x%x-x%x\\n\",\n\t\t\t\t\tsglq_entry_first->sli4_xritag,\n\t\t\t\t\t(sglq_entry_first->sli4_xritag +\n\t\t\t\t\t post_cnt - 1));\n\t\t\tlist_splice_init(&blck_sgl_list, &free_sgl_list);\n\t\t\ttotal_cnt -= post_cnt;\n\t\t}\n\n\t\t/* don't reset xirtag due to hole in xri block */\n\t\tif (block_cnt == 0)\n\t\t\tlast_xritag = NO_XRI;\n\n\t\t/* reset sgl post count for next round of posting */\n\t\tpost_cnt = 0;\n\t}\n\n\t/* free the sgls failed to post */\n\tlpfc_free_sgl_list(phba, &free_sgl_list);\n\n\t/* push sgls posted to the available list */\n\tif (!list_empty(&post_sgl_list)) {\n\t\tspin_lock_irq(&phba->hbalock);\n\t\tspin_lock(&phba->sli4_hba.sgl_list_lock);\n\t\tlist_splice_init(&post_sgl_list, sgl_list);\n\t\tspin_unlock(&phba->sli4_hba.sgl_list_lock);\n\t\tspin_unlock_irq(&phba->hbalock);\n\t} else {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"3161 Failure to post sgl to port.\\n\");\n\t\treturn -EIO;\n\t}\n\n\t/* return the number of XRIs actually posted */\n\treturn total_cnt;\n}\n\n/**\n * lpfc_sli4_repost_io_sgl_list - Repost all the allocated nvme buffer sgls\n * @phba: pointer to lpfc hba data structure.\n *\n * This routine walks the list of nvme buffers that have been allocated and\n * repost them to the port by using SGL block post. This is needed after a\n * pci_function_reset/warm_start or start. The lpfc_hba_down_post_s4 routine\n * is responsible for moving all nvme buffers on the lpfc_abts_nvme_sgl_list\n * to the lpfc_io_buf_list. If the repost fails, reject all nvme buffers.\n *\n * Returns: 0 = success, non-zero failure.\n **/\nstatic int\nlpfc_sli4_repost_io_sgl_list(struct lpfc_hba *phba)\n{\n\tLIST_HEAD(post_nblist);\n\tint num_posted, rc = 0;\n\n\t/* get all NVME buffers need to repost to a local list */\n\tlpfc_io_buf_flush(phba, &post_nblist);\n\n\t/* post the list of nvme buffer sgls to port if available */\n\tif (!list_empty(&post_nblist)) {\n\t\tnum_posted = lpfc_sli4_post_io_sgl_list(\n\t\t\tphba, &post_nblist, phba->sli4_hba.io_xri_cnt);\n\t\t/* failed to post any nvme buffer, return error */\n\t\tif (num_posted == 0)\n\t\t\trc = -EIO;\n\t}\n\treturn rc;\n}\n\nstatic void\nlpfc_set_host_data(struct lpfc_hba *phba, LPFC_MBOXQ_t *mbox)\n{\n\tuint32_t len;\n\n\tlen = sizeof(struct lpfc_mbx_set_host_data) -\n\t\tsizeof(struct lpfc_sli4_cfg_mhdr);\n\tlpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_COMMON,\n\t\t\t LPFC_MBOX_OPCODE_SET_HOST_DATA, len,\n\t\t\t LPFC_SLI4_MBX_EMBED);\n\n\tmbox->u.mqe.un.set_host_data.param_id = LPFC_SET_HOST_OS_DRIVER_VERSION;\n\tmbox->u.mqe.un.set_host_data.param_len =\n\t\t\t\t\tLPFC_HOST_OS_DRIVER_VERSION_SIZE;\n\tsnprintf(mbox->u.mqe.un.set_host_data.data,\n\t\t LPFC_HOST_OS_DRIVER_VERSION_SIZE,\n\t\t \"Linux %s v\"LPFC_DRIVER_VERSION,\n\t\t (phba->hba_flag & HBA_FCOE_MODE) ? \"FCoE\" : \"FC\");\n}\n\nint\nlpfc_post_rq_buffer(struct lpfc_hba *phba, struct lpfc_queue *hrq,\n\t\t    struct lpfc_queue *drq, int count, int idx)\n{\n\tint rc, i;\n\tstruct lpfc_rqe hrqe;\n\tstruct lpfc_rqe drqe;\n\tstruct lpfc_rqb *rqbp;\n\tunsigned long flags;\n\tstruct rqb_dmabuf *rqb_buffer;\n\tLIST_HEAD(rqb_buf_list);\n\n\tspin_lock_irqsave(&phba->hbalock, flags);\n\trqbp = hrq->rqbp;\n\tfor (i = 0; i < count; i++) {\n\t\t/* IF RQ is already full, don't bother */\n\t\tif (rqbp->buffer_count + i >= rqbp->entry_count - 1)\n\t\t\tbreak;\n\t\trqb_buffer = rqbp->rqb_alloc_buffer(phba);\n\t\tif (!rqb_buffer)\n\t\t\tbreak;\n\t\trqb_buffer->hrq = hrq;\n\t\trqb_buffer->drq = drq;\n\t\trqb_buffer->idx = idx;\n\t\tlist_add_tail(&rqb_buffer->hbuf.list, &rqb_buf_list);\n\t}\n\twhile (!list_empty(&rqb_buf_list)) {\n\t\tlist_remove_head(&rqb_buf_list, rqb_buffer, struct rqb_dmabuf,\n\t\t\t\t hbuf.list);\n\n\t\thrqe.address_lo = putPaddrLow(rqb_buffer->hbuf.phys);\n\t\thrqe.address_hi = putPaddrHigh(rqb_buffer->hbuf.phys);\n\t\tdrqe.address_lo = putPaddrLow(rqb_buffer->dbuf.phys);\n\t\tdrqe.address_hi = putPaddrHigh(rqb_buffer->dbuf.phys);\n\t\trc = lpfc_sli4_rq_put(hrq, drq, &hrqe, &drqe);\n\t\tif (rc < 0) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"6421 Cannot post to HRQ %d: %x %x %x \"\n\t\t\t\t\t\"DRQ %x %x\\n\",\n\t\t\t\t\thrq->queue_id,\n\t\t\t\t\thrq->host_index,\n\t\t\t\t\thrq->hba_index,\n\t\t\t\t\thrq->entry_count,\n\t\t\t\t\tdrq->host_index,\n\t\t\t\t\tdrq->hba_index);\n\t\t\trqbp->rqb_free_buffer(phba, rqb_buffer);\n\t\t} else {\n\t\t\tlist_add_tail(&rqb_buffer->hbuf.list,\n\t\t\t\t      &rqbp->rqb_buffer_list);\n\t\t\trqbp->buffer_count++;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&phba->hbalock, flags);\n\treturn 1;\n}\n\n/**\n * lpfc_init_idle_stat_hb - Initialize idle_stat tracking\n * @phba: pointer to lpfc hba data structure.\n *\n * This routine initializes the per-cq idle_stat to dynamically dictate\n * polling decisions.\n *\n * Return codes:\n *   None\n **/\nstatic void lpfc_init_idle_stat_hb(struct lpfc_hba *phba)\n{\n\tint i;\n\tstruct lpfc_sli4_hdw_queue *hdwq;\n\tstruct lpfc_queue *cq;\n\tstruct lpfc_idle_stat *idle_stat;\n\tu64 wall;\n\n\tfor_each_present_cpu(i) {\n\t\thdwq = &phba->sli4_hba.hdwq[phba->sli4_hba.cpu_map[i].hdwq];\n\t\tcq = hdwq->io_cq;\n\n\t\t/* Skip if we've already handled this cq's primary CPU */\n\t\tif (cq->chann != i)\n\t\t\tcontinue;\n\n\t\tidle_stat = &phba->sli4_hba.idle_stat[i];\n\n\t\tidle_stat->prev_idle = get_cpu_idle_time(i, &wall, 1);\n\t\tidle_stat->prev_wall = wall;\n\n\t\tif (phba->nvmet_support)\n\t\t\tcq->poll_mode = LPFC_QUEUE_WORK;\n\t\telse\n\t\t\tcq->poll_mode = LPFC_IRQ_POLL;\n\t}\n\n\tif (!phba->nvmet_support)\n\t\tschedule_delayed_work(&phba->idle_stat_delay_work,\n\t\t\t\t      msecs_to_jiffies(LPFC_IDLE_STAT_DELAY));\n}\n\nstatic void lpfc_sli4_dip(struct lpfc_hba *phba)\n{\n\tuint32_t if_type;\n\n\tif_type = bf_get(lpfc_sli_intf_if_type, &phba->sli4_hba.sli_intf);\n\tif (if_type == LPFC_SLI_INTF_IF_TYPE_2 ||\n\t    if_type == LPFC_SLI_INTF_IF_TYPE_6) {\n\t\tstruct lpfc_register reg_data;\n\n\t\tif (lpfc_readl(phba->sli4_hba.u.if_type2.STATUSregaddr,\n\t\t\t       &reg_data.word0))\n\t\t\treturn;\n\n\t\tif (bf_get(lpfc_sliport_status_dip, &reg_data))\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"2904 Firmware Dump Image Present\"\n\t\t\t\t\t\" on Adapter\");\n\t}\n}\n\n/**\n * lpfc_sli4_hba_setup - SLI4 device initialization PCI function\n * @phba: Pointer to HBA context object.\n *\n * This function is the main SLI4 device initialization PCI function. This\n * function is called by the HBA initialization code, HBA reset code and\n * HBA error attention handler code. Caller is not required to hold any\n * locks.\n **/\nint\nlpfc_sli4_hba_setup(struct lpfc_hba *phba)\n{\n\tint rc, i, cnt, len, dd;\n\tLPFC_MBOXQ_t *mboxq;\n\tstruct lpfc_mqe *mqe;\n\tuint8_t *vpd;\n\tuint32_t vpd_size;\n\tuint32_t ftr_rsp = 0;\n\tstruct Scsi_Host *shost = lpfc_shost_from_vport(phba->pport);\n\tstruct lpfc_vport *vport = phba->pport;\n\tstruct lpfc_dmabuf *mp;\n\tstruct lpfc_rqb *rqbp;\n\n\t/* Perform a PCI function reset to start from clean */\n\trc = lpfc_pci_function_reset(phba);\n\tif (unlikely(rc))\n\t\treturn -ENODEV;\n\n\t/* Check the HBA Host Status Register for readyness */\n\trc = lpfc_sli4_post_status_check(phba);\n\tif (unlikely(rc))\n\t\treturn -ENODEV;\n\telse {\n\t\tspin_lock_irq(&phba->hbalock);\n\t\tphba->sli.sli_flag |= LPFC_SLI_ACTIVE;\n\t\tspin_unlock_irq(&phba->hbalock);\n\t}\n\n\tlpfc_sli4_dip(phba);\n\n\t/*\n\t * Allocate a single mailbox container for initializing the\n\t * port.\n\t */\n\tmboxq = (LPFC_MBOXQ_t *) mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mboxq)\n\t\treturn -ENOMEM;\n\n\t/* Issue READ_REV to collect vpd and FW information. */\n\tvpd_size = SLI4_PAGE_SIZE;\n\tvpd = kzalloc(vpd_size, GFP_KERNEL);\n\tif (!vpd) {\n\t\trc = -ENOMEM;\n\t\tgoto out_free_mbox;\n\t}\n\n\trc = lpfc_sli4_read_rev(phba, mboxq, vpd, &vpd_size);\n\tif (unlikely(rc)) {\n\t\tkfree(vpd);\n\t\tgoto out_free_mbox;\n\t}\n\n\tmqe = &mboxq->u.mqe;\n\tphba->sli_rev = bf_get(lpfc_mbx_rd_rev_sli_lvl, &mqe->un.read_rev);\n\tif (bf_get(lpfc_mbx_rd_rev_fcoe, &mqe->un.read_rev)) {\n\t\tphba->hba_flag |= HBA_FCOE_MODE;\n\t\tphba->fcp_embed_io = 0;\t/* SLI4 FC support only */\n\t} else {\n\t\tphba->hba_flag &= ~HBA_FCOE_MODE;\n\t}\n\n\tif (bf_get(lpfc_mbx_rd_rev_cee_ver, &mqe->un.read_rev) ==\n\t\tLPFC_DCBX_CEE_MODE)\n\t\tphba->hba_flag |= HBA_FIP_SUPPORT;\n\telse\n\t\tphba->hba_flag &= ~HBA_FIP_SUPPORT;\n\n\tphba->hba_flag &= ~HBA_IOQ_FLUSH;\n\n\tif (phba->sli_rev != LPFC_SLI_REV4) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"0376 READ_REV Error. SLI Level %d \"\n\t\t\t\"FCoE enabled %d\\n\",\n\t\t\tphba->sli_rev, phba->hba_flag & HBA_FCOE_MODE);\n\t\trc = -EIO;\n\t\tkfree(vpd);\n\t\tgoto out_free_mbox;\n\t}\n\n\t/*\n\t * Continue initialization with default values even if driver failed\n\t * to read FCoE param config regions, only read parameters if the\n\t * board is FCoE\n\t */\n\tif (phba->hba_flag & HBA_FCOE_MODE &&\n\t    lpfc_sli4_read_fcoe_params(phba))\n\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_MBOX | LOG_INIT,\n\t\t\t\"2570 Failed to read FCoE parameters\\n\");\n\n\t/*\n\t * Retrieve sli4 device physical port name, failure of doing it\n\t * is considered as non-fatal.\n\t */\n\trc = lpfc_sli4_retrieve_pport_name(phba);\n\tif (!rc)\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_MBOX | LOG_SLI,\n\t\t\t\t\"3080 Successful retrieving SLI4 device \"\n\t\t\t\t\"physical port name: %s.\\n\", phba->Port);\n\n\trc = lpfc_sli4_get_ctl_attr(phba);\n\tif (!rc)\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_MBOX | LOG_SLI,\n\t\t\t\t\"8351 Successful retrieving SLI4 device \"\n\t\t\t\t\"CTL ATTR\\n\");\n\n\t/*\n\t * Evaluate the read rev and vpd data. Populate the driver\n\t * state with the results. If this routine fails, the failure\n\t * is not fatal as the driver will use generic values.\n\t */\n\trc = lpfc_parse_vpd(phba, vpd, vpd_size);\n\tif (unlikely(!rc)) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0377 Error %d parsing vpd. \"\n\t\t\t\t\"Using defaults.\\n\", rc);\n\t\trc = 0;\n\t}\n\tkfree(vpd);\n\n\t/* Save information as VPD data */\n\tphba->vpd.rev.biuRev = mqe->un.read_rev.first_hw_rev;\n\tphba->vpd.rev.smRev = mqe->un.read_rev.second_hw_rev;\n\n\t/*\n\t * This is because first G7 ASIC doesn't support the standard\n\t * 0x5a NVME cmd descriptor type/subtype\n\t */\n\tif ((bf_get(lpfc_sli_intf_if_type, &phba->sli4_hba.sli_intf) ==\n\t\t\tLPFC_SLI_INTF_IF_TYPE_6) &&\n\t    (phba->vpd.rev.biuRev == LPFC_G7_ASIC_1) &&\n\t    (phba->vpd.rev.smRev == 0) &&\n\t    (phba->cfg_nvme_embed_cmd == 1))\n\t\tphba->cfg_nvme_embed_cmd = 0;\n\n\tphba->vpd.rev.endecRev = mqe->un.read_rev.third_hw_rev;\n\tphba->vpd.rev.fcphHigh = bf_get(lpfc_mbx_rd_rev_fcph_high,\n\t\t\t\t\t &mqe->un.read_rev);\n\tphba->vpd.rev.fcphLow = bf_get(lpfc_mbx_rd_rev_fcph_low,\n\t\t\t\t       &mqe->un.read_rev);\n\tphba->vpd.rev.feaLevelHigh = bf_get(lpfc_mbx_rd_rev_ftr_lvl_high,\n\t\t\t\t\t    &mqe->un.read_rev);\n\tphba->vpd.rev.feaLevelLow = bf_get(lpfc_mbx_rd_rev_ftr_lvl_low,\n\t\t\t\t\t   &mqe->un.read_rev);\n\tphba->vpd.rev.sli1FwRev = mqe->un.read_rev.fw_id_rev;\n\tmemcpy(phba->vpd.rev.sli1FwName, mqe->un.read_rev.fw_name, 16);\n\tphba->vpd.rev.sli2FwRev = mqe->un.read_rev.ulp_fw_id_rev;\n\tmemcpy(phba->vpd.rev.sli2FwName, mqe->un.read_rev.ulp_fw_name, 16);\n\tphba->vpd.rev.opFwRev = mqe->un.read_rev.fw_id_rev;\n\tmemcpy(phba->vpd.rev.opFwName, mqe->un.read_rev.fw_name, 16);\n\tlpfc_printf_log(phba, KERN_INFO, LOG_MBOX | LOG_SLI,\n\t\t\t\"(%d):0380 READ_REV Status x%x \"\n\t\t\t\"fw_rev:%s fcphHi:%x fcphLo:%x flHi:%x flLo:%x\\n\",\n\t\t\tmboxq->vport ? mboxq->vport->vpi : 0,\n\t\t\tbf_get(lpfc_mqe_status, mqe),\n\t\t\tphba->vpd.rev.opFwName,\n\t\t\tphba->vpd.rev.fcphHigh, phba->vpd.rev.fcphLow,\n\t\t\tphba->vpd.rev.feaLevelHigh, phba->vpd.rev.feaLevelLow);\n\n\tif (bf_get(lpfc_sli_intf_if_type, &phba->sli4_hba.sli_intf) ==\n\t    LPFC_SLI_INTF_IF_TYPE_0) {\n\t\tlpfc_set_features(phba, mboxq, LPFC_SET_UE_RECOVERY);\n\t\trc = lpfc_sli_issue_mbox(phba, mboxq, MBX_POLL);\n\t\tif (rc == MBX_SUCCESS) {\n\t\t\tphba->hba_flag |= HBA_RECOVERABLE_UE;\n\t\t\t/* Set 1Sec interval to detect UE */\n\t\t\tphba->eratt_poll_interval = 1;\n\t\t\tphba->sli4_hba.ue_to_sr = bf_get(\n\t\t\t\t\tlpfc_mbx_set_feature_UESR,\n\t\t\t\t\t&mboxq->u.mqe.un.set_feature);\n\t\t\tphba->sli4_hba.ue_to_rp = bf_get(\n\t\t\t\t\tlpfc_mbx_set_feature_UERP,\n\t\t\t\t\t&mboxq->u.mqe.un.set_feature);\n\t\t}\n\t}\n\n\tif (phba->cfg_enable_mds_diags && phba->mds_diags_support) {\n\t\t/* Enable MDS Diagnostics only if the SLI Port supports it */\n\t\tlpfc_set_features(phba, mboxq, LPFC_SET_MDS_DIAGS);\n\t\trc = lpfc_sli_issue_mbox(phba, mboxq, MBX_POLL);\n\t\tif (rc != MBX_SUCCESS)\n\t\t\tphba->mds_diags_support = 0;\n\t}\n\n\t/*\n\t * Discover the port's supported feature set and match it against the\n\t * hosts requests.\n\t */\n\tlpfc_request_features(phba, mboxq);\n\trc = lpfc_sli_issue_mbox(phba, mboxq, MBX_POLL);\n\tif (unlikely(rc)) {\n\t\trc = -EIO;\n\t\tgoto out_free_mbox;\n\t}\n\n\t/*\n\t * The port must support FCP initiator mode as this is the\n\t * only mode running in the host.\n\t */\n\tif (!(bf_get(lpfc_mbx_rq_ftr_rsp_fcpi, &mqe->un.req_ftrs))) {\n\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_MBOX | LOG_SLI,\n\t\t\t\t\"0378 No support for fcpi mode.\\n\");\n\t\tftr_rsp++;\n\t}\n\n\t/* Performance Hints are ONLY for FCoE */\n\tif (phba->hba_flag & HBA_FCOE_MODE) {\n\t\tif (bf_get(lpfc_mbx_rq_ftr_rsp_perfh, &mqe->un.req_ftrs))\n\t\t\tphba->sli3_options |= LPFC_SLI4_PERFH_ENABLED;\n\t\telse\n\t\t\tphba->sli3_options &= ~LPFC_SLI4_PERFH_ENABLED;\n\t}\n\n\t/*\n\t * If the port cannot support the host's requested features\n\t * then turn off the global config parameters to disable the\n\t * feature in the driver.  This is not a fatal error.\n\t */\n\tif (phba->sli3_options & LPFC_SLI3_BG_ENABLED) {\n\t\tif (!(bf_get(lpfc_mbx_rq_ftr_rsp_dif, &mqe->un.req_ftrs))) {\n\t\t\tphba->cfg_enable_bg = 0;\n\t\t\tphba->sli3_options &= ~LPFC_SLI3_BG_ENABLED;\n\t\t\tftr_rsp++;\n\t\t}\n\t}\n\n\tif (phba->max_vpi && phba->cfg_enable_npiv &&\n\t    !(bf_get(lpfc_mbx_rq_ftr_rsp_npiv, &mqe->un.req_ftrs)))\n\t\tftr_rsp++;\n\n\tif (ftr_rsp) {\n\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_MBOX | LOG_SLI,\n\t\t\t\t\"0379 Feature Mismatch Data: x%08x %08x \"\n\t\t\t\t\"x%x x%x x%x\\n\", mqe->un.req_ftrs.word2,\n\t\t\t\tmqe->un.req_ftrs.word3, phba->cfg_enable_bg,\n\t\t\t\tphba->cfg_enable_npiv, phba->max_vpi);\n\t\tif (!(bf_get(lpfc_mbx_rq_ftr_rsp_dif, &mqe->un.req_ftrs)))\n\t\t\tphba->cfg_enable_bg = 0;\n\t\tif (!(bf_get(lpfc_mbx_rq_ftr_rsp_npiv, &mqe->un.req_ftrs)))\n\t\t\tphba->cfg_enable_npiv = 0;\n\t}\n\n\t/* These SLI3 features are assumed in SLI4 */\n\tspin_lock_irq(&phba->hbalock);\n\tphba->sli3_options |= (LPFC_SLI3_NPIV_ENABLED | LPFC_SLI3_HBQ_ENABLED);\n\tspin_unlock_irq(&phba->hbalock);\n\n\t/* Always try to enable dual dump feature if we can */\n\tlpfc_set_features(phba, mboxq, LPFC_SET_DUAL_DUMP);\n\trc = lpfc_sli_issue_mbox(phba, mboxq, MBX_POLL);\n\tdd = bf_get(lpfc_mbx_set_feature_dd, &mboxq->u.mqe.un.set_feature);\n\tif ((rc == MBX_SUCCESS) && (dd == LPFC_ENABLE_DUAL_DUMP))\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_SLI,\n\t\t\t\t\"6448 Dual Dump is enabled\\n\");\n\telse\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI | LOG_INIT,\n\t\t\t\t\"6447 Dual Dump Mailbox x%x (x%x/x%x) failed, \"\n\t\t\t\t\"rc:x%x dd:x%x\\n\",\n\t\t\t\tbf_get(lpfc_mqe_command, &mboxq->u.mqe),\n\t\t\t\tlpfc_sli_config_mbox_subsys_get(\n\t\t\t\t\tphba, mboxq),\n\t\t\t\tlpfc_sli_config_mbox_opcode_get(\n\t\t\t\t\tphba, mboxq),\n\t\t\t\trc, dd);\n\t/*\n\t * Allocate all resources (xri,rpi,vpi,vfi) now.  Subsequent\n\t * calls depends on these resources to complete port setup.\n\t */\n\trc = lpfc_sli4_alloc_resource_identifiers(phba);\n\tif (rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2920 Failed to alloc Resource IDs \"\n\t\t\t\t\"rc = x%x\\n\", rc);\n\t\tgoto out_free_mbox;\n\t}\n\n\tlpfc_set_host_data(phba, mboxq);\n\n\trc = lpfc_sli_issue_mbox(phba, mboxq, MBX_POLL);\n\tif (rc) {\n\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_MBOX | LOG_SLI,\n\t\t\t\t\"2134 Failed to set host os driver version %x\",\n\t\t\t\trc);\n\t}\n\n\t/* Read the port's service parameters. */\n\trc = lpfc_read_sparam(phba, mboxq, vport->vpi);\n\tif (rc) {\n\t\tphba->link_state = LPFC_HBA_ERROR;\n\t\trc = -ENOMEM;\n\t\tgoto out_free_mbox;\n\t}\n\n\tmboxq->vport = vport;\n\trc = lpfc_sli_issue_mbox(phba, mboxq, MBX_POLL);\n\tmp = (struct lpfc_dmabuf *)mboxq->ctx_buf;\n\tif (rc == MBX_SUCCESS) {\n\t\tmemcpy(&vport->fc_sparam, mp->virt, sizeof(struct serv_parm));\n\t\trc = 0;\n\t}\n\n\t/*\n\t * This memory was allocated by the lpfc_read_sparam routine. Release\n\t * it to the mbuf pool.\n\t */\n\tlpfc_mbuf_free(phba, mp->virt, mp->phys);\n\tkfree(mp);\n\tmboxq->ctx_buf = NULL;\n\tif (unlikely(rc)) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0382 READ_SPARAM command failed \"\n\t\t\t\t\"status %d, mbxStatus x%x\\n\",\n\t\t\t\trc, bf_get(lpfc_mqe_status, mqe));\n\t\tphba->link_state = LPFC_HBA_ERROR;\n\t\trc = -EIO;\n\t\tgoto out_free_mbox;\n\t}\n\n\tlpfc_update_vport_wwn(vport);\n\n\t/* Update the fc_host data structures with new wwn. */\n\tfc_host_node_name(shost) = wwn_to_u64(vport->fc_nodename.u.wwn);\n\tfc_host_port_name(shost) = wwn_to_u64(vport->fc_portname.u.wwn);\n\n\t/* Create all the SLI4 queues */\n\trc = lpfc_sli4_queue_create(phba);\n\tif (rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"3089 Failed to allocate queues\\n\");\n\t\trc = -ENODEV;\n\t\tgoto out_free_mbox;\n\t}\n\t/* Set up all the queues to the device */\n\trc = lpfc_sli4_queue_setup(phba);\n\tif (unlikely(rc)) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0381 Error %d during queue setup.\\n \", rc);\n\t\tgoto out_stop_timers;\n\t}\n\t/* Initialize the driver internal SLI layer lists. */\n\tlpfc_sli4_setup(phba);\n\tlpfc_sli4_queue_init(phba);\n\n\t/* update host els xri-sgl sizes and mappings */\n\trc = lpfc_sli4_els_sgl_update(phba);\n\tif (unlikely(rc)) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"1400 Failed to update xri-sgl size and \"\n\t\t\t\t\"mapping: %d\\n\", rc);\n\t\tgoto out_destroy_queue;\n\t}\n\n\t/* register the els sgl pool to the port */\n\trc = lpfc_sli4_repost_sgl_list(phba, &phba->sli4_hba.lpfc_els_sgl_list,\n\t\t\t\t       phba->sli4_hba.els_xri_cnt);\n\tif (unlikely(rc < 0)) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0582 Error %d during els sgl post \"\n\t\t\t\t\"operation\\n\", rc);\n\t\trc = -ENODEV;\n\t\tgoto out_destroy_queue;\n\t}\n\tphba->sli4_hba.els_xri_cnt = rc;\n\n\tif (phba->nvmet_support) {\n\t\t/* update host nvmet xri-sgl sizes and mappings */\n\t\trc = lpfc_sli4_nvmet_sgl_update(phba);\n\t\tif (unlikely(rc)) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"6308 Failed to update nvmet-sgl size \"\n\t\t\t\t\t\"and mapping: %d\\n\", rc);\n\t\t\tgoto out_destroy_queue;\n\t\t}\n\n\t\t/* register the nvmet sgl pool to the port */\n\t\trc = lpfc_sli4_repost_sgl_list(\n\t\t\tphba,\n\t\t\t&phba->sli4_hba.lpfc_nvmet_sgl_list,\n\t\t\tphba->sli4_hba.nvmet_xri_cnt);\n\t\tif (unlikely(rc < 0)) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"3117 Error %d during nvmet \"\n\t\t\t\t\t\"sgl post\\n\", rc);\n\t\t\trc = -ENODEV;\n\t\t\tgoto out_destroy_queue;\n\t\t}\n\t\tphba->sli4_hba.nvmet_xri_cnt = rc;\n\n\t\t/* We allocate an iocbq for every receive context SGL.\n\t\t * The additional allocation is for abort and ls handling.\n\t\t */\n\t\tcnt = phba->sli4_hba.nvmet_xri_cnt +\n\t\t\tphba->sli4_hba.max_cfg_param.max_xri;\n\t} else {\n\t\t/* update host common xri-sgl sizes and mappings */\n\t\trc = lpfc_sli4_io_sgl_update(phba);\n\t\tif (unlikely(rc)) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"6082 Failed to update nvme-sgl size \"\n\t\t\t\t\t\"and mapping: %d\\n\", rc);\n\t\t\tgoto out_destroy_queue;\n\t\t}\n\n\t\t/* register the allocated common sgl pool to the port */\n\t\trc = lpfc_sli4_repost_io_sgl_list(phba);\n\t\tif (unlikely(rc)) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"6116 Error %d during nvme sgl post \"\n\t\t\t\t\t\"operation\\n\", rc);\n\t\t\t/* Some NVME buffers were moved to abort nvme list */\n\t\t\t/* A pci function reset will repost them */\n\t\t\trc = -ENODEV;\n\t\t\tgoto out_destroy_queue;\n\t\t}\n\t\t/* Each lpfc_io_buf job structure has an iocbq element.\n\t\t * This cnt provides for abort, els, ct and ls requests.\n\t\t */\n\t\tcnt = phba->sli4_hba.max_cfg_param.max_xri;\n\t}\n\n\tif (!phba->sli.iocbq_lookup) {\n\t\t/* Initialize and populate the iocb list per host */\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_INIT,\n\t\t\t\t\"2821 initialize iocb list with %d entries\\n\",\n\t\t\t\tcnt);\n\t\trc = lpfc_init_iocb_list(phba, cnt);\n\t\tif (rc) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"1413 Failed to init iocb list.\\n\");\n\t\t\tgoto out_destroy_queue;\n\t\t}\n\t}\n\n\tif (phba->nvmet_support)\n\t\tlpfc_nvmet_create_targetport(phba);\n\n\tif (phba->nvmet_support && phba->cfg_nvmet_mrq) {\n\t\t/* Post initial buffers to all RQs created */\n\t\tfor (i = 0; i < phba->cfg_nvmet_mrq; i++) {\n\t\t\trqbp = phba->sli4_hba.nvmet_mrq_hdr[i]->rqbp;\n\t\t\tINIT_LIST_HEAD(&rqbp->rqb_buffer_list);\n\t\t\trqbp->rqb_alloc_buffer = lpfc_sli4_nvmet_alloc;\n\t\t\trqbp->rqb_free_buffer = lpfc_sli4_nvmet_free;\n\t\t\trqbp->entry_count = LPFC_NVMET_RQE_DEF_COUNT;\n\t\t\trqbp->buffer_count = 0;\n\n\t\t\tlpfc_post_rq_buffer(\n\t\t\t\tphba, phba->sli4_hba.nvmet_mrq_hdr[i],\n\t\t\t\tphba->sli4_hba.nvmet_mrq_data[i],\n\t\t\t\tphba->cfg_nvmet_mrq_post, i);\n\t\t}\n\t}\n\n\t/* Post the rpi header region to the device. */\n\trc = lpfc_sli4_post_all_rpi_hdrs(phba);\n\tif (unlikely(rc)) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0393 Error %d during rpi post operation\\n\",\n\t\t\t\trc);\n\t\trc = -ENODEV;\n\t\tgoto out_destroy_queue;\n\t}\n\tlpfc_sli4_node_prep(phba);\n\n\tif (!(phba->hba_flag & HBA_FCOE_MODE)) {\n\t\tif ((phba->nvmet_support == 0) || (phba->cfg_nvmet_mrq == 1)) {\n\t\t\t/*\n\t\t\t * The FC Port needs to register FCFI (index 0)\n\t\t\t */\n\t\t\tlpfc_reg_fcfi(phba, mboxq);\n\t\t\tmboxq->vport = phba->pport;\n\t\t\trc = lpfc_sli_issue_mbox(phba, mboxq, MBX_POLL);\n\t\t\tif (rc != MBX_SUCCESS)\n\t\t\t\tgoto out_unset_queue;\n\t\t\trc = 0;\n\t\t\tphba->fcf.fcfi = bf_get(lpfc_reg_fcfi_fcfi,\n\t\t\t\t\t\t&mboxq->u.mqe.un.reg_fcfi);\n\t\t} else {\n\t\t\t/* We are a NVME Target mode with MRQ > 1 */\n\n\t\t\t/* First register the FCFI */\n\t\t\tlpfc_reg_fcfi_mrq(phba, mboxq, 0);\n\t\t\tmboxq->vport = phba->pport;\n\t\t\trc = lpfc_sli_issue_mbox(phba, mboxq, MBX_POLL);\n\t\t\tif (rc != MBX_SUCCESS)\n\t\t\t\tgoto out_unset_queue;\n\t\t\trc = 0;\n\t\t\tphba->fcf.fcfi = bf_get(lpfc_reg_fcfi_mrq_fcfi,\n\t\t\t\t\t\t&mboxq->u.mqe.un.reg_fcfi_mrq);\n\n\t\t\t/* Next register the MRQs */\n\t\t\tlpfc_reg_fcfi_mrq(phba, mboxq, 1);\n\t\t\tmboxq->vport = phba->pport;\n\t\t\trc = lpfc_sli_issue_mbox(phba, mboxq, MBX_POLL);\n\t\t\tif (rc != MBX_SUCCESS)\n\t\t\t\tgoto out_unset_queue;\n\t\t\trc = 0;\n\t\t}\n\t\t/* Check if the port is configured to be disabled */\n\t\tlpfc_sli_read_link_ste(phba);\n\t}\n\n\t/* Don't post more new bufs if repost already recovered\n\t * the nvme sgls.\n\t */\n\tif (phba->nvmet_support == 0) {\n\t\tif (phba->sli4_hba.io_xri_cnt == 0) {\n\t\t\tlen = lpfc_new_io_buf(\n\t\t\t\t\t      phba, phba->sli4_hba.io_xri_max);\n\t\t\tif (len == 0) {\n\t\t\t\trc = -ENOMEM;\n\t\t\t\tgoto out_unset_queue;\n\t\t\t}\n\n\t\t\tif (phba->cfg_xri_rebalancing)\n\t\t\t\tlpfc_create_multixri_pools(phba);\n\t\t}\n\t} else {\n\t\tphba->cfg_xri_rebalancing = 0;\n\t}\n\n\t/* Allow asynchronous mailbox command to go through */\n\tspin_lock_irq(&phba->hbalock);\n\tphba->sli.sli_flag &= ~LPFC_SLI_ASYNC_MBX_BLK;\n\tspin_unlock_irq(&phba->hbalock);\n\n\t/* Post receive buffers to the device */\n\tlpfc_sli4_rb_setup(phba);\n\n\t/* Reset HBA FCF states after HBA reset */\n\tphba->fcf.fcf_flag = 0;\n\tphba->fcf.current_rec.flag = 0;\n\n\t/* Start the ELS watchdog timer */\n\tmod_timer(&vport->els_tmofunc,\n\t\t  jiffies + msecs_to_jiffies(1000 * (phba->fc_ratov * 2)));\n\n\t/* Start heart beat timer */\n\tmod_timer(&phba->hb_tmofunc,\n\t\t  jiffies + msecs_to_jiffies(1000 * LPFC_HB_MBOX_INTERVAL));\n\tphba->hb_outstanding = 0;\n\tphba->last_completion_time = jiffies;\n\n\t/* start eq_delay heartbeat */\n\tif (phba->cfg_auto_imax)\n\t\tqueue_delayed_work(phba->wq, &phba->eq_delay_work,\n\t\t\t\t   msecs_to_jiffies(LPFC_EQ_DELAY_MSECS));\n\n\t/* start per phba idle_stat_delay heartbeat */\n\tlpfc_init_idle_stat_hb(phba);\n\n\t/* Start error attention (ERATT) polling timer */\n\tmod_timer(&phba->eratt_poll,\n\t\t  jiffies + msecs_to_jiffies(1000 * phba->eratt_poll_interval));\n\n\t/* Enable PCIe device Advanced Error Reporting (AER) if configured */\n\tif (phba->cfg_aer_support == 1 && !(phba->hba_flag & HBA_AER_ENABLED)) {\n\t\trc = pci_enable_pcie_error_reporting(phba->pcidev);\n\t\tif (!rc) {\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_INIT,\n\t\t\t\t\t\"2829 This device supports \"\n\t\t\t\t\t\"Advanced Error Reporting (AER)\\n\");\n\t\t\tspin_lock_irq(&phba->hbalock);\n\t\t\tphba->hba_flag |= HBA_AER_ENABLED;\n\t\t\tspin_unlock_irq(&phba->hbalock);\n\t\t} else {\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_INIT,\n\t\t\t\t\t\"2830 This device does not support \"\n\t\t\t\t\t\"Advanced Error Reporting (AER)\\n\");\n\t\t\tphba->cfg_aer_support = 0;\n\t\t}\n\t\trc = 0;\n\t}\n\n\t/*\n\t * The port is ready, set the host's link state to LINK_DOWN\n\t * in preparation for link interrupts.\n\t */\n\tspin_lock_irq(&phba->hbalock);\n\tphba->link_state = LPFC_LINK_DOWN;\n\n\t/* Check if physical ports are trunked */\n\tif (bf_get(lpfc_conf_trunk_port0, &phba->sli4_hba))\n\t\tphba->trunk_link.link0.state = LPFC_LINK_DOWN;\n\tif (bf_get(lpfc_conf_trunk_port1, &phba->sli4_hba))\n\t\tphba->trunk_link.link1.state = LPFC_LINK_DOWN;\n\tif (bf_get(lpfc_conf_trunk_port2, &phba->sli4_hba))\n\t\tphba->trunk_link.link2.state = LPFC_LINK_DOWN;\n\tif (bf_get(lpfc_conf_trunk_port3, &phba->sli4_hba))\n\t\tphba->trunk_link.link3.state = LPFC_LINK_DOWN;\n\tspin_unlock_irq(&phba->hbalock);\n\n\t/* Arm the CQs and then EQs on device */\n\tlpfc_sli4_arm_cqeq_intr(phba);\n\n\t/* Indicate device interrupt mode */\n\tphba->sli4_hba.intr_enable = 1;\n\n\tif (!(phba->hba_flag & HBA_FCOE_MODE) &&\n\t    (phba->hba_flag & LINK_DISABLED)) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"3103 Adapter Link is disabled.\\n\");\n\t\tlpfc_down_link(phba, mboxq);\n\t\trc = lpfc_sli_issue_mbox(phba, mboxq, MBX_POLL);\n\t\tif (rc != MBX_SUCCESS) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"3104 Adapter failed to issue \"\n\t\t\t\t\t\"DOWN_LINK mbox cmd, rc:x%x\\n\", rc);\n\t\t\tgoto out_io_buff_free;\n\t\t}\n\t} else if (phba->cfg_suppress_link_up == LPFC_INITIALIZE_LINK) {\n\t\t/* don't perform init_link on SLI4 FC port loopback test */\n\t\tif (!(phba->link_flag & LS_LOOPBACK_MODE)) {\n\t\t\trc = phba->lpfc_hba_init_link(phba, MBX_NOWAIT);\n\t\t\tif (rc)\n\t\t\t\tgoto out_io_buff_free;\n\t\t}\n\t}\n\tmempool_free(mboxq, phba->mbox_mem_pool);\n\treturn rc;\nout_io_buff_free:\n\t/* Free allocated IO Buffers */\n\tlpfc_io_free(phba);\nout_unset_queue:\n\t/* Unset all the queues set up in this routine when error out */\n\tlpfc_sli4_queue_unset(phba);\nout_destroy_queue:\n\tlpfc_free_iocb_list(phba);\n\tlpfc_sli4_queue_destroy(phba);\nout_stop_timers:\n\tlpfc_stop_hba_timers(phba);\nout_free_mbox:\n\tmempool_free(mboxq, phba->mbox_mem_pool);\n\treturn rc;\n}\n\n/**\n * lpfc_mbox_timeout - Timeout call back function for mbox timer\n * @t: Context to fetch pointer to hba structure from.\n *\n * This is the callback function for mailbox timer. The mailbox\n * timer is armed when a new mailbox command is issued and the timer\n * is deleted when the mailbox complete. The function is called by\n * the kernel timer code when a mailbox does not complete within\n * expected time. This function wakes up the worker thread to\n * process the mailbox timeout and returns. All the processing is\n * done by the worker thread function lpfc_mbox_timeout_handler.\n **/\nvoid\nlpfc_mbox_timeout(struct timer_list *t)\n{\n\tstruct lpfc_hba  *phba = from_timer(phba, t, sli.mbox_tmo);\n\tunsigned long iflag;\n\tuint32_t tmo_posted;\n\n\tspin_lock_irqsave(&phba->pport->work_port_lock, iflag);\n\ttmo_posted = phba->pport->work_port_events & WORKER_MBOX_TMO;\n\tif (!tmo_posted)\n\t\tphba->pport->work_port_events |= WORKER_MBOX_TMO;\n\tspin_unlock_irqrestore(&phba->pport->work_port_lock, iflag);\n\n\tif (!tmo_posted)\n\t\tlpfc_worker_wake_up(phba);\n\treturn;\n}\n\n/**\n * lpfc_sli4_mbox_completions_pending - check to see if any mailbox completions\n *                                    are pending\n * @phba: Pointer to HBA context object.\n *\n * This function checks if any mailbox completions are present on the mailbox\n * completion queue.\n **/\nstatic bool\nlpfc_sli4_mbox_completions_pending(struct lpfc_hba *phba)\n{\n\n\tuint32_t idx;\n\tstruct lpfc_queue *mcq;\n\tstruct lpfc_mcqe *mcqe;\n\tbool pending_completions = false;\n\tuint8_t\tqe_valid;\n\n\tif (unlikely(!phba) || (phba->sli_rev != LPFC_SLI_REV4))\n\t\treturn false;\n\n\t/* Check for completions on mailbox completion queue */\n\n\tmcq = phba->sli4_hba.mbx_cq;\n\tidx = mcq->hba_index;\n\tqe_valid = mcq->qe_valid;\n\twhile (bf_get_le32(lpfc_cqe_valid,\n\t       (struct lpfc_cqe *)lpfc_sli4_qe(mcq, idx)) == qe_valid) {\n\t\tmcqe = (struct lpfc_mcqe *)(lpfc_sli4_qe(mcq, idx));\n\t\tif (bf_get_le32(lpfc_trailer_completed, mcqe) &&\n\t\t    (!bf_get_le32(lpfc_trailer_async, mcqe))) {\n\t\t\tpending_completions = true;\n\t\t\tbreak;\n\t\t}\n\t\tidx = (idx + 1) % mcq->entry_count;\n\t\tif (mcq->hba_index == idx)\n\t\t\tbreak;\n\n\t\t/* if the index wrapped around, toggle the valid bit */\n\t\tif (phba->sli4_hba.pc_sli4_params.cqav && !idx)\n\t\t\tqe_valid = (qe_valid) ? 0 : 1;\n\t}\n\treturn pending_completions;\n\n}\n\n/**\n * lpfc_sli4_process_missed_mbox_completions - process mbox completions\n *\t\t\t\t\t      that were missed.\n * @phba: Pointer to HBA context object.\n *\n * For sli4, it is possible to miss an interrupt. As such mbox completions\n * maybe missed causing erroneous mailbox timeouts to occur. This function\n * checks to see if mbox completions are on the mailbox completion queue\n * and will process all the completions associated with the eq for the\n * mailbox completion queue.\n **/\nstatic bool\nlpfc_sli4_process_missed_mbox_completions(struct lpfc_hba *phba)\n{\n\tstruct lpfc_sli4_hba *sli4_hba = &phba->sli4_hba;\n\tuint32_t eqidx;\n\tstruct lpfc_queue *fpeq = NULL;\n\tstruct lpfc_queue *eq;\n\tbool mbox_pending;\n\n\tif (unlikely(!phba) || (phba->sli_rev != LPFC_SLI_REV4))\n\t\treturn false;\n\n\t/* Find the EQ associated with the mbox CQ */\n\tif (sli4_hba->hdwq) {\n\t\tfor (eqidx = 0; eqidx < phba->cfg_irq_chann; eqidx++) {\n\t\t\teq = phba->sli4_hba.hba_eq_hdl[eqidx].eq;\n\t\t\tif (eq && eq->queue_id == sli4_hba->mbx_cq->assoc_qid) {\n\t\t\t\tfpeq = eq;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tif (!fpeq)\n\t\treturn false;\n\n\t/* Turn off interrupts from this EQ */\n\n\tsli4_hba->sli4_eq_clr_intr(fpeq);\n\n\t/* Check to see if a mbox completion is pending */\n\n\tmbox_pending = lpfc_sli4_mbox_completions_pending(phba);\n\n\t/*\n\t * If a mbox completion is pending, process all the events on EQ\n\t * associated with the mbox completion queue (this could include\n\t * mailbox commands, async events, els commands, receive queue data\n\t * and fcp commands)\n\t */\n\n\tif (mbox_pending)\n\t\t/* process and rearm the EQ */\n\t\tlpfc_sli4_process_eq(phba, fpeq, LPFC_QUEUE_REARM);\n\telse\n\t\t/* Always clear and re-arm the EQ */\n\t\tsli4_hba->sli4_write_eq_db(phba, fpeq, 0, LPFC_QUEUE_REARM);\n\n\treturn mbox_pending;\n\n}\n\n/**\n * lpfc_mbox_timeout_handler - Worker thread function to handle mailbox timeout\n * @phba: Pointer to HBA context object.\n *\n * This function is called from worker thread when a mailbox command times out.\n * The caller is not required to hold any locks. This function will reset the\n * HBA and recover all the pending commands.\n **/\nvoid\nlpfc_mbox_timeout_handler(struct lpfc_hba *phba)\n{\n\tLPFC_MBOXQ_t *pmbox = phba->sli.mbox_active;\n\tMAILBOX_t *mb = NULL;\n\n\tstruct lpfc_sli *psli = &phba->sli;\n\n\t/* If the mailbox completed, process the completion and return */\n\tif (lpfc_sli4_process_missed_mbox_completions(phba))\n\t\treturn;\n\n\tif (pmbox != NULL)\n\t\tmb = &pmbox->u.mb;\n\t/* Check the pmbox pointer first.  There is a race condition\n\t * between the mbox timeout handler getting executed in the\n\t * worklist and the mailbox actually completing. When this\n\t * race condition occurs, the mbox_active will be NULL.\n\t */\n\tspin_lock_irq(&phba->hbalock);\n\tif (pmbox == NULL) {\n\t\tlpfc_printf_log(phba, KERN_WARNING,\n\t\t\t\tLOG_MBOX | LOG_SLI,\n\t\t\t\t\"0353 Active Mailbox cleared - mailbox timeout \"\n\t\t\t\t\"exiting\\n\");\n\t\tspin_unlock_irq(&phba->hbalock);\n\t\treturn;\n\t}\n\n\t/* Mbox cmd <mbxCommand> timeout */\n\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"0310 Mailbox command x%x timeout Data: x%x x%x x%px\\n\",\n\t\t\tmb->mbxCommand,\n\t\t\tphba->pport->port_state,\n\t\t\tphba->sli.sli_flag,\n\t\t\tphba->sli.mbox_active);\n\tspin_unlock_irq(&phba->hbalock);\n\n\t/* Setting state unknown so lpfc_sli_abort_iocb_ring\n\t * would get IOCB_ERROR from lpfc_sli_issue_iocb, allowing\n\t * it to fail all outstanding SCSI IO.\n\t */\n\tspin_lock_irq(&phba->pport->work_port_lock);\n\tphba->pport->work_port_events &= ~WORKER_MBOX_TMO;\n\tspin_unlock_irq(&phba->pport->work_port_lock);\n\tspin_lock_irq(&phba->hbalock);\n\tphba->link_state = LPFC_LINK_UNKNOWN;\n\tpsli->sli_flag &= ~LPFC_SLI_ACTIVE;\n\tspin_unlock_irq(&phba->hbalock);\n\n\tlpfc_sli_abort_fcp_rings(phba);\n\n\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"0345 Resetting board due to mailbox timeout\\n\");\n\n\t/* Reset the HBA device */\n\tlpfc_reset_hba(phba);\n}\n\n/**\n * lpfc_sli_issue_mbox_s3 - Issue an SLI3 mailbox command to firmware\n * @phba: Pointer to HBA context object.\n * @pmbox: Pointer to mailbox object.\n * @flag: Flag indicating how the mailbox need to be processed.\n *\n * This function is called by discovery code and HBA management code\n * to submit a mailbox command to firmware with SLI-3 interface spec. This\n * function gets the hbalock to protect the data structures.\n * The mailbox command can be submitted in polling mode, in which case\n * this function will wait in a polling loop for the completion of the\n * mailbox.\n * If the mailbox is submitted in no_wait mode (not polling) the\n * function will submit the command and returns immediately without waiting\n * for the mailbox completion. The no_wait is supported only when HBA\n * is in SLI2/SLI3 mode - interrupts are enabled.\n * The SLI interface allows only one mailbox pending at a time. If the\n * mailbox is issued in polling mode and there is already a mailbox\n * pending, then the function will return an error. If the mailbox is issued\n * in NO_WAIT mode and there is a mailbox pending already, the function\n * will return MBX_BUSY after queuing the mailbox into mailbox queue.\n * The sli layer owns the mailbox object until the completion of mailbox\n * command if this function return MBX_BUSY or MBX_SUCCESS. For all other\n * return codes the caller owns the mailbox command after the return of\n * the function.\n **/\nstatic int\nlpfc_sli_issue_mbox_s3(struct lpfc_hba *phba, LPFC_MBOXQ_t *pmbox,\n\t\t       uint32_t flag)\n{\n\tMAILBOX_t *mbx;\n\tstruct lpfc_sli *psli = &phba->sli;\n\tuint32_t status, evtctr;\n\tuint32_t ha_copy, hc_copy;\n\tint i;\n\tunsigned long timeout;\n\tunsigned long drvr_flag = 0;\n\tuint32_t word0, ldata;\n\tvoid __iomem *to_slim;\n\tint processing_queue = 0;\n\n\tspin_lock_irqsave(&phba->hbalock, drvr_flag);\n\tif (!pmbox) {\n\t\tphba->sli.sli_flag &= ~LPFC_SLI_MBOX_ACTIVE;\n\t\t/* processing mbox queue from intr_handler */\n\t\tif (unlikely(psli->sli_flag & LPFC_SLI_ASYNC_MBX_BLK)) {\n\t\t\tspin_unlock_irqrestore(&phba->hbalock, drvr_flag);\n\t\t\treturn MBX_SUCCESS;\n\t\t}\n\t\tprocessing_queue = 1;\n\t\tpmbox = lpfc_mbox_get(phba);\n\t\tif (!pmbox) {\n\t\t\tspin_unlock_irqrestore(&phba->hbalock, drvr_flag);\n\t\t\treturn MBX_SUCCESS;\n\t\t}\n\t}\n\n\tif (pmbox->mbox_cmpl && pmbox->mbox_cmpl != lpfc_sli_def_mbox_cmpl &&\n\t\tpmbox->mbox_cmpl != lpfc_sli_wake_mbox_wait) {\n\t\tif(!pmbox->vport) {\n\t\t\tspin_unlock_irqrestore(&phba->hbalock, drvr_flag);\n\t\t\tlpfc_printf_log(phba, KERN_ERR,\n\t\t\t\t\tLOG_MBOX | LOG_VPORT,\n\t\t\t\t\t\"1806 Mbox x%x failed. No vport\\n\",\n\t\t\t\t\tpmbox->u.mb.mbxCommand);\n\t\t\tdump_stack();\n\t\t\tgoto out_not_finished;\n\t\t}\n\t}\n\n\t/* If the PCI channel is in offline state, do not post mbox. */\n\tif (unlikely(pci_channel_offline(phba->pcidev))) {\n\t\tspin_unlock_irqrestore(&phba->hbalock, drvr_flag);\n\t\tgoto out_not_finished;\n\t}\n\n\t/* If HBA has a deferred error attention, fail the iocb. */\n\tif (unlikely(phba->hba_flag & DEFER_ERATT)) {\n\t\tspin_unlock_irqrestore(&phba->hbalock, drvr_flag);\n\t\tgoto out_not_finished;\n\t}\n\n\tpsli = &phba->sli;\n\n\tmbx = &pmbox->u.mb;\n\tstatus = MBX_SUCCESS;\n\n\tif (phba->link_state == LPFC_HBA_ERROR) {\n\t\tspin_unlock_irqrestore(&phba->hbalock, drvr_flag);\n\n\t\t/* Mbox command <mbxCommand> cannot issue */\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"(%d):0311 Mailbox command x%x cannot \"\n\t\t\t\t\"issue Data: x%x x%x\\n\",\n\t\t\t\tpmbox->vport ? pmbox->vport->vpi : 0,\n\t\t\t\tpmbox->u.mb.mbxCommand, psli->sli_flag, flag);\n\t\tgoto out_not_finished;\n\t}\n\n\tif (mbx->mbxCommand != MBX_KILL_BOARD && flag & MBX_NOWAIT) {\n\t\tif (lpfc_readl(phba->HCregaddr, &hc_copy) ||\n\t\t\t!(hc_copy & HC_MBINT_ENA)) {\n\t\t\tspin_unlock_irqrestore(&phba->hbalock, drvr_flag);\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"(%d):2528 Mailbox command x%x cannot \"\n\t\t\t\t\"issue Data: x%x x%x\\n\",\n\t\t\t\tpmbox->vport ? pmbox->vport->vpi : 0,\n\t\t\t\tpmbox->u.mb.mbxCommand, psli->sli_flag, flag);\n\t\t\tgoto out_not_finished;\n\t\t}\n\t}\n\n\tif (psli->sli_flag & LPFC_SLI_MBOX_ACTIVE) {\n\t\t/* Polling for a mbox command when another one is already active\n\t\t * is not allowed in SLI. Also, the driver must have established\n\t\t * SLI2 mode to queue and process multiple mbox commands.\n\t\t */\n\n\t\tif (flag & MBX_POLL) {\n\t\t\tspin_unlock_irqrestore(&phba->hbalock, drvr_flag);\n\n\t\t\t/* Mbox command <mbxCommand> cannot issue */\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"(%d):2529 Mailbox command x%x \"\n\t\t\t\t\t\"cannot issue Data: x%x x%x\\n\",\n\t\t\t\t\tpmbox->vport ? pmbox->vport->vpi : 0,\n\t\t\t\t\tpmbox->u.mb.mbxCommand,\n\t\t\t\t\tpsli->sli_flag, flag);\n\t\t\tgoto out_not_finished;\n\t\t}\n\n\t\tif (!(psli->sli_flag & LPFC_SLI_ACTIVE)) {\n\t\t\tspin_unlock_irqrestore(&phba->hbalock, drvr_flag);\n\t\t\t/* Mbox command <mbxCommand> cannot issue */\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"(%d):2530 Mailbox command x%x \"\n\t\t\t\t\t\"cannot issue Data: x%x x%x\\n\",\n\t\t\t\t\tpmbox->vport ? pmbox->vport->vpi : 0,\n\t\t\t\t\tpmbox->u.mb.mbxCommand,\n\t\t\t\t\tpsli->sli_flag, flag);\n\t\t\tgoto out_not_finished;\n\t\t}\n\n\t\t/* Another mailbox command is still being processed, queue this\n\t\t * command to be processed later.\n\t\t */\n\t\tlpfc_mbox_put(phba, pmbox);\n\n\t\t/* Mbox cmd issue - BUSY */\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_MBOX | LOG_SLI,\n\t\t\t\t\"(%d):0308 Mbox cmd issue - BUSY Data: \"\n\t\t\t\t\"x%x x%x x%x x%x\\n\",\n\t\t\t\tpmbox->vport ? pmbox->vport->vpi : 0xffffff,\n\t\t\t\tmbx->mbxCommand,\n\t\t\t\tphba->pport ? phba->pport->port_state : 0xff,\n\t\t\t\tpsli->sli_flag, flag);\n\n\t\tpsli->slistat.mbox_busy++;\n\t\tspin_unlock_irqrestore(&phba->hbalock, drvr_flag);\n\n\t\tif (pmbox->vport) {\n\t\t\tlpfc_debugfs_disc_trc(pmbox->vport,\n\t\t\t\tLPFC_DISC_TRC_MBOX_VPORT,\n\t\t\t\t\"MBOX Bsy vport:  cmd:x%x mb:x%x x%x\",\n\t\t\t\t(uint32_t)mbx->mbxCommand,\n\t\t\t\tmbx->un.varWords[0], mbx->un.varWords[1]);\n\t\t}\n\t\telse {\n\t\t\tlpfc_debugfs_disc_trc(phba->pport,\n\t\t\t\tLPFC_DISC_TRC_MBOX,\n\t\t\t\t\"MBOX Bsy:        cmd:x%x mb:x%x x%x\",\n\t\t\t\t(uint32_t)mbx->mbxCommand,\n\t\t\t\tmbx->un.varWords[0], mbx->un.varWords[1]);\n\t\t}\n\n\t\treturn MBX_BUSY;\n\t}\n\n\tpsli->sli_flag |= LPFC_SLI_MBOX_ACTIVE;\n\n\t/* If we are not polling, we MUST be in SLI2 mode */\n\tif (flag != MBX_POLL) {\n\t\tif (!(psli->sli_flag & LPFC_SLI_ACTIVE) &&\n\t\t    (mbx->mbxCommand != MBX_KILL_BOARD)) {\n\t\t\tpsli->sli_flag &= ~LPFC_SLI_MBOX_ACTIVE;\n\t\t\tspin_unlock_irqrestore(&phba->hbalock, drvr_flag);\n\t\t\t/* Mbox command <mbxCommand> cannot issue */\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"(%d):2531 Mailbox command x%x \"\n\t\t\t\t\t\"cannot issue Data: x%x x%x\\n\",\n\t\t\t\t\tpmbox->vport ? pmbox->vport->vpi : 0,\n\t\t\t\t\tpmbox->u.mb.mbxCommand,\n\t\t\t\t\tpsli->sli_flag, flag);\n\t\t\tgoto out_not_finished;\n\t\t}\n\t\t/* timeout active mbox command */\n\t\ttimeout = msecs_to_jiffies(lpfc_mbox_tmo_val(phba, pmbox) *\n\t\t\t\t\t   1000);\n\t\tmod_timer(&psli->mbox_tmo, jiffies + timeout);\n\t}\n\n\t/* Mailbox cmd <cmd> issue */\n\tlpfc_printf_log(phba, KERN_INFO, LOG_MBOX | LOG_SLI,\n\t\t\t\"(%d):0309 Mailbox cmd x%x issue Data: x%x x%x \"\n\t\t\t\"x%x\\n\",\n\t\t\tpmbox->vport ? pmbox->vport->vpi : 0,\n\t\t\tmbx->mbxCommand,\n\t\t\tphba->pport ? phba->pport->port_state : 0xff,\n\t\t\tpsli->sli_flag, flag);\n\n\tif (mbx->mbxCommand != MBX_HEARTBEAT) {\n\t\tif (pmbox->vport) {\n\t\t\tlpfc_debugfs_disc_trc(pmbox->vport,\n\t\t\t\tLPFC_DISC_TRC_MBOX_VPORT,\n\t\t\t\t\"MBOX Send vport: cmd:x%x mb:x%x x%x\",\n\t\t\t\t(uint32_t)mbx->mbxCommand,\n\t\t\t\tmbx->un.varWords[0], mbx->un.varWords[1]);\n\t\t}\n\t\telse {\n\t\t\tlpfc_debugfs_disc_trc(phba->pport,\n\t\t\t\tLPFC_DISC_TRC_MBOX,\n\t\t\t\t\"MBOX Send:       cmd:x%x mb:x%x x%x\",\n\t\t\t\t(uint32_t)mbx->mbxCommand,\n\t\t\t\tmbx->un.varWords[0], mbx->un.varWords[1]);\n\t\t}\n\t}\n\n\tpsli->slistat.mbox_cmd++;\n\tevtctr = psli->slistat.mbox_event;\n\n\t/* next set own bit for the adapter and copy over command word */\n\tmbx->mbxOwner = OWN_CHIP;\n\n\tif (psli->sli_flag & LPFC_SLI_ACTIVE) {\n\t\t/* Populate mbox extension offset word. */\n\t\tif (pmbox->in_ext_byte_len || pmbox->out_ext_byte_len) {\n\t\t\t*(((uint32_t *)mbx) + pmbox->mbox_offset_word)\n\t\t\t\t= (uint8_t *)phba->mbox_ext\n\t\t\t\t  - (uint8_t *)phba->mbox;\n\t\t}\n\n\t\t/* Copy the mailbox extension data */\n\t\tif (pmbox->in_ext_byte_len && pmbox->ctx_buf) {\n\t\t\tlpfc_sli_pcimem_bcopy(pmbox->ctx_buf,\n\t\t\t\t\t      (uint8_t *)phba->mbox_ext,\n\t\t\t\t\t      pmbox->in_ext_byte_len);\n\t\t}\n\t\t/* Copy command data to host SLIM area */\n\t\tlpfc_sli_pcimem_bcopy(mbx, phba->mbox, MAILBOX_CMD_SIZE);\n\t} else {\n\t\t/* Populate mbox extension offset word. */\n\t\tif (pmbox->in_ext_byte_len || pmbox->out_ext_byte_len)\n\t\t\t*(((uint32_t *)mbx) + pmbox->mbox_offset_word)\n\t\t\t\t= MAILBOX_HBA_EXT_OFFSET;\n\n\t\t/* Copy the mailbox extension data */\n\t\tif (pmbox->in_ext_byte_len && pmbox->ctx_buf)\n\t\t\tlpfc_memcpy_to_slim(phba->MBslimaddr +\n\t\t\t\tMAILBOX_HBA_EXT_OFFSET,\n\t\t\t\tpmbox->ctx_buf, pmbox->in_ext_byte_len);\n\n\t\tif (mbx->mbxCommand == MBX_CONFIG_PORT)\n\t\t\t/* copy command data into host mbox for cmpl */\n\t\t\tlpfc_sli_pcimem_bcopy(mbx, phba->mbox,\n\t\t\t\t\t      MAILBOX_CMD_SIZE);\n\n\t\t/* First copy mbox command data to HBA SLIM, skip past first\n\t\t   word */\n\t\tto_slim = phba->MBslimaddr + sizeof (uint32_t);\n\t\tlpfc_memcpy_to_slim(to_slim, &mbx->un.varWords[0],\n\t\t\t    MAILBOX_CMD_SIZE - sizeof (uint32_t));\n\n\t\t/* Next copy over first word, with mbxOwner set */\n\t\tldata = *((uint32_t *)mbx);\n\t\tto_slim = phba->MBslimaddr;\n\t\twritel(ldata, to_slim);\n\t\treadl(to_slim); /* flush */\n\n\t\tif (mbx->mbxCommand == MBX_CONFIG_PORT)\n\t\t\t/* switch over to host mailbox */\n\t\t\tpsli->sli_flag |= LPFC_SLI_ACTIVE;\n\t}\n\n\twmb();\n\n\tswitch (flag) {\n\tcase MBX_NOWAIT:\n\t\t/* Set up reference to mailbox command */\n\t\tpsli->mbox_active = pmbox;\n\t\t/* Interrupt board to do it */\n\t\twritel(CA_MBATT, phba->CAregaddr);\n\t\treadl(phba->CAregaddr); /* flush */\n\t\t/* Don't wait for it to finish, just return */\n\t\tbreak;\n\n\tcase MBX_POLL:\n\t\t/* Set up null reference to mailbox command */\n\t\tpsli->mbox_active = NULL;\n\t\t/* Interrupt board to do it */\n\t\twritel(CA_MBATT, phba->CAregaddr);\n\t\treadl(phba->CAregaddr); /* flush */\n\n\t\tif (psli->sli_flag & LPFC_SLI_ACTIVE) {\n\t\t\t/* First read mbox status word */\n\t\t\tword0 = *((uint32_t *)phba->mbox);\n\t\t\tword0 = le32_to_cpu(word0);\n\t\t} else {\n\t\t\t/* First read mbox status word */\n\t\t\tif (lpfc_readl(phba->MBslimaddr, &word0)) {\n\t\t\t\tspin_unlock_irqrestore(&phba->hbalock,\n\t\t\t\t\t\t       drvr_flag);\n\t\t\t\tgoto out_not_finished;\n\t\t\t}\n\t\t}\n\n\t\t/* Read the HBA Host Attention Register */\n\t\tif (lpfc_readl(phba->HAregaddr, &ha_copy)) {\n\t\t\tspin_unlock_irqrestore(&phba->hbalock,\n\t\t\t\t\t\t       drvr_flag);\n\t\t\tgoto out_not_finished;\n\t\t}\n\t\ttimeout = msecs_to_jiffies(lpfc_mbox_tmo_val(phba, pmbox) *\n\t\t\t\t\t\t\t1000) + jiffies;\n\t\ti = 0;\n\t\t/* Wait for command to complete */\n\t\twhile (((word0 & OWN_CHIP) == OWN_CHIP) ||\n\t\t       (!(ha_copy & HA_MBATT) &&\n\t\t\t(phba->link_state > LPFC_WARM_START))) {\n\t\t\tif (time_after(jiffies, timeout)) {\n\t\t\t\tpsli->sli_flag &= ~LPFC_SLI_MBOX_ACTIVE;\n\t\t\t\tspin_unlock_irqrestore(&phba->hbalock,\n\t\t\t\t\t\t       drvr_flag);\n\t\t\t\tgoto out_not_finished;\n\t\t\t}\n\n\t\t\t/* Check if we took a mbox interrupt while we were\n\t\t\t   polling */\n\t\t\tif (((word0 & OWN_CHIP) != OWN_CHIP)\n\t\t\t    && (evtctr != psli->slistat.mbox_event))\n\t\t\t\tbreak;\n\n\t\t\tif (i++ > 10) {\n\t\t\t\tspin_unlock_irqrestore(&phba->hbalock,\n\t\t\t\t\t\t       drvr_flag);\n\t\t\t\tmsleep(1);\n\t\t\t\tspin_lock_irqsave(&phba->hbalock, drvr_flag);\n\t\t\t}\n\n\t\t\tif (psli->sli_flag & LPFC_SLI_ACTIVE) {\n\t\t\t\t/* First copy command data */\n\t\t\t\tword0 = *((uint32_t *)phba->mbox);\n\t\t\t\tword0 = le32_to_cpu(word0);\n\t\t\t\tif (mbx->mbxCommand == MBX_CONFIG_PORT) {\n\t\t\t\t\tMAILBOX_t *slimmb;\n\t\t\t\t\tuint32_t slimword0;\n\t\t\t\t\t/* Check real SLIM for any errors */\n\t\t\t\t\tslimword0 = readl(phba->MBslimaddr);\n\t\t\t\t\tslimmb = (MAILBOX_t *) & slimword0;\n\t\t\t\t\tif (((slimword0 & OWN_CHIP) != OWN_CHIP)\n\t\t\t\t\t    && slimmb->mbxStatus) {\n\t\t\t\t\t\tpsli->sli_flag &=\n\t\t\t\t\t\t    ~LPFC_SLI_ACTIVE;\n\t\t\t\t\t\tword0 = slimword0;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t/* First copy command data */\n\t\t\t\tword0 = readl(phba->MBslimaddr);\n\t\t\t}\n\t\t\t/* Read the HBA Host Attention Register */\n\t\t\tif (lpfc_readl(phba->HAregaddr, &ha_copy)) {\n\t\t\t\tspin_unlock_irqrestore(&phba->hbalock,\n\t\t\t\t\t\t       drvr_flag);\n\t\t\t\tgoto out_not_finished;\n\t\t\t}\n\t\t}\n\n\t\tif (psli->sli_flag & LPFC_SLI_ACTIVE) {\n\t\t\t/* copy results back to user */\n\t\t\tlpfc_sli_pcimem_bcopy(phba->mbox, mbx,\n\t\t\t\t\t\tMAILBOX_CMD_SIZE);\n\t\t\t/* Copy the mailbox extension data */\n\t\t\tif (pmbox->out_ext_byte_len && pmbox->ctx_buf) {\n\t\t\t\tlpfc_sli_pcimem_bcopy(phba->mbox_ext,\n\t\t\t\t\t\t      pmbox->ctx_buf,\n\t\t\t\t\t\t      pmbox->out_ext_byte_len);\n\t\t\t}\n\t\t} else {\n\t\t\t/* First copy command data */\n\t\t\tlpfc_memcpy_from_slim(mbx, phba->MBslimaddr,\n\t\t\t\t\t\tMAILBOX_CMD_SIZE);\n\t\t\t/* Copy the mailbox extension data */\n\t\t\tif (pmbox->out_ext_byte_len && pmbox->ctx_buf) {\n\t\t\t\tlpfc_memcpy_from_slim(\n\t\t\t\t\tpmbox->ctx_buf,\n\t\t\t\t\tphba->MBslimaddr +\n\t\t\t\t\tMAILBOX_HBA_EXT_OFFSET,\n\t\t\t\t\tpmbox->out_ext_byte_len);\n\t\t\t}\n\t\t}\n\n\t\twritel(HA_MBATT, phba->HAregaddr);\n\t\treadl(phba->HAregaddr); /* flush */\n\n\t\tpsli->sli_flag &= ~LPFC_SLI_MBOX_ACTIVE;\n\t\tstatus = mbx->mbxStatus;\n\t}\n\n\tspin_unlock_irqrestore(&phba->hbalock, drvr_flag);\n\treturn status;\n\nout_not_finished:\n\tif (processing_queue) {\n\t\tpmbox->u.mb.mbxStatus = MBX_NOT_FINISHED;\n\t\tlpfc_mbox_cmpl_put(phba, pmbox);\n\t}\n\treturn MBX_NOT_FINISHED;\n}\n\n/**\n * lpfc_sli4_async_mbox_block - Block posting SLI4 asynchronous mailbox command\n * @phba: Pointer to HBA context object.\n *\n * The function blocks the posting of SLI4 asynchronous mailbox commands from\n * the driver internal pending mailbox queue. It will then try to wait out the\n * possible outstanding mailbox command before return.\n *\n * Returns:\n * \t0 - the outstanding mailbox command completed; otherwise, the wait for\n * \tthe outstanding mailbox command timed out.\n **/\nstatic int\nlpfc_sli4_async_mbox_block(struct lpfc_hba *phba)\n{\n\tstruct lpfc_sli *psli = &phba->sli;\n\tint rc = 0;\n\tunsigned long timeout = 0;\n\n\t/* Mark the asynchronous mailbox command posting as blocked */\n\tspin_lock_irq(&phba->hbalock);\n\tpsli->sli_flag |= LPFC_SLI_ASYNC_MBX_BLK;\n\t/* Determine how long we might wait for the active mailbox\n\t * command to be gracefully completed by firmware.\n\t */\n\tif (phba->sli.mbox_active)\n\t\ttimeout = msecs_to_jiffies(lpfc_mbox_tmo_val(phba,\n\t\t\t\t\t\tphba->sli.mbox_active) *\n\t\t\t\t\t\t1000) + jiffies;\n\tspin_unlock_irq(&phba->hbalock);\n\n\t/* Make sure the mailbox is really active */\n\tif (timeout)\n\t\tlpfc_sli4_process_missed_mbox_completions(phba);\n\n\t/* Wait for the outstnading mailbox command to complete */\n\twhile (phba->sli.mbox_active) {\n\t\t/* Check active mailbox complete status every 2ms */\n\t\tmsleep(2);\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\t/* Timeout, marked the outstanding cmd not complete */\n\t\t\trc = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Can not cleanly block async mailbox command, fails it */\n\tif (rc) {\n\t\tspin_lock_irq(&phba->hbalock);\n\t\tpsli->sli_flag &= ~LPFC_SLI_ASYNC_MBX_BLK;\n\t\tspin_unlock_irq(&phba->hbalock);\n\t}\n\treturn rc;\n}\n\n/**\n * lpfc_sli4_async_mbox_unblock - Block posting SLI4 async mailbox command\n * @phba: Pointer to HBA context object.\n *\n * The function unblocks and resume posting of SLI4 asynchronous mailbox\n * commands from the driver internal pending mailbox queue. It makes sure\n * that there is no outstanding mailbox command before resuming posting\n * asynchronous mailbox commands. If, for any reason, there is outstanding\n * mailbox command, it will try to wait it out before resuming asynchronous\n * mailbox command posting.\n **/\nstatic void\nlpfc_sli4_async_mbox_unblock(struct lpfc_hba *phba)\n{\n\tstruct lpfc_sli *psli = &phba->sli;\n\n\tspin_lock_irq(&phba->hbalock);\n\tif (!(psli->sli_flag & LPFC_SLI_ASYNC_MBX_BLK)) {\n\t\t/* Asynchronous mailbox posting is not blocked, do nothing */\n\t\tspin_unlock_irq(&phba->hbalock);\n\t\treturn;\n\t}\n\n\t/* Outstanding synchronous mailbox command is guaranteed to be done,\n\t * successful or timeout, after timing-out the outstanding mailbox\n\t * command shall always be removed, so just unblock posting async\n\t * mailbox command and resume\n\t */\n\tpsli->sli_flag &= ~LPFC_SLI_ASYNC_MBX_BLK;\n\tspin_unlock_irq(&phba->hbalock);\n\n\t/* wake up worker thread to post asynchronous mailbox command */\n\tlpfc_worker_wake_up(phba);\n}\n\n/**\n * lpfc_sli4_wait_bmbx_ready - Wait for bootstrap mailbox register ready\n * @phba: Pointer to HBA context object.\n * @mboxq: Pointer to mailbox object.\n *\n * The function waits for the bootstrap mailbox register ready bit from\n * port for twice the regular mailbox command timeout value.\n *\n *      0 - no timeout on waiting for bootstrap mailbox register ready.\n *      MBXERR_ERROR - wait for bootstrap mailbox register timed out.\n **/\nstatic int\nlpfc_sli4_wait_bmbx_ready(struct lpfc_hba *phba, LPFC_MBOXQ_t *mboxq)\n{\n\tuint32_t db_ready;\n\tunsigned long timeout;\n\tstruct lpfc_register bmbx_reg;\n\n\ttimeout = msecs_to_jiffies(lpfc_mbox_tmo_val(phba, mboxq)\n\t\t\t\t   * 1000) + jiffies;\n\n\tdo {\n\t\tbmbx_reg.word0 = readl(phba->sli4_hba.BMBXregaddr);\n\t\tdb_ready = bf_get(lpfc_bmbx_rdy, &bmbx_reg);\n\t\tif (!db_ready)\n\t\t\tmdelay(2);\n\n\t\tif (time_after(jiffies, timeout))\n\t\t\treturn MBXERR_ERROR;\n\t} while (!db_ready);\n\n\treturn 0;\n}\n\n/**\n * lpfc_sli4_post_sync_mbox - Post an SLI4 mailbox to the bootstrap mailbox\n * @phba: Pointer to HBA context object.\n * @mboxq: Pointer to mailbox object.\n *\n * The function posts a mailbox to the port.  The mailbox is expected\n * to be comletely filled in and ready for the port to operate on it.\n * This routine executes a synchronous completion operation on the\n * mailbox by polling for its completion.\n *\n * The caller must not be holding any locks when calling this routine.\n *\n * Returns:\n *\tMBX_SUCCESS - mailbox posted successfully\n *\tAny of the MBX error values.\n **/\nstatic int\nlpfc_sli4_post_sync_mbox(struct lpfc_hba *phba, LPFC_MBOXQ_t *mboxq)\n{\n\tint rc = MBX_SUCCESS;\n\tunsigned long iflag;\n\tuint32_t mcqe_status;\n\tuint32_t mbx_cmnd;\n\tstruct lpfc_sli *psli = &phba->sli;\n\tstruct lpfc_mqe *mb = &mboxq->u.mqe;\n\tstruct lpfc_bmbx_create *mbox_rgn;\n\tstruct dma_address *dma_address;\n\n\t/*\n\t * Only one mailbox can be active to the bootstrap mailbox region\n\t * at a time and there is no queueing provided.\n\t */\n\tspin_lock_irqsave(&phba->hbalock, iflag);\n\tif (psli->sli_flag & LPFC_SLI_MBOX_ACTIVE) {\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"(%d):2532 Mailbox command x%x (x%x/x%x) \"\n\t\t\t\t\"cannot issue Data: x%x x%x\\n\",\n\t\t\t\tmboxq->vport ? mboxq->vport->vpi : 0,\n\t\t\t\tmboxq->u.mb.mbxCommand,\n\t\t\t\tlpfc_sli_config_mbox_subsys_get(phba, mboxq),\n\t\t\t\tlpfc_sli_config_mbox_opcode_get(phba, mboxq),\n\t\t\t\tpsli->sli_flag, MBX_POLL);\n\t\treturn MBXERR_ERROR;\n\t}\n\t/* The server grabs the token and owns it until release */\n\tpsli->sli_flag |= LPFC_SLI_MBOX_ACTIVE;\n\tphba->sli.mbox_active = mboxq;\n\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\n\t/* wait for bootstrap mbox register for readyness */\n\trc = lpfc_sli4_wait_bmbx_ready(phba, mboxq);\n\tif (rc)\n\t\tgoto exit;\n\t/*\n\t * Initialize the bootstrap memory region to avoid stale data areas\n\t * in the mailbox post.  Then copy the caller's mailbox contents to\n\t * the bmbx mailbox region.\n\t */\n\tmbx_cmnd = bf_get(lpfc_mqe_command, mb);\n\tmemset(phba->sli4_hba.bmbx.avirt, 0, sizeof(struct lpfc_bmbx_create));\n\tlpfc_sli4_pcimem_bcopy(mb, phba->sli4_hba.bmbx.avirt,\n\t\t\t       sizeof(struct lpfc_mqe));\n\n\t/* Post the high mailbox dma address to the port and wait for ready. */\n\tdma_address = &phba->sli4_hba.bmbx.dma_address;\n\twritel(dma_address->addr_hi, phba->sli4_hba.BMBXregaddr);\n\n\t/* wait for bootstrap mbox register for hi-address write done */\n\trc = lpfc_sli4_wait_bmbx_ready(phba, mboxq);\n\tif (rc)\n\t\tgoto exit;\n\n\t/* Post the low mailbox dma address to the port. */\n\twritel(dma_address->addr_lo, phba->sli4_hba.BMBXregaddr);\n\n\t/* wait for bootstrap mbox register for low address write done */\n\trc = lpfc_sli4_wait_bmbx_ready(phba, mboxq);\n\tif (rc)\n\t\tgoto exit;\n\n\t/*\n\t * Read the CQ to ensure the mailbox has completed.\n\t * If so, update the mailbox status so that the upper layers\n\t * can complete the request normally.\n\t */\n\tlpfc_sli4_pcimem_bcopy(phba->sli4_hba.bmbx.avirt, mb,\n\t\t\t       sizeof(struct lpfc_mqe));\n\tmbox_rgn = (struct lpfc_bmbx_create *) phba->sli4_hba.bmbx.avirt;\n\tlpfc_sli4_pcimem_bcopy(&mbox_rgn->mcqe, &mboxq->mcqe,\n\t\t\t       sizeof(struct lpfc_mcqe));\n\tmcqe_status = bf_get(lpfc_mcqe_status, &mbox_rgn->mcqe);\n\t/*\n\t * When the CQE status indicates a failure and the mailbox status\n\t * indicates success then copy the CQE status into the mailbox status\n\t * (and prefix it with x4000).\n\t */\n\tif (mcqe_status != MB_CQE_STATUS_SUCCESS) {\n\t\tif (bf_get(lpfc_mqe_status, mb) == MBX_SUCCESS)\n\t\t\tbf_set(lpfc_mqe_status, mb,\n\t\t\t       (LPFC_MBX_ERROR_RANGE | mcqe_status));\n\t\trc = MBXERR_ERROR;\n\t} else\n\t\tlpfc_sli4_swap_str(phba, mboxq);\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_MBOX | LOG_SLI,\n\t\t\t\"(%d):0356 Mailbox cmd x%x (x%x/x%x) Status x%x \"\n\t\t\t\"Data: x%x x%x x%x x%x x%x x%x x%x x%x x%x x%x x%x\"\n\t\t\t\" x%x x%x CQ: x%x x%x x%x x%x\\n\",\n\t\t\tmboxq->vport ? mboxq->vport->vpi : 0, mbx_cmnd,\n\t\t\tlpfc_sli_config_mbox_subsys_get(phba, mboxq),\n\t\t\tlpfc_sli_config_mbox_opcode_get(phba, mboxq),\n\t\t\tbf_get(lpfc_mqe_status, mb),\n\t\t\tmb->un.mb_words[0], mb->un.mb_words[1],\n\t\t\tmb->un.mb_words[2], mb->un.mb_words[3],\n\t\t\tmb->un.mb_words[4], mb->un.mb_words[5],\n\t\t\tmb->un.mb_words[6], mb->un.mb_words[7],\n\t\t\tmb->un.mb_words[8], mb->un.mb_words[9],\n\t\t\tmb->un.mb_words[10], mb->un.mb_words[11],\n\t\t\tmb->un.mb_words[12], mboxq->mcqe.word0,\n\t\t\tmboxq->mcqe.mcqe_tag0, \tmboxq->mcqe.mcqe_tag1,\n\t\t\tmboxq->mcqe.trailer);\nexit:\n\t/* We are holding the token, no needed for lock when release */\n\tspin_lock_irqsave(&phba->hbalock, iflag);\n\tpsli->sli_flag &= ~LPFC_SLI_MBOX_ACTIVE;\n\tphba->sli.mbox_active = NULL;\n\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\treturn rc;\n}\n\n/**\n * lpfc_sli_issue_mbox_s4 - Issue an SLI4 mailbox command to firmware\n * @phba: Pointer to HBA context object.\n * @mboxq: Pointer to mailbox object.\n * @flag: Flag indicating how the mailbox need to be processed.\n *\n * This function is called by discovery code and HBA management code to submit\n * a mailbox command to firmware with SLI-4 interface spec.\n *\n * Return codes the caller owns the mailbox command after the return of the\n * function.\n **/\nstatic int\nlpfc_sli_issue_mbox_s4(struct lpfc_hba *phba, LPFC_MBOXQ_t *mboxq,\n\t\t       uint32_t flag)\n{\n\tstruct lpfc_sli *psli = &phba->sli;\n\tunsigned long iflags;\n\tint rc;\n\n\t/* dump from issue mailbox command if setup */\n\tlpfc_idiag_mbxacc_dump_issue_mbox(phba, &mboxq->u.mb);\n\n\trc = lpfc_mbox_dev_check(phba);\n\tif (unlikely(rc)) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"(%d):2544 Mailbox command x%x (x%x/x%x) \"\n\t\t\t\t\"cannot issue Data: x%x x%x\\n\",\n\t\t\t\tmboxq->vport ? mboxq->vport->vpi : 0,\n\t\t\t\tmboxq->u.mb.mbxCommand,\n\t\t\t\tlpfc_sli_config_mbox_subsys_get(phba, mboxq),\n\t\t\t\tlpfc_sli_config_mbox_opcode_get(phba, mboxq),\n\t\t\t\tpsli->sli_flag, flag);\n\t\tgoto out_not_finished;\n\t}\n\n\t/* Detect polling mode and jump to a handler */\n\tif (!phba->sli4_hba.intr_enable) {\n\t\tif (flag == MBX_POLL)\n\t\t\trc = lpfc_sli4_post_sync_mbox(phba, mboxq);\n\t\telse\n\t\t\trc = -EIO;\n\t\tif (rc != MBX_SUCCESS)\n\t\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_MBOX | LOG_SLI,\n\t\t\t\t\t\"(%d):2541 Mailbox command x%x \"\n\t\t\t\t\t\"(x%x/x%x) failure: \"\n\t\t\t\t\t\"mqe_sta: x%x mcqe_sta: x%x/x%x \"\n\t\t\t\t\t\"Data: x%x x%x\\n,\",\n\t\t\t\t\tmboxq->vport ? mboxq->vport->vpi : 0,\n\t\t\t\t\tmboxq->u.mb.mbxCommand,\n\t\t\t\t\tlpfc_sli_config_mbox_subsys_get(phba,\n\t\t\t\t\t\t\t\t\tmboxq),\n\t\t\t\t\tlpfc_sli_config_mbox_opcode_get(phba,\n\t\t\t\t\t\t\t\t\tmboxq),\n\t\t\t\t\tbf_get(lpfc_mqe_status, &mboxq->u.mqe),\n\t\t\t\t\tbf_get(lpfc_mcqe_status, &mboxq->mcqe),\n\t\t\t\t\tbf_get(lpfc_mcqe_ext_status,\n\t\t\t\t\t       &mboxq->mcqe),\n\t\t\t\t\tpsli->sli_flag, flag);\n\t\treturn rc;\n\t} else if (flag == MBX_POLL) {\n\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_MBOX | LOG_SLI,\n\t\t\t\t\"(%d):2542 Try to issue mailbox command \"\n\t\t\t\t\"x%x (x%x/x%x) synchronously ahead of async \"\n\t\t\t\t\"mailbox command queue: x%x x%x\\n\",\n\t\t\t\tmboxq->vport ? mboxq->vport->vpi : 0,\n\t\t\t\tmboxq->u.mb.mbxCommand,\n\t\t\t\tlpfc_sli_config_mbox_subsys_get(phba, mboxq),\n\t\t\t\tlpfc_sli_config_mbox_opcode_get(phba, mboxq),\n\t\t\t\tpsli->sli_flag, flag);\n\t\t/* Try to block the asynchronous mailbox posting */\n\t\trc = lpfc_sli4_async_mbox_block(phba);\n\t\tif (!rc) {\n\t\t\t/* Successfully blocked, now issue sync mbox cmd */\n\t\t\trc = lpfc_sli4_post_sync_mbox(phba, mboxq);\n\t\t\tif (rc != MBX_SUCCESS)\n\t\t\t\tlpfc_printf_log(phba, KERN_WARNING,\n\t\t\t\t\tLOG_MBOX | LOG_SLI,\n\t\t\t\t\t\"(%d):2597 Sync Mailbox command \"\n\t\t\t\t\t\"x%x (x%x/x%x) failure: \"\n\t\t\t\t\t\"mqe_sta: x%x mcqe_sta: x%x/x%x \"\n\t\t\t\t\t\"Data: x%x x%x\\n,\",\n\t\t\t\t\tmboxq->vport ? mboxq->vport->vpi : 0,\n\t\t\t\t\tmboxq->u.mb.mbxCommand,\n\t\t\t\t\tlpfc_sli_config_mbox_subsys_get(phba,\n\t\t\t\t\t\t\t\t\tmboxq),\n\t\t\t\t\tlpfc_sli_config_mbox_opcode_get(phba,\n\t\t\t\t\t\t\t\t\tmboxq),\n\t\t\t\t\tbf_get(lpfc_mqe_status, &mboxq->u.mqe),\n\t\t\t\t\tbf_get(lpfc_mcqe_status, &mboxq->mcqe),\n\t\t\t\t\tbf_get(lpfc_mcqe_ext_status,\n\t\t\t\t\t       &mboxq->mcqe),\n\t\t\t\t\tpsli->sli_flag, flag);\n\t\t\t/* Unblock the async mailbox posting afterward */\n\t\t\tlpfc_sli4_async_mbox_unblock(phba);\n\t\t}\n\t\treturn rc;\n\t}\n\n\t/* Now, interrupt mode asynchronous mailbox command */\n\trc = lpfc_mbox_cmd_check(phba, mboxq);\n\tif (rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"(%d):2543 Mailbox command x%x (x%x/x%x) \"\n\t\t\t\t\"cannot issue Data: x%x x%x\\n\",\n\t\t\t\tmboxq->vport ? mboxq->vport->vpi : 0,\n\t\t\t\tmboxq->u.mb.mbxCommand,\n\t\t\t\tlpfc_sli_config_mbox_subsys_get(phba, mboxq),\n\t\t\t\tlpfc_sli_config_mbox_opcode_get(phba, mboxq),\n\t\t\t\tpsli->sli_flag, flag);\n\t\tgoto out_not_finished;\n\t}\n\n\t/* Put the mailbox command to the driver internal FIFO */\n\tpsli->slistat.mbox_busy++;\n\tspin_lock_irqsave(&phba->hbalock, iflags);\n\tlpfc_mbox_put(phba, mboxq);\n\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\tlpfc_printf_log(phba, KERN_INFO, LOG_MBOX | LOG_SLI,\n\t\t\t\"(%d):0354 Mbox cmd issue - Enqueue Data: \"\n\t\t\t\"x%x (x%x/x%x) x%x x%x x%x\\n\",\n\t\t\tmboxq->vport ? mboxq->vport->vpi : 0xffffff,\n\t\t\tbf_get(lpfc_mqe_command, &mboxq->u.mqe),\n\t\t\tlpfc_sli_config_mbox_subsys_get(phba, mboxq),\n\t\t\tlpfc_sli_config_mbox_opcode_get(phba, mboxq),\n\t\t\tphba->pport->port_state,\n\t\t\tpsli->sli_flag, MBX_NOWAIT);\n\t/* Wake up worker thread to transport mailbox command from head */\n\tlpfc_worker_wake_up(phba);\n\n\treturn MBX_BUSY;\n\nout_not_finished:\n\treturn MBX_NOT_FINISHED;\n}\n\n/**\n * lpfc_sli4_post_async_mbox - Post an SLI4 mailbox command to device\n * @phba: Pointer to HBA context object.\n *\n * This function is called by worker thread to send a mailbox command to\n * SLI4 HBA firmware.\n *\n **/\nint\nlpfc_sli4_post_async_mbox(struct lpfc_hba *phba)\n{\n\tstruct lpfc_sli *psli = &phba->sli;\n\tLPFC_MBOXQ_t *mboxq;\n\tint rc = MBX_SUCCESS;\n\tunsigned long iflags;\n\tstruct lpfc_mqe *mqe;\n\tuint32_t mbx_cmnd;\n\n\t/* Check interrupt mode before post async mailbox command */\n\tif (unlikely(!phba->sli4_hba.intr_enable))\n\t\treturn MBX_NOT_FINISHED;\n\n\t/* Check for mailbox command service token */\n\tspin_lock_irqsave(&phba->hbalock, iflags);\n\tif (unlikely(psli->sli_flag & LPFC_SLI_ASYNC_MBX_BLK)) {\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t\treturn MBX_NOT_FINISHED;\n\t}\n\tif (psli->sli_flag & LPFC_SLI_MBOX_ACTIVE) {\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t\treturn MBX_NOT_FINISHED;\n\t}\n\tif (unlikely(phba->sli.mbox_active)) {\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0384 There is pending active mailbox cmd\\n\");\n\t\treturn MBX_NOT_FINISHED;\n\t}\n\t/* Take the mailbox command service token */\n\tpsli->sli_flag |= LPFC_SLI_MBOX_ACTIVE;\n\n\t/* Get the next mailbox command from head of queue */\n\tmboxq = lpfc_mbox_get(phba);\n\n\t/* If no more mailbox command waiting for post, we're done */\n\tif (!mboxq) {\n\t\tpsli->sli_flag &= ~LPFC_SLI_MBOX_ACTIVE;\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t\treturn MBX_SUCCESS;\n\t}\n\tphba->sli.mbox_active = mboxq;\n\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\n\t/* Check device readiness for posting mailbox command */\n\trc = lpfc_mbox_dev_check(phba);\n\tif (unlikely(rc))\n\t\t/* Driver clean routine will clean up pending mailbox */\n\t\tgoto out_not_finished;\n\n\t/* Prepare the mbox command to be posted */\n\tmqe = &mboxq->u.mqe;\n\tmbx_cmnd = bf_get(lpfc_mqe_command, mqe);\n\n\t/* Start timer for the mbox_tmo and log some mailbox post messages */\n\tmod_timer(&psli->mbox_tmo, (jiffies +\n\t\t  msecs_to_jiffies(1000 * lpfc_mbox_tmo_val(phba, mboxq))));\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_MBOX | LOG_SLI,\n\t\t\t\"(%d):0355 Mailbox cmd x%x (x%x/x%x) issue Data: \"\n\t\t\t\"x%x x%x\\n\",\n\t\t\tmboxq->vport ? mboxq->vport->vpi : 0, mbx_cmnd,\n\t\t\tlpfc_sli_config_mbox_subsys_get(phba, mboxq),\n\t\t\tlpfc_sli_config_mbox_opcode_get(phba, mboxq),\n\t\t\tphba->pport->port_state, psli->sli_flag);\n\n\tif (mbx_cmnd != MBX_HEARTBEAT) {\n\t\tif (mboxq->vport) {\n\t\t\tlpfc_debugfs_disc_trc(mboxq->vport,\n\t\t\t\tLPFC_DISC_TRC_MBOX_VPORT,\n\t\t\t\t\"MBOX Send vport: cmd:x%x mb:x%x x%x\",\n\t\t\t\tmbx_cmnd, mqe->un.mb_words[0],\n\t\t\t\tmqe->un.mb_words[1]);\n\t\t} else {\n\t\t\tlpfc_debugfs_disc_trc(phba->pport,\n\t\t\t\tLPFC_DISC_TRC_MBOX,\n\t\t\t\t\"MBOX Send: cmd:x%x mb:x%x x%x\",\n\t\t\t\tmbx_cmnd, mqe->un.mb_words[0],\n\t\t\t\tmqe->un.mb_words[1]);\n\t\t}\n\t}\n\tpsli->slistat.mbox_cmd++;\n\n\t/* Post the mailbox command to the port */\n\trc = lpfc_sli4_mq_put(phba->sli4_hba.mbx_wq, mqe);\n\tif (rc != MBX_SUCCESS) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"(%d):2533 Mailbox command x%x (x%x/x%x) \"\n\t\t\t\t\"cannot issue Data: x%x x%x\\n\",\n\t\t\t\tmboxq->vport ? mboxq->vport->vpi : 0,\n\t\t\t\tmboxq->u.mb.mbxCommand,\n\t\t\t\tlpfc_sli_config_mbox_subsys_get(phba, mboxq),\n\t\t\t\tlpfc_sli_config_mbox_opcode_get(phba, mboxq),\n\t\t\t\tpsli->sli_flag, MBX_NOWAIT);\n\t\tgoto out_not_finished;\n\t}\n\n\treturn rc;\n\nout_not_finished:\n\tspin_lock_irqsave(&phba->hbalock, iflags);\n\tif (phba->sli.mbox_active) {\n\t\tmboxq->u.mb.mbxStatus = MBX_NOT_FINISHED;\n\t\t__lpfc_mbox_cmpl_put(phba, mboxq);\n\t\t/* Release the token */\n\t\tpsli->sli_flag &= ~LPFC_SLI_MBOX_ACTIVE;\n\t\tphba->sli.mbox_active = NULL;\n\t}\n\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\n\treturn MBX_NOT_FINISHED;\n}\n\n/**\n * lpfc_sli_issue_mbox - Wrapper func for issuing mailbox command\n * @phba: Pointer to HBA context object.\n * @pmbox: Pointer to mailbox object.\n * @flag: Flag indicating how the mailbox need to be processed.\n *\n * This routine wraps the actual SLI3 or SLI4 mailbox issuing routine from\n * the API jump table function pointer from the lpfc_hba struct.\n *\n * Return codes the caller owns the mailbox command after the return of the\n * function.\n **/\nint\nlpfc_sli_issue_mbox(struct lpfc_hba *phba, LPFC_MBOXQ_t *pmbox, uint32_t flag)\n{\n\treturn phba->lpfc_sli_issue_mbox(phba, pmbox, flag);\n}\n\n/**\n * lpfc_mbox_api_table_setup - Set up mbox api function jump table\n * @phba: The hba struct for which this call is being executed.\n * @dev_grp: The HBA PCI-Device group number.\n *\n * This routine sets up the mbox interface API function jump table in @phba\n * struct.\n * Returns: 0 - success, -ENODEV - failure.\n **/\nint\nlpfc_mbox_api_table_setup(struct lpfc_hba *phba, uint8_t dev_grp)\n{\n\n\tswitch (dev_grp) {\n\tcase LPFC_PCI_DEV_LP:\n\t\tphba->lpfc_sli_issue_mbox = lpfc_sli_issue_mbox_s3;\n\t\tphba->lpfc_sli_handle_slow_ring_event =\n\t\t\t\tlpfc_sli_handle_slow_ring_event_s3;\n\t\tphba->lpfc_sli_hbq_to_firmware = lpfc_sli_hbq_to_firmware_s3;\n\t\tphba->lpfc_sli_brdrestart = lpfc_sli_brdrestart_s3;\n\t\tphba->lpfc_sli_brdready = lpfc_sli_brdready_s3;\n\t\tbreak;\n\tcase LPFC_PCI_DEV_OC:\n\t\tphba->lpfc_sli_issue_mbox = lpfc_sli_issue_mbox_s4;\n\t\tphba->lpfc_sli_handle_slow_ring_event =\n\t\t\t\tlpfc_sli_handle_slow_ring_event_s4;\n\t\tphba->lpfc_sli_hbq_to_firmware = lpfc_sli_hbq_to_firmware_s4;\n\t\tphba->lpfc_sli_brdrestart = lpfc_sli_brdrestart_s4;\n\t\tphba->lpfc_sli_brdready = lpfc_sli_brdready_s4;\n\t\tbreak;\n\tdefault:\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"1420 Invalid HBA PCI-device group: 0x%x\\n\",\n\t\t\t\tdev_grp);\n\t\treturn -ENODEV;\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\n/**\n * __lpfc_sli_ringtx_put - Add an iocb to the txq\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n * @piocb: Pointer to address of newly added command iocb.\n *\n * This function is called with hbalock held for SLI3 ports or\n * the ring lock held for SLI4 ports to add a command\n * iocb to the txq when SLI layer cannot submit the command iocb\n * to the ring.\n **/\nvoid\n__lpfc_sli_ringtx_put(struct lpfc_hba *phba, struct lpfc_sli_ring *pring,\n\t\t    struct lpfc_iocbq *piocb)\n{\n\tif (phba->sli_rev == LPFC_SLI_REV4)\n\t\tlockdep_assert_held(&pring->ring_lock);\n\telse\n\t\tlockdep_assert_held(&phba->hbalock);\n\t/* Insert the caller's iocb in the txq tail for later processing. */\n\tlist_add_tail(&piocb->list, &pring->txq);\n}\n\n/**\n * lpfc_sli_next_iocb - Get the next iocb in the txq\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n * @piocb: Pointer to address of newly added command iocb.\n *\n * This function is called with hbalock held before a new\n * iocb is submitted to the firmware. This function checks\n * txq to flush the iocbs in txq to Firmware before\n * submitting new iocbs to the Firmware.\n * If there are iocbs in the txq which need to be submitted\n * to firmware, lpfc_sli_next_iocb returns the first element\n * of the txq after dequeuing it from txq.\n * If there is no iocb in the txq then the function will return\n * *piocb and *piocb is set to NULL. Caller needs to check\n * *piocb to find if there are more commands in the txq.\n **/\nstatic struct lpfc_iocbq *\nlpfc_sli_next_iocb(struct lpfc_hba *phba, struct lpfc_sli_ring *pring,\n\t\t   struct lpfc_iocbq **piocb)\n{\n\tstruct lpfc_iocbq * nextiocb;\n\n\tlockdep_assert_held(&phba->hbalock);\n\n\tnextiocb = lpfc_sli_ringtx_get(phba, pring);\n\tif (!nextiocb) {\n\t\tnextiocb = *piocb;\n\t\t*piocb = NULL;\n\t}\n\n\treturn nextiocb;\n}\n\n/**\n * __lpfc_sli_issue_iocb_s3 - SLI3 device lockless ver of lpfc_sli_issue_iocb\n * @phba: Pointer to HBA context object.\n * @ring_number: SLI ring number to issue iocb on.\n * @piocb: Pointer to command iocb.\n * @flag: Flag indicating if this command can be put into txq.\n *\n * __lpfc_sli_issue_iocb_s3 is used by other functions in the driver to issue\n * an iocb command to an HBA with SLI-3 interface spec. If the PCI slot is\n * recovering from error state, if HBA is resetting or if LPFC_STOP_IOCB_EVENT\n * flag is turned on, the function returns IOCB_ERROR. When the link is down,\n * this function allows only iocbs for posting buffers. This function finds\n * next available slot in the command ring and posts the command to the\n * available slot and writes the port attention register to request HBA start\n * processing new iocb. If there is no slot available in the ring and\n * flag & SLI_IOCB_RET_IOCB is set, the new iocb is added to the txq, otherwise\n * the function returns IOCB_BUSY.\n *\n * This function is called with hbalock held. The function will return success\n * after it successfully submit the iocb to firmware or after adding to the\n * txq.\n **/\nstatic int\n__lpfc_sli_issue_iocb_s3(struct lpfc_hba *phba, uint32_t ring_number,\n\t\t    struct lpfc_iocbq *piocb, uint32_t flag)\n{\n\tstruct lpfc_iocbq *nextiocb;\n\tIOCB_t *iocb;\n\tstruct lpfc_sli_ring *pring = &phba->sli.sli3_ring[ring_number];\n\n\tlockdep_assert_held(&phba->hbalock);\n\n\tif (piocb->iocb_cmpl && (!piocb->vport) &&\n\t   (piocb->iocb.ulpCommand != CMD_ABORT_XRI_CN) &&\n\t   (piocb->iocb.ulpCommand != CMD_CLOSE_XRI_CN)) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"1807 IOCB x%x failed. No vport\\n\",\n\t\t\t\tpiocb->iocb.ulpCommand);\n\t\tdump_stack();\n\t\treturn IOCB_ERROR;\n\t}\n\n\n\t/* If the PCI channel is in offline state, do not post iocbs. */\n\tif (unlikely(pci_channel_offline(phba->pcidev)))\n\t\treturn IOCB_ERROR;\n\n\t/* If HBA has a deferred error attention, fail the iocb. */\n\tif (unlikely(phba->hba_flag & DEFER_ERATT))\n\t\treturn IOCB_ERROR;\n\n\t/*\n\t * We should never get an IOCB if we are in a < LINK_DOWN state\n\t */\n\tif (unlikely(phba->link_state < LPFC_LINK_DOWN))\n\t\treturn IOCB_ERROR;\n\n\t/*\n\t * Check to see if we are blocking IOCB processing because of a\n\t * outstanding event.\n\t */\n\tif (unlikely(pring->flag & LPFC_STOP_IOCB_EVENT))\n\t\tgoto iocb_busy;\n\n\tif (unlikely(phba->link_state == LPFC_LINK_DOWN)) {\n\t\t/*\n\t\t * Only CREATE_XRI, CLOSE_XRI, and QUE_RING_BUF\n\t\t * can be issued if the link is not up.\n\t\t */\n\t\tswitch (piocb->iocb.ulpCommand) {\n\t\tcase CMD_GEN_REQUEST64_CR:\n\t\tcase CMD_GEN_REQUEST64_CX:\n\t\t\tif (!(phba->sli.sli_flag & LPFC_MENLO_MAINT) ||\n\t\t\t\t(piocb->iocb.un.genreq64.w5.hcsw.Rctl !=\n\t\t\t\t\tFC_RCTL_DD_UNSOL_CMD) ||\n\t\t\t\t(piocb->iocb.un.genreq64.w5.hcsw.Type !=\n\t\t\t\t\tMENLO_TRANSPORT_TYPE))\n\n\t\t\t\tgoto iocb_busy;\n\t\t\tbreak;\n\t\tcase CMD_QUE_RING_BUF_CN:\n\t\tcase CMD_QUE_RING_BUF64_CN:\n\t\t\t/*\n\t\t\t * For IOCBs, like QUE_RING_BUF, that have no rsp ring\n\t\t\t * completion, iocb_cmpl MUST be 0.\n\t\t\t */\n\t\t\tif (piocb->iocb_cmpl)\n\t\t\t\tpiocb->iocb_cmpl = NULL;\n\t\t\tfallthrough;\n\t\tcase CMD_CREATE_XRI_CR:\n\t\tcase CMD_CLOSE_XRI_CN:\n\t\tcase CMD_CLOSE_XRI_CX:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto iocb_busy;\n\t\t}\n\n\t/*\n\t * For FCP commands, we must be in a state where we can process link\n\t * attention events.\n\t */\n\t} else if (unlikely(pring->ringno == LPFC_FCP_RING &&\n\t\t\t    !(phba->sli.sli_flag & LPFC_PROCESS_LA))) {\n\t\tgoto iocb_busy;\n\t}\n\n\twhile ((iocb = lpfc_sli_next_iocb_slot(phba, pring)) &&\n\t       (nextiocb = lpfc_sli_next_iocb(phba, pring, &piocb)))\n\t\tlpfc_sli_submit_iocb(phba, pring, iocb, nextiocb);\n\n\tif (iocb)\n\t\tlpfc_sli_update_ring(phba, pring);\n\telse\n\t\tlpfc_sli_update_full_ring(phba, pring);\n\n\tif (!piocb)\n\t\treturn IOCB_SUCCESS;\n\n\tgoto out_busy;\n\n iocb_busy:\n\tpring->stats.iocb_cmd_delay++;\n\n out_busy:\n\n\tif (!(flag & SLI_IOCB_RET_IOCB)) {\n\t\t__lpfc_sli_ringtx_put(phba, pring, piocb);\n\t\treturn IOCB_SUCCESS;\n\t}\n\n\treturn IOCB_BUSY;\n}\n\n/**\n * lpfc_sli4_bpl2sgl - Convert the bpl/bde to a sgl.\n * @phba: Pointer to HBA context object.\n * @piocbq: Pointer to command iocb.\n * @sglq: Pointer to the scatter gather queue object.\n *\n * This routine converts the bpl or bde that is in the IOCB\n * to a sgl list for the sli4 hardware. The physical address\n * of the bpl/bde is converted back to a virtual address.\n * If the IOCB contains a BPL then the list of BDE's is\n * converted to sli4_sge's. If the IOCB contains a single\n * BDE then it is converted to a single sli_sge.\n * The IOCB is still in cpu endianess so the contents of\n * the bpl can be used without byte swapping.\n *\n * Returns valid XRI = Success, NO_XRI = Failure.\n**/\nstatic uint16_t\nlpfc_sli4_bpl2sgl(struct lpfc_hba *phba, struct lpfc_iocbq *piocbq,\n\t\tstruct lpfc_sglq *sglq)\n{\n\tuint16_t xritag = NO_XRI;\n\tstruct ulp_bde64 *bpl = NULL;\n\tstruct ulp_bde64 bde;\n\tstruct sli4_sge *sgl  = NULL;\n\tstruct lpfc_dmabuf *dmabuf;\n\tIOCB_t *icmd;\n\tint numBdes = 0;\n\tint i = 0;\n\tuint32_t offset = 0; /* accumulated offset in the sg request list */\n\tint inbound = 0; /* number of sg reply entries inbound from firmware */\n\n\tif (!piocbq || !sglq)\n\t\treturn xritag;\n\n\tsgl  = (struct sli4_sge *)sglq->sgl;\n\ticmd = &piocbq->iocb;\n\tif (icmd->ulpCommand == CMD_XMIT_BLS_RSP64_CX)\n\t\treturn sglq->sli4_xritag;\n\tif (icmd->un.genreq64.bdl.bdeFlags == BUFF_TYPE_BLP_64) {\n\t\tnumBdes = icmd->un.genreq64.bdl.bdeSize /\n\t\t\t\tsizeof(struct ulp_bde64);\n\t\t/* The addrHigh and addrLow fields within the IOCB\n\t\t * have not been byteswapped yet so there is no\n\t\t * need to swap them back.\n\t\t */\n\t\tif (piocbq->context3)\n\t\t\tdmabuf = (struct lpfc_dmabuf *)piocbq->context3;\n\t\telse\n\t\t\treturn xritag;\n\n\t\tbpl  = (struct ulp_bde64 *)dmabuf->virt;\n\t\tif (!bpl)\n\t\t\treturn xritag;\n\n\t\tfor (i = 0; i < numBdes; i++) {\n\t\t\t/* Should already be byte swapped. */\n\t\t\tsgl->addr_hi = bpl->addrHigh;\n\t\t\tsgl->addr_lo = bpl->addrLow;\n\n\t\t\tsgl->word2 = le32_to_cpu(sgl->word2);\n\t\t\tif ((i+1) == numBdes)\n\t\t\t\tbf_set(lpfc_sli4_sge_last, sgl, 1);\n\t\t\telse\n\t\t\t\tbf_set(lpfc_sli4_sge_last, sgl, 0);\n\t\t\t/* swap the size field back to the cpu so we\n\t\t\t * can assign it to the sgl.\n\t\t\t */\n\t\t\tbde.tus.w = le32_to_cpu(bpl->tus.w);\n\t\t\tsgl->sge_len = cpu_to_le32(bde.tus.f.bdeSize);\n\t\t\t/* The offsets in the sgl need to be accumulated\n\t\t\t * separately for the request and reply lists.\n\t\t\t * The request is always first, the reply follows.\n\t\t\t */\n\t\t\tif (piocbq->iocb.ulpCommand == CMD_GEN_REQUEST64_CR) {\n\t\t\t\t/* add up the reply sg entries */\n\t\t\t\tif (bpl->tus.f.bdeFlags == BUFF_TYPE_BDE_64I)\n\t\t\t\t\tinbound++;\n\t\t\t\t/* first inbound? reset the offset */\n\t\t\t\tif (inbound == 1)\n\t\t\t\t\toffset = 0;\n\t\t\t\tbf_set(lpfc_sli4_sge_offset, sgl, offset);\n\t\t\t\tbf_set(lpfc_sli4_sge_type, sgl,\n\t\t\t\t\tLPFC_SGE_TYPE_DATA);\n\t\t\t\toffset += bde.tus.f.bdeSize;\n\t\t\t}\n\t\t\tsgl->word2 = cpu_to_le32(sgl->word2);\n\t\t\tbpl++;\n\t\t\tsgl++;\n\t\t}\n\t} else if (icmd->un.genreq64.bdl.bdeFlags == BUFF_TYPE_BDE_64) {\n\t\t\t/* The addrHigh and addrLow fields of the BDE have not\n\t\t\t * been byteswapped yet so they need to be swapped\n\t\t\t * before putting them in the sgl.\n\t\t\t */\n\t\t\tsgl->addr_hi =\n\t\t\t\tcpu_to_le32(icmd->un.genreq64.bdl.addrHigh);\n\t\t\tsgl->addr_lo =\n\t\t\t\tcpu_to_le32(icmd->un.genreq64.bdl.addrLow);\n\t\t\tsgl->word2 = le32_to_cpu(sgl->word2);\n\t\t\tbf_set(lpfc_sli4_sge_last, sgl, 1);\n\t\t\tsgl->word2 = cpu_to_le32(sgl->word2);\n\t\t\tsgl->sge_len =\n\t\t\t\tcpu_to_le32(icmd->un.genreq64.bdl.bdeSize);\n\t}\n\treturn sglq->sli4_xritag;\n}\n\n/**\n * lpfc_sli_iocb2wqe - Convert the IOCB to a work queue entry.\n * @phba: Pointer to HBA context object.\n * @iocbq: Pointer to command iocb.\n * @wqe: Pointer to the work queue entry.\n *\n * This routine converts the iocb command to its Work Queue Entry\n * equivalent. The wqe pointer should not have any fields set when\n * this routine is called because it will memcpy over them.\n * This routine does not set the CQ_ID or the WQEC bits in the\n * wqe.\n *\n * Returns: 0 = Success, IOCB_ERROR = Failure.\n **/\nstatic int\nlpfc_sli4_iocb2wqe(struct lpfc_hba *phba, struct lpfc_iocbq *iocbq,\n\t\tunion lpfc_wqe128 *wqe)\n{\n\tuint32_t xmit_len = 0, total_len = 0;\n\tuint8_t ct = 0;\n\tuint32_t fip;\n\tuint32_t abort_tag;\n\tuint8_t command_type = ELS_COMMAND_NON_FIP;\n\tuint8_t cmnd;\n\tuint16_t xritag;\n\tuint16_t abrt_iotag;\n\tstruct lpfc_iocbq *abrtiocbq;\n\tstruct ulp_bde64 *bpl = NULL;\n\tuint32_t els_id = LPFC_ELS_ID_DEFAULT;\n\tint numBdes, i;\n\tstruct ulp_bde64 bde;\n\tstruct lpfc_nodelist *ndlp;\n\tuint32_t *pcmd;\n\tuint32_t if_type;\n\n\tfip = phba->hba_flag & HBA_FIP_SUPPORT;\n\t/* The fcp commands will set command type */\n\tif (iocbq->iocb_flag &  LPFC_IO_FCP)\n\t\tcommand_type = FCP_COMMAND;\n\telse if (fip && (iocbq->iocb_flag & LPFC_FIP_ELS_ID_MASK))\n\t\tcommand_type = ELS_COMMAND_FIP;\n\telse\n\t\tcommand_type = ELS_COMMAND_NON_FIP;\n\n\tif (phba->fcp_embed_io)\n\t\tmemset(wqe, 0, sizeof(union lpfc_wqe128));\n\t/* Some of the fields are in the right position already */\n\tmemcpy(wqe, &iocbq->iocb, sizeof(union lpfc_wqe));\n\t/* The ct field has moved so reset */\n\twqe->generic.wqe_com.word7 = 0;\n\twqe->generic.wqe_com.word10 = 0;\n\n\tabort_tag = (uint32_t) iocbq->iotag;\n\txritag = iocbq->sli4_xritag;\n\t/* words0-2 bpl convert bde */\n\tif (iocbq->iocb.un.genreq64.bdl.bdeFlags == BUFF_TYPE_BLP_64) {\n\t\tnumBdes = iocbq->iocb.un.genreq64.bdl.bdeSize /\n\t\t\t\tsizeof(struct ulp_bde64);\n\t\tbpl  = (struct ulp_bde64 *)\n\t\t\t((struct lpfc_dmabuf *)iocbq->context3)->virt;\n\t\tif (!bpl)\n\t\t\treturn IOCB_ERROR;\n\n\t\t/* Should already be byte swapped. */\n\t\twqe->generic.bde.addrHigh =  le32_to_cpu(bpl->addrHigh);\n\t\twqe->generic.bde.addrLow =  le32_to_cpu(bpl->addrLow);\n\t\t/* swap the size field back to the cpu so we\n\t\t * can assign it to the sgl.\n\t\t */\n\t\twqe->generic.bde.tus.w  = le32_to_cpu(bpl->tus.w);\n\t\txmit_len = wqe->generic.bde.tus.f.bdeSize;\n\t\ttotal_len = 0;\n\t\tfor (i = 0; i < numBdes; i++) {\n\t\t\tbde.tus.w  = le32_to_cpu(bpl[i].tus.w);\n\t\t\ttotal_len += bde.tus.f.bdeSize;\n\t\t}\n\t} else\n\t\txmit_len = iocbq->iocb.un.fcpi64.bdl.bdeSize;\n\n\tiocbq->iocb.ulpIoTag = iocbq->iotag;\n\tcmnd = iocbq->iocb.ulpCommand;\n\n\tswitch (iocbq->iocb.ulpCommand) {\n\tcase CMD_ELS_REQUEST64_CR:\n\t\tif (iocbq->iocb_flag & LPFC_IO_LIBDFC)\n\t\t\tndlp = iocbq->context_un.ndlp;\n\t\telse\n\t\t\tndlp = (struct lpfc_nodelist *)iocbq->context1;\n\t\tif (!iocbq->iocb.ulpLe) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2007 Only Limited Edition cmd Format\"\n\t\t\t\t\" supported 0x%x\\n\",\n\t\t\t\tiocbq->iocb.ulpCommand);\n\t\t\treturn IOCB_ERROR;\n\t\t}\n\n\t\twqe->els_req.payload_len = xmit_len;\n\t\t/* Els_reguest64 has a TMO */\n\t\tbf_set(wqe_tmo, &wqe->els_req.wqe_com,\n\t\t\tiocbq->iocb.ulpTimeout);\n\t\t/* Need a VF for word 4 set the vf bit*/\n\t\tbf_set(els_req64_vf, &wqe->els_req, 0);\n\t\t/* And a VFID for word 12 */\n\t\tbf_set(els_req64_vfid, &wqe->els_req, 0);\n\t\tct = ((iocbq->iocb.ulpCt_h << 1) | iocbq->iocb.ulpCt_l);\n\t\tbf_set(wqe_ctxt_tag, &wqe->els_req.wqe_com,\n\t\t       iocbq->iocb.ulpContext);\n\t\tbf_set(wqe_ct, &wqe->els_req.wqe_com, ct);\n\t\tbf_set(wqe_pu, &wqe->els_req.wqe_com, 0);\n\t\t/* CCP CCPE PV PRI in word10 were set in the memcpy */\n\t\tif (command_type == ELS_COMMAND_FIP)\n\t\t\tels_id = ((iocbq->iocb_flag & LPFC_FIP_ELS_ID_MASK)\n\t\t\t\t\t>> LPFC_FIP_ELS_ID_SHIFT);\n\t\tpcmd = (uint32_t *) (((struct lpfc_dmabuf *)\n\t\t\t\t\tiocbq->context2)->virt);\n\t\tif_type = bf_get(lpfc_sli_intf_if_type,\n\t\t\t\t\t&phba->sli4_hba.sli_intf);\n\t\tif (if_type >= LPFC_SLI_INTF_IF_TYPE_2) {\n\t\t\tif (pcmd && (*pcmd == ELS_CMD_FLOGI ||\n\t\t\t\t*pcmd == ELS_CMD_SCR ||\n\t\t\t\t*pcmd == ELS_CMD_RDF ||\n\t\t\t\t*pcmd == ELS_CMD_RSCN_XMT ||\n\t\t\t\t*pcmd == ELS_CMD_FDISC ||\n\t\t\t\t*pcmd == ELS_CMD_LOGO ||\n\t\t\t\t*pcmd == ELS_CMD_PLOGI)) {\n\t\t\t\tbf_set(els_req64_sp, &wqe->els_req, 1);\n\t\t\t\tbf_set(els_req64_sid, &wqe->els_req,\n\t\t\t\t\tiocbq->vport->fc_myDID);\n\t\t\t\tif ((*pcmd == ELS_CMD_FLOGI) &&\n\t\t\t\t\t!(phba->fc_topology ==\n\t\t\t\t\t\tLPFC_TOPOLOGY_LOOP))\n\t\t\t\t\tbf_set(els_req64_sid, &wqe->els_req, 0);\n\t\t\t\tbf_set(wqe_ct, &wqe->els_req.wqe_com, 1);\n\t\t\t\tbf_set(wqe_ctxt_tag, &wqe->els_req.wqe_com,\n\t\t\t\t\tphba->vpi_ids[iocbq->vport->vpi]);\n\t\t\t} else if (pcmd && iocbq->context1) {\n\t\t\t\tbf_set(wqe_ct, &wqe->els_req.wqe_com, 0);\n\t\t\t\tbf_set(wqe_ctxt_tag, &wqe->els_req.wqe_com,\n\t\t\t\t\tphba->sli4_hba.rpi_ids[ndlp->nlp_rpi]);\n\t\t\t}\n\t\t}\n\t\tbf_set(wqe_temp_rpi, &wqe->els_req.wqe_com,\n\t\t       phba->sli4_hba.rpi_ids[ndlp->nlp_rpi]);\n\t\tbf_set(wqe_els_id, &wqe->els_req.wqe_com, els_id);\n\t\tbf_set(wqe_dbde, &wqe->els_req.wqe_com, 1);\n\t\tbf_set(wqe_iod, &wqe->els_req.wqe_com, LPFC_WQE_IOD_READ);\n\t\tbf_set(wqe_qosd, &wqe->els_req.wqe_com, 1);\n\t\tbf_set(wqe_lenloc, &wqe->els_req.wqe_com, LPFC_WQE_LENLOC_NONE);\n\t\tbf_set(wqe_ebde_cnt, &wqe->els_req.wqe_com, 0);\n\t\twqe->els_req.max_response_payload_len = total_len - xmit_len;\n\t\tbreak;\n\tcase CMD_XMIT_SEQUENCE64_CX:\n\t\tbf_set(wqe_ctxt_tag, &wqe->xmit_sequence.wqe_com,\n\t\t       iocbq->iocb.un.ulpWord[3]);\n\t\tbf_set(wqe_rcvoxid, &wqe->xmit_sequence.wqe_com,\n\t\t       iocbq->iocb.unsli3.rcvsli3.ox_id);\n\t\t/* The entire sequence is transmitted for this IOCB */\n\t\txmit_len = total_len;\n\t\tcmnd = CMD_XMIT_SEQUENCE64_CR;\n\t\tif (phba->link_flag & LS_LOOPBACK_MODE)\n\t\t\tbf_set(wqe_xo, &wqe->xmit_sequence.wge_ctl, 1);\n\t\tfallthrough;\n\tcase CMD_XMIT_SEQUENCE64_CR:\n\t\t/* word3 iocb=io_tag32 wqe=reserved */\n\t\twqe->xmit_sequence.rsvd3 = 0;\n\t\t/* word4 relative_offset memcpy */\n\t\t/* word5 r_ctl/df_ctl memcpy */\n\t\tbf_set(wqe_pu, &wqe->xmit_sequence.wqe_com, 0);\n\t\tbf_set(wqe_dbde, &wqe->xmit_sequence.wqe_com, 1);\n\t\tbf_set(wqe_iod, &wqe->xmit_sequence.wqe_com,\n\t\t       LPFC_WQE_IOD_WRITE);\n\t\tbf_set(wqe_lenloc, &wqe->xmit_sequence.wqe_com,\n\t\t       LPFC_WQE_LENLOC_WORD12);\n\t\tbf_set(wqe_ebde_cnt, &wqe->xmit_sequence.wqe_com, 0);\n\t\twqe->xmit_sequence.xmit_len = xmit_len;\n\t\tcommand_type = OTHER_COMMAND;\n\t\tbreak;\n\tcase CMD_XMIT_BCAST64_CN:\n\t\t/* word3 iocb=iotag32 wqe=seq_payload_len */\n\t\twqe->xmit_bcast64.seq_payload_len = xmit_len;\n\t\t/* word4 iocb=rsvd wqe=rsvd */\n\t\t/* word5 iocb=rctl/type/df_ctl wqe=rctl/type/df_ctl memcpy */\n\t\t/* word6 iocb=ctxt_tag/io_tag wqe=ctxt_tag/xri */\n\t\tbf_set(wqe_ct, &wqe->xmit_bcast64.wqe_com,\n\t\t\t((iocbq->iocb.ulpCt_h << 1) | iocbq->iocb.ulpCt_l));\n\t\tbf_set(wqe_dbde, &wqe->xmit_bcast64.wqe_com, 1);\n\t\tbf_set(wqe_iod, &wqe->xmit_bcast64.wqe_com, LPFC_WQE_IOD_WRITE);\n\t\tbf_set(wqe_lenloc, &wqe->xmit_bcast64.wqe_com,\n\t\t       LPFC_WQE_LENLOC_WORD3);\n\t\tbf_set(wqe_ebde_cnt, &wqe->xmit_bcast64.wqe_com, 0);\n\t\tbreak;\n\tcase CMD_FCP_IWRITE64_CR:\n\t\tcommand_type = FCP_COMMAND_DATA_OUT;\n\t\t/* word3 iocb=iotag wqe=payload_offset_len */\n\t\t/* Add the FCP_CMD and FCP_RSP sizes to get the offset */\n\t\tbf_set(payload_offset_len, &wqe->fcp_iwrite,\n\t\t       xmit_len + sizeof(struct fcp_rsp));\n\t\tbf_set(cmd_buff_len, &wqe->fcp_iwrite,\n\t\t       0);\n\t\t/* word4 iocb=parameter wqe=total_xfer_length memcpy */\n\t\t/* word5 iocb=initial_xfer_len wqe=initial_xfer_len memcpy */\n\t\tbf_set(wqe_erp, &wqe->fcp_iwrite.wqe_com,\n\t\t       iocbq->iocb.ulpFCP2Rcvy);\n\t\tbf_set(wqe_lnk, &wqe->fcp_iwrite.wqe_com, iocbq->iocb.ulpXS);\n\t\t/* Always open the exchange */\n\t\tbf_set(wqe_iod, &wqe->fcp_iwrite.wqe_com, LPFC_WQE_IOD_WRITE);\n\t\tbf_set(wqe_lenloc, &wqe->fcp_iwrite.wqe_com,\n\t\t       LPFC_WQE_LENLOC_WORD4);\n\t\tbf_set(wqe_pu, &wqe->fcp_iwrite.wqe_com, iocbq->iocb.ulpPU);\n\t\tbf_set(wqe_dbde, &wqe->fcp_iwrite.wqe_com, 1);\n\t\tif (iocbq->iocb_flag & LPFC_IO_OAS) {\n\t\t\tbf_set(wqe_oas, &wqe->fcp_iwrite.wqe_com, 1);\n\t\t\tbf_set(wqe_ccpe, &wqe->fcp_iwrite.wqe_com, 1);\n\t\t\tif (iocbq->priority) {\n\t\t\t\tbf_set(wqe_ccp, &wqe->fcp_iwrite.wqe_com,\n\t\t\t\t       (iocbq->priority << 1));\n\t\t\t} else {\n\t\t\t\tbf_set(wqe_ccp, &wqe->fcp_iwrite.wqe_com,\n\t\t\t\t       (phba->cfg_XLanePriority << 1));\n\t\t\t}\n\t\t}\n\t\t/* Note, word 10 is already initialized to 0 */\n\n\t\t/* Don't set PBDE for Perf hints, just lpfc_enable_pbde */\n\t\tif (phba->cfg_enable_pbde)\n\t\t\tbf_set(wqe_pbde, &wqe->fcp_iwrite.wqe_com, 1);\n\t\telse\n\t\t\tbf_set(wqe_pbde, &wqe->fcp_iwrite.wqe_com, 0);\n\n\t\tif (phba->fcp_embed_io) {\n\t\t\tstruct lpfc_io_buf *lpfc_cmd;\n\t\t\tstruct sli4_sge *sgl;\n\t\t\tstruct fcp_cmnd *fcp_cmnd;\n\t\t\tuint32_t *ptr;\n\n\t\t\t/* 128 byte wqe support here */\n\n\t\t\tlpfc_cmd = iocbq->context1;\n\t\t\tsgl = (struct sli4_sge *)lpfc_cmd->dma_sgl;\n\t\t\tfcp_cmnd = lpfc_cmd->fcp_cmnd;\n\n\t\t\t/* Word 0-2 - FCP_CMND */\n\t\t\twqe->generic.bde.tus.f.bdeFlags =\n\t\t\t\tBUFF_TYPE_BDE_IMMED;\n\t\t\twqe->generic.bde.tus.f.bdeSize = sgl->sge_len;\n\t\t\twqe->generic.bde.addrHigh = 0;\n\t\t\twqe->generic.bde.addrLow =  88;  /* Word 22 */\n\n\t\t\tbf_set(wqe_wqes, &wqe->fcp_iwrite.wqe_com, 1);\n\t\t\tbf_set(wqe_dbde, &wqe->fcp_iwrite.wqe_com, 0);\n\n\t\t\t/* Word 22-29  FCP CMND Payload */\n\t\t\tptr = &wqe->words[22];\n\t\t\tmemcpy(ptr, fcp_cmnd, sizeof(struct fcp_cmnd));\n\t\t}\n\t\tbreak;\n\tcase CMD_FCP_IREAD64_CR:\n\t\t/* word3 iocb=iotag wqe=payload_offset_len */\n\t\t/* Add the FCP_CMD and FCP_RSP sizes to get the offset */\n\t\tbf_set(payload_offset_len, &wqe->fcp_iread,\n\t\t       xmit_len + sizeof(struct fcp_rsp));\n\t\tbf_set(cmd_buff_len, &wqe->fcp_iread,\n\t\t       0);\n\t\t/* word4 iocb=parameter wqe=total_xfer_length memcpy */\n\t\t/* word5 iocb=initial_xfer_len wqe=initial_xfer_len memcpy */\n\t\tbf_set(wqe_erp, &wqe->fcp_iread.wqe_com,\n\t\t       iocbq->iocb.ulpFCP2Rcvy);\n\t\tbf_set(wqe_lnk, &wqe->fcp_iread.wqe_com, iocbq->iocb.ulpXS);\n\t\t/* Always open the exchange */\n\t\tbf_set(wqe_iod, &wqe->fcp_iread.wqe_com, LPFC_WQE_IOD_READ);\n\t\tbf_set(wqe_lenloc, &wqe->fcp_iread.wqe_com,\n\t\t       LPFC_WQE_LENLOC_WORD4);\n\t\tbf_set(wqe_pu, &wqe->fcp_iread.wqe_com, iocbq->iocb.ulpPU);\n\t\tbf_set(wqe_dbde, &wqe->fcp_iread.wqe_com, 1);\n\t\tif (iocbq->iocb_flag & LPFC_IO_OAS) {\n\t\t\tbf_set(wqe_oas, &wqe->fcp_iread.wqe_com, 1);\n\t\t\tbf_set(wqe_ccpe, &wqe->fcp_iread.wqe_com, 1);\n\t\t\tif (iocbq->priority) {\n\t\t\t\tbf_set(wqe_ccp, &wqe->fcp_iread.wqe_com,\n\t\t\t\t       (iocbq->priority << 1));\n\t\t\t} else {\n\t\t\t\tbf_set(wqe_ccp, &wqe->fcp_iread.wqe_com,\n\t\t\t\t       (phba->cfg_XLanePriority << 1));\n\t\t\t}\n\t\t}\n\t\t/* Note, word 10 is already initialized to 0 */\n\n\t\t/* Don't set PBDE for Perf hints, just lpfc_enable_pbde */\n\t\tif (phba->cfg_enable_pbde)\n\t\t\tbf_set(wqe_pbde, &wqe->fcp_iread.wqe_com, 1);\n\t\telse\n\t\t\tbf_set(wqe_pbde, &wqe->fcp_iread.wqe_com, 0);\n\n\t\tif (phba->fcp_embed_io) {\n\t\t\tstruct lpfc_io_buf *lpfc_cmd;\n\t\t\tstruct sli4_sge *sgl;\n\t\t\tstruct fcp_cmnd *fcp_cmnd;\n\t\t\tuint32_t *ptr;\n\n\t\t\t/* 128 byte wqe support here */\n\n\t\t\tlpfc_cmd = iocbq->context1;\n\t\t\tsgl = (struct sli4_sge *)lpfc_cmd->dma_sgl;\n\t\t\tfcp_cmnd = lpfc_cmd->fcp_cmnd;\n\n\t\t\t/* Word 0-2 - FCP_CMND */\n\t\t\twqe->generic.bde.tus.f.bdeFlags =\n\t\t\t\tBUFF_TYPE_BDE_IMMED;\n\t\t\twqe->generic.bde.tus.f.bdeSize = sgl->sge_len;\n\t\t\twqe->generic.bde.addrHigh = 0;\n\t\t\twqe->generic.bde.addrLow =  88;  /* Word 22 */\n\n\t\t\tbf_set(wqe_wqes, &wqe->fcp_iread.wqe_com, 1);\n\t\t\tbf_set(wqe_dbde, &wqe->fcp_iread.wqe_com, 0);\n\n\t\t\t/* Word 22-29  FCP CMND Payload */\n\t\t\tptr = &wqe->words[22];\n\t\t\tmemcpy(ptr, fcp_cmnd, sizeof(struct fcp_cmnd));\n\t\t}\n\t\tbreak;\n\tcase CMD_FCP_ICMND64_CR:\n\t\t/* word3 iocb=iotag wqe=payload_offset_len */\n\t\t/* Add the FCP_CMD and FCP_RSP sizes to get the offset */\n\t\tbf_set(payload_offset_len, &wqe->fcp_icmd,\n\t\t       xmit_len + sizeof(struct fcp_rsp));\n\t\tbf_set(cmd_buff_len, &wqe->fcp_icmd,\n\t\t       0);\n\t\t/* word3 iocb=IO_TAG wqe=reserved */\n\t\tbf_set(wqe_pu, &wqe->fcp_icmd.wqe_com, 0);\n\t\t/* Always open the exchange */\n\t\tbf_set(wqe_dbde, &wqe->fcp_icmd.wqe_com, 1);\n\t\tbf_set(wqe_iod, &wqe->fcp_icmd.wqe_com, LPFC_WQE_IOD_WRITE);\n\t\tbf_set(wqe_qosd, &wqe->fcp_icmd.wqe_com, 1);\n\t\tbf_set(wqe_lenloc, &wqe->fcp_icmd.wqe_com,\n\t\t       LPFC_WQE_LENLOC_NONE);\n\t\tbf_set(wqe_erp, &wqe->fcp_icmd.wqe_com,\n\t\t       iocbq->iocb.ulpFCP2Rcvy);\n\t\tif (iocbq->iocb_flag & LPFC_IO_OAS) {\n\t\t\tbf_set(wqe_oas, &wqe->fcp_icmd.wqe_com, 1);\n\t\t\tbf_set(wqe_ccpe, &wqe->fcp_icmd.wqe_com, 1);\n\t\t\tif (iocbq->priority) {\n\t\t\t\tbf_set(wqe_ccp, &wqe->fcp_icmd.wqe_com,\n\t\t\t\t       (iocbq->priority << 1));\n\t\t\t} else {\n\t\t\t\tbf_set(wqe_ccp, &wqe->fcp_icmd.wqe_com,\n\t\t\t\t       (phba->cfg_XLanePriority << 1));\n\t\t\t}\n\t\t}\n\t\t/* Note, word 10 is already initialized to 0 */\n\n\t\tif (phba->fcp_embed_io) {\n\t\t\tstruct lpfc_io_buf *lpfc_cmd;\n\t\t\tstruct sli4_sge *sgl;\n\t\t\tstruct fcp_cmnd *fcp_cmnd;\n\t\t\tuint32_t *ptr;\n\n\t\t\t/* 128 byte wqe support here */\n\n\t\t\tlpfc_cmd = iocbq->context1;\n\t\t\tsgl = (struct sli4_sge *)lpfc_cmd->dma_sgl;\n\t\t\tfcp_cmnd = lpfc_cmd->fcp_cmnd;\n\n\t\t\t/* Word 0-2 - FCP_CMND */\n\t\t\twqe->generic.bde.tus.f.bdeFlags =\n\t\t\t\tBUFF_TYPE_BDE_IMMED;\n\t\t\twqe->generic.bde.tus.f.bdeSize = sgl->sge_len;\n\t\t\twqe->generic.bde.addrHigh = 0;\n\t\t\twqe->generic.bde.addrLow =  88;  /* Word 22 */\n\n\t\t\tbf_set(wqe_wqes, &wqe->fcp_icmd.wqe_com, 1);\n\t\t\tbf_set(wqe_dbde, &wqe->fcp_icmd.wqe_com, 0);\n\n\t\t\t/* Word 22-29  FCP CMND Payload */\n\t\t\tptr = &wqe->words[22];\n\t\t\tmemcpy(ptr, fcp_cmnd, sizeof(struct fcp_cmnd));\n\t\t}\n\t\tbreak;\n\tcase CMD_GEN_REQUEST64_CR:\n\t\t/* For this command calculate the xmit length of the\n\t\t * request bde.\n\t\t */\n\t\txmit_len = 0;\n\t\tnumBdes = iocbq->iocb.un.genreq64.bdl.bdeSize /\n\t\t\tsizeof(struct ulp_bde64);\n\t\tfor (i = 0; i < numBdes; i++) {\n\t\t\tbde.tus.w = le32_to_cpu(bpl[i].tus.w);\n\t\t\tif (bde.tus.f.bdeFlags != BUFF_TYPE_BDE_64)\n\t\t\t\tbreak;\n\t\t\txmit_len += bde.tus.f.bdeSize;\n\t\t}\n\t\t/* word3 iocb=IO_TAG wqe=request_payload_len */\n\t\twqe->gen_req.request_payload_len = xmit_len;\n\t\t/* word4 iocb=parameter wqe=relative_offset memcpy */\n\t\t/* word5 [rctl, type, df_ctl, la] copied in memcpy */\n\t\t/* word6 context tag copied in memcpy */\n\t\tif (iocbq->iocb.ulpCt_h  || iocbq->iocb.ulpCt_l) {\n\t\t\tct = ((iocbq->iocb.ulpCt_h << 1) | iocbq->iocb.ulpCt_l);\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2015 Invalid CT %x command 0x%x\\n\",\n\t\t\t\tct, iocbq->iocb.ulpCommand);\n\t\t\treturn IOCB_ERROR;\n\t\t}\n\t\tbf_set(wqe_ct, &wqe->gen_req.wqe_com, 0);\n\t\tbf_set(wqe_tmo, &wqe->gen_req.wqe_com, iocbq->iocb.ulpTimeout);\n\t\tbf_set(wqe_pu, &wqe->gen_req.wqe_com, iocbq->iocb.ulpPU);\n\t\tbf_set(wqe_dbde, &wqe->gen_req.wqe_com, 1);\n\t\tbf_set(wqe_iod, &wqe->gen_req.wqe_com, LPFC_WQE_IOD_READ);\n\t\tbf_set(wqe_qosd, &wqe->gen_req.wqe_com, 1);\n\t\tbf_set(wqe_lenloc, &wqe->gen_req.wqe_com, LPFC_WQE_LENLOC_NONE);\n\t\tbf_set(wqe_ebde_cnt, &wqe->gen_req.wqe_com, 0);\n\t\twqe->gen_req.max_response_payload_len = total_len - xmit_len;\n\t\tcommand_type = OTHER_COMMAND;\n\t\tbreak;\n\tcase CMD_XMIT_ELS_RSP64_CX:\n\t\tndlp = (struct lpfc_nodelist *)iocbq->context1;\n\t\t/* words0-2 BDE memcpy */\n\t\t/* word3 iocb=iotag32 wqe=response_payload_len */\n\t\twqe->xmit_els_rsp.response_payload_len = xmit_len;\n\t\t/* word4 */\n\t\twqe->xmit_els_rsp.word4 = 0;\n\t\t/* word5 iocb=rsvd wge=did */\n\t\tbf_set(wqe_els_did, &wqe->xmit_els_rsp.wqe_dest,\n\t\t\t iocbq->iocb.un.xseq64.xmit_els_remoteID);\n\n\t\tif_type = bf_get(lpfc_sli_intf_if_type,\n\t\t\t\t\t&phba->sli4_hba.sli_intf);\n\t\tif (if_type >= LPFC_SLI_INTF_IF_TYPE_2) {\n\t\t\tif (iocbq->vport->fc_flag & FC_PT2PT) {\n\t\t\t\tbf_set(els_rsp64_sp, &wqe->xmit_els_rsp, 1);\n\t\t\t\tbf_set(els_rsp64_sid, &wqe->xmit_els_rsp,\n\t\t\t\t\tiocbq->vport->fc_myDID);\n\t\t\t\tif (iocbq->vport->fc_myDID == Fabric_DID) {\n\t\t\t\t\tbf_set(wqe_els_did,\n\t\t\t\t\t\t&wqe->xmit_els_rsp.wqe_dest, 0);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tbf_set(wqe_ct, &wqe->xmit_els_rsp.wqe_com,\n\t\t       ((iocbq->iocb.ulpCt_h << 1) | iocbq->iocb.ulpCt_l));\n\t\tbf_set(wqe_pu, &wqe->xmit_els_rsp.wqe_com, iocbq->iocb.ulpPU);\n\t\tbf_set(wqe_rcvoxid, &wqe->xmit_els_rsp.wqe_com,\n\t\t       iocbq->iocb.unsli3.rcvsli3.ox_id);\n\t\tif (!iocbq->iocb.ulpCt_h && iocbq->iocb.ulpCt_l)\n\t\t\tbf_set(wqe_ctxt_tag, &wqe->xmit_els_rsp.wqe_com,\n\t\t\t       phba->vpi_ids[iocbq->vport->vpi]);\n\t\tbf_set(wqe_dbde, &wqe->xmit_els_rsp.wqe_com, 1);\n\t\tbf_set(wqe_iod, &wqe->xmit_els_rsp.wqe_com, LPFC_WQE_IOD_WRITE);\n\t\tbf_set(wqe_qosd, &wqe->xmit_els_rsp.wqe_com, 1);\n\t\tbf_set(wqe_lenloc, &wqe->xmit_els_rsp.wqe_com,\n\t\t       LPFC_WQE_LENLOC_WORD3);\n\t\tbf_set(wqe_ebde_cnt, &wqe->xmit_els_rsp.wqe_com, 0);\n\t\tbf_set(wqe_rsp_temp_rpi, &wqe->xmit_els_rsp,\n\t\t       phba->sli4_hba.rpi_ids[ndlp->nlp_rpi]);\n\t\tpcmd = (uint32_t *) (((struct lpfc_dmabuf *)\n\t\t\t\t\tiocbq->context2)->virt);\n\t\tif (phba->fc_topology == LPFC_TOPOLOGY_LOOP) {\n\t\t\t\tbf_set(els_rsp64_sp, &wqe->xmit_els_rsp, 1);\n\t\t\t\tbf_set(els_rsp64_sid, &wqe->xmit_els_rsp,\n\t\t\t\t\tiocbq->vport->fc_myDID);\n\t\t\t\tbf_set(wqe_ct, &wqe->xmit_els_rsp.wqe_com, 1);\n\t\t\t\tbf_set(wqe_ctxt_tag, &wqe->xmit_els_rsp.wqe_com,\n\t\t\t\t\tphba->vpi_ids[phba->pport->vpi]);\n\t\t}\n\t\tcommand_type = OTHER_COMMAND;\n\t\tbreak;\n\tcase CMD_CLOSE_XRI_CN:\n\tcase CMD_ABORT_XRI_CN:\n\tcase CMD_ABORT_XRI_CX:\n\t\t/* words 0-2 memcpy should be 0 rserved */\n\t\t/* port will send abts */\n\t\tabrt_iotag = iocbq->iocb.un.acxri.abortContextTag;\n\t\tif (abrt_iotag != 0 && abrt_iotag <= phba->sli.last_iotag) {\n\t\t\tabrtiocbq = phba->sli.iocbq_lookup[abrt_iotag];\n\t\t\tfip = abrtiocbq->iocb_flag & LPFC_FIP_ELS_ID_MASK;\n\t\t} else\n\t\t\tfip = 0;\n\n\t\tif ((iocbq->iocb.ulpCommand == CMD_CLOSE_XRI_CN) || fip)\n\t\t\t/*\n\t\t\t * The link is down, or the command was ELS_FIP\n\t\t\t * so the fw does not need to send abts\n\t\t\t * on the wire.\n\t\t\t */\n\t\t\tbf_set(abort_cmd_ia, &wqe->abort_cmd, 1);\n\t\telse\n\t\t\tbf_set(abort_cmd_ia, &wqe->abort_cmd, 0);\n\t\tbf_set(abort_cmd_criteria, &wqe->abort_cmd, T_XRI_TAG);\n\t\t/* word5 iocb=CONTEXT_TAG|IO_TAG wqe=reserved */\n\t\twqe->abort_cmd.rsrvd5 = 0;\n\t\tbf_set(wqe_ct, &wqe->abort_cmd.wqe_com,\n\t\t\t((iocbq->iocb.ulpCt_h << 1) | iocbq->iocb.ulpCt_l));\n\t\tabort_tag = iocbq->iocb.un.acxri.abortIoTag;\n\t\t/*\n\t\t * The abort handler will send us CMD_ABORT_XRI_CN or\n\t\t * CMD_CLOSE_XRI_CN and the fw only accepts CMD_ABORT_XRI_CX\n\t\t */\n\t\tbf_set(wqe_cmnd, &wqe->abort_cmd.wqe_com, CMD_ABORT_XRI_CX);\n\t\tbf_set(wqe_qosd, &wqe->abort_cmd.wqe_com, 1);\n\t\tbf_set(wqe_lenloc, &wqe->abort_cmd.wqe_com,\n\t\t       LPFC_WQE_LENLOC_NONE);\n\t\tcmnd = CMD_ABORT_XRI_CX;\n\t\tcommand_type = OTHER_COMMAND;\n\t\txritag = 0;\n\t\tbreak;\n\tcase CMD_XMIT_BLS_RSP64_CX:\n\t\tndlp = (struct lpfc_nodelist *)iocbq->context1;\n\t\t/* As BLS ABTS RSP WQE is very different from other WQEs,\n\t\t * we re-construct this WQE here based on information in\n\t\t * iocbq from scratch.\n\t\t */\n\t\tmemset(wqe, 0, sizeof(*wqe));\n\t\t/* OX_ID is invariable to who sent ABTS to CT exchange */\n\t\tbf_set(xmit_bls_rsp64_oxid, &wqe->xmit_bls_rsp,\n\t\t       bf_get(lpfc_abts_oxid, &iocbq->iocb.un.bls_rsp));\n\t\tif (bf_get(lpfc_abts_orig, &iocbq->iocb.un.bls_rsp) ==\n\t\t    LPFC_ABTS_UNSOL_INT) {\n\t\t\t/* ABTS sent by initiator to CT exchange, the\n\t\t\t * RX_ID field will be filled with the newly\n\t\t\t * allocated responder XRI.\n\t\t\t */\n\t\t\tbf_set(xmit_bls_rsp64_rxid, &wqe->xmit_bls_rsp,\n\t\t\t       iocbq->sli4_xritag);\n\t\t} else {\n\t\t\t/* ABTS sent by responder to CT exchange, the\n\t\t\t * RX_ID field will be filled with the responder\n\t\t\t * RX_ID from ABTS.\n\t\t\t */\n\t\t\tbf_set(xmit_bls_rsp64_rxid, &wqe->xmit_bls_rsp,\n\t\t\t       bf_get(lpfc_abts_rxid, &iocbq->iocb.un.bls_rsp));\n\t\t}\n\t\tbf_set(xmit_bls_rsp64_seqcnthi, &wqe->xmit_bls_rsp, 0xffff);\n\t\tbf_set(wqe_xmit_bls_pt, &wqe->xmit_bls_rsp.wqe_dest, 0x1);\n\n\t\t/* Use CT=VPI */\n\t\tbf_set(wqe_els_did, &wqe->xmit_bls_rsp.wqe_dest,\n\t\t\tndlp->nlp_DID);\n\t\tbf_set(xmit_bls_rsp64_temprpi, &wqe->xmit_bls_rsp,\n\t\t\tiocbq->iocb.ulpContext);\n\t\tbf_set(wqe_ct, &wqe->xmit_bls_rsp.wqe_com, 1);\n\t\tbf_set(wqe_ctxt_tag, &wqe->xmit_bls_rsp.wqe_com,\n\t\t\tphba->vpi_ids[phba->pport->vpi]);\n\t\tbf_set(wqe_qosd, &wqe->xmit_bls_rsp.wqe_com, 1);\n\t\tbf_set(wqe_lenloc, &wqe->xmit_bls_rsp.wqe_com,\n\t\t       LPFC_WQE_LENLOC_NONE);\n\t\t/* Overwrite the pre-set comnd type with OTHER_COMMAND */\n\t\tcommand_type = OTHER_COMMAND;\n\t\tif (iocbq->iocb.un.xseq64.w5.hcsw.Rctl == FC_RCTL_BA_RJT) {\n\t\t\tbf_set(xmit_bls_rsp64_rjt_vspec, &wqe->xmit_bls_rsp,\n\t\t\t       bf_get(lpfc_vndr_code, &iocbq->iocb.un.bls_rsp));\n\t\t\tbf_set(xmit_bls_rsp64_rjt_expc, &wqe->xmit_bls_rsp,\n\t\t\t       bf_get(lpfc_rsn_expln, &iocbq->iocb.un.bls_rsp));\n\t\t\tbf_set(xmit_bls_rsp64_rjt_rsnc, &wqe->xmit_bls_rsp,\n\t\t\t       bf_get(lpfc_rsn_code, &iocbq->iocb.un.bls_rsp));\n\t\t}\n\n\t\tbreak;\n\tcase CMD_SEND_FRAME:\n\t\tbf_set(wqe_cmnd, &wqe->generic.wqe_com, CMD_SEND_FRAME);\n\t\tbf_set(wqe_sof, &wqe->generic.wqe_com, 0x2E); /* SOF byte */\n\t\tbf_set(wqe_eof, &wqe->generic.wqe_com, 0x41); /* EOF byte */\n\t\tbf_set(wqe_lenloc, &wqe->generic.wqe_com, 1);\n\t\tbf_set(wqe_xbl, &wqe->generic.wqe_com, 1);\n\t\tbf_set(wqe_dbde, &wqe->generic.wqe_com, 1);\n\t\tbf_set(wqe_xc, &wqe->generic.wqe_com, 1);\n\t\tbf_set(wqe_cmd_type, &wqe->generic.wqe_com, 0xA);\n\t\tbf_set(wqe_cqid, &wqe->generic.wqe_com, LPFC_WQE_CQ_ID_DEFAULT);\n\t\tbf_set(wqe_xri_tag, &wqe->generic.wqe_com, xritag);\n\t\tbf_set(wqe_reqtag, &wqe->generic.wqe_com, iocbq->iotag);\n\t\treturn 0;\n\tcase CMD_XRI_ABORTED_CX:\n\tcase CMD_CREATE_XRI_CR: /* Do we expect to use this? */\n\tcase CMD_IOCB_FCP_IBIDIR64_CR: /* bidirectional xfer */\n\tcase CMD_FCP_TSEND64_CX: /* Target mode send xfer-ready */\n\tcase CMD_FCP_TRSP64_CX: /* Target mode rcv */\n\tcase CMD_FCP_AUTO_TRSP_CX: /* Auto target rsp */\n\tdefault:\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2014 Invalid command 0x%x\\n\",\n\t\t\t\tiocbq->iocb.ulpCommand);\n\t\treturn IOCB_ERROR;\n\t\tbreak;\n\t}\n\n\tif (iocbq->iocb_flag & LPFC_IO_DIF_PASS)\n\t\tbf_set(wqe_dif, &wqe->generic.wqe_com, LPFC_WQE_DIF_PASSTHRU);\n\telse if (iocbq->iocb_flag & LPFC_IO_DIF_STRIP)\n\t\tbf_set(wqe_dif, &wqe->generic.wqe_com, LPFC_WQE_DIF_STRIP);\n\telse if (iocbq->iocb_flag & LPFC_IO_DIF_INSERT)\n\t\tbf_set(wqe_dif, &wqe->generic.wqe_com, LPFC_WQE_DIF_INSERT);\n\tiocbq->iocb_flag &= ~(LPFC_IO_DIF_PASS | LPFC_IO_DIF_STRIP |\n\t\t\t      LPFC_IO_DIF_INSERT);\n\tbf_set(wqe_xri_tag, &wqe->generic.wqe_com, xritag);\n\tbf_set(wqe_reqtag, &wqe->generic.wqe_com, iocbq->iotag);\n\twqe->generic.wqe_com.abort_tag = abort_tag;\n\tbf_set(wqe_cmd_type, &wqe->generic.wqe_com, command_type);\n\tbf_set(wqe_cmnd, &wqe->generic.wqe_com, cmnd);\n\tbf_set(wqe_class, &wqe->generic.wqe_com, iocbq->iocb.ulpClass);\n\tbf_set(wqe_cqid, &wqe->generic.wqe_com, LPFC_WQE_CQ_ID_DEFAULT);\n\treturn 0;\n}\n\n/**\n * __lpfc_sli_issue_iocb_s4 - SLI4 device lockless ver of lpfc_sli_issue_iocb\n * @phba: Pointer to HBA context object.\n * @ring_number: SLI ring number to issue iocb on.\n * @piocb: Pointer to command iocb.\n * @flag: Flag indicating if this command can be put into txq.\n *\n * __lpfc_sli_issue_iocb_s4 is used by other functions in the driver to issue\n * an iocb command to an HBA with SLI-4 interface spec.\n *\n * This function is called with ringlock held. The function will return success\n * after it successfully submit the iocb to firmware or after adding to the\n * txq.\n **/\nstatic int\n__lpfc_sli_issue_iocb_s4(struct lpfc_hba *phba, uint32_t ring_number,\n\t\t\t struct lpfc_iocbq *piocb, uint32_t flag)\n{\n\tstruct lpfc_sglq *sglq;\n\tunion lpfc_wqe128 wqe;\n\tstruct lpfc_queue *wq;\n\tstruct lpfc_sli_ring *pring;\n\n\t/* Get the WQ */\n\tif ((piocb->iocb_flag & LPFC_IO_FCP) ||\n\t    (piocb->iocb_flag & LPFC_USE_FCPWQIDX)) {\n\t\twq = phba->sli4_hba.hdwq[piocb->hba_wqidx].io_wq;\n\t} else {\n\t\twq = phba->sli4_hba.els_wq;\n\t}\n\n\t/* Get corresponding ring */\n\tpring = wq->pring;\n\n\t/*\n\t * The WQE can be either 64 or 128 bytes,\n\t */\n\n\tlockdep_assert_held(&pring->ring_lock);\n\n\tif (piocb->sli4_xritag == NO_XRI) {\n\t\tif (piocb->iocb.ulpCommand == CMD_ABORT_XRI_CN ||\n\t\t    piocb->iocb.ulpCommand == CMD_CLOSE_XRI_CN)\n\t\t\tsglq = NULL;\n\t\telse {\n\t\t\tif (!list_empty(&pring->txq)) {\n\t\t\t\tif (!(flag & SLI_IOCB_RET_IOCB)) {\n\t\t\t\t\t__lpfc_sli_ringtx_put(phba,\n\t\t\t\t\t\tpring, piocb);\n\t\t\t\t\treturn IOCB_SUCCESS;\n\t\t\t\t} else {\n\t\t\t\t\treturn IOCB_BUSY;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tsglq = __lpfc_sli_get_els_sglq(phba, piocb);\n\t\t\t\tif (!sglq) {\n\t\t\t\t\tif (!(flag & SLI_IOCB_RET_IOCB)) {\n\t\t\t\t\t\t__lpfc_sli_ringtx_put(phba,\n\t\t\t\t\t\t\t\tpring,\n\t\t\t\t\t\t\t\tpiocb);\n\t\t\t\t\t\treturn IOCB_SUCCESS;\n\t\t\t\t\t} else\n\t\t\t\t\t\treturn IOCB_BUSY;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else if (piocb->iocb_flag &  LPFC_IO_FCP)\n\t\t/* These IO's already have an XRI and a mapped sgl. */\n\t\tsglq = NULL;\n\telse {\n\t\t/*\n\t\t * This is a continuation of a commandi,(CX) so this\n\t\t * sglq is on the active list\n\t\t */\n\t\tsglq = __lpfc_get_active_sglq(phba, piocb->sli4_lxritag);\n\t\tif (!sglq)\n\t\t\treturn IOCB_ERROR;\n\t}\n\n\tif (sglq) {\n\t\tpiocb->sli4_lxritag = sglq->sli4_lxritag;\n\t\tpiocb->sli4_xritag = sglq->sli4_xritag;\n\t\tif (NO_XRI == lpfc_sli4_bpl2sgl(phba, piocb, sglq))\n\t\t\treturn IOCB_ERROR;\n\t}\n\n\tif (lpfc_sli4_iocb2wqe(phba, piocb, &wqe))\n\t\treturn IOCB_ERROR;\n\n\tif (lpfc_sli4_wq_put(wq, &wqe))\n\t\treturn IOCB_ERROR;\n\tlpfc_sli_ringtxcmpl_put(phba, pring, piocb);\n\n\treturn 0;\n}\n\n/*\n * __lpfc_sli_issue_iocb - Wrapper func of lockless version for issuing iocb\n *\n * This routine wraps the actual lockless version for issusing IOCB function\n * pointer from the lpfc_hba struct.\n *\n * Return codes:\n * IOCB_ERROR - Error\n * IOCB_SUCCESS - Success\n * IOCB_BUSY - Busy\n **/\nint\n__lpfc_sli_issue_iocb(struct lpfc_hba *phba, uint32_t ring_number,\n\t\tstruct lpfc_iocbq *piocb, uint32_t flag)\n{\n\treturn phba->__lpfc_sli_issue_iocb(phba, ring_number, piocb, flag);\n}\n\n/**\n * lpfc_sli_api_table_setup - Set up sli api function jump table\n * @phba: The hba struct for which this call is being executed.\n * @dev_grp: The HBA PCI-Device group number.\n *\n * This routine sets up the SLI interface API function jump table in @phba\n * struct.\n * Returns: 0 - success, -ENODEV - failure.\n **/\nint\nlpfc_sli_api_table_setup(struct lpfc_hba *phba, uint8_t dev_grp)\n{\n\n\tswitch (dev_grp) {\n\tcase LPFC_PCI_DEV_LP:\n\t\tphba->__lpfc_sli_issue_iocb = __lpfc_sli_issue_iocb_s3;\n\t\tphba->__lpfc_sli_release_iocbq = __lpfc_sli_release_iocbq_s3;\n\t\tbreak;\n\tcase LPFC_PCI_DEV_OC:\n\t\tphba->__lpfc_sli_issue_iocb = __lpfc_sli_issue_iocb_s4;\n\t\tphba->__lpfc_sli_release_iocbq = __lpfc_sli_release_iocbq_s4;\n\t\tbreak;\n\tdefault:\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"1419 Invalid HBA PCI-device group: 0x%x\\n\",\n\t\t\t\tdev_grp);\n\t\treturn -ENODEV;\n\t\tbreak;\n\t}\n\tphba->lpfc_get_iocb_from_iocbq = lpfc_get_iocb_from_iocbq;\n\treturn 0;\n}\n\n/**\n * lpfc_sli4_calc_ring - Calculates which ring to use\n * @phba: Pointer to HBA context object.\n * @piocb: Pointer to command iocb.\n *\n * For SLI4 only, FCP IO can deferred to one fo many WQs, based on\n * hba_wqidx, thus we need to calculate the corresponding ring.\n * Since ABORTS must go on the same WQ of the command they are\n * aborting, we use command's hba_wqidx.\n */\nstruct lpfc_sli_ring *\nlpfc_sli4_calc_ring(struct lpfc_hba *phba, struct lpfc_iocbq *piocb)\n{\n\tstruct lpfc_io_buf *lpfc_cmd;\n\n\tif (piocb->iocb_flag & (LPFC_IO_FCP | LPFC_USE_FCPWQIDX)) {\n\t\tif (unlikely(!phba->sli4_hba.hdwq))\n\t\t\treturn NULL;\n\t\t/*\n\t\t * for abort iocb hba_wqidx should already\n\t\t * be setup based on what work queue we used.\n\t\t */\n\t\tif (!(piocb->iocb_flag & LPFC_USE_FCPWQIDX)) {\n\t\t\tlpfc_cmd = (struct lpfc_io_buf *)piocb->context1;\n\t\t\tpiocb->hba_wqidx = lpfc_cmd->hdwq_no;\n\t\t}\n\t\treturn phba->sli4_hba.hdwq[piocb->hba_wqidx].io_wq->pring;\n\t} else {\n\t\tif (unlikely(!phba->sli4_hba.els_wq))\n\t\t\treturn NULL;\n\t\tpiocb->hba_wqidx = 0;\n\t\treturn phba->sli4_hba.els_wq->pring;\n\t}\n}\n\n/**\n * lpfc_sli_issue_iocb - Wrapper function for __lpfc_sli_issue_iocb\n * @phba: Pointer to HBA context object.\n * @ring_number: Ring number\n * @piocb: Pointer to command iocb.\n * @flag: Flag indicating if this command can be put into txq.\n *\n * lpfc_sli_issue_iocb is a wrapper around __lpfc_sli_issue_iocb\n * function. This function gets the hbalock and calls\n * __lpfc_sli_issue_iocb function and will return the error returned\n * by __lpfc_sli_issue_iocb function. This wrapper is used by\n * functions which do not hold hbalock.\n **/\nint\nlpfc_sli_issue_iocb(struct lpfc_hba *phba, uint32_t ring_number,\n\t\t    struct lpfc_iocbq *piocb, uint32_t flag)\n{\n\tstruct lpfc_sli_ring *pring;\n\tstruct lpfc_queue *eq;\n\tunsigned long iflags;\n\tint rc;\n\n\tif (phba->sli_rev == LPFC_SLI_REV4) {\n\t\teq = phba->sli4_hba.hdwq[piocb->hba_wqidx].hba_eq;\n\n\t\tpring = lpfc_sli4_calc_ring(phba, piocb);\n\t\tif (unlikely(pring == NULL))\n\t\t\treturn IOCB_ERROR;\n\n\t\tspin_lock_irqsave(&pring->ring_lock, iflags);\n\t\trc = __lpfc_sli_issue_iocb(phba, ring_number, piocb, flag);\n\t\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n\n\t\tlpfc_sli4_poll_eq(eq, LPFC_POLL_FASTPATH);\n\t} else {\n\t\t/* For now, SLI2/3 will still use hbalock */\n\t\tspin_lock_irqsave(&phba->hbalock, iflags);\n\t\trc = __lpfc_sli_issue_iocb(phba, ring_number, piocb, flag);\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t}\n\treturn rc;\n}\n\n/**\n * lpfc_extra_ring_setup - Extra ring setup function\n * @phba: Pointer to HBA context object.\n *\n * This function is called while driver attaches with the\n * HBA to setup the extra ring. The extra ring is used\n * only when driver needs to support target mode functionality\n * or IP over FC functionalities.\n *\n * This function is called with no lock held. SLI3 only.\n **/\nstatic int\nlpfc_extra_ring_setup( struct lpfc_hba *phba)\n{\n\tstruct lpfc_sli *psli;\n\tstruct lpfc_sli_ring *pring;\n\n\tpsli = &phba->sli;\n\n\t/* Adjust cmd/rsp ring iocb entries more evenly */\n\n\t/* Take some away from the FCP ring */\n\tpring = &psli->sli3_ring[LPFC_FCP_RING];\n\tpring->sli.sli3.numCiocb -= SLI2_IOCB_CMD_R1XTRA_ENTRIES;\n\tpring->sli.sli3.numRiocb -= SLI2_IOCB_RSP_R1XTRA_ENTRIES;\n\tpring->sli.sli3.numCiocb -= SLI2_IOCB_CMD_R3XTRA_ENTRIES;\n\tpring->sli.sli3.numRiocb -= SLI2_IOCB_RSP_R3XTRA_ENTRIES;\n\n\t/* and give them to the extra ring */\n\tpring = &psli->sli3_ring[LPFC_EXTRA_RING];\n\n\tpring->sli.sli3.numCiocb += SLI2_IOCB_CMD_R1XTRA_ENTRIES;\n\tpring->sli.sli3.numRiocb += SLI2_IOCB_RSP_R1XTRA_ENTRIES;\n\tpring->sli.sli3.numCiocb += SLI2_IOCB_CMD_R3XTRA_ENTRIES;\n\tpring->sli.sli3.numRiocb += SLI2_IOCB_RSP_R3XTRA_ENTRIES;\n\n\t/* Setup default profile for this ring */\n\tpring->iotag_max = 4096;\n\tpring->num_mask = 1;\n\tpring->prt[0].profile = 0;      /* Mask 0 */\n\tpring->prt[0].rctl = phba->cfg_multi_ring_rctl;\n\tpring->prt[0].type = phba->cfg_multi_ring_type;\n\tpring->prt[0].lpfc_sli_rcv_unsol_event = NULL;\n\treturn 0;\n}\n\n/* lpfc_sli_abts_err_handler - handle a failed ABTS request from an SLI3 port.\n * @phba: Pointer to HBA context object.\n * @iocbq: Pointer to iocb object.\n *\n * The async_event handler calls this routine when it receives\n * an ASYNC_STATUS_CN event from the port.  The port generates\n * this event when an Abort Sequence request to an rport fails\n * twice in succession.  The abort could be originated by the\n * driver or by the port.  The ABTS could have been for an ELS\n * or FCP IO.  The port only generates this event when an ABTS\n * fails to complete after one retry.\n */\nstatic void\nlpfc_sli_abts_err_handler(struct lpfc_hba *phba,\n\t\t\t  struct lpfc_iocbq *iocbq)\n{\n\tstruct lpfc_nodelist *ndlp = NULL;\n\tuint16_t rpi = 0, vpi = 0;\n\tstruct lpfc_vport *vport = NULL;\n\n\t/* The rpi in the ulpContext is vport-sensitive. */\n\tvpi = iocbq->iocb.un.asyncstat.sub_ctxt_tag;\n\trpi = iocbq->iocb.ulpContext;\n\n\tlpfc_printf_log(phba, KERN_WARNING, LOG_SLI,\n\t\t\t\"3092 Port generated ABTS async event \"\n\t\t\t\"on vpi %d rpi %d status 0x%x\\n\",\n\t\t\tvpi, rpi, iocbq->iocb.ulpStatus);\n\n\tvport = lpfc_find_vport_by_vpid(phba, vpi);\n\tif (!vport)\n\t\tgoto err_exit;\n\tndlp = lpfc_findnode_rpi(vport, rpi);\n\tif (!ndlp || !NLP_CHK_NODE_ACT(ndlp))\n\t\tgoto err_exit;\n\n\tif (iocbq->iocb.ulpStatus == IOSTAT_LOCAL_REJECT)\n\t\tlpfc_sli_abts_recover_port(vport, ndlp);\n\treturn;\n\n err_exit:\n\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\"3095 Event Context not found, no \"\n\t\t\t\"action on vpi %d rpi %d status 0x%x, reason 0x%x\\n\",\n\t\t\tiocbq->iocb.ulpContext, iocbq->iocb.ulpStatus,\n\t\t\tvpi, rpi);\n}\n\n/* lpfc_sli4_abts_err_handler - handle a failed ABTS request from an SLI4 port.\n * @phba: pointer to HBA context object.\n * @ndlp: nodelist pointer for the impacted rport.\n * @axri: pointer to the wcqe containing the failed exchange.\n *\n * The driver calls this routine when it receives an ABORT_XRI_FCP CQE from the\n * port.  The port generates this event when an abort exchange request to an\n * rport fails twice in succession with no reply.  The abort could be originated\n * by the driver or by the port.  The ABTS could have been for an ELS or FCP IO.\n */\nvoid\nlpfc_sli4_abts_err_handler(struct lpfc_hba *phba,\n\t\t\t   struct lpfc_nodelist *ndlp,\n\t\t\t   struct sli4_wcqe_xri_aborted *axri)\n{\n\tstruct lpfc_vport *vport;\n\tuint32_t ext_status = 0;\n\n\tif (!ndlp || !NLP_CHK_NODE_ACT(ndlp)) {\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\t\"3115 Node Context not found, driver \"\n\t\t\t\t\"ignoring abts err event\\n\");\n\t\treturn;\n\t}\n\n\tvport = ndlp->vport;\n\tlpfc_printf_log(phba, KERN_WARNING, LOG_SLI,\n\t\t\t\"3116 Port generated FCP XRI ABORT event on \"\n\t\t\t\"vpi %d rpi %d xri x%x status 0x%x parameter x%x\\n\",\n\t\t\tndlp->vport->vpi, phba->sli4_hba.rpi_ids[ndlp->nlp_rpi],\n\t\t\tbf_get(lpfc_wcqe_xa_xri, axri),\n\t\t\tbf_get(lpfc_wcqe_xa_status, axri),\n\t\t\taxri->parameter);\n\n\t/*\n\t * Catch the ABTS protocol failure case.  Older OCe FW releases returned\n\t * LOCAL_REJECT and 0 for a failed ABTS exchange and later OCe and\n\t * LPe FW releases returned LOCAL_REJECT and SEQUENCE_TIMEOUT.\n\t */\n\text_status = axri->parameter & IOERR_PARAM_MASK;\n\tif ((bf_get(lpfc_wcqe_xa_status, axri) == IOSTAT_LOCAL_REJECT) &&\n\t    ((ext_status == IOERR_SEQUENCE_TIMEOUT) || (ext_status == 0)))\n\t\tlpfc_sli_abts_recover_port(vport, ndlp);\n}\n\n/**\n * lpfc_sli_async_event_handler - ASYNC iocb handler function\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n * @iocbq: Pointer to iocb object.\n *\n * This function is called by the slow ring event handler\n * function when there is an ASYNC event iocb in the ring.\n * This function is called with no lock held.\n * Currently this function handles only temperature related\n * ASYNC events. The function decodes the temperature sensor\n * event message and posts events for the management applications.\n **/\nstatic void\nlpfc_sli_async_event_handler(struct lpfc_hba * phba,\n\tstruct lpfc_sli_ring * pring, struct lpfc_iocbq * iocbq)\n{\n\tIOCB_t *icmd;\n\tuint16_t evt_code;\n\tstruct temp_event temp_event_data;\n\tstruct Scsi_Host *shost;\n\tuint32_t *iocb_w;\n\n\ticmd = &iocbq->iocb;\n\tevt_code = icmd->un.asyncstat.evt_code;\n\n\tswitch (evt_code) {\n\tcase ASYNC_TEMP_WARN:\n\tcase ASYNC_TEMP_SAFE:\n\t\ttemp_event_data.data = (uint32_t) icmd->ulpContext;\n\t\ttemp_event_data.event_type = FC_REG_TEMPERATURE_EVENT;\n\t\tif (evt_code == ASYNC_TEMP_WARN) {\n\t\t\ttemp_event_data.event_code = LPFC_THRESHOLD_TEMP;\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0347 Adapter is very hot, please take \"\n\t\t\t\t\"corrective action. temperature : %d Celsius\\n\",\n\t\t\t\t(uint32_t) icmd->ulpContext);\n\t\t} else {\n\t\t\ttemp_event_data.event_code = LPFC_NORMAL_TEMP;\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0340 Adapter temperature is OK now. \"\n\t\t\t\t\"temperature : %d Celsius\\n\",\n\t\t\t\t(uint32_t) icmd->ulpContext);\n\t\t}\n\n\t\t/* Send temperature change event to applications */\n\t\tshost = lpfc_shost_from_vport(phba->pport);\n\t\tfc_host_post_vendor_event(shost, fc_get_event_number(),\n\t\t\tsizeof(temp_event_data), (char *) &temp_event_data,\n\t\t\tLPFC_NL_VENDOR_ID);\n\t\tbreak;\n\tcase ASYNC_STATUS_CN:\n\t\tlpfc_sli_abts_err_handler(phba, iocbq);\n\t\tbreak;\n\tdefault:\n\t\tiocb_w = (uint32_t *) icmd;\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"0346 Ring %d handler: unexpected ASYNC_STATUS\"\n\t\t\t\" evt_code 0x%x\\n\"\n\t\t\t\"W0  0x%08x W1  0x%08x W2  0x%08x W3  0x%08x\\n\"\n\t\t\t\"W4  0x%08x W5  0x%08x W6  0x%08x W7  0x%08x\\n\"\n\t\t\t\"W8  0x%08x W9  0x%08x W10 0x%08x W11 0x%08x\\n\"\n\t\t\t\"W12 0x%08x W13 0x%08x W14 0x%08x W15 0x%08x\\n\",\n\t\t\tpring->ringno, icmd->un.asyncstat.evt_code,\n\t\t\tiocb_w[0], iocb_w[1], iocb_w[2], iocb_w[3],\n\t\t\tiocb_w[4], iocb_w[5], iocb_w[6], iocb_w[7],\n\t\t\tiocb_w[8], iocb_w[9], iocb_w[10], iocb_w[11],\n\t\t\tiocb_w[12], iocb_w[13], iocb_w[14], iocb_w[15]);\n\n\t\tbreak;\n\t}\n}\n\n\n/**\n * lpfc_sli4_setup - SLI ring setup function\n * @phba: Pointer to HBA context object.\n *\n * lpfc_sli_setup sets up rings of the SLI interface with\n * number of iocbs per ring and iotags. This function is\n * called while driver attach to the HBA and before the\n * interrupts are enabled. So there is no need for locking.\n *\n * This function always returns 0.\n **/\nint\nlpfc_sli4_setup(struct lpfc_hba *phba)\n{\n\tstruct lpfc_sli_ring *pring;\n\n\tpring = phba->sli4_hba.els_wq->pring;\n\tpring->num_mask = LPFC_MAX_RING_MASK;\n\tpring->prt[0].profile = 0;\t/* Mask 0 */\n\tpring->prt[0].rctl = FC_RCTL_ELS_REQ;\n\tpring->prt[0].type = FC_TYPE_ELS;\n\tpring->prt[0].lpfc_sli_rcv_unsol_event =\n\t    lpfc_els_unsol_event;\n\tpring->prt[1].profile = 0;\t/* Mask 1 */\n\tpring->prt[1].rctl = FC_RCTL_ELS_REP;\n\tpring->prt[1].type = FC_TYPE_ELS;\n\tpring->prt[1].lpfc_sli_rcv_unsol_event =\n\t    lpfc_els_unsol_event;\n\tpring->prt[2].profile = 0;\t/* Mask 2 */\n\t/* NameServer Inquiry */\n\tpring->prt[2].rctl = FC_RCTL_DD_UNSOL_CTL;\n\t/* NameServer */\n\tpring->prt[2].type = FC_TYPE_CT;\n\tpring->prt[2].lpfc_sli_rcv_unsol_event =\n\t    lpfc_ct_unsol_event;\n\tpring->prt[3].profile = 0;\t/* Mask 3 */\n\t/* NameServer response */\n\tpring->prt[3].rctl = FC_RCTL_DD_SOL_CTL;\n\t/* NameServer */\n\tpring->prt[3].type = FC_TYPE_CT;\n\tpring->prt[3].lpfc_sli_rcv_unsol_event =\n\t    lpfc_ct_unsol_event;\n\treturn 0;\n}\n\n/**\n * lpfc_sli_setup - SLI ring setup function\n * @phba: Pointer to HBA context object.\n *\n * lpfc_sli_setup sets up rings of the SLI interface with\n * number of iocbs per ring and iotags. This function is\n * called while driver attach to the HBA and before the\n * interrupts are enabled. So there is no need for locking.\n *\n * This function always returns 0. SLI3 only.\n **/\nint\nlpfc_sli_setup(struct lpfc_hba *phba)\n{\n\tint i, totiocbsize = 0;\n\tstruct lpfc_sli *psli = &phba->sli;\n\tstruct lpfc_sli_ring *pring;\n\n\tpsli->num_rings = MAX_SLI3_CONFIGURED_RINGS;\n\tpsli->sli_flag = 0;\n\n\tpsli->iocbq_lookup = NULL;\n\tpsli->iocbq_lookup_len = 0;\n\tpsli->last_iotag = 0;\n\n\tfor (i = 0; i < psli->num_rings; i++) {\n\t\tpring = &psli->sli3_ring[i];\n\t\tswitch (i) {\n\t\tcase LPFC_FCP_RING:\t/* ring 0 - FCP */\n\t\t\t/* numCiocb and numRiocb are used in config_port */\n\t\t\tpring->sli.sli3.numCiocb = SLI2_IOCB_CMD_R0_ENTRIES;\n\t\t\tpring->sli.sli3.numRiocb = SLI2_IOCB_RSP_R0_ENTRIES;\n\t\t\tpring->sli.sli3.numCiocb +=\n\t\t\t\tSLI2_IOCB_CMD_R1XTRA_ENTRIES;\n\t\t\tpring->sli.sli3.numRiocb +=\n\t\t\t\tSLI2_IOCB_RSP_R1XTRA_ENTRIES;\n\t\t\tpring->sli.sli3.numCiocb +=\n\t\t\t\tSLI2_IOCB_CMD_R3XTRA_ENTRIES;\n\t\t\tpring->sli.sli3.numRiocb +=\n\t\t\t\tSLI2_IOCB_RSP_R3XTRA_ENTRIES;\n\t\t\tpring->sli.sli3.sizeCiocb = (phba->sli_rev == 3) ?\n\t\t\t\t\t\t\tSLI3_IOCB_CMD_SIZE :\n\t\t\t\t\t\t\tSLI2_IOCB_CMD_SIZE;\n\t\t\tpring->sli.sli3.sizeRiocb = (phba->sli_rev == 3) ?\n\t\t\t\t\t\t\tSLI3_IOCB_RSP_SIZE :\n\t\t\t\t\t\t\tSLI2_IOCB_RSP_SIZE;\n\t\t\tpring->iotag_ctr = 0;\n\t\t\tpring->iotag_max =\n\t\t\t    (phba->cfg_hba_queue_depth * 2);\n\t\t\tpring->fast_iotag = pring->iotag_max;\n\t\t\tpring->num_mask = 0;\n\t\t\tbreak;\n\t\tcase LPFC_EXTRA_RING:\t/* ring 1 - EXTRA */\n\t\t\t/* numCiocb and numRiocb are used in config_port */\n\t\t\tpring->sli.sli3.numCiocb = SLI2_IOCB_CMD_R1_ENTRIES;\n\t\t\tpring->sli.sli3.numRiocb = SLI2_IOCB_RSP_R1_ENTRIES;\n\t\t\tpring->sli.sli3.sizeCiocb = (phba->sli_rev == 3) ?\n\t\t\t\t\t\t\tSLI3_IOCB_CMD_SIZE :\n\t\t\t\t\t\t\tSLI2_IOCB_CMD_SIZE;\n\t\t\tpring->sli.sli3.sizeRiocb = (phba->sli_rev == 3) ?\n\t\t\t\t\t\t\tSLI3_IOCB_RSP_SIZE :\n\t\t\t\t\t\t\tSLI2_IOCB_RSP_SIZE;\n\t\t\tpring->iotag_max = phba->cfg_hba_queue_depth;\n\t\t\tpring->num_mask = 0;\n\t\t\tbreak;\n\t\tcase LPFC_ELS_RING:\t/* ring 2 - ELS / CT */\n\t\t\t/* numCiocb and numRiocb are used in config_port */\n\t\t\tpring->sli.sli3.numCiocb = SLI2_IOCB_CMD_R2_ENTRIES;\n\t\t\tpring->sli.sli3.numRiocb = SLI2_IOCB_RSP_R2_ENTRIES;\n\t\t\tpring->sli.sli3.sizeCiocb = (phba->sli_rev == 3) ?\n\t\t\t\t\t\t\tSLI3_IOCB_CMD_SIZE :\n\t\t\t\t\t\t\tSLI2_IOCB_CMD_SIZE;\n\t\t\tpring->sli.sli3.sizeRiocb = (phba->sli_rev == 3) ?\n\t\t\t\t\t\t\tSLI3_IOCB_RSP_SIZE :\n\t\t\t\t\t\t\tSLI2_IOCB_RSP_SIZE;\n\t\t\tpring->fast_iotag = 0;\n\t\t\tpring->iotag_ctr = 0;\n\t\t\tpring->iotag_max = 4096;\n\t\t\tpring->lpfc_sli_rcv_async_status =\n\t\t\t\tlpfc_sli_async_event_handler;\n\t\t\tpring->num_mask = LPFC_MAX_RING_MASK;\n\t\t\tpring->prt[0].profile = 0;\t/* Mask 0 */\n\t\t\tpring->prt[0].rctl = FC_RCTL_ELS_REQ;\n\t\t\tpring->prt[0].type = FC_TYPE_ELS;\n\t\t\tpring->prt[0].lpfc_sli_rcv_unsol_event =\n\t\t\t    lpfc_els_unsol_event;\n\t\t\tpring->prt[1].profile = 0;\t/* Mask 1 */\n\t\t\tpring->prt[1].rctl = FC_RCTL_ELS_REP;\n\t\t\tpring->prt[1].type = FC_TYPE_ELS;\n\t\t\tpring->prt[1].lpfc_sli_rcv_unsol_event =\n\t\t\t    lpfc_els_unsol_event;\n\t\t\tpring->prt[2].profile = 0;\t/* Mask 2 */\n\t\t\t/* NameServer Inquiry */\n\t\t\tpring->prt[2].rctl = FC_RCTL_DD_UNSOL_CTL;\n\t\t\t/* NameServer */\n\t\t\tpring->prt[2].type = FC_TYPE_CT;\n\t\t\tpring->prt[2].lpfc_sli_rcv_unsol_event =\n\t\t\t    lpfc_ct_unsol_event;\n\t\t\tpring->prt[3].profile = 0;\t/* Mask 3 */\n\t\t\t/* NameServer response */\n\t\t\tpring->prt[3].rctl = FC_RCTL_DD_SOL_CTL;\n\t\t\t/* NameServer */\n\t\t\tpring->prt[3].type = FC_TYPE_CT;\n\t\t\tpring->prt[3].lpfc_sli_rcv_unsol_event =\n\t\t\t    lpfc_ct_unsol_event;\n\t\t\tbreak;\n\t\t}\n\t\ttotiocbsize += (pring->sli.sli3.numCiocb *\n\t\t\tpring->sli.sli3.sizeCiocb) +\n\t\t\t(pring->sli.sli3.numRiocb * pring->sli.sli3.sizeRiocb);\n\t}\n\tif (totiocbsize > MAX_SLIM_IOCB_SIZE) {\n\t\t/* Too many cmd / rsp ring entries in SLI2 SLIM */\n\t\tprintk(KERN_ERR \"%d:0462 Too many cmd / rsp ring entries in \"\n\t\t       \"SLI2 SLIM Data: x%x x%lx\\n\",\n\t\t       phba->brd_no, totiocbsize,\n\t\t       (unsigned long) MAX_SLIM_IOCB_SIZE);\n\t}\n\tif (phba->cfg_multi_ring_support == 2)\n\t\tlpfc_extra_ring_setup(phba);\n\n\treturn 0;\n}\n\n/**\n * lpfc_sli4_queue_init - Queue initialization function\n * @phba: Pointer to HBA context object.\n *\n * lpfc_sli4_queue_init sets up mailbox queues and iocb queues for each\n * ring. This function also initializes ring indices of each ring.\n * This function is called during the initialization of the SLI\n * interface of an HBA.\n * This function is called with no lock held and always returns\n * 1.\n **/\nvoid\nlpfc_sli4_queue_init(struct lpfc_hba *phba)\n{\n\tstruct lpfc_sli *psli;\n\tstruct lpfc_sli_ring *pring;\n\tint i;\n\n\tpsli = &phba->sli;\n\tspin_lock_irq(&phba->hbalock);\n\tINIT_LIST_HEAD(&psli->mboxq);\n\tINIT_LIST_HEAD(&psli->mboxq_cmpl);\n\t/* Initialize list headers for txq and txcmplq as double linked lists */\n\tfor (i = 0; i < phba->cfg_hdw_queue; i++) {\n\t\tpring = phba->sli4_hba.hdwq[i].io_wq->pring;\n\t\tpring->flag = 0;\n\t\tpring->ringno = LPFC_FCP_RING;\n\t\tpring->txcmplq_cnt = 0;\n\t\tINIT_LIST_HEAD(&pring->txq);\n\t\tINIT_LIST_HEAD(&pring->txcmplq);\n\t\tINIT_LIST_HEAD(&pring->iocb_continueq);\n\t\tspin_lock_init(&pring->ring_lock);\n\t}\n\tpring = phba->sli4_hba.els_wq->pring;\n\tpring->flag = 0;\n\tpring->ringno = LPFC_ELS_RING;\n\tpring->txcmplq_cnt = 0;\n\tINIT_LIST_HEAD(&pring->txq);\n\tINIT_LIST_HEAD(&pring->txcmplq);\n\tINIT_LIST_HEAD(&pring->iocb_continueq);\n\tspin_lock_init(&pring->ring_lock);\n\n\tif (phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME) {\n\t\tpring = phba->sli4_hba.nvmels_wq->pring;\n\t\tpring->flag = 0;\n\t\tpring->ringno = LPFC_ELS_RING;\n\t\tpring->txcmplq_cnt = 0;\n\t\tINIT_LIST_HEAD(&pring->txq);\n\t\tINIT_LIST_HEAD(&pring->txcmplq);\n\t\tINIT_LIST_HEAD(&pring->iocb_continueq);\n\t\tspin_lock_init(&pring->ring_lock);\n\t}\n\n\tspin_unlock_irq(&phba->hbalock);\n}\n\n/**\n * lpfc_sli_queue_init - Queue initialization function\n * @phba: Pointer to HBA context object.\n *\n * lpfc_sli_queue_init sets up mailbox queues and iocb queues for each\n * ring. This function also initializes ring indices of each ring.\n * This function is called during the initialization of the SLI\n * interface of an HBA.\n * This function is called with no lock held and always returns\n * 1.\n **/\nvoid\nlpfc_sli_queue_init(struct lpfc_hba *phba)\n{\n\tstruct lpfc_sli *psli;\n\tstruct lpfc_sli_ring *pring;\n\tint i;\n\n\tpsli = &phba->sli;\n\tspin_lock_irq(&phba->hbalock);\n\tINIT_LIST_HEAD(&psli->mboxq);\n\tINIT_LIST_HEAD(&psli->mboxq_cmpl);\n\t/* Initialize list headers for txq and txcmplq as double linked lists */\n\tfor (i = 0; i < psli->num_rings; i++) {\n\t\tpring = &psli->sli3_ring[i];\n\t\tpring->ringno = i;\n\t\tpring->sli.sli3.next_cmdidx  = 0;\n\t\tpring->sli.sli3.local_getidx = 0;\n\t\tpring->sli.sli3.cmdidx = 0;\n\t\tINIT_LIST_HEAD(&pring->iocb_continueq);\n\t\tINIT_LIST_HEAD(&pring->iocb_continue_saveq);\n\t\tINIT_LIST_HEAD(&pring->postbufq);\n\t\tpring->flag = 0;\n\t\tINIT_LIST_HEAD(&pring->txq);\n\t\tINIT_LIST_HEAD(&pring->txcmplq);\n\t\tspin_lock_init(&pring->ring_lock);\n\t}\n\tspin_unlock_irq(&phba->hbalock);\n}\n\n/**\n * lpfc_sli_mbox_sys_flush - Flush mailbox command sub-system\n * @phba: Pointer to HBA context object.\n *\n * This routine flushes the mailbox command subsystem. It will unconditionally\n * flush all the mailbox commands in the three possible stages in the mailbox\n * command sub-system: pending mailbox command queue; the outstanding mailbox\n * command; and completed mailbox command queue. It is caller's responsibility\n * to make sure that the driver is in the proper state to flush the mailbox\n * command sub-system. Namely, the posting of mailbox commands into the\n * pending mailbox command queue from the various clients must be stopped;\n * either the HBA is in a state that it will never works on the outstanding\n * mailbox command (such as in EEH or ERATT conditions) or the outstanding\n * mailbox command has been completed.\n **/\nstatic void\nlpfc_sli_mbox_sys_flush(struct lpfc_hba *phba)\n{\n\tLIST_HEAD(completions);\n\tstruct lpfc_sli *psli = &phba->sli;\n\tLPFC_MBOXQ_t *pmb;\n\tunsigned long iflag;\n\n\t/* Disable softirqs, including timers from obtaining phba->hbalock */\n\tlocal_bh_disable();\n\n\t/* Flush all the mailbox commands in the mbox system */\n\tspin_lock_irqsave(&phba->hbalock, iflag);\n\n\t/* The pending mailbox command queue */\n\tlist_splice_init(&phba->sli.mboxq, &completions);\n\t/* The outstanding active mailbox command */\n\tif (psli->mbox_active) {\n\t\tlist_add_tail(&psli->mbox_active->list, &completions);\n\t\tpsli->mbox_active = NULL;\n\t\tpsli->sli_flag &= ~LPFC_SLI_MBOX_ACTIVE;\n\t}\n\t/* The completed mailbox command queue */\n\tlist_splice_init(&phba->sli.mboxq_cmpl, &completions);\n\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\n\t/* Enable softirqs again, done with phba->hbalock */\n\tlocal_bh_enable();\n\n\t/* Return all flushed mailbox commands with MBX_NOT_FINISHED status */\n\twhile (!list_empty(&completions)) {\n\t\tlist_remove_head(&completions, pmb, LPFC_MBOXQ_t, list);\n\t\tpmb->u.mb.mbxStatus = MBX_NOT_FINISHED;\n\t\tif (pmb->mbox_cmpl)\n\t\t\tpmb->mbox_cmpl(phba, pmb);\n\t}\n}\n\n/**\n * lpfc_sli_host_down - Vport cleanup function\n * @vport: Pointer to virtual port object.\n *\n * lpfc_sli_host_down is called to clean up the resources\n * associated with a vport before destroying virtual\n * port data structures.\n * This function does following operations:\n * - Free discovery resources associated with this virtual\n *   port.\n * - Free iocbs associated with this virtual port in\n *   the txq.\n * - Send abort for all iocb commands associated with this\n *   vport in txcmplq.\n *\n * This function is called with no lock held and always returns 1.\n **/\nint\nlpfc_sli_host_down(struct lpfc_vport *vport)\n{\n\tLIST_HEAD(completions);\n\tstruct lpfc_hba *phba = vport->phba;\n\tstruct lpfc_sli *psli = &phba->sli;\n\tstruct lpfc_queue *qp = NULL;\n\tstruct lpfc_sli_ring *pring;\n\tstruct lpfc_iocbq *iocb, *next_iocb;\n\tint i;\n\tunsigned long flags = 0;\n\tuint16_t prev_pring_flag;\n\n\tlpfc_cleanup_discovery_resources(vport);\n\n\tspin_lock_irqsave(&phba->hbalock, flags);\n\n\t/*\n\t * Error everything on the txq since these iocbs\n\t * have not been given to the FW yet.\n\t * Also issue ABTS for everything on the txcmplq\n\t */\n\tif (phba->sli_rev != LPFC_SLI_REV4) {\n\t\tfor (i = 0; i < psli->num_rings; i++) {\n\t\t\tpring = &psli->sli3_ring[i];\n\t\t\tprev_pring_flag = pring->flag;\n\t\t\t/* Only slow rings */\n\t\t\tif (pring->ringno == LPFC_ELS_RING) {\n\t\t\t\tpring->flag |= LPFC_DEFERRED_RING_EVENT;\n\t\t\t\t/* Set the lpfc data pending flag */\n\t\t\t\tset_bit(LPFC_DATA_READY, &phba->data_flags);\n\t\t\t}\n\t\t\tlist_for_each_entry_safe(iocb, next_iocb,\n\t\t\t\t\t\t &pring->txq, list) {\n\t\t\t\tif (iocb->vport != vport)\n\t\t\t\t\tcontinue;\n\t\t\t\tlist_move_tail(&iocb->list, &completions);\n\t\t\t}\n\t\t\tlist_for_each_entry_safe(iocb, next_iocb,\n\t\t\t\t\t\t &pring->txcmplq, list) {\n\t\t\t\tif (iocb->vport != vport)\n\t\t\t\t\tcontinue;\n\t\t\t\tlpfc_sli_issue_abort_iotag(phba, pring, iocb);\n\t\t\t}\n\t\t\tpring->flag = prev_pring_flag;\n\t\t}\n\t} else {\n\t\tlist_for_each_entry(qp, &phba->sli4_hba.lpfc_wq_list, wq_list) {\n\t\t\tpring = qp->pring;\n\t\t\tif (!pring)\n\t\t\t\tcontinue;\n\t\t\tif (pring == phba->sli4_hba.els_wq->pring) {\n\t\t\t\tpring->flag |= LPFC_DEFERRED_RING_EVENT;\n\t\t\t\t/* Set the lpfc data pending flag */\n\t\t\t\tset_bit(LPFC_DATA_READY, &phba->data_flags);\n\t\t\t}\n\t\t\tprev_pring_flag = pring->flag;\n\t\t\tspin_lock(&pring->ring_lock);\n\t\t\tlist_for_each_entry_safe(iocb, next_iocb,\n\t\t\t\t\t\t &pring->txq, list) {\n\t\t\t\tif (iocb->vport != vport)\n\t\t\t\t\tcontinue;\n\t\t\t\tlist_move_tail(&iocb->list, &completions);\n\t\t\t}\n\t\t\tspin_unlock(&pring->ring_lock);\n\t\t\tlist_for_each_entry_safe(iocb, next_iocb,\n\t\t\t\t\t\t &pring->txcmplq, list) {\n\t\t\t\tif (iocb->vport != vport)\n\t\t\t\t\tcontinue;\n\t\t\t\tlpfc_sli_issue_abort_iotag(phba, pring, iocb);\n\t\t\t}\n\t\t\tpring->flag = prev_pring_flag;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&phba->hbalock, flags);\n\n\t/* Cancel all the IOCBs from the completions list */\n\tlpfc_sli_cancel_iocbs(phba, &completions, IOSTAT_LOCAL_REJECT,\n\t\t\t      IOERR_SLI_DOWN);\n\treturn 1;\n}\n\n/**\n * lpfc_sli_hba_down - Resource cleanup function for the HBA\n * @phba: Pointer to HBA context object.\n *\n * This function cleans up all iocb, buffers, mailbox commands\n * while shutting down the HBA. This function is called with no\n * lock held and always returns 1.\n * This function does the following to cleanup driver resources:\n * - Free discovery resources for each virtual port\n * - Cleanup any pending fabric iocbs\n * - Iterate through the iocb txq and free each entry\n *   in the list.\n * - Free up any buffer posted to the HBA\n * - Free mailbox commands in the mailbox queue.\n **/\nint\nlpfc_sli_hba_down(struct lpfc_hba *phba)\n{\n\tLIST_HEAD(completions);\n\tstruct lpfc_sli *psli = &phba->sli;\n\tstruct lpfc_queue *qp = NULL;\n\tstruct lpfc_sli_ring *pring;\n\tstruct lpfc_dmabuf *buf_ptr;\n\tunsigned long flags = 0;\n\tint i;\n\n\t/* Shutdown the mailbox command sub-system */\n\tlpfc_sli_mbox_sys_shutdown(phba, LPFC_MBX_WAIT);\n\n\tlpfc_hba_down_prep(phba);\n\n\t/* Disable softirqs, including timers from obtaining phba->hbalock */\n\tlocal_bh_disable();\n\n\tlpfc_fabric_abort_hba(phba);\n\n\tspin_lock_irqsave(&phba->hbalock, flags);\n\n\t/*\n\t * Error everything on the txq since these iocbs\n\t * have not been given to the FW yet.\n\t */\n\tif (phba->sli_rev != LPFC_SLI_REV4) {\n\t\tfor (i = 0; i < psli->num_rings; i++) {\n\t\t\tpring = &psli->sli3_ring[i];\n\t\t\t/* Only slow rings */\n\t\t\tif (pring->ringno == LPFC_ELS_RING) {\n\t\t\t\tpring->flag |= LPFC_DEFERRED_RING_EVENT;\n\t\t\t\t/* Set the lpfc data pending flag */\n\t\t\t\tset_bit(LPFC_DATA_READY, &phba->data_flags);\n\t\t\t}\n\t\t\tlist_splice_init(&pring->txq, &completions);\n\t\t}\n\t} else {\n\t\tlist_for_each_entry(qp, &phba->sli4_hba.lpfc_wq_list, wq_list) {\n\t\t\tpring = qp->pring;\n\t\t\tif (!pring)\n\t\t\t\tcontinue;\n\t\t\tspin_lock(&pring->ring_lock);\n\t\t\tlist_splice_init(&pring->txq, &completions);\n\t\t\tspin_unlock(&pring->ring_lock);\n\t\t\tif (pring == phba->sli4_hba.els_wq->pring) {\n\t\t\t\tpring->flag |= LPFC_DEFERRED_RING_EVENT;\n\t\t\t\t/* Set the lpfc data pending flag */\n\t\t\t\tset_bit(LPFC_DATA_READY, &phba->data_flags);\n\t\t\t}\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&phba->hbalock, flags);\n\n\t/* Cancel all the IOCBs from the completions list */\n\tlpfc_sli_cancel_iocbs(phba, &completions, IOSTAT_LOCAL_REJECT,\n\t\t\t      IOERR_SLI_DOWN);\n\n\tspin_lock_irqsave(&phba->hbalock, flags);\n\tlist_splice_init(&phba->elsbuf, &completions);\n\tphba->elsbuf_cnt = 0;\n\tphba->elsbuf_prev_cnt = 0;\n\tspin_unlock_irqrestore(&phba->hbalock, flags);\n\n\twhile (!list_empty(&completions)) {\n\t\tlist_remove_head(&completions, buf_ptr,\n\t\t\tstruct lpfc_dmabuf, list);\n\t\tlpfc_mbuf_free(phba, buf_ptr->virt, buf_ptr->phys);\n\t\tkfree(buf_ptr);\n\t}\n\n\t/* Enable softirqs again, done with phba->hbalock */\n\tlocal_bh_enable();\n\n\t/* Return any active mbox cmds */\n\tdel_timer_sync(&psli->mbox_tmo);\n\n\tspin_lock_irqsave(&phba->pport->work_port_lock, flags);\n\tphba->pport->work_port_events &= ~WORKER_MBOX_TMO;\n\tspin_unlock_irqrestore(&phba->pport->work_port_lock, flags);\n\n\treturn 1;\n}\n\n/**\n * lpfc_sli_pcimem_bcopy - SLI memory copy function\n * @srcp: Source memory pointer.\n * @destp: Destination memory pointer.\n * @cnt: Number of words required to be copied.\n *\n * This function is used for copying data between driver memory\n * and the SLI memory. This function also changes the endianness\n * of each word if native endianness is different from SLI\n * endianness. This function can be called with or without\n * lock.\n **/\nvoid\nlpfc_sli_pcimem_bcopy(void *srcp, void *destp, uint32_t cnt)\n{\n\tuint32_t *src = srcp;\n\tuint32_t *dest = destp;\n\tuint32_t ldata;\n\tint i;\n\n\tfor (i = 0; i < (int)cnt; i += sizeof (uint32_t)) {\n\t\tldata = *src;\n\t\tldata = le32_to_cpu(ldata);\n\t\t*dest = ldata;\n\t\tsrc++;\n\t\tdest++;\n\t}\n}\n\n\n/**\n * lpfc_sli_bemem_bcopy - SLI memory copy function\n * @srcp: Source memory pointer.\n * @destp: Destination memory pointer.\n * @cnt: Number of words required to be copied.\n *\n * This function is used for copying data between a data structure\n * with big endian representation to local endianness.\n * This function can be called with or without lock.\n **/\nvoid\nlpfc_sli_bemem_bcopy(void *srcp, void *destp, uint32_t cnt)\n{\n\tuint32_t *src = srcp;\n\tuint32_t *dest = destp;\n\tuint32_t ldata;\n\tint i;\n\n\tfor (i = 0; i < (int)cnt; i += sizeof(uint32_t)) {\n\t\tldata = *src;\n\t\tldata = be32_to_cpu(ldata);\n\t\t*dest = ldata;\n\t\tsrc++;\n\t\tdest++;\n\t}\n}\n\n/**\n * lpfc_sli_ringpostbuf_put - Function to add a buffer to postbufq\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n * @mp: Pointer to driver buffer object.\n *\n * This function is called with no lock held.\n * It always return zero after adding the buffer to the postbufq\n * buffer list.\n **/\nint\nlpfc_sli_ringpostbuf_put(struct lpfc_hba *phba, struct lpfc_sli_ring *pring,\n\t\t\t struct lpfc_dmabuf *mp)\n{\n\t/* Stick struct lpfc_dmabuf at end of postbufq so driver can look it up\n\t   later */\n\tspin_lock_irq(&phba->hbalock);\n\tlist_add_tail(&mp->list, &pring->postbufq);\n\tpring->postbufq_cnt++;\n\tspin_unlock_irq(&phba->hbalock);\n\treturn 0;\n}\n\n/**\n * lpfc_sli_get_buffer_tag - allocates a tag for a CMD_QUE_XRI64_CX buffer\n * @phba: Pointer to HBA context object.\n *\n * When HBQ is enabled, buffers are searched based on tags. This function\n * allocates a tag for buffer posted using CMD_QUE_XRI64_CX iocb. The\n * tag is bit wise or-ed with QUE_BUFTAG_BIT to make sure that the tag\n * does not conflict with tags of buffer posted for unsolicited events.\n * The function returns the allocated tag. The function is called with\n * no locks held.\n **/\nuint32_t\nlpfc_sli_get_buffer_tag(struct lpfc_hba *phba)\n{\n\tspin_lock_irq(&phba->hbalock);\n\tphba->buffer_tag_count++;\n\t/*\n\t * Always set the QUE_BUFTAG_BIT to distiguish between\n\t * a tag assigned by HBQ.\n\t */\n\tphba->buffer_tag_count |= QUE_BUFTAG_BIT;\n\tspin_unlock_irq(&phba->hbalock);\n\treturn phba->buffer_tag_count;\n}\n\n/**\n * lpfc_sli_ring_taggedbuf_get - find HBQ buffer associated with given tag\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n * @tag: Buffer tag.\n *\n * Buffers posted using CMD_QUE_XRI64_CX iocb are in pring->postbufq\n * list. After HBA DMA data to these buffers, CMD_IOCB_RET_XRI64_CX\n * iocb is posted to the response ring with the tag of the buffer.\n * This function searches the pring->postbufq list using the tag\n * to find buffer associated with CMD_IOCB_RET_XRI64_CX\n * iocb. If the buffer is found then lpfc_dmabuf object of the\n * buffer is returned to the caller else NULL is returned.\n * This function is called with no lock held.\n **/\nstruct lpfc_dmabuf *\nlpfc_sli_ring_taggedbuf_get(struct lpfc_hba *phba, struct lpfc_sli_ring *pring,\n\t\t\tuint32_t tag)\n{\n\tstruct lpfc_dmabuf *mp, *next_mp;\n\tstruct list_head *slp = &pring->postbufq;\n\n\t/* Search postbufq, from the beginning, looking for a match on tag */\n\tspin_lock_irq(&phba->hbalock);\n\tlist_for_each_entry_safe(mp, next_mp, &pring->postbufq, list) {\n\t\tif (mp->buffer_tag == tag) {\n\t\t\tlist_del_init(&mp->list);\n\t\t\tpring->postbufq_cnt--;\n\t\t\tspin_unlock_irq(&phba->hbalock);\n\t\t\treturn mp;\n\t\t}\n\t}\n\n\tspin_unlock_irq(&phba->hbalock);\n\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"0402 Cannot find virtual addr for buffer tag on \"\n\t\t\t\"ring %d Data x%lx x%px x%px x%x\\n\",\n\t\t\tpring->ringno, (unsigned long) tag,\n\t\t\tslp->next, slp->prev, pring->postbufq_cnt);\n\n\treturn NULL;\n}\n\n/**\n * lpfc_sli_ringpostbuf_get - search buffers for unsolicited CT and ELS events\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n * @phys: DMA address of the buffer.\n *\n * This function searches the buffer list using the dma_address\n * of unsolicited event to find the driver's lpfc_dmabuf object\n * corresponding to the dma_address. The function returns the\n * lpfc_dmabuf object if a buffer is found else it returns NULL.\n * This function is called by the ct and els unsolicited event\n * handlers to get the buffer associated with the unsolicited\n * event.\n *\n * This function is called with no lock held.\n **/\nstruct lpfc_dmabuf *\nlpfc_sli_ringpostbuf_get(struct lpfc_hba *phba, struct lpfc_sli_ring *pring,\n\t\t\t dma_addr_t phys)\n{\n\tstruct lpfc_dmabuf *mp, *next_mp;\n\tstruct list_head *slp = &pring->postbufq;\n\n\t/* Search postbufq, from the beginning, looking for a match on phys */\n\tspin_lock_irq(&phba->hbalock);\n\tlist_for_each_entry_safe(mp, next_mp, &pring->postbufq, list) {\n\t\tif (mp->phys == phys) {\n\t\t\tlist_del_init(&mp->list);\n\t\t\tpring->postbufq_cnt--;\n\t\t\tspin_unlock_irq(&phba->hbalock);\n\t\t\treturn mp;\n\t\t}\n\t}\n\n\tspin_unlock_irq(&phba->hbalock);\n\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"0410 Cannot find virtual addr for mapped buf on \"\n\t\t\t\"ring %d Data x%llx x%px x%px x%x\\n\",\n\t\t\tpring->ringno, (unsigned long long)phys,\n\t\t\tslp->next, slp->prev, pring->postbufq_cnt);\n\treturn NULL;\n}\n\n/**\n * lpfc_sli_abort_els_cmpl - Completion handler for the els abort iocbs\n * @phba: Pointer to HBA context object.\n * @cmdiocb: Pointer to driver command iocb object.\n * @rspiocb: Pointer to driver response iocb object.\n *\n * This function is the completion handler for the abort iocbs for\n * ELS commands. This function is called from the ELS ring event\n * handler with no lock held. This function frees memory resources\n * associated with the abort iocb.\n **/\nstatic void\nlpfc_sli_abort_els_cmpl(struct lpfc_hba *phba, struct lpfc_iocbq *cmdiocb,\n\t\t\tstruct lpfc_iocbq *rspiocb)\n{\n\tIOCB_t *irsp = &rspiocb->iocb;\n\tuint16_t abort_iotag, abort_context;\n\tstruct lpfc_iocbq *abort_iocb = NULL;\n\n\tif (irsp->ulpStatus) {\n\n\t\t/*\n\t\t * Assume that the port already completed and returned, or\n\t\t * will return the iocb. Just Log the message.\n\t\t */\n\t\tabort_context = cmdiocb->iocb.un.acxri.abortContextTag;\n\t\tabort_iotag = cmdiocb->iocb.un.acxri.abortIoTag;\n\n\t\tspin_lock_irq(&phba->hbalock);\n\t\tif (phba->sli_rev < LPFC_SLI_REV4) {\n\t\t\tif (irsp->ulpCommand == CMD_ABORT_XRI_CX &&\n\t\t\t    irsp->ulpStatus == IOSTAT_LOCAL_REJECT &&\n\t\t\t    irsp->un.ulpWord[4] == IOERR_ABORT_REQUESTED) {\n\t\t\t\tspin_unlock_irq(&phba->hbalock);\n\t\t\t\tgoto release_iocb;\n\t\t\t}\n\t\t\tif (abort_iotag != 0 &&\n\t\t\t\tabort_iotag <= phba->sli.last_iotag)\n\t\t\t\tabort_iocb =\n\t\t\t\t\tphba->sli.iocbq_lookup[abort_iotag];\n\t\t} else\n\t\t\t/* For sli4 the abort_tag is the XRI,\n\t\t\t * so the abort routine puts the iotag  of the iocb\n\t\t\t * being aborted in the context field of the abort\n\t\t\t * IOCB.\n\t\t\t */\n\t\t\tabort_iocb = phba->sli.iocbq_lookup[abort_context];\n\n\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_ELS | LOG_SLI,\n\t\t\t\t\"0327 Cannot abort els iocb x%px \"\n\t\t\t\t\"with tag %x context %x, abort status %x, \"\n\t\t\t\t\"abort code %x\\n\",\n\t\t\t\tabort_iocb, abort_iotag, abort_context,\n\t\t\t\tirsp->ulpStatus, irsp->un.ulpWord[4]);\n\n\t\tspin_unlock_irq(&phba->hbalock);\n\t}\nrelease_iocb:\n\tlpfc_sli_release_iocbq(phba, cmdiocb);\n\treturn;\n}\n\n/**\n * lpfc_ignore_els_cmpl - Completion handler for aborted ELS command\n * @phba: Pointer to HBA context object.\n * @cmdiocb: Pointer to driver command iocb object.\n * @rspiocb: Pointer to driver response iocb object.\n *\n * The function is called from SLI ring event handler with no\n * lock held. This function is the completion handler for ELS commands\n * which are aborted. The function frees memory resources used for\n * the aborted ELS commands.\n **/\nstatic void\nlpfc_ignore_els_cmpl(struct lpfc_hba *phba, struct lpfc_iocbq *cmdiocb,\n\t\t     struct lpfc_iocbq *rspiocb)\n{\n\tIOCB_t *irsp = &rspiocb->iocb;\n\n\t/* ELS cmd tag <ulpIoTag> completes */\n\tlpfc_printf_log(phba, KERN_INFO, LOG_ELS,\n\t\t\t\"0139 Ignoring ELS cmd tag x%x completion Data: \"\n\t\t\t\"x%x x%x x%x\\n\",\n\t\t\tirsp->ulpIoTag, irsp->ulpStatus,\n\t\t\tirsp->un.ulpWord[4], irsp->ulpTimeout);\n\tif (cmdiocb->iocb.ulpCommand == CMD_GEN_REQUEST64_CR)\n\t\tlpfc_ct_free_iocb(phba, cmdiocb);\n\telse\n\t\tlpfc_els_free_iocb(phba, cmdiocb);\n\treturn;\n}\n\n/**\n * lpfc_sli_abort_iotag_issue - Issue abort for a command iocb\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n * @cmdiocb: Pointer to driver command iocb object.\n *\n * This function issues an abort iocb for the provided command iocb down to\n * the port. Other than the case the outstanding command iocb is an abort\n * request, this function issues abort out unconditionally. This function is\n * called with hbalock held. The function returns 0 when it fails due to\n * memory allocation failure or when the command iocb is an abort request.\n * The hbalock is asserted held in the code path calling this routine.\n **/\nstatic int\nlpfc_sli_abort_iotag_issue(struct lpfc_hba *phba, struct lpfc_sli_ring *pring,\n\t\t\t   struct lpfc_iocbq *cmdiocb)\n{\n\tstruct lpfc_vport *vport = cmdiocb->vport;\n\tstruct lpfc_iocbq *abtsiocbp;\n\tIOCB_t *icmd = NULL;\n\tIOCB_t *iabt = NULL;\n\tint retval;\n\tunsigned long iflags;\n\tstruct lpfc_nodelist *ndlp;\n\n\t/*\n\t * There are certain command types we don't want to abort.  And we\n\t * don't want to abort commands that are already in the process of\n\t * being aborted.\n\t */\n\ticmd = &cmdiocb->iocb;\n\tif (icmd->ulpCommand == CMD_ABORT_XRI_CN ||\n\t    icmd->ulpCommand == CMD_CLOSE_XRI_CN ||\n\t    (cmdiocb->iocb_flag & LPFC_DRIVER_ABORTED) != 0)\n\t\treturn 0;\n\n\t/* issue ABTS for this IOCB based on iotag */\n\tabtsiocbp = __lpfc_sli_get_iocbq(phba);\n\tif (abtsiocbp == NULL)\n\t\treturn 0;\n\n\t/* This signals the response to set the correct status\n\t * before calling the completion handler\n\t */\n\tcmdiocb->iocb_flag |= LPFC_DRIVER_ABORTED;\n\n\tiabt = &abtsiocbp->iocb;\n\tiabt->un.acxri.abortType = ABORT_TYPE_ABTS;\n\tiabt->un.acxri.abortContextTag = icmd->ulpContext;\n\tif (phba->sli_rev == LPFC_SLI_REV4) {\n\t\tiabt->un.acxri.abortIoTag = cmdiocb->sli4_xritag;\n\t\tiabt->un.acxri.abortContextTag = cmdiocb->iotag;\n\t} else {\n\t\tiabt->un.acxri.abortIoTag = icmd->ulpIoTag;\n\t\tif (pring->ringno == LPFC_ELS_RING) {\n\t\t\tndlp = (struct lpfc_nodelist *)(cmdiocb->context1);\n\t\t\tiabt->un.acxri.abortContextTag = ndlp->nlp_rpi;\n\t\t}\n\t}\n\tiabt->ulpLe = 1;\n\tiabt->ulpClass = icmd->ulpClass;\n\n\t/* ABTS WQE must go to the same WQ as the WQE to be aborted */\n\tabtsiocbp->hba_wqidx = cmdiocb->hba_wqidx;\n\tif (cmdiocb->iocb_flag & LPFC_IO_FCP)\n\t\tabtsiocbp->iocb_flag |= LPFC_USE_FCPWQIDX;\n\tif (cmdiocb->iocb_flag & LPFC_IO_FOF)\n\t\tabtsiocbp->iocb_flag |= LPFC_IO_FOF;\n\n\tif (phba->link_state >= LPFC_LINK_UP)\n\t\tiabt->ulpCommand = CMD_ABORT_XRI_CN;\n\telse\n\t\tiabt->ulpCommand = CMD_CLOSE_XRI_CN;\n\n\tabtsiocbp->iocb_cmpl = lpfc_sli_abort_els_cmpl;\n\tabtsiocbp->vport = vport;\n\n\tlpfc_printf_vlog(vport, KERN_INFO, LOG_SLI,\n\t\t\t \"0339 Abort xri x%x, original iotag x%x, \"\n\t\t\t \"abort cmd iotag x%x\\n\",\n\t\t\t iabt->un.acxri.abortIoTag,\n\t\t\t iabt->un.acxri.abortContextTag,\n\t\t\t abtsiocbp->iotag);\n\n\tif (phba->sli_rev == LPFC_SLI_REV4) {\n\t\tpring = lpfc_sli4_calc_ring(phba, abtsiocbp);\n\t\tif (unlikely(pring == NULL))\n\t\t\treturn 0;\n\t\t/* Note: both hbalock and ring_lock need to be set here */\n\t\tspin_lock_irqsave(&pring->ring_lock, iflags);\n\t\tretval = __lpfc_sli_issue_iocb(phba, pring->ringno,\n\t\t\tabtsiocbp, 0);\n\t\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n\t} else {\n\t\tretval = __lpfc_sli_issue_iocb(phba, pring->ringno,\n\t\t\tabtsiocbp, 0);\n\t}\n\n\tif (retval)\n\t\t__lpfc_sli_release_iocbq(phba, abtsiocbp);\n\n\t/*\n\t * Caller to this routine should check for IOCB_ERROR\n\t * and handle it properly.  This routine no longer removes\n\t * iocb off txcmplq and call compl in case of IOCB_ERROR.\n\t */\n\treturn retval;\n}\n\n/**\n * lpfc_sli_issue_abort_iotag - Abort function for a command iocb\n * @phba: Pointer to HBA context object.\n * @pring: Pointer to driver SLI ring object.\n * @cmdiocb: Pointer to driver command iocb object.\n *\n * This function issues an abort iocb for the provided command iocb. In case\n * of unloading, the abort iocb will not be issued to commands on the ELS\n * ring. Instead, the callback function shall be changed to those commands\n * so that nothing happens when them finishes. This function is called with\n * hbalock held. The function returns 0 when the command iocb is an abort\n * request.\n **/\nint\nlpfc_sli_issue_abort_iotag(struct lpfc_hba *phba, struct lpfc_sli_ring *pring,\n\t\t\t   struct lpfc_iocbq *cmdiocb)\n{\n\tstruct lpfc_vport *vport = cmdiocb->vport;\n\tint retval = IOCB_ERROR;\n\tIOCB_t *icmd = NULL;\n\n\tlockdep_assert_held(&phba->hbalock);\n\n\t/*\n\t * There are certain command types we don't want to abort.  And we\n\t * don't want to abort commands that are already in the process of\n\t * being aborted.\n\t */\n\ticmd = &cmdiocb->iocb;\n\tif (icmd->ulpCommand == CMD_ABORT_XRI_CN ||\n\t    icmd->ulpCommand == CMD_CLOSE_XRI_CN ||\n\t    (cmdiocb->iocb_flag & LPFC_DRIVER_ABORTED) != 0)\n\t\treturn 0;\n\n\tif (!pring) {\n\t\tif (cmdiocb->iocb_flag & LPFC_IO_FABRIC)\n\t\t\tcmdiocb->fabric_iocb_cmpl = lpfc_ignore_els_cmpl;\n\t\telse\n\t\t\tcmdiocb->iocb_cmpl = lpfc_ignore_els_cmpl;\n\t\tgoto abort_iotag_exit;\n\t}\n\n\t/*\n\t * If we're unloading, don't abort iocb on the ELS ring, but change\n\t * the callback so that nothing happens when it finishes.\n\t */\n\tif ((vport->load_flag & FC_UNLOADING) &&\n\t    (pring->ringno == LPFC_ELS_RING)) {\n\t\tif (cmdiocb->iocb_flag & LPFC_IO_FABRIC)\n\t\t\tcmdiocb->fabric_iocb_cmpl = lpfc_ignore_els_cmpl;\n\t\telse\n\t\t\tcmdiocb->iocb_cmpl = lpfc_ignore_els_cmpl;\n\t\tgoto abort_iotag_exit;\n\t}\n\n\t/* Now, we try to issue the abort to the cmdiocb out */\n\tretval = lpfc_sli_abort_iotag_issue(phba, pring, cmdiocb);\n\nabort_iotag_exit:\n\t/*\n\t * Caller to this routine should check for IOCB_ERROR\n\t * and handle it properly.  This routine no longer removes\n\t * iocb off txcmplq and call compl in case of IOCB_ERROR.\n\t */\n\treturn retval;\n}\n\n/**\n * lpfc_sli_hba_iocb_abort - Abort all iocbs to an hba.\n * @phba: pointer to lpfc HBA data structure.\n *\n * This routine will abort all pending and outstanding iocbs to an HBA.\n **/\nvoid\nlpfc_sli_hba_iocb_abort(struct lpfc_hba *phba)\n{\n\tstruct lpfc_sli *psli = &phba->sli;\n\tstruct lpfc_sli_ring *pring;\n\tstruct lpfc_queue *qp = NULL;\n\tint i;\n\n\tif (phba->sli_rev != LPFC_SLI_REV4) {\n\t\tfor (i = 0; i < psli->num_rings; i++) {\n\t\t\tpring = &psli->sli3_ring[i];\n\t\t\tlpfc_sli_abort_iocb_ring(phba, pring);\n\t\t}\n\t\treturn;\n\t}\n\tlist_for_each_entry(qp, &phba->sli4_hba.lpfc_wq_list, wq_list) {\n\t\tpring = qp->pring;\n\t\tif (!pring)\n\t\t\tcontinue;\n\t\tlpfc_sli_abort_iocb_ring(phba, pring);\n\t}\n}\n\n/**\n * lpfc_sli_validate_fcp_iocb - find commands associated with a vport or LUN\n * @iocbq: Pointer to driver iocb object.\n * @vport: Pointer to driver virtual port object.\n * @tgt_id: SCSI ID of the target.\n * @lun_id: LUN ID of the scsi device.\n * @ctx_cmd: LPFC_CTX_LUN/LPFC_CTX_TGT/LPFC_CTX_HOST\n *\n * This function acts as an iocb filter for functions which abort or count\n * all FCP iocbs pending on a lun/SCSI target/SCSI host. It will return\n * 0 if the filtering criteria is met for the given iocb and will return\n * 1 if the filtering criteria is not met.\n * If ctx_cmd == LPFC_CTX_LUN, the function returns 0 only if the\n * given iocb is for the SCSI device specified by vport, tgt_id and\n * lun_id parameter.\n * If ctx_cmd == LPFC_CTX_TGT,  the function returns 0 only if the\n * given iocb is for the SCSI target specified by vport and tgt_id\n * parameters.\n * If ctx_cmd == LPFC_CTX_HOST, the function returns 0 only if the\n * given iocb is for the SCSI host associated with the given vport.\n * This function is called with no locks held.\n **/\nstatic int\nlpfc_sli_validate_fcp_iocb(struct lpfc_iocbq *iocbq, struct lpfc_vport *vport,\n\t\t\t   uint16_t tgt_id, uint64_t lun_id,\n\t\t\t   lpfc_ctx_cmd ctx_cmd)\n{\n\tstruct lpfc_io_buf *lpfc_cmd;\n\tint rc = 1;\n\n\tif (iocbq->vport != vport)\n\t\treturn rc;\n\n\tif (!(iocbq->iocb_flag &  LPFC_IO_FCP) ||\n\t    !(iocbq->iocb_flag & LPFC_IO_ON_TXCMPLQ))\n\t\treturn rc;\n\n\tlpfc_cmd = container_of(iocbq, struct lpfc_io_buf, cur_iocbq);\n\n\tif (lpfc_cmd->pCmd == NULL)\n\t\treturn rc;\n\n\tswitch (ctx_cmd) {\n\tcase LPFC_CTX_LUN:\n\t\tif ((lpfc_cmd->rdata) && (lpfc_cmd->rdata->pnode) &&\n\t\t    (lpfc_cmd->rdata->pnode->nlp_sid == tgt_id) &&\n\t\t    (scsilun_to_int(&lpfc_cmd->fcp_cmnd->fcp_lun) == lun_id))\n\t\t\trc = 0;\n\t\tbreak;\n\tcase LPFC_CTX_TGT:\n\t\tif ((lpfc_cmd->rdata) && (lpfc_cmd->rdata->pnode) &&\n\t\t    (lpfc_cmd->rdata->pnode->nlp_sid == tgt_id))\n\t\t\trc = 0;\n\t\tbreak;\n\tcase LPFC_CTX_HOST:\n\t\trc = 0;\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ERR \"%s: Unknown context cmd type, value %d\\n\",\n\t\t\t__func__, ctx_cmd);\n\t\tbreak;\n\t}\n\n\treturn rc;\n}\n\n/**\n * lpfc_sli_sum_iocb - Function to count the number of FCP iocbs pending\n * @vport: Pointer to virtual port.\n * @tgt_id: SCSI ID of the target.\n * @lun_id: LUN ID of the scsi device.\n * @ctx_cmd: LPFC_CTX_LUN/LPFC_CTX_TGT/LPFC_CTX_HOST.\n *\n * This function returns number of FCP commands pending for the vport.\n * When ctx_cmd == LPFC_CTX_LUN, the function returns number of FCP\n * commands pending on the vport associated with SCSI device specified\n * by tgt_id and lun_id parameters.\n * When ctx_cmd == LPFC_CTX_TGT, the function returns number of FCP\n * commands pending on the vport associated with SCSI target specified\n * by tgt_id parameter.\n * When ctx_cmd == LPFC_CTX_HOST, the function returns number of FCP\n * commands pending on the vport.\n * This function returns the number of iocbs which satisfy the filter.\n * This function is called without any lock held.\n **/\nint\nlpfc_sli_sum_iocb(struct lpfc_vport *vport, uint16_t tgt_id, uint64_t lun_id,\n\t\t  lpfc_ctx_cmd ctx_cmd)\n{\n\tstruct lpfc_hba *phba = vport->phba;\n\tstruct lpfc_iocbq *iocbq;\n\tint sum, i;\n\n\tspin_lock_irq(&phba->hbalock);\n\tfor (i = 1, sum = 0; i <= phba->sli.last_iotag; i++) {\n\t\tiocbq = phba->sli.iocbq_lookup[i];\n\n\t\tif (lpfc_sli_validate_fcp_iocb (iocbq, vport, tgt_id, lun_id,\n\t\t\t\t\t\tctx_cmd) == 0)\n\t\t\tsum++;\n\t}\n\tspin_unlock_irq(&phba->hbalock);\n\n\treturn sum;\n}\n\n/**\n * lpfc_sli_abort_fcp_cmpl - Completion handler function for aborted FCP IOCBs\n * @phba: Pointer to HBA context object\n * @cmdiocb: Pointer to command iocb object.\n * @rspiocb: Pointer to response iocb object.\n *\n * This function is called when an aborted FCP iocb completes. This\n * function is called by the ring event handler with no lock held.\n * This function frees the iocb.\n **/\nvoid\nlpfc_sli_abort_fcp_cmpl(struct lpfc_hba *phba, struct lpfc_iocbq *cmdiocb,\n\t\t\tstruct lpfc_iocbq *rspiocb)\n{\n\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\"3096 ABORT_XRI_CN completing on rpi x%x \"\n\t\t\t\"original iotag x%x, abort cmd iotag x%x \"\n\t\t\t\"status 0x%x, reason 0x%x\\n\",\n\t\t\tcmdiocb->iocb.un.acxri.abortContextTag,\n\t\t\tcmdiocb->iocb.un.acxri.abortIoTag,\n\t\t\tcmdiocb->iotag, rspiocb->iocb.ulpStatus,\n\t\t\trspiocb->iocb.un.ulpWord[4]);\n\tlpfc_sli_release_iocbq(phba, cmdiocb);\n\treturn;\n}\n\n/**\n * lpfc_sli_abort_iocb - issue abort for all commands on a host/target/LUN\n * @vport: Pointer to virtual port.\n * @pring: Pointer to driver SLI ring object.\n * @tgt_id: SCSI ID of the target.\n * @lun_id: LUN ID of the scsi device.\n * @abort_cmd: LPFC_CTX_LUN/LPFC_CTX_TGT/LPFC_CTX_HOST.\n *\n * This function sends an abort command for every SCSI command\n * associated with the given virtual port pending on the ring\n * filtered by lpfc_sli_validate_fcp_iocb function.\n * When abort_cmd == LPFC_CTX_LUN, the function sends abort only to the\n * FCP iocbs associated with lun specified by tgt_id and lun_id\n * parameters\n * When abort_cmd == LPFC_CTX_TGT, the function sends abort only to the\n * FCP iocbs associated with SCSI target specified by tgt_id parameter.\n * When abort_cmd == LPFC_CTX_HOST, the function sends abort to all\n * FCP iocbs associated with virtual port.\n * This function returns number of iocbs it failed to abort.\n * This function is called with no locks held.\n **/\nint\nlpfc_sli_abort_iocb(struct lpfc_vport *vport, struct lpfc_sli_ring *pring,\n\t\t    uint16_t tgt_id, uint64_t lun_id, lpfc_ctx_cmd abort_cmd)\n{\n\tstruct lpfc_hba *phba = vport->phba;\n\tstruct lpfc_iocbq *iocbq;\n\tstruct lpfc_iocbq *abtsiocb;\n\tstruct lpfc_sli_ring *pring_s4;\n\tIOCB_t *cmd = NULL;\n\tint errcnt = 0, ret_val = 0;\n\tint i;\n\n\t/* all I/Os are in process of being flushed */\n\tif (phba->hba_flag & HBA_IOQ_FLUSH)\n\t\treturn errcnt;\n\n\tfor (i = 1; i <= phba->sli.last_iotag; i++) {\n\t\tiocbq = phba->sli.iocbq_lookup[i];\n\n\t\tif (lpfc_sli_validate_fcp_iocb(iocbq, vport, tgt_id, lun_id,\n\t\t\t\t\t       abort_cmd) != 0)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * If the iocbq is already being aborted, don't take a second\n\t\t * action, but do count it.\n\t\t */\n\t\tif (iocbq->iocb_flag & LPFC_DRIVER_ABORTED)\n\t\t\tcontinue;\n\n\t\t/* issue ABTS for this IOCB based on iotag */\n\t\tabtsiocb = lpfc_sli_get_iocbq(phba);\n\t\tif (abtsiocb == NULL) {\n\t\t\terrcnt++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* indicate the IO is being aborted by the driver. */\n\t\tiocbq->iocb_flag |= LPFC_DRIVER_ABORTED;\n\n\t\tcmd = &iocbq->iocb;\n\t\tabtsiocb->iocb.un.acxri.abortType = ABORT_TYPE_ABTS;\n\t\tabtsiocb->iocb.un.acxri.abortContextTag = cmd->ulpContext;\n\t\tif (phba->sli_rev == LPFC_SLI_REV4)\n\t\t\tabtsiocb->iocb.un.acxri.abortIoTag = iocbq->sli4_xritag;\n\t\telse\n\t\t\tabtsiocb->iocb.un.acxri.abortIoTag = cmd->ulpIoTag;\n\t\tabtsiocb->iocb.ulpLe = 1;\n\t\tabtsiocb->iocb.ulpClass = cmd->ulpClass;\n\t\tabtsiocb->vport = vport;\n\n\t\t/* ABTS WQE must go to the same WQ as the WQE to be aborted */\n\t\tabtsiocb->hba_wqidx = iocbq->hba_wqidx;\n\t\tif (iocbq->iocb_flag & LPFC_IO_FCP)\n\t\t\tabtsiocb->iocb_flag |= LPFC_USE_FCPWQIDX;\n\t\tif (iocbq->iocb_flag & LPFC_IO_FOF)\n\t\t\tabtsiocb->iocb_flag |= LPFC_IO_FOF;\n\n\t\tif (lpfc_is_link_up(phba))\n\t\t\tabtsiocb->iocb.ulpCommand = CMD_ABORT_XRI_CN;\n\t\telse\n\t\t\tabtsiocb->iocb.ulpCommand = CMD_CLOSE_XRI_CN;\n\n\t\t/* Setup callback routine and issue the command. */\n\t\tabtsiocb->iocb_cmpl = lpfc_sli_abort_fcp_cmpl;\n\t\tif (phba->sli_rev == LPFC_SLI_REV4) {\n\t\t\tpring_s4 = lpfc_sli4_calc_ring(phba, iocbq);\n\t\t\tif (!pring_s4)\n\t\t\t\tcontinue;\n\t\t\tret_val = lpfc_sli_issue_iocb(phba, pring_s4->ringno,\n\t\t\t\t\t\t      abtsiocb, 0);\n\t\t} else\n\t\t\tret_val = lpfc_sli_issue_iocb(phba, pring->ringno,\n\t\t\t\t\t\t      abtsiocb, 0);\n\t\tif (ret_val == IOCB_ERROR) {\n\t\t\tlpfc_sli_release_iocbq(phba, abtsiocb);\n\t\t\terrcnt++;\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\treturn errcnt;\n}\n\n/**\n * lpfc_sli_abort_taskmgmt - issue abort for all commands on a host/target/LUN\n * @vport: Pointer to virtual port.\n * @pring: Pointer to driver SLI ring object.\n * @tgt_id: SCSI ID of the target.\n * @lun_id: LUN ID of the scsi device.\n * @cmd: LPFC_CTX_LUN/LPFC_CTX_TGT/LPFC_CTX_HOST.\n *\n * This function sends an abort command for every SCSI command\n * associated with the given virtual port pending on the ring\n * filtered by lpfc_sli_validate_fcp_iocb function.\n * When taskmgmt_cmd == LPFC_CTX_LUN, the function sends abort only to the\n * FCP iocbs associated with lun specified by tgt_id and lun_id\n * parameters\n * When taskmgmt_cmd == LPFC_CTX_TGT, the function sends abort only to the\n * FCP iocbs associated with SCSI target specified by tgt_id parameter.\n * When taskmgmt_cmd == LPFC_CTX_HOST, the function sends abort to all\n * FCP iocbs associated with virtual port.\n * This function returns number of iocbs it aborted .\n * This function is called with no locks held right after a taskmgmt\n * command is sent.\n **/\nint\nlpfc_sli_abort_taskmgmt(struct lpfc_vport *vport, struct lpfc_sli_ring *pring,\n\t\t\tuint16_t tgt_id, uint64_t lun_id, lpfc_ctx_cmd cmd)\n{\n\tstruct lpfc_hba *phba = vport->phba;\n\tstruct lpfc_io_buf *lpfc_cmd;\n\tstruct lpfc_iocbq *abtsiocbq;\n\tstruct lpfc_nodelist *ndlp;\n\tstruct lpfc_iocbq *iocbq;\n\tIOCB_t *icmd;\n\tint sum, i, ret_val;\n\tunsigned long iflags;\n\tstruct lpfc_sli_ring *pring_s4 = NULL;\n\n\tspin_lock_irqsave(&phba->hbalock, iflags);\n\n\t/* all I/Os are in process of being flushed */\n\tif (phba->hba_flag & HBA_IOQ_FLUSH) {\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t\treturn 0;\n\t}\n\tsum = 0;\n\n\tfor (i = 1; i <= phba->sli.last_iotag; i++) {\n\t\tiocbq = phba->sli.iocbq_lookup[i];\n\n\t\tif (lpfc_sli_validate_fcp_iocb(iocbq, vport, tgt_id, lun_id,\n\t\t\t\t\t       cmd) != 0)\n\t\t\tcontinue;\n\n\t\t/* Guard against IO completion being called at same time */\n\t\tlpfc_cmd = container_of(iocbq, struct lpfc_io_buf, cur_iocbq);\n\t\tspin_lock(&lpfc_cmd->buf_lock);\n\n\t\tif (!lpfc_cmd->pCmd) {\n\t\t\tspin_unlock(&lpfc_cmd->buf_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (phba->sli_rev == LPFC_SLI_REV4) {\n\t\t\tpring_s4 =\n\t\t\t    phba->sli4_hba.hdwq[iocbq->hba_wqidx].io_wq->pring;\n\t\t\tif (!pring_s4) {\n\t\t\t\tspin_unlock(&lpfc_cmd->buf_lock);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t/* Note: both hbalock and ring_lock must be set here */\n\t\t\tspin_lock(&pring_s4->ring_lock);\n\t\t}\n\n\t\t/*\n\t\t * If the iocbq is already being aborted, don't take a second\n\t\t * action, but do count it.\n\t\t */\n\t\tif ((iocbq->iocb_flag & LPFC_DRIVER_ABORTED) ||\n\t\t    !(iocbq->iocb_flag & LPFC_IO_ON_TXCMPLQ)) {\n\t\t\tif (phba->sli_rev == LPFC_SLI_REV4)\n\t\t\t\tspin_unlock(&pring_s4->ring_lock);\n\t\t\tspin_unlock(&lpfc_cmd->buf_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* issue ABTS for this IOCB based on iotag */\n\t\tabtsiocbq = __lpfc_sli_get_iocbq(phba);\n\t\tif (!abtsiocbq) {\n\t\t\tif (phba->sli_rev == LPFC_SLI_REV4)\n\t\t\t\tspin_unlock(&pring_s4->ring_lock);\n\t\t\tspin_unlock(&lpfc_cmd->buf_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\ticmd = &iocbq->iocb;\n\t\tabtsiocbq->iocb.un.acxri.abortType = ABORT_TYPE_ABTS;\n\t\tabtsiocbq->iocb.un.acxri.abortContextTag = icmd->ulpContext;\n\t\tif (phba->sli_rev == LPFC_SLI_REV4)\n\t\t\tabtsiocbq->iocb.un.acxri.abortIoTag =\n\t\t\t\t\t\t\t iocbq->sli4_xritag;\n\t\telse\n\t\t\tabtsiocbq->iocb.un.acxri.abortIoTag = icmd->ulpIoTag;\n\t\tabtsiocbq->iocb.ulpLe = 1;\n\t\tabtsiocbq->iocb.ulpClass = icmd->ulpClass;\n\t\tabtsiocbq->vport = vport;\n\n\t\t/* ABTS WQE must go to the same WQ as the WQE to be aborted */\n\t\tabtsiocbq->hba_wqidx = iocbq->hba_wqidx;\n\t\tif (iocbq->iocb_flag & LPFC_IO_FCP)\n\t\t\tabtsiocbq->iocb_flag |= LPFC_USE_FCPWQIDX;\n\t\tif (iocbq->iocb_flag & LPFC_IO_FOF)\n\t\t\tabtsiocbq->iocb_flag |= LPFC_IO_FOF;\n\n\t\tndlp = lpfc_cmd->rdata->pnode;\n\n\t\tif (lpfc_is_link_up(phba) &&\n\t\t    (ndlp && ndlp->nlp_state == NLP_STE_MAPPED_NODE))\n\t\t\tabtsiocbq->iocb.ulpCommand = CMD_ABORT_XRI_CN;\n\t\telse\n\t\t\tabtsiocbq->iocb.ulpCommand = CMD_CLOSE_XRI_CN;\n\n\t\t/* Setup callback routine and issue the command. */\n\t\tabtsiocbq->iocb_cmpl = lpfc_sli_abort_fcp_cmpl;\n\n\t\t/*\n\t\t * Indicate the IO is being aborted by the driver and set\n\t\t * the caller's flag into the aborted IO.\n\t\t */\n\t\tiocbq->iocb_flag |= LPFC_DRIVER_ABORTED;\n\n\t\tif (phba->sli_rev == LPFC_SLI_REV4) {\n\t\t\tret_val = __lpfc_sli_issue_iocb(phba, pring_s4->ringno,\n\t\t\t\t\t\t\tabtsiocbq, 0);\n\t\t\tspin_unlock(&pring_s4->ring_lock);\n\t\t} else {\n\t\t\tret_val = __lpfc_sli_issue_iocb(phba, pring->ringno,\n\t\t\t\t\t\t\tabtsiocbq, 0);\n\t\t}\n\n\t\tspin_unlock(&lpfc_cmd->buf_lock);\n\n\t\tif (ret_val == IOCB_ERROR)\n\t\t\t__lpfc_sli_release_iocbq(phba, abtsiocbq);\n\t\telse\n\t\t\tsum++;\n\t}\n\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\treturn sum;\n}\n\n/**\n * lpfc_sli_wake_iocb_wait - lpfc_sli_issue_iocb_wait's completion handler\n * @phba: Pointer to HBA context object.\n * @cmdiocbq: Pointer to command iocb.\n * @rspiocbq: Pointer to response iocb.\n *\n * This function is the completion handler for iocbs issued using\n * lpfc_sli_issue_iocb_wait function. This function is called by the\n * ring event handler function without any lock held. This function\n * can be called from both worker thread context and interrupt\n * context. This function also can be called from other thread which\n * cleans up the SLI layer objects.\n * This function copy the contents of the response iocb to the\n * response iocb memory object provided by the caller of\n * lpfc_sli_issue_iocb_wait and then wakes up the thread which\n * sleeps for the iocb completion.\n **/\nstatic void\nlpfc_sli_wake_iocb_wait(struct lpfc_hba *phba,\n\t\t\tstruct lpfc_iocbq *cmdiocbq,\n\t\t\tstruct lpfc_iocbq *rspiocbq)\n{\n\twait_queue_head_t *pdone_q;\n\tunsigned long iflags;\n\tstruct lpfc_io_buf *lpfc_cmd;\n\n\tspin_lock_irqsave(&phba->hbalock, iflags);\n\tif (cmdiocbq->iocb_flag & LPFC_IO_WAKE_TMO) {\n\n\t\t/*\n\t\t * A time out has occurred for the iocb.  If a time out\n\t\t * completion handler has been supplied, call it.  Otherwise,\n\t\t * just free the iocbq.\n\t\t */\n\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t\tcmdiocbq->iocb_cmpl = cmdiocbq->wait_iocb_cmpl;\n\t\tcmdiocbq->wait_iocb_cmpl = NULL;\n\t\tif (cmdiocbq->iocb_cmpl)\n\t\t\t(cmdiocbq->iocb_cmpl)(phba, cmdiocbq, NULL);\n\t\telse\n\t\t\tlpfc_sli_release_iocbq(phba, cmdiocbq);\n\t\treturn;\n\t}\n\n\tcmdiocbq->iocb_flag |= LPFC_IO_WAKE;\n\tif (cmdiocbq->context2 && rspiocbq)\n\t\tmemcpy(&((struct lpfc_iocbq *)cmdiocbq->context2)->iocb,\n\t\t       &rspiocbq->iocb, sizeof(IOCB_t));\n\n\t/* Set the exchange busy flag for task management commands */\n\tif ((cmdiocbq->iocb_flag & LPFC_IO_FCP) &&\n\t\t!(cmdiocbq->iocb_flag & LPFC_IO_LIBDFC)) {\n\t\tlpfc_cmd = container_of(cmdiocbq, struct lpfc_io_buf,\n\t\t\tcur_iocbq);\n\t\tif (rspiocbq && (rspiocbq->iocb_flag & LPFC_EXCHANGE_BUSY))\n\t\t\tlpfc_cmd->flags |= LPFC_SBUF_XBUSY;\n\t\telse\n\t\t\tlpfc_cmd->flags &= ~LPFC_SBUF_XBUSY;\n\t}\n\n\tpdone_q = cmdiocbq->context_un.wait_queue;\n\tif (pdone_q)\n\t\twake_up(pdone_q);\n\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\treturn;\n}\n\n/**\n * lpfc_chk_iocb_flg - Test IOCB flag with lock held.\n * @phba: Pointer to HBA context object..\n * @piocbq: Pointer to command iocb.\n * @flag: Flag to test.\n *\n * This routine grabs the hbalock and then test the iocb_flag to\n * see if the passed in flag is set.\n * Returns:\n * 1 if flag is set.\n * 0 if flag is not set.\n **/\nstatic int\nlpfc_chk_iocb_flg(struct lpfc_hba *phba,\n\t\t struct lpfc_iocbq *piocbq, uint32_t flag)\n{\n\tunsigned long iflags;\n\tint ret;\n\n\tspin_lock_irqsave(&phba->hbalock, iflags);\n\tret = piocbq->iocb_flag & flag;\n\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\treturn ret;\n\n}\n\n/**\n * lpfc_sli_issue_iocb_wait - Synchronous function to issue iocb commands\n * @phba: Pointer to HBA context object..\n * @ring_number: Ring number\n * @piocb: Pointer to command iocb.\n * @prspiocbq: Pointer to response iocb.\n * @timeout: Timeout in number of seconds.\n *\n * This function issues the iocb to firmware and waits for the\n * iocb to complete. The iocb_cmpl field of the shall be used\n * to handle iocbs which time out. If the field is NULL, the\n * function shall free the iocbq structure.  If more clean up is\n * needed, the caller is expected to provide a completion function\n * that will provide the needed clean up.  If the iocb command is\n * not completed within timeout seconds, the function will either\n * free the iocbq structure (if iocb_cmpl == NULL) or execute the\n * completion function set in the iocb_cmpl field and then return\n * a status of IOCB_TIMEDOUT.  The caller should not free the iocb\n * resources if this function returns IOCB_TIMEDOUT.\n * The function waits for the iocb completion using an\n * non-interruptible wait.\n * This function will sleep while waiting for iocb completion.\n * So, this function should not be called from any context which\n * does not allow sleeping. Due to the same reason, this function\n * cannot be called with interrupt disabled.\n * This function assumes that the iocb completions occur while\n * this function sleep. So, this function cannot be called from\n * the thread which process iocb completion for this ring.\n * This function clears the iocb_flag of the iocb object before\n * issuing the iocb and the iocb completion handler sets this\n * flag and wakes this thread when the iocb completes.\n * The contents of the response iocb will be copied to prspiocbq\n * by the completion handler when the command completes.\n * This function returns IOCB_SUCCESS when success.\n * This function is called with no lock held.\n **/\nint\nlpfc_sli_issue_iocb_wait(struct lpfc_hba *phba,\n\t\t\t uint32_t ring_number,\n\t\t\t struct lpfc_iocbq *piocb,\n\t\t\t struct lpfc_iocbq *prspiocbq,\n\t\t\t uint32_t timeout)\n{\n\tDECLARE_WAIT_QUEUE_HEAD_ONSTACK(done_q);\n\tlong timeleft, timeout_req = 0;\n\tint retval = IOCB_SUCCESS;\n\tuint32_t creg_val;\n\tstruct lpfc_iocbq *iocb;\n\tint txq_cnt = 0;\n\tint txcmplq_cnt = 0;\n\tstruct lpfc_sli_ring *pring;\n\tunsigned long iflags;\n\tbool iocb_completed = true;\n\n\tif (phba->sli_rev >= LPFC_SLI_REV4)\n\t\tpring = lpfc_sli4_calc_ring(phba, piocb);\n\telse\n\t\tpring = &phba->sli.sli3_ring[ring_number];\n\t/*\n\t * If the caller has provided a response iocbq buffer, then context2\n\t * is NULL or its an error.\n\t */\n\tif (prspiocbq) {\n\t\tif (piocb->context2)\n\t\t\treturn IOCB_ERROR;\n\t\tpiocb->context2 = prspiocbq;\n\t}\n\n\tpiocb->wait_iocb_cmpl = piocb->iocb_cmpl;\n\tpiocb->iocb_cmpl = lpfc_sli_wake_iocb_wait;\n\tpiocb->context_un.wait_queue = &done_q;\n\tpiocb->iocb_flag &= ~(LPFC_IO_WAKE | LPFC_IO_WAKE_TMO);\n\n\tif (phba->cfg_poll & DISABLE_FCP_RING_INT) {\n\t\tif (lpfc_readl(phba->HCregaddr, &creg_val))\n\t\t\treturn IOCB_ERROR;\n\t\tcreg_val |= (HC_R0INT_ENA << LPFC_FCP_RING);\n\t\twritel(creg_val, phba->HCregaddr);\n\t\treadl(phba->HCregaddr); /* flush */\n\t}\n\n\tretval = lpfc_sli_issue_iocb(phba, ring_number, piocb,\n\t\t\t\t     SLI_IOCB_RET_IOCB);\n\tif (retval == IOCB_SUCCESS) {\n\t\ttimeout_req = msecs_to_jiffies(timeout * 1000);\n\t\ttimeleft = wait_event_timeout(done_q,\n\t\t\t\tlpfc_chk_iocb_flg(phba, piocb, LPFC_IO_WAKE),\n\t\t\t\ttimeout_req);\n\t\tspin_lock_irqsave(&phba->hbalock, iflags);\n\t\tif (!(piocb->iocb_flag & LPFC_IO_WAKE)) {\n\n\t\t\t/*\n\t\t\t * IOCB timed out.  Inform the wake iocb wait\n\t\t\t * completion function and set local status\n\t\t\t */\n\n\t\t\tiocb_completed = false;\n\t\t\tpiocb->iocb_flag |= LPFC_IO_WAKE_TMO;\n\t\t}\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t\tif (iocb_completed) {\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\t\t\"0331 IOCB wake signaled\\n\");\n\t\t\t/* Note: we are not indicating if the IOCB has a success\n\t\t\t * status or not - that's for the caller to check.\n\t\t\t * IOCB_SUCCESS means just that the command was sent and\n\t\t\t * completed. Not that it completed successfully.\n\t\t\t * */\n\t\t} else if (timeleft == 0) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"0338 IOCB wait timeout error - no \"\n\t\t\t\t\t\"wake response Data x%x\\n\", timeout);\n\t\t\tretval = IOCB_TIMEDOUT;\n\t\t} else {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"0330 IOCB wake NOT set, \"\n\t\t\t\t\t\"Data x%x x%lx\\n\",\n\t\t\t\t\ttimeout, (timeleft / jiffies));\n\t\t\tretval = IOCB_TIMEDOUT;\n\t\t}\n\t} else if (retval == IOCB_BUSY) {\n\t\tif (phba->cfg_log_verbose & LOG_SLI) {\n\t\t\tlist_for_each_entry(iocb, &pring->txq, list) {\n\t\t\t\ttxq_cnt++;\n\t\t\t}\n\t\t\tlist_for_each_entry(iocb, &pring->txcmplq, list) {\n\t\t\t\ttxcmplq_cnt++;\n\t\t\t}\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\t\"2818 Max IOCBs %d txq cnt %d txcmplq cnt %d\\n\",\n\t\t\t\tphba->iocb_cnt, txq_cnt, txcmplq_cnt);\n\t\t}\n\t\treturn retval;\n\t} else {\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\t\"0332 IOCB wait issue failed, Data x%x\\n\",\n\t\t\t\tretval);\n\t\tretval = IOCB_ERROR;\n\t}\n\n\tif (phba->cfg_poll & DISABLE_FCP_RING_INT) {\n\t\tif (lpfc_readl(phba->HCregaddr, &creg_val))\n\t\t\treturn IOCB_ERROR;\n\t\tcreg_val &= ~(HC_R0INT_ENA << LPFC_FCP_RING);\n\t\twritel(creg_val, phba->HCregaddr);\n\t\treadl(phba->HCregaddr); /* flush */\n\t}\n\n\tif (prspiocbq)\n\t\tpiocb->context2 = NULL;\n\n\tpiocb->context_un.wait_queue = NULL;\n\tpiocb->iocb_cmpl = NULL;\n\treturn retval;\n}\n\n/**\n * lpfc_sli_issue_mbox_wait - Synchronous function to issue mailbox\n * @phba: Pointer to HBA context object.\n * @pmboxq: Pointer to driver mailbox object.\n * @timeout: Timeout in number of seconds.\n *\n * This function issues the mailbox to firmware and waits for the\n * mailbox command to complete. If the mailbox command is not\n * completed within timeout seconds, it returns MBX_TIMEOUT.\n * The function waits for the mailbox completion using an\n * interruptible wait. If the thread is woken up due to a\n * signal, MBX_TIMEOUT error is returned to the caller. Caller\n * should not free the mailbox resources, if this function returns\n * MBX_TIMEOUT.\n * This function will sleep while waiting for mailbox completion.\n * So, this function should not be called from any context which\n * does not allow sleeping. Due to the same reason, this function\n * cannot be called with interrupt disabled.\n * This function assumes that the mailbox completion occurs while\n * this function sleep. So, this function cannot be called from\n * the worker thread which processes mailbox completion.\n * This function is called in the context of HBA management\n * applications.\n * This function returns MBX_SUCCESS when successful.\n * This function is called with no lock held.\n **/\nint\nlpfc_sli_issue_mbox_wait(struct lpfc_hba *phba, LPFC_MBOXQ_t *pmboxq,\n\t\t\t uint32_t timeout)\n{\n\tstruct completion mbox_done;\n\tint retval;\n\tunsigned long flag;\n\n\tpmboxq->mbox_flag &= ~LPFC_MBX_WAKE;\n\t/* setup wake call as IOCB callback */\n\tpmboxq->mbox_cmpl = lpfc_sli_wake_mbox_wait;\n\n\t/* setup context3 field to pass wait_queue pointer to wake function  */\n\tinit_completion(&mbox_done);\n\tpmboxq->context3 = &mbox_done;\n\t/* now issue the command */\n\tretval = lpfc_sli_issue_mbox(phba, pmboxq, MBX_NOWAIT);\n\tif (retval == MBX_BUSY || retval == MBX_SUCCESS) {\n\t\twait_for_completion_timeout(&mbox_done,\n\t\t\t\t\t    msecs_to_jiffies(timeout * 1000));\n\n\t\tspin_lock_irqsave(&phba->hbalock, flag);\n\t\tpmboxq->context3 = NULL;\n\t\t/*\n\t\t * if LPFC_MBX_WAKE flag is set the mailbox is completed\n\t\t * else do not free the resources.\n\t\t */\n\t\tif (pmboxq->mbox_flag & LPFC_MBX_WAKE) {\n\t\t\tretval = MBX_SUCCESS;\n\t\t} else {\n\t\t\tretval = MBX_TIMEOUT;\n\t\t\tpmboxq->mbox_cmpl = lpfc_sli_def_mbox_cmpl;\n\t\t}\n\t\tspin_unlock_irqrestore(&phba->hbalock, flag);\n\t}\n\treturn retval;\n}\n\n/**\n * lpfc_sli_mbox_sys_shutdown - shutdown mailbox command sub-system\n * @phba: Pointer to HBA context.\n * @mbx_action: Mailbox shutdown options.\n *\n * This function is called to shutdown the driver's mailbox sub-system.\n * It first marks the mailbox sub-system is in a block state to prevent\n * the asynchronous mailbox command from issued off the pending mailbox\n * command queue. If the mailbox command sub-system shutdown is due to\n * HBA error conditions such as EEH or ERATT, this routine shall invoke\n * the mailbox sub-system flush routine to forcefully bring down the\n * mailbox sub-system. Otherwise, if it is due to normal condition (such\n * as with offline or HBA function reset), this routine will wait for the\n * outstanding mailbox command to complete before invoking the mailbox\n * sub-system flush routine to gracefully bring down mailbox sub-system.\n **/\nvoid\nlpfc_sli_mbox_sys_shutdown(struct lpfc_hba *phba, int mbx_action)\n{\n\tstruct lpfc_sli *psli = &phba->sli;\n\tunsigned long timeout;\n\n\tif (mbx_action == LPFC_MBX_NO_WAIT) {\n\t\t/* delay 100ms for port state */\n\t\tmsleep(100);\n\t\tlpfc_sli_mbox_sys_flush(phba);\n\t\treturn;\n\t}\n\ttimeout = msecs_to_jiffies(LPFC_MBOX_TMO * 1000) + jiffies;\n\n\t/* Disable softirqs, including timers from obtaining phba->hbalock */\n\tlocal_bh_disable();\n\n\tspin_lock_irq(&phba->hbalock);\n\tpsli->sli_flag |= LPFC_SLI_ASYNC_MBX_BLK;\n\n\tif (psli->sli_flag & LPFC_SLI_ACTIVE) {\n\t\t/* Determine how long we might wait for the active mailbox\n\t\t * command to be gracefully completed by firmware.\n\t\t */\n\t\tif (phba->sli.mbox_active)\n\t\t\ttimeout = msecs_to_jiffies(lpfc_mbox_tmo_val(phba,\n\t\t\t\t\t\tphba->sli.mbox_active) *\n\t\t\t\t\t\t1000) + jiffies;\n\t\tspin_unlock_irq(&phba->hbalock);\n\n\t\t/* Enable softirqs again, done with phba->hbalock */\n\t\tlocal_bh_enable();\n\n\t\twhile (phba->sli.mbox_active) {\n\t\t\t/* Check active mailbox complete status every 2ms */\n\t\t\tmsleep(2);\n\t\t\tif (time_after(jiffies, timeout))\n\t\t\t\t/* Timeout, let the mailbox flush routine to\n\t\t\t\t * forcefully release active mailbox command\n\t\t\t\t */\n\t\t\t\tbreak;\n\t\t}\n\t} else {\n\t\tspin_unlock_irq(&phba->hbalock);\n\n\t\t/* Enable softirqs again, done with phba->hbalock */\n\t\tlocal_bh_enable();\n\t}\n\n\tlpfc_sli_mbox_sys_flush(phba);\n}\n\n/**\n * lpfc_sli_eratt_read - read sli-3 error attention events\n * @phba: Pointer to HBA context.\n *\n * This function is called to read the SLI3 device error attention registers\n * for possible error attention events. The caller must hold the hostlock\n * with spin_lock_irq().\n *\n * This function returns 1 when there is Error Attention in the Host Attention\n * Register and returns 0 otherwise.\n **/\nstatic int\nlpfc_sli_eratt_read(struct lpfc_hba *phba)\n{\n\tuint32_t ha_copy;\n\n\t/* Read chip Host Attention (HA) register */\n\tif (lpfc_readl(phba->HAregaddr, &ha_copy))\n\t\tgoto unplug_err;\n\n\tif (ha_copy & HA_ERATT) {\n\t\t/* Read host status register to retrieve error event */\n\t\tif (lpfc_sli_read_hs(phba))\n\t\t\tgoto unplug_err;\n\n\t\t/* Check if there is a deferred error condition is active */\n\t\tif ((HS_FFER1 & phba->work_hs) &&\n\t\t    ((HS_FFER2 | HS_FFER3 | HS_FFER4 | HS_FFER5 |\n\t\t      HS_FFER6 | HS_FFER7 | HS_FFER8) & phba->work_hs)) {\n\t\t\tphba->hba_flag |= DEFER_ERATT;\n\t\t\t/* Clear all interrupt enable conditions */\n\t\t\twritel(0, phba->HCregaddr);\n\t\t\treadl(phba->HCregaddr);\n\t\t}\n\n\t\t/* Set the driver HA work bitmap */\n\t\tphba->work_ha |= HA_ERATT;\n\t\t/* Indicate polling handles this ERATT */\n\t\tphba->hba_flag |= HBA_ERATT_HANDLED;\n\t\treturn 1;\n\t}\n\treturn 0;\n\nunplug_err:\n\t/* Set the driver HS work bitmap */\n\tphba->work_hs |= UNPLUG_ERR;\n\t/* Set the driver HA work bitmap */\n\tphba->work_ha |= HA_ERATT;\n\t/* Indicate polling handles this ERATT */\n\tphba->hba_flag |= HBA_ERATT_HANDLED;\n\treturn 1;\n}\n\n/**\n * lpfc_sli4_eratt_read - read sli-4 error attention events\n * @phba: Pointer to HBA context.\n *\n * This function is called to read the SLI4 device error attention registers\n * for possible error attention events. The caller must hold the hostlock\n * with spin_lock_irq().\n *\n * This function returns 1 when there is Error Attention in the Host Attention\n * Register and returns 0 otherwise.\n **/\nstatic int\nlpfc_sli4_eratt_read(struct lpfc_hba *phba)\n{\n\tuint32_t uerr_sta_hi, uerr_sta_lo;\n\tuint32_t if_type, portsmphr;\n\tstruct lpfc_register portstat_reg;\n\n\t/*\n\t * For now, use the SLI4 device internal unrecoverable error\n\t * registers for error attention. This can be changed later.\n\t */\n\tif_type = bf_get(lpfc_sli_intf_if_type, &phba->sli4_hba.sli_intf);\n\tswitch (if_type) {\n\tcase LPFC_SLI_INTF_IF_TYPE_0:\n\t\tif (lpfc_readl(phba->sli4_hba.u.if_type0.UERRLOregaddr,\n\t\t\t&uerr_sta_lo) ||\n\t\t\tlpfc_readl(phba->sli4_hba.u.if_type0.UERRHIregaddr,\n\t\t\t&uerr_sta_hi)) {\n\t\t\tphba->work_hs |= UNPLUG_ERR;\n\t\t\tphba->work_ha |= HA_ERATT;\n\t\t\tphba->hba_flag |= HBA_ERATT_HANDLED;\n\t\t\treturn 1;\n\t\t}\n\t\tif ((~phba->sli4_hba.ue_mask_lo & uerr_sta_lo) ||\n\t\t    (~phba->sli4_hba.ue_mask_hi & uerr_sta_hi)) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"1423 HBA Unrecoverable error: \"\n\t\t\t\t\t\"uerr_lo_reg=0x%x, uerr_hi_reg=0x%x, \"\n\t\t\t\t\t\"ue_mask_lo_reg=0x%x, \"\n\t\t\t\t\t\"ue_mask_hi_reg=0x%x\\n\",\n\t\t\t\t\tuerr_sta_lo, uerr_sta_hi,\n\t\t\t\t\tphba->sli4_hba.ue_mask_lo,\n\t\t\t\t\tphba->sli4_hba.ue_mask_hi);\n\t\t\tphba->work_status[0] = uerr_sta_lo;\n\t\t\tphba->work_status[1] = uerr_sta_hi;\n\t\t\tphba->work_ha |= HA_ERATT;\n\t\t\tphba->hba_flag |= HBA_ERATT_HANDLED;\n\t\t\treturn 1;\n\t\t}\n\t\tbreak;\n\tcase LPFC_SLI_INTF_IF_TYPE_2:\n\tcase LPFC_SLI_INTF_IF_TYPE_6:\n\t\tif (lpfc_readl(phba->sli4_hba.u.if_type2.STATUSregaddr,\n\t\t\t&portstat_reg.word0) ||\n\t\t\tlpfc_readl(phba->sli4_hba.PSMPHRregaddr,\n\t\t\t&portsmphr)){\n\t\t\tphba->work_hs |= UNPLUG_ERR;\n\t\t\tphba->work_ha |= HA_ERATT;\n\t\t\tphba->hba_flag |= HBA_ERATT_HANDLED;\n\t\t\treturn 1;\n\t\t}\n\t\tif (bf_get(lpfc_sliport_status_err, &portstat_reg)) {\n\t\t\tphba->work_status[0] =\n\t\t\t\treadl(phba->sli4_hba.u.if_type2.ERR1regaddr);\n\t\t\tphba->work_status[1] =\n\t\t\t\treadl(phba->sli4_hba.u.if_type2.ERR2regaddr);\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"2885 Port Status Event: \"\n\t\t\t\t\t\"port status reg 0x%x, \"\n\t\t\t\t\t\"port smphr reg 0x%x, \"\n\t\t\t\t\t\"error 1=0x%x, error 2=0x%x\\n\",\n\t\t\t\t\tportstat_reg.word0,\n\t\t\t\t\tportsmphr,\n\t\t\t\t\tphba->work_status[0],\n\t\t\t\t\tphba->work_status[1]);\n\t\t\tphba->work_ha |= HA_ERATT;\n\t\t\tphba->hba_flag |= HBA_ERATT_HANDLED;\n\t\t\treturn 1;\n\t\t}\n\t\tbreak;\n\tcase LPFC_SLI_INTF_IF_TYPE_1:\n\tdefault:\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2886 HBA Error Attention on unsupported \"\n\t\t\t\t\"if type %d.\", if_type);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n/**\n * lpfc_sli_check_eratt - check error attention events\n * @phba: Pointer to HBA context.\n *\n * This function is called from timer soft interrupt context to check HBA's\n * error attention register bit for error attention events.\n *\n * This function returns 1 when there is Error Attention in the Host Attention\n * Register and returns 0 otherwise.\n **/\nint\nlpfc_sli_check_eratt(struct lpfc_hba *phba)\n{\n\tuint32_t ha_copy;\n\n\t/* If somebody is waiting to handle an eratt, don't process it\n\t * here. The brdkill function will do this.\n\t */\n\tif (phba->link_flag & LS_IGNORE_ERATT)\n\t\treturn 0;\n\n\t/* Check if interrupt handler handles this ERATT */\n\tspin_lock_irq(&phba->hbalock);\n\tif (phba->hba_flag & HBA_ERATT_HANDLED) {\n\t\t/* Interrupt handler has handled ERATT */\n\t\tspin_unlock_irq(&phba->hbalock);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * If there is deferred error attention, do not check for error\n\t * attention\n\t */\n\tif (unlikely(phba->hba_flag & DEFER_ERATT)) {\n\t\tspin_unlock_irq(&phba->hbalock);\n\t\treturn 0;\n\t}\n\n\t/* If PCI channel is offline, don't process it */\n\tif (unlikely(pci_channel_offline(phba->pcidev))) {\n\t\tspin_unlock_irq(&phba->hbalock);\n\t\treturn 0;\n\t}\n\n\tswitch (phba->sli_rev) {\n\tcase LPFC_SLI_REV2:\n\tcase LPFC_SLI_REV3:\n\t\t/* Read chip Host Attention (HA) register */\n\t\tha_copy = lpfc_sli_eratt_read(phba);\n\t\tbreak;\n\tcase LPFC_SLI_REV4:\n\t\t/* Read device Uncoverable Error (UERR) registers */\n\t\tha_copy = lpfc_sli4_eratt_read(phba);\n\t\tbreak;\n\tdefault:\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0299 Invalid SLI revision (%d)\\n\",\n\t\t\t\tphba->sli_rev);\n\t\tha_copy = 0;\n\t\tbreak;\n\t}\n\tspin_unlock_irq(&phba->hbalock);\n\n\treturn ha_copy;\n}\n\n/**\n * lpfc_intr_state_check - Check device state for interrupt handling\n * @phba: Pointer to HBA context.\n *\n * This inline routine checks whether a device or its PCI slot is in a state\n * that the interrupt should be handled.\n *\n * This function returns 0 if the device or the PCI slot is in a state that\n * interrupt should be handled, otherwise -EIO.\n */\nstatic inline int\nlpfc_intr_state_check(struct lpfc_hba *phba)\n{\n\t/* If the pci channel is offline, ignore all the interrupts */\n\tif (unlikely(pci_channel_offline(phba->pcidev)))\n\t\treturn -EIO;\n\n\t/* Update device level interrupt statistics */\n\tphba->sli.slistat.sli_intr++;\n\n\t/* Ignore all interrupts during initialization. */\n\tif (unlikely(phba->link_state < LPFC_LINK_DOWN))\n\t\treturn -EIO;\n\n\treturn 0;\n}\n\n/**\n * lpfc_sli_sp_intr_handler - Slow-path interrupt handler to SLI-3 device\n * @irq: Interrupt number.\n * @dev_id: The device context pointer.\n *\n * This function is directly called from the PCI layer as an interrupt\n * service routine when device with SLI-3 interface spec is enabled with\n * MSI-X multi-message interrupt mode and there are slow-path events in\n * the HBA. However, when the device is enabled with either MSI or Pin-IRQ\n * interrupt mode, this function is called as part of the device-level\n * interrupt handler. When the PCI slot is in error recovery or the HBA\n * is undergoing initialization, the interrupt handler will not process\n * the interrupt. The link attention and ELS ring attention events are\n * handled by the worker thread. The interrupt handler signals the worker\n * thread and returns for these events. This function is called without\n * any lock held. It gets the hbalock to access and update SLI data\n * structures.\n *\n * This function returns IRQ_HANDLED when interrupt is handled else it\n * returns IRQ_NONE.\n **/\nirqreturn_t\nlpfc_sli_sp_intr_handler(int irq, void *dev_id)\n{\n\tstruct lpfc_hba  *phba;\n\tuint32_t ha_copy, hc_copy;\n\tuint32_t work_ha_copy;\n\tunsigned long status;\n\tunsigned long iflag;\n\tuint32_t control;\n\n\tMAILBOX_t *mbox, *pmbox;\n\tstruct lpfc_vport *vport;\n\tstruct lpfc_nodelist *ndlp;\n\tstruct lpfc_dmabuf *mp;\n\tLPFC_MBOXQ_t *pmb;\n\tint rc;\n\n\t/*\n\t * Get the driver's phba structure from the dev_id and\n\t * assume the HBA is not interrupting.\n\t */\n\tphba = (struct lpfc_hba *)dev_id;\n\n\tif (unlikely(!phba))\n\t\treturn IRQ_NONE;\n\n\t/*\n\t * Stuff needs to be attented to when this function is invoked as an\n\t * individual interrupt handler in MSI-X multi-message interrupt mode\n\t */\n\tif (phba->intr_type == MSIX) {\n\t\t/* Check device state for handling interrupt */\n\t\tif (lpfc_intr_state_check(phba))\n\t\t\treturn IRQ_NONE;\n\t\t/* Need to read HA REG for slow-path events */\n\t\tspin_lock_irqsave(&phba->hbalock, iflag);\n\t\tif (lpfc_readl(phba->HAregaddr, &ha_copy))\n\t\t\tgoto unplug_error;\n\t\t/* If somebody is waiting to handle an eratt don't process it\n\t\t * here. The brdkill function will do this.\n\t\t */\n\t\tif (phba->link_flag & LS_IGNORE_ERATT)\n\t\t\tha_copy &= ~HA_ERATT;\n\t\t/* Check the need for handling ERATT in interrupt handler */\n\t\tif (ha_copy & HA_ERATT) {\n\t\t\tif (phba->hba_flag & HBA_ERATT_HANDLED)\n\t\t\t\t/* ERATT polling has handled ERATT */\n\t\t\t\tha_copy &= ~HA_ERATT;\n\t\t\telse\n\t\t\t\t/* Indicate interrupt handler handles ERATT */\n\t\t\t\tphba->hba_flag |= HBA_ERATT_HANDLED;\n\t\t}\n\n\t\t/*\n\t\t * If there is deferred error attention, do not check for any\n\t\t * interrupt.\n\t\t */\n\t\tif (unlikely(phba->hba_flag & DEFER_ERATT)) {\n\t\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\t\treturn IRQ_NONE;\n\t\t}\n\n\t\t/* Clear up only attention source related to slow-path */\n\t\tif (lpfc_readl(phba->HCregaddr, &hc_copy))\n\t\t\tgoto unplug_error;\n\n\t\twritel(hc_copy & ~(HC_MBINT_ENA | HC_R2INT_ENA |\n\t\t\tHC_LAINT_ENA | HC_ERINT_ENA),\n\t\t\tphba->HCregaddr);\n\t\twritel((ha_copy & (HA_MBATT | HA_R2_CLR_MSK)),\n\t\t\tphba->HAregaddr);\n\t\twritel(hc_copy, phba->HCregaddr);\n\t\treadl(phba->HAregaddr); /* flush */\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t} else\n\t\tha_copy = phba->ha_copy;\n\n\twork_ha_copy = ha_copy & phba->work_ha_mask;\n\n\tif (work_ha_copy) {\n\t\tif (work_ha_copy & HA_LATT) {\n\t\t\tif (phba->sli.sli_flag & LPFC_PROCESS_LA) {\n\t\t\t\t/*\n\t\t\t\t * Turn off Link Attention interrupts\n\t\t\t\t * until CLEAR_LA done\n\t\t\t\t */\n\t\t\t\tspin_lock_irqsave(&phba->hbalock, iflag);\n\t\t\t\tphba->sli.sli_flag &= ~LPFC_PROCESS_LA;\n\t\t\t\tif (lpfc_readl(phba->HCregaddr, &control))\n\t\t\t\t\tgoto unplug_error;\n\t\t\t\tcontrol &= ~HC_LAINT_ENA;\n\t\t\t\twritel(control, phba->HCregaddr);\n\t\t\t\treadl(phba->HCregaddr); /* flush */\n\t\t\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\t\t}\n\t\t\telse\n\t\t\t\twork_ha_copy &= ~HA_LATT;\n\t\t}\n\n\t\tif (work_ha_copy & ~(HA_ERATT | HA_MBATT | HA_LATT)) {\n\t\t\t/*\n\t\t\t * Turn off Slow Rings interrupts, LPFC_ELS_RING is\n\t\t\t * the only slow ring.\n\t\t\t */\n\t\t\tstatus = (work_ha_copy &\n\t\t\t\t(HA_RXMASK  << (4*LPFC_ELS_RING)));\n\t\t\tstatus >>= (4*LPFC_ELS_RING);\n\t\t\tif (status & HA_RXMASK) {\n\t\t\t\tspin_lock_irqsave(&phba->hbalock, iflag);\n\t\t\t\tif (lpfc_readl(phba->HCregaddr, &control))\n\t\t\t\t\tgoto unplug_error;\n\n\t\t\t\tlpfc_debugfs_slow_ring_trc(phba,\n\t\t\t\t\"ISR slow ring:   ctl:x%x stat:x%x isrcnt:x%x\",\n\t\t\t\tcontrol, status,\n\t\t\t\t(uint32_t)phba->sli.slistat.sli_intr);\n\n\t\t\t\tif (control & (HC_R0INT_ENA << LPFC_ELS_RING)) {\n\t\t\t\t\tlpfc_debugfs_slow_ring_trc(phba,\n\t\t\t\t\t\t\"ISR Disable ring:\"\n\t\t\t\t\t\t\"pwork:x%x hawork:x%x wait:x%x\",\n\t\t\t\t\t\tphba->work_ha, work_ha_copy,\n\t\t\t\t\t\t(uint32_t)((unsigned long)\n\t\t\t\t\t\t&phba->work_waitq));\n\n\t\t\t\t\tcontrol &=\n\t\t\t\t\t    ~(HC_R0INT_ENA << LPFC_ELS_RING);\n\t\t\t\t\twritel(control, phba->HCregaddr);\n\t\t\t\t\treadl(phba->HCregaddr); /* flush */\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tlpfc_debugfs_slow_ring_trc(phba,\n\t\t\t\t\t\t\"ISR slow ring:   pwork:\"\n\t\t\t\t\t\t\"x%x hawork:x%x wait:x%x\",\n\t\t\t\t\t\tphba->work_ha, work_ha_copy,\n\t\t\t\t\t\t(uint32_t)((unsigned long)\n\t\t\t\t\t\t&phba->work_waitq));\n\t\t\t\t}\n\t\t\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\t\t}\n\t\t}\n\t\tspin_lock_irqsave(&phba->hbalock, iflag);\n\t\tif (work_ha_copy & HA_ERATT) {\n\t\t\tif (lpfc_sli_read_hs(phba))\n\t\t\t\tgoto unplug_error;\n\t\t\t/*\n\t\t\t * Check if there is a deferred error condition\n\t\t\t * is active\n\t\t\t */\n\t\t\tif ((HS_FFER1 & phba->work_hs) &&\n\t\t\t\t((HS_FFER2 | HS_FFER3 | HS_FFER4 | HS_FFER5 |\n\t\t\t\t  HS_FFER6 | HS_FFER7 | HS_FFER8) &\n\t\t\t\t  phba->work_hs)) {\n\t\t\t\tphba->hba_flag |= DEFER_ERATT;\n\t\t\t\t/* Clear all interrupt enable conditions */\n\t\t\t\twritel(0, phba->HCregaddr);\n\t\t\t\treadl(phba->HCregaddr);\n\t\t\t}\n\t\t}\n\n\t\tif ((work_ha_copy & HA_MBATT) && (phba->sli.mbox_active)) {\n\t\t\tpmb = phba->sli.mbox_active;\n\t\t\tpmbox = &pmb->u.mb;\n\t\t\tmbox = phba->mbox;\n\t\t\tvport = pmb->vport;\n\n\t\t\t/* First check out the status word */\n\t\t\tlpfc_sli_pcimem_bcopy(mbox, pmbox, sizeof(uint32_t));\n\t\t\tif (pmbox->mbxOwner != OWN_HOST) {\n\t\t\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\t\t\t/*\n\t\t\t\t * Stray Mailbox Interrupt, mbxCommand <cmd>\n\t\t\t\t * mbxStatus <status>\n\t\t\t\t */\n\t\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\t\"(%d):0304 Stray Mailbox \"\n\t\t\t\t\t\t\"Interrupt mbxCommand x%x \"\n\t\t\t\t\t\t\"mbxStatus x%x\\n\",\n\t\t\t\t\t\t(vport ? vport->vpi : 0),\n\t\t\t\t\t\tpmbox->mbxCommand,\n\t\t\t\t\t\tpmbox->mbxStatus);\n\t\t\t\t/* clear mailbox attention bit */\n\t\t\t\twork_ha_copy &= ~HA_MBATT;\n\t\t\t} else {\n\t\t\t\tphba->sli.mbox_active = NULL;\n\t\t\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\t\t\tphba->last_completion_time = jiffies;\n\t\t\t\tdel_timer(&phba->sli.mbox_tmo);\n\t\t\t\tif (pmb->mbox_cmpl) {\n\t\t\t\t\tlpfc_sli_pcimem_bcopy(mbox, pmbox,\n\t\t\t\t\t\t\tMAILBOX_CMD_SIZE);\n\t\t\t\t\tif (pmb->out_ext_byte_len &&\n\t\t\t\t\t\tpmb->ctx_buf)\n\t\t\t\t\t\tlpfc_sli_pcimem_bcopy(\n\t\t\t\t\t\tphba->mbox_ext,\n\t\t\t\t\t\tpmb->ctx_buf,\n\t\t\t\t\t\tpmb->out_ext_byte_len);\n\t\t\t\t}\n\t\t\t\tif (pmb->mbox_flag & LPFC_MBX_IMED_UNREG) {\n\t\t\t\t\tpmb->mbox_flag &= ~LPFC_MBX_IMED_UNREG;\n\n\t\t\t\t\tlpfc_debugfs_disc_trc(vport,\n\t\t\t\t\t\tLPFC_DISC_TRC_MBOX_VPORT,\n\t\t\t\t\t\t\"MBOX dflt rpi: : \"\n\t\t\t\t\t\t\"status:x%x rpi:x%x\",\n\t\t\t\t\t\t(uint32_t)pmbox->mbxStatus,\n\t\t\t\t\t\tpmbox->un.varWords[0], 0);\n\n\t\t\t\t\tif (!pmbox->mbxStatus) {\n\t\t\t\t\t\tmp = (struct lpfc_dmabuf *)\n\t\t\t\t\t\t\t(pmb->ctx_buf);\n\t\t\t\t\t\tndlp = (struct lpfc_nodelist *)\n\t\t\t\t\t\t\tpmb->ctx_ndlp;\n\n\t\t\t\t\t\t/* Reg_LOGIN of dflt RPI was\n\t\t\t\t\t\t * successful. new lets get\n\t\t\t\t\t\t * rid of the RPI using the\n\t\t\t\t\t\t * same mbox buffer.\n\t\t\t\t\t\t */\n\t\t\t\t\t\tlpfc_unreg_login(phba,\n\t\t\t\t\t\t\tvport->vpi,\n\t\t\t\t\t\t\tpmbox->un.varWords[0],\n\t\t\t\t\t\t\tpmb);\n\t\t\t\t\t\tpmb->mbox_cmpl =\n\t\t\t\t\t\t\tlpfc_mbx_cmpl_dflt_rpi;\n\t\t\t\t\t\tpmb->ctx_buf = mp;\n\t\t\t\t\t\tpmb->ctx_ndlp = ndlp;\n\t\t\t\t\t\tpmb->vport = vport;\n\t\t\t\t\t\trc = lpfc_sli_issue_mbox(phba,\n\t\t\t\t\t\t\t\tpmb,\n\t\t\t\t\t\t\t\tMBX_NOWAIT);\n\t\t\t\t\t\tif (rc != MBX_BUSY)\n\t\t\t\t\t\t\tlpfc_printf_log(phba,\n\t\t\t\t\t\t\tKERN_ERR,\n\t\t\t\t\t\t\tLOG_TRACE_EVENT,\n\t\t\t\t\t\t\t\"0350 rc should have\"\n\t\t\t\t\t\t\t\"been MBX_BUSY\\n\");\n\t\t\t\t\t\tif (rc != MBX_NOT_FINISHED)\n\t\t\t\t\t\t\tgoto send_current_mbox;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tspin_lock_irqsave(\n\t\t\t\t\t\t&phba->pport->work_port_lock,\n\t\t\t\t\t\tiflag);\n\t\t\t\tphba->pport->work_port_events &=\n\t\t\t\t\t~WORKER_MBOX_TMO;\n\t\t\t\tspin_unlock_irqrestore(\n\t\t\t\t\t\t&phba->pport->work_port_lock,\n\t\t\t\t\t\tiflag);\n\t\t\t\tlpfc_mbox_cmpl_put(phba, pmb);\n\t\t\t}\n\t\t} else\n\t\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\n\t\tif ((work_ha_copy & HA_MBATT) &&\n\t\t    (phba->sli.mbox_active == NULL)) {\nsend_current_mbox:\n\t\t\t/* Process next mailbox command if there is one */\n\t\t\tdo {\n\t\t\t\trc = lpfc_sli_issue_mbox(phba, NULL,\n\t\t\t\t\t\t\t MBX_NOWAIT);\n\t\t\t} while (rc == MBX_NOT_FINISHED);\n\t\t\tif (rc != MBX_SUCCESS)\n\t\t\t\tlpfc_printf_log(phba, KERN_ERR,\n\t\t\t\t\t\tLOG_TRACE_EVENT,\n\t\t\t\t\t\t\"0349 rc should be \"\n\t\t\t\t\t\t\"MBX_SUCCESS\\n\");\n\t\t}\n\n\t\tspin_lock_irqsave(&phba->hbalock, iflag);\n\t\tphba->work_ha |= work_ha_copy;\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\tlpfc_worker_wake_up(phba);\n\t}\n\treturn IRQ_HANDLED;\nunplug_error:\n\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\treturn IRQ_HANDLED;\n\n} /* lpfc_sli_sp_intr_handler */\n\n/**\n * lpfc_sli_fp_intr_handler - Fast-path interrupt handler to SLI-3 device.\n * @irq: Interrupt number.\n * @dev_id: The device context pointer.\n *\n * This function is directly called from the PCI layer as an interrupt\n * service routine when device with SLI-3 interface spec is enabled with\n * MSI-X multi-message interrupt mode and there is a fast-path FCP IOCB\n * ring event in the HBA. However, when the device is enabled with either\n * MSI or Pin-IRQ interrupt mode, this function is called as part of the\n * device-level interrupt handler. When the PCI slot is in error recovery\n * or the HBA is undergoing initialization, the interrupt handler will not\n * process the interrupt. The SCSI FCP fast-path ring event are handled in\n * the intrrupt context. This function is called without any lock held.\n * It gets the hbalock to access and update SLI data structures.\n *\n * This function returns IRQ_HANDLED when interrupt is handled else it\n * returns IRQ_NONE.\n **/\nirqreturn_t\nlpfc_sli_fp_intr_handler(int irq, void *dev_id)\n{\n\tstruct lpfc_hba  *phba;\n\tuint32_t ha_copy;\n\tunsigned long status;\n\tunsigned long iflag;\n\tstruct lpfc_sli_ring *pring;\n\n\t/* Get the driver's phba structure from the dev_id and\n\t * assume the HBA is not interrupting.\n\t */\n\tphba = (struct lpfc_hba *) dev_id;\n\n\tif (unlikely(!phba))\n\t\treturn IRQ_NONE;\n\n\t/*\n\t * Stuff needs to be attented to when this function is invoked as an\n\t * individual interrupt handler in MSI-X multi-message interrupt mode\n\t */\n\tif (phba->intr_type == MSIX) {\n\t\t/* Check device state for handling interrupt */\n\t\tif (lpfc_intr_state_check(phba))\n\t\t\treturn IRQ_NONE;\n\t\t/* Need to read HA REG for FCP ring and other ring events */\n\t\tif (lpfc_readl(phba->HAregaddr, &ha_copy))\n\t\t\treturn IRQ_HANDLED;\n\t\t/* Clear up only attention source related to fast-path */\n\t\tspin_lock_irqsave(&phba->hbalock, iflag);\n\t\t/*\n\t\t * If there is deferred error attention, do not check for\n\t\t * any interrupt.\n\t\t */\n\t\tif (unlikely(phba->hba_flag & DEFER_ERATT)) {\n\t\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\t\treturn IRQ_NONE;\n\t\t}\n\t\twritel((ha_copy & (HA_R0_CLR_MSK | HA_R1_CLR_MSK)),\n\t\t\tphba->HAregaddr);\n\t\treadl(phba->HAregaddr); /* flush */\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t} else\n\t\tha_copy = phba->ha_copy;\n\n\t/*\n\t * Process all events on FCP ring. Take the optimized path for FCP IO.\n\t */\n\tha_copy &= ~(phba->work_ha_mask);\n\n\tstatus = (ha_copy & (HA_RXMASK << (4*LPFC_FCP_RING)));\n\tstatus >>= (4*LPFC_FCP_RING);\n\tpring = &phba->sli.sli3_ring[LPFC_FCP_RING];\n\tif (status & HA_RXMASK)\n\t\tlpfc_sli_handle_fast_ring_event(phba, pring, status);\n\n\tif (phba->cfg_multi_ring_support == 2) {\n\t\t/*\n\t\t * Process all events on extra ring. Take the optimized path\n\t\t * for extra ring IO.\n\t\t */\n\t\tstatus = (ha_copy & (HA_RXMASK << (4*LPFC_EXTRA_RING)));\n\t\tstatus >>= (4*LPFC_EXTRA_RING);\n\t\tif (status & HA_RXMASK) {\n\t\t\tlpfc_sli_handle_fast_ring_event(phba,\n\t\t\t\t\t&phba->sli.sli3_ring[LPFC_EXTRA_RING],\n\t\t\t\t\tstatus);\n\t\t}\n\t}\n\treturn IRQ_HANDLED;\n}  /* lpfc_sli_fp_intr_handler */\n\n/**\n * lpfc_sli_intr_handler - Device-level interrupt handler to SLI-3 device\n * @irq: Interrupt number.\n * @dev_id: The device context pointer.\n *\n * This function is the HBA device-level interrupt handler to device with\n * SLI-3 interface spec, called from the PCI layer when either MSI or\n * Pin-IRQ interrupt mode is enabled and there is an event in the HBA which\n * requires driver attention. This function invokes the slow-path interrupt\n * attention handling function and fast-path interrupt attention handling\n * function in turn to process the relevant HBA attention events. This\n * function is called without any lock held. It gets the hbalock to access\n * and update SLI data structures.\n *\n * This function returns IRQ_HANDLED when interrupt is handled, else it\n * returns IRQ_NONE.\n **/\nirqreturn_t\nlpfc_sli_intr_handler(int irq, void *dev_id)\n{\n\tstruct lpfc_hba  *phba;\n\tirqreturn_t sp_irq_rc, fp_irq_rc;\n\tunsigned long status1, status2;\n\tuint32_t hc_copy;\n\n\t/*\n\t * Get the driver's phba structure from the dev_id and\n\t * assume the HBA is not interrupting.\n\t */\n\tphba = (struct lpfc_hba *) dev_id;\n\n\tif (unlikely(!phba))\n\t\treturn IRQ_NONE;\n\n\t/* Check device state for handling interrupt */\n\tif (lpfc_intr_state_check(phba))\n\t\treturn IRQ_NONE;\n\n\tspin_lock(&phba->hbalock);\n\tif (lpfc_readl(phba->HAregaddr, &phba->ha_copy)) {\n\t\tspin_unlock(&phba->hbalock);\n\t\treturn IRQ_HANDLED;\n\t}\n\n\tif (unlikely(!phba->ha_copy)) {\n\t\tspin_unlock(&phba->hbalock);\n\t\treturn IRQ_NONE;\n\t} else if (phba->ha_copy & HA_ERATT) {\n\t\tif (phba->hba_flag & HBA_ERATT_HANDLED)\n\t\t\t/* ERATT polling has handled ERATT */\n\t\t\tphba->ha_copy &= ~HA_ERATT;\n\t\telse\n\t\t\t/* Indicate interrupt handler handles ERATT */\n\t\t\tphba->hba_flag |= HBA_ERATT_HANDLED;\n\t}\n\n\t/*\n\t * If there is deferred error attention, do not check for any interrupt.\n\t */\n\tif (unlikely(phba->hba_flag & DEFER_ERATT)) {\n\t\tspin_unlock(&phba->hbalock);\n\t\treturn IRQ_NONE;\n\t}\n\n\t/* Clear attention sources except link and error attentions */\n\tif (lpfc_readl(phba->HCregaddr, &hc_copy)) {\n\t\tspin_unlock(&phba->hbalock);\n\t\treturn IRQ_HANDLED;\n\t}\n\twritel(hc_copy & ~(HC_MBINT_ENA | HC_R0INT_ENA | HC_R1INT_ENA\n\t\t| HC_R2INT_ENA | HC_LAINT_ENA | HC_ERINT_ENA),\n\t\tphba->HCregaddr);\n\twritel((phba->ha_copy & ~(HA_LATT | HA_ERATT)), phba->HAregaddr);\n\twritel(hc_copy, phba->HCregaddr);\n\treadl(phba->HAregaddr); /* flush */\n\tspin_unlock(&phba->hbalock);\n\n\t/*\n\t * Invokes slow-path host attention interrupt handling as appropriate.\n\t */\n\n\t/* status of events with mailbox and link attention */\n\tstatus1 = phba->ha_copy & (HA_MBATT | HA_LATT | HA_ERATT);\n\n\t/* status of events with ELS ring */\n\tstatus2 = (phba->ha_copy & (HA_RXMASK  << (4*LPFC_ELS_RING)));\n\tstatus2 >>= (4*LPFC_ELS_RING);\n\n\tif (status1 || (status2 & HA_RXMASK))\n\t\tsp_irq_rc = lpfc_sli_sp_intr_handler(irq, dev_id);\n\telse\n\t\tsp_irq_rc = IRQ_NONE;\n\n\t/*\n\t * Invoke fast-path host attention interrupt handling as appropriate.\n\t */\n\n\t/* status of events with FCP ring */\n\tstatus1 = (phba->ha_copy & (HA_RXMASK << (4*LPFC_FCP_RING)));\n\tstatus1 >>= (4*LPFC_FCP_RING);\n\n\t/* status of events with extra ring */\n\tif (phba->cfg_multi_ring_support == 2) {\n\t\tstatus2 = (phba->ha_copy & (HA_RXMASK << (4*LPFC_EXTRA_RING)));\n\t\tstatus2 >>= (4*LPFC_EXTRA_RING);\n\t} else\n\t\tstatus2 = 0;\n\n\tif ((status1 & HA_RXMASK) || (status2 & HA_RXMASK))\n\t\tfp_irq_rc = lpfc_sli_fp_intr_handler(irq, dev_id);\n\telse\n\t\tfp_irq_rc = IRQ_NONE;\n\n\t/* Return device-level interrupt handling status */\n\treturn (sp_irq_rc == IRQ_HANDLED) ? sp_irq_rc : fp_irq_rc;\n}  /* lpfc_sli_intr_handler */\n\n/**\n * lpfc_sli4_els_xri_abort_event_proc - Process els xri abort event\n * @phba: pointer to lpfc hba data structure.\n *\n * This routine is invoked by the worker thread to process all the pending\n * SLI4 els abort xri events.\n **/\nvoid lpfc_sli4_els_xri_abort_event_proc(struct lpfc_hba *phba)\n{\n\tstruct lpfc_cq_event *cq_event;\n\n\t/* First, declare the els xri abort event has been handled */\n\tspin_lock_irq(&phba->hbalock);\n\tphba->hba_flag &= ~ELS_XRI_ABORT_EVENT;\n\tspin_unlock_irq(&phba->hbalock);\n\t/* Now, handle all the els xri abort events */\n\twhile (!list_empty(&phba->sli4_hba.sp_els_xri_aborted_work_queue)) {\n\t\t/* Get the first event from the head of the event queue */\n\t\tspin_lock_irq(&phba->hbalock);\n\t\tlist_remove_head(&phba->sli4_hba.sp_els_xri_aborted_work_queue,\n\t\t\t\t cq_event, struct lpfc_cq_event, list);\n\t\tspin_unlock_irq(&phba->hbalock);\n\t\t/* Notify aborted XRI for ELS work queue */\n\t\tlpfc_sli4_els_xri_aborted(phba, &cq_event->cqe.wcqe_axri);\n\t\t/* Free the event processed back to the free pool */\n\t\tlpfc_sli4_cq_event_release(phba, cq_event);\n\t}\n}\n\n/**\n * lpfc_sli4_iocb_param_transfer - Transfer pIocbOut and cmpl status to pIocbIn\n * @phba: pointer to lpfc hba data structure\n * @pIocbIn: pointer to the rspiocbq\n * @pIocbOut: pointer to the cmdiocbq\n * @wcqe: pointer to the complete wcqe\n *\n * This routine transfers the fields of a command iocbq to a response iocbq\n * by copying all the IOCB fields from command iocbq and transferring the\n * completion status information from the complete wcqe.\n **/\nstatic void\nlpfc_sli4_iocb_param_transfer(struct lpfc_hba *phba,\n\t\t\t      struct lpfc_iocbq *pIocbIn,\n\t\t\t      struct lpfc_iocbq *pIocbOut,\n\t\t\t      struct lpfc_wcqe_complete *wcqe)\n{\n\tint numBdes, i;\n\tunsigned long iflags;\n\tuint32_t status, max_response;\n\tstruct lpfc_dmabuf *dmabuf;\n\tstruct ulp_bde64 *bpl, bde;\n\tsize_t offset = offsetof(struct lpfc_iocbq, iocb);\n\n\tmemcpy((char *)pIocbIn + offset, (char *)pIocbOut + offset,\n\t       sizeof(struct lpfc_iocbq) - offset);\n\t/* Map WCQE parameters into irspiocb parameters */\n\tstatus = bf_get(lpfc_wcqe_c_status, wcqe);\n\tpIocbIn->iocb.ulpStatus = (status & LPFC_IOCB_STATUS_MASK);\n\tif (pIocbOut->iocb_flag & LPFC_IO_FCP)\n\t\tif (pIocbIn->iocb.ulpStatus == IOSTAT_FCP_RSP_ERROR)\n\t\t\tpIocbIn->iocb.un.fcpi.fcpi_parm =\n\t\t\t\t\tpIocbOut->iocb.un.fcpi.fcpi_parm -\n\t\t\t\t\twcqe->total_data_placed;\n\t\telse\n\t\t\tpIocbIn->iocb.un.ulpWord[4] = wcqe->parameter;\n\telse {\n\t\tpIocbIn->iocb.un.ulpWord[4] = wcqe->parameter;\n\t\tswitch (pIocbOut->iocb.ulpCommand) {\n\t\tcase CMD_ELS_REQUEST64_CR:\n\t\t\tdmabuf = (struct lpfc_dmabuf *)pIocbOut->context3;\n\t\t\tbpl  = (struct ulp_bde64 *)dmabuf->virt;\n\t\t\tbde.tus.w = le32_to_cpu(bpl[1].tus.w);\n\t\t\tmax_response = bde.tus.f.bdeSize;\n\t\t\tbreak;\n\t\tcase CMD_GEN_REQUEST64_CR:\n\t\t\tmax_response = 0;\n\t\t\tif (!pIocbOut->context3)\n\t\t\t\tbreak;\n\t\t\tnumBdes = pIocbOut->iocb.un.genreq64.bdl.bdeSize/\n\t\t\t\t\tsizeof(struct ulp_bde64);\n\t\t\tdmabuf = (struct lpfc_dmabuf *)pIocbOut->context3;\n\t\t\tbpl = (struct ulp_bde64 *)dmabuf->virt;\n\t\t\tfor (i = 0; i < numBdes; i++) {\n\t\t\t\tbde.tus.w = le32_to_cpu(bpl[i].tus.w);\n\t\t\t\tif (bde.tus.f.bdeFlags != BUFF_TYPE_BDE_64)\n\t\t\t\t\tmax_response += bde.tus.f.bdeSize;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tmax_response = wcqe->total_data_placed;\n\t\t\tbreak;\n\t\t}\n\t\tif (max_response < wcqe->total_data_placed)\n\t\t\tpIocbIn->iocb.un.genreq64.bdl.bdeSize = max_response;\n\t\telse\n\t\t\tpIocbIn->iocb.un.genreq64.bdl.bdeSize =\n\t\t\t\twcqe->total_data_placed;\n\t}\n\n\t/* Convert BG errors for completion status */\n\tif (status == CQE_STATUS_DI_ERROR) {\n\t\tpIocbIn->iocb.ulpStatus = IOSTAT_LOCAL_REJECT;\n\n\t\tif (bf_get(lpfc_wcqe_c_bg_edir, wcqe))\n\t\t\tpIocbIn->iocb.un.ulpWord[4] = IOERR_RX_DMA_FAILED;\n\t\telse\n\t\t\tpIocbIn->iocb.un.ulpWord[4] = IOERR_TX_DMA_FAILED;\n\n\t\tpIocbIn->iocb.unsli3.sli3_bg.bgstat = 0;\n\t\tif (bf_get(lpfc_wcqe_c_bg_ge, wcqe)) /* Guard Check failed */\n\t\t\tpIocbIn->iocb.unsli3.sli3_bg.bgstat |=\n\t\t\t\tBGS_GUARD_ERR_MASK;\n\t\tif (bf_get(lpfc_wcqe_c_bg_ae, wcqe)) /* App Tag Check failed */\n\t\t\tpIocbIn->iocb.unsli3.sli3_bg.bgstat |=\n\t\t\t\tBGS_APPTAG_ERR_MASK;\n\t\tif (bf_get(lpfc_wcqe_c_bg_re, wcqe)) /* Ref Tag Check failed */\n\t\t\tpIocbIn->iocb.unsli3.sli3_bg.bgstat |=\n\t\t\t\tBGS_REFTAG_ERR_MASK;\n\n\t\t/* Check to see if there was any good data before the error */\n\t\tif (bf_get(lpfc_wcqe_c_bg_tdpv, wcqe)) {\n\t\t\tpIocbIn->iocb.unsli3.sli3_bg.bgstat |=\n\t\t\t\tBGS_HI_WATER_MARK_PRESENT_MASK;\n\t\t\tpIocbIn->iocb.unsli3.sli3_bg.bghm =\n\t\t\t\twcqe->total_data_placed;\n\t\t}\n\n\t\t/*\n\t\t* Set ALL the error bits to indicate we don't know what\n\t\t* type of error it is.\n\t\t*/\n\t\tif (!pIocbIn->iocb.unsli3.sli3_bg.bgstat)\n\t\t\tpIocbIn->iocb.unsli3.sli3_bg.bgstat |=\n\t\t\t\t(BGS_REFTAG_ERR_MASK | BGS_APPTAG_ERR_MASK |\n\t\t\t\tBGS_GUARD_ERR_MASK);\n\t}\n\n\t/* Pick up HBA exchange busy condition */\n\tif (bf_get(lpfc_wcqe_c_xb, wcqe)) {\n\t\tspin_lock_irqsave(&phba->hbalock, iflags);\n\t\tpIocbIn->iocb_flag |= LPFC_EXCHANGE_BUSY;\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t}\n}\n\n/**\n * lpfc_sli4_els_wcqe_to_rspiocbq - Get response iocbq from els wcqe\n * @phba: Pointer to HBA context object.\n * @irspiocbq: Pointer to work-queue completion queue entry.\n *\n * This routine handles an ELS work-queue completion event and construct\n * a pseudo response ELS IODBQ from the SLI4 ELS WCQE for the common\n * discovery engine to handle.\n *\n * Return: Pointer to the receive IOCBQ, NULL otherwise.\n **/\nstatic struct lpfc_iocbq *\nlpfc_sli4_els_wcqe_to_rspiocbq(struct lpfc_hba *phba,\n\t\t\t       struct lpfc_iocbq *irspiocbq)\n{\n\tstruct lpfc_sli_ring *pring;\n\tstruct lpfc_iocbq *cmdiocbq;\n\tstruct lpfc_wcqe_complete *wcqe;\n\tunsigned long iflags;\n\n\tpring = lpfc_phba_elsring(phba);\n\tif (unlikely(!pring))\n\t\treturn NULL;\n\n\twcqe = &irspiocbq->cq_event.cqe.wcqe_cmpl;\n\tpring->stats.iocb_event++;\n\t/* Look up the ELS command IOCB and create pseudo response IOCB */\n\tcmdiocbq = lpfc_sli_iocbq_lookup_by_tag(phba, pring,\n\t\t\t\tbf_get(lpfc_wcqe_c_request_tag, wcqe));\n\tif (unlikely(!cmdiocbq)) {\n\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_SLI,\n\t\t\t\t\"0386 ELS complete with no corresponding \"\n\t\t\t\t\"cmdiocb: 0x%x 0x%x 0x%x 0x%x\\n\",\n\t\t\t\twcqe->word0, wcqe->total_data_placed,\n\t\t\t\twcqe->parameter, wcqe->word3);\n\t\tlpfc_sli_release_iocbq(phba, irspiocbq);\n\t\treturn NULL;\n\t}\n\n\tspin_lock_irqsave(&pring->ring_lock, iflags);\n\t/* Put the iocb back on the txcmplq */\n\tlpfc_sli_ringtxcmpl_put(phba, pring, cmdiocbq);\n\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n\n\t/* Fake the irspiocbq and copy necessary response information */\n\tlpfc_sli4_iocb_param_transfer(phba, irspiocbq, cmdiocbq, wcqe);\n\n\treturn irspiocbq;\n}\n\ninline struct lpfc_cq_event *\nlpfc_cq_event_setup(struct lpfc_hba *phba, void *entry, int size)\n{\n\tstruct lpfc_cq_event *cq_event;\n\n\t/* Allocate a new internal CQ_EVENT entry */\n\tcq_event = lpfc_sli4_cq_event_alloc(phba);\n\tif (!cq_event) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0602 Failed to alloc CQ_EVENT entry\\n\");\n\t\treturn NULL;\n\t}\n\n\t/* Move the CQE into the event */\n\tmemcpy(&cq_event->cqe, entry, size);\n\treturn cq_event;\n}\n\n/**\n * lpfc_sli4_sp_handle_async_event - Handle an asynchronous event\n * @phba: Pointer to HBA context object.\n * @mcqe: Pointer to mailbox completion queue entry.\n *\n * This routine process a mailbox completion queue entry with asynchronous\n * event.\n *\n * Return: true if work posted to worker thread, otherwise false.\n **/\nstatic bool\nlpfc_sli4_sp_handle_async_event(struct lpfc_hba *phba, struct lpfc_mcqe *mcqe)\n{\n\tstruct lpfc_cq_event *cq_event;\n\tunsigned long iflags;\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\"0392 Async Event: word0:x%x, word1:x%x, \"\n\t\t\t\"word2:x%x, word3:x%x\\n\", mcqe->word0,\n\t\t\tmcqe->mcqe_tag0, mcqe->mcqe_tag1, mcqe->trailer);\n\n\tcq_event = lpfc_cq_event_setup(phba, mcqe, sizeof(struct lpfc_mcqe));\n\tif (!cq_event)\n\t\treturn false;\n\tspin_lock_irqsave(&phba->hbalock, iflags);\n\tlist_add_tail(&cq_event->list, &phba->sli4_hba.sp_asynce_work_queue);\n\t/* Set the async event flag */\n\tphba->hba_flag |= ASYNC_EVENT;\n\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\n\treturn true;\n}\n\n/**\n * lpfc_sli4_sp_handle_mbox_event - Handle a mailbox completion event\n * @phba: Pointer to HBA context object.\n * @mcqe: Pointer to mailbox completion queue entry.\n *\n * This routine process a mailbox completion queue entry with mailbox\n * completion event.\n *\n * Return: true if work posted to worker thread, otherwise false.\n **/\nstatic bool\nlpfc_sli4_sp_handle_mbox_event(struct lpfc_hba *phba, struct lpfc_mcqe *mcqe)\n{\n\tuint32_t mcqe_status;\n\tMAILBOX_t *mbox, *pmbox;\n\tstruct lpfc_mqe *mqe;\n\tstruct lpfc_vport *vport;\n\tstruct lpfc_nodelist *ndlp;\n\tstruct lpfc_dmabuf *mp;\n\tunsigned long iflags;\n\tLPFC_MBOXQ_t *pmb;\n\tbool workposted = false;\n\tint rc;\n\n\t/* If not a mailbox complete MCQE, out by checking mailbox consume */\n\tif (!bf_get(lpfc_trailer_completed, mcqe))\n\t\tgoto out_no_mqe_complete;\n\n\t/* Get the reference to the active mbox command */\n\tspin_lock_irqsave(&phba->hbalock, iflags);\n\tpmb = phba->sli.mbox_active;\n\tif (unlikely(!pmb)) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"1832 No pending MBOX command to handle\\n\");\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t\tgoto out_no_mqe_complete;\n\t}\n\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\tmqe = &pmb->u.mqe;\n\tpmbox = (MAILBOX_t *)&pmb->u.mqe;\n\tmbox = phba->mbox;\n\tvport = pmb->vport;\n\n\t/* Reset heartbeat timer */\n\tphba->last_completion_time = jiffies;\n\tdel_timer(&phba->sli.mbox_tmo);\n\n\t/* Move mbox data to caller's mailbox region, do endian swapping */\n\tif (pmb->mbox_cmpl && mbox)\n\t\tlpfc_sli4_pcimem_bcopy(mbox, mqe, sizeof(struct lpfc_mqe));\n\n\t/*\n\t * For mcqe errors, conditionally move a modified error code to\n\t * the mbox so that the error will not be missed.\n\t */\n\tmcqe_status = bf_get(lpfc_mcqe_status, mcqe);\n\tif (mcqe_status != MB_CQE_STATUS_SUCCESS) {\n\t\tif (bf_get(lpfc_mqe_status, mqe) == MBX_SUCCESS)\n\t\t\tbf_set(lpfc_mqe_status, mqe,\n\t\t\t       (LPFC_MBX_ERROR_RANGE | mcqe_status));\n\t}\n\tif (pmb->mbox_flag & LPFC_MBX_IMED_UNREG) {\n\t\tpmb->mbox_flag &= ~LPFC_MBX_IMED_UNREG;\n\t\tlpfc_debugfs_disc_trc(vport, LPFC_DISC_TRC_MBOX_VPORT,\n\t\t\t\t      \"MBOX dflt rpi: status:x%x rpi:x%x\",\n\t\t\t\t      mcqe_status,\n\t\t\t\t      pmbox->un.varWords[0], 0);\n\t\tif (mcqe_status == MB_CQE_STATUS_SUCCESS) {\n\t\t\tmp = (struct lpfc_dmabuf *)(pmb->ctx_buf);\n\t\t\tndlp = (struct lpfc_nodelist *)pmb->ctx_ndlp;\n\t\t\t/* Reg_LOGIN of dflt RPI was successful. Now lets get\n\t\t\t * RID of the PPI using the same mbox buffer.\n\t\t\t */\n\t\t\tlpfc_unreg_login(phba, vport->vpi,\n\t\t\t\t\t pmbox->un.varWords[0], pmb);\n\t\t\tpmb->mbox_cmpl = lpfc_mbx_cmpl_dflt_rpi;\n\t\t\tpmb->ctx_buf = mp;\n\t\t\tpmb->ctx_ndlp = ndlp;\n\t\t\tpmb->vport = vport;\n\t\t\trc = lpfc_sli_issue_mbox(phba, pmb, MBX_NOWAIT);\n\t\t\tif (rc != MBX_BUSY)\n\t\t\t\tlpfc_printf_log(phba, KERN_ERR,\n\t\t\t\t\t\tLOG_TRACE_EVENT,\n\t\t\t\t\t\t\"0385 rc should \"\n\t\t\t\t\t\t\"have been MBX_BUSY\\n\");\n\t\t\tif (rc != MBX_NOT_FINISHED)\n\t\t\t\tgoto send_current_mbox;\n\t\t}\n\t}\n\tspin_lock_irqsave(&phba->pport->work_port_lock, iflags);\n\tphba->pport->work_port_events &= ~WORKER_MBOX_TMO;\n\tspin_unlock_irqrestore(&phba->pport->work_port_lock, iflags);\n\n\t/* There is mailbox completion work to do */\n\tspin_lock_irqsave(&phba->hbalock, iflags);\n\t__lpfc_mbox_cmpl_put(phba, pmb);\n\tphba->work_ha |= HA_MBATT;\n\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\tworkposted = true;\n\nsend_current_mbox:\n\tspin_lock_irqsave(&phba->hbalock, iflags);\n\t/* Release the mailbox command posting token */\n\tphba->sli.sli_flag &= ~LPFC_SLI_MBOX_ACTIVE;\n\t/* Setting active mailbox pointer need to be in sync to flag clear */\n\tphba->sli.mbox_active = NULL;\n\tif (bf_get(lpfc_trailer_consumed, mcqe))\n\t\tlpfc_sli4_mq_release(phba->sli4_hba.mbx_wq);\n\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t/* Wake up worker thread to post the next pending mailbox command */\n\tlpfc_worker_wake_up(phba);\n\treturn workposted;\n\nout_no_mqe_complete:\n\tspin_lock_irqsave(&phba->hbalock, iflags);\n\tif (bf_get(lpfc_trailer_consumed, mcqe))\n\t\tlpfc_sli4_mq_release(phba->sli4_hba.mbx_wq);\n\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\treturn false;\n}\n\n/**\n * lpfc_sli4_sp_handle_mcqe - Process a mailbox completion queue entry\n * @phba: Pointer to HBA context object.\n * @cq: Pointer to associated CQ\n * @cqe: Pointer to mailbox completion queue entry.\n *\n * This routine process a mailbox completion queue entry, it invokes the\n * proper mailbox complete handling or asynchronous event handling routine\n * according to the MCQE's async bit.\n *\n * Return: true if work posted to worker thread, otherwise false.\n **/\nstatic bool\nlpfc_sli4_sp_handle_mcqe(struct lpfc_hba *phba, struct lpfc_queue *cq,\n\t\t\t struct lpfc_cqe *cqe)\n{\n\tstruct lpfc_mcqe mcqe;\n\tbool workposted;\n\n\tcq->CQ_mbox++;\n\n\t/* Copy the mailbox MCQE and convert endian order as needed */\n\tlpfc_sli4_pcimem_bcopy(cqe, &mcqe, sizeof(struct lpfc_mcqe));\n\n\t/* Invoke the proper event handling routine */\n\tif (!bf_get(lpfc_trailer_async, &mcqe))\n\t\tworkposted = lpfc_sli4_sp_handle_mbox_event(phba, &mcqe);\n\telse\n\t\tworkposted = lpfc_sli4_sp_handle_async_event(phba, &mcqe);\n\treturn workposted;\n}\n\n/**\n * lpfc_sli4_sp_handle_els_wcqe - Handle els work-queue completion event\n * @phba: Pointer to HBA context object.\n * @cq: Pointer to associated CQ\n * @wcqe: Pointer to work-queue completion queue entry.\n *\n * This routine handles an ELS work-queue completion event.\n *\n * Return: true if work posted to worker thread, otherwise false.\n **/\nstatic bool\nlpfc_sli4_sp_handle_els_wcqe(struct lpfc_hba *phba, struct lpfc_queue *cq,\n\t\t\t     struct lpfc_wcqe_complete *wcqe)\n{\n\tstruct lpfc_iocbq *irspiocbq;\n\tunsigned long iflags;\n\tstruct lpfc_sli_ring *pring = cq->pring;\n\tint txq_cnt = 0;\n\tint txcmplq_cnt = 0;\n\n\t/* Check for response status */\n\tif (unlikely(bf_get(lpfc_wcqe_c_status, wcqe))) {\n\t\t/* Log the error status */\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\t\"0357 ELS CQE error: status=x%x: \"\n\t\t\t\t\"CQE: %08x %08x %08x %08x\\n\",\n\t\t\t\tbf_get(lpfc_wcqe_c_status, wcqe),\n\t\t\t\twcqe->word0, wcqe->total_data_placed,\n\t\t\t\twcqe->parameter, wcqe->word3);\n\t}\n\n\t/* Get an irspiocbq for later ELS response processing use */\n\tirspiocbq = lpfc_sli_get_iocbq(phba);\n\tif (!irspiocbq) {\n\t\tif (!list_empty(&pring->txq))\n\t\t\ttxq_cnt++;\n\t\tif (!list_empty(&pring->txcmplq))\n\t\t\ttxcmplq_cnt++;\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"0387 NO IOCBQ data: txq_cnt=%d iocb_cnt=%d \"\n\t\t\t\"els_txcmplq_cnt=%d\\n\",\n\t\t\ttxq_cnt, phba->iocb_cnt,\n\t\t\ttxcmplq_cnt);\n\t\treturn false;\n\t}\n\n\t/* Save off the slow-path queue event for work thread to process */\n\tmemcpy(&irspiocbq->cq_event.cqe.wcqe_cmpl, wcqe, sizeof(*wcqe));\n\tspin_lock_irqsave(&phba->hbalock, iflags);\n\tlist_add_tail(&irspiocbq->cq_event.list,\n\t\t      &phba->sli4_hba.sp_queue_event);\n\tphba->hba_flag |= HBA_SP_QUEUE_EVT;\n\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\n\treturn true;\n}\n\n/**\n * lpfc_sli4_sp_handle_rel_wcqe - Handle slow-path WQ entry consumed event\n * @phba: Pointer to HBA context object.\n * @wcqe: Pointer to work-queue completion queue entry.\n *\n * This routine handles slow-path WQ entry consumed event by invoking the\n * proper WQ release routine to the slow-path WQ.\n **/\nstatic void\nlpfc_sli4_sp_handle_rel_wcqe(struct lpfc_hba *phba,\n\t\t\t     struct lpfc_wcqe_release *wcqe)\n{\n\t/* sanity check on queue memory */\n\tif (unlikely(!phba->sli4_hba.els_wq))\n\t\treturn;\n\t/* Check for the slow-path ELS work queue */\n\tif (bf_get(lpfc_wcqe_r_wq_id, wcqe) == phba->sli4_hba.els_wq->queue_id)\n\t\tlpfc_sli4_wq_release(phba->sli4_hba.els_wq,\n\t\t\t\t     bf_get(lpfc_wcqe_r_wqe_index, wcqe));\n\telse\n\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_SLI,\n\t\t\t\t\"2579 Slow-path wqe consume event carries \"\n\t\t\t\t\"miss-matched qid: wcqe-qid=x%x, sp-qid=x%x\\n\",\n\t\t\t\tbf_get(lpfc_wcqe_r_wqe_index, wcqe),\n\t\t\t\tphba->sli4_hba.els_wq->queue_id);\n}\n\n/**\n * lpfc_sli4_sp_handle_abort_xri_wcqe - Handle a xri abort event\n * @phba: Pointer to HBA context object.\n * @cq: Pointer to a WQ completion queue.\n * @wcqe: Pointer to work-queue completion queue entry.\n *\n * This routine handles an XRI abort event.\n *\n * Return: true if work posted to worker thread, otherwise false.\n **/\nstatic bool\nlpfc_sli4_sp_handle_abort_xri_wcqe(struct lpfc_hba *phba,\n\t\t\t\t   struct lpfc_queue *cq,\n\t\t\t\t   struct sli4_wcqe_xri_aborted *wcqe)\n{\n\tbool workposted = false;\n\tstruct lpfc_cq_event *cq_event;\n\tunsigned long iflags;\n\n\tswitch (cq->subtype) {\n\tcase LPFC_IO:\n\t\tlpfc_sli4_io_xri_aborted(phba, wcqe, cq->hdwq);\n\t\tif (phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME) {\n\t\t\t/* Notify aborted XRI for NVME work queue */\n\t\t\tif (phba->nvmet_support)\n\t\t\t\tlpfc_sli4_nvmet_xri_aborted(phba, wcqe);\n\t\t}\n\t\tworkposted = false;\n\t\tbreak;\n\tcase LPFC_NVME_LS: /* NVME LS uses ELS resources */\n\tcase LPFC_ELS:\n\t\tcq_event = lpfc_cq_event_setup(\n\t\t\tphba, wcqe, sizeof(struct sli4_wcqe_xri_aborted));\n\t\tif (!cq_event)\n\t\t\treturn false;\n\t\tcq_event->hdwq = cq->hdwq;\n\t\tspin_lock_irqsave(&phba->hbalock, iflags);\n\t\tlist_add_tail(&cq_event->list,\n\t\t\t      &phba->sli4_hba.sp_els_xri_aborted_work_queue);\n\t\t/* Set the els xri abort event flag */\n\t\tphba->hba_flag |= ELS_XRI_ABORT_EVENT;\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t\tworkposted = true;\n\t\tbreak;\n\tdefault:\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0603 Invalid CQ subtype %d: \"\n\t\t\t\t\"%08x %08x %08x %08x\\n\",\n\t\t\t\tcq->subtype, wcqe->word0, wcqe->parameter,\n\t\t\t\twcqe->word2, wcqe->word3);\n\t\tworkposted = false;\n\t\tbreak;\n\t}\n\treturn workposted;\n}\n\n#define FC_RCTL_MDS_DIAGS\t0xF4\n\n/**\n * lpfc_sli4_sp_handle_rcqe - Process a receive-queue completion queue entry\n * @phba: Pointer to HBA context object.\n * @rcqe: Pointer to receive-queue completion queue entry.\n *\n * This routine process a receive-queue completion queue entry.\n *\n * Return: true if work posted to worker thread, otherwise false.\n **/\nstatic bool\nlpfc_sli4_sp_handle_rcqe(struct lpfc_hba *phba, struct lpfc_rcqe *rcqe)\n{\n\tbool workposted = false;\n\tstruct fc_frame_header *fc_hdr;\n\tstruct lpfc_queue *hrq = phba->sli4_hba.hdr_rq;\n\tstruct lpfc_queue *drq = phba->sli4_hba.dat_rq;\n\tstruct lpfc_nvmet_tgtport *tgtp;\n\tstruct hbq_dmabuf *dma_buf;\n\tuint32_t status, rq_id;\n\tunsigned long iflags;\n\n\t/* sanity check on queue memory */\n\tif (unlikely(!hrq) || unlikely(!drq))\n\t\treturn workposted;\n\n\tif (bf_get(lpfc_cqe_code, rcqe) == CQE_CODE_RECEIVE_V1)\n\t\trq_id = bf_get(lpfc_rcqe_rq_id_v1, rcqe);\n\telse\n\t\trq_id = bf_get(lpfc_rcqe_rq_id, rcqe);\n\tif (rq_id != hrq->queue_id)\n\t\tgoto out;\n\n\tstatus = bf_get(lpfc_rcqe_status, rcqe);\n\tswitch (status) {\n\tcase FC_STATUS_RQ_BUF_LEN_EXCEEDED:\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2537 Receive Frame Truncated!!\\n\");\n\t\tfallthrough;\n\tcase FC_STATUS_RQ_SUCCESS:\n\t\tspin_lock_irqsave(&phba->hbalock, iflags);\n\t\tlpfc_sli4_rq_release(hrq, drq);\n\t\tdma_buf = lpfc_sli_hbqbuf_get(&phba->hbqs[0].hbq_buffer_list);\n\t\tif (!dma_buf) {\n\t\t\thrq->RQ_no_buf_found++;\n\t\t\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t\t\tgoto out;\n\t\t}\n\t\thrq->RQ_rcv_buf++;\n\t\thrq->RQ_buf_posted--;\n\t\tmemcpy(&dma_buf->cq_event.cqe.rcqe_cmpl, rcqe, sizeof(*rcqe));\n\n\t\tfc_hdr = (struct fc_frame_header *)dma_buf->hbuf.virt;\n\n\t\tif (fc_hdr->fh_r_ctl == FC_RCTL_MDS_DIAGS ||\n\t\t    fc_hdr->fh_r_ctl == FC_RCTL_DD_UNSOL_DATA) {\n\t\t\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t\t\t/* Handle MDS Loopback frames */\n\t\t\tif  (!(phba->pport->load_flag & FC_UNLOADING))\n\t\t\t\tlpfc_sli4_handle_mds_loopback(phba->pport,\n\t\t\t\t\t\t\t      dma_buf);\n\t\t\telse\n\t\t\t\tlpfc_in_buf_free(phba, &dma_buf->dbuf);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* save off the frame for the work thread to process */\n\t\tlist_add_tail(&dma_buf->cq_event.list,\n\t\t\t      &phba->sli4_hba.sp_queue_event);\n\t\t/* Frame received */\n\t\tphba->hba_flag |= HBA_SP_QUEUE_EVT;\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t\tworkposted = true;\n\t\tbreak;\n\tcase FC_STATUS_INSUFF_BUF_FRM_DISC:\n\t\tif (phba->nvmet_support) {\n\t\t\ttgtp = phba->targetport->private;\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"6402 RQE Error x%x, posted %d err_cnt \"\n\t\t\t\t\t\"%d: %x %x %x\\n\",\n\t\t\t\t\tstatus, hrq->RQ_buf_posted,\n\t\t\t\t\thrq->RQ_no_posted_buf,\n\t\t\t\t\tatomic_read(&tgtp->rcv_fcp_cmd_in),\n\t\t\t\t\tatomic_read(&tgtp->rcv_fcp_cmd_out),\n\t\t\t\t\tatomic_read(&tgtp->xmt_fcp_release));\n\t\t}\n\t\tfallthrough;\n\n\tcase FC_STATUS_INSUFF_BUF_NEED_BUF:\n\t\thrq->RQ_no_posted_buf++;\n\t\t/* Post more buffers if possible */\n\t\tspin_lock_irqsave(&phba->hbalock, iflags);\n\t\tphba->hba_flag |= HBA_POST_RECEIVE_BUFFER;\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t\tworkposted = true;\n\t\tbreak;\n\t}\nout:\n\treturn workposted;\n}\n\n/**\n * lpfc_sli4_sp_handle_cqe - Process a slow path completion queue entry\n * @phba: Pointer to HBA context object.\n * @cq: Pointer to the completion queue.\n * @cqe: Pointer to a completion queue entry.\n *\n * This routine process a slow-path work-queue or receive queue completion queue\n * entry.\n *\n * Return: true if work posted to worker thread, otherwise false.\n **/\nstatic bool\nlpfc_sli4_sp_handle_cqe(struct lpfc_hba *phba, struct lpfc_queue *cq,\n\t\t\t struct lpfc_cqe *cqe)\n{\n\tstruct lpfc_cqe cqevt;\n\tbool workposted = false;\n\n\t/* Copy the work queue CQE and convert endian order if needed */\n\tlpfc_sli4_pcimem_bcopy(cqe, &cqevt, sizeof(struct lpfc_cqe));\n\n\t/* Check and process for different type of WCQE and dispatch */\n\tswitch (bf_get(lpfc_cqe_code, &cqevt)) {\n\tcase CQE_CODE_COMPL_WQE:\n\t\t/* Process the WQ/RQ complete event */\n\t\tphba->last_completion_time = jiffies;\n\t\tworkposted = lpfc_sli4_sp_handle_els_wcqe(phba, cq,\n\t\t\t\t(struct lpfc_wcqe_complete *)&cqevt);\n\t\tbreak;\n\tcase CQE_CODE_RELEASE_WQE:\n\t\t/* Process the WQ release event */\n\t\tlpfc_sli4_sp_handle_rel_wcqe(phba,\n\t\t\t\t(struct lpfc_wcqe_release *)&cqevt);\n\t\tbreak;\n\tcase CQE_CODE_XRI_ABORTED:\n\t\t/* Process the WQ XRI abort event */\n\t\tphba->last_completion_time = jiffies;\n\t\tworkposted = lpfc_sli4_sp_handle_abort_xri_wcqe(phba, cq,\n\t\t\t\t(struct sli4_wcqe_xri_aborted *)&cqevt);\n\t\tbreak;\n\tcase CQE_CODE_RECEIVE:\n\tcase CQE_CODE_RECEIVE_V1:\n\t\t/* Process the RQ event */\n\t\tphba->last_completion_time = jiffies;\n\t\tworkposted = lpfc_sli4_sp_handle_rcqe(phba,\n\t\t\t\t(struct lpfc_rcqe *)&cqevt);\n\t\tbreak;\n\tdefault:\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0388 Not a valid WCQE code: x%x\\n\",\n\t\t\t\tbf_get(lpfc_cqe_code, &cqevt));\n\t\tbreak;\n\t}\n\treturn workposted;\n}\n\n/**\n * lpfc_sli4_sp_handle_eqe - Process a slow-path event queue entry\n * @phba: Pointer to HBA context object.\n * @eqe: Pointer to fast-path event queue entry.\n * @speq: Pointer to slow-path event queue.\n *\n * This routine process a event queue entry from the slow-path event queue.\n * It will check the MajorCode and MinorCode to determine this is for a\n * completion event on a completion queue, if not, an error shall be logged\n * and just return. Otherwise, it will get to the corresponding completion\n * queue and process all the entries on that completion queue, rearm the\n * completion queue, and then return.\n *\n **/\nstatic void\nlpfc_sli4_sp_handle_eqe(struct lpfc_hba *phba, struct lpfc_eqe *eqe,\n\tstruct lpfc_queue *speq)\n{\n\tstruct lpfc_queue *cq = NULL, *childq;\n\tuint16_t cqid;\n\tint ret = 0;\n\n\t/* Get the reference to the corresponding CQ */\n\tcqid = bf_get_le32(lpfc_eqe_resource_id, eqe);\n\n\tlist_for_each_entry(childq, &speq->child_list, list) {\n\t\tif (childq->queue_id == cqid) {\n\t\t\tcq = childq;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (unlikely(!cq)) {\n\t\tif (phba->sli.sli_flag & LPFC_SLI_ACTIVE)\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"0365 Slow-path CQ identifier \"\n\t\t\t\t\t\"(%d) does not exist\\n\", cqid);\n\t\treturn;\n\t}\n\n\t/* Save EQ associated with this CQ */\n\tcq->assoc_qp = speq;\n\n\tif (is_kdump_kernel())\n\t\tret = queue_work(phba->wq, &cq->spwork);\n\telse\n\t\tret = queue_work_on(cq->chann, phba->wq, &cq->spwork);\n\n\tif (!ret)\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0390 Cannot schedule queue work \"\n\t\t\t\t\"for CQ eqcqid=%d, cqid=%d on CPU %d\\n\",\n\t\t\t\tcqid, cq->queue_id, raw_smp_processor_id());\n}\n\n/**\n * __lpfc_sli4_process_cq - Process elements of a CQ\n * @phba: Pointer to HBA context object.\n * @cq: Pointer to CQ to be processed\n * @handler: Routine to process each cqe\n * @delay: Pointer to usdelay to set in case of rescheduling of the handler\n * @poll_mode: Polling mode we were called from\n *\n * This routine processes completion queue entries in a CQ. While a valid\n * queue element is found, the handler is called. During processing checks\n * are made for periodic doorbell writes to let the hardware know of\n * element consumption.\n *\n * If the max limit on cqes to process is hit, or there are no more valid\n * entries, the loop stops. If we processed a sufficient number of elements,\n * meaning there is sufficient load, rather than rearming and generating\n * another interrupt, a cq rescheduling delay will be set. A delay of 0\n * indicates no rescheduling.\n *\n * Returns True if work scheduled, False otherwise.\n **/\nstatic bool\n__lpfc_sli4_process_cq(struct lpfc_hba *phba, struct lpfc_queue *cq,\n\tbool (*handler)(struct lpfc_hba *, struct lpfc_queue *,\n\t\t\tstruct lpfc_cqe *), unsigned long *delay,\n\t\t\tenum lpfc_poll_mode poll_mode)\n{\n\tstruct lpfc_cqe *cqe;\n\tbool workposted = false;\n\tint count = 0, consumed = 0;\n\tbool arm = true;\n\n\t/* default - no reschedule */\n\t*delay = 0;\n\n\tif (cmpxchg(&cq->queue_claimed, 0, 1) != 0)\n\t\tgoto rearm_and_exit;\n\n\t/* Process all the entries to the CQ */\n\tcq->q_flag = 0;\n\tcqe = lpfc_sli4_cq_get(cq);\n\twhile (cqe) {\n\t\tworkposted |= handler(phba, cq, cqe);\n\t\t__lpfc_sli4_consume_cqe(phba, cq, cqe);\n\n\t\tconsumed++;\n\t\tif (!(++count % cq->max_proc_limit))\n\t\t\tbreak;\n\n\t\tif (!(count % cq->notify_interval)) {\n\t\t\tphba->sli4_hba.sli4_write_cq_db(phba, cq, consumed,\n\t\t\t\t\t\tLPFC_QUEUE_NOARM);\n\t\t\tconsumed = 0;\n\t\t\tcq->assoc_qp->q_flag |= HBA_EQ_DELAY_CHK;\n\t\t}\n\n\t\tif (count == LPFC_NVMET_CQ_NOTIFY)\n\t\t\tcq->q_flag |= HBA_NVMET_CQ_NOTIFY;\n\n\t\tcqe = lpfc_sli4_cq_get(cq);\n\t}\n\tif (count >= phba->cfg_cq_poll_threshold) {\n\t\t*delay = 1;\n\t\tarm = false;\n\t}\n\n\t/* Note: complete the irq_poll softirq before rearming CQ */\n\tif (poll_mode == LPFC_IRQ_POLL)\n\t\tirq_poll_complete(&cq->iop);\n\n\t/* Track the max number of CQEs processed in 1 EQ */\n\tif (count > cq->CQ_max_cqe)\n\t\tcq->CQ_max_cqe = count;\n\n\tcq->assoc_qp->EQ_cqe_cnt += count;\n\n\t/* Catch the no cq entry condition */\n\tif (unlikely(count == 0))\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\t\"0369 No entry from completion queue \"\n\t\t\t\t\"qid=%d\\n\", cq->queue_id);\n\n\txchg(&cq->queue_claimed, 0);\n\nrearm_and_exit:\n\tphba->sli4_hba.sli4_write_cq_db(phba, cq, consumed,\n\t\t\tarm ?  LPFC_QUEUE_REARM : LPFC_QUEUE_NOARM);\n\n\treturn workposted;\n}\n\n/**\n * lpfc_sli4_sp_process_cq - Process a slow-path event queue entry\n * @cq: pointer to CQ to process\n *\n * This routine calls the cq processing routine with a handler specific\n * to the type of queue bound to it.\n *\n * The CQ routine returns two values: the first is the calling status,\n * which indicates whether work was queued to the  background discovery\n * thread. If true, the routine should wakeup the discovery thread;\n * the second is the delay parameter. If non-zero, rather than rearming\n * the CQ and yet another interrupt, the CQ handler should be queued so\n * that it is processed in a subsequent polling action. The value of\n * the delay indicates when to reschedule it.\n **/\nstatic void\n__lpfc_sli4_sp_process_cq(struct lpfc_queue *cq)\n{\n\tstruct lpfc_hba *phba = cq->phba;\n\tunsigned long delay;\n\tbool workposted = false;\n\tint ret = 0;\n\n\t/* Process and rearm the CQ */\n\tswitch (cq->type) {\n\tcase LPFC_MCQ:\n\t\tworkposted |= __lpfc_sli4_process_cq(phba, cq,\n\t\t\t\t\t\tlpfc_sli4_sp_handle_mcqe,\n\t\t\t\t\t\t&delay, LPFC_QUEUE_WORK);\n\t\tbreak;\n\tcase LPFC_WCQ:\n\t\tif (cq->subtype == LPFC_IO)\n\t\t\tworkposted |= __lpfc_sli4_process_cq(phba, cq,\n\t\t\t\t\t\tlpfc_sli4_fp_handle_cqe,\n\t\t\t\t\t\t&delay, LPFC_QUEUE_WORK);\n\t\telse\n\t\t\tworkposted |= __lpfc_sli4_process_cq(phba, cq,\n\t\t\t\t\t\tlpfc_sli4_sp_handle_cqe,\n\t\t\t\t\t\t&delay, LPFC_QUEUE_WORK);\n\t\tbreak;\n\tdefault:\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0370 Invalid completion queue type (%d)\\n\",\n\t\t\t\tcq->type);\n\t\treturn;\n\t}\n\n\tif (delay) {\n\t\tif (is_kdump_kernel())\n\t\t\tret = queue_delayed_work(phba->wq, &cq->sched_spwork,\n\t\t\t\t\t\tdelay);\n\t\telse\n\t\t\tret = queue_delayed_work_on(cq->chann, phba->wq,\n\t\t\t\t\t\t&cq->sched_spwork, delay);\n\t\tif (!ret)\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0394 Cannot schedule queue work \"\n\t\t\t\t\"for cqid=%d on CPU %d\\n\",\n\t\t\t\tcq->queue_id, cq->chann);\n\t}\n\n\t/* wake up worker thread if there are works to be done */\n\tif (workposted)\n\t\tlpfc_worker_wake_up(phba);\n}\n\n/**\n * lpfc_sli4_sp_process_cq - slow-path work handler when started by\n *   interrupt\n * @work: pointer to work element\n *\n * translates from the work handler and calls the slow-path handler.\n **/\nstatic void\nlpfc_sli4_sp_process_cq(struct work_struct *work)\n{\n\tstruct lpfc_queue *cq = container_of(work, struct lpfc_queue, spwork);\n\n\t__lpfc_sli4_sp_process_cq(cq);\n}\n\n/**\n * lpfc_sli4_dly_sp_process_cq - slow-path work handler when started by timer\n * @work: pointer to work element\n *\n * translates from the work handler and calls the slow-path handler.\n **/\nstatic void\nlpfc_sli4_dly_sp_process_cq(struct work_struct *work)\n{\n\tstruct lpfc_queue *cq = container_of(to_delayed_work(work),\n\t\t\t\t\tstruct lpfc_queue, sched_spwork);\n\n\t__lpfc_sli4_sp_process_cq(cq);\n}\n\n/**\n * lpfc_sli4_fp_handle_fcp_wcqe - Process fast-path work queue completion entry\n * @phba: Pointer to HBA context object.\n * @cq: Pointer to associated CQ\n * @wcqe: Pointer to work-queue completion queue entry.\n *\n * This routine process a fast-path work queue completion entry from fast-path\n * event queue for FCP command response completion.\n **/\nstatic void\nlpfc_sli4_fp_handle_fcp_wcqe(struct lpfc_hba *phba, struct lpfc_queue *cq,\n\t\t\t     struct lpfc_wcqe_complete *wcqe)\n{\n\tstruct lpfc_sli_ring *pring = cq->pring;\n\tstruct lpfc_iocbq *cmdiocbq;\n\tstruct lpfc_iocbq irspiocbq;\n\tunsigned long iflags;\n\n\t/* Check for response status */\n\tif (unlikely(bf_get(lpfc_wcqe_c_status, wcqe))) {\n\t\t/* If resource errors reported from HBA, reduce queue\n\t\t * depth of the SCSI device.\n\t\t */\n\t\tif (((bf_get(lpfc_wcqe_c_status, wcqe) ==\n\t\t     IOSTAT_LOCAL_REJECT)) &&\n\t\t    ((wcqe->parameter & IOERR_PARAM_MASK) ==\n\t\t     IOERR_NO_RESOURCES))\n\t\t\tphba->lpfc_rampdown_queue_depth(phba);\n\n\t\t/* Log the cmpl status */\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\t\"0373 FCP CQE cmpl: status=x%x: \"\n\t\t\t\t\"CQE: %08x %08x %08x %08x\\n\",\n\t\t\t\tbf_get(lpfc_wcqe_c_status, wcqe),\n\t\t\t\twcqe->word0, wcqe->total_data_placed,\n\t\t\t\twcqe->parameter, wcqe->word3);\n\t}\n\n\t/* Look up the FCP command IOCB and create pseudo response IOCB */\n\tspin_lock_irqsave(&pring->ring_lock, iflags);\n\tpring->stats.iocb_event++;\n\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n\tcmdiocbq = lpfc_sli_iocbq_lookup_by_tag(phba, pring,\n\t\t\t\tbf_get(lpfc_wcqe_c_request_tag, wcqe));\n\tif (unlikely(!cmdiocbq)) {\n\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_SLI,\n\t\t\t\t\"0374 FCP complete with no corresponding \"\n\t\t\t\t\"cmdiocb: iotag (%d)\\n\",\n\t\t\t\tbf_get(lpfc_wcqe_c_request_tag, wcqe));\n\t\treturn;\n\t}\n#ifdef CONFIG_SCSI_LPFC_DEBUG_FS\n\tcmdiocbq->isr_timestamp = cq->isr_timestamp;\n#endif\n\tif (cmdiocbq->iocb_cmpl == NULL) {\n\t\tif (cmdiocbq->wqe_cmpl) {\n\t\t\tif (cmdiocbq->iocb_flag & LPFC_DRIVER_ABORTED) {\n\t\t\t\tspin_lock_irqsave(&phba->hbalock, iflags);\n\t\t\t\tcmdiocbq->iocb_flag &= ~LPFC_DRIVER_ABORTED;\n\t\t\t\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t\t\t}\n\n\t\t\t/* Pass the cmd_iocb and the wcqe to the upper layer */\n\t\t\t(cmdiocbq->wqe_cmpl)(phba, cmdiocbq, wcqe);\n\t\t\treturn;\n\t\t}\n\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_SLI,\n\t\t\t\t\"0375 FCP cmdiocb not callback function \"\n\t\t\t\t\"iotag: (%d)\\n\",\n\t\t\t\tbf_get(lpfc_wcqe_c_request_tag, wcqe));\n\t\treturn;\n\t}\n\n\t/* Fake the irspiocb and copy necessary response information */\n\tlpfc_sli4_iocb_param_transfer(phba, &irspiocbq, cmdiocbq, wcqe);\n\n\tif (cmdiocbq->iocb_flag & LPFC_DRIVER_ABORTED) {\n\t\tspin_lock_irqsave(&phba->hbalock, iflags);\n\t\tcmdiocbq->iocb_flag &= ~LPFC_DRIVER_ABORTED;\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t}\n\n\t/* Pass the cmd_iocb and the rsp state to the upper layer */\n\t(cmdiocbq->iocb_cmpl)(phba, cmdiocbq, &irspiocbq);\n}\n\n/**\n * lpfc_sli4_fp_handle_rel_wcqe - Handle fast-path WQ entry consumed event\n * @phba: Pointer to HBA context object.\n * @cq: Pointer to completion queue.\n * @wcqe: Pointer to work-queue completion queue entry.\n *\n * This routine handles an fast-path WQ entry consumed event by invoking the\n * proper WQ release routine to the slow-path WQ.\n **/\nstatic void\nlpfc_sli4_fp_handle_rel_wcqe(struct lpfc_hba *phba, struct lpfc_queue *cq,\n\t\t\t     struct lpfc_wcqe_release *wcqe)\n{\n\tstruct lpfc_queue *childwq;\n\tbool wqid_matched = false;\n\tuint16_t hba_wqid;\n\n\t/* Check for fast-path FCP work queue release */\n\thba_wqid = bf_get(lpfc_wcqe_r_wq_id, wcqe);\n\tlist_for_each_entry(childwq, &cq->child_list, list) {\n\t\tif (childwq->queue_id == hba_wqid) {\n\t\t\tlpfc_sli4_wq_release(childwq,\n\t\t\t\t\tbf_get(lpfc_wcqe_r_wqe_index, wcqe));\n\t\t\tif (childwq->q_flag & HBA_NVMET_WQFULL)\n\t\t\t\tlpfc_nvmet_wqfull_process(phba, childwq);\n\t\t\twqid_matched = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\t/* Report warning log message if no match found */\n\tif (wqid_matched != true)\n\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_SLI,\n\t\t\t\t\"2580 Fast-path wqe consume event carries \"\n\t\t\t\t\"miss-matched qid: wcqe-qid=x%x\\n\", hba_wqid);\n}\n\n/**\n * lpfc_sli4_nvmet_handle_rcqe - Process a receive-queue completion queue entry\n * @phba: Pointer to HBA context object.\n * @cq: Pointer to completion queue.\n * @rcqe: Pointer to receive-queue completion queue entry.\n *\n * This routine process a receive-queue completion queue entry.\n *\n * Return: true if work posted to worker thread, otherwise false.\n **/\nstatic bool\nlpfc_sli4_nvmet_handle_rcqe(struct lpfc_hba *phba, struct lpfc_queue *cq,\n\t\t\t    struct lpfc_rcqe *rcqe)\n{\n\tbool workposted = false;\n\tstruct lpfc_queue *hrq;\n\tstruct lpfc_queue *drq;\n\tstruct rqb_dmabuf *dma_buf;\n\tstruct fc_frame_header *fc_hdr;\n\tstruct lpfc_nvmet_tgtport *tgtp;\n\tuint32_t status, rq_id;\n\tunsigned long iflags;\n\tuint32_t fctl, idx;\n\n\tif ((phba->nvmet_support == 0) ||\n\t    (phba->sli4_hba.nvmet_cqset == NULL))\n\t\treturn workposted;\n\n\tidx = cq->queue_id - phba->sli4_hba.nvmet_cqset[0]->queue_id;\n\thrq = phba->sli4_hba.nvmet_mrq_hdr[idx];\n\tdrq = phba->sli4_hba.nvmet_mrq_data[idx];\n\n\t/* sanity check on queue memory */\n\tif (unlikely(!hrq) || unlikely(!drq))\n\t\treturn workposted;\n\n\tif (bf_get(lpfc_cqe_code, rcqe) == CQE_CODE_RECEIVE_V1)\n\t\trq_id = bf_get(lpfc_rcqe_rq_id_v1, rcqe);\n\telse\n\t\trq_id = bf_get(lpfc_rcqe_rq_id, rcqe);\n\n\tif ((phba->nvmet_support == 0) ||\n\t    (rq_id != hrq->queue_id))\n\t\treturn workposted;\n\n\tstatus = bf_get(lpfc_rcqe_status, rcqe);\n\tswitch (status) {\n\tcase FC_STATUS_RQ_BUF_LEN_EXCEEDED:\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6126 Receive Frame Truncated!!\\n\");\n\t\tfallthrough;\n\tcase FC_STATUS_RQ_SUCCESS:\n\t\tspin_lock_irqsave(&phba->hbalock, iflags);\n\t\tlpfc_sli4_rq_release(hrq, drq);\n\t\tdma_buf = lpfc_sli_rqbuf_get(phba, hrq);\n\t\tif (!dma_buf) {\n\t\t\thrq->RQ_no_buf_found++;\n\t\t\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t\t\tgoto out;\n\t\t}\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t\thrq->RQ_rcv_buf++;\n\t\thrq->RQ_buf_posted--;\n\t\tfc_hdr = (struct fc_frame_header *)dma_buf->hbuf.virt;\n\n\t\t/* Just some basic sanity checks on FCP Command frame */\n\t\tfctl = (fc_hdr->fh_f_ctl[0] << 16 |\n\t\t\tfc_hdr->fh_f_ctl[1] << 8 |\n\t\t\tfc_hdr->fh_f_ctl[2]);\n\t\tif (((fctl &\n\t\t    (FC_FC_FIRST_SEQ | FC_FC_END_SEQ | FC_FC_SEQ_INIT)) !=\n\t\t    (FC_FC_FIRST_SEQ | FC_FC_END_SEQ | FC_FC_SEQ_INIT)) ||\n\t\t    (fc_hdr->fh_seq_cnt != 0)) /* 0 byte swapped is still 0 */\n\t\t\tgoto drop;\n\n\t\tif (fc_hdr->fh_type == FC_TYPE_FCP) {\n\t\t\tdma_buf->bytes_recv = bf_get(lpfc_rcqe_length, rcqe);\n\t\t\tlpfc_nvmet_unsol_fcp_event(\n\t\t\t\tphba, idx, dma_buf, cq->isr_timestamp,\n\t\t\t\tcq->q_flag & HBA_NVMET_CQ_NOTIFY);\n\t\t\treturn false;\n\t\t}\ndrop:\n\t\tlpfc_rq_buf_free(phba, &dma_buf->hbuf);\n\t\tbreak;\n\tcase FC_STATUS_INSUFF_BUF_FRM_DISC:\n\t\tif (phba->nvmet_support) {\n\t\t\ttgtp = phba->targetport->private;\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"6401 RQE Error x%x, posted %d err_cnt \"\n\t\t\t\t\t\"%d: %x %x %x\\n\",\n\t\t\t\t\tstatus, hrq->RQ_buf_posted,\n\t\t\t\t\thrq->RQ_no_posted_buf,\n\t\t\t\t\tatomic_read(&tgtp->rcv_fcp_cmd_in),\n\t\t\t\t\tatomic_read(&tgtp->rcv_fcp_cmd_out),\n\t\t\t\t\tatomic_read(&tgtp->xmt_fcp_release));\n\t\t}\n\t\tfallthrough;\n\n\tcase FC_STATUS_INSUFF_BUF_NEED_BUF:\n\t\thrq->RQ_no_posted_buf++;\n\t\t/* Post more buffers if possible */\n\t\tbreak;\n\t}\nout:\n\treturn workposted;\n}\n\n/**\n * lpfc_sli4_fp_handle_cqe - Process fast-path work queue completion entry\n * @phba: adapter with cq\n * @cq: Pointer to the completion queue.\n * @cqe: Pointer to fast-path completion queue entry.\n *\n * This routine process a fast-path work queue completion entry from fast-path\n * event queue for FCP command response completion.\n *\n * Return: true if work posted to worker thread, otherwise false.\n **/\nstatic bool\nlpfc_sli4_fp_handle_cqe(struct lpfc_hba *phba, struct lpfc_queue *cq,\n\t\t\t struct lpfc_cqe *cqe)\n{\n\tstruct lpfc_wcqe_release wcqe;\n\tbool workposted = false;\n\n\t/* Copy the work queue CQE and convert endian order if needed */\n\tlpfc_sli4_pcimem_bcopy(cqe, &wcqe, sizeof(struct lpfc_cqe));\n\n\t/* Check and process for different type of WCQE and dispatch */\n\tswitch (bf_get(lpfc_wcqe_c_code, &wcqe)) {\n\tcase CQE_CODE_COMPL_WQE:\n\tcase CQE_CODE_NVME_ERSP:\n\t\tcq->CQ_wq++;\n\t\t/* Process the WQ complete event */\n\t\tphba->last_completion_time = jiffies;\n\t\tif (cq->subtype == LPFC_IO || cq->subtype == LPFC_NVME_LS)\n\t\t\tlpfc_sli4_fp_handle_fcp_wcqe(phba, cq,\n\t\t\t\t(struct lpfc_wcqe_complete *)&wcqe);\n\t\tbreak;\n\tcase CQE_CODE_RELEASE_WQE:\n\t\tcq->CQ_release_wqe++;\n\t\t/* Process the WQ release event */\n\t\tlpfc_sli4_fp_handle_rel_wcqe(phba, cq,\n\t\t\t\t(struct lpfc_wcqe_release *)&wcqe);\n\t\tbreak;\n\tcase CQE_CODE_XRI_ABORTED:\n\t\tcq->CQ_xri_aborted++;\n\t\t/* Process the WQ XRI abort event */\n\t\tphba->last_completion_time = jiffies;\n\t\tworkposted = lpfc_sli4_sp_handle_abort_xri_wcqe(phba, cq,\n\t\t\t\t(struct sli4_wcqe_xri_aborted *)&wcqe);\n\t\tbreak;\n\tcase CQE_CODE_RECEIVE_V1:\n\tcase CQE_CODE_RECEIVE:\n\t\tphba->last_completion_time = jiffies;\n\t\tif (cq->subtype == LPFC_NVMET) {\n\t\t\tworkposted = lpfc_sli4_nvmet_handle_rcqe(\n\t\t\t\tphba, cq, (struct lpfc_rcqe *)&wcqe);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0144 Not a valid CQE code: x%x\\n\",\n\t\t\t\tbf_get(lpfc_wcqe_c_code, &wcqe));\n\t\tbreak;\n\t}\n\treturn workposted;\n}\n\n/**\n * lpfc_sli4_sched_cq_work - Schedules cq work\n * @phba: Pointer to HBA context object.\n * @cq: Pointer to CQ\n * @cqid: CQ ID\n *\n * This routine checks the poll mode of the CQ corresponding to\n * cq->chann, then either schedules a softirq or queue_work to complete\n * cq work.\n *\n * queue_work path is taken if in NVMET mode, or if poll_mode is in\n * LPFC_QUEUE_WORK mode.  Otherwise, softirq path is taken.\n *\n **/\nstatic void lpfc_sli4_sched_cq_work(struct lpfc_hba *phba,\n\t\t\t\t    struct lpfc_queue *cq, uint16_t cqid)\n{\n\tint ret = 0;\n\n\tswitch (cq->poll_mode) {\n\tcase LPFC_IRQ_POLL:\n\t\tirq_poll_sched(&cq->iop);\n\t\tbreak;\n\tcase LPFC_QUEUE_WORK:\n\tdefault:\n\t\tif (is_kdump_kernel())\n\t\t\tret = queue_work(phba->wq, &cq->irqwork);\n\t\telse\n\t\t\tret = queue_work_on(cq->chann, phba->wq, &cq->irqwork);\n\t\tif (!ret)\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"0383 Cannot schedule queue work \"\n\t\t\t\t\t\"for CQ eqcqid=%d, cqid=%d on CPU %d\\n\",\n\t\t\t\t\tcqid, cq->queue_id,\n\t\t\t\t\traw_smp_processor_id());\n\t}\n}\n\n/**\n * lpfc_sli4_hba_handle_eqe - Process a fast-path event queue entry\n * @phba: Pointer to HBA context object.\n * @eq: Pointer to the queue structure.\n * @eqe: Pointer to fast-path event queue entry.\n *\n * This routine process a event queue entry from the fast-path event queue.\n * It will check the MajorCode and MinorCode to determine this is for a\n * completion event on a completion queue, if not, an error shall be logged\n * and just return. Otherwise, it will get to the corresponding completion\n * queue and process all the entries on the completion queue, rearm the\n * completion queue, and then return.\n **/\nstatic void\nlpfc_sli4_hba_handle_eqe(struct lpfc_hba *phba, struct lpfc_queue *eq,\n\t\t\t struct lpfc_eqe *eqe)\n{\n\tstruct lpfc_queue *cq = NULL;\n\tuint32_t qidx = eq->hdwq;\n\tuint16_t cqid, id;\n\n\tif (unlikely(bf_get_le32(lpfc_eqe_major_code, eqe) != 0)) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0366 Not a valid completion \"\n\t\t\t\t\"event: majorcode=x%x, minorcode=x%x\\n\",\n\t\t\t\tbf_get_le32(lpfc_eqe_major_code, eqe),\n\t\t\t\tbf_get_le32(lpfc_eqe_minor_code, eqe));\n\t\treturn;\n\t}\n\n\t/* Get the reference to the corresponding CQ */\n\tcqid = bf_get_le32(lpfc_eqe_resource_id, eqe);\n\n\t/* Use the fast lookup method first */\n\tif (cqid <= phba->sli4_hba.cq_max) {\n\t\tcq = phba->sli4_hba.cq_lookup[cqid];\n\t\tif (cq)\n\t\t\tgoto  work_cq;\n\t}\n\n\t/* Next check for NVMET completion */\n\tif (phba->cfg_nvmet_mrq && phba->sli4_hba.nvmet_cqset) {\n\t\tid = phba->sli4_hba.nvmet_cqset[0]->queue_id;\n\t\tif ((cqid >= id) && (cqid < (id + phba->cfg_nvmet_mrq))) {\n\t\t\t/* Process NVMET unsol rcv */\n\t\t\tcq = phba->sli4_hba.nvmet_cqset[cqid - id];\n\t\t\tgoto  process_cq;\n\t\t}\n\t}\n\n\tif (phba->sli4_hba.nvmels_cq &&\n\t    (cqid == phba->sli4_hba.nvmels_cq->queue_id)) {\n\t\t/* Process NVME unsol rcv */\n\t\tcq = phba->sli4_hba.nvmels_cq;\n\t}\n\n\t/* Otherwise this is a Slow path event */\n\tif (cq == NULL) {\n\t\tlpfc_sli4_sp_handle_eqe(phba, eqe,\n\t\t\t\t\tphba->sli4_hba.hdwq[qidx].hba_eq);\n\t\treturn;\n\t}\n\nprocess_cq:\n\tif (unlikely(cqid != cq->queue_id)) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0368 Miss-matched fast-path completion \"\n\t\t\t\t\"queue identifier: eqcqid=%d, fcpcqid=%d\\n\",\n\t\t\t\tcqid, cq->queue_id);\n\t\treturn;\n\t}\n\nwork_cq:\n#if defined(CONFIG_SCSI_LPFC_DEBUG_FS)\n\tif (phba->ktime_on)\n\t\tcq->isr_timestamp = ktime_get_ns();\n\telse\n\t\tcq->isr_timestamp = 0;\n#endif\n\tlpfc_sli4_sched_cq_work(phba, cq, cqid);\n}\n\n/**\n * __lpfc_sli4_hba_process_cq - Process a fast-path event queue entry\n * @cq: Pointer to CQ to be processed\n * @poll_mode: Enum lpfc_poll_state to determine poll mode\n *\n * This routine calls the cq processing routine with the handler for\n * fast path CQEs.\n *\n * The CQ routine returns two values: the first is the calling status,\n * which indicates whether work was queued to the  background discovery\n * thread. If true, the routine should wakeup the discovery thread;\n * the second is the delay parameter. If non-zero, rather than rearming\n * the CQ and yet another interrupt, the CQ handler should be queued so\n * that it is processed in a subsequent polling action. The value of\n * the delay indicates when to reschedule it.\n **/\nstatic void\n__lpfc_sli4_hba_process_cq(struct lpfc_queue *cq,\n\t\t\t   enum lpfc_poll_mode poll_mode)\n{\n\tstruct lpfc_hba *phba = cq->phba;\n\tunsigned long delay;\n\tbool workposted = false;\n\tint ret = 0;\n\n\t/* process and rearm the CQ */\n\tworkposted |= __lpfc_sli4_process_cq(phba, cq, lpfc_sli4_fp_handle_cqe,\n\t\t\t\t\t     &delay, poll_mode);\n\n\tif (delay) {\n\t\tif (is_kdump_kernel())\n\t\t\tret = queue_delayed_work(phba->wq, &cq->sched_irqwork,\n\t\t\t\t\t\tdelay);\n\t\telse\n\t\t\tret = queue_delayed_work_on(cq->chann, phba->wq,\n\t\t\t\t\t\t&cq->sched_irqwork, delay);\n\t\tif (!ret)\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"0367 Cannot schedule queue work \"\n\t\t\t\t\t\"for cqid=%d on CPU %d\\n\",\n\t\t\t\t\tcq->queue_id, cq->chann);\n\t}\n\n\t/* wake up worker thread if there are works to be done */\n\tif (workposted)\n\t\tlpfc_worker_wake_up(phba);\n}\n\n/**\n * lpfc_sli4_hba_process_cq - fast-path work handler when started by\n *   interrupt\n * @work: pointer to work element\n *\n * translates from the work handler and calls the fast-path handler.\n **/\nstatic void\nlpfc_sli4_hba_process_cq(struct work_struct *work)\n{\n\tstruct lpfc_queue *cq = container_of(work, struct lpfc_queue, irqwork);\n\n\t__lpfc_sli4_hba_process_cq(cq, LPFC_QUEUE_WORK);\n}\n\n/**\n * lpfc_sli4_hba_process_cq - fast-path work handler when started by timer\n * @work: pointer to work element\n *\n * translates from the work handler and calls the fast-path handler.\n **/\nstatic void\nlpfc_sli4_dly_hba_process_cq(struct work_struct *work)\n{\n\tstruct lpfc_queue *cq = container_of(to_delayed_work(work),\n\t\t\t\t\tstruct lpfc_queue, sched_irqwork);\n\n\t__lpfc_sli4_hba_process_cq(cq, LPFC_QUEUE_WORK);\n}\n\n/**\n * lpfc_sli4_hba_intr_handler - HBA interrupt handler to SLI-4 device\n * @irq: Interrupt number.\n * @dev_id: The device context pointer.\n *\n * This function is directly called from the PCI layer as an interrupt\n * service routine when device with SLI-4 interface spec is enabled with\n * MSI-X multi-message interrupt mode and there is a fast-path FCP IOCB\n * ring event in the HBA. However, when the device is enabled with either\n * MSI or Pin-IRQ interrupt mode, this function is called as part of the\n * device-level interrupt handler. When the PCI slot is in error recovery\n * or the HBA is undergoing initialization, the interrupt handler will not\n * process the interrupt. The SCSI FCP fast-path ring event are handled in\n * the intrrupt context. This function is called without any lock held.\n * It gets the hbalock to access and update SLI data structures. Note that,\n * the FCP EQ to FCP CQ are one-to-one map such that the FCP EQ index is\n * equal to that of FCP CQ index.\n *\n * The link attention and ELS ring attention events are handled\n * by the worker thread. The interrupt handler signals the worker thread\n * and returns for these events. This function is called without any lock\n * held. It gets the hbalock to access and update SLI data structures.\n *\n * This function returns IRQ_HANDLED when interrupt is handled else it\n * returns IRQ_NONE.\n **/\nirqreturn_t\nlpfc_sli4_hba_intr_handler(int irq, void *dev_id)\n{\n\tstruct lpfc_hba *phba;\n\tstruct lpfc_hba_eq_hdl *hba_eq_hdl;\n\tstruct lpfc_queue *fpeq;\n\tunsigned long iflag;\n\tint ecount = 0;\n\tint hba_eqidx;\n\tstruct lpfc_eq_intr_info *eqi;\n\n\t/* Get the driver's phba structure from the dev_id */\n\thba_eq_hdl = (struct lpfc_hba_eq_hdl *)dev_id;\n\tphba = hba_eq_hdl->phba;\n\thba_eqidx = hba_eq_hdl->idx;\n\n\tif (unlikely(!phba))\n\t\treturn IRQ_NONE;\n\tif (unlikely(!phba->sli4_hba.hdwq))\n\t\treturn IRQ_NONE;\n\n\t/* Get to the EQ struct associated with this vector */\n\tfpeq = phba->sli4_hba.hba_eq_hdl[hba_eqidx].eq;\n\tif (unlikely(!fpeq))\n\t\treturn IRQ_NONE;\n\n\t/* Check device state for handling interrupt */\n\tif (unlikely(lpfc_intr_state_check(phba))) {\n\t\t/* Check again for link_state with lock held */\n\t\tspin_lock_irqsave(&phba->hbalock, iflag);\n\t\tif (phba->link_state < LPFC_LINK_DOWN)\n\t\t\t/* Flush, clear interrupt, and rearm the EQ */\n\t\t\tlpfc_sli4_eqcq_flush(phba, fpeq);\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\treturn IRQ_NONE;\n\t}\n\n\teqi = this_cpu_ptr(phba->sli4_hba.eq_info);\n\teqi->icnt++;\n\n\tfpeq->last_cpu = raw_smp_processor_id();\n\n\tif (eqi->icnt > LPFC_EQD_ISR_TRIGGER &&\n\t    fpeq->q_flag & HBA_EQ_DELAY_CHK &&\n\t    phba->cfg_auto_imax &&\n\t    fpeq->q_mode != LPFC_MAX_AUTO_EQ_DELAY &&\n\t    phba->sli.sli_flag & LPFC_SLI_USE_EQDR)\n\t\tlpfc_sli4_mod_hba_eq_delay(phba, fpeq, LPFC_MAX_AUTO_EQ_DELAY);\n\n\t/* process and rearm the EQ */\n\tecount = lpfc_sli4_process_eq(phba, fpeq, LPFC_QUEUE_REARM);\n\n\tif (unlikely(ecount == 0)) {\n\t\tfpeq->EQ_no_entry++;\n\t\tif (phba->intr_type == MSIX)\n\t\t\t/* MSI-X treated interrupt served as no EQ share INT */\n\t\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_SLI,\n\t\t\t\t\t\"0358 MSI-X interrupt with no EQE\\n\");\n\t\telse\n\t\t\t/* Non MSI-X treated on interrupt as EQ share INT */\n\t\t\treturn IRQ_NONE;\n\t}\n\n\treturn IRQ_HANDLED;\n} /* lpfc_sli4_fp_intr_handler */\n\n/**\n * lpfc_sli4_intr_handler - Device-level interrupt handler for SLI-4 device\n * @irq: Interrupt number.\n * @dev_id: The device context pointer.\n *\n * This function is the device-level interrupt handler to device with SLI-4\n * interface spec, called from the PCI layer when either MSI or Pin-IRQ\n * interrupt mode is enabled and there is an event in the HBA which requires\n * driver attention. This function invokes the slow-path interrupt attention\n * handling function and fast-path interrupt attention handling function in\n * turn to process the relevant HBA attention events. This function is called\n * without any lock held. It gets the hbalock to access and update SLI data\n * structures.\n *\n * This function returns IRQ_HANDLED when interrupt is handled, else it\n * returns IRQ_NONE.\n **/\nirqreturn_t\nlpfc_sli4_intr_handler(int irq, void *dev_id)\n{\n\tstruct lpfc_hba  *phba;\n\tirqreturn_t hba_irq_rc;\n\tbool hba_handled = false;\n\tint qidx;\n\n\t/* Get the driver's phba structure from the dev_id */\n\tphba = (struct lpfc_hba *)dev_id;\n\n\tif (unlikely(!phba))\n\t\treturn IRQ_NONE;\n\n\t/*\n\t * Invoke fast-path host attention interrupt handling as appropriate.\n\t */\n\tfor (qidx = 0; qidx < phba->cfg_irq_chann; qidx++) {\n\t\thba_irq_rc = lpfc_sli4_hba_intr_handler(irq,\n\t\t\t\t\t&phba->sli4_hba.hba_eq_hdl[qidx]);\n\t\tif (hba_irq_rc == IRQ_HANDLED)\n\t\t\thba_handled |= true;\n\t}\n\n\treturn (hba_handled == true) ? IRQ_HANDLED : IRQ_NONE;\n} /* lpfc_sli4_intr_handler */\n\nvoid lpfc_sli4_poll_hbtimer(struct timer_list *t)\n{\n\tstruct lpfc_hba *phba = from_timer(phba, t, cpuhp_poll_timer);\n\tstruct lpfc_queue *eq;\n\tint i = 0;\n\n\trcu_read_lock();\n\n\tlist_for_each_entry_rcu(eq, &phba->poll_list, _poll_list)\n\t\ti += lpfc_sli4_poll_eq(eq, LPFC_POLL_SLOWPATH);\n\tif (!list_empty(&phba->poll_list))\n\t\tmod_timer(&phba->cpuhp_poll_timer,\n\t\t\t  jiffies + msecs_to_jiffies(LPFC_POLL_HB));\n\n\trcu_read_unlock();\n}\n\ninline int lpfc_sli4_poll_eq(struct lpfc_queue *eq, uint8_t path)\n{\n\tstruct lpfc_hba *phba = eq->phba;\n\tint i = 0;\n\n\t/*\n\t * Unlocking an irq is one of the entry point to check\n\t * for re-schedule, but we are good for io submission\n\t * path as midlayer does a get_cpu to glue us in. Flush\n\t * out the invalidate queue so we can see the updated\n\t * value for flag.\n\t */\n\tsmp_rmb();\n\n\tif (READ_ONCE(eq->mode) == LPFC_EQ_POLL)\n\t\t/* We will not likely get the completion for the caller\n\t\t * during this iteration but i guess that's fine.\n\t\t * Future io's coming on this eq should be able to\n\t\t * pick it up.  As for the case of single io's, they\n\t\t * will be handled through a sched from polling timer\n\t\t * function which is currently triggered every 1msec.\n\t\t */\n\t\ti = lpfc_sli4_process_eq(phba, eq, LPFC_QUEUE_NOARM);\n\n\treturn i;\n}\n\nstatic inline void lpfc_sli4_add_to_poll_list(struct lpfc_queue *eq)\n{\n\tstruct lpfc_hba *phba = eq->phba;\n\n\t/* kickstart slowpath processing if needed */\n\tif (list_empty(&phba->poll_list))\n\t\tmod_timer(&phba->cpuhp_poll_timer,\n\t\t\t  jiffies + msecs_to_jiffies(LPFC_POLL_HB));\n\n\tlist_add_rcu(&eq->_poll_list, &phba->poll_list);\n\tsynchronize_rcu();\n}\n\nstatic inline void lpfc_sli4_remove_from_poll_list(struct lpfc_queue *eq)\n{\n\tstruct lpfc_hba *phba = eq->phba;\n\n\t/* Disable slowpath processing for this eq.  Kick start the eq\n\t * by RE-ARMING the eq's ASAP\n\t */\n\tlist_del_rcu(&eq->_poll_list);\n\tsynchronize_rcu();\n\n\tif (list_empty(&phba->poll_list))\n\t\tdel_timer_sync(&phba->cpuhp_poll_timer);\n}\n\nvoid lpfc_sli4_cleanup_poll_list(struct lpfc_hba *phba)\n{\n\tstruct lpfc_queue *eq, *next;\n\n\tlist_for_each_entry_safe(eq, next, &phba->poll_list, _poll_list)\n\t\tlist_del(&eq->_poll_list);\n\n\tINIT_LIST_HEAD(&phba->poll_list);\n\tsynchronize_rcu();\n}\n\nstatic inline void\n__lpfc_sli4_switch_eqmode(struct lpfc_queue *eq, uint8_t mode)\n{\n\tif (mode == eq->mode)\n\t\treturn;\n\t/*\n\t * currently this function is only called during a hotplug\n\t * event and the cpu on which this function is executing\n\t * is going offline.  By now the hotplug has instructed\n\t * the scheduler to remove this cpu from cpu active mask.\n\t * So we don't need to work about being put aside by the\n\t * scheduler for a high priority process.  Yes, the inte-\n\t * rrupts could come but they are known to retire ASAP.\n\t */\n\n\t/* Disable polling in the fastpath */\n\tWRITE_ONCE(eq->mode, mode);\n\t/* flush out the store buffer */\n\tsmp_wmb();\n\n\t/*\n\t * Add this eq to the polling list and start polling. For\n\t * a grace period both interrupt handler and poller will\n\t * try to process the eq _but_ that's fine.  We have a\n\t * synchronization mechanism in place (queue_claimed) to\n\t * deal with it.  This is just a draining phase for int-\n\t * errupt handler (not eq's) as we have guranteed through\n\t * barrier that all the CPUs have seen the new CQ_POLLED\n\t * state. which will effectively disable the REARMING of\n\t * the EQ.  The whole idea is eq's die off eventually as\n\t * we are not rearming EQ's anymore.\n\t */\n\tmode ? lpfc_sli4_add_to_poll_list(eq) :\n\t       lpfc_sli4_remove_from_poll_list(eq);\n}\n\nvoid lpfc_sli4_start_polling(struct lpfc_queue *eq)\n{\n\t__lpfc_sli4_switch_eqmode(eq, LPFC_EQ_POLL);\n}\n\nvoid lpfc_sli4_stop_polling(struct lpfc_queue *eq)\n{\n\tstruct lpfc_hba *phba = eq->phba;\n\n\t__lpfc_sli4_switch_eqmode(eq, LPFC_EQ_INTERRUPT);\n\n\t/* Kick start for the pending io's in h/w.\n\t * Once we switch back to interrupt processing on a eq\n\t * the io path completion will only arm eq's when it\n\t * receives a completion.  But since eq's are in disa-\n\t * rmed state it doesn't receive a completion.  This\n\t * creates a deadlock scenaro.\n\t */\n\tphba->sli4_hba.sli4_write_eq_db(phba, eq, 0, LPFC_QUEUE_REARM);\n}\n\n/**\n * lpfc_sli4_queue_free - free a queue structure and associated memory\n * @queue: The queue structure to free.\n *\n * This function frees a queue structure and the DMAable memory used for\n * the host resident queue. This function must be called after destroying the\n * queue on the HBA.\n **/\nvoid\nlpfc_sli4_queue_free(struct lpfc_queue *queue)\n{\n\tstruct lpfc_dmabuf *dmabuf;\n\n\tif (!queue)\n\t\treturn;\n\n\tif (!list_empty(&queue->wq_list))\n\t\tlist_del(&queue->wq_list);\n\n\twhile (!list_empty(&queue->page_list)) {\n\t\tlist_remove_head(&queue->page_list, dmabuf, struct lpfc_dmabuf,\n\t\t\t\t list);\n\t\tdma_free_coherent(&queue->phba->pcidev->dev, queue->page_size,\n\t\t\t\t  dmabuf->virt, dmabuf->phys);\n\t\tkfree(dmabuf);\n\t}\n\tif (queue->rqbp) {\n\t\tlpfc_free_rq_buffer(queue->phba, queue);\n\t\tkfree(queue->rqbp);\n\t}\n\n\tif (!list_empty(&queue->cpu_list))\n\t\tlist_del(&queue->cpu_list);\n\n\tkfree(queue);\n\treturn;\n}\n\n/**\n * lpfc_sli4_queue_alloc - Allocate and initialize a queue structure\n * @phba: The HBA that this queue is being created on.\n * @page_size: The size of a queue page\n * @entry_size: The size of each queue entry for this queue.\n * @entry_count: The number of entries that this queue will handle.\n * @cpu: The cpu that will primarily utilize this queue.\n *\n * This function allocates a queue structure and the DMAable memory used for\n * the host resident queue. This function must be called before creating the\n * queue on the HBA.\n **/\nstruct lpfc_queue *\nlpfc_sli4_queue_alloc(struct lpfc_hba *phba, uint32_t page_size,\n\t\t      uint32_t entry_size, uint32_t entry_count, int cpu)\n{\n\tstruct lpfc_queue *queue;\n\tstruct lpfc_dmabuf *dmabuf;\n\tuint32_t hw_page_size = phba->sli4_hba.pc_sli4_params.if_page_sz;\n\tuint16_t x, pgcnt;\n\n\tif (!phba->sli4_hba.pc_sli4_params.supported)\n\t\thw_page_size = page_size;\n\n\tpgcnt = ALIGN(entry_size * entry_count, hw_page_size) / hw_page_size;\n\n\t/* If needed, Adjust page count to match the max the adapter supports */\n\tif (pgcnt > phba->sli4_hba.pc_sli4_params.wqpcnt)\n\t\tpgcnt = phba->sli4_hba.pc_sli4_params.wqpcnt;\n\n\tqueue = kzalloc_node(sizeof(*queue) + (sizeof(void *) * pgcnt),\n\t\t\t     GFP_KERNEL, cpu_to_node(cpu));\n\tif (!queue)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&queue->list);\n\tINIT_LIST_HEAD(&queue->_poll_list);\n\tINIT_LIST_HEAD(&queue->wq_list);\n\tINIT_LIST_HEAD(&queue->wqfull_list);\n\tINIT_LIST_HEAD(&queue->page_list);\n\tINIT_LIST_HEAD(&queue->child_list);\n\tINIT_LIST_HEAD(&queue->cpu_list);\n\n\t/* Set queue parameters now.  If the system cannot provide memory\n\t * resources, the free routine needs to know what was allocated.\n\t */\n\tqueue->page_count = pgcnt;\n\tqueue->q_pgs = (void **)&queue[1];\n\tqueue->entry_cnt_per_pg = hw_page_size / entry_size;\n\tqueue->entry_size = entry_size;\n\tqueue->entry_count = entry_count;\n\tqueue->page_size = hw_page_size;\n\tqueue->phba = phba;\n\n\tfor (x = 0; x < queue->page_count; x++) {\n\t\tdmabuf = kzalloc_node(sizeof(*dmabuf), GFP_KERNEL,\n\t\t\t\t      dev_to_node(&phba->pcidev->dev));\n\t\tif (!dmabuf)\n\t\t\tgoto out_fail;\n\t\tdmabuf->virt = dma_alloc_coherent(&phba->pcidev->dev,\n\t\t\t\t\t\t  hw_page_size, &dmabuf->phys,\n\t\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!dmabuf->virt) {\n\t\t\tkfree(dmabuf);\n\t\t\tgoto out_fail;\n\t\t}\n\t\tdmabuf->buffer_tag = x;\n\t\tlist_add_tail(&dmabuf->list, &queue->page_list);\n\t\t/* use lpfc_sli4_qe to index a paritcular entry in this page */\n\t\tqueue->q_pgs[x] = dmabuf->virt;\n\t}\n\tINIT_WORK(&queue->irqwork, lpfc_sli4_hba_process_cq);\n\tINIT_WORK(&queue->spwork, lpfc_sli4_sp_process_cq);\n\tINIT_DELAYED_WORK(&queue->sched_irqwork, lpfc_sli4_dly_hba_process_cq);\n\tINIT_DELAYED_WORK(&queue->sched_spwork, lpfc_sli4_dly_sp_process_cq);\n\n\t/* notify_interval will be set during q creation */\n\n\treturn queue;\nout_fail:\n\tlpfc_sli4_queue_free(queue);\n\treturn NULL;\n}\n\n/**\n * lpfc_dual_chute_pci_bar_map - Map pci base address register to host memory\n * @phba: HBA structure that indicates port to create a queue on.\n * @pci_barset: PCI BAR set flag.\n *\n * This function shall perform iomap of the specified PCI BAR address to host\n * memory address if not already done so and return it. The returned host\n * memory address can be NULL.\n */\nstatic void __iomem *\nlpfc_dual_chute_pci_bar_map(struct lpfc_hba *phba, uint16_t pci_barset)\n{\n\tif (!phba->pcidev)\n\t\treturn NULL;\n\n\tswitch (pci_barset) {\n\tcase WQ_PCI_BAR_0_AND_1:\n\t\treturn phba->pci_bar0_memmap_p;\n\tcase WQ_PCI_BAR_2_AND_3:\n\t\treturn phba->pci_bar2_memmap_p;\n\tcase WQ_PCI_BAR_4_AND_5:\n\t\treturn phba->pci_bar4_memmap_p;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn NULL;\n}\n\n/**\n * lpfc_modify_hba_eq_delay - Modify Delay Multiplier on EQs\n * @phba: HBA structure that EQs are on.\n * @startq: The starting EQ index to modify\n * @numq: The number of EQs (consecutive indexes) to modify\n * @usdelay: amount of delay\n *\n * This function revises the EQ delay on 1 or more EQs. The EQ delay\n * is set either by writing to a register (if supported by the SLI Port)\n * or by mailbox command. The mailbox command allows several EQs to be\n * updated at once.\n *\n * The @phba struct is used to send a mailbox command to HBA. The @startq\n * is used to get the starting EQ index to change. The @numq value is\n * used to specify how many consecutive EQ indexes, starting at EQ index,\n * are to be changed. This function is asynchronous and will wait for any\n * mailbox commands to finish before returning.\n *\n * On success this function will return a zero. If unable to allocate\n * enough memory this function will return -ENOMEM. If a mailbox command\n * fails this function will return -ENXIO. Note: on ENXIO, some EQs may\n * have had their delay multipler changed.\n **/\nvoid\nlpfc_modify_hba_eq_delay(struct lpfc_hba *phba, uint32_t startq,\n\t\t\t uint32_t numq, uint32_t usdelay)\n{\n\tstruct lpfc_mbx_modify_eq_delay *eq_delay;\n\tLPFC_MBOXQ_t *mbox;\n\tstruct lpfc_queue *eq;\n\tint cnt = 0, rc, length;\n\tuint32_t shdr_status, shdr_add_status;\n\tuint32_t dmult;\n\tint qidx;\n\tunion lpfc_sli4_cfg_shdr *shdr;\n\n\tif (startq >= phba->cfg_irq_chann)\n\t\treturn;\n\n\tif (usdelay > 0xFFFF) {\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_INIT | LOG_FCP | LOG_NVME,\n\t\t\t\t\"6429 usdelay %d too large. Scaled down to \"\n\t\t\t\t\"0xFFFF.\\n\", usdelay);\n\t\tusdelay = 0xFFFF;\n\t}\n\n\t/* set values by EQ_DELAY register if supported */\n\tif (phba->sli.sli_flag & LPFC_SLI_USE_EQDR) {\n\t\tfor (qidx = startq; qidx < phba->cfg_irq_chann; qidx++) {\n\t\t\teq = phba->sli4_hba.hba_eq_hdl[qidx].eq;\n\t\t\tif (!eq)\n\t\t\t\tcontinue;\n\n\t\t\tlpfc_sli4_mod_hba_eq_delay(phba, eq, usdelay);\n\n\t\t\tif (++cnt >= numq)\n\t\t\t\tbreak;\n\t\t}\n\t\treturn;\n\t}\n\n\t/* Otherwise, set values by mailbox cmd */\n\n\tmbox = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6428 Failed allocating mailbox cmd buffer.\"\n\t\t\t\t\" EQ delay was not set.\\n\");\n\t\treturn;\n\t}\n\tlength = (sizeof(struct lpfc_mbx_modify_eq_delay) -\n\t\t  sizeof(struct lpfc_sli4_cfg_mhdr));\n\tlpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_COMMON,\n\t\t\t LPFC_MBOX_OPCODE_MODIFY_EQ_DELAY,\n\t\t\t length, LPFC_SLI4_MBX_EMBED);\n\teq_delay = &mbox->u.mqe.un.eq_delay;\n\n\t/* Calculate delay multiper from maximum interrupt per second */\n\tdmult = (usdelay * LPFC_DMULT_CONST) / LPFC_SEC_TO_USEC;\n\tif (dmult)\n\t\tdmult--;\n\tif (dmult > LPFC_DMULT_MAX)\n\t\tdmult = LPFC_DMULT_MAX;\n\n\tfor (qidx = startq; qidx < phba->cfg_irq_chann; qidx++) {\n\t\teq = phba->sli4_hba.hba_eq_hdl[qidx].eq;\n\t\tif (!eq)\n\t\t\tcontinue;\n\t\teq->q_mode = usdelay;\n\t\teq_delay->u.request.eq[cnt].eq_id = eq->queue_id;\n\t\teq_delay->u.request.eq[cnt].phase = 0;\n\t\teq_delay->u.request.eq[cnt].delay_multi = dmult;\n\n\t\tif (++cnt >= numq)\n\t\t\tbreak;\n\t}\n\teq_delay->u.request.num_eq = cnt;\n\n\tmbox->vport = phba->pport;\n\tmbox->mbox_cmpl = lpfc_sli_def_mbox_cmpl;\n\tmbox->ctx_buf = NULL;\n\tmbox->ctx_ndlp = NULL;\n\trc = lpfc_sli_issue_mbox(phba, mbox, MBX_POLL);\n\tshdr = (union lpfc_sli4_cfg_shdr *) &eq_delay->header.cfg_shdr;\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2512 MODIFY_EQ_DELAY mailbox failed with \"\n\t\t\t\t\"status x%x add_status x%x, mbx status x%x\\n\",\n\t\t\t\tshdr_status, shdr_add_status, rc);\n\t}\n\tmempool_free(mbox, phba->mbox_mem_pool);\n\treturn;\n}\n\n/**\n * lpfc_eq_create - Create an Event Queue on the HBA\n * @phba: HBA structure that indicates port to create a queue on.\n * @eq: The queue structure to use to create the event queue.\n * @imax: The maximum interrupt per second limit.\n *\n * This function creates an event queue, as detailed in @eq, on a port,\n * described by @phba by sending an EQ_CREATE mailbox command to the HBA.\n *\n * The @phba struct is used to send mailbox command to HBA. The @eq struct\n * is used to get the entry count and entry size that are necessary to\n * determine the number of pages to allocate and use for this queue. This\n * function will send the EQ_CREATE mailbox command to the HBA to setup the\n * event queue. This function is asynchronous and will wait for the mailbox\n * command to finish before continuing.\n *\n * On success this function will return a zero. If unable to allocate enough\n * memory this function will return -ENOMEM. If the queue create mailbox command\n * fails this function will return -ENXIO.\n **/\nint\nlpfc_eq_create(struct lpfc_hba *phba, struct lpfc_queue *eq, uint32_t imax)\n{\n\tstruct lpfc_mbx_eq_create *eq_create;\n\tLPFC_MBOXQ_t *mbox;\n\tint rc, length, status = 0;\n\tstruct lpfc_dmabuf *dmabuf;\n\tuint32_t shdr_status, shdr_add_status;\n\tunion lpfc_sli4_cfg_shdr *shdr;\n\tuint16_t dmult;\n\tuint32_t hw_page_size = phba->sli4_hba.pc_sli4_params.if_page_sz;\n\n\t/* sanity check on queue memory */\n\tif (!eq)\n\t\treturn -ENODEV;\n\tif (!phba->sli4_hba.pc_sli4_params.supported)\n\t\thw_page_size = SLI4_PAGE_SIZE;\n\n\tmbox = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox)\n\t\treturn -ENOMEM;\n\tlength = (sizeof(struct lpfc_mbx_eq_create) -\n\t\t  sizeof(struct lpfc_sli4_cfg_mhdr));\n\tlpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_COMMON,\n\t\t\t LPFC_MBOX_OPCODE_EQ_CREATE,\n\t\t\t length, LPFC_SLI4_MBX_EMBED);\n\teq_create = &mbox->u.mqe.un.eq_create;\n\tshdr = (union lpfc_sli4_cfg_shdr *) &eq_create->header.cfg_shdr;\n\tbf_set(lpfc_mbx_eq_create_num_pages, &eq_create->u.request,\n\t       eq->page_count);\n\tbf_set(lpfc_eq_context_size, &eq_create->u.request.context,\n\t       LPFC_EQE_SIZE);\n\tbf_set(lpfc_eq_context_valid, &eq_create->u.request.context, 1);\n\n\t/* Use version 2 of CREATE_EQ if eqav is set */\n\tif (phba->sli4_hba.pc_sli4_params.eqav) {\n\t\tbf_set(lpfc_mbox_hdr_version, &shdr->request,\n\t\t       LPFC_Q_CREATE_VERSION_2);\n\t\tbf_set(lpfc_eq_context_autovalid, &eq_create->u.request.context,\n\t\t       phba->sli4_hba.pc_sli4_params.eqav);\n\t}\n\n\t/* don't setup delay multiplier using EQ_CREATE */\n\tdmult = 0;\n\tbf_set(lpfc_eq_context_delay_multi, &eq_create->u.request.context,\n\t       dmult);\n\tswitch (eq->entry_count) {\n\tdefault:\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0360 Unsupported EQ count. (%d)\\n\",\n\t\t\t\teq->entry_count);\n\t\tif (eq->entry_count < 256) {\n\t\t\tstatus = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tfallthrough;\t/* otherwise default to smallest count */\n\tcase 256:\n\t\tbf_set(lpfc_eq_context_count, &eq_create->u.request.context,\n\t\t       LPFC_EQ_CNT_256);\n\t\tbreak;\n\tcase 512:\n\t\tbf_set(lpfc_eq_context_count, &eq_create->u.request.context,\n\t\t       LPFC_EQ_CNT_512);\n\t\tbreak;\n\tcase 1024:\n\t\tbf_set(lpfc_eq_context_count, &eq_create->u.request.context,\n\t\t       LPFC_EQ_CNT_1024);\n\t\tbreak;\n\tcase 2048:\n\t\tbf_set(lpfc_eq_context_count, &eq_create->u.request.context,\n\t\t       LPFC_EQ_CNT_2048);\n\t\tbreak;\n\tcase 4096:\n\t\tbf_set(lpfc_eq_context_count, &eq_create->u.request.context,\n\t\t       LPFC_EQ_CNT_4096);\n\t\tbreak;\n\t}\n\tlist_for_each_entry(dmabuf, &eq->page_list, list) {\n\t\tmemset(dmabuf->virt, 0, hw_page_size);\n\t\teq_create->u.request.page[dmabuf->buffer_tag].addr_lo =\n\t\t\t\t\tputPaddrLow(dmabuf->phys);\n\t\teq_create->u.request.page[dmabuf->buffer_tag].addr_hi =\n\t\t\t\t\tputPaddrHigh(dmabuf->phys);\n\t}\n\tmbox->vport = phba->pport;\n\tmbox->mbox_cmpl = lpfc_sli_def_mbox_cmpl;\n\tmbox->ctx_buf = NULL;\n\tmbox->ctx_ndlp = NULL;\n\trc = lpfc_sli_issue_mbox(phba, mbox, MBX_POLL);\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2500 EQ_CREATE mailbox failed with \"\n\t\t\t\t\"status x%x add_status x%x, mbx status x%x\\n\",\n\t\t\t\tshdr_status, shdr_add_status, rc);\n\t\tstatus = -ENXIO;\n\t}\n\teq->type = LPFC_EQ;\n\teq->subtype = LPFC_NONE;\n\teq->queue_id = bf_get(lpfc_mbx_eq_create_q_id, &eq_create->u.response);\n\tif (eq->queue_id == 0xFFFF)\n\t\tstatus = -ENXIO;\n\teq->host_index = 0;\n\teq->notify_interval = LPFC_EQ_NOTIFY_INTRVL;\n\teq->max_proc_limit = LPFC_EQ_MAX_PROC_LIMIT;\nout:\n\tmempool_free(mbox, phba->mbox_mem_pool);\n\treturn status;\n}\n\nstatic int lpfc_cq_poll_hdler(struct irq_poll *iop, int budget)\n{\n\tstruct lpfc_queue *cq = container_of(iop, struct lpfc_queue, iop);\n\n\t__lpfc_sli4_hba_process_cq(cq, LPFC_IRQ_POLL);\n\n\treturn 1;\n}\n\n/**\n * lpfc_cq_create - Create a Completion Queue on the HBA\n * @phba: HBA structure that indicates port to create a queue on.\n * @cq: The queue structure to use to create the completion queue.\n * @eq: The event queue to bind this completion queue to.\n * @type: Type of queue (EQ, GCQ, MCQ, WCQ, etc).\n * @subtype: Functional purpose of the queue (MBOX, IO, ELS, NVMET, etc).\n *\n * This function creates a completion queue, as detailed in @wq, on a port,\n * described by @phba by sending a CQ_CREATE mailbox command to the HBA.\n *\n * The @phba struct is used to send mailbox command to HBA. The @cq struct\n * is used to get the entry count and entry size that are necessary to\n * determine the number of pages to allocate and use for this queue. The @eq\n * is used to indicate which event queue to bind this completion queue to. This\n * function will send the CQ_CREATE mailbox command to the HBA to setup the\n * completion queue. This function is asynchronous and will wait for the mailbox\n * command to finish before continuing.\n *\n * On success this function will return a zero. If unable to allocate enough\n * memory this function will return -ENOMEM. If the queue create mailbox command\n * fails this function will return -ENXIO.\n **/\nint\nlpfc_cq_create(struct lpfc_hba *phba, struct lpfc_queue *cq,\n\t       struct lpfc_queue *eq, uint32_t type, uint32_t subtype)\n{\n\tstruct lpfc_mbx_cq_create *cq_create;\n\tstruct lpfc_dmabuf *dmabuf;\n\tLPFC_MBOXQ_t *mbox;\n\tint rc, length, status = 0;\n\tuint32_t shdr_status, shdr_add_status;\n\tunion lpfc_sli4_cfg_shdr *shdr;\n\n\t/* sanity check on queue memory */\n\tif (!cq || !eq)\n\t\treturn -ENODEV;\n\n\tmbox = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox)\n\t\treturn -ENOMEM;\n\tlength = (sizeof(struct lpfc_mbx_cq_create) -\n\t\t  sizeof(struct lpfc_sli4_cfg_mhdr));\n\tlpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_COMMON,\n\t\t\t LPFC_MBOX_OPCODE_CQ_CREATE,\n\t\t\t length, LPFC_SLI4_MBX_EMBED);\n\tcq_create = &mbox->u.mqe.un.cq_create;\n\tshdr = (union lpfc_sli4_cfg_shdr *) &cq_create->header.cfg_shdr;\n\tbf_set(lpfc_mbx_cq_create_num_pages, &cq_create->u.request,\n\t\t    cq->page_count);\n\tbf_set(lpfc_cq_context_event, &cq_create->u.request.context, 1);\n\tbf_set(lpfc_cq_context_valid, &cq_create->u.request.context, 1);\n\tbf_set(lpfc_mbox_hdr_version, &shdr->request,\n\t       phba->sli4_hba.pc_sli4_params.cqv);\n\tif (phba->sli4_hba.pc_sli4_params.cqv == LPFC_Q_CREATE_VERSION_2) {\n\t\tbf_set(lpfc_mbx_cq_create_page_size, &cq_create->u.request,\n\t\t       (cq->page_size / SLI4_PAGE_SIZE));\n\t\tbf_set(lpfc_cq_eq_id_2, &cq_create->u.request.context,\n\t\t       eq->queue_id);\n\t\tbf_set(lpfc_cq_context_autovalid, &cq_create->u.request.context,\n\t\t       phba->sli4_hba.pc_sli4_params.cqav);\n\t} else {\n\t\tbf_set(lpfc_cq_eq_id, &cq_create->u.request.context,\n\t\t       eq->queue_id);\n\t}\n\tswitch (cq->entry_count) {\n\tcase 2048:\n\tcase 4096:\n\t\tif (phba->sli4_hba.pc_sli4_params.cqv ==\n\t\t    LPFC_Q_CREATE_VERSION_2) {\n\t\t\tcq_create->u.request.context.lpfc_cq_context_count =\n\t\t\t\tcq->entry_count;\n\t\t\tbf_set(lpfc_cq_context_count,\n\t\t\t       &cq_create->u.request.context,\n\t\t\t       LPFC_CQ_CNT_WORD7);\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\tdefault:\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0361 Unsupported CQ count: \"\n\t\t\t\t\"entry cnt %d sz %d pg cnt %d\\n\",\n\t\t\t\tcq->entry_count, cq->entry_size,\n\t\t\t\tcq->page_count);\n\t\tif (cq->entry_count < 256) {\n\t\t\tstatus = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tfallthrough;\t/* otherwise default to smallest count */\n\tcase 256:\n\t\tbf_set(lpfc_cq_context_count, &cq_create->u.request.context,\n\t\t       LPFC_CQ_CNT_256);\n\t\tbreak;\n\tcase 512:\n\t\tbf_set(lpfc_cq_context_count, &cq_create->u.request.context,\n\t\t       LPFC_CQ_CNT_512);\n\t\tbreak;\n\tcase 1024:\n\t\tbf_set(lpfc_cq_context_count, &cq_create->u.request.context,\n\t\t       LPFC_CQ_CNT_1024);\n\t\tbreak;\n\t}\n\tlist_for_each_entry(dmabuf, &cq->page_list, list) {\n\t\tmemset(dmabuf->virt, 0, cq->page_size);\n\t\tcq_create->u.request.page[dmabuf->buffer_tag].addr_lo =\n\t\t\t\t\tputPaddrLow(dmabuf->phys);\n\t\tcq_create->u.request.page[dmabuf->buffer_tag].addr_hi =\n\t\t\t\t\tputPaddrHigh(dmabuf->phys);\n\t}\n\trc = lpfc_sli_issue_mbox(phba, mbox, MBX_POLL);\n\n\t/* The IOCTL status is embedded in the mailbox subheader. */\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2501 CQ_CREATE mailbox failed with \"\n\t\t\t\t\"status x%x add_status x%x, mbx status x%x\\n\",\n\t\t\t\tshdr_status, shdr_add_status, rc);\n\t\tstatus = -ENXIO;\n\t\tgoto out;\n\t}\n\tcq->queue_id = bf_get(lpfc_mbx_cq_create_q_id, &cq_create->u.response);\n\tif (cq->queue_id == 0xFFFF) {\n\t\tstatus = -ENXIO;\n\t\tgoto out;\n\t}\n\t/* link the cq onto the parent eq child list */\n\tlist_add_tail(&cq->list, &eq->child_list);\n\t/* Set up completion queue's type and subtype */\n\tcq->type = type;\n\tcq->subtype = subtype;\n\tcq->queue_id = bf_get(lpfc_mbx_cq_create_q_id, &cq_create->u.response);\n\tcq->assoc_qid = eq->queue_id;\n\tcq->assoc_qp = eq;\n\tcq->host_index = 0;\n\tcq->notify_interval = LPFC_CQ_NOTIFY_INTRVL;\n\tcq->max_proc_limit = min(phba->cfg_cq_max_proc_limit, cq->entry_count);\n\n\tif (cq->queue_id > phba->sli4_hba.cq_max)\n\t\tphba->sli4_hba.cq_max = cq->queue_id;\n\n\tirq_poll_init(&cq->iop, LPFC_IRQ_POLL_WEIGHT, lpfc_cq_poll_hdler);\nout:\n\tmempool_free(mbox, phba->mbox_mem_pool);\n\treturn status;\n}\n\n/**\n * lpfc_cq_create_set - Create a set of Completion Queues on the HBA for MRQ\n * @phba: HBA structure that indicates port to create a queue on.\n * @cqp: The queue structure array to use to create the completion queues.\n * @hdwq: The hardware queue array  with the EQ to bind completion queues to.\n * @type: Type of queue (EQ, GCQ, MCQ, WCQ, etc).\n * @subtype: Functional purpose of the queue (MBOX, IO, ELS, NVMET, etc).\n *\n * This function creates a set of  completion queue, s to support MRQ\n * as detailed in @cqp, on a port,\n * described by @phba by sending a CREATE_CQ_SET mailbox command to the HBA.\n *\n * The @phba struct is used to send mailbox command to HBA. The @cq struct\n * is used to get the entry count and entry size that are necessary to\n * determine the number of pages to allocate and use for this queue. The @eq\n * is used to indicate which event queue to bind this completion queue to. This\n * function will send the CREATE_CQ_SET mailbox command to the HBA to setup the\n * completion queue. This function is asynchronous and will wait for the mailbox\n * command to finish before continuing.\n *\n * On success this function will return a zero. If unable to allocate enough\n * memory this function will return -ENOMEM. If the queue create mailbox command\n * fails this function will return -ENXIO.\n **/\nint\nlpfc_cq_create_set(struct lpfc_hba *phba, struct lpfc_queue **cqp,\n\t\t   struct lpfc_sli4_hdw_queue *hdwq, uint32_t type,\n\t\t   uint32_t subtype)\n{\n\tstruct lpfc_queue *cq;\n\tstruct lpfc_queue *eq;\n\tstruct lpfc_mbx_cq_create_set *cq_set;\n\tstruct lpfc_dmabuf *dmabuf;\n\tLPFC_MBOXQ_t *mbox;\n\tint rc, length, alloclen, status = 0;\n\tint cnt, idx, numcq, page_idx = 0;\n\tuint32_t shdr_status, shdr_add_status;\n\tunion lpfc_sli4_cfg_shdr *shdr;\n\tuint32_t hw_page_size = phba->sli4_hba.pc_sli4_params.if_page_sz;\n\n\t/* sanity check on queue memory */\n\tnumcq = phba->cfg_nvmet_mrq;\n\tif (!cqp || !hdwq || !numcq)\n\t\treturn -ENODEV;\n\n\tmbox = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox)\n\t\treturn -ENOMEM;\n\n\tlength = sizeof(struct lpfc_mbx_cq_create_set);\n\tlength += ((numcq * cqp[0]->page_count) *\n\t\t   sizeof(struct dma_address));\n\talloclen = lpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_FCOE,\n\t\t\tLPFC_MBOX_OPCODE_FCOE_CQ_CREATE_SET, length,\n\t\t\tLPFC_SLI4_MBX_NEMBED);\n\tif (alloclen < length) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"3098 Allocated DMA memory size (%d) is \"\n\t\t\t\t\"less than the requested DMA memory size \"\n\t\t\t\t\"(%d)\\n\", alloclen, length);\n\t\tstatus = -ENOMEM;\n\t\tgoto out;\n\t}\n\tcq_set = mbox->sge_array->addr[0];\n\tshdr = (union lpfc_sli4_cfg_shdr *)&cq_set->cfg_shdr;\n\tbf_set(lpfc_mbox_hdr_version, &shdr->request, 0);\n\n\tfor (idx = 0; idx < numcq; idx++) {\n\t\tcq = cqp[idx];\n\t\teq = hdwq[idx].hba_eq;\n\t\tif (!cq || !eq) {\n\t\t\tstatus = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tif (!phba->sli4_hba.pc_sli4_params.supported)\n\t\t\thw_page_size = cq->page_size;\n\n\t\tswitch (idx) {\n\t\tcase 0:\n\t\t\tbf_set(lpfc_mbx_cq_create_set_page_size,\n\t\t\t       &cq_set->u.request,\n\t\t\t       (hw_page_size / SLI4_PAGE_SIZE));\n\t\t\tbf_set(lpfc_mbx_cq_create_set_num_pages,\n\t\t\t       &cq_set->u.request, cq->page_count);\n\t\t\tbf_set(lpfc_mbx_cq_create_set_evt,\n\t\t\t       &cq_set->u.request, 1);\n\t\t\tbf_set(lpfc_mbx_cq_create_set_valid,\n\t\t\t       &cq_set->u.request, 1);\n\t\t\tbf_set(lpfc_mbx_cq_create_set_cqe_size,\n\t\t\t       &cq_set->u.request, 0);\n\t\t\tbf_set(lpfc_mbx_cq_create_set_num_cq,\n\t\t\t       &cq_set->u.request, numcq);\n\t\t\tbf_set(lpfc_mbx_cq_create_set_autovalid,\n\t\t\t       &cq_set->u.request,\n\t\t\t       phba->sli4_hba.pc_sli4_params.cqav);\n\t\t\tswitch (cq->entry_count) {\n\t\t\tcase 2048:\n\t\t\tcase 4096:\n\t\t\t\tif (phba->sli4_hba.pc_sli4_params.cqv ==\n\t\t\t\t    LPFC_Q_CREATE_VERSION_2) {\n\t\t\t\t\tbf_set(lpfc_mbx_cq_create_set_cqe_cnt,\n\t\t\t\t\t       &cq_set->u.request,\n\t\t\t\t\t\tcq->entry_count);\n\t\t\t\t\tbf_set(lpfc_mbx_cq_create_set_cqe_cnt,\n\t\t\t\t\t       &cq_set->u.request,\n\t\t\t\t\t       LPFC_CQ_CNT_WORD7);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tfallthrough;\n\t\t\tdefault:\n\t\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\t\"3118 Bad CQ count. (%d)\\n\",\n\t\t\t\t\t\tcq->entry_count);\n\t\t\t\tif (cq->entry_count < 256) {\n\t\t\t\t\tstatus = -EINVAL;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tfallthrough;\t/* otherwise default to smallest */\n\t\t\tcase 256:\n\t\t\t\tbf_set(lpfc_mbx_cq_create_set_cqe_cnt,\n\t\t\t\t       &cq_set->u.request, LPFC_CQ_CNT_256);\n\t\t\t\tbreak;\n\t\t\tcase 512:\n\t\t\t\tbf_set(lpfc_mbx_cq_create_set_cqe_cnt,\n\t\t\t\t       &cq_set->u.request, LPFC_CQ_CNT_512);\n\t\t\t\tbreak;\n\t\t\tcase 1024:\n\t\t\t\tbf_set(lpfc_mbx_cq_create_set_cqe_cnt,\n\t\t\t\t       &cq_set->u.request, LPFC_CQ_CNT_1024);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbf_set(lpfc_mbx_cq_create_set_eq_id0,\n\t\t\t       &cq_set->u.request, eq->queue_id);\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tbf_set(lpfc_mbx_cq_create_set_eq_id1,\n\t\t\t       &cq_set->u.request, eq->queue_id);\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tbf_set(lpfc_mbx_cq_create_set_eq_id2,\n\t\t\t       &cq_set->u.request, eq->queue_id);\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tbf_set(lpfc_mbx_cq_create_set_eq_id3,\n\t\t\t       &cq_set->u.request, eq->queue_id);\n\t\t\tbreak;\n\t\tcase 4:\n\t\t\tbf_set(lpfc_mbx_cq_create_set_eq_id4,\n\t\t\t       &cq_set->u.request, eq->queue_id);\n\t\t\tbreak;\n\t\tcase 5:\n\t\t\tbf_set(lpfc_mbx_cq_create_set_eq_id5,\n\t\t\t       &cq_set->u.request, eq->queue_id);\n\t\t\tbreak;\n\t\tcase 6:\n\t\t\tbf_set(lpfc_mbx_cq_create_set_eq_id6,\n\t\t\t       &cq_set->u.request, eq->queue_id);\n\t\t\tbreak;\n\t\tcase 7:\n\t\t\tbf_set(lpfc_mbx_cq_create_set_eq_id7,\n\t\t\t       &cq_set->u.request, eq->queue_id);\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tbf_set(lpfc_mbx_cq_create_set_eq_id8,\n\t\t\t       &cq_set->u.request, eq->queue_id);\n\t\t\tbreak;\n\t\tcase 9:\n\t\t\tbf_set(lpfc_mbx_cq_create_set_eq_id9,\n\t\t\t       &cq_set->u.request, eq->queue_id);\n\t\t\tbreak;\n\t\tcase 10:\n\t\t\tbf_set(lpfc_mbx_cq_create_set_eq_id10,\n\t\t\t       &cq_set->u.request, eq->queue_id);\n\t\t\tbreak;\n\t\tcase 11:\n\t\t\tbf_set(lpfc_mbx_cq_create_set_eq_id11,\n\t\t\t       &cq_set->u.request, eq->queue_id);\n\t\t\tbreak;\n\t\tcase 12:\n\t\t\tbf_set(lpfc_mbx_cq_create_set_eq_id12,\n\t\t\t       &cq_set->u.request, eq->queue_id);\n\t\t\tbreak;\n\t\tcase 13:\n\t\t\tbf_set(lpfc_mbx_cq_create_set_eq_id13,\n\t\t\t       &cq_set->u.request, eq->queue_id);\n\t\t\tbreak;\n\t\tcase 14:\n\t\t\tbf_set(lpfc_mbx_cq_create_set_eq_id14,\n\t\t\t       &cq_set->u.request, eq->queue_id);\n\t\t\tbreak;\n\t\tcase 15:\n\t\t\tbf_set(lpfc_mbx_cq_create_set_eq_id15,\n\t\t\t       &cq_set->u.request, eq->queue_id);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* link the cq onto the parent eq child list */\n\t\tlist_add_tail(&cq->list, &eq->child_list);\n\t\t/* Set up completion queue's type and subtype */\n\t\tcq->type = type;\n\t\tcq->subtype = subtype;\n\t\tcq->assoc_qid = eq->queue_id;\n\t\tcq->assoc_qp = eq;\n\t\tcq->host_index = 0;\n\t\tcq->notify_interval = LPFC_CQ_NOTIFY_INTRVL;\n\t\tcq->max_proc_limit = min(phba->cfg_cq_max_proc_limit,\n\t\t\t\t\t cq->entry_count);\n\t\tcq->chann = idx;\n\n\t\trc = 0;\n\t\tlist_for_each_entry(dmabuf, &cq->page_list, list) {\n\t\t\tmemset(dmabuf->virt, 0, hw_page_size);\n\t\t\tcnt = page_idx + dmabuf->buffer_tag;\n\t\t\tcq_set->u.request.page[cnt].addr_lo =\n\t\t\t\t\tputPaddrLow(dmabuf->phys);\n\t\t\tcq_set->u.request.page[cnt].addr_hi =\n\t\t\t\t\tputPaddrHigh(dmabuf->phys);\n\t\t\trc++;\n\t\t}\n\t\tpage_idx += rc;\n\t}\n\n\trc = lpfc_sli_issue_mbox(phba, mbox, MBX_POLL);\n\n\t/* The IOCTL status is embedded in the mailbox subheader. */\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"3119 CQ_CREATE_SET mailbox failed with \"\n\t\t\t\t\"status x%x add_status x%x, mbx status x%x\\n\",\n\t\t\t\tshdr_status, shdr_add_status, rc);\n\t\tstatus = -ENXIO;\n\t\tgoto out;\n\t}\n\trc = bf_get(lpfc_mbx_cq_create_set_base_id, &cq_set->u.response);\n\tif (rc == 0xFFFF) {\n\t\tstatus = -ENXIO;\n\t\tgoto out;\n\t}\n\n\tfor (idx = 0; idx < numcq; idx++) {\n\t\tcq = cqp[idx];\n\t\tcq->queue_id = rc + idx;\n\t\tif (cq->queue_id > phba->sli4_hba.cq_max)\n\t\t\tphba->sli4_hba.cq_max = cq->queue_id;\n\t}\n\nout:\n\tlpfc_sli4_mbox_cmd_free(phba, mbox);\n\treturn status;\n}\n\n/**\n * lpfc_mq_create_fb_init - Send MCC_CREATE without async events registration\n * @phba: HBA structure that indicates port to create a queue on.\n * @mq: The queue structure to use to create the mailbox queue.\n * @mbox: An allocated pointer to type LPFC_MBOXQ_t\n * @cq: The completion queue to associate with this cq.\n *\n * This function provides failback (fb) functionality when the\n * mq_create_ext fails on older FW generations.  It's purpose is identical\n * to mq_create_ext otherwise.\n *\n * This routine cannot fail as all attributes were previously accessed and\n * initialized in mq_create_ext.\n **/\nstatic void\nlpfc_mq_create_fb_init(struct lpfc_hba *phba, struct lpfc_queue *mq,\n\t\t       LPFC_MBOXQ_t *mbox, struct lpfc_queue *cq)\n{\n\tstruct lpfc_mbx_mq_create *mq_create;\n\tstruct lpfc_dmabuf *dmabuf;\n\tint length;\n\n\tlength = (sizeof(struct lpfc_mbx_mq_create) -\n\t\t  sizeof(struct lpfc_sli4_cfg_mhdr));\n\tlpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_COMMON,\n\t\t\t LPFC_MBOX_OPCODE_MQ_CREATE,\n\t\t\t length, LPFC_SLI4_MBX_EMBED);\n\tmq_create = &mbox->u.mqe.un.mq_create;\n\tbf_set(lpfc_mbx_mq_create_num_pages, &mq_create->u.request,\n\t       mq->page_count);\n\tbf_set(lpfc_mq_context_cq_id, &mq_create->u.request.context,\n\t       cq->queue_id);\n\tbf_set(lpfc_mq_context_valid, &mq_create->u.request.context, 1);\n\tswitch (mq->entry_count) {\n\tcase 16:\n\t\tbf_set(lpfc_mq_context_ring_size, &mq_create->u.request.context,\n\t\t       LPFC_MQ_RING_SIZE_16);\n\t\tbreak;\n\tcase 32:\n\t\tbf_set(lpfc_mq_context_ring_size, &mq_create->u.request.context,\n\t\t       LPFC_MQ_RING_SIZE_32);\n\t\tbreak;\n\tcase 64:\n\t\tbf_set(lpfc_mq_context_ring_size, &mq_create->u.request.context,\n\t\t       LPFC_MQ_RING_SIZE_64);\n\t\tbreak;\n\tcase 128:\n\t\tbf_set(lpfc_mq_context_ring_size, &mq_create->u.request.context,\n\t\t       LPFC_MQ_RING_SIZE_128);\n\t\tbreak;\n\t}\n\tlist_for_each_entry(dmabuf, &mq->page_list, list) {\n\t\tmq_create->u.request.page[dmabuf->buffer_tag].addr_lo =\n\t\t\tputPaddrLow(dmabuf->phys);\n\t\tmq_create->u.request.page[dmabuf->buffer_tag].addr_hi =\n\t\t\tputPaddrHigh(dmabuf->phys);\n\t}\n}\n\n/**\n * lpfc_mq_create - Create a mailbox Queue on the HBA\n * @phba: HBA structure that indicates port to create a queue on.\n * @mq: The queue structure to use to create the mailbox queue.\n * @cq: The completion queue to associate with this cq.\n * @subtype: The queue's subtype.\n *\n * This function creates a mailbox queue, as detailed in @mq, on a port,\n * described by @phba by sending a MQ_CREATE mailbox command to the HBA.\n *\n * The @phba struct is used to send mailbox command to HBA. The @cq struct\n * is used to get the entry count and entry size that are necessary to\n * determine the number of pages to allocate and use for this queue. This\n * function will send the MQ_CREATE mailbox command to the HBA to setup the\n * mailbox queue. This function is asynchronous and will wait for the mailbox\n * command to finish before continuing.\n *\n * On success this function will return a zero. If unable to allocate enough\n * memory this function will return -ENOMEM. If the queue create mailbox command\n * fails this function will return -ENXIO.\n **/\nint32_t\nlpfc_mq_create(struct lpfc_hba *phba, struct lpfc_queue *mq,\n\t       struct lpfc_queue *cq, uint32_t subtype)\n{\n\tstruct lpfc_mbx_mq_create *mq_create;\n\tstruct lpfc_mbx_mq_create_ext *mq_create_ext;\n\tstruct lpfc_dmabuf *dmabuf;\n\tLPFC_MBOXQ_t *mbox;\n\tint rc, length, status = 0;\n\tuint32_t shdr_status, shdr_add_status;\n\tunion lpfc_sli4_cfg_shdr *shdr;\n\tuint32_t hw_page_size = phba->sli4_hba.pc_sli4_params.if_page_sz;\n\n\t/* sanity check on queue memory */\n\tif (!mq || !cq)\n\t\treturn -ENODEV;\n\tif (!phba->sli4_hba.pc_sli4_params.supported)\n\t\thw_page_size = SLI4_PAGE_SIZE;\n\n\tmbox = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox)\n\t\treturn -ENOMEM;\n\tlength = (sizeof(struct lpfc_mbx_mq_create_ext) -\n\t\t  sizeof(struct lpfc_sli4_cfg_mhdr));\n\tlpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_COMMON,\n\t\t\t LPFC_MBOX_OPCODE_MQ_CREATE_EXT,\n\t\t\t length, LPFC_SLI4_MBX_EMBED);\n\n\tmq_create_ext = &mbox->u.mqe.un.mq_create_ext;\n\tshdr = (union lpfc_sli4_cfg_shdr *) &mq_create_ext->header.cfg_shdr;\n\tbf_set(lpfc_mbx_mq_create_ext_num_pages,\n\t       &mq_create_ext->u.request, mq->page_count);\n\tbf_set(lpfc_mbx_mq_create_ext_async_evt_link,\n\t       &mq_create_ext->u.request, 1);\n\tbf_set(lpfc_mbx_mq_create_ext_async_evt_fip,\n\t       &mq_create_ext->u.request, 1);\n\tbf_set(lpfc_mbx_mq_create_ext_async_evt_group5,\n\t       &mq_create_ext->u.request, 1);\n\tbf_set(lpfc_mbx_mq_create_ext_async_evt_fc,\n\t       &mq_create_ext->u.request, 1);\n\tbf_set(lpfc_mbx_mq_create_ext_async_evt_sli,\n\t       &mq_create_ext->u.request, 1);\n\tbf_set(lpfc_mq_context_valid, &mq_create_ext->u.request.context, 1);\n\tbf_set(lpfc_mbox_hdr_version, &shdr->request,\n\t       phba->sli4_hba.pc_sli4_params.mqv);\n\tif (phba->sli4_hba.pc_sli4_params.mqv == LPFC_Q_CREATE_VERSION_1)\n\t\tbf_set(lpfc_mbx_mq_create_ext_cq_id, &mq_create_ext->u.request,\n\t\t       cq->queue_id);\n\telse\n\t\tbf_set(lpfc_mq_context_cq_id, &mq_create_ext->u.request.context,\n\t\t       cq->queue_id);\n\tswitch (mq->entry_count) {\n\tdefault:\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0362 Unsupported MQ count. (%d)\\n\",\n\t\t\t\tmq->entry_count);\n\t\tif (mq->entry_count < 16) {\n\t\t\tstatus = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tfallthrough;\t/* otherwise default to smallest count */\n\tcase 16:\n\t\tbf_set(lpfc_mq_context_ring_size,\n\t\t       &mq_create_ext->u.request.context,\n\t\t       LPFC_MQ_RING_SIZE_16);\n\t\tbreak;\n\tcase 32:\n\t\tbf_set(lpfc_mq_context_ring_size,\n\t\t       &mq_create_ext->u.request.context,\n\t\t       LPFC_MQ_RING_SIZE_32);\n\t\tbreak;\n\tcase 64:\n\t\tbf_set(lpfc_mq_context_ring_size,\n\t\t       &mq_create_ext->u.request.context,\n\t\t       LPFC_MQ_RING_SIZE_64);\n\t\tbreak;\n\tcase 128:\n\t\tbf_set(lpfc_mq_context_ring_size,\n\t\t       &mq_create_ext->u.request.context,\n\t\t       LPFC_MQ_RING_SIZE_128);\n\t\tbreak;\n\t}\n\tlist_for_each_entry(dmabuf, &mq->page_list, list) {\n\t\tmemset(dmabuf->virt, 0, hw_page_size);\n\t\tmq_create_ext->u.request.page[dmabuf->buffer_tag].addr_lo =\n\t\t\t\t\tputPaddrLow(dmabuf->phys);\n\t\tmq_create_ext->u.request.page[dmabuf->buffer_tag].addr_hi =\n\t\t\t\t\tputPaddrHigh(dmabuf->phys);\n\t}\n\trc = lpfc_sli_issue_mbox(phba, mbox, MBX_POLL);\n\tmq->queue_id = bf_get(lpfc_mbx_mq_create_q_id,\n\t\t\t      &mq_create_ext->u.response);\n\tif (rc != MBX_SUCCESS) {\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_INIT,\n\t\t\t\t\"2795 MQ_CREATE_EXT failed with \"\n\t\t\t\t\"status x%x. Failback to MQ_CREATE.\\n\",\n\t\t\t\trc);\n\t\tlpfc_mq_create_fb_init(phba, mq, mbox, cq);\n\t\tmq_create = &mbox->u.mqe.un.mq_create;\n\t\trc = lpfc_sli_issue_mbox(phba, mbox, MBX_POLL);\n\t\tshdr = (union lpfc_sli4_cfg_shdr *) &mq_create->header.cfg_shdr;\n\t\tmq->queue_id = bf_get(lpfc_mbx_mq_create_q_id,\n\t\t\t\t      &mq_create->u.response);\n\t}\n\n\t/* The IOCTL status is embedded in the mailbox subheader. */\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2502 MQ_CREATE mailbox failed with \"\n\t\t\t\t\"status x%x add_status x%x, mbx status x%x\\n\",\n\t\t\t\tshdr_status, shdr_add_status, rc);\n\t\tstatus = -ENXIO;\n\t\tgoto out;\n\t}\n\tif (mq->queue_id == 0xFFFF) {\n\t\tstatus = -ENXIO;\n\t\tgoto out;\n\t}\n\tmq->type = LPFC_MQ;\n\tmq->assoc_qid = cq->queue_id;\n\tmq->subtype = subtype;\n\tmq->host_index = 0;\n\tmq->hba_index = 0;\n\n\t/* link the mq onto the parent cq child list */\n\tlist_add_tail(&mq->list, &cq->child_list);\nout:\n\tmempool_free(mbox, phba->mbox_mem_pool);\n\treturn status;\n}\n\n/**\n * lpfc_wq_create - Create a Work Queue on the HBA\n * @phba: HBA structure that indicates port to create a queue on.\n * @wq: The queue structure to use to create the work queue.\n * @cq: The completion queue to bind this work queue to.\n * @subtype: The subtype of the work queue indicating its functionality.\n *\n * This function creates a work queue, as detailed in @wq, on a port, described\n * by @phba by sending a WQ_CREATE mailbox command to the HBA.\n *\n * The @phba struct is used to send mailbox command to HBA. The @wq struct\n * is used to get the entry count and entry size that are necessary to\n * determine the number of pages to allocate and use for this queue. The @cq\n * is used to indicate which completion queue to bind this work queue to. This\n * function will send the WQ_CREATE mailbox command to the HBA to setup the\n * work queue. This function is asynchronous and will wait for the mailbox\n * command to finish before continuing.\n *\n * On success this function will return a zero. If unable to allocate enough\n * memory this function will return -ENOMEM. If the queue create mailbox command\n * fails this function will return -ENXIO.\n **/\nint\nlpfc_wq_create(struct lpfc_hba *phba, struct lpfc_queue *wq,\n\t       struct lpfc_queue *cq, uint32_t subtype)\n{\n\tstruct lpfc_mbx_wq_create *wq_create;\n\tstruct lpfc_dmabuf *dmabuf;\n\tLPFC_MBOXQ_t *mbox;\n\tint rc, length, status = 0;\n\tuint32_t shdr_status, shdr_add_status;\n\tunion lpfc_sli4_cfg_shdr *shdr;\n\tuint32_t hw_page_size = phba->sli4_hba.pc_sli4_params.if_page_sz;\n\tstruct dma_address *page;\n\tvoid __iomem *bar_memmap_p;\n\tuint32_t db_offset;\n\tuint16_t pci_barset;\n\tuint8_t dpp_barset;\n\tuint32_t dpp_offset;\n\tuint8_t wq_create_version;\n#ifdef CONFIG_X86\n\tunsigned long pg_addr;\n#endif\n\n\t/* sanity check on queue memory */\n\tif (!wq || !cq)\n\t\treturn -ENODEV;\n\tif (!phba->sli4_hba.pc_sli4_params.supported)\n\t\thw_page_size = wq->page_size;\n\n\tmbox = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox)\n\t\treturn -ENOMEM;\n\tlength = (sizeof(struct lpfc_mbx_wq_create) -\n\t\t  sizeof(struct lpfc_sli4_cfg_mhdr));\n\tlpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_FCOE,\n\t\t\t LPFC_MBOX_OPCODE_FCOE_WQ_CREATE,\n\t\t\t length, LPFC_SLI4_MBX_EMBED);\n\twq_create = &mbox->u.mqe.un.wq_create;\n\tshdr = (union lpfc_sli4_cfg_shdr *) &wq_create->header.cfg_shdr;\n\tbf_set(lpfc_mbx_wq_create_num_pages, &wq_create->u.request,\n\t\t    wq->page_count);\n\tbf_set(lpfc_mbx_wq_create_cq_id, &wq_create->u.request,\n\t\t    cq->queue_id);\n\n\t/* wqv is the earliest version supported, NOT the latest */\n\tbf_set(lpfc_mbox_hdr_version, &shdr->request,\n\t       phba->sli4_hba.pc_sli4_params.wqv);\n\n\tif ((phba->sli4_hba.pc_sli4_params.wqsize & LPFC_WQ_SZ128_SUPPORT) ||\n\t    (wq->page_size > SLI4_PAGE_SIZE))\n\t\twq_create_version = LPFC_Q_CREATE_VERSION_1;\n\telse\n\t\twq_create_version = LPFC_Q_CREATE_VERSION_0;\n\n\n\tif (phba->sli4_hba.pc_sli4_params.wqsize & LPFC_WQ_SZ128_SUPPORT)\n\t\twq_create_version = LPFC_Q_CREATE_VERSION_1;\n\telse\n\t\twq_create_version = LPFC_Q_CREATE_VERSION_0;\n\n\tswitch (wq_create_version) {\n\tcase LPFC_Q_CREATE_VERSION_1:\n\t\tbf_set(lpfc_mbx_wq_create_wqe_count, &wq_create->u.request_1,\n\t\t       wq->entry_count);\n\t\tbf_set(lpfc_mbox_hdr_version, &shdr->request,\n\t\t       LPFC_Q_CREATE_VERSION_1);\n\n\t\tswitch (wq->entry_size) {\n\t\tdefault:\n\t\tcase 64:\n\t\t\tbf_set(lpfc_mbx_wq_create_wqe_size,\n\t\t\t       &wq_create->u.request_1,\n\t\t\t       LPFC_WQ_WQE_SIZE_64);\n\t\t\tbreak;\n\t\tcase 128:\n\t\t\tbf_set(lpfc_mbx_wq_create_wqe_size,\n\t\t\t       &wq_create->u.request_1,\n\t\t\t       LPFC_WQ_WQE_SIZE_128);\n\t\t\tbreak;\n\t\t}\n\t\t/* Request DPP by default */\n\t\tbf_set(lpfc_mbx_wq_create_dpp_req, &wq_create->u.request_1, 1);\n\t\tbf_set(lpfc_mbx_wq_create_page_size,\n\t\t       &wq_create->u.request_1,\n\t\t       (wq->page_size / SLI4_PAGE_SIZE));\n\t\tpage = wq_create->u.request_1.page;\n\t\tbreak;\n\tdefault:\n\t\tpage = wq_create->u.request.page;\n\t\tbreak;\n\t}\n\n\tlist_for_each_entry(dmabuf, &wq->page_list, list) {\n\t\tmemset(dmabuf->virt, 0, hw_page_size);\n\t\tpage[dmabuf->buffer_tag].addr_lo = putPaddrLow(dmabuf->phys);\n\t\tpage[dmabuf->buffer_tag].addr_hi = putPaddrHigh(dmabuf->phys);\n\t}\n\n\tif (phba->sli4_hba.fw_func_mode & LPFC_DUA_MODE)\n\t\tbf_set(lpfc_mbx_wq_create_dua, &wq_create->u.request, 1);\n\n\trc = lpfc_sli_issue_mbox(phba, mbox, MBX_POLL);\n\t/* The IOCTL status is embedded in the mailbox subheader. */\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2503 WQ_CREATE mailbox failed with \"\n\t\t\t\t\"status x%x add_status x%x, mbx status x%x\\n\",\n\t\t\t\tshdr_status, shdr_add_status, rc);\n\t\tstatus = -ENXIO;\n\t\tgoto out;\n\t}\n\n\tif (wq_create_version == LPFC_Q_CREATE_VERSION_0)\n\t\twq->queue_id = bf_get(lpfc_mbx_wq_create_q_id,\n\t\t\t\t\t&wq_create->u.response);\n\telse\n\t\twq->queue_id = bf_get(lpfc_mbx_wq_create_v1_q_id,\n\t\t\t\t\t&wq_create->u.response_1);\n\n\tif (wq->queue_id == 0xFFFF) {\n\t\tstatus = -ENXIO;\n\t\tgoto out;\n\t}\n\n\twq->db_format = LPFC_DB_LIST_FORMAT;\n\tif (wq_create_version == LPFC_Q_CREATE_VERSION_0) {\n\t\tif (phba->sli4_hba.fw_func_mode & LPFC_DUA_MODE) {\n\t\t\twq->db_format = bf_get(lpfc_mbx_wq_create_db_format,\n\t\t\t\t\t       &wq_create->u.response);\n\t\t\tif ((wq->db_format != LPFC_DB_LIST_FORMAT) &&\n\t\t\t    (wq->db_format != LPFC_DB_RING_FORMAT)) {\n\t\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\t\"3265 WQ[%d] doorbell format \"\n\t\t\t\t\t\t\"not supported: x%x\\n\",\n\t\t\t\t\t\twq->queue_id, wq->db_format);\n\t\t\t\tstatus = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tpci_barset = bf_get(lpfc_mbx_wq_create_bar_set,\n\t\t\t\t\t    &wq_create->u.response);\n\t\t\tbar_memmap_p = lpfc_dual_chute_pci_bar_map(phba,\n\t\t\t\t\t\t\t\t   pci_barset);\n\t\t\tif (!bar_memmap_p) {\n\t\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\t\"3263 WQ[%d] failed to memmap \"\n\t\t\t\t\t\t\"pci barset:x%x\\n\",\n\t\t\t\t\t\twq->queue_id, pci_barset);\n\t\t\t\tstatus = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tdb_offset = wq_create->u.response.doorbell_offset;\n\t\t\tif ((db_offset != LPFC_ULP0_WQ_DOORBELL) &&\n\t\t\t    (db_offset != LPFC_ULP1_WQ_DOORBELL)) {\n\t\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\t\"3252 WQ[%d] doorbell offset \"\n\t\t\t\t\t\t\"not supported: x%x\\n\",\n\t\t\t\t\t\twq->queue_id, db_offset);\n\t\t\t\tstatus = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\twq->db_regaddr = bar_memmap_p + db_offset;\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_INIT,\n\t\t\t\t\t\"3264 WQ[%d]: barset:x%x, offset:x%x, \"\n\t\t\t\t\t\"format:x%x\\n\", wq->queue_id,\n\t\t\t\t\tpci_barset, db_offset, wq->db_format);\n\t\t} else\n\t\t\twq->db_regaddr = phba->sli4_hba.WQDBregaddr;\n\t} else {\n\t\t/* Check if DPP was honored by the firmware */\n\t\twq->dpp_enable = bf_get(lpfc_mbx_wq_create_dpp_rsp,\n\t\t\t\t    &wq_create->u.response_1);\n\t\tif (wq->dpp_enable) {\n\t\t\tpci_barset = bf_get(lpfc_mbx_wq_create_v1_bar_set,\n\t\t\t\t\t    &wq_create->u.response_1);\n\t\t\tbar_memmap_p = lpfc_dual_chute_pci_bar_map(phba,\n\t\t\t\t\t\t\t\t   pci_barset);\n\t\t\tif (!bar_memmap_p) {\n\t\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\t\"3267 WQ[%d] failed to memmap \"\n\t\t\t\t\t\t\"pci barset:x%x\\n\",\n\t\t\t\t\t\twq->queue_id, pci_barset);\n\t\t\t\tstatus = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tdb_offset = wq_create->u.response_1.doorbell_offset;\n\t\t\twq->db_regaddr = bar_memmap_p + db_offset;\n\t\t\twq->dpp_id = bf_get(lpfc_mbx_wq_create_dpp_id,\n\t\t\t\t\t    &wq_create->u.response_1);\n\t\t\tdpp_barset = bf_get(lpfc_mbx_wq_create_dpp_bar,\n\t\t\t\t\t    &wq_create->u.response_1);\n\t\t\tbar_memmap_p = lpfc_dual_chute_pci_bar_map(phba,\n\t\t\t\t\t\t\t\t   dpp_barset);\n\t\t\tif (!bar_memmap_p) {\n\t\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\t\"3268 WQ[%d] failed to memmap \"\n\t\t\t\t\t\t\"pci barset:x%x\\n\",\n\t\t\t\t\t\twq->queue_id, dpp_barset);\n\t\t\t\tstatus = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tdpp_offset = wq_create->u.response_1.dpp_offset;\n\t\t\twq->dpp_regaddr = bar_memmap_p + dpp_offset;\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_INIT,\n\t\t\t\t\t\"3271 WQ[%d]: barset:x%x, offset:x%x, \"\n\t\t\t\t\t\"dpp_id:x%x dpp_barset:x%x \"\n\t\t\t\t\t\"dpp_offset:x%x\\n\",\n\t\t\t\t\twq->queue_id, pci_barset, db_offset,\n\t\t\t\t\twq->dpp_id, dpp_barset, dpp_offset);\n\n#ifdef CONFIG_X86\n\t\t\t/* Enable combined writes for DPP aperture */\n\t\t\tpg_addr = (unsigned long)(wq->dpp_regaddr) & PAGE_MASK;\n\t\t\trc = set_memory_wc(pg_addr, 1);\n\t\t\tif (rc) {\n\t\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_INIT,\n\t\t\t\t\t\"3272 Cannot setup Combined \"\n\t\t\t\t\t\"Write on WQ[%d] - disable DPP\\n\",\n\t\t\t\t\twq->queue_id);\n\t\t\t\tphba->cfg_enable_dpp = 0;\n\t\t\t}\n#else\n\t\t\tphba->cfg_enable_dpp = 0;\n#endif\n\t\t} else\n\t\t\twq->db_regaddr = phba->sli4_hba.WQDBregaddr;\n\t}\n\twq->pring = kzalloc(sizeof(struct lpfc_sli_ring), GFP_KERNEL);\n\tif (wq->pring == NULL) {\n\t\tstatus = -ENOMEM;\n\t\tgoto out;\n\t}\n\twq->type = LPFC_WQ;\n\twq->assoc_qid = cq->queue_id;\n\twq->subtype = subtype;\n\twq->host_index = 0;\n\twq->hba_index = 0;\n\twq->notify_interval = LPFC_WQ_NOTIFY_INTRVL;\n\n\t/* link the wq onto the parent cq child list */\n\tlist_add_tail(&wq->list, &cq->child_list);\nout:\n\tmempool_free(mbox, phba->mbox_mem_pool);\n\treturn status;\n}\n\n/**\n * lpfc_rq_create - Create a Receive Queue on the HBA\n * @phba: HBA structure that indicates port to create a queue on.\n * @hrq: The queue structure to use to create the header receive queue.\n * @drq: The queue structure to use to create the data receive queue.\n * @cq: The completion queue to bind this work queue to.\n * @subtype: The subtype of the work queue indicating its functionality.\n *\n * This function creates a receive buffer queue pair , as detailed in @hrq and\n * @drq, on a port, described by @phba by sending a RQ_CREATE mailbox command\n * to the HBA.\n *\n * The @phba struct is used to send mailbox command to HBA. The @drq and @hrq\n * struct is used to get the entry count that is necessary to determine the\n * number of pages to use for this queue. The @cq is used to indicate which\n * completion queue to bind received buffers that are posted to these queues to.\n * This function will send the RQ_CREATE mailbox command to the HBA to setup the\n * receive queue pair. This function is asynchronous and will wait for the\n * mailbox command to finish before continuing.\n *\n * On success this function will return a zero. If unable to allocate enough\n * memory this function will return -ENOMEM. If the queue create mailbox command\n * fails this function will return -ENXIO.\n **/\nint\nlpfc_rq_create(struct lpfc_hba *phba, struct lpfc_queue *hrq,\n\t       struct lpfc_queue *drq, struct lpfc_queue *cq, uint32_t subtype)\n{\n\tstruct lpfc_mbx_rq_create *rq_create;\n\tstruct lpfc_dmabuf *dmabuf;\n\tLPFC_MBOXQ_t *mbox;\n\tint rc, length, status = 0;\n\tuint32_t shdr_status, shdr_add_status;\n\tunion lpfc_sli4_cfg_shdr *shdr;\n\tuint32_t hw_page_size = phba->sli4_hba.pc_sli4_params.if_page_sz;\n\tvoid __iomem *bar_memmap_p;\n\tuint32_t db_offset;\n\tuint16_t pci_barset;\n\n\t/* sanity check on queue memory */\n\tif (!hrq || !drq || !cq)\n\t\treturn -ENODEV;\n\tif (!phba->sli4_hba.pc_sli4_params.supported)\n\t\thw_page_size = SLI4_PAGE_SIZE;\n\n\tif (hrq->entry_count != drq->entry_count)\n\t\treturn -EINVAL;\n\tmbox = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox)\n\t\treturn -ENOMEM;\n\tlength = (sizeof(struct lpfc_mbx_rq_create) -\n\t\t  sizeof(struct lpfc_sli4_cfg_mhdr));\n\tlpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_FCOE,\n\t\t\t LPFC_MBOX_OPCODE_FCOE_RQ_CREATE,\n\t\t\t length, LPFC_SLI4_MBX_EMBED);\n\trq_create = &mbox->u.mqe.un.rq_create;\n\tshdr = (union lpfc_sli4_cfg_shdr *) &rq_create->header.cfg_shdr;\n\tbf_set(lpfc_mbox_hdr_version, &shdr->request,\n\t       phba->sli4_hba.pc_sli4_params.rqv);\n\tif (phba->sli4_hba.pc_sli4_params.rqv == LPFC_Q_CREATE_VERSION_1) {\n\t\tbf_set(lpfc_rq_context_rqe_count_1,\n\t\t       &rq_create->u.request.context,\n\t\t       hrq->entry_count);\n\t\trq_create->u.request.context.buffer_size = LPFC_HDR_BUF_SIZE;\n\t\tbf_set(lpfc_rq_context_rqe_size,\n\t\t       &rq_create->u.request.context,\n\t\t       LPFC_RQE_SIZE_8);\n\t\tbf_set(lpfc_rq_context_page_size,\n\t\t       &rq_create->u.request.context,\n\t\t       LPFC_RQ_PAGE_SIZE_4096);\n\t} else {\n\t\tswitch (hrq->entry_count) {\n\t\tdefault:\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"2535 Unsupported RQ count. (%d)\\n\",\n\t\t\t\t\thrq->entry_count);\n\t\t\tif (hrq->entry_count < 512) {\n\t\t\t\tstatus = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tfallthrough;\t/* otherwise default to smallest count */\n\t\tcase 512:\n\t\t\tbf_set(lpfc_rq_context_rqe_count,\n\t\t\t       &rq_create->u.request.context,\n\t\t\t       LPFC_RQ_RING_SIZE_512);\n\t\t\tbreak;\n\t\tcase 1024:\n\t\t\tbf_set(lpfc_rq_context_rqe_count,\n\t\t\t       &rq_create->u.request.context,\n\t\t\t       LPFC_RQ_RING_SIZE_1024);\n\t\t\tbreak;\n\t\tcase 2048:\n\t\t\tbf_set(lpfc_rq_context_rqe_count,\n\t\t\t       &rq_create->u.request.context,\n\t\t\t       LPFC_RQ_RING_SIZE_2048);\n\t\t\tbreak;\n\t\tcase 4096:\n\t\t\tbf_set(lpfc_rq_context_rqe_count,\n\t\t\t       &rq_create->u.request.context,\n\t\t\t       LPFC_RQ_RING_SIZE_4096);\n\t\t\tbreak;\n\t\t}\n\t\tbf_set(lpfc_rq_context_buf_size, &rq_create->u.request.context,\n\t\t       LPFC_HDR_BUF_SIZE);\n\t}\n\tbf_set(lpfc_rq_context_cq_id, &rq_create->u.request.context,\n\t       cq->queue_id);\n\tbf_set(lpfc_mbx_rq_create_num_pages, &rq_create->u.request,\n\t       hrq->page_count);\n\tlist_for_each_entry(dmabuf, &hrq->page_list, list) {\n\t\tmemset(dmabuf->virt, 0, hw_page_size);\n\t\trq_create->u.request.page[dmabuf->buffer_tag].addr_lo =\n\t\t\t\t\tputPaddrLow(dmabuf->phys);\n\t\trq_create->u.request.page[dmabuf->buffer_tag].addr_hi =\n\t\t\t\t\tputPaddrHigh(dmabuf->phys);\n\t}\n\tif (phba->sli4_hba.fw_func_mode & LPFC_DUA_MODE)\n\t\tbf_set(lpfc_mbx_rq_create_dua, &rq_create->u.request, 1);\n\n\trc = lpfc_sli_issue_mbox(phba, mbox, MBX_POLL);\n\t/* The IOCTL status is embedded in the mailbox subheader. */\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2504 RQ_CREATE mailbox failed with \"\n\t\t\t\t\"status x%x add_status x%x, mbx status x%x\\n\",\n\t\t\t\tshdr_status, shdr_add_status, rc);\n\t\tstatus = -ENXIO;\n\t\tgoto out;\n\t}\n\thrq->queue_id = bf_get(lpfc_mbx_rq_create_q_id, &rq_create->u.response);\n\tif (hrq->queue_id == 0xFFFF) {\n\t\tstatus = -ENXIO;\n\t\tgoto out;\n\t}\n\n\tif (phba->sli4_hba.fw_func_mode & LPFC_DUA_MODE) {\n\t\thrq->db_format = bf_get(lpfc_mbx_rq_create_db_format,\n\t\t\t\t\t&rq_create->u.response);\n\t\tif ((hrq->db_format != LPFC_DB_LIST_FORMAT) &&\n\t\t    (hrq->db_format != LPFC_DB_RING_FORMAT)) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"3262 RQ [%d] doorbell format not \"\n\t\t\t\t\t\"supported: x%x\\n\", hrq->queue_id,\n\t\t\t\t\thrq->db_format);\n\t\t\tstatus = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tpci_barset = bf_get(lpfc_mbx_rq_create_bar_set,\n\t\t\t\t    &rq_create->u.response);\n\t\tbar_memmap_p = lpfc_dual_chute_pci_bar_map(phba, pci_barset);\n\t\tif (!bar_memmap_p) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"3269 RQ[%d] failed to memmap pci \"\n\t\t\t\t\t\"barset:x%x\\n\", hrq->queue_id,\n\t\t\t\t\tpci_barset);\n\t\t\tstatus = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tdb_offset = rq_create->u.response.doorbell_offset;\n\t\tif ((db_offset != LPFC_ULP0_RQ_DOORBELL) &&\n\t\t    (db_offset != LPFC_ULP1_RQ_DOORBELL)) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"3270 RQ[%d] doorbell offset not \"\n\t\t\t\t\t\"supported: x%x\\n\", hrq->queue_id,\n\t\t\t\t\tdb_offset);\n\t\t\tstatus = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\thrq->db_regaddr = bar_memmap_p + db_offset;\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_INIT,\n\t\t\t\t\"3266 RQ[qid:%d]: barset:x%x, offset:x%x, \"\n\t\t\t\t\"format:x%x\\n\", hrq->queue_id, pci_barset,\n\t\t\t\tdb_offset, hrq->db_format);\n\t} else {\n\t\thrq->db_format = LPFC_DB_RING_FORMAT;\n\t\thrq->db_regaddr = phba->sli4_hba.RQDBregaddr;\n\t}\n\thrq->type = LPFC_HRQ;\n\thrq->assoc_qid = cq->queue_id;\n\thrq->subtype = subtype;\n\thrq->host_index = 0;\n\thrq->hba_index = 0;\n\thrq->notify_interval = LPFC_RQ_NOTIFY_INTRVL;\n\n\t/* now create the data queue */\n\tlpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_FCOE,\n\t\t\t LPFC_MBOX_OPCODE_FCOE_RQ_CREATE,\n\t\t\t length, LPFC_SLI4_MBX_EMBED);\n\tbf_set(lpfc_mbox_hdr_version, &shdr->request,\n\t       phba->sli4_hba.pc_sli4_params.rqv);\n\tif (phba->sli4_hba.pc_sli4_params.rqv == LPFC_Q_CREATE_VERSION_1) {\n\t\tbf_set(lpfc_rq_context_rqe_count_1,\n\t\t       &rq_create->u.request.context, hrq->entry_count);\n\t\tif (subtype == LPFC_NVMET)\n\t\t\trq_create->u.request.context.buffer_size =\n\t\t\t\tLPFC_NVMET_DATA_BUF_SIZE;\n\t\telse\n\t\t\trq_create->u.request.context.buffer_size =\n\t\t\t\tLPFC_DATA_BUF_SIZE;\n\t\tbf_set(lpfc_rq_context_rqe_size, &rq_create->u.request.context,\n\t\t       LPFC_RQE_SIZE_8);\n\t\tbf_set(lpfc_rq_context_page_size, &rq_create->u.request.context,\n\t\t       (PAGE_SIZE/SLI4_PAGE_SIZE));\n\t} else {\n\t\tswitch (drq->entry_count) {\n\t\tdefault:\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"2536 Unsupported RQ count. (%d)\\n\",\n\t\t\t\t\tdrq->entry_count);\n\t\t\tif (drq->entry_count < 512) {\n\t\t\t\tstatus = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tfallthrough;\t/* otherwise default to smallest count */\n\t\tcase 512:\n\t\t\tbf_set(lpfc_rq_context_rqe_count,\n\t\t\t       &rq_create->u.request.context,\n\t\t\t       LPFC_RQ_RING_SIZE_512);\n\t\t\tbreak;\n\t\tcase 1024:\n\t\t\tbf_set(lpfc_rq_context_rqe_count,\n\t\t\t       &rq_create->u.request.context,\n\t\t\t       LPFC_RQ_RING_SIZE_1024);\n\t\t\tbreak;\n\t\tcase 2048:\n\t\t\tbf_set(lpfc_rq_context_rqe_count,\n\t\t\t       &rq_create->u.request.context,\n\t\t\t       LPFC_RQ_RING_SIZE_2048);\n\t\t\tbreak;\n\t\tcase 4096:\n\t\t\tbf_set(lpfc_rq_context_rqe_count,\n\t\t\t       &rq_create->u.request.context,\n\t\t\t       LPFC_RQ_RING_SIZE_4096);\n\t\t\tbreak;\n\t\t}\n\t\tif (subtype == LPFC_NVMET)\n\t\t\tbf_set(lpfc_rq_context_buf_size,\n\t\t\t       &rq_create->u.request.context,\n\t\t\t       LPFC_NVMET_DATA_BUF_SIZE);\n\t\telse\n\t\t\tbf_set(lpfc_rq_context_buf_size,\n\t\t\t       &rq_create->u.request.context,\n\t\t\t       LPFC_DATA_BUF_SIZE);\n\t}\n\tbf_set(lpfc_rq_context_cq_id, &rq_create->u.request.context,\n\t       cq->queue_id);\n\tbf_set(lpfc_mbx_rq_create_num_pages, &rq_create->u.request,\n\t       drq->page_count);\n\tlist_for_each_entry(dmabuf, &drq->page_list, list) {\n\t\trq_create->u.request.page[dmabuf->buffer_tag].addr_lo =\n\t\t\t\t\tputPaddrLow(dmabuf->phys);\n\t\trq_create->u.request.page[dmabuf->buffer_tag].addr_hi =\n\t\t\t\t\tputPaddrHigh(dmabuf->phys);\n\t}\n\tif (phba->sli4_hba.fw_func_mode & LPFC_DUA_MODE)\n\t\tbf_set(lpfc_mbx_rq_create_dua, &rq_create->u.request, 1);\n\trc = lpfc_sli_issue_mbox(phba, mbox, MBX_POLL);\n\t/* The IOCTL status is embedded in the mailbox subheader. */\n\tshdr = (union lpfc_sli4_cfg_shdr *) &rq_create->header.cfg_shdr;\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tstatus = -ENXIO;\n\t\tgoto out;\n\t}\n\tdrq->queue_id = bf_get(lpfc_mbx_rq_create_q_id, &rq_create->u.response);\n\tif (drq->queue_id == 0xFFFF) {\n\t\tstatus = -ENXIO;\n\t\tgoto out;\n\t}\n\tdrq->type = LPFC_DRQ;\n\tdrq->assoc_qid = cq->queue_id;\n\tdrq->subtype = subtype;\n\tdrq->host_index = 0;\n\tdrq->hba_index = 0;\n\tdrq->notify_interval = LPFC_RQ_NOTIFY_INTRVL;\n\n\t/* link the header and data RQs onto the parent cq child list */\n\tlist_add_tail(&hrq->list, &cq->child_list);\n\tlist_add_tail(&drq->list, &cq->child_list);\n\nout:\n\tmempool_free(mbox, phba->mbox_mem_pool);\n\treturn status;\n}\n\n/**\n * lpfc_mrq_create - Create MRQ Receive Queues on the HBA\n * @phba: HBA structure that indicates port to create a queue on.\n * @hrqp: The queue structure array to use to create the header receive queues.\n * @drqp: The queue structure array to use to create the data receive queues.\n * @cqp: The completion queue array to bind these receive queues to.\n * @subtype: Functional purpose of the queue (MBOX, IO, ELS, NVMET, etc).\n *\n * This function creates a receive buffer queue pair , as detailed in @hrq and\n * @drq, on a port, described by @phba by sending a RQ_CREATE mailbox command\n * to the HBA.\n *\n * The @phba struct is used to send mailbox command to HBA. The @drq and @hrq\n * struct is used to get the entry count that is necessary to determine the\n * number of pages to use for this queue. The @cq is used to indicate which\n * completion queue to bind received buffers that are posted to these queues to.\n * This function will send the RQ_CREATE mailbox command to the HBA to setup the\n * receive queue pair. This function is asynchronous and will wait for the\n * mailbox command to finish before continuing.\n *\n * On success this function will return a zero. If unable to allocate enough\n * memory this function will return -ENOMEM. If the queue create mailbox command\n * fails this function will return -ENXIO.\n **/\nint\nlpfc_mrq_create(struct lpfc_hba *phba, struct lpfc_queue **hrqp,\n\t\tstruct lpfc_queue **drqp, struct lpfc_queue **cqp,\n\t\tuint32_t subtype)\n{\n\tstruct lpfc_queue *hrq, *drq, *cq;\n\tstruct lpfc_mbx_rq_create_v2 *rq_create;\n\tstruct lpfc_dmabuf *dmabuf;\n\tLPFC_MBOXQ_t *mbox;\n\tint rc, length, alloclen, status = 0;\n\tint cnt, idx, numrq, page_idx = 0;\n\tuint32_t shdr_status, shdr_add_status;\n\tunion lpfc_sli4_cfg_shdr *shdr;\n\tuint32_t hw_page_size = phba->sli4_hba.pc_sli4_params.if_page_sz;\n\n\tnumrq = phba->cfg_nvmet_mrq;\n\t/* sanity check on array memory */\n\tif (!hrqp || !drqp || !cqp || !numrq)\n\t\treturn -ENODEV;\n\tif (!phba->sli4_hba.pc_sli4_params.supported)\n\t\thw_page_size = SLI4_PAGE_SIZE;\n\n\tmbox = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox)\n\t\treturn -ENOMEM;\n\n\tlength = sizeof(struct lpfc_mbx_rq_create_v2);\n\tlength += ((2 * numrq * hrqp[0]->page_count) *\n\t\t   sizeof(struct dma_address));\n\n\talloclen = lpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_FCOE,\n\t\t\t\t    LPFC_MBOX_OPCODE_FCOE_RQ_CREATE, length,\n\t\t\t\t    LPFC_SLI4_MBX_NEMBED);\n\tif (alloclen < length) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"3099 Allocated DMA memory size (%d) is \"\n\t\t\t\t\"less than the requested DMA memory size \"\n\t\t\t\t\"(%d)\\n\", alloclen, length);\n\t\tstatus = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\n\n\trq_create = mbox->sge_array->addr[0];\n\tshdr = (union lpfc_sli4_cfg_shdr *)&rq_create->cfg_shdr;\n\n\tbf_set(lpfc_mbox_hdr_version, &shdr->request, LPFC_Q_CREATE_VERSION_2);\n\tcnt = 0;\n\n\tfor (idx = 0; idx < numrq; idx++) {\n\t\thrq = hrqp[idx];\n\t\tdrq = drqp[idx];\n\t\tcq  = cqp[idx];\n\n\t\t/* sanity check on queue memory */\n\t\tif (!hrq || !drq || !cq) {\n\t\t\tstatus = -ENODEV;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (hrq->entry_count != drq->entry_count) {\n\t\t\tstatus = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (idx == 0) {\n\t\t\tbf_set(lpfc_mbx_rq_create_num_pages,\n\t\t\t       &rq_create->u.request,\n\t\t\t       hrq->page_count);\n\t\t\tbf_set(lpfc_mbx_rq_create_rq_cnt,\n\t\t\t       &rq_create->u.request, (numrq * 2));\n\t\t\tbf_set(lpfc_mbx_rq_create_dnb, &rq_create->u.request,\n\t\t\t       1);\n\t\t\tbf_set(lpfc_rq_context_base_cq,\n\t\t\t       &rq_create->u.request.context,\n\t\t\t       cq->queue_id);\n\t\t\tbf_set(lpfc_rq_context_data_size,\n\t\t\t       &rq_create->u.request.context,\n\t\t\t       LPFC_NVMET_DATA_BUF_SIZE);\n\t\t\tbf_set(lpfc_rq_context_hdr_size,\n\t\t\t       &rq_create->u.request.context,\n\t\t\t       LPFC_HDR_BUF_SIZE);\n\t\t\tbf_set(lpfc_rq_context_rqe_count_1,\n\t\t\t       &rq_create->u.request.context,\n\t\t\t       hrq->entry_count);\n\t\t\tbf_set(lpfc_rq_context_rqe_size,\n\t\t\t       &rq_create->u.request.context,\n\t\t\t       LPFC_RQE_SIZE_8);\n\t\t\tbf_set(lpfc_rq_context_page_size,\n\t\t\t       &rq_create->u.request.context,\n\t\t\t       (PAGE_SIZE/SLI4_PAGE_SIZE));\n\t\t}\n\t\trc = 0;\n\t\tlist_for_each_entry(dmabuf, &hrq->page_list, list) {\n\t\t\tmemset(dmabuf->virt, 0, hw_page_size);\n\t\t\tcnt = page_idx + dmabuf->buffer_tag;\n\t\t\trq_create->u.request.page[cnt].addr_lo =\n\t\t\t\t\tputPaddrLow(dmabuf->phys);\n\t\t\trq_create->u.request.page[cnt].addr_hi =\n\t\t\t\t\tputPaddrHigh(dmabuf->phys);\n\t\t\trc++;\n\t\t}\n\t\tpage_idx += rc;\n\n\t\trc = 0;\n\t\tlist_for_each_entry(dmabuf, &drq->page_list, list) {\n\t\t\tmemset(dmabuf->virt, 0, hw_page_size);\n\t\t\tcnt = page_idx + dmabuf->buffer_tag;\n\t\t\trq_create->u.request.page[cnt].addr_lo =\n\t\t\t\t\tputPaddrLow(dmabuf->phys);\n\t\t\trq_create->u.request.page[cnt].addr_hi =\n\t\t\t\t\tputPaddrHigh(dmabuf->phys);\n\t\t\trc++;\n\t\t}\n\t\tpage_idx += rc;\n\n\t\thrq->db_format = LPFC_DB_RING_FORMAT;\n\t\thrq->db_regaddr = phba->sli4_hba.RQDBregaddr;\n\t\thrq->type = LPFC_HRQ;\n\t\thrq->assoc_qid = cq->queue_id;\n\t\thrq->subtype = subtype;\n\t\thrq->host_index = 0;\n\t\thrq->hba_index = 0;\n\t\thrq->notify_interval = LPFC_RQ_NOTIFY_INTRVL;\n\n\t\tdrq->db_format = LPFC_DB_RING_FORMAT;\n\t\tdrq->db_regaddr = phba->sli4_hba.RQDBregaddr;\n\t\tdrq->type = LPFC_DRQ;\n\t\tdrq->assoc_qid = cq->queue_id;\n\t\tdrq->subtype = subtype;\n\t\tdrq->host_index = 0;\n\t\tdrq->hba_index = 0;\n\t\tdrq->notify_interval = LPFC_RQ_NOTIFY_INTRVL;\n\n\t\tlist_add_tail(&hrq->list, &cq->child_list);\n\t\tlist_add_tail(&drq->list, &cq->child_list);\n\t}\n\n\trc = lpfc_sli_issue_mbox(phba, mbox, MBX_POLL);\n\t/* The IOCTL status is embedded in the mailbox subheader. */\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"3120 RQ_CREATE mailbox failed with \"\n\t\t\t\t\"status x%x add_status x%x, mbx status x%x\\n\",\n\t\t\t\tshdr_status, shdr_add_status, rc);\n\t\tstatus = -ENXIO;\n\t\tgoto out;\n\t}\n\trc = bf_get(lpfc_mbx_rq_create_q_id, &rq_create->u.response);\n\tif (rc == 0xFFFF) {\n\t\tstatus = -ENXIO;\n\t\tgoto out;\n\t}\n\n\t/* Initialize all RQs with associated queue id */\n\tfor (idx = 0; idx < numrq; idx++) {\n\t\thrq = hrqp[idx];\n\t\thrq->queue_id = rc + (2 * idx);\n\t\tdrq = drqp[idx];\n\t\tdrq->queue_id = rc + (2 * idx) + 1;\n\t}\n\nout:\n\tlpfc_sli4_mbox_cmd_free(phba, mbox);\n\treturn status;\n}\n\n/**\n * lpfc_eq_destroy - Destroy an event Queue on the HBA\n * @phba: HBA structure that indicates port to destroy a queue on.\n * @eq: The queue structure associated with the queue to destroy.\n *\n * This function destroys a queue, as detailed in @eq by sending an mailbox\n * command, specific to the type of queue, to the HBA.\n *\n * The @eq struct is used to get the queue ID of the queue to destroy.\n *\n * On success this function will return a zero. If the queue destroy mailbox\n * command fails this function will return -ENXIO.\n **/\nint\nlpfc_eq_destroy(struct lpfc_hba *phba, struct lpfc_queue *eq)\n{\n\tLPFC_MBOXQ_t *mbox;\n\tint rc, length, status = 0;\n\tuint32_t shdr_status, shdr_add_status;\n\tunion lpfc_sli4_cfg_shdr *shdr;\n\n\t/* sanity check on queue memory */\n\tif (!eq)\n\t\treturn -ENODEV;\n\n\tmbox = mempool_alloc(eq->phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox)\n\t\treturn -ENOMEM;\n\tlength = (sizeof(struct lpfc_mbx_eq_destroy) -\n\t\t  sizeof(struct lpfc_sli4_cfg_mhdr));\n\tlpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_COMMON,\n\t\t\t LPFC_MBOX_OPCODE_EQ_DESTROY,\n\t\t\t length, LPFC_SLI4_MBX_EMBED);\n\tbf_set(lpfc_mbx_eq_destroy_q_id, &mbox->u.mqe.un.eq_destroy.u.request,\n\t       eq->queue_id);\n\tmbox->vport = eq->phba->pport;\n\tmbox->mbox_cmpl = lpfc_sli_def_mbox_cmpl;\n\n\trc = lpfc_sli_issue_mbox(eq->phba, mbox, MBX_POLL);\n\t/* The IOCTL status is embedded in the mailbox subheader. */\n\tshdr = (union lpfc_sli4_cfg_shdr *)\n\t\t&mbox->u.mqe.un.eq_destroy.header.cfg_shdr;\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2505 EQ_DESTROY mailbox failed with \"\n\t\t\t\t\"status x%x add_status x%x, mbx status x%x\\n\",\n\t\t\t\tshdr_status, shdr_add_status, rc);\n\t\tstatus = -ENXIO;\n\t}\n\n\t/* Remove eq from any list */\n\tlist_del_init(&eq->list);\n\tmempool_free(mbox, eq->phba->mbox_mem_pool);\n\treturn status;\n}\n\n/**\n * lpfc_cq_destroy - Destroy a Completion Queue on the HBA\n * @phba: HBA structure that indicates port to destroy a queue on.\n * @cq: The queue structure associated with the queue to destroy.\n *\n * This function destroys a queue, as detailed in @cq by sending an mailbox\n * command, specific to the type of queue, to the HBA.\n *\n * The @cq struct is used to get the queue ID of the queue to destroy.\n *\n * On success this function will return a zero. If the queue destroy mailbox\n * command fails this function will return -ENXIO.\n **/\nint\nlpfc_cq_destroy(struct lpfc_hba *phba, struct lpfc_queue *cq)\n{\n\tLPFC_MBOXQ_t *mbox;\n\tint rc, length, status = 0;\n\tuint32_t shdr_status, shdr_add_status;\n\tunion lpfc_sli4_cfg_shdr *shdr;\n\n\t/* sanity check on queue memory */\n\tif (!cq)\n\t\treturn -ENODEV;\n\tmbox = mempool_alloc(cq->phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox)\n\t\treturn -ENOMEM;\n\tlength = (sizeof(struct lpfc_mbx_cq_destroy) -\n\t\t  sizeof(struct lpfc_sli4_cfg_mhdr));\n\tlpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_COMMON,\n\t\t\t LPFC_MBOX_OPCODE_CQ_DESTROY,\n\t\t\t length, LPFC_SLI4_MBX_EMBED);\n\tbf_set(lpfc_mbx_cq_destroy_q_id, &mbox->u.mqe.un.cq_destroy.u.request,\n\t       cq->queue_id);\n\tmbox->vport = cq->phba->pport;\n\tmbox->mbox_cmpl = lpfc_sli_def_mbox_cmpl;\n\trc = lpfc_sli_issue_mbox(cq->phba, mbox, MBX_POLL);\n\t/* The IOCTL status is embedded in the mailbox subheader. */\n\tshdr = (union lpfc_sli4_cfg_shdr *)\n\t\t&mbox->u.mqe.un.wq_create.header.cfg_shdr;\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2506 CQ_DESTROY mailbox failed with \"\n\t\t\t\t\"status x%x add_status x%x, mbx status x%x\\n\",\n\t\t\t\tshdr_status, shdr_add_status, rc);\n\t\tstatus = -ENXIO;\n\t}\n\t/* Remove cq from any list */\n\tlist_del_init(&cq->list);\n\tmempool_free(mbox, cq->phba->mbox_mem_pool);\n\treturn status;\n}\n\n/**\n * lpfc_mq_destroy - Destroy a Mailbox Queue on the HBA\n * @phba: HBA structure that indicates port to destroy a queue on.\n * @mq: The queue structure associated with the queue to destroy.\n *\n * This function destroys a queue, as detailed in @mq by sending an mailbox\n * command, specific to the type of queue, to the HBA.\n *\n * The @mq struct is used to get the queue ID of the queue to destroy.\n *\n * On success this function will return a zero. If the queue destroy mailbox\n * command fails this function will return -ENXIO.\n **/\nint\nlpfc_mq_destroy(struct lpfc_hba *phba, struct lpfc_queue *mq)\n{\n\tLPFC_MBOXQ_t *mbox;\n\tint rc, length, status = 0;\n\tuint32_t shdr_status, shdr_add_status;\n\tunion lpfc_sli4_cfg_shdr *shdr;\n\n\t/* sanity check on queue memory */\n\tif (!mq)\n\t\treturn -ENODEV;\n\tmbox = mempool_alloc(mq->phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox)\n\t\treturn -ENOMEM;\n\tlength = (sizeof(struct lpfc_mbx_mq_destroy) -\n\t\t  sizeof(struct lpfc_sli4_cfg_mhdr));\n\tlpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_COMMON,\n\t\t\t LPFC_MBOX_OPCODE_MQ_DESTROY,\n\t\t\t length, LPFC_SLI4_MBX_EMBED);\n\tbf_set(lpfc_mbx_mq_destroy_q_id, &mbox->u.mqe.un.mq_destroy.u.request,\n\t       mq->queue_id);\n\tmbox->vport = mq->phba->pport;\n\tmbox->mbox_cmpl = lpfc_sli_def_mbox_cmpl;\n\trc = lpfc_sli_issue_mbox(mq->phba, mbox, MBX_POLL);\n\t/* The IOCTL status is embedded in the mailbox subheader. */\n\tshdr = (union lpfc_sli4_cfg_shdr *)\n\t\t&mbox->u.mqe.un.mq_destroy.header.cfg_shdr;\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2507 MQ_DESTROY mailbox failed with \"\n\t\t\t\t\"status x%x add_status x%x, mbx status x%x\\n\",\n\t\t\t\tshdr_status, shdr_add_status, rc);\n\t\tstatus = -ENXIO;\n\t}\n\t/* Remove mq from any list */\n\tlist_del_init(&mq->list);\n\tmempool_free(mbox, mq->phba->mbox_mem_pool);\n\treturn status;\n}\n\n/**\n * lpfc_wq_destroy - Destroy a Work Queue on the HBA\n * @phba: HBA structure that indicates port to destroy a queue on.\n * @wq: The queue structure associated with the queue to destroy.\n *\n * This function destroys a queue, as detailed in @wq by sending an mailbox\n * command, specific to the type of queue, to the HBA.\n *\n * The @wq struct is used to get the queue ID of the queue to destroy.\n *\n * On success this function will return a zero. If the queue destroy mailbox\n * command fails this function will return -ENXIO.\n **/\nint\nlpfc_wq_destroy(struct lpfc_hba *phba, struct lpfc_queue *wq)\n{\n\tLPFC_MBOXQ_t *mbox;\n\tint rc, length, status = 0;\n\tuint32_t shdr_status, shdr_add_status;\n\tunion lpfc_sli4_cfg_shdr *shdr;\n\n\t/* sanity check on queue memory */\n\tif (!wq)\n\t\treturn -ENODEV;\n\tmbox = mempool_alloc(wq->phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox)\n\t\treturn -ENOMEM;\n\tlength = (sizeof(struct lpfc_mbx_wq_destroy) -\n\t\t  sizeof(struct lpfc_sli4_cfg_mhdr));\n\tlpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_FCOE,\n\t\t\t LPFC_MBOX_OPCODE_FCOE_WQ_DESTROY,\n\t\t\t length, LPFC_SLI4_MBX_EMBED);\n\tbf_set(lpfc_mbx_wq_destroy_q_id, &mbox->u.mqe.un.wq_destroy.u.request,\n\t       wq->queue_id);\n\tmbox->vport = wq->phba->pport;\n\tmbox->mbox_cmpl = lpfc_sli_def_mbox_cmpl;\n\trc = lpfc_sli_issue_mbox(wq->phba, mbox, MBX_POLL);\n\tshdr = (union lpfc_sli4_cfg_shdr *)\n\t\t&mbox->u.mqe.un.wq_destroy.header.cfg_shdr;\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2508 WQ_DESTROY mailbox failed with \"\n\t\t\t\t\"status x%x add_status x%x, mbx status x%x\\n\",\n\t\t\t\tshdr_status, shdr_add_status, rc);\n\t\tstatus = -ENXIO;\n\t}\n\t/* Remove wq from any list */\n\tlist_del_init(&wq->list);\n\tkfree(wq->pring);\n\twq->pring = NULL;\n\tmempool_free(mbox, wq->phba->mbox_mem_pool);\n\treturn status;\n}\n\n/**\n * lpfc_rq_destroy - Destroy a Receive Queue on the HBA\n * @phba: HBA structure that indicates port to destroy a queue on.\n * @hrq: The queue structure associated with the queue to destroy.\n * @drq: The queue structure associated with the queue to destroy.\n *\n * This function destroys a queue, as detailed in @rq by sending an mailbox\n * command, specific to the type of queue, to the HBA.\n *\n * The @rq struct is used to get the queue ID of the queue to destroy.\n *\n * On success this function will return a zero. If the queue destroy mailbox\n * command fails this function will return -ENXIO.\n **/\nint\nlpfc_rq_destroy(struct lpfc_hba *phba, struct lpfc_queue *hrq,\n\t\tstruct lpfc_queue *drq)\n{\n\tLPFC_MBOXQ_t *mbox;\n\tint rc, length, status = 0;\n\tuint32_t shdr_status, shdr_add_status;\n\tunion lpfc_sli4_cfg_shdr *shdr;\n\n\t/* sanity check on queue memory */\n\tif (!hrq || !drq)\n\t\treturn -ENODEV;\n\tmbox = mempool_alloc(hrq->phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox)\n\t\treturn -ENOMEM;\n\tlength = (sizeof(struct lpfc_mbx_rq_destroy) -\n\t\t  sizeof(struct lpfc_sli4_cfg_mhdr));\n\tlpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_FCOE,\n\t\t\t LPFC_MBOX_OPCODE_FCOE_RQ_DESTROY,\n\t\t\t length, LPFC_SLI4_MBX_EMBED);\n\tbf_set(lpfc_mbx_rq_destroy_q_id, &mbox->u.mqe.un.rq_destroy.u.request,\n\t       hrq->queue_id);\n\tmbox->vport = hrq->phba->pport;\n\tmbox->mbox_cmpl = lpfc_sli_def_mbox_cmpl;\n\trc = lpfc_sli_issue_mbox(hrq->phba, mbox, MBX_POLL);\n\t/* The IOCTL status is embedded in the mailbox subheader. */\n\tshdr = (union lpfc_sli4_cfg_shdr *)\n\t\t&mbox->u.mqe.un.rq_destroy.header.cfg_shdr;\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2509 RQ_DESTROY mailbox failed with \"\n\t\t\t\t\"status x%x add_status x%x, mbx status x%x\\n\",\n\t\t\t\tshdr_status, shdr_add_status, rc);\n\t\tif (rc != MBX_TIMEOUT)\n\t\t\tmempool_free(mbox, hrq->phba->mbox_mem_pool);\n\t\treturn -ENXIO;\n\t}\n\tbf_set(lpfc_mbx_rq_destroy_q_id, &mbox->u.mqe.un.rq_destroy.u.request,\n\t       drq->queue_id);\n\trc = lpfc_sli_issue_mbox(drq->phba, mbox, MBX_POLL);\n\tshdr = (union lpfc_sli4_cfg_shdr *)\n\t\t&mbox->u.mqe.un.rq_destroy.header.cfg_shdr;\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2510 RQ_DESTROY mailbox failed with \"\n\t\t\t\t\"status x%x add_status x%x, mbx status x%x\\n\",\n\t\t\t\tshdr_status, shdr_add_status, rc);\n\t\tstatus = -ENXIO;\n\t}\n\tlist_del_init(&hrq->list);\n\tlist_del_init(&drq->list);\n\tmempool_free(mbox, hrq->phba->mbox_mem_pool);\n\treturn status;\n}\n\n/**\n * lpfc_sli4_post_sgl - Post scatter gather list for an XRI to HBA\n * @phba: The virtual port for which this call being executed.\n * @pdma_phys_addr0: Physical address of the 1st SGL page.\n * @pdma_phys_addr1: Physical address of the 2nd SGL page.\n * @xritag: the xritag that ties this io to the SGL pages.\n *\n * This routine will post the sgl pages for the IO that has the xritag\n * that is in the iocbq structure. The xritag is assigned during iocbq\n * creation and persists for as long as the driver is loaded.\n * if the caller has fewer than 256 scatter gather segments to map then\n * pdma_phys_addr1 should be 0.\n * If the caller needs to map more than 256 scatter gather segment then\n * pdma_phys_addr1 should be a valid physical address.\n * physical address for SGLs must be 64 byte aligned.\n * If you are going to map 2 SGL's then the first one must have 256 entries\n * the second sgl can have between 1 and 256 entries.\n *\n * Return codes:\n * \t0 - Success\n * \t-ENXIO, -ENOMEM - Failure\n **/\nint\nlpfc_sli4_post_sgl(struct lpfc_hba *phba,\n\t\tdma_addr_t pdma_phys_addr0,\n\t\tdma_addr_t pdma_phys_addr1,\n\t\tuint16_t xritag)\n{\n\tstruct lpfc_mbx_post_sgl_pages *post_sgl_pages;\n\tLPFC_MBOXQ_t *mbox;\n\tint rc;\n\tuint32_t shdr_status, shdr_add_status;\n\tuint32_t mbox_tmo;\n\tunion lpfc_sli4_cfg_shdr *shdr;\n\n\tif (xritag == NO_XRI) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0364 Invalid param:\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tmbox = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox)\n\t\treturn -ENOMEM;\n\n\tlpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_FCOE,\n\t\t\tLPFC_MBOX_OPCODE_FCOE_POST_SGL_PAGES,\n\t\t\tsizeof(struct lpfc_mbx_post_sgl_pages) -\n\t\t\tsizeof(struct lpfc_sli4_cfg_mhdr), LPFC_SLI4_MBX_EMBED);\n\n\tpost_sgl_pages = (struct lpfc_mbx_post_sgl_pages *)\n\t\t\t\t&mbox->u.mqe.un.post_sgl_pages;\n\tbf_set(lpfc_post_sgl_pages_xri, post_sgl_pages, xritag);\n\tbf_set(lpfc_post_sgl_pages_xricnt, post_sgl_pages, 1);\n\n\tpost_sgl_pages->sgl_pg_pairs[0].sgl_pg0_addr_lo\t=\n\t\t\t\tcpu_to_le32(putPaddrLow(pdma_phys_addr0));\n\tpost_sgl_pages->sgl_pg_pairs[0].sgl_pg0_addr_hi =\n\t\t\t\tcpu_to_le32(putPaddrHigh(pdma_phys_addr0));\n\n\tpost_sgl_pages->sgl_pg_pairs[0].sgl_pg1_addr_lo\t=\n\t\t\t\tcpu_to_le32(putPaddrLow(pdma_phys_addr1));\n\tpost_sgl_pages->sgl_pg_pairs[0].sgl_pg1_addr_hi =\n\t\t\t\tcpu_to_le32(putPaddrHigh(pdma_phys_addr1));\n\tif (!phba->sli4_hba.intr_enable)\n\t\trc = lpfc_sli_issue_mbox(phba, mbox, MBX_POLL);\n\telse {\n\t\tmbox_tmo = lpfc_mbox_tmo_val(phba, mbox);\n\t\trc = lpfc_sli_issue_mbox_wait(phba, mbox, mbox_tmo);\n\t}\n\t/* The IOCTL status is embedded in the mailbox subheader. */\n\tshdr = (union lpfc_sli4_cfg_shdr *) &post_sgl_pages->header.cfg_shdr;\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\tif (rc != MBX_TIMEOUT)\n\t\tmempool_free(mbox, phba->mbox_mem_pool);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2511 POST_SGL mailbox failed with \"\n\t\t\t\t\"status x%x add_status x%x, mbx status x%x\\n\",\n\t\t\t\tshdr_status, shdr_add_status, rc);\n\t}\n\treturn 0;\n}\n\n/**\n * lpfc_sli4_alloc_xri - Get an available rpi in the device's range\n * @phba: pointer to lpfc hba data structure.\n *\n * This routine is invoked to post rpi header templates to the\n * HBA consistent with the SLI-4 interface spec.  This routine\n * posts a SLI4_PAGE_SIZE memory region to the port to hold up to\n * SLI4_PAGE_SIZE modulo 64 rpi context headers.\n *\n * Returns\n *\tA nonzero rpi defined as rpi_base <= rpi < max_rpi if successful\n *\tLPFC_RPI_ALLOC_ERROR if no rpis are available.\n **/\nstatic uint16_t\nlpfc_sli4_alloc_xri(struct lpfc_hba *phba)\n{\n\tunsigned long xri;\n\n\t/*\n\t * Fetch the next logical xri.  Because this index is logical,\n\t * the driver starts at 0 each time.\n\t */\n\tspin_lock_irq(&phba->hbalock);\n\txri = find_next_zero_bit(phba->sli4_hba.xri_bmask,\n\t\t\t\t phba->sli4_hba.max_cfg_param.max_xri, 0);\n\tif (xri >= phba->sli4_hba.max_cfg_param.max_xri) {\n\t\tspin_unlock_irq(&phba->hbalock);\n\t\treturn NO_XRI;\n\t} else {\n\t\tset_bit(xri, phba->sli4_hba.xri_bmask);\n\t\tphba->sli4_hba.max_cfg_param.xri_used++;\n\t}\n\tspin_unlock_irq(&phba->hbalock);\n\treturn xri;\n}\n\n/**\n * lpfc_sli4_free_xri - Release an xri for reuse.\n * @phba: pointer to lpfc hba data structure.\n * @xri: xri to release.\n *\n * This routine is invoked to release an xri to the pool of\n * available rpis maintained by the driver.\n **/\nstatic void\n__lpfc_sli4_free_xri(struct lpfc_hba *phba, int xri)\n{\n\tif (test_and_clear_bit(xri, phba->sli4_hba.xri_bmask)) {\n\t\tphba->sli4_hba.max_cfg_param.xri_used--;\n\t}\n}\n\n/**\n * lpfc_sli4_free_xri - Release an xri for reuse.\n * @phba: pointer to lpfc hba data structure.\n * @xri: xri to release.\n *\n * This routine is invoked to release an xri to the pool of\n * available rpis maintained by the driver.\n **/\nvoid\nlpfc_sli4_free_xri(struct lpfc_hba *phba, int xri)\n{\n\tspin_lock_irq(&phba->hbalock);\n\t__lpfc_sli4_free_xri(phba, xri);\n\tspin_unlock_irq(&phba->hbalock);\n}\n\n/**\n * lpfc_sli4_next_xritag - Get an xritag for the io\n * @phba: Pointer to HBA context object.\n *\n * This function gets an xritag for the iocb. If there is no unused xritag\n * it will return 0xffff.\n * The function returns the allocated xritag if successful, else returns zero.\n * Zero is not a valid xritag.\n * The caller is not required to hold any lock.\n **/\nuint16_t\nlpfc_sli4_next_xritag(struct lpfc_hba *phba)\n{\n\tuint16_t xri_index;\n\n\txri_index = lpfc_sli4_alloc_xri(phba);\n\tif (xri_index == NO_XRI)\n\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_SLI,\n\t\t\t\t\"2004 Failed to allocate XRI.last XRITAG is %d\"\n\t\t\t\t\" Max XRI is %d, Used XRI is %d\\n\",\n\t\t\t\txri_index,\n\t\t\t\tphba->sli4_hba.max_cfg_param.max_xri,\n\t\t\t\tphba->sli4_hba.max_cfg_param.xri_used);\n\treturn xri_index;\n}\n\n/**\n * lpfc_sli4_post_sgl_list - post a block of ELS sgls to the port.\n * @phba: pointer to lpfc hba data structure.\n * @post_sgl_list: pointer to els sgl entry list.\n * @post_cnt: number of els sgl entries on the list.\n *\n * This routine is invoked to post a block of driver's sgl pages to the\n * HBA using non-embedded mailbox command. No Lock is held. This routine\n * is only called when the driver is loading and after all IO has been\n * stopped.\n **/\nstatic int\nlpfc_sli4_post_sgl_list(struct lpfc_hba *phba,\n\t\t\t    struct list_head *post_sgl_list,\n\t\t\t    int post_cnt)\n{\n\tstruct lpfc_sglq *sglq_entry = NULL, *sglq_next = NULL;\n\tstruct lpfc_mbx_post_uembed_sgl_page1 *sgl;\n\tstruct sgl_page_pairs *sgl_pg_pairs;\n\tvoid *viraddr;\n\tLPFC_MBOXQ_t *mbox;\n\tuint32_t reqlen, alloclen, pg_pairs;\n\tuint32_t mbox_tmo;\n\tuint16_t xritag_start = 0;\n\tint rc = 0;\n\tuint32_t shdr_status, shdr_add_status;\n\tunion lpfc_sli4_cfg_shdr *shdr;\n\n\treqlen = post_cnt * sizeof(struct sgl_page_pairs) +\n\t\t sizeof(union lpfc_sli4_cfg_shdr) + sizeof(uint32_t);\n\tif (reqlen > SLI4_PAGE_SIZE) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2559 Block sgl registration required DMA \"\n\t\t\t\t\"size (%d) great than a page\\n\", reqlen);\n\t\treturn -ENOMEM;\n\t}\n\n\tmbox = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox)\n\t\treturn -ENOMEM;\n\n\t/* Allocate DMA memory and set up the non-embedded mailbox command */\n\talloclen = lpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_FCOE,\n\t\t\t LPFC_MBOX_OPCODE_FCOE_POST_SGL_PAGES, reqlen,\n\t\t\t LPFC_SLI4_MBX_NEMBED);\n\n\tif (alloclen < reqlen) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"0285 Allocated DMA memory size (%d) is \"\n\t\t\t\t\"less than the requested DMA memory \"\n\t\t\t\t\"size (%d)\\n\", alloclen, reqlen);\n\t\tlpfc_sli4_mbox_cmd_free(phba, mbox);\n\t\treturn -ENOMEM;\n\t}\n\t/* Set up the SGL pages in the non-embedded DMA pages */\n\tviraddr = mbox->sge_array->addr[0];\n\tsgl = (struct lpfc_mbx_post_uembed_sgl_page1 *)viraddr;\n\tsgl_pg_pairs = &sgl->sgl_pg_pairs;\n\n\tpg_pairs = 0;\n\tlist_for_each_entry_safe(sglq_entry, sglq_next, post_sgl_list, list) {\n\t\t/* Set up the sge entry */\n\t\tsgl_pg_pairs->sgl_pg0_addr_lo =\n\t\t\t\tcpu_to_le32(putPaddrLow(sglq_entry->phys));\n\t\tsgl_pg_pairs->sgl_pg0_addr_hi =\n\t\t\t\tcpu_to_le32(putPaddrHigh(sglq_entry->phys));\n\t\tsgl_pg_pairs->sgl_pg1_addr_lo =\n\t\t\t\tcpu_to_le32(putPaddrLow(0));\n\t\tsgl_pg_pairs->sgl_pg1_addr_hi =\n\t\t\t\tcpu_to_le32(putPaddrHigh(0));\n\n\t\t/* Keep the first xritag on the list */\n\t\tif (pg_pairs == 0)\n\t\t\txritag_start = sglq_entry->sli4_xritag;\n\t\tsgl_pg_pairs++;\n\t\tpg_pairs++;\n\t}\n\n\t/* Complete initialization and perform endian conversion. */\n\tbf_set(lpfc_post_sgl_pages_xri, sgl, xritag_start);\n\tbf_set(lpfc_post_sgl_pages_xricnt, sgl, post_cnt);\n\tsgl->word0 = cpu_to_le32(sgl->word0);\n\n\tif (!phba->sli4_hba.intr_enable)\n\t\trc = lpfc_sli_issue_mbox(phba, mbox, MBX_POLL);\n\telse {\n\t\tmbox_tmo = lpfc_mbox_tmo_val(phba, mbox);\n\t\trc = lpfc_sli_issue_mbox_wait(phba, mbox, mbox_tmo);\n\t}\n\tshdr = (union lpfc_sli4_cfg_shdr *) &sgl->cfg_shdr;\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\tif (rc != MBX_TIMEOUT)\n\t\tlpfc_sli4_mbox_cmd_free(phba, mbox);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2513 POST_SGL_BLOCK mailbox command failed \"\n\t\t\t\t\"status x%x add_status x%x mbx status x%x\\n\",\n\t\t\t\tshdr_status, shdr_add_status, rc);\n\t\trc = -ENXIO;\n\t}\n\treturn rc;\n}\n\n/**\n * lpfc_sli4_post_io_sgl_block - post a block of nvme sgl list to firmware\n * @phba: pointer to lpfc hba data structure.\n * @nblist: pointer to nvme buffer list.\n * @count: number of scsi buffers on the list.\n *\n * This routine is invoked to post a block of @count scsi sgl pages from a\n * SCSI buffer list @nblist to the HBA using non-embedded mailbox command.\n * No Lock is held.\n *\n **/\nstatic int\nlpfc_sli4_post_io_sgl_block(struct lpfc_hba *phba, struct list_head *nblist,\n\t\t\t    int count)\n{\n\tstruct lpfc_io_buf *lpfc_ncmd;\n\tstruct lpfc_mbx_post_uembed_sgl_page1 *sgl;\n\tstruct sgl_page_pairs *sgl_pg_pairs;\n\tvoid *viraddr;\n\tLPFC_MBOXQ_t *mbox;\n\tuint32_t reqlen, alloclen, pg_pairs;\n\tuint32_t mbox_tmo;\n\tuint16_t xritag_start = 0;\n\tint rc = 0;\n\tuint32_t shdr_status, shdr_add_status;\n\tdma_addr_t pdma_phys_bpl1;\n\tunion lpfc_sli4_cfg_shdr *shdr;\n\n\t/* Calculate the requested length of the dma memory */\n\treqlen = count * sizeof(struct sgl_page_pairs) +\n\t\t sizeof(union lpfc_sli4_cfg_shdr) + sizeof(uint32_t);\n\tif (reqlen > SLI4_PAGE_SIZE) {\n\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_INIT,\n\t\t\t\t\"6118 Block sgl registration required DMA \"\n\t\t\t\t\"size (%d) great than a page\\n\", reqlen);\n\t\treturn -ENOMEM;\n\t}\n\tmbox = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6119 Failed to allocate mbox cmd memory\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\t/* Allocate DMA memory and set up the non-embedded mailbox command */\n\talloclen = lpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_FCOE,\n\t\t\t\t    LPFC_MBOX_OPCODE_FCOE_POST_SGL_PAGES,\n\t\t\t\t    reqlen, LPFC_SLI4_MBX_NEMBED);\n\n\tif (alloclen < reqlen) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6120 Allocated DMA memory size (%d) is \"\n\t\t\t\t\"less than the requested DMA memory \"\n\t\t\t\t\"size (%d)\\n\", alloclen, reqlen);\n\t\tlpfc_sli4_mbox_cmd_free(phba, mbox);\n\t\treturn -ENOMEM;\n\t}\n\n\t/* Get the first SGE entry from the non-embedded DMA memory */\n\tviraddr = mbox->sge_array->addr[0];\n\n\t/* Set up the SGL pages in the non-embedded DMA pages */\n\tsgl = (struct lpfc_mbx_post_uembed_sgl_page1 *)viraddr;\n\tsgl_pg_pairs = &sgl->sgl_pg_pairs;\n\n\tpg_pairs = 0;\n\tlist_for_each_entry(lpfc_ncmd, nblist, list) {\n\t\t/* Set up the sge entry */\n\t\tsgl_pg_pairs->sgl_pg0_addr_lo =\n\t\t\tcpu_to_le32(putPaddrLow(lpfc_ncmd->dma_phys_sgl));\n\t\tsgl_pg_pairs->sgl_pg0_addr_hi =\n\t\t\tcpu_to_le32(putPaddrHigh(lpfc_ncmd->dma_phys_sgl));\n\t\tif (phba->cfg_sg_dma_buf_size > SGL_PAGE_SIZE)\n\t\t\tpdma_phys_bpl1 = lpfc_ncmd->dma_phys_sgl +\n\t\t\t\t\t\tSGL_PAGE_SIZE;\n\t\telse\n\t\t\tpdma_phys_bpl1 = 0;\n\t\tsgl_pg_pairs->sgl_pg1_addr_lo =\n\t\t\tcpu_to_le32(putPaddrLow(pdma_phys_bpl1));\n\t\tsgl_pg_pairs->sgl_pg1_addr_hi =\n\t\t\tcpu_to_le32(putPaddrHigh(pdma_phys_bpl1));\n\t\t/* Keep the first xritag on the list */\n\t\tif (pg_pairs == 0)\n\t\t\txritag_start = lpfc_ncmd->cur_iocbq.sli4_xritag;\n\t\tsgl_pg_pairs++;\n\t\tpg_pairs++;\n\t}\n\tbf_set(lpfc_post_sgl_pages_xri, sgl, xritag_start);\n\tbf_set(lpfc_post_sgl_pages_xricnt, sgl, pg_pairs);\n\t/* Perform endian conversion if necessary */\n\tsgl->word0 = cpu_to_le32(sgl->word0);\n\n\tif (!phba->sli4_hba.intr_enable) {\n\t\trc = lpfc_sli_issue_mbox(phba, mbox, MBX_POLL);\n\t} else {\n\t\tmbox_tmo = lpfc_mbox_tmo_val(phba, mbox);\n\t\trc = lpfc_sli_issue_mbox_wait(phba, mbox, mbox_tmo);\n\t}\n\tshdr = (union lpfc_sli4_cfg_shdr *)&sgl->cfg_shdr;\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\tif (rc != MBX_TIMEOUT)\n\t\tlpfc_sli4_mbox_cmd_free(phba, mbox);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"6125 POST_SGL_BLOCK mailbox command failed \"\n\t\t\t\t\"status x%x add_status x%x mbx status x%x\\n\",\n\t\t\t\tshdr_status, shdr_add_status, rc);\n\t\trc = -ENXIO;\n\t}\n\treturn rc;\n}\n\n/**\n * lpfc_sli4_post_io_sgl_list - Post blocks of nvme buffer sgls from a list\n * @phba: pointer to lpfc hba data structure.\n * @post_nblist: pointer to the nvme buffer list.\n * @sb_count: number of nvme buffers.\n *\n * This routine walks a list of nvme buffers that was passed in. It attempts\n * to construct blocks of nvme buffer sgls which contains contiguous xris and\n * uses the non-embedded SGL block post mailbox commands to post to the port.\n * For single NVME buffer sgl with non-contiguous xri, if any, it shall use\n * embedded SGL post mailbox command for posting. The @post_nblist passed in\n * must be local list, thus no lock is needed when manipulate the list.\n *\n * Returns: 0 = failure, non-zero number of successfully posted buffers.\n **/\nint\nlpfc_sli4_post_io_sgl_list(struct lpfc_hba *phba,\n\t\t\t   struct list_head *post_nblist, int sb_count)\n{\n\tstruct lpfc_io_buf *lpfc_ncmd, *lpfc_ncmd_next;\n\tint status, sgl_size;\n\tint post_cnt = 0, block_cnt = 0, num_posting = 0, num_posted = 0;\n\tdma_addr_t pdma_phys_sgl1;\n\tint last_xritag = NO_XRI;\n\tint cur_xritag;\n\tLIST_HEAD(prep_nblist);\n\tLIST_HEAD(blck_nblist);\n\tLIST_HEAD(nvme_nblist);\n\n\t/* sanity check */\n\tif (sb_count <= 0)\n\t\treturn -EINVAL;\n\n\tsgl_size = phba->cfg_sg_dma_buf_size;\n\tlist_for_each_entry_safe(lpfc_ncmd, lpfc_ncmd_next, post_nblist, list) {\n\t\tlist_del_init(&lpfc_ncmd->list);\n\t\tblock_cnt++;\n\t\tif ((last_xritag != NO_XRI) &&\n\t\t    (lpfc_ncmd->cur_iocbq.sli4_xritag != last_xritag + 1)) {\n\t\t\t/* a hole in xri block, form a sgl posting block */\n\t\t\tlist_splice_init(&prep_nblist, &blck_nblist);\n\t\t\tpost_cnt = block_cnt - 1;\n\t\t\t/* prepare list for next posting block */\n\t\t\tlist_add_tail(&lpfc_ncmd->list, &prep_nblist);\n\t\t\tblock_cnt = 1;\n\t\t} else {\n\t\t\t/* prepare list for next posting block */\n\t\t\tlist_add_tail(&lpfc_ncmd->list, &prep_nblist);\n\t\t\t/* enough sgls for non-embed sgl mbox command */\n\t\t\tif (block_cnt == LPFC_NEMBED_MBOX_SGL_CNT) {\n\t\t\t\tlist_splice_init(&prep_nblist, &blck_nblist);\n\t\t\t\tpost_cnt = block_cnt;\n\t\t\t\tblock_cnt = 0;\n\t\t\t}\n\t\t}\n\t\tnum_posting++;\n\t\tlast_xritag = lpfc_ncmd->cur_iocbq.sli4_xritag;\n\n\t\t/* end of repost sgl list condition for NVME buffers */\n\t\tif (num_posting == sb_count) {\n\t\t\tif (post_cnt == 0) {\n\t\t\t\t/* last sgl posting block */\n\t\t\t\tlist_splice_init(&prep_nblist, &blck_nblist);\n\t\t\t\tpost_cnt = block_cnt;\n\t\t\t} else if (block_cnt == 1) {\n\t\t\t\t/* last single sgl with non-contiguous xri */\n\t\t\t\tif (sgl_size > SGL_PAGE_SIZE)\n\t\t\t\t\tpdma_phys_sgl1 =\n\t\t\t\t\t\tlpfc_ncmd->dma_phys_sgl +\n\t\t\t\t\t\tSGL_PAGE_SIZE;\n\t\t\t\telse\n\t\t\t\t\tpdma_phys_sgl1 = 0;\n\t\t\t\tcur_xritag = lpfc_ncmd->cur_iocbq.sli4_xritag;\n\t\t\t\tstatus = lpfc_sli4_post_sgl(\n\t\t\t\t\t\tphba, lpfc_ncmd->dma_phys_sgl,\n\t\t\t\t\t\tpdma_phys_sgl1, cur_xritag);\n\t\t\t\tif (status) {\n\t\t\t\t\t/* Post error.  Buffer unavailable. */\n\t\t\t\t\tlpfc_ncmd->flags |=\n\t\t\t\t\t\tLPFC_SBUF_NOT_POSTED;\n\t\t\t\t} else {\n\t\t\t\t\t/* Post success. Bffer available. */\n\t\t\t\t\tlpfc_ncmd->flags &=\n\t\t\t\t\t\t~LPFC_SBUF_NOT_POSTED;\n\t\t\t\t\tlpfc_ncmd->status = IOSTAT_SUCCESS;\n\t\t\t\t\tnum_posted++;\n\t\t\t\t}\n\t\t\t\t/* success, put on NVME buffer sgl list */\n\t\t\t\tlist_add_tail(&lpfc_ncmd->list, &nvme_nblist);\n\t\t\t}\n\t\t}\n\n\t\t/* continue until a nembed page worth of sgls */\n\t\tif (post_cnt == 0)\n\t\t\tcontinue;\n\n\t\t/* post block of NVME buffer list sgls */\n\t\tstatus = lpfc_sli4_post_io_sgl_block(phba, &blck_nblist,\n\t\t\t\t\t\t     post_cnt);\n\n\t\t/* don't reset xirtag due to hole in xri block */\n\t\tif (block_cnt == 0)\n\t\t\tlast_xritag = NO_XRI;\n\n\t\t/* reset NVME buffer post count for next round of posting */\n\t\tpost_cnt = 0;\n\n\t\t/* put posted NVME buffer-sgl posted on NVME buffer sgl list */\n\t\twhile (!list_empty(&blck_nblist)) {\n\t\t\tlist_remove_head(&blck_nblist, lpfc_ncmd,\n\t\t\t\t\t struct lpfc_io_buf, list);\n\t\t\tif (status) {\n\t\t\t\t/* Post error.  Mark buffer unavailable. */\n\t\t\t\tlpfc_ncmd->flags |= LPFC_SBUF_NOT_POSTED;\n\t\t\t} else {\n\t\t\t\t/* Post success, Mark buffer available. */\n\t\t\t\tlpfc_ncmd->flags &= ~LPFC_SBUF_NOT_POSTED;\n\t\t\t\tlpfc_ncmd->status = IOSTAT_SUCCESS;\n\t\t\t\tnum_posted++;\n\t\t\t}\n\t\t\tlist_add_tail(&lpfc_ncmd->list, &nvme_nblist);\n\t\t}\n\t}\n\t/* Push NVME buffers with sgl posted to the available list */\n\tlpfc_io_buf_replenish(phba, &nvme_nblist);\n\n\treturn num_posted;\n}\n\n/**\n * lpfc_fc_frame_check - Check that this frame is a valid frame to handle\n * @phba: pointer to lpfc_hba struct that the frame was received on\n * @fc_hdr: A pointer to the FC Header data (In Big Endian Format)\n *\n * This function checks the fields in the @fc_hdr to see if the FC frame is a\n * valid type of frame that the LPFC driver will handle. This function will\n * return a zero if the frame is a valid frame or a non zero value when the\n * frame does not pass the check.\n **/\nstatic int\nlpfc_fc_frame_check(struct lpfc_hba *phba, struct fc_frame_header *fc_hdr)\n{\n\t/*  make rctl_names static to save stack space */\n\tstruct fc_vft_header *fc_vft_hdr;\n\tuint32_t *header = (uint32_t *) fc_hdr;\n\n#define FC_RCTL_MDS_DIAGS\t0xF4\n\n\tswitch (fc_hdr->fh_r_ctl) {\n\tcase FC_RCTL_DD_UNCAT:\t\t/* uncategorized information */\n\tcase FC_RCTL_DD_SOL_DATA:\t/* solicited data */\n\tcase FC_RCTL_DD_UNSOL_CTL:\t/* unsolicited control */\n\tcase FC_RCTL_DD_SOL_CTL:\t/* solicited control or reply */\n\tcase FC_RCTL_DD_UNSOL_DATA:\t/* unsolicited data */\n\tcase FC_RCTL_DD_DATA_DESC:\t/* data descriptor */\n\tcase FC_RCTL_DD_UNSOL_CMD:\t/* unsolicited command */\n\tcase FC_RCTL_DD_CMD_STATUS:\t/* command status */\n\tcase FC_RCTL_ELS_REQ:\t/* extended link services request */\n\tcase FC_RCTL_ELS_REP:\t/* extended link services reply */\n\tcase FC_RCTL_ELS4_REQ:\t/* FC-4 ELS request */\n\tcase FC_RCTL_ELS4_REP:\t/* FC-4 ELS reply */\n\tcase FC_RCTL_BA_NOP:  \t/* basic link service NOP */\n\tcase FC_RCTL_BA_ABTS: \t/* basic link service abort */\n\tcase FC_RCTL_BA_RMC: \t/* remove connection */\n\tcase FC_RCTL_BA_ACC:\t/* basic accept */\n\tcase FC_RCTL_BA_RJT:\t/* basic reject */\n\tcase FC_RCTL_BA_PRMT:\n\tcase FC_RCTL_ACK_1:\t/* acknowledge_1 */\n\tcase FC_RCTL_ACK_0:\t/* acknowledge_0 */\n\tcase FC_RCTL_P_RJT:\t/* port reject */\n\tcase FC_RCTL_F_RJT:\t/* fabric reject */\n\tcase FC_RCTL_P_BSY:\t/* port busy */\n\tcase FC_RCTL_F_BSY:\t/* fabric busy to data frame */\n\tcase FC_RCTL_F_BSYL:\t/* fabric busy to link control frame */\n\tcase FC_RCTL_LCR:\t/* link credit reset */\n\tcase FC_RCTL_MDS_DIAGS: /* MDS Diagnostics */\n\tcase FC_RCTL_END:\t/* end */\n\t\tbreak;\n\tcase FC_RCTL_VFTH:\t/* Virtual Fabric tagging Header */\n\t\tfc_vft_hdr = (struct fc_vft_header *)fc_hdr;\n\t\tfc_hdr = &((struct fc_frame_header *)fc_vft_hdr)[1];\n\t\treturn lpfc_fc_frame_check(phba, fc_hdr);\n\tdefault:\n\t\tgoto drop;\n\t}\n\n\tswitch (fc_hdr->fh_type) {\n\tcase FC_TYPE_BLS:\n\tcase FC_TYPE_ELS:\n\tcase FC_TYPE_FCP:\n\tcase FC_TYPE_CT:\n\tcase FC_TYPE_NVME:\n\t\tbreak;\n\tcase FC_TYPE_IP:\n\tcase FC_TYPE_ILS:\n\tdefault:\n\t\tgoto drop;\n\t}\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_ELS,\n\t\t\t\"2538 Received frame rctl:x%x, type:x%x, \"\n\t\t\t\"frame Data:%08x %08x %08x %08x %08x %08x %08x\\n\",\n\t\t\tfc_hdr->fh_r_ctl, fc_hdr->fh_type,\n\t\t\tbe32_to_cpu(header[0]), be32_to_cpu(header[1]),\n\t\t\tbe32_to_cpu(header[2]), be32_to_cpu(header[3]),\n\t\t\tbe32_to_cpu(header[4]), be32_to_cpu(header[5]),\n\t\t\tbe32_to_cpu(header[6]));\n\treturn 0;\ndrop:\n\tlpfc_printf_log(phba, KERN_WARNING, LOG_ELS,\n\t\t\t\"2539 Dropped frame rctl:x%x type:x%x\\n\",\n\t\t\tfc_hdr->fh_r_ctl, fc_hdr->fh_type);\n\treturn 1;\n}\n\n/**\n * lpfc_fc_hdr_get_vfi - Get the VFI from an FC frame\n * @fc_hdr: A pointer to the FC Header data (In Big Endian Format)\n *\n * This function processes the FC header to retrieve the VFI from the VF\n * header, if one exists. This function will return the VFI if one exists\n * or 0 if no VSAN Header exists.\n **/\nstatic uint32_t\nlpfc_fc_hdr_get_vfi(struct fc_frame_header *fc_hdr)\n{\n\tstruct fc_vft_header *fc_vft_hdr = (struct fc_vft_header *)fc_hdr;\n\n\tif (fc_hdr->fh_r_ctl != FC_RCTL_VFTH)\n\t\treturn 0;\n\treturn bf_get(fc_vft_hdr_vf_id, fc_vft_hdr);\n}\n\n/**\n * lpfc_fc_frame_to_vport - Finds the vport that a frame is destined to\n * @phba: Pointer to the HBA structure to search for the vport on\n * @fc_hdr: A pointer to the FC Header data (In Big Endian Format)\n * @fcfi: The FC Fabric ID that the frame came from\n * @did: Destination ID to match against\n *\n * This function searches the @phba for a vport that matches the content of the\n * @fc_hdr passed in and the @fcfi. This function uses the @fc_hdr to fetch the\n * VFI, if the Virtual Fabric Tagging Header exists, and the DID. This function\n * returns the matching vport pointer or NULL if unable to match frame to a\n * vport.\n **/\nstatic struct lpfc_vport *\nlpfc_fc_frame_to_vport(struct lpfc_hba *phba, struct fc_frame_header *fc_hdr,\n\t\t       uint16_t fcfi, uint32_t did)\n{\n\tstruct lpfc_vport **vports;\n\tstruct lpfc_vport *vport = NULL;\n\tint i;\n\n\tif (did == Fabric_DID)\n\t\treturn phba->pport;\n\tif ((phba->pport->fc_flag & FC_PT2PT) &&\n\t\t!(phba->link_state == LPFC_HBA_READY))\n\t\treturn phba->pport;\n\n\tvports = lpfc_create_vport_work_array(phba);\n\tif (vports != NULL) {\n\t\tfor (i = 0; i <= phba->max_vpi && vports[i] != NULL; i++) {\n\t\t\tif (phba->fcf.fcfi == fcfi &&\n\t\t\t    vports[i]->vfi == lpfc_fc_hdr_get_vfi(fc_hdr) &&\n\t\t\t    vports[i]->fc_myDID == did) {\n\t\t\t\tvport = vports[i];\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tlpfc_destroy_vport_work_array(phba, vports);\n\treturn vport;\n}\n\n/**\n * lpfc_update_rcv_time_stamp - Update vport's rcv seq time stamp\n * @vport: The vport to work on.\n *\n * This function updates the receive sequence time stamp for this vport. The\n * receive sequence time stamp indicates the time that the last frame of the\n * the sequence that has been idle for the longest amount of time was received.\n * the driver uses this time stamp to indicate if any received sequences have\n * timed out.\n **/\nstatic void\nlpfc_update_rcv_time_stamp(struct lpfc_vport *vport)\n{\n\tstruct lpfc_dmabuf *h_buf;\n\tstruct hbq_dmabuf *dmabuf = NULL;\n\n\t/* get the oldest sequence on the rcv list */\n\th_buf = list_get_first(&vport->rcv_buffer_list,\n\t\t\t       struct lpfc_dmabuf, list);\n\tif (!h_buf)\n\t\treturn;\n\tdmabuf = container_of(h_buf, struct hbq_dmabuf, hbuf);\n\tvport->rcv_buffer_time_stamp = dmabuf->time_stamp;\n}\n\n/**\n * lpfc_cleanup_rcv_buffers - Cleans up all outstanding receive sequences.\n * @vport: The vport that the received sequences were sent to.\n *\n * This function cleans up all outstanding received sequences. This is called\n * by the driver when a link event or user action invalidates all the received\n * sequences.\n **/\nvoid\nlpfc_cleanup_rcv_buffers(struct lpfc_vport *vport)\n{\n\tstruct lpfc_dmabuf *h_buf, *hnext;\n\tstruct lpfc_dmabuf *d_buf, *dnext;\n\tstruct hbq_dmabuf *dmabuf = NULL;\n\n\t/* start with the oldest sequence on the rcv list */\n\tlist_for_each_entry_safe(h_buf, hnext, &vport->rcv_buffer_list, list) {\n\t\tdmabuf = container_of(h_buf, struct hbq_dmabuf, hbuf);\n\t\tlist_del_init(&dmabuf->hbuf.list);\n\t\tlist_for_each_entry_safe(d_buf, dnext,\n\t\t\t\t\t &dmabuf->dbuf.list, list) {\n\t\t\tlist_del_init(&d_buf->list);\n\t\t\tlpfc_in_buf_free(vport->phba, d_buf);\n\t\t}\n\t\tlpfc_in_buf_free(vport->phba, &dmabuf->dbuf);\n\t}\n}\n\n/**\n * lpfc_rcv_seq_check_edtov - Cleans up timed out receive sequences.\n * @vport: The vport that the received sequences were sent to.\n *\n * This function determines whether any received sequences have timed out by\n * first checking the vport's rcv_buffer_time_stamp. If this time_stamp\n * indicates that there is at least one timed out sequence this routine will\n * go through the received sequences one at a time from most inactive to most\n * active to determine which ones need to be cleaned up. Once it has determined\n * that a sequence needs to be cleaned up it will simply free up the resources\n * without sending an abort.\n **/\nvoid\nlpfc_rcv_seq_check_edtov(struct lpfc_vport *vport)\n{\n\tstruct lpfc_dmabuf *h_buf, *hnext;\n\tstruct lpfc_dmabuf *d_buf, *dnext;\n\tstruct hbq_dmabuf *dmabuf = NULL;\n\tunsigned long timeout;\n\tint abort_count = 0;\n\n\ttimeout = (msecs_to_jiffies(vport->phba->fc_edtov) +\n\t\t   vport->rcv_buffer_time_stamp);\n\tif (list_empty(&vport->rcv_buffer_list) ||\n\t    time_before(jiffies, timeout))\n\t\treturn;\n\t/* start with the oldest sequence on the rcv list */\n\tlist_for_each_entry_safe(h_buf, hnext, &vport->rcv_buffer_list, list) {\n\t\tdmabuf = container_of(h_buf, struct hbq_dmabuf, hbuf);\n\t\ttimeout = (msecs_to_jiffies(vport->phba->fc_edtov) +\n\t\t\t   dmabuf->time_stamp);\n\t\tif (time_before(jiffies, timeout))\n\t\t\tbreak;\n\t\tabort_count++;\n\t\tlist_del_init(&dmabuf->hbuf.list);\n\t\tlist_for_each_entry_safe(d_buf, dnext,\n\t\t\t\t\t &dmabuf->dbuf.list, list) {\n\t\t\tlist_del_init(&d_buf->list);\n\t\t\tlpfc_in_buf_free(vport->phba, d_buf);\n\t\t}\n\t\tlpfc_in_buf_free(vport->phba, &dmabuf->dbuf);\n\t}\n\tif (abort_count)\n\t\tlpfc_update_rcv_time_stamp(vport);\n}\n\n/**\n * lpfc_fc_frame_add - Adds a frame to the vport's list of received sequences\n * @vport: pointer to a vitural port\n * @dmabuf: pointer to a dmabuf that describes the hdr and data of the FC frame\n *\n * This function searches through the existing incomplete sequences that have\n * been sent to this @vport. If the frame matches one of the incomplete\n * sequences then the dbuf in the @dmabuf is added to the list of frames that\n * make up that sequence. If no sequence is found that matches this frame then\n * the function will add the hbuf in the @dmabuf to the @vport's rcv_buffer_list\n * This function returns a pointer to the first dmabuf in the sequence list that\n * the frame was linked to.\n **/\nstatic struct hbq_dmabuf *\nlpfc_fc_frame_add(struct lpfc_vport *vport, struct hbq_dmabuf *dmabuf)\n{\n\tstruct fc_frame_header *new_hdr;\n\tstruct fc_frame_header *temp_hdr;\n\tstruct lpfc_dmabuf *d_buf;\n\tstruct lpfc_dmabuf *h_buf;\n\tstruct hbq_dmabuf *seq_dmabuf = NULL;\n\tstruct hbq_dmabuf *temp_dmabuf = NULL;\n\tuint8_t\tfound = 0;\n\n\tINIT_LIST_HEAD(&dmabuf->dbuf.list);\n\tdmabuf->time_stamp = jiffies;\n\tnew_hdr = (struct fc_frame_header *)dmabuf->hbuf.virt;\n\n\t/* Use the hdr_buf to find the sequence that this frame belongs to */\n\tlist_for_each_entry(h_buf, &vport->rcv_buffer_list, list) {\n\t\ttemp_hdr = (struct fc_frame_header *)h_buf->virt;\n\t\tif ((temp_hdr->fh_seq_id != new_hdr->fh_seq_id) ||\n\t\t    (temp_hdr->fh_ox_id != new_hdr->fh_ox_id) ||\n\t\t    (memcmp(&temp_hdr->fh_s_id, &new_hdr->fh_s_id, 3)))\n\t\t\tcontinue;\n\t\t/* found a pending sequence that matches this frame */\n\t\tseq_dmabuf = container_of(h_buf, struct hbq_dmabuf, hbuf);\n\t\tbreak;\n\t}\n\tif (!seq_dmabuf) {\n\t\t/*\n\t\t * This indicates first frame received for this sequence.\n\t\t * Queue the buffer on the vport's rcv_buffer_list.\n\t\t */\n\t\tlist_add_tail(&dmabuf->hbuf.list, &vport->rcv_buffer_list);\n\t\tlpfc_update_rcv_time_stamp(vport);\n\t\treturn dmabuf;\n\t}\n\ttemp_hdr = seq_dmabuf->hbuf.virt;\n\tif (be16_to_cpu(new_hdr->fh_seq_cnt) <\n\t\tbe16_to_cpu(temp_hdr->fh_seq_cnt)) {\n\t\tlist_del_init(&seq_dmabuf->hbuf.list);\n\t\tlist_add_tail(&dmabuf->hbuf.list, &vport->rcv_buffer_list);\n\t\tlist_add_tail(&dmabuf->dbuf.list, &seq_dmabuf->dbuf.list);\n\t\tlpfc_update_rcv_time_stamp(vport);\n\t\treturn dmabuf;\n\t}\n\t/* move this sequence to the tail to indicate a young sequence */\n\tlist_move_tail(&seq_dmabuf->hbuf.list, &vport->rcv_buffer_list);\n\tseq_dmabuf->time_stamp = jiffies;\n\tlpfc_update_rcv_time_stamp(vport);\n\tif (list_empty(&seq_dmabuf->dbuf.list)) {\n\t\ttemp_hdr = dmabuf->hbuf.virt;\n\t\tlist_add_tail(&dmabuf->dbuf.list, &seq_dmabuf->dbuf.list);\n\t\treturn seq_dmabuf;\n\t}\n\t/* find the correct place in the sequence to insert this frame */\n\td_buf = list_entry(seq_dmabuf->dbuf.list.prev, typeof(*d_buf), list);\n\twhile (!found) {\n\t\ttemp_dmabuf = container_of(d_buf, struct hbq_dmabuf, dbuf);\n\t\ttemp_hdr = (struct fc_frame_header *)temp_dmabuf->hbuf.virt;\n\t\t/*\n\t\t * If the frame's sequence count is greater than the frame on\n\t\t * the list then insert the frame right after this frame\n\t\t */\n\t\tif (be16_to_cpu(new_hdr->fh_seq_cnt) >\n\t\t\tbe16_to_cpu(temp_hdr->fh_seq_cnt)) {\n\t\t\tlist_add(&dmabuf->dbuf.list, &temp_dmabuf->dbuf.list);\n\t\t\tfound = 1;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (&d_buf->list == &seq_dmabuf->dbuf.list)\n\t\t\tbreak;\n\t\td_buf = list_entry(d_buf->list.prev, typeof(*d_buf), list);\n\t}\n\n\tif (found)\n\t\treturn seq_dmabuf;\n\treturn NULL;\n}\n\n/**\n * lpfc_sli4_abort_partial_seq - Abort partially assembled unsol sequence\n * @vport: pointer to a vitural port\n * @dmabuf: pointer to a dmabuf that describes the FC sequence\n *\n * This function tries to abort from the partially assembed sequence, described\n * by the information from basic abbort @dmabuf. It checks to see whether such\n * partially assembled sequence held by the driver. If so, it shall free up all\n * the frames from the partially assembled sequence.\n *\n * Return\n * true  -- if there is matching partially assembled sequence present and all\n *          the frames freed with the sequence;\n * false -- if there is no matching partially assembled sequence present so\n *          nothing got aborted in the lower layer driver\n **/\nstatic bool\nlpfc_sli4_abort_partial_seq(struct lpfc_vport *vport,\n\t\t\t    struct hbq_dmabuf *dmabuf)\n{\n\tstruct fc_frame_header *new_hdr;\n\tstruct fc_frame_header *temp_hdr;\n\tstruct lpfc_dmabuf *d_buf, *n_buf, *h_buf;\n\tstruct hbq_dmabuf *seq_dmabuf = NULL;\n\n\t/* Use the hdr_buf to find the sequence that matches this frame */\n\tINIT_LIST_HEAD(&dmabuf->dbuf.list);\n\tINIT_LIST_HEAD(&dmabuf->hbuf.list);\n\tnew_hdr = (struct fc_frame_header *)dmabuf->hbuf.virt;\n\tlist_for_each_entry(h_buf, &vport->rcv_buffer_list, list) {\n\t\ttemp_hdr = (struct fc_frame_header *)h_buf->virt;\n\t\tif ((temp_hdr->fh_seq_id != new_hdr->fh_seq_id) ||\n\t\t    (temp_hdr->fh_ox_id != new_hdr->fh_ox_id) ||\n\t\t    (memcmp(&temp_hdr->fh_s_id, &new_hdr->fh_s_id, 3)))\n\t\t\tcontinue;\n\t\t/* found a pending sequence that matches this frame */\n\t\tseq_dmabuf = container_of(h_buf, struct hbq_dmabuf, hbuf);\n\t\tbreak;\n\t}\n\n\t/* Free up all the frames from the partially assembled sequence */\n\tif (seq_dmabuf) {\n\t\tlist_for_each_entry_safe(d_buf, n_buf,\n\t\t\t\t\t &seq_dmabuf->dbuf.list, list) {\n\t\t\tlist_del_init(&d_buf->list);\n\t\t\tlpfc_in_buf_free(vport->phba, d_buf);\n\t\t}\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/**\n * lpfc_sli4_abort_ulp_seq - Abort assembled unsol sequence from ulp\n * @vport: pointer to a vitural port\n * @dmabuf: pointer to a dmabuf that describes the FC sequence\n *\n * This function tries to abort from the assembed sequence from upper level\n * protocol, described by the information from basic abbort @dmabuf. It\n * checks to see whether such pending context exists at upper level protocol.\n * If so, it shall clean up the pending context.\n *\n * Return\n * true  -- if there is matching pending context of the sequence cleaned\n *          at ulp;\n * false -- if there is no matching pending context of the sequence present\n *          at ulp.\n **/\nstatic bool\nlpfc_sli4_abort_ulp_seq(struct lpfc_vport *vport, struct hbq_dmabuf *dmabuf)\n{\n\tstruct lpfc_hba *phba = vport->phba;\n\tint handled;\n\n\t/* Accepting abort at ulp with SLI4 only */\n\tif (phba->sli_rev < LPFC_SLI_REV4)\n\t\treturn false;\n\n\t/* Register all caring upper level protocols to attend abort */\n\thandled = lpfc_ct_handle_unsol_abort(phba, dmabuf);\n\tif (handled)\n\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * lpfc_sli4_seq_abort_rsp_cmpl - BLS ABORT RSP seq abort iocb complete handler\n * @phba: Pointer to HBA context object.\n * @cmd_iocbq: pointer to the command iocbq structure.\n * @rsp_iocbq: pointer to the response iocbq structure.\n *\n * This function handles the sequence abort response iocb command complete\n * event. It properly releases the memory allocated to the sequence abort\n * accept iocb.\n **/\nstatic void\nlpfc_sli4_seq_abort_rsp_cmpl(struct lpfc_hba *phba,\n\t\t\t     struct lpfc_iocbq *cmd_iocbq,\n\t\t\t     struct lpfc_iocbq *rsp_iocbq)\n{\n\tstruct lpfc_nodelist *ndlp;\n\n\tif (cmd_iocbq) {\n\t\tndlp = (struct lpfc_nodelist *)cmd_iocbq->context1;\n\t\tlpfc_nlp_put(ndlp);\n\t\tlpfc_nlp_not_used(ndlp);\n\t\tlpfc_sli_release_iocbq(phba, cmd_iocbq);\n\t}\n\n\t/* Failure means BLS ABORT RSP did not get delivered to remote node*/\n\tif (rsp_iocbq && rsp_iocbq->iocb.ulpStatus)\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"3154 BLS ABORT RSP failed, data:  x%x/x%x\\n\",\n\t\t\trsp_iocbq->iocb.ulpStatus,\n\t\t\trsp_iocbq->iocb.un.ulpWord[4]);\n}\n\n/**\n * lpfc_sli4_xri_inrange - check xri is in range of xris owned by driver.\n * @phba: Pointer to HBA context object.\n * @xri: xri id in transaction.\n *\n * This function validates the xri maps to the known range of XRIs allocated an\n * used by the driver.\n **/\nuint16_t\nlpfc_sli4_xri_inrange(struct lpfc_hba *phba,\n\t\t      uint16_t xri)\n{\n\tuint16_t i;\n\n\tfor (i = 0; i < phba->sli4_hba.max_cfg_param.max_xri; i++) {\n\t\tif (xri == phba->sli4_hba.xri_ids[i])\n\t\t\treturn i;\n\t}\n\treturn NO_XRI;\n}\n\n/**\n * lpfc_sli4_seq_abort_rsp - bls rsp to sequence abort\n * @vport: pointer to a vitural port.\n * @fc_hdr: pointer to a FC frame header.\n * @aborted: was the partially assembled receive sequence successfully aborted\n *\n * This function sends a basic response to a previous unsol sequence abort\n * event after aborting the sequence handling.\n **/\nvoid\nlpfc_sli4_seq_abort_rsp(struct lpfc_vport *vport,\n\t\t\tstruct fc_frame_header *fc_hdr, bool aborted)\n{\n\tstruct lpfc_hba *phba = vport->phba;\n\tstruct lpfc_iocbq *ctiocb = NULL;\n\tstruct lpfc_nodelist *ndlp;\n\tuint16_t oxid, rxid, xri, lxri;\n\tuint32_t sid, fctl;\n\tIOCB_t *icmd;\n\tint rc;\n\n\tif (!lpfc_is_link_up(phba))\n\t\treturn;\n\n\tsid = sli4_sid_from_fc_hdr(fc_hdr);\n\toxid = be16_to_cpu(fc_hdr->fh_ox_id);\n\trxid = be16_to_cpu(fc_hdr->fh_rx_id);\n\n\tndlp = lpfc_findnode_did(vport, sid);\n\tif (!ndlp) {\n\t\tndlp = lpfc_nlp_init(vport, sid);\n\t\tif (!ndlp) {\n\t\t\tlpfc_printf_vlog(vport, KERN_WARNING, LOG_ELS,\n\t\t\t\t\t \"1268 Failed to allocate ndlp for \"\n\t\t\t\t\t \"oxid:x%x SID:x%x\\n\", oxid, sid);\n\t\t\treturn;\n\t\t}\n\t\t/* Put ndlp onto pport node list */\n\t\tlpfc_enqueue_node(vport, ndlp);\n\t} else if (!NLP_CHK_NODE_ACT(ndlp)) {\n\t\t/* re-setup ndlp without removing from node list */\n\t\tndlp = lpfc_enable_node(vport, ndlp, NLP_STE_UNUSED_NODE);\n\t\tif (!ndlp) {\n\t\t\tlpfc_printf_vlog(vport, KERN_WARNING, LOG_ELS,\n\t\t\t\t\t \"3275 Failed to active ndlp found \"\n\t\t\t\t\t \"for oxid:x%x SID:x%x\\n\", oxid, sid);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/* Allocate buffer for rsp iocb */\n\tctiocb = lpfc_sli_get_iocbq(phba);\n\tif (!ctiocb)\n\t\treturn;\n\n\t/* Extract the F_CTL field from FC_HDR */\n\tfctl = sli4_fctl_from_fc_hdr(fc_hdr);\n\n\ticmd = &ctiocb->iocb;\n\ticmd->un.xseq64.bdl.bdeSize = 0;\n\ticmd->un.xseq64.bdl.ulpIoTag32 = 0;\n\ticmd->un.xseq64.w5.hcsw.Dfctl = 0;\n\ticmd->un.xseq64.w5.hcsw.Rctl = FC_RCTL_BA_ACC;\n\ticmd->un.xseq64.w5.hcsw.Type = FC_TYPE_BLS;\n\n\t/* Fill in the rest of iocb fields */\n\ticmd->ulpCommand = CMD_XMIT_BLS_RSP64_CX;\n\ticmd->ulpBdeCount = 0;\n\ticmd->ulpLe = 1;\n\ticmd->ulpClass = CLASS3;\n\ticmd->ulpContext = phba->sli4_hba.rpi_ids[ndlp->nlp_rpi];\n\tctiocb->context1 = lpfc_nlp_get(ndlp);\n\n\tctiocb->vport = phba->pport;\n\tctiocb->iocb_cmpl = lpfc_sli4_seq_abort_rsp_cmpl;\n\tctiocb->sli4_lxritag = NO_XRI;\n\tctiocb->sli4_xritag = NO_XRI;\n\n\tif (fctl & FC_FC_EX_CTX)\n\t\t/* Exchange responder sent the abort so we\n\t\t * own the oxid.\n\t\t */\n\t\txri = oxid;\n\telse\n\t\txri = rxid;\n\tlxri = lpfc_sli4_xri_inrange(phba, xri);\n\tif (lxri != NO_XRI)\n\t\tlpfc_set_rrq_active(phba, ndlp, lxri,\n\t\t\t(xri == oxid) ? rxid : oxid, 0);\n\t/* For BA_ABTS from exchange responder, if the logical xri with\n\t * the oxid maps to the FCP XRI range, the port no longer has\n\t * that exchange context, send a BLS_RJT. Override the IOCB for\n\t * a BA_RJT.\n\t */\n\tif ((fctl & FC_FC_EX_CTX) &&\n\t    (lxri > lpfc_sli4_get_iocb_cnt(phba))) {\n\t\ticmd->un.xseq64.w5.hcsw.Rctl = FC_RCTL_BA_RJT;\n\t\tbf_set(lpfc_vndr_code, &icmd->un.bls_rsp, 0);\n\t\tbf_set(lpfc_rsn_expln, &icmd->un.bls_rsp, FC_BA_RJT_INV_XID);\n\t\tbf_set(lpfc_rsn_code, &icmd->un.bls_rsp, FC_BA_RJT_UNABLE);\n\t}\n\n\t/* If BA_ABTS failed to abort a partially assembled receive sequence,\n\t * the driver no longer has that exchange, send a BLS_RJT. Override\n\t * the IOCB for a BA_RJT.\n\t */\n\tif (aborted == false) {\n\t\ticmd->un.xseq64.w5.hcsw.Rctl = FC_RCTL_BA_RJT;\n\t\tbf_set(lpfc_vndr_code, &icmd->un.bls_rsp, 0);\n\t\tbf_set(lpfc_rsn_expln, &icmd->un.bls_rsp, FC_BA_RJT_INV_XID);\n\t\tbf_set(lpfc_rsn_code, &icmd->un.bls_rsp, FC_BA_RJT_UNABLE);\n\t}\n\n\tif (fctl & FC_FC_EX_CTX) {\n\t\t/* ABTS sent by responder to CT exchange, construction\n\t\t * of BA_ACC will use OX_ID from ABTS for the XRI_TAG\n\t\t * field and RX_ID from ABTS for RX_ID field.\n\t\t */\n\t\tbf_set(lpfc_abts_orig, &icmd->un.bls_rsp, LPFC_ABTS_UNSOL_RSP);\n\t} else {\n\t\t/* ABTS sent by initiator to CT exchange, construction\n\t\t * of BA_ACC will need to allocate a new XRI as for the\n\t\t * XRI_TAG field.\n\t\t */\n\t\tbf_set(lpfc_abts_orig, &icmd->un.bls_rsp, LPFC_ABTS_UNSOL_INT);\n\t}\n\tbf_set(lpfc_abts_rxid, &icmd->un.bls_rsp, rxid);\n\tbf_set(lpfc_abts_oxid, &icmd->un.bls_rsp, oxid);\n\n\t/* Xmit CT abts response on exchange <xid> */\n\tlpfc_printf_vlog(vport, KERN_INFO, LOG_ELS,\n\t\t\t \"1200 Send BLS cmd x%x on oxid x%x Data: x%x\\n\",\n\t\t\t icmd->un.xseq64.w5.hcsw.Rctl, oxid, phba->link_state);\n\n\trc = lpfc_sli_issue_iocb(phba, LPFC_ELS_RING, ctiocb, 0);\n\tif (rc == IOCB_ERROR) {\n\t\tlpfc_printf_vlog(vport, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t \"2925 Failed to issue CT ABTS RSP x%x on \"\n\t\t\t\t \"xri x%x, Data x%x\\n\",\n\t\t\t\t icmd->un.xseq64.w5.hcsw.Rctl, oxid,\n\t\t\t\t phba->link_state);\n\t\tlpfc_nlp_put(ndlp);\n\t\tctiocb->context1 = NULL;\n\t\tlpfc_sli_release_iocbq(phba, ctiocb);\n\t}\n}\n\n/**\n * lpfc_sli4_handle_unsol_abort - Handle sli-4 unsolicited abort event\n * @vport: Pointer to the vport on which this sequence was received\n * @dmabuf: pointer to a dmabuf that describes the FC sequence\n *\n * This function handles an SLI-4 unsolicited abort event. If the unsolicited\n * receive sequence is only partially assembed by the driver, it shall abort\n * the partially assembled frames for the sequence. Otherwise, if the\n * unsolicited receive sequence has been completely assembled and passed to\n * the Upper Layer Protocol (ULP), it then mark the per oxid status for the\n * unsolicited sequence has been aborted. After that, it will issue a basic\n * accept to accept the abort.\n **/\nstatic void\nlpfc_sli4_handle_unsol_abort(struct lpfc_vport *vport,\n\t\t\t     struct hbq_dmabuf *dmabuf)\n{\n\tstruct lpfc_hba *phba = vport->phba;\n\tstruct fc_frame_header fc_hdr;\n\tuint32_t fctl;\n\tbool aborted;\n\n\t/* Make a copy of fc_hdr before the dmabuf being released */\n\tmemcpy(&fc_hdr, dmabuf->hbuf.virt, sizeof(struct fc_frame_header));\n\tfctl = sli4_fctl_from_fc_hdr(&fc_hdr);\n\n\tif (fctl & FC_FC_EX_CTX) {\n\t\t/* ABTS by responder to exchange, no cleanup needed */\n\t\taborted = true;\n\t} else {\n\t\t/* ABTS by initiator to exchange, need to do cleanup */\n\t\taborted = lpfc_sli4_abort_partial_seq(vport, dmabuf);\n\t\tif (aborted == false)\n\t\t\taborted = lpfc_sli4_abort_ulp_seq(vport, dmabuf);\n\t}\n\tlpfc_in_buf_free(phba, &dmabuf->dbuf);\n\n\tif (phba->nvmet_support) {\n\t\tlpfc_nvmet_rcv_unsol_abort(vport, &fc_hdr);\n\t\treturn;\n\t}\n\n\t/* Respond with BA_ACC or BA_RJT accordingly */\n\tlpfc_sli4_seq_abort_rsp(vport, &fc_hdr, aborted);\n}\n\n/**\n * lpfc_seq_complete - Indicates if a sequence is complete\n * @dmabuf: pointer to a dmabuf that describes the FC sequence\n *\n * This function checks the sequence, starting with the frame described by\n * @dmabuf, to see if all the frames associated with this sequence are present.\n * the frames associated with this sequence are linked to the @dmabuf using the\n * dbuf list. This function looks for two major things. 1) That the first frame\n * has a sequence count of zero. 2) There is a frame with last frame of sequence\n * set. 3) That there are no holes in the sequence count. The function will\n * return 1 when the sequence is complete, otherwise it will return 0.\n **/\nstatic int\nlpfc_seq_complete(struct hbq_dmabuf *dmabuf)\n{\n\tstruct fc_frame_header *hdr;\n\tstruct lpfc_dmabuf *d_buf;\n\tstruct hbq_dmabuf *seq_dmabuf;\n\tuint32_t fctl;\n\tint seq_count = 0;\n\n\thdr = (struct fc_frame_header *)dmabuf->hbuf.virt;\n\t/* make sure first fame of sequence has a sequence count of zero */\n\tif (hdr->fh_seq_cnt != seq_count)\n\t\treturn 0;\n\tfctl = (hdr->fh_f_ctl[0] << 16 |\n\t\thdr->fh_f_ctl[1] << 8 |\n\t\thdr->fh_f_ctl[2]);\n\t/* If last frame of sequence we can return success. */\n\tif (fctl & FC_FC_END_SEQ)\n\t\treturn 1;\n\tlist_for_each_entry(d_buf, &dmabuf->dbuf.list, list) {\n\t\tseq_dmabuf = container_of(d_buf, struct hbq_dmabuf, dbuf);\n\t\thdr = (struct fc_frame_header *)seq_dmabuf->hbuf.virt;\n\t\t/* If there is a hole in the sequence count then fail. */\n\t\tif (++seq_count != be16_to_cpu(hdr->fh_seq_cnt))\n\t\t\treturn 0;\n\t\tfctl = (hdr->fh_f_ctl[0] << 16 |\n\t\t\thdr->fh_f_ctl[1] << 8 |\n\t\t\thdr->fh_f_ctl[2]);\n\t\t/* If last frame of sequence we can return success. */\n\t\tif (fctl & FC_FC_END_SEQ)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n/**\n * lpfc_prep_seq - Prep sequence for ULP processing\n * @vport: Pointer to the vport on which this sequence was received\n * @seq_dmabuf: pointer to a dmabuf that describes the FC sequence\n *\n * This function takes a sequence, described by a list of frames, and creates\n * a list of iocbq structures to describe the sequence. This iocbq list will be\n * used to issue to the generic unsolicited sequence handler. This routine\n * returns a pointer to the first iocbq in the list. If the function is unable\n * to allocate an iocbq then it throw out the received frames that were not\n * able to be described and return a pointer to the first iocbq. If unable to\n * allocate any iocbqs (including the first) this function will return NULL.\n **/\nstatic struct lpfc_iocbq *\nlpfc_prep_seq(struct lpfc_vport *vport, struct hbq_dmabuf *seq_dmabuf)\n{\n\tstruct hbq_dmabuf *hbq_buf;\n\tstruct lpfc_dmabuf *d_buf, *n_buf;\n\tstruct lpfc_iocbq *first_iocbq, *iocbq;\n\tstruct fc_frame_header *fc_hdr;\n\tuint32_t sid;\n\tuint32_t len, tot_len;\n\tstruct ulp_bde64 *pbde;\n\n\tfc_hdr = (struct fc_frame_header *)seq_dmabuf->hbuf.virt;\n\t/* remove from receive buffer list */\n\tlist_del_init(&seq_dmabuf->hbuf.list);\n\tlpfc_update_rcv_time_stamp(vport);\n\t/* get the Remote Port's SID */\n\tsid = sli4_sid_from_fc_hdr(fc_hdr);\n\ttot_len = 0;\n\t/* Get an iocbq struct to fill in. */\n\tfirst_iocbq = lpfc_sli_get_iocbq(vport->phba);\n\tif (first_iocbq) {\n\t\t/* Initialize the first IOCB. */\n\t\tfirst_iocbq->iocb.unsli3.rcvsli3.acc_len = 0;\n\t\tfirst_iocbq->iocb.ulpStatus = IOSTAT_SUCCESS;\n\t\tfirst_iocbq->vport = vport;\n\n\t\t/* Check FC Header to see what TYPE of frame we are rcv'ing */\n\t\tif (sli4_type_from_fc_hdr(fc_hdr) == FC_TYPE_ELS) {\n\t\t\tfirst_iocbq->iocb.ulpCommand = CMD_IOCB_RCV_ELS64_CX;\n\t\t\tfirst_iocbq->iocb.un.rcvels.parmRo =\n\t\t\t\tsli4_did_from_fc_hdr(fc_hdr);\n\t\t\tfirst_iocbq->iocb.ulpPU = PARM_NPIV_DID;\n\t\t} else\n\t\t\tfirst_iocbq->iocb.ulpCommand = CMD_IOCB_RCV_SEQ64_CX;\n\t\tfirst_iocbq->iocb.ulpContext = NO_XRI;\n\t\tfirst_iocbq->iocb.unsli3.rcvsli3.ox_id =\n\t\t\tbe16_to_cpu(fc_hdr->fh_ox_id);\n\t\t/* iocbq is prepped for internal consumption.  Physical vpi. */\n\t\tfirst_iocbq->iocb.unsli3.rcvsli3.vpi =\n\t\t\tvport->phba->vpi_ids[vport->vpi];\n\t\t/* put the first buffer into the first IOCBq */\n\t\ttot_len = bf_get(lpfc_rcqe_length,\n\t\t\t\t       &seq_dmabuf->cq_event.cqe.rcqe_cmpl);\n\n\t\tfirst_iocbq->context2 = &seq_dmabuf->dbuf;\n\t\tfirst_iocbq->context3 = NULL;\n\t\tfirst_iocbq->iocb.ulpBdeCount = 1;\n\t\tif (tot_len > LPFC_DATA_BUF_SIZE)\n\t\t\tfirst_iocbq->iocb.un.cont64[0].tus.f.bdeSize =\n\t\t\t\t\t\t\tLPFC_DATA_BUF_SIZE;\n\t\telse\n\t\t\tfirst_iocbq->iocb.un.cont64[0].tus.f.bdeSize = tot_len;\n\n\t\tfirst_iocbq->iocb.un.rcvels.remoteID = sid;\n\n\t\tfirst_iocbq->iocb.unsli3.rcvsli3.acc_len = tot_len;\n\t}\n\tiocbq = first_iocbq;\n\t/*\n\t * Each IOCBq can have two Buffers assigned, so go through the list\n\t * of buffers for this sequence and save two buffers in each IOCBq\n\t */\n\tlist_for_each_entry_safe(d_buf, n_buf, &seq_dmabuf->dbuf.list, list) {\n\t\tif (!iocbq) {\n\t\t\tlpfc_in_buf_free(vport->phba, d_buf);\n\t\t\tcontinue;\n\t\t}\n\t\tif (!iocbq->context3) {\n\t\t\tiocbq->context3 = d_buf;\n\t\t\tiocbq->iocb.ulpBdeCount++;\n\t\t\t/* We need to get the size out of the right CQE */\n\t\t\thbq_buf = container_of(d_buf, struct hbq_dmabuf, dbuf);\n\t\t\tlen = bf_get(lpfc_rcqe_length,\n\t\t\t\t       &hbq_buf->cq_event.cqe.rcqe_cmpl);\n\t\t\tpbde = (struct ulp_bde64 *)\n\t\t\t\t\t&iocbq->iocb.unsli3.sli3Words[4];\n\t\t\tif (len > LPFC_DATA_BUF_SIZE)\n\t\t\t\tpbde->tus.f.bdeSize = LPFC_DATA_BUF_SIZE;\n\t\t\telse\n\t\t\t\tpbde->tus.f.bdeSize = len;\n\n\t\t\tiocbq->iocb.unsli3.rcvsli3.acc_len += len;\n\t\t\ttot_len += len;\n\t\t} else {\n\t\t\tiocbq = lpfc_sli_get_iocbq(vport->phba);\n\t\t\tif (!iocbq) {\n\t\t\t\tif (first_iocbq) {\n\t\t\t\t\tfirst_iocbq->iocb.ulpStatus =\n\t\t\t\t\t\t\tIOSTAT_FCP_RSP_ERROR;\n\t\t\t\t\tfirst_iocbq->iocb.un.ulpWord[4] =\n\t\t\t\t\t\t\tIOERR_NO_RESOURCES;\n\t\t\t\t}\n\t\t\t\tlpfc_in_buf_free(vport->phba, d_buf);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t/* We need to get the size out of the right CQE */\n\t\t\thbq_buf = container_of(d_buf, struct hbq_dmabuf, dbuf);\n\t\t\tlen = bf_get(lpfc_rcqe_length,\n\t\t\t\t       &hbq_buf->cq_event.cqe.rcqe_cmpl);\n\t\t\tiocbq->context2 = d_buf;\n\t\t\tiocbq->context3 = NULL;\n\t\t\tiocbq->iocb.ulpBdeCount = 1;\n\t\t\tif (len > LPFC_DATA_BUF_SIZE)\n\t\t\t\tiocbq->iocb.un.cont64[0].tus.f.bdeSize =\n\t\t\t\t\t\t\tLPFC_DATA_BUF_SIZE;\n\t\t\telse\n\t\t\t\tiocbq->iocb.un.cont64[0].tus.f.bdeSize = len;\n\n\t\t\ttot_len += len;\n\t\t\tiocbq->iocb.unsli3.rcvsli3.acc_len = tot_len;\n\n\t\t\tiocbq->iocb.un.rcvels.remoteID = sid;\n\t\t\tlist_add_tail(&iocbq->list, &first_iocbq->list);\n\t\t}\n\t}\n\t/* Free the sequence's header buffer */\n\tif (!first_iocbq)\n\t\tlpfc_in_buf_free(vport->phba, &seq_dmabuf->dbuf);\n\n\treturn first_iocbq;\n}\n\nstatic void\nlpfc_sli4_send_seq_to_ulp(struct lpfc_vport *vport,\n\t\t\t  struct hbq_dmabuf *seq_dmabuf)\n{\n\tstruct fc_frame_header *fc_hdr;\n\tstruct lpfc_iocbq *iocbq, *curr_iocb, *next_iocb;\n\tstruct lpfc_hba *phba = vport->phba;\n\n\tfc_hdr = (struct fc_frame_header *)seq_dmabuf->hbuf.virt;\n\tiocbq = lpfc_prep_seq(vport, seq_dmabuf);\n\tif (!iocbq) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2707 Ring %d handler: Failed to allocate \"\n\t\t\t\t\"iocb Rctl x%x Type x%x received\\n\",\n\t\t\t\tLPFC_ELS_RING,\n\t\t\t\tfc_hdr->fh_r_ctl, fc_hdr->fh_type);\n\t\treturn;\n\t}\n\tif (!lpfc_complete_unsol_iocb(phba,\n\t\t\t\t      phba->sli4_hba.els_wq->pring,\n\t\t\t\t      iocbq, fc_hdr->fh_r_ctl,\n\t\t\t\t      fc_hdr->fh_type))\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2540 Ring %d handler: unexpected Rctl \"\n\t\t\t\t\"x%x Type x%x received\\n\",\n\t\t\t\tLPFC_ELS_RING,\n\t\t\t\tfc_hdr->fh_r_ctl, fc_hdr->fh_type);\n\n\t/* Free iocb created in lpfc_prep_seq */\n\tlist_for_each_entry_safe(curr_iocb, next_iocb,\n\t\t&iocbq->list, list) {\n\t\tlist_del_init(&curr_iocb->list);\n\t\tlpfc_sli_release_iocbq(phba, curr_iocb);\n\t}\n\tlpfc_sli_release_iocbq(phba, iocbq);\n}\n\nstatic void\nlpfc_sli4_mds_loopback_cmpl(struct lpfc_hba *phba, struct lpfc_iocbq *cmdiocb,\n\t\t\t    struct lpfc_iocbq *rspiocb)\n{\n\tstruct lpfc_dmabuf *pcmd = cmdiocb->context2;\n\n\tif (pcmd && pcmd->virt)\n\t\tdma_pool_free(phba->lpfc_drb_pool, pcmd->virt, pcmd->phys);\n\tkfree(pcmd);\n\tlpfc_sli_release_iocbq(phba, cmdiocb);\n\tlpfc_drain_txq(phba);\n}\n\nstatic void\nlpfc_sli4_handle_mds_loopback(struct lpfc_vport *vport,\n\t\t\t      struct hbq_dmabuf *dmabuf)\n{\n\tstruct fc_frame_header *fc_hdr;\n\tstruct lpfc_hba *phba = vport->phba;\n\tstruct lpfc_iocbq *iocbq = NULL;\n\tunion  lpfc_wqe *wqe;\n\tstruct lpfc_dmabuf *pcmd = NULL;\n\tuint32_t frame_len;\n\tint rc;\n\tunsigned long iflags;\n\n\tfc_hdr = (struct fc_frame_header *)dmabuf->hbuf.virt;\n\tframe_len = bf_get(lpfc_rcqe_length, &dmabuf->cq_event.cqe.rcqe_cmpl);\n\n\t/* Send the received frame back */\n\tiocbq = lpfc_sli_get_iocbq(phba);\n\tif (!iocbq) {\n\t\t/* Queue cq event and wakeup worker thread to process it */\n\t\tspin_lock_irqsave(&phba->hbalock, iflags);\n\t\tlist_add_tail(&dmabuf->cq_event.list,\n\t\t\t      &phba->sli4_hba.sp_queue_event);\n\t\tphba->hba_flag |= HBA_SP_QUEUE_EVT;\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflags);\n\t\tlpfc_worker_wake_up(phba);\n\t\treturn;\n\t}\n\n\t/* Allocate buffer for command payload */\n\tpcmd = kmalloc(sizeof(struct lpfc_dmabuf), GFP_KERNEL);\n\tif (pcmd)\n\t\tpcmd->virt = dma_pool_alloc(phba->lpfc_drb_pool, GFP_KERNEL,\n\t\t\t\t\t    &pcmd->phys);\n\tif (!pcmd || !pcmd->virt)\n\t\tgoto exit;\n\n\tINIT_LIST_HEAD(&pcmd->list);\n\n\t/* copyin the payload */\n\tmemcpy(pcmd->virt, dmabuf->dbuf.virt, frame_len);\n\n\t/* fill in BDE's for command */\n\tiocbq->iocb.un.xseq64.bdl.addrHigh = putPaddrHigh(pcmd->phys);\n\tiocbq->iocb.un.xseq64.bdl.addrLow = putPaddrLow(pcmd->phys);\n\tiocbq->iocb.un.xseq64.bdl.bdeFlags = BUFF_TYPE_BDE_64;\n\tiocbq->iocb.un.xseq64.bdl.bdeSize = frame_len;\n\n\tiocbq->context2 = pcmd;\n\tiocbq->vport = vport;\n\tiocbq->iocb_flag &= ~LPFC_FIP_ELS_ID_MASK;\n\tiocbq->iocb_flag |= LPFC_USE_FCPWQIDX;\n\n\t/*\n\t * Setup rest of the iocb as though it were a WQE\n\t * Build the SEND_FRAME WQE\n\t */\n\twqe = (union lpfc_wqe *)&iocbq->iocb;\n\n\twqe->send_frame.frame_len = frame_len;\n\twqe->send_frame.fc_hdr_wd0 = be32_to_cpu(*((uint32_t *)fc_hdr));\n\twqe->send_frame.fc_hdr_wd1 = be32_to_cpu(*((uint32_t *)fc_hdr + 1));\n\twqe->send_frame.fc_hdr_wd2 = be32_to_cpu(*((uint32_t *)fc_hdr + 2));\n\twqe->send_frame.fc_hdr_wd3 = be32_to_cpu(*((uint32_t *)fc_hdr + 3));\n\twqe->send_frame.fc_hdr_wd4 = be32_to_cpu(*((uint32_t *)fc_hdr + 4));\n\twqe->send_frame.fc_hdr_wd5 = be32_to_cpu(*((uint32_t *)fc_hdr + 5));\n\n\tiocbq->iocb.ulpCommand = CMD_SEND_FRAME;\n\tiocbq->iocb.ulpLe = 1;\n\tiocbq->iocb_cmpl = lpfc_sli4_mds_loopback_cmpl;\n\trc = lpfc_sli_issue_iocb(phba, LPFC_ELS_RING, iocbq, 0);\n\tif (rc == IOCB_ERROR)\n\t\tgoto exit;\n\n\tlpfc_in_buf_free(phba, &dmabuf->dbuf);\n\treturn;\n\nexit:\n\tlpfc_printf_log(phba, KERN_WARNING, LOG_SLI,\n\t\t\t\"2023 Unable to process MDS loopback frame\\n\");\n\tif (pcmd && pcmd->virt)\n\t\tdma_pool_free(phba->lpfc_drb_pool, pcmd->virt, pcmd->phys);\n\tkfree(pcmd);\n\tif (iocbq)\n\t\tlpfc_sli_release_iocbq(phba, iocbq);\n\tlpfc_in_buf_free(phba, &dmabuf->dbuf);\n}\n\n/**\n * lpfc_sli4_handle_received_buffer - Handle received buffers from firmware\n * @phba: Pointer to HBA context object.\n * @dmabuf: Pointer to a dmabuf that describes the FC sequence.\n *\n * This function is called with no lock held. This function processes all\n * the received buffers and gives it to upper layers when a received buffer\n * indicates that it is the final frame in the sequence. The interrupt\n * service routine processes received buffers at interrupt contexts.\n * Worker thread calls lpfc_sli4_handle_received_buffer, which will call the\n * appropriate receive function when the final frame in a sequence is received.\n **/\nvoid\nlpfc_sli4_handle_received_buffer(struct lpfc_hba *phba,\n\t\t\t\t struct hbq_dmabuf *dmabuf)\n{\n\tstruct hbq_dmabuf *seq_dmabuf;\n\tstruct fc_frame_header *fc_hdr;\n\tstruct lpfc_vport *vport;\n\tuint32_t fcfi;\n\tuint32_t did;\n\n\t/* Process each received buffer */\n\tfc_hdr = (struct fc_frame_header *)dmabuf->hbuf.virt;\n\n\tif (fc_hdr->fh_r_ctl == FC_RCTL_MDS_DIAGS ||\n\t    fc_hdr->fh_r_ctl == FC_RCTL_DD_UNSOL_DATA) {\n\t\tvport = phba->pport;\n\t\t/* Handle MDS Loopback frames */\n\t\tif  (!(phba->pport->load_flag & FC_UNLOADING))\n\t\t\tlpfc_sli4_handle_mds_loopback(vport, dmabuf);\n\t\telse\n\t\t\tlpfc_in_buf_free(phba, &dmabuf->dbuf);\n\t\treturn;\n\t}\n\n\t/* check to see if this a valid type of frame */\n\tif (lpfc_fc_frame_check(phba, fc_hdr)) {\n\t\tlpfc_in_buf_free(phba, &dmabuf->dbuf);\n\t\treturn;\n\t}\n\n\tif ((bf_get(lpfc_cqe_code,\n\t\t    &dmabuf->cq_event.cqe.rcqe_cmpl) == CQE_CODE_RECEIVE_V1))\n\t\tfcfi = bf_get(lpfc_rcqe_fcf_id_v1,\n\t\t\t      &dmabuf->cq_event.cqe.rcqe_cmpl);\n\telse\n\t\tfcfi = bf_get(lpfc_rcqe_fcf_id,\n\t\t\t      &dmabuf->cq_event.cqe.rcqe_cmpl);\n\n\tif (fc_hdr->fh_r_ctl == 0xF4 && fc_hdr->fh_type == 0xFF) {\n\t\tvport = phba->pport;\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\t\"2023 MDS Loopback %d bytes\\n\",\n\t\t\t\tbf_get(lpfc_rcqe_length,\n\t\t\t\t       &dmabuf->cq_event.cqe.rcqe_cmpl));\n\t\t/* Handle MDS Loopback frames */\n\t\tlpfc_sli4_handle_mds_loopback(vport, dmabuf);\n\t\treturn;\n\t}\n\n\t/* d_id this frame is directed to */\n\tdid = sli4_did_from_fc_hdr(fc_hdr);\n\n\tvport = lpfc_fc_frame_to_vport(phba, fc_hdr, fcfi, did);\n\tif (!vport) {\n\t\t/* throw out the frame */\n\t\tlpfc_in_buf_free(phba, &dmabuf->dbuf);\n\t\treturn;\n\t}\n\n\t/* vport is registered unless we rcv a FLOGI directed to Fabric_DID */\n\tif (!(vport->vpi_state & LPFC_VPI_REGISTERED) &&\n\t\t(did != Fabric_DID)) {\n\t\t/*\n\t\t * Throw out the frame if we are not pt2pt.\n\t\t * The pt2pt protocol allows for discovery frames\n\t\t * to be received without a registered VPI.\n\t\t */\n\t\tif (!(vport->fc_flag & FC_PT2PT) ||\n\t\t\t(phba->link_state == LPFC_HBA_READY)) {\n\t\t\tlpfc_in_buf_free(phba, &dmabuf->dbuf);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/* Handle the basic abort sequence (BA_ABTS) event */\n\tif (fc_hdr->fh_r_ctl == FC_RCTL_BA_ABTS) {\n\t\tlpfc_sli4_handle_unsol_abort(vport, dmabuf);\n\t\treturn;\n\t}\n\n\t/* Link this frame */\n\tseq_dmabuf = lpfc_fc_frame_add(vport, dmabuf);\n\tif (!seq_dmabuf) {\n\t\t/* unable to add frame to vport - throw it out */\n\t\tlpfc_in_buf_free(phba, &dmabuf->dbuf);\n\t\treturn;\n\t}\n\t/* If not last frame in sequence continue processing frames. */\n\tif (!lpfc_seq_complete(seq_dmabuf))\n\t\treturn;\n\n\t/* Send the complete sequence to the upper layer protocol */\n\tlpfc_sli4_send_seq_to_ulp(vport, seq_dmabuf);\n}\n\n/**\n * lpfc_sli4_post_all_rpi_hdrs - Post the rpi header memory region to the port\n * @phba: pointer to lpfc hba data structure.\n *\n * This routine is invoked to post rpi header templates to the\n * HBA consistent with the SLI-4 interface spec.  This routine\n * posts a SLI4_PAGE_SIZE memory region to the port to hold up to\n * SLI4_PAGE_SIZE modulo 64 rpi context headers.\n *\n * This routine does not require any locks.  It's usage is expected\n * to be driver load or reset recovery when the driver is\n * sequential.\n *\n * Return codes\n * \t0 - successful\n *      -EIO - The mailbox failed to complete successfully.\n * \tWhen this error occurs, the driver is not guaranteed\n *\tto have any rpi regions posted to the device and\n *\tmust either attempt to repost the regions or take a\n *\tfatal error.\n **/\nint\nlpfc_sli4_post_all_rpi_hdrs(struct lpfc_hba *phba)\n{\n\tstruct lpfc_rpi_hdr *rpi_page;\n\tuint32_t rc = 0;\n\tuint16_t lrpi = 0;\n\n\t/* SLI4 ports that support extents do not require RPI headers. */\n\tif (!phba->sli4_hba.rpi_hdrs_in_use)\n\t\tgoto exit;\n\tif (phba->sli4_hba.extents_in_use)\n\t\treturn -EIO;\n\n\tlist_for_each_entry(rpi_page, &phba->sli4_hba.lpfc_rpi_hdr_list, list) {\n\t\t/*\n\t\t * Assign the rpi headers a physical rpi only if the driver\n\t\t * has not initialized those resources.  A port reset only\n\t\t * needs the headers posted.\n\t\t */\n\t\tif (bf_get(lpfc_rpi_rsrc_rdy, &phba->sli4_hba.sli4_flags) !=\n\t\t    LPFC_RPI_RSRC_RDY)\n\t\t\trpi_page->start_rpi = phba->sli4_hba.rpi_ids[lrpi];\n\n\t\trc = lpfc_sli4_post_rpi_hdr(phba, rpi_page);\n\t\tif (rc != MBX_SUCCESS) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"2008 Error %d posting all rpi \"\n\t\t\t\t\t\"headers\\n\", rc);\n\t\t\trc = -EIO;\n\t\t\tbreak;\n\t\t}\n\t}\n\n exit:\n\tbf_set(lpfc_rpi_rsrc_rdy, &phba->sli4_hba.sli4_flags,\n\t       LPFC_RPI_RSRC_RDY);\n\treturn rc;\n}\n\n/**\n * lpfc_sli4_post_rpi_hdr - Post an rpi header memory region to the port\n * @phba: pointer to lpfc hba data structure.\n * @rpi_page:  pointer to the rpi memory region.\n *\n * This routine is invoked to post a single rpi header to the\n * HBA consistent with the SLI-4 interface spec.  This memory region\n * maps up to 64 rpi context regions.\n *\n * Return codes\n * \t0 - successful\n * \t-ENOMEM - No available memory\n *      -EIO - The mailbox failed to complete successfully.\n **/\nint\nlpfc_sli4_post_rpi_hdr(struct lpfc_hba *phba, struct lpfc_rpi_hdr *rpi_page)\n{\n\tLPFC_MBOXQ_t *mboxq;\n\tstruct lpfc_mbx_post_hdr_tmpl *hdr_tmpl;\n\tuint32_t rc = 0;\n\tuint32_t shdr_status, shdr_add_status;\n\tunion lpfc_sli4_cfg_shdr *shdr;\n\n\t/* SLI4 ports that support extents do not require RPI headers. */\n\tif (!phba->sli4_hba.rpi_hdrs_in_use)\n\t\treturn rc;\n\tif (phba->sli4_hba.extents_in_use)\n\t\treturn -EIO;\n\n\t/* The port is notified of the header region via a mailbox command. */\n\tmboxq = (LPFC_MBOXQ_t *) mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mboxq) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2001 Unable to allocate memory for issuing \"\n\t\t\t\t\"SLI_CONFIG_SPECIAL mailbox command\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\t/* Post all rpi memory regions to the port. */\n\thdr_tmpl = &mboxq->u.mqe.un.hdr_tmpl;\n\tlpfc_sli4_config(phba, mboxq, LPFC_MBOX_SUBSYSTEM_FCOE,\n\t\t\t LPFC_MBOX_OPCODE_FCOE_POST_HDR_TEMPLATE,\n\t\t\t sizeof(struct lpfc_mbx_post_hdr_tmpl) -\n\t\t\t sizeof(struct lpfc_sli4_cfg_mhdr),\n\t\t\t LPFC_SLI4_MBX_EMBED);\n\n\n\t/* Post the physical rpi to the port for this rpi header. */\n\tbf_set(lpfc_mbx_post_hdr_tmpl_rpi_offset, hdr_tmpl,\n\t       rpi_page->start_rpi);\n\tbf_set(lpfc_mbx_post_hdr_tmpl_page_cnt,\n\t       hdr_tmpl, rpi_page->page_count);\n\n\thdr_tmpl->rpi_paddr_lo = putPaddrLow(rpi_page->dmabuf->phys);\n\thdr_tmpl->rpi_paddr_hi = putPaddrHigh(rpi_page->dmabuf->phys);\n\trc = lpfc_sli_issue_mbox(phba, mboxq, MBX_POLL);\n\tshdr = (union lpfc_sli4_cfg_shdr *) &hdr_tmpl->header.cfg_shdr;\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\tif (rc != MBX_TIMEOUT)\n\t\tmempool_free(mboxq, phba->mbox_mem_pool);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2514 POST_RPI_HDR mailbox failed with \"\n\t\t\t\t\"status x%x add_status x%x, mbx status x%x\\n\",\n\t\t\t\tshdr_status, shdr_add_status, rc);\n\t\trc = -ENXIO;\n\t} else {\n\t\t/*\n\t\t * The next_rpi stores the next logical module-64 rpi value used\n\t\t * to post physical rpis in subsequent rpi postings.\n\t\t */\n\t\tspin_lock_irq(&phba->hbalock);\n\t\tphba->sli4_hba.next_rpi = rpi_page->next_rpi;\n\t\tspin_unlock_irq(&phba->hbalock);\n\t}\n\treturn rc;\n}\n\n/**\n * lpfc_sli4_alloc_rpi - Get an available rpi in the device's range\n * @phba: pointer to lpfc hba data structure.\n *\n * This routine is invoked to post rpi header templates to the\n * HBA consistent with the SLI-4 interface spec.  This routine\n * posts a SLI4_PAGE_SIZE memory region to the port to hold up to\n * SLI4_PAGE_SIZE modulo 64 rpi context headers.\n *\n * Returns\n * \tA nonzero rpi defined as rpi_base <= rpi < max_rpi if successful\n * \tLPFC_RPI_ALLOC_ERROR if no rpis are available.\n **/\nint\nlpfc_sli4_alloc_rpi(struct lpfc_hba *phba)\n{\n\tunsigned long rpi;\n\tuint16_t max_rpi, rpi_limit;\n\tuint16_t rpi_remaining, lrpi = 0;\n\tstruct lpfc_rpi_hdr *rpi_hdr;\n\tunsigned long iflag;\n\n\t/*\n\t * Fetch the next logical rpi.  Because this index is logical,\n\t * the  driver starts at 0 each time.\n\t */\n\tspin_lock_irqsave(&phba->hbalock, iflag);\n\tmax_rpi = phba->sli4_hba.max_cfg_param.max_rpi;\n\trpi_limit = phba->sli4_hba.next_rpi;\n\n\trpi = find_next_zero_bit(phba->sli4_hba.rpi_bmask, rpi_limit, 0);\n\tif (rpi >= rpi_limit)\n\t\trpi = LPFC_RPI_ALLOC_ERROR;\n\telse {\n\t\tset_bit(rpi, phba->sli4_hba.rpi_bmask);\n\t\tphba->sli4_hba.max_cfg_param.rpi_used++;\n\t\tphba->sli4_hba.rpi_count++;\n\t}\n\tlpfc_printf_log(phba, KERN_INFO,\n\t\t\tLOG_NODE | LOG_DISCOVERY,\n\t\t\t\"0001 Allocated rpi:x%x max:x%x lim:x%x\\n\",\n\t\t\t(int) rpi, max_rpi, rpi_limit);\n\n\t/*\n\t * Don't try to allocate more rpi header regions if the device limit\n\t * has been exhausted.\n\t */\n\tif ((rpi == LPFC_RPI_ALLOC_ERROR) &&\n\t    (phba->sli4_hba.rpi_count >= max_rpi)) {\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\treturn rpi;\n\t}\n\n\t/*\n\t * RPI header postings are not required for SLI4 ports capable of\n\t * extents.\n\t */\n\tif (!phba->sli4_hba.rpi_hdrs_in_use) {\n\t\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\t\treturn rpi;\n\t}\n\n\t/*\n\t * If the driver is running low on rpi resources, allocate another\n\t * page now.  Note that the next_rpi value is used because\n\t * it represents how many are actually in use whereas max_rpi notes\n\t * how many are supported max by the device.\n\t */\n\trpi_remaining = phba->sli4_hba.next_rpi - phba->sli4_hba.rpi_count;\n\tspin_unlock_irqrestore(&phba->hbalock, iflag);\n\tif (rpi_remaining < LPFC_RPI_LOW_WATER_MARK) {\n\t\trpi_hdr = lpfc_sli4_create_rpi_hdr(phba);\n\t\tif (!rpi_hdr) {\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"2002 Error Could not grow rpi \"\n\t\t\t\t\t\"count\\n\");\n\t\t} else {\n\t\t\tlrpi = rpi_hdr->start_rpi;\n\t\t\trpi_hdr->start_rpi = phba->sli4_hba.rpi_ids[lrpi];\n\t\t\tlpfc_sli4_post_rpi_hdr(phba, rpi_hdr);\n\t\t}\n\t}\n\n\treturn rpi;\n}\n\n/**\n * lpfc_sli4_free_rpi - Release an rpi for reuse.\n * @phba: pointer to lpfc hba data structure.\n * @rpi: rpi to free\n *\n * This routine is invoked to release an rpi to the pool of\n * available rpis maintained by the driver.\n **/\nstatic void\n__lpfc_sli4_free_rpi(struct lpfc_hba *phba, int rpi)\n{\n\t/*\n\t * if the rpi value indicates a prior unreg has already\n\t * been done, skip the unreg.\n\t */\n\tif (rpi == LPFC_RPI_ALLOC_ERROR)\n\t\treturn;\n\n\tif (test_and_clear_bit(rpi, phba->sli4_hba.rpi_bmask)) {\n\t\tphba->sli4_hba.rpi_count--;\n\t\tphba->sli4_hba.max_cfg_param.rpi_used--;\n\t} else {\n\t\tlpfc_printf_log(phba, KERN_INFO,\n\t\t\t\tLOG_NODE | LOG_DISCOVERY,\n\t\t\t\t\"2016 rpi %x not inuse\\n\",\n\t\t\t\trpi);\n\t}\n}\n\n/**\n * lpfc_sli4_free_rpi - Release an rpi for reuse.\n * @phba: pointer to lpfc hba data structure.\n * @rpi: rpi to free\n *\n * This routine is invoked to release an rpi to the pool of\n * available rpis maintained by the driver.\n **/\nvoid\nlpfc_sli4_free_rpi(struct lpfc_hba *phba, int rpi)\n{\n\tspin_lock_irq(&phba->hbalock);\n\t__lpfc_sli4_free_rpi(phba, rpi);\n\tspin_unlock_irq(&phba->hbalock);\n}\n\n/**\n * lpfc_sli4_remove_rpis - Remove the rpi bitmask region\n * @phba: pointer to lpfc hba data structure.\n *\n * This routine is invoked to remove the memory region that\n * provided rpi via a bitmask.\n **/\nvoid\nlpfc_sli4_remove_rpis(struct lpfc_hba *phba)\n{\n\tkfree(phba->sli4_hba.rpi_bmask);\n\tkfree(phba->sli4_hba.rpi_ids);\n\tbf_set(lpfc_rpi_rsrc_rdy, &phba->sli4_hba.sli4_flags, 0);\n}\n\n/**\n * lpfc_sli4_resume_rpi - Remove the rpi bitmask region\n * @ndlp: pointer to lpfc nodelist data structure.\n * @cmpl: completion call-back.\n * @arg: data to load as MBox 'caller buffer information'\n *\n * This routine is invoked to remove the memory region that\n * provided rpi via a bitmask.\n **/\nint\nlpfc_sli4_resume_rpi(struct lpfc_nodelist *ndlp,\n\tvoid (*cmpl)(struct lpfc_hba *, LPFC_MBOXQ_t *), void *arg)\n{\n\tLPFC_MBOXQ_t *mboxq;\n\tstruct lpfc_hba *phba = ndlp->phba;\n\tint rc;\n\n\t/* The port is notified of the header region via a mailbox command. */\n\tmboxq = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mboxq)\n\t\treturn -ENOMEM;\n\n\t/* Post all rpi memory regions to the port. */\n\tlpfc_resume_rpi(mboxq, ndlp);\n\tif (cmpl) {\n\t\tmboxq->mbox_cmpl = cmpl;\n\t\tmboxq->ctx_buf = arg;\n\t\tmboxq->ctx_ndlp = ndlp;\n\t} else\n\t\tmboxq->mbox_cmpl = lpfc_sli_def_mbox_cmpl;\n\tmboxq->vport = ndlp->vport;\n\trc = lpfc_sli_issue_mbox(phba, mboxq, MBX_NOWAIT);\n\tif (rc == MBX_NOT_FINISHED) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2010 Resume RPI Mailbox failed \"\n\t\t\t\t\"status %d, mbxStatus x%x\\n\", rc,\n\t\t\t\tbf_get(lpfc_mqe_status, &mboxq->u.mqe));\n\t\tmempool_free(mboxq, phba->mbox_mem_pool);\n\t\treturn -EIO;\n\t}\n\treturn 0;\n}\n\n/**\n * lpfc_sli4_init_vpi - Initialize a vpi with the port\n * @vport: Pointer to the vport for which the vpi is being initialized\n *\n * This routine is invoked to activate a vpi with the port.\n *\n * Returns:\n *    0 success\n *    -Evalue otherwise\n **/\nint\nlpfc_sli4_init_vpi(struct lpfc_vport *vport)\n{\n\tLPFC_MBOXQ_t *mboxq;\n\tint rc = 0;\n\tint retval = MBX_SUCCESS;\n\tuint32_t mbox_tmo;\n\tstruct lpfc_hba *phba = vport->phba;\n\tmboxq = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mboxq)\n\t\treturn -ENOMEM;\n\tlpfc_init_vpi(phba, mboxq, vport->vpi);\n\tmbox_tmo = lpfc_mbox_tmo_val(phba, mboxq);\n\trc = lpfc_sli_issue_mbox_wait(phba, mboxq, mbox_tmo);\n\tif (rc != MBX_SUCCESS) {\n\t\tlpfc_printf_vlog(vport, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2022 INIT VPI Mailbox failed \"\n\t\t\t\t\"status %d, mbxStatus x%x\\n\", rc,\n\t\t\t\tbf_get(lpfc_mqe_status, &mboxq->u.mqe));\n\t\tretval = -EIO;\n\t}\n\tif (rc != MBX_TIMEOUT)\n\t\tmempool_free(mboxq, vport->phba->mbox_mem_pool);\n\n\treturn retval;\n}\n\n/**\n * lpfc_mbx_cmpl_add_fcf_record - add fcf mbox completion handler.\n * @phba: pointer to lpfc hba data structure.\n * @mboxq: Pointer to mailbox object.\n *\n * This routine is invoked to manually add a single FCF record. The caller\n * must pass a completely initialized FCF_Record.  This routine takes\n * care of the nonembedded mailbox operations.\n **/\nstatic void\nlpfc_mbx_cmpl_add_fcf_record(struct lpfc_hba *phba, LPFC_MBOXQ_t *mboxq)\n{\n\tvoid *virt_addr;\n\tunion lpfc_sli4_cfg_shdr *shdr;\n\tuint32_t shdr_status, shdr_add_status;\n\n\tvirt_addr = mboxq->sge_array->addr[0];\n\t/* The IOCTL status is embedded in the mailbox subheader. */\n\tshdr = (union lpfc_sli4_cfg_shdr *) virt_addr;\n\tshdr_status = bf_get(lpfc_mbox_hdr_status, &shdr->response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status, &shdr->response);\n\n\tif ((shdr_status || shdr_add_status) &&\n\t\t(shdr_status != STATUS_FCF_IN_USE))\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"2558 ADD_FCF_RECORD mailbox failed with \"\n\t\t\t\"status x%x add_status x%x\\n\",\n\t\t\tshdr_status, shdr_add_status);\n\n\tlpfc_sli4_mbox_cmd_free(phba, mboxq);\n}\n\n/**\n * lpfc_sli4_add_fcf_record - Manually add an FCF Record.\n * @phba: pointer to lpfc hba data structure.\n * @fcf_record:  pointer to the initialized fcf record to add.\n *\n * This routine is invoked to manually add a single FCF record. The caller\n * must pass a completely initialized FCF_Record.  This routine takes\n * care of the nonembedded mailbox operations.\n **/\nint\nlpfc_sli4_add_fcf_record(struct lpfc_hba *phba, struct fcf_record *fcf_record)\n{\n\tint rc = 0;\n\tLPFC_MBOXQ_t *mboxq;\n\tuint8_t *bytep;\n\tvoid *virt_addr;\n\tstruct lpfc_mbx_sge sge;\n\tuint32_t alloc_len, req_len;\n\tuint32_t fcfindex;\n\n\tmboxq = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mboxq) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"2009 Failed to allocate mbox for ADD_FCF cmd\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\treq_len = sizeof(struct fcf_record) + sizeof(union lpfc_sli4_cfg_shdr) +\n\t\t  sizeof(uint32_t);\n\n\t/* Allocate DMA memory and set up the non-embedded mailbox command */\n\talloc_len = lpfc_sli4_config(phba, mboxq, LPFC_MBOX_SUBSYSTEM_FCOE,\n\t\t\t\t     LPFC_MBOX_OPCODE_FCOE_ADD_FCF,\n\t\t\t\t     req_len, LPFC_SLI4_MBX_NEMBED);\n\tif (alloc_len < req_len) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"2523 Allocated DMA memory size (x%x) is \"\n\t\t\t\"less than the requested DMA memory \"\n\t\t\t\"size (x%x)\\n\", alloc_len, req_len);\n\t\tlpfc_sli4_mbox_cmd_free(phba, mboxq);\n\t\treturn -ENOMEM;\n\t}\n\n\t/*\n\t * Get the first SGE entry from the non-embedded DMA memory.  This\n\t * routine only uses a single SGE.\n\t */\n\tlpfc_sli4_mbx_sge_get(mboxq, 0, &sge);\n\tvirt_addr = mboxq->sge_array->addr[0];\n\t/*\n\t * Configure the FCF record for FCFI 0.  This is the driver's\n\t * hardcoded default and gets used in nonFIP mode.\n\t */\n\tfcfindex = bf_get(lpfc_fcf_record_fcf_index, fcf_record);\n\tbytep = virt_addr + sizeof(union lpfc_sli4_cfg_shdr);\n\tlpfc_sli_pcimem_bcopy(&fcfindex, bytep, sizeof(uint32_t));\n\n\t/*\n\t * Copy the fcf_index and the FCF Record Data. The data starts after\n\t * the FCoE header plus word10. The data copy needs to be endian\n\t * correct.\n\t */\n\tbytep += sizeof(uint32_t);\n\tlpfc_sli_pcimem_bcopy(fcf_record, bytep, sizeof(struct fcf_record));\n\tmboxq->vport = phba->pport;\n\tmboxq->mbox_cmpl = lpfc_mbx_cmpl_add_fcf_record;\n\trc = lpfc_sli_issue_mbox(phba, mboxq, MBX_NOWAIT);\n\tif (rc == MBX_NOT_FINISHED) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"2515 ADD_FCF_RECORD mailbox failed with \"\n\t\t\t\"status 0x%x\\n\", rc);\n\t\tlpfc_sli4_mbox_cmd_free(phba, mboxq);\n\t\trc = -EIO;\n\t} else\n\t\trc = 0;\n\n\treturn rc;\n}\n\n/**\n * lpfc_sli4_build_dflt_fcf_record - Build the driver's default FCF Record.\n * @phba: pointer to lpfc hba data structure.\n * @fcf_record:  pointer to the fcf record to write the default data.\n * @fcf_index: FCF table entry index.\n *\n * This routine is invoked to build the driver's default FCF record.  The\n * values used are hardcoded.  This routine handles memory initialization.\n *\n **/\nvoid\nlpfc_sli4_build_dflt_fcf_record(struct lpfc_hba *phba,\n\t\t\t\tstruct fcf_record *fcf_record,\n\t\t\t\tuint16_t fcf_index)\n{\n\tmemset(fcf_record, 0, sizeof(struct fcf_record));\n\tfcf_record->max_rcv_size = LPFC_FCOE_MAX_RCV_SIZE;\n\tfcf_record->fka_adv_period = LPFC_FCOE_FKA_ADV_PER;\n\tfcf_record->fip_priority = LPFC_FCOE_FIP_PRIORITY;\n\tbf_set(lpfc_fcf_record_mac_0, fcf_record, phba->fc_map[0]);\n\tbf_set(lpfc_fcf_record_mac_1, fcf_record, phba->fc_map[1]);\n\tbf_set(lpfc_fcf_record_mac_2, fcf_record, phba->fc_map[2]);\n\tbf_set(lpfc_fcf_record_mac_3, fcf_record, LPFC_FCOE_FCF_MAC3);\n\tbf_set(lpfc_fcf_record_mac_4, fcf_record, LPFC_FCOE_FCF_MAC4);\n\tbf_set(lpfc_fcf_record_mac_5, fcf_record, LPFC_FCOE_FCF_MAC5);\n\tbf_set(lpfc_fcf_record_fc_map_0, fcf_record, phba->fc_map[0]);\n\tbf_set(lpfc_fcf_record_fc_map_1, fcf_record, phba->fc_map[1]);\n\tbf_set(lpfc_fcf_record_fc_map_2, fcf_record, phba->fc_map[2]);\n\tbf_set(lpfc_fcf_record_fcf_valid, fcf_record, 1);\n\tbf_set(lpfc_fcf_record_fcf_avail, fcf_record, 1);\n\tbf_set(lpfc_fcf_record_fcf_index, fcf_record, fcf_index);\n\tbf_set(lpfc_fcf_record_mac_addr_prov, fcf_record,\n\t\tLPFC_FCF_FPMA | LPFC_FCF_SPMA);\n\t/* Set the VLAN bit map */\n\tif (phba->valid_vlan) {\n\t\tfcf_record->vlan_bitmap[phba->vlan_id / 8]\n\t\t\t= 1 << (phba->vlan_id % 8);\n\t}\n}\n\n/**\n * lpfc_sli4_fcf_scan_read_fcf_rec - Read hba fcf record for fcf scan.\n * @phba: pointer to lpfc hba data structure.\n * @fcf_index: FCF table entry offset.\n *\n * This routine is invoked to scan the entire FCF table by reading FCF\n * record and processing it one at a time starting from the @fcf_index\n * for initial FCF discovery or fast FCF failover rediscovery.\n *\n * Return 0 if the mailbox command is submitted successfully, none 0\n * otherwise.\n **/\nint\nlpfc_sli4_fcf_scan_read_fcf_rec(struct lpfc_hba *phba, uint16_t fcf_index)\n{\n\tint rc = 0, error;\n\tLPFC_MBOXQ_t *mboxq;\n\n\tphba->fcoe_eventtag_at_fcf_scan = phba->fcoe_eventtag;\n\tphba->fcoe_cvl_eventtag_attn = phba->fcoe_cvl_eventtag;\n\tmboxq = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mboxq) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2000 Failed to allocate mbox for \"\n\t\t\t\t\"READ_FCF cmd\\n\");\n\t\terror = -ENOMEM;\n\t\tgoto fail_fcf_scan;\n\t}\n\t/* Construct the read FCF record mailbox command */\n\trc = lpfc_sli4_mbx_read_fcf_rec(phba, mboxq, fcf_index);\n\tif (rc) {\n\t\terror = -EINVAL;\n\t\tgoto fail_fcf_scan;\n\t}\n\t/* Issue the mailbox command asynchronously */\n\tmboxq->vport = phba->pport;\n\tmboxq->mbox_cmpl = lpfc_mbx_cmpl_fcf_scan_read_fcf_rec;\n\n\tspin_lock_irq(&phba->hbalock);\n\tphba->hba_flag |= FCF_TS_INPROG;\n\tspin_unlock_irq(&phba->hbalock);\n\n\trc = lpfc_sli_issue_mbox(phba, mboxq, MBX_NOWAIT);\n\tif (rc == MBX_NOT_FINISHED)\n\t\terror = -EIO;\n\telse {\n\t\t/* Reset eligible FCF count for new scan */\n\t\tif (fcf_index == LPFC_FCOE_FCF_GET_FIRST)\n\t\t\tphba->fcf.eligible_fcf_cnt = 0;\n\t\terror = 0;\n\t}\nfail_fcf_scan:\n\tif (error) {\n\t\tif (mboxq)\n\t\t\tlpfc_sli4_mbox_cmd_free(phba, mboxq);\n\t\t/* FCF scan failed, clear FCF_TS_INPROG flag */\n\t\tspin_lock_irq(&phba->hbalock);\n\t\tphba->hba_flag &= ~FCF_TS_INPROG;\n\t\tspin_unlock_irq(&phba->hbalock);\n\t}\n\treturn error;\n}\n\n/**\n * lpfc_sli4_fcf_rr_read_fcf_rec - Read hba fcf record for roundrobin fcf.\n * @phba: pointer to lpfc hba data structure.\n * @fcf_index: FCF table entry offset.\n *\n * This routine is invoked to read an FCF record indicated by @fcf_index\n * and to use it for FLOGI roundrobin FCF failover.\n *\n * Return 0 if the mailbox command is submitted successfully, none 0\n * otherwise.\n **/\nint\nlpfc_sli4_fcf_rr_read_fcf_rec(struct lpfc_hba *phba, uint16_t fcf_index)\n{\n\tint rc = 0, error;\n\tLPFC_MBOXQ_t *mboxq;\n\n\tmboxq = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mboxq) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_FIP | LOG_INIT,\n\t\t\t\t\"2763 Failed to allocate mbox for \"\n\t\t\t\t\"READ_FCF cmd\\n\");\n\t\terror = -ENOMEM;\n\t\tgoto fail_fcf_read;\n\t}\n\t/* Construct the read FCF record mailbox command */\n\trc = lpfc_sli4_mbx_read_fcf_rec(phba, mboxq, fcf_index);\n\tif (rc) {\n\t\terror = -EINVAL;\n\t\tgoto fail_fcf_read;\n\t}\n\t/* Issue the mailbox command asynchronously */\n\tmboxq->vport = phba->pport;\n\tmboxq->mbox_cmpl = lpfc_mbx_cmpl_fcf_rr_read_fcf_rec;\n\trc = lpfc_sli_issue_mbox(phba, mboxq, MBX_NOWAIT);\n\tif (rc == MBX_NOT_FINISHED)\n\t\terror = -EIO;\n\telse\n\t\terror = 0;\n\nfail_fcf_read:\n\tif (error && mboxq)\n\t\tlpfc_sli4_mbox_cmd_free(phba, mboxq);\n\treturn error;\n}\n\n/**\n * lpfc_sli4_read_fcf_rec - Read hba fcf record for update eligible fcf bmask.\n * @phba: pointer to lpfc hba data structure.\n * @fcf_index: FCF table entry offset.\n *\n * This routine is invoked to read an FCF record indicated by @fcf_index to\n * determine whether it's eligible for FLOGI roundrobin failover list.\n *\n * Return 0 if the mailbox command is submitted successfully, none 0\n * otherwise.\n **/\nint\nlpfc_sli4_read_fcf_rec(struct lpfc_hba *phba, uint16_t fcf_index)\n{\n\tint rc = 0, error;\n\tLPFC_MBOXQ_t *mboxq;\n\n\tmboxq = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mboxq) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_FIP | LOG_INIT,\n\t\t\t\t\"2758 Failed to allocate mbox for \"\n\t\t\t\t\"READ_FCF cmd\\n\");\n\t\t\t\terror = -ENOMEM;\n\t\t\t\tgoto fail_fcf_read;\n\t}\n\t/* Construct the read FCF record mailbox command */\n\trc = lpfc_sli4_mbx_read_fcf_rec(phba, mboxq, fcf_index);\n\tif (rc) {\n\t\terror = -EINVAL;\n\t\tgoto fail_fcf_read;\n\t}\n\t/* Issue the mailbox command asynchronously */\n\tmboxq->vport = phba->pport;\n\tmboxq->mbox_cmpl = lpfc_mbx_cmpl_read_fcf_rec;\n\trc = lpfc_sli_issue_mbox(phba, mboxq, MBX_NOWAIT);\n\tif (rc == MBX_NOT_FINISHED)\n\t\terror = -EIO;\n\telse\n\t\terror = 0;\n\nfail_fcf_read:\n\tif (error && mboxq)\n\t\tlpfc_sli4_mbox_cmd_free(phba, mboxq);\n\treturn error;\n}\n\n/**\n * lpfc_check_next_fcf_pri_level\n * @phba: pointer to the lpfc_hba struct for this port.\n * This routine is called from the lpfc_sli4_fcf_rr_next_index_get\n * routine when the rr_bmask is empty. The FCF indecies are put into the\n * rr_bmask based on their priority level. Starting from the highest priority\n * to the lowest. The most likely FCF candidate will be in the highest\n * priority group. When this routine is called it searches the fcf_pri list for\n * next lowest priority group and repopulates the rr_bmask with only those\n * fcf_indexes.\n * returns:\n * 1=success 0=failure\n **/\nstatic int\nlpfc_check_next_fcf_pri_level(struct lpfc_hba *phba)\n{\n\tuint16_t next_fcf_pri;\n\tuint16_t last_index;\n\tstruct lpfc_fcf_pri *fcf_pri;\n\tint rc;\n\tint ret = 0;\n\n\tlast_index = find_first_bit(phba->fcf.fcf_rr_bmask,\n\t\t\tLPFC_SLI4_FCF_TBL_INDX_MAX);\n\tlpfc_printf_log(phba, KERN_INFO, LOG_FIP,\n\t\t\t\"3060 Last IDX %d\\n\", last_index);\n\n\t/* Verify the priority list has 2 or more entries */\n\tspin_lock_irq(&phba->hbalock);\n\tif (list_empty(&phba->fcf.fcf_pri_list) ||\n\t    list_is_singular(&phba->fcf.fcf_pri_list)) {\n\t\tspin_unlock_irq(&phba->hbalock);\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_FIP,\n\t\t\t\"3061 Last IDX %d\\n\", last_index);\n\t\treturn 0; /* Empty rr list */\n\t}\n\tspin_unlock_irq(&phba->hbalock);\n\n\tnext_fcf_pri = 0;\n\t/*\n\t * Clear the rr_bmask and set all of the bits that are at this\n\t * priority.\n\t */\n\tmemset(phba->fcf.fcf_rr_bmask, 0,\n\t\t\tsizeof(*phba->fcf.fcf_rr_bmask));\n\tspin_lock_irq(&phba->hbalock);\n\tlist_for_each_entry(fcf_pri, &phba->fcf.fcf_pri_list, list) {\n\t\tif (fcf_pri->fcf_rec.flag & LPFC_FCF_FLOGI_FAILED)\n\t\t\tcontinue;\n\t\t/*\n\t\t * the 1st priority that has not FLOGI failed\n\t\t * will be the highest.\n\t\t */\n\t\tif (!next_fcf_pri)\n\t\t\tnext_fcf_pri = fcf_pri->fcf_rec.priority;\n\t\tspin_unlock_irq(&phba->hbalock);\n\t\tif (fcf_pri->fcf_rec.priority == next_fcf_pri) {\n\t\t\trc = lpfc_sli4_fcf_rr_index_set(phba,\n\t\t\t\t\t\tfcf_pri->fcf_rec.fcf_index);\n\t\t\tif (rc)\n\t\t\t\treturn 0;\n\t\t}\n\t\tspin_lock_irq(&phba->hbalock);\n\t}\n\t/*\n\t * if next_fcf_pri was not set above and the list is not empty then\n\t * we have failed flogis on all of them. So reset flogi failed\n\t * and start at the beginning.\n\t */\n\tif (!next_fcf_pri && !list_empty(&phba->fcf.fcf_pri_list)) {\n\t\tlist_for_each_entry(fcf_pri, &phba->fcf.fcf_pri_list, list) {\n\t\t\tfcf_pri->fcf_rec.flag &= ~LPFC_FCF_FLOGI_FAILED;\n\t\t\t/*\n\t\t\t * the 1st priority that has not FLOGI failed\n\t\t\t * will be the highest.\n\t\t\t */\n\t\t\tif (!next_fcf_pri)\n\t\t\t\tnext_fcf_pri = fcf_pri->fcf_rec.priority;\n\t\t\tspin_unlock_irq(&phba->hbalock);\n\t\t\tif (fcf_pri->fcf_rec.priority == next_fcf_pri) {\n\t\t\t\trc = lpfc_sli4_fcf_rr_index_set(phba,\n\t\t\t\t\t\tfcf_pri->fcf_rec.fcf_index);\n\t\t\t\tif (rc)\n\t\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tspin_lock_irq(&phba->hbalock);\n\t\t}\n\t} else\n\t\tret = 1;\n\tspin_unlock_irq(&phba->hbalock);\n\n\treturn ret;\n}\n/**\n * lpfc_sli4_fcf_rr_next_index_get - Get next eligible fcf record index\n * @phba: pointer to lpfc hba data structure.\n *\n * This routine is to get the next eligible FCF record index in a round\n * robin fashion. If the next eligible FCF record index equals to the\n * initial roundrobin FCF record index, LPFC_FCOE_FCF_NEXT_NONE (0xFFFF)\n * shall be returned, otherwise, the next eligible FCF record's index\n * shall be returned.\n **/\nuint16_t\nlpfc_sli4_fcf_rr_next_index_get(struct lpfc_hba *phba)\n{\n\tuint16_t next_fcf_index;\n\ninitial_priority:\n\t/* Search start from next bit of currently registered FCF index */\n\tnext_fcf_index = phba->fcf.current_rec.fcf_indx;\n\nnext_priority:\n\t/* Determine the next fcf index to check */\n\tnext_fcf_index = (next_fcf_index + 1) % LPFC_SLI4_FCF_TBL_INDX_MAX;\n\tnext_fcf_index = find_next_bit(phba->fcf.fcf_rr_bmask,\n\t\t\t\t       LPFC_SLI4_FCF_TBL_INDX_MAX,\n\t\t\t\t       next_fcf_index);\n\n\t/* Wrap around condition on phba->fcf.fcf_rr_bmask */\n\tif (next_fcf_index >= LPFC_SLI4_FCF_TBL_INDX_MAX) {\n\t\t/*\n\t\t * If we have wrapped then we need to clear the bits that\n\t\t * have been tested so that we can detect when we should\n\t\t * change the priority level.\n\t\t */\n\t\tnext_fcf_index = find_next_bit(phba->fcf.fcf_rr_bmask,\n\t\t\t\t\t       LPFC_SLI4_FCF_TBL_INDX_MAX, 0);\n\t}\n\n\n\t/* Check roundrobin failover list empty condition */\n\tif (next_fcf_index >= LPFC_SLI4_FCF_TBL_INDX_MAX ||\n\t\tnext_fcf_index == phba->fcf.current_rec.fcf_indx) {\n\t\t/*\n\t\t * If next fcf index is not found check if there are lower\n\t\t * Priority level fcf's in the fcf_priority list.\n\t\t * Set up the rr_bmask with all of the avaiable fcf bits\n\t\t * at that level and continue the selection process.\n\t\t */\n\t\tif (lpfc_check_next_fcf_pri_level(phba))\n\t\t\tgoto initial_priority;\n\t\tlpfc_printf_log(phba, KERN_WARNING, LOG_FIP,\n\t\t\t\t\"2844 No roundrobin failover FCF available\\n\");\n\n\t\treturn LPFC_FCOE_FCF_NEXT_NONE;\n\t}\n\n\tif (next_fcf_index < LPFC_SLI4_FCF_TBL_INDX_MAX &&\n\t\tphba->fcf.fcf_pri[next_fcf_index].fcf_rec.flag &\n\t\tLPFC_FCF_FLOGI_FAILED) {\n\t\tif (list_is_singular(&phba->fcf.fcf_pri_list))\n\t\t\treturn LPFC_FCOE_FCF_NEXT_NONE;\n\n\t\tgoto next_priority;\n\t}\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_FIP,\n\t\t\t\"2845 Get next roundrobin failover FCF (x%x)\\n\",\n\t\t\tnext_fcf_index);\n\n\treturn next_fcf_index;\n}\n\n/**\n * lpfc_sli4_fcf_rr_index_set - Set bmask with eligible fcf record index\n * @phba: pointer to lpfc hba data structure.\n * @fcf_index: index into the FCF table to 'set'\n *\n * This routine sets the FCF record index in to the eligible bmask for\n * roundrobin failover search. It checks to make sure that the index\n * does not go beyond the range of the driver allocated bmask dimension\n * before setting the bit.\n *\n * Returns 0 if the index bit successfully set, otherwise, it returns\n * -EINVAL.\n **/\nint\nlpfc_sli4_fcf_rr_index_set(struct lpfc_hba *phba, uint16_t fcf_index)\n{\n\tif (fcf_index >= LPFC_SLI4_FCF_TBL_INDX_MAX) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_FIP,\n\t\t\t\t\"2610 FCF (x%x) reached driver's book \"\n\t\t\t\t\"keeping dimension:x%x\\n\",\n\t\t\t\tfcf_index, LPFC_SLI4_FCF_TBL_INDX_MAX);\n\t\treturn -EINVAL;\n\t}\n\t/* Set the eligible FCF record index bmask */\n\tset_bit(fcf_index, phba->fcf.fcf_rr_bmask);\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_FIP,\n\t\t\t\"2790 Set FCF (x%x) to roundrobin FCF failover \"\n\t\t\t\"bmask\\n\", fcf_index);\n\n\treturn 0;\n}\n\n/**\n * lpfc_sli4_fcf_rr_index_clear - Clear bmask from eligible fcf record index\n * @phba: pointer to lpfc hba data structure.\n * @fcf_index: index into the FCF table to 'clear'\n *\n * This routine clears the FCF record index from the eligible bmask for\n * roundrobin failover search. It checks to make sure that the index\n * does not go beyond the range of the driver allocated bmask dimension\n * before clearing the bit.\n **/\nvoid\nlpfc_sli4_fcf_rr_index_clear(struct lpfc_hba *phba, uint16_t fcf_index)\n{\n\tstruct lpfc_fcf_pri *fcf_pri, *fcf_pri_next;\n\tif (fcf_index >= LPFC_SLI4_FCF_TBL_INDX_MAX) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_FIP,\n\t\t\t\t\"2762 FCF (x%x) reached driver's book \"\n\t\t\t\t\"keeping dimension:x%x\\n\",\n\t\t\t\tfcf_index, LPFC_SLI4_FCF_TBL_INDX_MAX);\n\t\treturn;\n\t}\n\t/* Clear the eligible FCF record index bmask */\n\tspin_lock_irq(&phba->hbalock);\n\tlist_for_each_entry_safe(fcf_pri, fcf_pri_next, &phba->fcf.fcf_pri_list,\n\t\t\t\t list) {\n\t\tif (fcf_pri->fcf_rec.fcf_index == fcf_index) {\n\t\t\tlist_del_init(&fcf_pri->list);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irq(&phba->hbalock);\n\tclear_bit(fcf_index, phba->fcf.fcf_rr_bmask);\n\n\tlpfc_printf_log(phba, KERN_INFO, LOG_FIP,\n\t\t\t\"2791 Clear FCF (x%x) from roundrobin failover \"\n\t\t\t\"bmask\\n\", fcf_index);\n}\n\n/**\n * lpfc_mbx_cmpl_redisc_fcf_table - completion routine for rediscover FCF table\n * @phba: pointer to lpfc hba data structure.\n * @mbox: An allocated pointer to type LPFC_MBOXQ_t\n *\n * This routine is the completion routine for the rediscover FCF table mailbox\n * command. If the mailbox command returned failure, it will try to stop the\n * FCF rediscover wait timer.\n **/\nstatic void\nlpfc_mbx_cmpl_redisc_fcf_table(struct lpfc_hba *phba, LPFC_MBOXQ_t *mbox)\n{\n\tstruct lpfc_mbx_redisc_fcf_tbl *redisc_fcf;\n\tuint32_t shdr_status, shdr_add_status;\n\n\tredisc_fcf = &mbox->u.mqe.un.redisc_fcf_tbl;\n\n\tshdr_status = bf_get(lpfc_mbox_hdr_status,\n\t\t\t     &redisc_fcf->header.cfg_shdr.response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status,\n\t\t\t     &redisc_fcf->header.cfg_shdr.response);\n\tif (shdr_status || shdr_add_status) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_FIP,\n\t\t\t\t\"2746 Requesting for FCF rediscovery failed \"\n\t\t\t\t\"status x%x add_status x%x\\n\",\n\t\t\t\tshdr_status, shdr_add_status);\n\t\tif (phba->fcf.fcf_flag & FCF_ACVL_DISC) {\n\t\t\tspin_lock_irq(&phba->hbalock);\n\t\t\tphba->fcf.fcf_flag &= ~FCF_ACVL_DISC;\n\t\t\tspin_unlock_irq(&phba->hbalock);\n\t\t\t/*\n\t\t\t * CVL event triggered FCF rediscover request failed,\n\t\t\t * last resort to re-try current registered FCF entry.\n\t\t\t */\n\t\t\tlpfc_retry_pport_discovery(phba);\n\t\t} else {\n\t\t\tspin_lock_irq(&phba->hbalock);\n\t\t\tphba->fcf.fcf_flag &= ~FCF_DEAD_DISC;\n\t\t\tspin_unlock_irq(&phba->hbalock);\n\t\t\t/*\n\t\t\t * DEAD FCF event triggered FCF rediscover request\n\t\t\t * failed, last resort to fail over as a link down\n\t\t\t * to FCF registration.\n\t\t\t */\n\t\t\tlpfc_sli4_fcf_dead_failthrough(phba);\n\t\t}\n\t} else {\n\t\tlpfc_printf_log(phba, KERN_INFO, LOG_FIP,\n\t\t\t\t\"2775 Start FCF rediscover quiescent timer\\n\");\n\t\t/*\n\t\t * Start FCF rediscovery wait timer for pending FCF\n\t\t * before rescan FCF record table.\n\t\t */\n\t\tlpfc_fcf_redisc_wait_start_timer(phba);\n\t}\n\n\tmempool_free(mbox, phba->mbox_mem_pool);\n}\n\n/**\n * lpfc_sli4_redisc_fcf_table - Request to rediscover entire FCF table by port.\n * @phba: pointer to lpfc hba data structure.\n *\n * This routine is invoked to request for rediscovery of the entire FCF table\n * by the port.\n **/\nint\nlpfc_sli4_redisc_fcf_table(struct lpfc_hba *phba)\n{\n\tLPFC_MBOXQ_t *mbox;\n\tstruct lpfc_mbx_redisc_fcf_tbl *redisc_fcf;\n\tint rc, length;\n\n\t/* Cancel retry delay timers to all vports before FCF rediscover */\n\tlpfc_cancel_all_vport_retry_delay_timer(phba);\n\n\tmbox = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2745 Failed to allocate mbox for \"\n\t\t\t\t\"requesting FCF rediscover.\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tlength = (sizeof(struct lpfc_mbx_redisc_fcf_tbl) -\n\t\t  sizeof(struct lpfc_sli4_cfg_mhdr));\n\tlpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_FCOE,\n\t\t\t LPFC_MBOX_OPCODE_FCOE_REDISCOVER_FCF,\n\t\t\t length, LPFC_SLI4_MBX_EMBED);\n\n\tredisc_fcf = &mbox->u.mqe.un.redisc_fcf_tbl;\n\t/* Set count to 0 for invalidating the entire FCF database */\n\tbf_set(lpfc_mbx_redisc_fcf_count, redisc_fcf, 0);\n\n\t/* Issue the mailbox command asynchronously */\n\tmbox->vport = phba->pport;\n\tmbox->mbox_cmpl = lpfc_mbx_cmpl_redisc_fcf_table;\n\trc = lpfc_sli_issue_mbox(phba, mbox, MBX_NOWAIT);\n\n\tif (rc == MBX_NOT_FINISHED) {\n\t\tmempool_free(mbox, phba->mbox_mem_pool);\n\t\treturn -EIO;\n\t}\n\treturn 0;\n}\n\n/**\n * lpfc_sli4_fcf_dead_failthrough - Failthrough routine to fcf dead event\n * @phba: pointer to lpfc hba data structure.\n *\n * This function is the failover routine as a last resort to the FCF DEAD\n * event when driver failed to perform fast FCF failover.\n **/\nvoid\nlpfc_sli4_fcf_dead_failthrough(struct lpfc_hba *phba)\n{\n\tuint32_t link_state;\n\n\t/*\n\t * Last resort as FCF DEAD event failover will treat this as\n\t * a link down, but save the link state because we don't want\n\t * it to be changed to Link Down unless it is already down.\n\t */\n\tlink_state = phba->link_state;\n\tlpfc_linkdown(phba);\n\tphba->link_state = link_state;\n\n\t/* Unregister FCF if no devices connected to it */\n\tlpfc_unregister_unused_fcf(phba);\n}\n\n/**\n * lpfc_sli_get_config_region23 - Get sli3 port region 23 data.\n * @phba: pointer to lpfc hba data structure.\n * @rgn23_data: pointer to configure region 23 data.\n *\n * This function gets SLI3 port configure region 23 data through memory dump\n * mailbox command. When it successfully retrieves data, the size of the data\n * will be returned, otherwise, 0 will be returned.\n **/\nstatic uint32_t\nlpfc_sli_get_config_region23(struct lpfc_hba *phba, char *rgn23_data)\n{\n\tLPFC_MBOXQ_t *pmb = NULL;\n\tMAILBOX_t *mb;\n\tuint32_t offset = 0;\n\tint i, rc;\n\n\tif (!rgn23_data)\n\t\treturn 0;\n\n\tpmb = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!pmb) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2600 failed to allocate mailbox memory\\n\");\n\t\treturn 0;\n\t}\n\tmb = &pmb->u.mb;\n\n\tdo {\n\t\tlpfc_dump_mem(phba, pmb, offset, DMP_REGION_23);\n\t\trc = lpfc_sli_issue_mbox(phba, pmb, MBX_POLL);\n\n\t\tif (rc != MBX_SUCCESS) {\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_INIT,\n\t\t\t\t\t\"2601 failed to read config \"\n\t\t\t\t\t\"region 23, rc 0x%x Status 0x%x\\n\",\n\t\t\t\t\trc, mb->mbxStatus);\n\t\t\tmb->un.varDmp.word_cnt = 0;\n\t\t}\n\t\t/*\n\t\t * dump mem may return a zero when finished or we got a\n\t\t * mailbox error, either way we are done.\n\t\t */\n\t\tif (mb->un.varDmp.word_cnt == 0)\n\t\t\tbreak;\n\n\t\ti =  mb->un.varDmp.word_cnt * sizeof(uint32_t);\n\t\tif (offset + i >  DMP_RGN23_SIZE)\n\t\t\ti =  DMP_RGN23_SIZE - offset;\n\t\tlpfc_sli_pcimem_bcopy(((uint8_t *)mb) + DMP_RSP_OFFSET,\n\t\t\t\t      rgn23_data  + offset, i);\n\t\toffset += i;\n\t} while (offset < DMP_RGN23_SIZE);\n\n\tmempool_free(pmb, phba->mbox_mem_pool);\n\treturn offset;\n}\n\n/**\n * lpfc_sli4_get_config_region23 - Get sli4 port region 23 data.\n * @phba: pointer to lpfc hba data structure.\n * @rgn23_data: pointer to configure region 23 data.\n *\n * This function gets SLI4 port configure region 23 data through memory dump\n * mailbox command. When it successfully retrieves data, the size of the data\n * will be returned, otherwise, 0 will be returned.\n **/\nstatic uint32_t\nlpfc_sli4_get_config_region23(struct lpfc_hba *phba, char *rgn23_data)\n{\n\tLPFC_MBOXQ_t *mboxq = NULL;\n\tstruct lpfc_dmabuf *mp = NULL;\n\tstruct lpfc_mqe *mqe;\n\tuint32_t data_length = 0;\n\tint rc;\n\n\tif (!rgn23_data)\n\t\treturn 0;\n\n\tmboxq = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mboxq) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"3105 failed to allocate mailbox memory\\n\");\n\t\treturn 0;\n\t}\n\n\tif (lpfc_sli4_dump_cfg_rg23(phba, mboxq))\n\t\tgoto out;\n\tmqe = &mboxq->u.mqe;\n\tmp = (struct lpfc_dmabuf *)mboxq->ctx_buf;\n\trc = lpfc_sli_issue_mbox(phba, mboxq, MBX_POLL);\n\tif (rc)\n\t\tgoto out;\n\tdata_length = mqe->un.mb_words[5];\n\tif (data_length == 0)\n\t\tgoto out;\n\tif (data_length > DMP_RGN23_SIZE) {\n\t\tdata_length = 0;\n\t\tgoto out;\n\t}\n\tlpfc_sli_pcimem_bcopy((char *)mp->virt, rgn23_data, data_length);\nout:\n\tmempool_free(mboxq, phba->mbox_mem_pool);\n\tif (mp) {\n\t\tlpfc_mbuf_free(phba, mp->virt, mp->phys);\n\t\tkfree(mp);\n\t}\n\treturn data_length;\n}\n\n/**\n * lpfc_sli_read_link_ste - Read region 23 to decide if link is disabled.\n * @phba: pointer to lpfc hba data structure.\n *\n * This function read region 23 and parse TLV for port status to\n * decide if the user disaled the port. If the TLV indicates the\n * port is disabled, the hba_flag is set accordingly.\n **/\nvoid\nlpfc_sli_read_link_ste(struct lpfc_hba *phba)\n{\n\tuint8_t *rgn23_data = NULL;\n\tuint32_t if_type, data_size, sub_tlv_len, tlv_offset;\n\tuint32_t offset = 0;\n\n\t/* Get adapter Region 23 data */\n\trgn23_data = kzalloc(DMP_RGN23_SIZE, GFP_KERNEL);\n\tif (!rgn23_data)\n\t\tgoto out;\n\n\tif (phba->sli_rev < LPFC_SLI_REV4)\n\t\tdata_size = lpfc_sli_get_config_region23(phba, rgn23_data);\n\telse {\n\t\tif_type = bf_get(lpfc_sli_intf_if_type,\n\t\t\t\t &phba->sli4_hba.sli_intf);\n\t\tif (if_type == LPFC_SLI_INTF_IF_TYPE_0)\n\t\t\tgoto out;\n\t\tdata_size = lpfc_sli4_get_config_region23(phba, rgn23_data);\n\t}\n\n\tif (!data_size)\n\t\tgoto out;\n\n\t/* Check the region signature first */\n\tif (memcmp(&rgn23_data[offset], LPFC_REGION23_SIGNATURE, 4)) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"2619 Config region 23 has bad signature\\n\");\n\t\t\tgoto out;\n\t}\n\toffset += 4;\n\n\t/* Check the data structure version */\n\tif (rgn23_data[offset] != LPFC_REGION23_VERSION) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\"2620 Config region 23 has bad version\\n\");\n\t\tgoto out;\n\t}\n\toffset += 4;\n\n\t/* Parse TLV entries in the region */\n\twhile (offset < data_size) {\n\t\tif (rgn23_data[offset] == LPFC_REGION23_LAST_REC)\n\t\t\tbreak;\n\t\t/*\n\t\t * If the TLV is not driver specific TLV or driver id is\n\t\t * not linux driver id, skip the record.\n\t\t */\n\t\tif ((rgn23_data[offset] != DRIVER_SPECIFIC_TYPE) ||\n\t\t    (rgn23_data[offset + 2] != LINUX_DRIVER_ID) ||\n\t\t    (rgn23_data[offset + 3] != 0)) {\n\t\t\toffset += rgn23_data[offset + 1] * 4 + 4;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Driver found a driver specific TLV in the config region */\n\t\tsub_tlv_len = rgn23_data[offset + 1] * 4;\n\t\toffset += 4;\n\t\ttlv_offset = 0;\n\n\t\t/*\n\t\t * Search for configured port state sub-TLV.\n\t\t */\n\t\twhile ((offset < data_size) &&\n\t\t\t(tlv_offset < sub_tlv_len)) {\n\t\t\tif (rgn23_data[offset] == LPFC_REGION23_LAST_REC) {\n\t\t\t\toffset += 4;\n\t\t\t\ttlv_offset += 4;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (rgn23_data[offset] != PORT_STE_TYPE) {\n\t\t\t\toffset += rgn23_data[offset + 1] * 4 + 4;\n\t\t\t\ttlv_offset += rgn23_data[offset + 1] * 4 + 4;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* This HBA contains PORT_STE configured */\n\t\t\tif (!rgn23_data[offset + 2])\n\t\t\t\tphba->hba_flag |= LINK_DISABLED;\n\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\tkfree(rgn23_data);\n\treturn;\n}\n\n/**\n * lpfc_wr_object - write an object to the firmware\n * @phba: HBA structure that indicates port to create a queue on.\n * @dmabuf_list: list of dmabufs to write to the port.\n * @size: the total byte value of the objects to write to the port.\n * @offset: the current offset to be used to start the transfer.\n *\n * This routine will create a wr_object mailbox command to send to the port.\n * the mailbox command will be constructed using the dma buffers described in\n * @dmabuf_list to create a list of BDEs. This routine will fill in as many\n * BDEs that the imbedded mailbox can support. The @offset variable will be\n * used to indicate the starting offset of the transfer and will also return\n * the offset after the write object mailbox has completed. @size is used to\n * determine the end of the object and whether the eof bit should be set.\n *\n * Return 0 is successful and offset will contain the the new offset to use\n * for the next write.\n * Return negative value for error cases.\n **/\nint\nlpfc_wr_object(struct lpfc_hba *phba, struct list_head *dmabuf_list,\n\t       uint32_t size, uint32_t *offset)\n{\n\tstruct lpfc_mbx_wr_object *wr_object;\n\tLPFC_MBOXQ_t *mbox;\n\tint rc = 0, i = 0;\n\tuint32_t shdr_status, shdr_add_status, shdr_change_status, shdr_csf;\n\tuint32_t mbox_tmo;\n\tstruct lpfc_dmabuf *dmabuf;\n\tuint32_t written = 0;\n\tbool check_change_status = false;\n\n\tmbox = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL);\n\tif (!mbox)\n\t\treturn -ENOMEM;\n\n\tlpfc_sli4_config(phba, mbox, LPFC_MBOX_SUBSYSTEM_COMMON,\n\t\t\tLPFC_MBOX_OPCODE_WRITE_OBJECT,\n\t\t\tsizeof(struct lpfc_mbx_wr_object) -\n\t\t\tsizeof(struct lpfc_sli4_cfg_mhdr), LPFC_SLI4_MBX_EMBED);\n\n\twr_object = (struct lpfc_mbx_wr_object *)&mbox->u.mqe.un.wr_object;\n\twr_object->u.request.write_offset = *offset;\n\tsprintf((uint8_t *)wr_object->u.request.object_name, \"/\");\n\twr_object->u.request.object_name[0] =\n\t\tcpu_to_le32(wr_object->u.request.object_name[0]);\n\tbf_set(lpfc_wr_object_eof, &wr_object->u.request, 0);\n\tlist_for_each_entry(dmabuf, dmabuf_list, list) {\n\t\tif (i >= LPFC_MBX_WR_CONFIG_MAX_BDE || written >= size)\n\t\t\tbreak;\n\t\twr_object->u.request.bde[i].addrLow = putPaddrLow(dmabuf->phys);\n\t\twr_object->u.request.bde[i].addrHigh =\n\t\t\tputPaddrHigh(dmabuf->phys);\n\t\tif (written + SLI4_PAGE_SIZE >= size) {\n\t\t\twr_object->u.request.bde[i].tus.f.bdeSize =\n\t\t\t\t(size - written);\n\t\t\twritten += (size - written);\n\t\t\tbf_set(lpfc_wr_object_eof, &wr_object->u.request, 1);\n\t\t\tbf_set(lpfc_wr_object_eas, &wr_object->u.request, 1);\n\t\t\tcheck_change_status = true;\n\t\t} else {\n\t\t\twr_object->u.request.bde[i].tus.f.bdeSize =\n\t\t\t\tSLI4_PAGE_SIZE;\n\t\t\twritten += SLI4_PAGE_SIZE;\n\t\t}\n\t\ti++;\n\t}\n\twr_object->u.request.bde_count = i;\n\tbf_set(lpfc_wr_object_write_length, &wr_object->u.request, written);\n\tif (!phba->sli4_hba.intr_enable)\n\t\trc = lpfc_sli_issue_mbox(phba, mbox, MBX_POLL);\n\telse {\n\t\tmbox_tmo = lpfc_mbox_tmo_val(phba, mbox);\n\t\trc = lpfc_sli_issue_mbox_wait(phba, mbox, mbox_tmo);\n\t}\n\t/* The IOCTL status is embedded in the mailbox subheader. */\n\tshdr_status = bf_get(lpfc_mbox_hdr_status,\n\t\t\t     &wr_object->header.cfg_shdr.response);\n\tshdr_add_status = bf_get(lpfc_mbox_hdr_add_status,\n\t\t\t\t &wr_object->header.cfg_shdr.response);\n\tif (check_change_status) {\n\t\tshdr_change_status = bf_get(lpfc_wr_object_change_status,\n\t\t\t\t\t    &wr_object->u.response);\n\n\t\tif (shdr_change_status == LPFC_CHANGE_STATUS_FW_RESET ||\n\t\t    shdr_change_status == LPFC_CHANGE_STATUS_PORT_MIGRATION) {\n\t\t\tshdr_csf = bf_get(lpfc_wr_object_csf,\n\t\t\t\t\t  &wr_object->u.response);\n\t\t\tif (shdr_csf)\n\t\t\t\tshdr_change_status =\n\t\t\t\t\t\t   LPFC_CHANGE_STATUS_PCI_RESET;\n\t\t}\n\n\t\tswitch (shdr_change_status) {\n\t\tcase (LPFC_CHANGE_STATUS_PHYS_DEV_RESET):\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_INIT,\n\t\t\t\t\t\"3198 Firmware write complete: System \"\n\t\t\t\t\t\"reboot required to instantiate\\n\");\n\t\t\tbreak;\n\t\tcase (LPFC_CHANGE_STATUS_FW_RESET):\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_INIT,\n\t\t\t\t\t\"3199 Firmware write complete: Firmware\"\n\t\t\t\t\t\" reset required to instantiate\\n\");\n\t\t\tbreak;\n\t\tcase (LPFC_CHANGE_STATUS_PORT_MIGRATION):\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_INIT,\n\t\t\t\t\t\"3200 Firmware write complete: Port \"\n\t\t\t\t\t\"Migration or PCI Reset required to \"\n\t\t\t\t\t\"instantiate\\n\");\n\t\t\tbreak;\n\t\tcase (LPFC_CHANGE_STATUS_PCI_RESET):\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_INIT,\n\t\t\t\t\t\"3201 Firmware write complete: PCI \"\n\t\t\t\t\t\"Reset required to instantiate\\n\");\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (rc != MBX_TIMEOUT)\n\t\tmempool_free(mbox, phba->mbox_mem_pool);\n\tif (shdr_status || shdr_add_status || rc) {\n\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"3025 Write Object mailbox failed with \"\n\t\t\t\t\"status x%x add_status x%x, mbx status x%x\\n\",\n\t\t\t\tshdr_status, shdr_add_status, rc);\n\t\trc = -ENXIO;\n\t\t*offset = shdr_add_status;\n\t} else\n\t\t*offset += wr_object->u.response.actual_write_length;\n\treturn rc;\n}\n\n/**\n * lpfc_cleanup_pending_mbox - Free up vport discovery mailbox commands.\n * @vport: pointer to vport data structure.\n *\n * This function iterate through the mailboxq and clean up all REG_LOGIN\n * and REG_VPI mailbox commands associated with the vport. This function\n * is called when driver want to restart discovery of the vport due to\n * a Clear Virtual Link event.\n **/\nvoid\nlpfc_cleanup_pending_mbox(struct lpfc_vport *vport)\n{\n\tstruct lpfc_hba *phba = vport->phba;\n\tLPFC_MBOXQ_t *mb, *nextmb;\n\tstruct lpfc_dmabuf *mp;\n\tstruct lpfc_nodelist *ndlp;\n\tstruct lpfc_nodelist *act_mbx_ndlp = NULL;\n\tstruct Scsi_Host  *shost = lpfc_shost_from_vport(vport);\n\tLIST_HEAD(mbox_cmd_list);\n\tuint8_t restart_loop;\n\n\t/* Clean up internally queued mailbox commands with the vport */\n\tspin_lock_irq(&phba->hbalock);\n\tlist_for_each_entry_safe(mb, nextmb, &phba->sli.mboxq, list) {\n\t\tif (mb->vport != vport)\n\t\t\tcontinue;\n\n\t\tif ((mb->u.mb.mbxCommand != MBX_REG_LOGIN64) &&\n\t\t\t(mb->u.mb.mbxCommand != MBX_REG_VPI))\n\t\t\tcontinue;\n\n\t\tlist_del(&mb->list);\n\t\tlist_add_tail(&mb->list, &mbox_cmd_list);\n\t}\n\t/* Clean up active mailbox command with the vport */\n\tmb = phba->sli.mbox_active;\n\tif (mb && (mb->vport == vport)) {\n\t\tif ((mb->u.mb.mbxCommand == MBX_REG_LOGIN64) ||\n\t\t\t(mb->u.mb.mbxCommand == MBX_REG_VPI))\n\t\t\tmb->mbox_cmpl = lpfc_sli_def_mbox_cmpl;\n\t\tif (mb->u.mb.mbxCommand == MBX_REG_LOGIN64) {\n\t\t\tact_mbx_ndlp = (struct lpfc_nodelist *)mb->ctx_ndlp;\n\t\t\t/* Put reference count for delayed processing */\n\t\t\tact_mbx_ndlp = lpfc_nlp_get(act_mbx_ndlp);\n\t\t\t/* Unregister the RPI when mailbox complete */\n\t\t\tmb->mbox_flag |= LPFC_MBX_IMED_UNREG;\n\t\t}\n\t}\n\t/* Cleanup any mailbox completions which are not yet processed */\n\tdo {\n\t\trestart_loop = 0;\n\t\tlist_for_each_entry(mb, &phba->sli.mboxq_cmpl, list) {\n\t\t\t/*\n\t\t\t * If this mailox is already processed or it is\n\t\t\t * for another vport ignore it.\n\t\t\t */\n\t\t\tif ((mb->vport != vport) ||\n\t\t\t\t(mb->mbox_flag & LPFC_MBX_IMED_UNREG))\n\t\t\t\tcontinue;\n\n\t\t\tif ((mb->u.mb.mbxCommand != MBX_REG_LOGIN64) &&\n\t\t\t\t(mb->u.mb.mbxCommand != MBX_REG_VPI))\n\t\t\t\tcontinue;\n\n\t\t\tmb->mbox_cmpl = lpfc_sli_def_mbox_cmpl;\n\t\t\tif (mb->u.mb.mbxCommand == MBX_REG_LOGIN64) {\n\t\t\t\tndlp = (struct lpfc_nodelist *)mb->ctx_ndlp;\n\t\t\t\t/* Unregister the RPI when mailbox complete */\n\t\t\t\tmb->mbox_flag |= LPFC_MBX_IMED_UNREG;\n\t\t\t\trestart_loop = 1;\n\t\t\t\tspin_unlock_irq(&phba->hbalock);\n\t\t\t\tspin_lock(shost->host_lock);\n\t\t\t\tndlp->nlp_flag &= ~NLP_IGNR_REG_CMPL;\n\t\t\t\tspin_unlock(shost->host_lock);\n\t\t\t\tspin_lock_irq(&phba->hbalock);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t} while (restart_loop);\n\n\tspin_unlock_irq(&phba->hbalock);\n\n\t/* Release the cleaned-up mailbox commands */\n\twhile (!list_empty(&mbox_cmd_list)) {\n\t\tlist_remove_head(&mbox_cmd_list, mb, LPFC_MBOXQ_t, list);\n\t\tif (mb->u.mb.mbxCommand == MBX_REG_LOGIN64) {\n\t\t\tmp = (struct lpfc_dmabuf *)(mb->ctx_buf);\n\t\t\tif (mp) {\n\t\t\t\t__lpfc_mbuf_free(phba, mp->virt, mp->phys);\n\t\t\t\tkfree(mp);\n\t\t\t}\n\t\t\tmb->ctx_buf = NULL;\n\t\t\tndlp = (struct lpfc_nodelist *)mb->ctx_ndlp;\n\t\t\tmb->ctx_ndlp = NULL;\n\t\t\tif (ndlp) {\n\t\t\t\tspin_lock(shost->host_lock);\n\t\t\t\tndlp->nlp_flag &= ~NLP_IGNR_REG_CMPL;\n\t\t\t\tspin_unlock(shost->host_lock);\n\t\t\t\tlpfc_nlp_put(ndlp);\n\t\t\t}\n\t\t}\n\t\tmempool_free(mb, phba->mbox_mem_pool);\n\t}\n\n\t/* Release the ndlp with the cleaned-up active mailbox command */\n\tif (act_mbx_ndlp) {\n\t\tspin_lock(shost->host_lock);\n\t\tact_mbx_ndlp->nlp_flag &= ~NLP_IGNR_REG_CMPL;\n\t\tspin_unlock(shost->host_lock);\n\t\tlpfc_nlp_put(act_mbx_ndlp);\n\t}\n}\n\n/**\n * lpfc_drain_txq - Drain the txq\n * @phba: Pointer to HBA context object.\n *\n * This function attempt to submit IOCBs on the txq\n * to the adapter.  For SLI4 adapters, the txq contains\n * ELS IOCBs that have been deferred because the there\n * are no SGLs.  This congestion can occur with large\n * vport counts during node discovery.\n **/\n\nuint32_t\nlpfc_drain_txq(struct lpfc_hba *phba)\n{\n\tLIST_HEAD(completions);\n\tstruct lpfc_sli_ring *pring;\n\tstruct lpfc_iocbq *piocbq = NULL;\n\tunsigned long iflags = 0;\n\tchar *fail_msg = NULL;\n\tstruct lpfc_sglq *sglq;\n\tunion lpfc_wqe128 wqe;\n\tuint32_t txq_cnt = 0;\n\tstruct lpfc_queue *wq;\n\n\tif (phba->link_flag & LS_MDS_LOOPBACK) {\n\t\t/* MDS WQE are posted only to first WQ*/\n\t\twq = phba->sli4_hba.hdwq[0].io_wq;\n\t\tif (unlikely(!wq))\n\t\t\treturn 0;\n\t\tpring = wq->pring;\n\t} else {\n\t\twq = phba->sli4_hba.els_wq;\n\t\tif (unlikely(!wq))\n\t\t\treturn 0;\n\t\tpring = lpfc_phba_elsring(phba);\n\t}\n\n\tif (unlikely(!pring) || list_empty(&pring->txq))\n\t\treturn 0;\n\n\tspin_lock_irqsave(&pring->ring_lock, iflags);\n\tlist_for_each_entry(piocbq, &pring->txq, list) {\n\t\ttxq_cnt++;\n\t}\n\n\tif (txq_cnt > pring->txq_max)\n\t\tpring->txq_max = txq_cnt;\n\n\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n\n\twhile (!list_empty(&pring->txq)) {\n\t\tspin_lock_irqsave(&pring->ring_lock, iflags);\n\n\t\tpiocbq = lpfc_sli_ringtx_get(phba, pring);\n\t\tif (!piocbq) {\n\t\t\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\"2823 txq empty and txq_cnt is %d\\n \",\n\t\t\t\ttxq_cnt);\n\t\t\tbreak;\n\t\t}\n\t\tsglq = __lpfc_sli_get_els_sglq(phba, piocbq);\n\t\tif (!sglq) {\n\t\t\t__lpfc_sli_ringtx_put(phba, pring, piocbq);\n\t\t\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n\t\t\tbreak;\n\t\t}\n\t\ttxq_cnt--;\n\n\t\t/* The xri and iocb resources secured,\n\t\t * attempt to issue request\n\t\t */\n\t\tpiocbq->sli4_lxritag = sglq->sli4_lxritag;\n\t\tpiocbq->sli4_xritag = sglq->sli4_xritag;\n\t\tif (NO_XRI == lpfc_sli4_bpl2sgl(phba, piocbq, sglq))\n\t\t\tfail_msg = \"to convert bpl to sgl\";\n\t\telse if (lpfc_sli4_iocb2wqe(phba, piocbq, &wqe))\n\t\t\tfail_msg = \"to convert iocb to wqe\";\n\t\telse if (lpfc_sli4_wq_put(wq, &wqe))\n\t\t\tfail_msg = \" - Wq is full\";\n\t\telse\n\t\t\tlpfc_sli_ringtxcmpl_put(phba, pring, piocbq);\n\n\t\tif (fail_msg) {\n\t\t\t/* Failed means we can't issue and need to cancel */\n\t\t\tlpfc_printf_log(phba, KERN_ERR, LOG_TRACE_EVENT,\n\t\t\t\t\t\"2822 IOCB failed %s iotag 0x%x \"\n\t\t\t\t\t\"xri 0x%x\\n\",\n\t\t\t\t\tfail_msg,\n\t\t\t\t\tpiocbq->iotag, piocbq->sli4_xritag);\n\t\t\tlist_add_tail(&piocbq->list, &completions);\n\t\t}\n\t\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n\t}\n\n\t/* Cancel all the IOCBs that cannot be issued */\n\tlpfc_sli_cancel_iocbs(phba, &completions, IOSTAT_LOCAL_REJECT,\n\t\t\t\tIOERR_SLI_ABORTED);\n\n\treturn txq_cnt;\n}\n\n/**\n * lpfc_wqe_bpl2sgl - Convert the bpl/bde to a sgl.\n * @phba: Pointer to HBA context object.\n * @pwqeq: Pointer to command WQE.\n * @sglq: Pointer to the scatter gather queue object.\n *\n * This routine converts the bpl or bde that is in the WQE\n * to a sgl list for the sli4 hardware. The physical address\n * of the bpl/bde is converted back to a virtual address.\n * If the WQE contains a BPL then the list of BDE's is\n * converted to sli4_sge's. If the WQE contains a single\n * BDE then it is converted to a single sli_sge.\n * The WQE is still in cpu endianness so the contents of\n * the bpl can be used without byte swapping.\n *\n * Returns valid XRI = Success, NO_XRI = Failure.\n */\nstatic uint16_t\nlpfc_wqe_bpl2sgl(struct lpfc_hba *phba, struct lpfc_iocbq *pwqeq,\n\t\t struct lpfc_sglq *sglq)\n{\n\tuint16_t xritag = NO_XRI;\n\tstruct ulp_bde64 *bpl = NULL;\n\tstruct ulp_bde64 bde;\n\tstruct sli4_sge *sgl  = NULL;\n\tstruct lpfc_dmabuf *dmabuf;\n\tunion lpfc_wqe128 *wqe;\n\tint numBdes = 0;\n\tint i = 0;\n\tuint32_t offset = 0; /* accumulated offset in the sg request list */\n\tint inbound = 0; /* number of sg reply entries inbound from firmware */\n\tuint32_t cmd;\n\n\tif (!pwqeq || !sglq)\n\t\treturn xritag;\n\n\tsgl  = (struct sli4_sge *)sglq->sgl;\n\twqe = &pwqeq->wqe;\n\tpwqeq->iocb.ulpIoTag = pwqeq->iotag;\n\n\tcmd = bf_get(wqe_cmnd, &wqe->generic.wqe_com);\n\tif (cmd == CMD_XMIT_BLS_RSP64_WQE)\n\t\treturn sglq->sli4_xritag;\n\tnumBdes = pwqeq->rsvd2;\n\tif (numBdes) {\n\t\t/* The addrHigh and addrLow fields within the WQE\n\t\t * have not been byteswapped yet so there is no\n\t\t * need to swap them back.\n\t\t */\n\t\tif (pwqeq->context3)\n\t\t\tdmabuf = (struct lpfc_dmabuf *)pwqeq->context3;\n\t\telse\n\t\t\treturn xritag;\n\n\t\tbpl  = (struct ulp_bde64 *)dmabuf->virt;\n\t\tif (!bpl)\n\t\t\treturn xritag;\n\n\t\tfor (i = 0; i < numBdes; i++) {\n\t\t\t/* Should already be byte swapped. */\n\t\t\tsgl->addr_hi = bpl->addrHigh;\n\t\t\tsgl->addr_lo = bpl->addrLow;\n\n\t\t\tsgl->word2 = le32_to_cpu(sgl->word2);\n\t\t\tif ((i+1) == numBdes)\n\t\t\t\tbf_set(lpfc_sli4_sge_last, sgl, 1);\n\t\t\telse\n\t\t\t\tbf_set(lpfc_sli4_sge_last, sgl, 0);\n\t\t\t/* swap the size field back to the cpu so we\n\t\t\t * can assign it to the sgl.\n\t\t\t */\n\t\t\tbde.tus.w = le32_to_cpu(bpl->tus.w);\n\t\t\tsgl->sge_len = cpu_to_le32(bde.tus.f.bdeSize);\n\t\t\t/* The offsets in the sgl need to be accumulated\n\t\t\t * separately for the request and reply lists.\n\t\t\t * The request is always first, the reply follows.\n\t\t\t */\n\t\t\tswitch (cmd) {\n\t\t\tcase CMD_GEN_REQUEST64_WQE:\n\t\t\t\t/* add up the reply sg entries */\n\t\t\t\tif (bpl->tus.f.bdeFlags == BUFF_TYPE_BDE_64I)\n\t\t\t\t\tinbound++;\n\t\t\t\t/* first inbound? reset the offset */\n\t\t\t\tif (inbound == 1)\n\t\t\t\t\toffset = 0;\n\t\t\t\tbf_set(lpfc_sli4_sge_offset, sgl, offset);\n\t\t\t\tbf_set(lpfc_sli4_sge_type, sgl,\n\t\t\t\t\tLPFC_SGE_TYPE_DATA);\n\t\t\t\toffset += bde.tus.f.bdeSize;\n\t\t\t\tbreak;\n\t\t\tcase CMD_FCP_TRSP64_WQE:\n\t\t\t\tbf_set(lpfc_sli4_sge_offset, sgl, 0);\n\t\t\t\tbf_set(lpfc_sli4_sge_type, sgl,\n\t\t\t\t\tLPFC_SGE_TYPE_DATA);\n\t\t\t\tbreak;\n\t\t\tcase CMD_FCP_TSEND64_WQE:\n\t\t\tcase CMD_FCP_TRECEIVE64_WQE:\n\t\t\t\tbf_set(lpfc_sli4_sge_type, sgl,\n\t\t\t\t\tbpl->tus.f.bdeFlags);\n\t\t\t\tif (i < 3)\n\t\t\t\t\toffset = 0;\n\t\t\t\telse\n\t\t\t\t\toffset += bde.tus.f.bdeSize;\n\t\t\t\tbf_set(lpfc_sli4_sge_offset, sgl, offset);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tsgl->word2 = cpu_to_le32(sgl->word2);\n\t\t\tbpl++;\n\t\t\tsgl++;\n\t\t}\n\t} else if (wqe->gen_req.bde.tus.f.bdeFlags == BUFF_TYPE_BDE_64) {\n\t\t/* The addrHigh and addrLow fields of the BDE have not\n\t\t * been byteswapped yet so they need to be swapped\n\t\t * before putting them in the sgl.\n\t\t */\n\t\tsgl->addr_hi = cpu_to_le32(wqe->gen_req.bde.addrHigh);\n\t\tsgl->addr_lo = cpu_to_le32(wqe->gen_req.bde.addrLow);\n\t\tsgl->word2 = le32_to_cpu(sgl->word2);\n\t\tbf_set(lpfc_sli4_sge_last, sgl, 1);\n\t\tsgl->word2 = cpu_to_le32(sgl->word2);\n\t\tsgl->sge_len = cpu_to_le32(wqe->gen_req.bde.tus.f.bdeSize);\n\t}\n\treturn sglq->sli4_xritag;\n}\n\n/**\n * lpfc_sli4_issue_wqe - Issue an SLI4 Work Queue Entry (WQE)\n * @phba: Pointer to HBA context object.\n * @qp: Pointer to HDW queue.\n * @pwqe: Pointer to command WQE.\n **/\nint\nlpfc_sli4_issue_wqe(struct lpfc_hba *phba, struct lpfc_sli4_hdw_queue *qp,\n\t\t    struct lpfc_iocbq *pwqe)\n{\n\tunion lpfc_wqe128 *wqe = &pwqe->wqe;\n\tstruct lpfc_async_xchg_ctx *ctxp;\n\tstruct lpfc_queue *wq;\n\tstruct lpfc_sglq *sglq;\n\tstruct lpfc_sli_ring *pring;\n\tunsigned long iflags;\n\tuint32_t ret = 0;\n\n\t/* NVME_LS and NVME_LS ABTS requests. */\n\tif (pwqe->iocb_flag & LPFC_IO_NVME_LS) {\n\t\tpring =  phba->sli4_hba.nvmels_wq->pring;\n\t\tlpfc_qp_spin_lock_irqsave(&pring->ring_lock, iflags,\n\t\t\t\t\t  qp, wq_access);\n\t\tsglq = __lpfc_sli_get_els_sglq(phba, pwqe);\n\t\tif (!sglq) {\n\t\t\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n\t\t\treturn WQE_BUSY;\n\t\t}\n\t\tpwqe->sli4_lxritag = sglq->sli4_lxritag;\n\t\tpwqe->sli4_xritag = sglq->sli4_xritag;\n\t\tif (lpfc_wqe_bpl2sgl(phba, pwqe, sglq) == NO_XRI) {\n\t\t\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n\t\t\treturn WQE_ERROR;\n\t\t}\n\t\tbf_set(wqe_xri_tag, &pwqe->wqe.xmit_bls_rsp.wqe_com,\n\t\t       pwqe->sli4_xritag);\n\t\tret = lpfc_sli4_wq_put(phba->sli4_hba.nvmels_wq, wqe);\n\t\tif (ret) {\n\t\t\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n\t\t\treturn ret;\n\t\t}\n\n\t\tlpfc_sli_ringtxcmpl_put(phba, pring, pwqe);\n\t\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n\n\t\tlpfc_sli4_poll_eq(qp->hba_eq, LPFC_POLL_FASTPATH);\n\t\treturn 0;\n\t}\n\n\t/* NVME_FCREQ and NVME_ABTS requests */\n\tif (pwqe->iocb_flag & LPFC_IO_NVME) {\n\t\t/* Get the IO distribution (hba_wqidx) for WQ assignment. */\n\t\twq = qp->io_wq;\n\t\tpring = wq->pring;\n\n\t\tbf_set(wqe_cqid, &wqe->generic.wqe_com, qp->io_cq_map);\n\n\t\tlpfc_qp_spin_lock_irqsave(&pring->ring_lock, iflags,\n\t\t\t\t\t  qp, wq_access);\n\t\tret = lpfc_sli4_wq_put(wq, wqe);\n\t\tif (ret) {\n\t\t\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n\t\t\treturn ret;\n\t\t}\n\t\tlpfc_sli_ringtxcmpl_put(phba, pring, pwqe);\n\t\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n\n\t\tlpfc_sli4_poll_eq(qp->hba_eq, LPFC_POLL_FASTPATH);\n\t\treturn 0;\n\t}\n\n\t/* NVMET requests */\n\tif (pwqe->iocb_flag & LPFC_IO_NVMET) {\n\t\t/* Get the IO distribution (hba_wqidx) for WQ assignment. */\n\t\twq = qp->io_wq;\n\t\tpring = wq->pring;\n\n\t\tctxp = pwqe->context2;\n\t\tsglq = ctxp->ctxbuf->sglq;\n\t\tif (pwqe->sli4_xritag ==  NO_XRI) {\n\t\t\tpwqe->sli4_lxritag = sglq->sli4_lxritag;\n\t\t\tpwqe->sli4_xritag = sglq->sli4_xritag;\n\t\t}\n\t\tbf_set(wqe_xri_tag, &pwqe->wqe.xmit_bls_rsp.wqe_com,\n\t\t       pwqe->sli4_xritag);\n\t\tbf_set(wqe_cqid, &wqe->generic.wqe_com, qp->io_cq_map);\n\n\t\tlpfc_qp_spin_lock_irqsave(&pring->ring_lock, iflags,\n\t\t\t\t\t  qp, wq_access);\n\t\tret = lpfc_sli4_wq_put(wq, wqe);\n\t\tif (ret) {\n\t\t\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n\t\t\treturn ret;\n\t\t}\n\t\tlpfc_sli_ringtxcmpl_put(phba, pring, pwqe);\n\t\tspin_unlock_irqrestore(&pring->ring_lock, iflags);\n\n\t\tlpfc_sli4_poll_eq(qp->hba_eq, LPFC_POLL_FASTPATH);\n\t\treturn 0;\n\t}\n\treturn WQE_ERROR;\n}\n\n#ifdef LPFC_MXP_STAT\n/**\n * lpfc_snapshot_mxp - Snapshot pbl, pvt and busy count\n * @phba: pointer to lpfc hba data structure.\n * @hwqid: belong to which HWQ.\n *\n * The purpose of this routine is to take a snapshot of pbl, pvt and busy count\n * 15 seconds after a test case is running.\n *\n * The user should call lpfc_debugfs_multixripools_write before running a test\n * case to clear stat_snapshot_taken. Then the user starts a test case. During\n * test case is running, stat_snapshot_taken is incremented by 1 every time when\n * this routine is called from heartbeat timer. When stat_snapshot_taken is\n * equal to LPFC_MXP_SNAPSHOT_TAKEN, a snapshot is taken.\n **/\nvoid lpfc_snapshot_mxp(struct lpfc_hba *phba, u32 hwqid)\n{\n\tstruct lpfc_sli4_hdw_queue *qp;\n\tstruct lpfc_multixri_pool *multixri_pool;\n\tstruct lpfc_pvt_pool *pvt_pool;\n\tstruct lpfc_pbl_pool *pbl_pool;\n\tu32 txcmplq_cnt;\n\n\tqp = &phba->sli4_hba.hdwq[hwqid];\n\tmultixri_pool = qp->p_multixri_pool;\n\tif (!multixri_pool)\n\t\treturn;\n\n\tif (multixri_pool->stat_snapshot_taken == LPFC_MXP_SNAPSHOT_TAKEN) {\n\t\tpvt_pool = &qp->p_multixri_pool->pvt_pool;\n\t\tpbl_pool = &qp->p_multixri_pool->pbl_pool;\n\t\ttxcmplq_cnt = qp->io_wq->pring->txcmplq_cnt;\n\n\t\tmultixri_pool->stat_pbl_count = pbl_pool->count;\n\t\tmultixri_pool->stat_pvt_count = pvt_pool->count;\n\t\tmultixri_pool->stat_busy_count = txcmplq_cnt;\n\t}\n\n\tmultixri_pool->stat_snapshot_taken++;\n}\n#endif\n\n/**\n * lpfc_adjust_pvt_pool_count - Adjust private pool count\n * @phba: pointer to lpfc hba data structure.\n * @hwqid: belong to which HWQ.\n *\n * This routine moves some XRIs from private to public pool when private pool\n * is not busy.\n **/\nvoid lpfc_adjust_pvt_pool_count(struct lpfc_hba *phba, u32 hwqid)\n{\n\tstruct lpfc_multixri_pool *multixri_pool;\n\tu32 io_req_count;\n\tu32 prev_io_req_count;\n\n\tmultixri_pool = phba->sli4_hba.hdwq[hwqid].p_multixri_pool;\n\tif (!multixri_pool)\n\t\treturn;\n\tio_req_count = multixri_pool->io_req_count;\n\tprev_io_req_count = multixri_pool->prev_io_req_count;\n\n\tif (prev_io_req_count != io_req_count) {\n\t\t/* Private pool is busy */\n\t\tmultixri_pool->prev_io_req_count = io_req_count;\n\t} else {\n\t\t/* Private pool is not busy.\n\t\t * Move XRIs from private to public pool.\n\t\t */\n\t\tlpfc_move_xri_pvt_to_pbl(phba, hwqid);\n\t}\n}\n\n/**\n * lpfc_adjust_high_watermark - Adjust high watermark\n * @phba: pointer to lpfc hba data structure.\n * @hwqid: belong to which HWQ.\n *\n * This routine sets high watermark as number of outstanding XRIs,\n * but make sure the new value is between xri_limit/2 and xri_limit.\n **/\nvoid lpfc_adjust_high_watermark(struct lpfc_hba *phba, u32 hwqid)\n{\n\tu32 new_watermark;\n\tu32 watermark_max;\n\tu32 watermark_min;\n\tu32 xri_limit;\n\tu32 txcmplq_cnt;\n\tu32 abts_io_bufs;\n\tstruct lpfc_multixri_pool *multixri_pool;\n\tstruct lpfc_sli4_hdw_queue *qp;\n\n\tqp = &phba->sli4_hba.hdwq[hwqid];\n\tmultixri_pool = qp->p_multixri_pool;\n\tif (!multixri_pool)\n\t\treturn;\n\txri_limit = multixri_pool->xri_limit;\n\n\twatermark_max = xri_limit;\n\twatermark_min = xri_limit / 2;\n\n\ttxcmplq_cnt = qp->io_wq->pring->txcmplq_cnt;\n\tabts_io_bufs = qp->abts_scsi_io_bufs;\n\tabts_io_bufs += qp->abts_nvme_io_bufs;\n\n\tnew_watermark = txcmplq_cnt + abts_io_bufs;\n\tnew_watermark = min(watermark_max, new_watermark);\n\tnew_watermark = max(watermark_min, new_watermark);\n\tmultixri_pool->pvt_pool.high_watermark = new_watermark;\n\n#ifdef LPFC_MXP_STAT\n\tmultixri_pool->stat_max_hwm = max(multixri_pool->stat_max_hwm,\n\t\t\t\t\t  new_watermark);\n#endif\n}\n\n/**\n * lpfc_move_xri_pvt_to_pbl - Move some XRIs from private to public pool\n * @phba: pointer to lpfc hba data structure.\n * @hwqid: belong to which HWQ.\n *\n * This routine is called from hearbeat timer when pvt_pool is idle.\n * All free XRIs are moved from private to public pool on hwqid with 2 steps.\n * The first step moves (all - low_watermark) amount of XRIs.\n * The second step moves the rest of XRIs.\n **/\nvoid lpfc_move_xri_pvt_to_pbl(struct lpfc_hba *phba, u32 hwqid)\n{\n\tstruct lpfc_pbl_pool *pbl_pool;\n\tstruct lpfc_pvt_pool *pvt_pool;\n\tstruct lpfc_sli4_hdw_queue *qp;\n\tstruct lpfc_io_buf *lpfc_ncmd;\n\tstruct lpfc_io_buf *lpfc_ncmd_next;\n\tunsigned long iflag;\n\tstruct list_head tmp_list;\n\tu32 tmp_count;\n\n\tqp = &phba->sli4_hba.hdwq[hwqid];\n\tpbl_pool = &qp->p_multixri_pool->pbl_pool;\n\tpvt_pool = &qp->p_multixri_pool->pvt_pool;\n\ttmp_count = 0;\n\n\tlpfc_qp_spin_lock_irqsave(&pbl_pool->lock, iflag, qp, mv_to_pub_pool);\n\tlpfc_qp_spin_lock(&pvt_pool->lock, qp, mv_from_pvt_pool);\n\n\tif (pvt_pool->count > pvt_pool->low_watermark) {\n\t\t/* Step 1: move (all - low_watermark) from pvt_pool\n\t\t * to pbl_pool\n\t\t */\n\n\t\t/* Move low watermark of bufs from pvt_pool to tmp_list */\n\t\tINIT_LIST_HEAD(&tmp_list);\n\t\tlist_for_each_entry_safe(lpfc_ncmd, lpfc_ncmd_next,\n\t\t\t\t\t &pvt_pool->list, list) {\n\t\t\tlist_move_tail(&lpfc_ncmd->list, &tmp_list);\n\t\t\ttmp_count++;\n\t\t\tif (tmp_count >= pvt_pool->low_watermark)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t/* Move all bufs from pvt_pool to pbl_pool */\n\t\tlist_splice_init(&pvt_pool->list, &pbl_pool->list);\n\n\t\t/* Move all bufs from tmp_list to pvt_pool */\n\t\tlist_splice(&tmp_list, &pvt_pool->list);\n\n\t\tpbl_pool->count += (pvt_pool->count - tmp_count);\n\t\tpvt_pool->count = tmp_count;\n\t} else {\n\t\t/* Step 2: move the rest from pvt_pool to pbl_pool */\n\t\tlist_splice_init(&pvt_pool->list, &pbl_pool->list);\n\t\tpbl_pool->count += pvt_pool->count;\n\t\tpvt_pool->count = 0;\n\t}\n\n\tspin_unlock(&pvt_pool->lock);\n\tspin_unlock_irqrestore(&pbl_pool->lock, iflag);\n}\n\n/**\n * _lpfc_move_xri_pbl_to_pvt - Move some XRIs from public to private pool\n * @phba: pointer to lpfc hba data structure\n * @qp: pointer to HDW queue\n * @pbl_pool: specified public free XRI pool\n * @pvt_pool: specified private free XRI pool\n * @count: number of XRIs to move\n *\n * This routine tries to move some free common bufs from the specified pbl_pool\n * to the specified pvt_pool. It might move less than count XRIs if there's not\n * enough in public pool.\n *\n * Return:\n *   true - if XRIs are successfully moved from the specified pbl_pool to the\n *          specified pvt_pool\n *   false - if the specified pbl_pool is empty or locked by someone else\n **/\nstatic bool\n_lpfc_move_xri_pbl_to_pvt(struct lpfc_hba *phba, struct lpfc_sli4_hdw_queue *qp,\n\t\t\t  struct lpfc_pbl_pool *pbl_pool,\n\t\t\t  struct lpfc_pvt_pool *pvt_pool, u32 count)\n{\n\tstruct lpfc_io_buf *lpfc_ncmd;\n\tstruct lpfc_io_buf *lpfc_ncmd_next;\n\tunsigned long iflag;\n\tint ret;\n\n\tret = spin_trylock_irqsave(&pbl_pool->lock, iflag);\n\tif (ret) {\n\t\tif (pbl_pool->count) {\n\t\t\t/* Move a batch of XRIs from public to private pool */\n\t\t\tlpfc_qp_spin_lock(&pvt_pool->lock, qp, mv_to_pvt_pool);\n\t\t\tlist_for_each_entry_safe(lpfc_ncmd,\n\t\t\t\t\t\t lpfc_ncmd_next,\n\t\t\t\t\t\t &pbl_pool->list,\n\t\t\t\t\t\t list) {\n\t\t\t\tlist_move_tail(&lpfc_ncmd->list,\n\t\t\t\t\t       &pvt_pool->list);\n\t\t\t\tpvt_pool->count++;\n\t\t\t\tpbl_pool->count--;\n\t\t\t\tcount--;\n\t\t\t\tif (count == 0)\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tspin_unlock(&pvt_pool->lock);\n\t\t\tspin_unlock_irqrestore(&pbl_pool->lock, iflag);\n\t\t\treturn true;\n\t\t}\n\t\tspin_unlock_irqrestore(&pbl_pool->lock, iflag);\n\t}\n\n\treturn false;\n}\n\n/**\n * lpfc_move_xri_pbl_to_pvt - Move some XRIs from public to private pool\n * @phba: pointer to lpfc hba data structure.\n * @hwqid: belong to which HWQ.\n * @count: number of XRIs to move\n *\n * This routine tries to find some free common bufs in one of public pools with\n * Round Robin method. The search always starts from local hwqid, then the next\n * HWQ which was found last time (rrb_next_hwqid). Once a public pool is found,\n * a batch of free common bufs are moved to private pool on hwqid.\n * It might move less than count XRIs if there's not enough in public pool.\n **/\nvoid lpfc_move_xri_pbl_to_pvt(struct lpfc_hba *phba, u32 hwqid, u32 count)\n{\n\tstruct lpfc_multixri_pool *multixri_pool;\n\tstruct lpfc_multixri_pool *next_multixri_pool;\n\tstruct lpfc_pvt_pool *pvt_pool;\n\tstruct lpfc_pbl_pool *pbl_pool;\n\tstruct lpfc_sli4_hdw_queue *qp;\n\tu32 next_hwqid;\n\tu32 hwq_count;\n\tint ret;\n\n\tqp = &phba->sli4_hba.hdwq[hwqid];\n\tmultixri_pool = qp->p_multixri_pool;\n\tpvt_pool = &multixri_pool->pvt_pool;\n\tpbl_pool = &multixri_pool->pbl_pool;\n\n\t/* Check if local pbl_pool is available */\n\tret = _lpfc_move_xri_pbl_to_pvt(phba, qp, pbl_pool, pvt_pool, count);\n\tif (ret) {\n#ifdef LPFC_MXP_STAT\n\t\tmultixri_pool->local_pbl_hit_count++;\n#endif\n\t\treturn;\n\t}\n\n\thwq_count = phba->cfg_hdw_queue;\n\n\t/* Get the next hwqid which was found last time */\n\tnext_hwqid = multixri_pool->rrb_next_hwqid;\n\n\tdo {\n\t\t/* Go to next hwq */\n\t\tnext_hwqid = (next_hwqid + 1) % hwq_count;\n\n\t\tnext_multixri_pool =\n\t\t\tphba->sli4_hba.hdwq[next_hwqid].p_multixri_pool;\n\t\tpbl_pool = &next_multixri_pool->pbl_pool;\n\n\t\t/* Check if the public free xri pool is available */\n\t\tret = _lpfc_move_xri_pbl_to_pvt(\n\t\t\tphba, qp, pbl_pool, pvt_pool, count);\n\n\t\t/* Exit while-loop if success or all hwqid are checked */\n\t} while (!ret && next_hwqid != multixri_pool->rrb_next_hwqid);\n\n\t/* Starting point for the next time */\n\tmultixri_pool->rrb_next_hwqid = next_hwqid;\n\n\tif (!ret) {\n\t\t/* stats: all public pools are empty*/\n\t\tmultixri_pool->pbl_empty_count++;\n\t}\n\n#ifdef LPFC_MXP_STAT\n\tif (ret) {\n\t\tif (next_hwqid == hwqid)\n\t\t\tmultixri_pool->local_pbl_hit_count++;\n\t\telse\n\t\t\tmultixri_pool->other_pbl_hit_count++;\n\t}\n#endif\n}\n\n/**\n * lpfc_keep_pvt_pool_above_lowwm - Keep pvt_pool above low watermark\n * @phba: pointer to lpfc hba data structure.\n * @hwqid: belong to which HWQ.\n *\n * This routine get a batch of XRIs from pbl_pool if pvt_pool is less than\n * low watermark.\n **/\nvoid lpfc_keep_pvt_pool_above_lowwm(struct lpfc_hba *phba, u32 hwqid)\n{\n\tstruct lpfc_multixri_pool *multixri_pool;\n\tstruct lpfc_pvt_pool *pvt_pool;\n\n\tmultixri_pool = phba->sli4_hba.hdwq[hwqid].p_multixri_pool;\n\tpvt_pool = &multixri_pool->pvt_pool;\n\n\tif (pvt_pool->count < pvt_pool->low_watermark)\n\t\tlpfc_move_xri_pbl_to_pvt(phba, hwqid, XRI_BATCH);\n}\n\n/**\n * lpfc_release_io_buf - Return one IO buf back to free pool\n * @phba: pointer to lpfc hba data structure.\n * @lpfc_ncmd: IO buf to be returned.\n * @qp: belong to which HWQ.\n *\n * This routine returns one IO buf back to free pool. If this is an urgent IO,\n * the IO buf is returned to expedite pool. If cfg_xri_rebalancing==1,\n * the IO buf is returned to pbl_pool or pvt_pool based on watermark and\n * xri_limit.  If cfg_xri_rebalancing==0, the IO buf is returned to\n * lpfc_io_buf_list_put.\n **/\nvoid lpfc_release_io_buf(struct lpfc_hba *phba, struct lpfc_io_buf *lpfc_ncmd,\n\t\t\t struct lpfc_sli4_hdw_queue *qp)\n{\n\tunsigned long iflag;\n\tstruct lpfc_pbl_pool *pbl_pool;\n\tstruct lpfc_pvt_pool *pvt_pool;\n\tstruct lpfc_epd_pool *epd_pool;\n\tu32 txcmplq_cnt;\n\tu32 xri_owned;\n\tu32 xri_limit;\n\tu32 abts_io_bufs;\n\n\t/* MUST zero fields if buffer is reused by another protocol */\n\tlpfc_ncmd->nvmeCmd = NULL;\n\tlpfc_ncmd->cur_iocbq.wqe_cmpl = NULL;\n\tlpfc_ncmd->cur_iocbq.iocb_cmpl = NULL;\n\n\tif (phba->cfg_xpsgl && !phba->nvmet_support &&\n\t    !list_empty(&lpfc_ncmd->dma_sgl_xtra_list))\n\t\tlpfc_put_sgl_per_hdwq(phba, lpfc_ncmd);\n\n\tif (!list_empty(&lpfc_ncmd->dma_cmd_rsp_list))\n\t\tlpfc_put_cmd_rsp_buf_per_hdwq(phba, lpfc_ncmd);\n\n\tif (phba->cfg_xri_rebalancing) {\n\t\tif (lpfc_ncmd->expedite) {\n\t\t\t/* Return to expedite pool */\n\t\t\tepd_pool = &phba->epd_pool;\n\t\t\tspin_lock_irqsave(&epd_pool->lock, iflag);\n\t\t\tlist_add_tail(&lpfc_ncmd->list, &epd_pool->list);\n\t\t\tepd_pool->count++;\n\t\t\tspin_unlock_irqrestore(&epd_pool->lock, iflag);\n\t\t\treturn;\n\t\t}\n\n\t\t/* Avoid invalid access if an IO sneaks in and is being rejected\n\t\t * just _after_ xri pools are destroyed in lpfc_offline.\n\t\t * Nothing much can be done at this point.\n\t\t */\n\t\tif (!qp->p_multixri_pool)\n\t\t\treturn;\n\n\t\tpbl_pool = &qp->p_multixri_pool->pbl_pool;\n\t\tpvt_pool = &qp->p_multixri_pool->pvt_pool;\n\n\t\ttxcmplq_cnt = qp->io_wq->pring->txcmplq_cnt;\n\t\tabts_io_bufs = qp->abts_scsi_io_bufs;\n\t\tabts_io_bufs += qp->abts_nvme_io_bufs;\n\n\t\txri_owned = pvt_pool->count + txcmplq_cnt + abts_io_bufs;\n\t\txri_limit = qp->p_multixri_pool->xri_limit;\n\n#ifdef LPFC_MXP_STAT\n\t\tif (xri_owned <= xri_limit)\n\t\t\tqp->p_multixri_pool->below_limit_count++;\n\t\telse\n\t\t\tqp->p_multixri_pool->above_limit_count++;\n#endif\n\n\t\t/* XRI goes to either public or private free xri pool\n\t\t *     based on watermark and xri_limit\n\t\t */\n\t\tif ((pvt_pool->count < pvt_pool->low_watermark) ||\n\t\t    (xri_owned < xri_limit &&\n\t\t     pvt_pool->count < pvt_pool->high_watermark)) {\n\t\t\tlpfc_qp_spin_lock_irqsave(&pvt_pool->lock, iflag,\n\t\t\t\t\t\t  qp, free_pvt_pool);\n\t\t\tlist_add_tail(&lpfc_ncmd->list,\n\t\t\t\t      &pvt_pool->list);\n\t\t\tpvt_pool->count++;\n\t\t\tspin_unlock_irqrestore(&pvt_pool->lock, iflag);\n\t\t} else {\n\t\t\tlpfc_qp_spin_lock_irqsave(&pbl_pool->lock, iflag,\n\t\t\t\t\t\t  qp, free_pub_pool);\n\t\t\tlist_add_tail(&lpfc_ncmd->list,\n\t\t\t\t      &pbl_pool->list);\n\t\t\tpbl_pool->count++;\n\t\t\tspin_unlock_irqrestore(&pbl_pool->lock, iflag);\n\t\t}\n\t} else {\n\t\tlpfc_qp_spin_lock_irqsave(&qp->io_buf_list_put_lock, iflag,\n\t\t\t\t\t  qp, free_xri);\n\t\tlist_add_tail(&lpfc_ncmd->list,\n\t\t\t      &qp->lpfc_io_buf_list_put);\n\t\tqp->put_io_bufs++;\n\t\tspin_unlock_irqrestore(&qp->io_buf_list_put_lock,\n\t\t\t\t       iflag);\n\t}\n}\n\n/**\n * lpfc_get_io_buf_from_private_pool - Get one free IO buf from private pool\n * @phba: pointer to lpfc hba data structure.\n * @qp: pointer to HDW queue\n * @pvt_pool: pointer to private pool data structure.\n * @ndlp: pointer to lpfc nodelist data structure.\n *\n * This routine tries to get one free IO buf from private pool.\n *\n * Return:\n *   pointer to one free IO buf - if private pool is not empty\n *   NULL - if private pool is empty\n **/\nstatic struct lpfc_io_buf *\nlpfc_get_io_buf_from_private_pool(struct lpfc_hba *phba,\n\t\t\t\t  struct lpfc_sli4_hdw_queue *qp,\n\t\t\t\t  struct lpfc_pvt_pool *pvt_pool,\n\t\t\t\t  struct lpfc_nodelist *ndlp)\n{\n\tstruct lpfc_io_buf *lpfc_ncmd;\n\tstruct lpfc_io_buf *lpfc_ncmd_next;\n\tunsigned long iflag;\n\n\tlpfc_qp_spin_lock_irqsave(&pvt_pool->lock, iflag, qp, alloc_pvt_pool);\n\tlist_for_each_entry_safe(lpfc_ncmd, lpfc_ncmd_next,\n\t\t\t\t &pvt_pool->list, list) {\n\t\tif (lpfc_test_rrq_active(\n\t\t\tphba, ndlp, lpfc_ncmd->cur_iocbq.sli4_lxritag))\n\t\t\tcontinue;\n\t\tlist_del(&lpfc_ncmd->list);\n\t\tpvt_pool->count--;\n\t\tspin_unlock_irqrestore(&pvt_pool->lock, iflag);\n\t\treturn lpfc_ncmd;\n\t}\n\tspin_unlock_irqrestore(&pvt_pool->lock, iflag);\n\n\treturn NULL;\n}\n\n/**\n * lpfc_get_io_buf_from_expedite_pool - Get one free IO buf from expedite pool\n * @phba: pointer to lpfc hba data structure.\n *\n * This routine tries to get one free IO buf from expedite pool.\n *\n * Return:\n *   pointer to one free IO buf - if expedite pool is not empty\n *   NULL - if expedite pool is empty\n **/\nstatic struct lpfc_io_buf *\nlpfc_get_io_buf_from_expedite_pool(struct lpfc_hba *phba)\n{\n\tstruct lpfc_io_buf *lpfc_ncmd;\n\tstruct lpfc_io_buf *lpfc_ncmd_next;\n\tunsigned long iflag;\n\tstruct lpfc_epd_pool *epd_pool;\n\n\tepd_pool = &phba->epd_pool;\n\tlpfc_ncmd = NULL;\n\n\tspin_lock_irqsave(&epd_pool->lock, iflag);\n\tif (epd_pool->count > 0) {\n\t\tlist_for_each_entry_safe(lpfc_ncmd, lpfc_ncmd_next,\n\t\t\t\t\t &epd_pool->list, list) {\n\t\t\tlist_del(&lpfc_ncmd->list);\n\t\t\tepd_pool->count--;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&epd_pool->lock, iflag);\n\n\treturn lpfc_ncmd;\n}\n\n/**\n * lpfc_get_io_buf_from_multixri_pools - Get one free IO bufs\n * @phba: pointer to lpfc hba data structure.\n * @ndlp: pointer to lpfc nodelist data structure.\n * @hwqid: belong to which HWQ\n * @expedite: 1 means this request is urgent.\n *\n * This routine will do the following actions and then return a pointer to\n * one free IO buf.\n *\n * 1. If private free xri count is empty, move some XRIs from public to\n *    private pool.\n * 2. Get one XRI from private free xri pool.\n * 3. If we fail to get one from pvt_pool and this is an expedite request,\n *    get one free xri from expedite pool.\n *\n * Note: ndlp is only used on SCSI side for RRQ testing.\n *       The caller should pass NULL for ndlp on NVME side.\n *\n * Return:\n *   pointer to one free IO buf - if private pool is not empty\n *   NULL - if private pool is empty\n **/\nstatic struct lpfc_io_buf *\nlpfc_get_io_buf_from_multixri_pools(struct lpfc_hba *phba,\n\t\t\t\t    struct lpfc_nodelist *ndlp,\n\t\t\t\t    int hwqid, int expedite)\n{\n\tstruct lpfc_sli4_hdw_queue *qp;\n\tstruct lpfc_multixri_pool *multixri_pool;\n\tstruct lpfc_pvt_pool *pvt_pool;\n\tstruct lpfc_io_buf *lpfc_ncmd;\n\n\tqp = &phba->sli4_hba.hdwq[hwqid];\n\tlpfc_ncmd = NULL;\n\tmultixri_pool = qp->p_multixri_pool;\n\tpvt_pool = &multixri_pool->pvt_pool;\n\tmultixri_pool->io_req_count++;\n\n\t/* If pvt_pool is empty, move some XRIs from public to private pool */\n\tif (pvt_pool->count == 0)\n\t\tlpfc_move_xri_pbl_to_pvt(phba, hwqid, XRI_BATCH);\n\n\t/* Get one XRI from private free xri pool */\n\tlpfc_ncmd = lpfc_get_io_buf_from_private_pool(phba, qp, pvt_pool, ndlp);\n\n\tif (lpfc_ncmd) {\n\t\tlpfc_ncmd->hdwq = qp;\n\t\tlpfc_ncmd->hdwq_no = hwqid;\n\t} else if (expedite) {\n\t\t/* If we fail to get one from pvt_pool and this is an expedite\n\t\t * request, get one free xri from expedite pool.\n\t\t */\n\t\tlpfc_ncmd = lpfc_get_io_buf_from_expedite_pool(phba);\n\t}\n\n\treturn lpfc_ncmd;\n}\n\nstatic inline struct lpfc_io_buf *\nlpfc_io_buf(struct lpfc_hba *phba, struct lpfc_nodelist *ndlp, int idx)\n{\n\tstruct lpfc_sli4_hdw_queue *qp;\n\tstruct lpfc_io_buf *lpfc_cmd, *lpfc_cmd_next;\n\n\tqp = &phba->sli4_hba.hdwq[idx];\n\tlist_for_each_entry_safe(lpfc_cmd, lpfc_cmd_next,\n\t\t\t\t &qp->lpfc_io_buf_list_get, list) {\n\t\tif (lpfc_test_rrq_active(phba, ndlp,\n\t\t\t\t\t lpfc_cmd->cur_iocbq.sli4_lxritag))\n\t\t\tcontinue;\n\n\t\tif (lpfc_cmd->flags & LPFC_SBUF_NOT_POSTED)\n\t\t\tcontinue;\n\n\t\tlist_del_init(&lpfc_cmd->list);\n\t\tqp->get_io_bufs--;\n\t\tlpfc_cmd->hdwq = qp;\n\t\tlpfc_cmd->hdwq_no = idx;\n\t\treturn lpfc_cmd;\n\t}\n\treturn NULL;\n}\n\n/**\n * lpfc_get_io_buf - Get one IO buffer from free pool\n * @phba: The HBA for which this call is being executed.\n * @ndlp: pointer to lpfc nodelist data structure.\n * @hwqid: belong to which HWQ\n * @expedite: 1 means this request is urgent.\n *\n * This routine gets one IO buffer from free pool. If cfg_xri_rebalancing==1,\n * removes a IO buffer from multiXRI pools. If cfg_xri_rebalancing==0, removes\n * a IO buffer from head of @hdwq io_buf_list and returns to caller.\n *\n * Note: ndlp is only used on SCSI side for RRQ testing.\n *       The caller should pass NULL for ndlp on NVME side.\n *\n * Return codes:\n *   NULL - Error\n *   Pointer to lpfc_io_buf - Success\n **/\nstruct lpfc_io_buf *lpfc_get_io_buf(struct lpfc_hba *phba,\n\t\t\t\t    struct lpfc_nodelist *ndlp,\n\t\t\t\t    u32 hwqid, int expedite)\n{\n\tstruct lpfc_sli4_hdw_queue *qp;\n\tunsigned long iflag;\n\tstruct lpfc_io_buf *lpfc_cmd;\n\n\tqp = &phba->sli4_hba.hdwq[hwqid];\n\tlpfc_cmd = NULL;\n\n\tif (phba->cfg_xri_rebalancing)\n\t\tlpfc_cmd = lpfc_get_io_buf_from_multixri_pools(\n\t\t\tphba, ndlp, hwqid, expedite);\n\telse {\n\t\tlpfc_qp_spin_lock_irqsave(&qp->io_buf_list_get_lock, iflag,\n\t\t\t\t\t  qp, alloc_xri_get);\n\t\tif (qp->get_io_bufs > LPFC_NVME_EXPEDITE_XRICNT || expedite)\n\t\t\tlpfc_cmd = lpfc_io_buf(phba, ndlp, hwqid);\n\t\tif (!lpfc_cmd) {\n\t\t\tlpfc_qp_spin_lock(&qp->io_buf_list_put_lock,\n\t\t\t\t\t  qp, alloc_xri_put);\n\t\t\tlist_splice(&qp->lpfc_io_buf_list_put,\n\t\t\t\t    &qp->lpfc_io_buf_list_get);\n\t\t\tqp->get_io_bufs += qp->put_io_bufs;\n\t\t\tINIT_LIST_HEAD(&qp->lpfc_io_buf_list_put);\n\t\t\tqp->put_io_bufs = 0;\n\t\t\tspin_unlock(&qp->io_buf_list_put_lock);\n\t\t\tif (qp->get_io_bufs > LPFC_NVME_EXPEDITE_XRICNT ||\n\t\t\t    expedite)\n\t\t\t\tlpfc_cmd = lpfc_io_buf(phba, ndlp, hwqid);\n\t\t}\n\t\tspin_unlock_irqrestore(&qp->io_buf_list_get_lock, iflag);\n\t}\n\n\treturn lpfc_cmd;\n}\n\n/**\n * lpfc_get_sgl_per_hdwq - Get one SGL chunk from hdwq's pool\n * @phba: The HBA for which this call is being executed.\n * @lpfc_buf: IO buf structure to append the SGL chunk\n *\n * This routine gets one SGL chunk buffer from hdwq's SGL chunk pool,\n * and will allocate an SGL chunk if the pool is empty.\n *\n * Return codes:\n *   NULL - Error\n *   Pointer to sli4_hybrid_sgl - Success\n **/\nstruct sli4_hybrid_sgl *\nlpfc_get_sgl_per_hdwq(struct lpfc_hba *phba, struct lpfc_io_buf *lpfc_buf)\n{\n\tstruct sli4_hybrid_sgl *list_entry = NULL;\n\tstruct sli4_hybrid_sgl *tmp = NULL;\n\tstruct sli4_hybrid_sgl *allocated_sgl = NULL;\n\tstruct lpfc_sli4_hdw_queue *hdwq = lpfc_buf->hdwq;\n\tstruct list_head *buf_list = &hdwq->sgl_list;\n\tunsigned long iflags;\n\n\tspin_lock_irqsave(&hdwq->hdwq_lock, iflags);\n\n\tif (likely(!list_empty(buf_list))) {\n\t\t/* break off 1 chunk from the sgl_list */\n\t\tlist_for_each_entry_safe(list_entry, tmp,\n\t\t\t\t\t buf_list, list_node) {\n\t\t\tlist_move_tail(&list_entry->list_node,\n\t\t\t\t       &lpfc_buf->dma_sgl_xtra_list);\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\t/* allocate more */\n\t\tspin_unlock_irqrestore(&hdwq->hdwq_lock, iflags);\n\t\ttmp = kmalloc_node(sizeof(*tmp), GFP_ATOMIC,\n\t\t\t\t   cpu_to_node(hdwq->io_wq->chann));\n\t\tif (!tmp) {\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\t\t\"8353 error kmalloc memory for HDWQ \"\n\t\t\t\t\t\"%d %s\\n\",\n\t\t\t\t\tlpfc_buf->hdwq_no, __func__);\n\t\t\treturn NULL;\n\t\t}\n\n\t\ttmp->dma_sgl = dma_pool_alloc(phba->lpfc_sg_dma_buf_pool,\n\t\t\t\t\t      GFP_ATOMIC, &tmp->dma_phys_sgl);\n\t\tif (!tmp->dma_sgl) {\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\t\t\"8354 error pool_alloc memory for HDWQ \"\n\t\t\t\t\t\"%d %s\\n\",\n\t\t\t\t\tlpfc_buf->hdwq_no, __func__);\n\t\t\tkfree(tmp);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tspin_lock_irqsave(&hdwq->hdwq_lock, iflags);\n\t\tlist_add_tail(&tmp->list_node, &lpfc_buf->dma_sgl_xtra_list);\n\t}\n\n\tallocated_sgl = list_last_entry(&lpfc_buf->dma_sgl_xtra_list,\n\t\t\t\t\tstruct sli4_hybrid_sgl,\n\t\t\t\t\tlist_node);\n\n\tspin_unlock_irqrestore(&hdwq->hdwq_lock, iflags);\n\n\treturn allocated_sgl;\n}\n\n/**\n * lpfc_put_sgl_per_hdwq - Put one SGL chunk into hdwq pool\n * @phba: The HBA for which this call is being executed.\n * @lpfc_buf: IO buf structure with the SGL chunk\n *\n * This routine puts one SGL chunk buffer into hdwq's SGL chunk pool.\n *\n * Return codes:\n *   0 - Success\n *   -EINVAL - Error\n **/\nint\nlpfc_put_sgl_per_hdwq(struct lpfc_hba *phba, struct lpfc_io_buf *lpfc_buf)\n{\n\tint rc = 0;\n\tstruct sli4_hybrid_sgl *list_entry = NULL;\n\tstruct sli4_hybrid_sgl *tmp = NULL;\n\tstruct lpfc_sli4_hdw_queue *hdwq = lpfc_buf->hdwq;\n\tstruct list_head *buf_list = &hdwq->sgl_list;\n\tunsigned long iflags;\n\n\tspin_lock_irqsave(&hdwq->hdwq_lock, iflags);\n\n\tif (likely(!list_empty(&lpfc_buf->dma_sgl_xtra_list))) {\n\t\tlist_for_each_entry_safe(list_entry, tmp,\n\t\t\t\t\t &lpfc_buf->dma_sgl_xtra_list,\n\t\t\t\t\t list_node) {\n\t\t\tlist_move_tail(&list_entry->list_node,\n\t\t\t\t       buf_list);\n\t\t}\n\t} else {\n\t\trc = -EINVAL;\n\t}\n\n\tspin_unlock_irqrestore(&hdwq->hdwq_lock, iflags);\n\treturn rc;\n}\n\n/**\n * lpfc_free_sgl_per_hdwq - Free all SGL chunks of hdwq pool\n * @phba: phba object\n * @hdwq: hdwq to cleanup sgl buff resources on\n *\n * This routine frees all SGL chunks of hdwq SGL chunk pool.\n *\n * Return codes:\n *   None\n **/\nvoid\nlpfc_free_sgl_per_hdwq(struct lpfc_hba *phba,\n\t\t       struct lpfc_sli4_hdw_queue *hdwq)\n{\n\tstruct list_head *buf_list = &hdwq->sgl_list;\n\tstruct sli4_hybrid_sgl *list_entry = NULL;\n\tstruct sli4_hybrid_sgl *tmp = NULL;\n\tunsigned long iflags;\n\n\tspin_lock_irqsave(&hdwq->hdwq_lock, iflags);\n\n\t/* Free sgl pool */\n\tlist_for_each_entry_safe(list_entry, tmp,\n\t\t\t\t buf_list, list_node) {\n\t\tdma_pool_free(phba->lpfc_sg_dma_buf_pool,\n\t\t\t      list_entry->dma_sgl,\n\t\t\t      list_entry->dma_phys_sgl);\n\t\tlist_del(&list_entry->list_node);\n\t\tkfree(list_entry);\n\t}\n\n\tspin_unlock_irqrestore(&hdwq->hdwq_lock, iflags);\n}\n\n/**\n * lpfc_get_cmd_rsp_buf_per_hdwq - Get one CMD/RSP buffer from hdwq\n * @phba: The HBA for which this call is being executed.\n * @lpfc_buf: IO buf structure to attach the CMD/RSP buffer\n *\n * This routine gets one CMD/RSP buffer from hdwq's CMD/RSP pool,\n * and will allocate an CMD/RSP buffer if the pool is empty.\n *\n * Return codes:\n *   NULL - Error\n *   Pointer to fcp_cmd_rsp_buf - Success\n **/\nstruct fcp_cmd_rsp_buf *\nlpfc_get_cmd_rsp_buf_per_hdwq(struct lpfc_hba *phba,\n\t\t\t      struct lpfc_io_buf *lpfc_buf)\n{\n\tstruct fcp_cmd_rsp_buf *list_entry = NULL;\n\tstruct fcp_cmd_rsp_buf *tmp = NULL;\n\tstruct fcp_cmd_rsp_buf *allocated_buf = NULL;\n\tstruct lpfc_sli4_hdw_queue *hdwq = lpfc_buf->hdwq;\n\tstruct list_head *buf_list = &hdwq->cmd_rsp_buf_list;\n\tunsigned long iflags;\n\n\tspin_lock_irqsave(&hdwq->hdwq_lock, iflags);\n\n\tif (likely(!list_empty(buf_list))) {\n\t\t/* break off 1 chunk from the list */\n\t\tlist_for_each_entry_safe(list_entry, tmp,\n\t\t\t\t\t buf_list,\n\t\t\t\t\t list_node) {\n\t\t\tlist_move_tail(&list_entry->list_node,\n\t\t\t\t       &lpfc_buf->dma_cmd_rsp_list);\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\t/* allocate more */\n\t\tspin_unlock_irqrestore(&hdwq->hdwq_lock, iflags);\n\t\ttmp = kmalloc_node(sizeof(*tmp), GFP_ATOMIC,\n\t\t\t\t   cpu_to_node(hdwq->io_wq->chann));\n\t\tif (!tmp) {\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\t\t\"8355 error kmalloc memory for HDWQ \"\n\t\t\t\t\t\"%d %s\\n\",\n\t\t\t\t\tlpfc_buf->hdwq_no, __func__);\n\t\t\treturn NULL;\n\t\t}\n\n\t\ttmp->fcp_cmnd = dma_pool_alloc(phba->lpfc_cmd_rsp_buf_pool,\n\t\t\t\t\t\tGFP_ATOMIC,\n\t\t\t\t\t\t&tmp->fcp_cmd_rsp_dma_handle);\n\n\t\tif (!tmp->fcp_cmnd) {\n\t\t\tlpfc_printf_log(phba, KERN_INFO, LOG_SLI,\n\t\t\t\t\t\"8356 error pool_alloc memory for HDWQ \"\n\t\t\t\t\t\"%d %s\\n\",\n\t\t\t\t\tlpfc_buf->hdwq_no, __func__);\n\t\t\tkfree(tmp);\n\t\t\treturn NULL;\n\t\t}\n\n\t\ttmp->fcp_rsp = (struct fcp_rsp *)((uint8_t *)tmp->fcp_cmnd +\n\t\t\t\tsizeof(struct fcp_cmnd));\n\n\t\tspin_lock_irqsave(&hdwq->hdwq_lock, iflags);\n\t\tlist_add_tail(&tmp->list_node, &lpfc_buf->dma_cmd_rsp_list);\n\t}\n\n\tallocated_buf = list_last_entry(&lpfc_buf->dma_cmd_rsp_list,\n\t\t\t\t\tstruct fcp_cmd_rsp_buf,\n\t\t\t\t\tlist_node);\n\n\tspin_unlock_irqrestore(&hdwq->hdwq_lock, iflags);\n\n\treturn allocated_buf;\n}\n\n/**\n * lpfc_put_cmd_rsp_buf_per_hdwq - Put one CMD/RSP buffer into hdwq pool\n * @phba: The HBA for which this call is being executed.\n * @lpfc_buf: IO buf structure with the CMD/RSP buf\n *\n * This routine puts one CMD/RSP buffer into executing CPU's CMD/RSP pool.\n *\n * Return codes:\n *   0 - Success\n *   -EINVAL - Error\n **/\nint\nlpfc_put_cmd_rsp_buf_per_hdwq(struct lpfc_hba *phba,\n\t\t\t      struct lpfc_io_buf *lpfc_buf)\n{\n\tint rc = 0;\n\tstruct fcp_cmd_rsp_buf *list_entry = NULL;\n\tstruct fcp_cmd_rsp_buf *tmp = NULL;\n\tstruct lpfc_sli4_hdw_queue *hdwq = lpfc_buf->hdwq;\n\tstruct list_head *buf_list = &hdwq->cmd_rsp_buf_list;\n\tunsigned long iflags;\n\n\tspin_lock_irqsave(&hdwq->hdwq_lock, iflags);\n\n\tif (likely(!list_empty(&lpfc_buf->dma_cmd_rsp_list))) {\n\t\tlist_for_each_entry_safe(list_entry, tmp,\n\t\t\t\t\t &lpfc_buf->dma_cmd_rsp_list,\n\t\t\t\t\t list_node) {\n\t\t\tlist_move_tail(&list_entry->list_node,\n\t\t\t\t       buf_list);\n\t\t}\n\t} else {\n\t\trc = -EINVAL;\n\t}\n\n\tspin_unlock_irqrestore(&hdwq->hdwq_lock, iflags);\n\treturn rc;\n}\n\n/**\n * lpfc_free_cmd_rsp_buf_per_hdwq - Free all CMD/RSP chunks of hdwq pool\n * @phba: phba object\n * @hdwq: hdwq to cleanup cmd rsp buff resources on\n *\n * This routine frees all CMD/RSP buffers of hdwq's CMD/RSP buf pool.\n *\n * Return codes:\n *   None\n **/\nvoid\nlpfc_free_cmd_rsp_buf_per_hdwq(struct lpfc_hba *phba,\n\t\t\t       struct lpfc_sli4_hdw_queue *hdwq)\n{\n\tstruct list_head *buf_list = &hdwq->cmd_rsp_buf_list;\n\tstruct fcp_cmd_rsp_buf *list_entry = NULL;\n\tstruct fcp_cmd_rsp_buf *tmp = NULL;\n\tunsigned long iflags;\n\n\tspin_lock_irqsave(&hdwq->hdwq_lock, iflags);\n\n\t/* Free cmd_rsp buf pool */\n\tlist_for_each_entry_safe(list_entry, tmp,\n\t\t\t\t buf_list,\n\t\t\t\t list_node) {\n\t\tdma_pool_free(phba->lpfc_cmd_rsp_buf_pool,\n\t\t\t      list_entry->fcp_cmnd,\n\t\t\t      list_entry->fcp_cmd_rsp_dma_handle);\n\t\tlist_del(&list_entry->list_node);\n\t\tkfree(list_entry);\n\t}\n\n\tspin_unlock_irqrestore(&hdwq->hdwq_lock, iflags);\n}\n"}}, "reports": [{"events": [{"location": {"col": 5, "file": 0, "line": 14109}, "message": "WARNING: Comparison to bool"}], "macros": [], "notes": [], "path": "/src/drivers/scsi/lpfc/lpfc_sli.c", "reportHash": "7279a2bd95af59d75a77e5e8d89a04f5", "checkerName": "coccinelle", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 6, "file": 0, "line": 18007}, "message": "WARNING: Comparison to bool"}], "macros": [], "notes": [], "path": "/src/drivers/scsi/lpfc/lpfc_sli.c", "reportHash": "4b13f47ff520b83c7e675af524d59ad0", "checkerName": "coccinelle", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 9, "file": 0, "line": 14616}, "message": "WARNING: Comparison to bool"}], "macros": [], "notes": [], "path": "/src/drivers/scsi/lpfc/lpfc_sli.c", "reportHash": "2e4375afabf4897104106a12cac45558", "checkerName": "coccinelle", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 5, "file": 0, "line": 17934}, "message": "WARNING: Comparison to bool"}], "macros": [], "notes": [], "path": "/src/drivers/scsi/lpfc/lpfc_sli.c", "reportHash": "8ecc7e6313b2a0c38c1808f9ce1640d3", "checkerName": "coccinelle", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
