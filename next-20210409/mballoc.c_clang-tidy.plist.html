<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"0": {"id": 0, "path": "/src/fs/ext4/mballoc.c", "content": "// SPDX-License-Identifier: GPL-2.0\n/*\n * Copyright (c) 2003-2006, Cluster File Systems, Inc, info@clusterfs.com\n * Written by Alex Tomas <alex@clusterfs.com>\n */\n\n\n/*\n * mballoc.c contains the multiblocks allocation routines\n */\n\n#include \"ext4_jbd2.h\"\n#include \"mballoc.h\"\n#include <linux/log2.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/nospec.h>\n#include <linux/backing-dev.h>\n#include <trace/events/ext4.h>\n\n/*\n * MUSTDO:\n *   - test ext4_ext_search_left() and ext4_ext_search_right()\n *   - search for metadata in few groups\n *\n * TODO v4:\n *   - normalization should take into account whether file is still open\n *   - discard preallocations if no free space left (policy?)\n *   - don't normalize tails\n *   - quota\n *   - reservation for superuser\n *\n * TODO v3:\n *   - bitmap read-ahead (proposed by Oleg Drokin aka green)\n *   - track min/max extents in each group for better group selection\n *   - mb_mark_used() may allocate chunk right after splitting buddy\n *   - tree of groups sorted by number of free blocks\n *   - error handling\n */\n\n/*\n * The allocation request involve request for multiple number of blocks\n * near to the goal(block) value specified.\n *\n * During initialization phase of the allocator we decide to use the\n * group preallocation or inode preallocation depending on the size of\n * the file. The size of the file could be the resulting file size we\n * would have after allocation, or the current file size, which ever\n * is larger. If the size is less than sbi->s_mb_stream_request we\n * select to use the group preallocation. The default value of\n * s_mb_stream_request is 16 blocks. This can also be tuned via\n * /sys/fs/ext4/<partition>/mb_stream_req. The value is represented in\n * terms of number of blocks.\n *\n * The main motivation for having small file use group preallocation is to\n * ensure that we have small files closer together on the disk.\n *\n * First stage the allocator looks at the inode prealloc list,\n * ext4_inode_info->i_prealloc_list, which contains list of prealloc\n * spaces for this particular inode. The inode prealloc space is\n * represented as:\n *\n * pa_lstart -> the logical start block for this prealloc space\n * pa_pstart -> the physical start block for this prealloc space\n * pa_len    -> length for this prealloc space (in clusters)\n * pa_free   ->  free space available in this prealloc space (in clusters)\n *\n * The inode preallocation space is used looking at the _logical_ start\n * block. If only the logical file block falls within the range of prealloc\n * space we will consume the particular prealloc space. This makes sure that\n * we have contiguous physical blocks representing the file blocks\n *\n * The important thing to be noted in case of inode prealloc space is that\n * we don't modify the values associated to inode prealloc space except\n * pa_free.\n *\n * If we are not able to find blocks in the inode prealloc space and if we\n * have the group allocation flag set then we look at the locality group\n * prealloc space. These are per CPU prealloc list represented as\n *\n * ext4_sb_info.s_locality_groups[smp_processor_id()]\n *\n * The reason for having a per cpu locality group is to reduce the contention\n * between CPUs. It is possible to get scheduled at this point.\n *\n * The locality group prealloc space is used looking at whether we have\n * enough free space (pa_free) within the prealloc space.\n *\n * If we can't allocate blocks via inode prealloc or/and locality group\n * prealloc then we look at the buddy cache. The buddy cache is represented\n * by ext4_sb_info.s_buddy_cache (struct inode) whose file offset gets\n * mapped to the buddy and bitmap information regarding different\n * groups. The buddy information is attached to buddy cache inode so that\n * we can access them through the page cache. The information regarding\n * each group is loaded via ext4_mb_load_buddy.  The information involve\n * block bitmap and buddy information. The information are stored in the\n * inode as:\n *\n *  {                        page                        }\n *  [ group 0 bitmap][ group 0 buddy] [group 1][ group 1]...\n *\n *\n * one block each for bitmap and buddy information.  So for each group we\n * take up 2 blocks. A page can contain blocks_per_page (PAGE_SIZE /\n * blocksize) blocks.  So it can have information regarding groups_per_page\n * which is blocks_per_page/2\n *\n * The buddy cache inode is not stored on disk. The inode is thrown\n * away when the filesystem is unmounted.\n *\n * We look for count number of blocks in the buddy cache. If we were able\n * to locate that many free blocks we return with additional information\n * regarding rest of the contiguous physical block available\n *\n * Before allocating blocks via buddy cache we normalize the request\n * blocks. This ensure we ask for more blocks that we needed. The extra\n * blocks that we get after allocation is added to the respective prealloc\n * list. In case of inode preallocation we follow a list of heuristics\n * based on file size. This can be found in ext4_mb_normalize_request. If\n * we are doing a group prealloc we try to normalize the request to\n * sbi->s_mb_group_prealloc.  The default value of s_mb_group_prealloc is\n * dependent on the cluster size; for non-bigalloc file systems, it is\n * 512 blocks. This can be tuned via\n * /sys/fs/ext4/<partition>/mb_group_prealloc. The value is represented in\n * terms of number of blocks. If we have mounted the file system with -O\n * stripe=<value> option the group prealloc request is normalized to the\n * smallest multiple of the stripe value (sbi->s_stripe) which is\n * greater than the default mb_group_prealloc.\n *\n * The regular allocator (using the buddy cache) supports a few tunables.\n *\n * /sys/fs/ext4/<partition>/mb_min_to_scan\n * /sys/fs/ext4/<partition>/mb_max_to_scan\n * /sys/fs/ext4/<partition>/mb_order2_req\n *\n * The regular allocator uses buddy scan only if the request len is power of\n * 2 blocks and the order of allocation is >= sbi->s_mb_order2_reqs. The\n * value of s_mb_order2_reqs can be tuned via\n * /sys/fs/ext4/<partition>/mb_order2_req.  If the request len is equal to\n * stripe size (sbi->s_stripe), we try to search for contiguous block in\n * stripe size. This should result in better allocation on RAID setups. If\n * not, we search in the specific group using bitmap for best extents. The\n * tunable min_to_scan and max_to_scan control the behaviour here.\n * min_to_scan indicate how long the mballoc __must__ look for a best\n * extent and max_to_scan indicates how long the mballoc __can__ look for a\n * best extent in the found extents. Searching for the blocks starts with\n * the group specified as the goal value in allocation context via\n * ac_g_ex. Each group is first checked based on the criteria whether it\n * can be used for allocation. ext4_mb_good_group explains how the groups are\n * checked.\n *\n * Both the prealloc space are getting populated as above. So for the first\n * request we will hit the buddy cache which will result in this prealloc\n * space getting filled. The prealloc space is then later used for the\n * subsequent request.\n */\n\n/*\n * mballoc operates on the following data:\n *  - on-disk bitmap\n *  - in-core buddy (actually includes buddy and bitmap)\n *  - preallocation descriptors (PAs)\n *\n * there are two types of preallocations:\n *  - inode\n *    assiged to specific inode and can be used for this inode only.\n *    it describes part of inode's space preallocated to specific\n *    physical blocks. any block from that preallocated can be used\n *    independent. the descriptor just tracks number of blocks left\n *    unused. so, before taking some block from descriptor, one must\n *    make sure corresponded logical block isn't allocated yet. this\n *    also means that freeing any block within descriptor's range\n *    must discard all preallocated blocks.\n *  - locality group\n *    assigned to specific locality group which does not translate to\n *    permanent set of inodes: inode can join and leave group. space\n *    from this type of preallocation can be used for any inode. thus\n *    it's consumed from the beginning to the end.\n *\n * relation between them can be expressed as:\n *    in-core buddy = on-disk bitmap + preallocation descriptors\n *\n * this mean blocks mballoc considers used are:\n *  - allocated blocks (persistent)\n *  - preallocated blocks (non-persistent)\n *\n * consistency in mballoc world means that at any time a block is either\n * free or used in ALL structures. notice: \"any time\" should not be read\n * literally -- time is discrete and delimited by locks.\n *\n *  to keep it simple, we don't use block numbers, instead we count number of\n *  blocks: how many blocks marked used/free in on-disk bitmap, buddy and PA.\n *\n * all operations can be expressed as:\n *  - init buddy:\t\t\tbuddy = on-disk + PAs\n *  - new PA:\t\t\t\tbuddy += N; PA = N\n *  - use inode PA:\t\t\ton-disk += N; PA -= N\n *  - discard inode PA\t\t\tbuddy -= on-disk - PA; PA = 0\n *  - use locality group PA\t\ton-disk += N; PA -= N\n *  - discard locality group PA\t\tbuddy -= PA; PA = 0\n *  note: 'buddy -= on-disk - PA' is used to show that on-disk bitmap\n *        is used in real operation because we can't know actual used\n *        bits from PA, only from on-disk bitmap\n *\n * if we follow this strict logic, then all operations above should be atomic.\n * given some of them can block, we'd have to use something like semaphores\n * killing performance on high-end SMP hardware. let's try to relax it using\n * the following knowledge:\n *  1) if buddy is referenced, it's already initialized\n *  2) while block is used in buddy and the buddy is referenced,\n *     nobody can re-allocate that block\n *  3) we work on bitmaps and '+' actually means 'set bits'. if on-disk has\n *     bit set and PA claims same block, it's OK. IOW, one can set bit in\n *     on-disk bitmap if buddy has same bit set or/and PA covers corresponded\n *     block\n *\n * so, now we're building a concurrency table:\n *  - init buddy vs.\n *    - new PA\n *      blocks for PA are allocated in the buddy, buddy must be referenced\n *      until PA is linked to allocation group to avoid concurrent buddy init\n *    - use inode PA\n *      we need to make sure that either on-disk bitmap or PA has uptodate data\n *      given (3) we care that PA-=N operation doesn't interfere with init\n *    - discard inode PA\n *      the simplest way would be to have buddy initialized by the discard\n *    - use locality group PA\n *      again PA-=N must be serialized with init\n *    - discard locality group PA\n *      the simplest way would be to have buddy initialized by the discard\n *  - new PA vs.\n *    - use inode PA\n *      i_data_sem serializes them\n *    - discard inode PA\n *      discard process must wait until PA isn't used by another process\n *    - use locality group PA\n *      some mutex should serialize them\n *    - discard locality group PA\n *      discard process must wait until PA isn't used by another process\n *  - use inode PA\n *    - use inode PA\n *      i_data_sem or another mutex should serializes them\n *    - discard inode PA\n *      discard process must wait until PA isn't used by another process\n *    - use locality group PA\n *      nothing wrong here -- they're different PAs covering different blocks\n *    - discard locality group PA\n *      discard process must wait until PA isn't used by another process\n *\n * now we're ready to make few consequences:\n *  - PA is referenced and while it is no discard is possible\n *  - PA is referenced until block isn't marked in on-disk bitmap\n *  - PA changes only after on-disk bitmap\n *  - discard must not compete with init. either init is done before\n *    any discard or they're serialized somehow\n *  - buddy init as sum of on-disk bitmap and PAs is done atomically\n *\n * a special case when we've used PA to emptiness. no need to modify buddy\n * in this case, but we should care about concurrent init\n *\n */\n\n /*\n * Logic in few words:\n *\n *  - allocation:\n *    load group\n *    find blocks\n *    mark bits in on-disk bitmap\n *    release group\n *\n *  - use preallocation:\n *    find proper PA (per-inode or group)\n *    load group\n *    mark bits in on-disk bitmap\n *    release group\n *    release PA\n *\n *  - free:\n *    load group\n *    mark bits in on-disk bitmap\n *    release group\n *\n *  - discard preallocations in group:\n *    mark PAs deleted\n *    move them onto local list\n *    load on-disk bitmap\n *    load group\n *    remove PA from object (inode or locality group)\n *    mark free blocks in-core\n *\n *  - discard inode's preallocations:\n */\n\n/*\n * Locking rules\n *\n * Locks:\n *  - bitlock on a group\t(group)\n *  - object (inode/locality)\t(object)\n *  - per-pa lock\t\t(pa)\n *\n * Paths:\n *  - new pa\n *    object\n *    group\n *\n *  - find and use pa:\n *    pa\n *\n *  - release consumed pa:\n *    pa\n *    group\n *    object\n *\n *  - generate in-core bitmap:\n *    group\n *        pa\n *\n *  - discard all for given object (inode, locality group):\n *    object\n *        pa\n *    group\n *\n *  - discard all for given group:\n *    group\n *        pa\n *    group\n *        object\n *\n */\nstatic struct kmem_cache *ext4_pspace_cachep;\nstatic struct kmem_cache *ext4_ac_cachep;\nstatic struct kmem_cache *ext4_free_data_cachep;\n\n/* We create slab caches for groupinfo data structures based on the\n * superblock block size.  There will be one per mounted filesystem for\n * each unique s_blocksize_bits */\n#define NR_GRPINFO_CACHES 8\nstatic struct kmem_cache *ext4_groupinfo_caches[NR_GRPINFO_CACHES];\n\nstatic const char * const ext4_groupinfo_slab_names[NR_GRPINFO_CACHES] = {\n\t\"ext4_groupinfo_1k\", \"ext4_groupinfo_2k\", \"ext4_groupinfo_4k\",\n\t\"ext4_groupinfo_8k\", \"ext4_groupinfo_16k\", \"ext4_groupinfo_32k\",\n\t\"ext4_groupinfo_64k\", \"ext4_groupinfo_128k\"\n};\n\nstatic void ext4_mb_generate_from_pa(struct super_block *sb, void *bitmap,\n\t\t\t\t\text4_group_t group);\nstatic void ext4_mb_generate_from_freelist(struct super_block *sb, void *bitmap,\n\t\t\t\t\t\text4_group_t group);\nstatic void ext4_mb_new_preallocation(struct ext4_allocation_context *ac);\n\n/*\n * The algorithm using this percpu seq counter goes below:\n * 1. We sample the percpu discard_pa_seq counter before trying for block\n *    allocation in ext4_mb_new_blocks().\n * 2. We increment this percpu discard_pa_seq counter when we either allocate\n *    or free these blocks i.e. while marking those blocks as used/free in\n *    mb_mark_used()/mb_free_blocks().\n * 3. We also increment this percpu seq counter when we successfully identify\n *    that the bb_prealloc_list is not empty and hence proceed for discarding\n *    of those PAs inside ext4_mb_discard_group_preallocations().\n *\n * Now to make sure that the regular fast path of block allocation is not\n * affected, as a small optimization we only sample the percpu seq counter\n * on that cpu. Only when the block allocation fails and when freed blocks\n * found were 0, that is when we sample percpu seq counter for all cpus using\n * below function ext4_get_discard_pa_seq_sum(). This happens after making\n * sure that all the PAs on grp->bb_prealloc_list got freed or if it's empty.\n */\nstatic DEFINE_PER_CPU(u64, discard_pa_seq);\nstatic inline u64 ext4_get_discard_pa_seq_sum(void)\n{\n\tint __cpu;\n\tu64 __seq = 0;\n\n\tfor_each_possible_cpu(__cpu)\n\t\t__seq += per_cpu(discard_pa_seq, __cpu);\n\treturn __seq;\n}\n\nstatic inline void *mb_correct_addr_and_bit(int *bit, void *addr)\n{\n#if BITS_PER_LONG == 64\n\t*bit += ((unsigned long) addr & 7UL) << 3;\n\taddr = (void *) ((unsigned long) addr & ~7UL);\n#elif BITS_PER_LONG == 32\n\t*bit += ((unsigned long) addr & 3UL) << 3;\n\taddr = (void *) ((unsigned long) addr & ~3UL);\n#else\n#error \"how many bits you are?!\"\n#endif\n\treturn addr;\n}\n\nstatic inline int mb_test_bit(int bit, void *addr)\n{\n\t/*\n\t * ext4_test_bit on architecture like powerpc\n\t * needs unsigned long aligned address\n\t */\n\taddr = mb_correct_addr_and_bit(&bit, addr);\n\treturn ext4_test_bit(bit, addr);\n}\n\nstatic inline void mb_set_bit(int bit, void *addr)\n{\n\taddr = mb_correct_addr_and_bit(&bit, addr);\n\text4_set_bit(bit, addr);\n}\n\nstatic inline void mb_clear_bit(int bit, void *addr)\n{\n\taddr = mb_correct_addr_and_bit(&bit, addr);\n\text4_clear_bit(bit, addr);\n}\n\nstatic inline int mb_test_and_clear_bit(int bit, void *addr)\n{\n\taddr = mb_correct_addr_and_bit(&bit, addr);\n\treturn ext4_test_and_clear_bit(bit, addr);\n}\n\nstatic inline int mb_find_next_zero_bit(void *addr, int max, int start)\n{\n\tint fix = 0, ret, tmpmax;\n\taddr = mb_correct_addr_and_bit(&fix, addr);\n\ttmpmax = max + fix;\n\tstart += fix;\n\n\tret = ext4_find_next_zero_bit(addr, tmpmax, start) - fix;\n\tif (ret > max)\n\t\treturn max;\n\treturn ret;\n}\n\nstatic inline int mb_find_next_bit(void *addr, int max, int start)\n{\n\tint fix = 0, ret, tmpmax;\n\taddr = mb_correct_addr_and_bit(&fix, addr);\n\ttmpmax = max + fix;\n\tstart += fix;\n\n\tret = ext4_find_next_bit(addr, tmpmax, start) - fix;\n\tif (ret > max)\n\t\treturn max;\n\treturn ret;\n}\n\nstatic void *mb_find_buddy(struct ext4_buddy *e4b, int order, int *max)\n{\n\tchar *bb;\n\n\tBUG_ON(e4b->bd_bitmap == e4b->bd_buddy);\n\tBUG_ON(max == NULL);\n\n\tif (order > e4b->bd_blkbits + 1) {\n\t\t*max = 0;\n\t\treturn NULL;\n\t}\n\n\t/* at order 0 we see each particular block */\n\tif (order == 0) {\n\t\t*max = 1 << (e4b->bd_blkbits + 3);\n\t\treturn e4b->bd_bitmap;\n\t}\n\n\tbb = e4b->bd_buddy + EXT4_SB(e4b->bd_sb)->s_mb_offsets[order];\n\t*max = EXT4_SB(e4b->bd_sb)->s_mb_maxs[order];\n\n\treturn bb;\n}\n\n#ifdef DOUBLE_CHECK\nstatic void mb_free_blocks_double(struct inode *inode, struct ext4_buddy *e4b,\n\t\t\t   int first, int count)\n{\n\tint i;\n\tstruct super_block *sb = e4b->bd_sb;\n\n\tif (unlikely(e4b->bd_info->bb_bitmap == NULL))\n\t\treturn;\n\tassert_spin_locked(ext4_group_lock_ptr(sb, e4b->bd_group));\n\tfor (i = 0; i < count; i++) {\n\t\tif (!mb_test_bit(first + i, e4b->bd_info->bb_bitmap)) {\n\t\t\text4_fsblk_t blocknr;\n\n\t\t\tblocknr = ext4_group_first_block_no(sb, e4b->bd_group);\n\t\t\tblocknr += EXT4_C2B(EXT4_SB(sb), first + i);\n\t\t\text4_grp_locked_error(sb, e4b->bd_group,\n\t\t\t\t\t      inode ? inode->i_ino : 0,\n\t\t\t\t\t      blocknr,\n\t\t\t\t\t      \"freeing block already freed \"\n\t\t\t\t\t      \"(bit %u)\",\n\t\t\t\t\t      first + i);\n\t\t\text4_mark_group_bitmap_corrupted(sb, e4b->bd_group,\n\t\t\t\t\tEXT4_GROUP_INFO_BBITMAP_CORRUPT);\n\t\t}\n\t\tmb_clear_bit(first + i, e4b->bd_info->bb_bitmap);\n\t}\n}\n\nstatic void mb_mark_used_double(struct ext4_buddy *e4b, int first, int count)\n{\n\tint i;\n\n\tif (unlikely(e4b->bd_info->bb_bitmap == NULL))\n\t\treturn;\n\tassert_spin_locked(ext4_group_lock_ptr(e4b->bd_sb, e4b->bd_group));\n\tfor (i = 0; i < count; i++) {\n\t\tBUG_ON(mb_test_bit(first + i, e4b->bd_info->bb_bitmap));\n\t\tmb_set_bit(first + i, e4b->bd_info->bb_bitmap);\n\t}\n}\n\nstatic void mb_cmp_bitmaps(struct ext4_buddy *e4b, void *bitmap)\n{\n\tif (unlikely(e4b->bd_info->bb_bitmap == NULL))\n\t\treturn;\n\tif (memcmp(e4b->bd_info->bb_bitmap, bitmap, e4b->bd_sb->s_blocksize)) {\n\t\tunsigned char *b1, *b2;\n\t\tint i;\n\t\tb1 = (unsigned char *) e4b->bd_info->bb_bitmap;\n\t\tb2 = (unsigned char *) bitmap;\n\t\tfor (i = 0; i < e4b->bd_sb->s_blocksize; i++) {\n\t\t\tif (b1[i] != b2[i]) {\n\t\t\t\text4_msg(e4b->bd_sb, KERN_ERR,\n\t\t\t\t\t \"corruption in group %u \"\n\t\t\t\t\t \"at byte %u(%u): %x in copy != %x \"\n\t\t\t\t\t \"on disk/prealloc\",\n\t\t\t\t\t e4b->bd_group, i, i * 8, b1[i], b2[i]);\n\t\t\t\tBUG();\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void mb_group_bb_bitmap_alloc(struct super_block *sb,\n\t\t\tstruct ext4_group_info *grp, ext4_group_t group)\n{\n\tstruct buffer_head *bh;\n\n\tgrp->bb_bitmap = kmalloc(sb->s_blocksize, GFP_NOFS);\n\tif (!grp->bb_bitmap)\n\t\treturn;\n\n\tbh = ext4_read_block_bitmap(sb, group);\n\tif (IS_ERR_OR_NULL(bh)) {\n\t\tkfree(grp->bb_bitmap);\n\t\tgrp->bb_bitmap = NULL;\n\t\treturn;\n\t}\n\n\tmemcpy(grp->bb_bitmap, bh->b_data, sb->s_blocksize);\n\tput_bh(bh);\n}\n\nstatic void mb_group_bb_bitmap_free(struct ext4_group_info *grp)\n{\n\tkfree(grp->bb_bitmap);\n}\n\n#else\nstatic inline void mb_free_blocks_double(struct inode *inode,\n\t\t\t\tstruct ext4_buddy *e4b, int first, int count)\n{\n\treturn;\n}\nstatic inline void mb_mark_used_double(struct ext4_buddy *e4b,\n\t\t\t\t\t\tint first, int count)\n{\n\treturn;\n}\nstatic inline void mb_cmp_bitmaps(struct ext4_buddy *e4b, void *bitmap)\n{\n\treturn;\n}\n\nstatic inline void mb_group_bb_bitmap_alloc(struct super_block *sb,\n\t\t\tstruct ext4_group_info *grp, ext4_group_t group)\n{\n\treturn;\n}\n\nstatic inline void mb_group_bb_bitmap_free(struct ext4_group_info *grp)\n{\n\treturn;\n}\n#endif\n\n#ifdef AGGRESSIVE_CHECK\n\n#define MB_CHECK_ASSERT(assert)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (!(assert)) {\t\t\t\t\t\t\\\n\t\tprintk(KERN_EMERG\t\t\t\t\t\\\n\t\t\t\"Assertion failure in %s() at %s:%d: \\\"%s\\\"\\n\",\t\\\n\t\t\tfunction, file, line, # assert);\t\t\\\n\t\tBUG();\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\nstatic int __mb_check_buddy(struct ext4_buddy *e4b, char *file,\n\t\t\t\tconst char *function, int line)\n{\n\tstruct super_block *sb = e4b->bd_sb;\n\tint order = e4b->bd_blkbits + 1;\n\tint max;\n\tint max2;\n\tint i;\n\tint j;\n\tint k;\n\tint count;\n\tstruct ext4_group_info *grp;\n\tint fragments = 0;\n\tint fstart;\n\tstruct list_head *cur;\n\tvoid *buddy;\n\tvoid *buddy2;\n\n\tif (e4b->bd_info->bb_check_counter++ % 10)\n\t\treturn 0;\n\n\twhile (order > 1) {\n\t\tbuddy = mb_find_buddy(e4b, order, &max);\n\t\tMB_CHECK_ASSERT(buddy);\n\t\tbuddy2 = mb_find_buddy(e4b, order - 1, &max2);\n\t\tMB_CHECK_ASSERT(buddy2);\n\t\tMB_CHECK_ASSERT(buddy != buddy2);\n\t\tMB_CHECK_ASSERT(max * 2 == max2);\n\n\t\tcount = 0;\n\t\tfor (i = 0; i < max; i++) {\n\n\t\t\tif (mb_test_bit(i, buddy)) {\n\t\t\t\t/* only single bit in buddy2 may be 1 */\n\t\t\t\tif (!mb_test_bit(i << 1, buddy2)) {\n\t\t\t\t\tMB_CHECK_ASSERT(\n\t\t\t\t\t\tmb_test_bit((i<<1)+1, buddy2));\n\t\t\t\t} else if (!mb_test_bit((i << 1) + 1, buddy2)) {\n\t\t\t\t\tMB_CHECK_ASSERT(\n\t\t\t\t\t\tmb_test_bit(i << 1, buddy2));\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* both bits in buddy2 must be 1 */\n\t\t\tMB_CHECK_ASSERT(mb_test_bit(i << 1, buddy2));\n\t\t\tMB_CHECK_ASSERT(mb_test_bit((i << 1) + 1, buddy2));\n\n\t\t\tfor (j = 0; j < (1 << order); j++) {\n\t\t\t\tk = (i * (1 << order)) + j;\n\t\t\t\tMB_CHECK_ASSERT(\n\t\t\t\t\t!mb_test_bit(k, e4b->bd_bitmap));\n\t\t\t}\n\t\t\tcount++;\n\t\t}\n\t\tMB_CHECK_ASSERT(e4b->bd_info->bb_counters[order] == count);\n\t\torder--;\n\t}\n\n\tfstart = -1;\n\tbuddy = mb_find_buddy(e4b, 0, &max);\n\tfor (i = 0; i < max; i++) {\n\t\tif (!mb_test_bit(i, buddy)) {\n\t\t\tMB_CHECK_ASSERT(i >= e4b->bd_info->bb_first_free);\n\t\t\tif (fstart == -1) {\n\t\t\t\tfragments++;\n\t\t\t\tfstart = i;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\tfstart = -1;\n\t\t/* check used bits only */\n\t\tfor (j = 0; j < e4b->bd_blkbits + 1; j++) {\n\t\t\tbuddy2 = mb_find_buddy(e4b, j, &max2);\n\t\t\tk = i >> j;\n\t\t\tMB_CHECK_ASSERT(k < max2);\n\t\t\tMB_CHECK_ASSERT(mb_test_bit(k, buddy2));\n\t\t}\n\t}\n\tMB_CHECK_ASSERT(!EXT4_MB_GRP_NEED_INIT(e4b->bd_info));\n\tMB_CHECK_ASSERT(e4b->bd_info->bb_fragments == fragments);\n\n\tgrp = ext4_get_group_info(sb, e4b->bd_group);\n\tlist_for_each(cur, &grp->bb_prealloc_list) {\n\t\text4_group_t groupnr;\n\t\tstruct ext4_prealloc_space *pa;\n\t\tpa = list_entry(cur, struct ext4_prealloc_space, pa_group_list);\n\t\text4_get_group_no_and_offset(sb, pa->pa_pstart, &groupnr, &k);\n\t\tMB_CHECK_ASSERT(groupnr == e4b->bd_group);\n\t\tfor (i = 0; i < pa->pa_len; i++)\n\t\t\tMB_CHECK_ASSERT(mb_test_bit(k + i, buddy));\n\t}\n\treturn 0;\n}\n#undef MB_CHECK_ASSERT\n#define mb_check_buddy(e4b) __mb_check_buddy(e4b,\t\\\n\t\t\t\t\t__FILE__, __func__, __LINE__)\n#else\n#define mb_check_buddy(e4b)\n#endif\n\n/*\n * Divide blocks started from @first with length @len into\n * smaller chunks with power of 2 blocks.\n * Clear the bits in bitmap which the blocks of the chunk(s) covered,\n * then increase bb_counters[] for corresponded chunk size.\n */\nstatic void ext4_mb_mark_free_simple(struct super_block *sb,\n\t\t\t\tvoid *buddy, ext4_grpblk_t first, ext4_grpblk_t len,\n\t\t\t\t\tstruct ext4_group_info *grp)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_grpblk_t min;\n\text4_grpblk_t max;\n\text4_grpblk_t chunk;\n\tunsigned int border;\n\n\tBUG_ON(len > EXT4_CLUSTERS_PER_GROUP(sb));\n\n\tborder = 2 << sb->s_blocksize_bits;\n\n\twhile (len > 0) {\n\t\t/* find how many blocks can be covered since this position */\n\t\tmax = ffs(first | border) - 1;\n\n\t\t/* find how many blocks of power 2 we need to mark */\n\t\tmin = fls(len) - 1;\n\n\t\tif (max < min)\n\t\t\tmin = max;\n\t\tchunk = 1 << min;\n\n\t\t/* mark multiblock chunks only */\n\t\tgrp->bb_counters[min]++;\n\t\tif (min > 0)\n\t\t\tmb_clear_bit(first >> min,\n\t\t\t\t     buddy + sbi->s_mb_offsets[min]);\n\n\t\tlen -= chunk;\n\t\tfirst += chunk;\n\t}\n}\n\n/*\n * Cache the order of the largest free extent we have available in this block\n * group.\n */\nstatic void\nmb_set_largest_free_order(struct super_block *sb, struct ext4_group_info *grp)\n{\n\tint i;\n\tint bits;\n\n\tgrp->bb_largest_free_order = -1; /* uninit */\n\n\tbits = sb->s_blocksize_bits + 1;\n\tfor (i = bits; i >= 0; i--) {\n\t\tif (grp->bb_counters[i] > 0) {\n\t\t\tgrp->bb_largest_free_order = i;\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic noinline_for_stack\nvoid ext4_mb_generate_buddy(struct super_block *sb,\n\t\t\t\tvoid *buddy, void *bitmap, ext4_group_t group)\n{\n\tstruct ext4_group_info *grp = ext4_get_group_info(sb, group);\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_grpblk_t max = EXT4_CLUSTERS_PER_GROUP(sb);\n\text4_grpblk_t i = 0;\n\text4_grpblk_t first;\n\text4_grpblk_t len;\n\tunsigned free = 0;\n\tunsigned fragments = 0;\n\tunsigned long long period = get_cycles();\n\n\t/* initialize buddy from bitmap which is aggregation\n\t * of on-disk bitmap and preallocations */\n\ti = mb_find_next_zero_bit(bitmap, max, 0);\n\tgrp->bb_first_free = i;\n\twhile (i < max) {\n\t\tfragments++;\n\t\tfirst = i;\n\t\ti = mb_find_next_bit(bitmap, max, i);\n\t\tlen = i - first;\n\t\tfree += len;\n\t\tif (len > 1)\n\t\t\text4_mb_mark_free_simple(sb, buddy, first, len, grp);\n\t\telse\n\t\t\tgrp->bb_counters[0]++;\n\t\tif (i < max)\n\t\t\ti = mb_find_next_zero_bit(bitmap, max, i);\n\t}\n\tgrp->bb_fragments = fragments;\n\n\tif (free != grp->bb_free) {\n\t\text4_grp_locked_error(sb, group, 0, 0,\n\t\t\t\t      \"block bitmap and bg descriptor \"\n\t\t\t\t      \"inconsistent: %u vs %u free clusters\",\n\t\t\t\t      free, grp->bb_free);\n\t\t/*\n\t\t * If we intend to continue, we consider group descriptor\n\t\t * corrupt and update bb_free using bitmap value\n\t\t */\n\t\tgrp->bb_free = free;\n\t\text4_mark_group_bitmap_corrupted(sb, group,\n\t\t\t\t\tEXT4_GROUP_INFO_BBITMAP_CORRUPT);\n\t}\n\tmb_set_largest_free_order(sb, grp);\n\n\tclear_bit(EXT4_GROUP_INFO_NEED_INIT_BIT, &(grp->bb_state));\n\n\tperiod = get_cycles() - period;\n\tspin_lock(&sbi->s_bal_lock);\n\tsbi->s_mb_buddies_generated++;\n\tsbi->s_mb_generation_time += period;\n\tspin_unlock(&sbi->s_bal_lock);\n}\n\n/* The buddy information is attached the buddy cache inode\n * for convenience. The information regarding each group\n * is loaded via ext4_mb_load_buddy. The information involve\n * block bitmap and buddy information. The information are\n * stored in the inode as\n *\n * {                        page                        }\n * [ group 0 bitmap][ group 0 buddy] [group 1][ group 1]...\n *\n *\n * one block each for bitmap and buddy information.\n * So for each group we take up 2 blocks. A page can\n * contain blocks_per_page (PAGE_SIZE / blocksize)  blocks.\n * So it can have information regarding groups_per_page which\n * is blocks_per_page/2\n *\n * Locking note:  This routine takes the block group lock of all groups\n * for this page; do not hold this lock when calling this routine!\n */\n\nstatic int ext4_mb_init_cache(struct page *page, char *incore, gfp_t gfp)\n{\n\text4_group_t ngroups;\n\tint blocksize;\n\tint blocks_per_page;\n\tint groups_per_page;\n\tint err = 0;\n\tint i;\n\text4_group_t first_group, group;\n\tint first_block;\n\tstruct super_block *sb;\n\tstruct buffer_head *bhs;\n\tstruct buffer_head **bh = NULL;\n\tstruct inode *inode;\n\tchar *data;\n\tchar *bitmap;\n\tstruct ext4_group_info *grinfo;\n\n\tinode = page->mapping->host;\n\tsb = inode->i_sb;\n\tngroups = ext4_get_groups_count(sb);\n\tblocksize = i_blocksize(inode);\n\tblocks_per_page = PAGE_SIZE / blocksize;\n\n\tmb_debug(sb, \"init page %lu\\n\", page->index);\n\n\tgroups_per_page = blocks_per_page >> 1;\n\tif (groups_per_page == 0)\n\t\tgroups_per_page = 1;\n\n\t/* allocate buffer_heads to read bitmaps */\n\tif (groups_per_page > 1) {\n\t\ti = sizeof(struct buffer_head *) * groups_per_page;\n\t\tbh = kzalloc(i, gfp);\n\t\tif (bh == NULL) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t} else\n\t\tbh = &bhs;\n\n\tfirst_group = page->index * blocks_per_page / 2;\n\n\t/* read all groups the page covers into the cache */\n\tfor (i = 0, group = first_group; i < groups_per_page; i++, group++) {\n\t\tif (group >= ngroups)\n\t\t\tbreak;\n\n\t\tgrinfo = ext4_get_group_info(sb, group);\n\t\t/*\n\t\t * If page is uptodate then we came here after online resize\n\t\t * which added some new uninitialized group info structs, so\n\t\t * we must skip all initialized uptodate buddies on the page,\n\t\t * which may be currently in use by an allocating task.\n\t\t */\n\t\tif (PageUptodate(page) && !EXT4_MB_GRP_NEED_INIT(grinfo)) {\n\t\t\tbh[i] = NULL;\n\t\t\tcontinue;\n\t\t}\n\t\tbh[i] = ext4_read_block_bitmap_nowait(sb, group, false);\n\t\tif (IS_ERR(bh[i])) {\n\t\t\terr = PTR_ERR(bh[i]);\n\t\t\tbh[i] = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tmb_debug(sb, \"read bitmap for group %u\\n\", group);\n\t}\n\n\t/* wait for I/O completion */\n\tfor (i = 0, group = first_group; i < groups_per_page; i++, group++) {\n\t\tint err2;\n\n\t\tif (!bh[i])\n\t\t\tcontinue;\n\t\terr2 = ext4_wait_block_bitmap(sb, group, bh[i]);\n\t\tif (!err)\n\t\t\terr = err2;\n\t}\n\n\tfirst_block = page->index * blocks_per_page;\n\tfor (i = 0; i < blocks_per_page; i++) {\n\t\tgroup = (first_block + i) >> 1;\n\t\tif (group >= ngroups)\n\t\t\tbreak;\n\n\t\tif (!bh[group - first_group])\n\t\t\t/* skip initialized uptodate buddy */\n\t\t\tcontinue;\n\n\t\tif (!buffer_verified(bh[group - first_group]))\n\t\t\t/* Skip faulty bitmaps */\n\t\t\tcontinue;\n\t\terr = 0;\n\n\t\t/*\n\t\t * data carry information regarding this\n\t\t * particular group in the format specified\n\t\t * above\n\t\t *\n\t\t */\n\t\tdata = page_address(page) + (i * blocksize);\n\t\tbitmap = bh[group - first_group]->b_data;\n\n\t\t/*\n\t\t * We place the buddy block and bitmap block\n\t\t * close together\n\t\t */\n\t\tif ((first_block + i) & 1) {\n\t\t\t/* this is block of buddy */\n\t\t\tBUG_ON(incore == NULL);\n\t\t\tmb_debug(sb, \"put buddy for group %u in page %lu/%x\\n\",\n\t\t\t\tgroup, page->index, i * blocksize);\n\t\t\ttrace_ext4_mb_buddy_bitmap_load(sb, group);\n\t\t\tgrinfo = ext4_get_group_info(sb, group);\n\t\t\tgrinfo->bb_fragments = 0;\n\t\t\tmemset(grinfo->bb_counters, 0,\n\t\t\t       sizeof(*grinfo->bb_counters) *\n\t\t\t\t(sb->s_blocksize_bits+2));\n\t\t\t/*\n\t\t\t * incore got set to the group block bitmap below\n\t\t\t */\n\t\t\text4_lock_group(sb, group);\n\t\t\t/* init the buddy */\n\t\t\tmemset(data, 0xff, blocksize);\n\t\t\text4_mb_generate_buddy(sb, data, incore, group);\n\t\t\text4_unlock_group(sb, group);\n\t\t\tincore = NULL;\n\t\t} else {\n\t\t\t/* this is block of bitmap */\n\t\t\tBUG_ON(incore != NULL);\n\t\t\tmb_debug(sb, \"put bitmap for group %u in page %lu/%x\\n\",\n\t\t\t\tgroup, page->index, i * blocksize);\n\t\t\ttrace_ext4_mb_bitmap_load(sb, group);\n\n\t\t\t/* see comments in ext4_mb_put_pa() */\n\t\t\text4_lock_group(sb, group);\n\t\t\tmemcpy(data, bitmap, blocksize);\n\n\t\t\t/* mark all preallocated blks used in in-core bitmap */\n\t\t\text4_mb_generate_from_pa(sb, data, group);\n\t\t\text4_mb_generate_from_freelist(sb, data, group);\n\t\t\text4_unlock_group(sb, group);\n\n\t\t\t/* set incore so that the buddy information can be\n\t\t\t * generated using this\n\t\t\t */\n\t\t\tincore = data;\n\t\t}\n\t}\n\tSetPageUptodate(page);\n\nout:\n\tif (bh) {\n\t\tfor (i = 0; i < groups_per_page; i++)\n\t\t\tbrelse(bh[i]);\n\t\tif (bh != &bhs)\n\t\t\tkfree(bh);\n\t}\n\treturn err;\n}\n\n/*\n * Lock the buddy and bitmap pages. This make sure other parallel init_group\n * on the same buddy page doesn't happen whild holding the buddy page lock.\n * Return locked buddy and bitmap pages on e4b struct. If buddy and bitmap\n * are on the same page e4b->bd_buddy_page is NULL and return value is 0.\n */\nstatic int ext4_mb_get_buddy_page_lock(struct super_block *sb,\n\t\text4_group_t group, struct ext4_buddy *e4b, gfp_t gfp)\n{\n\tstruct inode *inode = EXT4_SB(sb)->s_buddy_cache;\n\tint block, pnum, poff;\n\tint blocks_per_page;\n\tstruct page *page;\n\n\te4b->bd_buddy_page = NULL;\n\te4b->bd_bitmap_page = NULL;\n\n\tblocks_per_page = PAGE_SIZE / sb->s_blocksize;\n\t/*\n\t * the buddy cache inode stores the block bitmap\n\t * and buddy information in consecutive blocks.\n\t * So for each group we need two blocks.\n\t */\n\tblock = group * 2;\n\tpnum = block / blocks_per_page;\n\tpoff = block % blocks_per_page;\n\tpage = find_or_create_page(inode->i_mapping, pnum, gfp);\n\tif (!page)\n\t\treturn -ENOMEM;\n\tBUG_ON(page->mapping != inode->i_mapping);\n\te4b->bd_bitmap_page = page;\n\te4b->bd_bitmap = page_address(page) + (poff * sb->s_blocksize);\n\n\tif (blocks_per_page >= 2) {\n\t\t/* buddy and bitmap are on the same page */\n\t\treturn 0;\n\t}\n\n\tblock++;\n\tpnum = block / blocks_per_page;\n\tpage = find_or_create_page(inode->i_mapping, pnum, gfp);\n\tif (!page)\n\t\treturn -ENOMEM;\n\tBUG_ON(page->mapping != inode->i_mapping);\n\te4b->bd_buddy_page = page;\n\treturn 0;\n}\n\nstatic void ext4_mb_put_buddy_page_lock(struct ext4_buddy *e4b)\n{\n\tif (e4b->bd_bitmap_page) {\n\t\tunlock_page(e4b->bd_bitmap_page);\n\t\tput_page(e4b->bd_bitmap_page);\n\t}\n\tif (e4b->bd_buddy_page) {\n\t\tunlock_page(e4b->bd_buddy_page);\n\t\tput_page(e4b->bd_buddy_page);\n\t}\n}\n\n/*\n * Locking note:  This routine calls ext4_mb_init_cache(), which takes the\n * block group lock of all groups for this page; do not hold the BG lock when\n * calling this routine!\n */\nstatic noinline_for_stack\nint ext4_mb_init_group(struct super_block *sb, ext4_group_t group, gfp_t gfp)\n{\n\n\tstruct ext4_group_info *this_grp;\n\tstruct ext4_buddy e4b;\n\tstruct page *page;\n\tint ret = 0;\n\n\tmight_sleep();\n\tmb_debug(sb, \"init group %u\\n\", group);\n\tthis_grp = ext4_get_group_info(sb, group);\n\t/*\n\t * This ensures that we don't reinit the buddy cache\n\t * page which map to the group from which we are already\n\t * allocating. If we are looking at the buddy cache we would\n\t * have taken a reference using ext4_mb_load_buddy and that\n\t * would have pinned buddy page to page cache.\n\t * The call to ext4_mb_get_buddy_page_lock will mark the\n\t * page accessed.\n\t */\n\tret = ext4_mb_get_buddy_page_lock(sb, group, &e4b, gfp);\n\tif (ret || !EXT4_MB_GRP_NEED_INIT(this_grp)) {\n\t\t/*\n\t\t * somebody initialized the group\n\t\t * return without doing anything\n\t\t */\n\t\tgoto err;\n\t}\n\n\tpage = e4b.bd_bitmap_page;\n\tret = ext4_mb_init_cache(page, NULL, gfp);\n\tif (ret)\n\t\tgoto err;\n\tif (!PageUptodate(page)) {\n\t\tret = -EIO;\n\t\tgoto err;\n\t}\n\n\tif (e4b.bd_buddy_page == NULL) {\n\t\t/*\n\t\t * If both the bitmap and buddy are in\n\t\t * the same page we don't need to force\n\t\t * init the buddy\n\t\t */\n\t\tret = 0;\n\t\tgoto err;\n\t}\n\t/* init buddy cache */\n\tpage = e4b.bd_buddy_page;\n\tret = ext4_mb_init_cache(page, e4b.bd_bitmap, gfp);\n\tif (ret)\n\t\tgoto err;\n\tif (!PageUptodate(page)) {\n\t\tret = -EIO;\n\t\tgoto err;\n\t}\nerr:\n\text4_mb_put_buddy_page_lock(&e4b);\n\treturn ret;\n}\n\n/*\n * Locking note:  This routine calls ext4_mb_init_cache(), which takes the\n * block group lock of all groups for this page; do not hold the BG lock when\n * calling this routine!\n */\nstatic noinline_for_stack int\next4_mb_load_buddy_gfp(struct super_block *sb, ext4_group_t group,\n\t\t       struct ext4_buddy *e4b, gfp_t gfp)\n{\n\tint blocks_per_page;\n\tint block;\n\tint pnum;\n\tint poff;\n\tstruct page *page;\n\tint ret;\n\tstruct ext4_group_info *grp;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct inode *inode = sbi->s_buddy_cache;\n\n\tmight_sleep();\n\tmb_debug(sb, \"load group %u\\n\", group);\n\n\tblocks_per_page = PAGE_SIZE / sb->s_blocksize;\n\tgrp = ext4_get_group_info(sb, group);\n\n\te4b->bd_blkbits = sb->s_blocksize_bits;\n\te4b->bd_info = grp;\n\te4b->bd_sb = sb;\n\te4b->bd_group = group;\n\te4b->bd_buddy_page = NULL;\n\te4b->bd_bitmap_page = NULL;\n\n\tif (unlikely(EXT4_MB_GRP_NEED_INIT(grp))) {\n\t\t/*\n\t\t * we need full data about the group\n\t\t * to make a good selection\n\t\t */\n\t\tret = ext4_mb_init_group(sb, group, gfp);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * the buddy cache inode stores the block bitmap\n\t * and buddy information in consecutive blocks.\n\t * So for each group we need two blocks.\n\t */\n\tblock = group * 2;\n\tpnum = block / blocks_per_page;\n\tpoff = block % blocks_per_page;\n\n\t/* we could use find_or_create_page(), but it locks page\n\t * what we'd like to avoid in fast path ... */\n\tpage = find_get_page_flags(inode->i_mapping, pnum, FGP_ACCESSED);\n\tif (page == NULL || !PageUptodate(page)) {\n\t\tif (page)\n\t\t\t/*\n\t\t\t * drop the page reference and try\n\t\t\t * to get the page with lock. If we\n\t\t\t * are not uptodate that implies\n\t\t\t * somebody just created the page but\n\t\t\t * is yet to initialize the same. So\n\t\t\t * wait for it to initialize.\n\t\t\t */\n\t\t\tput_page(page);\n\t\tpage = find_or_create_page(inode->i_mapping, pnum, gfp);\n\t\tif (page) {\n\t\t\tBUG_ON(page->mapping != inode->i_mapping);\n\t\t\tif (!PageUptodate(page)) {\n\t\t\t\tret = ext4_mb_init_cache(page, NULL, gfp);\n\t\t\t\tif (ret) {\n\t\t\t\t\tunlock_page(page);\n\t\t\t\t\tgoto err;\n\t\t\t\t}\n\t\t\t\tmb_cmp_bitmaps(e4b, page_address(page) +\n\t\t\t\t\t       (poff * sb->s_blocksize));\n\t\t\t}\n\t\t\tunlock_page(page);\n\t\t}\n\t}\n\tif (page == NULL) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\tif (!PageUptodate(page)) {\n\t\tret = -EIO;\n\t\tgoto err;\n\t}\n\n\t/* Pages marked accessed already */\n\te4b->bd_bitmap_page = page;\n\te4b->bd_bitmap = page_address(page) + (poff * sb->s_blocksize);\n\n\tblock++;\n\tpnum = block / blocks_per_page;\n\tpoff = block % blocks_per_page;\n\n\tpage = find_get_page_flags(inode->i_mapping, pnum, FGP_ACCESSED);\n\tif (page == NULL || !PageUptodate(page)) {\n\t\tif (page)\n\t\t\tput_page(page);\n\t\tpage = find_or_create_page(inode->i_mapping, pnum, gfp);\n\t\tif (page) {\n\t\t\tBUG_ON(page->mapping != inode->i_mapping);\n\t\t\tif (!PageUptodate(page)) {\n\t\t\t\tret = ext4_mb_init_cache(page, e4b->bd_bitmap,\n\t\t\t\t\t\t\t gfp);\n\t\t\t\tif (ret) {\n\t\t\t\t\tunlock_page(page);\n\t\t\t\t\tgoto err;\n\t\t\t\t}\n\t\t\t}\n\t\t\tunlock_page(page);\n\t\t}\n\t}\n\tif (page == NULL) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\tif (!PageUptodate(page)) {\n\t\tret = -EIO;\n\t\tgoto err;\n\t}\n\n\t/* Pages marked accessed already */\n\te4b->bd_buddy_page = page;\n\te4b->bd_buddy = page_address(page) + (poff * sb->s_blocksize);\n\n\treturn 0;\n\nerr:\n\tif (page)\n\t\tput_page(page);\n\tif (e4b->bd_bitmap_page)\n\t\tput_page(e4b->bd_bitmap_page);\n\tif (e4b->bd_buddy_page)\n\t\tput_page(e4b->bd_buddy_page);\n\te4b->bd_buddy = NULL;\n\te4b->bd_bitmap = NULL;\n\treturn ret;\n}\n\nstatic int ext4_mb_load_buddy(struct super_block *sb, ext4_group_t group,\n\t\t\t      struct ext4_buddy *e4b)\n{\n\treturn ext4_mb_load_buddy_gfp(sb, group, e4b, GFP_NOFS);\n}\n\nstatic void ext4_mb_unload_buddy(struct ext4_buddy *e4b)\n{\n\tif (e4b->bd_bitmap_page)\n\t\tput_page(e4b->bd_bitmap_page);\n\tif (e4b->bd_buddy_page)\n\t\tput_page(e4b->bd_buddy_page);\n}\n\n\nstatic int mb_find_order_for_block(struct ext4_buddy *e4b, int block)\n{\n\tint order = 1, max;\n\tvoid *bb;\n\n\tBUG_ON(e4b->bd_bitmap == e4b->bd_buddy);\n\tBUG_ON(block >= (1 << (e4b->bd_blkbits + 3)));\n\n\twhile (order <= e4b->bd_blkbits + 1) {\n\t\tbb = mb_find_buddy(e4b, order, &max);\n\t\tif (!mb_test_bit(block >> order, bb)) {\n\t\t\t/* this block is part of buddy of order 'order' */\n\t\t\treturn order;\n\t\t}\n\t\torder++;\n\t}\n\treturn 0;\n}\n\nstatic void mb_clear_bits(void *bm, int cur, int len)\n{\n\t__u32 *addr;\n\n\tlen = cur + len;\n\twhile (cur < len) {\n\t\tif ((cur & 31) == 0 && (len - cur) >= 32) {\n\t\t\t/* fast path: clear whole word at once */\n\t\t\taddr = bm + (cur >> 3);\n\t\t\t*addr = 0;\n\t\t\tcur += 32;\n\t\t\tcontinue;\n\t\t}\n\t\tmb_clear_bit(cur, bm);\n\t\tcur++;\n\t}\n}\n\n/* clear bits in given range\n * will return first found zero bit if any, -1 otherwise\n */\nstatic int mb_test_and_clear_bits(void *bm, int cur, int len)\n{\n\t__u32 *addr;\n\tint zero_bit = -1;\n\n\tlen = cur + len;\n\twhile (cur < len) {\n\t\tif ((cur & 31) == 0 && (len - cur) >= 32) {\n\t\t\t/* fast path: clear whole word at once */\n\t\t\taddr = bm + (cur >> 3);\n\t\t\tif (*addr != (__u32)(-1) && zero_bit == -1)\n\t\t\t\tzero_bit = cur + mb_find_next_zero_bit(addr, 32, 0);\n\t\t\t*addr = 0;\n\t\t\tcur += 32;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!mb_test_and_clear_bit(cur, bm) && zero_bit == -1)\n\t\t\tzero_bit = cur;\n\t\tcur++;\n\t}\n\n\treturn zero_bit;\n}\n\nvoid ext4_set_bits(void *bm, int cur, int len)\n{\n\t__u32 *addr;\n\n\tlen = cur + len;\n\twhile (cur < len) {\n\t\tif ((cur & 31) == 0 && (len - cur) >= 32) {\n\t\t\t/* fast path: set whole word at once */\n\t\t\taddr = bm + (cur >> 3);\n\t\t\t*addr = 0xffffffff;\n\t\t\tcur += 32;\n\t\t\tcontinue;\n\t\t}\n\t\tmb_set_bit(cur, bm);\n\t\tcur++;\n\t}\n}\n\nstatic inline int mb_buddy_adjust_border(int* bit, void* bitmap, int side)\n{\n\tif (mb_test_bit(*bit + side, bitmap)) {\n\t\tmb_clear_bit(*bit, bitmap);\n\t\t(*bit) -= side;\n\t\treturn 1;\n\t}\n\telse {\n\t\t(*bit) += side;\n\t\tmb_set_bit(*bit, bitmap);\n\t\treturn -1;\n\t}\n}\n\nstatic void mb_buddy_mark_free(struct ext4_buddy *e4b, int first, int last)\n{\n\tint max;\n\tint order = 1;\n\tvoid *buddy = mb_find_buddy(e4b, order, &max);\n\n\twhile (buddy) {\n\t\tvoid *buddy2;\n\n\t\t/* Bits in range [first; last] are known to be set since\n\t\t * corresponding blocks were allocated. Bits in range\n\t\t * (first; last) will stay set because they form buddies on\n\t\t * upper layer. We just deal with borders if they don't\n\t\t * align with upper layer and then go up.\n\t\t * Releasing entire group is all about clearing\n\t\t * single bit of highest order buddy.\n\t\t */\n\n\t\t/* Example:\n\t\t * ---------------------------------\n\t\t * |   1   |   1   |   1   |   1   |\n\t\t * ---------------------------------\n\t\t * | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n\t\t * ---------------------------------\n\t\t *   0   1   2   3   4   5   6   7\n\t\t *      \\_____________________/\n\t\t *\n\t\t * Neither [1] nor [6] is aligned to above layer.\n\t\t * Left neighbour [0] is free, so mark it busy,\n\t\t * decrease bb_counters and extend range to\n\t\t * [0; 6]\n\t\t * Right neighbour [7] is busy. It can't be coaleasced with [6], so\n\t\t * mark [6] free, increase bb_counters and shrink range to\n\t\t * [0; 5].\n\t\t * Then shift range to [0; 2], go up and do the same.\n\t\t */\n\n\n\t\tif (first & 1)\n\t\t\te4b->bd_info->bb_counters[order] += mb_buddy_adjust_border(&first, buddy, -1);\n\t\tif (!(last & 1))\n\t\t\te4b->bd_info->bb_counters[order] += mb_buddy_adjust_border(&last, buddy, 1);\n\t\tif (first > last)\n\t\t\tbreak;\n\t\torder++;\n\n\t\tif (first == last || !(buddy2 = mb_find_buddy(e4b, order, &max))) {\n\t\t\tmb_clear_bits(buddy, first, last - first + 1);\n\t\t\te4b->bd_info->bb_counters[order - 1] += last - first + 1;\n\t\t\tbreak;\n\t\t}\n\t\tfirst >>= 1;\n\t\tlast >>= 1;\n\t\tbuddy = buddy2;\n\t}\n}\n\nstatic void mb_free_blocks(struct inode *inode, struct ext4_buddy *e4b,\n\t\t\t   int first, int count)\n{\n\tint left_is_free = 0;\n\tint right_is_free = 0;\n\tint block;\n\tint last = first + count - 1;\n\tstruct super_block *sb = e4b->bd_sb;\n\n\tif (WARN_ON(count == 0))\n\t\treturn;\n\tBUG_ON(last >= (sb->s_blocksize << 3));\n\tassert_spin_locked(ext4_group_lock_ptr(sb, e4b->bd_group));\n\t/* Don't bother if the block group is corrupt. */\n\tif (unlikely(EXT4_MB_GRP_BBITMAP_CORRUPT(e4b->bd_info)))\n\t\treturn;\n\n\tmb_check_buddy(e4b);\n\tmb_free_blocks_double(inode, e4b, first, count);\n\n\tthis_cpu_inc(discard_pa_seq);\n\te4b->bd_info->bb_free += count;\n\tif (first < e4b->bd_info->bb_first_free)\n\t\te4b->bd_info->bb_first_free = first;\n\n\t/* access memory sequentially: check left neighbour,\n\t * clear range and then check right neighbour\n\t */\n\tif (first != 0)\n\t\tleft_is_free = !mb_test_bit(first - 1, e4b->bd_bitmap);\n\tblock = mb_test_and_clear_bits(e4b->bd_bitmap, first, count);\n\tif (last + 1 < EXT4_SB(sb)->s_mb_maxs[0])\n\t\tright_is_free = !mb_test_bit(last + 1, e4b->bd_bitmap);\n\n\tif (unlikely(block != -1)) {\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\t\text4_fsblk_t blocknr;\n\n\t\tblocknr = ext4_group_first_block_no(sb, e4b->bd_group);\n\t\tblocknr += EXT4_C2B(sbi, block);\n\t\tif (!(sbi->s_mount_state & EXT4_FC_REPLAY)) {\n\t\t\text4_grp_locked_error(sb, e4b->bd_group,\n\t\t\t\t\t      inode ? inode->i_ino : 0,\n\t\t\t\t\t      blocknr,\n\t\t\t\t\t      \"freeing already freed block (bit %u); block bitmap corrupt.\",\n\t\t\t\t\t      block);\n\t\t\text4_mark_group_bitmap_corrupted(\n\t\t\t\tsb, e4b->bd_group,\n\t\t\t\tEXT4_GROUP_INFO_BBITMAP_CORRUPT);\n\t\t}\n\t\tgoto done;\n\t}\n\n\t/* let's maintain fragments counter */\n\tif (left_is_free && right_is_free)\n\t\te4b->bd_info->bb_fragments--;\n\telse if (!left_is_free && !right_is_free)\n\t\te4b->bd_info->bb_fragments++;\n\n\t/* buddy[0] == bd_bitmap is a special case, so handle\n\t * it right away and let mb_buddy_mark_free stay free of\n\t * zero order checks.\n\t * Check if neighbours are to be coaleasced,\n\t * adjust bitmap bb_counters and borders appropriately.\n\t */\n\tif (first & 1) {\n\t\tfirst += !left_is_free;\n\t\te4b->bd_info->bb_counters[0] += left_is_free ? -1 : 1;\n\t}\n\tif (!(last & 1)) {\n\t\tlast -= !right_is_free;\n\t\te4b->bd_info->bb_counters[0] += right_is_free ? -1 : 1;\n\t}\n\n\tif (first <= last)\n\t\tmb_buddy_mark_free(e4b, first >> 1, last >> 1);\n\ndone:\n\tmb_set_largest_free_order(sb, e4b->bd_info);\n\tmb_check_buddy(e4b);\n}\n\nstatic int mb_find_extent(struct ext4_buddy *e4b, int block,\n\t\t\t\tint needed, struct ext4_free_extent *ex)\n{\n\tint next = block;\n\tint max, order;\n\tvoid *buddy;\n\n\tassert_spin_locked(ext4_group_lock_ptr(e4b->bd_sb, e4b->bd_group));\n\tBUG_ON(ex == NULL);\n\n\tbuddy = mb_find_buddy(e4b, 0, &max);\n\tBUG_ON(buddy == NULL);\n\tBUG_ON(block >= max);\n\tif (mb_test_bit(block, buddy)) {\n\t\tex->fe_len = 0;\n\t\tex->fe_start = 0;\n\t\tex->fe_group = 0;\n\t\treturn 0;\n\t}\n\n\t/* find actual order */\n\torder = mb_find_order_for_block(e4b, block);\n\tblock = block >> order;\n\n\tex->fe_len = 1 << order;\n\tex->fe_start = block << order;\n\tex->fe_group = e4b->bd_group;\n\n\t/* calc difference from given start */\n\tnext = next - ex->fe_start;\n\tex->fe_len -= next;\n\tex->fe_start += next;\n\n\twhile (needed > ex->fe_len &&\n\t       mb_find_buddy(e4b, order, &max)) {\n\n\t\tif (block + 1 >= max)\n\t\t\tbreak;\n\n\t\tnext = (block + 1) * (1 << order);\n\t\tif (mb_test_bit(next, e4b->bd_bitmap))\n\t\t\tbreak;\n\n\t\torder = mb_find_order_for_block(e4b, next);\n\n\t\tblock = next >> order;\n\t\tex->fe_len += 1 << order;\n\t}\n\n\tif (ex->fe_start + ex->fe_len > EXT4_CLUSTERS_PER_GROUP(e4b->bd_sb)) {\n\t\t/* Should never happen! (but apparently sometimes does?!?) */\n\t\tWARN_ON(1);\n\t\text4_error(e4b->bd_sb, \"corruption or bug in mb_find_extent \"\n\t\t\t   \"block=%d, order=%d needed=%d ex=%u/%d/%d@%u\",\n\t\t\t   block, order, needed, ex->fe_group, ex->fe_start,\n\t\t\t   ex->fe_len, ex->fe_logical);\n\t\tex->fe_len = 0;\n\t\tex->fe_start = 0;\n\t\tex->fe_group = 0;\n\t}\n\treturn ex->fe_len;\n}\n\nstatic int mb_mark_used(struct ext4_buddy *e4b, struct ext4_free_extent *ex)\n{\n\tint ord;\n\tint mlen = 0;\n\tint max = 0;\n\tint cur;\n\tint start = ex->fe_start;\n\tint len = ex->fe_len;\n\tunsigned ret = 0;\n\tint len0 = len;\n\tvoid *buddy;\n\n\tBUG_ON(start + len > (e4b->bd_sb->s_blocksize << 3));\n\tBUG_ON(e4b->bd_group != ex->fe_group);\n\tassert_spin_locked(ext4_group_lock_ptr(e4b->bd_sb, e4b->bd_group));\n\tmb_check_buddy(e4b);\n\tmb_mark_used_double(e4b, start, len);\n\n\tthis_cpu_inc(discard_pa_seq);\n\te4b->bd_info->bb_free -= len;\n\tif (e4b->bd_info->bb_first_free == start)\n\t\te4b->bd_info->bb_first_free += len;\n\n\t/* let's maintain fragments counter */\n\tif (start != 0)\n\t\tmlen = !mb_test_bit(start - 1, e4b->bd_bitmap);\n\tif (start + len < EXT4_SB(e4b->bd_sb)->s_mb_maxs[0])\n\t\tmax = !mb_test_bit(start + len, e4b->bd_bitmap);\n\tif (mlen && max)\n\t\te4b->bd_info->bb_fragments++;\n\telse if (!mlen && !max)\n\t\te4b->bd_info->bb_fragments--;\n\n\t/* let's maintain buddy itself */\n\twhile (len) {\n\t\tord = mb_find_order_for_block(e4b, start);\n\n\t\tif (((start >> ord) << ord) == start && len >= (1 << ord)) {\n\t\t\t/* the whole chunk may be allocated at once! */\n\t\t\tmlen = 1 << ord;\n\t\t\tbuddy = mb_find_buddy(e4b, ord, &max);\n\t\t\tBUG_ON((start >> ord) >= max);\n\t\t\tmb_set_bit(start >> ord, buddy);\n\t\t\te4b->bd_info->bb_counters[ord]--;\n\t\t\tstart += mlen;\n\t\t\tlen -= mlen;\n\t\t\tBUG_ON(len < 0);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* store for history */\n\t\tif (ret == 0)\n\t\t\tret = len | (ord << 16);\n\n\t\t/* we have to split large buddy */\n\t\tBUG_ON(ord <= 0);\n\t\tbuddy = mb_find_buddy(e4b, ord, &max);\n\t\tmb_set_bit(start >> ord, buddy);\n\t\te4b->bd_info->bb_counters[ord]--;\n\n\t\tord--;\n\t\tcur = (start >> ord) & ~1U;\n\t\tbuddy = mb_find_buddy(e4b, ord, &max);\n\t\tmb_clear_bit(cur, buddy);\n\t\tmb_clear_bit(cur + 1, buddy);\n\t\te4b->bd_info->bb_counters[ord]++;\n\t\te4b->bd_info->bb_counters[ord]++;\n\t}\n\tmb_set_largest_free_order(e4b->bd_sb, e4b->bd_info);\n\n\text4_set_bits(e4b->bd_bitmap, ex->fe_start, len0);\n\tmb_check_buddy(e4b);\n\n\treturn ret;\n}\n\n/*\n * Must be called under group lock!\n */\nstatic void ext4_mb_use_best_found(struct ext4_allocation_context *ac,\n\t\t\t\t\tstruct ext4_buddy *e4b)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(ac->ac_sb);\n\tint ret;\n\n\tBUG_ON(ac->ac_b_ex.fe_group != e4b->bd_group);\n\tBUG_ON(ac->ac_status == AC_STATUS_FOUND);\n\n\tac->ac_b_ex.fe_len = min(ac->ac_b_ex.fe_len, ac->ac_g_ex.fe_len);\n\tac->ac_b_ex.fe_logical = ac->ac_g_ex.fe_logical;\n\tret = mb_mark_used(e4b, &ac->ac_b_ex);\n\n\t/* preallocation can change ac_b_ex, thus we store actually\n\t * allocated blocks for history */\n\tac->ac_f_ex = ac->ac_b_ex;\n\n\tac->ac_status = AC_STATUS_FOUND;\n\tac->ac_tail = ret & 0xffff;\n\tac->ac_buddy = ret >> 16;\n\n\t/*\n\t * take the page reference. We want the page to be pinned\n\t * so that we don't get a ext4_mb_init_cache_call for this\n\t * group until we update the bitmap. That would mean we\n\t * double allocate blocks. The reference is dropped\n\t * in ext4_mb_release_context\n\t */\n\tac->ac_bitmap_page = e4b->bd_bitmap_page;\n\tget_page(ac->ac_bitmap_page);\n\tac->ac_buddy_page = e4b->bd_buddy_page;\n\tget_page(ac->ac_buddy_page);\n\t/* store last allocated for subsequent stream allocation */\n\tif (ac->ac_flags & EXT4_MB_STREAM_ALLOC) {\n\t\tspin_lock(&sbi->s_md_lock);\n\t\tsbi->s_mb_last_group = ac->ac_f_ex.fe_group;\n\t\tsbi->s_mb_last_start = ac->ac_f_ex.fe_start;\n\t\tspin_unlock(&sbi->s_md_lock);\n\t}\n\t/*\n\t * As we've just preallocated more space than\n\t * user requested originally, we store allocated\n\t * space in a special descriptor.\n\t */\n\tif (ac->ac_o_ex.fe_len < ac->ac_b_ex.fe_len)\n\t\text4_mb_new_preallocation(ac);\n\n}\n\nstatic void ext4_mb_check_limits(struct ext4_allocation_context *ac,\n\t\t\t\t\tstruct ext4_buddy *e4b,\n\t\t\t\t\tint finish_group)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(ac->ac_sb);\n\tstruct ext4_free_extent *bex = &ac->ac_b_ex;\n\tstruct ext4_free_extent *gex = &ac->ac_g_ex;\n\tstruct ext4_free_extent ex;\n\tint max;\n\n\tif (ac->ac_status == AC_STATUS_FOUND)\n\t\treturn;\n\t/*\n\t * We don't want to scan for a whole year\n\t */\n\tif (ac->ac_found > sbi->s_mb_max_to_scan &&\n\t\t\t!(ac->ac_flags & EXT4_MB_HINT_FIRST)) {\n\t\tac->ac_status = AC_STATUS_BREAK;\n\t\treturn;\n\t}\n\n\t/*\n\t * Haven't found good chunk so far, let's continue\n\t */\n\tif (bex->fe_len < gex->fe_len)\n\t\treturn;\n\n\tif ((finish_group || ac->ac_found > sbi->s_mb_min_to_scan)\n\t\t\t&& bex->fe_group == e4b->bd_group) {\n\t\t/* recheck chunk's availability - we don't know\n\t\t * when it was found (within this lock-unlock\n\t\t * period or not) */\n\t\tmax = mb_find_extent(e4b, bex->fe_start, gex->fe_len, &ex);\n\t\tif (max >= gex->fe_len) {\n\t\t\text4_mb_use_best_found(ac, e4b);\n\t\t\treturn;\n\t\t}\n\t}\n}\n\n/*\n * The routine checks whether found extent is good enough. If it is,\n * then the extent gets marked used and flag is set to the context\n * to stop scanning. Otherwise, the extent is compared with the\n * previous found extent and if new one is better, then it's stored\n * in the context. Later, the best found extent will be used, if\n * mballoc can't find good enough extent.\n *\n * FIXME: real allocation policy is to be designed yet!\n */\nstatic void ext4_mb_measure_extent(struct ext4_allocation_context *ac,\n\t\t\t\t\tstruct ext4_free_extent *ex,\n\t\t\t\t\tstruct ext4_buddy *e4b)\n{\n\tstruct ext4_free_extent *bex = &ac->ac_b_ex;\n\tstruct ext4_free_extent *gex = &ac->ac_g_ex;\n\n\tBUG_ON(ex->fe_len <= 0);\n\tBUG_ON(ex->fe_len > EXT4_CLUSTERS_PER_GROUP(ac->ac_sb));\n\tBUG_ON(ex->fe_start >= EXT4_CLUSTERS_PER_GROUP(ac->ac_sb));\n\tBUG_ON(ac->ac_status != AC_STATUS_CONTINUE);\n\n\tac->ac_found++;\n\n\t/*\n\t * The special case - take what you catch first\n\t */\n\tif (unlikely(ac->ac_flags & EXT4_MB_HINT_FIRST)) {\n\t\t*bex = *ex;\n\t\text4_mb_use_best_found(ac, e4b);\n\t\treturn;\n\t}\n\n\t/*\n\t * Let's check whether the chuck is good enough\n\t */\n\tif (ex->fe_len == gex->fe_len) {\n\t\t*bex = *ex;\n\t\text4_mb_use_best_found(ac, e4b);\n\t\treturn;\n\t}\n\n\t/*\n\t * If this is first found extent, just store it in the context\n\t */\n\tif (bex->fe_len == 0) {\n\t\t*bex = *ex;\n\t\treturn;\n\t}\n\n\t/*\n\t * If new found extent is better, store it in the context\n\t */\n\tif (bex->fe_len < gex->fe_len) {\n\t\t/* if the request isn't satisfied, any found extent\n\t\t * larger than previous best one is better */\n\t\tif (ex->fe_len > bex->fe_len)\n\t\t\t*bex = *ex;\n\t} else if (ex->fe_len > gex->fe_len) {\n\t\t/* if the request is satisfied, then we try to find\n\t\t * an extent that still satisfy the request, but is\n\t\t * smaller than previous one */\n\t\tif (ex->fe_len < bex->fe_len)\n\t\t\t*bex = *ex;\n\t}\n\n\text4_mb_check_limits(ac, e4b, 0);\n}\n\nstatic noinline_for_stack\nint ext4_mb_try_best_found(struct ext4_allocation_context *ac,\n\t\t\t\t\tstruct ext4_buddy *e4b)\n{\n\tstruct ext4_free_extent ex = ac->ac_b_ex;\n\text4_group_t group = ex.fe_group;\n\tint max;\n\tint err;\n\n\tBUG_ON(ex.fe_len <= 0);\n\terr = ext4_mb_load_buddy(ac->ac_sb, group, e4b);\n\tif (err)\n\t\treturn err;\n\n\text4_lock_group(ac->ac_sb, group);\n\tmax = mb_find_extent(e4b, ex.fe_start, ex.fe_len, &ex);\n\n\tif (max > 0) {\n\t\tac->ac_b_ex = ex;\n\t\text4_mb_use_best_found(ac, e4b);\n\t}\n\n\text4_unlock_group(ac->ac_sb, group);\n\text4_mb_unload_buddy(e4b);\n\n\treturn 0;\n}\n\nstatic noinline_for_stack\nint ext4_mb_find_by_goal(struct ext4_allocation_context *ac,\n\t\t\t\tstruct ext4_buddy *e4b)\n{\n\text4_group_t group = ac->ac_g_ex.fe_group;\n\tint max;\n\tint err;\n\tstruct ext4_sb_info *sbi = EXT4_SB(ac->ac_sb);\n\tstruct ext4_group_info *grp = ext4_get_group_info(ac->ac_sb, group);\n\tstruct ext4_free_extent ex;\n\n\tif (!(ac->ac_flags & EXT4_MB_HINT_TRY_GOAL))\n\t\treturn 0;\n\tif (grp->bb_free == 0)\n\t\treturn 0;\n\n\terr = ext4_mb_load_buddy(ac->ac_sb, group, e4b);\n\tif (err)\n\t\treturn err;\n\n\tif (unlikely(EXT4_MB_GRP_BBITMAP_CORRUPT(e4b->bd_info))) {\n\t\text4_mb_unload_buddy(e4b);\n\t\treturn 0;\n\t}\n\n\text4_lock_group(ac->ac_sb, group);\n\tmax = mb_find_extent(e4b, ac->ac_g_ex.fe_start,\n\t\t\t     ac->ac_g_ex.fe_len, &ex);\n\tex.fe_logical = 0xDEADFA11; /* debug value */\n\n\tif (max >= ac->ac_g_ex.fe_len && ac->ac_g_ex.fe_len == sbi->s_stripe) {\n\t\text4_fsblk_t start;\n\n\t\tstart = ext4_group_first_block_no(ac->ac_sb, e4b->bd_group) +\n\t\t\tex.fe_start;\n\t\t/* use do_div to get remainder (would be 64-bit modulo) */\n\t\tif (do_div(start, sbi->s_stripe) == 0) {\n\t\t\tac->ac_found++;\n\t\t\tac->ac_b_ex = ex;\n\t\t\text4_mb_use_best_found(ac, e4b);\n\t\t}\n\t} else if (max >= ac->ac_g_ex.fe_len) {\n\t\tBUG_ON(ex.fe_len <= 0);\n\t\tBUG_ON(ex.fe_group != ac->ac_g_ex.fe_group);\n\t\tBUG_ON(ex.fe_start != ac->ac_g_ex.fe_start);\n\t\tac->ac_found++;\n\t\tac->ac_b_ex = ex;\n\t\text4_mb_use_best_found(ac, e4b);\n\t} else if (max > 0 && (ac->ac_flags & EXT4_MB_HINT_MERGE)) {\n\t\t/* Sometimes, caller may want to merge even small\n\t\t * number of blocks to an existing extent */\n\t\tBUG_ON(ex.fe_len <= 0);\n\t\tBUG_ON(ex.fe_group != ac->ac_g_ex.fe_group);\n\t\tBUG_ON(ex.fe_start != ac->ac_g_ex.fe_start);\n\t\tac->ac_found++;\n\t\tac->ac_b_ex = ex;\n\t\text4_mb_use_best_found(ac, e4b);\n\t}\n\text4_unlock_group(ac->ac_sb, group);\n\text4_mb_unload_buddy(e4b);\n\n\treturn 0;\n}\n\n/*\n * The routine scans buddy structures (not bitmap!) from given order\n * to max order and tries to find big enough chunk to satisfy the req\n */\nstatic noinline_for_stack\nvoid ext4_mb_simple_scan_group(struct ext4_allocation_context *ac,\n\t\t\t\t\tstruct ext4_buddy *e4b)\n{\n\tstruct super_block *sb = ac->ac_sb;\n\tstruct ext4_group_info *grp = e4b->bd_info;\n\tvoid *buddy;\n\tint i;\n\tint k;\n\tint max;\n\n\tBUG_ON(ac->ac_2order <= 0);\n\tfor (i = ac->ac_2order; i <= sb->s_blocksize_bits + 1; i++) {\n\t\tif (grp->bb_counters[i] == 0)\n\t\t\tcontinue;\n\n\t\tbuddy = mb_find_buddy(e4b, i, &max);\n\t\tBUG_ON(buddy == NULL);\n\n\t\tk = mb_find_next_zero_bit(buddy, max, 0);\n\t\tif (k >= max) {\n\t\t\text4_grp_locked_error(ac->ac_sb, e4b->bd_group, 0, 0,\n\t\t\t\t\"%d free clusters of order %d. But found 0\",\n\t\t\t\tgrp->bb_counters[i], i);\n\t\t\text4_mark_group_bitmap_corrupted(ac->ac_sb,\n\t\t\t\t\t e4b->bd_group,\n\t\t\t\t\tEXT4_GROUP_INFO_BBITMAP_CORRUPT);\n\t\t\tbreak;\n\t\t}\n\t\tac->ac_found++;\n\n\t\tac->ac_b_ex.fe_len = 1 << i;\n\t\tac->ac_b_ex.fe_start = k << i;\n\t\tac->ac_b_ex.fe_group = e4b->bd_group;\n\n\t\text4_mb_use_best_found(ac, e4b);\n\n\t\tBUG_ON(ac->ac_f_ex.fe_len != ac->ac_g_ex.fe_len);\n\n\t\tif (EXT4_SB(sb)->s_mb_stats)\n\t\t\tatomic_inc(&EXT4_SB(sb)->s_bal_2orders);\n\n\t\tbreak;\n\t}\n}\n\n/*\n * The routine scans the group and measures all found extents.\n * In order to optimize scanning, caller must pass number of\n * free blocks in the group, so the routine can know upper limit.\n */\nstatic noinline_for_stack\nvoid ext4_mb_complex_scan_group(struct ext4_allocation_context *ac,\n\t\t\t\t\tstruct ext4_buddy *e4b)\n{\n\tstruct super_block *sb = ac->ac_sb;\n\tvoid *bitmap = e4b->bd_bitmap;\n\tstruct ext4_free_extent ex;\n\tint i;\n\tint free;\n\n\tfree = e4b->bd_info->bb_free;\n\tif (WARN_ON(free <= 0))\n\t\treturn;\n\n\ti = e4b->bd_info->bb_first_free;\n\n\twhile (free && ac->ac_status == AC_STATUS_CONTINUE) {\n\t\ti = mb_find_next_zero_bit(bitmap,\n\t\t\t\t\t\tEXT4_CLUSTERS_PER_GROUP(sb), i);\n\t\tif (i >= EXT4_CLUSTERS_PER_GROUP(sb)) {\n\t\t\t/*\n\t\t\t * IF we have corrupt bitmap, we won't find any\n\t\t\t * free blocks even though group info says we\n\t\t\t * have free blocks\n\t\t\t */\n\t\t\text4_grp_locked_error(sb, e4b->bd_group, 0, 0,\n\t\t\t\t\t\"%d free clusters as per \"\n\t\t\t\t\t\"group info. But bitmap says 0\",\n\t\t\t\t\tfree);\n\t\t\text4_mark_group_bitmap_corrupted(sb, e4b->bd_group,\n\t\t\t\t\tEXT4_GROUP_INFO_BBITMAP_CORRUPT);\n\t\t\tbreak;\n\t\t}\n\n\t\tmb_find_extent(e4b, i, ac->ac_g_ex.fe_len, &ex);\n\t\tif (WARN_ON(ex.fe_len <= 0))\n\t\t\tbreak;\n\t\tif (free < ex.fe_len) {\n\t\t\text4_grp_locked_error(sb, e4b->bd_group, 0, 0,\n\t\t\t\t\t\"%d free clusters as per \"\n\t\t\t\t\t\"group info. But got %d blocks\",\n\t\t\t\t\tfree, ex.fe_len);\n\t\t\text4_mark_group_bitmap_corrupted(sb, e4b->bd_group,\n\t\t\t\t\tEXT4_GROUP_INFO_BBITMAP_CORRUPT);\n\t\t\t/*\n\t\t\t * The number of free blocks differs. This mostly\n\t\t\t * indicate that the bitmap is corrupt. So exit\n\t\t\t * without claiming the space.\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\t\tex.fe_logical = 0xDEADC0DE; /* debug value */\n\t\text4_mb_measure_extent(ac, &ex, e4b);\n\n\t\ti += ex.fe_len;\n\t\tfree -= ex.fe_len;\n\t}\n\n\text4_mb_check_limits(ac, e4b, 1);\n}\n\n/*\n * This is a special case for storages like raid5\n * we try to find stripe-aligned chunks for stripe-size-multiple requests\n */\nstatic noinline_for_stack\nvoid ext4_mb_scan_aligned(struct ext4_allocation_context *ac,\n\t\t\t\t struct ext4_buddy *e4b)\n{\n\tstruct super_block *sb = ac->ac_sb;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tvoid *bitmap = e4b->bd_bitmap;\n\tstruct ext4_free_extent ex;\n\text4_fsblk_t first_group_block;\n\text4_fsblk_t a;\n\text4_grpblk_t i;\n\tint max;\n\n\tBUG_ON(sbi->s_stripe == 0);\n\n\t/* find first stripe-aligned block in group */\n\tfirst_group_block = ext4_group_first_block_no(sb, e4b->bd_group);\n\n\ta = first_group_block + sbi->s_stripe - 1;\n\tdo_div(a, sbi->s_stripe);\n\ti = (a * sbi->s_stripe) - first_group_block;\n\n\twhile (i < EXT4_CLUSTERS_PER_GROUP(sb)) {\n\t\tif (!mb_test_bit(i, bitmap)) {\n\t\t\tmax = mb_find_extent(e4b, i, sbi->s_stripe, &ex);\n\t\t\tif (max >= sbi->s_stripe) {\n\t\t\t\tac->ac_found++;\n\t\t\t\tex.fe_logical = 0xDEADF00D; /* debug value */\n\t\t\t\tac->ac_b_ex = ex;\n\t\t\t\text4_mb_use_best_found(ac, e4b);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\ti += sbi->s_stripe;\n\t}\n}\n\n/*\n * This is also called BEFORE we load the buddy bitmap.\n * Returns either 1 or 0 indicating that the group is either suitable\n * for the allocation or not.\n */\nstatic bool ext4_mb_good_group(struct ext4_allocation_context *ac,\n\t\t\t\text4_group_t group, int cr)\n{\n\text4_grpblk_t free, fragments;\n\tint flex_size = ext4_flex_bg_size(EXT4_SB(ac->ac_sb));\n\tstruct ext4_group_info *grp = ext4_get_group_info(ac->ac_sb, group);\n\n\tBUG_ON(cr < 0 || cr >= 4);\n\n\tif (unlikely(EXT4_MB_GRP_BBITMAP_CORRUPT(grp)))\n\t\treturn false;\n\n\tfree = grp->bb_free;\n\tif (free == 0)\n\t\treturn false;\n\n\tfragments = grp->bb_fragments;\n\tif (fragments == 0)\n\t\treturn false;\n\n\tswitch (cr) {\n\tcase 0:\n\t\tBUG_ON(ac->ac_2order == 0);\n\n\t\t/* Avoid using the first bg of a flexgroup for data files */\n\t\tif ((ac->ac_flags & EXT4_MB_HINT_DATA) &&\n\t\t    (flex_size >= EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME) &&\n\t\t    ((group % flex_size) == 0))\n\t\t\treturn false;\n\n\t\tif (free < ac->ac_g_ex.fe_len)\n\t\t\treturn false;\n\n\t\tif (ac->ac_2order > ac->ac_sb->s_blocksize_bits+1)\n\t\t\treturn true;\n\n\t\tif (grp->bb_largest_free_order < ac->ac_2order)\n\t\t\treturn false;\n\n\t\treturn true;\n\tcase 1:\n\t\tif ((free / fragments) >= ac->ac_g_ex.fe_len)\n\t\t\treturn true;\n\t\tbreak;\n\tcase 2:\n\t\tif (free >= ac->ac_g_ex.fe_len)\n\t\t\treturn true;\n\t\tbreak;\n\tcase 3:\n\t\treturn true;\n\tdefault:\n\t\tBUG();\n\t}\n\n\treturn false;\n}\n\n/*\n * This could return negative error code if something goes wrong\n * during ext4_mb_init_group(). This should not be called with\n * ext4_lock_group() held.\n */\nstatic int ext4_mb_good_group_nolock(struct ext4_allocation_context *ac,\n\t\t\t\t     ext4_group_t group, int cr)\n{\n\tstruct ext4_group_info *grp = ext4_get_group_info(ac->ac_sb, group);\n\tstruct super_block *sb = ac->ac_sb;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tbool should_lock = ac->ac_flags & EXT4_MB_STRICT_CHECK;\n\text4_grpblk_t free;\n\tint ret = 0;\n\n\tif (should_lock)\n\t\text4_lock_group(sb, group);\n\tfree = grp->bb_free;\n\tif (free == 0)\n\t\tgoto out;\n\tif (cr <= 2 && free < ac->ac_g_ex.fe_len)\n\t\tgoto out;\n\tif (unlikely(EXT4_MB_GRP_BBITMAP_CORRUPT(grp)))\n\t\tgoto out;\n\tif (should_lock)\n\t\text4_unlock_group(sb, group);\n\n\t/* We only do this if the grp has never been initialized */\n\tif (unlikely(EXT4_MB_GRP_NEED_INIT(grp))) {\n\t\tstruct ext4_group_desc *gdp =\n\t\t\text4_get_group_desc(sb, group, NULL);\n\t\tint ret;\n\n\t\t/* cr=0/1 is a very optimistic search to find large\n\t\t * good chunks almost for free.  If buddy data is not\n\t\t * ready, then this optimization makes no sense.  But\n\t\t * we never skip the first block group in a flex_bg,\n\t\t * since this gets used for metadata block allocation,\n\t\t * and we want to make sure we locate metadata blocks\n\t\t * in the first block group in the flex_bg if possible.\n\t\t */\n\t\tif (cr < 2 &&\n\t\t    (!sbi->s_log_groups_per_flex ||\n\t\t     ((group & ((1 << sbi->s_log_groups_per_flex) - 1)) != 0)) &&\n\t\t    !(ext4_has_group_desc_csum(sb) &&\n\t\t      (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT))))\n\t\t\treturn 0;\n\t\tret = ext4_mb_init_group(sb, group, GFP_NOFS);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (should_lock)\n\t\text4_lock_group(sb, group);\n\tret = ext4_mb_good_group(ac, group, cr);\nout:\n\tif (should_lock)\n\t\text4_unlock_group(sb, group);\n\treturn ret;\n}\n\n/*\n * Start prefetching @nr block bitmaps starting at @group.\n * Return the next group which needs to be prefetched.\n */\next4_group_t ext4_mb_prefetch(struct super_block *sb, ext4_group_t group,\n\t\t\t      unsigned int nr, int *cnt)\n{\n\text4_group_t ngroups = ext4_get_groups_count(sb);\n\tstruct buffer_head *bh;\n\tstruct blk_plug plug;\n\n\tblk_start_plug(&plug);\n\twhile (nr-- > 0) {\n\t\tstruct ext4_group_desc *gdp = ext4_get_group_desc(sb, group,\n\t\t\t\t\t\t\t\t  NULL);\n\t\tstruct ext4_group_info *grp = ext4_get_group_info(sb, group);\n\n\t\t/*\n\t\t * Prefetch block groups with free blocks; but don't\n\t\t * bother if it is marked uninitialized on disk, since\n\t\t * it won't require I/O to read.  Also only try to\n\t\t * prefetch once, so we avoid getblk() call, which can\n\t\t * be expensive.\n\t\t */\n\t\tif (!EXT4_MB_GRP_TEST_AND_SET_READ(grp) &&\n\t\t    EXT4_MB_GRP_NEED_INIT(grp) &&\n\t\t    ext4_free_group_clusters(sb, gdp) > 0 &&\n\t\t    !(ext4_has_group_desc_csum(sb) &&\n\t\t      (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)))) {\n\t\t\tbh = ext4_read_block_bitmap_nowait(sb, group, true);\n\t\t\tif (bh && !IS_ERR(bh)) {\n\t\t\t\tif (!buffer_uptodate(bh) && cnt)\n\t\t\t\t\t(*cnt)++;\n\t\t\t\tbrelse(bh);\n\t\t\t}\n\t\t}\n\t\tif (++group >= ngroups)\n\t\t\tgroup = 0;\n\t}\n\tblk_finish_plug(&plug);\n\treturn group;\n}\n\n/*\n * Prefetching reads the block bitmap into the buffer cache; but we\n * need to make sure that the buddy bitmap in the page cache has been\n * initialized.  Note that ext4_mb_init_group() will block if the I/O\n * is not yet completed, or indeed if it was not initiated by\n * ext4_mb_prefetch did not start the I/O.\n *\n * TODO: We should actually kick off the buddy bitmap setup in a work\n * queue when the buffer I/O is completed, so that we don't block\n * waiting for the block allocation bitmap read to finish when\n * ext4_mb_prefetch_fini is called from ext4_mb_regular_allocator().\n */\nvoid ext4_mb_prefetch_fini(struct super_block *sb, ext4_group_t group,\n\t\t\t   unsigned int nr)\n{\n\twhile (nr-- > 0) {\n\t\tstruct ext4_group_desc *gdp = ext4_get_group_desc(sb, group,\n\t\t\t\t\t\t\t\t  NULL);\n\t\tstruct ext4_group_info *grp = ext4_get_group_info(sb, group);\n\n\t\tif (!group)\n\t\t\tgroup = ext4_get_groups_count(sb);\n\t\tgroup--;\n\t\tgrp = ext4_get_group_info(sb, group);\n\n\t\tif (EXT4_MB_GRP_NEED_INIT(grp) &&\n\t\t    ext4_free_group_clusters(sb, gdp) > 0 &&\n\t\t    !(ext4_has_group_desc_csum(sb) &&\n\t\t      (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)))) {\n\t\t\tif (ext4_mb_init_group(sb, group, GFP_NOFS))\n\t\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic noinline_for_stack int\next4_mb_regular_allocator(struct ext4_allocation_context *ac)\n{\n\text4_group_t prefetch_grp = 0, ngroups, group, i;\n\tint cr = -1;\n\tint err = 0, first_err = 0;\n\tunsigned int nr = 0, prefetch_ios = 0;\n\tstruct ext4_sb_info *sbi;\n\tstruct super_block *sb;\n\tstruct ext4_buddy e4b;\n\tint lost;\n\n\tsb = ac->ac_sb;\n\tsbi = EXT4_SB(sb);\n\tngroups = ext4_get_groups_count(sb);\n\t/* non-extent files are limited to low blocks/groups */\n\tif (!(ext4_test_inode_flag(ac->ac_inode, EXT4_INODE_EXTENTS)))\n\t\tngroups = sbi->s_blockfile_groups;\n\n\tBUG_ON(ac->ac_status == AC_STATUS_FOUND);\n\n\t/* first, try the goal */\n\terr = ext4_mb_find_by_goal(ac, &e4b);\n\tif (err || ac->ac_status == AC_STATUS_FOUND)\n\t\tgoto out;\n\n\tif (unlikely(ac->ac_flags & EXT4_MB_HINT_GOAL_ONLY))\n\t\tgoto out;\n\n\t/*\n\t * ac->ac_2order is set only if the fe_len is a power of 2\n\t * if ac->ac_2order is set we also set criteria to 0 so that we\n\t * try exact allocation using buddy.\n\t */\n\ti = fls(ac->ac_g_ex.fe_len);\n\tac->ac_2order = 0;\n\t/*\n\t * We search using buddy data only if the order of the request\n\t * is greater than equal to the sbi_s_mb_order2_reqs\n\t * You can tune it via /sys/fs/ext4/<partition>/mb_order2_req\n\t * We also support searching for power-of-two requests only for\n\t * requests upto maximum buddy size we have constructed.\n\t */\n\tif (i >= sbi->s_mb_order2_reqs && i <= sb->s_blocksize_bits + 2) {\n\t\t/*\n\t\t * This should tell if fe_len is exactly power of 2\n\t\t */\n\t\tif ((ac->ac_g_ex.fe_len & (~(1 << (i - 1)))) == 0)\n\t\t\tac->ac_2order = array_index_nospec(i - 1,\n\t\t\t\t\t\t\t   sb->s_blocksize_bits + 2);\n\t}\n\n\t/* if stream allocation is enabled, use global goal */\n\tif (ac->ac_flags & EXT4_MB_STREAM_ALLOC) {\n\t\t/* TBD: may be hot point */\n\t\tspin_lock(&sbi->s_md_lock);\n\t\tac->ac_g_ex.fe_group = sbi->s_mb_last_group;\n\t\tac->ac_g_ex.fe_start = sbi->s_mb_last_start;\n\t\tspin_unlock(&sbi->s_md_lock);\n\t}\n\n\t/* Let's just scan groups to find more-less suitable blocks */\n\tcr = ac->ac_2order ? 0 : 1;\n\t/*\n\t * cr == 0 try to get exact allocation,\n\t * cr == 3  try to get anything\n\t */\nrepeat:\n\tfor (; cr < 4 && ac->ac_status == AC_STATUS_CONTINUE; cr++) {\n\t\tac->ac_criteria = cr;\n\t\t/*\n\t\t * searching for the right group start\n\t\t * from the goal value specified\n\t\t */\n\t\tgroup = ac->ac_g_ex.fe_group;\n\t\tprefetch_grp = group;\n\n\t\tfor (i = 0; i < ngroups; group++, i++) {\n\t\t\tint ret = 0;\n\t\t\tcond_resched();\n\t\t\t/*\n\t\t\t * Artificially restricted ngroups for non-extent\n\t\t\t * files makes group > ngroups possible on first loop.\n\t\t\t */\n\t\t\tif (group >= ngroups)\n\t\t\t\tgroup = 0;\n\n\t\t\t/*\n\t\t\t * Batch reads of the block allocation bitmaps\n\t\t\t * to get multiple READs in flight; limit\n\t\t\t * prefetching at cr=0/1, otherwise mballoc can\n\t\t\t * spend a lot of time loading imperfect groups\n\t\t\t */\n\t\t\tif ((prefetch_grp == group) &&\n\t\t\t    (cr > 1 ||\n\t\t\t     prefetch_ios < sbi->s_mb_prefetch_limit)) {\n\t\t\t\tunsigned int curr_ios = prefetch_ios;\n\n\t\t\t\tnr = sbi->s_mb_prefetch;\n\t\t\t\tif (ext4_has_feature_flex_bg(sb)) {\n\t\t\t\t\tnr = 1 << sbi->s_log_groups_per_flex;\n\t\t\t\t\tnr -= group & (nr - 1);\n\t\t\t\t\tnr = min(nr, sbi->s_mb_prefetch);\n\t\t\t\t}\n\t\t\t\tprefetch_grp = ext4_mb_prefetch(sb, group,\n\t\t\t\t\t\t\tnr, &prefetch_ios);\n\t\t\t\tif (prefetch_ios == curr_ios)\n\t\t\t\t\tnr = 0;\n\t\t\t}\n\n\t\t\t/* This now checks without needing the buddy page */\n\t\t\tret = ext4_mb_good_group_nolock(ac, group, cr);\n\t\t\tif (ret <= 0) {\n\t\t\t\tif (!first_err)\n\t\t\t\t\tfirst_err = ret;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\terr = ext4_mb_load_buddy(sb, group, &e4b);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\text4_lock_group(sb, group);\n\n\t\t\t/*\n\t\t\t * We need to check again after locking the\n\t\t\t * block group\n\t\t\t */\n\t\t\tret = ext4_mb_good_group(ac, group, cr);\n\t\t\tif (ret == 0) {\n\t\t\t\text4_unlock_group(sb, group);\n\t\t\t\text4_mb_unload_buddy(&e4b);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tac->ac_groups_scanned++;\n\t\t\tif (cr == 0)\n\t\t\t\text4_mb_simple_scan_group(ac, &e4b);\n\t\t\telse if (cr == 1 && sbi->s_stripe &&\n\t\t\t\t\t!(ac->ac_g_ex.fe_len % sbi->s_stripe))\n\t\t\t\text4_mb_scan_aligned(ac, &e4b);\n\t\t\telse\n\t\t\t\text4_mb_complex_scan_group(ac, &e4b);\n\n\t\t\text4_unlock_group(sb, group);\n\t\t\text4_mb_unload_buddy(&e4b);\n\n\t\t\tif (ac->ac_status != AC_STATUS_CONTINUE)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (ac->ac_b_ex.fe_len > 0 && ac->ac_status != AC_STATUS_FOUND &&\n\t    !(ac->ac_flags & EXT4_MB_HINT_FIRST)) {\n\t\t/*\n\t\t * We've been searching too long. Let's try to allocate\n\t\t * the best chunk we've found so far\n\t\t */\n\t\text4_mb_try_best_found(ac, &e4b);\n\t\tif (ac->ac_status != AC_STATUS_FOUND) {\n\t\t\t/*\n\t\t\t * Someone more lucky has already allocated it.\n\t\t\t * The only thing we can do is just take first\n\t\t\t * found block(s)\n\t\t\t */\n\t\t\tlost = atomic_inc_return(&sbi->s_mb_lost_chunks);\n\t\t\tmb_debug(sb, \"lost chunk, group: %u, start: %d, len: %d, lost: %d\\n\",\n\t\t\t\t ac->ac_b_ex.fe_group, ac->ac_b_ex.fe_start,\n\t\t\t\t ac->ac_b_ex.fe_len, lost);\n\n\t\t\tac->ac_b_ex.fe_group = 0;\n\t\t\tac->ac_b_ex.fe_start = 0;\n\t\t\tac->ac_b_ex.fe_len = 0;\n\t\t\tac->ac_status = AC_STATUS_CONTINUE;\n\t\t\tac->ac_flags |= EXT4_MB_HINT_FIRST;\n\t\t\tcr = 3;\n\t\t\tgoto repeat;\n\t\t}\n\t}\nout:\n\tif (!err && ac->ac_status != AC_STATUS_FOUND && first_err)\n\t\terr = first_err;\n\n\tmb_debug(sb, \"Best len %d, origin len %d, ac_status %u, ac_flags 0x%x, cr %d ret %d\\n\",\n\t\t ac->ac_b_ex.fe_len, ac->ac_o_ex.fe_len, ac->ac_status,\n\t\t ac->ac_flags, cr, err);\n\n\tif (nr)\n\t\text4_mb_prefetch_fini(sb, prefetch_grp, nr);\n\n\treturn err;\n}\n\nstatic void *ext4_mb_seq_groups_start(struct seq_file *seq, loff_t *pos)\n{\n\tstruct super_block *sb = PDE_DATA(file_inode(seq->file));\n\text4_group_t group;\n\n\tif (*pos < 0 || *pos >= ext4_get_groups_count(sb))\n\t\treturn NULL;\n\tgroup = *pos + 1;\n\treturn (void *) ((unsigned long) group);\n}\n\nstatic void *ext4_mb_seq_groups_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct super_block *sb = PDE_DATA(file_inode(seq->file));\n\text4_group_t group;\n\n\t++*pos;\n\tif (*pos < 0 || *pos >= ext4_get_groups_count(sb))\n\t\treturn NULL;\n\tgroup = *pos + 1;\n\treturn (void *) ((unsigned long) group);\n}\n\nstatic int ext4_mb_seq_groups_show(struct seq_file *seq, void *v)\n{\n\tstruct super_block *sb = PDE_DATA(file_inode(seq->file));\n\text4_group_t group = (ext4_group_t) ((unsigned long) v);\n\tint i;\n\tint err, buddy_loaded = 0;\n\tstruct ext4_buddy e4b;\n\tstruct ext4_group_info *grinfo;\n\tunsigned char blocksize_bits = min_t(unsigned char,\n\t\t\t\t\t     sb->s_blocksize_bits,\n\t\t\t\t\t     EXT4_MAX_BLOCK_LOG_SIZE);\n\tstruct sg {\n\t\tstruct ext4_group_info info;\n\t\text4_grpblk_t counters[EXT4_MAX_BLOCK_LOG_SIZE + 2];\n\t} sg;\n\n\tgroup--;\n\tif (group == 0)\n\t\tseq_puts(seq, \"#group: free  frags first [\"\n\t\t\t      \" 2^0   2^1   2^2   2^3   2^4   2^5   2^6  \"\n\t\t\t      \" 2^7   2^8   2^9   2^10  2^11  2^12  2^13  ]\\n\");\n\n\ti = (blocksize_bits + 2) * sizeof(sg.info.bb_counters[0]) +\n\t\tsizeof(struct ext4_group_info);\n\n\tgrinfo = ext4_get_group_info(sb, group);\n\t/* Load the group info in memory only if not already loaded. */\n\tif (unlikely(EXT4_MB_GRP_NEED_INIT(grinfo))) {\n\t\terr = ext4_mb_load_buddy(sb, group, &e4b);\n\t\tif (err) {\n\t\t\tseq_printf(seq, \"#%-5u: I/O error\\n\", group);\n\t\t\treturn 0;\n\t\t}\n\t\tbuddy_loaded = 1;\n\t}\n\n\tmemcpy(&sg, ext4_get_group_info(sb, group), i);\n\n\tif (buddy_loaded)\n\t\text4_mb_unload_buddy(&e4b);\n\n\tseq_printf(seq, \"#%-5u: %-5u %-5u %-5u [\", group, sg.info.bb_free,\n\t\t\tsg.info.bb_fragments, sg.info.bb_first_free);\n\tfor (i = 0; i <= 13; i++)\n\t\tseq_printf(seq, \" %-5u\", i <= blocksize_bits + 1 ?\n\t\t\t\tsg.info.bb_counters[i] : 0);\n\tseq_puts(seq, \" ]\\n\");\n\n\treturn 0;\n}\n\nstatic void ext4_mb_seq_groups_stop(struct seq_file *seq, void *v)\n{\n}\n\nconst struct seq_operations ext4_mb_seq_groups_ops = {\n\t.start  = ext4_mb_seq_groups_start,\n\t.next   = ext4_mb_seq_groups_next,\n\t.stop   = ext4_mb_seq_groups_stop,\n\t.show   = ext4_mb_seq_groups_show,\n};\n\nstatic struct kmem_cache *get_groupinfo_cache(int blocksize_bits)\n{\n\tint cache_index = blocksize_bits - EXT4_MIN_BLOCK_LOG_SIZE;\n\tstruct kmem_cache *cachep = ext4_groupinfo_caches[cache_index];\n\n\tBUG_ON(!cachep);\n\treturn cachep;\n}\n\n/*\n * Allocate the top-level s_group_info array for the specified number\n * of groups\n */\nint ext4_mb_alloc_groupinfo(struct super_block *sb, ext4_group_t ngroups)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tunsigned size;\n\tstruct ext4_group_info ***old_groupinfo, ***new_groupinfo;\n\n\tsize = (ngroups + EXT4_DESC_PER_BLOCK(sb) - 1) >>\n\t\tEXT4_DESC_PER_BLOCK_BITS(sb);\n\tif (size <= sbi->s_group_info_size)\n\t\treturn 0;\n\n\tsize = roundup_pow_of_two(sizeof(*sbi->s_group_info) * size);\n\tnew_groupinfo = kvzalloc(size, GFP_KERNEL);\n\tif (!new_groupinfo) {\n\t\text4_msg(sb, KERN_ERR, \"can't allocate buddy meta group\");\n\t\treturn -ENOMEM;\n\t}\n\trcu_read_lock();\n\told_groupinfo = rcu_dereference(sbi->s_group_info);\n\tif (old_groupinfo)\n\t\tmemcpy(new_groupinfo, old_groupinfo,\n\t\t       sbi->s_group_info_size * sizeof(*sbi->s_group_info));\n\trcu_read_unlock();\n\trcu_assign_pointer(sbi->s_group_info, new_groupinfo);\n\tsbi->s_group_info_size = size / sizeof(*sbi->s_group_info);\n\tif (old_groupinfo)\n\t\text4_kvfree_array_rcu(old_groupinfo);\n\text4_debug(\"allocated s_groupinfo array for %d meta_bg's\\n\", \n\t\t   sbi->s_group_info_size);\n\treturn 0;\n}\n\n/* Create and initialize ext4_group_info data for the given group. */\nint ext4_mb_add_groupinfo(struct super_block *sb, ext4_group_t group,\n\t\t\t  struct ext4_group_desc *desc)\n{\n\tint i;\n\tint metalen = 0;\n\tint idx = group >> EXT4_DESC_PER_BLOCK_BITS(sb);\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_info **meta_group_info;\n\tstruct kmem_cache *cachep = get_groupinfo_cache(sb->s_blocksize_bits);\n\n\t/*\n\t * First check if this group is the first of a reserved block.\n\t * If it's true, we have to allocate a new table of pointers\n\t * to ext4_group_info structures\n\t */\n\tif (group % EXT4_DESC_PER_BLOCK(sb) == 0) {\n\t\tmetalen = sizeof(*meta_group_info) <<\n\t\t\tEXT4_DESC_PER_BLOCK_BITS(sb);\n\t\tmeta_group_info = kmalloc(metalen, GFP_NOFS);\n\t\tif (meta_group_info == NULL) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't allocate mem \"\n\t\t\t\t \"for a buddy group\");\n\t\t\tgoto exit_meta_group_info;\n\t\t}\n\t\trcu_read_lock();\n\t\trcu_dereference(sbi->s_group_info)[idx] = meta_group_info;\n\t\trcu_read_unlock();\n\t}\n\n\tmeta_group_info = sbi_array_rcu_deref(sbi, s_group_info, idx);\n\ti = group & (EXT4_DESC_PER_BLOCK(sb) - 1);\n\n\tmeta_group_info[i] = kmem_cache_zalloc(cachep, GFP_NOFS);\n\tif (meta_group_info[i] == NULL) {\n\t\text4_msg(sb, KERN_ERR, \"can't allocate buddy mem\");\n\t\tgoto exit_group_info;\n\t}\n\tset_bit(EXT4_GROUP_INFO_NEED_INIT_BIT,\n\t\t&(meta_group_info[i]->bb_state));\n\n\t/*\n\t * initialize bb_free to be able to skip\n\t * empty groups without initialization\n\t */\n\tif (ext4_has_group_desc_csum(sb) &&\n\t    (desc->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT))) {\n\t\tmeta_group_info[i]->bb_free =\n\t\t\text4_free_clusters_after_init(sb, group, desc);\n\t} else {\n\t\tmeta_group_info[i]->bb_free =\n\t\t\text4_free_group_clusters(sb, desc);\n\t}\n\n\tINIT_LIST_HEAD(&meta_group_info[i]->bb_prealloc_list);\n\tinit_rwsem(&meta_group_info[i]->alloc_sem);\n\tmeta_group_info[i]->bb_free_root = RB_ROOT;\n\tmeta_group_info[i]->bb_largest_free_order = -1;  /* uninit */\n\n\tmb_group_bb_bitmap_alloc(sb, meta_group_info[i], group);\n\treturn 0;\n\nexit_group_info:\n\t/* If a meta_group_info table has been allocated, release it now */\n\tif (group % EXT4_DESC_PER_BLOCK(sb) == 0) {\n\t\tstruct ext4_group_info ***group_info;\n\n\t\trcu_read_lock();\n\t\tgroup_info = rcu_dereference(sbi->s_group_info);\n\t\tkfree(group_info[idx]);\n\t\tgroup_info[idx] = NULL;\n\t\trcu_read_unlock();\n\t}\nexit_meta_group_info:\n\treturn -ENOMEM;\n} /* ext4_mb_add_groupinfo */\n\nstatic int ext4_mb_init_backend(struct super_block *sb)\n{\n\text4_group_t ngroups = ext4_get_groups_count(sb);\n\text4_group_t i;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tint err;\n\tstruct ext4_group_desc *desc;\n\tstruct ext4_group_info ***group_info;\n\tstruct kmem_cache *cachep;\n\n\terr = ext4_mb_alloc_groupinfo(sb, ngroups);\n\tif (err)\n\t\treturn err;\n\n\tsbi->s_buddy_cache = new_inode(sb);\n\tif (sbi->s_buddy_cache == NULL) {\n\t\text4_msg(sb, KERN_ERR, \"can't get new inode\");\n\t\tgoto err_freesgi;\n\t}\n\t/* To avoid potentially colliding with an valid on-disk inode number,\n\t * use EXT4_BAD_INO for the buddy cache inode number.  This inode is\n\t * not in the inode hash, so it should never be found by iget(), but\n\t * this will avoid confusion if it ever shows up during debugging. */\n\tsbi->s_buddy_cache->i_ino = EXT4_BAD_INO;\n\tEXT4_I(sbi->s_buddy_cache)->i_disksize = 0;\n\tfor (i = 0; i < ngroups; i++) {\n\t\tcond_resched();\n\t\tdesc = ext4_get_group_desc(sb, i, NULL);\n\t\tif (desc == NULL) {\n\t\t\text4_msg(sb, KERN_ERR, \"can't read descriptor %u\", i);\n\t\t\tgoto err_freebuddy;\n\t\t}\n\t\tif (ext4_mb_add_groupinfo(sb, i, desc) != 0)\n\t\t\tgoto err_freebuddy;\n\t}\n\n\tif (ext4_has_feature_flex_bg(sb)) {\n\t\t/* a single flex group is supposed to be read by a single IO.\n\t\t * 2 ^ s_log_groups_per_flex != UINT_MAX as s_mb_prefetch is\n\t\t * unsigned integer, so the maximum shift is 32.\n\t\t */\n\t\tif (sbi->s_es->s_log_groups_per_flex >= 32) {\n\t\t\text4_msg(sb, KERN_ERR, \"too many log groups per flexible block group\");\n\t\t\tgoto err_freesgi;\n\t\t}\n\t\tsbi->s_mb_prefetch = min_t(uint, 1 << sbi->s_es->s_log_groups_per_flex,\n\t\t\tBLK_MAX_SEGMENT_SIZE >> (sb->s_blocksize_bits - 9));\n\t\tsbi->s_mb_prefetch *= 8; /* 8 prefetch IOs in flight at most */\n\t} else {\n\t\tsbi->s_mb_prefetch = 32;\n\t}\n\tif (sbi->s_mb_prefetch > ext4_get_groups_count(sb))\n\t\tsbi->s_mb_prefetch = ext4_get_groups_count(sb);\n\t/* now many real IOs to prefetch within a single allocation at cr=0\n\t * given cr=0 is an CPU-related optimization we shouldn't try to\n\t * load too many groups, at some point we should start to use what\n\t * we've got in memory.\n\t * with an average random access time 5ms, it'd take a second to get\n\t * 200 groups (* N with flex_bg), so let's make this limit 4\n\t */\n\tsbi->s_mb_prefetch_limit = sbi->s_mb_prefetch * 4;\n\tif (sbi->s_mb_prefetch_limit > ext4_get_groups_count(sb))\n\t\tsbi->s_mb_prefetch_limit = ext4_get_groups_count(sb);\n\n\treturn 0;\n\nerr_freebuddy:\n\tcachep = get_groupinfo_cache(sb->s_blocksize_bits);\n\twhile (i-- > 0)\n\t\tkmem_cache_free(cachep, ext4_get_group_info(sb, i));\n\ti = sbi->s_group_info_size;\n\trcu_read_lock();\n\tgroup_info = rcu_dereference(sbi->s_group_info);\n\twhile (i-- > 0)\n\t\tkfree(group_info[i]);\n\trcu_read_unlock();\n\tiput(sbi->s_buddy_cache);\nerr_freesgi:\n\trcu_read_lock();\n\tkvfree(rcu_dereference(sbi->s_group_info));\n\trcu_read_unlock();\n\treturn -ENOMEM;\n}\n\nstatic void ext4_groupinfo_destroy_slabs(void)\n{\n\tint i;\n\n\tfor (i = 0; i < NR_GRPINFO_CACHES; i++) {\n\t\tkmem_cache_destroy(ext4_groupinfo_caches[i]);\n\t\text4_groupinfo_caches[i] = NULL;\n\t}\n}\n\nstatic int ext4_groupinfo_create_slab(size_t size)\n{\n\tstatic DEFINE_MUTEX(ext4_grpinfo_slab_create_mutex);\n\tint slab_size;\n\tint blocksize_bits = order_base_2(size);\n\tint cache_index = blocksize_bits - EXT4_MIN_BLOCK_LOG_SIZE;\n\tstruct kmem_cache *cachep;\n\n\tif (cache_index >= NR_GRPINFO_CACHES)\n\t\treturn -EINVAL;\n\n\tif (unlikely(cache_index < 0))\n\t\tcache_index = 0;\n\n\tmutex_lock(&ext4_grpinfo_slab_create_mutex);\n\tif (ext4_groupinfo_caches[cache_index]) {\n\t\tmutex_unlock(&ext4_grpinfo_slab_create_mutex);\n\t\treturn 0;\t/* Already created */\n\t}\n\n\tslab_size = offsetof(struct ext4_group_info,\n\t\t\t\tbb_counters[blocksize_bits + 2]);\n\n\tcachep = kmem_cache_create(ext4_groupinfo_slab_names[cache_index],\n\t\t\t\t\tslab_size, 0, SLAB_RECLAIM_ACCOUNT,\n\t\t\t\t\tNULL);\n\n\text4_groupinfo_caches[cache_index] = cachep;\n\n\tmutex_unlock(&ext4_grpinfo_slab_create_mutex);\n\tif (!cachep) {\n\t\tprintk(KERN_EMERG\n\t\t       \"EXT4-fs: no memory for groupinfo slab cache\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nint ext4_mb_init(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tunsigned i, j;\n\tunsigned offset, offset_incr;\n\tunsigned max;\n\tint ret;\n\n\ti = (sb->s_blocksize_bits + 2) * sizeof(*sbi->s_mb_offsets);\n\n\tsbi->s_mb_offsets = kmalloc(i, GFP_KERNEL);\n\tif (sbi->s_mb_offsets == NULL) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\ti = (sb->s_blocksize_bits + 2) * sizeof(*sbi->s_mb_maxs);\n\tsbi->s_mb_maxs = kmalloc(i, GFP_KERNEL);\n\tif (sbi->s_mb_maxs == NULL) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tret = ext4_groupinfo_create_slab(sb->s_blocksize);\n\tif (ret < 0)\n\t\tgoto out;\n\n\t/* order 0 is regular bitmap */\n\tsbi->s_mb_maxs[0] = sb->s_blocksize << 3;\n\tsbi->s_mb_offsets[0] = 0;\n\n\ti = 1;\n\toffset = 0;\n\toffset_incr = 1 << (sb->s_blocksize_bits - 1);\n\tmax = sb->s_blocksize << 2;\n\tdo {\n\t\tsbi->s_mb_offsets[i] = offset;\n\t\tsbi->s_mb_maxs[i] = max;\n\t\toffset += offset_incr;\n\t\toffset_incr = offset_incr >> 1;\n\t\tmax = max >> 1;\n\t\ti++;\n\t} while (i <= sb->s_blocksize_bits + 1);\n\n\tspin_lock_init(&sbi->s_md_lock);\n\tspin_lock_init(&sbi->s_bal_lock);\n\tsbi->s_mb_free_pending = 0;\n\tINIT_LIST_HEAD(&sbi->s_freed_data_list);\n\n\tsbi->s_mb_max_to_scan = MB_DEFAULT_MAX_TO_SCAN;\n\tsbi->s_mb_min_to_scan = MB_DEFAULT_MIN_TO_SCAN;\n\tsbi->s_mb_stats = MB_DEFAULT_STATS;\n\tsbi->s_mb_stream_request = MB_DEFAULT_STREAM_THRESHOLD;\n\tsbi->s_mb_order2_reqs = MB_DEFAULT_ORDER2_REQS;\n\tsbi->s_mb_max_inode_prealloc = MB_DEFAULT_MAX_INODE_PREALLOC;\n\t/*\n\t * The default group preallocation is 512, which for 4k block\n\t * sizes translates to 2 megabytes.  However for bigalloc file\n\t * systems, this is probably too big (i.e, if the cluster size\n\t * is 1 megabyte, then group preallocation size becomes half a\n\t * gigabyte!).  As a default, we will keep a two megabyte\n\t * group pralloc size for cluster sizes up to 64k, and after\n\t * that, we will force a minimum group preallocation size of\n\t * 32 clusters.  This translates to 8 megs when the cluster\n\t * size is 256k, and 32 megs when the cluster size is 1 meg,\n\t * which seems reasonable as a default.\n\t */\n\tsbi->s_mb_group_prealloc = max(MB_DEFAULT_GROUP_PREALLOC >>\n\t\t\t\t       sbi->s_cluster_bits, 32);\n\t/*\n\t * If there is a s_stripe > 1, then we set the s_mb_group_prealloc\n\t * to the lowest multiple of s_stripe which is bigger than\n\t * the s_mb_group_prealloc as determined above. We want\n\t * the preallocation size to be an exact multiple of the\n\t * RAID stripe size so that preallocations don't fragment\n\t * the stripes.\n\t */\n\tif (sbi->s_stripe > 1) {\n\t\tsbi->s_mb_group_prealloc = roundup(\n\t\t\tsbi->s_mb_group_prealloc, sbi->s_stripe);\n\t}\n\n\tsbi->s_locality_groups = alloc_percpu(struct ext4_locality_group);\n\tif (sbi->s_locality_groups == NULL) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tfor_each_possible_cpu(i) {\n\t\tstruct ext4_locality_group *lg;\n\t\tlg = per_cpu_ptr(sbi->s_locality_groups, i);\n\t\tmutex_init(&lg->lg_mutex);\n\t\tfor (j = 0; j < PREALLOC_TB_SIZE; j++)\n\t\t\tINIT_LIST_HEAD(&lg->lg_prealloc_list[j]);\n\t\tspin_lock_init(&lg->lg_prealloc_lock);\n\t}\n\n\t/* init file for buddy data */\n\tret = ext4_mb_init_backend(sb);\n\tif (ret != 0)\n\t\tgoto out_free_locality_groups;\n\n\treturn 0;\n\nout_free_locality_groups:\n\tfree_percpu(sbi->s_locality_groups);\n\tsbi->s_locality_groups = NULL;\nout:\n\tkfree(sbi->s_mb_offsets);\n\tsbi->s_mb_offsets = NULL;\n\tkfree(sbi->s_mb_maxs);\n\tsbi->s_mb_maxs = NULL;\n\treturn ret;\n}\n\n/* need to called with the ext4 group lock held */\nstatic int ext4_mb_cleanup_pa(struct ext4_group_info *grp)\n{\n\tstruct ext4_prealloc_space *pa;\n\tstruct list_head *cur, *tmp;\n\tint count = 0;\n\n\tlist_for_each_safe(cur, tmp, &grp->bb_prealloc_list) {\n\t\tpa = list_entry(cur, struct ext4_prealloc_space, pa_group_list);\n\t\tlist_del(&pa->pa_group_list);\n\t\tcount++;\n\t\tkmem_cache_free(ext4_pspace_cachep, pa);\n\t}\n\treturn count;\n}\n\nint ext4_mb_release(struct super_block *sb)\n{\n\text4_group_t ngroups = ext4_get_groups_count(sb);\n\text4_group_t i;\n\tint num_meta_group_infos;\n\tstruct ext4_group_info *grinfo, ***group_info;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct kmem_cache *cachep = get_groupinfo_cache(sb->s_blocksize_bits);\n\tint count;\n\n\tif (sbi->s_group_info) {\n\t\tfor (i = 0; i < ngroups; i++) {\n\t\t\tcond_resched();\n\t\t\tgrinfo = ext4_get_group_info(sb, i);\n\t\t\tmb_group_bb_bitmap_free(grinfo);\n\t\t\text4_lock_group(sb, i);\n\t\t\tcount = ext4_mb_cleanup_pa(grinfo);\n\t\t\tif (count)\n\t\t\t\tmb_debug(sb, \"mballoc: %d PAs left\\n\",\n\t\t\t\t\t count);\n\t\t\text4_unlock_group(sb, i);\n\t\t\tkmem_cache_free(cachep, grinfo);\n\t\t}\n\t\tnum_meta_group_infos = (ngroups +\n\t\t\t\tEXT4_DESC_PER_BLOCK(sb) - 1) >>\n\t\t\tEXT4_DESC_PER_BLOCK_BITS(sb);\n\t\trcu_read_lock();\n\t\tgroup_info = rcu_dereference(sbi->s_group_info);\n\t\tfor (i = 0; i < num_meta_group_infos; i++)\n\t\t\tkfree(group_info[i]);\n\t\tkvfree(group_info);\n\t\trcu_read_unlock();\n\t}\n\tkfree(sbi->s_mb_offsets);\n\tkfree(sbi->s_mb_maxs);\n\tiput(sbi->s_buddy_cache);\n\tif (sbi->s_mb_stats) {\n\t\text4_msg(sb, KERN_INFO,\n\t\t       \"mballoc: %u blocks %u reqs (%u success)\",\n\t\t\t\tatomic_read(&sbi->s_bal_allocated),\n\t\t\t\tatomic_read(&sbi->s_bal_reqs),\n\t\t\t\tatomic_read(&sbi->s_bal_success));\n\t\text4_msg(sb, KERN_INFO,\n\t\t      \"mballoc: %u extents scanned, %u goal hits, \"\n\t\t\t\t\"%u 2^N hits, %u breaks, %u lost\",\n\t\t\t\tatomic_read(&sbi->s_bal_ex_scanned),\n\t\t\t\tatomic_read(&sbi->s_bal_goals),\n\t\t\t\tatomic_read(&sbi->s_bal_2orders),\n\t\t\t\tatomic_read(&sbi->s_bal_breaks),\n\t\t\t\tatomic_read(&sbi->s_mb_lost_chunks));\n\t\text4_msg(sb, KERN_INFO,\n\t\t       \"mballoc: %lu generated and it took %Lu\",\n\t\t\t\tsbi->s_mb_buddies_generated,\n\t\t\t\tsbi->s_mb_generation_time);\n\t\text4_msg(sb, KERN_INFO,\n\t\t       \"mballoc: %u preallocated, %u discarded\",\n\t\t\t\tatomic_read(&sbi->s_mb_preallocated),\n\t\t\t\tatomic_read(&sbi->s_mb_discarded));\n\t}\n\n\tfree_percpu(sbi->s_locality_groups);\n\n\treturn 0;\n}\n\nstatic inline int ext4_issue_discard(struct super_block *sb,\n\t\text4_group_t block_group, ext4_grpblk_t cluster, int count,\n\t\tstruct bio **biop)\n{\n\text4_fsblk_t discard_block;\n\n\tdiscard_block = (EXT4_C2B(EXT4_SB(sb), cluster) +\n\t\t\t ext4_group_first_block_no(sb, block_group));\n\tcount = EXT4_C2B(EXT4_SB(sb), count);\n\ttrace_ext4_discard_blocks(sb,\n\t\t\t(unsigned long long) discard_block, count);\n\tif (biop) {\n\t\treturn __blkdev_issue_discard(sb->s_bdev,\n\t\t\t(sector_t)discard_block << (sb->s_blocksize_bits - 9),\n\t\t\t(sector_t)count << (sb->s_blocksize_bits - 9),\n\t\t\tGFP_NOFS, 0, biop);\n\t} else\n\t\treturn sb_issue_discard(sb, discard_block, count, GFP_NOFS, 0);\n}\n\nstatic void ext4_free_data_in_buddy(struct super_block *sb,\n\t\t\t\t    struct ext4_free_data *entry)\n{\n\tstruct ext4_buddy e4b;\n\tstruct ext4_group_info *db;\n\tint err, count = 0, count2 = 0;\n\n\tmb_debug(sb, \"gonna free %u blocks in group %u (0x%p):\",\n\t\t entry->efd_count, entry->efd_group, entry);\n\n\terr = ext4_mb_load_buddy(sb, entry->efd_group, &e4b);\n\t/* we expect to find existing buddy because it's pinned */\n\tBUG_ON(err != 0);\n\n\tspin_lock(&EXT4_SB(sb)->s_md_lock);\n\tEXT4_SB(sb)->s_mb_free_pending -= entry->efd_count;\n\tspin_unlock(&EXT4_SB(sb)->s_md_lock);\n\n\tdb = e4b.bd_info;\n\t/* there are blocks to put in buddy to make them really free */\n\tcount += entry->efd_count;\n\tcount2++;\n\text4_lock_group(sb, entry->efd_group);\n\t/* Take it out of per group rb tree */\n\trb_erase(&entry->efd_node, &(db->bb_free_root));\n\tmb_free_blocks(NULL, &e4b, entry->efd_start_cluster, entry->efd_count);\n\n\t/*\n\t * Clear the trimmed flag for the group so that the next\n\t * ext4_trim_fs can trim it.\n\t * If the volume is mounted with -o discard, online discard\n\t * is supported and the free blocks will be trimmed online.\n\t */\n\tif (!test_opt(sb, DISCARD))\n\t\tEXT4_MB_GRP_CLEAR_TRIMMED(db);\n\n\tif (!db->bb_free_root.rb_node) {\n\t\t/* No more items in the per group rb tree\n\t\t * balance refcounts from ext4_mb_free_metadata()\n\t\t */\n\t\tput_page(e4b.bd_buddy_page);\n\t\tput_page(e4b.bd_bitmap_page);\n\t}\n\text4_unlock_group(sb, entry->efd_group);\n\tkmem_cache_free(ext4_free_data_cachep, entry);\n\text4_mb_unload_buddy(&e4b);\n\n\tmb_debug(sb, \"freed %d blocks in %d structures\\n\", count,\n\t\t count2);\n}\n\n/*\n * This function is called by the jbd2 layer once the commit has finished,\n * so we know we can free the blocks that were released with that commit.\n */\nvoid ext4_process_freed_data(struct super_block *sb, tid_t commit_tid)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_free_data *entry, *tmp;\n\tstruct bio *discard_bio = NULL;\n\tstruct list_head freed_data_list;\n\tstruct list_head *cut_pos = NULL;\n\tint err;\n\n\tINIT_LIST_HEAD(&freed_data_list);\n\n\tspin_lock(&sbi->s_md_lock);\n\tlist_for_each_entry(entry, &sbi->s_freed_data_list, efd_list) {\n\t\tif (entry->efd_tid != commit_tid)\n\t\t\tbreak;\n\t\tcut_pos = &entry->efd_list;\n\t}\n\tif (cut_pos)\n\t\tlist_cut_position(&freed_data_list, &sbi->s_freed_data_list,\n\t\t\t\t  cut_pos);\n\tspin_unlock(&sbi->s_md_lock);\n\n\tif (test_opt(sb, DISCARD)) {\n\t\tlist_for_each_entry(entry, &freed_data_list, efd_list) {\n\t\t\terr = ext4_issue_discard(sb, entry->efd_group,\n\t\t\t\t\t\t entry->efd_start_cluster,\n\t\t\t\t\t\t entry->efd_count,\n\t\t\t\t\t\t &discard_bio);\n\t\t\tif (err && err != -EOPNOTSUPP) {\n\t\t\t\text4_msg(sb, KERN_WARNING, \"discard request in\"\n\t\t\t\t\t \" group:%d block:%d count:%d failed\"\n\t\t\t\t\t \" with %d\", entry->efd_group,\n\t\t\t\t\t entry->efd_start_cluster,\n\t\t\t\t\t entry->efd_count, err);\n\t\t\t} else if (err == -EOPNOTSUPP)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (discard_bio) {\n\t\t\tsubmit_bio_wait(discard_bio);\n\t\t\tbio_put(discard_bio);\n\t\t}\n\t}\n\n\tlist_for_each_entry_safe(entry, tmp, &freed_data_list, efd_list)\n\t\text4_free_data_in_buddy(sb, entry);\n}\n\nint __init ext4_init_mballoc(void)\n{\n\text4_pspace_cachep = KMEM_CACHE(ext4_prealloc_space,\n\t\t\t\t\tSLAB_RECLAIM_ACCOUNT);\n\tif (ext4_pspace_cachep == NULL)\n\t\tgoto out;\n\n\text4_ac_cachep = KMEM_CACHE(ext4_allocation_context,\n\t\t\t\t    SLAB_RECLAIM_ACCOUNT);\n\tif (ext4_ac_cachep == NULL)\n\t\tgoto out_pa_free;\n\n\text4_free_data_cachep = KMEM_CACHE(ext4_free_data,\n\t\t\t\t\t   SLAB_RECLAIM_ACCOUNT);\n\tif (ext4_free_data_cachep == NULL)\n\t\tgoto out_ac_free;\n\n\treturn 0;\n\nout_ac_free:\n\tkmem_cache_destroy(ext4_ac_cachep);\nout_pa_free:\n\tkmem_cache_destroy(ext4_pspace_cachep);\nout:\n\treturn -ENOMEM;\n}\n\nvoid ext4_exit_mballoc(void)\n{\n\t/*\n\t * Wait for completion of call_rcu()'s on ext4_pspace_cachep\n\t * before destroying the slab cache.\n\t */\n\trcu_barrier();\n\tkmem_cache_destroy(ext4_pspace_cachep);\n\tkmem_cache_destroy(ext4_ac_cachep);\n\tkmem_cache_destroy(ext4_free_data_cachep);\n\text4_groupinfo_destroy_slabs();\n}\n\n\n/*\n * Check quota and mark chosen space (ac->ac_b_ex) non-free in bitmaps\n * Returns 0 if success or error code\n */\nstatic noinline_for_stack int\next4_mb_mark_diskspace_used(struct ext4_allocation_context *ac,\n\t\t\t\thandle_t *handle, unsigned int reserv_clstrs)\n{\n\tstruct buffer_head *bitmap_bh = NULL;\n\tstruct ext4_group_desc *gdp;\n\tstruct buffer_head *gdp_bh;\n\tstruct ext4_sb_info *sbi;\n\tstruct super_block *sb;\n\text4_fsblk_t block;\n\tint err, len;\n\n\tBUG_ON(ac->ac_status != AC_STATUS_FOUND);\n\tBUG_ON(ac->ac_b_ex.fe_len <= 0);\n\n\tsb = ac->ac_sb;\n\tsbi = EXT4_SB(sb);\n\n\tbitmap_bh = ext4_read_block_bitmap(sb, ac->ac_b_ex.fe_group);\n\tif (IS_ERR(bitmap_bh)) {\n\t\terr = PTR_ERR(bitmap_bh);\n\t\tbitmap_bh = NULL;\n\t\tgoto out_err;\n\t}\n\n\tBUFFER_TRACE(bitmap_bh, \"getting write access\");\n\terr = ext4_journal_get_write_access(handle, bitmap_bh);\n\tif (err)\n\t\tgoto out_err;\n\n\terr = -EIO;\n\tgdp = ext4_get_group_desc(sb, ac->ac_b_ex.fe_group, &gdp_bh);\n\tif (!gdp)\n\t\tgoto out_err;\n\n\text4_debug(\"using block group %u(%d)\\n\", ac->ac_b_ex.fe_group,\n\t\t\text4_free_group_clusters(sb, gdp));\n\n\tBUFFER_TRACE(gdp_bh, \"get_write_access\");\n\terr = ext4_journal_get_write_access(handle, gdp_bh);\n\tif (err)\n\t\tgoto out_err;\n\n\tblock = ext4_grp_offs_to_block(sb, &ac->ac_b_ex);\n\n\tlen = EXT4_C2B(sbi, ac->ac_b_ex.fe_len);\n\tif (!ext4_inode_block_valid(ac->ac_inode, block, len)) {\n\t\text4_error(sb, \"Allocating blocks %llu-%llu which overlap \"\n\t\t\t   \"fs metadata\", block, block+len);\n\t\t/* File system mounted not to panic on error\n\t\t * Fix the bitmap and return EFSCORRUPTED\n\t\t * We leak some of the blocks here.\n\t\t */\n\t\text4_lock_group(sb, ac->ac_b_ex.fe_group);\n\t\text4_set_bits(bitmap_bh->b_data, ac->ac_b_ex.fe_start,\n\t\t\t      ac->ac_b_ex.fe_len);\n\t\text4_unlock_group(sb, ac->ac_b_ex.fe_group);\n\t\terr = ext4_handle_dirty_metadata(handle, NULL, bitmap_bh);\n\t\tif (!err)\n\t\t\terr = -EFSCORRUPTED;\n\t\tgoto out_err;\n\t}\n\n\text4_lock_group(sb, ac->ac_b_ex.fe_group);\n#ifdef AGGRESSIVE_CHECK\n\t{\n\t\tint i;\n\t\tfor (i = 0; i < ac->ac_b_ex.fe_len; i++) {\n\t\t\tBUG_ON(mb_test_bit(ac->ac_b_ex.fe_start + i,\n\t\t\t\t\t\tbitmap_bh->b_data));\n\t\t}\n\t}\n#endif\n\text4_set_bits(bitmap_bh->b_data, ac->ac_b_ex.fe_start,\n\t\t      ac->ac_b_ex.fe_len);\n\tif (ext4_has_group_desc_csum(sb) &&\n\t    (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT))) {\n\t\tgdp->bg_flags &= cpu_to_le16(~EXT4_BG_BLOCK_UNINIT);\n\t\text4_free_group_clusters_set(sb, gdp,\n\t\t\t\t\t     ext4_free_clusters_after_init(sb,\n\t\t\t\t\t\tac->ac_b_ex.fe_group, gdp));\n\t}\n\tlen = ext4_free_group_clusters(sb, gdp) - ac->ac_b_ex.fe_len;\n\text4_free_group_clusters_set(sb, gdp, len);\n\text4_block_bitmap_csum_set(sb, ac->ac_b_ex.fe_group, gdp, bitmap_bh);\n\text4_group_desc_csum_set(sb, ac->ac_b_ex.fe_group, gdp);\n\n\text4_unlock_group(sb, ac->ac_b_ex.fe_group);\n\tpercpu_counter_sub(&sbi->s_freeclusters_counter, ac->ac_b_ex.fe_len);\n\t/*\n\t * Now reduce the dirty block count also. Should not go negative\n\t */\n\tif (!(ac->ac_flags & EXT4_MB_DELALLOC_RESERVED))\n\t\t/* release all the reserved blocks if non delalloc */\n\t\tpercpu_counter_sub(&sbi->s_dirtyclusters_counter,\n\t\t\t\t   reserv_clstrs);\n\n\tif (sbi->s_log_groups_per_flex) {\n\t\text4_group_t flex_group = ext4_flex_group(sbi,\n\t\t\t\t\t\t\t  ac->ac_b_ex.fe_group);\n\t\tatomic64_sub(ac->ac_b_ex.fe_len,\n\t\t\t     &sbi_array_rcu_deref(sbi, s_flex_groups,\n\t\t\t\t\t\t  flex_group)->free_clusters);\n\t}\n\n\terr = ext4_handle_dirty_metadata(handle, NULL, bitmap_bh);\n\tif (err)\n\t\tgoto out_err;\n\terr = ext4_handle_dirty_metadata(handle, NULL, gdp_bh);\n\nout_err:\n\tbrelse(bitmap_bh);\n\treturn err;\n}\n\n/*\n * Idempotent helper for Ext4 fast commit replay path to set the state of\n * blocks in bitmaps and update counters.\n */\nvoid ext4_mb_mark_bb(struct super_block *sb, ext4_fsblk_t block,\n\t\t\tint len, int state)\n{\n\tstruct buffer_head *bitmap_bh = NULL;\n\tstruct ext4_group_desc *gdp;\n\tstruct buffer_head *gdp_bh;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_group_t group;\n\text4_grpblk_t blkoff;\n\tint i, clen, err;\n\tint already;\n\n\tclen = EXT4_B2C(sbi, len);\n\n\text4_get_group_no_and_offset(sb, block, &group, &blkoff);\n\tbitmap_bh = ext4_read_block_bitmap(sb, group);\n\tif (IS_ERR(bitmap_bh)) {\n\t\terr = PTR_ERR(bitmap_bh);\n\t\tbitmap_bh = NULL;\n\t\tgoto out_err;\n\t}\n\n\terr = -EIO;\n\tgdp = ext4_get_group_desc(sb, group, &gdp_bh);\n\tif (!gdp)\n\t\tgoto out_err;\n\n\text4_lock_group(sb, group);\n\talready = 0;\n\tfor (i = 0; i < clen; i++)\n\t\tif (!mb_test_bit(blkoff + i, bitmap_bh->b_data) == !state)\n\t\t\talready++;\n\n\tif (state)\n\t\text4_set_bits(bitmap_bh->b_data, blkoff, clen);\n\telse\n\t\tmb_test_and_clear_bits(bitmap_bh->b_data, blkoff, clen);\n\tif (ext4_has_group_desc_csum(sb) &&\n\t    (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT))) {\n\t\tgdp->bg_flags &= cpu_to_le16(~EXT4_BG_BLOCK_UNINIT);\n\t\text4_free_group_clusters_set(sb, gdp,\n\t\t\t\t\t     ext4_free_clusters_after_init(sb,\n\t\t\t\t\t\tgroup, gdp));\n\t}\n\tif (state)\n\t\tclen = ext4_free_group_clusters(sb, gdp) - clen + already;\n\telse\n\t\tclen = ext4_free_group_clusters(sb, gdp) + clen - already;\n\n\text4_free_group_clusters_set(sb, gdp, clen);\n\text4_block_bitmap_csum_set(sb, group, gdp, bitmap_bh);\n\text4_group_desc_csum_set(sb, group, gdp);\n\n\text4_unlock_group(sb, group);\n\n\tif (sbi->s_log_groups_per_flex) {\n\t\text4_group_t flex_group = ext4_flex_group(sbi, group);\n\n\t\tatomic64_sub(len,\n\t\t\t     &sbi_array_rcu_deref(sbi, s_flex_groups,\n\t\t\t\t\t\t  flex_group)->free_clusters);\n\t}\n\n\terr = ext4_handle_dirty_metadata(NULL, NULL, bitmap_bh);\n\tif (err)\n\t\tgoto out_err;\n\tsync_dirty_buffer(bitmap_bh);\n\terr = ext4_handle_dirty_metadata(NULL, NULL, gdp_bh);\n\tsync_dirty_buffer(gdp_bh);\n\nout_err:\n\tbrelse(bitmap_bh);\n}\n\n/*\n * here we normalize request for locality group\n * Group request are normalized to s_mb_group_prealloc, which goes to\n * s_strip if we set the same via mount option.\n * s_mb_group_prealloc can be configured via\n * /sys/fs/ext4/<partition>/mb_group_prealloc\n *\n * XXX: should we try to preallocate more than the group has now?\n */\nstatic void ext4_mb_normalize_group_request(struct ext4_allocation_context *ac)\n{\n\tstruct super_block *sb = ac->ac_sb;\n\tstruct ext4_locality_group *lg = ac->ac_lg;\n\n\tBUG_ON(lg == NULL);\n\tac->ac_g_ex.fe_len = EXT4_SB(sb)->s_mb_group_prealloc;\n\tmb_debug(sb, \"goal %u blocks for locality group\\n\", ac->ac_g_ex.fe_len);\n}\n\n/*\n * Normalization means making request better in terms of\n * size and alignment\n */\nstatic noinline_for_stack void\next4_mb_normalize_request(struct ext4_allocation_context *ac,\n\t\t\t\tstruct ext4_allocation_request *ar)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(ac->ac_sb);\n\tint bsbits, max;\n\text4_lblk_t end;\n\tloff_t size, start_off;\n\tloff_t orig_size __maybe_unused;\n\text4_lblk_t start;\n\tstruct ext4_inode_info *ei = EXT4_I(ac->ac_inode);\n\tstruct ext4_prealloc_space *pa;\n\n\t/* do normalize only data requests, metadata requests\n\t   do not need preallocation */\n\tif (!(ac->ac_flags & EXT4_MB_HINT_DATA))\n\t\treturn;\n\n\t/* sometime caller may want exact blocks */\n\tif (unlikely(ac->ac_flags & EXT4_MB_HINT_GOAL_ONLY))\n\t\treturn;\n\n\t/* caller may indicate that preallocation isn't\n\t * required (it's a tail, for example) */\n\tif (ac->ac_flags & EXT4_MB_HINT_NOPREALLOC)\n\t\treturn;\n\n\tif (ac->ac_flags & EXT4_MB_HINT_GROUP_ALLOC) {\n\t\text4_mb_normalize_group_request(ac);\n\t\treturn ;\n\t}\n\n\tbsbits = ac->ac_sb->s_blocksize_bits;\n\n\t/* first, let's learn actual file size\n\t * given current request is allocated */\n\tsize = ac->ac_o_ex.fe_logical + EXT4_C2B(sbi, ac->ac_o_ex.fe_len);\n\tsize = size << bsbits;\n\tif (size < i_size_read(ac->ac_inode))\n\t\tsize = i_size_read(ac->ac_inode);\n\torig_size = size;\n\n\t/* max size of free chunks */\n\tmax = 2 << bsbits;\n\n#define NRL_CHECK_SIZE(req, size, max, chunk_size)\t\\\n\t\t(req <= (size) || max <= (chunk_size))\n\n\t/* first, try to predict filesize */\n\t/* XXX: should this table be tunable? */\n\tstart_off = 0;\n\tif (size <= 16 * 1024) {\n\t\tsize = 16 * 1024;\n\t} else if (size <= 32 * 1024) {\n\t\tsize = 32 * 1024;\n\t} else if (size <= 64 * 1024) {\n\t\tsize = 64 * 1024;\n\t} else if (size <= 128 * 1024) {\n\t\tsize = 128 * 1024;\n\t} else if (size <= 256 * 1024) {\n\t\tsize = 256 * 1024;\n\t} else if (size <= 512 * 1024) {\n\t\tsize = 512 * 1024;\n\t} else if (size <= 1024 * 1024) {\n\t\tsize = 1024 * 1024;\n\t} else if (NRL_CHECK_SIZE(size, 4 * 1024 * 1024, max, 2 * 1024)) {\n\t\tstart_off = ((loff_t)ac->ac_o_ex.fe_logical >>\n\t\t\t\t\t\t(21 - bsbits)) << 21;\n\t\tsize = 2 * 1024 * 1024;\n\t} else if (NRL_CHECK_SIZE(size, 8 * 1024 * 1024, max, 4 * 1024)) {\n\t\tstart_off = ((loff_t)ac->ac_o_ex.fe_logical >>\n\t\t\t\t\t\t\t(22 - bsbits)) << 22;\n\t\tsize = 4 * 1024 * 1024;\n\t} else if (NRL_CHECK_SIZE(ac->ac_o_ex.fe_len,\n\t\t\t\t\t(8<<20)>>bsbits, max, 8 * 1024)) {\n\t\tstart_off = ((loff_t)ac->ac_o_ex.fe_logical >>\n\t\t\t\t\t\t\t(23 - bsbits)) << 23;\n\t\tsize = 8 * 1024 * 1024;\n\t} else {\n\t\tstart_off = (loff_t) ac->ac_o_ex.fe_logical << bsbits;\n\t\tsize\t  = (loff_t) EXT4_C2B(EXT4_SB(ac->ac_sb),\n\t\t\t\t\t      ac->ac_o_ex.fe_len) << bsbits;\n\t}\n\tsize = size >> bsbits;\n\tstart = start_off >> bsbits;\n\n\t/* don't cover already allocated blocks in selected range */\n\tif (ar->pleft && start <= ar->lleft) {\n\t\tsize -= ar->lleft + 1 - start;\n\t\tstart = ar->lleft + 1;\n\t}\n\tif (ar->pright && start + size - 1 >= ar->lright)\n\t\tsize -= start + size - ar->lright;\n\n\t/*\n\t * Trim allocation request for filesystems with artificially small\n\t * groups.\n\t */\n\tif (size > EXT4_BLOCKS_PER_GROUP(ac->ac_sb))\n\t\tsize = EXT4_BLOCKS_PER_GROUP(ac->ac_sb);\n\n\tend = start + size;\n\n\t/* check we don't cross already preallocated blocks */\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(pa, &ei->i_prealloc_list, pa_inode_list) {\n\t\text4_lblk_t pa_end;\n\n\t\tif (pa->pa_deleted)\n\t\t\tcontinue;\n\t\tspin_lock(&pa->pa_lock);\n\t\tif (pa->pa_deleted) {\n\t\t\tspin_unlock(&pa->pa_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\tpa_end = pa->pa_lstart + EXT4_C2B(EXT4_SB(ac->ac_sb),\n\t\t\t\t\t\t  pa->pa_len);\n\n\t\t/* PA must not overlap original request */\n\t\tBUG_ON(!(ac->ac_o_ex.fe_logical >= pa_end ||\n\t\t\tac->ac_o_ex.fe_logical < pa->pa_lstart));\n\n\t\t/* skip PAs this normalized request doesn't overlap with */\n\t\tif (pa->pa_lstart >= end || pa_end <= start) {\n\t\t\tspin_unlock(&pa->pa_lock);\n\t\t\tcontinue;\n\t\t}\n\t\tBUG_ON(pa->pa_lstart <= start && pa_end >= end);\n\n\t\t/* adjust start or end to be adjacent to this pa */\n\t\tif (pa_end <= ac->ac_o_ex.fe_logical) {\n\t\t\tBUG_ON(pa_end < start);\n\t\t\tstart = pa_end;\n\t\t} else if (pa->pa_lstart > ac->ac_o_ex.fe_logical) {\n\t\t\tBUG_ON(pa->pa_lstart > end);\n\t\t\tend = pa->pa_lstart;\n\t\t}\n\t\tspin_unlock(&pa->pa_lock);\n\t}\n\trcu_read_unlock();\n\tsize = end - start;\n\n\t/* XXX: extra loop to check we really don't overlap preallocations */\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(pa, &ei->i_prealloc_list, pa_inode_list) {\n\t\text4_lblk_t pa_end;\n\n\t\tspin_lock(&pa->pa_lock);\n\t\tif (pa->pa_deleted == 0) {\n\t\t\tpa_end = pa->pa_lstart + EXT4_C2B(EXT4_SB(ac->ac_sb),\n\t\t\t\t\t\t\t  pa->pa_len);\n\t\t\tBUG_ON(!(start >= pa_end || end <= pa->pa_lstart));\n\t\t}\n\t\tspin_unlock(&pa->pa_lock);\n\t}\n\trcu_read_unlock();\n\n\tif (start + size <= ac->ac_o_ex.fe_logical &&\n\t\t\tstart > ac->ac_o_ex.fe_logical) {\n\t\text4_msg(ac->ac_sb, KERN_ERR,\n\t\t\t \"start %lu, size %lu, fe_logical %lu\",\n\t\t\t (unsigned long) start, (unsigned long) size,\n\t\t\t (unsigned long) ac->ac_o_ex.fe_logical);\n\t\tBUG();\n\t}\n\tBUG_ON(size <= 0 || size > EXT4_BLOCKS_PER_GROUP(ac->ac_sb));\n\n\t/* now prepare goal request */\n\n\t/* XXX: is it better to align blocks WRT to logical\n\t * placement or satisfy big request as is */\n\tac->ac_g_ex.fe_logical = start;\n\tac->ac_g_ex.fe_len = EXT4_NUM_B2C(sbi, size);\n\n\t/* define goal start in order to merge */\n\tif (ar->pright && (ar->lright == (start + size))) {\n\t\t/* merge to the right */\n\t\text4_get_group_no_and_offset(ac->ac_sb, ar->pright - size,\n\t\t\t\t\t\t&ac->ac_f_ex.fe_group,\n\t\t\t\t\t\t&ac->ac_f_ex.fe_start);\n\t\tac->ac_flags |= EXT4_MB_HINT_TRY_GOAL;\n\t}\n\tif (ar->pleft && (ar->lleft + 1 == start)) {\n\t\t/* merge to the left */\n\t\text4_get_group_no_and_offset(ac->ac_sb, ar->pleft + 1,\n\t\t\t\t\t\t&ac->ac_f_ex.fe_group,\n\t\t\t\t\t\t&ac->ac_f_ex.fe_start);\n\t\tac->ac_flags |= EXT4_MB_HINT_TRY_GOAL;\n\t}\n\n\tmb_debug(ac->ac_sb, \"goal: %lld(was %lld) blocks at %u\\n\", size,\n\t\t orig_size, start);\n}\n\nstatic void ext4_mb_collect_stats(struct ext4_allocation_context *ac)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(ac->ac_sb);\n\n\tif (sbi->s_mb_stats && ac->ac_g_ex.fe_len > 1) {\n\t\tatomic_inc(&sbi->s_bal_reqs);\n\t\tatomic_add(ac->ac_b_ex.fe_len, &sbi->s_bal_allocated);\n\t\tif (ac->ac_b_ex.fe_len >= ac->ac_o_ex.fe_len)\n\t\t\tatomic_inc(&sbi->s_bal_success);\n\t\tatomic_add(ac->ac_found, &sbi->s_bal_ex_scanned);\n\t\tif (ac->ac_g_ex.fe_start == ac->ac_b_ex.fe_start &&\n\t\t\t\tac->ac_g_ex.fe_group == ac->ac_b_ex.fe_group)\n\t\t\tatomic_inc(&sbi->s_bal_goals);\n\t\tif (ac->ac_found > sbi->s_mb_max_to_scan)\n\t\t\tatomic_inc(&sbi->s_bal_breaks);\n\t}\n\n\tif (ac->ac_op == EXT4_MB_HISTORY_ALLOC)\n\t\ttrace_ext4_mballoc_alloc(ac);\n\telse\n\t\ttrace_ext4_mballoc_prealloc(ac);\n}\n\n/*\n * Called on failure; free up any blocks from the inode PA for this\n * context.  We don't need this for MB_GROUP_PA because we only change\n * pa_free in ext4_mb_release_context(), but on failure, we've already\n * zeroed out ac->ac_b_ex.fe_len, so group_pa->pa_free is not changed.\n */\nstatic void ext4_discard_allocated_blocks(struct ext4_allocation_context *ac)\n{\n\tstruct ext4_prealloc_space *pa = ac->ac_pa;\n\tstruct ext4_buddy e4b;\n\tint err;\n\n\tif (pa == NULL) {\n\t\tif (ac->ac_f_ex.fe_len == 0)\n\t\t\treturn;\n\t\terr = ext4_mb_load_buddy(ac->ac_sb, ac->ac_f_ex.fe_group, &e4b);\n\t\tif (err) {\n\t\t\t/*\n\t\t\t * This should never happen since we pin the\n\t\t\t * pages in the ext4_allocation_context so\n\t\t\t * ext4_mb_load_buddy() should never fail.\n\t\t\t */\n\t\t\tWARN(1, \"mb_load_buddy failed (%d)\", err);\n\t\t\treturn;\n\t\t}\n\t\text4_lock_group(ac->ac_sb, ac->ac_f_ex.fe_group);\n\t\tmb_free_blocks(ac->ac_inode, &e4b, ac->ac_f_ex.fe_start,\n\t\t\t       ac->ac_f_ex.fe_len);\n\t\text4_unlock_group(ac->ac_sb, ac->ac_f_ex.fe_group);\n\t\text4_mb_unload_buddy(&e4b);\n\t\treturn;\n\t}\n\tif (pa->pa_type == MB_INODE_PA)\n\t\tpa->pa_free += ac->ac_b_ex.fe_len;\n}\n\n/*\n * use blocks preallocated to inode\n */\nstatic void ext4_mb_use_inode_pa(struct ext4_allocation_context *ac,\n\t\t\t\tstruct ext4_prealloc_space *pa)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(ac->ac_sb);\n\text4_fsblk_t start;\n\text4_fsblk_t end;\n\tint len;\n\n\t/* found preallocated blocks, use them */\n\tstart = pa->pa_pstart + (ac->ac_o_ex.fe_logical - pa->pa_lstart);\n\tend = min(pa->pa_pstart + EXT4_C2B(sbi, pa->pa_len),\n\t\t  start + EXT4_C2B(sbi, ac->ac_o_ex.fe_len));\n\tlen = EXT4_NUM_B2C(sbi, end - start);\n\text4_get_group_no_and_offset(ac->ac_sb, start, &ac->ac_b_ex.fe_group,\n\t\t\t\t\t&ac->ac_b_ex.fe_start);\n\tac->ac_b_ex.fe_len = len;\n\tac->ac_status = AC_STATUS_FOUND;\n\tac->ac_pa = pa;\n\n\tBUG_ON(start < pa->pa_pstart);\n\tBUG_ON(end > pa->pa_pstart + EXT4_C2B(sbi, pa->pa_len));\n\tBUG_ON(pa->pa_free < len);\n\tpa->pa_free -= len;\n\n\tmb_debug(ac->ac_sb, \"use %llu/%d from inode pa %p\\n\", start, len, pa);\n}\n\n/*\n * use blocks preallocated to locality group\n */\nstatic void ext4_mb_use_group_pa(struct ext4_allocation_context *ac,\n\t\t\t\tstruct ext4_prealloc_space *pa)\n{\n\tunsigned int len = ac->ac_o_ex.fe_len;\n\n\text4_get_group_no_and_offset(ac->ac_sb, pa->pa_pstart,\n\t\t\t\t\t&ac->ac_b_ex.fe_group,\n\t\t\t\t\t&ac->ac_b_ex.fe_start);\n\tac->ac_b_ex.fe_len = len;\n\tac->ac_status = AC_STATUS_FOUND;\n\tac->ac_pa = pa;\n\n\t/* we don't correct pa_pstart or pa_plen here to avoid\n\t * possible race when the group is being loaded concurrently\n\t * instead we correct pa later, after blocks are marked\n\t * in on-disk bitmap -- see ext4_mb_release_context()\n\t * Other CPUs are prevented from allocating from this pa by lg_mutex\n\t */\n\tmb_debug(ac->ac_sb, \"use %u/%u from group pa %p\\n\",\n\t\t pa->pa_lstart-len, len, pa);\n}\n\n/*\n * Return the prealloc space that have minimal distance\n * from the goal block. @cpa is the prealloc\n * space that is having currently known minimal distance\n * from the goal block.\n */\nstatic struct ext4_prealloc_space *\next4_mb_check_group_pa(ext4_fsblk_t goal_block,\n\t\t\tstruct ext4_prealloc_space *pa,\n\t\t\tstruct ext4_prealloc_space *cpa)\n{\n\text4_fsblk_t cur_distance, new_distance;\n\n\tif (cpa == NULL) {\n\t\tatomic_inc(&pa->pa_count);\n\t\treturn pa;\n\t}\n\tcur_distance = abs(goal_block - cpa->pa_pstart);\n\tnew_distance = abs(goal_block - pa->pa_pstart);\n\n\tif (cur_distance <= new_distance)\n\t\treturn cpa;\n\n\t/* drop the previous reference */\n\tatomic_dec(&cpa->pa_count);\n\tatomic_inc(&pa->pa_count);\n\treturn pa;\n}\n\n/*\n * search goal blocks in preallocated space\n */\nstatic noinline_for_stack bool\next4_mb_use_preallocated(struct ext4_allocation_context *ac)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(ac->ac_sb);\n\tint order, i;\n\tstruct ext4_inode_info *ei = EXT4_I(ac->ac_inode);\n\tstruct ext4_locality_group *lg;\n\tstruct ext4_prealloc_space *pa, *cpa = NULL;\n\text4_fsblk_t goal_block;\n\n\t/* only data can be preallocated */\n\tif (!(ac->ac_flags & EXT4_MB_HINT_DATA))\n\t\treturn false;\n\n\t/* first, try per-file preallocation */\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(pa, &ei->i_prealloc_list, pa_inode_list) {\n\n\t\t/* all fields in this condition don't change,\n\t\t * so we can skip locking for them */\n\t\tif (ac->ac_o_ex.fe_logical < pa->pa_lstart ||\n\t\t    ac->ac_o_ex.fe_logical >= (pa->pa_lstart +\n\t\t\t\t\t       EXT4_C2B(sbi, pa->pa_len)))\n\t\t\tcontinue;\n\n\t\t/* non-extent files can't have physical blocks past 2^32 */\n\t\tif (!(ext4_test_inode_flag(ac->ac_inode, EXT4_INODE_EXTENTS)) &&\n\t\t    (pa->pa_pstart + EXT4_C2B(sbi, pa->pa_len) >\n\t\t     EXT4_MAX_BLOCK_FILE_PHYS))\n\t\t\tcontinue;\n\n\t\t/* found preallocated blocks, use them */\n\t\tspin_lock(&pa->pa_lock);\n\t\tif (pa->pa_deleted == 0 && pa->pa_free) {\n\t\t\tatomic_inc(&pa->pa_count);\n\t\t\text4_mb_use_inode_pa(ac, pa);\n\t\t\tspin_unlock(&pa->pa_lock);\n\t\t\tac->ac_criteria = 10;\n\t\t\trcu_read_unlock();\n\t\t\treturn true;\n\t\t}\n\t\tspin_unlock(&pa->pa_lock);\n\t}\n\trcu_read_unlock();\n\n\t/* can we use group allocation? */\n\tif (!(ac->ac_flags & EXT4_MB_HINT_GROUP_ALLOC))\n\t\treturn false;\n\n\t/* inode may have no locality group for some reason */\n\tlg = ac->ac_lg;\n\tif (lg == NULL)\n\t\treturn false;\n\torder  = fls(ac->ac_o_ex.fe_len) - 1;\n\tif (order > PREALLOC_TB_SIZE - 1)\n\t\t/* The max size of hash table is PREALLOC_TB_SIZE */\n\t\torder = PREALLOC_TB_SIZE - 1;\n\n\tgoal_block = ext4_grp_offs_to_block(ac->ac_sb, &ac->ac_g_ex);\n\t/*\n\t * search for the prealloc space that is having\n\t * minimal distance from the goal block.\n\t */\n\tfor (i = order; i < PREALLOC_TB_SIZE; i++) {\n\t\trcu_read_lock();\n\t\tlist_for_each_entry_rcu(pa, &lg->lg_prealloc_list[i],\n\t\t\t\t\tpa_inode_list) {\n\t\t\tspin_lock(&pa->pa_lock);\n\t\t\tif (pa->pa_deleted == 0 &&\n\t\t\t\t\tpa->pa_free >= ac->ac_o_ex.fe_len) {\n\n\t\t\t\tcpa = ext4_mb_check_group_pa(goal_block,\n\t\t\t\t\t\t\t\tpa, cpa);\n\t\t\t}\n\t\t\tspin_unlock(&pa->pa_lock);\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\tif (cpa) {\n\t\text4_mb_use_group_pa(ac, cpa);\n\t\tac->ac_criteria = 20;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/*\n * the function goes through all block freed in the group\n * but not yet committed and marks them used in in-core bitmap.\n * buddy must be generated from this bitmap\n * Need to be called with the ext4 group lock held\n */\nstatic void ext4_mb_generate_from_freelist(struct super_block *sb, void *bitmap,\n\t\t\t\t\t\text4_group_t group)\n{\n\tstruct rb_node *n;\n\tstruct ext4_group_info *grp;\n\tstruct ext4_free_data *entry;\n\n\tgrp = ext4_get_group_info(sb, group);\n\tn = rb_first(&(grp->bb_free_root));\n\n\twhile (n) {\n\t\tentry = rb_entry(n, struct ext4_free_data, efd_node);\n\t\text4_set_bits(bitmap, entry->efd_start_cluster, entry->efd_count);\n\t\tn = rb_next(n);\n\t}\n\treturn;\n}\n\n/*\n * the function goes through all preallocation in this group and marks them\n * used in in-core bitmap. buddy must be generated from this bitmap\n * Need to be called with ext4 group lock held\n */\nstatic noinline_for_stack\nvoid ext4_mb_generate_from_pa(struct super_block *sb, void *bitmap,\n\t\t\t\t\text4_group_t group)\n{\n\tstruct ext4_group_info *grp = ext4_get_group_info(sb, group);\n\tstruct ext4_prealloc_space *pa;\n\tstruct list_head *cur;\n\text4_group_t groupnr;\n\text4_grpblk_t start;\n\tint preallocated = 0;\n\tint len;\n\n\t/* all form of preallocation discards first load group,\n\t * so the only competing code is preallocation use.\n\t * we don't need any locking here\n\t * notice we do NOT ignore preallocations with pa_deleted\n\t * otherwise we could leave used blocks available for\n\t * allocation in buddy when concurrent ext4_mb_put_pa()\n\t * is dropping preallocation\n\t */\n\tlist_for_each(cur, &grp->bb_prealloc_list) {\n\t\tpa = list_entry(cur, struct ext4_prealloc_space, pa_group_list);\n\t\tspin_lock(&pa->pa_lock);\n\t\text4_get_group_no_and_offset(sb, pa->pa_pstart,\n\t\t\t\t\t     &groupnr, &start);\n\t\tlen = pa->pa_len;\n\t\tspin_unlock(&pa->pa_lock);\n\t\tif (unlikely(len == 0))\n\t\t\tcontinue;\n\t\tBUG_ON(groupnr != group);\n\t\text4_set_bits(bitmap, start, len);\n\t\tpreallocated += len;\n\t}\n\tmb_debug(sb, \"preallocated %d for group %u\\n\", preallocated, group);\n}\n\nstatic void ext4_mb_mark_pa_deleted(struct super_block *sb,\n\t\t\t\t    struct ext4_prealloc_space *pa)\n{\n\tstruct ext4_inode_info *ei;\n\n\tif (pa->pa_deleted) {\n\t\text4_warning(sb, \"deleted pa, type:%d, pblk:%llu, lblk:%u, len:%d\\n\",\n\t\t\t     pa->pa_type, pa->pa_pstart, pa->pa_lstart,\n\t\t\t     pa->pa_len);\n\t\treturn;\n\t}\n\n\tpa->pa_deleted = 1;\n\n\tif (pa->pa_type == MB_INODE_PA) {\n\t\tei = EXT4_I(pa->pa_inode);\n\t\tatomic_dec(&ei->i_prealloc_active);\n\t}\n}\n\nstatic void ext4_mb_pa_callback(struct rcu_head *head)\n{\n\tstruct ext4_prealloc_space *pa;\n\tpa = container_of(head, struct ext4_prealloc_space, u.pa_rcu);\n\n\tBUG_ON(atomic_read(&pa->pa_count));\n\tBUG_ON(pa->pa_deleted == 0);\n\tkmem_cache_free(ext4_pspace_cachep, pa);\n}\n\n/*\n * drops a reference to preallocated space descriptor\n * if this was the last reference and the space is consumed\n */\nstatic void ext4_mb_put_pa(struct ext4_allocation_context *ac,\n\t\t\tstruct super_block *sb, struct ext4_prealloc_space *pa)\n{\n\text4_group_t grp;\n\text4_fsblk_t grp_blk;\n\n\t/* in this short window concurrent discard can set pa_deleted */\n\tspin_lock(&pa->pa_lock);\n\tif (!atomic_dec_and_test(&pa->pa_count) || pa->pa_free != 0) {\n\t\tspin_unlock(&pa->pa_lock);\n\t\treturn;\n\t}\n\n\tif (pa->pa_deleted == 1) {\n\t\tspin_unlock(&pa->pa_lock);\n\t\treturn;\n\t}\n\n\text4_mb_mark_pa_deleted(sb, pa);\n\tspin_unlock(&pa->pa_lock);\n\n\tgrp_blk = pa->pa_pstart;\n\t/*\n\t * If doing group-based preallocation, pa_pstart may be in the\n\t * next group when pa is used up\n\t */\n\tif (pa->pa_type == MB_GROUP_PA)\n\t\tgrp_blk--;\n\n\tgrp = ext4_get_group_number(sb, grp_blk);\n\n\t/*\n\t * possible race:\n\t *\n\t *  P1 (buddy init)\t\t\tP2 (regular allocation)\n\t *\t\t\t\t\tfind block B in PA\n\t *  copy on-disk bitmap to buddy\n\t *  \t\t\t\t\tmark B in on-disk bitmap\n\t *\t\t\t\t\tdrop PA from group\n\t *  mark all PAs in buddy\n\t *\n\t * thus, P1 initializes buddy with B available. to prevent this\n\t * we make \"copy\" and \"mark all PAs\" atomic and serialize \"drop PA\"\n\t * against that pair\n\t */\n\text4_lock_group(sb, grp);\n\tlist_del(&pa->pa_group_list);\n\text4_unlock_group(sb, grp);\n\n\tspin_lock(pa->pa_obj_lock);\n\tlist_del_rcu(&pa->pa_inode_list);\n\tspin_unlock(pa->pa_obj_lock);\n\n\tcall_rcu(&(pa)->u.pa_rcu, ext4_mb_pa_callback);\n}\n\n/*\n * creates new preallocated space for given inode\n */\nstatic noinline_for_stack void\next4_mb_new_inode_pa(struct ext4_allocation_context *ac)\n{\n\tstruct super_block *sb = ac->ac_sb;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_prealloc_space *pa;\n\tstruct ext4_group_info *grp;\n\tstruct ext4_inode_info *ei;\n\n\t/* preallocate only when found space is larger then requested */\n\tBUG_ON(ac->ac_o_ex.fe_len >= ac->ac_b_ex.fe_len);\n\tBUG_ON(ac->ac_status != AC_STATUS_FOUND);\n\tBUG_ON(!S_ISREG(ac->ac_inode->i_mode));\n\tBUG_ON(ac->ac_pa == NULL);\n\n\tpa = ac->ac_pa;\n\n\tif (ac->ac_b_ex.fe_len < ac->ac_g_ex.fe_len) {\n\t\tint winl;\n\t\tint wins;\n\t\tint win;\n\t\tint offs;\n\n\t\t/* we can't allocate as much as normalizer wants.\n\t\t * so, found space must get proper lstart\n\t\t * to cover original request */\n\t\tBUG_ON(ac->ac_g_ex.fe_logical > ac->ac_o_ex.fe_logical);\n\t\tBUG_ON(ac->ac_g_ex.fe_len < ac->ac_o_ex.fe_len);\n\n\t\t/* we're limited by original request in that\n\t\t * logical block must be covered any way\n\t\t * winl is window we can move our chunk within */\n\t\twinl = ac->ac_o_ex.fe_logical - ac->ac_g_ex.fe_logical;\n\n\t\t/* also, we should cover whole original request */\n\t\twins = EXT4_C2B(sbi, ac->ac_b_ex.fe_len - ac->ac_o_ex.fe_len);\n\n\t\t/* the smallest one defines real window */\n\t\twin = min(winl, wins);\n\n\t\toffs = ac->ac_o_ex.fe_logical %\n\t\t\tEXT4_C2B(sbi, ac->ac_b_ex.fe_len);\n\t\tif (offs && offs < win)\n\t\t\twin = offs;\n\n\t\tac->ac_b_ex.fe_logical = ac->ac_o_ex.fe_logical -\n\t\t\tEXT4_NUM_B2C(sbi, win);\n\t\tBUG_ON(ac->ac_o_ex.fe_logical < ac->ac_b_ex.fe_logical);\n\t\tBUG_ON(ac->ac_o_ex.fe_len > ac->ac_b_ex.fe_len);\n\t}\n\n\t/* preallocation can change ac_b_ex, thus we store actually\n\t * allocated blocks for history */\n\tac->ac_f_ex = ac->ac_b_ex;\n\n\tpa->pa_lstart = ac->ac_b_ex.fe_logical;\n\tpa->pa_pstart = ext4_grp_offs_to_block(sb, &ac->ac_b_ex);\n\tpa->pa_len = ac->ac_b_ex.fe_len;\n\tpa->pa_free = pa->pa_len;\n\tspin_lock_init(&pa->pa_lock);\n\tINIT_LIST_HEAD(&pa->pa_inode_list);\n\tINIT_LIST_HEAD(&pa->pa_group_list);\n\tpa->pa_deleted = 0;\n\tpa->pa_type = MB_INODE_PA;\n\n\tmb_debug(sb, \"new inode pa %p: %llu/%d for %u\\n\", pa, pa->pa_pstart,\n\t\t pa->pa_len, pa->pa_lstart);\n\ttrace_ext4_mb_new_inode_pa(ac, pa);\n\n\text4_mb_use_inode_pa(ac, pa);\n\tatomic_add(pa->pa_free, &sbi->s_mb_preallocated);\n\n\tei = EXT4_I(ac->ac_inode);\n\tgrp = ext4_get_group_info(sb, ac->ac_b_ex.fe_group);\n\n\tpa->pa_obj_lock = &ei->i_prealloc_lock;\n\tpa->pa_inode = ac->ac_inode;\n\n\tlist_add(&pa->pa_group_list, &grp->bb_prealloc_list);\n\n\tspin_lock(pa->pa_obj_lock);\n\tlist_add_rcu(&pa->pa_inode_list, &ei->i_prealloc_list);\n\tspin_unlock(pa->pa_obj_lock);\n\tatomic_inc(&ei->i_prealloc_active);\n}\n\n/*\n * creates new preallocated space for locality group inodes belongs to\n */\nstatic noinline_for_stack void\next4_mb_new_group_pa(struct ext4_allocation_context *ac)\n{\n\tstruct super_block *sb = ac->ac_sb;\n\tstruct ext4_locality_group *lg;\n\tstruct ext4_prealloc_space *pa;\n\tstruct ext4_group_info *grp;\n\n\t/* preallocate only when found space is larger then requested */\n\tBUG_ON(ac->ac_o_ex.fe_len >= ac->ac_b_ex.fe_len);\n\tBUG_ON(ac->ac_status != AC_STATUS_FOUND);\n\tBUG_ON(!S_ISREG(ac->ac_inode->i_mode));\n\tBUG_ON(ac->ac_pa == NULL);\n\n\tpa = ac->ac_pa;\n\n\t/* preallocation can change ac_b_ex, thus we store actually\n\t * allocated blocks for history */\n\tac->ac_f_ex = ac->ac_b_ex;\n\n\tpa->pa_pstart = ext4_grp_offs_to_block(sb, &ac->ac_b_ex);\n\tpa->pa_lstart = pa->pa_pstart;\n\tpa->pa_len = ac->ac_b_ex.fe_len;\n\tpa->pa_free = pa->pa_len;\n\tspin_lock_init(&pa->pa_lock);\n\tINIT_LIST_HEAD(&pa->pa_inode_list);\n\tINIT_LIST_HEAD(&pa->pa_group_list);\n\tpa->pa_deleted = 0;\n\tpa->pa_type = MB_GROUP_PA;\n\n\tmb_debug(sb, \"new group pa %p: %llu/%d for %u\\n\", pa, pa->pa_pstart,\n\t\t pa->pa_len, pa->pa_lstart);\n\ttrace_ext4_mb_new_group_pa(ac, pa);\n\n\text4_mb_use_group_pa(ac, pa);\n\tatomic_add(pa->pa_free, &EXT4_SB(sb)->s_mb_preallocated);\n\n\tgrp = ext4_get_group_info(sb, ac->ac_b_ex.fe_group);\n\tlg = ac->ac_lg;\n\tBUG_ON(lg == NULL);\n\n\tpa->pa_obj_lock = &lg->lg_prealloc_lock;\n\tpa->pa_inode = NULL;\n\n\tlist_add(&pa->pa_group_list, &grp->bb_prealloc_list);\n\n\t/*\n\t * We will later add the new pa to the right bucket\n\t * after updating the pa_free in ext4_mb_release_context\n\t */\n}\n\nstatic void ext4_mb_new_preallocation(struct ext4_allocation_context *ac)\n{\n\tif (ac->ac_flags & EXT4_MB_HINT_GROUP_ALLOC)\n\t\text4_mb_new_group_pa(ac);\n\telse\n\t\text4_mb_new_inode_pa(ac);\n}\n\n/*\n * finds all unused blocks in on-disk bitmap, frees them in\n * in-core bitmap and buddy.\n * @pa must be unlinked from inode and group lists, so that\n * nobody else can find/use it.\n * the caller MUST hold group/inode locks.\n * TODO: optimize the case when there are no in-core structures yet\n */\nstatic noinline_for_stack int\next4_mb_release_inode_pa(struct ext4_buddy *e4b, struct buffer_head *bitmap_bh,\n\t\t\tstruct ext4_prealloc_space *pa)\n{\n\tstruct super_block *sb = e4b->bd_sb;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tunsigned int end;\n\tunsigned int next;\n\text4_group_t group;\n\text4_grpblk_t bit;\n\tunsigned long long grp_blk_start;\n\tint free = 0;\n\n\tBUG_ON(pa->pa_deleted == 0);\n\text4_get_group_no_and_offset(sb, pa->pa_pstart, &group, &bit);\n\tgrp_blk_start = pa->pa_pstart - EXT4_C2B(sbi, bit);\n\tBUG_ON(group != e4b->bd_group && pa->pa_len != 0);\n\tend = bit + pa->pa_len;\n\n\twhile (bit < end) {\n\t\tbit = mb_find_next_zero_bit(bitmap_bh->b_data, end, bit);\n\t\tif (bit >= end)\n\t\t\tbreak;\n\t\tnext = mb_find_next_bit(bitmap_bh->b_data, end, bit);\n\t\tmb_debug(sb, \"free preallocated %u/%u in group %u\\n\",\n\t\t\t (unsigned) ext4_group_first_block_no(sb, group) + bit,\n\t\t\t (unsigned) next - bit, (unsigned) group);\n\t\tfree += next - bit;\n\n\t\ttrace_ext4_mballoc_discard(sb, NULL, group, bit, next - bit);\n\t\ttrace_ext4_mb_release_inode_pa(pa, (grp_blk_start +\n\t\t\t\t\t\t    EXT4_C2B(sbi, bit)),\n\t\t\t\t\t       next - bit);\n\t\tmb_free_blocks(pa->pa_inode, e4b, bit, next - bit);\n\t\tbit = next + 1;\n\t}\n\tif (free != pa->pa_free) {\n\t\text4_msg(e4b->bd_sb, KERN_CRIT,\n\t\t\t \"pa %p: logic %lu, phys. %lu, len %d\",\n\t\t\t pa, (unsigned long) pa->pa_lstart,\n\t\t\t (unsigned long) pa->pa_pstart,\n\t\t\t pa->pa_len);\n\t\text4_grp_locked_error(sb, group, 0, 0, \"free %u, pa_free %u\",\n\t\t\t\t\tfree, pa->pa_free);\n\t\t/*\n\t\t * pa is already deleted so we use the value obtained\n\t\t * from the bitmap and continue.\n\t\t */\n\t}\n\tatomic_add(free, &sbi->s_mb_discarded);\n\n\treturn 0;\n}\n\nstatic noinline_for_stack int\next4_mb_release_group_pa(struct ext4_buddy *e4b,\n\t\t\t\tstruct ext4_prealloc_space *pa)\n{\n\tstruct super_block *sb = e4b->bd_sb;\n\text4_group_t group;\n\text4_grpblk_t bit;\n\n\ttrace_ext4_mb_release_group_pa(sb, pa);\n\tBUG_ON(pa->pa_deleted == 0);\n\text4_get_group_no_and_offset(sb, pa->pa_pstart, &group, &bit);\n\tBUG_ON(group != e4b->bd_group && pa->pa_len != 0);\n\tmb_free_blocks(pa->pa_inode, e4b, bit, pa->pa_len);\n\tatomic_add(pa->pa_len, &EXT4_SB(sb)->s_mb_discarded);\n\ttrace_ext4_mballoc_discard(sb, NULL, group, bit, pa->pa_len);\n\n\treturn 0;\n}\n\n/*\n * releases all preallocations in given group\n *\n * first, we need to decide discard policy:\n * - when do we discard\n *   1) ENOSPC\n * - how many do we discard\n *   1) how many requested\n */\nstatic noinline_for_stack int\next4_mb_discard_group_preallocations(struct super_block *sb,\n\t\t\t\t\text4_group_t group, int needed)\n{\n\tstruct ext4_group_info *grp = ext4_get_group_info(sb, group);\n\tstruct buffer_head *bitmap_bh = NULL;\n\tstruct ext4_prealloc_space *pa, *tmp;\n\tstruct list_head list;\n\tstruct ext4_buddy e4b;\n\tint err;\n\tint busy = 0;\n\tint free, free_total = 0;\n\n\tmb_debug(sb, \"discard preallocation for group %u\\n\", group);\n\tif (list_empty(&grp->bb_prealloc_list))\n\t\tgoto out_dbg;\n\n\tbitmap_bh = ext4_read_block_bitmap(sb, group);\n\tif (IS_ERR(bitmap_bh)) {\n\t\terr = PTR_ERR(bitmap_bh);\n\t\text4_error_err(sb, -err,\n\t\t\t       \"Error %d reading block bitmap for %u\",\n\t\t\t       err, group);\n\t\tgoto out_dbg;\n\t}\n\n\terr = ext4_mb_load_buddy(sb, group, &e4b);\n\tif (err) {\n\t\text4_warning(sb, \"Error %d loading buddy information for %u\",\n\t\t\t     err, group);\n\t\tput_bh(bitmap_bh);\n\t\tgoto out_dbg;\n\t}\n\n\tif (needed == 0)\n\t\tneeded = EXT4_CLUSTERS_PER_GROUP(sb) + 1;\n\n\tINIT_LIST_HEAD(&list);\nrepeat:\n\tfree = 0;\n\text4_lock_group(sb, group);\n\tlist_for_each_entry_safe(pa, tmp,\n\t\t\t\t&grp->bb_prealloc_list, pa_group_list) {\n\t\tspin_lock(&pa->pa_lock);\n\t\tif (atomic_read(&pa->pa_count)) {\n\t\t\tspin_unlock(&pa->pa_lock);\n\t\t\tbusy = 1;\n\t\t\tcontinue;\n\t\t}\n\t\tif (pa->pa_deleted) {\n\t\t\tspin_unlock(&pa->pa_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* seems this one can be freed ... */\n\t\text4_mb_mark_pa_deleted(sb, pa);\n\n\t\tif (!free)\n\t\t\tthis_cpu_inc(discard_pa_seq);\n\n\t\t/* we can trust pa_free ... */\n\t\tfree += pa->pa_free;\n\n\t\tspin_unlock(&pa->pa_lock);\n\n\t\tlist_del(&pa->pa_group_list);\n\t\tlist_add(&pa->u.pa_tmp_list, &list);\n\t}\n\n\t/* now free all selected PAs */\n\tlist_for_each_entry_safe(pa, tmp, &list, u.pa_tmp_list) {\n\n\t\t/* remove from object (inode or locality group) */\n\t\tspin_lock(pa->pa_obj_lock);\n\t\tlist_del_rcu(&pa->pa_inode_list);\n\t\tspin_unlock(pa->pa_obj_lock);\n\n\t\tif (pa->pa_type == MB_GROUP_PA)\n\t\t\text4_mb_release_group_pa(&e4b, pa);\n\t\telse\n\t\t\text4_mb_release_inode_pa(&e4b, bitmap_bh, pa);\n\n\t\tlist_del(&pa->u.pa_tmp_list);\n\t\tcall_rcu(&(pa)->u.pa_rcu, ext4_mb_pa_callback);\n\t}\n\n\tfree_total += free;\n\n\t/* if we still need more blocks and some PAs were used, try again */\n\tif (free_total < needed && busy) {\n\t\text4_unlock_group(sb, group);\n\t\tcond_resched();\n\t\tbusy = 0;\n\t\tgoto repeat;\n\t}\n\text4_unlock_group(sb, group);\n\text4_mb_unload_buddy(&e4b);\n\tput_bh(bitmap_bh);\nout_dbg:\n\tmb_debug(sb, \"discarded (%d) blocks preallocated for group %u bb_free (%d)\\n\",\n\t\t free_total, group, grp->bb_free);\n\treturn free_total;\n}\n\n/*\n * releases all non-used preallocated blocks for given inode\n *\n * It's important to discard preallocations under i_data_sem\n * We don't want another block to be served from the prealloc\n * space when we are discarding the inode prealloc space.\n *\n * FIXME!! Make sure it is valid at all the call sites\n */\nvoid ext4_discard_preallocations(struct inode *inode, unsigned int needed)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct super_block *sb = inode->i_sb;\n\tstruct buffer_head *bitmap_bh = NULL;\n\tstruct ext4_prealloc_space *pa, *tmp;\n\text4_group_t group = 0;\n\tstruct list_head list;\n\tstruct ext4_buddy e4b;\n\tint err;\n\n\tif (!S_ISREG(inode->i_mode)) {\n\t\t/*BUG_ON(!list_empty(&ei->i_prealloc_list));*/\n\t\treturn;\n\t}\n\n\tif (EXT4_SB(sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn;\n\n\tmb_debug(sb, \"discard preallocation for inode %lu\\n\",\n\t\t inode->i_ino);\n\ttrace_ext4_discard_preallocations(inode,\n\t\t\tatomic_read(&ei->i_prealloc_active), needed);\n\n\tINIT_LIST_HEAD(&list);\n\n\tif (needed == 0)\n\t\tneeded = UINT_MAX;\n\nrepeat:\n\t/* first, collect all pa's in the inode */\n\tspin_lock(&ei->i_prealloc_lock);\n\twhile (!list_empty(&ei->i_prealloc_list) && needed) {\n\t\tpa = list_entry(ei->i_prealloc_list.prev,\n\t\t\t\tstruct ext4_prealloc_space, pa_inode_list);\n\t\tBUG_ON(pa->pa_obj_lock != &ei->i_prealloc_lock);\n\t\tspin_lock(&pa->pa_lock);\n\t\tif (atomic_read(&pa->pa_count)) {\n\t\t\t/* this shouldn't happen often - nobody should\n\t\t\t * use preallocation while we're discarding it */\n\t\t\tspin_unlock(&pa->pa_lock);\n\t\t\tspin_unlock(&ei->i_prealloc_lock);\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t \"uh-oh! used pa while discarding\");\n\t\t\tWARN_ON(1);\n\t\t\tschedule_timeout_uninterruptible(HZ);\n\t\t\tgoto repeat;\n\n\t\t}\n\t\tif (pa->pa_deleted == 0) {\n\t\t\text4_mb_mark_pa_deleted(sb, pa);\n\t\t\tspin_unlock(&pa->pa_lock);\n\t\t\tlist_del_rcu(&pa->pa_inode_list);\n\t\t\tlist_add(&pa->u.pa_tmp_list, &list);\n\t\t\tneeded--;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* someone is deleting pa right now */\n\t\tspin_unlock(&pa->pa_lock);\n\t\tspin_unlock(&ei->i_prealloc_lock);\n\n\t\t/* we have to wait here because pa_deleted\n\t\t * doesn't mean pa is already unlinked from\n\t\t * the list. as we might be called from\n\t\t * ->clear_inode() the inode will get freed\n\t\t * and concurrent thread which is unlinking\n\t\t * pa from inode's list may access already\n\t\t * freed memory, bad-bad-bad */\n\n\t\t/* XXX: if this happens too often, we can\n\t\t * add a flag to force wait only in case\n\t\t * of ->clear_inode(), but not in case of\n\t\t * regular truncate */\n\t\tschedule_timeout_uninterruptible(HZ);\n\t\tgoto repeat;\n\t}\n\tspin_unlock(&ei->i_prealloc_lock);\n\n\tlist_for_each_entry_safe(pa, tmp, &list, u.pa_tmp_list) {\n\t\tBUG_ON(pa->pa_type != MB_INODE_PA);\n\t\tgroup = ext4_get_group_number(sb, pa->pa_pstart);\n\n\t\terr = ext4_mb_load_buddy_gfp(sb, group, &e4b,\n\t\t\t\t\t     GFP_NOFS|__GFP_NOFAIL);\n\t\tif (err) {\n\t\t\text4_error_err(sb, -err, \"Error %d loading buddy information for %u\",\n\t\t\t\t       err, group);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbitmap_bh = ext4_read_block_bitmap(sb, group);\n\t\tif (IS_ERR(bitmap_bh)) {\n\t\t\terr = PTR_ERR(bitmap_bh);\n\t\t\text4_error_err(sb, -err, \"Error %d reading block bitmap for %u\",\n\t\t\t\t       err, group);\n\t\t\text4_mb_unload_buddy(&e4b);\n\t\t\tcontinue;\n\t\t}\n\n\t\text4_lock_group(sb, group);\n\t\tlist_del(&pa->pa_group_list);\n\t\text4_mb_release_inode_pa(&e4b, bitmap_bh, pa);\n\t\text4_unlock_group(sb, group);\n\n\t\text4_mb_unload_buddy(&e4b);\n\t\tput_bh(bitmap_bh);\n\n\t\tlist_del(&pa->u.pa_tmp_list);\n\t\tcall_rcu(&(pa)->u.pa_rcu, ext4_mb_pa_callback);\n\t}\n}\n\nstatic int ext4_mb_pa_alloc(struct ext4_allocation_context *ac)\n{\n\tstruct ext4_prealloc_space *pa;\n\n\tBUG_ON(ext4_pspace_cachep == NULL);\n\tpa = kmem_cache_zalloc(ext4_pspace_cachep, GFP_NOFS);\n\tif (!pa)\n\t\treturn -ENOMEM;\n\tatomic_set(&pa->pa_count, 1);\n\tac->ac_pa = pa;\n\treturn 0;\n}\n\nstatic void ext4_mb_pa_free(struct ext4_allocation_context *ac)\n{\n\tstruct ext4_prealloc_space *pa = ac->ac_pa;\n\n\tBUG_ON(!pa);\n\tac->ac_pa = NULL;\n\tWARN_ON(!atomic_dec_and_test(&pa->pa_count));\n\tkmem_cache_free(ext4_pspace_cachep, pa);\n}\n\n#ifdef CONFIG_EXT4_DEBUG\nstatic inline void ext4_mb_show_pa(struct super_block *sb)\n{\n\text4_group_t i, ngroups;\n\n\tif (ext4_test_mount_flag(sb, EXT4_MF_FS_ABORTED))\n\t\treturn;\n\n\tngroups = ext4_get_groups_count(sb);\n\tmb_debug(sb, \"groups: \");\n\tfor (i = 0; i < ngroups; i++) {\n\t\tstruct ext4_group_info *grp = ext4_get_group_info(sb, i);\n\t\tstruct ext4_prealloc_space *pa;\n\t\text4_grpblk_t start;\n\t\tstruct list_head *cur;\n\t\text4_lock_group(sb, i);\n\t\tlist_for_each(cur, &grp->bb_prealloc_list) {\n\t\t\tpa = list_entry(cur, struct ext4_prealloc_space,\n\t\t\t\t\tpa_group_list);\n\t\t\tspin_lock(&pa->pa_lock);\n\t\t\text4_get_group_no_and_offset(sb, pa->pa_pstart,\n\t\t\t\t\t\t     NULL, &start);\n\t\t\tspin_unlock(&pa->pa_lock);\n\t\t\tmb_debug(sb, \"PA:%u:%d:%d\\n\", i, start,\n\t\t\t\t pa->pa_len);\n\t\t}\n\t\text4_unlock_group(sb, i);\n\t\tmb_debug(sb, \"%u: %d/%d\\n\", i, grp->bb_free,\n\t\t\t grp->bb_fragments);\n\t}\n}\n\nstatic void ext4_mb_show_ac(struct ext4_allocation_context *ac)\n{\n\tstruct super_block *sb = ac->ac_sb;\n\n\tif (ext4_test_mount_flag(sb, EXT4_MF_FS_ABORTED))\n\t\treturn;\n\n\tmb_debug(sb, \"Can't allocate:\"\n\t\t\t\" Allocation context details:\");\n\tmb_debug(sb, \"status %u flags 0x%x\",\n\t\t\tac->ac_status, ac->ac_flags);\n\tmb_debug(sb, \"orig %lu/%lu/%lu@%lu, \"\n\t\t\t\"goal %lu/%lu/%lu@%lu, \"\n\t\t\t\"best %lu/%lu/%lu@%lu cr %d\",\n\t\t\t(unsigned long)ac->ac_o_ex.fe_group,\n\t\t\t(unsigned long)ac->ac_o_ex.fe_start,\n\t\t\t(unsigned long)ac->ac_o_ex.fe_len,\n\t\t\t(unsigned long)ac->ac_o_ex.fe_logical,\n\t\t\t(unsigned long)ac->ac_g_ex.fe_group,\n\t\t\t(unsigned long)ac->ac_g_ex.fe_start,\n\t\t\t(unsigned long)ac->ac_g_ex.fe_len,\n\t\t\t(unsigned long)ac->ac_g_ex.fe_logical,\n\t\t\t(unsigned long)ac->ac_b_ex.fe_group,\n\t\t\t(unsigned long)ac->ac_b_ex.fe_start,\n\t\t\t(unsigned long)ac->ac_b_ex.fe_len,\n\t\t\t(unsigned long)ac->ac_b_ex.fe_logical,\n\t\t\t(int)ac->ac_criteria);\n\tmb_debug(sb, \"%u found\", ac->ac_found);\n\text4_mb_show_pa(sb);\n}\n#else\nstatic inline void ext4_mb_show_pa(struct super_block *sb)\n{\n\treturn;\n}\nstatic inline void ext4_mb_show_ac(struct ext4_allocation_context *ac)\n{\n\text4_mb_show_pa(ac->ac_sb);\n\treturn;\n}\n#endif\n\n/*\n * We use locality group preallocation for small size file. The size of the\n * file is determined by the current size or the resulting size after\n * allocation which ever is larger\n *\n * One can tune this size via /sys/fs/ext4/<partition>/mb_stream_req\n */\nstatic void ext4_mb_group_or_file(struct ext4_allocation_context *ac)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(ac->ac_sb);\n\tint bsbits = ac->ac_sb->s_blocksize_bits;\n\tloff_t size, isize;\n\n\tif (!(ac->ac_flags & EXT4_MB_HINT_DATA))\n\t\treturn;\n\n\tif (unlikely(ac->ac_flags & EXT4_MB_HINT_GOAL_ONLY))\n\t\treturn;\n\n\tsize = ac->ac_o_ex.fe_logical + EXT4_C2B(sbi, ac->ac_o_ex.fe_len);\n\tisize = (i_size_read(ac->ac_inode) + ac->ac_sb->s_blocksize - 1)\n\t\t>> bsbits;\n\n\tif ((size == isize) && !ext4_fs_is_busy(sbi) &&\n\t    !inode_is_open_for_write(ac->ac_inode)) {\n\t\tac->ac_flags |= EXT4_MB_HINT_NOPREALLOC;\n\t\treturn;\n\t}\n\n\tif (sbi->s_mb_group_prealloc <= 0) {\n\t\tac->ac_flags |= EXT4_MB_STREAM_ALLOC;\n\t\treturn;\n\t}\n\n\t/* don't use group allocation for large files */\n\tsize = max(size, isize);\n\tif (size > sbi->s_mb_stream_request) {\n\t\tac->ac_flags |= EXT4_MB_STREAM_ALLOC;\n\t\treturn;\n\t}\n\n\tBUG_ON(ac->ac_lg != NULL);\n\t/*\n\t * locality group prealloc space are per cpu. The reason for having\n\t * per cpu locality group is to reduce the contention between block\n\t * request from multiple CPUs.\n\t */\n\tac->ac_lg = raw_cpu_ptr(sbi->s_locality_groups);\n\n\t/* we're going to use group allocation */\n\tac->ac_flags |= EXT4_MB_HINT_GROUP_ALLOC;\n\n\t/* serialize all allocations in the group */\n\tmutex_lock(&ac->ac_lg->lg_mutex);\n}\n\nstatic noinline_for_stack int\next4_mb_initialize_context(struct ext4_allocation_context *ac,\n\t\t\t\tstruct ext4_allocation_request *ar)\n{\n\tstruct super_block *sb = ar->inode->i_sb;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_super_block *es = sbi->s_es;\n\text4_group_t group;\n\tunsigned int len;\n\text4_fsblk_t goal;\n\text4_grpblk_t block;\n\n\t/* we can't allocate > group size */\n\tlen = ar->len;\n\n\t/* just a dirty hack to filter too big requests  */\n\tif (len >= EXT4_CLUSTERS_PER_GROUP(sb))\n\t\tlen = EXT4_CLUSTERS_PER_GROUP(sb);\n\n\t/* start searching from the goal */\n\tgoal = ar->goal;\n\tif (goal < le32_to_cpu(es->s_first_data_block) ||\n\t\t\tgoal >= ext4_blocks_count(es))\n\t\tgoal = le32_to_cpu(es->s_first_data_block);\n\text4_get_group_no_and_offset(sb, goal, &group, &block);\n\n\t/* set up allocation goals */\n\tac->ac_b_ex.fe_logical = EXT4_LBLK_CMASK(sbi, ar->logical);\n\tac->ac_status = AC_STATUS_CONTINUE;\n\tac->ac_sb = sb;\n\tac->ac_inode = ar->inode;\n\tac->ac_o_ex.fe_logical = ac->ac_b_ex.fe_logical;\n\tac->ac_o_ex.fe_group = group;\n\tac->ac_o_ex.fe_start = block;\n\tac->ac_o_ex.fe_len = len;\n\tac->ac_g_ex = ac->ac_o_ex;\n\tac->ac_flags = ar->flags;\n\n\t/* we have to define context: we'll work with a file or\n\t * locality group. this is a policy, actually */\n\text4_mb_group_or_file(ac);\n\n\tmb_debug(sb, \"init ac: %u blocks @ %u, goal %u, flags 0x%x, 2^%d, \"\n\t\t\t\"left: %u/%u, right %u/%u to %swritable\\n\",\n\t\t\t(unsigned) ar->len, (unsigned) ar->logical,\n\t\t\t(unsigned) ar->goal, ac->ac_flags, ac->ac_2order,\n\t\t\t(unsigned) ar->lleft, (unsigned) ar->pleft,\n\t\t\t(unsigned) ar->lright, (unsigned) ar->pright,\n\t\t\tinode_is_open_for_write(ar->inode) ? \"\" : \"non-\");\n\treturn 0;\n\n}\n\nstatic noinline_for_stack void\next4_mb_discard_lg_preallocations(struct super_block *sb,\n\t\t\t\t\tstruct ext4_locality_group *lg,\n\t\t\t\t\tint order, int total_entries)\n{\n\text4_group_t group = 0;\n\tstruct ext4_buddy e4b;\n\tstruct list_head discard_list;\n\tstruct ext4_prealloc_space *pa, *tmp;\n\n\tmb_debug(sb, \"discard locality group preallocation\\n\");\n\n\tINIT_LIST_HEAD(&discard_list);\n\n\tspin_lock(&lg->lg_prealloc_lock);\n\tlist_for_each_entry_rcu(pa, &lg->lg_prealloc_list[order],\n\t\t\t\tpa_inode_list,\n\t\t\t\tlockdep_is_held(&lg->lg_prealloc_lock)) {\n\t\tspin_lock(&pa->pa_lock);\n\t\tif (atomic_read(&pa->pa_count)) {\n\t\t\t/*\n\t\t\t * This is the pa that we just used\n\t\t\t * for block allocation. So don't\n\t\t\t * free that\n\t\t\t */\n\t\t\tspin_unlock(&pa->pa_lock);\n\t\t\tcontinue;\n\t\t}\n\t\tif (pa->pa_deleted) {\n\t\t\tspin_unlock(&pa->pa_lock);\n\t\t\tcontinue;\n\t\t}\n\t\t/* only lg prealloc space */\n\t\tBUG_ON(pa->pa_type != MB_GROUP_PA);\n\n\t\t/* seems this one can be freed ... */\n\t\text4_mb_mark_pa_deleted(sb, pa);\n\t\tspin_unlock(&pa->pa_lock);\n\n\t\tlist_del_rcu(&pa->pa_inode_list);\n\t\tlist_add(&pa->u.pa_tmp_list, &discard_list);\n\n\t\ttotal_entries--;\n\t\tif (total_entries <= 5) {\n\t\t\t/*\n\t\t\t * we want to keep only 5 entries\n\t\t\t * allowing it to grow to 8. This\n\t\t\t * mak sure we don't call discard\n\t\t\t * soon for this list.\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&lg->lg_prealloc_lock);\n\n\tlist_for_each_entry_safe(pa, tmp, &discard_list, u.pa_tmp_list) {\n\t\tint err;\n\n\t\tgroup = ext4_get_group_number(sb, pa->pa_pstart);\n\t\terr = ext4_mb_load_buddy_gfp(sb, group, &e4b,\n\t\t\t\t\t     GFP_NOFS|__GFP_NOFAIL);\n\t\tif (err) {\n\t\t\text4_error_err(sb, -err, \"Error %d loading buddy information for %u\",\n\t\t\t\t       err, group);\n\t\t\tcontinue;\n\t\t}\n\t\text4_lock_group(sb, group);\n\t\tlist_del(&pa->pa_group_list);\n\t\text4_mb_release_group_pa(&e4b, pa);\n\t\text4_unlock_group(sb, group);\n\n\t\text4_mb_unload_buddy(&e4b);\n\t\tlist_del(&pa->u.pa_tmp_list);\n\t\tcall_rcu(&(pa)->u.pa_rcu, ext4_mb_pa_callback);\n\t}\n}\n\n/*\n * We have incremented pa_count. So it cannot be freed at this\n * point. Also we hold lg_mutex. So no parallel allocation is\n * possible from this lg. That means pa_free cannot be updated.\n *\n * A parallel ext4_mb_discard_group_preallocations is possible.\n * which can cause the lg_prealloc_list to be updated.\n */\n\nstatic void ext4_mb_add_n_trim(struct ext4_allocation_context *ac)\n{\n\tint order, added = 0, lg_prealloc_count = 1;\n\tstruct super_block *sb = ac->ac_sb;\n\tstruct ext4_locality_group *lg = ac->ac_lg;\n\tstruct ext4_prealloc_space *tmp_pa, *pa = ac->ac_pa;\n\n\torder = fls(pa->pa_free) - 1;\n\tif (order > PREALLOC_TB_SIZE - 1)\n\t\t/* The max size of hash table is PREALLOC_TB_SIZE */\n\t\torder = PREALLOC_TB_SIZE - 1;\n\t/* Add the prealloc space to lg */\n\tspin_lock(&lg->lg_prealloc_lock);\n\tlist_for_each_entry_rcu(tmp_pa, &lg->lg_prealloc_list[order],\n\t\t\t\tpa_inode_list,\n\t\t\t\tlockdep_is_held(&lg->lg_prealloc_lock)) {\n\t\tspin_lock(&tmp_pa->pa_lock);\n\t\tif (tmp_pa->pa_deleted) {\n\t\t\tspin_unlock(&tmp_pa->pa_lock);\n\t\t\tcontinue;\n\t\t}\n\t\tif (!added && pa->pa_free < tmp_pa->pa_free) {\n\t\t\t/* Add to the tail of the previous entry */\n\t\t\tlist_add_tail_rcu(&pa->pa_inode_list,\n\t\t\t\t\t\t&tmp_pa->pa_inode_list);\n\t\t\tadded = 1;\n\t\t\t/*\n\t\t\t * we want to count the total\n\t\t\t * number of entries in the list\n\t\t\t */\n\t\t}\n\t\tspin_unlock(&tmp_pa->pa_lock);\n\t\tlg_prealloc_count++;\n\t}\n\tif (!added)\n\t\tlist_add_tail_rcu(&pa->pa_inode_list,\n\t\t\t\t\t&lg->lg_prealloc_list[order]);\n\tspin_unlock(&lg->lg_prealloc_lock);\n\n\t/* Now trim the list to be not more than 8 elements */\n\tif (lg_prealloc_count > 8) {\n\t\text4_mb_discard_lg_preallocations(sb, lg,\n\t\t\t\t\t\t  order, lg_prealloc_count);\n\t\treturn;\n\t}\n\treturn ;\n}\n\n/*\n * if per-inode prealloc list is too long, trim some PA\n */\nstatic void ext4_mb_trim_inode_pa(struct inode *inode)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tint count, delta;\n\n\tcount = atomic_read(&ei->i_prealloc_active);\n\tdelta = (sbi->s_mb_max_inode_prealloc >> 2) + 1;\n\tif (count > sbi->s_mb_max_inode_prealloc + delta) {\n\t\tcount -= sbi->s_mb_max_inode_prealloc;\n\t\text4_discard_preallocations(inode, count);\n\t}\n}\n\n/*\n * release all resource we used in allocation\n */\nstatic int ext4_mb_release_context(struct ext4_allocation_context *ac)\n{\n\tstruct inode *inode = ac->ac_inode;\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct ext4_sb_info *sbi = EXT4_SB(ac->ac_sb);\n\tstruct ext4_prealloc_space *pa = ac->ac_pa;\n\tif (pa) {\n\t\tif (pa->pa_type == MB_GROUP_PA) {\n\t\t\t/* see comment in ext4_mb_use_group_pa() */\n\t\t\tspin_lock(&pa->pa_lock);\n\t\t\tpa->pa_pstart += EXT4_C2B(sbi, ac->ac_b_ex.fe_len);\n\t\t\tpa->pa_lstart += EXT4_C2B(sbi, ac->ac_b_ex.fe_len);\n\t\t\tpa->pa_free -= ac->ac_b_ex.fe_len;\n\t\t\tpa->pa_len -= ac->ac_b_ex.fe_len;\n\t\t\tspin_unlock(&pa->pa_lock);\n\n\t\t\t/*\n\t\t\t * We want to add the pa to the right bucket.\n\t\t\t * Remove it from the list and while adding\n\t\t\t * make sure the list to which we are adding\n\t\t\t * doesn't grow big.\n\t\t\t */\n\t\t\tif (likely(pa->pa_free)) {\n\t\t\t\tspin_lock(pa->pa_obj_lock);\n\t\t\t\tlist_del_rcu(&pa->pa_inode_list);\n\t\t\t\tspin_unlock(pa->pa_obj_lock);\n\t\t\t\text4_mb_add_n_trim(ac);\n\t\t\t}\n\t\t}\n\n\t\tif (pa->pa_type == MB_INODE_PA) {\n\t\t\t/*\n\t\t\t * treat per-inode prealloc list as a lru list, then try\n\t\t\t * to trim the least recently used PA.\n\t\t\t */\n\t\t\tspin_lock(pa->pa_obj_lock);\n\t\t\tlist_move(&pa->pa_inode_list, &ei->i_prealloc_list);\n\t\t\tspin_unlock(pa->pa_obj_lock);\n\t\t}\n\n\t\text4_mb_put_pa(ac, ac->ac_sb, pa);\n\t}\n\tif (ac->ac_bitmap_page)\n\t\tput_page(ac->ac_bitmap_page);\n\tif (ac->ac_buddy_page)\n\t\tput_page(ac->ac_buddy_page);\n\tif (ac->ac_flags & EXT4_MB_HINT_GROUP_ALLOC)\n\t\tmutex_unlock(&ac->ac_lg->lg_mutex);\n\text4_mb_collect_stats(ac);\n\text4_mb_trim_inode_pa(inode);\n\treturn 0;\n}\n\nstatic int ext4_mb_discard_preallocations(struct super_block *sb, int needed)\n{\n\text4_group_t i, ngroups = ext4_get_groups_count(sb);\n\tint ret;\n\tint freed = 0;\n\n\ttrace_ext4_mb_discard_preallocations(sb, needed);\n\tfor (i = 0; i < ngroups && needed > 0; i++) {\n\t\tret = ext4_mb_discard_group_preallocations(sb, i, needed);\n\t\tfreed += ret;\n\t\tneeded -= ret;\n\t}\n\n\treturn freed;\n}\n\nstatic bool ext4_mb_discard_preallocations_should_retry(struct super_block *sb,\n\t\t\tstruct ext4_allocation_context *ac, u64 *seq)\n{\n\tint freed;\n\tu64 seq_retry = 0;\n\tbool ret = false;\n\n\tfreed = ext4_mb_discard_preallocations(sb, ac->ac_o_ex.fe_len);\n\tif (freed) {\n\t\tret = true;\n\t\tgoto out_dbg;\n\t}\n\tseq_retry = ext4_get_discard_pa_seq_sum();\n\tif (!(ac->ac_flags & EXT4_MB_STRICT_CHECK) || seq_retry != *seq) {\n\t\tac->ac_flags |= EXT4_MB_STRICT_CHECK;\n\t\t*seq = seq_retry;\n\t\tret = true;\n\t}\n\nout_dbg:\n\tmb_debug(sb, \"freed %d, retry ? %s\\n\", freed, ret ? \"yes\" : \"no\");\n\treturn ret;\n}\n\nstatic ext4_fsblk_t ext4_mb_new_blocks_simple(handle_t *handle,\n\t\t\t\tstruct ext4_allocation_request *ar, int *errp);\n\n/*\n * Main entry point into mballoc to allocate blocks\n * it tries to use preallocation first, then falls back\n * to usual allocation\n */\next4_fsblk_t ext4_mb_new_blocks(handle_t *handle,\n\t\t\t\tstruct ext4_allocation_request *ar, int *errp)\n{\n\tstruct ext4_allocation_context *ac = NULL;\n\tstruct ext4_sb_info *sbi;\n\tstruct super_block *sb;\n\text4_fsblk_t block = 0;\n\tunsigned int inquota = 0;\n\tunsigned int reserv_clstrs = 0;\n\tu64 seq;\n\n\tmight_sleep();\n\tsb = ar->inode->i_sb;\n\tsbi = EXT4_SB(sb);\n\n\ttrace_ext4_request_blocks(ar);\n\tif (sbi->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn ext4_mb_new_blocks_simple(handle, ar, errp);\n\n\t/* Allow to use superuser reservation for quota file */\n\tif (ext4_is_quota_file(ar->inode))\n\t\tar->flags |= EXT4_MB_USE_ROOT_BLOCKS;\n\n\tif ((ar->flags & EXT4_MB_DELALLOC_RESERVED) == 0) {\n\t\t/* Without delayed allocation we need to verify\n\t\t * there is enough free blocks to do block allocation\n\t\t * and verify allocation doesn't exceed the quota limits.\n\t\t */\n\t\twhile (ar->len &&\n\t\t\text4_claim_free_clusters(sbi, ar->len, ar->flags)) {\n\n\t\t\t/* let others to free the space */\n\t\t\tcond_resched();\n\t\t\tar->len = ar->len >> 1;\n\t\t}\n\t\tif (!ar->len) {\n\t\t\text4_mb_show_pa(sb);\n\t\t\t*errp = -ENOSPC;\n\t\t\treturn 0;\n\t\t}\n\t\treserv_clstrs = ar->len;\n\t\tif (ar->flags & EXT4_MB_USE_ROOT_BLOCKS) {\n\t\t\tdquot_alloc_block_nofail(ar->inode,\n\t\t\t\t\t\t EXT4_C2B(sbi, ar->len));\n\t\t} else {\n\t\t\twhile (ar->len &&\n\t\t\t\tdquot_alloc_block(ar->inode,\n\t\t\t\t\t\t  EXT4_C2B(sbi, ar->len))) {\n\n\t\t\t\tar->flags |= EXT4_MB_HINT_NOPREALLOC;\n\t\t\t\tar->len--;\n\t\t\t}\n\t\t}\n\t\tinquota = ar->len;\n\t\tif (ar->len == 0) {\n\t\t\t*errp = -EDQUOT;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tac = kmem_cache_zalloc(ext4_ac_cachep, GFP_NOFS);\n\tif (!ac) {\n\t\tar->len = 0;\n\t\t*errp = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t*errp = ext4_mb_initialize_context(ac, ar);\n\tif (*errp) {\n\t\tar->len = 0;\n\t\tgoto out;\n\t}\n\n\tac->ac_op = EXT4_MB_HISTORY_PREALLOC;\n\tseq = this_cpu_read(discard_pa_seq);\n\tif (!ext4_mb_use_preallocated(ac)) {\n\t\tac->ac_op = EXT4_MB_HISTORY_ALLOC;\n\t\text4_mb_normalize_request(ac, ar);\n\n\t\t*errp = ext4_mb_pa_alloc(ac);\n\t\tif (*errp)\n\t\t\tgoto errout;\nrepeat:\n\t\t/* allocate space in core */\n\t\t*errp = ext4_mb_regular_allocator(ac);\n\t\t/*\n\t\t * pa allocated above is added to grp->bb_prealloc_list only\n\t\t * when we were able to allocate some block i.e. when\n\t\t * ac->ac_status == AC_STATUS_FOUND.\n\t\t * And error from above mean ac->ac_status != AC_STATUS_FOUND\n\t\t * So we have to free this pa here itself.\n\t\t */\n\t\tif (*errp) {\n\t\t\text4_mb_pa_free(ac);\n\t\t\text4_discard_allocated_blocks(ac);\n\t\t\tgoto errout;\n\t\t}\n\t\tif (ac->ac_status == AC_STATUS_FOUND &&\n\t\t\tac->ac_o_ex.fe_len >= ac->ac_f_ex.fe_len)\n\t\t\text4_mb_pa_free(ac);\n\t}\n\tif (likely(ac->ac_status == AC_STATUS_FOUND)) {\n\t\t*errp = ext4_mb_mark_diskspace_used(ac, handle, reserv_clstrs);\n\t\tif (*errp) {\n\t\t\text4_discard_allocated_blocks(ac);\n\t\t\tgoto errout;\n\t\t} else {\n\t\t\tblock = ext4_grp_offs_to_block(sb, &ac->ac_b_ex);\n\t\t\tar->len = ac->ac_b_ex.fe_len;\n\t\t}\n\t} else {\n\t\tif (ext4_mb_discard_preallocations_should_retry(sb, ac, &seq))\n\t\t\tgoto repeat;\n\t\t/*\n\t\t * If block allocation fails then the pa allocated above\n\t\t * needs to be freed here itself.\n\t\t */\n\t\text4_mb_pa_free(ac);\n\t\t*errp = -ENOSPC;\n\t}\n\nerrout:\n\tif (*errp) {\n\t\tac->ac_b_ex.fe_len = 0;\n\t\tar->len = 0;\n\t\text4_mb_show_ac(ac);\n\t}\n\text4_mb_release_context(ac);\nout:\n\tif (ac)\n\t\tkmem_cache_free(ext4_ac_cachep, ac);\n\tif (inquota && ar->len < inquota)\n\t\tdquot_free_block(ar->inode, EXT4_C2B(sbi, inquota - ar->len));\n\tif (!ar->len) {\n\t\tif ((ar->flags & EXT4_MB_DELALLOC_RESERVED) == 0)\n\t\t\t/* release all the reserved blocks if non delalloc */\n\t\t\tpercpu_counter_sub(&sbi->s_dirtyclusters_counter,\n\t\t\t\t\t\treserv_clstrs);\n\t}\n\n\ttrace_ext4_allocate_blocks(ar, (unsigned long long)block);\n\n\treturn block;\n}\n\n/*\n * We can merge two free data extents only if the physical blocks\n * are contiguous, AND the extents were freed by the same transaction,\n * AND the blocks are associated with the same group.\n */\nstatic void ext4_try_merge_freed_extent(struct ext4_sb_info *sbi,\n\t\t\t\t\tstruct ext4_free_data *entry,\n\t\t\t\t\tstruct ext4_free_data *new_entry,\n\t\t\t\t\tstruct rb_root *entry_rb_root)\n{\n\tif ((entry->efd_tid != new_entry->efd_tid) ||\n\t    (entry->efd_group != new_entry->efd_group))\n\t\treturn;\n\tif (entry->efd_start_cluster + entry->efd_count ==\n\t    new_entry->efd_start_cluster) {\n\t\tnew_entry->efd_start_cluster = entry->efd_start_cluster;\n\t\tnew_entry->efd_count += entry->efd_count;\n\t} else if (new_entry->efd_start_cluster + new_entry->efd_count ==\n\t\t   entry->efd_start_cluster) {\n\t\tnew_entry->efd_count += entry->efd_count;\n\t} else\n\t\treturn;\n\tspin_lock(&sbi->s_md_lock);\n\tlist_del(&entry->efd_list);\n\tspin_unlock(&sbi->s_md_lock);\n\trb_erase(&entry->efd_node, entry_rb_root);\n\tkmem_cache_free(ext4_free_data_cachep, entry);\n}\n\nstatic noinline_for_stack int\next4_mb_free_metadata(handle_t *handle, struct ext4_buddy *e4b,\n\t\t      struct ext4_free_data *new_entry)\n{\n\text4_group_t group = e4b->bd_group;\n\text4_grpblk_t cluster;\n\text4_grpblk_t clusters = new_entry->efd_count;\n\tstruct ext4_free_data *entry;\n\tstruct ext4_group_info *db = e4b->bd_info;\n\tstruct super_block *sb = e4b->bd_sb;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct rb_node **n = &db->bb_free_root.rb_node, *node;\n\tstruct rb_node *parent = NULL, *new_node;\n\n\tBUG_ON(!ext4_handle_valid(handle));\n\tBUG_ON(e4b->bd_bitmap_page == NULL);\n\tBUG_ON(e4b->bd_buddy_page == NULL);\n\n\tnew_node = &new_entry->efd_node;\n\tcluster = new_entry->efd_start_cluster;\n\n\tif (!*n) {\n\t\t/* first free block exent. We need to\n\t\t   protect buddy cache from being freed,\n\t\t * otherwise we'll refresh it from\n\t\t * on-disk bitmap and lose not-yet-available\n\t\t * blocks */\n\t\tget_page(e4b->bd_buddy_page);\n\t\tget_page(e4b->bd_bitmap_page);\n\t}\n\twhile (*n) {\n\t\tparent = *n;\n\t\tentry = rb_entry(parent, struct ext4_free_data, efd_node);\n\t\tif (cluster < entry->efd_start_cluster)\n\t\t\tn = &(*n)->rb_left;\n\t\telse if (cluster >= (entry->efd_start_cluster + entry->efd_count))\n\t\t\tn = &(*n)->rb_right;\n\t\telse {\n\t\t\text4_grp_locked_error(sb, group, 0,\n\t\t\t\text4_group_first_block_no(sb, group) +\n\t\t\t\tEXT4_C2B(sbi, cluster),\n\t\t\t\t\"Block already on to-be-freed list\");\n\t\t\tkmem_cache_free(ext4_free_data_cachep, new_entry);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\trb_link_node(new_node, parent, n);\n\trb_insert_color(new_node, &db->bb_free_root);\n\n\t/* Now try to see the extent can be merged to left and right */\n\tnode = rb_prev(new_node);\n\tif (node) {\n\t\tentry = rb_entry(node, struct ext4_free_data, efd_node);\n\t\text4_try_merge_freed_extent(sbi, entry, new_entry,\n\t\t\t\t\t    &(db->bb_free_root));\n\t}\n\n\tnode = rb_next(new_node);\n\tif (node) {\n\t\tentry = rb_entry(node, struct ext4_free_data, efd_node);\n\t\text4_try_merge_freed_extent(sbi, entry, new_entry,\n\t\t\t\t\t    &(db->bb_free_root));\n\t}\n\n\tspin_lock(&sbi->s_md_lock);\n\tlist_add_tail(&new_entry->efd_list, &sbi->s_freed_data_list);\n\tsbi->s_mb_free_pending += clusters;\n\tspin_unlock(&sbi->s_md_lock);\n\treturn 0;\n}\n\n/*\n * Simple allocator for Ext4 fast commit replay path. It searches for blocks\n * linearly starting at the goal block and also excludes the blocks which\n * are going to be in use after fast commit replay.\n */\nstatic ext4_fsblk_t ext4_mb_new_blocks_simple(handle_t *handle,\n\t\t\t\tstruct ext4_allocation_request *ar, int *errp)\n{\n\tstruct buffer_head *bitmap_bh;\n\tstruct super_block *sb = ar->inode->i_sb;\n\text4_group_t group;\n\text4_grpblk_t blkoff;\n\tint i = sb->s_blocksize;\n\text4_fsblk_t goal, block;\n\tstruct ext4_super_block *es = EXT4_SB(sb)->s_es;\n\n\tgoal = ar->goal;\n\tif (goal < le32_to_cpu(es->s_first_data_block) ||\n\t\t\tgoal >= ext4_blocks_count(es))\n\t\tgoal = le32_to_cpu(es->s_first_data_block);\n\n\tar->len = 0;\n\text4_get_group_no_and_offset(sb, goal, &group, &blkoff);\n\tfor (; group < ext4_get_groups_count(sb); group++) {\n\t\tbitmap_bh = ext4_read_block_bitmap(sb, group);\n\t\tif (IS_ERR(bitmap_bh)) {\n\t\t\t*errp = PTR_ERR(bitmap_bh);\n\t\t\tpr_warn(\"Failed to read block bitmap\\n\");\n\t\t\treturn 0;\n\t\t}\n\n\t\text4_get_group_no_and_offset(sb,\n\t\t\tmax(ext4_group_first_block_no(sb, group), goal),\n\t\t\tNULL, &blkoff);\n\t\ti = mb_find_next_zero_bit(bitmap_bh->b_data, sb->s_blocksize,\n\t\t\t\t\t\tblkoff);\n\t\tbrelse(bitmap_bh);\n\t\tif (i >= sb->s_blocksize)\n\t\t\tcontinue;\n\t\tif (ext4_fc_replay_check_excluded(sb,\n\t\t\text4_group_first_block_no(sb, group) + i))\n\t\t\tcontinue;\n\t\tbreak;\n\t}\n\n\tif (group >= ext4_get_groups_count(sb) && i >= sb->s_blocksize)\n\t\treturn 0;\n\n\tblock = ext4_group_first_block_no(sb, group) + i;\n\text4_mb_mark_bb(sb, block, 1, 1);\n\tar->len = 1;\n\n\treturn block;\n}\n\nstatic void ext4_free_blocks_simple(struct inode *inode, ext4_fsblk_t block,\n\t\t\t\t\tunsigned long count)\n{\n\tstruct buffer_head *bitmap_bh;\n\tstruct super_block *sb = inode->i_sb;\n\tstruct ext4_group_desc *gdp;\n\tstruct buffer_head *gdp_bh;\n\text4_group_t group;\n\text4_grpblk_t blkoff;\n\tint already_freed = 0, err, i;\n\n\text4_get_group_no_and_offset(sb, block, &group, &blkoff);\n\tbitmap_bh = ext4_read_block_bitmap(sb, group);\n\tif (IS_ERR(bitmap_bh)) {\n\t\terr = PTR_ERR(bitmap_bh);\n\t\tpr_warn(\"Failed to read block bitmap\\n\");\n\t\treturn;\n\t}\n\tgdp = ext4_get_group_desc(sb, group, &gdp_bh);\n\tif (!gdp)\n\t\treturn;\n\n\tfor (i = 0; i < count; i++) {\n\t\tif (!mb_test_bit(blkoff + i, bitmap_bh->b_data))\n\t\t\talready_freed++;\n\t}\n\tmb_clear_bits(bitmap_bh->b_data, blkoff, count);\n\terr = ext4_handle_dirty_metadata(NULL, NULL, bitmap_bh);\n\tif (err)\n\t\treturn;\n\text4_free_group_clusters_set(\n\t\tsb, gdp, ext4_free_group_clusters(sb, gdp) +\n\t\tcount - already_freed);\n\text4_block_bitmap_csum_set(sb, group, gdp, bitmap_bh);\n\text4_group_desc_csum_set(sb, group, gdp);\n\text4_handle_dirty_metadata(NULL, NULL, gdp_bh);\n\tsync_dirty_buffer(bitmap_bh);\n\tsync_dirty_buffer(gdp_bh);\n\tbrelse(bitmap_bh);\n}\n\n/**\n * ext4_free_blocks() -- Free given blocks and update quota\n * @handle:\t\thandle for this transaction\n * @inode:\t\tinode\n * @bh:\t\t\toptional buffer of the block to be freed\n * @block:\t\tstarting physical block to be freed\n * @count:\t\tnumber of blocks to be freed\n * @flags:\t\tflags used by ext4_free_blocks\n */\nvoid ext4_free_blocks(handle_t *handle, struct inode *inode,\n\t\t      struct buffer_head *bh, ext4_fsblk_t block,\n\t\t      unsigned long count, int flags)\n{\n\tstruct buffer_head *bitmap_bh = NULL;\n\tstruct super_block *sb = inode->i_sb;\n\tstruct ext4_group_desc *gdp;\n\tunsigned int overflow;\n\text4_grpblk_t bit;\n\tstruct buffer_head *gd_bh;\n\text4_group_t block_group;\n\tstruct ext4_sb_info *sbi;\n\tstruct ext4_buddy e4b;\n\tunsigned int count_clusters;\n\tint err = 0;\n\tint ret;\n\n\tsbi = EXT4_SB(sb);\n\n\tif (sbi->s_mount_state & EXT4_FC_REPLAY) {\n\t\text4_free_blocks_simple(inode, block, count);\n\t\treturn;\n\t}\n\n\tmight_sleep();\n\tif (bh) {\n\t\tif (block)\n\t\t\tBUG_ON(block != bh->b_blocknr);\n\t\telse\n\t\t\tblock = bh->b_blocknr;\n\t}\n\n\tif (!(flags & EXT4_FREE_BLOCKS_VALIDATED) &&\n\t    !ext4_inode_block_valid(inode, block, count)) {\n\t\text4_error(sb, \"Freeing blocks not in datazone - \"\n\t\t\t   \"block = %llu, count = %lu\", block, count);\n\t\tgoto error_return;\n\t}\n\n\text4_debug(\"freeing block %llu\\n\", block);\n\ttrace_ext4_free_blocks(inode, block, count, flags);\n\n\tif (bh && (flags & EXT4_FREE_BLOCKS_FORGET)) {\n\t\tBUG_ON(count > 1);\n\n\t\text4_forget(handle, flags & EXT4_FREE_BLOCKS_METADATA,\n\t\t\t    inode, bh, block);\n\t}\n\n\t/*\n\t * If the extent to be freed does not begin on a cluster\n\t * boundary, we need to deal with partial clusters at the\n\t * beginning and end of the extent.  Normally we will free\n\t * blocks at the beginning or the end unless we are explicitly\n\t * requested to avoid doing so.\n\t */\n\toverflow = EXT4_PBLK_COFF(sbi, block);\n\tif (overflow) {\n\t\tif (flags & EXT4_FREE_BLOCKS_NOFREE_FIRST_CLUSTER) {\n\t\t\toverflow = sbi->s_cluster_ratio - overflow;\n\t\t\tblock += overflow;\n\t\t\tif (count > overflow)\n\t\t\t\tcount -= overflow;\n\t\t\telse\n\t\t\t\treturn;\n\t\t} else {\n\t\t\tblock -= overflow;\n\t\t\tcount += overflow;\n\t\t}\n\t}\n\toverflow = EXT4_LBLK_COFF(sbi, count);\n\tif (overflow) {\n\t\tif (flags & EXT4_FREE_BLOCKS_NOFREE_LAST_CLUSTER) {\n\t\t\tif (count > overflow)\n\t\t\t\tcount -= overflow;\n\t\t\telse\n\t\t\t\treturn;\n\t\t} else\n\t\t\tcount += sbi->s_cluster_ratio - overflow;\n\t}\n\n\tif (!bh && (flags & EXT4_FREE_BLOCKS_FORGET)) {\n\t\tint i;\n\t\tint is_metadata = flags & EXT4_FREE_BLOCKS_METADATA;\n\n\t\tfor (i = 0; i < count; i++) {\n\t\t\tcond_resched();\n\t\t\tif (is_metadata)\n\t\t\t\tbh = sb_find_get_block(inode->i_sb, block + i);\n\t\t\text4_forget(handle, is_metadata, inode, bh, block + i);\n\t\t}\n\t}\n\ndo_more:\n\toverflow = 0;\n\text4_get_group_no_and_offset(sb, block, &block_group, &bit);\n\n\tif (unlikely(EXT4_MB_GRP_BBITMAP_CORRUPT(\n\t\t\text4_get_group_info(sb, block_group))))\n\t\treturn;\n\n\t/*\n\t * Check to see if we are freeing blocks across a group\n\t * boundary.\n\t */\n\tif (EXT4_C2B(sbi, bit) + count > EXT4_BLOCKS_PER_GROUP(sb)) {\n\t\toverflow = EXT4_C2B(sbi, bit) + count -\n\t\t\tEXT4_BLOCKS_PER_GROUP(sb);\n\t\tcount -= overflow;\n\t}\n\tcount_clusters = EXT4_NUM_B2C(sbi, count);\n\tbitmap_bh = ext4_read_block_bitmap(sb, block_group);\n\tif (IS_ERR(bitmap_bh)) {\n\t\terr = PTR_ERR(bitmap_bh);\n\t\tbitmap_bh = NULL;\n\t\tgoto error_return;\n\t}\n\tgdp = ext4_get_group_desc(sb, block_group, &gd_bh);\n\tif (!gdp) {\n\t\terr = -EIO;\n\t\tgoto error_return;\n\t}\n\n\tif (in_range(ext4_block_bitmap(sb, gdp), block, count) ||\n\t    in_range(ext4_inode_bitmap(sb, gdp), block, count) ||\n\t    in_range(block, ext4_inode_table(sb, gdp),\n\t\t     sbi->s_itb_per_group) ||\n\t    in_range(block + count - 1, ext4_inode_table(sb, gdp),\n\t\t     sbi->s_itb_per_group)) {\n\n\t\text4_error(sb, \"Freeing blocks in system zone - \"\n\t\t\t   \"Block = %llu, count = %lu\", block, count);\n\t\t/* err = 0. ext4_std_error should be a no op */\n\t\tgoto error_return;\n\t}\n\n\tBUFFER_TRACE(bitmap_bh, \"getting write access\");\n\terr = ext4_journal_get_write_access(handle, bitmap_bh);\n\tif (err)\n\t\tgoto error_return;\n\n\t/*\n\t * We are about to modify some metadata.  Call the journal APIs\n\t * to unshare ->b_data if a currently-committing transaction is\n\t * using it\n\t */\n\tBUFFER_TRACE(gd_bh, \"get_write_access\");\n\terr = ext4_journal_get_write_access(handle, gd_bh);\n\tif (err)\n\t\tgoto error_return;\n#ifdef AGGRESSIVE_CHECK\n\t{\n\t\tint i;\n\t\tfor (i = 0; i < count_clusters; i++)\n\t\t\tBUG_ON(!mb_test_bit(bit + i, bitmap_bh->b_data));\n\t}\n#endif\n\ttrace_ext4_mballoc_free(sb, inode, block_group, bit, count_clusters);\n\n\t/* __GFP_NOFAIL: retry infinitely, ignore TIF_MEMDIE and memcg limit. */\n\terr = ext4_mb_load_buddy_gfp(sb, block_group, &e4b,\n\t\t\t\t     GFP_NOFS|__GFP_NOFAIL);\n\tif (err)\n\t\tgoto error_return;\n\n\t/*\n\t * We need to make sure we don't reuse the freed block until after the\n\t * transaction is committed. We make an exception if the inode is to be\n\t * written in writeback mode since writeback mode has weak data\n\t * consistency guarantees.\n\t */\n\tif (ext4_handle_valid(handle) &&\n\t    ((flags & EXT4_FREE_BLOCKS_METADATA) ||\n\t     !ext4_should_writeback_data(inode))) {\n\t\tstruct ext4_free_data *new_entry;\n\t\t/*\n\t\t * We use __GFP_NOFAIL because ext4_free_blocks() is not allowed\n\t\t * to fail.\n\t\t */\n\t\tnew_entry = kmem_cache_alloc(ext4_free_data_cachep,\n\t\t\t\tGFP_NOFS|__GFP_NOFAIL);\n\t\tnew_entry->efd_start_cluster = bit;\n\t\tnew_entry->efd_group = block_group;\n\t\tnew_entry->efd_count = count_clusters;\n\t\tnew_entry->efd_tid = handle->h_transaction->t_tid;\n\n\t\text4_lock_group(sb, block_group);\n\t\tmb_clear_bits(bitmap_bh->b_data, bit, count_clusters);\n\t\text4_mb_free_metadata(handle, &e4b, new_entry);\n\t} else {\n\t\t/* need to update group_info->bb_free and bitmap\n\t\t * with group lock held. generate_buddy look at\n\t\t * them with group lock_held\n\t\t */\n\t\tif (test_opt(sb, DISCARD)) {\n\t\t\terr = ext4_issue_discard(sb, block_group, bit, count,\n\t\t\t\t\t\t NULL);\n\t\t\tif (err && err != -EOPNOTSUPP)\n\t\t\t\text4_msg(sb, KERN_WARNING, \"discard request in\"\n\t\t\t\t\t \" group:%d block:%d count:%lu failed\"\n\t\t\t\t\t \" with %d\", block_group, bit, count,\n\t\t\t\t\t err);\n\t\t} else\n\t\t\tEXT4_MB_GRP_CLEAR_TRIMMED(e4b.bd_info);\n\n\t\text4_lock_group(sb, block_group);\n\t\tmb_clear_bits(bitmap_bh->b_data, bit, count_clusters);\n\t\tmb_free_blocks(inode, &e4b, bit, count_clusters);\n\t}\n\n\tret = ext4_free_group_clusters(sb, gdp) + count_clusters;\n\text4_free_group_clusters_set(sb, gdp, ret);\n\text4_block_bitmap_csum_set(sb, block_group, gdp, bitmap_bh);\n\text4_group_desc_csum_set(sb, block_group, gdp);\n\text4_unlock_group(sb, block_group);\n\n\tif (sbi->s_log_groups_per_flex) {\n\t\text4_group_t flex_group = ext4_flex_group(sbi, block_group);\n\t\tatomic64_add(count_clusters,\n\t\t\t     &sbi_array_rcu_deref(sbi, s_flex_groups,\n\t\t\t\t\t\t  flex_group)->free_clusters);\n\t}\n\n\t/*\n\t * on a bigalloc file system, defer the s_freeclusters_counter\n\t * update to the caller (ext4_remove_space and friends) so they\n\t * can determine if a cluster freed here should be rereserved\n\t */\n\tif (!(flags & EXT4_FREE_BLOCKS_RERESERVE_CLUSTER)) {\n\t\tif (!(flags & EXT4_FREE_BLOCKS_NO_QUOT_UPDATE))\n\t\t\tdquot_free_block(inode, EXT4_C2B(sbi, count_clusters));\n\t\tpercpu_counter_add(&sbi->s_freeclusters_counter,\n\t\t\t\t   count_clusters);\n\t}\n\n\text4_mb_unload_buddy(&e4b);\n\n\t/* We dirtied the bitmap block */\n\tBUFFER_TRACE(bitmap_bh, \"dirtied bitmap block\");\n\terr = ext4_handle_dirty_metadata(handle, NULL, bitmap_bh);\n\n\t/* And the group descriptor block */\n\tBUFFER_TRACE(gd_bh, \"dirtied group descriptor block\");\n\tret = ext4_handle_dirty_metadata(handle, NULL, gd_bh);\n\tif (!err)\n\t\terr = ret;\n\n\tif (overflow && !err) {\n\t\tblock += count;\n\t\tcount = overflow;\n\t\tput_bh(bitmap_bh);\n\t\tgoto do_more;\n\t}\nerror_return:\n\tbrelse(bitmap_bh);\n\text4_std_error(sb, err);\n\treturn;\n}\n\n/**\n * ext4_group_add_blocks() -- Add given blocks to an existing group\n * @handle:\t\t\thandle to this transaction\n * @sb:\t\t\t\tsuper block\n * @block:\t\t\tstart physical block to add to the block group\n * @count:\t\t\tnumber of blocks to free\n *\n * This marks the blocks as free in the bitmap and buddy.\n */\nint ext4_group_add_blocks(handle_t *handle, struct super_block *sb,\n\t\t\t ext4_fsblk_t block, unsigned long count)\n{\n\tstruct buffer_head *bitmap_bh = NULL;\n\tstruct buffer_head *gd_bh;\n\text4_group_t block_group;\n\text4_grpblk_t bit;\n\tunsigned int i;\n\tstruct ext4_group_desc *desc;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_buddy e4b;\n\tint err = 0, ret, free_clusters_count;\n\text4_grpblk_t clusters_freed;\n\text4_fsblk_t first_cluster = EXT4_B2C(sbi, block);\n\text4_fsblk_t last_cluster = EXT4_B2C(sbi, block + count - 1);\n\tunsigned long cluster_count = last_cluster - first_cluster + 1;\n\n\text4_debug(\"Adding block(s) %llu-%llu\\n\", block, block + count - 1);\n\n\tif (count == 0)\n\t\treturn 0;\n\n\text4_get_group_no_and_offset(sb, block, &block_group, &bit);\n\t/*\n\t * Check to see if we are freeing blocks across a group\n\t * boundary.\n\t */\n\tif (bit + cluster_count > EXT4_CLUSTERS_PER_GROUP(sb)) {\n\t\text4_warning(sb, \"too many blocks added to group %u\",\n\t\t\t     block_group);\n\t\terr = -EINVAL;\n\t\tgoto error_return;\n\t}\n\n\tbitmap_bh = ext4_read_block_bitmap(sb, block_group);\n\tif (IS_ERR(bitmap_bh)) {\n\t\terr = PTR_ERR(bitmap_bh);\n\t\tbitmap_bh = NULL;\n\t\tgoto error_return;\n\t}\n\n\tdesc = ext4_get_group_desc(sb, block_group, &gd_bh);\n\tif (!desc) {\n\t\terr = -EIO;\n\t\tgoto error_return;\n\t}\n\n\tif (in_range(ext4_block_bitmap(sb, desc), block, count) ||\n\t    in_range(ext4_inode_bitmap(sb, desc), block, count) ||\n\t    in_range(block, ext4_inode_table(sb, desc), sbi->s_itb_per_group) ||\n\t    in_range(block + count - 1, ext4_inode_table(sb, desc),\n\t\t     sbi->s_itb_per_group)) {\n\t\text4_error(sb, \"Adding blocks in system zones - \"\n\t\t\t   \"Block = %llu, count = %lu\",\n\t\t\t   block, count);\n\t\terr = -EINVAL;\n\t\tgoto error_return;\n\t}\n\n\tBUFFER_TRACE(bitmap_bh, \"getting write access\");\n\terr = ext4_journal_get_write_access(handle, bitmap_bh);\n\tif (err)\n\t\tgoto error_return;\n\n\t/*\n\t * We are about to modify some metadata.  Call the journal APIs\n\t * to unshare ->b_data if a currently-committing transaction is\n\t * using it\n\t */\n\tBUFFER_TRACE(gd_bh, \"get_write_access\");\n\terr = ext4_journal_get_write_access(handle, gd_bh);\n\tif (err)\n\t\tgoto error_return;\n\n\tfor (i = 0, clusters_freed = 0; i < cluster_count; i++) {\n\t\tBUFFER_TRACE(bitmap_bh, \"clear bit\");\n\t\tif (!mb_test_bit(bit + i, bitmap_bh->b_data)) {\n\t\t\text4_error(sb, \"bit already cleared for block %llu\",\n\t\t\t\t   (ext4_fsblk_t)(block + i));\n\t\t\tBUFFER_TRACE(bitmap_bh, \"bit already cleared\");\n\t\t} else {\n\t\t\tclusters_freed++;\n\t\t}\n\t}\n\n\terr = ext4_mb_load_buddy(sb, block_group, &e4b);\n\tif (err)\n\t\tgoto error_return;\n\n\t/*\n\t * need to update group_info->bb_free and bitmap\n\t * with group lock held. generate_buddy look at\n\t * them with group lock_held\n\t */\n\text4_lock_group(sb, block_group);\n\tmb_clear_bits(bitmap_bh->b_data, bit, cluster_count);\n\tmb_free_blocks(NULL, &e4b, bit, cluster_count);\n\tfree_clusters_count = clusters_freed +\n\t\text4_free_group_clusters(sb, desc);\n\text4_free_group_clusters_set(sb, desc, free_clusters_count);\n\text4_block_bitmap_csum_set(sb, block_group, desc, bitmap_bh);\n\text4_group_desc_csum_set(sb, block_group, desc);\n\text4_unlock_group(sb, block_group);\n\tpercpu_counter_add(&sbi->s_freeclusters_counter,\n\t\t\t   clusters_freed);\n\n\tif (sbi->s_log_groups_per_flex) {\n\t\text4_group_t flex_group = ext4_flex_group(sbi, block_group);\n\t\tatomic64_add(clusters_freed,\n\t\t\t     &sbi_array_rcu_deref(sbi, s_flex_groups,\n\t\t\t\t\t\t  flex_group)->free_clusters);\n\t}\n\n\text4_mb_unload_buddy(&e4b);\n\n\t/* We dirtied the bitmap block */\n\tBUFFER_TRACE(bitmap_bh, \"dirtied bitmap block\");\n\terr = ext4_handle_dirty_metadata(handle, NULL, bitmap_bh);\n\n\t/* And the group descriptor block */\n\tBUFFER_TRACE(gd_bh, \"dirtied group descriptor block\");\n\tret = ext4_handle_dirty_metadata(handle, NULL, gd_bh);\n\tif (!err)\n\t\terr = ret;\n\nerror_return:\n\tbrelse(bitmap_bh);\n\text4_std_error(sb, err);\n\treturn err;\n}\n\n/**\n * ext4_trim_extent -- function to TRIM one single free extent in the group\n * @sb:\t\tsuper block for the file system\n * @start:\tstarting block of the free extent in the alloc. group\n * @count:\tnumber of blocks to TRIM\n * @group:\talloc. group we are working with\n * @e4b:\text4 buddy for the group\n *\n * Trim \"count\" blocks starting at \"start\" in the \"group\". To assure that no\n * one will allocate those blocks, mark it as used in buddy bitmap. This must\n * be called with under the group lock.\n */\nstatic int ext4_trim_extent(struct super_block *sb, int start, int count,\n\t\t\t     ext4_group_t group, struct ext4_buddy *e4b)\n__releases(bitlock)\n__acquires(bitlock)\n{\n\tstruct ext4_free_extent ex;\n\tint ret = 0;\n\n\ttrace_ext4_trim_extent(sb, group, start, count);\n\n\tassert_spin_locked(ext4_group_lock_ptr(sb, group));\n\n\tex.fe_start = start;\n\tex.fe_group = group;\n\tex.fe_len = count;\n\n\t/*\n\t * Mark blocks used, so no one can reuse them while\n\t * being trimmed.\n\t */\n\tmb_mark_used(e4b, &ex);\n\text4_unlock_group(sb, group);\n\tret = ext4_issue_discard(sb, group, start, count, NULL);\n\text4_lock_group(sb, group);\n\tmb_free_blocks(NULL, e4b, start, ex.fe_len);\n\treturn ret;\n}\n\n/**\n * ext4_trim_all_free -- function to trim all free space in alloc. group\n * @sb:\t\t\tsuper block for file system\n * @group:\t\tgroup to be trimmed\n * @start:\t\tfirst group block to examine\n * @max:\t\tlast group block to examine\n * @minblocks:\t\tminimum extent block count\n *\n * ext4_trim_all_free walks through group's buddy bitmap searching for free\n * extents. When the free block is found, ext4_trim_extent is called to TRIM\n * the extent.\n *\n *\n * ext4_trim_all_free walks through group's block bitmap searching for free\n * extents. When the free extent is found, mark it as used in group buddy\n * bitmap. Then issue a TRIM command on this extent and free the extent in\n * the group buddy bitmap. This is done until whole group is scanned.\n */\nstatic ext4_grpblk_t\next4_trim_all_free(struct super_block *sb, ext4_group_t group,\n\t\t   ext4_grpblk_t start, ext4_grpblk_t max,\n\t\t   ext4_grpblk_t minblocks)\n{\n\tvoid *bitmap;\n\text4_grpblk_t next, count = 0, free_count = 0;\n\tstruct ext4_buddy e4b;\n\tint ret = 0;\n\n\ttrace_ext4_trim_all_free(sb, group, start, max);\n\n\tret = ext4_mb_load_buddy(sb, group, &e4b);\n\tif (ret) {\n\t\text4_warning(sb, \"Error %d loading buddy information for %u\",\n\t\t\t     ret, group);\n\t\treturn ret;\n\t}\n\tbitmap = e4b.bd_bitmap;\n\n\text4_lock_group(sb, group);\n\tif (EXT4_MB_GRP_WAS_TRIMMED(e4b.bd_info) &&\n\t    minblocks >= atomic_read(&EXT4_SB(sb)->s_last_trim_minblks))\n\t\tgoto out;\n\n\tstart = (e4b.bd_info->bb_first_free > start) ?\n\t\te4b.bd_info->bb_first_free : start;\n\n\twhile (start <= max) {\n\t\tstart = mb_find_next_zero_bit(bitmap, max + 1, start);\n\t\tif (start > max)\n\t\t\tbreak;\n\t\tnext = mb_find_next_bit(bitmap, max + 1, start);\n\n\t\tif ((next - start) >= minblocks) {\n\t\t\tret = ext4_trim_extent(sb, start,\n\t\t\t\t\t       next - start, group, &e4b);\n\t\t\tif (ret && ret != -EOPNOTSUPP)\n\t\t\t\tbreak;\n\t\t\tret = 0;\n\t\t\tcount += next - start;\n\t\t}\n\t\tfree_count += next - start;\n\t\tstart = next + 1;\n\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tcount = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (need_resched()) {\n\t\t\text4_unlock_group(sb, group);\n\t\t\tcond_resched();\n\t\t\text4_lock_group(sb, group);\n\t\t}\n\n\t\tif ((e4b.bd_info->bb_free - free_count) < minblocks)\n\t\t\tbreak;\n\t}\n\n\tif (!ret) {\n\t\tret = count;\n\t\tEXT4_MB_GRP_SET_TRIMMED(e4b.bd_info);\n\t}\nout:\n\text4_unlock_group(sb, group);\n\text4_mb_unload_buddy(&e4b);\n\n\text4_debug(\"trimmed %d blocks in the group %d\\n\",\n\t\tcount, group);\n\n\treturn ret;\n}\n\n/**\n * ext4_trim_fs() -- trim ioctl handle function\n * @sb:\t\t\tsuperblock for filesystem\n * @range:\t\tfstrim_range structure\n *\n * start:\tFirst Byte to trim\n * len:\t\tnumber of Bytes to trim from start\n * minlen:\tminimum extent length in Bytes\n * ext4_trim_fs goes through all allocation groups containing Bytes from\n * start to start+len. For each such a group ext4_trim_all_free function\n * is invoked to trim all free space.\n */\nint ext4_trim_fs(struct super_block *sb, struct fstrim_range *range)\n{\n\tstruct ext4_group_info *grp;\n\text4_group_t group, first_group, last_group;\n\text4_grpblk_t cnt = 0, first_cluster, last_cluster;\n\tuint64_t start, end, minlen, trimmed = 0;\n\text4_fsblk_t first_data_blk =\n\t\t\tle32_to_cpu(EXT4_SB(sb)->s_es->s_first_data_block);\n\text4_fsblk_t max_blks = ext4_blocks_count(EXT4_SB(sb)->s_es);\n\tint ret = 0;\n\n\tstart = range->start >> sb->s_blocksize_bits;\n\tend = start + (range->len >> sb->s_blocksize_bits) - 1;\n\tminlen = EXT4_NUM_B2C(EXT4_SB(sb),\n\t\t\t      range->minlen >> sb->s_blocksize_bits);\n\n\tif (minlen > EXT4_CLUSTERS_PER_GROUP(sb) ||\n\t    start >= max_blks ||\n\t    range->len < sb->s_blocksize)\n\t\treturn -EINVAL;\n\tif (end >= max_blks)\n\t\tend = max_blks - 1;\n\tif (end <= first_data_blk)\n\t\tgoto out;\n\tif (start < first_data_blk)\n\t\tstart = first_data_blk;\n\n\t/* Determine first and last group to examine based on start and end */\n\text4_get_group_no_and_offset(sb, (ext4_fsblk_t) start,\n\t\t\t\t     &first_group, &first_cluster);\n\text4_get_group_no_and_offset(sb, (ext4_fsblk_t) end,\n\t\t\t\t     &last_group, &last_cluster);\n\n\t/* end now represents the last cluster to discard in this group */\n\tend = EXT4_CLUSTERS_PER_GROUP(sb) - 1;\n\n\tfor (group = first_group; group <= last_group; group++) {\n\t\tgrp = ext4_get_group_info(sb, group);\n\t\t/* We only do this if the grp has never been initialized */\n\t\tif (unlikely(EXT4_MB_GRP_NEED_INIT(grp))) {\n\t\t\tret = ext4_mb_init_group(sb, group, GFP_NOFS);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * For all the groups except the last one, last cluster will\n\t\t * always be EXT4_CLUSTERS_PER_GROUP(sb)-1, so we only need to\n\t\t * change it for the last group, note that last_cluster is\n\t\t * already computed earlier by ext4_get_group_no_and_offset()\n\t\t */\n\t\tif (group == last_group)\n\t\t\tend = last_cluster;\n\n\t\tif (grp->bb_free >= minlen) {\n\t\t\tcnt = ext4_trim_all_free(sb, group, first_cluster,\n\t\t\t\t\t\tend, minlen);\n\t\t\tif (cnt < 0) {\n\t\t\t\tret = cnt;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttrimmed += cnt;\n\t\t}\n\n\t\t/*\n\t\t * For every group except the first one, we are sure\n\t\t * that the first cluster to discard will be cluster #0.\n\t\t */\n\t\tfirst_cluster = 0;\n\t}\n\n\tif (!ret)\n\t\tatomic_set(&EXT4_SB(sb)->s_last_trim_minblks, minlen);\n\nout:\n\trange->len = EXT4_C2B(EXT4_SB(sb), trimmed) << sb->s_blocksize_bits;\n\treturn ret;\n}\n\n/* Iterate all the free extents in the group. */\nint\next4_mballoc_query_range(\n\tstruct super_block\t\t*sb,\n\text4_group_t\t\t\tgroup,\n\text4_grpblk_t\t\t\tstart,\n\text4_grpblk_t\t\t\tend,\n\text4_mballoc_query_range_fn\tformatter,\n\tvoid\t\t\t\t*priv)\n{\n\tvoid\t\t\t\t*bitmap;\n\text4_grpblk_t\t\t\tnext;\n\tstruct ext4_buddy\t\te4b;\n\tint\t\t\t\terror;\n\n\terror = ext4_mb_load_buddy(sb, group, &e4b);\n\tif (error)\n\t\treturn error;\n\tbitmap = e4b.bd_bitmap;\n\n\text4_lock_group(sb, group);\n\n\tstart = (e4b.bd_info->bb_first_free > start) ?\n\t\te4b.bd_info->bb_first_free : start;\n\tif (end >= EXT4_CLUSTERS_PER_GROUP(sb))\n\t\tend = EXT4_CLUSTERS_PER_GROUP(sb) - 1;\n\n\twhile (start <= end) {\n\t\tstart = mb_find_next_zero_bit(bitmap, end + 1, start);\n\t\tif (start > end)\n\t\t\tbreak;\n\t\tnext = mb_find_next_bit(bitmap, end + 1, start);\n\n\t\text4_unlock_group(sb, group);\n\t\terror = formatter(sb, group, start, next - start, priv);\n\t\tif (error)\n\t\t\tgoto out_unload;\n\t\text4_lock_group(sb, group);\n\n\t\tstart = next + 1;\n\t}\n\n\text4_unlock_group(sb, group);\nout_unload:\n\text4_mb_unload_buddy(&e4b);\n\n\treturn error;\n}\n"}, "1": {"id": 1, "path": "/src/include/linux/kernel.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_KERNEL_H\n#define _LINUX_KERNEL_H\n\n#include <stdarg.h>\n#include <linux/align.h>\n#include <linux/limits.h>\n#include <linux/linkage.h>\n#include <linux/stddef.h>\n#include <linux/types.h>\n#include <linux/compiler.h>\n#include <linux/bitops.h>\n#include <linux/log2.h>\n#include <linux/math.h>\n#include <linux/minmax.h>\n#include <linux/typecheck.h>\n#include <linux/printk.h>\n#include <linux/build_bug.h>\n#include <linux/static_call_types.h>\n#include <asm/byteorder.h>\n\n#include <uapi/linux/kernel.h>\n\n#define STACK_MAGIC\t0xdeadbeef\n\n/**\n * REPEAT_BYTE - repeat the value @x multiple times as an unsigned long value\n * @x: value to repeat\n *\n * NOTE: @x is not checked for > 0xff; larger values produce odd results.\n */\n#define REPEAT_BYTE(x)\t((~0ul / 0xff) * (x))\n\n/* generic data direction definitions */\n#define READ\t\t\t0\n#define WRITE\t\t\t1\n\n/**\n * ARRAY_SIZE - get the number of elements in array @arr\n * @arr: array to be sized\n */\n#define ARRAY_SIZE(arr) (sizeof(arr) / sizeof((arr)[0]) + __must_be_array(arr))\n\n#define u64_to_user_ptr(x) (\t\t\\\n{\t\t\t\t\t\\\n\ttypecheck(u64, (x));\t\t\\\n\t(void __user *)(uintptr_t)(x);\t\\\n}\t\t\t\t\t\\\n)\n\n#define typeof_member(T, m)\ttypeof(((T*)0)->m)\n\n#define _RET_IP_\t\t(unsigned long)__builtin_return_address(0)\n#define _THIS_IP_  ({ __label__ __here; __here: (unsigned long)&&__here; })\n\n/**\n * upper_32_bits - return bits 32-63 of a number\n * @n: the number we're accessing\n *\n * A basic shift-right of a 64- or 32-bit quantity.  Use this to suppress\n * the \"right shift count >= width of type\" warning when that quantity is\n * 32-bits.\n */\n#define upper_32_bits(n) ((u32)(((n) >> 16) >> 16))\n\n/**\n * lower_32_bits - return bits 0-31 of a number\n * @n: the number we're accessing\n */\n#define lower_32_bits(n) ((u32)((n) & 0xffffffff))\n\nstruct completion;\nstruct pt_regs;\nstruct user;\n\n#ifdef CONFIG_PREEMPT_VOLUNTARY\n\nextern int __cond_resched(void);\n# define might_resched() __cond_resched()\n\n#elif defined(CONFIG_PREEMPT_DYNAMIC)\n\nextern int __cond_resched(void);\n\nDECLARE_STATIC_CALL(might_resched, __cond_resched);\n\nstatic __always_inline void might_resched(void)\n{\n\tstatic_call_mod(might_resched)();\n}\n\n#else\n\n# define might_resched() do { } while (0)\n\n#endif /* CONFIG_PREEMPT_* */\n\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\nextern void ___might_sleep(const char *file, int line, int preempt_offset);\nextern void __might_sleep(const char *file, int line, int preempt_offset);\nextern void __cant_sleep(const char *file, int line, int preempt_offset);\nextern void __cant_migrate(const char *file, int line);\n\n/**\n * might_sleep - annotation for functions that can sleep\n *\n * this macro will print a stack trace if it is executed in an atomic\n * context (spinlock, irq-handler, ...). Additional sections where blocking is\n * not allowed can be annotated with non_block_start() and non_block_end()\n * pairs.\n *\n * This is a useful debugging help to be able to catch problems early and not\n * be bitten later when the calling function happens to sleep when it is not\n * supposed to.\n */\n# define might_sleep() \\\n\tdo { __might_sleep(__FILE__, __LINE__, 0); might_resched(); } while (0)\n/**\n * cant_sleep - annotation for functions that cannot sleep\n *\n * this macro will print a stack trace if it is executed with preemption enabled\n */\n# define cant_sleep() \\\n\tdo { __cant_sleep(__FILE__, __LINE__, 0); } while (0)\n# define sched_annotate_sleep()\t(current->task_state_change = 0)\n\n/**\n * cant_migrate - annotation for functions that cannot migrate\n *\n * Will print a stack trace if executed in code which is migratable\n */\n# define cant_migrate()\t\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (IS_ENABLED(CONFIG_SMP))\t\t\t\t\\\n\t\t\t__cant_migrate(__FILE__, __LINE__);\t\t\\\n\t} while (0)\n\n/**\n * non_block_start - annotate the start of section where sleeping is prohibited\n *\n * This is on behalf of the oom reaper, specifically when it is calling the mmu\n * notifiers. The problem is that if the notifier were to block on, for example,\n * mutex_lock() and if the process which holds that mutex were to perform a\n * sleeping memory allocation, the oom reaper is now blocked on completion of\n * that memory allocation. Other blocking calls like wait_event() pose similar\n * issues.\n */\n# define non_block_start() (current->non_block_count++)\n/**\n * non_block_end - annotate the end of section where sleeping is prohibited\n *\n * Closes a section opened by non_block_start().\n */\n# define non_block_end() WARN_ON(current->non_block_count-- == 0)\n#else\n  static inline void ___might_sleep(const char *file, int line,\n\t\t\t\t   int preempt_offset) { }\n  static inline void __might_sleep(const char *file, int line,\n\t\t\t\t   int preempt_offset) { }\n# define might_sleep() do { might_resched(); } while (0)\n# define cant_sleep() do { } while (0)\n# define cant_migrate()\t\tdo { } while (0)\n# define sched_annotate_sleep() do { } while (0)\n# define non_block_start() do { } while (0)\n# define non_block_end() do { } while (0)\n#endif\n\n#define might_sleep_if(cond) do { if (cond) might_sleep(); } while (0)\n\n#if defined(CONFIG_MMU) && \\\n\t(defined(CONFIG_PROVE_LOCKING) || defined(CONFIG_DEBUG_ATOMIC_SLEEP))\n#define might_fault() __might_fault(__FILE__, __LINE__)\nvoid __might_fault(const char *file, int line);\n#else\nstatic inline void might_fault(void) { }\n#endif\n\nextern struct atomic_notifier_head panic_notifier_list;\nextern long (*panic_blink)(int state);\n__printf(1, 2)\nvoid panic(const char *fmt, ...) __noreturn __cold;\nvoid nmi_panic(struct pt_regs *regs, const char *msg);\nextern void oops_enter(void);\nextern void oops_exit(void);\nextern bool oops_may_print(void);\nvoid do_exit(long error_code) __noreturn;\nvoid complete_and_exit(struct completion *, long) __noreturn;\n\n/* Internal, do not use. */\nint __must_check _kstrtoul(const char *s, unsigned int base, unsigned long *res);\nint __must_check _kstrtol(const char *s, unsigned int base, long *res);\n\nint __must_check kstrtoull(const char *s, unsigned int base, unsigned long long *res);\nint __must_check kstrtoll(const char *s, unsigned int base, long long *res);\n\n/**\n * kstrtoul - convert a string to an unsigned long\n * @s: The start of the string. The string must be null-terminated, and may also\n *  include a single newline before its terminating null. The first character\n *  may also be a plus sign, but not a minus sign.\n * @base: The number base to use. The maximum supported base is 16. If base is\n *  given as 0, then the base of the string is automatically detected with the\n *  conventional semantics - If it begins with 0x the number will be parsed as a\n *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be\n *  parsed as an octal number. Otherwise it will be parsed as a decimal.\n * @res: Where to write the result of the conversion on success.\n *\n * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.\n * Preferred over simple_strtoul(). Return code must be checked.\n*/\nstatic inline int __must_check kstrtoul(const char *s, unsigned int base, unsigned long *res)\n{\n\t/*\n\t * We want to shortcut function call, but\n\t * __builtin_types_compatible_p(unsigned long, unsigned long long) = 0.\n\t */\n\tif (sizeof(unsigned long) == sizeof(unsigned long long) &&\n\t    __alignof__(unsigned long) == __alignof__(unsigned long long))\n\t\treturn kstrtoull(s, base, (unsigned long long *)res);\n\telse\n\t\treturn _kstrtoul(s, base, res);\n}\n\n/**\n * kstrtol - convert a string to a long\n * @s: The start of the string. The string must be null-terminated, and may also\n *  include a single newline before its terminating null. The first character\n *  may also be a plus sign or a minus sign.\n * @base: The number base to use. The maximum supported base is 16. If base is\n *  given as 0, then the base of the string is automatically detected with the\n *  conventional semantics - If it begins with 0x the number will be parsed as a\n *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be\n *  parsed as an octal number. Otherwise it will be parsed as a decimal.\n * @res: Where to write the result of the conversion on success.\n *\n * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.\n * Preferred over simple_strtol(). Return code must be checked.\n */\nstatic inline int __must_check kstrtol(const char *s, unsigned int base, long *res)\n{\n\t/*\n\t * We want to shortcut function call, but\n\t * __builtin_types_compatible_p(long, long long) = 0.\n\t */\n\tif (sizeof(long) == sizeof(long long) &&\n\t    __alignof__(long) == __alignof__(long long))\n\t\treturn kstrtoll(s, base, (long long *)res);\n\telse\n\t\treturn _kstrtol(s, base, res);\n}\n\nint __must_check kstrtouint(const char *s, unsigned int base, unsigned int *res);\nint __must_check kstrtoint(const char *s, unsigned int base, int *res);\n\nstatic inline int __must_check kstrtou64(const char *s, unsigned int base, u64 *res)\n{\n\treturn kstrtoull(s, base, res);\n}\n\nstatic inline int __must_check kstrtos64(const char *s, unsigned int base, s64 *res)\n{\n\treturn kstrtoll(s, base, res);\n}\n\nstatic inline int __must_check kstrtou32(const char *s, unsigned int base, u32 *res)\n{\n\treturn kstrtouint(s, base, res);\n}\n\nstatic inline int __must_check kstrtos32(const char *s, unsigned int base, s32 *res)\n{\n\treturn kstrtoint(s, base, res);\n}\n\nint __must_check kstrtou16(const char *s, unsigned int base, u16 *res);\nint __must_check kstrtos16(const char *s, unsigned int base, s16 *res);\nint __must_check kstrtou8(const char *s, unsigned int base, u8 *res);\nint __must_check kstrtos8(const char *s, unsigned int base, s8 *res);\nint __must_check kstrtobool(const char *s, bool *res);\n\nint __must_check kstrtoull_from_user(const char __user *s, size_t count, unsigned int base, unsigned long long *res);\nint __must_check kstrtoll_from_user(const char __user *s, size_t count, unsigned int base, long long *res);\nint __must_check kstrtoul_from_user(const char __user *s, size_t count, unsigned int base, unsigned long *res);\nint __must_check kstrtol_from_user(const char __user *s, size_t count, unsigned int base, long *res);\nint __must_check kstrtouint_from_user(const char __user *s, size_t count, unsigned int base, unsigned int *res);\nint __must_check kstrtoint_from_user(const char __user *s, size_t count, unsigned int base, int *res);\nint __must_check kstrtou16_from_user(const char __user *s, size_t count, unsigned int base, u16 *res);\nint __must_check kstrtos16_from_user(const char __user *s, size_t count, unsigned int base, s16 *res);\nint __must_check kstrtou8_from_user(const char __user *s, size_t count, unsigned int base, u8 *res);\nint __must_check kstrtos8_from_user(const char __user *s, size_t count, unsigned int base, s8 *res);\nint __must_check kstrtobool_from_user(const char __user *s, size_t count, bool *res);\n\nstatic inline int __must_check kstrtou64_from_user(const char __user *s, size_t count, unsigned int base, u64 *res)\n{\n\treturn kstrtoull_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtos64_from_user(const char __user *s, size_t count, unsigned int base, s64 *res)\n{\n\treturn kstrtoll_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtou32_from_user(const char __user *s, size_t count, unsigned int base, u32 *res)\n{\n\treturn kstrtouint_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtos32_from_user(const char __user *s, size_t count, unsigned int base, s32 *res)\n{\n\treturn kstrtoint_from_user(s, count, base, res);\n}\n\n/*\n * Use kstrto<foo> instead.\n *\n * NOTE: simple_strto<foo> does not check for the range overflow and,\n *\t depending on the input, may give interesting results.\n *\n * Use these functions if and only if you cannot use kstrto<foo>, because\n * the conversion ends on the first non-digit character, which may be far\n * beyond the supported range. It might be useful to parse the strings like\n * 10x50 or 12:21 without altering original string or temporary buffer in use.\n * Keep in mind above caveat.\n */\n\nextern unsigned long simple_strtoul(const char *,char **,unsigned int);\nextern long simple_strtol(const char *,char **,unsigned int);\nextern unsigned long long simple_strtoull(const char *,char **,unsigned int);\nextern long long simple_strtoll(const char *,char **,unsigned int);\n\nextern int num_to_str(char *buf, int size,\n\t\t      unsigned long long num, unsigned int width);\n\n/* lib/printf utilities */\n\nextern __printf(2, 3) int sprintf(char *buf, const char * fmt, ...);\nextern __printf(2, 0) int vsprintf(char *buf, const char *, va_list);\nextern __printf(3, 4)\nint snprintf(char *buf, size_t size, const char *fmt, ...);\nextern __printf(3, 0)\nint vsnprintf(char *buf, size_t size, const char *fmt, va_list args);\nextern __printf(3, 4)\nint scnprintf(char *buf, size_t size, const char *fmt, ...);\nextern __printf(3, 0)\nint vscnprintf(char *buf, size_t size, const char *fmt, va_list args);\nextern __printf(2, 3) __malloc\nchar *kasprintf(gfp_t gfp, const char *fmt, ...);\nextern __printf(2, 0) __malloc\nchar *kvasprintf(gfp_t gfp, const char *fmt, va_list args);\nextern __printf(2, 0)\nconst char *kvasprintf_const(gfp_t gfp, const char *fmt, va_list args);\n\nextern __scanf(2, 3)\nint sscanf(const char *, const char *, ...);\nextern __scanf(2, 0)\nint vsscanf(const char *, const char *, va_list);\n\nextern int get_option(char **str, int *pint);\nextern char *get_options(const char *str, int nints, int *ints);\nextern unsigned long long memparse(const char *ptr, char **retptr);\nextern bool parse_option_str(const char *str, const char *option);\nextern char *next_arg(char *args, char **param, char **val);\n\nextern int core_kernel_text(unsigned long addr);\nextern int init_kernel_text(unsigned long addr);\nextern int core_kernel_data(unsigned long addr);\nextern int __kernel_text_address(unsigned long addr);\nextern int kernel_text_address(unsigned long addr);\nextern int func_ptr_is_kernel_text(void *ptr);\n\n#ifdef CONFIG_SMP\nextern unsigned int sysctl_oops_all_cpu_backtrace;\n#else\n#define sysctl_oops_all_cpu_backtrace 0\n#endif /* CONFIG_SMP */\n\nextern void bust_spinlocks(int yes);\nextern int panic_timeout;\nextern unsigned long panic_print;\nextern int panic_on_oops;\nextern int panic_on_unrecovered_nmi;\nextern int panic_on_io_nmi;\nextern int panic_on_warn;\nextern unsigned long panic_on_taint;\nextern bool panic_on_taint_nousertaint;\nextern int sysctl_panic_on_rcu_stall;\nextern int sysctl_max_rcu_stall_to_panic;\nextern int sysctl_panic_on_stackoverflow;\n\nextern bool crash_kexec_post_notifiers;\n\n/*\n * panic_cpu is used for synchronizing panic() and crash_kexec() execution. It\n * holds a CPU number which is executing panic() currently. A value of\n * PANIC_CPU_INVALID means no CPU has entered panic() or crash_kexec().\n */\nextern atomic_t panic_cpu;\n#define PANIC_CPU_INVALID\t-1\n\n/*\n * Only to be used by arch init code. If the user over-wrote the default\n * CONFIG_PANIC_TIMEOUT, honor it.\n */\nstatic inline void set_arch_panic_timeout(int timeout, int arch_default_timeout)\n{\n\tif (panic_timeout == arch_default_timeout)\n\t\tpanic_timeout = timeout;\n}\nextern const char *print_tainted(void);\nenum lockdep_ok {\n\tLOCKDEP_STILL_OK,\n\tLOCKDEP_NOW_UNRELIABLE\n};\nextern void add_taint(unsigned flag, enum lockdep_ok);\nextern int test_taint(unsigned flag);\nextern unsigned long get_taint(void);\nextern int root_mountflags;\n\nextern bool early_boot_irqs_disabled;\n\n/*\n * Values used for system_state. Ordering of the states must not be changed\n * as code checks for <, <=, >, >= STATE.\n */\nextern enum system_states {\n\tSYSTEM_BOOTING,\n\tSYSTEM_SCHEDULING,\n\tSYSTEM_RUNNING,\n\tSYSTEM_HALT,\n\tSYSTEM_POWER_OFF,\n\tSYSTEM_RESTART,\n\tSYSTEM_SUSPEND,\n} system_state;\n\n/* This cannot be an enum because some may be used in assembly source. */\n#define TAINT_PROPRIETARY_MODULE\t0\n#define TAINT_FORCED_MODULE\t\t1\n#define TAINT_CPU_OUT_OF_SPEC\t\t2\n#define TAINT_FORCED_RMMOD\t\t3\n#define TAINT_MACHINE_CHECK\t\t4\n#define TAINT_BAD_PAGE\t\t\t5\n#define TAINT_USER\t\t\t6\n#define TAINT_DIE\t\t\t7\n#define TAINT_OVERRIDDEN_ACPI_TABLE\t8\n#define TAINT_WARN\t\t\t9\n#define TAINT_CRAP\t\t\t10\n#define TAINT_FIRMWARE_WORKAROUND\t11\n#define TAINT_OOT_MODULE\t\t12\n#define TAINT_UNSIGNED_MODULE\t\t13\n#define TAINT_SOFTLOCKUP\t\t14\n#define TAINT_LIVEPATCH\t\t\t15\n#define TAINT_AUX\t\t\t16\n#define TAINT_RANDSTRUCT\t\t17\n#define TAINT_FLAGS_COUNT\t\t18\n#define TAINT_FLAGS_MAX\t\t\t((1UL << TAINT_FLAGS_COUNT) - 1)\n\nstruct taint_flag {\n\tchar c_true;\t/* character printed when tainted */\n\tchar c_false;\t/* character printed when not tainted */\n\tbool module;\t/* also show as a per-module taint flag */\n};\n\nextern const struct taint_flag taint_flags[TAINT_FLAGS_COUNT];\n\nextern const char hex_asc[];\n#define hex_asc_lo(x)\thex_asc[((x) & 0x0f)]\n#define hex_asc_hi(x)\thex_asc[((x) & 0xf0) >> 4]\n\nstatic inline char *hex_byte_pack(char *buf, u8 byte)\n{\n\t*buf++ = hex_asc_hi(byte);\n\t*buf++ = hex_asc_lo(byte);\n\treturn buf;\n}\n\nextern const char hex_asc_upper[];\n#define hex_asc_upper_lo(x)\thex_asc_upper[((x) & 0x0f)]\n#define hex_asc_upper_hi(x)\thex_asc_upper[((x) & 0xf0) >> 4]\n\nstatic inline char *hex_byte_pack_upper(char *buf, u8 byte)\n{\n\t*buf++ = hex_asc_upper_hi(byte);\n\t*buf++ = hex_asc_upper_lo(byte);\n\treturn buf;\n}\n\nextern int hex_to_bin(char ch);\nextern int __must_check hex2bin(u8 *dst, const char *src, size_t count);\nextern char *bin2hex(char *dst, const void *src, size_t count);\n\nbool mac_pton(const char *s, u8 *mac);\n\n/*\n * General tracing related utility functions - trace_printk(),\n * tracing_on/tracing_off and tracing_start()/tracing_stop\n *\n * Use tracing_on/tracing_off when you want to quickly turn on or off\n * tracing. It simply enables or disables the recording of the trace events.\n * This also corresponds to the user space /sys/kernel/debug/tracing/tracing_on\n * file, which gives a means for the kernel and userspace to interact.\n * Place a tracing_off() in the kernel where you want tracing to end.\n * From user space, examine the trace, and then echo 1 > tracing_on\n * to continue tracing.\n *\n * tracing_stop/tracing_start has slightly more overhead. It is used\n * by things like suspend to ram where disabling the recording of the\n * trace is not enough, but tracing must actually stop because things\n * like calling smp_processor_id() may crash the system.\n *\n * Most likely, you want to use tracing_on/tracing_off.\n */\n\nenum ftrace_dump_mode {\n\tDUMP_NONE,\n\tDUMP_ALL,\n\tDUMP_ORIG,\n};\n\n#ifdef CONFIG_TRACING\nvoid tracing_on(void);\nvoid tracing_off(void);\nint tracing_is_on(void);\nvoid tracing_snapshot(void);\nvoid tracing_snapshot_alloc(void);\n\nextern void tracing_start(void);\nextern void tracing_stop(void);\n\nstatic inline __printf(1, 2)\nvoid ____trace_printk_check_format(const char *fmt, ...)\n{\n}\n#define __trace_printk_check_format(fmt, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\t\\\n\t\t____trace_printk_check_format(fmt, ##args);\t\t\\\n} while (0)\n\n/**\n * trace_printk - printf formatting in the ftrace buffer\n * @fmt: the printf format for printing\n *\n * Note: __trace_printk is an internal function for trace_printk() and\n *       the @ip is passed in via the trace_printk() macro.\n *\n * This function allows a kernel developer to debug fast path sections\n * that printk is not appropriate for. By scattering in various\n * printk like tracing in the code, a developer can quickly see\n * where problems are occurring.\n *\n * This is intended as a debugging tool for the developer only.\n * Please refrain from leaving trace_printks scattered around in\n * your code. (Extra memory is used for special buffers that are\n * allocated when trace_printk() is used.)\n *\n * A little optimization trick is done here. If there's only one\n * argument, there's no need to scan the string for printf formats.\n * The trace_puts() will suffice. But how can we take advantage of\n * using trace_puts() when trace_printk() has only one argument?\n * By stringifying the args and checking the size we can tell\n * whether or not there are args. __stringify((__VA_ARGS__)) will\n * turn into \"()\\0\" with a size of 3 when there are no args, anything\n * else will be bigger. All we need to do is define a string to this,\n * and then take its size and compare to 3. If it's bigger, use\n * do_trace_printk() otherwise, optimize it to trace_puts(). Then just\n * let gcc optimize the rest.\n */\n\n#define trace_printk(fmt, ...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\\\n\tchar _______STR[] = __stringify((__VA_ARGS__));\t\\\n\tif (sizeof(_______STR) > 3)\t\t\t\\\n\t\tdo_trace_printk(fmt, ##__VA_ARGS__);\t\\\n\telse\t\t\t\t\t\t\\\n\t\ttrace_puts(fmt);\t\t\t\\\n} while (0)\n\n#define do_trace_printk(fmt, args...)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tstatic const char *trace_printk_fmt __used\t\t\t\\\n\t\t__section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t__builtin_constant_p(fmt) ? fmt : NULL;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t__trace_printk_check_format(fmt, ##args);\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(fmt))\t\t\t\t\t\\\n\t\t__trace_bprintk(_THIS_IP_, trace_printk_fmt, ##args);\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t__trace_printk(_THIS_IP_, fmt, ##args);\t\t\t\\\n} while (0)\n\nextern __printf(2, 3)\nint __trace_bprintk(unsigned long ip, const char *fmt, ...);\n\nextern __printf(2, 3)\nint __trace_printk(unsigned long ip, const char *fmt, ...);\n\n/**\n * trace_puts - write a string into the ftrace buffer\n * @str: the string to record\n *\n * Note: __trace_bputs is an internal function for trace_puts and\n *       the @ip is passed in via the trace_puts macro.\n *\n * This is similar to trace_printk() but is made for those really fast\n * paths that a developer wants the least amount of \"Heisenbug\" effects,\n * where the processing of the print format is still too much.\n *\n * This function allows a kernel developer to debug fast path sections\n * that printk is not appropriate for. By scattering in various\n * printk like tracing in the code, a developer can quickly see\n * where problems are occurring.\n *\n * This is intended as a debugging tool for the developer only.\n * Please refrain from leaving trace_puts scattered around in\n * your code. (Extra memory is used for special buffers that are\n * allocated when trace_puts() is used.)\n *\n * Returns: 0 if nothing was written, positive # if string was.\n *  (1 when __trace_bputs is used, strlen(str) when __trace_puts is used)\n */\n\n#define trace_puts(str) ({\t\t\t\t\t\t\\\n\tstatic const char *trace_printk_fmt __used\t\t\t\\\n\t\t__section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t__builtin_constant_p(str) ? str : NULL;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(str))\t\t\t\t\t\\\n\t\t__trace_bputs(_THIS_IP_, trace_printk_fmt);\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t__trace_puts(_THIS_IP_, str, strlen(str));\t\t\\\n})\nextern int __trace_bputs(unsigned long ip, const char *str);\nextern int __trace_puts(unsigned long ip, const char *str, int size);\n\nextern void trace_dump_stack(int skip);\n\n/*\n * The double __builtin_constant_p is because gcc will give us an error\n * if we try to allocate the static variable to fmt if it is not a\n * constant. Even with the outer if statement.\n */\n#define ftrace_vprintk(fmt, vargs)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(fmt)) {\t\t\t\t\\\n\t\tstatic const char *trace_printk_fmt __used\t\t\\\n\t\t  __section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t\t__builtin_constant_p(fmt) ? fmt : NULL;\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t__ftrace_vbprintk(_THIS_IP_, trace_printk_fmt, vargs);\t\\\n\t} else\t\t\t\t\t\t\t\t\\\n\t\t__ftrace_vprintk(_THIS_IP_, fmt, vargs);\t\t\\\n} while (0)\n\nextern __printf(2, 0) int\n__ftrace_vbprintk(unsigned long ip, const char *fmt, va_list ap);\n\nextern __printf(2, 0) int\n__ftrace_vprintk(unsigned long ip, const char *fmt, va_list ap);\n\nextern void ftrace_dump(enum ftrace_dump_mode oops_dump_mode);\n#else\nstatic inline void tracing_start(void) { }\nstatic inline void tracing_stop(void) { }\nstatic inline void trace_dump_stack(int skip) { }\n\nstatic inline void tracing_on(void) { }\nstatic inline void tracing_off(void) { }\nstatic inline int tracing_is_on(void) { return 0; }\nstatic inline void tracing_snapshot(void) { }\nstatic inline void tracing_snapshot_alloc(void) { }\n\nstatic inline __printf(1, 2)\nint trace_printk(const char *fmt, ...)\n{\n\treturn 0;\n}\nstatic __printf(1, 0) inline int\nftrace_vprintk(const char *fmt, va_list ap)\n{\n\treturn 0;\n}\nstatic inline void ftrace_dump(enum ftrace_dump_mode oops_dump_mode) { }\n#endif /* CONFIG_TRACING */\n\n/* This counts to 12. Any more, it will return 13th argument. */\n#define __COUNT_ARGS(_0, _1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _n, X...) _n\n#define COUNT_ARGS(X...) __COUNT_ARGS(, ##X, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0)\n\n#define __CONCAT(a, b) a ## b\n#define CONCATENATE(a, b) __CONCAT(a, b)\n\n/**\n * container_of - cast a member of a structure out to the containing structure\n * @ptr:\tthe pointer to the member.\n * @type:\tthe type of the container struct this is embedded in.\n * @member:\tthe name of the member within the struct.\n *\n */\n#define container_of(ptr, type, member) ({\t\t\t\t\\\n\tvoid *__mptr = (void *)(ptr);\t\t\t\t\t\\\n\tBUILD_BUG_ON_MSG(!__same_type(*(ptr), ((type *)0)->member) &&\t\\\n\t\t\t !__same_type(*(ptr), void),\t\t\t\\\n\t\t\t \"pointer type mismatch in container_of()\");\t\\\n\t((type *)(__mptr - offsetof(type, member))); })\n\n/**\n * container_of_safe - cast a member of a structure out to the containing structure\n * @ptr:\tthe pointer to the member.\n * @type:\tthe type of the container struct this is embedded in.\n * @member:\tthe name of the member within the struct.\n *\n * If IS_ERR_OR_NULL(ptr), ptr is returned unchanged.\n */\n#define container_of_safe(ptr, type, member) ({\t\t\t\t\\\n\tvoid *__mptr = (void *)(ptr);\t\t\t\t\t\\\n\tBUILD_BUG_ON_MSG(!__same_type(*(ptr), ((type *)0)->member) &&\t\\\n\t\t\t !__same_type(*(ptr), void),\t\t\t\\\n\t\t\t \"pointer type mismatch in container_of()\");\t\\\n\tIS_ERR_OR_NULL(__mptr) ? ERR_CAST(__mptr) :\t\t\t\\\n\t\t((type *)(__mptr - offsetof(type, member))); })\n\n/* Rebuild everything on CONFIG_FTRACE_MCOUNT_RECORD */\n#ifdef CONFIG_FTRACE_MCOUNT_RECORD\n# define REBUILD_DUE_TO_FTRACE_MCOUNT_RECORD\n#endif\n\n/* Permissions on a sysfs file: you didn't miss the 0 prefix did you? */\n#define VERIFY_OCTAL_PERMISSIONS(perms)\t\t\t\t\t\t\\\n\t(BUILD_BUG_ON_ZERO((perms) < 0) +\t\t\t\t\t\\\n\t BUILD_BUG_ON_ZERO((perms) > 0777) +\t\t\t\t\t\\\n\t /* USER_READABLE >= GROUP_READABLE >= OTHER_READABLE */\t\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 6) & 4) < (((perms) >> 3) & 4)) +\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 3) & 4) < ((perms) & 4)) +\t\t\\\n\t /* USER_WRITABLE >= GROUP_WRITABLE */\t\t\t\t\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 6) & 2) < (((perms) >> 3) & 2)) +\t\\\n\t /* OTHER_WRITABLE?  Generally considered a bad idea. */\t\t\\\n\t BUILD_BUG_ON_ZERO((perms) & 2) +\t\t\t\t\t\\\n\t (perms))\n#endif\n"}, "2": {"id": 2, "path": "/src/fs/ext4/mballoc.h", "content": "// SPDX-License-Identifier: GPL-2.0\n/*\n *  fs/ext4/mballoc.h\n *\n *  Written by: Alex Tomas <alex@clusterfs.com>\n *\n */\n#ifndef _EXT4_MBALLOC_H\n#define _EXT4_MBALLOC_H\n\n#include <linux/time.h>\n#include <linux/fs.h>\n#include <linux/namei.h>\n#include <linux/quotaops.h>\n#include <linux/buffer_head.h>\n#include <linux/module.h>\n#include <linux/swap.h>\n#include <linux/proc_fs.h>\n#include <linux/pagemap.h>\n#include <linux/seq_file.h>\n#include <linux/blkdev.h>\n#include <linux/mutex.h>\n#include \"ext4_jbd2.h\"\n#include \"ext4.h\"\n\n/*\n * mb_debug() dynamic printk msgs could be used to debug mballoc code.\n */\n#ifdef CONFIG_EXT4_DEBUG\n#define mb_debug(sb, fmt, ...)\t\t\t\t\t\t\\\n\tpr_debug(\"[%s/%d] EXT4-fs (%s): (%s, %d): %s: \" fmt,\t\t\\\n\t\tcurrent->comm, task_pid_nr(current), sb->s_id,\t\t\\\n\t       __FILE__, __LINE__, __func__, ##__VA_ARGS__)\n#else\n#define mb_debug(sb, fmt, ...)\tno_printk(fmt, ##__VA_ARGS__)\n#endif\n\n#define EXT4_MB_HISTORY_ALLOC\t\t1\t/* allocation */\n#define EXT4_MB_HISTORY_PREALLOC\t2\t/* preallocated blocks used */\n\n/*\n * How long mballoc can look for a best extent (in found extents)\n */\n#define MB_DEFAULT_MAX_TO_SCAN\t\t200\n\n/*\n * How long mballoc must look for a best extent\n */\n#define MB_DEFAULT_MIN_TO_SCAN\t\t10\n\n/*\n * with 'ext4_mb_stats' allocator will collect stats that will be\n * shown at umount. The collecting costs though!\n */\n#define MB_DEFAULT_STATS\t\t0\n\n/*\n * files smaller than MB_DEFAULT_STREAM_THRESHOLD are served\n * by the stream allocator, which purpose is to pack requests\n * as close each to other as possible to produce smooth I/O traffic\n * We use locality group prealloc space for stream request.\n * We can tune the same via /proc/fs/ext4/<parition>/stream_req\n */\n#define MB_DEFAULT_STREAM_THRESHOLD\t16\t/* 64K */\n\n/*\n * for which requests use 2^N search using buddies\n */\n#define MB_DEFAULT_ORDER2_REQS\t\t2\n\n/*\n * default group prealloc size 512 blocks\n */\n#define MB_DEFAULT_GROUP_PREALLOC\t512\n\n/*\n * maximum length of inode prealloc list\n */\n#define MB_DEFAULT_MAX_INODE_PREALLOC\t512\n\nstruct ext4_free_data {\n\t/* this links the free block information from sb_info */\n\tstruct list_head\t\tefd_list;\n\n\t/* this links the free block information from group_info */\n\tstruct rb_node\t\t\tefd_node;\n\n\t/* group which free block extent belongs */\n\text4_group_t\t\t\tefd_group;\n\n\t/* free block extent */\n\text4_grpblk_t\t\t\tefd_start_cluster;\n\text4_grpblk_t\t\t\tefd_count;\n\n\t/* transaction which freed this extent */\n\ttid_t\t\t\t\tefd_tid;\n};\n\nstruct ext4_prealloc_space {\n\tstruct list_head\tpa_inode_list;\n\tstruct list_head\tpa_group_list;\n\tunion {\n\t\tstruct list_head pa_tmp_list;\n\t\tstruct rcu_head\tpa_rcu;\n\t} u;\n\tspinlock_t\t\tpa_lock;\n\tatomic_t\t\tpa_count;\n\tunsigned\t\tpa_deleted;\n\text4_fsblk_t\t\tpa_pstart;\t/* phys. block */\n\text4_lblk_t\t\tpa_lstart;\t/* log. block */\n\text4_grpblk_t\t\tpa_len;\t\t/* len of preallocated chunk */\n\text4_grpblk_t\t\tpa_free;\t/* how many blocks are free */\n\tunsigned short\t\tpa_type;\t/* pa type. inode or group */\n\tspinlock_t\t\t*pa_obj_lock;\n\tstruct inode\t\t*pa_inode;\t/* hack, for history only */\n};\n\nenum {\n\tMB_INODE_PA = 0,\n\tMB_GROUP_PA = 1\n};\n\nstruct ext4_free_extent {\n\text4_lblk_t fe_logical;\n\text4_grpblk_t fe_start;\t/* In cluster units */\n\text4_group_t fe_group;\n\text4_grpblk_t fe_len;\t/* In cluster units */\n};\n\n/*\n * Locality group:\n *   we try to group all related changes together\n *   so that writeback can flush/allocate them together as well\n *   Size of lg_prealloc_list hash is determined by MB_DEFAULT_GROUP_PREALLOC\n *   (512). We store prealloc space into the hash based on the pa_free blocks\n *   order value.ie, fls(pa_free)-1;\n */\n#define PREALLOC_TB_SIZE 10\nstruct ext4_locality_group {\n\t/* for allocator */\n\t/* to serialize allocates */\n\tstruct mutex\t\tlg_mutex;\n\t/* list of preallocations */\n\tstruct list_head\tlg_prealloc_list[PREALLOC_TB_SIZE];\n\tspinlock_t\t\tlg_prealloc_lock;\n};\n\nstruct ext4_allocation_context {\n\tstruct inode *ac_inode;\n\tstruct super_block *ac_sb;\n\n\t/* original request */\n\tstruct ext4_free_extent ac_o_ex;\n\n\t/* goal request (normalized ac_o_ex) */\n\tstruct ext4_free_extent ac_g_ex;\n\n\t/* the best found extent */\n\tstruct ext4_free_extent ac_b_ex;\n\n\t/* copy of the best found extent taken before preallocation efforts */\n\tstruct ext4_free_extent ac_f_ex;\n\n\t__u16 ac_groups_scanned;\n\t__u16 ac_found;\n\t__u16 ac_tail;\n\t__u16 ac_buddy;\n\t__u16 ac_flags;\t\t/* allocation hints */\n\t__u8 ac_status;\n\t__u8 ac_criteria;\n\t__u8 ac_2order;\t\t/* if request is to allocate 2^N blocks and\n\t\t\t\t * N > 0, the field stores N, otherwise 0 */\n\t__u8 ac_op;\t\t/* operation, for history only */\n\tstruct page *ac_bitmap_page;\n\tstruct page *ac_buddy_page;\n\tstruct ext4_prealloc_space *ac_pa;\n\tstruct ext4_locality_group *ac_lg;\n};\n\n#define AC_STATUS_CONTINUE\t1\n#define AC_STATUS_FOUND\t\t2\n#define AC_STATUS_BREAK\t\t3\n\nstruct ext4_buddy {\n\tstruct page *bd_buddy_page;\n\tvoid *bd_buddy;\n\tstruct page *bd_bitmap_page;\n\tvoid *bd_bitmap;\n\tstruct ext4_group_info *bd_info;\n\tstruct super_block *bd_sb;\n\t__u16 bd_blkbits;\n\text4_group_t bd_group;\n};\n\nstatic inline ext4_fsblk_t ext4_grp_offs_to_block(struct super_block *sb,\n\t\t\t\t\tstruct ext4_free_extent *fex)\n{\n\treturn ext4_group_first_block_no(sb, fex->fe_group) +\n\t\t(fex->fe_start << EXT4_SB(sb)->s_cluster_bits);\n}\n\ntypedef int (*ext4_mballoc_query_range_fn)(\n\tstruct super_block\t\t*sb,\n\text4_group_t\t\t\tagno,\n\text4_grpblk_t\t\t\tstart,\n\text4_grpblk_t\t\t\tlen,\n\tvoid\t\t\t\t*priv);\n\nint\next4_mballoc_query_range(\n\tstruct super_block\t\t*sb,\n\text4_group_t\t\t\tagno,\n\text4_grpblk_t\t\t\tstart,\n\text4_grpblk_t\t\t\tend,\n\text4_mballoc_query_range_fn\tformatter,\n\tvoid\t\t\t\t*priv);\n\n#endif\n"}, "3": {"id": 3, "path": "/src/include/linux/printk.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __KERNEL_PRINTK__\n#define __KERNEL_PRINTK__\n\n#include <stdarg.h>\n#include <linux/init.h>\n#include <linux/kern_levels.h>\n#include <linux/linkage.h>\n#include <linux/cache.h>\n#include <linux/ratelimit_types.h>\n\nextern const char linux_banner[];\nextern const char linux_proc_banner[];\n\nextern int oops_in_progress;\t/* If set, an oops, panic(), BUG() or die() is in progress */\n\n#define PRINTK_MAX_SINGLE_HEADER_LEN 2\n\nstatic inline int printk_get_level(const char *buffer)\n{\n\tif (buffer[0] == KERN_SOH_ASCII && buffer[1]) {\n\t\tswitch (buffer[1]) {\n\t\tcase '0' ... '7':\n\t\tcase 'c':\t/* KERN_CONT */\n\t\t\treturn buffer[1];\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic inline const char *printk_skip_level(const char *buffer)\n{\n\tif (printk_get_level(buffer))\n\t\treturn buffer + 2;\n\n\treturn buffer;\n}\n\nstatic inline const char *printk_skip_headers(const char *buffer)\n{\n\twhile (printk_get_level(buffer))\n\t\tbuffer = printk_skip_level(buffer);\n\n\treturn buffer;\n}\n\n#define CONSOLE_EXT_LOG_MAX\t8192\n\n/* printk's without a loglevel use this.. */\n#define MESSAGE_LOGLEVEL_DEFAULT CONFIG_MESSAGE_LOGLEVEL_DEFAULT\n\n/* We show everything that is MORE important than this.. */\n#define CONSOLE_LOGLEVEL_SILENT  0 /* Mum's the word */\n#define CONSOLE_LOGLEVEL_MIN\t 1 /* Minimum loglevel we let people use */\n#define CONSOLE_LOGLEVEL_DEBUG\t10 /* issue debug messages */\n#define CONSOLE_LOGLEVEL_MOTORMOUTH 15\t/* You can't shut this one up */\n\n/*\n * Default used to be hard-coded at 7, quiet used to be hardcoded at 4,\n * we're now allowing both to be set from kernel config.\n */\n#define CONSOLE_LOGLEVEL_DEFAULT CONFIG_CONSOLE_LOGLEVEL_DEFAULT\n#define CONSOLE_LOGLEVEL_QUIET\t CONFIG_CONSOLE_LOGLEVEL_QUIET\n\nextern int console_printk[];\n\n#define console_loglevel (console_printk[0])\n#define default_message_loglevel (console_printk[1])\n#define minimum_console_loglevel (console_printk[2])\n#define default_console_loglevel (console_printk[3])\n\nstatic inline void console_silent(void)\n{\n\tconsole_loglevel = CONSOLE_LOGLEVEL_SILENT;\n}\n\nstatic inline void console_verbose(void)\n{\n\tif (console_loglevel)\n\t\tconsole_loglevel = CONSOLE_LOGLEVEL_MOTORMOUTH;\n}\n\n/* strlen(\"ratelimit\") + 1 */\n#define DEVKMSG_STR_MAX_SIZE 10\nextern char devkmsg_log_str[];\nstruct ctl_table;\n\nextern int suppress_printk;\n\nstruct va_format {\n\tconst char *fmt;\n\tva_list *va;\n};\n\n/*\n * FW_BUG\n * Add this to a message where you are sure the firmware is buggy or behaves\n * really stupid or out of spec. Be aware that the responsible BIOS developer\n * should be able to fix this issue or at least get a concrete idea of the\n * problem by reading your message without the need of looking at the kernel\n * code.\n *\n * Use it for definite and high priority BIOS bugs.\n *\n * FW_WARN\n * Use it for not that clear (e.g. could the kernel messed up things already?)\n * and medium priority BIOS bugs.\n *\n * FW_INFO\n * Use this one if you want to tell the user or vendor about something\n * suspicious, but generally harmless related to the firmware.\n *\n * Use it for information or very low priority BIOS bugs.\n */\n#define FW_BUG\t\t\"[Firmware Bug]: \"\n#define FW_WARN\t\t\"[Firmware Warn]: \"\n#define FW_INFO\t\t\"[Firmware Info]: \"\n\n/*\n * HW_ERR\n * Add this to a message for hardware errors, so that user can report\n * it to hardware vendor instead of LKML or software vendor.\n */\n#define HW_ERR\t\t\"[Hardware Error]: \"\n\n/*\n * DEPRECATED\n * Add this to a message whenever you want to warn user space about the use\n * of a deprecated aspect of an API so they can stop using it\n */\n#define DEPRECATED\t\"[Deprecated]: \"\n\n/*\n * Dummy printk for disabled debugging statements to use whilst maintaining\n * gcc's format checking.\n */\n#define no_printk(fmt, ...)\t\t\t\t\\\n({\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\\\n\t\tprintk(fmt, ##__VA_ARGS__);\t\t\\\n\t0;\t\t\t\t\t\t\\\n})\n\n#ifdef CONFIG_EARLY_PRINTK\nextern asmlinkage __printf(1, 2)\nvoid early_printk(const char *fmt, ...);\n#else\nstatic inline __printf(1, 2) __cold\nvoid early_printk(const char *s, ...) { }\n#endif\n\n#ifdef CONFIG_PRINTK_NMI\nextern void printk_nmi_enter(void);\nextern void printk_nmi_exit(void);\nextern void printk_nmi_direct_enter(void);\nextern void printk_nmi_direct_exit(void);\n#else\nstatic inline void printk_nmi_enter(void) { }\nstatic inline void printk_nmi_exit(void) { }\nstatic inline void printk_nmi_direct_enter(void) { }\nstatic inline void printk_nmi_direct_exit(void) { }\n#endif /* PRINTK_NMI */\n\nstruct dev_printk_info;\n\n#ifdef CONFIG_PRINTK\nasmlinkage __printf(4, 0)\nint vprintk_emit(int facility, int level,\n\t\t const struct dev_printk_info *dev_info,\n\t\t const char *fmt, va_list args);\n\nasmlinkage __printf(1, 0)\nint vprintk(const char *fmt, va_list args);\n\nasmlinkage __printf(1, 2) __cold\nint printk(const char *fmt, ...);\n\n/*\n * Special printk facility for scheduler/timekeeping use only, _DO_NOT_USE_ !\n */\n__printf(1, 2) __cold int printk_deferred(const char *fmt, ...);\n\n/*\n * Please don't use printk_ratelimit(), because it shares ratelimiting state\n * with all other unrelated printk_ratelimit() callsites.  Instead use\n * printk_ratelimited() or plain old __ratelimit().\n */\nextern int __printk_ratelimit(const char *func);\n#define printk_ratelimit() __printk_ratelimit(__func__)\nextern bool printk_timed_ratelimit(unsigned long *caller_jiffies,\n\t\t\t\t   unsigned int interval_msec);\n\nextern int printk_delay_msec;\nextern int dmesg_restrict;\n\nextern int\ndevkmsg_sysctl_set_loglvl(struct ctl_table *table, int write, void *buf,\n\t\t\t  size_t *lenp, loff_t *ppos);\n\nextern void wake_up_klogd(void);\n\nchar *log_buf_addr_get(void);\nu32 log_buf_len_get(void);\nvoid log_buf_vmcoreinfo_setup(void);\nvoid __init setup_log_buf(int early);\n__printf(1, 2) void dump_stack_set_arch_desc(const char *fmt, ...);\nvoid dump_stack_print_info(const char *log_lvl);\nvoid show_regs_print_info(const char *log_lvl);\nextern asmlinkage void dump_stack(void) __cold;\nextern void printk_safe_flush(void);\nextern void printk_safe_flush_on_panic(void);\n#else\nstatic inline __printf(1, 0)\nint vprintk(const char *s, va_list args)\n{\n\treturn 0;\n}\nstatic inline __printf(1, 2) __cold\nint printk(const char *s, ...)\n{\n\treturn 0;\n}\nstatic inline __printf(1, 2) __cold\nint printk_deferred(const char *s, ...)\n{\n\treturn 0;\n}\nstatic inline int printk_ratelimit(void)\n{\n\treturn 0;\n}\nstatic inline bool printk_timed_ratelimit(unsigned long *caller_jiffies,\n\t\t\t\t\t  unsigned int interval_msec)\n{\n\treturn false;\n}\n\nstatic inline void wake_up_klogd(void)\n{\n}\n\nstatic inline char *log_buf_addr_get(void)\n{\n\treturn NULL;\n}\n\nstatic inline u32 log_buf_len_get(void)\n{\n\treturn 0;\n}\n\nstatic inline void log_buf_vmcoreinfo_setup(void)\n{\n}\n\nstatic inline void setup_log_buf(int early)\n{\n}\n\nstatic inline __printf(1, 2) void dump_stack_set_arch_desc(const char *fmt, ...)\n{\n}\n\nstatic inline void dump_stack_print_info(const char *log_lvl)\n{\n}\n\nstatic inline void show_regs_print_info(const char *log_lvl)\n{\n}\n\nstatic inline void dump_stack(void)\n{\n}\n\nstatic inline void printk_safe_flush(void)\n{\n}\n\nstatic inline void printk_safe_flush_on_panic(void)\n{\n}\n#endif\n\nextern int kptr_restrict;\n\n/**\n * pr_fmt - used by the pr_*() macros to generate the printk format string\n * @fmt: format string passed from a pr_*() macro\n *\n * This macro can be used to generate a unified format string for pr_*()\n * macros. A common use is to prefix all pr_*() messages in a file with a common\n * string. For example, defining this at the top of a source file:\n *\n *        #define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n *\n * would prefix all pr_info, pr_emerg... messages in the file with the module\n * name.\n */\n#ifndef pr_fmt\n#define pr_fmt(fmt) fmt\n#endif\n\n/**\n * pr_emerg - Print an emergency-level message\n * @fmt: format string\n * @...: arguments for the format string\n *\n * This macro expands to a printk with KERN_EMERG loglevel. It uses pr_fmt() to\n * generate the format string.\n */\n#define pr_emerg(fmt, ...) \\\n\tprintk(KERN_EMERG pr_fmt(fmt), ##__VA_ARGS__)\n/**\n * pr_alert - Print an alert-level message\n * @fmt: format string\n * @...: arguments for the format string\n *\n * This macro expands to a printk with KERN_ALERT loglevel. It uses pr_fmt() to\n * generate the format string.\n */\n#define pr_alert(fmt, ...) \\\n\tprintk(KERN_ALERT pr_fmt(fmt), ##__VA_ARGS__)\n/**\n * pr_crit - Print a critical-level message\n * @fmt: format string\n * @...: arguments for the format string\n *\n * This macro expands to a printk with KERN_CRIT loglevel. It uses pr_fmt() to\n * generate the format string.\n */\n#define pr_crit(fmt, ...) \\\n\tprintk(KERN_CRIT pr_fmt(fmt), ##__VA_ARGS__)\n/**\n * pr_err - Print an error-level message\n * @fmt: format string\n * @...: arguments for the format string\n *\n * This macro expands to a printk with KERN_ERR loglevel. It uses pr_fmt() to\n * generate the format string.\n */\n#define pr_err(fmt, ...) \\\n\tprintk(KERN_ERR pr_fmt(fmt), ##__VA_ARGS__)\n/**\n * pr_warn - Print a warning-level message\n * @fmt: format string\n * @...: arguments for the format string\n *\n * This macro expands to a printk with KERN_WARNING loglevel. It uses pr_fmt()\n * to generate the format string.\n */\n#define pr_warn(fmt, ...) \\\n\tprintk(KERN_WARNING pr_fmt(fmt), ##__VA_ARGS__)\n/**\n * pr_notice - Print a notice-level message\n * @fmt: format string\n * @...: arguments for the format string\n *\n * This macro expands to a printk with KERN_NOTICE loglevel. It uses pr_fmt() to\n * generate the format string.\n */\n#define pr_notice(fmt, ...) \\\n\tprintk(KERN_NOTICE pr_fmt(fmt), ##__VA_ARGS__)\n/**\n * pr_info - Print an info-level message\n * @fmt: format string\n * @...: arguments for the format string\n *\n * This macro expands to a printk with KERN_INFO loglevel. It uses pr_fmt() to\n * generate the format string.\n */\n#define pr_info(fmt, ...) \\\n\tprintk(KERN_INFO pr_fmt(fmt), ##__VA_ARGS__)\n\n/**\n * pr_cont - Continues a previous log message in the same line.\n * @fmt: format string\n * @...: arguments for the format string\n *\n * This macro expands to a printk with KERN_CONT loglevel. It should only be\n * used when continuing a log message with no newline ('\\n') enclosed. Otherwise\n * it defaults back to KERN_DEFAULT loglevel.\n */\n#define pr_cont(fmt, ...) \\\n\tprintk(KERN_CONT fmt, ##__VA_ARGS__)\n\n/**\n * pr_devel - Print a debug-level message conditionally\n * @fmt: format string\n * @...: arguments for the format string\n *\n * This macro expands to a printk with KERN_DEBUG loglevel if DEBUG is\n * defined. Otherwise it does nothing.\n *\n * It uses pr_fmt() to generate the format string.\n */\n#ifdef DEBUG\n#define pr_devel(fmt, ...) \\\n\tprintk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#else\n#define pr_devel(fmt, ...) \\\n\tno_printk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#endif\n\n\n/* If you are writing a driver, please use dev_dbg instead */\n#if defined(CONFIG_DYNAMIC_DEBUG) || \\\n\t(defined(CONFIG_DYNAMIC_DEBUG_CORE) && defined(DYNAMIC_DEBUG_MODULE))\n#include <linux/dynamic_debug.h>\n\n/**\n * pr_debug - Print a debug-level message conditionally\n * @fmt: format string\n * @...: arguments for the format string\n *\n * This macro expands to dynamic_pr_debug() if CONFIG_DYNAMIC_DEBUG is\n * set. Otherwise, if DEBUG is defined, it's equivalent to a printk with\n * KERN_DEBUG loglevel. If DEBUG is not defined it does nothing.\n *\n * It uses pr_fmt() to generate the format string (dynamic_pr_debug() uses\n * pr_fmt() internally).\n */\n#define pr_debug(fmt, ...)\t\t\t\\\n\tdynamic_pr_debug(fmt, ##__VA_ARGS__)\n#elif defined(DEBUG)\n#define pr_debug(fmt, ...) \\\n\tprintk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#else\n#define pr_debug(fmt, ...) \\\n\tno_printk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#endif\n\n/*\n * Print a one-time message (analogous to WARN_ONCE() et al):\n */\n\n#ifdef CONFIG_PRINTK\n#define printk_once(fmt, ...)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tstatic bool __section(\".data.once\") __print_once;\t\\\n\tbool __ret_print_once = !__print_once;\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (!__print_once) {\t\t\t\t\t\\\n\t\t__print_once = true;\t\t\t\t\\\n\t\tprintk(fmt, ##__VA_ARGS__);\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_print_once);\t\t\t\t\\\n})\n#define printk_deferred_once(fmt, ...)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tstatic bool __section(\".data.once\") __print_once;\t\\\n\tbool __ret_print_once = !__print_once;\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (!__print_once) {\t\t\t\t\t\\\n\t\t__print_once = true;\t\t\t\t\\\n\t\tprintk_deferred(fmt, ##__VA_ARGS__);\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_print_once);\t\t\t\t\\\n})\n#else\n#define printk_once(fmt, ...)\t\t\t\t\t\\\n\tno_printk(fmt, ##__VA_ARGS__)\n#define printk_deferred_once(fmt, ...)\t\t\t\t\\\n\tno_printk(fmt, ##__VA_ARGS__)\n#endif\n\n#define pr_emerg_once(fmt, ...)\t\t\t\t\t\\\n\tprintk_once(KERN_EMERG pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_alert_once(fmt, ...)\t\t\t\t\t\\\n\tprintk_once(KERN_ALERT pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_crit_once(fmt, ...)\t\t\t\t\t\\\n\tprintk_once(KERN_CRIT pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_err_once(fmt, ...)\t\t\t\t\t\\\n\tprintk_once(KERN_ERR pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_warn_once(fmt, ...)\t\t\t\t\t\\\n\tprintk_once(KERN_WARNING pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_notice_once(fmt, ...)\t\t\t\t\\\n\tprintk_once(KERN_NOTICE pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_info_once(fmt, ...)\t\t\t\t\t\\\n\tprintk_once(KERN_INFO pr_fmt(fmt), ##__VA_ARGS__)\n/* no pr_cont_once, don't do that... */\n\n#if defined(DEBUG)\n#define pr_devel_once(fmt, ...)\t\t\t\t\t\\\n\tprintk_once(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#else\n#define pr_devel_once(fmt, ...)\t\t\t\t\t\\\n\tno_printk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#endif\n\n/* If you are writing a driver, please use dev_dbg instead */\n#if defined(DEBUG)\n#define pr_debug_once(fmt, ...)\t\t\t\t\t\\\n\tprintk_once(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#else\n#define pr_debug_once(fmt, ...)\t\t\t\t\t\\\n\tno_printk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#endif\n\n/*\n * ratelimited messages with local ratelimit_state,\n * no local ratelimit_state used in the !PRINTK case\n */\n#ifdef CONFIG_PRINTK\n#define printk_ratelimited(fmt, ...)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstatic DEFINE_RATELIMIT_STATE(_rs,\t\t\t\t\\\n\t\t\t\t      DEFAULT_RATELIMIT_INTERVAL,\t\\\n\t\t\t\t      DEFAULT_RATELIMIT_BURST);\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (__ratelimit(&_rs))\t\t\t\t\t\t\\\n\t\tprintk(fmt, ##__VA_ARGS__);\t\t\t\t\\\n})\n#else\n#define printk_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tno_printk(fmt, ##__VA_ARGS__)\n#endif\n\n#define pr_emerg_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tprintk_ratelimited(KERN_EMERG pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_alert_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tprintk_ratelimited(KERN_ALERT pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_crit_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tprintk_ratelimited(KERN_CRIT pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_err_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tprintk_ratelimited(KERN_ERR pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_warn_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tprintk_ratelimited(KERN_WARNING pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_notice_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tprintk_ratelimited(KERN_NOTICE pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_info_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tprintk_ratelimited(KERN_INFO pr_fmt(fmt), ##__VA_ARGS__)\n/* no pr_cont_ratelimited, don't do that... */\n\n#if defined(DEBUG)\n#define pr_devel_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tprintk_ratelimited(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#else\n#define pr_devel_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tno_printk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#endif\n\n/* If you are writing a driver, please use dev_dbg instead */\n#if defined(CONFIG_DYNAMIC_DEBUG) || \\\n\t(defined(CONFIG_DYNAMIC_DEBUG_CORE) && defined(DYNAMIC_DEBUG_MODULE))\n/* descriptor check is first to prevent flooding with \"callbacks suppressed\" */\n#define pr_debug_ratelimited(fmt, ...)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tstatic DEFINE_RATELIMIT_STATE(_rs,\t\t\t\t\\\n\t\t\t\t      DEFAULT_RATELIMIT_INTERVAL,\t\\\n\t\t\t\t      DEFAULT_RATELIMIT_BURST);\t\t\\\n\tDEFINE_DYNAMIC_DEBUG_METADATA(descriptor, pr_fmt(fmt));\t\t\\\n\tif (DYNAMIC_DEBUG_BRANCH(descriptor) &&\t\t\t\t\\\n\t    __ratelimit(&_rs))\t\t\t\t\t\t\\\n\t\t__dynamic_pr_debug(&descriptor, pr_fmt(fmt), ##__VA_ARGS__);\t\\\n} while (0)\n#elif defined(DEBUG)\n#define pr_debug_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tprintk_ratelimited(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#else\n#define pr_debug_ratelimited(fmt, ...) \\\n\tno_printk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#endif\n\nextern const struct file_operations kmsg_fops;\n\nenum {\n\tDUMP_PREFIX_NONE,\n\tDUMP_PREFIX_ADDRESS,\n\tDUMP_PREFIX_OFFSET\n};\nextern int hex_dump_to_buffer(const void *buf, size_t len, int rowsize,\n\t\t\t      int groupsize, char *linebuf, size_t linebuflen,\n\t\t\t      bool ascii);\n#ifdef CONFIG_PRINTK\nextern void print_hex_dump(const char *level, const char *prefix_str,\n\t\t\t   int prefix_type, int rowsize, int groupsize,\n\t\t\t   const void *buf, size_t len, bool ascii);\n#else\nstatic inline void print_hex_dump(const char *level, const char *prefix_str,\n\t\t\t\t  int prefix_type, int rowsize, int groupsize,\n\t\t\t\t  const void *buf, size_t len, bool ascii)\n{\n}\nstatic inline void print_hex_dump_bytes(const char *prefix_str, int prefix_type,\n\t\t\t\t\tconst void *buf, size_t len)\n{\n}\n\n#endif\n\n#if defined(CONFIG_DYNAMIC_DEBUG) || \\\n\t(defined(CONFIG_DYNAMIC_DEBUG_CORE) && defined(DYNAMIC_DEBUG_MODULE))\n#define print_hex_dump_debug(prefix_str, prefix_type, rowsize,\t\\\n\t\t\t     groupsize, buf, len, ascii)\t\\\n\tdynamic_hex_dump(prefix_str, prefix_type, rowsize,\t\\\n\t\t\t groupsize, buf, len, ascii)\n#elif defined(DEBUG)\n#define print_hex_dump_debug(prefix_str, prefix_type, rowsize,\t\t\\\n\t\t\t     groupsize, buf, len, ascii)\t\t\\\n\tprint_hex_dump(KERN_DEBUG, prefix_str, prefix_type, rowsize,\t\\\n\t\t       groupsize, buf, len, ascii)\n#else\nstatic inline void print_hex_dump_debug(const char *prefix_str, int prefix_type,\n\t\t\t\t\tint rowsize, int groupsize,\n\t\t\t\t\tconst void *buf, size_t len, bool ascii)\n{\n}\n#endif\n\n/**\n * print_hex_dump_bytes - shorthand form of print_hex_dump() with default params\n * @prefix_str: string to prefix each line with;\n *  caller supplies trailing spaces for alignment if desired\n * @prefix_type: controls whether prefix of an offset, address, or none\n *  is printed (%DUMP_PREFIX_OFFSET, %DUMP_PREFIX_ADDRESS, %DUMP_PREFIX_NONE)\n * @buf: data blob to dump\n * @len: number of bytes in the @buf\n *\n * Calls print_hex_dump(), with log level of KERN_DEBUG,\n * rowsize of 16, groupsize of 1, and ASCII output included.\n */\n#define print_hex_dump_bytes(prefix_str, prefix_type, buf, len)\t\\\n\tprint_hex_dump_debug(prefix_str, prefix_type, 16, 1, buf, len, true)\n\n#endif\n"}, "4": {"id": 4, "path": "/src/include/linux/compiler.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_COMPILER_H\n#define __LINUX_COMPILER_H\n\n#include <linux/compiler_types.h>\n\n#ifndef __ASSEMBLY__\n\n#ifdef __KERNEL__\n\n/*\n * Note: DISABLE_BRANCH_PROFILING can be used by special lowlevel code\n * to disable branch tracing on a per file basis.\n */\n#if defined(CONFIG_TRACE_BRANCH_PROFILING) \\\n    && !defined(DISABLE_BRANCH_PROFILING) && !defined(__CHECKER__)\nvoid ftrace_likely_update(struct ftrace_likely_data *f, int val,\n\t\t\t  int expect, int is_constant);\n\n#define likely_notrace(x)\t__builtin_expect(!!(x), 1)\n#define unlikely_notrace(x)\t__builtin_expect(!!(x), 0)\n\n#define __branch_check__(x, expect, is_constant) ({\t\t\t\\\n\t\t\tlong ______r;\t\t\t\t\t\\\n\t\t\tstatic struct ftrace_likely_data\t\t\\\n\t\t\t\t__aligned(4)\t\t\t\t\\\n\t\t\t\t__section(\"_ftrace_annotated_branch\")\t\\\n\t\t\t\t______f = {\t\t\t\t\\\n\t\t\t\t.data.func = __func__,\t\t\t\\\n\t\t\t\t.data.file = __FILE__,\t\t\t\\\n\t\t\t\t.data.line = __LINE__,\t\t\t\\\n\t\t\t};\t\t\t\t\t\t\\\n\t\t\t______r = __builtin_expect(!!(x), expect);\t\\\n\t\t\tftrace_likely_update(&______f, ______r,\t\t\\\n\t\t\t\t\t     expect, is_constant);\t\\\n\t\t\t______r;\t\t\t\t\t\\\n\t\t})\n\n/*\n * Using __builtin_constant_p(x) to ignore cases where the return\n * value is always the same.  This idea is taken from a similar patch\n * written by Daniel Walker.\n */\n# ifndef likely\n#  define likely(x)\t(__branch_check__(x, 1, __builtin_constant_p(x)))\n# endif\n# ifndef unlikely\n#  define unlikely(x)\t(__branch_check__(x, 0, __builtin_constant_p(x)))\n# endif\n\n#ifdef CONFIG_PROFILE_ALL_BRANCHES\n/*\n * \"Define 'is'\", Bill Clinton\n * \"Define 'if'\", Steven Rostedt\n */\n#define if(cond, ...) if ( __trace_if_var( !!(cond , ## __VA_ARGS__) ) )\n\n#define __trace_if_var(cond) (__builtin_constant_p(cond) ? (cond) : __trace_if_value(cond))\n\n#define __trace_if_value(cond) ({\t\t\t\\\n\tstatic struct ftrace_branch_data\t\t\\\n\t\t__aligned(4)\t\t\t\t\\\n\t\t__section(\"_ftrace_branch\")\t\t\\\n\t\t__if_trace = {\t\t\t\t\\\n\t\t\t.func = __func__,\t\t\\\n\t\t\t.file = __FILE__,\t\t\\\n\t\t\t.line = __LINE__,\t\t\\\n\t\t};\t\t\t\t\t\\\n\t(cond) ?\t\t\t\t\t\\\n\t\t(__if_trace.miss_hit[1]++,1) :\t\t\\\n\t\t(__if_trace.miss_hit[0]++,0);\t\t\\\n})\n\n#endif /* CONFIG_PROFILE_ALL_BRANCHES */\n\n#else\n# define likely(x)\t__builtin_expect(!!(x), 1)\n# define unlikely(x)\t__builtin_expect(!!(x), 0)\n# define likely_notrace(x)\tlikely(x)\n# define unlikely_notrace(x)\tunlikely(x)\n#endif\n\n/* Optimization barrier */\n#ifndef barrier\n/* The \"volatile\" is due to gcc bugs */\n# define barrier() __asm__ __volatile__(\"\": : :\"memory\")\n#endif\n\n#ifndef barrier_data\n/*\n * This version is i.e. to prevent dead stores elimination on @ptr\n * where gcc and llvm may behave differently when otherwise using\n * normal barrier(): while gcc behavior gets along with a normal\n * barrier(), llvm needs an explicit input variable to be assumed\n * clobbered. The issue is as follows: while the inline asm might\n * access any memory it wants, the compiler could have fit all of\n * @ptr into memory registers instead, and since @ptr never escaped\n * from that, it proved that the inline asm wasn't touching any of\n * it. This version works well with both compilers, i.e. we're telling\n * the compiler that the inline asm absolutely may see the contents\n * of @ptr. See also: https://llvm.org/bugs/show_bug.cgi?id=15495\n */\n# define barrier_data(ptr) __asm__ __volatile__(\"\": :\"r\"(ptr) :\"memory\")\n#endif\n\n/* workaround for GCC PR82365 if needed */\n#ifndef barrier_before_unreachable\n# define barrier_before_unreachable() do { } while (0)\n#endif\n\n/* Unreachable code */\n#ifdef CONFIG_STACK_VALIDATION\n/*\n * These macros help objtool understand GCC code flow for unreachable code.\n * The __COUNTER__ based labels are a hack to make each instance of the macros\n * unique, to convince GCC not to merge duplicate inline asm statements.\n */\n#define annotate_reachable() ({\t\t\t\t\t\t\\\n\tasm volatile(\"%c0:\\n\\t\"\t\t\t\t\t\t\\\n\t\t     \".pushsection .discard.reachable\\n\\t\"\t\t\\\n\t\t     \".long %c0b - .\\n\\t\"\t\t\t\t\\\n\t\t     \".popsection\\n\\t\" : : \"i\" (__COUNTER__));\t\t\\\n})\n#define annotate_unreachable() ({\t\t\t\t\t\\\n\tasm volatile(\"%c0:\\n\\t\"\t\t\t\t\t\t\\\n\t\t     \".pushsection .discard.unreachable\\n\\t\"\t\t\\\n\t\t     \".long %c0b - .\\n\\t\"\t\t\t\t\\\n\t\t     \".popsection\\n\\t\" : : \"i\" (__COUNTER__));\t\t\\\n})\n#define ASM_UNREACHABLE\t\t\t\t\t\t\t\\\n\t\"999:\\n\\t\"\t\t\t\t\t\t\t\\\n\t\".pushsection .discard.unreachable\\n\\t\"\t\t\t\t\\\n\t\".long 999b - .\\n\\t\"\t\t\t\t\t\t\\\n\t\".popsection\\n\\t\"\n\n/* Annotate a C jump table to allow objtool to follow the code flow */\n#define __annotate_jump_table __section(\".rodata..c_jump_table\")\n\n#else\n#define annotate_reachable()\n#define annotate_unreachable()\n#define __annotate_jump_table\n#endif\n\n#ifndef ASM_UNREACHABLE\n# define ASM_UNREACHABLE\n#endif\n#ifndef unreachable\n# define unreachable() do {\t\t\\\n\tannotate_unreachable();\t\t\\\n\t__builtin_unreachable();\t\\\n} while (0)\n#endif\n\n/*\n * KENTRY - kernel entry point\n * This can be used to annotate symbols (functions or data) that are used\n * without their linker symbol being referenced explicitly. For example,\n * interrupt vector handlers, or functions in the kernel image that are found\n * programatically.\n *\n * Not required for symbols exported with EXPORT_SYMBOL, or initcalls. Those\n * are handled in their own way (with KEEP() in linker scripts).\n *\n * KENTRY can be avoided if the symbols in question are marked as KEEP() in the\n * linker script. For example an architecture could KEEP() its entire\n * boot/exception vector code rather than annotate each function and data.\n */\n#ifndef KENTRY\n# define KENTRY(sym)\t\t\t\t\t\t\\\n\textern typeof(sym) sym;\t\t\t\t\t\\\n\tstatic const unsigned long __kentry_##sym\t\t\\\n\t__used\t\t\t\t\t\t\t\\\n\t__attribute__((__section__(\"___kentry+\" #sym)))\t\t\\\n\t= (unsigned long)&sym;\n#endif\n\n#ifndef RELOC_HIDE\n# define RELOC_HIDE(ptr, off)\t\t\t\t\t\\\n  ({ unsigned long __ptr;\t\t\t\t\t\\\n     __ptr = (unsigned long) (ptr);\t\t\t\t\\\n    (typeof(ptr)) (__ptr + (off)); })\n#endif\n\n#ifndef OPTIMIZER_HIDE_VAR\n/* Make the optimizer believe the variable can be manipulated arbitrarily. */\n#define OPTIMIZER_HIDE_VAR(var)\t\t\t\t\t\t\\\n\t__asm__ (\"\" : \"=r\" (var) : \"0\" (var))\n#endif\n\n/* Not-quite-unique ID. */\n#ifndef __UNIQUE_ID\n# define __UNIQUE_ID(prefix) __PASTE(__PASTE(__UNIQUE_ID_, prefix), __LINE__)\n#endif\n\n/**\n * data_race - mark an expression as containing intentional data races\n *\n * This data_race() macro is useful for situations in which data races\n * should be forgiven.  One example is diagnostic code that accesses\n * shared variables but is not a part of the core synchronization design.\n *\n * This macro *does not* affect normal code generation, but is a hint\n * to tooling that data races here are to be ignored.\n */\n#define data_race(expr)\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__unqual_scalar_typeof(({ expr; })) __v = ({\t\t\t\\\n\t\t__kcsan_disable_current();\t\t\t\t\\\n\t\texpr;\t\t\t\t\t\t\t\\\n\t});\t\t\t\t\t\t\t\t\\\n\t__kcsan_enable_current();\t\t\t\t\t\\\n\t__v;\t\t\t\t\t\t\t\t\\\n})\n\n#endif /* __KERNEL__ */\n\n/*\n * Force the compiler to emit 'sym' as a symbol, so that we can reference\n * it from inline assembler. Necessary in case 'sym' could be inlined\n * otherwise, or eliminated entirely due to lack of references that are\n * visible to the compiler.\n */\n#define __ADDRESSABLE(sym) \\\n\tstatic void * __section(\".discard.addressable\") __used \\\n\t\t__UNIQUE_ID(__PASTE(__addressable_,sym)) = (void *)&sym;\n\n/**\n * offset_to_ptr - convert a relative memory offset to an absolute pointer\n * @off:\tthe address of the 32-bit offset value\n */\nstatic inline void *offset_to_ptr(const int *off)\n{\n\treturn (void *)((unsigned long)off + *off);\n}\n\n#endif /* __ASSEMBLY__ */\n\n/* &a[0] degrades to a pointer: a different type from an array */\n#define __must_be_array(a)\tBUILD_BUG_ON_ZERO(__same_type((a), &(a)[0]))\n\n/*\n * This is needed in functions which generate the stack canary, see\n * arch/x86/kernel/smpboot.c::start_secondary() for an example.\n */\n#define prevent_tail_call_optimization()\tmb()\n\n#include <asm/rwonce.h>\n\n#endif /* __LINUX_COMPILER_H */\n"}, "5": {"id": 5, "path": "/src/include/asm-generic/bug.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_GENERIC_BUG_H\n#define _ASM_GENERIC_BUG_H\n\n#include <linux/compiler.h>\n#include <linux/instrumentation.h>\n\n#define CUT_HERE\t\t\"------------[ cut here ]------------\\n\"\n\n#ifdef CONFIG_GENERIC_BUG\n#define BUGFLAG_WARNING\t\t(1 << 0)\n#define BUGFLAG_ONCE\t\t(1 << 1)\n#define BUGFLAG_DONE\t\t(1 << 2)\n#define BUGFLAG_NO_CUT_HERE\t(1 << 3)\t/* CUT_HERE already sent */\n#define BUGFLAG_TAINT(taint)\t((taint) << 8)\n#define BUG_GET_TAINT(bug)\t((bug)->flags >> 8)\n#endif\n\n#ifndef __ASSEMBLY__\n#include <linux/kernel.h>\n\n#ifdef CONFIG_BUG\n\n#ifdef CONFIG_GENERIC_BUG\nstruct bug_entry {\n#ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS\n\tunsigned long\tbug_addr;\n#else\n\tsigned int\tbug_addr_disp;\n#endif\n#ifdef CONFIG_DEBUG_BUGVERBOSE\n#ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS\n\tconst char\t*file;\n#else\n\tsigned int\tfile_disp;\n#endif\n\tunsigned short\tline;\n#endif\n\tunsigned short\tflags;\n};\n#endif\t/* CONFIG_GENERIC_BUG */\n\n/*\n * Don't use BUG() or BUG_ON() unless there's really no way out; one\n * example might be detecting data structure corruption in the middle\n * of an operation that can't be backed out of.  If the (sub)system\n * can somehow continue operating, perhaps with reduced functionality,\n * it's probably not BUG-worthy.\n *\n * If you're tempted to BUG(), think again:  is completely giving up\n * really the *only* solution?  There are usually better options, where\n * users don't need to reboot ASAP and can mostly shut down cleanly.\n */\n#ifndef HAVE_ARCH_BUG\n#define BUG() do { \\\n\tprintk(\"BUG: failure at %s:%d/%s()!\\n\", __FILE__, __LINE__, __func__); \\\n\tbarrier_before_unreachable(); \\\n\tpanic(\"BUG!\"); \\\n} while (0)\n#endif\n\n#ifndef HAVE_ARCH_BUG_ON\n#define BUG_ON(condition) do { if (unlikely(condition)) BUG(); } while (0)\n#endif\n\n/*\n * WARN(), WARN_ON(), WARN_ON_ONCE, and so on can be used to report\n * significant kernel issues that need prompt attention if they should ever\n * appear at runtime.\n *\n * Do not use these macros when checking for invalid external inputs\n * (e.g. invalid system call arguments, or invalid data coming from\n * network/devices), and on transient conditions like ENOMEM or EAGAIN.\n * These macros should be used for recoverable kernel issues only.\n * For invalid external inputs, transient conditions, etc use\n * pr_err[_once/_ratelimited]() followed by dump_stack(), if necessary.\n * Do not include \"BUG\"/\"WARNING\" in format strings manually to make these\n * conditions distinguishable from kernel issues.\n *\n * Use the versions with printk format strings to provide better diagnostics.\n */\n#ifndef __WARN_FLAGS\nextern __printf(4, 5)\nvoid warn_slowpath_fmt(const char *file, const int line, unsigned taint,\n\t\t       const char *fmt, ...);\n#define __WARN()\t\t__WARN_printf(TAINT_WARN, NULL)\n#define __WARN_printf(taint, arg...) do {\t\t\t\t\\\n\t\tinstrumentation_begin();\t\t\t\t\\\n\t\twarn_slowpath_fmt(__FILE__, __LINE__, taint, arg);\t\\\n\t\tinstrumentation_end();\t\t\t\t\t\\\n\t} while (0)\n#else\nextern __printf(1, 2) void __warn_printk(const char *fmt, ...);\n#define __WARN()\t\t__WARN_FLAGS(BUGFLAG_TAINT(TAINT_WARN))\n#define __WARN_printf(taint, arg...) do {\t\t\t\t\\\n\t\tinstrumentation_begin();\t\t\t\t\\\n\t\t__warn_printk(arg);\t\t\t\t\t\\\n\t\t__WARN_FLAGS(BUGFLAG_NO_CUT_HERE | BUGFLAG_TAINT(taint));\\\n\t\tinstrumentation_end();\t\t\t\t\t\\\n\t} while (0)\n#define WARN_ON_ONCE(condition) ({\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\\\n\t\t__WARN_FLAGS(BUGFLAG_ONCE |\t\t\t\\\n\t\t\t     BUGFLAG_TAINT(TAINT_WARN));\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\\\n})\n#endif\n\n/* used internally by panic.c */\nstruct warn_args;\nstruct pt_regs;\n\nvoid __warn(const char *file, int line, void *caller, unsigned taint,\n\t    struct pt_regs *regs, struct warn_args *args);\n\n#ifndef WARN_ON\n#define WARN_ON(condition) ({\t\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\t\\\n\t\t__WARN();\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#ifndef WARN\n#define WARN(condition, format...) ({\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\t\\\n\t\t__WARN_printf(TAINT_WARN, format);\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#define WARN_TAINT(condition, taint, format...) ({\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\t\\\n\t\t__WARN_printf(taint, format);\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n\n#ifndef WARN_ON_ONCE\n#define WARN_ON_ONCE(condition)\t({\t\t\t\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\\\n\tint __ret_warn_once = !!(condition);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (unlikely(__ret_warn_once && !__warned)) {\t\t\\\n\t\t__warned = true;\t\t\t\t\\\n\t\tWARN_ON(1);\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\\\n})\n#endif\n\n#define WARN_ONCE(condition, format...)\t({\t\t\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\\\n\tint __ret_warn_once = !!(condition);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (unlikely(__ret_warn_once && !__warned)) {\t\t\\\n\t\t__warned = true;\t\t\t\t\\\n\t\tWARN(1, format);\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\\\n})\n\n#define WARN_TAINT_ONCE(condition, taint, format...)\t({\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\\\n\tint __ret_warn_once = !!(condition);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (unlikely(__ret_warn_once && !__warned)) {\t\t\\\n\t\t__warned = true;\t\t\t\t\\\n\t\tWARN_TAINT(1, taint, format);\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\\\n})\n\n#else /* !CONFIG_BUG */\n#ifndef HAVE_ARCH_BUG\n#define BUG() do {} while (1)\n#endif\n\n#ifndef HAVE_ARCH_BUG_ON\n#define BUG_ON(condition) do { if (unlikely(condition)) BUG(); } while (0)\n#endif\n\n#ifndef HAVE_ARCH_WARN_ON\n#define WARN_ON(condition) ({\t\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#ifndef WARN\n#define WARN(condition, format...) ({\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tno_printk(format);\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#define WARN_ON_ONCE(condition) WARN_ON(condition)\n#define WARN_ONCE(condition, format...) WARN(condition, format)\n#define WARN_TAINT(condition, taint, format...) WARN(condition, format)\n#define WARN_TAINT_ONCE(condition, taint, format...) WARN(condition, format)\n\n#endif\n\n/*\n * WARN_ON_SMP() is for cases that the warning is either\n * meaningless for !SMP or may even cause failures.\n * It can also be used with values that are only defined\n * on SMP:\n *\n * struct foo {\n *  [...]\n * #ifdef CONFIG_SMP\n *\tint bar;\n * #endif\n * };\n *\n * void func(struct foo *zoot)\n * {\n *\tWARN_ON_SMP(!zoot->bar);\n *\n * For CONFIG_SMP, WARN_ON_SMP() should act the same as WARN_ON(),\n * and should be a nop and return false for uniprocessor.\n *\n * if (WARN_ON_SMP(x)) returns true only when CONFIG_SMP is set\n * and x is true.\n */\n#ifdef CONFIG_SMP\n# define WARN_ON_SMP(x)\t\t\tWARN_ON(x)\n#else\n/*\n * Use of ({0;}) because WARN_ON_SMP(x) may be used either as\n * a stand alone line statement or as a condition in an if ()\n * statement.\n * A simple \"0\" would cause gcc to give a \"statement has no effect\"\n * warning.\n */\n# define WARN_ON_SMP(x)\t\t\t({0;})\n#endif\n\n/*\n * WARN_ON_FUNCTION_MISMATCH() warns if a value doesn't match a\n * function address, and can be useful for catching issues with\n * callback functions, for example.\n *\n * With CONFIG_CFI_CLANG, the warning is disabled because the\n * compiler replaces function addresses taken in C code with\n * local jump table addresses, which breaks cross-module function\n * address equality.\n */\n#if defined(CONFIG_CFI_CLANG) && defined(CONFIG_MODULES)\n# define WARN_ON_FUNCTION_MISMATCH(x, fn) ({ 0; })\n#else\n# define WARN_ON_FUNCTION_MISMATCH(x, fn) WARN_ON_ONCE((x) != (fn))\n#endif\n\n#endif /* __ASSEMBLY__ */\n\n#endif\n"}, "6": {"id": 6, "path": "/src/fs/ext4/ext4.h", "content": "// SPDX-License-Identifier: GPL-2.0\n/*\n *  ext4.h\n *\n * Copyright (C) 1992, 1993, 1994, 1995\n * Remy Card (card@masi.ibp.fr)\n * Laboratoire MASI - Institut Blaise Pascal\n * Universite Pierre et Marie Curie (Paris VI)\n *\n *  from\n *\n *  linux/include/linux/minix_fs.h\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n */\n\n#ifndef _EXT4_H\n#define _EXT4_H\n\n#include <linux/types.h>\n#include <linux/blkdev.h>\n#include <linux/magic.h>\n#include <linux/jbd2.h>\n#include <linux/quota.h>\n#include <linux/rwsem.h>\n#include <linux/rbtree.h>\n#include <linux/seqlock.h>\n#include <linux/mutex.h>\n#include <linux/timer.h>\n#include <linux/wait.h>\n#include <linux/sched/signal.h>\n#include <linux/blockgroup_lock.h>\n#include <linux/percpu_counter.h>\n#include <linux/ratelimit.h>\n#include <crypto/hash.h>\n#include <linux/falloc.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/fiemap.h>\n#ifdef __KERNEL__\n#include <linux/compat.h>\n#endif\n\n#include <linux/fscrypt.h>\n#include <linux/fsverity.h>\n\n#include <linux/compiler.h>\n\n/*\n * The fourth extended filesystem constants/structures\n */\n\n/*\n * with AGGRESSIVE_CHECK allocator runs consistency checks over\n * structures. these checks slow things down a lot\n */\n#define AGGRESSIVE_CHECK__\n\n/*\n * with DOUBLE_CHECK defined mballoc creates persistent in-core\n * bitmaps, maintains and uses them to check for double allocations\n */\n#define DOUBLE_CHECK__\n\n/*\n * Define EXT4FS_DEBUG to produce debug messages\n */\n#undef EXT4FS_DEBUG\n\n/*\n * Debug code\n */\n#ifdef EXT4FS_DEBUG\n#define ext4_debug(f, a...)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tprintk(KERN_DEBUG \"EXT4-fs DEBUG (%s, %d): %s:\",\t\\\n\t\t\t__FILE__, __LINE__, __func__);\t\t\t\\\n\t\tprintk(KERN_DEBUG f, ## a);\t\t\t\t\\\n\t} while (0)\n#else\n#define ext4_debug(fmt, ...)\tno_printk(fmt, ##__VA_ARGS__)\n#endif\n\n /*\n  * Turn on EXT_DEBUG to enable ext4_ext_show_path/leaf/move in extents.c\n  */\n#define EXT_DEBUG__\n\n/*\n * Dynamic printk for controlled extents debugging.\n */\n#ifdef CONFIG_EXT4_DEBUG\n#define ext_debug(ino, fmt, ...)\t\t\t\t\t\\\n\tpr_debug(\"[%s/%d] EXT4-fs (%s): ino %lu: (%s, %d): %s:\" fmt,\t\\\n\t\t current->comm, task_pid_nr(current),\t\t\t\\\n\t\t ino->i_sb->s_id, ino->i_ino, __FILE__, __LINE__,\t\\\n\t\t __func__, ##__VA_ARGS__)\n#else\n#define ext_debug(ino, fmt, ...)\tno_printk(fmt, ##__VA_ARGS__)\n#endif\n\n#define ASSERT(assert)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (unlikely(!(assert))) {\t\t\t\t\t\\\n\t\tprintk(KERN_EMERG\t\t\t\t\t\\\n\t\t       \"Assertion failure in %s() at %s:%d: '%s'\\n\",\t\\\n\t\t       __func__, __FILE__, __LINE__, #assert);\t\t\\\n\t\tBUG();\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n/* data type for block offset of block group */\ntypedef int ext4_grpblk_t;\n\n/* data type for filesystem-wide blocks number */\ntypedef unsigned long long ext4_fsblk_t;\n\n/* data type for file logical block number */\ntypedef __u32 ext4_lblk_t;\n\n/* data type for block group number */\ntypedef unsigned int ext4_group_t;\n\nenum SHIFT_DIRECTION {\n\tSHIFT_LEFT = 0,\n\tSHIFT_RIGHT,\n};\n\n/*\n * Flags used in mballoc's allocation_context flags field.\n *\n * Also used to show what's going on for debugging purposes when the\n * flag field is exported via the traceport interface\n */\n\n/* prefer goal again. length */\n#define EXT4_MB_HINT_MERGE\t\t0x0001\n/* blocks already reserved */\n#define EXT4_MB_HINT_RESERVED\t\t0x0002\n/* metadata is being allocated */\n#define EXT4_MB_HINT_METADATA\t\t0x0004\n/* first blocks in the file */\n#define EXT4_MB_HINT_FIRST\t\t0x0008\n/* search for the best chunk */\n#define EXT4_MB_HINT_BEST\t\t0x0010\n/* data is being allocated */\n#define EXT4_MB_HINT_DATA\t\t0x0020\n/* don't preallocate (for tails) */\n#define EXT4_MB_HINT_NOPREALLOC\t\t0x0040\n/* allocate for locality group */\n#define EXT4_MB_HINT_GROUP_ALLOC\t0x0080\n/* allocate goal blocks or none */\n#define EXT4_MB_HINT_GOAL_ONLY\t\t0x0100\n/* goal is meaningful */\n#define EXT4_MB_HINT_TRY_GOAL\t\t0x0200\n/* blocks already pre-reserved by delayed allocation */\n#define EXT4_MB_DELALLOC_RESERVED\t0x0400\n/* We are doing stream allocation */\n#define EXT4_MB_STREAM_ALLOC\t\t0x0800\n/* Use reserved root blocks if needed */\n#define EXT4_MB_USE_ROOT_BLOCKS\t\t0x1000\n/* Use blocks from reserved pool */\n#define EXT4_MB_USE_RESERVED\t\t0x2000\n/* Do strict check for free blocks while retrying block allocation */\n#define EXT4_MB_STRICT_CHECK\t\t0x4000\n\nstruct ext4_allocation_request {\n\t/* target inode for block we're allocating */\n\tstruct inode *inode;\n\t/* how many blocks we want to allocate */\n\tunsigned int len;\n\t/* logical block in target inode */\n\text4_lblk_t logical;\n\t/* the closest logical allocated block to the left */\n\text4_lblk_t lleft;\n\t/* the closest logical allocated block to the right */\n\text4_lblk_t lright;\n\t/* phys. target (a hint) */\n\text4_fsblk_t goal;\n\t/* phys. block for the closest logical allocated block to the left */\n\text4_fsblk_t pleft;\n\t/* phys. block for the closest logical allocated block to the right */\n\text4_fsblk_t pright;\n\t/* flags. see above EXT4_MB_HINT_* */\n\tunsigned int flags;\n};\n\n/*\n * Logical to physical block mapping, used by ext4_map_blocks()\n *\n * This structure is used to pass requests into ext4_map_blocks() as\n * well as to store the information returned by ext4_map_blocks().  It\n * takes less room on the stack than a struct buffer_head.\n */\n#define EXT4_MAP_NEW\t\tBIT(BH_New)\n#define EXT4_MAP_MAPPED\t\tBIT(BH_Mapped)\n#define EXT4_MAP_UNWRITTEN\tBIT(BH_Unwritten)\n#define EXT4_MAP_BOUNDARY\tBIT(BH_Boundary)\n#define EXT4_MAP_FLAGS\t\t(EXT4_MAP_NEW | EXT4_MAP_MAPPED |\\\n\t\t\t\t EXT4_MAP_UNWRITTEN | EXT4_MAP_BOUNDARY)\n\nstruct ext4_map_blocks {\n\text4_fsblk_t m_pblk;\n\text4_lblk_t m_lblk;\n\tunsigned int m_len;\n\tunsigned int m_flags;\n};\n\n/*\n * Block validity checking, system zone rbtree.\n */\nstruct ext4_system_blocks {\n\tstruct rb_root root;\n\tstruct rcu_head rcu;\n};\n\n/*\n * Flags for ext4_io_end->flags\n */\n#define\tEXT4_IO_END_UNWRITTEN\t0x0001\n\nstruct ext4_io_end_vec {\n\tstruct list_head list;\t\t/* list of io_end_vec */\n\tloff_t offset;\t\t\t/* offset in the file */\n\tssize_t size;\t\t\t/* size of the extent */\n};\n\n/*\n * For converting unwritten extents on a work queue. 'handle' is used for\n * buffered writeback.\n */\ntypedef struct ext4_io_end {\n\tstruct list_head\tlist;\t\t/* per-file finished IO list */\n\thandle_t\t\t*handle;\t/* handle reserved for extent\n\t\t\t\t\t\t * conversion */\n\tstruct inode\t\t*inode;\t\t/* file being written to */\n\tstruct bio\t\t*bio;\t\t/* Linked list of completed\n\t\t\t\t\t\t * bios covering the extent */\n\tunsigned int\t\tflag;\t\t/* unwritten or not */\n\tatomic_t\t\tcount;\t\t/* reference counter */\n\tstruct list_head\tlist_vec;\t/* list of ext4_io_end_vec */\n} ext4_io_end_t;\n\nstruct ext4_io_submit {\n\tstruct writeback_control *io_wbc;\n\tstruct bio\t\t*io_bio;\n\text4_io_end_t\t\t*io_end;\n\tsector_t\t\tio_next_block;\n};\n\n/*\n * Special inodes numbers\n */\n#define\tEXT4_BAD_INO\t\t 1\t/* Bad blocks inode */\n#define EXT4_ROOT_INO\t\t 2\t/* Root inode */\n#define EXT4_USR_QUOTA_INO\t 3\t/* User quota inode */\n#define EXT4_GRP_QUOTA_INO\t 4\t/* Group quota inode */\n#define EXT4_BOOT_LOADER_INO\t 5\t/* Boot loader inode */\n#define EXT4_UNDEL_DIR_INO\t 6\t/* Undelete directory inode */\n#define EXT4_RESIZE_INO\t\t 7\t/* Reserved group descriptors inode */\n#define EXT4_JOURNAL_INO\t 8\t/* Journal inode */\n\n/* First non-reserved inode for old ext4 filesystems */\n#define EXT4_GOOD_OLD_FIRST_INO\t11\n\n/*\n * Maximal count of links to a file\n */\n#define EXT4_LINK_MAX\t\t65000\n\n/*\n * Macro-instructions used to manage several block sizes\n */\n#define EXT4_MIN_BLOCK_SIZE\t\t1024\n#define\tEXT4_MAX_BLOCK_SIZE\t\t65536\n#define EXT4_MIN_BLOCK_LOG_SIZE\t\t10\n#define EXT4_MAX_BLOCK_LOG_SIZE\t\t16\n#define EXT4_MAX_CLUSTER_LOG_SIZE\t30\n#ifdef __KERNEL__\n# define EXT4_BLOCK_SIZE(s)\t\t((s)->s_blocksize)\n#else\n# define EXT4_BLOCK_SIZE(s)\t\t(EXT4_MIN_BLOCK_SIZE << (s)->s_log_block_size)\n#endif\n#define\tEXT4_ADDR_PER_BLOCK(s)\t\t(EXT4_BLOCK_SIZE(s) / sizeof(__u32))\n#define EXT4_CLUSTER_SIZE(s)\t\t(EXT4_BLOCK_SIZE(s) << \\\n\t\t\t\t\t EXT4_SB(s)->s_cluster_bits)\n#ifdef __KERNEL__\n# define EXT4_BLOCK_SIZE_BITS(s)\t((s)->s_blocksize_bits)\n# define EXT4_CLUSTER_BITS(s)\t\t(EXT4_SB(s)->s_cluster_bits)\n#else\n# define EXT4_BLOCK_SIZE_BITS(s)\t((s)->s_log_block_size + 10)\n#endif\n#ifdef __KERNEL__\n#define\tEXT4_ADDR_PER_BLOCK_BITS(s)\t(EXT4_SB(s)->s_addr_per_block_bits)\n#define EXT4_INODE_SIZE(s)\t\t(EXT4_SB(s)->s_inode_size)\n#define EXT4_FIRST_INO(s)\t\t(EXT4_SB(s)->s_first_ino)\n#else\n#define EXT4_INODE_SIZE(s)\t(((s)->s_rev_level == EXT4_GOOD_OLD_REV) ? \\\n\t\t\t\t EXT4_GOOD_OLD_INODE_SIZE : \\\n\t\t\t\t (s)->s_inode_size)\n#define EXT4_FIRST_INO(s)\t(((s)->s_rev_level == EXT4_GOOD_OLD_REV) ? \\\n\t\t\t\t EXT4_GOOD_OLD_FIRST_INO : \\\n\t\t\t\t (s)->s_first_ino)\n#endif\n#define EXT4_BLOCK_ALIGN(size, blkbits)\t\tALIGN((size), (1 << (blkbits)))\n#define EXT4_MAX_BLOCKS(size, offset, blkbits) \\\n\t((EXT4_BLOCK_ALIGN(size + offset, blkbits) >> blkbits) - (offset >> \\\n\t\t\t\t\t\t\t\t  blkbits))\n\n/* Translate a block number to a cluster number */\n#define EXT4_B2C(sbi, blk)\t((blk) >> (sbi)->s_cluster_bits)\n/* Translate a cluster number to a block number */\n#define EXT4_C2B(sbi, cluster)\t((cluster) << (sbi)->s_cluster_bits)\n/* Translate # of blks to # of clusters */\n#define EXT4_NUM_B2C(sbi, blks)\t(((blks) + (sbi)->s_cluster_ratio - 1) >> \\\n\t\t\t\t (sbi)->s_cluster_bits)\n/* Mask out the low bits to get the starting block of the cluster */\n#define EXT4_PBLK_CMASK(s, pblk) ((pblk) &\t\t\t\t\\\n\t\t\t\t  ~((ext4_fsblk_t) (s)->s_cluster_ratio - 1))\n#define EXT4_LBLK_CMASK(s, lblk) ((lblk) &\t\t\t\t\\\n\t\t\t\t  ~((ext4_lblk_t) (s)->s_cluster_ratio - 1))\n/* Fill in the low bits to get the last block of the cluster */\n#define EXT4_LBLK_CFILL(sbi, lblk) ((lblk) |\t\t\t\t\\\n\t\t\t\t    ((ext4_lblk_t) (sbi)->s_cluster_ratio - 1))\n/* Get the cluster offset */\n#define EXT4_PBLK_COFF(s, pblk) ((pblk) &\t\t\t\t\\\n\t\t\t\t ((ext4_fsblk_t) (s)->s_cluster_ratio - 1))\n#define EXT4_LBLK_COFF(s, lblk) ((lblk) &\t\t\t\t\\\n\t\t\t\t ((ext4_lblk_t) (s)->s_cluster_ratio - 1))\n\n/*\n * Structure of a blocks group descriptor\n */\nstruct ext4_group_desc\n{\n\t__le32\tbg_block_bitmap_lo;\t/* Blocks bitmap block */\n\t__le32\tbg_inode_bitmap_lo;\t/* Inodes bitmap block */\n\t__le32\tbg_inode_table_lo;\t/* Inodes table block */\n\t__le16\tbg_free_blocks_count_lo;/* Free blocks count */\n\t__le16\tbg_free_inodes_count_lo;/* Free inodes count */\n\t__le16\tbg_used_dirs_count_lo;\t/* Directories count */\n\t__le16\tbg_flags;\t\t/* EXT4_BG_flags (INODE_UNINIT, etc) */\n\t__le32  bg_exclude_bitmap_lo;   /* Exclude bitmap for snapshots */\n\t__le16  bg_block_bitmap_csum_lo;/* crc32c(s_uuid+grp_num+bbitmap) LE */\n\t__le16  bg_inode_bitmap_csum_lo;/* crc32c(s_uuid+grp_num+ibitmap) LE */\n\t__le16  bg_itable_unused_lo;\t/* Unused inodes count */\n\t__le16  bg_checksum;\t\t/* crc16(sb_uuid+group+desc) */\n\t__le32\tbg_block_bitmap_hi;\t/* Blocks bitmap block MSB */\n\t__le32\tbg_inode_bitmap_hi;\t/* Inodes bitmap block MSB */\n\t__le32\tbg_inode_table_hi;\t/* Inodes table block MSB */\n\t__le16\tbg_free_blocks_count_hi;/* Free blocks count MSB */\n\t__le16\tbg_free_inodes_count_hi;/* Free inodes count MSB */\n\t__le16\tbg_used_dirs_count_hi;\t/* Directories count MSB */\n\t__le16  bg_itable_unused_hi;    /* Unused inodes count MSB */\n\t__le32  bg_exclude_bitmap_hi;   /* Exclude bitmap block MSB */\n\t__le16  bg_block_bitmap_csum_hi;/* crc32c(s_uuid+grp_num+bbitmap) BE */\n\t__le16  bg_inode_bitmap_csum_hi;/* crc32c(s_uuid+grp_num+ibitmap) BE */\n\t__u32   bg_reserved;\n};\n\n#define EXT4_BG_INODE_BITMAP_CSUM_HI_END\t\\\n\t(offsetof(struct ext4_group_desc, bg_inode_bitmap_csum_hi) + \\\n\t sizeof(__le16))\n#define EXT4_BG_BLOCK_BITMAP_CSUM_HI_END\t\\\n\t(offsetof(struct ext4_group_desc, bg_block_bitmap_csum_hi) + \\\n\t sizeof(__le16))\n\n/*\n * Structure of a flex block group info\n */\n\nstruct flex_groups {\n\tatomic64_t\tfree_clusters;\n\tatomic_t\tfree_inodes;\n\tatomic_t\tused_dirs;\n};\n\n#define EXT4_BG_INODE_UNINIT\t0x0001 /* Inode table/bitmap not in use */\n#define EXT4_BG_BLOCK_UNINIT\t0x0002 /* Block bitmap not in use */\n#define EXT4_BG_INODE_ZEROED\t0x0004 /* On-disk itable initialized to zero */\n\n/*\n * Macro-instructions used to manage group descriptors\n */\n#define EXT4_MIN_DESC_SIZE\t\t32\n#define EXT4_MIN_DESC_SIZE_64BIT\t64\n#define\tEXT4_MAX_DESC_SIZE\t\tEXT4_MIN_BLOCK_SIZE\n#define EXT4_DESC_SIZE(s)\t\t(EXT4_SB(s)->s_desc_size)\n#ifdef __KERNEL__\n# define EXT4_BLOCKS_PER_GROUP(s)\t(EXT4_SB(s)->s_blocks_per_group)\n# define EXT4_CLUSTERS_PER_GROUP(s)\t(EXT4_SB(s)->s_clusters_per_group)\n# define EXT4_DESC_PER_BLOCK(s)\t\t(EXT4_SB(s)->s_desc_per_block)\n# define EXT4_INODES_PER_GROUP(s)\t(EXT4_SB(s)->s_inodes_per_group)\n# define EXT4_DESC_PER_BLOCK_BITS(s)\t(EXT4_SB(s)->s_desc_per_block_bits)\n#else\n# define EXT4_BLOCKS_PER_GROUP(s)\t((s)->s_blocks_per_group)\n# define EXT4_DESC_PER_BLOCK(s)\t\t(EXT4_BLOCK_SIZE(s) / EXT4_DESC_SIZE(s))\n# define EXT4_INODES_PER_GROUP(s)\t((s)->s_inodes_per_group)\n#endif\n\n/*\n * Constants relative to the data blocks\n */\n#define\tEXT4_NDIR_BLOCKS\t\t12\n#define\tEXT4_IND_BLOCK\t\t\tEXT4_NDIR_BLOCKS\n#define\tEXT4_DIND_BLOCK\t\t\t(EXT4_IND_BLOCK + 1)\n#define\tEXT4_TIND_BLOCK\t\t\t(EXT4_DIND_BLOCK + 1)\n#define\tEXT4_N_BLOCKS\t\t\t(EXT4_TIND_BLOCK + 1)\n\n/*\n * Inode flags\n */\n#define\tEXT4_SECRM_FL\t\t\t0x00000001 /* Secure deletion */\n#define\tEXT4_UNRM_FL\t\t\t0x00000002 /* Undelete */\n#define\tEXT4_COMPR_FL\t\t\t0x00000004 /* Compress file */\n#define EXT4_SYNC_FL\t\t\t0x00000008 /* Synchronous updates */\n#define EXT4_IMMUTABLE_FL\t\t0x00000010 /* Immutable file */\n#define EXT4_APPEND_FL\t\t\t0x00000020 /* writes to file may only append */\n#define EXT4_NODUMP_FL\t\t\t0x00000040 /* do not dump file */\n#define EXT4_NOATIME_FL\t\t\t0x00000080 /* do not update atime */\n/* Reserved for compression usage... */\n#define EXT4_DIRTY_FL\t\t\t0x00000100\n#define EXT4_COMPRBLK_FL\t\t0x00000200 /* One or more compressed clusters */\n#define EXT4_NOCOMPR_FL\t\t\t0x00000400 /* Don't compress */\n\t/* nb: was previously EXT2_ECOMPR_FL */\n#define EXT4_ENCRYPT_FL\t\t\t0x00000800 /* encrypted file */\n/* End compression flags --- maybe not all used */\n#define EXT4_INDEX_FL\t\t\t0x00001000 /* hash-indexed directory */\n#define EXT4_IMAGIC_FL\t\t\t0x00002000 /* AFS directory */\n#define EXT4_JOURNAL_DATA_FL\t\t0x00004000 /* file data should be journaled */\n#define EXT4_NOTAIL_FL\t\t\t0x00008000 /* file tail should not be merged */\n#define EXT4_DIRSYNC_FL\t\t\t0x00010000 /* dirsync behaviour (directories only) */\n#define EXT4_TOPDIR_FL\t\t\t0x00020000 /* Top of directory hierarchies*/\n#define EXT4_HUGE_FILE_FL               0x00040000 /* Set to each huge file */\n#define EXT4_EXTENTS_FL\t\t\t0x00080000 /* Inode uses extents */\n#define EXT4_VERITY_FL\t\t\t0x00100000 /* Verity protected inode */\n#define EXT4_EA_INODE_FL\t        0x00200000 /* Inode used for large EA */\n/* 0x00400000 was formerly EXT4_EOFBLOCKS_FL */\n\n#define EXT4_DAX_FL\t\t\t0x02000000 /* Inode is DAX */\n\n#define EXT4_INLINE_DATA_FL\t\t0x10000000 /* Inode has inline data. */\n#define EXT4_PROJINHERIT_FL\t\t0x20000000 /* Create with parents projid */\n#define EXT4_CASEFOLD_FL\t\t0x40000000 /* Casefolded directory */\n#define EXT4_RESERVED_FL\t\t0x80000000 /* reserved for ext4 lib */\n\n/* User modifiable flags */\n#define EXT4_FL_USER_MODIFIABLE\t\t(EXT4_SECRM_FL | \\\n\t\t\t\t\t EXT4_UNRM_FL | \\\n\t\t\t\t\t EXT4_COMPR_FL | \\\n\t\t\t\t\t EXT4_SYNC_FL | \\\n\t\t\t\t\t EXT4_IMMUTABLE_FL | \\\n\t\t\t\t\t EXT4_APPEND_FL | \\\n\t\t\t\t\t EXT4_NODUMP_FL | \\\n\t\t\t\t\t EXT4_NOATIME_FL | \\\n\t\t\t\t\t EXT4_JOURNAL_DATA_FL | \\\n\t\t\t\t\t EXT4_NOTAIL_FL | \\\n\t\t\t\t\t EXT4_DIRSYNC_FL | \\\n\t\t\t\t\t EXT4_TOPDIR_FL | \\\n\t\t\t\t\t EXT4_EXTENTS_FL | \\\n\t\t\t\t\t 0x00400000 /* EXT4_EOFBLOCKS_FL */ | \\\n\t\t\t\t\t EXT4_DAX_FL | \\\n\t\t\t\t\t EXT4_PROJINHERIT_FL | \\\n\t\t\t\t\t EXT4_CASEFOLD_FL)\n\n/* User visible flags */\n#define EXT4_FL_USER_VISIBLE\t\t(EXT4_FL_USER_MODIFIABLE | \\\n\t\t\t\t\t EXT4_DIRTY_FL | \\\n\t\t\t\t\t EXT4_COMPRBLK_FL | \\\n\t\t\t\t\t EXT4_NOCOMPR_FL | \\\n\t\t\t\t\t EXT4_ENCRYPT_FL | \\\n\t\t\t\t\t EXT4_INDEX_FL | \\\n\t\t\t\t\t EXT4_VERITY_FL | \\\n\t\t\t\t\t EXT4_INLINE_DATA_FL)\n\n/* Flags we can manipulate with through FS_IOC_FSSETXATTR */\n#define EXT4_FL_XFLAG_VISIBLE\t\t(EXT4_SYNC_FL | \\\n\t\t\t\t\t EXT4_IMMUTABLE_FL | \\\n\t\t\t\t\t EXT4_APPEND_FL | \\\n\t\t\t\t\t EXT4_NODUMP_FL | \\\n\t\t\t\t\t EXT4_NOATIME_FL | \\\n\t\t\t\t\t EXT4_PROJINHERIT_FL | \\\n\t\t\t\t\t EXT4_DAX_FL)\n\n/* Flags that should be inherited by new inodes from their parent. */\n#define EXT4_FL_INHERITED (EXT4_SECRM_FL | EXT4_UNRM_FL | EXT4_COMPR_FL |\\\n\t\t\t   EXT4_SYNC_FL | EXT4_NODUMP_FL | EXT4_NOATIME_FL |\\\n\t\t\t   EXT4_NOCOMPR_FL | EXT4_JOURNAL_DATA_FL |\\\n\t\t\t   EXT4_NOTAIL_FL | EXT4_DIRSYNC_FL |\\\n\t\t\t   EXT4_PROJINHERIT_FL | EXT4_CASEFOLD_FL |\\\n\t\t\t   EXT4_DAX_FL)\n\n/* Flags that are appropriate for regular files (all but dir-specific ones). */\n#define EXT4_REG_FLMASK (~(EXT4_DIRSYNC_FL | EXT4_TOPDIR_FL | EXT4_CASEFOLD_FL |\\\n\t\t\t   EXT4_PROJINHERIT_FL))\n\n/* Flags that are appropriate for non-directories/regular files. */\n#define EXT4_OTHER_FLMASK (EXT4_NODUMP_FL | EXT4_NOATIME_FL)\n\n/* The only flags that should be swapped */\n#define EXT4_FL_SHOULD_SWAP (EXT4_HUGE_FILE_FL | EXT4_EXTENTS_FL)\n\n/* Flags which are mutually exclusive to DAX */\n#define EXT4_DAX_MUT_EXCL (EXT4_VERITY_FL | EXT4_ENCRYPT_FL |\\\n\t\t\t   EXT4_JOURNAL_DATA_FL | EXT4_INLINE_DATA_FL)\n\n/* Mask out flags that are inappropriate for the given type of inode. */\nstatic inline __u32 ext4_mask_flags(umode_t mode, __u32 flags)\n{\n\tif (S_ISDIR(mode))\n\t\treturn flags;\n\telse if (S_ISREG(mode))\n\t\treturn flags & EXT4_REG_FLMASK;\n\telse\n\t\treturn flags & EXT4_OTHER_FLMASK;\n}\n\n/*\n * Inode flags used for atomic set/get\n */\nenum {\n\tEXT4_INODE_SECRM\t= 0,\t/* Secure deletion */\n\tEXT4_INODE_UNRM\t\t= 1,\t/* Undelete */\n\tEXT4_INODE_COMPR\t= 2,\t/* Compress file */\n\tEXT4_INODE_SYNC\t\t= 3,\t/* Synchronous updates */\n\tEXT4_INODE_IMMUTABLE\t= 4,\t/* Immutable file */\n\tEXT4_INODE_APPEND\t= 5,\t/* writes to file may only append */\n\tEXT4_INODE_NODUMP\t= 6,\t/* do not dump file */\n\tEXT4_INODE_NOATIME\t= 7,\t/* do not update atime */\n/* Reserved for compression usage... */\n\tEXT4_INODE_DIRTY\t= 8,\n\tEXT4_INODE_COMPRBLK\t= 9,\t/* One or more compressed clusters */\n\tEXT4_INODE_NOCOMPR\t= 10,\t/* Don't compress */\n\tEXT4_INODE_ENCRYPT\t= 11,\t/* Encrypted file */\n/* End compression flags --- maybe not all used */\n\tEXT4_INODE_INDEX\t= 12,\t/* hash-indexed directory */\n\tEXT4_INODE_IMAGIC\t= 13,\t/* AFS directory */\n\tEXT4_INODE_JOURNAL_DATA\t= 14,\t/* file data should be journaled */\n\tEXT4_INODE_NOTAIL\t= 15,\t/* file tail should not be merged */\n\tEXT4_INODE_DIRSYNC\t= 16,\t/* dirsync behaviour (directories only) */\n\tEXT4_INODE_TOPDIR\t= 17,\t/* Top of directory hierarchies*/\n\tEXT4_INODE_HUGE_FILE\t= 18,\t/* Set to each huge file */\n\tEXT4_INODE_EXTENTS\t= 19,\t/* Inode uses extents */\n\tEXT4_INODE_VERITY\t= 20,\t/* Verity protected inode */\n\tEXT4_INODE_EA_INODE\t= 21,\t/* Inode used for large EA */\n/* 22 was formerly EXT4_INODE_EOFBLOCKS */\n\tEXT4_INODE_DAX\t\t= 25,\t/* Inode is DAX */\n\tEXT4_INODE_INLINE_DATA\t= 28,\t/* Data in inode. */\n\tEXT4_INODE_PROJINHERIT\t= 29,\t/* Create with parents projid */\n\tEXT4_INODE_CASEFOLD\t= 30,\t/* Casefolded directory */\n\tEXT4_INODE_RESERVED\t= 31,\t/* reserved for ext4 lib */\n};\n\n/*\n * Since it's pretty easy to mix up bit numbers and hex values, we use a\n * build-time check to make sure that EXT4_XXX_FL is consistent with respect to\n * EXT4_INODE_XXX. If all is well, the macros will be dropped, so, it won't cost\n * any extra space in the compiled kernel image, otherwise, the build will fail.\n * It's important that these values are the same, since we are using\n * EXT4_INODE_XXX to test for flag values, but EXT4_XXX_FL must be consistent\n * with the values of FS_XXX_FL defined in include/linux/fs.h and the on-disk\n * values found in ext2, ext3 and ext4 filesystems, and of course the values\n * defined in e2fsprogs.\n *\n * It's not paranoia if the Murphy's Law really *is* out to get you.  :-)\n */\n#define TEST_FLAG_VALUE(FLAG) (EXT4_##FLAG##_FL == (1 << EXT4_INODE_##FLAG))\n#define CHECK_FLAG_VALUE(FLAG) BUILD_BUG_ON(!TEST_FLAG_VALUE(FLAG))\n\nstatic inline void ext4_check_flag_values(void)\n{\n\tCHECK_FLAG_VALUE(SECRM);\n\tCHECK_FLAG_VALUE(UNRM);\n\tCHECK_FLAG_VALUE(COMPR);\n\tCHECK_FLAG_VALUE(SYNC);\n\tCHECK_FLAG_VALUE(IMMUTABLE);\n\tCHECK_FLAG_VALUE(APPEND);\n\tCHECK_FLAG_VALUE(NODUMP);\n\tCHECK_FLAG_VALUE(NOATIME);\n\tCHECK_FLAG_VALUE(DIRTY);\n\tCHECK_FLAG_VALUE(COMPRBLK);\n\tCHECK_FLAG_VALUE(NOCOMPR);\n\tCHECK_FLAG_VALUE(ENCRYPT);\n\tCHECK_FLAG_VALUE(INDEX);\n\tCHECK_FLAG_VALUE(IMAGIC);\n\tCHECK_FLAG_VALUE(JOURNAL_DATA);\n\tCHECK_FLAG_VALUE(NOTAIL);\n\tCHECK_FLAG_VALUE(DIRSYNC);\n\tCHECK_FLAG_VALUE(TOPDIR);\n\tCHECK_FLAG_VALUE(HUGE_FILE);\n\tCHECK_FLAG_VALUE(EXTENTS);\n\tCHECK_FLAG_VALUE(VERITY);\n\tCHECK_FLAG_VALUE(EA_INODE);\n\tCHECK_FLAG_VALUE(INLINE_DATA);\n\tCHECK_FLAG_VALUE(PROJINHERIT);\n\tCHECK_FLAG_VALUE(CASEFOLD);\n\tCHECK_FLAG_VALUE(RESERVED);\n}\n\n/* Used to pass group descriptor data when online resize is done */\nstruct ext4_new_group_input {\n\t__u32 group;\t\t/* Group number for this data */\n\t__u64 block_bitmap;\t/* Absolute block number of block bitmap */\n\t__u64 inode_bitmap;\t/* Absolute block number of inode bitmap */\n\t__u64 inode_table;\t/* Absolute block number of inode table start */\n\t__u32 blocks_count;\t/* Total number of blocks in this group */\n\t__u16 reserved_blocks;\t/* Number of reserved blocks in this group */\n\t__u16 unused;\n};\n\n#if defined(__KERNEL__) && defined(CONFIG_COMPAT)\nstruct compat_ext4_new_group_input {\n\tu32 group;\n\tcompat_u64 block_bitmap;\n\tcompat_u64 inode_bitmap;\n\tcompat_u64 inode_table;\n\tu32 blocks_count;\n\tu16 reserved_blocks;\n\tu16 unused;\n};\n#endif\n\n/* The struct ext4_new_group_input in kernel space, with free_blocks_count */\nstruct ext4_new_group_data {\n\t__u32 group;\n\t__u64 block_bitmap;\n\t__u64 inode_bitmap;\n\t__u64 inode_table;\n\t__u32 blocks_count;\n\t__u16 reserved_blocks;\n\t__u16 mdata_blocks;\n\t__u32 free_clusters_count;\n};\n\n/* Indexes used to index group tables in ext4_new_group_data */\nenum {\n\tBLOCK_BITMAP = 0,\t/* block bitmap */\n\tINODE_BITMAP,\t\t/* inode bitmap */\n\tINODE_TABLE,\t\t/* inode tables */\n\tGROUP_TABLE_COUNT,\n};\n\n/*\n * Flags used by ext4_map_blocks()\n */\n\t/* Allocate any needed blocks and/or convert an unwritten\n\t   extent to be an initialized ext4 */\n#define EXT4_GET_BLOCKS_CREATE\t\t\t0x0001\n\t/* Request the creation of an unwritten extent */\n#define EXT4_GET_BLOCKS_UNWRIT_EXT\t\t0x0002\n#define EXT4_GET_BLOCKS_CREATE_UNWRIT_EXT\t(EXT4_GET_BLOCKS_UNWRIT_EXT|\\\n\t\t\t\t\t\t EXT4_GET_BLOCKS_CREATE)\n\t/* Caller is from the delayed allocation writeout path\n\t * finally doing the actual allocation of delayed blocks */\n#define EXT4_GET_BLOCKS_DELALLOC_RESERVE\t0x0004\n\t/* caller is from the direct IO path, request to creation of an\n\tunwritten extents if not allocated, split the unwritten\n\textent if blocks has been preallocated already*/\n#define EXT4_GET_BLOCKS_PRE_IO\t\t\t0x0008\n#define EXT4_GET_BLOCKS_CONVERT\t\t\t0x0010\n#define EXT4_GET_BLOCKS_IO_CREATE_EXT\t\t(EXT4_GET_BLOCKS_PRE_IO|\\\n\t\t\t\t\t EXT4_GET_BLOCKS_CREATE_UNWRIT_EXT)\n\t/* Convert extent to initialized after IO complete */\n#define EXT4_GET_BLOCKS_IO_CONVERT_EXT\t\t(EXT4_GET_BLOCKS_CONVERT|\\\n\t\t\t\t\t EXT4_GET_BLOCKS_CREATE_UNWRIT_EXT)\n\t/* Eventual metadata allocation (due to growing extent tree)\n\t * should not fail, so try to use reserved blocks for that.*/\n#define EXT4_GET_BLOCKS_METADATA_NOFAIL\t\t0x0020\n\t/* Don't normalize allocation size (used for fallocate) */\n#define EXT4_GET_BLOCKS_NO_NORMALIZE\t\t0x0040\n\t/* Convert written extents to unwritten */\n#define EXT4_GET_BLOCKS_CONVERT_UNWRITTEN\t0x0100\n\t/* Write zeros to newly created written extents */\n#define EXT4_GET_BLOCKS_ZERO\t\t\t0x0200\n#define EXT4_GET_BLOCKS_CREATE_ZERO\t\t(EXT4_GET_BLOCKS_CREATE |\\\n\t\t\t\t\tEXT4_GET_BLOCKS_ZERO)\n\t/* Caller will submit data before dropping transaction handle. This\n\t * allows jbd2 to avoid submitting data before commit. */\n#define EXT4_GET_BLOCKS_IO_SUBMIT\t\t0x0400\n\n/*\n * The bit position of these flags must not overlap with any of the\n * EXT4_GET_BLOCKS_*.  They are used by ext4_find_extent(),\n * read_extent_tree_block(), ext4_split_extent_at(),\n * ext4_ext_insert_extent(), and ext4_ext_create_new_leaf().\n * EXT4_EX_NOCACHE is used to indicate that the we shouldn't be\n * caching the extents when reading from the extent tree while a\n * truncate or punch hole operation is in progress.\n */\n#define EXT4_EX_NOCACHE\t\t\t\t0x40000000\n#define EXT4_EX_FORCE_CACHE\t\t\t0x20000000\n#define EXT4_EX_NOFAIL\t\t\t\t0x10000000\n\n/*\n * Flags used by ext4_free_blocks\n */\n#define EXT4_FREE_BLOCKS_METADATA\t\t0x0001\n#define EXT4_FREE_BLOCKS_FORGET\t\t\t0x0002\n#define EXT4_FREE_BLOCKS_VALIDATED\t\t0x0004\n#define EXT4_FREE_BLOCKS_NO_QUOT_UPDATE\t\t0x0008\n#define EXT4_FREE_BLOCKS_NOFREE_FIRST_CLUSTER\t0x0010\n#define EXT4_FREE_BLOCKS_NOFREE_LAST_CLUSTER\t0x0020\n#define EXT4_FREE_BLOCKS_RERESERVE_CLUSTER      0x0040\n\n/*\n * ioctl commands\n */\n#define\tEXT4_IOC_GETVERSION\t\t_IOR('f', 3, long)\n#define\tEXT4_IOC_SETVERSION\t\t_IOW('f', 4, long)\n#define\tEXT4_IOC_GETVERSION_OLD\t\tFS_IOC_GETVERSION\n#define\tEXT4_IOC_SETVERSION_OLD\t\tFS_IOC_SETVERSION\n#define EXT4_IOC_GETRSVSZ\t\t_IOR('f', 5, long)\n#define EXT4_IOC_SETRSVSZ\t\t_IOW('f', 6, long)\n#define EXT4_IOC_GROUP_EXTEND\t\t_IOW('f', 7, unsigned long)\n#define EXT4_IOC_GROUP_ADD\t\t_IOW('f', 8, struct ext4_new_group_input)\n#define EXT4_IOC_MIGRATE\t\t_IO('f', 9)\n /* note ioctl 10 reserved for an early version of the FIEMAP ioctl */\n /* note ioctl 11 reserved for filesystem-independent FIEMAP ioctl */\n#define EXT4_IOC_ALLOC_DA_BLKS\t\t_IO('f', 12)\n#define EXT4_IOC_MOVE_EXT\t\t_IOWR('f', 15, struct move_extent)\n#define EXT4_IOC_RESIZE_FS\t\t_IOW('f', 16, __u64)\n#define EXT4_IOC_SWAP_BOOT\t\t_IO('f', 17)\n#define EXT4_IOC_PRECACHE_EXTENTS\t_IO('f', 18)\n/* ioctl codes 19--39 are reserved for fscrypt */\n#define EXT4_IOC_CLEAR_ES_CACHE\t\t_IO('f', 40)\n#define EXT4_IOC_GETSTATE\t\t_IOW('f', 41, __u32)\n#define EXT4_IOC_GET_ES_CACHE\t\t_IOWR('f', 42, struct fiemap)\n\n#define EXT4_IOC_SHUTDOWN _IOR ('X', 125, __u32)\n\n/*\n * Flags for going down operation\n */\n#define EXT4_GOING_FLAGS_DEFAULT\t\t0x0\t/* going down */\n#define EXT4_GOING_FLAGS_LOGFLUSH\t\t0x1\t/* flush log but not data */\n#define EXT4_GOING_FLAGS_NOLOGFLUSH\t\t0x2\t/* don't flush log nor data */\n\n/*\n * Flags returned by EXT4_IOC_GETSTATE\n *\n * We only expose to userspace a subset of the state flags in\n * i_state_flags\n */\n#define EXT4_STATE_FLAG_EXT_PRECACHED\t0x00000001\n#define EXT4_STATE_FLAG_NEW\t\t0x00000002\n#define EXT4_STATE_FLAG_NEWENTRY\t0x00000004\n#define EXT4_STATE_FLAG_DA_ALLOC_CLOSE\t0x00000008\n\n#if defined(__KERNEL__) && defined(CONFIG_COMPAT)\n/*\n * ioctl commands in 32 bit emulation\n */\n#define EXT4_IOC32_GETVERSION\t\t_IOR('f', 3, int)\n#define EXT4_IOC32_SETVERSION\t\t_IOW('f', 4, int)\n#define EXT4_IOC32_GETRSVSZ\t\t_IOR('f', 5, int)\n#define EXT4_IOC32_SETRSVSZ\t\t_IOW('f', 6, int)\n#define EXT4_IOC32_GROUP_EXTEND\t\t_IOW('f', 7, unsigned int)\n#define EXT4_IOC32_GROUP_ADD\t\t_IOW('f', 8, struct compat_ext4_new_group_input)\n#define EXT4_IOC32_GETVERSION_OLD\tFS_IOC32_GETVERSION\n#define EXT4_IOC32_SETVERSION_OLD\tFS_IOC32_SETVERSION\n#endif\n\n/*\n * Returned by EXT4_IOC_GET_ES_CACHE as an additional possible flag.\n * It indicates that the entry in extent status cache is for a hole.\n */\n#define EXT4_FIEMAP_EXTENT_HOLE\t\t0x08000000\n\n/* Max physical block we can address w/o extents */\n#define EXT4_MAX_BLOCK_FILE_PHYS\t0xFFFFFFFF\n\n/* Max logical block we can support */\n#define EXT4_MAX_LOGICAL_BLOCK\t\t0xFFFFFFFE\n\n/*\n * Structure of an inode on the disk\n */\nstruct ext4_inode {\n\t__le16\ti_mode;\t\t/* File mode */\n\t__le16\ti_uid;\t\t/* Low 16 bits of Owner Uid */\n\t__le32\ti_size_lo;\t/* Size in bytes */\n\t__le32\ti_atime;\t/* Access time */\n\t__le32\ti_ctime;\t/* Inode Change time */\n\t__le32\ti_mtime;\t/* Modification time */\n\t__le32\ti_dtime;\t/* Deletion Time */\n\t__le16\ti_gid;\t\t/* Low 16 bits of Group Id */\n\t__le16\ti_links_count;\t/* Links count */\n\t__le32\ti_blocks_lo;\t/* Blocks count */\n\t__le32\ti_flags;\t/* File flags */\n\tunion {\n\t\tstruct {\n\t\t\t__le32  l_i_version;\n\t\t} linux1;\n\t\tstruct {\n\t\t\t__u32  h_i_translator;\n\t\t} hurd1;\n\t\tstruct {\n\t\t\t__u32  m_i_reserved1;\n\t\t} masix1;\n\t} osd1;\t\t\t\t/* OS dependent 1 */\n\t__le32\ti_block[EXT4_N_BLOCKS];/* Pointers to blocks */\n\t__le32\ti_generation;\t/* File version (for NFS) */\n\t__le32\ti_file_acl_lo;\t/* File ACL */\n\t__le32\ti_size_high;\n\t__le32\ti_obso_faddr;\t/* Obsoleted fragment address */\n\tunion {\n\t\tstruct {\n\t\t\t__le16\tl_i_blocks_high; /* were l_i_reserved1 */\n\t\t\t__le16\tl_i_file_acl_high;\n\t\t\t__le16\tl_i_uid_high;\t/* these 2 fields */\n\t\t\t__le16\tl_i_gid_high;\t/* were reserved2[0] */\n\t\t\t__le16\tl_i_checksum_lo;/* crc32c(uuid+inum+inode) LE */\n\t\t\t__le16\tl_i_reserved;\n\t\t} linux2;\n\t\tstruct {\n\t\t\t__le16\th_i_reserved1;\t/* Obsoleted fragment number/size which are removed in ext4 */\n\t\t\t__u16\th_i_mode_high;\n\t\t\t__u16\th_i_uid_high;\n\t\t\t__u16\th_i_gid_high;\n\t\t\t__u32\th_i_author;\n\t\t} hurd2;\n\t\tstruct {\n\t\t\t__le16\th_i_reserved1;\t/* Obsoleted fragment number/size which are removed in ext4 */\n\t\t\t__le16\tm_i_file_acl_high;\n\t\t\t__u32\tm_i_reserved2[2];\n\t\t} masix2;\n\t} osd2;\t\t\t\t/* OS dependent 2 */\n\t__le16\ti_extra_isize;\n\t__le16\ti_checksum_hi;\t/* crc32c(uuid+inum+inode) BE */\n\t__le32  i_ctime_extra;  /* extra Change time      (nsec << 2 | epoch) */\n\t__le32  i_mtime_extra;  /* extra Modification time(nsec << 2 | epoch) */\n\t__le32  i_atime_extra;  /* extra Access time      (nsec << 2 | epoch) */\n\t__le32  i_crtime;       /* File Creation time */\n\t__le32  i_crtime_extra; /* extra FileCreationtime (nsec << 2 | epoch) */\n\t__le32  i_version_hi;\t/* high 32 bits for 64-bit version */\n\t__le32\ti_projid;\t/* Project ID */\n};\n\nstruct move_extent {\n\t__u32 reserved;\t\t/* should be zero */\n\t__u32 donor_fd;\t\t/* donor file descriptor */\n\t__u64 orig_start;\t/* logical start offset in block for orig */\n\t__u64 donor_start;\t/* logical start offset in block for donor */\n\t__u64 len;\t\t/* block length to be moved */\n\t__u64 moved_len;\t/* moved block length */\n};\n\n#define EXT4_EPOCH_BITS 2\n#define EXT4_EPOCH_MASK ((1 << EXT4_EPOCH_BITS) - 1)\n#define EXT4_NSEC_MASK  (~0UL << EXT4_EPOCH_BITS)\n\n/*\n * Extended fields will fit into an inode if the filesystem was formatted\n * with large inodes (-I 256 or larger) and there are not currently any EAs\n * consuming all of the available space. For new inodes we always reserve\n * enough space for the kernel's known extended fields, but for inodes\n * created with an old kernel this might not have been the case. None of\n * the extended inode fields is critical for correct filesystem operation.\n * This macro checks if a certain field fits in the inode. Note that\n * inode-size = GOOD_OLD_INODE_SIZE + i_extra_isize\n */\n#define EXT4_FITS_IN_INODE(ext4_inode, einode, field)\t\\\n\t((offsetof(typeof(*ext4_inode), field) +\t\\\n\t  sizeof((ext4_inode)->field))\t\t\t\\\n\t<= (EXT4_GOOD_OLD_INODE_SIZE +\t\t\t\\\n\t    (einode)->i_extra_isize))\t\t\t\\\n\n/*\n * We use an encoding that preserves the times for extra epoch \"00\":\n *\n * extra  msb of                         adjust for signed\n * epoch  32-bit                         32-bit tv_sec to\n * bits   time    decoded 64-bit tv_sec  64-bit tv_sec      valid time range\n * 0 0    1    -0x80000000..-0x00000001  0x000000000 1901-12-13..1969-12-31\n * 0 0    0    0x000000000..0x07fffffff  0x000000000 1970-01-01..2038-01-19\n * 0 1    1    0x080000000..0x0ffffffff  0x100000000 2038-01-19..2106-02-07\n * 0 1    0    0x100000000..0x17fffffff  0x100000000 2106-02-07..2174-02-25\n * 1 0    1    0x180000000..0x1ffffffff  0x200000000 2174-02-25..2242-03-16\n * 1 0    0    0x200000000..0x27fffffff  0x200000000 2242-03-16..2310-04-04\n * 1 1    1    0x280000000..0x2ffffffff  0x300000000 2310-04-04..2378-04-22\n * 1 1    0    0x300000000..0x37fffffff  0x300000000 2378-04-22..2446-05-10\n *\n * Note that previous versions of the kernel on 64-bit systems would\n * incorrectly use extra epoch bits 1,1 for dates between 1901 and\n * 1970.  e2fsck will correct this, assuming that it is run on the\n * affected filesystem before 2242.\n */\n\nstatic inline __le32 ext4_encode_extra_time(struct timespec64 *time)\n{\n\tu32 extra =((time->tv_sec - (s32)time->tv_sec) >> 32) & EXT4_EPOCH_MASK;\n\treturn cpu_to_le32(extra | (time->tv_nsec << EXT4_EPOCH_BITS));\n}\n\nstatic inline void ext4_decode_extra_time(struct timespec64 *time,\n\t\t\t\t\t  __le32 extra)\n{\n\tif (unlikely(extra & cpu_to_le32(EXT4_EPOCH_MASK)))\n\t\ttime->tv_sec += (u64)(le32_to_cpu(extra) & EXT4_EPOCH_MASK) << 32;\n\ttime->tv_nsec = (le32_to_cpu(extra) & EXT4_NSEC_MASK) >> EXT4_EPOCH_BITS;\n}\n\n#define EXT4_INODE_SET_XTIME(xtime, inode, raw_inode)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\t\\\n\tif (EXT4_FITS_IN_INODE(raw_inode, EXT4_I(inode), xtime ## _extra))     {\\\n\t\t(raw_inode)->xtime = cpu_to_le32((inode)->xtime.tv_sec);\t\\\n\t\t(raw_inode)->xtime ## _extra =\t\t\t\t\t\\\n\t\t\t\text4_encode_extra_time(&(inode)->xtime);\t\\\n\t\t}\t\t\t\t\t\t\t\t\\\n\telse\t\\\n\t\t(raw_inode)->xtime = cpu_to_le32(clamp_t(int32_t, (inode)->xtime.tv_sec, S32_MIN, S32_MAX));\t\\\n} while (0)\n\n#define EXT4_EINODE_SET_XTIME(xtime, einode, raw_inode)\t\t\t       \\\ndo {\t\t\t\t\t\t\t\t\t       \\\n\tif (EXT4_FITS_IN_INODE(raw_inode, einode, xtime))\t\t       \\\n\t\t(raw_inode)->xtime = cpu_to_le32((einode)->xtime.tv_sec);      \\\n\tif (EXT4_FITS_IN_INODE(raw_inode, einode, xtime ## _extra))\t       \\\n\t\t(raw_inode)->xtime ## _extra =\t\t\t\t       \\\n\t\t\t\text4_encode_extra_time(&(einode)->xtime);      \\\n} while (0)\n\n#define EXT4_INODE_GET_XTIME(xtime, inode, raw_inode)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\t\\\n\t(inode)->xtime.tv_sec = (signed)le32_to_cpu((raw_inode)->xtime);\t\\\n\tif (EXT4_FITS_IN_INODE(raw_inode, EXT4_I(inode), xtime ## _extra)) {\t\\\n\t\text4_decode_extra_time(&(inode)->xtime,\t\t\t\t\\\n\t\t\t\t       raw_inode->xtime ## _extra);\t\t\\\n\t\t}\t\t\t\t\t\t\t\t\\\n\telse\t\t\t\t\t\t\t\t\t\\\n\t\t(inode)->xtime.tv_nsec = 0;\t\t\t\t\t\\\n} while (0)\n\n\n#define EXT4_EINODE_GET_XTIME(xtime, einode, raw_inode)\t\t\t       \\\ndo {\t\t\t\t\t\t\t\t\t       \\\n\tif (EXT4_FITS_IN_INODE(raw_inode, einode, xtime))\t\t       \\\n\t\t(einode)->xtime.tv_sec = \t\t\t\t       \\\n\t\t\t(signed)le32_to_cpu((raw_inode)->xtime);\t       \\\n\telse\t\t\t\t\t\t\t\t       \\\n\t\t(einode)->xtime.tv_sec = 0;\t\t\t\t       \\\n\tif (EXT4_FITS_IN_INODE(raw_inode, einode, xtime ## _extra))\t       \\\n\t\text4_decode_extra_time(&(einode)->xtime,\t\t       \\\n\t\t\t\t       raw_inode->xtime ## _extra);\t       \\\n\telse\t\t\t\t\t\t\t\t       \\\n\t\t(einode)->xtime.tv_nsec = 0;\t\t\t\t       \\\n} while (0)\n\n#define i_disk_version osd1.linux1.l_i_version\n\n#if defined(__KERNEL__) || defined(__linux__)\n#define i_reserved1\tosd1.linux1.l_i_reserved1\n#define i_file_acl_high\tosd2.linux2.l_i_file_acl_high\n#define i_blocks_high\tosd2.linux2.l_i_blocks_high\n#define i_uid_low\ti_uid\n#define i_gid_low\ti_gid\n#define i_uid_high\tosd2.linux2.l_i_uid_high\n#define i_gid_high\tosd2.linux2.l_i_gid_high\n#define i_checksum_lo\tosd2.linux2.l_i_checksum_lo\n\n#elif defined(__GNU__)\n\n#define i_translator\tosd1.hurd1.h_i_translator\n#define i_uid_high\tosd2.hurd2.h_i_uid_high\n#define i_gid_high\tosd2.hurd2.h_i_gid_high\n#define i_author\tosd2.hurd2.h_i_author\n\n#elif defined(__masix__)\n\n#define i_reserved1\tosd1.masix1.m_i_reserved1\n#define i_file_acl_high\tosd2.masix2.m_i_file_acl_high\n#define i_reserved2\tosd2.masix2.m_i_reserved2\n\n#endif /* defined(__KERNEL__) || defined(__linux__) */\n\n#include \"extents_status.h\"\n#include \"fast_commit.h\"\n\n/*\n * Lock subclasses for i_data_sem in the ext4_inode_info structure.\n *\n * These are needed to avoid lockdep false positives when we need to\n * allocate blocks to the quota inode during ext4_map_blocks(), while\n * holding i_data_sem for a normal (non-quota) inode.  Since we don't\n * do quota tracking for the quota inode, this avoids deadlock (as\n * well as infinite recursion, since it isn't turtles all the way\n * down...)\n *\n *  I_DATA_SEM_NORMAL - Used for most inodes\n *  I_DATA_SEM_OTHER  - Used by move_inode.c for the second normal inode\n *\t\t\t  where the second inode has larger inode number\n *\t\t\t  than the first\n *  I_DATA_SEM_QUOTA  - Used for quota inodes only\n */\nenum {\n\tI_DATA_SEM_NORMAL = 0,\n\tI_DATA_SEM_OTHER,\n\tI_DATA_SEM_QUOTA,\n};\n\n\n/*\n * fourth extended file system inode data in memory\n */\nstruct ext4_inode_info {\n\t__le32\ti_data[15];\t/* unconverted */\n\t__u32\ti_dtime;\n\text4_fsblk_t\ti_file_acl;\n\n\t/*\n\t * i_block_group is the number of the block group which contains\n\t * this file's inode.  Constant across the lifetime of the inode,\n\t * it is used for making block allocation decisions - we try to\n\t * place a file's data blocks near its inode block, and new inodes\n\t * near to their parent directory's inode.\n\t */\n\text4_group_t\ti_block_group;\n\text4_lblk_t\ti_dir_start_lookup;\n#if (BITS_PER_LONG < 64)\n\tunsigned long\ti_state_flags;\t\t/* Dynamic state flags */\n#endif\n\tunsigned long\ti_flags;\n\n\t/*\n\t * Extended attributes can be read independently of the main file\n\t * data. Taking i_mutex even when reading would cause contention\n\t * between readers of EAs and writers of regular file data, so\n\t * instead we synchronize on xattr_sem when reading or changing\n\t * EAs.\n\t */\n\tstruct rw_semaphore xattr_sem;\n\n\tstruct list_head i_orphan;\t/* unlinked but open inodes */\n\n\t/* Fast commit related info */\n\n\tstruct list_head i_fc_list;\t/*\n\t\t\t\t\t * inodes that need fast commit\n\t\t\t\t\t * protected by sbi->s_fc_lock.\n\t\t\t\t\t */\n\n\t/* Start of lblk range that needs to be committed in this fast commit */\n\text4_lblk_t i_fc_lblk_start;\n\n\t/* End of lblk range that needs to be committed in this fast commit */\n\text4_lblk_t i_fc_lblk_len;\n\n\t/* Number of ongoing updates on this inode */\n\tatomic_t  i_fc_updates;\n\n\t/* Fast commit wait queue for this inode */\n\twait_queue_head_t i_fc_wait;\n\n\t/* Protect concurrent accesses on i_fc_lblk_start, i_fc_lblk_len */\n\tstruct mutex i_fc_lock;\n\n\t/*\n\t * i_disksize keeps track of what the inode size is ON DISK, not\n\t * in memory.  During truncate, i_size is set to the new size by\n\t * the VFS prior to calling ext4_truncate(), but the filesystem won't\n\t * set i_disksize to 0 until the truncate is actually under way.\n\t *\n\t * The intent is that i_disksize always represents the blocks which\n\t * are used by this file.  This allows recovery to restart truncate\n\t * on orphans if we crash during truncate.  We actually write i_disksize\n\t * into the on-disk inode when writing inodes out, instead of i_size.\n\t *\n\t * The only time when i_disksize and i_size may be different is when\n\t * a truncate is in progress.  The only things which change i_disksize\n\t * are ext4_get_block (growth) and ext4_truncate (shrinkth).\n\t */\n\tloff_t\ti_disksize;\n\n\t/*\n\t * i_data_sem is for serialising ext4_truncate() against\n\t * ext4_getblock().  In the 2.4 ext2 design, great chunks of inode's\n\t * data tree are chopped off during truncate. We can't do that in\n\t * ext4 because whenever we perform intermediate commits during\n\t * truncate, the inode and all the metadata blocks *must* be in a\n\t * consistent state which allows truncation of the orphans to restart\n\t * during recovery.  Hence we must fix the get_block-vs-truncate race\n\t * by other means, so we have i_data_sem.\n\t */\n\tstruct rw_semaphore i_data_sem;\n\t/*\n\t * i_mmap_sem is for serializing page faults with truncate / punch hole\n\t * operations. We have to make sure that new page cannot be faulted in\n\t * a section of the inode that is being punched. We cannot easily use\n\t * i_data_sem for this since we need protection for the whole punch\n\t * operation and i_data_sem ranks below transaction start so we have\n\t * to occasionally drop it.\n\t */\n\tstruct rw_semaphore i_mmap_sem;\n\tstruct inode vfs_inode;\n\tstruct jbd2_inode *jinode;\n\n\tspinlock_t i_raw_lock;\t/* protects updates to the raw inode */\n\n\t/*\n\t * File creation time. Its function is same as that of\n\t * struct timespec64 i_{a,c,m}time in the generic inode.\n\t */\n\tstruct timespec64 i_crtime;\n\n\t/* mballoc */\n\tatomic_t i_prealloc_active;\n\tstruct list_head i_prealloc_list;\n\tspinlock_t i_prealloc_lock;\n\n\t/* extents status tree */\n\tstruct ext4_es_tree i_es_tree;\n\trwlock_t i_es_lock;\n\tstruct list_head i_es_list;\n\tunsigned int i_es_all_nr;\t/* protected by i_es_lock */\n\tunsigned int i_es_shk_nr;\t/* protected by i_es_lock */\n\text4_lblk_t i_es_shrink_lblk;\t/* Offset where we start searching for\n\t\t\t\t\t   extents to shrink. Protected by\n\t\t\t\t\t   i_es_lock  */\n\n\t/* ialloc */\n\text4_group_t\ti_last_alloc_group;\n\n\t/* allocation reservation info for delalloc */\n\t/* In case of bigalloc, this refer to clusters rather than blocks */\n\tunsigned int i_reserved_data_blocks;\n\n\t/* pending cluster reservations for bigalloc file systems */\n\tstruct ext4_pending_tree i_pending_tree;\n\n\t/* on-disk additional length */\n\t__u16 i_extra_isize;\n\n\t/* Indicate the inline data space. */\n\tu16 i_inline_off;\n\tu16 i_inline_size;\n\n#ifdef CONFIG_QUOTA\n\t/* quota space reservation, managed internally by quota code */\n\tqsize_t i_reserved_quota;\n#endif\n\n\t/* Lock protecting lists below */\n\tspinlock_t i_completed_io_lock;\n\t/*\n\t * Completed IOs that need unwritten extents handling and have\n\t * transaction reserved\n\t */\n\tstruct list_head i_rsv_conversion_list;\n\tstruct work_struct i_rsv_conversion_work;\n\tatomic_t i_unwritten; /* Nr. of inflight conversions pending */\n\n\tspinlock_t i_block_reservation_lock;\n\n\t/*\n\t * Transactions that contain inode's metadata needed to complete\n\t * fsync and fdatasync, respectively.\n\t */\n\ttid_t i_sync_tid;\n\ttid_t i_datasync_tid;\n\n#ifdef CONFIG_QUOTA\n\tstruct dquot *i_dquot[MAXQUOTAS];\n#endif\n\n\t/* Precomputed uuid+inum+igen checksum for seeding inode checksums */\n\t__u32 i_csum_seed;\n\n\tkprojid_t i_projid;\n};\n\n/*\n * File system states\n */\n#define\tEXT4_VALID_FS\t\t\t0x0001\t/* Unmounted cleanly */\n#define\tEXT4_ERROR_FS\t\t\t0x0002\t/* Errors detected */\n#define\tEXT4_ORPHAN_FS\t\t\t0x0004\t/* Orphans being recovered */\n#define EXT4_FC_REPLAY\t\t\t0x0020\t/* Fast commit replay ongoing */\n\n/*\n * Misc. filesystem flags\n */\n#define EXT2_FLAGS_SIGNED_HASH\t\t0x0001  /* Signed dirhash in use */\n#define EXT2_FLAGS_UNSIGNED_HASH\t0x0002  /* Unsigned dirhash in use */\n#define EXT2_FLAGS_TEST_FILESYS\t\t0x0004\t/* to test development code */\n\n/*\n * Mount flags set via mount options or defaults\n */\n#define EXT4_MOUNT_NO_MBCACHE\t\t0x00001 /* Do not use mbcache */\n#define EXT4_MOUNT_GRPID\t\t0x00004\t/* Create files with directory's group */\n#define EXT4_MOUNT_DEBUG\t\t0x00008\t/* Some debugging messages */\n#define EXT4_MOUNT_ERRORS_CONT\t\t0x00010\t/* Continue on errors */\n#define EXT4_MOUNT_ERRORS_RO\t\t0x00020\t/* Remount fs ro on errors */\n#define EXT4_MOUNT_ERRORS_PANIC\t\t0x00040\t/* Panic on errors */\n#define EXT4_MOUNT_ERRORS_MASK\t\t0x00070\n#define EXT4_MOUNT_MINIX_DF\t\t0x00080\t/* Mimics the Minix statfs */\n#define EXT4_MOUNT_NOLOAD\t\t0x00100\t/* Don't use existing journal*/\n#ifdef CONFIG_FS_DAX\n#define EXT4_MOUNT_DAX_ALWAYS\t\t0x00200\t/* Direct Access */\n#else\n#define EXT4_MOUNT_DAX_ALWAYS\t\t0\n#endif\n#define EXT4_MOUNT_DATA_FLAGS\t\t0x00C00\t/* Mode for data writes: */\n#define EXT4_MOUNT_JOURNAL_DATA\t\t0x00400\t/* Write data to journal */\n#define EXT4_MOUNT_ORDERED_DATA\t\t0x00800\t/* Flush data before commit */\n#define EXT4_MOUNT_WRITEBACK_DATA\t0x00C00\t/* No data ordering */\n#define EXT4_MOUNT_UPDATE_JOURNAL\t0x01000\t/* Update the journal format */\n#define EXT4_MOUNT_NO_UID32\t\t0x02000  /* Disable 32-bit UIDs */\n#define EXT4_MOUNT_XATTR_USER\t\t0x04000\t/* Extended user attributes */\n#define EXT4_MOUNT_POSIX_ACL\t\t0x08000\t/* POSIX Access Control Lists */\n#define EXT4_MOUNT_NO_AUTO_DA_ALLOC\t0x10000\t/* No auto delalloc mapping */\n#define EXT4_MOUNT_BARRIER\t\t0x20000 /* Use block barriers */\n#define EXT4_MOUNT_QUOTA\t\t0x40000 /* Some quota option set */\n#define EXT4_MOUNT_USRQUOTA\t\t0x80000 /* \"old\" user quota,\n\t\t\t\t\t\t * enable enforcement for hidden\n\t\t\t\t\t\t * quota files */\n#define EXT4_MOUNT_GRPQUOTA\t\t0x100000 /* \"old\" group quota, enable\n\t\t\t\t\t\t  * enforcement for hidden quota\n\t\t\t\t\t\t  * files */\n#define EXT4_MOUNT_PRJQUOTA\t\t0x200000 /* Enable project quota\n\t\t\t\t\t\t  * enforcement */\n#define EXT4_MOUNT_DIOREAD_NOLOCK\t0x400000 /* Enable support for dio read nolocking */\n#define EXT4_MOUNT_JOURNAL_CHECKSUM\t0x800000 /* Journal checksums */\n#define EXT4_MOUNT_JOURNAL_ASYNC_COMMIT\t0x1000000 /* Journal Async Commit */\n#define EXT4_MOUNT_WARN_ON_ERROR\t0x2000000 /* Trigger WARN_ON on error */\n#define EXT4_MOUNT_PREFETCH_BLOCK_BITMAPS 0x4000000\n#define EXT4_MOUNT_DELALLOC\t\t0x8000000 /* Delalloc support */\n#define EXT4_MOUNT_DATA_ERR_ABORT\t0x10000000 /* Abort on file data write */\n#define EXT4_MOUNT_BLOCK_VALIDITY\t0x20000000 /* Block validity checking */\n#define EXT4_MOUNT_DISCARD\t\t0x40000000 /* Issue DISCARD requests */\n#define EXT4_MOUNT_INIT_INODE_TABLE\t0x80000000 /* Initialize uninitialized itables */\n\n/*\n * Mount flags set either automatically (could not be set by mount option)\n * based on per file system feature or property or in special cases such as\n * distinguishing between explicit mount option definition and default.\n */\n#define EXT4_MOUNT2_EXPLICIT_DELALLOC\t0x00000001 /* User explicitly\n\t\t\t\t\t\t      specified delalloc */\n#define EXT4_MOUNT2_STD_GROUP_SIZE\t0x00000002 /* We have standard group\n\t\t\t\t\t\t      size of blocksize * 8\n\t\t\t\t\t\t      blocks */\n#define EXT4_MOUNT2_HURD_COMPAT\t\t0x00000004 /* Support HURD-castrated\n\t\t\t\t\t\t      file systems */\n#define EXT4_MOUNT2_EXPLICIT_JOURNAL_CHECKSUM\t0x00000008 /* User explicitly\n\t\t\t\t\t\tspecified journal checksum */\n\n#define EXT4_MOUNT2_JOURNAL_FAST_COMMIT\t0x00000010 /* Journal fast commit */\n#define EXT4_MOUNT2_DAX_NEVER\t\t0x00000020 /* Do not allow Direct Access */\n#define EXT4_MOUNT2_DAX_INODE\t\t0x00000040 /* For printing options only */\n\n\n#define clear_opt(sb, opt)\t\tEXT4_SB(sb)->s_mount_opt &= \\\n\t\t\t\t\t\t~EXT4_MOUNT_##opt\n#define set_opt(sb, opt)\t\tEXT4_SB(sb)->s_mount_opt |= \\\n\t\t\t\t\t\tEXT4_MOUNT_##opt\n#define test_opt(sb, opt)\t\t(EXT4_SB(sb)->s_mount_opt & \\\n\t\t\t\t\t EXT4_MOUNT_##opt)\n\n#define clear_opt2(sb, opt)\t\tEXT4_SB(sb)->s_mount_opt2 &= \\\n\t\t\t\t\t\t~EXT4_MOUNT2_##opt\n#define set_opt2(sb, opt)\t\tEXT4_SB(sb)->s_mount_opt2 |= \\\n\t\t\t\t\t\tEXT4_MOUNT2_##opt\n#define test_opt2(sb, opt)\t\t(EXT4_SB(sb)->s_mount_opt2 & \\\n\t\t\t\t\t EXT4_MOUNT2_##opt)\n\n#define ext4_test_and_set_bit\t\t__test_and_set_bit_le\n#define ext4_set_bit\t\t\t__set_bit_le\n#define ext4_set_bit_atomic\t\text2_set_bit_atomic\n#define ext4_test_and_clear_bit\t\t__test_and_clear_bit_le\n#define ext4_clear_bit\t\t\t__clear_bit_le\n#define ext4_clear_bit_atomic\t\text2_clear_bit_atomic\n#define ext4_test_bit\t\t\ttest_bit_le\n#define ext4_find_next_zero_bit\t\tfind_next_zero_bit_le\n#define ext4_find_next_bit\t\tfind_next_bit_le\n\nextern void ext4_set_bits(void *bm, int cur, int len);\n\n/*\n * Maximal mount counts between two filesystem checks\n */\n#define EXT4_DFL_MAX_MNT_COUNT\t\t20\t/* Allow 20 mounts */\n#define EXT4_DFL_CHECKINTERVAL\t\t0\t/* Don't use interval check */\n\n/*\n * Behaviour when detecting errors\n */\n#define EXT4_ERRORS_CONTINUE\t\t1\t/* Continue execution */\n#define EXT4_ERRORS_RO\t\t\t2\t/* Remount fs read-only */\n#define EXT4_ERRORS_PANIC\t\t3\t/* Panic */\n#define EXT4_ERRORS_DEFAULT\t\tEXT4_ERRORS_CONTINUE\n\n/* Metadata checksum algorithm codes */\n#define EXT4_CRC32C_CHKSUM\t\t1\n\n/*\n * Structure of the super block\n */\nstruct ext4_super_block {\n/*00*/\t__le32\ts_inodes_count;\t\t/* Inodes count */\n\t__le32\ts_blocks_count_lo;\t/* Blocks count */\n\t__le32\ts_r_blocks_count_lo;\t/* Reserved blocks count */\n\t__le32\ts_free_blocks_count_lo;\t/* Free blocks count */\n/*10*/\t__le32\ts_free_inodes_count;\t/* Free inodes count */\n\t__le32\ts_first_data_block;\t/* First Data Block */\n\t__le32\ts_log_block_size;\t/* Block size */\n\t__le32\ts_log_cluster_size;\t/* Allocation cluster size */\n/*20*/\t__le32\ts_blocks_per_group;\t/* # Blocks per group */\n\t__le32\ts_clusters_per_group;\t/* # Clusters per group */\n\t__le32\ts_inodes_per_group;\t/* # Inodes per group */\n\t__le32\ts_mtime;\t\t/* Mount time */\n/*30*/\t__le32\ts_wtime;\t\t/* Write time */\n\t__le16\ts_mnt_count;\t\t/* Mount count */\n\t__le16\ts_max_mnt_count;\t/* Maximal mount count */\n\t__le16\ts_magic;\t\t/* Magic signature */\n\t__le16\ts_state;\t\t/* File system state */\n\t__le16\ts_errors;\t\t/* Behaviour when detecting errors */\n\t__le16\ts_minor_rev_level;\t/* minor revision level */\n/*40*/\t__le32\ts_lastcheck;\t\t/* time of last check */\n\t__le32\ts_checkinterval;\t/* max. time between checks */\n\t__le32\ts_creator_os;\t\t/* OS */\n\t__le32\ts_rev_level;\t\t/* Revision level */\n/*50*/\t__le16\ts_def_resuid;\t\t/* Default uid for reserved blocks */\n\t__le16\ts_def_resgid;\t\t/* Default gid for reserved blocks */\n\t/*\n\t * These fields are for EXT4_DYNAMIC_REV superblocks only.\n\t *\n\t * Note: the difference between the compatible feature set and\n\t * the incompatible feature set is that if there is a bit set\n\t * in the incompatible feature set that the kernel doesn't\n\t * know about, it should refuse to mount the filesystem.\n\t *\n\t * e2fsck's requirements are more strict; if it doesn't know\n\t * about a feature in either the compatible or incompatible\n\t * feature set, it must abort and not try to meddle with\n\t * things it doesn't understand...\n\t */\n\t__le32\ts_first_ino;\t\t/* First non-reserved inode */\n\t__le16  s_inode_size;\t\t/* size of inode structure */\n\t__le16\ts_block_group_nr;\t/* block group # of this superblock */\n\t__le32\ts_feature_compat;\t/* compatible feature set */\n/*60*/\t__le32\ts_feature_incompat;\t/* incompatible feature set */\n\t__le32\ts_feature_ro_compat;\t/* readonly-compatible feature set */\n/*68*/\t__u8\ts_uuid[16];\t\t/* 128-bit uuid for volume */\n/*78*/\tchar\ts_volume_name[16];\t/* volume name */\n/*88*/\tchar\ts_last_mounted[64] __nonstring;\t/* directory where last mounted */\n/*C8*/\t__le32\ts_algorithm_usage_bitmap; /* For compression */\n\t/*\n\t * Performance hints.  Directory preallocation should only\n\t * happen if the EXT4_FEATURE_COMPAT_DIR_PREALLOC flag is on.\n\t */\n\t__u8\ts_prealloc_blocks;\t/* Nr of blocks to try to preallocate*/\n\t__u8\ts_prealloc_dir_blocks;\t/* Nr to preallocate for dirs */\n\t__le16\ts_reserved_gdt_blocks;\t/* Per group desc for online growth */\n\t/*\n\t * Journaling support valid if EXT4_FEATURE_COMPAT_HAS_JOURNAL set.\n\t */\n/*D0*/\t__u8\ts_journal_uuid[16];\t/* uuid of journal superblock */\n/*E0*/\t__le32\ts_journal_inum;\t\t/* inode number of journal file */\n\t__le32\ts_journal_dev;\t\t/* device number of journal file */\n\t__le32\ts_last_orphan;\t\t/* start of list of inodes to delete */\n\t__le32\ts_hash_seed[4];\t\t/* HTREE hash seed */\n\t__u8\ts_def_hash_version;\t/* Default hash version to use */\n\t__u8\ts_jnl_backup_type;\n\t__le16  s_desc_size;\t\t/* size of group descriptor */\n/*100*/\t__le32\ts_default_mount_opts;\n\t__le32\ts_first_meta_bg;\t/* First metablock block group */\n\t__le32\ts_mkfs_time;\t\t/* When the filesystem was created */\n\t__le32\ts_jnl_blocks[17];\t/* Backup of the journal inode */\n\t/* 64bit support valid if EXT4_FEATURE_COMPAT_64BIT */\n/*150*/\t__le32\ts_blocks_count_hi;\t/* Blocks count */\n\t__le32\ts_r_blocks_count_hi;\t/* Reserved blocks count */\n\t__le32\ts_free_blocks_count_hi;\t/* Free blocks count */\n\t__le16\ts_min_extra_isize;\t/* All inodes have at least # bytes */\n\t__le16\ts_want_extra_isize; \t/* New inodes should reserve # bytes */\n\t__le32\ts_flags;\t\t/* Miscellaneous flags */\n\t__le16  s_raid_stride;\t\t/* RAID stride */\n\t__le16  s_mmp_update_interval;  /* # seconds to wait in MMP checking */\n\t__le64  s_mmp_block;            /* Block for multi-mount protection */\n\t__le32  s_raid_stripe_width;    /* blocks on all data disks (N*stride)*/\n\t__u8\ts_log_groups_per_flex;  /* FLEX_BG group size */\n\t__u8\ts_checksum_type;\t/* metadata checksum algorithm used */\n\t__u8\ts_encryption_level;\t/* versioning level for encryption */\n\t__u8\ts_reserved_pad;\t\t/* Padding to next 32bits */\n\t__le64\ts_kbytes_written;\t/* nr of lifetime kilobytes written */\n\t__le32\ts_snapshot_inum;\t/* Inode number of active snapshot */\n\t__le32\ts_snapshot_id;\t\t/* sequential ID of active snapshot */\n\t__le64\ts_snapshot_r_blocks_count; /* reserved blocks for active\n\t\t\t\t\t      snapshot's future use */\n\t__le32\ts_snapshot_list;\t/* inode number of the head of the\n\t\t\t\t\t   on-disk snapshot list */\n#define EXT4_S_ERR_START offsetof(struct ext4_super_block, s_error_count)\n\t__le32\ts_error_count;\t\t/* number of fs errors */\n\t__le32\ts_first_error_time;\t/* first time an error happened */\n\t__le32\ts_first_error_ino;\t/* inode involved in first error */\n\t__le64\ts_first_error_block;\t/* block involved of first error */\n\t__u8\ts_first_error_func[32] __nonstring;\t/* function where the error happened */\n\t__le32\ts_first_error_line;\t/* line number where error happened */\n\t__le32\ts_last_error_time;\t/* most recent time of an error */\n\t__le32\ts_last_error_ino;\t/* inode involved in last error */\n\t__le32\ts_last_error_line;\t/* line number where error happened */\n\t__le64\ts_last_error_block;\t/* block involved of last error */\n\t__u8\ts_last_error_func[32] __nonstring;\t/* function where the error happened */\n#define EXT4_S_ERR_END offsetof(struct ext4_super_block, s_mount_opts)\n\t__u8\ts_mount_opts[64];\n\t__le32\ts_usr_quota_inum;\t/* inode for tracking user quota */\n\t__le32\ts_grp_quota_inum;\t/* inode for tracking group quota */\n\t__le32\ts_overhead_clusters;\t/* overhead blocks/clusters in fs */\n\t__le32\ts_backup_bgs[2];\t/* groups with sparse_super2 SBs */\n\t__u8\ts_encrypt_algos[4];\t/* Encryption algorithms in use  */\n\t__u8\ts_encrypt_pw_salt[16];\t/* Salt used for string2key algorithm */\n\t__le32\ts_lpf_ino;\t\t/* Location of the lost+found inode */\n\t__le32\ts_prj_quota_inum;\t/* inode for tracking project quota */\n\t__le32\ts_checksum_seed;\t/* crc32c(uuid) if csum_seed set */\n\t__u8\ts_wtime_hi;\n\t__u8\ts_mtime_hi;\n\t__u8\ts_mkfs_time_hi;\n\t__u8\ts_lastcheck_hi;\n\t__u8\ts_first_error_time_hi;\n\t__u8\ts_last_error_time_hi;\n\t__u8\ts_first_error_errcode;\n\t__u8    s_last_error_errcode;\n\t__le16  s_encoding;\t\t/* Filename charset encoding */\n\t__le16  s_encoding_flags;\t/* Filename charset encoding flags */\n\t__le32\ts_reserved[95];\t\t/* Padding to the end of the block */\n\t__le32\ts_checksum;\t\t/* crc32c(superblock) */\n};\n\n#define EXT4_S_ERR_LEN (EXT4_S_ERR_END - EXT4_S_ERR_START)\n\n#ifdef __KERNEL__\n\n#ifdef CONFIG_FS_ENCRYPTION\n#define DUMMY_ENCRYPTION_ENABLED(sbi) ((sbi)->s_dummy_enc_policy.policy != NULL)\n#else\n#define DUMMY_ENCRYPTION_ENABLED(sbi) (0)\n#endif\n\n/* Number of quota types we support */\n#define EXT4_MAXQUOTAS 3\n\n#define EXT4_ENC_UTF8_12_1\t1\n\n/*\n * fourth extended-fs super-block data in memory\n */\nstruct ext4_sb_info {\n\tunsigned long s_desc_size;\t/* Size of a group descriptor in bytes */\n\tunsigned long s_inodes_per_block;/* Number of inodes per block */\n\tunsigned long s_blocks_per_group;/* Number of blocks in a group */\n\tunsigned long s_clusters_per_group; /* Number of clusters in a group */\n\tunsigned long s_inodes_per_group;/* Number of inodes in a group */\n\tunsigned long s_itb_per_group;\t/* Number of inode table blocks per group */\n\tunsigned long s_gdb_count;\t/* Number of group descriptor blocks */\n\tunsigned long s_desc_per_block;\t/* Number of group descriptors per block */\n\text4_group_t s_groups_count;\t/* Number of groups in the fs */\n\text4_group_t s_blockfile_groups;/* Groups acceptable for non-extent files */\n\tunsigned long s_overhead;  /* # of fs overhead clusters */\n\tunsigned int s_cluster_ratio;\t/* Number of blocks per cluster */\n\tunsigned int s_cluster_bits;\t/* log2 of s_cluster_ratio */\n\tloff_t s_bitmap_maxbytes;\t/* max bytes for bitmap files */\n\tstruct buffer_head * s_sbh;\t/* Buffer containing the super block */\n\tstruct ext4_super_block *s_es;\t/* Pointer to the super block in the buffer */\n\tstruct buffer_head * __rcu *s_group_desc;\n\tunsigned int s_mount_opt;\n\tunsigned int s_mount_opt2;\n\tunsigned long s_mount_flags;\n\tunsigned int s_def_mount_opt;\n\text4_fsblk_t s_sb_block;\n\tatomic64_t s_resv_clusters;\n\tkuid_t s_resuid;\n\tkgid_t s_resgid;\n\tunsigned short s_mount_state;\n\tunsigned short s_pad;\n\tint s_addr_per_block_bits;\n\tint s_desc_per_block_bits;\n\tint s_inode_size;\n\tint s_first_ino;\n\tunsigned int s_inode_readahead_blks;\n\tunsigned int s_inode_goal;\n\tu32 s_hash_seed[4];\n\tint s_def_hash_version;\n\tint s_hash_unsigned;\t/* 3 if hash should be signed, 0 if not */\n\tstruct percpu_counter s_freeclusters_counter;\n\tstruct percpu_counter s_freeinodes_counter;\n\tstruct percpu_counter s_dirs_counter;\n\tstruct percpu_counter s_dirtyclusters_counter;\n\tstruct percpu_counter s_sra_exceeded_retry_limit;\n\tstruct blockgroup_lock *s_blockgroup_lock;\n\tstruct proc_dir_entry *s_proc;\n\tstruct kobject s_kobj;\n\tstruct completion s_kobj_unregister;\n\tstruct super_block *s_sb;\n\n\t/* Journaling */\n\tstruct journal_s *s_journal;\n\tstruct list_head s_orphan;\n\tstruct mutex s_orphan_lock;\n\tunsigned long s_ext4_flags;\t\t/* Ext4 superblock flags */\n\tunsigned long s_commit_interval;\n\tu32 s_max_batch_time;\n\tu32 s_min_batch_time;\n\tstruct block_device *s_journal_bdev;\n#ifdef CONFIG_QUOTA\n\t/* Names of quota files with journalled quota */\n\tchar __rcu *s_qf_names[EXT4_MAXQUOTAS];\n\tint s_jquota_fmt;\t\t\t/* Format of quota to use */\n#endif\n\tunsigned int s_want_extra_isize; /* New inodes should reserve # bytes */\n\tstruct ext4_system_blocks __rcu *s_system_blks;\n\n#ifdef EXTENTS_STATS\n\t/* ext4 extents stats */\n\tunsigned long s_ext_min;\n\tunsigned long s_ext_max;\n\tunsigned long s_depth_max;\n\tspinlock_t s_ext_stats_lock;\n\tunsigned long s_ext_blocks;\n\tunsigned long s_ext_extents;\n#endif\n\n\t/* for buddy allocator */\n\tstruct ext4_group_info ** __rcu *s_group_info;\n\tstruct inode *s_buddy_cache;\n\tspinlock_t s_md_lock;\n\tunsigned short *s_mb_offsets;\n\tunsigned int *s_mb_maxs;\n\tunsigned int s_group_info_size;\n\tunsigned int s_mb_free_pending;\n\tstruct list_head s_freed_data_list;\t/* List of blocks to be freed\n\t\t\t\t\t\t   after commit completed */\n\n\t/* tunables */\n\tunsigned long s_stripe;\n\tunsigned int s_mb_stream_request;\n\tunsigned int s_mb_max_to_scan;\n\tunsigned int s_mb_min_to_scan;\n\tunsigned int s_mb_stats;\n\tunsigned int s_mb_order2_reqs;\n\tunsigned int s_mb_group_prealloc;\n\tunsigned int s_mb_max_inode_prealloc;\n\tunsigned int s_max_dir_size_kb;\n\t/* where last allocation was done - for stream allocation */\n\tunsigned long s_mb_last_group;\n\tunsigned long s_mb_last_start;\n\tunsigned int s_mb_prefetch;\n\tunsigned int s_mb_prefetch_limit;\n\n\t/* stats for buddy allocator */\n\tatomic_t s_bal_reqs;\t/* number of reqs with len > 1 */\n\tatomic_t s_bal_success;\t/* we found long enough chunks */\n\tatomic_t s_bal_allocated;\t/* in blocks */\n\tatomic_t s_bal_ex_scanned;\t/* total extents scanned */\n\tatomic_t s_bal_goals;\t/* goal hits */\n\tatomic_t s_bal_breaks;\t/* too long searches */\n\tatomic_t s_bal_2orders;\t/* 2^order hits */\n\tspinlock_t s_bal_lock;\n\tunsigned long s_mb_buddies_generated;\n\tunsigned long long s_mb_generation_time;\n\tatomic_t s_mb_lost_chunks;\n\tatomic_t s_mb_preallocated;\n\tatomic_t s_mb_discarded;\n\tatomic_t s_lock_busy;\n\n\t/* locality groups */\n\tstruct ext4_locality_group __percpu *s_locality_groups;\n\n\t/* for write statistics */\n\tunsigned long s_sectors_written_start;\n\tu64 s_kbytes_written;\n\n\t/* the size of zero-out chunk */\n\tunsigned int s_extent_max_zeroout_kb;\n\n\tunsigned int s_log_groups_per_flex;\n\tstruct flex_groups * __rcu *s_flex_groups;\n\text4_group_t s_flex_groups_allocated;\n\n\t/* workqueue for reserved extent conversions (buffered io) */\n\tstruct workqueue_struct *rsv_conversion_wq;\n\n\t/* timer for periodic error stats printing */\n\tstruct timer_list s_err_report;\n\n\t/* Lazy inode table initialization info */\n\tstruct ext4_li_request *s_li_request;\n\t/* Wait multiplier for lazy initialization thread */\n\tunsigned int s_li_wait_mult;\n\n\t/* Kernel thread for multiple mount protection */\n\tstruct task_struct *s_mmp_tsk;\n\n\t/* record the last minlen when FITRIM is called. */\n\tatomic_t s_last_trim_minblks;\n\n\t/* Reference to checksum algorithm driver via cryptoapi */\n\tstruct crypto_shash *s_chksum_driver;\n\n\t/* Precomputed FS UUID checksum for seeding other checksums */\n\t__u32 s_csum_seed;\n\n\t/* Reclaim extents from extent status tree */\n\tstruct shrinker s_es_shrinker;\n\tstruct list_head s_es_list;\t/* List of inodes with reclaimable extents */\n\tlong s_es_nr_inode;\n\tstruct ext4_es_stats s_es_stats;\n\tstruct mb_cache *s_ea_block_cache;\n\tstruct mb_cache *s_ea_inode_cache;\n\tspinlock_t s_es_lock ____cacheline_aligned_in_smp;\n\n\t/* Ratelimit ext4 messages. */\n\tstruct ratelimit_state s_err_ratelimit_state;\n\tstruct ratelimit_state s_warning_ratelimit_state;\n\tstruct ratelimit_state s_msg_ratelimit_state;\n\tatomic_t s_warning_count;\n\tatomic_t s_msg_count;\n\n\t/* Encryption policy for '-o test_dummy_encryption' */\n\tstruct fscrypt_dummy_policy s_dummy_enc_policy;\n\n\t/*\n\t * Barrier between writepages ops and changing any inode's JOURNAL_DATA\n\t * or EXTENTS flag.\n\t */\n\tstruct percpu_rw_semaphore s_writepages_rwsem;\n\tstruct dax_device *s_daxdev;\n#ifdef CONFIG_EXT4_DEBUG\n\tunsigned long s_simulate_fail;\n#endif\n\t/* Record the errseq of the backing block device */\n\terrseq_t s_bdev_wb_err;\n\tspinlock_t s_bdev_wb_lock;\n\n\t/* Information about errors that happened during this mount */\n\tspinlock_t s_error_lock;\n\tint s_add_error_count;\n\tint s_first_error_code;\n\t__u32 s_first_error_line;\n\t__u32 s_first_error_ino;\n\t__u64 s_first_error_block;\n\tconst char *s_first_error_func;\n\ttime64_t s_first_error_time;\n\tint s_last_error_code;\n\t__u32 s_last_error_line;\n\t__u32 s_last_error_ino;\n\t__u64 s_last_error_block;\n\tconst char *s_last_error_func;\n\ttime64_t s_last_error_time;\n\t/*\n\t * If we are in a context where we cannot update error information in\n\t * the on-disk superblock, we queue this work to do it.\n\t */\n\tstruct work_struct s_error_work;\n\n\t/* Ext4 fast commit stuff */\n\tatomic_t s_fc_subtid;\n\tatomic_t s_fc_ineligible_updates;\n\t/*\n\t * After commit starts, the main queue gets locked, and the further\n\t * updates get added in the staging queue.\n\t */\n#define FC_Q_MAIN\t0\n#define FC_Q_STAGING\t1\n\tstruct list_head s_fc_q[2];\t/* Inodes staged for fast commit\n\t\t\t\t\t * that have data changes in them.\n\t\t\t\t\t */\n\tstruct list_head s_fc_dentry_q[2];\t/* directory entry updates */\n\tunsigned int s_fc_bytes;\n\t/*\n\t * Main fast commit lock. This lock protects accesses to the\n\t * following fields:\n\t * ei->i_fc_list, s_fc_dentry_q, s_fc_q, s_fc_bytes, s_fc_bh.\n\t */\n\tspinlock_t s_fc_lock;\n\tstruct buffer_head *s_fc_bh;\n\tstruct ext4_fc_stats s_fc_stats;\n\tu64 s_fc_avg_commit_time;\n#ifdef CONFIG_EXT4_DEBUG\n\tint s_fc_debug_max_replay;\n#endif\n\tstruct ext4_fc_replay_state s_fc_replay_state;\n};\n\nstatic inline struct ext4_sb_info *EXT4_SB(struct super_block *sb)\n{\n\treturn sb->s_fs_info;\n}\nstatic inline struct ext4_inode_info *EXT4_I(struct inode *inode)\n{\n\treturn container_of(inode, struct ext4_inode_info, vfs_inode);\n}\n\nstatic inline int ext4_valid_inum(struct super_block *sb, unsigned long ino)\n{\n\treturn ino == EXT4_ROOT_INO ||\n\t\t(ino >= EXT4_FIRST_INO(sb) &&\n\t\t ino <= le32_to_cpu(EXT4_SB(sb)->s_es->s_inodes_count));\n}\n\n/*\n * Returns: sbi->field[index]\n * Used to access an array element from the following sbi fields which require\n * rcu protection to avoid dereferencing an invalid pointer due to reassignment\n * - s_group_desc\n * - s_group_info\n * - s_flex_group\n */\n#define sbi_array_rcu_deref(sbi, field, index)\t\t\t\t   \\\n({\t\t\t\t\t\t\t\t\t   \\\n\ttypeof(*((sbi)->field)) _v;\t\t\t\t\t   \\\n\trcu_read_lock();\t\t\t\t\t\t   \\\n\t_v = ((typeof(_v)*)rcu_dereference((sbi)->field))[index];\t   \\\n\trcu_read_unlock();\t\t\t\t\t\t   \\\n\t_v;\t\t\t\t\t\t\t\t   \\\n})\n\n/*\n * run-time mount flags\n */\nenum {\n\tEXT4_MF_MNTDIR_SAMPLED,\n\tEXT4_MF_FS_ABORTED,\t/* Fatal error detected */\n\tEXT4_MF_FC_INELIGIBLE,\t/* Fast commit ineligible */\n\tEXT4_MF_FC_COMMITTING\t/* File system underoing a fast\n\t\t\t\t * commit.\n\t\t\t\t */\n};\n\nstatic inline void ext4_set_mount_flag(struct super_block *sb, int bit)\n{\n\tset_bit(bit, &EXT4_SB(sb)->s_mount_flags);\n}\n\nstatic inline void ext4_clear_mount_flag(struct super_block *sb, int bit)\n{\n\tclear_bit(bit, &EXT4_SB(sb)->s_mount_flags);\n}\n\nstatic inline int ext4_test_mount_flag(struct super_block *sb, int bit)\n{\n\treturn test_bit(bit, &EXT4_SB(sb)->s_mount_flags);\n}\n\n\n/*\n * Simulate_fail codes\n */\n#define EXT4_SIM_BBITMAP_EIO\t1\n#define EXT4_SIM_BBITMAP_CRC\t2\n#define EXT4_SIM_IBITMAP_EIO\t3\n#define EXT4_SIM_IBITMAP_CRC\t4\n#define EXT4_SIM_INODE_EIO\t5\n#define EXT4_SIM_INODE_CRC\t6\n#define EXT4_SIM_DIRBLOCK_EIO\t7\n#define EXT4_SIM_DIRBLOCK_CRC\t8\n\nstatic inline bool ext4_simulate_fail(struct super_block *sb,\n\t\t\t\t     unsigned long code)\n{\n#ifdef CONFIG_EXT4_DEBUG\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\tif (unlikely(sbi->s_simulate_fail == code)) {\n\t\tsbi->s_simulate_fail = 0;\n\t\treturn true;\n\t}\n#endif\n\treturn false;\n}\n\nstatic inline void ext4_simulate_fail_bh(struct super_block *sb,\n\t\t\t\t\t struct buffer_head *bh,\n\t\t\t\t\t unsigned long code)\n{\n\tif (!IS_ERR(bh) && ext4_simulate_fail(sb, code))\n\t\tclear_buffer_uptodate(bh);\n}\n\n/*\n * Error number codes for s_{first,last}_error_errno\n *\n * Linux errno numbers are architecture specific, so we need to translate\n * them into something which is architecture independent.   We don't define\n * codes for all errno's; just the ones which are most likely to be the cause\n * of an ext4_error() call.\n */\n#define EXT4_ERR_UNKNOWN\t 1\n#define EXT4_ERR_EIO\t\t 2\n#define EXT4_ERR_ENOMEM\t\t 3\n#define EXT4_ERR_EFSBADCRC\t 4\n#define EXT4_ERR_EFSCORRUPTED\t 5\n#define EXT4_ERR_ENOSPC\t\t 6\n#define EXT4_ERR_ENOKEY\t\t 7\n#define EXT4_ERR_EROFS\t\t 8\n#define EXT4_ERR_EFBIG\t\t 9\n#define EXT4_ERR_EEXIST\t\t10\n#define EXT4_ERR_ERANGE\t\t11\n#define EXT4_ERR_EOVERFLOW\t12\n#define EXT4_ERR_EBUSY\t\t13\n#define EXT4_ERR_ENOTDIR\t14\n#define EXT4_ERR_ENOTEMPTY\t15\n#define EXT4_ERR_ESHUTDOWN\t16\n#define EXT4_ERR_EFAULT\t\t17\n\n/*\n * Inode dynamic state flags\n */\nenum {\n\tEXT4_STATE_JDATA,\t\t/* journaled data exists */\n\tEXT4_STATE_NEW,\t\t\t/* inode is newly created */\n\tEXT4_STATE_XATTR,\t\t/* has in-inode xattrs */\n\tEXT4_STATE_NO_EXPAND,\t\t/* No space for expansion */\n\tEXT4_STATE_DA_ALLOC_CLOSE,\t/* Alloc DA blks on close */\n\tEXT4_STATE_EXT_MIGRATE,\t\t/* Inode is migrating */\n\tEXT4_STATE_NEWENTRY,\t\t/* File just added to dir */\n\tEXT4_STATE_MAY_INLINE_DATA,\t/* may have in-inode data */\n\tEXT4_STATE_EXT_PRECACHED,\t/* extents have been precached */\n\tEXT4_STATE_LUSTRE_EA_INODE,\t/* Lustre-style ea_inode */\n\tEXT4_STATE_VERITY_IN_PROGRESS,\t/* building fs-verity Merkle tree */\n\tEXT4_STATE_FC_COMMITTING,\t/* Fast commit ongoing */\n};\n\n#define EXT4_INODE_BIT_FNS(name, field, offset)\t\t\t\t\\\nstatic inline int ext4_test_inode_##name(struct inode *inode, int bit)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn test_bit(bit + (offset), &EXT4_I(inode)->i_##field);\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic inline void ext4_set_inode_##name(struct inode *inode, int bit)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tset_bit(bit + (offset), &EXT4_I(inode)->i_##field);\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic inline void ext4_clear_inode_##name(struct inode *inode, int bit) \\\n{\t\t\t\t\t\t\t\t\t\\\n\tclear_bit(bit + (offset), &EXT4_I(inode)->i_##field);\t\t\\\n}\n\n/* Add these declarations here only so that these functions can be\n * found by name.  Otherwise, they are very hard to locate. */\nstatic inline int ext4_test_inode_flag(struct inode *inode, int bit);\nstatic inline void ext4_set_inode_flag(struct inode *inode, int bit);\nstatic inline void ext4_clear_inode_flag(struct inode *inode, int bit);\nEXT4_INODE_BIT_FNS(flag, flags, 0)\n\n/* Add these declarations here only so that these functions can be\n * found by name.  Otherwise, they are very hard to locate. */\nstatic inline int ext4_test_inode_state(struct inode *inode, int bit);\nstatic inline void ext4_set_inode_state(struct inode *inode, int bit);\nstatic inline void ext4_clear_inode_state(struct inode *inode, int bit);\n#if (BITS_PER_LONG < 64)\nEXT4_INODE_BIT_FNS(state, state_flags, 0)\n\nstatic inline void ext4_clear_state_flags(struct ext4_inode_info *ei)\n{\n\t(ei)->i_state_flags = 0;\n}\n#else\nEXT4_INODE_BIT_FNS(state, flags, 32)\n\nstatic inline void ext4_clear_state_flags(struct ext4_inode_info *ei)\n{\n\t/* We depend on the fact that callers will set i_flags */\n}\n#endif\n#else\n/* Assume that user mode programs are passing in an ext4fs superblock, not\n * a kernel struct super_block.  This will allow us to call the feature-test\n * macros from user land. */\n#define EXT4_SB(sb)\t(sb)\n#endif\n\nstatic inline bool ext4_verity_in_progress(struct inode *inode)\n{\n\treturn IS_ENABLED(CONFIG_FS_VERITY) &&\n\t       ext4_test_inode_state(inode, EXT4_STATE_VERITY_IN_PROGRESS);\n}\n\n#define NEXT_ORPHAN(inode) EXT4_I(inode)->i_dtime\n\n/*\n * Codes for operating systems\n */\n#define EXT4_OS_LINUX\t\t0\n#define EXT4_OS_HURD\t\t1\n#define EXT4_OS_MASIX\t\t2\n#define EXT4_OS_FREEBSD\t\t3\n#define EXT4_OS_LITES\t\t4\n\n/*\n * Revision levels\n */\n#define EXT4_GOOD_OLD_REV\t0\t/* The good old (original) format */\n#define EXT4_DYNAMIC_REV\t1\t/* V2 format w/ dynamic inode sizes */\n\n#define EXT4_MAX_SUPP_REV\tEXT4_DYNAMIC_REV\n\n#define EXT4_GOOD_OLD_INODE_SIZE 128\n\n#define EXT4_EXTRA_TIMESTAMP_MAX\t(((s64)1 << 34) - 1  + S32_MIN)\n#define EXT4_NON_EXTRA_TIMESTAMP_MAX\tS32_MAX\n#define EXT4_TIMESTAMP_MIN\t\tS32_MIN\n\n/*\n * Feature set definitions\n */\n\n#define EXT4_FEATURE_COMPAT_DIR_PREALLOC\t0x0001\n#define EXT4_FEATURE_COMPAT_IMAGIC_INODES\t0x0002\n#define EXT4_FEATURE_COMPAT_HAS_JOURNAL\t\t0x0004\n#define EXT4_FEATURE_COMPAT_EXT_ATTR\t\t0x0008\n#define EXT4_FEATURE_COMPAT_RESIZE_INODE\t0x0010\n#define EXT4_FEATURE_COMPAT_DIR_INDEX\t\t0x0020\n#define EXT4_FEATURE_COMPAT_SPARSE_SUPER2\t0x0200\n/*\n * The reason why \"FAST_COMMIT\" is a compat feature is that, FS becomes\n * incompatible only if fast commit blocks are present in the FS. Since we\n * clear the journal (and thus the fast commit blocks), we don't mark FS as\n * incompatible. We also have a JBD2 incompat feature, which gets set when\n * there are fast commit blocks present in the journal.\n */\n#define EXT4_FEATURE_COMPAT_FAST_COMMIT\t\t0x0400\n#define EXT4_FEATURE_COMPAT_STABLE_INODES\t0x0800\n\n#define EXT4_FEATURE_RO_COMPAT_SPARSE_SUPER\t0x0001\n#define EXT4_FEATURE_RO_COMPAT_LARGE_FILE\t0x0002\n#define EXT4_FEATURE_RO_COMPAT_BTREE_DIR\t0x0004\n#define EXT4_FEATURE_RO_COMPAT_HUGE_FILE        0x0008\n#define EXT4_FEATURE_RO_COMPAT_GDT_CSUM\t\t0x0010\n#define EXT4_FEATURE_RO_COMPAT_DIR_NLINK\t0x0020\n#define EXT4_FEATURE_RO_COMPAT_EXTRA_ISIZE\t0x0040\n#define EXT4_FEATURE_RO_COMPAT_QUOTA\t\t0x0100\n#define EXT4_FEATURE_RO_COMPAT_BIGALLOC\t\t0x0200\n/*\n * METADATA_CSUM also enables group descriptor checksums (GDT_CSUM).  When\n * METADATA_CSUM is set, group descriptor checksums use the same algorithm as\n * all other data structures' checksums.  However, the METADATA_CSUM and\n * GDT_CSUM bits are mutually exclusive.\n */\n#define EXT4_FEATURE_RO_COMPAT_METADATA_CSUM\t0x0400\n#define EXT4_FEATURE_RO_COMPAT_READONLY\t\t0x1000\n#define EXT4_FEATURE_RO_COMPAT_PROJECT\t\t0x2000\n#define EXT4_FEATURE_RO_COMPAT_VERITY\t\t0x8000\n\n#define EXT4_FEATURE_INCOMPAT_COMPRESSION\t0x0001\n#define EXT4_FEATURE_INCOMPAT_FILETYPE\t\t0x0002\n#define EXT4_FEATURE_INCOMPAT_RECOVER\t\t0x0004 /* Needs recovery */\n#define EXT4_FEATURE_INCOMPAT_JOURNAL_DEV\t0x0008 /* Journal device */\n#define EXT4_FEATURE_INCOMPAT_META_BG\t\t0x0010\n#define EXT4_FEATURE_INCOMPAT_EXTENTS\t\t0x0040 /* extents support */\n#define EXT4_FEATURE_INCOMPAT_64BIT\t\t0x0080\n#define EXT4_FEATURE_INCOMPAT_MMP               0x0100\n#define EXT4_FEATURE_INCOMPAT_FLEX_BG\t\t0x0200\n#define EXT4_FEATURE_INCOMPAT_EA_INODE\t\t0x0400 /* EA in inode */\n#define EXT4_FEATURE_INCOMPAT_DIRDATA\t\t0x1000 /* data in dirent */\n#define EXT4_FEATURE_INCOMPAT_CSUM_SEED\t\t0x2000\n#define EXT4_FEATURE_INCOMPAT_LARGEDIR\t\t0x4000 /* >2GB or 3-lvl htree */\n#define EXT4_FEATURE_INCOMPAT_INLINE_DATA\t0x8000 /* data in inode */\n#define EXT4_FEATURE_INCOMPAT_ENCRYPT\t\t0x10000\n#define EXT4_FEATURE_INCOMPAT_CASEFOLD\t\t0x20000\n\nextern void ext4_update_dynamic_rev(struct super_block *sb);\n\n#define EXT4_FEATURE_COMPAT_FUNCS(name, flagname) \\\nstatic inline bool ext4_has_feature_##name(struct super_block *sb) \\\n{ \\\n\treturn ((EXT4_SB(sb)->s_es->s_feature_compat & \\\n\t\tcpu_to_le32(EXT4_FEATURE_COMPAT_##flagname)) != 0); \\\n} \\\nstatic inline void ext4_set_feature_##name(struct super_block *sb) \\\n{ \\\n\text4_update_dynamic_rev(sb); \\\n\tEXT4_SB(sb)->s_es->s_feature_compat |= \\\n\t\tcpu_to_le32(EXT4_FEATURE_COMPAT_##flagname); \\\n} \\\nstatic inline void ext4_clear_feature_##name(struct super_block *sb) \\\n{ \\\n\tEXT4_SB(sb)->s_es->s_feature_compat &= \\\n\t\t~cpu_to_le32(EXT4_FEATURE_COMPAT_##flagname); \\\n}\n\n#define EXT4_FEATURE_RO_COMPAT_FUNCS(name, flagname) \\\nstatic inline bool ext4_has_feature_##name(struct super_block *sb) \\\n{ \\\n\treturn ((EXT4_SB(sb)->s_es->s_feature_ro_compat & \\\n\t\tcpu_to_le32(EXT4_FEATURE_RO_COMPAT_##flagname)) != 0); \\\n} \\\nstatic inline void ext4_set_feature_##name(struct super_block *sb) \\\n{ \\\n\text4_update_dynamic_rev(sb); \\\n\tEXT4_SB(sb)->s_es->s_feature_ro_compat |= \\\n\t\tcpu_to_le32(EXT4_FEATURE_RO_COMPAT_##flagname); \\\n} \\\nstatic inline void ext4_clear_feature_##name(struct super_block *sb) \\\n{ \\\n\tEXT4_SB(sb)->s_es->s_feature_ro_compat &= \\\n\t\t~cpu_to_le32(EXT4_FEATURE_RO_COMPAT_##flagname); \\\n}\n\n#define EXT4_FEATURE_INCOMPAT_FUNCS(name, flagname) \\\nstatic inline bool ext4_has_feature_##name(struct super_block *sb) \\\n{ \\\n\treturn ((EXT4_SB(sb)->s_es->s_feature_incompat & \\\n\t\tcpu_to_le32(EXT4_FEATURE_INCOMPAT_##flagname)) != 0); \\\n} \\\nstatic inline void ext4_set_feature_##name(struct super_block *sb) \\\n{ \\\n\text4_update_dynamic_rev(sb); \\\n\tEXT4_SB(sb)->s_es->s_feature_incompat |= \\\n\t\tcpu_to_le32(EXT4_FEATURE_INCOMPAT_##flagname); \\\n} \\\nstatic inline void ext4_clear_feature_##name(struct super_block *sb) \\\n{ \\\n\tEXT4_SB(sb)->s_es->s_feature_incompat &= \\\n\t\t~cpu_to_le32(EXT4_FEATURE_INCOMPAT_##flagname); \\\n}\n\nEXT4_FEATURE_COMPAT_FUNCS(dir_prealloc,\t\tDIR_PREALLOC)\nEXT4_FEATURE_COMPAT_FUNCS(imagic_inodes,\tIMAGIC_INODES)\nEXT4_FEATURE_COMPAT_FUNCS(journal,\t\tHAS_JOURNAL)\nEXT4_FEATURE_COMPAT_FUNCS(xattr,\t\tEXT_ATTR)\nEXT4_FEATURE_COMPAT_FUNCS(resize_inode,\t\tRESIZE_INODE)\nEXT4_FEATURE_COMPAT_FUNCS(dir_index,\t\tDIR_INDEX)\nEXT4_FEATURE_COMPAT_FUNCS(sparse_super2,\tSPARSE_SUPER2)\nEXT4_FEATURE_COMPAT_FUNCS(fast_commit,\t\tFAST_COMMIT)\nEXT4_FEATURE_COMPAT_FUNCS(stable_inodes,\tSTABLE_INODES)\n\nEXT4_FEATURE_RO_COMPAT_FUNCS(sparse_super,\tSPARSE_SUPER)\nEXT4_FEATURE_RO_COMPAT_FUNCS(large_file,\tLARGE_FILE)\nEXT4_FEATURE_RO_COMPAT_FUNCS(btree_dir,\t\tBTREE_DIR)\nEXT4_FEATURE_RO_COMPAT_FUNCS(huge_file,\t\tHUGE_FILE)\nEXT4_FEATURE_RO_COMPAT_FUNCS(gdt_csum,\t\tGDT_CSUM)\nEXT4_FEATURE_RO_COMPAT_FUNCS(dir_nlink,\t\tDIR_NLINK)\nEXT4_FEATURE_RO_COMPAT_FUNCS(extra_isize,\tEXTRA_ISIZE)\nEXT4_FEATURE_RO_COMPAT_FUNCS(quota,\t\tQUOTA)\nEXT4_FEATURE_RO_COMPAT_FUNCS(bigalloc,\t\tBIGALLOC)\nEXT4_FEATURE_RO_COMPAT_FUNCS(metadata_csum,\tMETADATA_CSUM)\nEXT4_FEATURE_RO_COMPAT_FUNCS(readonly,\t\tREADONLY)\nEXT4_FEATURE_RO_COMPAT_FUNCS(project,\t\tPROJECT)\nEXT4_FEATURE_RO_COMPAT_FUNCS(verity,\t\tVERITY)\n\nEXT4_FEATURE_INCOMPAT_FUNCS(compression,\tCOMPRESSION)\nEXT4_FEATURE_INCOMPAT_FUNCS(filetype,\t\tFILETYPE)\nEXT4_FEATURE_INCOMPAT_FUNCS(journal_needs_recovery,\tRECOVER)\nEXT4_FEATURE_INCOMPAT_FUNCS(journal_dev,\tJOURNAL_DEV)\nEXT4_FEATURE_INCOMPAT_FUNCS(meta_bg,\t\tMETA_BG)\nEXT4_FEATURE_INCOMPAT_FUNCS(extents,\t\tEXTENTS)\nEXT4_FEATURE_INCOMPAT_FUNCS(64bit,\t\t64BIT)\nEXT4_FEATURE_INCOMPAT_FUNCS(mmp,\t\tMMP)\nEXT4_FEATURE_INCOMPAT_FUNCS(flex_bg,\t\tFLEX_BG)\nEXT4_FEATURE_INCOMPAT_FUNCS(ea_inode,\t\tEA_INODE)\nEXT4_FEATURE_INCOMPAT_FUNCS(dirdata,\t\tDIRDATA)\nEXT4_FEATURE_INCOMPAT_FUNCS(csum_seed,\t\tCSUM_SEED)\nEXT4_FEATURE_INCOMPAT_FUNCS(largedir,\t\tLARGEDIR)\nEXT4_FEATURE_INCOMPAT_FUNCS(inline_data,\tINLINE_DATA)\nEXT4_FEATURE_INCOMPAT_FUNCS(encrypt,\t\tENCRYPT)\nEXT4_FEATURE_INCOMPAT_FUNCS(casefold,\t\tCASEFOLD)\n\n#define EXT2_FEATURE_COMPAT_SUPP\tEXT4_FEATURE_COMPAT_EXT_ATTR\n#define EXT2_FEATURE_INCOMPAT_SUPP\t(EXT4_FEATURE_INCOMPAT_FILETYPE| \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_META_BG)\n#define EXT2_FEATURE_RO_COMPAT_SUPP\t(EXT4_FEATURE_RO_COMPAT_SPARSE_SUPER| \\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_LARGE_FILE| \\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_BTREE_DIR)\n\n#define EXT3_FEATURE_COMPAT_SUPP\tEXT4_FEATURE_COMPAT_EXT_ATTR\n#define EXT3_FEATURE_INCOMPAT_SUPP\t(EXT4_FEATURE_INCOMPAT_FILETYPE| \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_RECOVER| \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_META_BG)\n#define EXT3_FEATURE_RO_COMPAT_SUPP\t(EXT4_FEATURE_RO_COMPAT_SPARSE_SUPER| \\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_LARGE_FILE| \\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_BTREE_DIR)\n\n#define EXT4_FEATURE_COMPAT_SUPP\tEXT4_FEATURE_COMPAT_EXT_ATTR\n#define EXT4_FEATURE_INCOMPAT_SUPP\t(EXT4_FEATURE_INCOMPAT_FILETYPE| \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_RECOVER| \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_META_BG| \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_EXTENTS| \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_64BIT| \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_FLEX_BG| \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_EA_INODE| \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_MMP | \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_INLINE_DATA | \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_ENCRYPT | \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_CASEFOLD | \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_CSUM_SEED | \\\n\t\t\t\t\t EXT4_FEATURE_INCOMPAT_LARGEDIR)\n#define EXT4_FEATURE_RO_COMPAT_SUPP\t(EXT4_FEATURE_RO_COMPAT_SPARSE_SUPER| \\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_LARGE_FILE| \\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_GDT_CSUM| \\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_DIR_NLINK | \\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_EXTRA_ISIZE | \\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_BTREE_DIR |\\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_HUGE_FILE |\\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_BIGALLOC |\\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_METADATA_CSUM|\\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_QUOTA |\\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_PROJECT |\\\n\t\t\t\t\t EXT4_FEATURE_RO_COMPAT_VERITY)\n\n#define EXTN_FEATURE_FUNCS(ver) \\\nstatic inline bool ext4_has_unknown_ext##ver##_compat_features(struct super_block *sb) \\\n{ \\\n\treturn ((EXT4_SB(sb)->s_es->s_feature_compat & \\\n\t\tcpu_to_le32(~EXT##ver##_FEATURE_COMPAT_SUPP)) != 0); \\\n} \\\nstatic inline bool ext4_has_unknown_ext##ver##_ro_compat_features(struct super_block *sb) \\\n{ \\\n\treturn ((EXT4_SB(sb)->s_es->s_feature_ro_compat & \\\n\t\tcpu_to_le32(~EXT##ver##_FEATURE_RO_COMPAT_SUPP)) != 0); \\\n} \\\nstatic inline bool ext4_has_unknown_ext##ver##_incompat_features(struct super_block *sb) \\\n{ \\\n\treturn ((EXT4_SB(sb)->s_es->s_feature_incompat & \\\n\t\tcpu_to_le32(~EXT##ver##_FEATURE_INCOMPAT_SUPP)) != 0); \\\n}\n\nEXTN_FEATURE_FUNCS(2)\nEXTN_FEATURE_FUNCS(3)\nEXTN_FEATURE_FUNCS(4)\n\nstatic inline bool ext4_has_compat_features(struct super_block *sb)\n{\n\treturn (EXT4_SB(sb)->s_es->s_feature_compat != 0);\n}\nstatic inline bool ext4_has_ro_compat_features(struct super_block *sb)\n{\n\treturn (EXT4_SB(sb)->s_es->s_feature_ro_compat != 0);\n}\nstatic inline bool ext4_has_incompat_features(struct super_block *sb)\n{\n\treturn (EXT4_SB(sb)->s_es->s_feature_incompat != 0);\n}\n\n/*\n * Superblock flags\n */\n#define EXT4_FLAGS_RESIZING\t0\n#define EXT4_FLAGS_SHUTDOWN\t1\n#define EXT4_FLAGS_BDEV_IS_DAX\t2\n\nstatic inline int ext4_forced_shutdown(struct ext4_sb_info *sbi)\n{\n\treturn test_bit(EXT4_FLAGS_SHUTDOWN, &sbi->s_ext4_flags);\n}\n\n\n/*\n * Default values for user and/or group using reserved blocks\n */\n#define\tEXT4_DEF_RESUID\t\t0\n#define\tEXT4_DEF_RESGID\t\t0\n\n/*\n * Default project ID\n */\n#define\tEXT4_DEF_PROJID\t\t0\n\n#define EXT4_DEF_INODE_READAHEAD_BLKS\t32\n\n/*\n * Default mount options\n */\n#define EXT4_DEFM_DEBUG\t\t0x0001\n#define EXT4_DEFM_BSDGROUPS\t0x0002\n#define EXT4_DEFM_XATTR_USER\t0x0004\n#define EXT4_DEFM_ACL\t\t0x0008\n#define EXT4_DEFM_UID16\t\t0x0010\n#define EXT4_DEFM_JMODE\t\t0x0060\n#define EXT4_DEFM_JMODE_DATA\t0x0020\n#define EXT4_DEFM_JMODE_ORDERED\t0x0040\n#define EXT4_DEFM_JMODE_WBACK\t0x0060\n#define EXT4_DEFM_NOBARRIER\t0x0100\n#define EXT4_DEFM_BLOCK_VALIDITY 0x0200\n#define EXT4_DEFM_DISCARD\t0x0400\n#define EXT4_DEFM_NODELALLOC\t0x0800\n\n/*\n * Default journal batch times\n */\n#define EXT4_DEF_MIN_BATCH_TIME\t0\n#define EXT4_DEF_MAX_BATCH_TIME\t15000 /* 15ms */\n\n/*\n * Minimum number of groups in a flexgroup before we separate out\n * directories into the first block group of a flexgroup\n */\n#define EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME\t4\n\n/*\n * Structure of a directory entry\n */\n#define EXT4_NAME_LEN 255\n\nstruct ext4_dir_entry {\n\t__le32\tinode;\t\t\t/* Inode number */\n\t__le16\trec_len;\t\t/* Directory entry length */\n\t__le16\tname_len;\t\t/* Name length */\n\tchar\tname[EXT4_NAME_LEN];\t/* File name */\n};\n\n/*\n * The new version of the directory entry.  Since EXT4 structures are\n * stored in intel byte order, and the name_len field could never be\n * bigger than 255 chars, it's safe to reclaim the extra byte for the\n * file_type field.\n */\nstruct ext4_dir_entry_2 {\n\t__le32\tinode;\t\t\t/* Inode number */\n\t__le16\trec_len;\t\t/* Directory entry length */\n\t__u8\tname_len;\t\t/* Name length */\n\t__u8\tfile_type;\t\t/* See file type macros EXT4_FT_* below */\n\tchar\tname[EXT4_NAME_LEN];\t/* File name */\n};\n\n/*\n * This is a bogus directory entry at the end of each leaf block that\n * records checksums.\n */\nstruct ext4_dir_entry_tail {\n\t__le32\tdet_reserved_zero1;\t/* Pretend to be unused */\n\t__le16\tdet_rec_len;\t\t/* 12 */\n\t__u8\tdet_reserved_zero2;\t/* Zero name length */\n\t__u8\tdet_reserved_ft;\t/* 0xDE, fake file type */\n\t__le32\tdet_checksum;\t\t/* crc32c(uuid+inum+dirblock) */\n};\n\n#define EXT4_DIRENT_TAIL(block, blocksize) \\\n\t((struct ext4_dir_entry_tail *)(((void *)(block)) + \\\n\t\t\t\t\t((blocksize) - \\\n\t\t\t\t\t sizeof(struct ext4_dir_entry_tail))))\n\n/*\n * Ext4 directory file types.  Only the low 3 bits are used.  The\n * other bits are reserved for now.\n */\n#define EXT4_FT_UNKNOWN\t\t0\n#define EXT4_FT_REG_FILE\t1\n#define EXT4_FT_DIR\t\t2\n#define EXT4_FT_CHRDEV\t\t3\n#define EXT4_FT_BLKDEV\t\t4\n#define EXT4_FT_FIFO\t\t5\n#define EXT4_FT_SOCK\t\t6\n#define EXT4_FT_SYMLINK\t\t7\n\n#define EXT4_FT_MAX\t\t8\n\n#define EXT4_FT_DIR_CSUM\t0xDE\n\n/*\n * EXT4_DIR_PAD defines the directory entries boundaries\n *\n * NOTE: It must be a multiple of 4\n */\n#define EXT4_DIR_PAD\t\t\t4\n#define EXT4_DIR_ROUND\t\t\t(EXT4_DIR_PAD - 1)\n#define EXT4_DIR_REC_LEN(name_len)\t(((name_len) + 8 + EXT4_DIR_ROUND) & \\\n\t\t\t\t\t ~EXT4_DIR_ROUND)\n#define EXT4_MAX_REC_LEN\t\t((1<<16)-1)\n\n/*\n * If we ever get support for fs block sizes > page_size, we'll need\n * to remove the #if statements in the next two functions...\n */\nstatic inline unsigned int\next4_rec_len_from_disk(__le16 dlen, unsigned blocksize)\n{\n\tunsigned len = le16_to_cpu(dlen);\n\n#if (PAGE_SIZE >= 65536)\n\tif (len == EXT4_MAX_REC_LEN || len == 0)\n\t\treturn blocksize;\n\treturn (len & 65532) | ((len & 3) << 16);\n#else\n\treturn len;\n#endif\n}\n\nstatic inline __le16 ext4_rec_len_to_disk(unsigned len, unsigned blocksize)\n{\n\tif ((len > blocksize) || (blocksize > (1 << 18)) || (len & 3))\n\t\tBUG();\n#if (PAGE_SIZE >= 65536)\n\tif (len < 65536)\n\t\treturn cpu_to_le16(len);\n\tif (len == blocksize) {\n\t\tif (blocksize == 65536)\n\t\t\treturn cpu_to_le16(EXT4_MAX_REC_LEN);\n\t\telse\n\t\t\treturn cpu_to_le16(0);\n\t}\n\treturn cpu_to_le16((len & 65532) | ((len >> 16) & 3));\n#else\n\treturn cpu_to_le16(len);\n#endif\n}\n\n/*\n * Hash Tree Directory indexing\n * (c) Daniel Phillips, 2001\n */\n\n#define is_dx(dir) (ext4_has_feature_dir_index((dir)->i_sb) && \\\n\t\t    ext4_test_inode_flag((dir), EXT4_INODE_INDEX))\n#define EXT4_DIR_LINK_MAX(dir) unlikely((dir)->i_nlink >= EXT4_LINK_MAX && \\\n\t\t    !(ext4_has_feature_dir_nlink((dir)->i_sb) && is_dx(dir)))\n#define EXT4_DIR_LINK_EMPTY(dir) ((dir)->i_nlink == 2 || (dir)->i_nlink == 1)\n\n/* Legal values for the dx_root hash_version field: */\n\n#define DX_HASH_LEGACY\t\t\t0\n#define DX_HASH_HALF_MD4\t\t1\n#define DX_HASH_TEA\t\t\t2\n#define DX_HASH_LEGACY_UNSIGNED\t\t3\n#define DX_HASH_HALF_MD4_UNSIGNED\t4\n#define DX_HASH_TEA_UNSIGNED\t\t5\n\nstatic inline u32 ext4_chksum(struct ext4_sb_info *sbi, u32 crc,\n\t\t\t      const void *address, unsigned int length)\n{\n\tstruct {\n\t\tstruct shash_desc shash;\n\t\tchar ctx[4];\n\t} desc;\n\n\tBUG_ON(crypto_shash_descsize(sbi->s_chksum_driver)!=sizeof(desc.ctx));\n\n\tdesc.shash.tfm = sbi->s_chksum_driver;\n\t*(u32 *)desc.ctx = crc;\n\n\tBUG_ON(crypto_shash_update(&desc.shash, address, length));\n\n\treturn *(u32 *)desc.ctx;\n}\n\n#ifdef __KERNEL__\n\n/* hash info structure used by the directory hash */\nstruct dx_hash_info\n{\n\tu32\t\thash;\n\tu32\t\tminor_hash;\n\tint\t\thash_version;\n\tu32\t\t*seed;\n};\n\n\n/* 32 and 64 bit signed EOF for dx directories */\n#define EXT4_HTREE_EOF_32BIT   ((1UL  << (32 - 1)) - 1)\n#define EXT4_HTREE_EOF_64BIT   ((1ULL << (64 - 1)) - 1)\n\n\n/*\n * Control parameters used by ext4_htree_next_block\n */\n#define HASH_NB_ALWAYS\t\t1\n\nstruct ext4_filename {\n\tconst struct qstr *usr_fname;\n\tstruct fscrypt_str disk_name;\n\tstruct dx_hash_info hinfo;\n#ifdef CONFIG_FS_ENCRYPTION\n\tstruct fscrypt_str crypto_buf;\n#endif\n#ifdef CONFIG_UNICODE\n\tstruct fscrypt_str cf_name;\n#endif\n};\n\n#define fname_name(p) ((p)->disk_name.name)\n#define fname_len(p)  ((p)->disk_name.len)\n\n/*\n * Describe an inode's exact location on disk and in memory\n */\nstruct ext4_iloc\n{\n\tstruct buffer_head *bh;\n\tunsigned long offset;\n\text4_group_t block_group;\n};\n\nstatic inline struct ext4_inode *ext4_raw_inode(struct ext4_iloc *iloc)\n{\n\treturn (struct ext4_inode *) (iloc->bh->b_data + iloc->offset);\n}\n\nstatic inline bool ext4_is_quota_file(struct inode *inode)\n{\n\treturn IS_NOQUOTA(inode) &&\n\t       !(EXT4_I(inode)->i_flags & EXT4_EA_INODE_FL);\n}\n\n/*\n * This structure is stuffed into the struct file's private_data field\n * for directories.  It is where we put information so that we can do\n * readdir operations in hash tree order.\n */\nstruct dir_private_info {\n\tstruct rb_root\troot;\n\tstruct rb_node\t*curr_node;\n\tstruct fname\t*extra_fname;\n\tloff_t\t\tlast_pos;\n\t__u32\t\tcurr_hash;\n\t__u32\t\tcurr_minor_hash;\n\t__u32\t\tnext_hash;\n};\n\n/* calculate the first block number of the group */\nstatic inline ext4_fsblk_t\next4_group_first_block_no(struct super_block *sb, ext4_group_t group_no)\n{\n\treturn group_no * (ext4_fsblk_t)EXT4_BLOCKS_PER_GROUP(sb) +\n\t\tle32_to_cpu(EXT4_SB(sb)->s_es->s_first_data_block);\n}\n\n/*\n * Special error return code only used by dx_probe() and its callers.\n */\n#define ERR_BAD_DX_DIR\t(-(MAX_ERRNO - 1))\n\n/* htree levels for ext4 */\n#define\tEXT4_HTREE_LEVEL_COMPAT\t2\n#define\tEXT4_HTREE_LEVEL\t3\n\nstatic inline int ext4_dir_htree_level(struct super_block *sb)\n{\n\treturn ext4_has_feature_largedir(sb) ?\n\t\tEXT4_HTREE_LEVEL : EXT4_HTREE_LEVEL_COMPAT;\n}\n\n/*\n * Timeout and state flag for lazy initialization inode thread.\n */\n#define EXT4_DEF_LI_WAIT_MULT\t\t\t10\n#define EXT4_DEF_LI_MAX_START_DELAY\t\t5\n#define EXT4_LAZYINIT_QUIT\t\t\t0x0001\n#define EXT4_LAZYINIT_RUNNING\t\t\t0x0002\n\n/*\n * Lazy inode table initialization info\n */\nstruct ext4_lazy_init {\n\tunsigned long\t\tli_state;\n\tstruct list_head\tli_request_list;\n\tstruct mutex\t\tli_list_mtx;\n};\n\nenum ext4_li_mode {\n\tEXT4_LI_MODE_PREFETCH_BBITMAP,\n\tEXT4_LI_MODE_ITABLE,\n};\n\nstruct ext4_li_request {\n\tstruct super_block\t*lr_super;\n\tenum ext4_li_mode\tlr_mode;\n\text4_group_t\t\tlr_first_not_zeroed;\n\text4_group_t\t\tlr_next_group;\n\tstruct list_head\tlr_request;\n\tunsigned long\t\tlr_next_sched;\n\tunsigned long\t\tlr_timeout;\n};\n\nstruct ext4_features {\n\tstruct kobject f_kobj;\n\tstruct completion f_kobj_unregister;\n};\n\n/*\n * This structure will be used for multiple mount protection. It will be\n * written into the block number saved in the s_mmp_block field in the\n * superblock. Programs that check MMP should assume that if\n * SEQ_FSCK (or any unknown code above SEQ_MAX) is present then it is NOT safe\n * to use the filesystem, regardless of how old the timestamp is.\n */\n#define EXT4_MMP_MAGIC     0x004D4D50U /* ASCII for MMP */\n#define EXT4_MMP_SEQ_CLEAN 0xFF4D4D50U /* mmp_seq value for clean unmount */\n#define EXT4_MMP_SEQ_FSCK  0xE24D4D50U /* mmp_seq value when being fscked */\n#define EXT4_MMP_SEQ_MAX   0xE24D4D4FU /* maximum valid mmp_seq value */\n\nstruct mmp_struct {\n\t__le32\tmmp_magic;\t\t/* Magic number for MMP */\n\t__le32\tmmp_seq;\t\t/* Sequence no. updated periodically */\n\n\t/*\n\t * mmp_time, mmp_nodename & mmp_bdevname are only used for information\n\t * purposes and do not affect the correctness of the algorithm\n\t */\n\t__le64\tmmp_time;\t\t/* Time last updated */\n\tchar\tmmp_nodename[64];\t/* Node which last updated MMP block */\n\tchar\tmmp_bdevname[32];\t/* Bdev which last updated MMP block */\n\n\t/*\n\t * mmp_check_interval is used to verify if the MMP block has been\n\t * updated on the block device. The value is updated based on the\n\t * maximum time to write the MMP block during an update cycle.\n\t */\n\t__le16\tmmp_check_interval;\n\n\t__le16\tmmp_pad1;\n\t__le32\tmmp_pad2[226];\n\t__le32\tmmp_checksum;\t\t/* crc32c(uuid+mmp_block) */\n};\n\n/* arguments passed to the mmp thread */\nstruct mmpd_data {\n\tstruct buffer_head *bh; /* bh from initial read_mmp_block() */\n\tstruct super_block *sb;  /* super block of the fs */\n};\n\n/*\n * Check interval multiplier\n * The MMP block is written every update interval and initially checked every\n * update interval x the multiplier (the value is then adapted based on the\n * write latency). The reason is that writes can be delayed under load and we\n * don't want readers to incorrectly assume that the filesystem is no longer\n * in use.\n */\n#define EXT4_MMP_CHECK_MULT\t\t2UL\n\n/*\n * Minimum interval for MMP checking in seconds.\n */\n#define EXT4_MMP_MIN_CHECK_INTERVAL\t5UL\n\n/*\n * Maximum interval for MMP checking in seconds.\n */\n#define EXT4_MMP_MAX_CHECK_INTERVAL\t300UL\n\n/*\n * Function prototypes\n */\n\n/*\n * Ok, these declarations are also in <linux/kernel.h> but none of the\n * ext4 source programs needs to include it so they are duplicated here.\n */\n# define NORET_TYPE\t/**/\n# define ATTRIB_NORET\t__attribute__((noreturn))\n# define NORET_AND\tnoreturn,\n\n/* bitmap.c */\nextern unsigned int ext4_count_free(char *bitmap, unsigned numchars);\nvoid ext4_inode_bitmap_csum_set(struct super_block *sb, ext4_group_t group,\n\t\t\t\tstruct ext4_group_desc *gdp,\n\t\t\t\tstruct buffer_head *bh, int sz);\nint ext4_inode_bitmap_csum_verify(struct super_block *sb, ext4_group_t group,\n\t\t\t\t  struct ext4_group_desc *gdp,\n\t\t\t\t  struct buffer_head *bh, int sz);\nvoid ext4_block_bitmap_csum_set(struct super_block *sb, ext4_group_t group,\n\t\t\t\tstruct ext4_group_desc *gdp,\n\t\t\t\tstruct buffer_head *bh);\nint ext4_block_bitmap_csum_verify(struct super_block *sb, ext4_group_t group,\n\t\t\t\t  struct ext4_group_desc *gdp,\n\t\t\t\t  struct buffer_head *bh);\n\n/* balloc.c */\nextern void ext4_get_group_no_and_offset(struct super_block *sb,\n\t\t\t\t\t ext4_fsblk_t blocknr,\n\t\t\t\t\t ext4_group_t *blockgrpp,\n\t\t\t\t\t ext4_grpblk_t *offsetp);\nextern ext4_group_t ext4_get_group_number(struct super_block *sb,\n\t\t\t\t\t  ext4_fsblk_t block);\n\nextern unsigned int ext4_block_group(struct super_block *sb,\n\t\t\text4_fsblk_t blocknr);\nextern ext4_grpblk_t ext4_block_group_offset(struct super_block *sb,\n\t\t\text4_fsblk_t blocknr);\nextern int ext4_bg_has_super(struct super_block *sb, ext4_group_t group);\nextern unsigned long ext4_bg_num_gdb(struct super_block *sb,\n\t\t\text4_group_t group);\nextern ext4_fsblk_t ext4_new_meta_blocks(handle_t *handle, struct inode *inode,\n\t\t\t\t\t ext4_fsblk_t goal,\n\t\t\t\t\t unsigned int flags,\n\t\t\t\t\t unsigned long *count,\n\t\t\t\t\t int *errp);\nextern int ext4_claim_free_clusters(struct ext4_sb_info *sbi,\n\t\t\t\t    s64 nclusters, unsigned int flags);\nextern ext4_fsblk_t ext4_count_free_clusters(struct super_block *);\nextern void ext4_check_blocks_bitmap(struct super_block *);\nextern struct ext4_group_desc * ext4_get_group_desc(struct super_block * sb,\n\t\t\t\t\t\t    ext4_group_t block_group,\n\t\t\t\t\t\t    struct buffer_head ** bh);\nextern int ext4_should_retry_alloc(struct super_block *sb, int *retries);\n\nextern struct buffer_head *ext4_read_block_bitmap_nowait(struct super_block *sb,\n\t\t\t\t\t\text4_group_t block_group,\n\t\t\t\t\t\tbool ignore_locked);\nextern int ext4_wait_block_bitmap(struct super_block *sb,\n\t\t\t\t  ext4_group_t block_group,\n\t\t\t\t  struct buffer_head *bh);\nextern struct buffer_head *ext4_read_block_bitmap(struct super_block *sb,\n\t\t\t\t\t\t  ext4_group_t block_group);\nextern unsigned ext4_free_clusters_after_init(struct super_block *sb,\n\t\t\t\t\t      ext4_group_t block_group,\n\t\t\t\t\t      struct ext4_group_desc *gdp);\next4_fsblk_t ext4_inode_to_goal_block(struct inode *);\n\n#ifdef CONFIG_UNICODE\nextern void ext4_fname_setup_ci_filename(struct inode *dir,\n\t\t\t\t\t const struct qstr *iname,\n\t\t\t\t\t struct fscrypt_str *fname);\n#endif\n\n#ifdef CONFIG_FS_ENCRYPTION\nstatic inline void ext4_fname_from_fscrypt_name(struct ext4_filename *dst,\n\t\t\t\t\t\tconst struct fscrypt_name *src)\n{\n\tmemset(dst, 0, sizeof(*dst));\n\n\tdst->usr_fname = src->usr_fname;\n\tdst->disk_name = src->disk_name;\n\tdst->hinfo.hash = src->hash;\n\tdst->hinfo.minor_hash = src->minor_hash;\n\tdst->crypto_buf = src->crypto_buf;\n}\n\nstatic inline int ext4_fname_setup_filename(struct inode *dir,\n\t\t\t\t\t    const struct qstr *iname,\n\t\t\t\t\t    int lookup,\n\t\t\t\t\t    struct ext4_filename *fname)\n{\n\tstruct fscrypt_name name;\n\tint err;\n\n\terr = fscrypt_setup_filename(dir, iname, lookup, &name);\n\tif (err)\n\t\treturn err;\n\n\text4_fname_from_fscrypt_name(fname, &name);\n\n#ifdef CONFIG_UNICODE\n\text4_fname_setup_ci_filename(dir, iname, &fname->cf_name);\n#endif\n\treturn 0;\n}\n\nstatic inline int ext4_fname_prepare_lookup(struct inode *dir,\n\t\t\t\t\t    struct dentry *dentry,\n\t\t\t\t\t    struct ext4_filename *fname)\n{\n\tstruct fscrypt_name name;\n\tint err;\n\n\terr = fscrypt_prepare_lookup(dir, dentry, &name);\n\tif (err)\n\t\treturn err;\n\n\text4_fname_from_fscrypt_name(fname, &name);\n\n#ifdef CONFIG_UNICODE\n\text4_fname_setup_ci_filename(dir, &dentry->d_name, &fname->cf_name);\n#endif\n\treturn 0;\n}\n\nstatic inline void ext4_fname_free_filename(struct ext4_filename *fname)\n{\n\tstruct fscrypt_name name;\n\n\tname.crypto_buf = fname->crypto_buf;\n\tfscrypt_free_filename(&name);\n\n\tfname->crypto_buf.name = NULL;\n\tfname->usr_fname = NULL;\n\tfname->disk_name.name = NULL;\n\n#ifdef CONFIG_UNICODE\n\tkfree(fname->cf_name.name);\n\tfname->cf_name.name = NULL;\n#endif\n}\n#else /* !CONFIG_FS_ENCRYPTION */\nstatic inline int ext4_fname_setup_filename(struct inode *dir,\n\t\t\t\t\t    const struct qstr *iname,\n\t\t\t\t\t    int lookup,\n\t\t\t\t\t    struct ext4_filename *fname)\n{\n\tfname->usr_fname = iname;\n\tfname->disk_name.name = (unsigned char *) iname->name;\n\tfname->disk_name.len = iname->len;\n\n#ifdef CONFIG_UNICODE\n\text4_fname_setup_ci_filename(dir, iname, &fname->cf_name);\n#endif\n\n\treturn 0;\n}\n\nstatic inline int ext4_fname_prepare_lookup(struct inode *dir,\n\t\t\t\t\t    struct dentry *dentry,\n\t\t\t\t\t    struct ext4_filename *fname)\n{\n\treturn ext4_fname_setup_filename(dir, &dentry->d_name, 1, fname);\n}\n\nstatic inline void ext4_fname_free_filename(struct ext4_filename *fname)\n{\n#ifdef CONFIG_UNICODE\n\tkfree(fname->cf_name.name);\n\tfname->cf_name.name = NULL;\n#endif\n}\n#endif /* !CONFIG_FS_ENCRYPTION */\n\n/* dir.c */\nextern int __ext4_check_dir_entry(const char *, unsigned int, struct inode *,\n\t\t\t\t  struct file *,\n\t\t\t\t  struct ext4_dir_entry_2 *,\n\t\t\t\t  struct buffer_head *, char *, int,\n\t\t\t\t  unsigned int);\n#define ext4_check_dir_entry(dir, filp, de, bh, buf, size, offset)\t\\\n\tunlikely(__ext4_check_dir_entry(__func__, __LINE__, (dir), (filp), \\\n\t\t\t\t\t(de), (bh), (buf), (size), (offset)))\nextern int ext4_htree_store_dirent(struct file *dir_file, __u32 hash,\n\t\t\t\t__u32 minor_hash,\n\t\t\t\tstruct ext4_dir_entry_2 *dirent,\n\t\t\t\tstruct fscrypt_str *ent_name);\nextern void ext4_htree_free_dir_info(struct dir_private_info *p);\nextern int ext4_find_dest_de(struct inode *dir, struct inode *inode,\n\t\t\t     struct buffer_head *bh,\n\t\t\t     void *buf, int buf_size,\n\t\t\t     struct ext4_filename *fname,\n\t\t\t     struct ext4_dir_entry_2 **dest_de);\nvoid ext4_insert_dentry(struct inode *inode,\n\t\t\tstruct ext4_dir_entry_2 *de,\n\t\t\tint buf_size,\n\t\t\tstruct ext4_filename *fname);\nstatic inline void ext4_update_dx_flag(struct inode *inode)\n{\n\tif (!ext4_has_feature_dir_index(inode->i_sb) &&\n\t    ext4_test_inode_flag(inode, EXT4_INODE_INDEX)) {\n\t\t/* ext4_iget() should have caught this... */\n\t\tWARN_ON_ONCE(ext4_has_feature_metadata_csum(inode->i_sb));\n\t\text4_clear_inode_flag(inode, EXT4_INODE_INDEX);\n\t}\n}\nstatic const unsigned char ext4_filetype_table[] = {\n\tDT_UNKNOWN, DT_REG, DT_DIR, DT_CHR, DT_BLK, DT_FIFO, DT_SOCK, DT_LNK\n};\n\nstatic inline  unsigned char get_dtype(struct super_block *sb, int filetype)\n{\n\tif (!ext4_has_feature_filetype(sb) || filetype >= EXT4_FT_MAX)\n\t\treturn DT_UNKNOWN;\n\n\treturn ext4_filetype_table[filetype];\n}\nextern int ext4_check_all_de(struct inode *dir, struct buffer_head *bh,\n\t\t\t     void *buf, int buf_size);\n\n/* fsync.c */\nextern int ext4_sync_file(struct file *, loff_t, loff_t, int);\n\n/* hash.c */\nextern int ext4fs_dirhash(const struct inode *dir, const char *name, int len,\n\t\t\t  struct dx_hash_info *hinfo);\n\n/* ialloc.c */\nextern int ext4_mark_inode_used(struct super_block *sb, int ino);\nextern struct inode *__ext4_new_inode(struct user_namespace *, handle_t *,\n\t\t\t\t      struct inode *, umode_t,\n\t\t\t\t      const struct qstr *qstr, __u32 goal,\n\t\t\t\t      uid_t *owner, __u32 i_flags,\n\t\t\t\t      int handle_type, unsigned int line_no,\n\t\t\t\t      int nblocks);\n\n#define ext4_new_inode(handle, dir, mode, qstr, goal, owner, i_flags)          \\\n\t__ext4_new_inode(&init_user_ns, (handle), (dir), (mode), (qstr),       \\\n\t\t\t (goal), (owner), i_flags, 0, 0, 0)\n#define ext4_new_inode_start_handle(mnt_userns, dir, mode, qstr, goal, owner, \\\n\t\t\t\t    type, nblocks)\t\t    \\\n\t__ext4_new_inode((mnt_userns), NULL, (dir), (mode), (qstr), (goal), (owner), \\\n\t\t\t 0, (type), __LINE__, (nblocks))\n\n\nextern void ext4_free_inode(handle_t *, struct inode *);\nextern struct inode * ext4_orphan_get(struct super_block *, unsigned long);\nextern unsigned long ext4_count_free_inodes(struct super_block *);\nextern unsigned long ext4_count_dirs(struct super_block *);\nextern void ext4_check_inodes_bitmap(struct super_block *);\nextern void ext4_mark_bitmap_end(int start_bit, int end_bit, char *bitmap);\nextern int ext4_init_inode_table(struct super_block *sb,\n\t\t\t\t ext4_group_t group, int barrier);\nextern void ext4_end_bitmap_read(struct buffer_head *bh, int uptodate);\n\n/* fast_commit.c */\nint ext4_fc_info_show(struct seq_file *seq, void *v);\nvoid ext4_fc_init(struct super_block *sb, journal_t *journal);\nvoid ext4_fc_init_inode(struct inode *inode);\nvoid ext4_fc_track_range(handle_t *handle, struct inode *inode, ext4_lblk_t start,\n\t\t\t ext4_lblk_t end);\nvoid __ext4_fc_track_unlink(handle_t *handle, struct inode *inode,\n\tstruct dentry *dentry);\nvoid __ext4_fc_track_link(handle_t *handle, struct inode *inode,\n\tstruct dentry *dentry);\nvoid ext4_fc_track_unlink(handle_t *handle, struct dentry *dentry);\nvoid ext4_fc_track_link(handle_t *handle, struct dentry *dentry);\nvoid __ext4_fc_track_create(handle_t *handle, struct inode *inode,\n\t\t\t    struct dentry *dentry);\nvoid ext4_fc_track_create(handle_t *handle, struct dentry *dentry);\nvoid ext4_fc_track_inode(handle_t *handle, struct inode *inode);\nvoid ext4_fc_mark_ineligible(struct super_block *sb, int reason);\nvoid ext4_fc_start_ineligible(struct super_block *sb, int reason);\nvoid ext4_fc_stop_ineligible(struct super_block *sb);\nvoid ext4_fc_start_update(struct inode *inode);\nvoid ext4_fc_stop_update(struct inode *inode);\nvoid ext4_fc_del(struct inode *inode);\nbool ext4_fc_replay_check_excluded(struct super_block *sb, ext4_fsblk_t block);\nvoid ext4_fc_replay_cleanup(struct super_block *sb);\nint ext4_fc_commit(journal_t *journal, tid_t commit_tid);\nint __init ext4_fc_init_dentry_cache(void);\n\n/* mballoc.c */\nextern const struct seq_operations ext4_mb_seq_groups_ops;\nextern long ext4_mb_stats;\nextern long ext4_mb_max_to_scan;\nextern int ext4_mb_init(struct super_block *);\nextern int ext4_mb_release(struct super_block *);\nextern ext4_fsblk_t ext4_mb_new_blocks(handle_t *,\n\t\t\t\tstruct ext4_allocation_request *, int *);\nextern int ext4_mb_reserve_blocks(struct super_block *, int);\nextern void ext4_discard_preallocations(struct inode *, unsigned int);\nextern int __init ext4_init_mballoc(void);\nextern void ext4_exit_mballoc(void);\nextern ext4_group_t ext4_mb_prefetch(struct super_block *sb,\n\t\t\t\t     ext4_group_t group,\n\t\t\t\t     unsigned int nr, int *cnt);\nextern void ext4_mb_prefetch_fini(struct super_block *sb, ext4_group_t group,\n\t\t\t\t  unsigned int nr);\n\nextern void ext4_free_blocks(handle_t *handle, struct inode *inode,\n\t\t\t     struct buffer_head *bh, ext4_fsblk_t block,\n\t\t\t     unsigned long count, int flags);\nextern int ext4_mb_alloc_groupinfo(struct super_block *sb,\n\t\t\t\t   ext4_group_t ngroups);\nextern int ext4_mb_add_groupinfo(struct super_block *sb,\n\t\text4_group_t i, struct ext4_group_desc *desc);\nextern int ext4_group_add_blocks(handle_t *handle, struct super_block *sb,\n\t\t\t\text4_fsblk_t block, unsigned long count);\nextern int ext4_trim_fs(struct super_block *, struct fstrim_range *);\nextern void ext4_process_freed_data(struct super_block *sb, tid_t commit_tid);\nextern void ext4_mb_mark_bb(struct super_block *sb, ext4_fsblk_t block,\n\t\t       int len, int state);\n\n/* inode.c */\nvoid ext4_inode_csum_set(struct inode *inode, struct ext4_inode *raw,\n\t\t\t struct ext4_inode_info *ei);\nint ext4_inode_is_fast_symlink(struct inode *inode);\nstruct buffer_head *ext4_getblk(handle_t *, struct inode *, ext4_lblk_t, int);\nstruct buffer_head *ext4_bread(handle_t *, struct inode *, ext4_lblk_t, int);\nint ext4_bread_batch(struct inode *inode, ext4_lblk_t block, int bh_count,\n\t\t     bool wait, struct buffer_head **bhs);\nint ext4_get_block_unwritten(struct inode *inode, sector_t iblock,\n\t\t\t     struct buffer_head *bh_result, int create);\nint ext4_get_block(struct inode *inode, sector_t iblock,\n\t\t   struct buffer_head *bh_result, int create);\nint ext4_da_get_block_prep(struct inode *inode, sector_t iblock,\n\t\t\t   struct buffer_head *bh, int create);\nint ext4_walk_page_buffers(handle_t *handle,\n\t\t\t   struct buffer_head *head,\n\t\t\t   unsigned from,\n\t\t\t   unsigned to,\n\t\t\t   int *partial,\n\t\t\t   int (*fn)(handle_t *handle,\n\t\t\t\t     struct buffer_head *bh));\nint do_journal_get_write_access(handle_t *handle,\n\t\t\t\tstruct buffer_head *bh);\n#define FALL_BACK_TO_NONDELALLOC 1\n#define CONVERT_INLINE_DATA\t 2\n\ntypedef enum {\n\tEXT4_IGET_NORMAL =\t0,\n\tEXT4_IGET_SPECIAL =\t0x0001, /* OK to iget a system inode */\n\tEXT4_IGET_HANDLE = \t0x0002\t/* Inode # is from a handle */\n} ext4_iget_flags;\n\nextern struct inode *__ext4_iget(struct super_block *sb, unsigned long ino,\n\t\t\t\t ext4_iget_flags flags, const char *function,\n\t\t\t\t unsigned int line);\n\n#define ext4_iget(sb, ino, flags) \\\n\t__ext4_iget((sb), (ino), (flags), __func__, __LINE__)\n\nextern int  ext4_write_inode(struct inode *, struct writeback_control *);\nextern int  ext4_setattr(struct user_namespace *, struct dentry *,\n\t\t\t struct iattr *);\nextern int  ext4_getattr(struct user_namespace *, const struct path *,\n\t\t\t struct kstat *, u32, unsigned int);\nextern void ext4_evict_inode(struct inode *);\nextern void ext4_clear_inode(struct inode *);\nextern int  ext4_file_getattr(struct user_namespace *, const struct path *,\n\t\t\t      struct kstat *, u32, unsigned int);\nextern int  ext4_sync_inode(handle_t *, struct inode *);\nextern void ext4_dirty_inode(struct inode *, int);\nextern int ext4_change_inode_journal_flag(struct inode *, int);\nextern int ext4_get_inode_loc(struct inode *, struct ext4_iloc *);\nextern int ext4_get_fc_inode_loc(struct super_block *sb, unsigned long ino,\n\t\t\t  struct ext4_iloc *iloc);\nextern int ext4_inode_attach_jinode(struct inode *inode);\nextern int ext4_can_truncate(struct inode *inode);\nextern int ext4_truncate(struct inode *);\nextern int ext4_break_layouts(struct inode *);\nextern int ext4_punch_hole(struct inode *inode, loff_t offset, loff_t length);\nextern void ext4_set_inode_flags(struct inode *, bool init);\nextern int ext4_alloc_da_blocks(struct inode *inode);\nextern void ext4_set_aops(struct inode *inode);\nextern int ext4_writepage_trans_blocks(struct inode *);\nextern int ext4_chunk_trans_blocks(struct inode *, int nrblocks);\nextern int ext4_zero_partial_blocks(handle_t *handle, struct inode *inode,\n\t\t\t     loff_t lstart, loff_t lend);\nextern vm_fault_t ext4_page_mkwrite(struct vm_fault *vmf);\nextern vm_fault_t ext4_filemap_fault(struct vm_fault *vmf);\nextern qsize_t *ext4_get_reserved_space(struct inode *inode);\nextern int ext4_get_projid(struct inode *inode, kprojid_t *projid);\nextern void ext4_da_release_space(struct inode *inode, int to_free);\nextern void ext4_da_update_reserve_space(struct inode *inode,\n\t\t\t\t\tint used, int quota_claim);\nextern int ext4_issue_zeroout(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t      ext4_fsblk_t pblk, ext4_lblk_t len);\n\n/* indirect.c */\nextern int ext4_ind_map_blocks(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_map_blocks *map, int flags);\nextern int ext4_ind_trans_blocks(struct inode *inode, int nrblocks);\nextern void ext4_ind_truncate(handle_t *, struct inode *inode);\nextern int ext4_ind_remove_space(handle_t *handle, struct inode *inode,\n\t\t\t\t ext4_lblk_t start, ext4_lblk_t end);\n\n/* ioctl.c */\nextern long ext4_ioctl(struct file *, unsigned int, unsigned long);\nextern long ext4_compat_ioctl(struct file *, unsigned int, unsigned long);\nextern void ext4_reset_inode_seed(struct inode *inode);\n\n/* migrate.c */\nextern int ext4_ext_migrate(struct inode *);\nextern int ext4_ind_migrate(struct inode *inode);\n\n/* namei.c */\nextern int ext4_init_new_dir(handle_t *handle, struct inode *dir,\n\t\t\t     struct inode *inode);\nextern int ext4_dirblock_csum_verify(struct inode *inode,\n\t\t\t\t     struct buffer_head *bh);\nextern int ext4_orphan_add(handle_t *, struct inode *);\nextern int ext4_orphan_del(handle_t *, struct inode *);\nextern int ext4_htree_fill_tree(struct file *dir_file, __u32 start_hash,\n\t\t\t\t__u32 start_minor_hash, __u32 *next_hash);\nextern int ext4_search_dir(struct buffer_head *bh,\n\t\t\t   char *search_buf,\n\t\t\t   int buf_size,\n\t\t\t   struct inode *dir,\n\t\t\t   struct ext4_filename *fname,\n\t\t\t   unsigned int offset,\n\t\t\t   struct ext4_dir_entry_2 **res_dir);\nextern int ext4_generic_delete_entry(struct inode *dir,\n\t\t\t\t     struct ext4_dir_entry_2 *de_del,\n\t\t\t\t     struct buffer_head *bh,\n\t\t\t\t     void *entry_buf,\n\t\t\t\t     int buf_size,\n\t\t\t\t     int csum_size);\nextern bool ext4_empty_dir(struct inode *inode);\n\n/* resize.c */\nextern void ext4_kvfree_array_rcu(void *to_free);\nextern int ext4_group_add(struct super_block *sb,\n\t\t\t\tstruct ext4_new_group_data *input);\nextern int ext4_group_extend(struct super_block *sb,\n\t\t\t\tstruct ext4_super_block *es,\n\t\t\t\text4_fsblk_t n_blocks_count);\nextern int ext4_resize_fs(struct super_block *sb, ext4_fsblk_t n_blocks_count);\n\n/* super.c */\nextern struct buffer_head *ext4_sb_bread(struct super_block *sb,\n\t\t\t\t\t sector_t block, int op_flags);\nextern struct buffer_head *ext4_sb_bread_unmovable(struct super_block *sb,\n\t\t\t\t\t\t   sector_t block);\nextern void ext4_read_bh_nowait(struct buffer_head *bh, int op_flags,\n\t\t\t\tbh_end_io_t *end_io);\nextern int ext4_read_bh(struct buffer_head *bh, int op_flags,\n\t\t\tbh_end_io_t *end_io);\nextern int ext4_read_bh_lock(struct buffer_head *bh, int op_flags, bool wait);\nextern void ext4_sb_breadahead_unmovable(struct super_block *sb, sector_t block);\nextern int ext4_seq_options_show(struct seq_file *seq, void *offset);\nextern int ext4_calculate_overhead(struct super_block *sb);\nextern void ext4_superblock_csum_set(struct super_block *sb);\nextern int ext4_alloc_flex_bg_array(struct super_block *sb,\n\t\t\t\t    ext4_group_t ngroup);\nextern const char *ext4_decode_error(struct super_block *sb, int errno,\n\t\t\t\t     char nbuf[16]);\nextern void ext4_mark_group_bitmap_corrupted(struct super_block *sb,\n\t\t\t\t\t     ext4_group_t block_group,\n\t\t\t\t\t     unsigned int flags);\n\nextern __printf(7, 8)\nvoid __ext4_error(struct super_block *, const char *, unsigned int, bool,\n\t\t  int, __u64, const char *, ...);\nextern __printf(6, 7)\nvoid __ext4_error_inode(struct inode *, const char *, unsigned int,\n\t\t\text4_fsblk_t, int, const char *, ...);\nextern __printf(5, 6)\nvoid __ext4_error_file(struct file *, const char *, unsigned int, ext4_fsblk_t,\n\t\t     const char *, ...);\nextern void __ext4_std_error(struct super_block *, const char *,\n\t\t\t     unsigned int, int);\nextern __printf(4, 5)\nvoid __ext4_warning(struct super_block *, const char *, unsigned int,\n\t\t    const char *, ...);\nextern __printf(4, 5)\nvoid __ext4_warning_inode(const struct inode *inode, const char *function,\n\t\t\t  unsigned int line, const char *fmt, ...);\nextern __printf(3, 4)\nvoid __ext4_msg(struct super_block *, const char *, const char *, ...);\nextern void __dump_mmp_msg(struct super_block *, struct mmp_struct *mmp,\n\t\t\t   const char *, unsigned int, const char *);\nextern __printf(7, 8)\nvoid __ext4_grp_locked_error(const char *, unsigned int,\n\t\t\t     struct super_block *, ext4_group_t,\n\t\t\t     unsigned long, ext4_fsblk_t,\n\t\t\t     const char *, ...);\n\n#define EXT4_ERROR_INODE(inode, fmt, a...) \\\n\text4_error_inode((inode), __func__, __LINE__, 0, (fmt), ## a)\n\n#define EXT4_ERROR_INODE_ERR(inode, err, fmt, a...)\t\t\t\\\n\t__ext4_error_inode((inode), __func__, __LINE__, 0, (err), (fmt), ## a)\n\n#define ext4_error_inode_block(inode, block, err, fmt, a...)\t\t\\\n\t__ext4_error_inode((inode), __func__, __LINE__, (block), (err),\t\\\n\t\t\t   (fmt), ## a)\n\n#define EXT4_ERROR_FILE(file, block, fmt, a...)\t\t\t\t\\\n\text4_error_file((file), __func__, __LINE__, (block), (fmt), ## a)\n\n#define ext4_abort(sb, err, fmt, a...)\t\t\t\t\t\\\n\t__ext4_error((sb), __func__, __LINE__, true, (err), 0, (fmt), ## a)\n\n#ifdef CONFIG_PRINTK\n\n#define ext4_error_inode(inode, func, line, block, fmt, ...)\t\t\\\n\t__ext4_error_inode(inode, func, line, block, 0, fmt, ##__VA_ARGS__)\n#define ext4_error_inode_err(inode, func, line, block, err, fmt, ...)\t\\\n\t__ext4_error_inode((inode), (func), (line), (block), \t\t\\\n\t\t\t   (err), (fmt), ##__VA_ARGS__)\n#define ext4_error_file(file, func, line, block, fmt, ...)\t\t\\\n\t__ext4_error_file(file, func, line, block, fmt, ##__VA_ARGS__)\n#define ext4_error(sb, fmt, ...)\t\t\t\t\t\\\n\t__ext4_error((sb), __func__, __LINE__, false, 0, 0, (fmt),\t\\\n\t\t##__VA_ARGS__)\n#define ext4_error_err(sb, err, fmt, ...)\t\t\t\t\\\n\t__ext4_error((sb), __func__, __LINE__, false, (err), 0, (fmt),\t\\\n\t\t##__VA_ARGS__)\n#define ext4_warning(sb, fmt, ...)\t\t\t\t\t\\\n\t__ext4_warning(sb, __func__, __LINE__, fmt, ##__VA_ARGS__)\n#define ext4_warning_inode(inode, fmt, ...)\t\t\t\t\\\n\t__ext4_warning_inode(inode, __func__, __LINE__, fmt, ##__VA_ARGS__)\n#define ext4_msg(sb, level, fmt, ...)\t\t\t\t\\\n\t__ext4_msg(sb, level, fmt, ##__VA_ARGS__)\n#define dump_mmp_msg(sb, mmp, msg)\t\t\t\t\t\\\n\t__dump_mmp_msg(sb, mmp, __func__, __LINE__, msg)\n#define ext4_grp_locked_error(sb, grp, ino, block, fmt, ...)\t\t\\\n\t__ext4_grp_locked_error(__func__, __LINE__, sb, grp, ino, block, \\\n\t\t\t\tfmt, ##__VA_ARGS__)\n\n#else\n\n#define ext4_error_inode(inode, func, line, block, fmt, ...)\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tno_printk(fmt, ##__VA_ARGS__);\t\t\t\t\t\\\n\t__ext4_error_inode(inode, \"\", 0, block, 0, \" \");\t\t\\\n} while (0)\n#define ext4_error_inode_err(inode, func, line, block, err, fmt, ...)\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tno_printk(fmt, ##__VA_ARGS__);\t\t\t\t\t\\\n\t__ext4_error_inode(inode, \"\", 0, block, err, \" \");\t\t\\\n} while (0)\n#define ext4_error_file(file, func, line, block, fmt, ...)\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tno_printk(fmt, ##__VA_ARGS__);\t\t\t\t\t\\\n\t__ext4_error_file(file, \"\", 0, block, \" \");\t\t\t\\\n} while (0)\n#define ext4_error(sb, fmt, ...)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tno_printk(fmt, ##__VA_ARGS__);\t\t\t\t\t\\\n\t__ext4_error(sb, \"\", 0, false, 0, 0, \" \");\t\t\t\\\n} while (0)\n#define ext4_error_err(sb, err, fmt, ...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tno_printk(fmt, ##__VA_ARGS__);\t\t\t\t\t\\\n\t__ext4_error(sb, \"\", 0, false, err, 0, \" \");\t\t\t\\\n} while (0)\n#define ext4_warning(sb, fmt, ...)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tno_printk(fmt, ##__VA_ARGS__);\t\t\t\t\t\\\n\t__ext4_warning(sb, \"\", 0, \" \");\t\t\t\t\t\\\n} while (0)\n#define ext4_warning_inode(inode, fmt, ...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tno_printk(fmt, ##__VA_ARGS__);\t\t\t\t\t\\\n\t__ext4_warning_inode(inode, \"\", 0, \" \");\t\t\t\\\n} while (0)\n#define ext4_msg(sb, level, fmt, ...)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tno_printk(fmt, ##__VA_ARGS__);\t\t\t\t\t\\\n\t__ext4_msg(sb, \"\", \" \");\t\t\t\t\t\\\n} while (0)\n#define dump_mmp_msg(sb, mmp, msg)\t\t\t\t\t\\\n\t__dump_mmp_msg(sb, mmp, \"\", 0, \"\")\n#define ext4_grp_locked_error(sb, grp, ino, block, fmt, ...)\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tno_printk(fmt, ##__VA_ARGS__);\t\t\t\t\\\n\t__ext4_grp_locked_error(\"\", 0, sb, grp, ino, block, \" \");\t\\\n} while (0)\n\n#endif\n\nextern ext4_fsblk_t ext4_block_bitmap(struct super_block *sb,\n\t\t\t\t      struct ext4_group_desc *bg);\nextern ext4_fsblk_t ext4_inode_bitmap(struct super_block *sb,\n\t\t\t\t      struct ext4_group_desc *bg);\nextern ext4_fsblk_t ext4_inode_table(struct super_block *sb,\n\t\t\t\t     struct ext4_group_desc *bg);\nextern __u32 ext4_free_group_clusters(struct super_block *sb,\n\t\t\t\t      struct ext4_group_desc *bg);\nextern __u32 ext4_free_inodes_count(struct super_block *sb,\n\t\t\t\t struct ext4_group_desc *bg);\nextern __u32 ext4_used_dirs_count(struct super_block *sb,\n\t\t\t\tstruct ext4_group_desc *bg);\nextern __u32 ext4_itable_unused_count(struct super_block *sb,\n\t\t\t\t   struct ext4_group_desc *bg);\nextern void ext4_block_bitmap_set(struct super_block *sb,\n\t\t\t\t  struct ext4_group_desc *bg, ext4_fsblk_t blk);\nextern void ext4_inode_bitmap_set(struct super_block *sb,\n\t\t\t\t  struct ext4_group_desc *bg, ext4_fsblk_t blk);\nextern void ext4_inode_table_set(struct super_block *sb,\n\t\t\t\t struct ext4_group_desc *bg, ext4_fsblk_t blk);\nextern void ext4_free_group_clusters_set(struct super_block *sb,\n\t\t\t\t\t struct ext4_group_desc *bg,\n\t\t\t\t\t __u32 count);\nextern void ext4_free_inodes_set(struct super_block *sb,\n\t\t\t\tstruct ext4_group_desc *bg, __u32 count);\nextern void ext4_used_dirs_set(struct super_block *sb,\n\t\t\t\tstruct ext4_group_desc *bg, __u32 count);\nextern void ext4_itable_unused_set(struct super_block *sb,\n\t\t\t\t   struct ext4_group_desc *bg, __u32 count);\nextern int ext4_group_desc_csum_verify(struct super_block *sb, __u32 group,\n\t\t\t\t       struct ext4_group_desc *gdp);\nextern void ext4_group_desc_csum_set(struct super_block *sb, __u32 group,\n\t\t\t\t     struct ext4_group_desc *gdp);\nextern int ext4_register_li_request(struct super_block *sb,\n\t\t\t\t    ext4_group_t first_not_zeroed);\n\nstatic inline int ext4_has_metadata_csum(struct super_block *sb)\n{\n\tWARN_ON_ONCE(ext4_has_feature_metadata_csum(sb) &&\n\t\t     !EXT4_SB(sb)->s_chksum_driver);\n\n\treturn ext4_has_feature_metadata_csum(sb) &&\n\t       (EXT4_SB(sb)->s_chksum_driver != NULL);\n}\n\nstatic inline int ext4_has_group_desc_csum(struct super_block *sb)\n{\n\treturn ext4_has_feature_gdt_csum(sb) || ext4_has_metadata_csum(sb);\n}\n\n#define ext4_read_incompat_64bit_val(es, name) \\\n\t(((es)->s_feature_incompat & cpu_to_le32(EXT4_FEATURE_INCOMPAT_64BIT) \\\n\t\t? (ext4_fsblk_t)le32_to_cpu(es->name##_hi) << 32 : 0) | \\\n\t\tle32_to_cpu(es->name##_lo))\n\nstatic inline ext4_fsblk_t ext4_blocks_count(struct ext4_super_block *es)\n{\n\treturn ext4_read_incompat_64bit_val(es, s_blocks_count);\n}\n\nstatic inline ext4_fsblk_t ext4_r_blocks_count(struct ext4_super_block *es)\n{\n\treturn ext4_read_incompat_64bit_val(es, s_r_blocks_count);\n}\n\nstatic inline ext4_fsblk_t ext4_free_blocks_count(struct ext4_super_block *es)\n{\n\treturn ext4_read_incompat_64bit_val(es, s_free_blocks_count);\n}\n\nstatic inline void ext4_blocks_count_set(struct ext4_super_block *es,\n\t\t\t\t\t ext4_fsblk_t blk)\n{\n\tes->s_blocks_count_lo = cpu_to_le32((u32)blk);\n\tes->s_blocks_count_hi = cpu_to_le32(blk >> 32);\n}\n\nstatic inline void ext4_free_blocks_count_set(struct ext4_super_block *es,\n\t\t\t\t\t      ext4_fsblk_t blk)\n{\n\tes->s_free_blocks_count_lo = cpu_to_le32((u32)blk);\n\tes->s_free_blocks_count_hi = cpu_to_le32(blk >> 32);\n}\n\nstatic inline void ext4_r_blocks_count_set(struct ext4_super_block *es,\n\t\t\t\t\t   ext4_fsblk_t blk)\n{\n\tes->s_r_blocks_count_lo = cpu_to_le32((u32)blk);\n\tes->s_r_blocks_count_hi = cpu_to_le32(blk >> 32);\n}\n\nstatic inline loff_t ext4_isize(struct super_block *sb,\n\t\t\t\tstruct ext4_inode *raw_inode)\n{\n\tif (ext4_has_feature_largedir(sb) ||\n\t    S_ISREG(le16_to_cpu(raw_inode->i_mode)))\n\t\treturn ((loff_t)le32_to_cpu(raw_inode->i_size_high) << 32) |\n\t\t\tle32_to_cpu(raw_inode->i_size_lo);\n\n\treturn (loff_t) le32_to_cpu(raw_inode->i_size_lo);\n}\n\nstatic inline void ext4_isize_set(struct ext4_inode *raw_inode, loff_t i_size)\n{\n\traw_inode->i_size_lo = cpu_to_le32(i_size);\n\traw_inode->i_size_high = cpu_to_le32(i_size >> 32);\n}\n\nstatic inline\nstruct ext4_group_info *ext4_get_group_info(struct super_block *sb,\n\t\t\t\t\t    ext4_group_t group)\n{\n\t struct ext4_group_info **grp_info;\n\t long indexv, indexh;\n\t BUG_ON(group >= EXT4_SB(sb)->s_groups_count);\n\t indexv = group >> (EXT4_DESC_PER_BLOCK_BITS(sb));\n\t indexh = group & ((EXT4_DESC_PER_BLOCK(sb)) - 1);\n\t grp_info = sbi_array_rcu_deref(EXT4_SB(sb), s_group_info, indexv);\n\t return grp_info[indexh];\n}\n\n/*\n * Reading s_groups_count requires using smp_rmb() afterwards.  See\n * the locking protocol documented in the comments of ext4_group_add()\n * in resize.c\n */\nstatic inline ext4_group_t ext4_get_groups_count(struct super_block *sb)\n{\n\text4_group_t\tngroups = EXT4_SB(sb)->s_groups_count;\n\n\tsmp_rmb();\n\treturn ngroups;\n}\n\nstatic inline ext4_group_t ext4_flex_group(struct ext4_sb_info *sbi,\n\t\t\t\t\t     ext4_group_t block_group)\n{\n\treturn block_group >> sbi->s_log_groups_per_flex;\n}\n\nstatic inline unsigned int ext4_flex_bg_size(struct ext4_sb_info *sbi)\n{\n\treturn 1 << sbi->s_log_groups_per_flex;\n}\n\n#define ext4_std_error(sb, errno)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif ((errno))\t\t\t\t\t\t\\\n\t\t__ext4_std_error((sb), __func__, __LINE__, (errno));\t\\\n} while (0)\n\n#ifdef CONFIG_SMP\n/* Each CPU can accumulate percpu_counter_batch clusters in their local\n * counters. So we need to make sure we have free clusters more\n * than percpu_counter_batch  * nr_cpu_ids. Also add a window of 4 times.\n */\n#define EXT4_FREECLUSTERS_WATERMARK (4 * (percpu_counter_batch * nr_cpu_ids))\n#else\n#define EXT4_FREECLUSTERS_WATERMARK 0\n#endif\n\n/* Update i_disksize. Requires i_mutex to avoid races with truncate */\nstatic inline void ext4_update_i_disksize(struct inode *inode, loff_t newsize)\n{\n\tWARN_ON_ONCE(S_ISREG(inode->i_mode) &&\n\t\t     !inode_is_locked(inode));\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\tif (newsize > EXT4_I(inode)->i_disksize)\n\t\tWRITE_ONCE(EXT4_I(inode)->i_disksize, newsize);\n\tup_write(&EXT4_I(inode)->i_data_sem);\n}\n\n/* Update i_size, i_disksize. Requires i_mutex to avoid races with truncate */\nstatic inline int ext4_update_inode_size(struct inode *inode, loff_t newsize)\n{\n\tint changed = 0;\n\n\tif (newsize > inode->i_size) {\n\t\ti_size_write(inode, newsize);\n\t\tchanged = 1;\n\t}\n\tif (newsize > EXT4_I(inode)->i_disksize) {\n\t\text4_update_i_disksize(inode, newsize);\n\t\tchanged |= 2;\n\t}\n\treturn changed;\n}\n\nint ext4_update_disksize_before_punch(struct inode *inode, loff_t offset,\n\t\t\t\t      loff_t len);\n\nstruct ext4_group_info {\n\tunsigned long   bb_state;\n#ifdef AGGRESSIVE_CHECK\n\tunsigned long\tbb_check_counter;\n#endif\n\tstruct rb_root  bb_free_root;\n\text4_grpblk_t\tbb_first_free;\t/* first free block */\n\text4_grpblk_t\tbb_free;\t/* total free blocks */\n\text4_grpblk_t\tbb_fragments;\t/* nr of freespace fragments */\n\text4_grpblk_t\tbb_largest_free_order;/* order of largest frag in BG */\n\tstruct          list_head bb_prealloc_list;\n#ifdef DOUBLE_CHECK\n\tvoid            *bb_bitmap;\n#endif\n\tstruct rw_semaphore alloc_sem;\n\text4_grpblk_t\tbb_counters[];\t/* Nr of free power-of-two-block\n\t\t\t\t\t * regions, index is order.\n\t\t\t\t\t * bb_counters[3] = 5 means\n\t\t\t\t\t * 5 free 8-block regions. */\n};\n\n#define EXT4_GROUP_INFO_NEED_INIT_BIT\t\t0\n#define EXT4_GROUP_INFO_WAS_TRIMMED_BIT\t\t1\n#define EXT4_GROUP_INFO_BBITMAP_CORRUPT_BIT\t2\n#define EXT4_GROUP_INFO_IBITMAP_CORRUPT_BIT\t3\n#define EXT4_GROUP_INFO_BBITMAP_CORRUPT\t\t\\\n\t(1 << EXT4_GROUP_INFO_BBITMAP_CORRUPT_BIT)\n#define EXT4_GROUP_INFO_IBITMAP_CORRUPT\t\t\\\n\t(1 << EXT4_GROUP_INFO_IBITMAP_CORRUPT_BIT)\n#define EXT4_GROUP_INFO_BBITMAP_READ_BIT\t4\n\n#define EXT4_MB_GRP_NEED_INIT(grp)\t\\\n\t(test_bit(EXT4_GROUP_INFO_NEED_INIT_BIT, &((grp)->bb_state)))\n#define EXT4_MB_GRP_BBITMAP_CORRUPT(grp)\t\\\n\t(test_bit(EXT4_GROUP_INFO_BBITMAP_CORRUPT_BIT, &((grp)->bb_state)))\n#define EXT4_MB_GRP_IBITMAP_CORRUPT(grp)\t\\\n\t(test_bit(EXT4_GROUP_INFO_IBITMAP_CORRUPT_BIT, &((grp)->bb_state)))\n\n#define EXT4_MB_GRP_WAS_TRIMMED(grp)\t\\\n\t(test_bit(EXT4_GROUP_INFO_WAS_TRIMMED_BIT, &((grp)->bb_state)))\n#define EXT4_MB_GRP_SET_TRIMMED(grp)\t\\\n\t(set_bit(EXT4_GROUP_INFO_WAS_TRIMMED_BIT, &((grp)->bb_state)))\n#define EXT4_MB_GRP_CLEAR_TRIMMED(grp)\t\\\n\t(clear_bit(EXT4_GROUP_INFO_WAS_TRIMMED_BIT, &((grp)->bb_state)))\n#define EXT4_MB_GRP_TEST_AND_SET_READ(grp)\t\\\n\t(test_and_set_bit(EXT4_GROUP_INFO_BBITMAP_READ_BIT, &((grp)->bb_state)))\n\n#define EXT4_MAX_CONTENTION\t\t8\n#define EXT4_CONTENTION_THRESHOLD\t2\n\nstatic inline spinlock_t *ext4_group_lock_ptr(struct super_block *sb,\n\t\t\t\t\t      ext4_group_t group)\n{\n\treturn bgl_lock_ptr(EXT4_SB(sb)->s_blockgroup_lock, group);\n}\n\n/*\n * Returns true if the filesystem is busy enough that attempts to\n * access the block group locks has run into contention.\n */\nstatic inline int ext4_fs_is_busy(struct ext4_sb_info *sbi)\n{\n\treturn (atomic_read(&sbi->s_lock_busy) > EXT4_CONTENTION_THRESHOLD);\n}\n\nstatic inline void ext4_lock_group(struct super_block *sb, ext4_group_t group)\n{\n\tspinlock_t *lock = ext4_group_lock_ptr(sb, group);\n\tif (spin_trylock(lock))\n\t\t/*\n\t\t * We're able to grab the lock right away, so drop the\n\t\t * lock contention counter.\n\t\t */\n\t\tatomic_add_unless(&EXT4_SB(sb)->s_lock_busy, -1, 0);\n\telse {\n\t\t/*\n\t\t * The lock is busy, so bump the contention counter,\n\t\t * and then wait on the spin lock.\n\t\t */\n\t\tatomic_add_unless(&EXT4_SB(sb)->s_lock_busy, 1,\n\t\t\t\t  EXT4_MAX_CONTENTION);\n\t\tspin_lock(lock);\n\t}\n}\n\nstatic inline void ext4_unlock_group(struct super_block *sb,\n\t\t\t\t\text4_group_t group)\n{\n\tspin_unlock(ext4_group_lock_ptr(sb, group));\n}\n\n#ifdef CONFIG_QUOTA\nstatic inline bool ext4_quota_capable(struct super_block *sb)\n{\n\treturn (test_opt(sb, QUOTA) || ext4_has_feature_quota(sb));\n}\n\nstatic inline bool ext4_is_quota_journalled(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\treturn (ext4_has_feature_quota(sb) ||\n\t\tsbi->s_qf_names[USRQUOTA] || sbi->s_qf_names[GRPQUOTA]);\n}\n#endif\n\n/*\n * Block validity checking\n */\n#define ext4_check_indirect_blockref(inode, bh)\t\t\t\t\\\n\text4_check_blockref(__func__, __LINE__, inode,\t\t\t\\\n\t\t\t    (__le32 *)(bh)->b_data,\t\t\t\\\n\t\t\t    EXT4_ADDR_PER_BLOCK((inode)->i_sb))\n\n#define ext4_ind_check_inode(inode)\t\t\t\t\t\\\n\text4_check_blockref(__func__, __LINE__, inode,\t\t\t\\\n\t\t\t    EXT4_I(inode)->i_data,\t\t\t\\\n\t\t\t    EXT4_NDIR_BLOCKS)\n\n/*\n * Inodes and files operations\n */\n\n/* dir.c */\nextern const struct file_operations ext4_dir_operations;\n\n/* file.c */\nextern const struct inode_operations ext4_file_inode_operations;\nextern const struct file_operations ext4_file_operations;\nextern loff_t ext4_llseek(struct file *file, loff_t offset, int origin);\n\n/* inline.c */\nextern int ext4_get_max_inline_size(struct inode *inode);\nextern int ext4_find_inline_data_nolock(struct inode *inode);\nextern int ext4_init_inline_data(handle_t *handle, struct inode *inode,\n\t\t\t\t unsigned int len);\nextern int ext4_destroy_inline_data(handle_t *handle, struct inode *inode);\n\nextern int ext4_readpage_inline(struct inode *inode, struct page *page);\nextern int ext4_try_to_write_inline_data(struct address_space *mapping,\n\t\t\t\t\t struct inode *inode,\n\t\t\t\t\t loff_t pos, unsigned len,\n\t\t\t\t\t unsigned flags,\n\t\t\t\t\t struct page **pagep);\nextern int ext4_write_inline_data_end(struct inode *inode,\n\t\t\t\t      loff_t pos, unsigned len,\n\t\t\t\t      unsigned copied,\n\t\t\t\t      struct page *page);\nextern struct buffer_head *\next4_journalled_write_inline_data(struct inode *inode,\n\t\t\t\t  unsigned len,\n\t\t\t\t  struct page *page);\nextern int ext4_da_write_inline_data_begin(struct address_space *mapping,\n\t\t\t\t\t   struct inode *inode,\n\t\t\t\t\t   loff_t pos, unsigned len,\n\t\t\t\t\t   unsigned flags,\n\t\t\t\t\t   struct page **pagep,\n\t\t\t\t\t   void **fsdata);\nextern int ext4_da_write_inline_data_end(struct inode *inode, loff_t pos,\n\t\t\t\t\t unsigned len, unsigned copied,\n\t\t\t\t\t struct page *page);\nextern int ext4_try_add_inline_entry(handle_t *handle,\n\t\t\t\t     struct ext4_filename *fname,\n\t\t\t\t     struct inode *dir, struct inode *inode);\nextern int ext4_try_create_inline_dir(handle_t *handle,\n\t\t\t\t      struct inode *parent,\n\t\t\t\t      struct inode *inode);\nextern int ext4_read_inline_dir(struct file *filp,\n\t\t\t\tstruct dir_context *ctx,\n\t\t\t\tint *has_inline_data);\nextern int ext4_inlinedir_to_tree(struct file *dir_file,\n\t\t\t\t  struct inode *dir, ext4_lblk_t block,\n\t\t\t\t  struct dx_hash_info *hinfo,\n\t\t\t\t  __u32 start_hash, __u32 start_minor_hash,\n\t\t\t\t  int *has_inline_data);\nextern struct buffer_head *ext4_find_inline_entry(struct inode *dir,\n\t\t\t\t\tstruct ext4_filename *fname,\n\t\t\t\t\tstruct ext4_dir_entry_2 **res_dir,\n\t\t\t\t\tint *has_inline_data);\nextern int ext4_delete_inline_entry(handle_t *handle,\n\t\t\t\t    struct inode *dir,\n\t\t\t\t    struct ext4_dir_entry_2 *de_del,\n\t\t\t\t    struct buffer_head *bh,\n\t\t\t\t    int *has_inline_data);\nextern bool empty_inline_dir(struct inode *dir, int *has_inline_data);\nextern struct buffer_head *ext4_get_first_inline_block(struct inode *inode,\n\t\t\t\t\tstruct ext4_dir_entry_2 **parent_de,\n\t\t\t\t\tint *retval);\nextern int ext4_inline_data_fiemap(struct inode *inode,\n\t\t\t\t   struct fiemap_extent_info *fieinfo,\n\t\t\t\t   int *has_inline, __u64 start, __u64 len);\n\nstruct iomap;\nextern int ext4_inline_data_iomap(struct inode *inode, struct iomap *iomap);\n\nextern int ext4_inline_data_truncate(struct inode *inode, int *has_inline);\n\nextern int ext4_convert_inline_data(struct inode *inode);\n\nstatic inline int ext4_has_inline_data(struct inode *inode)\n{\n\treturn ext4_test_inode_flag(inode, EXT4_INODE_INLINE_DATA) &&\n\t       EXT4_I(inode)->i_inline_off;\n}\n\n/* namei.c */\nextern const struct inode_operations ext4_dir_inode_operations;\nextern const struct inode_operations ext4_special_inode_operations;\nextern struct dentry *ext4_get_parent(struct dentry *child);\nextern struct ext4_dir_entry_2 *ext4_init_dot_dotdot(struct inode *inode,\n\t\t\t\t struct ext4_dir_entry_2 *de,\n\t\t\t\t int blocksize, int csum_size,\n\t\t\t\t unsigned int parent_ino, int dotdot_real_len);\nextern void ext4_initialize_dirent_tail(struct buffer_head *bh,\n\t\t\t\t\tunsigned int blocksize);\nextern int ext4_handle_dirty_dirblock(handle_t *handle, struct inode *inode,\n\t\t\t\t      struct buffer_head *bh);\nextern int ext4_ci_compare(const struct inode *parent,\n\t\t\t   const struct qstr *fname,\n\t\t\t   const struct qstr *entry, bool quick);\nextern int __ext4_unlink(handle_t *handle, struct inode *dir, const struct qstr *d_name,\n\t\t\t struct inode *inode);\nextern int __ext4_link(struct inode *dir, struct inode *inode,\n\t\t       struct dentry *dentry);\n\n#define S_SHIFT 12\nstatic const unsigned char ext4_type_by_mode[(S_IFMT >> S_SHIFT) + 1] = {\n\t[S_IFREG >> S_SHIFT]\t= EXT4_FT_REG_FILE,\n\t[S_IFDIR >> S_SHIFT]\t= EXT4_FT_DIR,\n\t[S_IFCHR >> S_SHIFT]\t= EXT4_FT_CHRDEV,\n\t[S_IFBLK >> S_SHIFT]\t= EXT4_FT_BLKDEV,\n\t[S_IFIFO >> S_SHIFT]\t= EXT4_FT_FIFO,\n\t[S_IFSOCK >> S_SHIFT]\t= EXT4_FT_SOCK,\n\t[S_IFLNK >> S_SHIFT]\t= EXT4_FT_SYMLINK,\n};\n\nstatic inline void ext4_set_de_type(struct super_block *sb,\n\t\t\t\tstruct ext4_dir_entry_2 *de,\n\t\t\t\tumode_t mode) {\n\tif (ext4_has_feature_filetype(sb))\n\t\tde->file_type = ext4_type_by_mode[(mode & S_IFMT)>>S_SHIFT];\n}\n\n/* readpages.c */\nextern int ext4_mpage_readpages(struct inode *inode,\n\t\tstruct readahead_control *rac, struct page *page);\nextern int __init ext4_init_post_read_processing(void);\nextern void ext4_exit_post_read_processing(void);\n\n/* symlink.c */\nextern const struct inode_operations ext4_encrypted_symlink_inode_operations;\nextern const struct inode_operations ext4_symlink_inode_operations;\nextern const struct inode_operations ext4_fast_symlink_inode_operations;\n\n/* sysfs.c */\nextern int ext4_register_sysfs(struct super_block *sb);\nextern void ext4_unregister_sysfs(struct super_block *sb);\nextern int __init ext4_init_sysfs(void);\nextern void ext4_exit_sysfs(void);\n\n/* block_validity */\nextern void ext4_release_system_zone(struct super_block *sb);\nextern int ext4_setup_system_zone(struct super_block *sb);\nextern int __init ext4_init_system_zone(void);\nextern void ext4_exit_system_zone(void);\nextern int ext4_inode_block_valid(struct inode *inode,\n\t\t\t\t  ext4_fsblk_t start_blk,\n\t\t\t\t  unsigned int count);\nextern int ext4_check_blockref(const char *, unsigned int,\n\t\t\t       struct inode *, __le32 *, unsigned int);\n\n/* extents.c */\nstruct ext4_ext_path;\nstruct ext4_extent;\n\n/*\n * Maximum number of logical blocks in a file; ext4_extent's ee_block is\n * __le32.\n */\n#define EXT_MAX_BLOCKS\t0xffffffff\n\nextern void ext4_ext_tree_init(handle_t *handle, struct inode *inode);\nextern int ext4_ext_index_trans_blocks(struct inode *inode, int extents);\nextern int ext4_ext_map_blocks(handle_t *handle, struct inode *inode,\n\t\t\t       struct ext4_map_blocks *map, int flags);\nextern int ext4_ext_truncate(handle_t *, struct inode *);\nextern int ext4_ext_remove_space(struct inode *inode, ext4_lblk_t start,\n\t\t\t\t ext4_lblk_t end);\nextern void ext4_ext_init(struct super_block *);\nextern void ext4_ext_release(struct super_block *);\nextern long ext4_fallocate(struct file *file, int mode, loff_t offset,\n\t\t\t  loff_t len);\nextern int ext4_convert_unwritten_extents(handle_t *handle, struct inode *inode,\n\t\t\t\t\t  loff_t offset, ssize_t len);\nextern int ext4_convert_unwritten_io_end_vec(handle_t *handle,\n\t\t\t\t\t     ext4_io_end_t *io_end);\nextern int ext4_map_blocks(handle_t *handle, struct inode *inode,\n\t\t\t   struct ext4_map_blocks *map, int flags);\nextern int ext4_ext_calc_credits_for_single_extent(struct inode *inode,\n\t\t\t\t\t\t   int num,\n\t\t\t\t\t\t   struct ext4_ext_path *path);\nextern int ext4_ext_insert_extent(handle_t *, struct inode *,\n\t\t\t\t  struct ext4_ext_path **,\n\t\t\t\t  struct ext4_extent *, int);\nextern struct ext4_ext_path *ext4_find_extent(struct inode *, ext4_lblk_t,\n\t\t\t\t\t      struct ext4_ext_path **,\n\t\t\t\t\t      int flags);\nextern void ext4_ext_drop_refs(struct ext4_ext_path *);\nextern int ext4_ext_check_inode(struct inode *inode);\nextern ext4_lblk_t ext4_ext_next_allocated_block(struct ext4_ext_path *path);\nextern int ext4_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\t\t__u64 start, __u64 len);\nextern int ext4_get_es_cache(struct inode *inode,\n\t\t\t     struct fiemap_extent_info *fieinfo,\n\t\t\t     __u64 start, __u64 len);\nextern int ext4_ext_precache(struct inode *inode);\nextern int ext4_swap_extents(handle_t *handle, struct inode *inode1,\n\t\t\t\tstruct inode *inode2, ext4_lblk_t lblk1,\n\t\t\t     ext4_lblk_t lblk2,  ext4_lblk_t count,\n\t\t\t     int mark_unwritten,int *err);\nextern int ext4_clu_mapped(struct inode *inode, ext4_lblk_t lclu);\nextern int ext4_datasem_ensure_credits(handle_t *handle, struct inode *inode,\n\t\t\t\t       int check_cred, int restart_cred,\n\t\t\t\t       int revoke_cred);\nextern void ext4_ext_replay_shrink_inode(struct inode *inode, ext4_lblk_t end);\nextern int ext4_ext_replay_set_iblocks(struct inode *inode);\nextern int ext4_ext_replay_update_ex(struct inode *inode, ext4_lblk_t start,\n\t\tint len, int unwritten, ext4_fsblk_t pblk);\nextern int ext4_ext_clear_bb(struct inode *inode);\n\n\n/* move_extent.c */\nextern void ext4_double_down_write_data_sem(struct inode *first,\n\t\t\t\t\t    struct inode *second);\nextern void ext4_double_up_write_data_sem(struct inode *orig_inode,\n\t\t\t\t\t  struct inode *donor_inode);\nextern int ext4_move_extents(struct file *o_filp, struct file *d_filp,\n\t\t\t     __u64 start_orig, __u64 start_donor,\n\t\t\t     __u64 len, __u64 *moved_len);\n\n/* page-io.c */\nextern int __init ext4_init_pageio(void);\nextern void ext4_exit_pageio(void);\nextern ext4_io_end_t *ext4_init_io_end(struct inode *inode, gfp_t flags);\nextern ext4_io_end_t *ext4_get_io_end(ext4_io_end_t *io_end);\nextern int ext4_put_io_end(ext4_io_end_t *io_end);\nextern void ext4_put_io_end_defer(ext4_io_end_t *io_end);\nextern void ext4_io_submit_init(struct ext4_io_submit *io,\n\t\t\t\tstruct writeback_control *wbc);\nextern void ext4_end_io_rsv_work(struct work_struct *work);\nextern void ext4_io_submit(struct ext4_io_submit *io);\nextern int ext4_bio_write_page(struct ext4_io_submit *io,\n\t\t\t       struct page *page,\n\t\t\t       int len,\n\t\t\t       bool keep_towrite);\nextern struct ext4_io_end_vec *ext4_alloc_io_end_vec(ext4_io_end_t *io_end);\nextern struct ext4_io_end_vec *ext4_last_io_end_vec(ext4_io_end_t *io_end);\n\n/* mmp.c */\nextern int ext4_multi_mount_protect(struct super_block *, ext4_fsblk_t);\n\n/* verity.c */\nextern const struct fsverity_operations ext4_verityops;\n\n/*\n * Add new method to test whether block and inode bitmaps are properly\n * initialized. With uninit_bg reading the block from disk is not enough\n * to mark the bitmap uptodate. We need to also zero-out the bitmap\n */\n#define BH_BITMAP_UPTODATE BH_JBDPrivateStart\n\nstatic inline int bitmap_uptodate(struct buffer_head *bh)\n{\n\treturn (buffer_uptodate(bh) &&\n\t\t\ttest_bit(BH_BITMAP_UPTODATE, &(bh)->b_state));\n}\nstatic inline void set_bitmap_uptodate(struct buffer_head *bh)\n{\n\tset_bit(BH_BITMAP_UPTODATE, &(bh)->b_state);\n}\n\n#define in_range(b, first, len)\t((b) >= (first) && (b) <= (first) + (len) - 1)\n\n/* For ioend & aio unwritten conversion wait queues */\n#define EXT4_WQ_HASH_SZ\t\t37\n#define ext4_ioend_wq(v)   (&ext4__ioend_wq[((unsigned long)(v)) %\\\n\t\t\t\t\t    EXT4_WQ_HASH_SZ])\nextern wait_queue_head_t ext4__ioend_wq[EXT4_WQ_HASH_SZ];\n\nextern int ext4_resize_begin(struct super_block *sb);\nextern void ext4_resize_end(struct super_block *sb);\n\nstatic inline void ext4_set_io_unwritten_flag(struct inode *inode,\n\t\t\t\t\t      struct ext4_io_end *io_end)\n{\n\tif (!(io_end->flag & EXT4_IO_END_UNWRITTEN)) {\n\t\tio_end->flag |= EXT4_IO_END_UNWRITTEN;\n\t\tatomic_inc(&EXT4_I(inode)->i_unwritten);\n\t}\n}\n\nstatic inline void ext4_clear_io_unwritten_flag(ext4_io_end_t *io_end)\n{\n\tstruct inode *inode = io_end->inode;\n\n\tif (io_end->flag & EXT4_IO_END_UNWRITTEN) {\n\t\tio_end->flag &= ~EXT4_IO_END_UNWRITTEN;\n\t\t/* Wake up anyone waiting on unwritten extent conversion */\n\t\tif (atomic_dec_and_test(&EXT4_I(inode)->i_unwritten))\n\t\t\twake_up_all(ext4_ioend_wq(inode));\n\t}\n}\n\nextern const struct iomap_ops ext4_iomap_ops;\nextern const struct iomap_ops ext4_iomap_overwrite_ops;\nextern const struct iomap_ops ext4_iomap_report_ops;\n\nstatic inline int ext4_buffer_uptodate(struct buffer_head *bh)\n{\n\t/*\n\t * If the buffer has the write error flag, we have failed\n\t * to write out data in the block.  In this  case, we don't\n\t * have to read the block because we may read the old data\n\t * successfully.\n\t */\n\tif (!buffer_uptodate(bh) && buffer_write_io_error(bh))\n\t\tset_buffer_uptodate(bh);\n\treturn buffer_uptodate(bh);\n}\n\n#endif\t/* __KERNEL__ */\n\n#define EFSBADCRC\tEBADMSG\t\t/* Bad CRC detected */\n#define EFSCORRUPTED\tEUCLEAN\t\t/* Filesystem is corrupted */\n\n#endif\t/* _EXT4_H */\n"}, "7": {"id": 7, "path": "/src/include/linux/spinlock.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_SPINLOCK_H\n#define __LINUX_SPINLOCK_H\n\n/*\n * include/linux/spinlock.h - generic spinlock/rwlock declarations\n *\n * here's the role of the various spinlock/rwlock related include files:\n *\n * on SMP builds:\n *\n *  asm/spinlock_types.h: contains the arch_spinlock_t/arch_rwlock_t and the\n *                        initializers\n *\n *  linux/spinlock_types.h:\n *                        defines the generic type and initializers\n *\n *  asm/spinlock.h:       contains the arch_spin_*()/etc. lowlevel\n *                        implementations, mostly inline assembly code\n *\n *   (also included on UP-debug builds:)\n *\n *  linux/spinlock_api_smp.h:\n *                        contains the prototypes for the _spin_*() APIs.\n *\n *  linux/spinlock.h:     builds the final spin_*() APIs.\n *\n * on UP builds:\n *\n *  linux/spinlock_type_up.h:\n *                        contains the generic, simplified UP spinlock type.\n *                        (which is an empty structure on non-debug builds)\n *\n *  linux/spinlock_types.h:\n *                        defines the generic type and initializers\n *\n *  linux/spinlock_up.h:\n *                        contains the arch_spin_*()/etc. version of UP\n *                        builds. (which are NOPs on non-debug, non-preempt\n *                        builds)\n *\n *   (included on UP-non-debug builds:)\n *\n *  linux/spinlock_api_up.h:\n *                        builds the _spin_*() APIs.\n *\n *  linux/spinlock.h:     builds the final spin_*() APIs.\n */\n\n#include <linux/typecheck.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n#include <linux/compiler.h>\n#include <linux/irqflags.h>\n#include <linux/thread_info.h>\n#include <linux/kernel.h>\n#include <linux/stringify.h>\n#include <linux/bottom_half.h>\n#include <linux/lockdep.h>\n#include <asm/barrier.h>\n#include <asm/mmiowb.h>\n\n\n/*\n * Must define these before including other files, inline functions need them\n */\n#define LOCK_SECTION_NAME \".text..lock.\"KBUILD_BASENAME\n\n#define LOCK_SECTION_START(extra)               \\\n        \".subsection 1\\n\\t\"                     \\\n        extra                                   \\\n        \".ifndef \" LOCK_SECTION_NAME \"\\n\\t\"     \\\n        LOCK_SECTION_NAME \":\\n\\t\"               \\\n        \".endif\\n\"\n\n#define LOCK_SECTION_END                        \\\n        \".previous\\n\\t\"\n\n#define __lockfunc __section(\".spinlock.text\")\n\n/*\n * Pull the arch_spinlock_t and arch_rwlock_t definitions:\n */\n#include <linux/spinlock_types.h>\n\n/*\n * Pull the arch_spin*() functions/declarations (UP-nondebug doesn't need them):\n */\n#ifdef CONFIG_SMP\n# include <asm/spinlock.h>\n#else\n# include <linux/spinlock_up.h>\n#endif\n\n#ifdef CONFIG_DEBUG_SPINLOCK\n  extern void __raw_spin_lock_init(raw_spinlock_t *lock, const char *name,\n\t\t\t\t   struct lock_class_key *key, short inner);\n\n# define raw_spin_lock_init(lock)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tstatic struct lock_class_key __key;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t__raw_spin_lock_init((lock), #lock, &__key, LD_WAIT_SPIN);\t\\\n} while (0)\n\n#else\n# define raw_spin_lock_init(lock)\t\t\t\t\\\n\tdo { *(lock) = __RAW_SPIN_LOCK_UNLOCKED(lock); } while (0)\n#endif\n\n#define raw_spin_is_locked(lock)\tarch_spin_is_locked(&(lock)->raw_lock)\n\n#ifdef arch_spin_is_contended\n#define raw_spin_is_contended(lock)\tarch_spin_is_contended(&(lock)->raw_lock)\n#else\n#define raw_spin_is_contended(lock)\t(((void)(lock), 0))\n#endif /*arch_spin_is_contended*/\n\n/*\n * smp_mb__after_spinlock() provides the equivalent of a full memory barrier\n * between program-order earlier lock acquisitions and program-order later\n * memory accesses.\n *\n * This guarantees that the following two properties hold:\n *\n *   1) Given the snippet:\n *\n *\t  { X = 0;  Y = 0; }\n *\n *\t  CPU0\t\t\t\tCPU1\n *\n *\t  WRITE_ONCE(X, 1);\t\tWRITE_ONCE(Y, 1);\n *\t  spin_lock(S);\t\t\tsmp_mb();\n *\t  smp_mb__after_spinlock();\tr1 = READ_ONCE(X);\n *\t  r0 = READ_ONCE(Y);\n *\t  spin_unlock(S);\n *\n *      it is forbidden that CPU0 does not observe CPU1's store to Y (r0 = 0)\n *      and CPU1 does not observe CPU0's store to X (r1 = 0); see the comments\n *      preceding the call to smp_mb__after_spinlock() in __schedule() and in\n *      try_to_wake_up().\n *\n *   2) Given the snippet:\n *\n *  { X = 0;  Y = 0; }\n *\n *  CPU0\t\tCPU1\t\t\t\tCPU2\n *\n *  spin_lock(S);\tspin_lock(S);\t\t\tr1 = READ_ONCE(Y);\n *  WRITE_ONCE(X, 1);\tsmp_mb__after_spinlock();\tsmp_rmb();\n *  spin_unlock(S);\tr0 = READ_ONCE(X);\t\tr2 = READ_ONCE(X);\n *\t\t\tWRITE_ONCE(Y, 1);\n *\t\t\tspin_unlock(S);\n *\n *      it is forbidden that CPU0's critical section executes before CPU1's\n *      critical section (r0 = 1), CPU2 observes CPU1's store to Y (r1 = 1)\n *      and CPU2 does not observe CPU0's store to X (r2 = 0); see the comments\n *      preceding the calls to smp_rmb() in try_to_wake_up() for similar\n *      snippets but \"projected\" onto two CPUs.\n *\n * Property (2) upgrades the lock to an RCsc lock.\n *\n * Since most load-store architectures implement ACQUIRE with an smp_mb() after\n * the LL/SC loop, they need no further barriers. Similarly all our TSO\n * architectures imply an smp_mb() for each atomic instruction and equally don't\n * need more.\n *\n * Architectures that can implement ACQUIRE better need to take care.\n */\n#ifndef smp_mb__after_spinlock\n#define smp_mb__after_spinlock()\tdo { } while (0)\n#endif\n\n#ifdef CONFIG_DEBUG_SPINLOCK\n extern void do_raw_spin_lock(raw_spinlock_t *lock) __acquires(lock);\n#define do_raw_spin_lock_flags(lock, flags) do_raw_spin_lock(lock)\n extern int do_raw_spin_trylock(raw_spinlock_t *lock);\n extern void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock);\n#else\nstatic inline void do_raw_spin_lock(raw_spinlock_t *lock) __acquires(lock)\n{\n\t__acquire(lock);\n\tarch_spin_lock(&lock->raw_lock);\n\tmmiowb_spin_lock();\n}\n\n#ifndef arch_spin_lock_flags\n#define arch_spin_lock_flags(lock, flags)\tarch_spin_lock(lock)\n#endif\n\nstatic inline void\ndo_raw_spin_lock_flags(raw_spinlock_t *lock, unsigned long *flags) __acquires(lock)\n{\n\t__acquire(lock);\n\tarch_spin_lock_flags(&lock->raw_lock, *flags);\n\tmmiowb_spin_lock();\n}\n\nstatic inline int do_raw_spin_trylock(raw_spinlock_t *lock)\n{\n\tint ret = arch_spin_trylock(&(lock)->raw_lock);\n\n\tif (ret)\n\t\tmmiowb_spin_lock();\n\n\treturn ret;\n}\n\nstatic inline void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock)\n{\n\tmmiowb_spin_unlock();\n\tarch_spin_unlock(&lock->raw_lock);\n\t__release(lock);\n}\n#endif\n\n/*\n * Define the various spin_lock methods.  Note we define these\n * regardless of whether CONFIG_SMP or CONFIG_PREEMPTION are set. The\n * various methods are defined as nops in the case they are not\n * required.\n */\n#define raw_spin_trylock(lock)\t__cond_lock(lock, _raw_spin_trylock(lock))\n\n#define raw_spin_lock(lock)\t_raw_spin_lock(lock)\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n# define raw_spin_lock_nested(lock, subclass) \\\n\t_raw_spin_lock_nested(lock, subclass)\n\n# define raw_spin_lock_nest_lock(lock, nest_lock)\t\t\t\\\n\t do {\t\t\t\t\t\t\t\t\\\n\t\t typecheck(struct lockdep_map *, &(nest_lock)->dep_map);\\\n\t\t _raw_spin_lock_nest_lock(lock, &(nest_lock)->dep_map);\t\\\n\t } while (0)\n#else\n/*\n * Always evaluate the 'subclass' argument to avoid that the compiler\n * warns about set-but-not-used variables when building with\n * CONFIG_DEBUG_LOCK_ALLOC=n and with W=1.\n */\n# define raw_spin_lock_nested(lock, subclass)\t\t\\\n\t_raw_spin_lock(((void)(subclass), (lock)))\n# define raw_spin_lock_nest_lock(lock, nest_lock)\t_raw_spin_lock(lock)\n#endif\n\n#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)\n\n#define raw_spin_lock_irqsave(lock, flags)\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\\\n\t\tflags = _raw_spin_lock_irqsave(lock);\t\\\n\t} while (0)\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n#define raw_spin_lock_irqsave_nested(lock, flags, subclass)\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\t\t\\\n\t\tflags = _raw_spin_lock_irqsave_nested(lock, subclass);\t\\\n\t} while (0)\n#else\n#define raw_spin_lock_irqsave_nested(lock, flags, subclass)\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\t\t\\\n\t\tflags = _raw_spin_lock_irqsave(lock);\t\t\t\\\n\t} while (0)\n#endif\n\n#else\n\n#define raw_spin_lock_irqsave(lock, flags)\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\\\n\t\t_raw_spin_lock_irqsave(lock, flags);\t\\\n\t} while (0)\n\n#define raw_spin_lock_irqsave_nested(lock, flags, subclass)\t\\\n\traw_spin_lock_irqsave(lock, flags)\n\n#endif\n\n#define raw_spin_lock_irq(lock)\t\t_raw_spin_lock_irq(lock)\n#define raw_spin_lock_bh(lock)\t\t_raw_spin_lock_bh(lock)\n#define raw_spin_unlock(lock)\t\t_raw_spin_unlock(lock)\n#define raw_spin_unlock_irq(lock)\t_raw_spin_unlock_irq(lock)\n\n#define raw_spin_unlock_irqrestore(lock, flags)\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\t\\\n\t\t_raw_spin_unlock_irqrestore(lock, flags);\t\\\n\t} while (0)\n#define raw_spin_unlock_bh(lock)\t_raw_spin_unlock_bh(lock)\n\n#define raw_spin_trylock_bh(lock) \\\n\t__cond_lock(lock, _raw_spin_trylock_bh(lock))\n\n#define raw_spin_trylock_irq(lock) \\\n({ \\\n\tlocal_irq_disable(); \\\n\traw_spin_trylock(lock) ? \\\n\t1 : ({ local_irq_enable(); 0;  }); \\\n})\n\n#define raw_spin_trylock_irqsave(lock, flags) \\\n({ \\\n\tlocal_irq_save(flags); \\\n\traw_spin_trylock(lock) ? \\\n\t1 : ({ local_irq_restore(flags); 0; }); \\\n})\n\n/* Include rwlock functions */\n#include <linux/rwlock.h>\n\n/*\n * Pull the _spin_*()/_read_*()/_write_*() functions/declarations:\n */\n#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)\n# include <linux/spinlock_api_smp.h>\n#else\n# include <linux/spinlock_api_up.h>\n#endif\n\n/*\n * Map the spin_lock functions to the raw variants for PREEMPT_RT=n\n */\n\nstatic __always_inline raw_spinlock_t *spinlock_check(spinlock_t *lock)\n{\n\treturn &lock->rlock;\n}\n\n#ifdef CONFIG_DEBUG_SPINLOCK\n\nstatic inline void __spin_lock_init(spinlock_t *lock, const char *name,\n\t\t\t\t    struct lock_class_key *key)\n{\n\t__raw_spin_lock_init(spinlock_check(lock), name, key, LD_WAIT_CONFIG);\n}\n\n# define spin_lock_init(lock)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tstatic struct lock_class_key __key;\t\\\n\t\t\t\t\t\t\\\n\t__spin_lock_init(lock, #lock, &__key);\t\\\n} while (0)\n\n#else\n\n# define spin_lock_init(_lock)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tspinlock_check(_lock);\t\t\t\\\n\t*(_lock) = __SPIN_LOCK_UNLOCKED(_lock);\t\\\n} while (0)\n\n#endif\n\nstatic __always_inline void spin_lock(spinlock_t *lock)\n{\n\traw_spin_lock(&lock->rlock);\n}\n\nstatic __always_inline void spin_lock_bh(spinlock_t *lock)\n{\n\traw_spin_lock_bh(&lock->rlock);\n}\n\nstatic __always_inline int spin_trylock(spinlock_t *lock)\n{\n\treturn raw_spin_trylock(&lock->rlock);\n}\n\n#define spin_lock_nested(lock, subclass)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_nested(spinlock_check(lock), subclass);\t\\\n} while (0)\n\n#define spin_lock_nest_lock(lock, nest_lock)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_nest_lock(spinlock_check(lock), nest_lock);\t\\\n} while (0)\n\nstatic __always_inline void spin_lock_irq(spinlock_t *lock)\n{\n\traw_spin_lock_irq(&lock->rlock);\n}\n\n#define spin_lock_irqsave(lock, flags)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_irqsave(spinlock_check(lock), flags);\t\\\n} while (0)\n\n#define spin_lock_irqsave_nested(lock, flags, subclass)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \\\n} while (0)\n\nstatic __always_inline void spin_unlock(spinlock_t *lock)\n{\n\traw_spin_unlock(&lock->rlock);\n}\n\nstatic __always_inline void spin_unlock_bh(spinlock_t *lock)\n{\n\traw_spin_unlock_bh(&lock->rlock);\n}\n\nstatic __always_inline void spin_unlock_irq(spinlock_t *lock)\n{\n\traw_spin_unlock_irq(&lock->rlock);\n}\n\nstatic __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)\n{\n\traw_spin_unlock_irqrestore(&lock->rlock, flags);\n}\n\nstatic __always_inline int spin_trylock_bh(spinlock_t *lock)\n{\n\treturn raw_spin_trylock_bh(&lock->rlock);\n}\n\nstatic __always_inline int spin_trylock_irq(spinlock_t *lock)\n{\n\treturn raw_spin_trylock_irq(&lock->rlock);\n}\n\n#define spin_trylock_irqsave(lock, flags)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\traw_spin_trylock_irqsave(spinlock_check(lock), flags); \\\n})\n\n/**\n * spin_is_locked() - Check whether a spinlock is locked.\n * @lock: Pointer to the spinlock.\n *\n * This function is NOT required to provide any memory ordering\n * guarantees; it could be used for debugging purposes or, when\n * additional synchronization is needed, accompanied with other\n * constructs (memory barriers) enforcing the synchronization.\n *\n * Returns: 1 if @lock is locked, 0 otherwise.\n *\n * Note that the function only tells you that the spinlock is\n * seen to be locked, not that it is locked on your CPU.\n *\n * Further, on CONFIG_SMP=n builds with CONFIG_DEBUG_SPINLOCK=n,\n * the return value is always 0 (see include/linux/spinlock_up.h).\n * Therefore you should not rely heavily on the return value.\n */\nstatic __always_inline int spin_is_locked(spinlock_t *lock)\n{\n\treturn raw_spin_is_locked(&lock->rlock);\n}\n\nstatic __always_inline int spin_is_contended(spinlock_t *lock)\n{\n\treturn raw_spin_is_contended(&lock->rlock);\n}\n\n#define assert_spin_locked(lock)\tassert_raw_spin_locked(&(lock)->rlock)\n\n/*\n * Pull the atomic_t declaration:\n * (asm-mips/atomic.h needs above definitions)\n */\n#include <linux/atomic.h>\n/**\n * atomic_dec_and_lock - lock on reaching reference count zero\n * @atomic: the atomic counter\n * @lock: the spinlock in question\n *\n * Decrements @atomic by 1.  If the result is 0, returns true and locks\n * @lock.  Returns false for all other cases.\n */\nextern int _atomic_dec_and_lock(atomic_t *atomic, spinlock_t *lock);\n#define atomic_dec_and_lock(atomic, lock) \\\n\t\t__cond_lock(lock, _atomic_dec_and_lock(atomic, lock))\n\nextern int _atomic_dec_and_lock_irqsave(atomic_t *atomic, spinlock_t *lock,\n\t\t\t\t\tunsigned long *flags);\n#define atomic_dec_and_lock_irqsave(atomic, lock, flags) \\\n\t\t__cond_lock(lock, _atomic_dec_and_lock_irqsave(atomic, lock, &(flags)))\n\nint __alloc_bucket_spinlocks(spinlock_t **locks, unsigned int *lock_mask,\n\t\t\t     size_t max_size, unsigned int cpu_mult,\n\t\t\t     gfp_t gfp, const char *name,\n\t\t\t     struct lock_class_key *key);\n\n#define alloc_bucket_spinlocks(locks, lock_mask, max_size, cpu_mult, gfp)    \\\n\t({\t\t\t\t\t\t\t\t     \\\n\t\tstatic struct lock_class_key key;\t\t\t     \\\n\t\tint ret;\t\t\t\t\t\t     \\\n\t\t\t\t\t\t\t\t\t     \\\n\t\tret = __alloc_bucket_spinlocks(locks, lock_mask, max_size,   \\\n\t\t\t\t\t       cpu_mult, gfp, #locks, &key); \\\n\t\tret;\t\t\t\t\t\t\t     \\\n\t})\n\nvoid free_bucket_spinlocks(spinlock_t *locks);\n\n#endif /* __LINUX_SPINLOCK_H */\n"}, "8": {"id": 8, "path": "/src/include/linux/spinlock_api_smp.h", "content": "#ifndef __LINUX_SPINLOCK_API_SMP_H\n#define __LINUX_SPINLOCK_API_SMP_H\n\n#ifndef __LINUX_SPINLOCK_H\n# error \"please don't include this file directly\"\n#endif\n\n/*\n * include/linux/spinlock_api_smp.h\n *\n * spinlock API declarations on SMP (and debug)\n * (implemented in kernel/spinlock.c)\n *\n * portions Copyright 2005, Red Hat, Inc., Ingo Molnar\n * Released under the General Public License (GPL).\n */\n\nint in_lock_functions(unsigned long addr);\n\n#define assert_raw_spin_locked(x)\tBUG_ON(!raw_spin_is_locked(x))\n\nvoid __lockfunc _raw_spin_lock(raw_spinlock_t *lock)\t\t__acquires(lock);\nvoid __lockfunc _raw_spin_lock_nested(raw_spinlock_t *lock, int subclass)\n\t\t\t\t\t\t\t\t__acquires(lock);\nvoid __lockfunc\n_raw_spin_lock_nest_lock(raw_spinlock_t *lock, struct lockdep_map *map)\n\t\t\t\t\t\t\t\t__acquires(lock);\nvoid __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)\t\t__acquires(lock);\nvoid __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)\n\t\t\t\t\t\t\t\t__acquires(lock);\n\nunsigned long __lockfunc _raw_spin_lock_irqsave(raw_spinlock_t *lock)\n\t\t\t\t\t\t\t\t__acquires(lock);\nunsigned long __lockfunc\n_raw_spin_lock_irqsave_nested(raw_spinlock_t *lock, int subclass)\n\t\t\t\t\t\t\t\t__acquires(lock);\nint __lockfunc _raw_spin_trylock(raw_spinlock_t *lock);\nint __lockfunc _raw_spin_trylock_bh(raw_spinlock_t *lock);\nvoid __lockfunc _raw_spin_unlock(raw_spinlock_t *lock)\t\t__releases(lock);\nvoid __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)\t__releases(lock);\nvoid __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)\t__releases(lock);\nvoid __lockfunc\n_raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)\n\t\t\t\t\t\t\t\t__releases(lock);\n\n#ifdef CONFIG_INLINE_SPIN_LOCK\n#define _raw_spin_lock(lock) __raw_spin_lock(lock)\n#endif\n\n#ifdef CONFIG_INLINE_SPIN_LOCK_BH\n#define _raw_spin_lock_bh(lock) __raw_spin_lock_bh(lock)\n#endif\n\n#ifdef CONFIG_INLINE_SPIN_LOCK_IRQ\n#define _raw_spin_lock_irq(lock) __raw_spin_lock_irq(lock)\n#endif\n\n#ifdef CONFIG_INLINE_SPIN_LOCK_IRQSAVE\n#define _raw_spin_lock_irqsave(lock) __raw_spin_lock_irqsave(lock)\n#endif\n\n#ifdef CONFIG_INLINE_SPIN_TRYLOCK\n#define _raw_spin_trylock(lock) __raw_spin_trylock(lock)\n#endif\n\n#ifdef CONFIG_INLINE_SPIN_TRYLOCK_BH\n#define _raw_spin_trylock_bh(lock) __raw_spin_trylock_bh(lock)\n#endif\n\n#ifndef CONFIG_UNINLINE_SPIN_UNLOCK\n#define _raw_spin_unlock(lock) __raw_spin_unlock(lock)\n#endif\n\n#ifdef CONFIG_INLINE_SPIN_UNLOCK_BH\n#define _raw_spin_unlock_bh(lock) __raw_spin_unlock_bh(lock)\n#endif\n\n#ifdef CONFIG_INLINE_SPIN_UNLOCK_IRQ\n#define _raw_spin_unlock_irq(lock) __raw_spin_unlock_irq(lock)\n#endif\n\n#ifdef CONFIG_INLINE_SPIN_UNLOCK_IRQRESTORE\n#define _raw_spin_unlock_irqrestore(lock, flags) __raw_spin_unlock_irqrestore(lock, flags)\n#endif\n\nstatic inline int __raw_spin_trylock(raw_spinlock_t *lock)\n{\n\tpreempt_disable();\n\tif (do_raw_spin_trylock(lock)) {\n\t\tspin_acquire(&lock->dep_map, 0, 1, _RET_IP_);\n\t\treturn 1;\n\t}\n\tpreempt_enable();\n\treturn 0;\n}\n\n/*\n * If lockdep is enabled then we use the non-preemption spin-ops\n * even on CONFIG_PREEMPTION, because lockdep assumes that interrupts are\n * not re-enabled during lock-acquire (which the preempt-spin-ops do):\n */\n#if !defined(CONFIG_GENERIC_LOCKBREAK) || defined(CONFIG_DEBUG_LOCK_ALLOC)\n\nstatic inline unsigned long __raw_spin_lock_irqsave(raw_spinlock_t *lock)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\t/*\n\t * On lockdep we dont want the hand-coded irq-enable of\n\t * do_raw_spin_lock_flags() code, because lockdep assumes\n\t * that interrupts are not re-enabled during lock-acquire:\n\t */\n#ifdef CONFIG_LOCKDEP\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n#else\n\tdo_raw_spin_lock_flags(lock, &flags);\n#endif\n\treturn flags;\n}\n\nstatic inline void __raw_spin_lock_irq(raw_spinlock_t *lock)\n{\n\tlocal_irq_disable();\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n}\n\nstatic inline void __raw_spin_lock_bh(raw_spinlock_t *lock)\n{\n\t__local_bh_disable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET);\n\tspin_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n}\n\nstatic inline void __raw_spin_lock(raw_spinlock_t *lock)\n{\n\tpreempt_disable();\n\tspin_acquire(&lock->dep_map, 0, 0, _RET_IP_);\n\tLOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);\n}\n\n#endif /* !CONFIG_GENERIC_LOCKBREAK || CONFIG_DEBUG_LOCK_ALLOC */\n\nstatic inline void __raw_spin_unlock(raw_spinlock_t *lock)\n{\n\tspin_release(&lock->dep_map, _RET_IP_);\n\tdo_raw_spin_unlock(lock);\n\tpreempt_enable();\n}\n\nstatic inline void __raw_spin_unlock_irqrestore(raw_spinlock_t *lock,\n\t\t\t\t\t    unsigned long flags)\n{\n\tspin_release(&lock->dep_map, _RET_IP_);\n\tdo_raw_spin_unlock(lock);\n\tlocal_irq_restore(flags);\n\tpreempt_enable();\n}\n\nstatic inline void __raw_spin_unlock_irq(raw_spinlock_t *lock)\n{\n\tspin_release(&lock->dep_map, _RET_IP_);\n\tdo_raw_spin_unlock(lock);\n\tlocal_irq_enable();\n\tpreempt_enable();\n}\n\nstatic inline void __raw_spin_unlock_bh(raw_spinlock_t *lock)\n{\n\tspin_release(&lock->dep_map, _RET_IP_);\n\tdo_raw_spin_unlock(lock);\n\t__local_bh_enable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET);\n}\n\nstatic inline int __raw_spin_trylock_bh(raw_spinlock_t *lock)\n{\n\t__local_bh_disable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET);\n\tif (do_raw_spin_trylock(lock)) {\n\t\tspin_acquire(&lock->dep_map, 0, 1, _RET_IP_);\n\t\treturn 1;\n\t}\n\t__local_bh_enable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET);\n\treturn 0;\n}\n\n#include <linux/rwlock_api_smp.h>\n\n#endif /* __LINUX_SPINLOCK_API_SMP_H */\n"}, "9": {"id": 9, "path": "/src/arch/x86/include/asm/bug.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_X86_BUG_H\n#define _ASM_X86_BUG_H\n\n#include <linux/stringify.h>\n#include <linux/instrumentation.h>\n\n/*\n * Despite that some emulators terminate on UD2, we use it for WARN().\n *\n * Since various instruction decoders/specs disagree on the encoding of\n * UD0/UD1.\n */\n\n#define ASM_UD0\t\t\".byte 0x0f, 0xff\" /* + ModRM (for Intel) */\n#define ASM_UD1\t\t\".byte 0x0f, 0xb9\" /* + ModRM */\n#define ASM_UD2\t\t\".byte 0x0f, 0x0b\"\n\n#define INSN_UD0\t0xff0f\n#define INSN_UD2\t0x0b0f\n\n#define LEN_UD2\t\t2\n\n#ifdef CONFIG_GENERIC_BUG\n\n#ifdef CONFIG_X86_32\n# define __BUG_REL(val)\t\".long \" __stringify(val)\n#else\n# define __BUG_REL(val)\t\".long \" __stringify(val) \" - 2b\"\n#endif\n\n#ifdef CONFIG_DEBUG_BUGVERBOSE\n\n#define _BUG_FLAGS(ins, flags)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tasm_inline volatile(\"1:\\t\" ins \"\\n\"\t\t\t\t\\\n\t\t     \".pushsection __bug_table,\\\"aw\\\"\\n\"\t\t\\\n\t\t     \"2:\\t\" __BUG_REL(1b) \"\\t# bug_entry::bug_addr\\n\"\t\\\n\t\t     \"\\t\"  __BUG_REL(%c0) \"\\t# bug_entry::file\\n\"\t\\\n\t\t     \"\\t.word %c1\"        \"\\t# bug_entry::line\\n\"\t\\\n\t\t     \"\\t.word %c2\"        \"\\t# bug_entry::flags\\n\"\t\\\n\t\t     \"\\t.org 2b+%c3\\n\"\t\t\t\t\t\\\n\t\t     \".popsection\"\t\t\t\t\t\\\n\t\t     : : \"i\" (__FILE__), \"i\" (__LINE__),\t\t\\\n\t\t\t \"i\" (flags),\t\t\t\t\t\\\n\t\t\t \"i\" (sizeof(struct bug_entry)));\t\t\\\n} while (0)\n\n#else /* !CONFIG_DEBUG_BUGVERBOSE */\n\n#define _BUG_FLAGS(ins, flags)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tasm_inline volatile(\"1:\\t\" ins \"\\n\"\t\t\t\t\\\n\t\t     \".pushsection __bug_table,\\\"aw\\\"\\n\"\t\t\\\n\t\t     \"2:\\t\" __BUG_REL(1b) \"\\t# bug_entry::bug_addr\\n\"\t\\\n\t\t     \"\\t.word %c0\"        \"\\t# bug_entry::flags\\n\"\t\\\n\t\t     \"\\t.org 2b+%c1\\n\"\t\t\t\t\t\\\n\t\t     \".popsection\"\t\t\t\t\t\\\n\t\t     : : \"i\" (flags),\t\t\t\t\t\\\n\t\t\t \"i\" (sizeof(struct bug_entry)));\t\t\\\n} while (0)\n\n#endif /* CONFIG_DEBUG_BUGVERBOSE */\n\n#else\n\n#define _BUG_FLAGS(ins, flags)  asm volatile(ins)\n\n#endif /* CONFIG_GENERIC_BUG */\n\n#define HAVE_ARCH_BUG\n#define BUG()\t\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tinstrumentation_begin();\t\t\t\t\\\n\t_BUG_FLAGS(ASM_UD2, 0);\t\t\t\t\t\\\n\tunreachable();\t\t\t\t\t\t\\\n} while (0)\n\n/*\n * This instrumentation_begin() is strictly speaking incorrect; but it\n * suppresses the complaints from WARN()s in noinstr code. If such a WARN()\n * were to trigger, we'd rather wreck the machine in an attempt to get the\n * message out than not know about it.\n */\n#define __WARN_FLAGS(flags)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tinstrumentation_begin();\t\t\t\t\\\n\t_BUG_FLAGS(ASM_UD2, BUGFLAG_WARNING|(flags));\t\t\\\n\tannotate_reachable();\t\t\t\t\t\\\n\tinstrumentation_end();\t\t\t\t\t\\\n} while (0)\n\n#include <asm-generic/bug.h>\n\n#endif /* _ASM_X86_BUG_H */\n"}, "10": {"id": 10, "path": "/src/include/linux/instrumentation.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_INSTRUMENTATION_H\n#define __LINUX_INSTRUMENTATION_H\n\n#if defined(CONFIG_DEBUG_ENTRY) && defined(CONFIG_STACK_VALIDATION)\n\n/* Begin/end of an instrumentation safe region */\n#define instrumentation_begin() ({\t\t\t\t\t\\\n\tasm volatile(\"%c0: nop\\n\\t\"\t\t\t\t\t\t\\\n\t\t     \".pushsection .discard.instr_begin\\n\\t\"\t\t\\\n\t\t     \".long %c0b - .\\n\\t\"\t\t\t\t\\\n\t\t     \".popsection\\n\\t\" : : \"i\" (__COUNTER__));\t\t\\\n})\n\n/*\n * Because instrumentation_{begin,end}() can nest, objtool validation considers\n * _begin() a +1 and _end() a -1 and computes a sum over the instructions.\n * When the value is greater than 0, we consider instrumentation allowed.\n *\n * There is a problem with code like:\n *\n * noinstr void foo()\n * {\n *\tinstrumentation_begin();\n *\t...\n *\tif (cond) {\n *\t\tinstrumentation_begin();\n *\t\t...\n *\t\tinstrumentation_end();\n *\t}\n *\tbar();\n *\tinstrumentation_end();\n * }\n *\n * If instrumentation_end() would be an empty label, like all the other\n * annotations, the inner _end(), which is at the end of a conditional block,\n * would land on the instruction after the block.\n *\n * If we then consider the sum of the !cond path, we'll see that the call to\n * bar() is with a 0-value, even though, we meant it to happen with a positive\n * value.\n *\n * To avoid this, have _end() be a NOP instruction, this ensures it will be\n * part of the condition block and does not escape.\n */\n#define instrumentation_end() ({\t\t\t\t\t\\\n\tasm volatile(\"%c0: nop\\n\\t\"\t\t\t\t\t\\\n\t\t     \".pushsection .discard.instr_end\\n\\t\"\t\t\\\n\t\t     \".long %c0b - .\\n\\t\"\t\t\t\t\\\n\t\t     \".popsection\\n\\t\" : : \"i\" (__COUNTER__));\t\t\\\n})\n#else\n# define instrumentation_begin()\tdo { } while(0)\n# define instrumentation_end()\t\tdo { } while(0)\n#endif\n\n#endif /* __LINUX_INSTRUMENTATION_H */\n"}, "11": {"id": 11, "path": "/src/include/linux/list.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_LIST_H\n#define _LINUX_LIST_H\n\n#include <linux/types.h>\n#include <linux/stddef.h>\n#include <linux/poison.h>\n#include <linux/const.h>\n#include <linux/kernel.h>\n\n/*\n * Circular doubly linked list implementation.\n *\n * Some of the internal functions (\"__xxx\") are useful when\n * manipulating whole lists rather than single entries, as\n * sometimes we already know the next/prev entries and we can\n * generate better code by using them directly rather than\n * using the generic single-entry routines.\n */\n\n#define LIST_HEAD_INIT(name) { &(name), &(name) }\n\n#define LIST_HEAD(name) \\\n\tstruct list_head name = LIST_HEAD_INIT(name)\n\n/**\n * INIT_LIST_HEAD - Initialize a list_head structure\n * @list: list_head structure to be initialized.\n *\n * Initializes the list_head to point to itself.  If it is a list header,\n * the result is an empty list.\n */\nstatic inline void INIT_LIST_HEAD(struct list_head *list)\n{\n\tWRITE_ONCE(list->next, list);\n\tlist->prev = list;\n}\n\n#ifdef CONFIG_DEBUG_LIST\nextern bool __list_add_valid(struct list_head *new,\n\t\t\t      struct list_head *prev,\n\t\t\t      struct list_head *next);\nextern bool __list_del_entry_valid(struct list_head *entry);\n#else\nstatic inline bool __list_add_valid(struct list_head *new,\n\t\t\t\tstruct list_head *prev,\n\t\t\t\tstruct list_head *next)\n{\n\treturn true;\n}\nstatic inline bool __list_del_entry_valid(struct list_head *entry)\n{\n\treturn true;\n}\n#endif\n\n/*\n * Insert a new entry between two known consecutive entries.\n *\n * This is only for internal list manipulation where we know\n * the prev/next entries already!\n */\nstatic inline void __list_add(struct list_head *new,\n\t\t\t      struct list_head *prev,\n\t\t\t      struct list_head *next)\n{\n\tif (!__list_add_valid(new, prev, next))\n\t\treturn;\n\n\tnext->prev = new;\n\tnew->next = next;\n\tnew->prev = prev;\n\tWRITE_ONCE(prev->next, new);\n}\n\n/**\n * list_add - add a new entry\n * @new: new entry to be added\n * @head: list head to add it after\n *\n * Insert a new entry after the specified head.\n * This is good for implementing stacks.\n */\nstatic inline void list_add(struct list_head *new, struct list_head *head)\n{\n\t__list_add(new, head, head->next);\n}\n\n\n/**\n * list_add_tail - add a new entry\n * @new: new entry to be added\n * @head: list head to add it before\n *\n * Insert a new entry before the specified head.\n * This is useful for implementing queues.\n */\nstatic inline void list_add_tail(struct list_head *new, struct list_head *head)\n{\n\t__list_add(new, head->prev, head);\n}\n\n/*\n * Delete a list entry by making the prev/next entries\n * point to each other.\n *\n * This is only for internal list manipulation where we know\n * the prev/next entries already!\n */\nstatic inline void __list_del(struct list_head * prev, struct list_head * next)\n{\n\tnext->prev = prev;\n\tWRITE_ONCE(prev->next, next);\n}\n\n/*\n * Delete a list entry and clear the 'prev' pointer.\n *\n * This is a special-purpose list clearing method used in the networking code\n * for lists allocated as per-cpu, where we don't want to incur the extra\n * WRITE_ONCE() overhead of a regular list_del_init(). The code that uses this\n * needs to check the node 'prev' pointer instead of calling list_empty().\n */\nstatic inline void __list_del_clearprev(struct list_head *entry)\n{\n\t__list_del(entry->prev, entry->next);\n\tentry->prev = NULL;\n}\n\nstatic inline void __list_del_entry(struct list_head *entry)\n{\n\tif (!__list_del_entry_valid(entry))\n\t\treturn;\n\n\t__list_del(entry->prev, entry->next);\n}\n\n/**\n * list_del - deletes entry from list.\n * @entry: the element to delete from the list.\n * Note: list_empty() on entry does not return true after this, the entry is\n * in an undefined state.\n */\nstatic inline void list_del(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tentry->next = LIST_POISON1;\n\tentry->prev = LIST_POISON2;\n}\n\n/**\n * list_replace - replace old entry by new one\n * @old : the element to be replaced\n * @new : the new element to insert\n *\n * If @old was empty, it will be overwritten.\n */\nstatic inline void list_replace(struct list_head *old,\n\t\t\t\tstruct list_head *new)\n{\n\tnew->next = old->next;\n\tnew->next->prev = new;\n\tnew->prev = old->prev;\n\tnew->prev->next = new;\n}\n\n/**\n * list_replace_init - replace old entry by new one and initialize the old one\n * @old : the element to be replaced\n * @new : the new element to insert\n *\n * If @old was empty, it will be overwritten.\n */\nstatic inline void list_replace_init(struct list_head *old,\n\t\t\t\t     struct list_head *new)\n{\n\tlist_replace(old, new);\n\tINIT_LIST_HEAD(old);\n}\n\n/**\n * list_swap - replace entry1 with entry2 and re-add entry1 at entry2's position\n * @entry1: the location to place entry2\n * @entry2: the location to place entry1\n */\nstatic inline void list_swap(struct list_head *entry1,\n\t\t\t     struct list_head *entry2)\n{\n\tstruct list_head *pos = entry2->prev;\n\n\tlist_del(entry2);\n\tlist_replace(entry1, entry2);\n\tif (pos == entry1)\n\t\tpos = entry2;\n\tlist_add(entry1, pos);\n}\n\n/**\n * list_del_init - deletes entry from list and reinitialize it.\n * @entry: the element to delete from the list.\n */\nstatic inline void list_del_init(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tINIT_LIST_HEAD(entry);\n}\n\n/**\n * list_move - delete from one list and add as another's head\n * @list: the entry to move\n * @head: the head that will precede our entry\n */\nstatic inline void list_move(struct list_head *list, struct list_head *head)\n{\n\t__list_del_entry(list);\n\tlist_add(list, head);\n}\n\n/**\n * list_move_tail - delete from one list and add as another's tail\n * @list: the entry to move\n * @head: the head that will follow our entry\n */\nstatic inline void list_move_tail(struct list_head *list,\n\t\t\t\t  struct list_head *head)\n{\n\t__list_del_entry(list);\n\tlist_add_tail(list, head);\n}\n\n/**\n * list_bulk_move_tail - move a subsection of a list to its tail\n * @head: the head that will follow our entry\n * @first: first entry to move\n * @last: last entry to move, can be the same as first\n *\n * Move all entries between @first and including @last before @head.\n * All three entries must belong to the same linked list.\n */\nstatic inline void list_bulk_move_tail(struct list_head *head,\n\t\t\t\t       struct list_head *first,\n\t\t\t\t       struct list_head *last)\n{\n\tfirst->prev->next = last->next;\n\tlast->next->prev = first->prev;\n\n\thead->prev->next = first;\n\tfirst->prev = head->prev;\n\n\tlast->next = head;\n\thead->prev = last;\n}\n\n/**\n * list_is_first -- tests whether @list is the first entry in list @head\n * @list: the entry to test\n * @head: the head of the list\n */\nstatic inline int list_is_first(const struct list_head *list,\n\t\t\t\t\tconst struct list_head *head)\n{\n\treturn list->prev == head;\n}\n\n/**\n * list_is_last - tests whether @list is the last entry in list @head\n * @list: the entry to test\n * @head: the head of the list\n */\nstatic inline int list_is_last(const struct list_head *list,\n\t\t\t\tconst struct list_head *head)\n{\n\treturn list->next == head;\n}\n\n/**\n * list_empty - tests whether a list is empty\n * @head: the list to test.\n */\nstatic inline int list_empty(const struct list_head *head)\n{\n\treturn READ_ONCE(head->next) == head;\n}\n\n/**\n * list_del_init_careful - deletes entry from list and reinitialize it.\n * @entry: the element to delete from the list.\n *\n * This is the same as list_del_init(), except designed to be used\n * together with list_empty_careful() in a way to guarantee ordering\n * of other memory operations.\n *\n * Any memory operations done before a list_del_init_careful() are\n * guaranteed to be visible after a list_empty_careful() test.\n */\nstatic inline void list_del_init_careful(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tentry->prev = entry;\n\tsmp_store_release(&entry->next, entry);\n}\n\n/**\n * list_empty_careful - tests whether a list is empty and not being modified\n * @head: the list to test\n *\n * Description:\n * tests whether a list is empty _and_ checks that no other CPU might be\n * in the process of modifying either member (next or prev)\n *\n * NOTE: using list_empty_careful() without synchronization\n * can only be safe if the only activity that can happen\n * to the list entry is list_del_init(). Eg. it cannot be used\n * if another CPU could re-list_add() it.\n */\nstatic inline int list_empty_careful(const struct list_head *head)\n{\n\tstruct list_head *next = smp_load_acquire(&head->next);\n\treturn (next == head) && (next == head->prev);\n}\n\n/**\n * list_rotate_left - rotate the list to the left\n * @head: the head of the list\n */\nstatic inline void list_rotate_left(struct list_head *head)\n{\n\tstruct list_head *first;\n\n\tif (!list_empty(head)) {\n\t\tfirst = head->next;\n\t\tlist_move_tail(first, head);\n\t}\n}\n\n/**\n * list_rotate_to_front() - Rotate list to specific item.\n * @list: The desired new front of the list.\n * @head: The head of the list.\n *\n * Rotates list so that @list becomes the new front of the list.\n */\nstatic inline void list_rotate_to_front(struct list_head *list,\n\t\t\t\t\tstruct list_head *head)\n{\n\t/*\n\t * Deletes the list head from the list denoted by @head and\n\t * places it as the tail of @list, this effectively rotates the\n\t * list so that @list is at the front.\n\t */\n\tlist_move_tail(head, list);\n}\n\n/**\n * list_is_singular - tests whether a list has just one entry.\n * @head: the list to test.\n */\nstatic inline int list_is_singular(const struct list_head *head)\n{\n\treturn !list_empty(head) && (head->next == head->prev);\n}\n\nstatic inline void __list_cut_position(struct list_head *list,\n\t\tstruct list_head *head, struct list_head *entry)\n{\n\tstruct list_head *new_first = entry->next;\n\tlist->next = head->next;\n\tlist->next->prev = list;\n\tlist->prev = entry;\n\tentry->next = list;\n\thead->next = new_first;\n\tnew_first->prev = head;\n}\n\n/**\n * list_cut_position - cut a list into two\n * @list: a new list to add all removed entries\n * @head: a list with entries\n * @entry: an entry within head, could be the head itself\n *\tand if so we won't cut the list\n *\n * This helper moves the initial part of @head, up to and\n * including @entry, from @head to @list. You should\n * pass on @entry an element you know is on @head. @list\n * should be an empty list or a list you do not care about\n * losing its data.\n *\n */\nstatic inline void list_cut_position(struct list_head *list,\n\t\tstruct list_head *head, struct list_head *entry)\n{\n\tif (list_empty(head))\n\t\treturn;\n\tif (list_is_singular(head) &&\n\t\t(head->next != entry && head != entry))\n\t\treturn;\n\tif (entry == head)\n\t\tINIT_LIST_HEAD(list);\n\telse\n\t\t__list_cut_position(list, head, entry);\n}\n\n/**\n * list_cut_before - cut a list into two, before given entry\n * @list: a new list to add all removed entries\n * @head: a list with entries\n * @entry: an entry within head, could be the head itself\n *\n * This helper moves the initial part of @head, up to but\n * excluding @entry, from @head to @list.  You should pass\n * in @entry an element you know is on @head.  @list should\n * be an empty list or a list you do not care about losing\n * its data.\n * If @entry == @head, all entries on @head are moved to\n * @list.\n */\nstatic inline void list_cut_before(struct list_head *list,\n\t\t\t\t   struct list_head *head,\n\t\t\t\t   struct list_head *entry)\n{\n\tif (head->next == entry) {\n\t\tINIT_LIST_HEAD(list);\n\t\treturn;\n\t}\n\tlist->next = head->next;\n\tlist->next->prev = list;\n\tlist->prev = entry->prev;\n\tlist->prev->next = list;\n\thead->next = entry;\n\tentry->prev = head;\n}\n\nstatic inline void __list_splice(const struct list_head *list,\n\t\t\t\t struct list_head *prev,\n\t\t\t\t struct list_head *next)\n{\n\tstruct list_head *first = list->next;\n\tstruct list_head *last = list->prev;\n\n\tfirst->prev = prev;\n\tprev->next = first;\n\n\tlast->next = next;\n\tnext->prev = last;\n}\n\n/**\n * list_splice - join two lists, this is designed for stacks\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n */\nstatic inline void list_splice(const struct list_head *list,\n\t\t\t\tstruct list_head *head)\n{\n\tif (!list_empty(list))\n\t\t__list_splice(list, head, head->next);\n}\n\n/**\n * list_splice_tail - join two lists, each list being a queue\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n */\nstatic inline void list_splice_tail(struct list_head *list,\n\t\t\t\tstruct list_head *head)\n{\n\tif (!list_empty(list))\n\t\t__list_splice(list, head->prev, head);\n}\n\n/**\n * list_splice_init - join two lists and reinitialise the emptied list.\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n *\n * The list at @list is reinitialised\n */\nstatic inline void list_splice_init(struct list_head *list,\n\t\t\t\t    struct list_head *head)\n{\n\tif (!list_empty(list)) {\n\t\t__list_splice(list, head, head->next);\n\t\tINIT_LIST_HEAD(list);\n\t}\n}\n\n/**\n * list_splice_tail_init - join two lists and reinitialise the emptied list\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n *\n * Each of the lists is a queue.\n * The list at @list is reinitialised\n */\nstatic inline void list_splice_tail_init(struct list_head *list,\n\t\t\t\t\t struct list_head *head)\n{\n\tif (!list_empty(list)) {\n\t\t__list_splice(list, head->prev, head);\n\t\tINIT_LIST_HEAD(list);\n\t}\n}\n\n/**\n * list_entry - get the struct for this entry\n * @ptr:\tthe &struct list_head pointer.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_entry(ptr, type, member) \\\n\tcontainer_of(ptr, type, member)\n\n/**\n * list_first_entry - get the first element from a list\n * @ptr:\tthe list head to take the element from.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n *\n * Note, that list is expected to be not empty.\n */\n#define list_first_entry(ptr, type, member) \\\n\tlist_entry((ptr)->next, type, member)\n\n/**\n * list_last_entry - get the last element from a list\n * @ptr:\tthe list head to take the element from.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n *\n * Note, that list is expected to be not empty.\n */\n#define list_last_entry(ptr, type, member) \\\n\tlist_entry((ptr)->prev, type, member)\n\n/**\n * list_first_entry_or_null - get the first element from a list\n * @ptr:\tthe list head to take the element from.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n *\n * Note that if the list is empty, it returns NULL.\n */\n#define list_first_entry_or_null(ptr, type, member) ({ \\\n\tstruct list_head *head__ = (ptr); \\\n\tstruct list_head *pos__ = READ_ONCE(head__->next); \\\n\tpos__ != head__ ? list_entry(pos__, type, member) : NULL; \\\n})\n\n/**\n * list_next_entry - get the next element in list\n * @pos:\tthe type * to cursor\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_next_entry(pos, member) \\\n\tlist_entry((pos)->member.next, typeof(*(pos)), member)\n\n/**\n * list_prev_entry - get the prev element in list\n * @pos:\tthe type * to cursor\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_prev_entry(pos, member) \\\n\tlist_entry((pos)->member.prev, typeof(*(pos)), member)\n\n/**\n * list_for_each\t-\titerate over a list\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @head:\tthe head for your list.\n */\n#define list_for_each(pos, head) \\\n\tfor (pos = (head)->next; pos != (head); pos = pos->next)\n\n/**\n * list_for_each_continue - continue iteration over a list\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @head:\tthe head for your list.\n *\n * Continue to iterate over a list, continuing after the current position.\n */\n#define list_for_each_continue(pos, head) \\\n\tfor (pos = pos->next; pos != (head); pos = pos->next)\n\n/**\n * list_for_each_prev\t-\titerate over a list backwards\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @head:\tthe head for your list.\n */\n#define list_for_each_prev(pos, head) \\\n\tfor (pos = (head)->prev; pos != (head); pos = pos->prev)\n\n/**\n * list_for_each_safe - iterate over a list safe against removal of list entry\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @n:\t\tanother &struct list_head to use as temporary storage\n * @head:\tthe head for your list.\n */\n#define list_for_each_safe(pos, n, head) \\\n\tfor (pos = (head)->next, n = pos->next; pos != (head); \\\n\t\tpos = n, n = pos->next)\n\n/**\n * list_for_each_prev_safe - iterate over a list backwards safe against removal of list entry\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @n:\t\tanother &struct list_head to use as temporary storage\n * @head:\tthe head for your list.\n */\n#define list_for_each_prev_safe(pos, n, head) \\\n\tfor (pos = (head)->prev, n = pos->prev; \\\n\t     pos != (head); \\\n\t     pos = n, n = pos->prev)\n\n/**\n * list_entry_is_head - test if the entry points to the head of the list\n * @pos:\tthe type * to cursor\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_entry_is_head(pos, head, member)\t\t\t\t\\\n\t(&pos->member == (head))\n\n/**\n * list_for_each_entry\t-\titerate over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_for_each_entry(pos, head, member)\t\t\t\t\\\n\tfor (pos = list_first_entry(head, typeof(*pos), member);\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_next_entry(pos, member))\n\n/**\n * list_for_each_entry_reverse - iterate backwards over list of given type.\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_for_each_entry_reverse(pos, head, member)\t\t\t\\\n\tfor (pos = list_last_entry(head, typeof(*pos), member);\t\t\\\n\t     !list_entry_is_head(pos, head, member); \t\t\t\\\n\t     pos = list_prev_entry(pos, member))\n\n/**\n * list_prepare_entry - prepare a pos entry for use in list_for_each_entry_continue()\n * @pos:\tthe type * to use as a start point\n * @head:\tthe head of the list\n * @member:\tthe name of the list_head within the struct.\n *\n * Prepares a pos entry for use as a start point in list_for_each_entry_continue().\n */\n#define list_prepare_entry(pos, head, member) \\\n\t((pos) ? : list_entry(head, typeof(*pos), member))\n\n/**\n * list_for_each_entry_continue - continue iteration over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Continue to iterate over list of given type, continuing after\n * the current position.\n */\n#define list_for_each_entry_continue(pos, head, member) \t\t\\\n\tfor (pos = list_next_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_next_entry(pos, member))\n\n/**\n * list_for_each_entry_continue_reverse - iterate backwards from the given point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Start to iterate over list of given type backwards, continuing after\n * the current position.\n */\n#define list_for_each_entry_continue_reverse(pos, head, member)\t\t\\\n\tfor (pos = list_prev_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_prev_entry(pos, member))\n\n/**\n * list_for_each_entry_from - iterate over list of given type from the current point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate over list of given type, continuing from current position.\n */\n#define list_for_each_entry_from(pos, head, member) \t\t\t\\\n\tfor (; !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_next_entry(pos, member))\n\n/**\n * list_for_each_entry_from_reverse - iterate backwards over list of given type\n *                                    from the current point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate backwards over list of given type, continuing from current position.\n */\n#define list_for_each_entry_from_reverse(pos, head, member)\t\t\\\n\tfor (; !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_prev_entry(pos, member))\n\n/**\n * list_for_each_entry_safe - iterate over list of given type safe against removal of list entry\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_for_each_entry_safe(pos, n, head, member)\t\t\t\\\n\tfor (pos = list_first_entry(head, typeof(*pos), member),\t\\\n\t\tn = list_next_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member); \t\t\t\\\n\t     pos = n, n = list_next_entry(n, member))\n\n/**\n * list_for_each_entry_safe_continue - continue list iteration safe against removal\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate over list of given type, continuing after current point,\n * safe against removal of list entry.\n */\n#define list_for_each_entry_safe_continue(pos, n, head, member) \t\t\\\n\tfor (pos = list_next_entry(pos, member), \t\t\t\t\\\n\t\tn = list_next_entry(pos, member);\t\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\t\\\n\t     pos = n, n = list_next_entry(n, member))\n\n/**\n * list_for_each_entry_safe_from - iterate over list from current point safe against removal\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate over list of given type from current point, safe against\n * removal of list entry.\n */\n#define list_for_each_entry_safe_from(pos, n, head, member) \t\t\t\\\n\tfor (n = list_next_entry(pos, member);\t\t\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\t\\\n\t     pos = n, n = list_next_entry(n, member))\n\n/**\n * list_for_each_entry_safe_reverse - iterate backwards over list safe against removal\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate backwards over list of given type, safe against removal\n * of list entry.\n */\n#define list_for_each_entry_safe_reverse(pos, n, head, member)\t\t\\\n\tfor (pos = list_last_entry(head, typeof(*pos), member),\t\t\\\n\t\tn = list_prev_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member); \t\t\t\\\n\t     pos = n, n = list_prev_entry(n, member))\n\n/**\n * list_safe_reset_next - reset a stale list_for_each_entry_safe loop\n * @pos:\tthe loop cursor used in the list_for_each_entry_safe loop\n * @n:\t\ttemporary storage used in list_for_each_entry_safe\n * @member:\tthe name of the list_head within the struct.\n *\n * list_safe_reset_next is not safe to use in general if the list may be\n * modified concurrently (eg. the lock is dropped in the loop body). An\n * exception to this is if the cursor element (pos) is pinned in the list,\n * and list_safe_reset_next is called after re-taking the lock and before\n * completing the current iteration of the loop body.\n */\n#define list_safe_reset_next(pos, n, member)\t\t\t\t\\\n\tn = list_next_entry(pos, member)\n\n/*\n * Double linked lists with a single pointer list head.\n * Mostly useful for hash tables where the two pointer list head is\n * too wasteful.\n * You lose the ability to access the tail in O(1).\n */\n\n#define HLIST_HEAD_INIT { .first = NULL }\n#define HLIST_HEAD(name) struct hlist_head name = {  .first = NULL }\n#define INIT_HLIST_HEAD(ptr) ((ptr)->first = NULL)\nstatic inline void INIT_HLIST_NODE(struct hlist_node *h)\n{\n\th->next = NULL;\n\th->pprev = NULL;\n}\n\n/**\n * hlist_unhashed - Has node been removed from list and reinitialized?\n * @h: Node to be checked\n *\n * Not that not all removal functions will leave a node in unhashed\n * state.  For example, hlist_nulls_del_init_rcu() does leave the\n * node in unhashed state, but hlist_nulls_del() does not.\n */\nstatic inline int hlist_unhashed(const struct hlist_node *h)\n{\n\treturn !h->pprev;\n}\n\n/**\n * hlist_unhashed_lockless - Version of hlist_unhashed for lockless use\n * @h: Node to be checked\n *\n * This variant of hlist_unhashed() must be used in lockless contexts\n * to avoid potential load-tearing.  The READ_ONCE() is paired with the\n * various WRITE_ONCE() in hlist helpers that are defined below.\n */\nstatic inline int hlist_unhashed_lockless(const struct hlist_node *h)\n{\n\treturn !READ_ONCE(h->pprev);\n}\n\n/**\n * hlist_empty - Is the specified hlist_head structure an empty hlist?\n * @h: Structure to check.\n */\nstatic inline int hlist_empty(const struct hlist_head *h)\n{\n\treturn !READ_ONCE(h->first);\n}\n\nstatic inline void __hlist_del(struct hlist_node *n)\n{\n\tstruct hlist_node *next = n->next;\n\tstruct hlist_node **pprev = n->pprev;\n\n\tWRITE_ONCE(*pprev, next);\n\tif (next)\n\t\tWRITE_ONCE(next->pprev, pprev);\n}\n\n/**\n * hlist_del - Delete the specified hlist_node from its list\n * @n: Node to delete.\n *\n * Note that this function leaves the node in hashed state.  Use\n * hlist_del_init() or similar instead to unhash @n.\n */\nstatic inline void hlist_del(struct hlist_node *n)\n{\n\t__hlist_del(n);\n\tn->next = LIST_POISON1;\n\tn->pprev = LIST_POISON2;\n}\n\n/**\n * hlist_del_init - Delete the specified hlist_node from its list and initialize\n * @n: Node to delete.\n *\n * Note that this function leaves the node in unhashed state.\n */\nstatic inline void hlist_del_init(struct hlist_node *n)\n{\n\tif (!hlist_unhashed(n)) {\n\t\t__hlist_del(n);\n\t\tINIT_HLIST_NODE(n);\n\t}\n}\n\n/**\n * hlist_add_head - add a new entry at the beginning of the hlist\n * @n: new entry to be added\n * @h: hlist head to add it after\n *\n * Insert a new entry after the specified head.\n * This is good for implementing stacks.\n */\nstatic inline void hlist_add_head(struct hlist_node *n, struct hlist_head *h)\n{\n\tstruct hlist_node *first = h->first;\n\tWRITE_ONCE(n->next, first);\n\tif (first)\n\t\tWRITE_ONCE(first->pprev, &n->next);\n\tWRITE_ONCE(h->first, n);\n\tWRITE_ONCE(n->pprev, &h->first);\n}\n\n/**\n * hlist_add_before - add a new entry before the one specified\n * @n: new entry to be added\n * @next: hlist node to add it before, which must be non-NULL\n */\nstatic inline void hlist_add_before(struct hlist_node *n,\n\t\t\t\t    struct hlist_node *next)\n{\n\tWRITE_ONCE(n->pprev, next->pprev);\n\tWRITE_ONCE(n->next, next);\n\tWRITE_ONCE(next->pprev, &n->next);\n\tWRITE_ONCE(*(n->pprev), n);\n}\n\n/**\n * hlist_add_behind - add a new entry after the one specified\n * @n: new entry to be added\n * @prev: hlist node to add it after, which must be non-NULL\n */\nstatic inline void hlist_add_behind(struct hlist_node *n,\n\t\t\t\t    struct hlist_node *prev)\n{\n\tWRITE_ONCE(n->next, prev->next);\n\tWRITE_ONCE(prev->next, n);\n\tWRITE_ONCE(n->pprev, &prev->next);\n\n\tif (n->next)\n\t\tWRITE_ONCE(n->next->pprev, &n->next);\n}\n\n/**\n * hlist_add_fake - create a fake hlist consisting of a single headless node\n * @n: Node to make a fake list out of\n *\n * This makes @n appear to be its own predecessor on a headless hlist.\n * The point of this is to allow things like hlist_del() to work correctly\n * in cases where there is no list.\n */\nstatic inline void hlist_add_fake(struct hlist_node *n)\n{\n\tn->pprev = &n->next;\n}\n\n/**\n * hlist_fake: Is this node a fake hlist?\n * @h: Node to check for being a self-referential fake hlist.\n */\nstatic inline bool hlist_fake(struct hlist_node *h)\n{\n\treturn h->pprev == &h->next;\n}\n\n/**\n * hlist_is_singular_node - is node the only element of the specified hlist?\n * @n: Node to check for singularity.\n * @h: Header for potentially singular list.\n *\n * Check whether the node is the only node of the head without\n * accessing head, thus avoiding unnecessary cache misses.\n */\nstatic inline bool\nhlist_is_singular_node(struct hlist_node *n, struct hlist_head *h)\n{\n\treturn !n->next && n->pprev == &h->first;\n}\n\n/**\n * hlist_move_list - Move an hlist\n * @old: hlist_head for old list.\n * @new: hlist_head for new list.\n *\n * Move a list from one list head to another. Fixup the pprev\n * reference of the first entry if it exists.\n */\nstatic inline void hlist_move_list(struct hlist_head *old,\n\t\t\t\t   struct hlist_head *new)\n{\n\tnew->first = old->first;\n\tif (new->first)\n\t\tnew->first->pprev = &new->first;\n\told->first = NULL;\n}\n\n#define hlist_entry(ptr, type, member) container_of(ptr,type,member)\n\n#define hlist_for_each(pos, head) \\\n\tfor (pos = (head)->first; pos ; pos = pos->next)\n\n#define hlist_for_each_safe(pos, n, head) \\\n\tfor (pos = (head)->first; pos && ({ n = pos->next; 1; }); \\\n\t     pos = n)\n\n#define hlist_entry_safe(ptr, type, member) \\\n\t({ typeof(ptr) ____ptr = (ptr); \\\n\t   ____ptr ? hlist_entry(____ptr, type, member) : NULL; \\\n\t})\n\n/**\n * hlist_for_each_entry\t- iterate over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry(pos, head, member)\t\t\t\t\\\n\tfor (pos = hlist_entry_safe((head)->first, typeof(*(pos)), member);\\\n\t     pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_continue - iterate over a hlist continuing after current point\n * @pos:\tthe type * to use as a loop cursor.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_continue(pos, member)\t\t\t\\\n\tfor (pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member);\\\n\t     pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_from - iterate over a hlist continuing from current point\n * @pos:\tthe type * to use as a loop cursor.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_from(pos, member)\t\t\t\t\\\n\tfor (; pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_safe - iterate over list of given type safe against removal of list entry\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\ta &struct hlist_node to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_safe(pos, n, head, member) \t\t\\\n\tfor (pos = hlist_entry_safe((head)->first, typeof(*pos), member);\\\n\t     pos && ({ n = pos->member.next; 1; });\t\t\t\\\n\t     pos = hlist_entry_safe(n, typeof(*pos), member))\n\n#endif\n"}, "12": {"id": 12, "path": "/src/include/linux/err.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_ERR_H\n#define _LINUX_ERR_H\n\n#include <linux/compiler.h>\n#include <linux/types.h>\n\n#include <asm/errno.h>\n\n/*\n * Kernel pointers have redundant information, so we can use a\n * scheme where we can return either an error code or a normal\n * pointer with the same return value.\n *\n * This should be a per-architecture thing, to allow different\n * error and pointer decisions.\n */\n#define MAX_ERRNO\t4095\n\n#ifndef __ASSEMBLY__\n\n#define IS_ERR_VALUE(x) unlikely((unsigned long)(void *)(x) >= (unsigned long)-MAX_ERRNO)\n\nstatic inline void * __must_check ERR_PTR(long error)\n{\n\treturn (void *) error;\n}\n\nstatic inline long __must_check PTR_ERR(__force const void *ptr)\n{\n\treturn (long) ptr;\n}\n\nstatic inline bool __must_check IS_ERR(__force const void *ptr)\n{\n\treturn IS_ERR_VALUE((unsigned long)ptr);\n}\n\nstatic inline bool __must_check IS_ERR_OR_NULL(__force const void *ptr)\n{\n\treturn unlikely(!ptr) || IS_ERR_VALUE((unsigned long)ptr);\n}\n\n/**\n * ERR_CAST - Explicitly cast an error-valued pointer to another pointer type\n * @ptr: The pointer to cast.\n *\n * Explicitly cast an error-valued pointer to another pointer type in such a\n * way as to make it clear that's what's going on.\n */\nstatic inline void * __must_check ERR_CAST(__force const void *ptr)\n{\n\t/* cast away the const */\n\treturn (void *) ptr;\n}\n\nstatic inline int __must_check PTR_ERR_OR_ZERO(__force const void *ptr)\n{\n\tif (IS_ERR(ptr))\n\t\treturn PTR_ERR(ptr);\n\telse\n\t\treturn 0;\n}\n\n#endif\n\n#endif /* _LINUX_ERR_H */\n"}, "13": {"id": 13, "path": "/src/include/asm-generic/rwonce.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n/*\n * Prevent the compiler from merging or refetching reads or writes. The\n * compiler is also forbidden from reordering successive instances of\n * READ_ONCE and WRITE_ONCE, but only when the compiler is aware of some\n * particular ordering. One way to make the compiler aware of ordering is to\n * put the two invocations of READ_ONCE or WRITE_ONCE in different C\n * statements.\n *\n * These two macros will also work on aggregate data types like structs or\n * unions.\n *\n * Their two major use cases are: (1) Mediating communication between\n * process-level code and irq/NMI handlers, all running on the same CPU,\n * and (2) Ensuring that the compiler does not fold, spindle, or otherwise\n * mutilate accesses that either do not require ordering or that interact\n * with an explicit memory barrier or atomic instruction that provides the\n * required ordering.\n */\n#ifndef __ASM_GENERIC_RWONCE_H\n#define __ASM_GENERIC_RWONCE_H\n\n#ifndef __ASSEMBLY__\n\n#include <linux/compiler_types.h>\n#include <linux/kasan-checks.h>\n#include <linux/kcsan-checks.h>\n\n/*\n * Yes, this permits 64-bit accesses on 32-bit architectures. These will\n * actually be atomic in some cases (namely Armv7 + LPAE), but for others we\n * rely on the access being split into 2x32-bit accesses for a 32-bit quantity\n * (e.g. a virtual address) and a strong prevailing wind.\n */\n#define compiletime_assert_rwonce_type(t)\t\t\t\t\t\\\n\tcompiletime_assert(__native_word(t) || sizeof(t) == sizeof(long long),\t\\\n\t\t\"Unsupported access size for {READ,WRITE}_ONCE().\")\n\n/*\n * Use __READ_ONCE() instead of READ_ONCE() if you do not require any\n * atomicity. Note that this may result in tears!\n */\n#ifndef __READ_ONCE\n#define __READ_ONCE(x)\t(*(const volatile __unqual_scalar_typeof(x) *)&(x))\n#endif\n\n#define READ_ONCE(x)\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tcompiletime_assert_rwonce_type(x);\t\t\t\t\\\n\t__READ_ONCE(x);\t\t\t\t\t\t\t\\\n})\n\n#define __WRITE_ONCE(x, val)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t*(volatile typeof(x) *)&(x) = (val);\t\t\t\t\\\n} while (0)\n\n#define WRITE_ONCE(x, val)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tcompiletime_assert_rwonce_type(x);\t\t\t\t\\\n\t__WRITE_ONCE(x, val);\t\t\t\t\t\t\\\n} while (0)\n\nstatic __no_sanitize_or_inline\nunsigned long __read_once_word_nocheck(const void *addr)\n{\n\treturn __READ_ONCE(*(unsigned long *)addr);\n}\n\n/*\n * Use READ_ONCE_NOCHECK() instead of READ_ONCE() if you need to load a\n * word from memory atomically but without telling KASAN/KCSAN. This is\n * usually used by unwinding code when walking the stack of a running process.\n */\n#define READ_ONCE_NOCHECK(x)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tcompiletime_assert(sizeof(x) == sizeof(unsigned long),\t\t\\\n\t\t\"Unsupported access size for READ_ONCE_NOCHECK().\");\t\\\n\t(typeof(x))__read_once_word_nocheck(&(x));\t\t\t\\\n})\n\nstatic __no_kasan_or_inline\nunsigned long read_word_at_a_time(const void *addr)\n{\n\tkasan_check_read(addr, 1);\n\treturn *(unsigned long *)addr;\n}\n\n#endif /* __ASSEMBLY__ */\n#endif\t/* __ASM_GENERIC_RWONCE_H */\n"}, "14": {"id": 14, "path": "/src/include/linux/compiler_types.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_COMPILER_TYPES_H\n#define __LINUX_COMPILER_TYPES_H\n\n#ifndef __ASSEMBLY__\n\n#ifdef __CHECKER__\n/* address spaces */\n# define __kernel\t__attribute__((address_space(0)))\n# define __user\t\t__attribute__((noderef, address_space(__user)))\n# define __iomem\t__attribute__((noderef, address_space(__iomem)))\n# define __percpu\t__attribute__((noderef, address_space(__percpu)))\n# define __rcu\t\t__attribute__((noderef, address_space(__rcu)))\nstatic inline void __chk_user_ptr(const volatile void __user *ptr) { }\nstatic inline void __chk_io_ptr(const volatile void __iomem *ptr) { }\n/* context/locking */\n# define __must_hold(x)\t__attribute__((context(x,1,1)))\n# define __acquires(x)\t__attribute__((context(x,0,1)))\n# define __releases(x)\t__attribute__((context(x,1,0)))\n# define __acquire(x)\t__context__(x,1)\n# define __release(x)\t__context__(x,-1)\n# define __cond_lock(x,c)\t((c) ? ({ __acquire(x); 1; }) : 0)\n/* other */\n# define __force\t__attribute__((force))\n# define __nocast\t__attribute__((nocast))\n# define __safe\t\t__attribute__((safe))\n# define __private\t__attribute__((noderef))\n# define ACCESS_PRIVATE(p, member) (*((typeof((p)->member) __force *) &(p)->member))\n#else /* __CHECKER__ */\n/* address spaces */\n# define __kernel\n# ifdef STRUCTLEAK_PLUGIN\n#  define __user\t__attribute__((user))\n# else\n#  define __user\n# endif\n# define __iomem\n# define __percpu\n# define __rcu\n# define __chk_user_ptr(x)\t(void)0\n# define __chk_io_ptr(x)\t(void)0\n/* context/locking */\n# define __must_hold(x)\n# define __acquires(x)\n# define __releases(x)\n# define __acquire(x)\t(void)0\n# define __release(x)\t(void)0\n# define __cond_lock(x,c) (c)\n/* other */\n# define __force\n# define __nocast\n# define __safe\n# define __private\n# define ACCESS_PRIVATE(p, member) ((p)->member)\n# define __builtin_warning(x, y...) (1)\n#endif /* __CHECKER__ */\n\n/* Indirect macros required for expanded argument pasting, eg. __LINE__. */\n#define ___PASTE(a,b) a##b\n#define __PASTE(a,b) ___PASTE(a,b)\n\n#ifdef __KERNEL__\n\n/* Attributes */\n#include <linux/compiler_attributes.h>\n\n/* Builtins */\n\n/*\n * __has_builtin is supported on gcc >= 10, clang >= 3 and icc >= 21.\n * In the meantime, to support gcc < 10, we implement __has_builtin\n * by hand.\n */\n#ifndef __has_builtin\n#define __has_builtin(x) (0)\n#endif\n\n/* Compiler specific macros. */\n#ifdef __clang__\n#include <linux/compiler-clang.h>\n#elif defined(__INTEL_COMPILER)\n#include <linux/compiler-intel.h>\n#elif defined(__GNUC__)\n/* The above compilers also define __GNUC__, so order is important here. */\n#include <linux/compiler-gcc.h>\n#else\n#error \"Unknown compiler\"\n#endif\n\n/*\n * Some architectures need to provide custom definitions of macros provided\n * by linux/compiler-*.h, and can do so using asm/compiler.h. We include that\n * conditionally rather than using an asm-generic wrapper in order to avoid\n * build failures if any C compilation, which will include this file via an\n * -include argument in c_flags, occurs prior to the asm-generic wrappers being\n * generated.\n */\n#ifdef CONFIG_HAVE_ARCH_COMPILER_H\n#include <asm/compiler.h>\n#endif\n\nstruct ftrace_branch_data {\n\tconst char *func;\n\tconst char *file;\n\tunsigned line;\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long correct;\n\t\t\tunsigned long incorrect;\n\t\t};\n\t\tstruct {\n\t\t\tunsigned long miss;\n\t\t\tunsigned long hit;\n\t\t};\n\t\tunsigned long miss_hit[2];\n\t};\n};\n\nstruct ftrace_likely_data {\n\tstruct ftrace_branch_data\tdata;\n\tunsigned long\t\t\tconstant;\n};\n\n#if defined(CC_USING_HOTPATCH)\n#define notrace\t\t\t__attribute__((hotpatch(0, 0)))\n#elif defined(CC_USING_PATCHABLE_FUNCTION_ENTRY)\n#define notrace\t\t\t__attribute__((patchable_function_entry(0, 0)))\n#else\n#define notrace\t\t\t__attribute__((__no_instrument_function__))\n#endif\n\n/*\n * it doesn't make sense on ARM (currently the only user of __naked)\n * to trace naked functions because then mcount is called without\n * stack and frame pointer being set up and there is no chance to\n * restore the lr register to the value before mcount was called.\n */\n#define __naked\t\t\t__attribute__((__naked__)) notrace\n\n#define __compiler_offsetof(a, b)\t__builtin_offsetof(a, b)\n\n/*\n * Prefer gnu_inline, so that extern inline functions do not emit an\n * externally visible function. This makes extern inline behave as per gnu89\n * semantics rather than c99. This prevents multiple symbol definition errors\n * of extern inline functions at link time.\n * A lot of inline functions can cause havoc with function tracing.\n */\n#define inline inline __gnu_inline __inline_maybe_unused notrace\n\n/*\n * gcc provides both __inline__ and __inline as alternate spellings of\n * the inline keyword, though the latter is undocumented. New kernel\n * code should only use the inline spelling, but some existing code\n * uses __inline__. Since we #define inline above, to ensure\n * __inline__ has the same semantics, we need this #define.\n *\n * However, the spelling __inline is strictly reserved for referring\n * to the bare keyword.\n */\n#define __inline__ inline\n\n/*\n * GCC does not warn about unused static inline functions for -Wunused-function.\n * Suppress the warning in clang as well by using __maybe_unused, but enable it\n * for W=1 build. This will allow clang to find unused functions. Remove the\n * __inline_maybe_unused entirely after fixing most of -Wunused-function warnings.\n */\n#ifdef KBUILD_EXTRA_WARN1\n#define __inline_maybe_unused\n#else\n#define __inline_maybe_unused __maybe_unused\n#endif\n\n/*\n * Rather then using noinline to prevent stack consumption, use\n * noinline_for_stack instead.  For documentation reasons.\n */\n#define noinline_for_stack noinline\n\n/*\n * Sanitizer helper attributes: Because using __always_inline and\n * __no_sanitize_* conflict, provide helper attributes that will either expand\n * to __no_sanitize_* in compilation units where instrumentation is enabled\n * (__SANITIZE_*__), or __always_inline in compilation units without\n * instrumentation (__SANITIZE_*__ undefined).\n */\n#ifdef __SANITIZE_ADDRESS__\n/*\n * We can't declare function 'inline' because __no_sanitize_address conflicts\n * with inlining. Attempt to inline it may cause a build failure.\n *     https://gcc.gnu.org/bugzilla/show_bug.cgi?id=67368\n * '__maybe_unused' allows us to avoid defined-but-not-used warnings.\n */\n# define __no_kasan_or_inline __no_sanitize_address notrace __maybe_unused\n# define __no_sanitize_or_inline __no_kasan_or_inline\n#else\n# define __no_kasan_or_inline __always_inline\n#endif\n\n#define __no_kcsan __no_sanitize_thread\n#ifdef __SANITIZE_THREAD__\n# define __no_sanitize_or_inline __no_kcsan notrace __maybe_unused\n#endif\n\n#ifndef __no_sanitize_or_inline\n#define __no_sanitize_or_inline __always_inline\n#endif\n\n/* Section for code which can't be instrumented at all */\n#define noinstr\t\t\t\t\t\t\t\t\\\n\tnoinline notrace __attribute((__section__(\".noinstr.text\")))\t\\\n\t__no_kcsan __no_sanitize_address\n\n#endif /* __KERNEL__ */\n\n#endif /* __ASSEMBLY__ */\n\n/*\n * The below symbols may be defined for one or more, but not ALL, of the above\n * compilers. We don't consider that to be an error, so set them to nothing.\n * For example, some of them are for compiler specific plugins.\n */\n#ifndef __latent_entropy\n# define __latent_entropy\n#endif\n\n#ifndef __randomize_layout\n# define __randomize_layout __designated_init\n#endif\n\n#ifndef __no_randomize_layout\n# define __no_randomize_layout\n#endif\n\n#ifndef randomized_struct_fields_start\n# define randomized_struct_fields_start\n# define randomized_struct_fields_end\n#endif\n\n#ifndef __noscs\n# define __noscs\n#endif\n\n#ifndef __nocfi\n# define __nocfi\n#endif\n\n#ifndef __cficanonical\n# define __cficanonical\n#endif\n\n#ifndef asm_volatile_goto\n#define asm_volatile_goto(x...) asm goto(x)\n#endif\n\n#ifdef CONFIG_CC_HAS_ASM_INLINE\n#define asm_inline asm __inline\n#else\n#define asm_inline asm\n#endif\n\n/* Are two types/vars the same type (ignoring qualifiers)? */\n#define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b))\n\n/*\n * __unqual_scalar_typeof(x) - Declare an unqualified scalar type, leaving\n *\t\t\t       non-scalar types unchanged.\n */\n/*\n * Prefer C11 _Generic for better compile-times and simpler code. Note: 'char'\n * is not type-compatible with 'signed char', and we define a separate case.\n */\n#define __scalar_type_to_expr_cases(type)\t\t\t\t\\\n\t\tunsigned type:\t(unsigned type)0,\t\t\t\\\n\t\tsigned type:\t(signed type)0\n\n#define __unqual_scalar_typeof(x) typeof(\t\t\t\t\\\n\t\t_Generic((x),\t\t\t\t\t\t\\\n\t\t\t char:\t(char)0,\t\t\t\t\\\n\t\t\t __scalar_type_to_expr_cases(char),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(short),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(int),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(long),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(long long),\t\\\n\t\t\t default: (x)))\n\n/* Is this type a native word size -- useful for atomic operations */\n#define __native_word(t) \\\n\t(sizeof(t) == sizeof(char) || sizeof(t) == sizeof(short) || \\\n\t sizeof(t) == sizeof(int) || sizeof(t) == sizeof(long))\n\n/* Compile time object size, -1 for unknown */\n#ifndef __compiletime_object_size\n# define __compiletime_object_size(obj) -1\n#endif\n#ifndef __compiletime_warning\n# define __compiletime_warning(message)\n#endif\n#ifndef __compiletime_error\n# define __compiletime_error(message)\n#endif\n\n#ifdef __OPTIMIZE__\n# define __compiletime_assert(condition, msg, prefix, suffix)\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\textern void prefix ## suffix(void) __compiletime_error(msg); \\\n\t\tif (!(condition))\t\t\t\t\t\\\n\t\t\tprefix ## suffix();\t\t\t\t\\\n\t} while (0)\n#else\n# define __compiletime_assert(condition, msg, prefix, suffix) do { } while (0)\n#endif\n\n#define _compiletime_assert(condition, msg, prefix, suffix) \\\n\t__compiletime_assert(condition, msg, prefix, suffix)\n\n/**\n * compiletime_assert - break build and emit msg if condition is false\n * @condition: a compile-time constant condition to check\n * @msg:       a message to emit if condition is false\n *\n * In tradition of POSIX assert, this macro will break the build if the\n * supplied condition is *false*, emitting the supplied error message if the\n * compiler has support to do so.\n */\n#define compiletime_assert(condition, msg) \\\n\t_compiletime_assert(condition, msg, __compiletime_assert_, __COUNTER__)\n\n#define compiletime_assert_atomic_type(t)\t\t\t\t\\\n\tcompiletime_assert(__native_word(t),\t\t\t\t\\\n\t\t\"Need native word sized stores/loads for atomicity.\")\n\n/* Helpers for emitting diagnostics in pragmas. */\n#ifndef __diag\n#define __diag(string)\n#endif\n\n#ifndef __diag_GCC\n#define __diag_GCC(version, severity, string)\n#endif\n\n#define __diag_push()\t__diag(push)\n#define __diag_pop()\t__diag(pop)\n\n#define __diag_ignore(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, ignore, option)\n#define __diag_warn(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, warn, option)\n#define __diag_error(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, error, option)\n\n#endif /* __LINUX_COMPILER_TYPES_H */\n"}, "15": {"id": 15, "path": "/src/include/linux/rculist.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_RCULIST_H\n#define _LINUX_RCULIST_H\n\n#ifdef __KERNEL__\n\n/*\n * RCU-protected list version\n */\n#include <linux/list.h>\n#include <linux/rcupdate.h>\n\n/*\n * Why is there no list_empty_rcu()?  Because list_empty() serves this\n * purpose.  The list_empty() function fetches the RCU-protected pointer\n * and compares it to the address of the list head, but neither dereferences\n * this pointer itself nor provides this pointer to the caller.  Therefore,\n * it is not necessary to use rcu_dereference(), so that list_empty() can\n * be used anywhere you would want to use a list_empty_rcu().\n */\n\n/*\n * INIT_LIST_HEAD_RCU - Initialize a list_head visible to RCU readers\n * @list: list to be initialized\n *\n * You should instead use INIT_LIST_HEAD() for normal initialization and\n * cleanup tasks, when readers have no access to the list being initialized.\n * However, if the list being initialized is visible to readers, you\n * need to keep the compiler from being too mischievous.\n */\nstatic inline void INIT_LIST_HEAD_RCU(struct list_head *list)\n{\n\tWRITE_ONCE(list->next, list);\n\tWRITE_ONCE(list->prev, list);\n}\n\n/*\n * return the ->next pointer of a list_head in an rcu safe\n * way, we must not access it directly\n */\n#define list_next_rcu(list)\t(*((struct list_head __rcu **)(&(list)->next)))\n\n/**\n * list_tail_rcu - returns the prev pointer of the head of the list\n * @head: the head of the list\n *\n * Note: This should only be used with the list header, and even then\n * only if list_del() and similar primitives are not also used on the\n * list header.\n */\n#define list_tail_rcu(head)\t(*((struct list_head __rcu **)(&(head)->prev)))\n\n/*\n * Check during list traversal that we are within an RCU reader\n */\n\n#define check_arg_count_one(dummy)\n\n#ifdef CONFIG_PROVE_RCU_LIST\n#define __list_check_rcu(dummy, cond, extra...)\t\t\t\t\\\n\t({\t\t\t\t\t\t\t\t\\\n\tcheck_arg_count_one(extra);\t\t\t\t\t\\\n\tRCU_LOCKDEP_WARN(!(cond) && !rcu_read_lock_any_held(),\t\t\\\n\t\t\t \"RCU-list traversed in non-reader section!\");\t\\\n\t})\n\n#define __list_check_srcu(cond)\t\t\t\t\t \\\n\t({\t\t\t\t\t\t\t\t \\\n\tRCU_LOCKDEP_WARN(!(cond),\t\t\t\t\t \\\n\t\t\"RCU-list traversed without holding the required lock!\");\\\n\t})\n#else\n#define __list_check_rcu(dummy, cond, extra...)\t\t\t\t\\\n\t({ check_arg_count_one(extra); })\n\n#define __list_check_srcu(cond) ({ })\n#endif\n\n/*\n * Insert a new entry between two known consecutive entries.\n *\n * This is only for internal list manipulation where we know\n * the prev/next entries already!\n */\nstatic inline void __list_add_rcu(struct list_head *new,\n\t\tstruct list_head *prev, struct list_head *next)\n{\n\tif (!__list_add_valid(new, prev, next))\n\t\treturn;\n\n\tnew->next = next;\n\tnew->prev = prev;\n\trcu_assign_pointer(list_next_rcu(prev), new);\n\tnext->prev = new;\n}\n\n/**\n * list_add_rcu - add a new entry to rcu-protected list\n * @new: new entry to be added\n * @head: list head to add it after\n *\n * Insert a new entry after the specified head.\n * This is good for implementing stacks.\n *\n * The caller must take whatever precautions are necessary\n * (such as holding appropriate locks) to avoid racing\n * with another list-mutation primitive, such as list_add_rcu()\n * or list_del_rcu(), running on this same list.\n * However, it is perfectly legal to run concurrently with\n * the _rcu list-traversal primitives, such as\n * list_for_each_entry_rcu().\n */\nstatic inline void list_add_rcu(struct list_head *new, struct list_head *head)\n{\n\t__list_add_rcu(new, head, head->next);\n}\n\n/**\n * list_add_tail_rcu - add a new entry to rcu-protected list\n * @new: new entry to be added\n * @head: list head to add it before\n *\n * Insert a new entry before the specified head.\n * This is useful for implementing queues.\n *\n * The caller must take whatever precautions are necessary\n * (such as holding appropriate locks) to avoid racing\n * with another list-mutation primitive, such as list_add_tail_rcu()\n * or list_del_rcu(), running on this same list.\n * However, it is perfectly legal to run concurrently with\n * the _rcu list-traversal primitives, such as\n * list_for_each_entry_rcu().\n */\nstatic inline void list_add_tail_rcu(struct list_head *new,\n\t\t\t\t\tstruct list_head *head)\n{\n\t__list_add_rcu(new, head->prev, head);\n}\n\n/**\n * list_del_rcu - deletes entry from list without re-initialization\n * @entry: the element to delete from the list.\n *\n * Note: list_empty() on entry does not return true after this,\n * the entry is in an undefined state. It is useful for RCU based\n * lockfree traversal.\n *\n * In particular, it means that we can not poison the forward\n * pointers that may still be used for walking the list.\n *\n * The caller must take whatever precautions are necessary\n * (such as holding appropriate locks) to avoid racing\n * with another list-mutation primitive, such as list_del_rcu()\n * or list_add_rcu(), running on this same list.\n * However, it is perfectly legal to run concurrently with\n * the _rcu list-traversal primitives, such as\n * list_for_each_entry_rcu().\n *\n * Note that the caller is not permitted to immediately free\n * the newly deleted entry.  Instead, either synchronize_rcu()\n * or call_rcu() must be used to defer freeing until an RCU\n * grace period has elapsed.\n */\nstatic inline void list_del_rcu(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tentry->prev = LIST_POISON2;\n}\n\n/**\n * hlist_del_init_rcu - deletes entry from hash list with re-initialization\n * @n: the element to delete from the hash list.\n *\n * Note: list_unhashed() on the node return true after this. It is\n * useful for RCU based read lockfree traversal if the writer side\n * must know if the list entry is still hashed or already unhashed.\n *\n * In particular, it means that we can not poison the forward pointers\n * that may still be used for walking the hash list and we can only\n * zero the pprev pointer so list_unhashed() will return true after\n * this.\n *\n * The caller must take whatever precautions are necessary (such as\n * holding appropriate locks) to avoid racing with another\n * list-mutation primitive, such as hlist_add_head_rcu() or\n * hlist_del_rcu(), running on this same list.  However, it is\n * perfectly legal to run concurrently with the _rcu list-traversal\n * primitives, such as hlist_for_each_entry_rcu().\n */\nstatic inline void hlist_del_init_rcu(struct hlist_node *n)\n{\n\tif (!hlist_unhashed(n)) {\n\t\t__hlist_del(n);\n\t\tWRITE_ONCE(n->pprev, NULL);\n\t}\n}\n\n/**\n * list_replace_rcu - replace old entry by new one\n * @old : the element to be replaced\n * @new : the new element to insert\n *\n * The @old entry will be replaced with the @new entry atomically.\n * Note: @old should not be empty.\n */\nstatic inline void list_replace_rcu(struct list_head *old,\n\t\t\t\tstruct list_head *new)\n{\n\tnew->next = old->next;\n\tnew->prev = old->prev;\n\trcu_assign_pointer(list_next_rcu(new->prev), new);\n\tnew->next->prev = new;\n\told->prev = LIST_POISON2;\n}\n\n/**\n * __list_splice_init_rcu - join an RCU-protected list into an existing list.\n * @list:\tthe RCU-protected list to splice\n * @prev:\tpoints to the last element of the existing list\n * @next:\tpoints to the first element of the existing list\n * @sync:\tsynchronize_rcu, synchronize_rcu_expedited, ...\n *\n * The list pointed to by @prev and @next can be RCU-read traversed\n * concurrently with this function.\n *\n * Note that this function blocks.\n *\n * Important note: the caller must take whatever action is necessary to prevent\n * any other updates to the existing list.  In principle, it is possible to\n * modify the list as soon as sync() begins execution. If this sort of thing\n * becomes necessary, an alternative version based on call_rcu() could be\n * created.  But only if -really- needed -- there is no shortage of RCU API\n * members.\n */\nstatic inline void __list_splice_init_rcu(struct list_head *list,\n\t\t\t\t\t  struct list_head *prev,\n\t\t\t\t\t  struct list_head *next,\n\t\t\t\t\t  void (*sync)(void))\n{\n\tstruct list_head *first = list->next;\n\tstruct list_head *last = list->prev;\n\n\t/*\n\t * \"first\" and \"last\" tracking list, so initialize it.  RCU readers\n\t * have access to this list, so we must use INIT_LIST_HEAD_RCU()\n\t * instead of INIT_LIST_HEAD().\n\t */\n\n\tINIT_LIST_HEAD_RCU(list);\n\n\t/*\n\t * At this point, the list body still points to the source list.\n\t * Wait for any readers to finish using the list before splicing\n\t * the list body into the new list.  Any new readers will see\n\t * an empty list.\n\t */\n\n\tsync();\n\tASSERT_EXCLUSIVE_ACCESS(*first);\n\tASSERT_EXCLUSIVE_ACCESS(*last);\n\n\t/*\n\t * Readers are finished with the source list, so perform splice.\n\t * The order is important if the new list is global and accessible\n\t * to concurrent RCU readers.  Note that RCU readers are not\n\t * permitted to traverse the prev pointers without excluding\n\t * this function.\n\t */\n\n\tlast->next = next;\n\trcu_assign_pointer(list_next_rcu(prev), first);\n\tfirst->prev = prev;\n\tnext->prev = last;\n}\n\n/**\n * list_splice_init_rcu - splice an RCU-protected list into an existing list,\n *                        designed for stacks.\n * @list:\tthe RCU-protected list to splice\n * @head:\tthe place in the existing list to splice the first list into\n * @sync:\tsynchronize_rcu, synchronize_rcu_expedited, ...\n */\nstatic inline void list_splice_init_rcu(struct list_head *list,\n\t\t\t\t\tstruct list_head *head,\n\t\t\t\t\tvoid (*sync)(void))\n{\n\tif (!list_empty(list))\n\t\t__list_splice_init_rcu(list, head, head->next, sync);\n}\n\n/**\n * list_splice_tail_init_rcu - splice an RCU-protected list into an existing\n *                             list, designed for queues.\n * @list:\tthe RCU-protected list to splice\n * @head:\tthe place in the existing list to splice the first list into\n * @sync:\tsynchronize_rcu, synchronize_rcu_expedited, ...\n */\nstatic inline void list_splice_tail_init_rcu(struct list_head *list,\n\t\t\t\t\t     struct list_head *head,\n\t\t\t\t\t     void (*sync)(void))\n{\n\tif (!list_empty(list))\n\t\t__list_splice_init_rcu(list, head->prev, head, sync);\n}\n\n/**\n * list_entry_rcu - get the struct for this entry\n * @ptr:        the &struct list_head pointer.\n * @type:       the type of the struct this is embedded in.\n * @member:     the name of the list_head within the struct.\n *\n * This primitive may safely run concurrently with the _rcu list-mutation\n * primitives such as list_add_rcu() as long as it's guarded by rcu_read_lock().\n */\n#define list_entry_rcu(ptr, type, member) \\\n\tcontainer_of(READ_ONCE(ptr), type, member)\n\n/*\n * Where are list_empty_rcu() and list_first_entry_rcu()?\n *\n * Implementing those functions following their counterparts list_empty() and\n * list_first_entry() is not advisable because they lead to subtle race\n * conditions as the following snippet shows:\n *\n * if (!list_empty_rcu(mylist)) {\n *\tstruct foo *bar = list_first_entry_rcu(mylist, struct foo, list_member);\n *\tdo_something(bar);\n * }\n *\n * The list may not be empty when list_empty_rcu checks it, but it may be when\n * list_first_entry_rcu rereads the ->next pointer.\n *\n * Rereading the ->next pointer is not a problem for list_empty() and\n * list_first_entry() because they would be protected by a lock that blocks\n * writers.\n *\n * See list_first_or_null_rcu for an alternative.\n */\n\n/**\n * list_first_or_null_rcu - get the first element from a list\n * @ptr:        the list head to take the element from.\n * @type:       the type of the struct this is embedded in.\n * @member:     the name of the list_head within the struct.\n *\n * Note that if the list is empty, it returns NULL.\n *\n * This primitive may safely run concurrently with the _rcu list-mutation\n * primitives such as list_add_rcu() as long as it's guarded by rcu_read_lock().\n */\n#define list_first_or_null_rcu(ptr, type, member) \\\n({ \\\n\tstruct list_head *__ptr = (ptr); \\\n\tstruct list_head *__next = READ_ONCE(__ptr->next); \\\n\tlikely(__ptr != __next) ? list_entry_rcu(__next, type, member) : NULL; \\\n})\n\n/**\n * list_next_or_null_rcu - get the first element from a list\n * @head:\tthe head for the list.\n * @ptr:        the list head to take the next element from.\n * @type:       the type of the struct this is embedded in.\n * @member:     the name of the list_head within the struct.\n *\n * Note that if the ptr is at the end of the list, NULL is returned.\n *\n * This primitive may safely run concurrently with the _rcu list-mutation\n * primitives such as list_add_rcu() as long as it's guarded by rcu_read_lock().\n */\n#define list_next_or_null_rcu(head, ptr, type, member) \\\n({ \\\n\tstruct list_head *__head = (head); \\\n\tstruct list_head *__ptr = (ptr); \\\n\tstruct list_head *__next = READ_ONCE(__ptr->next); \\\n\tlikely(__next != __head) ? list_entry_rcu(__next, type, \\\n\t\t\t\t\t\t  member) : NULL; \\\n})\n\n/**\n * list_for_each_entry_rcu\t-\titerate over rcu list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n * @cond:\toptional lockdep expression if called from non-RCU protection.\n *\n * This list-traversal primitive may safely run concurrently with\n * the _rcu list-mutation primitives such as list_add_rcu()\n * as long as the traversal is guarded by rcu_read_lock().\n */\n#define list_for_each_entry_rcu(pos, head, member, cond...)\t\t\\\n\tfor (__list_check_rcu(dummy, ## cond, 0),\t\t\t\\\n\t     pos = list_entry_rcu((head)->next, typeof(*pos), member);\t\\\n\t\t&pos->member != (head);\t\t\t\t\t\\\n\t\tpos = list_entry_rcu(pos->member.next, typeof(*pos), member))\n\n/**\n * list_for_each_entry_srcu\t-\titerate over rcu list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n * @cond:\tlockdep expression for the lock required to traverse the list.\n *\n * This list-traversal primitive may safely run concurrently with\n * the _rcu list-mutation primitives such as list_add_rcu()\n * as long as the traversal is guarded by srcu_read_lock().\n * The lockdep expression srcu_read_lock_held() can be passed as the\n * cond argument from read side.\n */\n#define list_for_each_entry_srcu(pos, head, member, cond)\t\t\\\n\tfor (__list_check_srcu(cond),\t\t\t\t\t\\\n\t     pos = list_entry_rcu((head)->next, typeof(*pos), member);\t\\\n\t\t&pos->member != (head);\t\t\t\t\t\\\n\t\tpos = list_entry_rcu(pos->member.next, typeof(*pos), member))\n\n/**\n * list_entry_lockless - get the struct for this entry\n * @ptr:        the &struct list_head pointer.\n * @type:       the type of the struct this is embedded in.\n * @member:     the name of the list_head within the struct.\n *\n * This primitive may safely run concurrently with the _rcu\n * list-mutation primitives such as list_add_rcu(), but requires some\n * implicit RCU read-side guarding.  One example is running within a special\n * exception-time environment where preemption is disabled and where lockdep\n * cannot be invoked.  Another example is when items are added to the list,\n * but never deleted.\n */\n#define list_entry_lockless(ptr, type, member) \\\n\tcontainer_of((typeof(ptr))READ_ONCE(ptr), type, member)\n\n/**\n * list_for_each_entry_lockless - iterate over rcu list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_struct within the struct.\n *\n * This primitive may safely run concurrently with the _rcu\n * list-mutation primitives such as list_add_rcu(), but requires some\n * implicit RCU read-side guarding.  One example is running within a special\n * exception-time environment where preemption is disabled and where lockdep\n * cannot be invoked.  Another example is when items are added to the list,\n * but never deleted.\n */\n#define list_for_each_entry_lockless(pos, head, member) \\\n\tfor (pos = list_entry_lockless((head)->next, typeof(*pos), member); \\\n\t     &pos->member != (head); \\\n\t     pos = list_entry_lockless(pos->member.next, typeof(*pos), member))\n\n/**\n * list_for_each_entry_continue_rcu - continue iteration over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Continue to iterate over list of given type, continuing after\n * the current position which must have been in the list when the RCU read\n * lock was taken.\n * This would typically require either that you obtained the node from a\n * previous walk of the list in the same RCU read-side critical section, or\n * that you held some sort of non-RCU reference (such as a reference count)\n * to keep the node alive *and* in the list.\n *\n * This iterator is similar to list_for_each_entry_from_rcu() except\n * this starts after the given position and that one starts at the given\n * position.\n */\n#define list_for_each_entry_continue_rcu(pos, head, member) \t\t\\\n\tfor (pos = list_entry_rcu(pos->member.next, typeof(*pos), member); \\\n\t     &pos->member != (head);\t\\\n\t     pos = list_entry_rcu(pos->member.next, typeof(*pos), member))\n\n/**\n * list_for_each_entry_from_rcu - iterate over a list from current point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_node within the struct.\n *\n * Iterate over the tail of a list starting from a given position,\n * which must have been in the list when the RCU read lock was taken.\n * This would typically require either that you obtained the node from a\n * previous walk of the list in the same RCU read-side critical section, or\n * that you held some sort of non-RCU reference (such as a reference count)\n * to keep the node alive *and* in the list.\n *\n * This iterator is similar to list_for_each_entry_continue_rcu() except\n * this starts from the given position and that one starts from the position\n * after the given position.\n */\n#define list_for_each_entry_from_rcu(pos, head, member)\t\t\t\\\n\tfor (; &(pos)->member != (head);\t\t\t\t\t\\\n\t\tpos = list_entry_rcu(pos->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_del_rcu - deletes entry from hash list without re-initialization\n * @n: the element to delete from the hash list.\n *\n * Note: list_unhashed() on entry does not return true after this,\n * the entry is in an undefined state. It is useful for RCU based\n * lockfree traversal.\n *\n * In particular, it means that we can not poison the forward\n * pointers that may still be used for walking the hash list.\n *\n * The caller must take whatever precautions are necessary\n * (such as holding appropriate locks) to avoid racing\n * with another list-mutation primitive, such as hlist_add_head_rcu()\n * or hlist_del_rcu(), running on this same list.\n * However, it is perfectly legal to run concurrently with\n * the _rcu list-traversal primitives, such as\n * hlist_for_each_entry().\n */\nstatic inline void hlist_del_rcu(struct hlist_node *n)\n{\n\t__hlist_del(n);\n\tWRITE_ONCE(n->pprev, LIST_POISON2);\n}\n\n/**\n * hlist_replace_rcu - replace old entry by new one\n * @old : the element to be replaced\n * @new : the new element to insert\n *\n * The @old entry will be replaced with the @new entry atomically.\n */\nstatic inline void hlist_replace_rcu(struct hlist_node *old,\n\t\t\t\t\tstruct hlist_node *new)\n{\n\tstruct hlist_node *next = old->next;\n\n\tnew->next = next;\n\tWRITE_ONCE(new->pprev, old->pprev);\n\trcu_assign_pointer(*(struct hlist_node __rcu **)new->pprev, new);\n\tif (next)\n\t\tWRITE_ONCE(new->next->pprev, &new->next);\n\tWRITE_ONCE(old->pprev, LIST_POISON2);\n}\n\n/**\n * hlists_swap_heads_rcu - swap the lists the hlist heads point to\n * @left:  The hlist head on the left\n * @right: The hlist head on the right\n *\n * The lists start out as [@left  ][node1 ... ] and\n *                        [@right ][node2 ... ]\n * The lists end up as    [@left  ][node2 ... ]\n *                        [@right ][node1 ... ]\n */\nstatic inline void hlists_swap_heads_rcu(struct hlist_head *left, struct hlist_head *right)\n{\n\tstruct hlist_node *node1 = left->first;\n\tstruct hlist_node *node2 = right->first;\n\n\trcu_assign_pointer(left->first, node2);\n\trcu_assign_pointer(right->first, node1);\n\tWRITE_ONCE(node2->pprev, &left->first);\n\tWRITE_ONCE(node1->pprev, &right->first);\n}\n\n/*\n * return the first or the next element in an RCU protected hlist\n */\n#define hlist_first_rcu(head)\t(*((struct hlist_node __rcu **)(&(head)->first)))\n#define hlist_next_rcu(node)\t(*((struct hlist_node __rcu **)(&(node)->next)))\n#define hlist_pprev_rcu(node)\t(*((struct hlist_node __rcu **)((node)->pprev)))\n\n/**\n * hlist_add_head_rcu\n * @n: the element to add to the hash list.\n * @h: the list to add to.\n *\n * Description:\n * Adds the specified element to the specified hlist,\n * while permitting racing traversals.\n *\n * The caller must take whatever precautions are necessary\n * (such as holding appropriate locks) to avoid racing\n * with another list-mutation primitive, such as hlist_add_head_rcu()\n * or hlist_del_rcu(), running on this same list.\n * However, it is perfectly legal to run concurrently with\n * the _rcu list-traversal primitives, such as\n * hlist_for_each_entry_rcu(), used to prevent memory-consistency\n * problems on Alpha CPUs.  Regardless of the type of CPU, the\n * list-traversal primitive must be guarded by rcu_read_lock().\n */\nstatic inline void hlist_add_head_rcu(struct hlist_node *n,\n\t\t\t\t\tstruct hlist_head *h)\n{\n\tstruct hlist_node *first = h->first;\n\n\tn->next = first;\n\tWRITE_ONCE(n->pprev, &h->first);\n\trcu_assign_pointer(hlist_first_rcu(h), n);\n\tif (first)\n\t\tWRITE_ONCE(first->pprev, &n->next);\n}\n\n/**\n * hlist_add_tail_rcu\n * @n: the element to add to the hash list.\n * @h: the list to add to.\n *\n * Description:\n * Adds the specified element to the specified hlist,\n * while permitting racing traversals.\n *\n * The caller must take whatever precautions are necessary\n * (such as holding appropriate locks) to avoid racing\n * with another list-mutation primitive, such as hlist_add_head_rcu()\n * or hlist_del_rcu(), running on this same list.\n * However, it is perfectly legal to run concurrently with\n * the _rcu list-traversal primitives, such as\n * hlist_for_each_entry_rcu(), used to prevent memory-consistency\n * problems on Alpha CPUs.  Regardless of the type of CPU, the\n * list-traversal primitive must be guarded by rcu_read_lock().\n */\nstatic inline void hlist_add_tail_rcu(struct hlist_node *n,\n\t\t\t\t      struct hlist_head *h)\n{\n\tstruct hlist_node *i, *last = NULL;\n\n\t/* Note: write side code, so rcu accessors are not needed. */\n\tfor (i = h->first; i; i = i->next)\n\t\tlast = i;\n\n\tif (last) {\n\t\tn->next = last->next;\n\t\tWRITE_ONCE(n->pprev, &last->next);\n\t\trcu_assign_pointer(hlist_next_rcu(last), n);\n\t} else {\n\t\thlist_add_head_rcu(n, h);\n\t}\n}\n\n/**\n * hlist_add_before_rcu\n * @n: the new element to add to the hash list.\n * @next: the existing element to add the new element before.\n *\n * Description:\n * Adds the specified element to the specified hlist\n * before the specified node while permitting racing traversals.\n *\n * The caller must take whatever precautions are necessary\n * (such as holding appropriate locks) to avoid racing\n * with another list-mutation primitive, such as hlist_add_head_rcu()\n * or hlist_del_rcu(), running on this same list.\n * However, it is perfectly legal to run concurrently with\n * the _rcu list-traversal primitives, such as\n * hlist_for_each_entry_rcu(), used to prevent memory-consistency\n * problems on Alpha CPUs.\n */\nstatic inline void hlist_add_before_rcu(struct hlist_node *n,\n\t\t\t\t\tstruct hlist_node *next)\n{\n\tWRITE_ONCE(n->pprev, next->pprev);\n\tn->next = next;\n\trcu_assign_pointer(hlist_pprev_rcu(n), n);\n\tWRITE_ONCE(next->pprev, &n->next);\n}\n\n/**\n * hlist_add_behind_rcu\n * @n: the new element to add to the hash list.\n * @prev: the existing element to add the new element after.\n *\n * Description:\n * Adds the specified element to the specified hlist\n * after the specified node while permitting racing traversals.\n *\n * The caller must take whatever precautions are necessary\n * (such as holding appropriate locks) to avoid racing\n * with another list-mutation primitive, such as hlist_add_head_rcu()\n * or hlist_del_rcu(), running on this same list.\n * However, it is perfectly legal to run concurrently with\n * the _rcu list-traversal primitives, such as\n * hlist_for_each_entry_rcu(), used to prevent memory-consistency\n * problems on Alpha CPUs.\n */\nstatic inline void hlist_add_behind_rcu(struct hlist_node *n,\n\t\t\t\t\tstruct hlist_node *prev)\n{\n\tn->next = prev->next;\n\tWRITE_ONCE(n->pprev, &prev->next);\n\trcu_assign_pointer(hlist_next_rcu(prev), n);\n\tif (n->next)\n\t\tWRITE_ONCE(n->next->pprev, &n->next);\n}\n\n#define __hlist_for_each_rcu(pos, head)\t\t\t\t\\\n\tfor (pos = rcu_dereference(hlist_first_rcu(head));\t\\\n\t     pos;\t\t\t\t\t\t\\\n\t     pos = rcu_dereference(hlist_next_rcu(pos)))\n\n/**\n * hlist_for_each_entry_rcu - iterate over rcu list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the hlist_node within the struct.\n * @cond:\toptional lockdep expression if called from non-RCU protection.\n *\n * This list-traversal primitive may safely run concurrently with\n * the _rcu list-mutation primitives such as hlist_add_head_rcu()\n * as long as the traversal is guarded by rcu_read_lock().\n */\n#define hlist_for_each_entry_rcu(pos, head, member, cond...)\t\t\\\n\tfor (__list_check_rcu(dummy, ## cond, 0),\t\t\t\\\n\t     pos = hlist_entry_safe(rcu_dereference_raw(hlist_first_rcu(head)),\\\n\t\t\ttypeof(*(pos)), member);\t\t\t\\\n\t\tpos;\t\t\t\t\t\t\t\\\n\t\tpos = hlist_entry_safe(rcu_dereference_raw(hlist_next_rcu(\\\n\t\t\t&(pos)->member)), typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_srcu - iterate over rcu list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the hlist_node within the struct.\n * @cond:\tlockdep expression for the lock required to traverse the list.\n *\n * This list-traversal primitive may safely run concurrently with\n * the _rcu list-mutation primitives such as hlist_add_head_rcu()\n * as long as the traversal is guarded by srcu_read_lock().\n * The lockdep expression srcu_read_lock_held() can be passed as the\n * cond argument from read side.\n */\n#define hlist_for_each_entry_srcu(pos, head, member, cond)\t\t\\\n\tfor (__list_check_srcu(cond),\t\t\t\t\t\\\n\t     pos = hlist_entry_safe(rcu_dereference_raw(hlist_first_rcu(head)),\\\n\t\t\ttypeof(*(pos)), member);\t\t\t\\\n\t\tpos;\t\t\t\t\t\t\t\\\n\t\tpos = hlist_entry_safe(rcu_dereference_raw(hlist_next_rcu(\\\n\t\t\t&(pos)->member)), typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_rcu_notrace - iterate over rcu list of given type (for tracing)\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the hlist_node within the struct.\n *\n * This list-traversal primitive may safely run concurrently with\n * the _rcu list-mutation primitives such as hlist_add_head_rcu()\n * as long as the traversal is guarded by rcu_read_lock().\n *\n * This is the same as hlist_for_each_entry_rcu() except that it does\n * not do any RCU debugging or tracing.\n */\n#define hlist_for_each_entry_rcu_notrace(pos, head, member)\t\t\t\\\n\tfor (pos = hlist_entry_safe(rcu_dereference_raw_check(hlist_first_rcu(head)),\\\n\t\t\ttypeof(*(pos)), member);\t\t\t\\\n\t\tpos;\t\t\t\t\t\t\t\\\n\t\tpos = hlist_entry_safe(rcu_dereference_raw_check(hlist_next_rcu(\\\n\t\t\t&(pos)->member)), typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_rcu_bh - iterate over rcu list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the hlist_node within the struct.\n *\n * This list-traversal primitive may safely run concurrently with\n * the _rcu list-mutation primitives such as hlist_add_head_rcu()\n * as long as the traversal is guarded by rcu_read_lock().\n */\n#define hlist_for_each_entry_rcu_bh(pos, head, member)\t\t\t\\\n\tfor (pos = hlist_entry_safe(rcu_dereference_bh(hlist_first_rcu(head)),\\\n\t\t\ttypeof(*(pos)), member);\t\t\t\\\n\t\tpos;\t\t\t\t\t\t\t\\\n\t\tpos = hlist_entry_safe(rcu_dereference_bh(hlist_next_rcu(\\\n\t\t\t&(pos)->member)), typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_continue_rcu - iterate over a hlist continuing after current point\n * @pos:\tthe type * to use as a loop cursor.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_continue_rcu(pos, member)\t\t\t\\\n\tfor (pos = hlist_entry_safe(rcu_dereference_raw(hlist_next_rcu( \\\n\t\t\t&(pos)->member)), typeof(*(pos)), member);\t\\\n\t     pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe(rcu_dereference_raw(hlist_next_rcu(\t\\\n\t\t\t&(pos)->member)), typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_continue_rcu_bh - iterate over a hlist continuing after current point\n * @pos:\tthe type * to use as a loop cursor.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_continue_rcu_bh(pos, member)\t\t\\\n\tfor (pos = hlist_entry_safe(rcu_dereference_bh(hlist_next_rcu(  \\\n\t\t\t&(pos)->member)), typeof(*(pos)), member);\t\\\n\t     pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe(rcu_dereference_bh(hlist_next_rcu(\t\\\n\t\t\t&(pos)->member)), typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_from_rcu - iterate over a hlist continuing from current point\n * @pos:\tthe type * to use as a loop cursor.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_from_rcu(pos, member)\t\t\t\\\n\tfor (; pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe(rcu_dereference_raw(hlist_next_rcu(\t\\\n\t\t\t&(pos)->member)), typeof(*(pos)), member))\n\n#endif\t/* __KERNEL__ */\n#endif\n"}}, "reports": [{"events": [{"location": {"col": 10, "file": 0, "line": 5886}, "message": "Calling 'ext4_mb_load_buddy'"}, {"location": {"col": 9, "file": 0, "line": 1278}, "message": "Calling 'ext4_mb_load_buddy_gfp'"}, {"location": {"col": 2, "file": 0, "line": 1152}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 24, "file": 1, "line": 160}, "message": "expanded from macro 'might_sleep'"}, {"location": {"col": 2, "file": 0, "line": 1153}, "message": "Taking false branch"}, {"location": {"col": 32, "file": 2, "line": 35}, "message": "expanded from macro 'mb_debug'"}, {"location": {"col": 2, "file": 3, "line": 139}, "message": "expanded from macro 'no_printk'"}, {"location": {"col": 6, "file": 0, "line": 1165}, "message": "Assuming the condition is false"}, {"location": {"col": 22, "file": 4, "line": 78}, "message": "expanded from macro 'unlikely'"}, {"location": {"col": 2, "file": 0, "line": 1165}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 0, "line": 1187}, "message": "Assuming 'page' is equal to NULL"}, {"location": {"col": 19, "file": 0, "line": 1187}, "message": "Left side of '||' is true"}, {"location": {"col": 7, "file": 0, "line": 1188}, "message": "'page' is null"}, {"location": {"col": 3, "file": 0, "line": 1188}, "message": "Taking false branch"}, {"location": {"col": 7, "file": 0, "line": 1199}, "message": "Assuming 'page' is non-null"}, {"location": {"col": 3, "file": 0, "line": 1199}, "message": "Taking true branch"}, {"location": {"col": 11, "file": 0, "line": 1200}, "message": "Assuming field 'mapping' is equal to field 'i_mapping'"}, {"location": {"col": 45, "file": 5, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 42, "file": 4, "line": 78}, "message": "expanded from macro 'unlikely'"}, {"location": {"col": 4, "file": 0, "line": 1200}, "message": "Taking false branch"}, {"location": {"col": 32, "file": 5, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 4, "file": 0, "line": 1200}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 27, "file": 5, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 4, "file": 0, "line": 1201}, "message": "Taking true branch"}, {"location": {"col": 11, "file": 0, "line": 1202}, "message": "Calling 'ext4_mb_init_cache'"}, {"location": {"col": 2, "file": 0, "line": 869}, "message": "Taking false branch"}, {"location": {"col": 32, "file": 2, "line": 35}, "message": "expanded from macro 'mb_debug'"}, {"location": {"col": 2, "file": 3, "line": 139}, "message": "expanded from macro 'no_printk'"}, {"location": {"col": 6, "file": 0, "line": 872}, "message": "Assuming 'groups_per_page' is not equal to 0"}, {"location": {"col": 2, "file": 0, "line": 872}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 0, "line": 876}, "message": "Assuming 'groups_per_page' is <= 1"}, {"location": {"col": 2, "file": 0, "line": 876}, "message": "Taking false branch"}, {"location": {"col": 35, "file": 0, "line": 889}, "message": "Assuming 'i' is < 'groups_per_page'"}, {"location": {"col": 2, "file": 0, "line": 889}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 7, "file": 0, "line": 890}, "message": "Assuming 'group' is >= 'ngroups'"}, {"location": {"col": 3, "file": 0, "line": 890}, "message": "Taking true branch"}, {"location": {"col": 4, "file": 0, "line": 891}, "message": "Execution continues on line 914"}, {"location": {"col": 7, "file": 0, "line": 914}, "message": "The value 0 is assigned to 'i'"}, {"location": {"col": 2, "file": 0, "line": 914}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 7, "file": 0, "line": 917}, "message": "Branch condition evaluates to a garbage value"}, {"location": {"col": 7, "file": 0, "line": 917}, "message": "Branch condition evaluates to a garbage value"}], "macros": [], "notes": [], "path": "/src/fs/ext4/mballoc.c", "reportHash": "bb86b5ae422db8b37e1d40d035800485", "checkerName": "clang-analyzer-core.uninitialized.Branch", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 2, "file": 6, "line": 3045}, "message": "expanded from macro 'ext4_error'"}, {"location": {"col": 6, "file": 0, "line": 2291}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 0, "line": 2291}, "message": "Taking false branch"}, {"location": {"col": 9, "file": 0, "line": 2294}, "message": "Assuming field 'ac_status' is not equal to 2"}, {"location": {"col": 45, "file": 5, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 42, "file": 4, "line": 78}, "message": "expanded from macro 'unlikely'"}, {"location": {"col": 2, "file": 0, "line": 2294}, "message": "Taking false branch"}, {"location": {"col": 32, "file": 5, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 2, "file": 0, "line": 2294}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 27, "file": 5, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 8, "file": 0, "line": 2297}, "message": "Calling 'ext4_mb_find_by_goal'"}, {"location": {"col": 6, "file": 0, "line": 1864}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 0, "line": 1864}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 0, "line": 1866}, "message": "Assuming field 'bb_free' is not equal to 0"}, {"location": {"col": 2, "file": 0, "line": 1866}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 0, "line": 1870}, "message": "Assuming 'err' is 0"}, {"location": {"col": 2, "file": 0, "line": 1870}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 0, "line": 1873}, "message": "Assuming the condition is false"}, {"location": {"col": 22, "file": 4, "line": 78}, "message": "expanded from macro 'unlikely'"}, {"location": {"col": 2, "file": 0, "line": 1873}, "message": "Taking false branch"}, {"location": {"col": 8, "file": 0, "line": 1879}, "message": "Calling 'mb_find_extent'"}, {"location": {"col": 2, "file": 0, "line": 1532}, "message": "Assuming the condition is false"}, {"location": {"col": 34, "file": 7, "line": 460}, "message": "expanded from macro 'assert_spin_locked'"}, {"location": {"col": 42, "file": 8, "line": 20}, "message": "expanded from macro 'assert_raw_spin_locked'"}, {"location": {"col": 45, "file": 5, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 42, "file": 4, "line": 78}, "message": "expanded from macro 'unlikely'"}, {"location": {"col": 2, "file": 0, "line": 1532}, "message": "Taking false branch"}, {"location": {"col": 34, "file": 7, "line": 460}, "message": "expanded from macro 'assert_spin_locked'"}, {"location": {"col": 35, "file": 8, "line": 20}, "message": "expanded from macro 'assert_raw_spin_locked'"}, {"location": {"col": 32, "file": 5, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 2, "file": 0, "line": 1532}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 34, "file": 7, "line": 460}, "message": "expanded from macro 'assert_spin_locked'"}, {"location": {"col": 35, "file": 8, "line": 20}, "message": "expanded from macro 'assert_raw_spin_locked'"}, {"location": {"col": 27, "file": 5, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 2, "file": 0, "line": 1533}, "message": "Taking false branch"}, {"location": {"col": 32, "file": 5, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 2, "file": 0, "line": 1533}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 27, "file": 5, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 9, "file": 0, "line": 1536}, "message": "Assuming 'buddy' is not equal to null"}, {"location": {"col": 45, "file": 5, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 42, "file": 4, "line": 78}, "message": "expanded from macro 'unlikely'"}, {"location": {"col": 2, "file": 0, "line": 1536}, "message": "Taking false branch"}, {"location": {"col": 32, "file": 5, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 2, "file": 0, "line": 1536}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 27, "file": 5, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 9, "file": 0, "line": 1537}, "message": "Assuming 'block' is < 'max'"}, {"location": {"col": 45, "file": 5, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 42, "file": 4, "line": 78}, "message": "expanded from macro 'unlikely'"}, {"location": {"col": 2, "file": 0, "line": 1537}, "message": "Taking false branch"}, {"location": {"col": 32, "file": 5, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 2, "file": 0, "line": 1537}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 27, "file": 5, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 6, "file": 0, "line": 1538}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 0, "line": 1538}, "message": "Taking false branch"}, {"location": {"col": 9, "file": 0, "line": 1558}, "message": "Assuming 'needed' is <= field 'fe_len'"}, {"location": {"col": 29, "file": 0, "line": 1558}, "message": "Left side of '&&' is false"}, {"location": {"col": 6, "file": 0, "line": 1574}, "message": "Assuming the condition is true"}, {"location": {"col": 2, "file": 0, "line": 1574}, "message": "Taking true branch"}, {"location": {"col": 3, "file": 0, "line": 1576}, "message": "Taking true branch"}, {"location": {"col": 2, "file": 5, "line": 120}, "message": "expanded from macro 'WARN_ON'"}, {"location": {"col": 3, "file": 0, "line": 1576}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 3, "file": 5, "line": 121}, "message": "expanded from macro 'WARN_ON'"}, {"location": {"col": 19, "file": 5, "line": 94}, "message": "expanded from macro '__WARN'"}, {"location": {"col": 2, "file": 9, "line": 87}, "message": "expanded from macro '__WARN_FLAGS'"}, {"location": {"col": 34, "file": 10, "line": 53}, "message": "expanded from macro 'instrumentation_begin'"}, {"location": {"col": 3, "file": 0, "line": 1576}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 3, "file": 5, "line": 121}, "message": "expanded from macro 'WARN_ON'"}, {"location": {"col": 19, "file": 5, "line": 94}, "message": "expanded from macro '__WARN'"}, {"location": {"col": 2, "file": 9, "line": 88}, "message": "expanded from macro '__WARN_FLAGS'"}, {"location": {"col": 37, "file": 9, "line": 34}, "message": "expanded from macro '_BUG_FLAGS'"}, {"location": {"col": 3, "file": 0, "line": 1576}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 3, "file": 5, "line": 121}, "message": "expanded from macro 'WARN_ON'"}, {"location": {"col": 19, "file": 5, "line": 94}, "message": "expanded from macro '__WARN'"}, {"location": {"col": 2, "file": 9, "line": 90}, "message": "expanded from macro '__WARN_FLAGS'"}, {"location": {"col": 33, "file": 10, "line": 54}, "message": "expanded from macro 'instrumentation_end'"}, {"location": {"col": 3, "file": 0, "line": 1576}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 3, "file": 5, "line": 121}, "message": "expanded from macro 'WARN_ON'"}, {"location": {"col": 19, "file": 5, "line": 94}, "message": "expanded from macro '__WARN'"}, {"location": {"col": 33, "file": 9, "line": 85}, "message": "expanded from macro '__WARN_FLAGS'"}, {"location": {"col": 3, "file": 0, "line": 1577}, "message": "14th function call argument is an uninitialized value"}, {"location": {"col": 2, "file": 6, "line": 3045}, "message": "expanded from macro 'ext4_error'"}, {"location": {"col": 3, "file": 0, "line": 1577}, "message": "14th function call argument is an uninitialized value"}], "macros": [], "notes": [], "path": "/src/fs/ext4/mballoc.c", "reportHash": "6f32e2f9aca5c60b9e60346efe0ffe14", "checkerName": "clang-analyzer-core.CallAndMessage", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 27, "file": 0, "line": 2258}, "message": "Value stored to 'grp' during its initialization is never read"}, {"location": {"col": 27, "file": 0, "line": 2258}, "message": "Value stored to 'grp' during its initialization is never read"}], "macros": [], "notes": [], "path": "/src/fs/ext4/mballoc.c", "reportHash": "9ce095a6585f7882957f4f092d502f40", "checkerName": "clang-analyzer-deadcode.DeadStores", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 4, "file": 0, "line": 2440}, "message": "Value stored to 'lost' is never read"}, {"location": {"col": 4, "file": 0, "line": 2440}, "message": "Value stored to 'lost' is never read"}], "macros": [], "notes": [], "path": "/src/fs/ext4/mballoc.c", "reportHash": "4daa65e6a37ad98de561ed265ffddcca", "checkerName": "clang-analyzer-deadcode.DeadStores", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 2, "file": 0, "line": 3043}, "message": "Value stored to 'count' is never read"}, {"location": {"col": 2, "file": 0, "line": 3043}, "message": "Value stored to 'count' is never read"}], "macros": [], "notes": [], "path": "/src/fs/ext4/mballoc.c", "reportHash": "d3e035c5576bf929bee3614114c25aca", "checkerName": "clang-analyzer-deadcode.DeadStores", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 0, "line": 3307}, "message": "Value stored to 'err' is never read"}, {"location": {"col": 3, "file": 0, "line": 3307}, "message": "Value stored to 'err' is never read"}], "macros": [], "notes": [], "path": "/src/fs/ext4/mballoc.c", "reportHash": "5eb7b5aa9495a5997eb017cba700e317", "checkerName": "clang-analyzer-deadcode.DeadStores", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 2, "file": 0, "line": 3312}, "message": "Value stored to 'err' is never read"}, {"location": {"col": 2, "file": 0, "line": 3312}, "message": "Value stored to 'err' is never read"}], "macros": [], "notes": [], "path": "/src/fs/ext4/mballoc.c", "reportHash": "354ca691eed27b4de4c8df5a99977238", "checkerName": "clang-analyzer-deadcode.DeadStores", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 2, "file": 0, "line": 3357}, "message": "Value stored to 'err' is never read"}, {"location": {"col": 2, "file": 0, "line": 3357}, "message": "Value stored to 'err' is never read"}], "macros": [], "notes": [], "path": "/src/fs/ext4/mballoc.c", "reportHash": "796b882a91f940498431cfb31da9516d", "checkerName": "clang-analyzer-deadcode.DeadStores", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 11, "line": 716}, "message": "expanded from macro 'list_for_each_entry_safe'"}, {"location": {"col": 2, "file": 11, "line": 555}, "message": "expanded from macro 'list_next_entry'"}, {"location": {"col": 2, "file": 11, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 2, "file": 1, "line": 701}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 2, "file": 0, "line": 4224}, "message": "Taking false branch"}, {"location": {"col": 32, "file": 2, "line": 35}, "message": "expanded from macro 'mb_debug'"}, {"location": {"col": 2, "file": 3, "line": 139}, "message": "expanded from macro 'no_printk'"}, {"location": {"col": 6, "file": 0, "line": 4225}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 0, "line": 4225}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 0, "line": 4229}, "message": "Calling 'IS_ERR'"}, {"location": {"col": 9, "file": 12, "line": 36}, "message": "Assuming the condition is false"}, {"location": {"col": 34, "file": 12, "line": 22}, "message": "expanded from macro 'IS_ERR_VALUE'"}, {"location": {"col": 42, "file": 4, "line": 78}, "message": "expanded from macro 'unlikely'"}, {"location": {"col": 2, "file": 12, "line": 36}, "message": "Returning zero, which participates in a condition later"}, {"location": {"col": 6, "file": 0, "line": 4229}, "message": "Returning from 'IS_ERR'"}, {"location": {"col": 2, "file": 0, "line": 4229}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 0, "line": 4238}, "message": "Assuming 'err' is 0"}, {"location": {"col": 2, "file": 0, "line": 4238}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 0, "line": 4245}, "message": "Assuming 'needed' is not equal to 0"}, {"location": {"col": 2, "file": 0, "line": 4245}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 0, "line": 4248}, "message": "Calling 'INIT_LIST_HEAD'"}, {"location": {"col": 2, "file": 11, "line": 35}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 13, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 13, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 14, "line": 290}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 11, "line": 35}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 13, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 13, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 14, "line": 290}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 11, "line": 35}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 13, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 13, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 14, "line": 290}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 11, "line": 35}, "message": "Left side of '||' is true"}, {"location": {"col": 2, "file": 13, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 38, "file": 13, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 35}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 13, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 13, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 14, "line": 328}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 14, "line": 316}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 14, "line": 308}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 35}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 13, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 13, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 14, "line": 328}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 14, "line": 316}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 14, "line": 306}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 35}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 13, "line": 61}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 35, "file": 13, "line": 53}, "message": "expanded from macro '__WRITE_ONCE'"}, {"location": {"col": 2, "file": 11, "line": 35}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 33, "file": 13, "line": 58}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 0, "line": 4248}, "message": "Returning from 'INIT_LIST_HEAD'"}, {"location": {"col": 2, "file": 0, "line": 4252}, "message": "Left side of '&&' is false"}, {"location": {"col": 13, "file": 11, "line": 715}, "message": "expanded from macro 'list_for_each_entry_safe'"}, {"location": {"col": 2, "file": 11, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 11, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 61, "file": 1, "line": 702}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 2, "file": 0, "line": 4252}, "message": "Taking false branch"}, {"location": {"col": 13, "file": 11, "line": 715}, "message": "expanded from macro 'list_for_each_entry_safe'"}, {"location": {"col": 2, "file": 11, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 11, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 2, "file": 0, "line": 4281}, "message": "Assigned value is garbage or undefined"}], "macros": [], "notes": [], "path": "/src/fs/ext4/mballoc.c", "reportHash": "e1b34cb388cae361bdc223d4536f49e5", "checkerName": "clang-analyzer-core.uninitialized.Assign", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 11, "line": 716}, "message": "expanded from macro 'list_for_each_entry_safe'"}, {"location": {"col": 2, "file": 11, "line": 555}, "message": "expanded from macro 'list_next_entry'"}, {"location": {"col": 2, "file": 11, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 2, "file": 1, "line": 701}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 2, "file": 0, "line": 4653}, "message": "Taking false branch"}, {"location": {"col": 32, "file": 2, "line": 35}, "message": "expanded from macro 'mb_debug'"}, {"location": {"col": 2, "file": 3, "line": 139}, "message": "expanded from macro 'no_printk'"}, {"location": {"col": 2, "file": 0, "line": 4655}, "message": "Calling 'INIT_LIST_HEAD'"}, {"location": {"col": 2, "file": 11, "line": 35}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 13, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 13, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 14, "line": 290}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 11, "line": 35}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 13, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 13, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 14, "line": 290}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 11, "line": 35}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 13, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 13, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 14, "line": 290}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 11, "line": 35}, "message": "Left side of '||' is true"}, {"location": {"col": 2, "file": 13, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 38, "file": 13, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 35}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 13, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 13, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 14, "line": 328}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 14, "line": 316}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 14, "line": 308}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 35}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 13, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 13, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 14, "line": 328}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 14, "line": 316}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 14, "line": 306}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 35}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 13, "line": 61}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 35, "file": 13, "line": 53}, "message": "expanded from macro '__WRITE_ONCE'"}, {"location": {"col": 2, "file": 11, "line": 35}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 33, "file": 13, "line": 58}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 0, "line": 4655}, "message": "Returning from 'INIT_LIST_HEAD'"}, {"location": {"col": 2, "file": 0, "line": 4658}, "message": "Left side of '||' is false"}, {"location": {"col": 13, "file": 15, "line": 392}, "message": "expanded from macro 'list_for_each_entry_rcu'"}, {"location": {"col": 15, "file": 15, "line": 316}, "message": "expanded from macro 'list_entry_rcu'"}, {"location": {"col": 2, "file": 13, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 21, "file": 13, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 14, "line": 290}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 4658}, "message": "Left side of '||' is false"}, {"location": {"col": 13, "file": 15, "line": 392}, "message": "expanded from macro 'list_for_each_entry_rcu'"}, {"location": {"col": 15, "file": 15, "line": 316}, "message": "expanded from macro 'list_entry_rcu'"}, {"location": {"col": 2, "file": 13, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 21, "file": 13, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 14, "line": 290}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 4658}, "message": "Left side of '||' is false"}, {"location": {"col": 13, "file": 15, "line": 392}, "message": "expanded from macro 'list_for_each_entry_rcu'"}, {"location": {"col": 15, "file": 15, "line": 316}, "message": "expanded from macro 'list_entry_rcu'"}, {"location": {"col": 2, "file": 13, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 21, "file": 13, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 14, "line": 290}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 4658}, "message": "Left side of '||' is true"}, {"location": {"col": 13, "file": 15, "line": 392}, "message": "expanded from macro 'list_for_each_entry_rcu'"}, {"location": {"col": 15, "file": 15, "line": 316}, "message": "expanded from macro 'list_entry_rcu'"}, {"location": {"col": 2, "file": 13, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 38, "file": 13, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 0, "line": 4658}, "message": "Taking false branch"}, {"location": {"col": 13, "file": 15, "line": 392}, "message": "expanded from macro 'list_for_each_entry_rcu'"}, {"location": {"col": 15, "file": 15, "line": 316}, "message": "expanded from macro 'list_entry_rcu'"}, {"location": {"col": 2, "file": 13, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 2, "file": 0, "line": 4698}, "message": "Assigned value is garbage or undefined"}], "macros": [], "notes": [], "path": "/src/fs/ext4/mballoc.c", "reportHash": "d19fe98243f4c238748bfb438c870d25", "checkerName": "clang-analyzer-core.uninitialized.Assign", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 0, "line": 5213}, "message": "Value stored to 'err' is never read"}, {"location": {"col": 3, "file": 0, "line": 5213}, "message": "Value stored to 'err' is never read"}], "macros": [], "notes": [], "path": "/src/fs/ext4/mballoc.c", "reportHash": "5eb7b5aa9495a5997eb017cba700e317", "checkerName": "clang-analyzer-deadcode.DeadStores", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
