<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"0": {"id": 0, "path": "/src/drivers/md/raid5.c", "content": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * raid5.c : Multiple Devices driver for Linux\n *\t   Copyright (C) 1996, 1997 Ingo Molnar, Miguel de Icaza, Gadi Oxman\n *\t   Copyright (C) 1999, 2000 Ingo Molnar\n *\t   Copyright (C) 2002, 2003 H. Peter Anvin\n *\n * RAID-4/5/6 management functions.\n * Thanks to Penguin Computing for making the RAID-6 development possible\n * by donating a test server!\n */\n\n/*\n * BITMAP UNPLUGGING:\n *\n * The sequencing for updating the bitmap reliably is a little\n * subtle (and I got it wrong the first time) so it deserves some\n * explanation.\n *\n * We group bitmap updates into batches.  Each batch has a number.\n * We may write out several batches at once, but that isn't very important.\n * conf->seq_write is the number of the last batch successfully written.\n * conf->seq_flush is the number of the last batch that was closed to\n *    new additions.\n * When we discover that we will need to write to any block in a stripe\n * (in add_stripe_bio) we update the in-memory bitmap and record in sh->bm_seq\n * the number of the batch it will be in. This is seq_flush+1.\n * When we are ready to do a write, if that batch hasn't been written yet,\n *   we plug the array and queue the stripe for later.\n * When an unplug happens, we increment bm_flush, thus closing the current\n *   batch.\n * When we notice that bm_flush > bm_write, we write out all pending updates\n * to the bitmap, and advance bm_write to where bm_flush was.\n * This may occasionally write a bit out twice, but is sure never to\n * miss any bits.\n */\n\n#include <linux/blkdev.h>\n#include <linux/kthread.h>\n#include <linux/raid/pq.h>\n#include <linux/async_tx.h>\n#include <linux/module.h>\n#include <linux/async.h>\n#include <linux/seq_file.h>\n#include <linux/cpu.h>\n#include <linux/slab.h>\n#include <linux/ratelimit.h>\n#include <linux/nodemask.h>\n\n#include <trace/events/block.h>\n#include <linux/list_sort.h>\n\n#include \"md.h\"\n#include \"raid5.h\"\n#include \"raid0.h\"\n#include \"md-bitmap.h\"\n#include \"raid5-log.h\"\n\n#define UNSUPPORTED_MDDEV_FLAGS\t(1L << MD_FAILFAST_SUPPORTED)\n\n#define cpu_to_group(cpu) cpu_to_node(cpu)\n#define ANY_GROUP NUMA_NO_NODE\n\nstatic bool devices_handle_discard_safely = false;\nmodule_param(devices_handle_discard_safely, bool, 0644);\nMODULE_PARM_DESC(devices_handle_discard_safely,\n\t\t \"Set to Y if all devices in each array reliably return zeroes on reads from discarded regions\");\nstatic struct workqueue_struct *raid5_wq;\n\nstatic inline struct hlist_head *stripe_hash(struct r5conf *conf, sector_t sect)\n{\n\tint hash = (sect >> RAID5_STRIPE_SHIFT(conf)) & HASH_MASK;\n\treturn &conf->stripe_hashtbl[hash];\n}\n\nstatic inline int stripe_hash_locks_hash(struct r5conf *conf, sector_t sect)\n{\n\treturn (sect >> RAID5_STRIPE_SHIFT(conf)) & STRIPE_HASH_LOCKS_MASK;\n}\n\nstatic inline void lock_device_hash_lock(struct r5conf *conf, int hash)\n{\n\tspin_lock_irq(conf->hash_locks + hash);\n\tspin_lock(&conf->device_lock);\n}\n\nstatic inline void unlock_device_hash_lock(struct r5conf *conf, int hash)\n{\n\tspin_unlock(&conf->device_lock);\n\tspin_unlock_irq(conf->hash_locks + hash);\n}\n\nstatic inline void lock_all_device_hash_locks_irq(struct r5conf *conf)\n{\n\tint i;\n\tspin_lock_irq(conf->hash_locks);\n\tfor (i = 1; i < NR_STRIPE_HASH_LOCKS; i++)\n\t\tspin_lock_nest_lock(conf->hash_locks + i, conf->hash_locks);\n\tspin_lock(&conf->device_lock);\n}\n\nstatic inline void unlock_all_device_hash_locks_irq(struct r5conf *conf)\n{\n\tint i;\n\tspin_unlock(&conf->device_lock);\n\tfor (i = NR_STRIPE_HASH_LOCKS - 1; i; i--)\n\t\tspin_unlock(conf->hash_locks + i);\n\tspin_unlock_irq(conf->hash_locks);\n}\n\n/* Find first data disk in a raid6 stripe */\nstatic inline int raid6_d0(struct stripe_head *sh)\n{\n\tif (sh->ddf_layout)\n\t\t/* ddf always start from first device */\n\t\treturn 0;\n\t/* md starts just after Q block */\n\tif (sh->qd_idx == sh->disks - 1)\n\t\treturn 0;\n\telse\n\t\treturn sh->qd_idx + 1;\n}\nstatic inline int raid6_next_disk(int disk, int raid_disks)\n{\n\tdisk++;\n\treturn (disk < raid_disks) ? disk : 0;\n}\n\n/* When walking through the disks in a raid5, starting at raid6_d0,\n * We need to map each disk to a 'slot', where the data disks are slot\n * 0 .. raid_disks-3, the parity disk is raid_disks-2 and the Q disk\n * is raid_disks-1.  This help does that mapping.\n */\nstatic int raid6_idx_to_slot(int idx, struct stripe_head *sh,\n\t\t\t     int *count, int syndrome_disks)\n{\n\tint slot = *count;\n\n\tif (sh->ddf_layout)\n\t\t(*count)++;\n\tif (idx == sh->pd_idx)\n\t\treturn syndrome_disks;\n\tif (idx == sh->qd_idx)\n\t\treturn syndrome_disks + 1;\n\tif (!sh->ddf_layout)\n\t\t(*count)++;\n\treturn slot;\n}\n\nstatic void print_raid5_conf (struct r5conf *conf);\n\nstatic int stripe_operations_active(struct stripe_head *sh)\n{\n\treturn sh->check_state || sh->reconstruct_state ||\n\t       test_bit(STRIPE_BIOFILL_RUN, &sh->state) ||\n\t       test_bit(STRIPE_COMPUTE_RUN, &sh->state);\n}\n\nstatic bool stripe_is_lowprio(struct stripe_head *sh)\n{\n\treturn (test_bit(STRIPE_R5C_FULL_STRIPE, &sh->state) ||\n\t\ttest_bit(STRIPE_R5C_PARTIAL_STRIPE, &sh->state)) &&\n\t       !test_bit(STRIPE_R5C_CACHING, &sh->state);\n}\n\nstatic void raid5_wakeup_stripe_thread(struct stripe_head *sh)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\tstruct r5worker_group *group;\n\tint thread_cnt;\n\tint i, cpu = sh->cpu;\n\n\tif (!cpu_online(cpu)) {\n\t\tcpu = cpumask_any(cpu_online_mask);\n\t\tsh->cpu = cpu;\n\t}\n\n\tif (list_empty(&sh->lru)) {\n\t\tstruct r5worker_group *group;\n\t\tgroup = conf->worker_groups + cpu_to_group(cpu);\n\t\tif (stripe_is_lowprio(sh))\n\t\t\tlist_add_tail(&sh->lru, &group->loprio_list);\n\t\telse\n\t\t\tlist_add_tail(&sh->lru, &group->handle_list);\n\t\tgroup->stripes_cnt++;\n\t\tsh->group = group;\n\t}\n\n\tif (conf->worker_cnt_per_group == 0) {\n\t\tmd_wakeup_thread(conf->mddev->thread);\n\t\treturn;\n\t}\n\n\tgroup = conf->worker_groups + cpu_to_group(sh->cpu);\n\n\tgroup->workers[0].working = true;\n\t/* at least one worker should run to avoid race */\n\tqueue_work_on(sh->cpu, raid5_wq, &group->workers[0].work);\n\n\tthread_cnt = group->stripes_cnt / MAX_STRIPE_BATCH - 1;\n\t/* wakeup more workers */\n\tfor (i = 1; i < conf->worker_cnt_per_group && thread_cnt > 0; i++) {\n\t\tif (group->workers[i].working == false) {\n\t\t\tgroup->workers[i].working = true;\n\t\t\tqueue_work_on(sh->cpu, raid5_wq,\n\t\t\t\t      &group->workers[i].work);\n\t\t\tthread_cnt--;\n\t\t}\n\t}\n}\n\nstatic void do_release_stripe(struct r5conf *conf, struct stripe_head *sh,\n\t\t\t      struct list_head *temp_inactive_list)\n{\n\tint i;\n\tint injournal = 0;\t/* number of date pages with R5_InJournal */\n\n\tBUG_ON(!list_empty(&sh->lru));\n\tBUG_ON(atomic_read(&conf->active_stripes)==0);\n\n\tif (r5c_is_writeback(conf->log))\n\t\tfor (i = sh->disks; i--; )\n\t\t\tif (test_bit(R5_InJournal, &sh->dev[i].flags))\n\t\t\t\tinjournal++;\n\t/*\n\t * In the following cases, the stripe cannot be released to cached\n\t * lists. Therefore, we make the stripe write out and set\n\t * STRIPE_HANDLE:\n\t *   1. when quiesce in r5c write back;\n\t *   2. when resync is requested fot the stripe.\n\t */\n\tif (test_bit(STRIPE_SYNC_REQUESTED, &sh->state) ||\n\t    (conf->quiesce && r5c_is_writeback(conf->log) &&\n\t     !test_bit(STRIPE_HANDLE, &sh->state) && injournal != 0)) {\n\t\tif (test_bit(STRIPE_R5C_CACHING, &sh->state))\n\t\t\tr5c_make_stripe_write_out(sh);\n\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t}\n\n\tif (test_bit(STRIPE_HANDLE, &sh->state)) {\n\t\tif (test_bit(STRIPE_DELAYED, &sh->state) &&\n\t\t    !test_bit(STRIPE_PREREAD_ACTIVE, &sh->state))\n\t\t\tlist_add_tail(&sh->lru, &conf->delayed_list);\n\t\telse if (test_bit(STRIPE_BIT_DELAY, &sh->state) &&\n\t\t\t   sh->bm_seq - conf->seq_write > 0)\n\t\t\tlist_add_tail(&sh->lru, &conf->bitmap_list);\n\t\telse {\n\t\t\tclear_bit(STRIPE_DELAYED, &sh->state);\n\t\t\tclear_bit(STRIPE_BIT_DELAY, &sh->state);\n\t\t\tif (conf->worker_cnt_per_group == 0) {\n\t\t\t\tif (stripe_is_lowprio(sh))\n\t\t\t\t\tlist_add_tail(&sh->lru,\n\t\t\t\t\t\t\t&conf->loprio_list);\n\t\t\t\telse\n\t\t\t\t\tlist_add_tail(&sh->lru,\n\t\t\t\t\t\t\t&conf->handle_list);\n\t\t\t} else {\n\t\t\t\traid5_wakeup_stripe_thread(sh);\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t\tmd_wakeup_thread(conf->mddev->thread);\n\t} else {\n\t\tBUG_ON(stripe_operations_active(sh));\n\t\tif (test_and_clear_bit(STRIPE_PREREAD_ACTIVE, &sh->state))\n\t\t\tif (atomic_dec_return(&conf->preread_active_stripes)\n\t\t\t    < IO_THRESHOLD)\n\t\t\t\tmd_wakeup_thread(conf->mddev->thread);\n\t\tatomic_dec(&conf->active_stripes);\n\t\tif (!test_bit(STRIPE_EXPANDING, &sh->state)) {\n\t\t\tif (!r5c_is_writeback(conf->log))\n\t\t\t\tlist_add_tail(&sh->lru, temp_inactive_list);\n\t\t\telse {\n\t\t\t\tWARN_ON(test_bit(R5_InJournal, &sh->dev[sh->pd_idx].flags));\n\t\t\t\tif (injournal == 0)\n\t\t\t\t\tlist_add_tail(&sh->lru, temp_inactive_list);\n\t\t\t\telse if (injournal == conf->raid_disks - conf->max_degraded) {\n\t\t\t\t\t/* full stripe */\n\t\t\t\t\tif (!test_and_set_bit(STRIPE_R5C_FULL_STRIPE, &sh->state))\n\t\t\t\t\t\tatomic_inc(&conf->r5c_cached_full_stripes);\n\t\t\t\t\tif (test_and_clear_bit(STRIPE_R5C_PARTIAL_STRIPE, &sh->state))\n\t\t\t\t\t\tatomic_dec(&conf->r5c_cached_partial_stripes);\n\t\t\t\t\tlist_add_tail(&sh->lru, &conf->r5c_full_stripe_list);\n\t\t\t\t\tr5c_check_cached_full_stripe(conf);\n\t\t\t\t} else\n\t\t\t\t\t/*\n\t\t\t\t\t * STRIPE_R5C_PARTIAL_STRIPE is set in\n\t\t\t\t\t * r5c_try_caching_write(). No need to\n\t\t\t\t\t * set it again.\n\t\t\t\t\t */\n\t\t\t\t\tlist_add_tail(&sh->lru, &conf->r5c_partial_stripe_list);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void __release_stripe(struct r5conf *conf, struct stripe_head *sh,\n\t\t\t     struct list_head *temp_inactive_list)\n{\n\tif (atomic_dec_and_test(&sh->count))\n\t\tdo_release_stripe(conf, sh, temp_inactive_list);\n}\n\n/*\n * @hash could be NR_STRIPE_HASH_LOCKS, then we have a list of inactive_list\n *\n * Be careful: Only one task can add/delete stripes from temp_inactive_list at\n * given time. Adding stripes only takes device lock, while deleting stripes\n * only takes hash lock.\n */\nstatic void release_inactive_stripe_list(struct r5conf *conf,\n\t\t\t\t\t struct list_head *temp_inactive_list,\n\t\t\t\t\t int hash)\n{\n\tint size;\n\tbool do_wakeup = false;\n\tunsigned long flags;\n\n\tif (hash == NR_STRIPE_HASH_LOCKS) {\n\t\tsize = NR_STRIPE_HASH_LOCKS;\n\t\thash = NR_STRIPE_HASH_LOCKS - 1;\n\t} else\n\t\tsize = 1;\n\twhile (size) {\n\t\tstruct list_head *list = &temp_inactive_list[size - 1];\n\n\t\t/*\n\t\t * We don't hold any lock here yet, raid5_get_active_stripe() might\n\t\t * remove stripes from the list\n\t\t */\n\t\tif (!list_empty_careful(list)) {\n\t\t\tspin_lock_irqsave(conf->hash_locks + hash, flags);\n\t\t\tif (list_empty(conf->inactive_list + hash) &&\n\t\t\t    !list_empty(list))\n\t\t\t\tatomic_dec(&conf->empty_inactive_list_nr);\n\t\t\tlist_splice_tail_init(list, conf->inactive_list + hash);\n\t\t\tdo_wakeup = true;\n\t\t\tspin_unlock_irqrestore(conf->hash_locks + hash, flags);\n\t\t}\n\t\tsize--;\n\t\thash--;\n\t}\n\n\tif (do_wakeup) {\n\t\twake_up(&conf->wait_for_stripe);\n\t\tif (atomic_read(&conf->active_stripes) == 0)\n\t\t\twake_up(&conf->wait_for_quiescent);\n\t\tif (conf->retry_read_aligned)\n\t\t\tmd_wakeup_thread(conf->mddev->thread);\n\t}\n}\n\n/* should hold conf->device_lock already */\nstatic int release_stripe_list(struct r5conf *conf,\n\t\t\t       struct list_head *temp_inactive_list)\n{\n\tstruct stripe_head *sh, *t;\n\tint count = 0;\n\tstruct llist_node *head;\n\n\thead = llist_del_all(&conf->released_stripes);\n\thead = llist_reverse_order(head);\n\tllist_for_each_entry_safe(sh, t, head, release_list) {\n\t\tint hash;\n\n\t\t/* sh could be readded after STRIPE_ON_RELEASE_LIST is cleard */\n\t\tsmp_mb();\n\t\tclear_bit(STRIPE_ON_RELEASE_LIST, &sh->state);\n\t\t/*\n\t\t * Don't worry the bit is set here, because if the bit is set\n\t\t * again, the count is always > 1. This is true for\n\t\t * STRIPE_ON_UNPLUG_LIST bit too.\n\t\t */\n\t\thash = sh->hash_lock_index;\n\t\t__release_stripe(conf, sh, &temp_inactive_list[hash]);\n\t\tcount++;\n\t}\n\n\treturn count;\n}\n\nvoid raid5_release_stripe(struct stripe_head *sh)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\tunsigned long flags;\n\tstruct list_head list;\n\tint hash;\n\tbool wakeup;\n\n\t/* Avoid release_list until the last reference.\n\t */\n\tif (atomic_add_unless(&sh->count, -1, 1))\n\t\treturn;\n\n\tif (unlikely(!conf->mddev->thread) ||\n\t\ttest_and_set_bit(STRIPE_ON_RELEASE_LIST, &sh->state))\n\t\tgoto slow_path;\n\twakeup = llist_add(&sh->release_list, &conf->released_stripes);\n\tif (wakeup)\n\t\tmd_wakeup_thread(conf->mddev->thread);\n\treturn;\nslow_path:\n\t/* we are ok here if STRIPE_ON_RELEASE_LIST is set or not */\n\tif (atomic_dec_and_lock_irqsave(&sh->count, &conf->device_lock, flags)) {\n\t\tINIT_LIST_HEAD(&list);\n\t\thash = sh->hash_lock_index;\n\t\tdo_release_stripe(conf, sh, &list);\n\t\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\t\trelease_inactive_stripe_list(conf, &list, hash);\n\t}\n}\n\nstatic inline void remove_hash(struct stripe_head *sh)\n{\n\tpr_debug(\"remove_hash(), stripe %llu\\n\",\n\t\t(unsigned long long)sh->sector);\n\n\thlist_del_init(&sh->hash);\n}\n\nstatic inline void insert_hash(struct r5conf *conf, struct stripe_head *sh)\n{\n\tstruct hlist_head *hp = stripe_hash(conf, sh->sector);\n\n\tpr_debug(\"insert_hash(), stripe %llu\\n\",\n\t\t(unsigned long long)sh->sector);\n\n\thlist_add_head(&sh->hash, hp);\n}\n\n/* find an idle stripe, make sure it is unhashed, and return it. */\nstatic struct stripe_head *get_free_stripe(struct r5conf *conf, int hash)\n{\n\tstruct stripe_head *sh = NULL;\n\tstruct list_head *first;\n\n\tif (list_empty(conf->inactive_list + hash))\n\t\tgoto out;\n\tfirst = (conf->inactive_list + hash)->next;\n\tsh = list_entry(first, struct stripe_head, lru);\n\tlist_del_init(first);\n\tremove_hash(sh);\n\tatomic_inc(&conf->active_stripes);\n\tBUG_ON(hash != sh->hash_lock_index);\n\tif (list_empty(conf->inactive_list + hash))\n\t\tatomic_inc(&conf->empty_inactive_list_nr);\nout:\n\treturn sh;\n}\n\n#if PAGE_SIZE != DEFAULT_STRIPE_SIZE\nstatic void free_stripe_pages(struct stripe_head *sh)\n{\n\tint i;\n\tstruct page *p;\n\n\t/* Have not allocate page pool */\n\tif (!sh->pages)\n\t\treturn;\n\n\tfor (i = 0; i < sh->nr_pages; i++) {\n\t\tp = sh->pages[i];\n\t\tif (p)\n\t\t\tput_page(p);\n\t\tsh->pages[i] = NULL;\n\t}\n}\n\nstatic int alloc_stripe_pages(struct stripe_head *sh, gfp_t gfp)\n{\n\tint i;\n\tstruct page *p;\n\n\tfor (i = 0; i < sh->nr_pages; i++) {\n\t\t/* The page have allocated. */\n\t\tif (sh->pages[i])\n\t\t\tcontinue;\n\n\t\tp = alloc_page(gfp);\n\t\tif (!p) {\n\t\t\tfree_stripe_pages(sh);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tsh->pages[i] = p;\n\t}\n\treturn 0;\n}\n\nstatic int\ninit_stripe_shared_pages(struct stripe_head *sh, struct r5conf *conf, int disks)\n{\n\tint nr_pages, cnt;\n\n\tif (sh->pages)\n\t\treturn 0;\n\n\t/* Each of the sh->dev[i] need one conf->stripe_size */\n\tcnt = PAGE_SIZE / conf->stripe_size;\n\tnr_pages = (disks + cnt - 1) / cnt;\n\n\tsh->pages = kcalloc(nr_pages, sizeof(struct page *), GFP_KERNEL);\n\tif (!sh->pages)\n\t\treturn -ENOMEM;\n\tsh->nr_pages = nr_pages;\n\tsh->stripes_per_page = cnt;\n\treturn 0;\n}\n#endif\n\nstatic void shrink_buffers(struct stripe_head *sh)\n{\n\tint i;\n\tint num = sh->raid_conf->pool_size;\n\n#if PAGE_SIZE == DEFAULT_STRIPE_SIZE\n\tfor (i = 0; i < num ; i++) {\n\t\tstruct page *p;\n\n\t\tWARN_ON(sh->dev[i].page != sh->dev[i].orig_page);\n\t\tp = sh->dev[i].page;\n\t\tif (!p)\n\t\t\tcontinue;\n\t\tsh->dev[i].page = NULL;\n\t\tput_page(p);\n\t}\n#else\n\tfor (i = 0; i < num; i++)\n\t\tsh->dev[i].page = NULL;\n\tfree_stripe_pages(sh); /* Free pages */\n#endif\n}\n\nstatic int grow_buffers(struct stripe_head *sh, gfp_t gfp)\n{\n\tint i;\n\tint num = sh->raid_conf->pool_size;\n\n#if PAGE_SIZE == DEFAULT_STRIPE_SIZE\n\tfor (i = 0; i < num; i++) {\n\t\tstruct page *page;\n\n\t\tif (!(page = alloc_page(gfp))) {\n\t\t\treturn 1;\n\t\t}\n\t\tsh->dev[i].page = page;\n\t\tsh->dev[i].orig_page = page;\n\t\tsh->dev[i].offset = 0;\n\t}\n#else\n\tif (alloc_stripe_pages(sh, gfp))\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < num; i++) {\n\t\tsh->dev[i].page = raid5_get_dev_page(sh, i);\n\t\tsh->dev[i].orig_page = sh->dev[i].page;\n\t\tsh->dev[i].offset = raid5_get_page_offset(sh, i);\n\t}\n#endif\n\treturn 0;\n}\n\nstatic void stripe_set_idx(sector_t stripe, struct r5conf *conf, int previous,\n\t\t\t    struct stripe_head *sh);\n\nstatic void init_stripe(struct stripe_head *sh, sector_t sector, int previous)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\tint i, seq;\n\n\tBUG_ON(atomic_read(&sh->count) != 0);\n\tBUG_ON(test_bit(STRIPE_HANDLE, &sh->state));\n\tBUG_ON(stripe_operations_active(sh));\n\tBUG_ON(sh->batch_head);\n\n\tpr_debug(\"init_stripe called, stripe %llu\\n\",\n\t\t(unsigned long long)sector);\nretry:\n\tseq = read_seqcount_begin(&conf->gen_lock);\n\tsh->generation = conf->generation - previous;\n\tsh->disks = previous ? conf->previous_raid_disks : conf->raid_disks;\n\tsh->sector = sector;\n\tstripe_set_idx(sector, conf, previous, sh);\n\tsh->state = 0;\n\n\tfor (i = sh->disks; i--; ) {\n\t\tstruct r5dev *dev = &sh->dev[i];\n\n\t\tif (dev->toread || dev->read || dev->towrite || dev->written ||\n\t\t    test_bit(R5_LOCKED, &dev->flags)) {\n\t\t\tpr_err(\"sector=%llx i=%d %p %p %p %p %d\\n\",\n\t\t\t       (unsigned long long)sh->sector, i, dev->toread,\n\t\t\t       dev->read, dev->towrite, dev->written,\n\t\t\t       test_bit(R5_LOCKED, &dev->flags));\n\t\t\tWARN_ON(1);\n\t\t}\n\t\tdev->flags = 0;\n\t\tdev->sector = raid5_compute_blocknr(sh, i, previous);\n\t}\n\tif (read_seqcount_retry(&conf->gen_lock, seq))\n\t\tgoto retry;\n\tsh->overwrite_disks = 0;\n\tinsert_hash(conf, sh);\n\tsh->cpu = smp_processor_id();\n\tset_bit(STRIPE_BATCH_READY, &sh->state);\n}\n\nstatic struct stripe_head *__find_stripe(struct r5conf *conf, sector_t sector,\n\t\t\t\t\t short generation)\n{\n\tstruct stripe_head *sh;\n\n\tpr_debug(\"__find_stripe, sector %llu\\n\", (unsigned long long)sector);\n\thlist_for_each_entry(sh, stripe_hash(conf, sector), hash)\n\t\tif (sh->sector == sector && sh->generation == generation)\n\t\t\treturn sh;\n\tpr_debug(\"__stripe %llu not in cache\\n\", (unsigned long long)sector);\n\treturn NULL;\n}\n\n/*\n * Need to check if array has failed when deciding whether to:\n *  - start an array\n *  - remove non-faulty devices\n *  - add a spare\n *  - allow a reshape\n * This determination is simple when no reshape is happening.\n * However if there is a reshape, we need to carefully check\n * both the before and after sections.\n * This is because some failed devices may only affect one\n * of the two sections, and some non-in_sync devices may\n * be insync in the section most affected by failed devices.\n */\nint raid5_calc_degraded(struct r5conf *conf)\n{\n\tint degraded, degraded2;\n\tint i;\n\n\trcu_read_lock();\n\tdegraded = 0;\n\tfor (i = 0; i < conf->previous_raid_disks; i++) {\n\t\tstruct md_rdev *rdev = rcu_dereference(conf->disks[i].rdev);\n\t\tif (rdev && test_bit(Faulty, &rdev->flags))\n\t\t\trdev = rcu_dereference(conf->disks[i].replacement);\n\t\tif (!rdev || test_bit(Faulty, &rdev->flags))\n\t\t\tdegraded++;\n\t\telse if (test_bit(In_sync, &rdev->flags))\n\t\t\t;\n\t\telse\n\t\t\t/* not in-sync or faulty.\n\t\t\t * If the reshape increases the number of devices,\n\t\t\t * this is being recovered by the reshape, so\n\t\t\t * this 'previous' section is not in_sync.\n\t\t\t * If the number of devices is being reduced however,\n\t\t\t * the device can only be part of the array if\n\t\t\t * we are reverting a reshape, so this section will\n\t\t\t * be in-sync.\n\t\t\t */\n\t\t\tif (conf->raid_disks >= conf->previous_raid_disks)\n\t\t\t\tdegraded++;\n\t}\n\trcu_read_unlock();\n\tif (conf->raid_disks == conf->previous_raid_disks)\n\t\treturn degraded;\n\trcu_read_lock();\n\tdegraded2 = 0;\n\tfor (i = 0; i < conf->raid_disks; i++) {\n\t\tstruct md_rdev *rdev = rcu_dereference(conf->disks[i].rdev);\n\t\tif (rdev && test_bit(Faulty, &rdev->flags))\n\t\t\trdev = rcu_dereference(conf->disks[i].replacement);\n\t\tif (!rdev || test_bit(Faulty, &rdev->flags))\n\t\t\tdegraded2++;\n\t\telse if (test_bit(In_sync, &rdev->flags))\n\t\t\t;\n\t\telse\n\t\t\t/* not in-sync or faulty.\n\t\t\t * If reshape increases the number of devices, this\n\t\t\t * section has already been recovered, else it\n\t\t\t * almost certainly hasn't.\n\t\t\t */\n\t\t\tif (conf->raid_disks <= conf->previous_raid_disks)\n\t\t\t\tdegraded2++;\n\t}\n\trcu_read_unlock();\n\tif (degraded2 > degraded)\n\t\treturn degraded2;\n\treturn degraded;\n}\n\nstatic int has_failed(struct r5conf *conf)\n{\n\tint degraded;\n\n\tif (conf->mddev->reshape_position == MaxSector)\n\t\treturn conf->mddev->degraded > conf->max_degraded;\n\n\tdegraded = raid5_calc_degraded(conf);\n\tif (degraded > conf->max_degraded)\n\t\treturn 1;\n\treturn 0;\n}\n\nstruct stripe_head *\nraid5_get_active_stripe(struct r5conf *conf, sector_t sector,\n\t\t\tint previous, int noblock, int noquiesce)\n{\n\tstruct stripe_head *sh;\n\tint hash = stripe_hash_locks_hash(conf, sector);\n\tint inc_empty_inactive_list_flag;\n\n\tpr_debug(\"get_stripe, sector %llu\\n\", (unsigned long long)sector);\n\n\tspin_lock_irq(conf->hash_locks + hash);\n\n\tdo {\n\t\twait_event_lock_irq(conf->wait_for_quiescent,\n\t\t\t\t    conf->quiesce == 0 || noquiesce,\n\t\t\t\t    *(conf->hash_locks + hash));\n\t\tsh = __find_stripe(conf, sector, conf->generation - previous);\n\t\tif (!sh) {\n\t\t\tif (!test_bit(R5_INACTIVE_BLOCKED, &conf->cache_state)) {\n\t\t\t\tsh = get_free_stripe(conf, hash);\n\t\t\t\tif (!sh && !test_bit(R5_DID_ALLOC,\n\t\t\t\t\t\t     &conf->cache_state))\n\t\t\t\t\tset_bit(R5_ALLOC_MORE,\n\t\t\t\t\t\t&conf->cache_state);\n\t\t\t}\n\t\t\tif (noblock && sh == NULL)\n\t\t\t\tbreak;\n\n\t\t\tr5c_check_stripe_cache_usage(conf);\n\t\t\tif (!sh) {\n\t\t\t\tset_bit(R5_INACTIVE_BLOCKED,\n\t\t\t\t\t&conf->cache_state);\n\t\t\t\tr5l_wake_reclaim(conf->log, 0);\n\t\t\t\twait_event_lock_irq(\n\t\t\t\t\tconf->wait_for_stripe,\n\t\t\t\t\t!list_empty(conf->inactive_list + hash) &&\n\t\t\t\t\t(atomic_read(&conf->active_stripes)\n\t\t\t\t\t < (conf->max_nr_stripes * 3 / 4)\n\t\t\t\t\t || !test_bit(R5_INACTIVE_BLOCKED,\n\t\t\t\t\t\t      &conf->cache_state)),\n\t\t\t\t\t*(conf->hash_locks + hash));\n\t\t\t\tclear_bit(R5_INACTIVE_BLOCKED,\n\t\t\t\t\t  &conf->cache_state);\n\t\t\t} else {\n\t\t\t\tinit_stripe(sh, sector, previous);\n\t\t\t\tatomic_inc(&sh->count);\n\t\t\t}\n\t\t} else if (!atomic_inc_not_zero(&sh->count)) {\n\t\t\tspin_lock(&conf->device_lock);\n\t\t\tif (!atomic_read(&sh->count)) {\n\t\t\t\tif (!test_bit(STRIPE_HANDLE, &sh->state))\n\t\t\t\t\tatomic_inc(&conf->active_stripes);\n\t\t\t\tBUG_ON(list_empty(&sh->lru) &&\n\t\t\t\t       !test_bit(STRIPE_EXPANDING, &sh->state));\n\t\t\t\tinc_empty_inactive_list_flag = 0;\n\t\t\t\tif (!list_empty(conf->inactive_list + hash))\n\t\t\t\t\tinc_empty_inactive_list_flag = 1;\n\t\t\t\tlist_del_init(&sh->lru);\n\t\t\t\tif (list_empty(conf->inactive_list + hash) && inc_empty_inactive_list_flag)\n\t\t\t\t\tatomic_inc(&conf->empty_inactive_list_nr);\n\t\t\t\tif (sh->group) {\n\t\t\t\t\tsh->group->stripes_cnt--;\n\t\t\t\t\tsh->group = NULL;\n\t\t\t\t}\n\t\t\t}\n\t\t\tatomic_inc(&sh->count);\n\t\t\tspin_unlock(&conf->device_lock);\n\t\t}\n\t} while (sh == NULL);\n\n\tspin_unlock_irq(conf->hash_locks + hash);\n\treturn sh;\n}\n\nstatic bool is_full_stripe_write(struct stripe_head *sh)\n{\n\tBUG_ON(sh->overwrite_disks > (sh->disks - sh->raid_conf->max_degraded));\n\treturn sh->overwrite_disks == (sh->disks - sh->raid_conf->max_degraded);\n}\n\nstatic void lock_two_stripes(struct stripe_head *sh1, struct stripe_head *sh2)\n\t\t__acquires(&sh1->stripe_lock)\n\t\t__acquires(&sh2->stripe_lock)\n{\n\tif (sh1 > sh2) {\n\t\tspin_lock_irq(&sh2->stripe_lock);\n\t\tspin_lock_nested(&sh1->stripe_lock, 1);\n\t} else {\n\t\tspin_lock_irq(&sh1->stripe_lock);\n\t\tspin_lock_nested(&sh2->stripe_lock, 1);\n\t}\n}\n\nstatic void unlock_two_stripes(struct stripe_head *sh1, struct stripe_head *sh2)\n\t\t__releases(&sh1->stripe_lock)\n\t\t__releases(&sh2->stripe_lock)\n{\n\tspin_unlock(&sh1->stripe_lock);\n\tspin_unlock_irq(&sh2->stripe_lock);\n}\n\n/* Only freshly new full stripe normal write stripe can be added to a batch list */\nstatic bool stripe_can_batch(struct stripe_head *sh)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\n\tif (raid5_has_log(conf) || raid5_has_ppl(conf))\n\t\treturn false;\n\treturn test_bit(STRIPE_BATCH_READY, &sh->state) &&\n\t\t!test_bit(STRIPE_BITMAP_PENDING, &sh->state) &&\n\t\tis_full_stripe_write(sh);\n}\n\n/* we only do back search */\nstatic void stripe_add_to_batch_list(struct r5conf *conf, struct stripe_head *sh)\n{\n\tstruct stripe_head *head;\n\tsector_t head_sector, tmp_sec;\n\tint hash;\n\tint dd_idx;\n\tint inc_empty_inactive_list_flag;\n\n\t/* Don't cross chunks, so stripe pd_idx/qd_idx is the same */\n\ttmp_sec = sh->sector;\n\tif (!sector_div(tmp_sec, conf->chunk_sectors))\n\t\treturn;\n\thead_sector = sh->sector - RAID5_STRIPE_SECTORS(conf);\n\n\thash = stripe_hash_locks_hash(conf, head_sector);\n\tspin_lock_irq(conf->hash_locks + hash);\n\thead = __find_stripe(conf, head_sector, conf->generation);\n\tif (head && !atomic_inc_not_zero(&head->count)) {\n\t\tspin_lock(&conf->device_lock);\n\t\tif (!atomic_read(&head->count)) {\n\t\t\tif (!test_bit(STRIPE_HANDLE, &head->state))\n\t\t\t\tatomic_inc(&conf->active_stripes);\n\t\t\tBUG_ON(list_empty(&head->lru) &&\n\t\t\t       !test_bit(STRIPE_EXPANDING, &head->state));\n\t\t\tinc_empty_inactive_list_flag = 0;\n\t\t\tif (!list_empty(conf->inactive_list + hash))\n\t\t\t\tinc_empty_inactive_list_flag = 1;\n\t\t\tlist_del_init(&head->lru);\n\t\t\tif (list_empty(conf->inactive_list + hash) && inc_empty_inactive_list_flag)\n\t\t\t\tatomic_inc(&conf->empty_inactive_list_nr);\n\t\t\tif (head->group) {\n\t\t\t\thead->group->stripes_cnt--;\n\t\t\t\thead->group = NULL;\n\t\t\t}\n\t\t}\n\t\tatomic_inc(&head->count);\n\t\tspin_unlock(&conf->device_lock);\n\t}\n\tspin_unlock_irq(conf->hash_locks + hash);\n\n\tif (!head)\n\t\treturn;\n\tif (!stripe_can_batch(head))\n\t\tgoto out;\n\n\tlock_two_stripes(head, sh);\n\t/* clear_batch_ready clear the flag */\n\tif (!stripe_can_batch(head) || !stripe_can_batch(sh))\n\t\tgoto unlock_out;\n\n\tif (sh->batch_head)\n\t\tgoto unlock_out;\n\n\tdd_idx = 0;\n\twhile (dd_idx == sh->pd_idx || dd_idx == sh->qd_idx)\n\t\tdd_idx++;\n\tif (head->dev[dd_idx].towrite->bi_opf != sh->dev[dd_idx].towrite->bi_opf ||\n\t    bio_op(head->dev[dd_idx].towrite) != bio_op(sh->dev[dd_idx].towrite))\n\t\tgoto unlock_out;\n\n\tif (head->batch_head) {\n\t\tspin_lock(&head->batch_head->batch_lock);\n\t\t/* This batch list is already running */\n\t\tif (!stripe_can_batch(head)) {\n\t\t\tspin_unlock(&head->batch_head->batch_lock);\n\t\t\tgoto unlock_out;\n\t\t}\n\t\t/*\n\t\t * We must assign batch_head of this stripe within the\n\t\t * batch_lock, otherwise clear_batch_ready of batch head\n\t\t * stripe could clear BATCH_READY bit of this stripe and\n\t\t * this stripe->batch_head doesn't get assigned, which\n\t\t * could confuse clear_batch_ready for this stripe\n\t\t */\n\t\tsh->batch_head = head->batch_head;\n\n\t\t/*\n\t\t * at this point, head's BATCH_READY could be cleared, but we\n\t\t * can still add the stripe to batch list\n\t\t */\n\t\tlist_add(&sh->batch_list, &head->batch_list);\n\t\tspin_unlock(&head->batch_head->batch_lock);\n\t} else {\n\t\thead->batch_head = head;\n\t\tsh->batch_head = head->batch_head;\n\t\tspin_lock(&head->batch_lock);\n\t\tlist_add_tail(&sh->batch_list, &head->batch_list);\n\t\tspin_unlock(&head->batch_lock);\n\t}\n\n\tif (test_and_clear_bit(STRIPE_PREREAD_ACTIVE, &sh->state))\n\t\tif (atomic_dec_return(&conf->preread_active_stripes)\n\t\t    < IO_THRESHOLD)\n\t\t\tmd_wakeup_thread(conf->mddev->thread);\n\n\tif (test_and_clear_bit(STRIPE_BIT_DELAY, &sh->state)) {\n\t\tint seq = sh->bm_seq;\n\t\tif (test_bit(STRIPE_BIT_DELAY, &sh->batch_head->state) &&\n\t\t    sh->batch_head->bm_seq > seq)\n\t\t\tseq = sh->batch_head->bm_seq;\n\t\tset_bit(STRIPE_BIT_DELAY, &sh->batch_head->state);\n\t\tsh->batch_head->bm_seq = seq;\n\t}\n\n\tatomic_inc(&sh->count);\nunlock_out:\n\tunlock_two_stripes(head, sh);\nout:\n\traid5_release_stripe(head);\n}\n\n/* Determine if 'data_offset' or 'new_data_offset' should be used\n * in this stripe_head.\n */\nstatic int use_new_offset(struct r5conf *conf, struct stripe_head *sh)\n{\n\tsector_t progress = conf->reshape_progress;\n\t/* Need a memory barrier to make sure we see the value\n\t * of conf->generation, or ->data_offset that was set before\n\t * reshape_progress was updated.\n\t */\n\tsmp_rmb();\n\tif (progress == MaxSector)\n\t\treturn 0;\n\tif (sh->generation == conf->generation - 1)\n\t\treturn 0;\n\t/* We are in a reshape, and this is a new-generation stripe,\n\t * so use new_data_offset.\n\t */\n\treturn 1;\n}\n\nstatic void dispatch_bio_list(struct bio_list *tmp)\n{\n\tstruct bio *bio;\n\n\twhile ((bio = bio_list_pop(tmp)))\n\t\tsubmit_bio_noacct(bio);\n}\n\nstatic int cmp_stripe(void *priv, const struct list_head *a,\n\t\t      const struct list_head *b)\n{\n\tconst struct r5pending_data *da = list_entry(a,\n\t\t\t\tstruct r5pending_data, sibling);\n\tconst struct r5pending_data *db = list_entry(b,\n\t\t\t\tstruct r5pending_data, sibling);\n\tif (da->sector > db->sector)\n\t\treturn 1;\n\tif (da->sector < db->sector)\n\t\treturn -1;\n\treturn 0;\n}\n\nstatic void dispatch_defer_bios(struct r5conf *conf, int target,\n\t\t\t\tstruct bio_list *list)\n{\n\tstruct r5pending_data *data;\n\tstruct list_head *first, *next = NULL;\n\tint cnt = 0;\n\n\tif (conf->pending_data_cnt == 0)\n\t\treturn;\n\n\tlist_sort(NULL, &conf->pending_list, cmp_stripe);\n\n\tfirst = conf->pending_list.next;\n\n\t/* temporarily move the head */\n\tif (conf->next_pending_data)\n\t\tlist_move_tail(&conf->pending_list,\n\t\t\t\t&conf->next_pending_data->sibling);\n\n\twhile (!list_empty(&conf->pending_list)) {\n\t\tdata = list_first_entry(&conf->pending_list,\n\t\t\tstruct r5pending_data, sibling);\n\t\tif (&data->sibling == first)\n\t\t\tfirst = data->sibling.next;\n\t\tnext = data->sibling.next;\n\n\t\tbio_list_merge(list, &data->bios);\n\t\tlist_move(&data->sibling, &conf->free_list);\n\t\tcnt++;\n\t\tif (cnt >= target)\n\t\t\tbreak;\n\t}\n\tconf->pending_data_cnt -= cnt;\n\tBUG_ON(conf->pending_data_cnt < 0 || cnt < target);\n\n\tif (next != &conf->pending_list)\n\t\tconf->next_pending_data = list_entry(next,\n\t\t\t\tstruct r5pending_data, sibling);\n\telse\n\t\tconf->next_pending_data = NULL;\n\t/* list isn't empty */\n\tif (first != &conf->pending_list)\n\t\tlist_move_tail(&conf->pending_list, first);\n}\n\nstatic void flush_deferred_bios(struct r5conf *conf)\n{\n\tstruct bio_list tmp = BIO_EMPTY_LIST;\n\n\tif (conf->pending_data_cnt == 0)\n\t\treturn;\n\n\tspin_lock(&conf->pending_bios_lock);\n\tdispatch_defer_bios(conf, conf->pending_data_cnt, &tmp);\n\tBUG_ON(conf->pending_data_cnt != 0);\n\tspin_unlock(&conf->pending_bios_lock);\n\n\tdispatch_bio_list(&tmp);\n}\n\nstatic void defer_issue_bios(struct r5conf *conf, sector_t sector,\n\t\t\t\tstruct bio_list *bios)\n{\n\tstruct bio_list tmp = BIO_EMPTY_LIST;\n\tstruct r5pending_data *ent;\n\n\tspin_lock(&conf->pending_bios_lock);\n\tent = list_first_entry(&conf->free_list, struct r5pending_data,\n\t\t\t\t\t\t\tsibling);\n\tlist_move_tail(&ent->sibling, &conf->pending_list);\n\tent->sector = sector;\n\tbio_list_init(&ent->bios);\n\tbio_list_merge(&ent->bios, bios);\n\tconf->pending_data_cnt++;\n\tif (conf->pending_data_cnt >= PENDING_IO_MAX)\n\t\tdispatch_defer_bios(conf, PENDING_IO_ONE_FLUSH, &tmp);\n\n\tspin_unlock(&conf->pending_bios_lock);\n\n\tdispatch_bio_list(&tmp);\n}\n\nstatic void\nraid5_end_read_request(struct bio *bi);\nstatic void\nraid5_end_write_request(struct bio *bi);\n\nstatic void ops_run_io(struct stripe_head *sh, struct stripe_head_state *s)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\tint i, disks = sh->disks;\n\tstruct stripe_head *head_sh = sh;\n\tstruct bio_list pending_bios = BIO_EMPTY_LIST;\n\tbool should_defer;\n\n\tmight_sleep();\n\n\tif (log_stripe(sh, s) == 0)\n\t\treturn;\n\n\tshould_defer = conf->batch_bio_dispatch && conf->group_cnt;\n\n\tfor (i = disks; i--; ) {\n\t\tint op, op_flags = 0;\n\t\tint replace_only = 0;\n\t\tstruct bio *bi, *rbi;\n\t\tstruct md_rdev *rdev, *rrdev = NULL;\n\n\t\tsh = head_sh;\n\t\tif (test_and_clear_bit(R5_Wantwrite, &sh->dev[i].flags)) {\n\t\t\top = REQ_OP_WRITE;\n\t\t\tif (test_and_clear_bit(R5_WantFUA, &sh->dev[i].flags))\n\t\t\t\top_flags = REQ_FUA;\n\t\t\tif (test_bit(R5_Discard, &sh->dev[i].flags))\n\t\t\t\top = REQ_OP_DISCARD;\n\t\t} else if (test_and_clear_bit(R5_Wantread, &sh->dev[i].flags))\n\t\t\top = REQ_OP_READ;\n\t\telse if (test_and_clear_bit(R5_WantReplace,\n\t\t\t\t\t    &sh->dev[i].flags)) {\n\t\t\top = REQ_OP_WRITE;\n\t\t\treplace_only = 1;\n\t\t} else\n\t\t\tcontinue;\n\t\tif (test_and_clear_bit(R5_SyncIO, &sh->dev[i].flags))\n\t\t\top_flags |= REQ_SYNC;\n\nagain:\n\t\tbi = &sh->dev[i].req;\n\t\trbi = &sh->dev[i].rreq; /* For writing to replacement */\n\n\t\trcu_read_lock();\n\t\trrdev = rcu_dereference(conf->disks[i].replacement);\n\t\tsmp_mb(); /* Ensure that if rrdev is NULL, rdev won't be */\n\t\trdev = rcu_dereference(conf->disks[i].rdev);\n\t\tif (!rdev) {\n\t\t\trdev = rrdev;\n\t\t\trrdev = NULL;\n\t\t}\n\t\tif (op_is_write(op)) {\n\t\t\tif (replace_only)\n\t\t\t\trdev = NULL;\n\t\t\tif (rdev == rrdev)\n\t\t\t\t/* We raced and saw duplicates */\n\t\t\t\trrdev = NULL;\n\t\t} else {\n\t\t\tif (test_bit(R5_ReadRepl, &head_sh->dev[i].flags) && rrdev)\n\t\t\t\trdev = rrdev;\n\t\t\trrdev = NULL;\n\t\t}\n\n\t\tif (rdev && test_bit(Faulty, &rdev->flags))\n\t\t\trdev = NULL;\n\t\tif (rdev)\n\t\t\tatomic_inc(&rdev->nr_pending);\n\t\tif (rrdev && test_bit(Faulty, &rrdev->flags))\n\t\t\trrdev = NULL;\n\t\tif (rrdev)\n\t\t\tatomic_inc(&rrdev->nr_pending);\n\t\trcu_read_unlock();\n\n\t\t/* We have already checked bad blocks for reads.  Now\n\t\t * need to check for writes.  We never accept write errors\n\t\t * on the replacement, so we don't to check rrdev.\n\t\t */\n\t\twhile (op_is_write(op) && rdev &&\n\t\t       test_bit(WriteErrorSeen, &rdev->flags)) {\n\t\t\tsector_t first_bad;\n\t\t\tint bad_sectors;\n\t\t\tint bad = is_badblock(rdev, sh->sector, RAID5_STRIPE_SECTORS(conf),\n\t\t\t\t\t      &first_bad, &bad_sectors);\n\t\t\tif (!bad)\n\t\t\t\tbreak;\n\n\t\t\tif (bad < 0) {\n\t\t\t\tset_bit(BlockedBadBlocks, &rdev->flags);\n\t\t\t\tif (!conf->mddev->external &&\n\t\t\t\t    conf->mddev->sb_flags) {\n\t\t\t\t\t/* It is very unlikely, but we might\n\t\t\t\t\t * still need to write out the\n\t\t\t\t\t * bad block log - better give it\n\t\t\t\t\t * a chance*/\n\t\t\t\t\tmd_check_recovery(conf->mddev);\n\t\t\t\t}\n\t\t\t\t/*\n\t\t\t\t * Because md_wait_for_blocked_rdev\n\t\t\t\t * will dec nr_pending, we must\n\t\t\t\t * increment it first.\n\t\t\t\t */\n\t\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t\t\tmd_wait_for_blocked_rdev(rdev, conf->mddev);\n\t\t\t} else {\n\t\t\t\t/* Acknowledged bad block - skip the write */\n\t\t\t\trdev_dec_pending(rdev, conf->mddev);\n\t\t\t\trdev = NULL;\n\t\t\t}\n\t\t}\n\n\t\tif (rdev) {\n\t\t\tif (s->syncing || s->expanding || s->expanded\n\t\t\t    || s->replacing)\n\t\t\t\tmd_sync_acct(rdev->bdev, RAID5_STRIPE_SECTORS(conf));\n\n\t\t\tset_bit(STRIPE_IO_STARTED, &sh->state);\n\n\t\t\tbio_set_dev(bi, rdev->bdev);\n\t\t\tbio_set_op_attrs(bi, op, op_flags);\n\t\t\tbi->bi_end_io = op_is_write(op)\n\t\t\t\t? raid5_end_write_request\n\t\t\t\t: raid5_end_read_request;\n\t\t\tbi->bi_private = sh;\n\n\t\t\tpr_debug(\"%s: for %llu schedule op %d on disc %d\\n\",\n\t\t\t\t__func__, (unsigned long long)sh->sector,\n\t\t\t\tbi->bi_opf, i);\n\t\t\tatomic_inc(&sh->count);\n\t\t\tif (sh != head_sh)\n\t\t\t\tatomic_inc(&head_sh->count);\n\t\t\tif (use_new_offset(conf, sh))\n\t\t\t\tbi->bi_iter.bi_sector = (sh->sector\n\t\t\t\t\t\t + rdev->new_data_offset);\n\t\t\telse\n\t\t\t\tbi->bi_iter.bi_sector = (sh->sector\n\t\t\t\t\t\t + rdev->data_offset);\n\t\t\tif (test_bit(R5_ReadNoMerge, &head_sh->dev[i].flags))\n\t\t\t\tbi->bi_opf |= REQ_NOMERGE;\n\n\t\t\tif (test_bit(R5_SkipCopy, &sh->dev[i].flags))\n\t\t\t\tWARN_ON(test_bit(R5_UPTODATE, &sh->dev[i].flags));\n\n\t\t\tif (!op_is_write(op) &&\n\t\t\t    test_bit(R5_InJournal, &sh->dev[i].flags))\n\t\t\t\t/*\n\t\t\t\t * issuing read for a page in journal, this\n\t\t\t\t * must be preparing for prexor in rmw; read\n\t\t\t\t * the data into orig_page\n\t\t\t\t */\n\t\t\t\tsh->dev[i].vec.bv_page = sh->dev[i].orig_page;\n\t\t\telse\n\t\t\t\tsh->dev[i].vec.bv_page = sh->dev[i].page;\n\t\t\tbi->bi_vcnt = 1;\n\t\t\tbi->bi_io_vec[0].bv_len = RAID5_STRIPE_SIZE(conf);\n\t\t\tbi->bi_io_vec[0].bv_offset = sh->dev[i].offset;\n\t\t\tbi->bi_iter.bi_size = RAID5_STRIPE_SIZE(conf);\n\t\t\tbi->bi_write_hint = sh->dev[i].write_hint;\n\t\t\tif (!rrdev)\n\t\t\t\tsh->dev[i].write_hint = RWH_WRITE_LIFE_NOT_SET;\n\t\t\t/*\n\t\t\t * If this is discard request, set bi_vcnt 0. We don't\n\t\t\t * want to confuse SCSI because SCSI will replace payload\n\t\t\t */\n\t\t\tif (op == REQ_OP_DISCARD)\n\t\t\t\tbi->bi_vcnt = 0;\n\t\t\tif (rrdev)\n\t\t\t\tset_bit(R5_DOUBLE_LOCKED, &sh->dev[i].flags);\n\n\t\t\tif (conf->mddev->gendisk)\n\t\t\t\ttrace_block_bio_remap(bi,\n\t\t\t\t\t\tdisk_devt(conf->mddev->gendisk),\n\t\t\t\t\t\tsh->dev[i].sector);\n\t\t\tif (should_defer && op_is_write(op))\n\t\t\t\tbio_list_add(&pending_bios, bi);\n\t\t\telse\n\t\t\t\tsubmit_bio_noacct(bi);\n\t\t}\n\t\tif (rrdev) {\n\t\t\tif (s->syncing || s->expanding || s->expanded\n\t\t\t    || s->replacing)\n\t\t\t\tmd_sync_acct(rrdev->bdev, RAID5_STRIPE_SECTORS(conf));\n\n\t\t\tset_bit(STRIPE_IO_STARTED, &sh->state);\n\n\t\t\tbio_set_dev(rbi, rrdev->bdev);\n\t\t\tbio_set_op_attrs(rbi, op, op_flags);\n\t\t\tBUG_ON(!op_is_write(op));\n\t\t\trbi->bi_end_io = raid5_end_write_request;\n\t\t\trbi->bi_private = sh;\n\n\t\t\tpr_debug(\"%s: for %llu schedule op %d on \"\n\t\t\t\t \"replacement disc %d\\n\",\n\t\t\t\t__func__, (unsigned long long)sh->sector,\n\t\t\t\trbi->bi_opf, i);\n\t\t\tatomic_inc(&sh->count);\n\t\t\tif (sh != head_sh)\n\t\t\t\tatomic_inc(&head_sh->count);\n\t\t\tif (use_new_offset(conf, sh))\n\t\t\t\trbi->bi_iter.bi_sector = (sh->sector\n\t\t\t\t\t\t  + rrdev->new_data_offset);\n\t\t\telse\n\t\t\t\trbi->bi_iter.bi_sector = (sh->sector\n\t\t\t\t\t\t  + rrdev->data_offset);\n\t\t\tif (test_bit(R5_SkipCopy, &sh->dev[i].flags))\n\t\t\t\tWARN_ON(test_bit(R5_UPTODATE, &sh->dev[i].flags));\n\t\t\tsh->dev[i].rvec.bv_page = sh->dev[i].page;\n\t\t\trbi->bi_vcnt = 1;\n\t\t\trbi->bi_io_vec[0].bv_len = RAID5_STRIPE_SIZE(conf);\n\t\t\trbi->bi_io_vec[0].bv_offset = sh->dev[i].offset;\n\t\t\trbi->bi_iter.bi_size = RAID5_STRIPE_SIZE(conf);\n\t\t\trbi->bi_write_hint = sh->dev[i].write_hint;\n\t\t\tsh->dev[i].write_hint = RWH_WRITE_LIFE_NOT_SET;\n\t\t\t/*\n\t\t\t * If this is discard request, set bi_vcnt 0. We don't\n\t\t\t * want to confuse SCSI because SCSI will replace payload\n\t\t\t */\n\t\t\tif (op == REQ_OP_DISCARD)\n\t\t\t\trbi->bi_vcnt = 0;\n\t\t\tif (conf->mddev->gendisk)\n\t\t\t\ttrace_block_bio_remap(rbi,\n\t\t\t\t\t\tdisk_devt(conf->mddev->gendisk),\n\t\t\t\t\t\tsh->dev[i].sector);\n\t\t\tif (should_defer && op_is_write(op))\n\t\t\t\tbio_list_add(&pending_bios, rbi);\n\t\t\telse\n\t\t\t\tsubmit_bio_noacct(rbi);\n\t\t}\n\t\tif (!rdev && !rrdev) {\n\t\t\tif (op_is_write(op))\n\t\t\t\tset_bit(STRIPE_DEGRADED, &sh->state);\n\t\t\tpr_debug(\"skip op %d on disc %d for sector %llu\\n\",\n\t\t\t\tbi->bi_opf, i, (unsigned long long)sh->sector);\n\t\t\tclear_bit(R5_LOCKED, &sh->dev[i].flags);\n\t\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\t}\n\n\t\tif (!head_sh->batch_head)\n\t\t\tcontinue;\n\t\tsh = list_first_entry(&sh->batch_list, struct stripe_head,\n\t\t\t\t      batch_list);\n\t\tif (sh != head_sh)\n\t\t\tgoto again;\n\t}\n\n\tif (should_defer && !bio_list_empty(&pending_bios))\n\t\tdefer_issue_bios(conf, head_sh->sector, &pending_bios);\n}\n\nstatic struct dma_async_tx_descriptor *\nasync_copy_data(int frombio, struct bio *bio, struct page **page,\n\tunsigned int poff, sector_t sector, struct dma_async_tx_descriptor *tx,\n\tstruct stripe_head *sh, int no_skipcopy)\n{\n\tstruct bio_vec bvl;\n\tstruct bvec_iter iter;\n\tstruct page *bio_page;\n\tint page_offset;\n\tstruct async_submit_ctl submit;\n\tenum async_tx_flags flags = 0;\n\tstruct r5conf *conf = sh->raid_conf;\n\n\tif (bio->bi_iter.bi_sector >= sector)\n\t\tpage_offset = (signed)(bio->bi_iter.bi_sector - sector) * 512;\n\telse\n\t\tpage_offset = (signed)(sector - bio->bi_iter.bi_sector) * -512;\n\n\tif (frombio)\n\t\tflags |= ASYNC_TX_FENCE;\n\tinit_async_submit(&submit, flags, tx, NULL, NULL, NULL);\n\n\tbio_for_each_segment(bvl, bio, iter) {\n\t\tint len = bvl.bv_len;\n\t\tint clen;\n\t\tint b_offset = 0;\n\n\t\tif (page_offset < 0) {\n\t\t\tb_offset = -page_offset;\n\t\t\tpage_offset += b_offset;\n\t\t\tlen -= b_offset;\n\t\t}\n\n\t\tif (len > 0 && page_offset + len > RAID5_STRIPE_SIZE(conf))\n\t\t\tclen = RAID5_STRIPE_SIZE(conf) - page_offset;\n\t\telse\n\t\t\tclen = len;\n\n\t\tif (clen > 0) {\n\t\t\tb_offset += bvl.bv_offset;\n\t\t\tbio_page = bvl.bv_page;\n\t\t\tif (frombio) {\n\t\t\t\tif (conf->skip_copy &&\n\t\t\t\t    b_offset == 0 && page_offset == 0 &&\n\t\t\t\t    clen == RAID5_STRIPE_SIZE(conf) &&\n\t\t\t\t    !no_skipcopy)\n\t\t\t\t\t*page = bio_page;\n\t\t\t\telse\n\t\t\t\t\ttx = async_memcpy(*page, bio_page, page_offset + poff,\n\t\t\t\t\t\t  b_offset, clen, &submit);\n\t\t\t} else\n\t\t\t\ttx = async_memcpy(bio_page, *page, b_offset,\n\t\t\t\t\t\t  page_offset + poff, clen, &submit);\n\t\t}\n\t\t/* chain the operations */\n\t\tsubmit.depend_tx = tx;\n\n\t\tif (clen < len) /* hit end of page */\n\t\t\tbreak;\n\t\tpage_offset +=  len;\n\t}\n\n\treturn tx;\n}\n\nstatic void ops_complete_biofill(void *stripe_head_ref)\n{\n\tstruct stripe_head *sh = stripe_head_ref;\n\tint i;\n\tstruct r5conf *conf = sh->raid_conf;\n\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\t/* clear completed biofills */\n\tfor (i = sh->disks; i--; ) {\n\t\tstruct r5dev *dev = &sh->dev[i];\n\n\t\t/* acknowledge completion of a biofill operation */\n\t\t/* and check if we need to reply to a read request,\n\t\t * new R5_Wantfill requests are held off until\n\t\t * !STRIPE_BIOFILL_RUN\n\t\t */\n\t\tif (test_and_clear_bit(R5_Wantfill, &dev->flags)) {\n\t\t\tstruct bio *rbi, *rbi2;\n\n\t\t\tBUG_ON(!dev->read);\n\t\t\trbi = dev->read;\n\t\t\tdev->read = NULL;\n\t\t\twhile (rbi && rbi->bi_iter.bi_sector <\n\t\t\t\tdev->sector + RAID5_STRIPE_SECTORS(conf)) {\n\t\t\t\trbi2 = r5_next_bio(conf, rbi, dev->sector);\n\t\t\t\tbio_endio(rbi);\n\t\t\t\trbi = rbi2;\n\t\t\t}\n\t\t}\n\t}\n\tclear_bit(STRIPE_BIOFILL_RUN, &sh->state);\n\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\traid5_release_stripe(sh);\n}\n\nstatic void ops_run_biofill(struct stripe_head *sh)\n{\n\tstruct dma_async_tx_descriptor *tx = NULL;\n\tstruct async_submit_ctl submit;\n\tint i;\n\tstruct r5conf *conf = sh->raid_conf;\n\n\tBUG_ON(sh->batch_head);\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\tfor (i = sh->disks; i--; ) {\n\t\tstruct r5dev *dev = &sh->dev[i];\n\t\tif (test_bit(R5_Wantfill, &dev->flags)) {\n\t\t\tstruct bio *rbi;\n\t\t\tspin_lock_irq(&sh->stripe_lock);\n\t\t\tdev->read = rbi = dev->toread;\n\t\t\tdev->toread = NULL;\n\t\t\tspin_unlock_irq(&sh->stripe_lock);\n\t\t\twhile (rbi && rbi->bi_iter.bi_sector <\n\t\t\t\tdev->sector + RAID5_STRIPE_SECTORS(conf)) {\n\t\t\t\ttx = async_copy_data(0, rbi, &dev->page,\n\t\t\t\t\t\t     dev->offset,\n\t\t\t\t\t\t     dev->sector, tx, sh, 0);\n\t\t\t\trbi = r5_next_bio(conf, rbi, dev->sector);\n\t\t\t}\n\t\t}\n\t}\n\n\tatomic_inc(&sh->count);\n\tinit_async_submit(&submit, ASYNC_TX_ACK, tx, ops_complete_biofill, sh, NULL);\n\tasync_trigger_callback(&submit);\n}\n\nstatic void mark_target_uptodate(struct stripe_head *sh, int target)\n{\n\tstruct r5dev *tgt;\n\n\tif (target < 0)\n\t\treturn;\n\n\ttgt = &sh->dev[target];\n\tset_bit(R5_UPTODATE, &tgt->flags);\n\tBUG_ON(!test_bit(R5_Wantcompute, &tgt->flags));\n\tclear_bit(R5_Wantcompute, &tgt->flags);\n}\n\nstatic void ops_complete_compute(void *stripe_head_ref)\n{\n\tstruct stripe_head *sh = stripe_head_ref;\n\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\t/* mark the computed target(s) as uptodate */\n\tmark_target_uptodate(sh, sh->ops.target);\n\tmark_target_uptodate(sh, sh->ops.target2);\n\n\tclear_bit(STRIPE_COMPUTE_RUN, &sh->state);\n\tif (sh->check_state == check_state_compute_run)\n\t\tsh->check_state = check_state_compute_result;\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\traid5_release_stripe(sh);\n}\n\n/* return a pointer to the address conversion region of the scribble buffer */\nstatic struct page **to_addr_page(struct raid5_percpu *percpu, int i)\n{\n\treturn percpu->scribble + i * percpu->scribble_obj_size;\n}\n\n/* return a pointer to the address conversion region of the scribble buffer */\nstatic addr_conv_t *to_addr_conv(struct stripe_head *sh,\n\t\t\t\t struct raid5_percpu *percpu, int i)\n{\n\treturn (void *) (to_addr_page(percpu, i) + sh->disks + 2);\n}\n\n/*\n * Return a pointer to record offset address.\n */\nstatic unsigned int *\nto_addr_offs(struct stripe_head *sh, struct raid5_percpu *percpu)\n{\n\treturn (unsigned int *) (to_addr_conv(sh, percpu, 0) + sh->disks + 2);\n}\n\nstatic struct dma_async_tx_descriptor *\nops_run_compute5(struct stripe_head *sh, struct raid5_percpu *percpu)\n{\n\tint disks = sh->disks;\n\tstruct page **xor_srcs = to_addr_page(percpu, 0);\n\tunsigned int *off_srcs = to_addr_offs(sh, percpu);\n\tint target = sh->ops.target;\n\tstruct r5dev *tgt = &sh->dev[target];\n\tstruct page *xor_dest = tgt->page;\n\tunsigned int off_dest = tgt->offset;\n\tint count = 0;\n\tstruct dma_async_tx_descriptor *tx;\n\tstruct async_submit_ctl submit;\n\tint i;\n\n\tBUG_ON(sh->batch_head);\n\n\tpr_debug(\"%s: stripe %llu block: %d\\n\",\n\t\t__func__, (unsigned long long)sh->sector, target);\n\tBUG_ON(!test_bit(R5_Wantcompute, &tgt->flags));\n\n\tfor (i = disks; i--; ) {\n\t\tif (i != target) {\n\t\t\toff_srcs[count] = sh->dev[i].offset;\n\t\t\txor_srcs[count++] = sh->dev[i].page;\n\t\t}\n\t}\n\n\tatomic_inc(&sh->count);\n\n\tinit_async_submit(&submit, ASYNC_TX_FENCE|ASYNC_TX_XOR_ZERO_DST, NULL,\n\t\t\t  ops_complete_compute, sh, to_addr_conv(sh, percpu, 0));\n\tif (unlikely(count == 1))\n\t\ttx = async_memcpy(xor_dest, xor_srcs[0], off_dest, off_srcs[0],\n\t\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf), &submit);\n\telse\n\t\ttx = async_xor_offs(xor_dest, off_dest, xor_srcs, off_srcs, count,\n\t\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf), &submit);\n\n\treturn tx;\n}\n\n/* set_syndrome_sources - populate source buffers for gen_syndrome\n * @srcs - (struct page *) array of size sh->disks\n * @offs - (unsigned int) array of offset for each page\n * @sh - stripe_head to parse\n *\n * Populates srcs in proper layout order for the stripe and returns the\n * 'count' of sources to be used in a call to async_gen_syndrome.  The P\n * destination buffer is recorded in srcs[count] and the Q destination\n * is recorded in srcs[count+1]].\n */\nstatic int set_syndrome_sources(struct page **srcs,\n\t\t\t\tunsigned int *offs,\n\t\t\t\tstruct stripe_head *sh,\n\t\t\t\tint srctype)\n{\n\tint disks = sh->disks;\n\tint syndrome_disks = sh->ddf_layout ? disks : (disks - 2);\n\tint d0_idx = raid6_d0(sh);\n\tint count;\n\tint i;\n\n\tfor (i = 0; i < disks; i++)\n\t\tsrcs[i] = NULL;\n\n\tcount = 0;\n\ti = d0_idx;\n\tdo {\n\t\tint slot = raid6_idx_to_slot(i, sh, &count, syndrome_disks);\n\t\tstruct r5dev *dev = &sh->dev[i];\n\n\t\tif (i == sh->qd_idx || i == sh->pd_idx ||\n\t\t    (srctype == SYNDROME_SRC_ALL) ||\n\t\t    (srctype == SYNDROME_SRC_WANT_DRAIN &&\n\t\t     (test_bit(R5_Wantdrain, &dev->flags) ||\n\t\t      test_bit(R5_InJournal, &dev->flags))) ||\n\t\t    (srctype == SYNDROME_SRC_WRITTEN &&\n\t\t     (dev->written ||\n\t\t      test_bit(R5_InJournal, &dev->flags)))) {\n\t\t\tif (test_bit(R5_InJournal, &dev->flags))\n\t\t\t\tsrcs[slot] = sh->dev[i].orig_page;\n\t\t\telse\n\t\t\t\tsrcs[slot] = sh->dev[i].page;\n\t\t\t/*\n\t\t\t * For R5_InJournal, PAGE_SIZE must be 4KB and will\n\t\t\t * not shared page. In that case, dev[i].offset\n\t\t\t * is 0.\n\t\t\t */\n\t\t\toffs[slot] = sh->dev[i].offset;\n\t\t}\n\t\ti = raid6_next_disk(i, disks);\n\t} while (i != d0_idx);\n\n\treturn syndrome_disks;\n}\n\nstatic struct dma_async_tx_descriptor *\nops_run_compute6_1(struct stripe_head *sh, struct raid5_percpu *percpu)\n{\n\tint disks = sh->disks;\n\tstruct page **blocks = to_addr_page(percpu, 0);\n\tunsigned int *offs = to_addr_offs(sh, percpu);\n\tint target;\n\tint qd_idx = sh->qd_idx;\n\tstruct dma_async_tx_descriptor *tx;\n\tstruct async_submit_ctl submit;\n\tstruct r5dev *tgt;\n\tstruct page *dest;\n\tunsigned int dest_off;\n\tint i;\n\tint count;\n\n\tBUG_ON(sh->batch_head);\n\tif (sh->ops.target < 0)\n\t\ttarget = sh->ops.target2;\n\telse if (sh->ops.target2 < 0)\n\t\ttarget = sh->ops.target;\n\telse\n\t\t/* we should only have one valid target */\n\t\tBUG();\n\tBUG_ON(target < 0);\n\tpr_debug(\"%s: stripe %llu block: %d\\n\",\n\t\t__func__, (unsigned long long)sh->sector, target);\n\n\ttgt = &sh->dev[target];\n\tBUG_ON(!test_bit(R5_Wantcompute, &tgt->flags));\n\tdest = tgt->page;\n\tdest_off = tgt->offset;\n\n\tatomic_inc(&sh->count);\n\n\tif (target == qd_idx) {\n\t\tcount = set_syndrome_sources(blocks, offs, sh, SYNDROME_SRC_ALL);\n\t\tblocks[count] = NULL; /* regenerating p is not necessary */\n\t\tBUG_ON(blocks[count+1] != dest); /* q should already be set */\n\t\tinit_async_submit(&submit, ASYNC_TX_FENCE, NULL,\n\t\t\t\t  ops_complete_compute, sh,\n\t\t\t\t  to_addr_conv(sh, percpu, 0));\n\t\ttx = async_gen_syndrome(blocks, offs, count+2,\n\t\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf), &submit);\n\t} else {\n\t\t/* Compute any data- or p-drive using XOR */\n\t\tcount = 0;\n\t\tfor (i = disks; i-- ; ) {\n\t\t\tif (i == target || i == qd_idx)\n\t\t\t\tcontinue;\n\t\t\toffs[count] = sh->dev[i].offset;\n\t\t\tblocks[count++] = sh->dev[i].page;\n\t\t}\n\n\t\tinit_async_submit(&submit, ASYNC_TX_FENCE|ASYNC_TX_XOR_ZERO_DST,\n\t\t\t\t  NULL, ops_complete_compute, sh,\n\t\t\t\t  to_addr_conv(sh, percpu, 0));\n\t\ttx = async_xor_offs(dest, dest_off, blocks, offs, count,\n\t\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf), &submit);\n\t}\n\n\treturn tx;\n}\n\nstatic struct dma_async_tx_descriptor *\nops_run_compute6_2(struct stripe_head *sh, struct raid5_percpu *percpu)\n{\n\tint i, count, disks = sh->disks;\n\tint syndrome_disks = sh->ddf_layout ? disks : disks-2;\n\tint d0_idx = raid6_d0(sh);\n\tint faila = -1, failb = -1;\n\tint target = sh->ops.target;\n\tint target2 = sh->ops.target2;\n\tstruct r5dev *tgt = &sh->dev[target];\n\tstruct r5dev *tgt2 = &sh->dev[target2];\n\tstruct dma_async_tx_descriptor *tx;\n\tstruct page **blocks = to_addr_page(percpu, 0);\n\tunsigned int *offs = to_addr_offs(sh, percpu);\n\tstruct async_submit_ctl submit;\n\n\tBUG_ON(sh->batch_head);\n\tpr_debug(\"%s: stripe %llu block1: %d block2: %d\\n\",\n\t\t __func__, (unsigned long long)sh->sector, target, target2);\n\tBUG_ON(target < 0 || target2 < 0);\n\tBUG_ON(!test_bit(R5_Wantcompute, &tgt->flags));\n\tBUG_ON(!test_bit(R5_Wantcompute, &tgt2->flags));\n\n\t/* we need to open-code set_syndrome_sources to handle the\n\t * slot number conversion for 'faila' and 'failb'\n\t */\n\tfor (i = 0; i < disks ; i++) {\n\t\toffs[i] = 0;\n\t\tblocks[i] = NULL;\n\t}\n\tcount = 0;\n\ti = d0_idx;\n\tdo {\n\t\tint slot = raid6_idx_to_slot(i, sh, &count, syndrome_disks);\n\n\t\toffs[slot] = sh->dev[i].offset;\n\t\tblocks[slot] = sh->dev[i].page;\n\n\t\tif (i == target)\n\t\t\tfaila = slot;\n\t\tif (i == target2)\n\t\t\tfailb = slot;\n\t\ti = raid6_next_disk(i, disks);\n\t} while (i != d0_idx);\n\n\tBUG_ON(faila == failb);\n\tif (failb < faila)\n\t\tswap(faila, failb);\n\tpr_debug(\"%s: stripe: %llu faila: %d failb: %d\\n\",\n\t\t __func__, (unsigned long long)sh->sector, faila, failb);\n\n\tatomic_inc(&sh->count);\n\n\tif (failb == syndrome_disks+1) {\n\t\t/* Q disk is one of the missing disks */\n\t\tif (faila == syndrome_disks) {\n\t\t\t/* Missing P+Q, just recompute */\n\t\t\tinit_async_submit(&submit, ASYNC_TX_FENCE, NULL,\n\t\t\t\t\t  ops_complete_compute, sh,\n\t\t\t\t\t  to_addr_conv(sh, percpu, 0));\n\t\t\treturn async_gen_syndrome(blocks, offs, syndrome_disks+2,\n\t\t\t\t\t\t  RAID5_STRIPE_SIZE(sh->raid_conf),\n\t\t\t\t\t\t  &submit);\n\t\t} else {\n\t\t\tstruct page *dest;\n\t\t\tunsigned int dest_off;\n\t\t\tint data_target;\n\t\t\tint qd_idx = sh->qd_idx;\n\n\t\t\t/* Missing D+Q: recompute D from P, then recompute Q */\n\t\t\tif (target == qd_idx)\n\t\t\t\tdata_target = target2;\n\t\t\telse\n\t\t\t\tdata_target = target;\n\n\t\t\tcount = 0;\n\t\t\tfor (i = disks; i-- ; ) {\n\t\t\t\tif (i == data_target || i == qd_idx)\n\t\t\t\t\tcontinue;\n\t\t\t\toffs[count] = sh->dev[i].offset;\n\t\t\t\tblocks[count++] = sh->dev[i].page;\n\t\t\t}\n\t\t\tdest = sh->dev[data_target].page;\n\t\t\tdest_off = sh->dev[data_target].offset;\n\t\t\tinit_async_submit(&submit,\n\t\t\t\t\t  ASYNC_TX_FENCE|ASYNC_TX_XOR_ZERO_DST,\n\t\t\t\t\t  NULL, NULL, NULL,\n\t\t\t\t\t  to_addr_conv(sh, percpu, 0));\n\t\t\ttx = async_xor_offs(dest, dest_off, blocks, offs, count,\n\t\t\t\t       RAID5_STRIPE_SIZE(sh->raid_conf),\n\t\t\t\t       &submit);\n\n\t\t\tcount = set_syndrome_sources(blocks, offs, sh, SYNDROME_SRC_ALL);\n\t\t\tinit_async_submit(&submit, ASYNC_TX_FENCE, tx,\n\t\t\t\t\t  ops_complete_compute, sh,\n\t\t\t\t\t  to_addr_conv(sh, percpu, 0));\n\t\t\treturn async_gen_syndrome(blocks, offs, count+2,\n\t\t\t\t\t\t  RAID5_STRIPE_SIZE(sh->raid_conf),\n\t\t\t\t\t\t  &submit);\n\t\t}\n\t} else {\n\t\tinit_async_submit(&submit, ASYNC_TX_FENCE, NULL,\n\t\t\t\t  ops_complete_compute, sh,\n\t\t\t\t  to_addr_conv(sh, percpu, 0));\n\t\tif (failb == syndrome_disks) {\n\t\t\t/* We're missing D+P. */\n\t\t\treturn async_raid6_datap_recov(syndrome_disks+2,\n\t\t\t\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf),\n\t\t\t\t\t\tfaila,\n\t\t\t\t\t\tblocks, offs, &submit);\n\t\t} else {\n\t\t\t/* We're missing D+D. */\n\t\t\treturn async_raid6_2data_recov(syndrome_disks+2,\n\t\t\t\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf),\n\t\t\t\t\t\tfaila, failb,\n\t\t\t\t\t\tblocks, offs, &submit);\n\t\t}\n\t}\n}\n\nstatic void ops_complete_prexor(void *stripe_head_ref)\n{\n\tstruct stripe_head *sh = stripe_head_ref;\n\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\tif (r5c_is_writeback(sh->raid_conf->log))\n\t\t/*\n\t\t * raid5-cache write back uses orig_page during prexor.\n\t\t * After prexor, it is time to free orig_page\n\t\t */\n\t\tr5c_release_extra_page(sh);\n}\n\nstatic struct dma_async_tx_descriptor *\nops_run_prexor5(struct stripe_head *sh, struct raid5_percpu *percpu,\n\t\tstruct dma_async_tx_descriptor *tx)\n{\n\tint disks = sh->disks;\n\tstruct page **xor_srcs = to_addr_page(percpu, 0);\n\tunsigned int *off_srcs = to_addr_offs(sh, percpu);\n\tint count = 0, pd_idx = sh->pd_idx, i;\n\tstruct async_submit_ctl submit;\n\n\t/* existing parity data subtracted */\n\tunsigned int off_dest = off_srcs[count] = sh->dev[pd_idx].offset;\n\tstruct page *xor_dest = xor_srcs[count++] = sh->dev[pd_idx].page;\n\n\tBUG_ON(sh->batch_head);\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\tfor (i = disks; i--; ) {\n\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t/* Only process blocks that are known to be uptodate */\n\t\tif (test_bit(R5_InJournal, &dev->flags)) {\n\t\t\t/*\n\t\t\t * For this case, PAGE_SIZE must be equal to 4KB and\n\t\t\t * page offset is zero.\n\t\t\t */\n\t\t\toff_srcs[count] = dev->offset;\n\t\t\txor_srcs[count++] = dev->orig_page;\n\t\t} else if (test_bit(R5_Wantdrain, &dev->flags)) {\n\t\t\toff_srcs[count] = dev->offset;\n\t\t\txor_srcs[count++] = dev->page;\n\t\t}\n\t}\n\n\tinit_async_submit(&submit, ASYNC_TX_FENCE|ASYNC_TX_XOR_DROP_DST, tx,\n\t\t\t  ops_complete_prexor, sh, to_addr_conv(sh, percpu, 0));\n\ttx = async_xor_offs(xor_dest, off_dest, xor_srcs, off_srcs, count,\n\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf), &submit);\n\n\treturn tx;\n}\n\nstatic struct dma_async_tx_descriptor *\nops_run_prexor6(struct stripe_head *sh, struct raid5_percpu *percpu,\n\t\tstruct dma_async_tx_descriptor *tx)\n{\n\tstruct page **blocks = to_addr_page(percpu, 0);\n\tunsigned int *offs = to_addr_offs(sh, percpu);\n\tint count;\n\tstruct async_submit_ctl submit;\n\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\tcount = set_syndrome_sources(blocks, offs, sh, SYNDROME_SRC_WANT_DRAIN);\n\n\tinit_async_submit(&submit, ASYNC_TX_FENCE|ASYNC_TX_PQ_XOR_DST, tx,\n\t\t\t  ops_complete_prexor, sh, to_addr_conv(sh, percpu, 0));\n\ttx = async_gen_syndrome(blocks, offs, count+2,\n\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf), &submit);\n\n\treturn tx;\n}\n\nstatic struct dma_async_tx_descriptor *\nops_run_biodrain(struct stripe_head *sh, struct dma_async_tx_descriptor *tx)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\tint disks = sh->disks;\n\tint i;\n\tstruct stripe_head *head_sh = sh;\n\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\tfor (i = disks; i--; ) {\n\t\tstruct r5dev *dev;\n\t\tstruct bio *chosen;\n\n\t\tsh = head_sh;\n\t\tif (test_and_clear_bit(R5_Wantdrain, &head_sh->dev[i].flags)) {\n\t\t\tstruct bio *wbi;\n\nagain:\n\t\t\tdev = &sh->dev[i];\n\t\t\t/*\n\t\t\t * clear R5_InJournal, so when rewriting a page in\n\t\t\t * journal, it is not skipped by r5l_log_stripe()\n\t\t\t */\n\t\t\tclear_bit(R5_InJournal, &dev->flags);\n\t\t\tspin_lock_irq(&sh->stripe_lock);\n\t\t\tchosen = dev->towrite;\n\t\t\tdev->towrite = NULL;\n\t\t\tsh->overwrite_disks = 0;\n\t\t\tBUG_ON(dev->written);\n\t\t\twbi = dev->written = chosen;\n\t\t\tspin_unlock_irq(&sh->stripe_lock);\n\t\t\tWARN_ON(dev->page != dev->orig_page);\n\n\t\t\twhile (wbi && wbi->bi_iter.bi_sector <\n\t\t\t\tdev->sector + RAID5_STRIPE_SECTORS(conf)) {\n\t\t\t\tif (wbi->bi_opf & REQ_FUA)\n\t\t\t\t\tset_bit(R5_WantFUA, &dev->flags);\n\t\t\t\tif (wbi->bi_opf & REQ_SYNC)\n\t\t\t\t\tset_bit(R5_SyncIO, &dev->flags);\n\t\t\t\tif (bio_op(wbi) == REQ_OP_DISCARD)\n\t\t\t\t\tset_bit(R5_Discard, &dev->flags);\n\t\t\t\telse {\n\t\t\t\t\ttx = async_copy_data(1, wbi, &dev->page,\n\t\t\t\t\t\t\t     dev->offset,\n\t\t\t\t\t\t\t     dev->sector, tx, sh,\n\t\t\t\t\t\t\t     r5c_is_writeback(conf->log));\n\t\t\t\t\tif (dev->page != dev->orig_page &&\n\t\t\t\t\t    !r5c_is_writeback(conf->log)) {\n\t\t\t\t\t\tset_bit(R5_SkipCopy, &dev->flags);\n\t\t\t\t\t\tclear_bit(R5_UPTODATE, &dev->flags);\n\t\t\t\t\t\tclear_bit(R5_OVERWRITE, &dev->flags);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\twbi = r5_next_bio(conf, wbi, dev->sector);\n\t\t\t}\n\n\t\t\tif (head_sh->batch_head) {\n\t\t\t\tsh = list_first_entry(&sh->batch_list,\n\t\t\t\t\t\t      struct stripe_head,\n\t\t\t\t\t\t      batch_list);\n\t\t\t\tif (sh == head_sh)\n\t\t\t\t\tcontinue;\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn tx;\n}\n\nstatic void ops_complete_reconstruct(void *stripe_head_ref)\n{\n\tstruct stripe_head *sh = stripe_head_ref;\n\tint disks = sh->disks;\n\tint pd_idx = sh->pd_idx;\n\tint qd_idx = sh->qd_idx;\n\tint i;\n\tbool fua = false, sync = false, discard = false;\n\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\tfor (i = disks; i--; ) {\n\t\tfua |= test_bit(R5_WantFUA, &sh->dev[i].flags);\n\t\tsync |= test_bit(R5_SyncIO, &sh->dev[i].flags);\n\t\tdiscard |= test_bit(R5_Discard, &sh->dev[i].flags);\n\t}\n\n\tfor (i = disks; i--; ) {\n\t\tstruct r5dev *dev = &sh->dev[i];\n\n\t\tif (dev->written || i == pd_idx || i == qd_idx) {\n\t\t\tif (!discard && !test_bit(R5_SkipCopy, &dev->flags)) {\n\t\t\t\tset_bit(R5_UPTODATE, &dev->flags);\n\t\t\t\tif (test_bit(STRIPE_EXPAND_READY, &sh->state))\n\t\t\t\t\tset_bit(R5_Expanded, &dev->flags);\n\t\t\t}\n\t\t\tif (fua)\n\t\t\t\tset_bit(R5_WantFUA, &dev->flags);\n\t\t\tif (sync)\n\t\t\t\tset_bit(R5_SyncIO, &dev->flags);\n\t\t}\n\t}\n\n\tif (sh->reconstruct_state == reconstruct_state_drain_run)\n\t\tsh->reconstruct_state = reconstruct_state_drain_result;\n\telse if (sh->reconstruct_state == reconstruct_state_prexor_drain_run)\n\t\tsh->reconstruct_state = reconstruct_state_prexor_drain_result;\n\telse {\n\t\tBUG_ON(sh->reconstruct_state != reconstruct_state_run);\n\t\tsh->reconstruct_state = reconstruct_state_result;\n\t}\n\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\traid5_release_stripe(sh);\n}\n\nstatic void\nops_run_reconstruct5(struct stripe_head *sh, struct raid5_percpu *percpu,\n\t\t     struct dma_async_tx_descriptor *tx)\n{\n\tint disks = sh->disks;\n\tstruct page **xor_srcs;\n\tunsigned int *off_srcs;\n\tstruct async_submit_ctl submit;\n\tint count, pd_idx = sh->pd_idx, i;\n\tstruct page *xor_dest;\n\tunsigned int off_dest;\n\tint prexor = 0;\n\tunsigned long flags;\n\tint j = 0;\n\tstruct stripe_head *head_sh = sh;\n\tint last_stripe;\n\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\tfor (i = 0; i < sh->disks; i++) {\n\t\tif (pd_idx == i)\n\t\t\tcontinue;\n\t\tif (!test_bit(R5_Discard, &sh->dev[i].flags))\n\t\t\tbreak;\n\t}\n\tif (i >= sh->disks) {\n\t\tatomic_inc(&sh->count);\n\t\tset_bit(R5_Discard, &sh->dev[pd_idx].flags);\n\t\tops_complete_reconstruct(sh);\n\t\treturn;\n\t}\nagain:\n\tcount = 0;\n\txor_srcs = to_addr_page(percpu, j);\n\toff_srcs = to_addr_offs(sh, percpu);\n\t/* check if prexor is active which means only process blocks\n\t * that are part of a read-modify-write (written)\n\t */\n\tif (head_sh->reconstruct_state == reconstruct_state_prexor_drain_run) {\n\t\tprexor = 1;\n\t\toff_dest = off_srcs[count] = sh->dev[pd_idx].offset;\n\t\txor_dest = xor_srcs[count++] = sh->dev[pd_idx].page;\n\t\tfor (i = disks; i--; ) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t\tif (head_sh->dev[i].written ||\n\t\t\t    test_bit(R5_InJournal, &head_sh->dev[i].flags)) {\n\t\t\t\toff_srcs[count] = dev->offset;\n\t\t\t\txor_srcs[count++] = dev->page;\n\t\t\t}\n\t\t}\n\t} else {\n\t\txor_dest = sh->dev[pd_idx].page;\n\t\toff_dest = sh->dev[pd_idx].offset;\n\t\tfor (i = disks; i--; ) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t\tif (i != pd_idx) {\n\t\t\t\toff_srcs[count] = dev->offset;\n\t\t\t\txor_srcs[count++] = dev->page;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* 1/ if we prexor'd then the dest is reused as a source\n\t * 2/ if we did not prexor then we are redoing the parity\n\t * set ASYNC_TX_XOR_DROP_DST and ASYNC_TX_XOR_ZERO_DST\n\t * for the synchronous xor case\n\t */\n\tlast_stripe = !head_sh->batch_head ||\n\t\tlist_first_entry(&sh->batch_list,\n\t\t\t\t struct stripe_head, batch_list) == head_sh;\n\tif (last_stripe) {\n\t\tflags = ASYNC_TX_ACK |\n\t\t\t(prexor ? ASYNC_TX_XOR_DROP_DST : ASYNC_TX_XOR_ZERO_DST);\n\n\t\tatomic_inc(&head_sh->count);\n\t\tinit_async_submit(&submit, flags, tx, ops_complete_reconstruct, head_sh,\n\t\t\t\t  to_addr_conv(sh, percpu, j));\n\t} else {\n\t\tflags = prexor ? ASYNC_TX_XOR_DROP_DST : ASYNC_TX_XOR_ZERO_DST;\n\t\tinit_async_submit(&submit, flags, tx, NULL, NULL,\n\t\t\t\t  to_addr_conv(sh, percpu, j));\n\t}\n\n\tif (unlikely(count == 1))\n\t\ttx = async_memcpy(xor_dest, xor_srcs[0], off_dest, off_srcs[0],\n\t\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf), &submit);\n\telse\n\t\ttx = async_xor_offs(xor_dest, off_dest, xor_srcs, off_srcs, count,\n\t\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf), &submit);\n\tif (!last_stripe) {\n\t\tj++;\n\t\tsh = list_first_entry(&sh->batch_list, struct stripe_head,\n\t\t\t\t      batch_list);\n\t\tgoto again;\n\t}\n}\n\nstatic void\nops_run_reconstruct6(struct stripe_head *sh, struct raid5_percpu *percpu,\n\t\t     struct dma_async_tx_descriptor *tx)\n{\n\tstruct async_submit_ctl submit;\n\tstruct page **blocks;\n\tunsigned int *offs;\n\tint count, i, j = 0;\n\tstruct stripe_head *head_sh = sh;\n\tint last_stripe;\n\tint synflags;\n\tunsigned long txflags;\n\n\tpr_debug(\"%s: stripe %llu\\n\", __func__, (unsigned long long)sh->sector);\n\n\tfor (i = 0; i < sh->disks; i++) {\n\t\tif (sh->pd_idx == i || sh->qd_idx == i)\n\t\t\tcontinue;\n\t\tif (!test_bit(R5_Discard, &sh->dev[i].flags))\n\t\t\tbreak;\n\t}\n\tif (i >= sh->disks) {\n\t\tatomic_inc(&sh->count);\n\t\tset_bit(R5_Discard, &sh->dev[sh->pd_idx].flags);\n\t\tset_bit(R5_Discard, &sh->dev[sh->qd_idx].flags);\n\t\tops_complete_reconstruct(sh);\n\t\treturn;\n\t}\n\nagain:\n\tblocks = to_addr_page(percpu, j);\n\toffs = to_addr_offs(sh, percpu);\n\n\tif (sh->reconstruct_state == reconstruct_state_prexor_drain_run) {\n\t\tsynflags = SYNDROME_SRC_WRITTEN;\n\t\ttxflags = ASYNC_TX_ACK | ASYNC_TX_PQ_XOR_DST;\n\t} else {\n\t\tsynflags = SYNDROME_SRC_ALL;\n\t\ttxflags = ASYNC_TX_ACK;\n\t}\n\n\tcount = set_syndrome_sources(blocks, offs, sh, synflags);\n\tlast_stripe = !head_sh->batch_head ||\n\t\tlist_first_entry(&sh->batch_list,\n\t\t\t\t struct stripe_head, batch_list) == head_sh;\n\n\tif (last_stripe) {\n\t\tatomic_inc(&head_sh->count);\n\t\tinit_async_submit(&submit, txflags, tx, ops_complete_reconstruct,\n\t\t\t\t  head_sh, to_addr_conv(sh, percpu, j));\n\t} else\n\t\tinit_async_submit(&submit, 0, tx, NULL, NULL,\n\t\t\t\t  to_addr_conv(sh, percpu, j));\n\ttx = async_gen_syndrome(blocks, offs, count+2,\n\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf),  &submit);\n\tif (!last_stripe) {\n\t\tj++;\n\t\tsh = list_first_entry(&sh->batch_list, struct stripe_head,\n\t\t\t\t      batch_list);\n\t\tgoto again;\n\t}\n}\n\nstatic void ops_complete_check(void *stripe_head_ref)\n{\n\tstruct stripe_head *sh = stripe_head_ref;\n\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\tsh->check_state = check_state_check_result;\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\traid5_release_stripe(sh);\n}\n\nstatic void ops_run_check_p(struct stripe_head *sh, struct raid5_percpu *percpu)\n{\n\tint disks = sh->disks;\n\tint pd_idx = sh->pd_idx;\n\tint qd_idx = sh->qd_idx;\n\tstruct page *xor_dest;\n\tunsigned int off_dest;\n\tstruct page **xor_srcs = to_addr_page(percpu, 0);\n\tunsigned int *off_srcs = to_addr_offs(sh, percpu);\n\tstruct dma_async_tx_descriptor *tx;\n\tstruct async_submit_ctl submit;\n\tint count;\n\tint i;\n\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\tBUG_ON(sh->batch_head);\n\tcount = 0;\n\txor_dest = sh->dev[pd_idx].page;\n\toff_dest = sh->dev[pd_idx].offset;\n\toff_srcs[count] = off_dest;\n\txor_srcs[count++] = xor_dest;\n\tfor (i = disks; i--; ) {\n\t\tif (i == pd_idx || i == qd_idx)\n\t\t\tcontinue;\n\t\toff_srcs[count] = sh->dev[i].offset;\n\t\txor_srcs[count++] = sh->dev[i].page;\n\t}\n\n\tinit_async_submit(&submit, 0, NULL, NULL, NULL,\n\t\t\t  to_addr_conv(sh, percpu, 0));\n\ttx = async_xor_val_offs(xor_dest, off_dest, xor_srcs, off_srcs, count,\n\t\t\t   RAID5_STRIPE_SIZE(sh->raid_conf),\n\t\t\t   &sh->ops.zero_sum_result, &submit);\n\n\tatomic_inc(&sh->count);\n\tinit_async_submit(&submit, ASYNC_TX_ACK, tx, ops_complete_check, sh, NULL);\n\ttx = async_trigger_callback(&submit);\n}\n\nstatic void ops_run_check_pq(struct stripe_head *sh, struct raid5_percpu *percpu, int checkp)\n{\n\tstruct page **srcs = to_addr_page(percpu, 0);\n\tunsigned int *offs = to_addr_offs(sh, percpu);\n\tstruct async_submit_ctl submit;\n\tint count;\n\n\tpr_debug(\"%s: stripe %llu checkp: %d\\n\", __func__,\n\t\t(unsigned long long)sh->sector, checkp);\n\n\tBUG_ON(sh->batch_head);\n\tcount = set_syndrome_sources(srcs, offs, sh, SYNDROME_SRC_ALL);\n\tif (!checkp)\n\t\tsrcs[count] = NULL;\n\n\tatomic_inc(&sh->count);\n\tinit_async_submit(&submit, ASYNC_TX_ACK, NULL, ops_complete_check,\n\t\t\t  sh, to_addr_conv(sh, percpu, 0));\n\tasync_syndrome_val(srcs, offs, count+2,\n\t\t\t   RAID5_STRIPE_SIZE(sh->raid_conf),\n\t\t\t   &sh->ops.zero_sum_result, percpu->spare_page, 0, &submit);\n}\n\nstatic void raid_run_ops(struct stripe_head *sh, unsigned long ops_request)\n{\n\tint overlap_clear = 0, i, disks = sh->disks;\n\tstruct dma_async_tx_descriptor *tx = NULL;\n\tstruct r5conf *conf = sh->raid_conf;\n\tint level = conf->level;\n\tstruct raid5_percpu *percpu;\n\tunsigned long cpu;\n\n\tcpu = get_cpu();\n\tpercpu = per_cpu_ptr(conf->percpu, cpu);\n\tif (test_bit(STRIPE_OP_BIOFILL, &ops_request)) {\n\t\tops_run_biofill(sh);\n\t\toverlap_clear++;\n\t}\n\n\tif (test_bit(STRIPE_OP_COMPUTE_BLK, &ops_request)) {\n\t\tif (level < 6)\n\t\t\ttx = ops_run_compute5(sh, percpu);\n\t\telse {\n\t\t\tif (sh->ops.target2 < 0 || sh->ops.target < 0)\n\t\t\t\ttx = ops_run_compute6_1(sh, percpu);\n\t\t\telse\n\t\t\t\ttx = ops_run_compute6_2(sh, percpu);\n\t\t}\n\t\t/* terminate the chain if reconstruct is not set to be run */\n\t\tif (tx && !test_bit(STRIPE_OP_RECONSTRUCT, &ops_request))\n\t\t\tasync_tx_ack(tx);\n\t}\n\n\tif (test_bit(STRIPE_OP_PREXOR, &ops_request)) {\n\t\tif (level < 6)\n\t\t\ttx = ops_run_prexor5(sh, percpu, tx);\n\t\telse\n\t\t\ttx = ops_run_prexor6(sh, percpu, tx);\n\t}\n\n\tif (test_bit(STRIPE_OP_PARTIAL_PARITY, &ops_request))\n\t\ttx = ops_run_partial_parity(sh, percpu, tx);\n\n\tif (test_bit(STRIPE_OP_BIODRAIN, &ops_request)) {\n\t\ttx = ops_run_biodrain(sh, tx);\n\t\toverlap_clear++;\n\t}\n\n\tif (test_bit(STRIPE_OP_RECONSTRUCT, &ops_request)) {\n\t\tif (level < 6)\n\t\t\tops_run_reconstruct5(sh, percpu, tx);\n\t\telse\n\t\t\tops_run_reconstruct6(sh, percpu, tx);\n\t}\n\n\tif (test_bit(STRIPE_OP_CHECK, &ops_request)) {\n\t\tif (sh->check_state == check_state_run)\n\t\t\tops_run_check_p(sh, percpu);\n\t\telse if (sh->check_state == check_state_run_q)\n\t\t\tops_run_check_pq(sh, percpu, 0);\n\t\telse if (sh->check_state == check_state_run_pq)\n\t\t\tops_run_check_pq(sh, percpu, 1);\n\t\telse\n\t\t\tBUG();\n\t}\n\n\tif (overlap_clear && !sh->batch_head)\n\t\tfor (i = disks; i--; ) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t\tif (test_and_clear_bit(R5_Overlap, &dev->flags))\n\t\t\t\twake_up(&sh->raid_conf->wait_for_overlap);\n\t\t}\n\tput_cpu();\n}\n\nstatic void free_stripe(struct kmem_cache *sc, struct stripe_head *sh)\n{\n#if PAGE_SIZE != DEFAULT_STRIPE_SIZE\n\tkfree(sh->pages);\n#endif\n\tif (sh->ppl_page)\n\t\t__free_page(sh->ppl_page);\n\tkmem_cache_free(sc, sh);\n}\n\nstatic struct stripe_head *alloc_stripe(struct kmem_cache *sc, gfp_t gfp,\n\tint disks, struct r5conf *conf)\n{\n\tstruct stripe_head *sh;\n\tint i;\n\n\tsh = kmem_cache_zalloc(sc, gfp);\n\tif (sh) {\n\t\tspin_lock_init(&sh->stripe_lock);\n\t\tspin_lock_init(&sh->batch_lock);\n\t\tINIT_LIST_HEAD(&sh->batch_list);\n\t\tINIT_LIST_HEAD(&sh->lru);\n\t\tINIT_LIST_HEAD(&sh->r5c);\n\t\tINIT_LIST_HEAD(&sh->log_list);\n\t\tatomic_set(&sh->count, 1);\n\t\tsh->raid_conf = conf;\n\t\tsh->log_start = MaxSector;\n\t\tfor (i = 0; i < disks; i++) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\n\t\t\tbio_init(&dev->req, &dev->vec, 1);\n\t\t\tbio_init(&dev->rreq, &dev->rvec, 1);\n\t\t}\n\n\t\tif (raid5_has_ppl(conf)) {\n\t\t\tsh->ppl_page = alloc_page(gfp);\n\t\t\tif (!sh->ppl_page) {\n\t\t\t\tfree_stripe(sc, sh);\n\t\t\t\treturn NULL;\n\t\t\t}\n\t\t}\n#if PAGE_SIZE != DEFAULT_STRIPE_SIZE\n\t\tif (init_stripe_shared_pages(sh, conf, disks)) {\n\t\t\tfree_stripe(sc, sh);\n\t\t\treturn NULL;\n\t\t}\n#endif\n\t}\n\treturn sh;\n}\nstatic int grow_one_stripe(struct r5conf *conf, gfp_t gfp)\n{\n\tstruct stripe_head *sh;\n\n\tsh = alloc_stripe(conf->slab_cache, gfp, conf->pool_size, conf);\n\tif (!sh)\n\t\treturn 0;\n\n\tif (grow_buffers(sh, gfp)) {\n\t\tshrink_buffers(sh);\n\t\tfree_stripe(conf->slab_cache, sh);\n\t\treturn 0;\n\t}\n\tsh->hash_lock_index =\n\t\tconf->max_nr_stripes % NR_STRIPE_HASH_LOCKS;\n\t/* we just created an active stripe so... */\n\tatomic_inc(&conf->active_stripes);\n\n\traid5_release_stripe(sh);\n\tconf->max_nr_stripes++;\n\treturn 1;\n}\n\nstatic int grow_stripes(struct r5conf *conf, int num)\n{\n\tstruct kmem_cache *sc;\n\tsize_t namelen = sizeof(conf->cache_name[0]);\n\tint devs = max(conf->raid_disks, conf->previous_raid_disks);\n\n\tif (conf->mddev->gendisk)\n\t\tsnprintf(conf->cache_name[0], namelen,\n\t\t\t\"raid%d-%s\", conf->level, mdname(conf->mddev));\n\telse\n\t\tsnprintf(conf->cache_name[0], namelen,\n\t\t\t\"raid%d-%p\", conf->level, conf->mddev);\n\tsnprintf(conf->cache_name[1], namelen, \"%.27s-alt\", conf->cache_name[0]);\n\n\tconf->active_name = 0;\n\tsc = kmem_cache_create(conf->cache_name[conf->active_name],\n\t\t\t       sizeof(struct stripe_head)+(devs-1)*sizeof(struct r5dev),\n\t\t\t       0, 0, NULL);\n\tif (!sc)\n\t\treturn 1;\n\tconf->slab_cache = sc;\n\tconf->pool_size = devs;\n\twhile (num--)\n\t\tif (!grow_one_stripe(conf, GFP_KERNEL))\n\t\t\treturn 1;\n\n\treturn 0;\n}\n\n/**\n * scribble_alloc - allocate percpu scribble buffer for required size\n *\t\t    of the scribble region\n * @percpu: from for_each_present_cpu() of the caller\n * @num: total number of disks in the array\n * @cnt: scribble objs count for required size of the scribble region\n *\n * The scribble buffer size must be enough to contain:\n * 1/ a struct page pointer for each device in the array +2\n * 2/ room to convert each entry in (1) to its corresponding dma\n *    (dma_map_page()) or page (page_address()) address.\n *\n * Note: the +2 is for the destination buffers of the ddf/raid6 case where we\n * calculate over all devices (not just the data blocks), using zeros in place\n * of the P and Q blocks.\n */\nstatic int scribble_alloc(struct raid5_percpu *percpu,\n\t\t\t  int num, int cnt)\n{\n\tsize_t obj_size =\n\t\tsizeof(struct page *) * (num + 2) +\n\t\tsizeof(addr_conv_t) * (num + 2) +\n\t\tsizeof(unsigned int) * (num + 2);\n\tvoid *scribble;\n\n\t/*\n\t * If here is in raid array suspend context, it is in memalloc noio\n\t * context as well, there is no potential recursive memory reclaim\n\t * I/Os with the GFP_KERNEL flag.\n\t */\n\tscribble = kvmalloc_array(cnt, obj_size, GFP_KERNEL);\n\tif (!scribble)\n\t\treturn -ENOMEM;\n\n\tkvfree(percpu->scribble);\n\n\tpercpu->scribble = scribble;\n\tpercpu->scribble_obj_size = obj_size;\n\treturn 0;\n}\n\nstatic int resize_chunks(struct r5conf *conf, int new_disks, int new_sectors)\n{\n\tunsigned long cpu;\n\tint err = 0;\n\n\t/*\n\t * Never shrink. And mddev_suspend() could deadlock if this is called\n\t * from raid5d. In that case, scribble_disks and scribble_sectors\n\t * should equal to new_disks and new_sectors\n\t */\n\tif (conf->scribble_disks >= new_disks &&\n\t    conf->scribble_sectors >= new_sectors)\n\t\treturn 0;\n\tmddev_suspend(conf->mddev);\n\tget_online_cpus();\n\n\tfor_each_present_cpu(cpu) {\n\t\tstruct raid5_percpu *percpu;\n\n\t\tpercpu = per_cpu_ptr(conf->percpu, cpu);\n\t\terr = scribble_alloc(percpu, new_disks,\n\t\t\t\t     new_sectors / RAID5_STRIPE_SECTORS(conf));\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\tput_online_cpus();\n\tmddev_resume(conf->mddev);\n\tif (!err) {\n\t\tconf->scribble_disks = new_disks;\n\t\tconf->scribble_sectors = new_sectors;\n\t}\n\treturn err;\n}\n\nstatic int resize_stripes(struct r5conf *conf, int newsize)\n{\n\t/* Make all the stripes able to hold 'newsize' devices.\n\t * New slots in each stripe get 'page' set to a new page.\n\t *\n\t * This happens in stages:\n\t * 1/ create a new kmem_cache and allocate the required number of\n\t *    stripe_heads.\n\t * 2/ gather all the old stripe_heads and transfer the pages across\n\t *    to the new stripe_heads.  This will have the side effect of\n\t *    freezing the array as once all stripe_heads have been collected,\n\t *    no IO will be possible.  Old stripe heads are freed once their\n\t *    pages have been transferred over, and the old kmem_cache is\n\t *    freed when all stripes are done.\n\t * 3/ reallocate conf->disks to be suitable bigger.  If this fails,\n\t *    we simple return a failure status - no need to clean anything up.\n\t * 4/ allocate new pages for the new slots in the new stripe_heads.\n\t *    If this fails, we don't bother trying the shrink the\n\t *    stripe_heads down again, we just leave them as they are.\n\t *    As each stripe_head is processed the new one is released into\n\t *    active service.\n\t *\n\t * Once step2 is started, we cannot afford to wait for a write,\n\t * so we use GFP_NOIO allocations.\n\t */\n\tstruct stripe_head *osh, *nsh;\n\tLIST_HEAD(newstripes);\n\tstruct disk_info *ndisks;\n\tint err = 0;\n\tstruct kmem_cache *sc;\n\tint i;\n\tint hash, cnt;\n\n\tmd_allow_write(conf->mddev);\n\n\t/* Step 1 */\n\tsc = kmem_cache_create(conf->cache_name[1-conf->active_name],\n\t\t\t       sizeof(struct stripe_head)+(newsize-1)*sizeof(struct r5dev),\n\t\t\t       0, 0, NULL);\n\tif (!sc)\n\t\treturn -ENOMEM;\n\n\t/* Need to ensure auto-resizing doesn't interfere */\n\tmutex_lock(&conf->cache_size_mutex);\n\n\tfor (i = conf->max_nr_stripes; i; i--) {\n\t\tnsh = alloc_stripe(sc, GFP_KERNEL, newsize, conf);\n\t\tif (!nsh)\n\t\t\tbreak;\n\n\t\tlist_add(&nsh->lru, &newstripes);\n\t}\n\tif (i) {\n\t\t/* didn't get enough, give up */\n\t\twhile (!list_empty(&newstripes)) {\n\t\t\tnsh = list_entry(newstripes.next, struct stripe_head, lru);\n\t\t\tlist_del(&nsh->lru);\n\t\t\tfree_stripe(sc, nsh);\n\t\t}\n\t\tkmem_cache_destroy(sc);\n\t\tmutex_unlock(&conf->cache_size_mutex);\n\t\treturn -ENOMEM;\n\t}\n\t/* Step 2 - Must use GFP_NOIO now.\n\t * OK, we have enough stripes, start collecting inactive\n\t * stripes and copying them over\n\t */\n\thash = 0;\n\tcnt = 0;\n\tlist_for_each_entry(nsh, &newstripes, lru) {\n\t\tlock_device_hash_lock(conf, hash);\n\t\twait_event_cmd(conf->wait_for_stripe,\n\t\t\t\t    !list_empty(conf->inactive_list + hash),\n\t\t\t\t    unlock_device_hash_lock(conf, hash),\n\t\t\t\t    lock_device_hash_lock(conf, hash));\n\t\tosh = get_free_stripe(conf, hash);\n\t\tunlock_device_hash_lock(conf, hash);\n\n#if PAGE_SIZE != DEFAULT_STRIPE_SIZE\n\tfor (i = 0; i < osh->nr_pages; i++) {\n\t\tnsh->pages[i] = osh->pages[i];\n\t\tosh->pages[i] = NULL;\n\t}\n#endif\n\t\tfor(i=0; i<conf->pool_size; i++) {\n\t\t\tnsh->dev[i].page = osh->dev[i].page;\n\t\t\tnsh->dev[i].orig_page = osh->dev[i].page;\n\t\t\tnsh->dev[i].offset = osh->dev[i].offset;\n\t\t}\n\t\tnsh->hash_lock_index = hash;\n\t\tfree_stripe(conf->slab_cache, osh);\n\t\tcnt++;\n\t\tif (cnt >= conf->max_nr_stripes / NR_STRIPE_HASH_LOCKS +\n\t\t    !!((conf->max_nr_stripes % NR_STRIPE_HASH_LOCKS) > hash)) {\n\t\t\thash++;\n\t\t\tcnt = 0;\n\t\t}\n\t}\n\tkmem_cache_destroy(conf->slab_cache);\n\n\t/* Step 3.\n\t * At this point, we are holding all the stripes so the array\n\t * is completely stalled, so now is a good time to resize\n\t * conf->disks and the scribble region\n\t */\n\tndisks = kcalloc(newsize, sizeof(struct disk_info), GFP_NOIO);\n\tif (ndisks) {\n\t\tfor (i = 0; i < conf->pool_size; i++)\n\t\t\tndisks[i] = conf->disks[i];\n\n\t\tfor (i = conf->pool_size; i < newsize; i++) {\n\t\t\tndisks[i].extra_page = alloc_page(GFP_NOIO);\n\t\t\tif (!ndisks[i].extra_page)\n\t\t\t\terr = -ENOMEM;\n\t\t}\n\n\t\tif (err) {\n\t\t\tfor (i = conf->pool_size; i < newsize; i++)\n\t\t\t\tif (ndisks[i].extra_page)\n\t\t\t\t\tput_page(ndisks[i].extra_page);\n\t\t\tkfree(ndisks);\n\t\t} else {\n\t\t\tkfree(conf->disks);\n\t\t\tconf->disks = ndisks;\n\t\t}\n\t} else\n\t\terr = -ENOMEM;\n\n\tconf->slab_cache = sc;\n\tconf->active_name = 1-conf->active_name;\n\n\t/* Step 4, return new stripes to service */\n\twhile(!list_empty(&newstripes)) {\n\t\tnsh = list_entry(newstripes.next, struct stripe_head, lru);\n\t\tlist_del_init(&nsh->lru);\n\n#if PAGE_SIZE != DEFAULT_STRIPE_SIZE\n\t\tfor (i = 0; i < nsh->nr_pages; i++) {\n\t\t\tif (nsh->pages[i])\n\t\t\t\tcontinue;\n\t\t\tnsh->pages[i] = alloc_page(GFP_NOIO);\n\t\t\tif (!nsh->pages[i])\n\t\t\t\terr = -ENOMEM;\n\t\t}\n\n\t\tfor (i = conf->raid_disks; i < newsize; i++) {\n\t\t\tif (nsh->dev[i].page)\n\t\t\t\tcontinue;\n\t\t\tnsh->dev[i].page = raid5_get_dev_page(nsh, i);\n\t\t\tnsh->dev[i].orig_page = nsh->dev[i].page;\n\t\t\tnsh->dev[i].offset = raid5_get_page_offset(nsh, i);\n\t\t}\n#else\n\t\tfor (i=conf->raid_disks; i < newsize; i++)\n\t\t\tif (nsh->dev[i].page == NULL) {\n\t\t\t\tstruct page *p = alloc_page(GFP_NOIO);\n\t\t\t\tnsh->dev[i].page = p;\n\t\t\t\tnsh->dev[i].orig_page = p;\n\t\t\t\tnsh->dev[i].offset = 0;\n\t\t\t\tif (!p)\n\t\t\t\t\terr = -ENOMEM;\n\t\t\t}\n#endif\n\t\traid5_release_stripe(nsh);\n\t}\n\t/* critical section pass, GFP_NOIO no longer needed */\n\n\tif (!err)\n\t\tconf->pool_size = newsize;\n\tmutex_unlock(&conf->cache_size_mutex);\n\n\treturn err;\n}\n\nstatic int drop_one_stripe(struct r5conf *conf)\n{\n\tstruct stripe_head *sh;\n\tint hash = (conf->max_nr_stripes - 1) & STRIPE_HASH_LOCKS_MASK;\n\n\tspin_lock_irq(conf->hash_locks + hash);\n\tsh = get_free_stripe(conf, hash);\n\tspin_unlock_irq(conf->hash_locks + hash);\n\tif (!sh)\n\t\treturn 0;\n\tBUG_ON(atomic_read(&sh->count));\n\tshrink_buffers(sh);\n\tfree_stripe(conf->slab_cache, sh);\n\tatomic_dec(&conf->active_stripes);\n\tconf->max_nr_stripes--;\n\treturn 1;\n}\n\nstatic void shrink_stripes(struct r5conf *conf)\n{\n\twhile (conf->max_nr_stripes &&\n\t       drop_one_stripe(conf))\n\t\t;\n\n\tkmem_cache_destroy(conf->slab_cache);\n\tconf->slab_cache = NULL;\n}\n\nstatic void raid5_end_read_request(struct bio * bi)\n{\n\tstruct stripe_head *sh = bi->bi_private;\n\tstruct r5conf *conf = sh->raid_conf;\n\tint disks = sh->disks, i;\n\tchar b[BDEVNAME_SIZE];\n\tstruct md_rdev *rdev = NULL;\n\tsector_t s;\n\n\tfor (i=0 ; i<disks; i++)\n\t\tif (bi == &sh->dev[i].req)\n\t\t\tbreak;\n\n\tpr_debug(\"end_read_request %llu/%d, count: %d, error %d.\\n\",\n\t\t(unsigned long long)sh->sector, i, atomic_read(&sh->count),\n\t\tbi->bi_status);\n\tif (i == disks) {\n\t\tbio_reset(bi);\n\t\tBUG();\n\t\treturn;\n\t}\n\tif (test_bit(R5_ReadRepl, &sh->dev[i].flags))\n\t\t/* If replacement finished while this request was outstanding,\n\t\t * 'replacement' might be NULL already.\n\t\t * In that case it moved down to 'rdev'.\n\t\t * rdev is not removed until all requests are finished.\n\t\t */\n\t\trdev = conf->disks[i].replacement;\n\tif (!rdev)\n\t\trdev = conf->disks[i].rdev;\n\n\tif (use_new_offset(conf, sh))\n\t\ts = sh->sector + rdev->new_data_offset;\n\telse\n\t\ts = sh->sector + rdev->data_offset;\n\tif (!bi->bi_status) {\n\t\tset_bit(R5_UPTODATE, &sh->dev[i].flags);\n\t\tif (test_bit(R5_ReadError, &sh->dev[i].flags)) {\n\t\t\t/* Note that this cannot happen on a\n\t\t\t * replacement device.  We just fail those on\n\t\t\t * any error\n\t\t\t */\n\t\t\tpr_info_ratelimited(\n\t\t\t\t\"md/raid:%s: read error corrected (%lu sectors at %llu on %s)\\n\",\n\t\t\t\tmdname(conf->mddev), RAID5_STRIPE_SECTORS(conf),\n\t\t\t\t(unsigned long long)s,\n\t\t\t\tbdevname(rdev->bdev, b));\n\t\t\tatomic_add(RAID5_STRIPE_SECTORS(conf), &rdev->corrected_errors);\n\t\t\tclear_bit(R5_ReadError, &sh->dev[i].flags);\n\t\t\tclear_bit(R5_ReWrite, &sh->dev[i].flags);\n\t\t} else if (test_bit(R5_ReadNoMerge, &sh->dev[i].flags))\n\t\t\tclear_bit(R5_ReadNoMerge, &sh->dev[i].flags);\n\n\t\tif (test_bit(R5_InJournal, &sh->dev[i].flags))\n\t\t\t/*\n\t\t\t * end read for a page in journal, this\n\t\t\t * must be preparing for prexor in rmw\n\t\t\t */\n\t\t\tset_bit(R5_OrigPageUPTDODATE, &sh->dev[i].flags);\n\n\t\tif (atomic_read(&rdev->read_errors))\n\t\t\tatomic_set(&rdev->read_errors, 0);\n\t} else {\n\t\tconst char *bdn = bdevname(rdev->bdev, b);\n\t\tint retry = 0;\n\t\tint set_bad = 0;\n\n\t\tclear_bit(R5_UPTODATE, &sh->dev[i].flags);\n\t\tif (!(bi->bi_status == BLK_STS_PROTECTION))\n\t\t\tatomic_inc(&rdev->read_errors);\n\t\tif (test_bit(R5_ReadRepl, &sh->dev[i].flags))\n\t\t\tpr_warn_ratelimited(\n\t\t\t\t\"md/raid:%s: read error on replacement device (sector %llu on %s).\\n\",\n\t\t\t\tmdname(conf->mddev),\n\t\t\t\t(unsigned long long)s,\n\t\t\t\tbdn);\n\t\telse if (conf->mddev->degraded >= conf->max_degraded) {\n\t\t\tset_bad = 1;\n\t\t\tpr_warn_ratelimited(\n\t\t\t\t\"md/raid:%s: read error not correctable (sector %llu on %s).\\n\",\n\t\t\t\tmdname(conf->mddev),\n\t\t\t\t(unsigned long long)s,\n\t\t\t\tbdn);\n\t\t} else if (test_bit(R5_ReWrite, &sh->dev[i].flags)) {\n\t\t\t/* Oh, no!!! */\n\t\t\tset_bad = 1;\n\t\t\tpr_warn_ratelimited(\n\t\t\t\t\"md/raid:%s: read error NOT corrected!! (sector %llu on %s).\\n\",\n\t\t\t\tmdname(conf->mddev),\n\t\t\t\t(unsigned long long)s,\n\t\t\t\tbdn);\n\t\t} else if (atomic_read(&rdev->read_errors)\n\t\t\t > conf->max_nr_stripes) {\n\t\t\tif (!test_bit(Faulty, &rdev->flags)) {\n\t\t\t\tpr_warn(\"md/raid:%s: %d read_errors > %d stripes\\n\",\n\t\t\t\t    mdname(conf->mddev),\n\t\t\t\t    atomic_read(&rdev->read_errors),\n\t\t\t\t    conf->max_nr_stripes);\n\t\t\t\tpr_warn(\"md/raid:%s: Too many read errors, failing device %s.\\n\",\n\t\t\t\t    mdname(conf->mddev), bdn);\n\t\t\t}\n\t\t} else\n\t\t\tretry = 1;\n\t\tif (set_bad && test_bit(In_sync, &rdev->flags)\n\t\t    && !test_bit(R5_ReadNoMerge, &sh->dev[i].flags))\n\t\t\tretry = 1;\n\t\tif (retry)\n\t\t\tif (sh->qd_idx >= 0 && sh->pd_idx == i)\n\t\t\t\tset_bit(R5_ReadError, &sh->dev[i].flags);\n\t\t\telse if (test_bit(R5_ReadNoMerge, &sh->dev[i].flags)) {\n\t\t\t\tset_bit(R5_ReadError, &sh->dev[i].flags);\n\t\t\t\tclear_bit(R5_ReadNoMerge, &sh->dev[i].flags);\n\t\t\t} else\n\t\t\t\tset_bit(R5_ReadNoMerge, &sh->dev[i].flags);\n\t\telse {\n\t\t\tclear_bit(R5_ReadError, &sh->dev[i].flags);\n\t\t\tclear_bit(R5_ReWrite, &sh->dev[i].flags);\n\t\t\tif (!(set_bad\n\t\t\t      && test_bit(In_sync, &rdev->flags)\n\t\t\t      && rdev_set_badblocks(\n\t\t\t\t      rdev, sh->sector, RAID5_STRIPE_SECTORS(conf), 0)))\n\t\t\t\tmd_error(conf->mddev, rdev);\n\t\t}\n\t}\n\trdev_dec_pending(rdev, conf->mddev);\n\tbio_reset(bi);\n\tclear_bit(R5_LOCKED, &sh->dev[i].flags);\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\traid5_release_stripe(sh);\n}\n\nstatic void raid5_end_write_request(struct bio *bi)\n{\n\tstruct stripe_head *sh = bi->bi_private;\n\tstruct r5conf *conf = sh->raid_conf;\n\tint disks = sh->disks, i;\n\tstruct md_rdev *rdev;\n\tsector_t first_bad;\n\tint bad_sectors;\n\tint replacement = 0;\n\n\tfor (i = 0 ; i < disks; i++) {\n\t\tif (bi == &sh->dev[i].req) {\n\t\t\trdev = conf->disks[i].rdev;\n\t\t\tbreak;\n\t\t}\n\t\tif (bi == &sh->dev[i].rreq) {\n\t\t\trdev = conf->disks[i].replacement;\n\t\t\tif (rdev)\n\t\t\t\treplacement = 1;\n\t\t\telse\n\t\t\t\t/* rdev was removed and 'replacement'\n\t\t\t\t * replaced it.  rdev is not removed\n\t\t\t\t * until all requests are finished.\n\t\t\t\t */\n\t\t\t\trdev = conf->disks[i].rdev;\n\t\t\tbreak;\n\t\t}\n\t}\n\tpr_debug(\"end_write_request %llu/%d, count %d, error: %d.\\n\",\n\t\t(unsigned long long)sh->sector, i, atomic_read(&sh->count),\n\t\tbi->bi_status);\n\tif (i == disks) {\n\t\tbio_reset(bi);\n\t\tBUG();\n\t\treturn;\n\t}\n\n\tif (replacement) {\n\t\tif (bi->bi_status)\n\t\t\tmd_error(conf->mddev, rdev);\n\t\telse if (is_badblock(rdev, sh->sector,\n\t\t\t\t     RAID5_STRIPE_SECTORS(conf),\n\t\t\t\t     &first_bad, &bad_sectors))\n\t\t\tset_bit(R5_MadeGoodRepl, &sh->dev[i].flags);\n\t} else {\n\t\tif (bi->bi_status) {\n\t\t\tset_bit(STRIPE_DEGRADED, &sh->state);\n\t\t\tset_bit(WriteErrorSeen, &rdev->flags);\n\t\t\tset_bit(R5_WriteError, &sh->dev[i].flags);\n\t\t\tif (!test_and_set_bit(WantReplacement, &rdev->flags))\n\t\t\t\tset_bit(MD_RECOVERY_NEEDED,\n\t\t\t\t\t&rdev->mddev->recovery);\n\t\t} else if (is_badblock(rdev, sh->sector,\n\t\t\t\t       RAID5_STRIPE_SECTORS(conf),\n\t\t\t\t       &first_bad, &bad_sectors)) {\n\t\t\tset_bit(R5_MadeGood, &sh->dev[i].flags);\n\t\t\tif (test_bit(R5_ReadError, &sh->dev[i].flags))\n\t\t\t\t/* That was a successful write so make\n\t\t\t\t * sure it looks like we already did\n\t\t\t\t * a re-write.\n\t\t\t\t */\n\t\t\t\tset_bit(R5_ReWrite, &sh->dev[i].flags);\n\t\t}\n\t}\n\trdev_dec_pending(rdev, conf->mddev);\n\n\tif (sh->batch_head && bi->bi_status && !replacement)\n\t\tset_bit(STRIPE_BATCH_ERR, &sh->batch_head->state);\n\n\tbio_reset(bi);\n\tif (!test_and_clear_bit(R5_DOUBLE_LOCKED, &sh->dev[i].flags))\n\t\tclear_bit(R5_LOCKED, &sh->dev[i].flags);\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\traid5_release_stripe(sh);\n\n\tif (sh->batch_head && sh != sh->batch_head)\n\t\traid5_release_stripe(sh->batch_head);\n}\n\nstatic void raid5_error(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tchar b[BDEVNAME_SIZE];\n\tstruct r5conf *conf = mddev->private;\n\tunsigned long flags;\n\tpr_debug(\"raid456: error called\\n\");\n\n\tspin_lock_irqsave(&conf->device_lock, flags);\n\n\tif (test_bit(In_sync, &rdev->flags) &&\n\t    mddev->degraded == conf->max_degraded) {\n\t\t/*\n\t\t * Don't allow to achieve failed state\n\t\t * Don't try to recover this device\n\t\t */\n\t\tconf->recovery_disabled = mddev->recovery_disabled;\n\t\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\t\treturn;\n\t}\n\n\tset_bit(Faulty, &rdev->flags);\n\tclear_bit(In_sync, &rdev->flags);\n\tmddev->degraded = raid5_calc_degraded(conf);\n\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\tset_bit(MD_RECOVERY_INTR, &mddev->recovery);\n\n\tset_bit(Blocked, &rdev->flags);\n\tset_mask_bits(&mddev->sb_flags, 0,\n\t\t      BIT(MD_SB_CHANGE_DEVS) | BIT(MD_SB_CHANGE_PENDING));\n\tpr_crit(\"md/raid:%s: Disk failure on %s, disabling device.\\n\"\n\t\t\"md/raid:%s: Operation continuing on %d devices.\\n\",\n\t\tmdname(mddev),\n\t\tbdevname(rdev->bdev, b),\n\t\tmdname(mddev),\n\t\tconf->raid_disks - mddev->degraded);\n\tr5c_update_on_rdev_error(mddev, rdev);\n}\n\n/*\n * Input: a 'big' sector number,\n * Output: index of the data and parity disk, and the sector # in them.\n */\nsector_t raid5_compute_sector(struct r5conf *conf, sector_t r_sector,\n\t\t\t      int previous, int *dd_idx,\n\t\t\t      struct stripe_head *sh)\n{\n\tsector_t stripe, stripe2;\n\tsector_t chunk_number;\n\tunsigned int chunk_offset;\n\tint pd_idx, qd_idx;\n\tint ddf_layout = 0;\n\tsector_t new_sector;\n\tint algorithm = previous ? conf->prev_algo\n\t\t\t\t : conf->algorithm;\n\tint sectors_per_chunk = previous ? conf->prev_chunk_sectors\n\t\t\t\t\t : conf->chunk_sectors;\n\tint raid_disks = previous ? conf->previous_raid_disks\n\t\t\t\t  : conf->raid_disks;\n\tint data_disks = raid_disks - conf->max_degraded;\n\n\t/* First compute the information on this sector */\n\n\t/*\n\t * Compute the chunk number and the sector offset inside the chunk\n\t */\n\tchunk_offset = sector_div(r_sector, sectors_per_chunk);\n\tchunk_number = r_sector;\n\n\t/*\n\t * Compute the stripe number\n\t */\n\tstripe = chunk_number;\n\t*dd_idx = sector_div(stripe, data_disks);\n\tstripe2 = stripe;\n\t/*\n\t * Select the parity disk based on the user selected algorithm.\n\t */\n\tpd_idx = qd_idx = -1;\n\tswitch(conf->level) {\n\tcase 4:\n\t\tpd_idx = data_disks;\n\t\tbreak;\n\tcase 5:\n\t\tswitch (algorithm) {\n\t\tcase ALGORITHM_LEFT_ASYMMETRIC:\n\t\t\tpd_idx = data_disks - sector_div(stripe2, raid_disks);\n\t\t\tif (*dd_idx >= pd_idx)\n\t\t\t\t(*dd_idx)++;\n\t\t\tbreak;\n\t\tcase ALGORITHM_RIGHT_ASYMMETRIC:\n\t\t\tpd_idx = sector_div(stripe2, raid_disks);\n\t\t\tif (*dd_idx >= pd_idx)\n\t\t\t\t(*dd_idx)++;\n\t\t\tbreak;\n\t\tcase ALGORITHM_LEFT_SYMMETRIC:\n\t\t\tpd_idx = data_disks - sector_div(stripe2, raid_disks);\n\t\t\t*dd_idx = (pd_idx + 1 + *dd_idx) % raid_disks;\n\t\t\tbreak;\n\t\tcase ALGORITHM_RIGHT_SYMMETRIC:\n\t\t\tpd_idx = sector_div(stripe2, raid_disks);\n\t\t\t*dd_idx = (pd_idx + 1 + *dd_idx) % raid_disks;\n\t\t\tbreak;\n\t\tcase ALGORITHM_PARITY_0:\n\t\t\tpd_idx = 0;\n\t\t\t(*dd_idx)++;\n\t\t\tbreak;\n\t\tcase ALGORITHM_PARITY_N:\n\t\t\tpd_idx = data_disks;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t\tbreak;\n\tcase 6:\n\n\t\tswitch (algorithm) {\n\t\tcase ALGORITHM_LEFT_ASYMMETRIC:\n\t\t\tpd_idx = raid_disks - 1 - sector_div(stripe2, raid_disks);\n\t\t\tqd_idx = pd_idx + 1;\n\t\t\tif (pd_idx == raid_disks-1) {\n\t\t\t\t(*dd_idx)++;\t/* Q D D D P */\n\t\t\t\tqd_idx = 0;\n\t\t\t} else if (*dd_idx >= pd_idx)\n\t\t\t\t(*dd_idx) += 2; /* D D P Q D */\n\t\t\tbreak;\n\t\tcase ALGORITHM_RIGHT_ASYMMETRIC:\n\t\t\tpd_idx = sector_div(stripe2, raid_disks);\n\t\t\tqd_idx = pd_idx + 1;\n\t\t\tif (pd_idx == raid_disks-1) {\n\t\t\t\t(*dd_idx)++;\t/* Q D D D P */\n\t\t\t\tqd_idx = 0;\n\t\t\t} else if (*dd_idx >= pd_idx)\n\t\t\t\t(*dd_idx) += 2; /* D D P Q D */\n\t\t\tbreak;\n\t\tcase ALGORITHM_LEFT_SYMMETRIC:\n\t\t\tpd_idx = raid_disks - 1 - sector_div(stripe2, raid_disks);\n\t\t\tqd_idx = (pd_idx + 1) % raid_disks;\n\t\t\t*dd_idx = (pd_idx + 2 + *dd_idx) % raid_disks;\n\t\t\tbreak;\n\t\tcase ALGORITHM_RIGHT_SYMMETRIC:\n\t\t\tpd_idx = sector_div(stripe2, raid_disks);\n\t\t\tqd_idx = (pd_idx + 1) % raid_disks;\n\t\t\t*dd_idx = (pd_idx + 2 + *dd_idx) % raid_disks;\n\t\t\tbreak;\n\n\t\tcase ALGORITHM_PARITY_0:\n\t\t\tpd_idx = 0;\n\t\t\tqd_idx = 1;\n\t\t\t(*dd_idx) += 2;\n\t\t\tbreak;\n\t\tcase ALGORITHM_PARITY_N:\n\t\t\tpd_idx = data_disks;\n\t\t\tqd_idx = data_disks + 1;\n\t\t\tbreak;\n\n\t\tcase ALGORITHM_ROTATING_ZERO_RESTART:\n\t\t\t/* Exactly the same as RIGHT_ASYMMETRIC, but or\n\t\t\t * of blocks for computing Q is different.\n\t\t\t */\n\t\t\tpd_idx = sector_div(stripe2, raid_disks);\n\t\t\tqd_idx = pd_idx + 1;\n\t\t\tif (pd_idx == raid_disks-1) {\n\t\t\t\t(*dd_idx)++;\t/* Q D D D P */\n\t\t\t\tqd_idx = 0;\n\t\t\t} else if (*dd_idx >= pd_idx)\n\t\t\t\t(*dd_idx) += 2; /* D D P Q D */\n\t\t\tddf_layout = 1;\n\t\t\tbreak;\n\n\t\tcase ALGORITHM_ROTATING_N_RESTART:\n\t\t\t/* Same a left_asymmetric, by first stripe is\n\t\t\t * D D D P Q  rather than\n\t\t\t * Q D D D P\n\t\t\t */\n\t\t\tstripe2 += 1;\n\t\t\tpd_idx = raid_disks - 1 - sector_div(stripe2, raid_disks);\n\t\t\tqd_idx = pd_idx + 1;\n\t\t\tif (pd_idx == raid_disks-1) {\n\t\t\t\t(*dd_idx)++;\t/* Q D D D P */\n\t\t\t\tqd_idx = 0;\n\t\t\t} else if (*dd_idx >= pd_idx)\n\t\t\t\t(*dd_idx) += 2; /* D D P Q D */\n\t\t\tddf_layout = 1;\n\t\t\tbreak;\n\n\t\tcase ALGORITHM_ROTATING_N_CONTINUE:\n\t\t\t/* Same as left_symmetric but Q is before P */\n\t\t\tpd_idx = raid_disks - 1 - sector_div(stripe2, raid_disks);\n\t\t\tqd_idx = (pd_idx + raid_disks - 1) % raid_disks;\n\t\t\t*dd_idx = (pd_idx + 1 + *dd_idx) % raid_disks;\n\t\t\tddf_layout = 1;\n\t\t\tbreak;\n\n\t\tcase ALGORITHM_LEFT_ASYMMETRIC_6:\n\t\t\t/* RAID5 left_asymmetric, with Q on last device */\n\t\t\tpd_idx = data_disks - sector_div(stripe2, raid_disks-1);\n\t\t\tif (*dd_idx >= pd_idx)\n\t\t\t\t(*dd_idx)++;\n\t\t\tqd_idx = raid_disks - 1;\n\t\t\tbreak;\n\n\t\tcase ALGORITHM_RIGHT_ASYMMETRIC_6:\n\t\t\tpd_idx = sector_div(stripe2, raid_disks-1);\n\t\t\tif (*dd_idx >= pd_idx)\n\t\t\t\t(*dd_idx)++;\n\t\t\tqd_idx = raid_disks - 1;\n\t\t\tbreak;\n\n\t\tcase ALGORITHM_LEFT_SYMMETRIC_6:\n\t\t\tpd_idx = data_disks - sector_div(stripe2, raid_disks-1);\n\t\t\t*dd_idx = (pd_idx + 1 + *dd_idx) % (raid_disks-1);\n\t\t\tqd_idx = raid_disks - 1;\n\t\t\tbreak;\n\n\t\tcase ALGORITHM_RIGHT_SYMMETRIC_6:\n\t\t\tpd_idx = sector_div(stripe2, raid_disks-1);\n\t\t\t*dd_idx = (pd_idx + 1 + *dd_idx) % (raid_disks-1);\n\t\t\tqd_idx = raid_disks - 1;\n\t\t\tbreak;\n\n\t\tcase ALGORITHM_PARITY_0_6:\n\t\t\tpd_idx = 0;\n\t\t\t(*dd_idx)++;\n\t\t\tqd_idx = raid_disks - 1;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t\tbreak;\n\t}\n\n\tif (sh) {\n\t\tsh->pd_idx = pd_idx;\n\t\tsh->qd_idx = qd_idx;\n\t\tsh->ddf_layout = ddf_layout;\n\t}\n\t/*\n\t * Finally, compute the new sector number\n\t */\n\tnew_sector = (sector_t)stripe * sectors_per_chunk + chunk_offset;\n\treturn new_sector;\n}\n\nsector_t raid5_compute_blocknr(struct stripe_head *sh, int i, int previous)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\tint raid_disks = sh->disks;\n\tint data_disks = raid_disks - conf->max_degraded;\n\tsector_t new_sector = sh->sector, check;\n\tint sectors_per_chunk = previous ? conf->prev_chunk_sectors\n\t\t\t\t\t : conf->chunk_sectors;\n\tint algorithm = previous ? conf->prev_algo\n\t\t\t\t : conf->algorithm;\n\tsector_t stripe;\n\tint chunk_offset;\n\tsector_t chunk_number;\n\tint dummy1, dd_idx = i;\n\tsector_t r_sector;\n\tstruct stripe_head sh2;\n\n\tchunk_offset = sector_div(new_sector, sectors_per_chunk);\n\tstripe = new_sector;\n\n\tif (i == sh->pd_idx)\n\t\treturn 0;\n\tswitch(conf->level) {\n\tcase 4: break;\n\tcase 5:\n\t\tswitch (algorithm) {\n\t\tcase ALGORITHM_LEFT_ASYMMETRIC:\n\t\tcase ALGORITHM_RIGHT_ASYMMETRIC:\n\t\t\tif (i > sh->pd_idx)\n\t\t\t\ti--;\n\t\t\tbreak;\n\t\tcase ALGORITHM_LEFT_SYMMETRIC:\n\t\tcase ALGORITHM_RIGHT_SYMMETRIC:\n\t\t\tif (i < sh->pd_idx)\n\t\t\t\ti += raid_disks;\n\t\t\ti -= (sh->pd_idx + 1);\n\t\t\tbreak;\n\t\tcase ALGORITHM_PARITY_0:\n\t\t\ti -= 1;\n\t\t\tbreak;\n\t\tcase ALGORITHM_PARITY_N:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t\tbreak;\n\tcase 6:\n\t\tif (i == sh->qd_idx)\n\t\t\treturn 0; /* It is the Q disk */\n\t\tswitch (algorithm) {\n\t\tcase ALGORITHM_LEFT_ASYMMETRIC:\n\t\tcase ALGORITHM_RIGHT_ASYMMETRIC:\n\t\tcase ALGORITHM_ROTATING_ZERO_RESTART:\n\t\tcase ALGORITHM_ROTATING_N_RESTART:\n\t\t\tif (sh->pd_idx == raid_disks-1)\n\t\t\t\ti--;\t/* Q D D D P */\n\t\t\telse if (i > sh->pd_idx)\n\t\t\t\ti -= 2; /* D D P Q D */\n\t\t\tbreak;\n\t\tcase ALGORITHM_LEFT_SYMMETRIC:\n\t\tcase ALGORITHM_RIGHT_SYMMETRIC:\n\t\t\tif (sh->pd_idx == raid_disks-1)\n\t\t\t\ti--; /* Q D D D P */\n\t\t\telse {\n\t\t\t\t/* D D P Q D */\n\t\t\t\tif (i < sh->pd_idx)\n\t\t\t\t\ti += raid_disks;\n\t\t\t\ti -= (sh->pd_idx + 2);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase ALGORITHM_PARITY_0:\n\t\t\ti -= 2;\n\t\t\tbreak;\n\t\tcase ALGORITHM_PARITY_N:\n\t\t\tbreak;\n\t\tcase ALGORITHM_ROTATING_N_CONTINUE:\n\t\t\t/* Like left_symmetric, but P is before Q */\n\t\t\tif (sh->pd_idx == 0)\n\t\t\t\ti--;\t/* P D D D Q */\n\t\t\telse {\n\t\t\t\t/* D D Q P D */\n\t\t\t\tif (i < sh->pd_idx)\n\t\t\t\t\ti += raid_disks;\n\t\t\t\ti -= (sh->pd_idx + 1);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase ALGORITHM_LEFT_ASYMMETRIC_6:\n\t\tcase ALGORITHM_RIGHT_ASYMMETRIC_6:\n\t\t\tif (i > sh->pd_idx)\n\t\t\t\ti--;\n\t\t\tbreak;\n\t\tcase ALGORITHM_LEFT_SYMMETRIC_6:\n\t\tcase ALGORITHM_RIGHT_SYMMETRIC_6:\n\t\t\tif (i < sh->pd_idx)\n\t\t\t\ti += data_disks + 1;\n\t\t\ti -= (sh->pd_idx + 1);\n\t\t\tbreak;\n\t\tcase ALGORITHM_PARITY_0_6:\n\t\t\ti -= 1;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t\tbreak;\n\t}\n\n\tchunk_number = stripe * data_disks + i;\n\tr_sector = chunk_number * sectors_per_chunk + chunk_offset;\n\n\tcheck = raid5_compute_sector(conf, r_sector,\n\t\t\t\t     previous, &dummy1, &sh2);\n\tif (check != sh->sector || dummy1 != dd_idx || sh2.pd_idx != sh->pd_idx\n\t\t|| sh2.qd_idx != sh->qd_idx) {\n\t\tpr_warn(\"md/raid:%s: compute_blocknr: map not correct\\n\",\n\t\t\tmdname(conf->mddev));\n\t\treturn 0;\n\t}\n\treturn r_sector;\n}\n\n/*\n * There are cases where we want handle_stripe_dirtying() and\n * schedule_reconstruction() to delay towrite to some dev of a stripe.\n *\n * This function checks whether we want to delay the towrite. Specifically,\n * we delay the towrite when:\n *\n *   1. degraded stripe has a non-overwrite to the missing dev, AND this\n *      stripe has data in journal (for other devices).\n *\n *      In this case, when reading data for the non-overwrite dev, it is\n *      necessary to handle complex rmw of write back cache (prexor with\n *      orig_page, and xor with page). To keep read path simple, we would\n *      like to flush data in journal to RAID disks first, so complex rmw\n *      is handled in the write patch (handle_stripe_dirtying).\n *\n *   2. when journal space is critical (R5C_LOG_CRITICAL=1)\n *\n *      It is important to be able to flush all stripes in raid5-cache.\n *      Therefore, we need reserve some space on the journal device for\n *      these flushes. If flush operation includes pending writes to the\n *      stripe, we need to reserve (conf->raid_disk + 1) pages per stripe\n *      for the flush out. If we exclude these pending writes from flush\n *      operation, we only need (conf->max_degraded + 1) pages per stripe.\n *      Therefore, excluding pending writes in these cases enables more\n *      efficient use of the journal device.\n *\n *      Note: To make sure the stripe makes progress, we only delay\n *      towrite for stripes with data already in journal (injournal > 0).\n *      When LOG_CRITICAL, stripes with injournal == 0 will be sent to\n *      no_space_stripes list.\n *\n *   3. during journal failure\n *      In journal failure, we try to flush all cached data to raid disks\n *      based on data in stripe cache. The array is read-only to upper\n *      layers, so we would skip all pending writes.\n *\n */\nstatic inline bool delay_towrite(struct r5conf *conf,\n\t\t\t\t struct r5dev *dev,\n\t\t\t\t struct stripe_head_state *s)\n{\n\t/* case 1 above */\n\tif (!test_bit(R5_OVERWRITE, &dev->flags) &&\n\t    !test_bit(R5_Insync, &dev->flags) && s->injournal)\n\t\treturn true;\n\t/* case 2 above */\n\tif (test_bit(R5C_LOG_CRITICAL, &conf->cache_state) &&\n\t    s->injournal > 0)\n\t\treturn true;\n\t/* case 3 above */\n\tif (s->log_failed && s->injournal)\n\t\treturn true;\n\treturn false;\n}\n\nstatic void\nschedule_reconstruction(struct stripe_head *sh, struct stripe_head_state *s,\n\t\t\t int rcw, int expand)\n{\n\tint i, pd_idx = sh->pd_idx, qd_idx = sh->qd_idx, disks = sh->disks;\n\tstruct r5conf *conf = sh->raid_conf;\n\tint level = conf->level;\n\n\tif (rcw) {\n\t\t/*\n\t\t * In some cases, handle_stripe_dirtying initially decided to\n\t\t * run rmw and allocates extra page for prexor. However, rcw is\n\t\t * cheaper later on. We need to free the extra page now,\n\t\t * because we won't be able to do that in ops_complete_prexor().\n\t\t */\n\t\tr5c_release_extra_page(sh);\n\n\t\tfor (i = disks; i--; ) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\n\t\t\tif (dev->towrite && !delay_towrite(conf, dev, s)) {\n\t\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\t\tset_bit(R5_Wantdrain, &dev->flags);\n\t\t\t\tif (!expand)\n\t\t\t\t\tclear_bit(R5_UPTODATE, &dev->flags);\n\t\t\t\ts->locked++;\n\t\t\t} else if (test_bit(R5_InJournal, &dev->flags)) {\n\t\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\t\ts->locked++;\n\t\t\t}\n\t\t}\n\t\t/* if we are not expanding this is a proper write request, and\n\t\t * there will be bios with new data to be drained into the\n\t\t * stripe cache\n\t\t */\n\t\tif (!expand) {\n\t\t\tif (!s->locked)\n\t\t\t\t/* False alarm, nothing to do */\n\t\t\t\treturn;\n\t\t\tsh->reconstruct_state = reconstruct_state_drain_run;\n\t\t\tset_bit(STRIPE_OP_BIODRAIN, &s->ops_request);\n\t\t} else\n\t\t\tsh->reconstruct_state = reconstruct_state_run;\n\n\t\tset_bit(STRIPE_OP_RECONSTRUCT, &s->ops_request);\n\n\t\tif (s->locked + conf->max_degraded == disks)\n\t\t\tif (!test_and_set_bit(STRIPE_FULL_WRITE, &sh->state))\n\t\t\t\tatomic_inc(&conf->pending_full_writes);\n\t} else {\n\t\tBUG_ON(!(test_bit(R5_UPTODATE, &sh->dev[pd_idx].flags) ||\n\t\t\ttest_bit(R5_Wantcompute, &sh->dev[pd_idx].flags)));\n\t\tBUG_ON(level == 6 &&\n\t\t\t(!(test_bit(R5_UPTODATE, &sh->dev[qd_idx].flags) ||\n\t\t\t   test_bit(R5_Wantcompute, &sh->dev[qd_idx].flags))));\n\n\t\tfor (i = disks; i--; ) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t\tif (i == pd_idx || i == qd_idx)\n\t\t\t\tcontinue;\n\n\t\t\tif (dev->towrite &&\n\t\t\t    (test_bit(R5_UPTODATE, &dev->flags) ||\n\t\t\t     test_bit(R5_Wantcompute, &dev->flags))) {\n\t\t\t\tset_bit(R5_Wantdrain, &dev->flags);\n\t\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\t\tclear_bit(R5_UPTODATE, &dev->flags);\n\t\t\t\ts->locked++;\n\t\t\t} else if (test_bit(R5_InJournal, &dev->flags)) {\n\t\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\t\ts->locked++;\n\t\t\t}\n\t\t}\n\t\tif (!s->locked)\n\t\t\t/* False alarm - nothing to do */\n\t\t\treturn;\n\t\tsh->reconstruct_state = reconstruct_state_prexor_drain_run;\n\t\tset_bit(STRIPE_OP_PREXOR, &s->ops_request);\n\t\tset_bit(STRIPE_OP_BIODRAIN, &s->ops_request);\n\t\tset_bit(STRIPE_OP_RECONSTRUCT, &s->ops_request);\n\t}\n\n\t/* keep the parity disk(s) locked while asynchronous operations\n\t * are in flight\n\t */\n\tset_bit(R5_LOCKED, &sh->dev[pd_idx].flags);\n\tclear_bit(R5_UPTODATE, &sh->dev[pd_idx].flags);\n\ts->locked++;\n\n\tif (level == 6) {\n\t\tint qd_idx = sh->qd_idx;\n\t\tstruct r5dev *dev = &sh->dev[qd_idx];\n\n\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\tclear_bit(R5_UPTODATE, &dev->flags);\n\t\ts->locked++;\n\t}\n\n\tif (raid5_has_ppl(sh->raid_conf) && sh->ppl_page &&\n\t    test_bit(STRIPE_OP_BIODRAIN, &s->ops_request) &&\n\t    !test_bit(STRIPE_FULL_WRITE, &sh->state) &&\n\t    test_bit(R5_Insync, &sh->dev[pd_idx].flags))\n\t\tset_bit(STRIPE_OP_PARTIAL_PARITY, &s->ops_request);\n\n\tpr_debug(\"%s: stripe %llu locked: %d ops_request: %lx\\n\",\n\t\t__func__, (unsigned long long)sh->sector,\n\t\ts->locked, s->ops_request);\n}\n\n/*\n * Each stripe/dev can have one or more bion attached.\n * toread/towrite point to the first in a chain.\n * The bi_next chain must be in order.\n */\nstatic int add_stripe_bio(struct stripe_head *sh, struct bio *bi, int dd_idx,\n\t\t\t  int forwrite, int previous)\n{\n\tstruct bio **bip;\n\tstruct r5conf *conf = sh->raid_conf;\n\tint firstwrite=0;\n\n\tpr_debug(\"adding bi b#%llu to stripe s#%llu\\n\",\n\t\t(unsigned long long)bi->bi_iter.bi_sector,\n\t\t(unsigned long long)sh->sector);\n\n\tspin_lock_irq(&sh->stripe_lock);\n\tsh->dev[dd_idx].write_hint = bi->bi_write_hint;\n\t/* Don't allow new IO added to stripes in batch list */\n\tif (sh->batch_head)\n\t\tgoto overlap;\n\tif (forwrite) {\n\t\tbip = &sh->dev[dd_idx].towrite;\n\t\tif (*bip == NULL)\n\t\t\tfirstwrite = 1;\n\t} else\n\t\tbip = &sh->dev[dd_idx].toread;\n\twhile (*bip && (*bip)->bi_iter.bi_sector < bi->bi_iter.bi_sector) {\n\t\tif (bio_end_sector(*bip) > bi->bi_iter.bi_sector)\n\t\t\tgoto overlap;\n\t\tbip = & (*bip)->bi_next;\n\t}\n\tif (*bip && (*bip)->bi_iter.bi_sector < bio_end_sector(bi))\n\t\tgoto overlap;\n\n\tif (forwrite && raid5_has_ppl(conf)) {\n\t\t/*\n\t\t * With PPL only writes to consecutive data chunks within a\n\t\t * stripe are allowed because for a single stripe_head we can\n\t\t * only have one PPL entry at a time, which describes one data\n\t\t * range. Not really an overlap, but wait_for_overlap can be\n\t\t * used to handle this.\n\t\t */\n\t\tsector_t sector;\n\t\tsector_t first = 0;\n\t\tsector_t last = 0;\n\t\tint count = 0;\n\t\tint i;\n\n\t\tfor (i = 0; i < sh->disks; i++) {\n\t\t\tif (i != sh->pd_idx &&\n\t\t\t    (i == dd_idx || sh->dev[i].towrite)) {\n\t\t\t\tsector = sh->dev[i].sector;\n\t\t\t\tif (count == 0 || sector < first)\n\t\t\t\t\tfirst = sector;\n\t\t\t\tif (sector > last)\n\t\t\t\t\tlast = sector;\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\n\t\tif (first + conf->chunk_sectors * (count - 1) != last)\n\t\t\tgoto overlap;\n\t}\n\n\tif (!forwrite || previous)\n\t\tclear_bit(STRIPE_BATCH_READY, &sh->state);\n\n\tBUG_ON(*bip && bi->bi_next && (*bip) != bi->bi_next);\n\tif (*bip)\n\t\tbi->bi_next = *bip;\n\t*bip = bi;\n\tbio_inc_remaining(bi);\n\tmd_write_inc(conf->mddev, bi);\n\n\tif (forwrite) {\n\t\t/* check if page is covered */\n\t\tsector_t sector = sh->dev[dd_idx].sector;\n\t\tfor (bi=sh->dev[dd_idx].towrite;\n\t\t     sector < sh->dev[dd_idx].sector + RAID5_STRIPE_SECTORS(conf) &&\n\t\t\t     bi && bi->bi_iter.bi_sector <= sector;\n\t\t     bi = r5_next_bio(conf, bi, sh->dev[dd_idx].sector)) {\n\t\t\tif (bio_end_sector(bi) >= sector)\n\t\t\t\tsector = bio_end_sector(bi);\n\t\t}\n\t\tif (sector >= sh->dev[dd_idx].sector + RAID5_STRIPE_SECTORS(conf))\n\t\t\tif (!test_and_set_bit(R5_OVERWRITE, &sh->dev[dd_idx].flags))\n\t\t\t\tsh->overwrite_disks++;\n\t}\n\n\tpr_debug(\"added bi b#%llu to stripe s#%llu, disk %d.\\n\",\n\t\t(unsigned long long)(*bip)->bi_iter.bi_sector,\n\t\t(unsigned long long)sh->sector, dd_idx);\n\n\tif (conf->mddev->bitmap && firstwrite) {\n\t\t/* Cannot hold spinlock over bitmap_startwrite,\n\t\t * but must ensure this isn't added to a batch until\n\t\t * we have added to the bitmap and set bm_seq.\n\t\t * So set STRIPE_BITMAP_PENDING to prevent\n\t\t * batching.\n\t\t * If multiple add_stripe_bio() calls race here they\n\t\t * much all set STRIPE_BITMAP_PENDING.  So only the first one\n\t\t * to complete \"bitmap_startwrite\" gets to set\n\t\t * STRIPE_BIT_DELAY.  This is important as once a stripe\n\t\t * is added to a batch, STRIPE_BIT_DELAY cannot be changed\n\t\t * any more.\n\t\t */\n\t\tset_bit(STRIPE_BITMAP_PENDING, &sh->state);\n\t\tspin_unlock_irq(&sh->stripe_lock);\n\t\tmd_bitmap_startwrite(conf->mddev->bitmap, sh->sector,\n\t\t\t\t     RAID5_STRIPE_SECTORS(conf), 0);\n\t\tspin_lock_irq(&sh->stripe_lock);\n\t\tclear_bit(STRIPE_BITMAP_PENDING, &sh->state);\n\t\tif (!sh->batch_head) {\n\t\t\tsh->bm_seq = conf->seq_flush+1;\n\t\t\tset_bit(STRIPE_BIT_DELAY, &sh->state);\n\t\t}\n\t}\n\tspin_unlock_irq(&sh->stripe_lock);\n\n\tif (stripe_can_batch(sh))\n\t\tstripe_add_to_batch_list(conf, sh);\n\treturn 1;\n\n overlap:\n\tset_bit(R5_Overlap, &sh->dev[dd_idx].flags);\n\tspin_unlock_irq(&sh->stripe_lock);\n\treturn 0;\n}\n\nstatic void end_reshape(struct r5conf *conf);\n\nstatic void stripe_set_idx(sector_t stripe, struct r5conf *conf, int previous,\n\t\t\t    struct stripe_head *sh)\n{\n\tint sectors_per_chunk =\n\t\tprevious ? conf->prev_chunk_sectors : conf->chunk_sectors;\n\tint dd_idx;\n\tint chunk_offset = sector_div(stripe, sectors_per_chunk);\n\tint disks = previous ? conf->previous_raid_disks : conf->raid_disks;\n\n\traid5_compute_sector(conf,\n\t\t\t     stripe * (disks - conf->max_degraded)\n\t\t\t     *sectors_per_chunk + chunk_offset,\n\t\t\t     previous,\n\t\t\t     &dd_idx, sh);\n}\n\nstatic void\nhandle_failed_stripe(struct r5conf *conf, struct stripe_head *sh,\n\t\t     struct stripe_head_state *s, int disks)\n{\n\tint i;\n\tBUG_ON(sh->batch_head);\n\tfor (i = disks; i--; ) {\n\t\tstruct bio *bi;\n\t\tint bitmap_end = 0;\n\n\t\tif (test_bit(R5_ReadError, &sh->dev[i].flags)) {\n\t\t\tstruct md_rdev *rdev;\n\t\t\trcu_read_lock();\n\t\t\trdev = rcu_dereference(conf->disks[i].rdev);\n\t\t\tif (rdev && test_bit(In_sync, &rdev->flags) &&\n\t\t\t    !test_bit(Faulty, &rdev->flags))\n\t\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t\telse\n\t\t\t\trdev = NULL;\n\t\t\trcu_read_unlock();\n\t\t\tif (rdev) {\n\t\t\t\tif (!rdev_set_badblocks(\n\t\t\t\t\t    rdev,\n\t\t\t\t\t    sh->sector,\n\t\t\t\t\t    RAID5_STRIPE_SECTORS(conf), 0))\n\t\t\t\t\tmd_error(conf->mddev, rdev);\n\t\t\t\trdev_dec_pending(rdev, conf->mddev);\n\t\t\t}\n\t\t}\n\t\tspin_lock_irq(&sh->stripe_lock);\n\t\t/* fail all writes first */\n\t\tbi = sh->dev[i].towrite;\n\t\tsh->dev[i].towrite = NULL;\n\t\tsh->overwrite_disks = 0;\n\t\tspin_unlock_irq(&sh->stripe_lock);\n\t\tif (bi)\n\t\t\tbitmap_end = 1;\n\n\t\tlog_stripe_write_finished(sh);\n\n\t\tif (test_and_clear_bit(R5_Overlap, &sh->dev[i].flags))\n\t\t\twake_up(&conf->wait_for_overlap);\n\n\t\twhile (bi && bi->bi_iter.bi_sector <\n\t\t\tsh->dev[i].sector + RAID5_STRIPE_SECTORS(conf)) {\n\t\t\tstruct bio *nextbi = r5_next_bio(conf, bi, sh->dev[i].sector);\n\n\t\t\tmd_write_end(conf->mddev);\n\t\t\tbio_io_error(bi);\n\t\t\tbi = nextbi;\n\t\t}\n\t\tif (bitmap_end)\n\t\t\tmd_bitmap_endwrite(conf->mddev->bitmap, sh->sector,\n\t\t\t\t\t   RAID5_STRIPE_SECTORS(conf), 0, 0);\n\t\tbitmap_end = 0;\n\t\t/* and fail all 'written' */\n\t\tbi = sh->dev[i].written;\n\t\tsh->dev[i].written = NULL;\n\t\tif (test_and_clear_bit(R5_SkipCopy, &sh->dev[i].flags)) {\n\t\t\tWARN_ON(test_bit(R5_UPTODATE, &sh->dev[i].flags));\n\t\t\tsh->dev[i].page = sh->dev[i].orig_page;\n\t\t}\n\n\t\tif (bi) bitmap_end = 1;\n\t\twhile (bi && bi->bi_iter.bi_sector <\n\t\t       sh->dev[i].sector + RAID5_STRIPE_SECTORS(conf)) {\n\t\t\tstruct bio *bi2 = r5_next_bio(conf, bi, sh->dev[i].sector);\n\n\t\t\tmd_write_end(conf->mddev);\n\t\t\tbio_io_error(bi);\n\t\t\tbi = bi2;\n\t\t}\n\n\t\t/* fail any reads if this device is non-operational and\n\t\t * the data has not reached the cache yet.\n\t\t */\n\t\tif (!test_bit(R5_Wantfill, &sh->dev[i].flags) &&\n\t\t    s->failed > conf->max_degraded &&\n\t\t    (!test_bit(R5_Insync, &sh->dev[i].flags) ||\n\t\t      test_bit(R5_ReadError, &sh->dev[i].flags))) {\n\t\t\tspin_lock_irq(&sh->stripe_lock);\n\t\t\tbi = sh->dev[i].toread;\n\t\t\tsh->dev[i].toread = NULL;\n\t\t\tspin_unlock_irq(&sh->stripe_lock);\n\t\t\tif (test_and_clear_bit(R5_Overlap, &sh->dev[i].flags))\n\t\t\t\twake_up(&conf->wait_for_overlap);\n\t\t\tif (bi)\n\t\t\t\ts->to_read--;\n\t\t\twhile (bi && bi->bi_iter.bi_sector <\n\t\t\t       sh->dev[i].sector + RAID5_STRIPE_SECTORS(conf)) {\n\t\t\t\tstruct bio *nextbi =\n\t\t\t\t\tr5_next_bio(conf, bi, sh->dev[i].sector);\n\n\t\t\t\tbio_io_error(bi);\n\t\t\t\tbi = nextbi;\n\t\t\t}\n\t\t}\n\t\tif (bitmap_end)\n\t\t\tmd_bitmap_endwrite(conf->mddev->bitmap, sh->sector,\n\t\t\t\t\t   RAID5_STRIPE_SECTORS(conf), 0, 0);\n\t\t/* If we were in the middle of a write the parity block might\n\t\t * still be locked - so just clear all R5_LOCKED flags\n\t\t */\n\t\tclear_bit(R5_LOCKED, &sh->dev[i].flags);\n\t}\n\ts->to_write = 0;\n\ts->written = 0;\n\n\tif (test_and_clear_bit(STRIPE_FULL_WRITE, &sh->state))\n\t\tif (atomic_dec_and_test(&conf->pending_full_writes))\n\t\t\tmd_wakeup_thread(conf->mddev->thread);\n}\n\nstatic void\nhandle_failed_sync(struct r5conf *conf, struct stripe_head *sh,\n\t\t   struct stripe_head_state *s)\n{\n\tint abort = 0;\n\tint i;\n\n\tBUG_ON(sh->batch_head);\n\tclear_bit(STRIPE_SYNCING, &sh->state);\n\tif (test_and_clear_bit(R5_Overlap, &sh->dev[sh->pd_idx].flags))\n\t\twake_up(&conf->wait_for_overlap);\n\ts->syncing = 0;\n\ts->replacing = 0;\n\t/* There is nothing more to do for sync/check/repair.\n\t * Don't even need to abort as that is handled elsewhere\n\t * if needed, and not always wanted e.g. if there is a known\n\t * bad block here.\n\t * For recover/replace we need to record a bad block on all\n\t * non-sync devices, or abort the recovery\n\t */\n\tif (test_bit(MD_RECOVERY_RECOVER, &conf->mddev->recovery)) {\n\t\t/* During recovery devices cannot be removed, so\n\t\t * locking and refcounting of rdevs is not needed\n\t\t */\n\t\trcu_read_lock();\n\t\tfor (i = 0; i < conf->raid_disks; i++) {\n\t\t\tstruct md_rdev *rdev = rcu_dereference(conf->disks[i].rdev);\n\t\t\tif (rdev\n\t\t\t    && !test_bit(Faulty, &rdev->flags)\n\t\t\t    && !test_bit(In_sync, &rdev->flags)\n\t\t\t    && !rdev_set_badblocks(rdev, sh->sector,\n\t\t\t\t\t\t   RAID5_STRIPE_SECTORS(conf), 0))\n\t\t\t\tabort = 1;\n\t\t\trdev = rcu_dereference(conf->disks[i].replacement);\n\t\t\tif (rdev\n\t\t\t    && !test_bit(Faulty, &rdev->flags)\n\t\t\t    && !test_bit(In_sync, &rdev->flags)\n\t\t\t    && !rdev_set_badblocks(rdev, sh->sector,\n\t\t\t\t\t\t   RAID5_STRIPE_SECTORS(conf), 0))\n\t\t\t\tabort = 1;\n\t\t}\n\t\trcu_read_unlock();\n\t\tif (abort)\n\t\t\tconf->recovery_disabled =\n\t\t\t\tconf->mddev->recovery_disabled;\n\t}\n\tmd_done_sync(conf->mddev, RAID5_STRIPE_SECTORS(conf), !abort);\n}\n\nstatic int want_replace(struct stripe_head *sh, int disk_idx)\n{\n\tstruct md_rdev *rdev;\n\tint rv = 0;\n\n\trcu_read_lock();\n\trdev = rcu_dereference(sh->raid_conf->disks[disk_idx].replacement);\n\tif (rdev\n\t    && !test_bit(Faulty, &rdev->flags)\n\t    && !test_bit(In_sync, &rdev->flags)\n\t    && (rdev->recovery_offset <= sh->sector\n\t\t|| rdev->mddev->recovery_cp <= sh->sector))\n\t\trv = 1;\n\trcu_read_unlock();\n\treturn rv;\n}\n\nstatic int need_this_block(struct stripe_head *sh, struct stripe_head_state *s,\n\t\t\t   int disk_idx, int disks)\n{\n\tstruct r5dev *dev = &sh->dev[disk_idx];\n\tstruct r5dev *fdev[2] = { &sh->dev[s->failed_num[0]],\n\t\t\t\t  &sh->dev[s->failed_num[1]] };\n\tint i;\n\tbool force_rcw = (sh->raid_conf->rmw_level == PARITY_DISABLE_RMW);\n\n\n\tif (test_bit(R5_LOCKED, &dev->flags) ||\n\t    test_bit(R5_UPTODATE, &dev->flags))\n\t\t/* No point reading this as we already have it or have\n\t\t * decided to get it.\n\t\t */\n\t\treturn 0;\n\n\tif (dev->toread ||\n\t    (dev->towrite && !test_bit(R5_OVERWRITE, &dev->flags)))\n\t\t/* We need this block to directly satisfy a request */\n\t\treturn 1;\n\n\tif (s->syncing || s->expanding ||\n\t    (s->replacing && want_replace(sh, disk_idx)))\n\t\t/* When syncing, or expanding we read everything.\n\t\t * When replacing, we need the replaced block.\n\t\t */\n\t\treturn 1;\n\n\tif ((s->failed >= 1 && fdev[0]->toread) ||\n\t    (s->failed >= 2 && fdev[1]->toread))\n\t\t/* If we want to read from a failed device, then\n\t\t * we need to actually read every other device.\n\t\t */\n\t\treturn 1;\n\n\t/* Sometimes neither read-modify-write nor reconstruct-write\n\t * cycles can work.  In those cases we read every block we\n\t * can.  Then the parity-update is certain to have enough to\n\t * work with.\n\t * This can only be a problem when we need to write something,\n\t * and some device has failed.  If either of those tests\n\t * fail we need look no further.\n\t */\n\tif (!s->failed || !s->to_write)\n\t\treturn 0;\n\n\tif (test_bit(R5_Insync, &dev->flags) &&\n\t    !test_bit(STRIPE_PREREAD_ACTIVE, &sh->state))\n\t\t/* Pre-reads at not permitted until after short delay\n\t\t * to gather multiple requests.  However if this\n\t\t * device is no Insync, the block could only be computed\n\t\t * and there is no need to delay that.\n\t\t */\n\t\treturn 0;\n\n\tfor (i = 0; i < s->failed && i < 2; i++) {\n\t\tif (fdev[i]->towrite &&\n\t\t    !test_bit(R5_UPTODATE, &fdev[i]->flags) &&\n\t\t    !test_bit(R5_OVERWRITE, &fdev[i]->flags))\n\t\t\t/* If we have a partial write to a failed\n\t\t\t * device, then we will need to reconstruct\n\t\t\t * the content of that device, so all other\n\t\t\t * devices must be read.\n\t\t\t */\n\t\t\treturn 1;\n\n\t\tif (s->failed >= 2 &&\n\t\t    (fdev[i]->towrite ||\n\t\t     s->failed_num[i] == sh->pd_idx ||\n\t\t     s->failed_num[i] == sh->qd_idx) &&\n\t\t    !test_bit(R5_UPTODATE, &fdev[i]->flags))\n\t\t\t/* In max degraded raid6, If the failed disk is P, Q,\n\t\t\t * or we want to read the failed disk, we need to do\n\t\t\t * reconstruct-write.\n\t\t\t */\n\t\t\tforce_rcw = true;\n\t}\n\n\t/* If we are forced to do a reconstruct-write, because parity\n\t * cannot be trusted and we are currently recovering it, there\n\t * is extra need to be careful.\n\t * If one of the devices that we would need to read, because\n\t * it is not being overwritten (and maybe not written at all)\n\t * is missing/faulty, then we need to read everything we can.\n\t */\n\tif (!force_rcw &&\n\t    sh->sector < sh->raid_conf->mddev->recovery_cp)\n\t\t/* reconstruct-write isn't being forced */\n\t\treturn 0;\n\tfor (i = 0; i < s->failed && i < 2; i++) {\n\t\tif (s->failed_num[i] != sh->pd_idx &&\n\t\t    s->failed_num[i] != sh->qd_idx &&\n\t\t    !test_bit(R5_UPTODATE, &fdev[i]->flags) &&\n\t\t    !test_bit(R5_OVERWRITE, &fdev[i]->flags))\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n/* fetch_block - checks the given member device to see if its data needs\n * to be read or computed to satisfy a request.\n *\n * Returns 1 when no more member devices need to be checked, otherwise returns\n * 0 to tell the loop in handle_stripe_fill to continue\n */\nstatic int fetch_block(struct stripe_head *sh, struct stripe_head_state *s,\n\t\t       int disk_idx, int disks)\n{\n\tstruct r5dev *dev = &sh->dev[disk_idx];\n\n\t/* is the data in this block needed, and can we get it? */\n\tif (need_this_block(sh, s, disk_idx, disks)) {\n\t\t/* we would like to get this block, possibly by computing it,\n\t\t * otherwise read it if the backing disk is insync\n\t\t */\n\t\tBUG_ON(test_bit(R5_Wantcompute, &dev->flags));\n\t\tBUG_ON(test_bit(R5_Wantread, &dev->flags));\n\t\tBUG_ON(sh->batch_head);\n\n\t\t/*\n\t\t * In the raid6 case if the only non-uptodate disk is P\n\t\t * then we already trusted P to compute the other failed\n\t\t * drives. It is safe to compute rather than re-read P.\n\t\t * In other cases we only compute blocks from failed\n\t\t * devices, otherwise check/repair might fail to detect\n\t\t * a real inconsistency.\n\t\t */\n\n\t\tif ((s->uptodate == disks - 1) &&\n\t\t    ((sh->qd_idx >= 0 && sh->pd_idx == disk_idx) ||\n\t\t    (s->failed && (disk_idx == s->failed_num[0] ||\n\t\t\t\t   disk_idx == s->failed_num[1])))) {\n\t\t\t/* have disk failed, and we're requested to fetch it;\n\t\t\t * do compute it\n\t\t\t */\n\t\t\tpr_debug(\"Computing stripe %llu block %d\\n\",\n\t\t\t       (unsigned long long)sh->sector, disk_idx);\n\t\t\tset_bit(STRIPE_COMPUTE_RUN, &sh->state);\n\t\t\tset_bit(STRIPE_OP_COMPUTE_BLK, &s->ops_request);\n\t\t\tset_bit(R5_Wantcompute, &dev->flags);\n\t\t\tsh->ops.target = disk_idx;\n\t\t\tsh->ops.target2 = -1; /* no 2nd target */\n\t\t\ts->req_compute = 1;\n\t\t\t/* Careful: from this point on 'uptodate' is in the eye\n\t\t\t * of raid_run_ops which services 'compute' operations\n\t\t\t * before writes. R5_Wantcompute flags a block that will\n\t\t\t * be R5_UPTODATE by the time it is needed for a\n\t\t\t * subsequent operation.\n\t\t\t */\n\t\t\ts->uptodate++;\n\t\t\treturn 1;\n\t\t} else if (s->uptodate == disks-2 && s->failed >= 2) {\n\t\t\t/* Computing 2-failure is *very* expensive; only\n\t\t\t * do it if failed >= 2\n\t\t\t */\n\t\t\tint other;\n\t\t\tfor (other = disks; other--; ) {\n\t\t\t\tif (other == disk_idx)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (!test_bit(R5_UPTODATE,\n\t\t\t\t      &sh->dev[other].flags))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tBUG_ON(other < 0);\n\t\t\tpr_debug(\"Computing stripe %llu blocks %d,%d\\n\",\n\t\t\t       (unsigned long long)sh->sector,\n\t\t\t       disk_idx, other);\n\t\t\tset_bit(STRIPE_COMPUTE_RUN, &sh->state);\n\t\t\tset_bit(STRIPE_OP_COMPUTE_BLK, &s->ops_request);\n\t\t\tset_bit(R5_Wantcompute, &sh->dev[disk_idx].flags);\n\t\t\tset_bit(R5_Wantcompute, &sh->dev[other].flags);\n\t\t\tsh->ops.target = disk_idx;\n\t\t\tsh->ops.target2 = other;\n\t\t\ts->uptodate += 2;\n\t\t\ts->req_compute = 1;\n\t\t\treturn 1;\n\t\t} else if (test_bit(R5_Insync, &dev->flags)) {\n\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\tset_bit(R5_Wantread, &dev->flags);\n\t\t\ts->locked++;\n\t\t\tpr_debug(\"Reading block %d (sync=%d)\\n\",\n\t\t\t\tdisk_idx, s->syncing);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n * handle_stripe_fill - read or compute data to satisfy pending requests.\n */\nstatic void handle_stripe_fill(struct stripe_head *sh,\n\t\t\t       struct stripe_head_state *s,\n\t\t\t       int disks)\n{\n\tint i;\n\n\t/* look for blocks to read/compute, skip this if a compute\n\t * is already in flight, or if the stripe contents are in the\n\t * midst of changing due to a write\n\t */\n\tif (!test_bit(STRIPE_COMPUTE_RUN, &sh->state) && !sh->check_state &&\n\t    !sh->reconstruct_state) {\n\n\t\t/*\n\t\t * For degraded stripe with data in journal, do not handle\n\t\t * read requests yet, instead, flush the stripe to raid\n\t\t * disks first, this avoids handling complex rmw of write\n\t\t * back cache (prexor with orig_page, and then xor with\n\t\t * page) in the read path\n\t\t */\n\t\tif (s->injournal && s->failed) {\n\t\t\tif (test_bit(STRIPE_R5C_CACHING, &sh->state))\n\t\t\t\tr5c_make_stripe_write_out(sh);\n\t\t\tgoto out;\n\t\t}\n\n\t\tfor (i = disks; i--; )\n\t\t\tif (fetch_block(sh, s, i, disks))\n\t\t\t\tbreak;\n\t}\nout:\n\tset_bit(STRIPE_HANDLE, &sh->state);\n}\n\nstatic void break_stripe_batch_list(struct stripe_head *head_sh,\n\t\t\t\t    unsigned long handle_flags);\n/* handle_stripe_clean_event\n * any written block on an uptodate or failed drive can be returned.\n * Note that if we 'wrote' to a failed drive, it will be UPTODATE, but\n * never LOCKED, so we don't need to test 'failed' directly.\n */\nstatic void handle_stripe_clean_event(struct r5conf *conf,\n\tstruct stripe_head *sh, int disks)\n{\n\tint i;\n\tstruct r5dev *dev;\n\tint discard_pending = 0;\n\tstruct stripe_head *head_sh = sh;\n\tbool do_endio = false;\n\n\tfor (i = disks; i--; )\n\t\tif (sh->dev[i].written) {\n\t\t\tdev = &sh->dev[i];\n\t\t\tif (!test_bit(R5_LOCKED, &dev->flags) &&\n\t\t\t    (test_bit(R5_UPTODATE, &dev->flags) ||\n\t\t\t     test_bit(R5_Discard, &dev->flags) ||\n\t\t\t     test_bit(R5_SkipCopy, &dev->flags))) {\n\t\t\t\t/* We can return any write requests */\n\t\t\t\tstruct bio *wbi, *wbi2;\n\t\t\t\tpr_debug(\"Return write for disc %d\\n\", i);\n\t\t\t\tif (test_and_clear_bit(R5_Discard, &dev->flags))\n\t\t\t\t\tclear_bit(R5_UPTODATE, &dev->flags);\n\t\t\t\tif (test_and_clear_bit(R5_SkipCopy, &dev->flags)) {\n\t\t\t\t\tWARN_ON(test_bit(R5_UPTODATE, &dev->flags));\n\t\t\t\t}\n\t\t\t\tdo_endio = true;\n\nreturnbi:\n\t\t\t\tdev->page = dev->orig_page;\n\t\t\t\twbi = dev->written;\n\t\t\t\tdev->written = NULL;\n\t\t\t\twhile (wbi && wbi->bi_iter.bi_sector <\n\t\t\t\t\tdev->sector + RAID5_STRIPE_SECTORS(conf)) {\n\t\t\t\t\twbi2 = r5_next_bio(conf, wbi, dev->sector);\n\t\t\t\t\tmd_write_end(conf->mddev);\n\t\t\t\t\tbio_endio(wbi);\n\t\t\t\t\twbi = wbi2;\n\t\t\t\t}\n\t\t\t\tmd_bitmap_endwrite(conf->mddev->bitmap, sh->sector,\n\t\t\t\t\t\t   RAID5_STRIPE_SECTORS(conf),\n\t\t\t\t\t\t   !test_bit(STRIPE_DEGRADED, &sh->state),\n\t\t\t\t\t\t   0);\n\t\t\t\tif (head_sh->batch_head) {\n\t\t\t\t\tsh = list_first_entry(&sh->batch_list,\n\t\t\t\t\t\t\t      struct stripe_head,\n\t\t\t\t\t\t\t      batch_list);\n\t\t\t\t\tif (sh != head_sh) {\n\t\t\t\t\t\tdev = &sh->dev[i];\n\t\t\t\t\t\tgoto returnbi;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tsh = head_sh;\n\t\t\t\tdev = &sh->dev[i];\n\t\t\t} else if (test_bit(R5_Discard, &dev->flags))\n\t\t\t\tdiscard_pending = 1;\n\t\t}\n\n\tlog_stripe_write_finished(sh);\n\n\tif (!discard_pending &&\n\t    test_bit(R5_Discard, &sh->dev[sh->pd_idx].flags)) {\n\t\tint hash;\n\t\tclear_bit(R5_Discard, &sh->dev[sh->pd_idx].flags);\n\t\tclear_bit(R5_UPTODATE, &sh->dev[sh->pd_idx].flags);\n\t\tif (sh->qd_idx >= 0) {\n\t\t\tclear_bit(R5_Discard, &sh->dev[sh->qd_idx].flags);\n\t\t\tclear_bit(R5_UPTODATE, &sh->dev[sh->qd_idx].flags);\n\t\t}\n\t\t/* now that discard is done we can proceed with any sync */\n\t\tclear_bit(STRIPE_DISCARD, &sh->state);\n\t\t/*\n\t\t * SCSI discard will change some bio fields and the stripe has\n\t\t * no updated data, so remove it from hash list and the stripe\n\t\t * will be reinitialized\n\t\t */\nunhash:\n\t\thash = sh->hash_lock_index;\n\t\tspin_lock_irq(conf->hash_locks + hash);\n\t\tremove_hash(sh);\n\t\tspin_unlock_irq(conf->hash_locks + hash);\n\t\tif (head_sh->batch_head) {\n\t\t\tsh = list_first_entry(&sh->batch_list,\n\t\t\t\t\t      struct stripe_head, batch_list);\n\t\t\tif (sh != head_sh)\n\t\t\t\t\tgoto unhash;\n\t\t}\n\t\tsh = head_sh;\n\n\t\tif (test_bit(STRIPE_SYNC_REQUESTED, &sh->state))\n\t\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\n\t}\n\n\tif (test_and_clear_bit(STRIPE_FULL_WRITE, &sh->state))\n\t\tif (atomic_dec_and_test(&conf->pending_full_writes))\n\t\t\tmd_wakeup_thread(conf->mddev->thread);\n\n\tif (head_sh->batch_head && do_endio)\n\t\tbreak_stripe_batch_list(head_sh, STRIPE_EXPAND_SYNC_FLAGS);\n}\n\n/*\n * For RMW in write back cache, we need extra page in prexor to store the\n * old data. This page is stored in dev->orig_page.\n *\n * This function checks whether we have data for prexor. The exact logic\n * is:\n *       R5_UPTODATE && (!R5_InJournal || R5_OrigPageUPTDODATE)\n */\nstatic inline bool uptodate_for_rmw(struct r5dev *dev)\n{\n\treturn (test_bit(R5_UPTODATE, &dev->flags)) &&\n\t\t(!test_bit(R5_InJournal, &dev->flags) ||\n\t\t test_bit(R5_OrigPageUPTDODATE, &dev->flags));\n}\n\nstatic int handle_stripe_dirtying(struct r5conf *conf,\n\t\t\t\t  struct stripe_head *sh,\n\t\t\t\t  struct stripe_head_state *s,\n\t\t\t\t  int disks)\n{\n\tint rmw = 0, rcw = 0, i;\n\tsector_t recovery_cp = conf->mddev->recovery_cp;\n\n\t/* Check whether resync is now happening or should start.\n\t * If yes, then the array is dirty (after unclean shutdown or\n\t * initial creation), so parity in some stripes might be inconsistent.\n\t * In this case, we need to always do reconstruct-write, to ensure\n\t * that in case of drive failure or read-error correction, we\n\t * generate correct data from the parity.\n\t */\n\tif (conf->rmw_level == PARITY_DISABLE_RMW ||\n\t    (recovery_cp < MaxSector && sh->sector >= recovery_cp &&\n\t     s->failed == 0)) {\n\t\t/* Calculate the real rcw later - for now make it\n\t\t * look like rcw is cheaper\n\t\t */\n\t\trcw = 1; rmw = 2;\n\t\tpr_debug(\"force RCW rmw_level=%u, recovery_cp=%llu sh->sector=%llu\\n\",\n\t\t\t conf->rmw_level, (unsigned long long)recovery_cp,\n\t\t\t (unsigned long long)sh->sector);\n\t} else for (i = disks; i--; ) {\n\t\t/* would I have to read this buffer for read_modify_write */\n\t\tstruct r5dev *dev = &sh->dev[i];\n\t\tif (((dev->towrite && !delay_towrite(conf, dev, s)) ||\n\t\t     i == sh->pd_idx || i == sh->qd_idx ||\n\t\t     test_bit(R5_InJournal, &dev->flags)) &&\n\t\t    !test_bit(R5_LOCKED, &dev->flags) &&\n\t\t    !(uptodate_for_rmw(dev) ||\n\t\t      test_bit(R5_Wantcompute, &dev->flags))) {\n\t\t\tif (test_bit(R5_Insync, &dev->flags))\n\t\t\t\trmw++;\n\t\t\telse\n\t\t\t\trmw += 2*disks;  /* cannot read it */\n\t\t}\n\t\t/* Would I have to read this buffer for reconstruct_write */\n\t\tif (!test_bit(R5_OVERWRITE, &dev->flags) &&\n\t\t    i != sh->pd_idx && i != sh->qd_idx &&\n\t\t    !test_bit(R5_LOCKED, &dev->flags) &&\n\t\t    !(test_bit(R5_UPTODATE, &dev->flags) ||\n\t\t      test_bit(R5_Wantcompute, &dev->flags))) {\n\t\t\tif (test_bit(R5_Insync, &dev->flags))\n\t\t\t\trcw++;\n\t\t\telse\n\t\t\t\trcw += 2*disks;\n\t\t}\n\t}\n\n\tpr_debug(\"for sector %llu state 0x%lx, rmw=%d rcw=%d\\n\",\n\t\t (unsigned long long)sh->sector, sh->state, rmw, rcw);\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\tif ((rmw < rcw || (rmw == rcw && conf->rmw_level == PARITY_PREFER_RMW)) && rmw > 0) {\n\t\t/* prefer read-modify-write, but need to get some data */\n\t\tif (conf->mddev->queue)\n\t\t\tblk_add_trace_msg(conf->mddev->queue,\n\t\t\t\t\t  \"raid5 rmw %llu %d\",\n\t\t\t\t\t  (unsigned long long)sh->sector, rmw);\n\t\tfor (i = disks; i--; ) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t\tif (test_bit(R5_InJournal, &dev->flags) &&\n\t\t\t    dev->page == dev->orig_page &&\n\t\t\t    !test_bit(R5_LOCKED, &sh->dev[sh->pd_idx].flags)) {\n\t\t\t\t/* alloc page for prexor */\n\t\t\t\tstruct page *p = alloc_page(GFP_NOIO);\n\n\t\t\t\tif (p) {\n\t\t\t\t\tdev->orig_page = p;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\t/*\n\t\t\t\t * alloc_page() failed, try use\n\t\t\t\t * disk_info->extra_page\n\t\t\t\t */\n\t\t\t\tif (!test_and_set_bit(R5C_EXTRA_PAGE_IN_USE,\n\t\t\t\t\t\t      &conf->cache_state)) {\n\t\t\t\t\tr5c_use_extra_page(sh);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\t/* extra_page in use, add to delayed_list */\n\t\t\t\tset_bit(STRIPE_DELAYED, &sh->state);\n\t\t\t\ts->waiting_extra_page = 1;\n\t\t\t\treturn -EAGAIN;\n\t\t\t}\n\t\t}\n\n\t\tfor (i = disks; i--; ) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t\tif (((dev->towrite && !delay_towrite(conf, dev, s)) ||\n\t\t\t     i == sh->pd_idx || i == sh->qd_idx ||\n\t\t\t     test_bit(R5_InJournal, &dev->flags)) &&\n\t\t\t    !test_bit(R5_LOCKED, &dev->flags) &&\n\t\t\t    !(uptodate_for_rmw(dev) ||\n\t\t\t      test_bit(R5_Wantcompute, &dev->flags)) &&\n\t\t\t    test_bit(R5_Insync, &dev->flags)) {\n\t\t\t\tif (test_bit(STRIPE_PREREAD_ACTIVE,\n\t\t\t\t\t     &sh->state)) {\n\t\t\t\t\tpr_debug(\"Read_old block %d for r-m-w\\n\",\n\t\t\t\t\t\t i);\n\t\t\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\t\t\tset_bit(R5_Wantread, &dev->flags);\n\t\t\t\t\ts->locked++;\n\t\t\t\t} else\n\t\t\t\t\tset_bit(STRIPE_DELAYED, &sh->state);\n\t\t\t}\n\t\t}\n\t}\n\tif ((rcw < rmw || (rcw == rmw && conf->rmw_level != PARITY_PREFER_RMW)) && rcw > 0) {\n\t\t/* want reconstruct write, but need to get some data */\n\t\tint qread =0;\n\t\trcw = 0;\n\t\tfor (i = disks; i--; ) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t\tif (!test_bit(R5_OVERWRITE, &dev->flags) &&\n\t\t\t    i != sh->pd_idx && i != sh->qd_idx &&\n\t\t\t    !test_bit(R5_LOCKED, &dev->flags) &&\n\t\t\t    !(test_bit(R5_UPTODATE, &dev->flags) ||\n\t\t\t      test_bit(R5_Wantcompute, &dev->flags))) {\n\t\t\t\trcw++;\n\t\t\t\tif (test_bit(R5_Insync, &dev->flags) &&\n\t\t\t\t    test_bit(STRIPE_PREREAD_ACTIVE,\n\t\t\t\t\t     &sh->state)) {\n\t\t\t\t\tpr_debug(\"Read_old block \"\n\t\t\t\t\t\t\"%d for Reconstruct\\n\", i);\n\t\t\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\t\t\tset_bit(R5_Wantread, &dev->flags);\n\t\t\t\t\ts->locked++;\n\t\t\t\t\tqread++;\n\t\t\t\t} else\n\t\t\t\t\tset_bit(STRIPE_DELAYED, &sh->state);\n\t\t\t}\n\t\t}\n\t\tif (rcw && conf->mddev->queue)\n\t\t\tblk_add_trace_msg(conf->mddev->queue, \"raid5 rcw %llu %d %d %d\",\n\t\t\t\t\t  (unsigned long long)sh->sector,\n\t\t\t\t\t  rcw, qread, test_bit(STRIPE_DELAYED, &sh->state));\n\t}\n\n\tif (rcw > disks && rmw > disks &&\n\t    !test_bit(STRIPE_PREREAD_ACTIVE, &sh->state))\n\t\tset_bit(STRIPE_DELAYED, &sh->state);\n\n\t/* now if nothing is locked, and if we have enough data,\n\t * we can start a write request\n\t */\n\t/* since handle_stripe can be called at any time we need to handle the\n\t * case where a compute block operation has been submitted and then a\n\t * subsequent call wants to start a write request.  raid_run_ops only\n\t * handles the case where compute block and reconstruct are requested\n\t * simultaneously.  If this is not the case then new writes need to be\n\t * held off until the compute completes.\n\t */\n\tif ((s->req_compute || !test_bit(STRIPE_COMPUTE_RUN, &sh->state)) &&\n\t    (s->locked == 0 && (rcw == 0 || rmw == 0) &&\n\t     !test_bit(STRIPE_BIT_DELAY, &sh->state)))\n\t\tschedule_reconstruction(sh, s, rcw == 0, 0);\n\treturn 0;\n}\n\nstatic void handle_parity_checks5(struct r5conf *conf, struct stripe_head *sh,\n\t\t\t\tstruct stripe_head_state *s, int disks)\n{\n\tstruct r5dev *dev = NULL;\n\n\tBUG_ON(sh->batch_head);\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\n\tswitch (sh->check_state) {\n\tcase check_state_idle:\n\t\t/* start a new check operation if there are no failures */\n\t\tif (s->failed == 0) {\n\t\t\tBUG_ON(s->uptodate != disks);\n\t\t\tsh->check_state = check_state_run;\n\t\t\tset_bit(STRIPE_OP_CHECK, &s->ops_request);\n\t\t\tclear_bit(R5_UPTODATE, &sh->dev[sh->pd_idx].flags);\n\t\t\ts->uptodate--;\n\t\t\tbreak;\n\t\t}\n\t\tdev = &sh->dev[s->failed_num[0]];\n\t\tfallthrough;\n\tcase check_state_compute_result:\n\t\tsh->check_state = check_state_idle;\n\t\tif (!dev)\n\t\t\tdev = &sh->dev[sh->pd_idx];\n\n\t\t/* check that a write has not made the stripe insync */\n\t\tif (test_bit(STRIPE_INSYNC, &sh->state))\n\t\t\tbreak;\n\n\t\t/* either failed parity check, or recovery is happening */\n\t\tBUG_ON(!test_bit(R5_UPTODATE, &dev->flags));\n\t\tBUG_ON(s->uptodate != disks);\n\n\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\ts->locked++;\n\t\tset_bit(R5_Wantwrite, &dev->flags);\n\n\t\tclear_bit(STRIPE_DEGRADED, &sh->state);\n\t\tset_bit(STRIPE_INSYNC, &sh->state);\n\t\tbreak;\n\tcase check_state_run:\n\t\tbreak; /* we will be called again upon completion */\n\tcase check_state_check_result:\n\t\tsh->check_state = check_state_idle;\n\n\t\t/* if a failure occurred during the check operation, leave\n\t\t * STRIPE_INSYNC not set and let the stripe be handled again\n\t\t */\n\t\tif (s->failed)\n\t\t\tbreak;\n\n\t\t/* handle a successful check operation, if parity is correct\n\t\t * we are done.  Otherwise update the mismatch count and repair\n\t\t * parity if !MD_RECOVERY_CHECK\n\t\t */\n\t\tif ((sh->ops.zero_sum_result & SUM_CHECK_P_RESULT) == 0)\n\t\t\t/* parity is correct (on disc,\n\t\t\t * not in buffer any more)\n\t\t\t */\n\t\t\tset_bit(STRIPE_INSYNC, &sh->state);\n\t\telse {\n\t\t\tatomic64_add(RAID5_STRIPE_SECTORS(conf), &conf->mddev->resync_mismatches);\n\t\t\tif (test_bit(MD_RECOVERY_CHECK, &conf->mddev->recovery)) {\n\t\t\t\t/* don't try to repair!! */\n\t\t\t\tset_bit(STRIPE_INSYNC, &sh->state);\n\t\t\t\tpr_warn_ratelimited(\"%s: mismatch sector in range \"\n\t\t\t\t\t\t    \"%llu-%llu\\n\", mdname(conf->mddev),\n\t\t\t\t\t\t    (unsigned long long) sh->sector,\n\t\t\t\t\t\t    (unsigned long long) sh->sector +\n\t\t\t\t\t\t    RAID5_STRIPE_SECTORS(conf));\n\t\t\t} else {\n\t\t\t\tsh->check_state = check_state_compute_run;\n\t\t\t\tset_bit(STRIPE_COMPUTE_RUN, &sh->state);\n\t\t\t\tset_bit(STRIPE_OP_COMPUTE_BLK, &s->ops_request);\n\t\t\t\tset_bit(R5_Wantcompute,\n\t\t\t\t\t&sh->dev[sh->pd_idx].flags);\n\t\t\t\tsh->ops.target = sh->pd_idx;\n\t\t\t\tsh->ops.target2 = -1;\n\t\t\t\ts->uptodate++;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase check_state_compute_run:\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"%s: unknown check_state: %d sector: %llu\\n\",\n\t\t       __func__, sh->check_state,\n\t\t       (unsigned long long) sh->sector);\n\t\tBUG();\n\t}\n}\n\nstatic void handle_parity_checks6(struct r5conf *conf, struct stripe_head *sh,\n\t\t\t\t  struct stripe_head_state *s,\n\t\t\t\t  int disks)\n{\n\tint pd_idx = sh->pd_idx;\n\tint qd_idx = sh->qd_idx;\n\tstruct r5dev *dev;\n\n\tBUG_ON(sh->batch_head);\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\n\tBUG_ON(s->failed > 2);\n\n\t/* Want to check and possibly repair P and Q.\n\t * However there could be one 'failed' device, in which\n\t * case we can only check one of them, possibly using the\n\t * other to generate missing data\n\t */\n\n\tswitch (sh->check_state) {\n\tcase check_state_idle:\n\t\t/* start a new check operation if there are < 2 failures */\n\t\tif (s->failed == s->q_failed) {\n\t\t\t/* The only possible failed device holds Q, so it\n\t\t\t * makes sense to check P (If anything else were failed,\n\t\t\t * we would have used P to recreate it).\n\t\t\t */\n\t\t\tsh->check_state = check_state_run;\n\t\t}\n\t\tif (!s->q_failed && s->failed < 2) {\n\t\t\t/* Q is not failed, and we didn't use it to generate\n\t\t\t * anything, so it makes sense to check it\n\t\t\t */\n\t\t\tif (sh->check_state == check_state_run)\n\t\t\t\tsh->check_state = check_state_run_pq;\n\t\t\telse\n\t\t\t\tsh->check_state = check_state_run_q;\n\t\t}\n\n\t\t/* discard potentially stale zero_sum_result */\n\t\tsh->ops.zero_sum_result = 0;\n\n\t\tif (sh->check_state == check_state_run) {\n\t\t\t/* async_xor_zero_sum destroys the contents of P */\n\t\t\tclear_bit(R5_UPTODATE, &sh->dev[pd_idx].flags);\n\t\t\ts->uptodate--;\n\t\t}\n\t\tif (sh->check_state >= check_state_run &&\n\t\t    sh->check_state <= check_state_run_pq) {\n\t\t\t/* async_syndrome_zero_sum preserves P and Q, so\n\t\t\t * no need to mark them !uptodate here\n\t\t\t */\n\t\t\tset_bit(STRIPE_OP_CHECK, &s->ops_request);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* we have 2-disk failure */\n\t\tBUG_ON(s->failed != 2);\n\t\tfallthrough;\n\tcase check_state_compute_result:\n\t\tsh->check_state = check_state_idle;\n\n\t\t/* check that a write has not made the stripe insync */\n\t\tif (test_bit(STRIPE_INSYNC, &sh->state))\n\t\t\tbreak;\n\n\t\t/* now write out any block on a failed drive,\n\t\t * or P or Q if they were recomputed\n\t\t */\n\t\tdev = NULL;\n\t\tif (s->failed == 2) {\n\t\t\tdev = &sh->dev[s->failed_num[1]];\n\t\t\ts->locked++;\n\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\tset_bit(R5_Wantwrite, &dev->flags);\n\t\t}\n\t\tif (s->failed >= 1) {\n\t\t\tdev = &sh->dev[s->failed_num[0]];\n\t\t\ts->locked++;\n\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\tset_bit(R5_Wantwrite, &dev->flags);\n\t\t}\n\t\tif (sh->ops.zero_sum_result & SUM_CHECK_P_RESULT) {\n\t\t\tdev = &sh->dev[pd_idx];\n\t\t\ts->locked++;\n\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\tset_bit(R5_Wantwrite, &dev->flags);\n\t\t}\n\t\tif (sh->ops.zero_sum_result & SUM_CHECK_Q_RESULT) {\n\t\t\tdev = &sh->dev[qd_idx];\n\t\t\ts->locked++;\n\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\tset_bit(R5_Wantwrite, &dev->flags);\n\t\t}\n\t\tif (WARN_ONCE(dev && !test_bit(R5_UPTODATE, &dev->flags),\n\t\t\t      \"%s: disk%td not up to date\\n\",\n\t\t\t      mdname(conf->mddev),\n\t\t\t      dev - (struct r5dev *) &sh->dev)) {\n\t\t\tclear_bit(R5_LOCKED, &dev->flags);\n\t\t\tclear_bit(R5_Wantwrite, &dev->flags);\n\t\t\ts->locked--;\n\t\t}\n\t\tclear_bit(STRIPE_DEGRADED, &sh->state);\n\n\t\tset_bit(STRIPE_INSYNC, &sh->state);\n\t\tbreak;\n\tcase check_state_run:\n\tcase check_state_run_q:\n\tcase check_state_run_pq:\n\t\tbreak; /* we will be called again upon completion */\n\tcase check_state_check_result:\n\t\tsh->check_state = check_state_idle;\n\n\t\t/* handle a successful check operation, if parity is correct\n\t\t * we are done.  Otherwise update the mismatch count and repair\n\t\t * parity if !MD_RECOVERY_CHECK\n\t\t */\n\t\tif (sh->ops.zero_sum_result == 0) {\n\t\t\t/* both parities are correct */\n\t\t\tif (!s->failed)\n\t\t\t\tset_bit(STRIPE_INSYNC, &sh->state);\n\t\t\telse {\n\t\t\t\t/* in contrast to the raid5 case we can validate\n\t\t\t\t * parity, but still have a failure to write\n\t\t\t\t * back\n\t\t\t\t */\n\t\t\t\tsh->check_state = check_state_compute_result;\n\t\t\t\t/* Returning at this point means that we may go\n\t\t\t\t * off and bring p and/or q uptodate again so\n\t\t\t\t * we make sure to check zero_sum_result again\n\t\t\t\t * to verify if p or q need writeback\n\t\t\t\t */\n\t\t\t}\n\t\t} else {\n\t\t\tatomic64_add(RAID5_STRIPE_SECTORS(conf), &conf->mddev->resync_mismatches);\n\t\t\tif (test_bit(MD_RECOVERY_CHECK, &conf->mddev->recovery)) {\n\t\t\t\t/* don't try to repair!! */\n\t\t\t\tset_bit(STRIPE_INSYNC, &sh->state);\n\t\t\t\tpr_warn_ratelimited(\"%s: mismatch sector in range \"\n\t\t\t\t\t\t    \"%llu-%llu\\n\", mdname(conf->mddev),\n\t\t\t\t\t\t    (unsigned long long) sh->sector,\n\t\t\t\t\t\t    (unsigned long long) sh->sector +\n\t\t\t\t\t\t    RAID5_STRIPE_SECTORS(conf));\n\t\t\t} else {\n\t\t\t\tint *target = &sh->ops.target;\n\n\t\t\t\tsh->ops.target = -1;\n\t\t\t\tsh->ops.target2 = -1;\n\t\t\t\tsh->check_state = check_state_compute_run;\n\t\t\t\tset_bit(STRIPE_COMPUTE_RUN, &sh->state);\n\t\t\t\tset_bit(STRIPE_OP_COMPUTE_BLK, &s->ops_request);\n\t\t\t\tif (sh->ops.zero_sum_result & SUM_CHECK_P_RESULT) {\n\t\t\t\t\tset_bit(R5_Wantcompute,\n\t\t\t\t\t\t&sh->dev[pd_idx].flags);\n\t\t\t\t\t*target = pd_idx;\n\t\t\t\t\ttarget = &sh->ops.target2;\n\t\t\t\t\ts->uptodate++;\n\t\t\t\t}\n\t\t\t\tif (sh->ops.zero_sum_result & SUM_CHECK_Q_RESULT) {\n\t\t\t\t\tset_bit(R5_Wantcompute,\n\t\t\t\t\t\t&sh->dev[qd_idx].flags);\n\t\t\t\t\t*target = qd_idx;\n\t\t\t\t\ts->uptodate++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase check_state_compute_run:\n\t\tbreak;\n\tdefault:\n\t\tpr_warn(\"%s: unknown check_state: %d sector: %llu\\n\",\n\t\t\t__func__, sh->check_state,\n\t\t\t(unsigned long long) sh->sector);\n\t\tBUG();\n\t}\n}\n\nstatic void handle_stripe_expansion(struct r5conf *conf, struct stripe_head *sh)\n{\n\tint i;\n\n\t/* We have read all the blocks in this stripe and now we need to\n\t * copy some of them into a target stripe for expand.\n\t */\n\tstruct dma_async_tx_descriptor *tx = NULL;\n\tBUG_ON(sh->batch_head);\n\tclear_bit(STRIPE_EXPAND_SOURCE, &sh->state);\n\tfor (i = 0; i < sh->disks; i++)\n\t\tif (i != sh->pd_idx && i != sh->qd_idx) {\n\t\t\tint dd_idx, j;\n\t\t\tstruct stripe_head *sh2;\n\t\t\tstruct async_submit_ctl submit;\n\n\t\t\tsector_t bn = raid5_compute_blocknr(sh, i, 1);\n\t\t\tsector_t s = raid5_compute_sector(conf, bn, 0,\n\t\t\t\t\t\t\t  &dd_idx, NULL);\n\t\t\tsh2 = raid5_get_active_stripe(conf, s, 0, 1, 1);\n\t\t\tif (sh2 == NULL)\n\t\t\t\t/* so far only the early blocks of this stripe\n\t\t\t\t * have been requested.  When later blocks\n\t\t\t\t * get requested, we will try again\n\t\t\t\t */\n\t\t\t\tcontinue;\n\t\t\tif (!test_bit(STRIPE_EXPANDING, &sh2->state) ||\n\t\t\t   test_bit(R5_Expanded, &sh2->dev[dd_idx].flags)) {\n\t\t\t\t/* must have already done this block */\n\t\t\t\traid5_release_stripe(sh2);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* place all the copies on one channel */\n\t\t\tinit_async_submit(&submit, 0, tx, NULL, NULL, NULL);\n\t\t\ttx = async_memcpy(sh2->dev[dd_idx].page,\n\t\t\t\t\t  sh->dev[i].page, sh2->dev[dd_idx].offset,\n\t\t\t\t\t  sh->dev[i].offset, RAID5_STRIPE_SIZE(conf),\n\t\t\t\t\t  &submit);\n\n\t\t\tset_bit(R5_Expanded, &sh2->dev[dd_idx].flags);\n\t\t\tset_bit(R5_UPTODATE, &sh2->dev[dd_idx].flags);\n\t\t\tfor (j = 0; j < conf->raid_disks; j++)\n\t\t\t\tif (j != sh2->pd_idx &&\n\t\t\t\t    j != sh2->qd_idx &&\n\t\t\t\t    !test_bit(R5_Expanded, &sh2->dev[j].flags))\n\t\t\t\t\tbreak;\n\t\t\tif (j == conf->raid_disks) {\n\t\t\t\tset_bit(STRIPE_EXPAND_READY, &sh2->state);\n\t\t\t\tset_bit(STRIPE_HANDLE, &sh2->state);\n\t\t\t}\n\t\t\traid5_release_stripe(sh2);\n\n\t\t}\n\t/* done submitting copies, wait for them to complete */\n\tasync_tx_quiesce(&tx);\n}\n\n/*\n * handle_stripe - do things to a stripe.\n *\n * We lock the stripe by setting STRIPE_ACTIVE and then examine the\n * state of various bits to see what needs to be done.\n * Possible results:\n *    return some read requests which now have data\n *    return some write requests which are safely on storage\n *    schedule a read on some buffers\n *    schedule a write of some buffers\n *    return confirmation of parity correctness\n *\n */\n\nstatic void analyse_stripe(struct stripe_head *sh, struct stripe_head_state *s)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\tint disks = sh->disks;\n\tstruct r5dev *dev;\n\tint i;\n\tint do_recovery = 0;\n\n\tmemset(s, 0, sizeof(*s));\n\n\ts->expanding = test_bit(STRIPE_EXPAND_SOURCE, &sh->state) && !sh->batch_head;\n\ts->expanded = test_bit(STRIPE_EXPAND_READY, &sh->state) && !sh->batch_head;\n\ts->failed_num[0] = -1;\n\ts->failed_num[1] = -1;\n\ts->log_failed = r5l_log_disk_error(conf);\n\n\t/* Now to look around and see what can be done */\n\trcu_read_lock();\n\tfor (i=disks; i--; ) {\n\t\tstruct md_rdev *rdev;\n\t\tsector_t first_bad;\n\t\tint bad_sectors;\n\t\tint is_bad = 0;\n\n\t\tdev = &sh->dev[i];\n\n\t\tpr_debug(\"check %d: state 0x%lx read %p write %p written %p\\n\",\n\t\t\t i, dev->flags,\n\t\t\t dev->toread, dev->towrite, dev->written);\n\t\t/* maybe we can reply to a read\n\t\t *\n\t\t * new wantfill requests are only permitted while\n\t\t * ops_complete_biofill is guaranteed to be inactive\n\t\t */\n\t\tif (test_bit(R5_UPTODATE, &dev->flags) && dev->toread &&\n\t\t    !test_bit(STRIPE_BIOFILL_RUN, &sh->state))\n\t\t\tset_bit(R5_Wantfill, &dev->flags);\n\n\t\t/* now count some things */\n\t\tif (test_bit(R5_LOCKED, &dev->flags))\n\t\t\ts->locked++;\n\t\tif (test_bit(R5_UPTODATE, &dev->flags))\n\t\t\ts->uptodate++;\n\t\tif (test_bit(R5_Wantcompute, &dev->flags)) {\n\t\t\ts->compute++;\n\t\t\tBUG_ON(s->compute > 2);\n\t\t}\n\n\t\tif (test_bit(R5_Wantfill, &dev->flags))\n\t\t\ts->to_fill++;\n\t\telse if (dev->toread)\n\t\t\ts->to_read++;\n\t\tif (dev->towrite) {\n\t\t\ts->to_write++;\n\t\t\tif (!test_bit(R5_OVERWRITE, &dev->flags))\n\t\t\t\ts->non_overwrite++;\n\t\t}\n\t\tif (dev->written)\n\t\t\ts->written++;\n\t\t/* Prefer to use the replacement for reads, but only\n\t\t * if it is recovered enough and has no bad blocks.\n\t\t */\n\t\trdev = rcu_dereference(conf->disks[i].replacement);\n\t\tif (rdev && !test_bit(Faulty, &rdev->flags) &&\n\t\t    rdev->recovery_offset >= sh->sector + RAID5_STRIPE_SECTORS(conf) &&\n\t\t    !is_badblock(rdev, sh->sector, RAID5_STRIPE_SECTORS(conf),\n\t\t\t\t &first_bad, &bad_sectors))\n\t\t\tset_bit(R5_ReadRepl, &dev->flags);\n\t\telse {\n\t\t\tif (rdev && !test_bit(Faulty, &rdev->flags))\n\t\t\t\tset_bit(R5_NeedReplace, &dev->flags);\n\t\t\telse\n\t\t\t\tclear_bit(R5_NeedReplace, &dev->flags);\n\t\t\trdev = rcu_dereference(conf->disks[i].rdev);\n\t\t\tclear_bit(R5_ReadRepl, &dev->flags);\n\t\t}\n\t\tif (rdev && test_bit(Faulty, &rdev->flags))\n\t\t\trdev = NULL;\n\t\tif (rdev) {\n\t\t\tis_bad = is_badblock(rdev, sh->sector, RAID5_STRIPE_SECTORS(conf),\n\t\t\t\t\t     &first_bad, &bad_sectors);\n\t\t\tif (s->blocked_rdev == NULL\n\t\t\t    && (test_bit(Blocked, &rdev->flags)\n\t\t\t\t|| is_bad < 0)) {\n\t\t\t\tif (is_bad < 0)\n\t\t\t\t\tset_bit(BlockedBadBlocks,\n\t\t\t\t\t\t&rdev->flags);\n\t\t\t\ts->blocked_rdev = rdev;\n\t\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t\t}\n\t\t}\n\t\tclear_bit(R5_Insync, &dev->flags);\n\t\tif (!rdev)\n\t\t\t/* Not in-sync */;\n\t\telse if (is_bad) {\n\t\t\t/* also not in-sync */\n\t\t\tif (!test_bit(WriteErrorSeen, &rdev->flags) &&\n\t\t\t    test_bit(R5_UPTODATE, &dev->flags)) {\n\t\t\t\t/* treat as in-sync, but with a read error\n\t\t\t\t * which we can now try to correct\n\t\t\t\t */\n\t\t\t\tset_bit(R5_Insync, &dev->flags);\n\t\t\t\tset_bit(R5_ReadError, &dev->flags);\n\t\t\t}\n\t\t} else if (test_bit(In_sync, &rdev->flags))\n\t\t\tset_bit(R5_Insync, &dev->flags);\n\t\telse if (sh->sector + RAID5_STRIPE_SECTORS(conf) <= rdev->recovery_offset)\n\t\t\t/* in sync if before recovery_offset */\n\t\t\tset_bit(R5_Insync, &dev->flags);\n\t\telse if (test_bit(R5_UPTODATE, &dev->flags) &&\n\t\t\t test_bit(R5_Expanded, &dev->flags))\n\t\t\t/* If we've reshaped into here, we assume it is Insync.\n\t\t\t * We will shortly update recovery_offset to make\n\t\t\t * it official.\n\t\t\t */\n\t\t\tset_bit(R5_Insync, &dev->flags);\n\n\t\tif (test_bit(R5_WriteError, &dev->flags)) {\n\t\t\t/* This flag does not apply to '.replacement'\n\t\t\t * only to .rdev, so make sure to check that*/\n\t\t\tstruct md_rdev *rdev2 = rcu_dereference(\n\t\t\t\tconf->disks[i].rdev);\n\t\t\tif (rdev2 == rdev)\n\t\t\t\tclear_bit(R5_Insync, &dev->flags);\n\t\t\tif (rdev2 && !test_bit(Faulty, &rdev2->flags)) {\n\t\t\t\ts->handle_bad_blocks = 1;\n\t\t\t\tatomic_inc(&rdev2->nr_pending);\n\t\t\t} else\n\t\t\t\tclear_bit(R5_WriteError, &dev->flags);\n\t\t}\n\t\tif (test_bit(R5_MadeGood, &dev->flags)) {\n\t\t\t/* This flag does not apply to '.replacement'\n\t\t\t * only to .rdev, so make sure to check that*/\n\t\t\tstruct md_rdev *rdev2 = rcu_dereference(\n\t\t\t\tconf->disks[i].rdev);\n\t\t\tif (rdev2 && !test_bit(Faulty, &rdev2->flags)) {\n\t\t\t\ts->handle_bad_blocks = 1;\n\t\t\t\tatomic_inc(&rdev2->nr_pending);\n\t\t\t} else\n\t\t\t\tclear_bit(R5_MadeGood, &dev->flags);\n\t\t}\n\t\tif (test_bit(R5_MadeGoodRepl, &dev->flags)) {\n\t\t\tstruct md_rdev *rdev2 = rcu_dereference(\n\t\t\t\tconf->disks[i].replacement);\n\t\t\tif (rdev2 && !test_bit(Faulty, &rdev2->flags)) {\n\t\t\t\ts->handle_bad_blocks = 1;\n\t\t\t\tatomic_inc(&rdev2->nr_pending);\n\t\t\t} else\n\t\t\t\tclear_bit(R5_MadeGoodRepl, &dev->flags);\n\t\t}\n\t\tif (!test_bit(R5_Insync, &dev->flags)) {\n\t\t\t/* The ReadError flag will just be confusing now */\n\t\t\tclear_bit(R5_ReadError, &dev->flags);\n\t\t\tclear_bit(R5_ReWrite, &dev->flags);\n\t\t}\n\t\tif (test_bit(R5_ReadError, &dev->flags))\n\t\t\tclear_bit(R5_Insync, &dev->flags);\n\t\tif (!test_bit(R5_Insync, &dev->flags)) {\n\t\t\tif (s->failed < 2)\n\t\t\t\ts->failed_num[s->failed] = i;\n\t\t\ts->failed++;\n\t\t\tif (rdev && !test_bit(Faulty, &rdev->flags))\n\t\t\t\tdo_recovery = 1;\n\t\t\telse if (!rdev) {\n\t\t\t\trdev = rcu_dereference(\n\t\t\t\t    conf->disks[i].replacement);\n\t\t\t\tif (rdev && !test_bit(Faulty, &rdev->flags))\n\t\t\t\t\tdo_recovery = 1;\n\t\t\t}\n\t\t}\n\n\t\tif (test_bit(R5_InJournal, &dev->flags))\n\t\t\ts->injournal++;\n\t\tif (test_bit(R5_InJournal, &dev->flags) && dev->written)\n\t\t\ts->just_cached++;\n\t}\n\tif (test_bit(STRIPE_SYNCING, &sh->state)) {\n\t\t/* If there is a failed device being replaced,\n\t\t *     we must be recovering.\n\t\t * else if we are after recovery_cp, we must be syncing\n\t\t * else if MD_RECOVERY_REQUESTED is set, we also are syncing.\n\t\t * else we can only be replacing\n\t\t * sync and recovery both need to read all devices, and so\n\t\t * use the same flag.\n\t\t */\n\t\tif (do_recovery ||\n\t\t    sh->sector >= conf->mddev->recovery_cp ||\n\t\t    test_bit(MD_RECOVERY_REQUESTED, &(conf->mddev->recovery)))\n\t\t\ts->syncing = 1;\n\t\telse\n\t\t\ts->replacing = 1;\n\t}\n\trcu_read_unlock();\n}\n\n/*\n * Return '1' if this is a member of batch, or '0' if it is a lone stripe or\n * a head which can now be handled.\n */\nstatic int clear_batch_ready(struct stripe_head *sh)\n{\n\tstruct stripe_head *tmp;\n\tif (!test_and_clear_bit(STRIPE_BATCH_READY, &sh->state))\n\t\treturn (sh->batch_head && sh->batch_head != sh);\n\tspin_lock(&sh->stripe_lock);\n\tif (!sh->batch_head) {\n\t\tspin_unlock(&sh->stripe_lock);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * this stripe could be added to a batch list before we check\n\t * BATCH_READY, skips it\n\t */\n\tif (sh->batch_head != sh) {\n\t\tspin_unlock(&sh->stripe_lock);\n\t\treturn 1;\n\t}\n\tspin_lock(&sh->batch_lock);\n\tlist_for_each_entry(tmp, &sh->batch_list, batch_list)\n\t\tclear_bit(STRIPE_BATCH_READY, &tmp->state);\n\tspin_unlock(&sh->batch_lock);\n\tspin_unlock(&sh->stripe_lock);\n\n\t/*\n\t * BATCH_READY is cleared, no new stripes can be added.\n\t * batch_list can be accessed without lock\n\t */\n\treturn 0;\n}\n\nstatic void break_stripe_batch_list(struct stripe_head *head_sh,\n\t\t\t\t    unsigned long handle_flags)\n{\n\tstruct stripe_head *sh, *next;\n\tint i;\n\tint do_wakeup = 0;\n\n\tlist_for_each_entry_safe(sh, next, &head_sh->batch_list, batch_list) {\n\n\t\tlist_del_init(&sh->batch_list);\n\n\t\tWARN_ONCE(sh->state & ((1 << STRIPE_ACTIVE) |\n\t\t\t\t\t  (1 << STRIPE_SYNCING) |\n\t\t\t\t\t  (1 << STRIPE_REPLACED) |\n\t\t\t\t\t  (1 << STRIPE_DELAYED) |\n\t\t\t\t\t  (1 << STRIPE_BIT_DELAY) |\n\t\t\t\t\t  (1 << STRIPE_FULL_WRITE) |\n\t\t\t\t\t  (1 << STRIPE_BIOFILL_RUN) |\n\t\t\t\t\t  (1 << STRIPE_COMPUTE_RUN)  |\n\t\t\t\t\t  (1 << STRIPE_DISCARD) |\n\t\t\t\t\t  (1 << STRIPE_BATCH_READY) |\n\t\t\t\t\t  (1 << STRIPE_BATCH_ERR) |\n\t\t\t\t\t  (1 << STRIPE_BITMAP_PENDING)),\n\t\t\t\"stripe state: %lx\\n\", sh->state);\n\t\tWARN_ONCE(head_sh->state & ((1 << STRIPE_DISCARD) |\n\t\t\t\t\t      (1 << STRIPE_REPLACED)),\n\t\t\t\"head stripe state: %lx\\n\", head_sh->state);\n\n\t\tset_mask_bits(&sh->state, ~(STRIPE_EXPAND_SYNC_FLAGS |\n\t\t\t\t\t    (1 << STRIPE_PREREAD_ACTIVE) |\n\t\t\t\t\t    (1 << STRIPE_DEGRADED) |\n\t\t\t\t\t    (1 << STRIPE_ON_UNPLUG_LIST)),\n\t\t\t      head_sh->state & (1 << STRIPE_INSYNC));\n\n\t\tsh->check_state = head_sh->check_state;\n\t\tsh->reconstruct_state = head_sh->reconstruct_state;\n\t\tspin_lock_irq(&sh->stripe_lock);\n\t\tsh->batch_head = NULL;\n\t\tspin_unlock_irq(&sh->stripe_lock);\n\t\tfor (i = 0; i < sh->disks; i++) {\n\t\t\tif (test_and_clear_bit(R5_Overlap, &sh->dev[i].flags))\n\t\t\t\tdo_wakeup = 1;\n\t\t\tsh->dev[i].flags = head_sh->dev[i].flags &\n\t\t\t\t(~((1 << R5_WriteError) | (1 << R5_Overlap)));\n\t\t}\n\t\tif (handle_flags == 0 ||\n\t\t    sh->state & handle_flags)\n\t\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\traid5_release_stripe(sh);\n\t}\n\tspin_lock_irq(&head_sh->stripe_lock);\n\thead_sh->batch_head = NULL;\n\tspin_unlock_irq(&head_sh->stripe_lock);\n\tfor (i = 0; i < head_sh->disks; i++)\n\t\tif (test_and_clear_bit(R5_Overlap, &head_sh->dev[i].flags))\n\t\t\tdo_wakeup = 1;\n\tif (head_sh->state & handle_flags)\n\t\tset_bit(STRIPE_HANDLE, &head_sh->state);\n\n\tif (do_wakeup)\n\t\twake_up(&head_sh->raid_conf->wait_for_overlap);\n}\n\nstatic void handle_stripe(struct stripe_head *sh)\n{\n\tstruct stripe_head_state s;\n\tstruct r5conf *conf = sh->raid_conf;\n\tint i;\n\tint prexor;\n\tint disks = sh->disks;\n\tstruct r5dev *pdev, *qdev;\n\n\tclear_bit(STRIPE_HANDLE, &sh->state);\n\n\t/*\n\t * handle_stripe should not continue handle the batched stripe, only\n\t * the head of batch list or lone stripe can continue. Otherwise we\n\t * could see break_stripe_batch_list warns about the STRIPE_ACTIVE\n\t * is set for the batched stripe.\n\t */\n\tif (clear_batch_ready(sh))\n\t\treturn;\n\n\tif (test_and_set_bit_lock(STRIPE_ACTIVE, &sh->state)) {\n\t\t/* already being handled, ensure it gets handled\n\t\t * again when current action finishes */\n\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\treturn;\n\t}\n\n\tif (test_and_clear_bit(STRIPE_BATCH_ERR, &sh->state))\n\t\tbreak_stripe_batch_list(sh, 0);\n\n\tif (test_bit(STRIPE_SYNC_REQUESTED, &sh->state) && !sh->batch_head) {\n\t\tspin_lock(&sh->stripe_lock);\n\t\t/*\n\t\t * Cannot process 'sync' concurrently with 'discard'.\n\t\t * Flush data in r5cache before 'sync'.\n\t\t */\n\t\tif (!test_bit(STRIPE_R5C_PARTIAL_STRIPE, &sh->state) &&\n\t\t    !test_bit(STRIPE_R5C_FULL_STRIPE, &sh->state) &&\n\t\t    !test_bit(STRIPE_DISCARD, &sh->state) &&\n\t\t    test_and_clear_bit(STRIPE_SYNC_REQUESTED, &sh->state)) {\n\t\t\tset_bit(STRIPE_SYNCING, &sh->state);\n\t\t\tclear_bit(STRIPE_INSYNC, &sh->state);\n\t\t\tclear_bit(STRIPE_REPLACED, &sh->state);\n\t\t}\n\t\tspin_unlock(&sh->stripe_lock);\n\t}\n\tclear_bit(STRIPE_DELAYED, &sh->state);\n\n\tpr_debug(\"handling stripe %llu, state=%#lx cnt=%d, \"\n\t\t\"pd_idx=%d, qd_idx=%d\\n, check:%d, reconstruct:%d\\n\",\n\t       (unsigned long long)sh->sector, sh->state,\n\t       atomic_read(&sh->count), sh->pd_idx, sh->qd_idx,\n\t       sh->check_state, sh->reconstruct_state);\n\n\tanalyse_stripe(sh, &s);\n\n\tif (test_bit(STRIPE_LOG_TRAPPED, &sh->state))\n\t\tgoto finish;\n\n\tif (s.handle_bad_blocks ||\n\t    test_bit(MD_SB_CHANGE_PENDING, &conf->mddev->sb_flags)) {\n\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\tgoto finish;\n\t}\n\n\tif (unlikely(s.blocked_rdev)) {\n\t\tif (s.syncing || s.expanding || s.expanded ||\n\t\t    s.replacing || s.to_write || s.written) {\n\t\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\t\tgoto finish;\n\t\t}\n\t\t/* There is nothing for the blocked_rdev to block */\n\t\trdev_dec_pending(s.blocked_rdev, conf->mddev);\n\t\ts.blocked_rdev = NULL;\n\t}\n\n\tif (s.to_fill && !test_bit(STRIPE_BIOFILL_RUN, &sh->state)) {\n\t\tset_bit(STRIPE_OP_BIOFILL, &s.ops_request);\n\t\tset_bit(STRIPE_BIOFILL_RUN, &sh->state);\n\t}\n\n\tpr_debug(\"locked=%d uptodate=%d to_read=%d\"\n\t       \" to_write=%d failed=%d failed_num=%d,%d\\n\",\n\t       s.locked, s.uptodate, s.to_read, s.to_write, s.failed,\n\t       s.failed_num[0], s.failed_num[1]);\n\t/*\n\t * check if the array has lost more than max_degraded devices and,\n\t * if so, some requests might need to be failed.\n\t *\n\t * When journal device failed (log_failed), we will only process\n\t * the stripe if there is data need write to raid disks\n\t */\n\tif (s.failed > conf->max_degraded ||\n\t    (s.log_failed && s.injournal == 0)) {\n\t\tsh->check_state = 0;\n\t\tsh->reconstruct_state = 0;\n\t\tbreak_stripe_batch_list(sh, 0);\n\t\tif (s.to_read+s.to_write+s.written)\n\t\t\thandle_failed_stripe(conf, sh, &s, disks);\n\t\tif (s.syncing + s.replacing)\n\t\t\thandle_failed_sync(conf, sh, &s);\n\t}\n\n\t/* Now we check to see if any write operations have recently\n\t * completed\n\t */\n\tprexor = 0;\n\tif (sh->reconstruct_state == reconstruct_state_prexor_drain_result)\n\t\tprexor = 1;\n\tif (sh->reconstruct_state == reconstruct_state_drain_result ||\n\t    sh->reconstruct_state == reconstruct_state_prexor_drain_result) {\n\t\tsh->reconstruct_state = reconstruct_state_idle;\n\n\t\t/* All the 'written' buffers and the parity block are ready to\n\t\t * be written back to disk\n\t\t */\n\t\tBUG_ON(!test_bit(R5_UPTODATE, &sh->dev[sh->pd_idx].flags) &&\n\t\t       !test_bit(R5_Discard, &sh->dev[sh->pd_idx].flags));\n\t\tBUG_ON(sh->qd_idx >= 0 &&\n\t\t       !test_bit(R5_UPTODATE, &sh->dev[sh->qd_idx].flags) &&\n\t\t       !test_bit(R5_Discard, &sh->dev[sh->qd_idx].flags));\n\t\tfor (i = disks; i--; ) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t\tif (test_bit(R5_LOCKED, &dev->flags) &&\n\t\t\t\t(i == sh->pd_idx || i == sh->qd_idx ||\n\t\t\t\t dev->written || test_bit(R5_InJournal,\n\t\t\t\t\t\t\t  &dev->flags))) {\n\t\t\t\tpr_debug(\"Writing block %d\\n\", i);\n\t\t\t\tset_bit(R5_Wantwrite, &dev->flags);\n\t\t\t\tif (prexor)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (s.failed > 1)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (!test_bit(R5_Insync, &dev->flags) ||\n\t\t\t\t    ((i == sh->pd_idx || i == sh->qd_idx)  &&\n\t\t\t\t     s.failed == 0))\n\t\t\t\t\tset_bit(STRIPE_INSYNC, &sh->state);\n\t\t\t}\n\t\t}\n\t\tif (test_and_clear_bit(STRIPE_PREREAD_ACTIVE, &sh->state))\n\t\t\ts.dec_preread_active = 1;\n\t}\n\n\t/*\n\t * might be able to return some write requests if the parity blocks\n\t * are safe, or on a failed drive\n\t */\n\tpdev = &sh->dev[sh->pd_idx];\n\ts.p_failed = (s.failed >= 1 && s.failed_num[0] == sh->pd_idx)\n\t\t|| (s.failed >= 2 && s.failed_num[1] == sh->pd_idx);\n\tqdev = &sh->dev[sh->qd_idx];\n\ts.q_failed = (s.failed >= 1 && s.failed_num[0] == sh->qd_idx)\n\t\t|| (s.failed >= 2 && s.failed_num[1] == sh->qd_idx)\n\t\t|| conf->level < 6;\n\n\tif (s.written &&\n\t    (s.p_failed || ((test_bit(R5_Insync, &pdev->flags)\n\t\t\t     && !test_bit(R5_LOCKED, &pdev->flags)\n\t\t\t     && (test_bit(R5_UPTODATE, &pdev->flags) ||\n\t\t\t\t test_bit(R5_Discard, &pdev->flags))))) &&\n\t    (s.q_failed || ((test_bit(R5_Insync, &qdev->flags)\n\t\t\t     && !test_bit(R5_LOCKED, &qdev->flags)\n\t\t\t     && (test_bit(R5_UPTODATE, &qdev->flags) ||\n\t\t\t\t test_bit(R5_Discard, &qdev->flags))))))\n\t\thandle_stripe_clean_event(conf, sh, disks);\n\n\tif (s.just_cached)\n\t\tr5c_handle_cached_data_endio(conf, sh, disks);\n\tlog_stripe_write_finished(sh);\n\n\t/* Now we might consider reading some blocks, either to check/generate\n\t * parity, or to satisfy requests\n\t * or to load a block that is being partially written.\n\t */\n\tif (s.to_read || s.non_overwrite\n\t    || (s.to_write && s.failed)\n\t    || (s.syncing && (s.uptodate + s.compute < disks))\n\t    || s.replacing\n\t    || s.expanding)\n\t\thandle_stripe_fill(sh, &s, disks);\n\n\t/*\n\t * When the stripe finishes full journal write cycle (write to journal\n\t * and raid disk), this is the clean up procedure so it is ready for\n\t * next operation.\n\t */\n\tr5c_finish_stripe_write_out(conf, sh, &s);\n\n\t/*\n\t * Now to consider new write requests, cache write back and what else,\n\t * if anything should be read.  We do not handle new writes when:\n\t * 1/ A 'write' operation (copy+xor) is already in flight.\n\t * 2/ A 'check' operation is in flight, as it may clobber the parity\n\t *    block.\n\t * 3/ A r5c cache log write is in flight.\n\t */\n\n\tif (!sh->reconstruct_state && !sh->check_state && !sh->log_io) {\n\t\tif (!r5c_is_writeback(conf->log)) {\n\t\t\tif (s.to_write)\n\t\t\t\thandle_stripe_dirtying(conf, sh, &s, disks);\n\t\t} else { /* write back cache */\n\t\t\tint ret = 0;\n\n\t\t\t/* First, try handle writes in caching phase */\n\t\t\tif (s.to_write)\n\t\t\t\tret = r5c_try_caching_write(conf, sh, &s,\n\t\t\t\t\t\t\t    disks);\n\t\t\t/*\n\t\t\t * If caching phase failed: ret == -EAGAIN\n\t\t\t *    OR\n\t\t\t * stripe under reclaim: !caching && injournal\n\t\t\t *\n\t\t\t * fall back to handle_stripe_dirtying()\n\t\t\t */\n\t\t\tif (ret == -EAGAIN ||\n\t\t\t    /* stripe under reclaim: !caching && injournal */\n\t\t\t    (!test_bit(STRIPE_R5C_CACHING, &sh->state) &&\n\t\t\t     s.injournal > 0)) {\n\t\t\t\tret = handle_stripe_dirtying(conf, sh, &s,\n\t\t\t\t\t\t\t     disks);\n\t\t\t\tif (ret == -EAGAIN)\n\t\t\t\t\tgoto finish;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* maybe we need to check and possibly fix the parity for this stripe\n\t * Any reads will already have been scheduled, so we just see if enough\n\t * data is available.  The parity check is held off while parity\n\t * dependent operations are in flight.\n\t */\n\tif (sh->check_state ||\n\t    (s.syncing && s.locked == 0 &&\n\t     !test_bit(STRIPE_COMPUTE_RUN, &sh->state) &&\n\t     !test_bit(STRIPE_INSYNC, &sh->state))) {\n\t\tif (conf->level == 6)\n\t\t\thandle_parity_checks6(conf, sh, &s, disks);\n\t\telse\n\t\t\thandle_parity_checks5(conf, sh, &s, disks);\n\t}\n\n\tif ((s.replacing || s.syncing) && s.locked == 0\n\t    && !test_bit(STRIPE_COMPUTE_RUN, &sh->state)\n\t    && !test_bit(STRIPE_REPLACED, &sh->state)) {\n\t\t/* Write out to replacement devices where possible */\n\t\tfor (i = 0; i < conf->raid_disks; i++)\n\t\t\tif (test_bit(R5_NeedReplace, &sh->dev[i].flags)) {\n\t\t\t\tWARN_ON(!test_bit(R5_UPTODATE, &sh->dev[i].flags));\n\t\t\t\tset_bit(R5_WantReplace, &sh->dev[i].flags);\n\t\t\t\tset_bit(R5_LOCKED, &sh->dev[i].flags);\n\t\t\t\ts.locked++;\n\t\t\t}\n\t\tif (s.replacing)\n\t\t\tset_bit(STRIPE_INSYNC, &sh->state);\n\t\tset_bit(STRIPE_REPLACED, &sh->state);\n\t}\n\tif ((s.syncing || s.replacing) && s.locked == 0 &&\n\t    !test_bit(STRIPE_COMPUTE_RUN, &sh->state) &&\n\t    test_bit(STRIPE_INSYNC, &sh->state)) {\n\t\tmd_done_sync(conf->mddev, RAID5_STRIPE_SECTORS(conf), 1);\n\t\tclear_bit(STRIPE_SYNCING, &sh->state);\n\t\tif (test_and_clear_bit(R5_Overlap, &sh->dev[sh->pd_idx].flags))\n\t\t\twake_up(&conf->wait_for_overlap);\n\t}\n\n\t/* If the failed drives are just a ReadError, then we might need\n\t * to progress the repair/check process\n\t */\n\tif (s.failed <= conf->max_degraded && !conf->mddev->ro)\n\t\tfor (i = 0; i < s.failed; i++) {\n\t\t\tstruct r5dev *dev = &sh->dev[s.failed_num[i]];\n\t\t\tif (test_bit(R5_ReadError, &dev->flags)\n\t\t\t    && !test_bit(R5_LOCKED, &dev->flags)\n\t\t\t    && test_bit(R5_UPTODATE, &dev->flags)\n\t\t\t\t) {\n\t\t\t\tif (!test_bit(R5_ReWrite, &dev->flags)) {\n\t\t\t\t\tset_bit(R5_Wantwrite, &dev->flags);\n\t\t\t\t\tset_bit(R5_ReWrite, &dev->flags);\n\t\t\t\t} else\n\t\t\t\t\t/* let's read it back */\n\t\t\t\t\tset_bit(R5_Wantread, &dev->flags);\n\t\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\t\ts.locked++;\n\t\t\t}\n\t\t}\n\n\t/* Finish reconstruct operations initiated by the expansion process */\n\tif (sh->reconstruct_state == reconstruct_state_result) {\n\t\tstruct stripe_head *sh_src\n\t\t\t= raid5_get_active_stripe(conf, sh->sector, 1, 1, 1);\n\t\tif (sh_src && test_bit(STRIPE_EXPAND_SOURCE, &sh_src->state)) {\n\t\t\t/* sh cannot be written until sh_src has been read.\n\t\t\t * so arrange for sh to be delayed a little\n\t\t\t */\n\t\t\tset_bit(STRIPE_DELAYED, &sh->state);\n\t\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\t\tif (!test_and_set_bit(STRIPE_PREREAD_ACTIVE,\n\t\t\t\t\t      &sh_src->state))\n\t\t\t\tatomic_inc(&conf->preread_active_stripes);\n\t\t\traid5_release_stripe(sh_src);\n\t\t\tgoto finish;\n\t\t}\n\t\tif (sh_src)\n\t\t\traid5_release_stripe(sh_src);\n\n\t\tsh->reconstruct_state = reconstruct_state_idle;\n\t\tclear_bit(STRIPE_EXPANDING, &sh->state);\n\t\tfor (i = conf->raid_disks; i--; ) {\n\t\t\tset_bit(R5_Wantwrite, &sh->dev[i].flags);\n\t\t\tset_bit(R5_LOCKED, &sh->dev[i].flags);\n\t\t\ts.locked++;\n\t\t}\n\t}\n\n\tif (s.expanded && test_bit(STRIPE_EXPANDING, &sh->state) &&\n\t    !sh->reconstruct_state) {\n\t\t/* Need to write out all blocks after computing parity */\n\t\tsh->disks = conf->raid_disks;\n\t\tstripe_set_idx(sh->sector, conf, 0, sh);\n\t\tschedule_reconstruction(sh, &s, 1, 1);\n\t} else if (s.expanded && !sh->reconstruct_state && s.locked == 0) {\n\t\tclear_bit(STRIPE_EXPAND_READY, &sh->state);\n\t\tatomic_dec(&conf->reshape_stripes);\n\t\twake_up(&conf->wait_for_overlap);\n\t\tmd_done_sync(conf->mddev, RAID5_STRIPE_SECTORS(conf), 1);\n\t}\n\n\tif (s.expanding && s.locked == 0 &&\n\t    !test_bit(STRIPE_COMPUTE_RUN, &sh->state))\n\t\thandle_stripe_expansion(conf, sh);\n\nfinish:\n\t/* wait for this device to become unblocked */\n\tif (unlikely(s.blocked_rdev)) {\n\t\tif (conf->mddev->external)\n\t\t\tmd_wait_for_blocked_rdev(s.blocked_rdev,\n\t\t\t\t\t\t conf->mddev);\n\t\telse\n\t\t\t/* Internal metadata will immediately\n\t\t\t * be written by raid5d, so we don't\n\t\t\t * need to wait here.\n\t\t\t */\n\t\t\trdev_dec_pending(s.blocked_rdev,\n\t\t\t\t\t conf->mddev);\n\t}\n\n\tif (s.handle_bad_blocks)\n\t\tfor (i = disks; i--; ) {\n\t\t\tstruct md_rdev *rdev;\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t\tif (test_and_clear_bit(R5_WriteError, &dev->flags)) {\n\t\t\t\t/* We own a safe reference to the rdev */\n\t\t\t\trdev = conf->disks[i].rdev;\n\t\t\t\tif (!rdev_set_badblocks(rdev, sh->sector,\n\t\t\t\t\t\t\tRAID5_STRIPE_SECTORS(conf), 0))\n\t\t\t\t\tmd_error(conf->mddev, rdev);\n\t\t\t\trdev_dec_pending(rdev, conf->mddev);\n\t\t\t}\n\t\t\tif (test_and_clear_bit(R5_MadeGood, &dev->flags)) {\n\t\t\t\trdev = conf->disks[i].rdev;\n\t\t\t\trdev_clear_badblocks(rdev, sh->sector,\n\t\t\t\t\t\t     RAID5_STRIPE_SECTORS(conf), 0);\n\t\t\t\trdev_dec_pending(rdev, conf->mddev);\n\t\t\t}\n\t\t\tif (test_and_clear_bit(R5_MadeGoodRepl, &dev->flags)) {\n\t\t\t\trdev = conf->disks[i].replacement;\n\t\t\t\tif (!rdev)\n\t\t\t\t\t/* rdev have been moved down */\n\t\t\t\t\trdev = conf->disks[i].rdev;\n\t\t\t\trdev_clear_badblocks(rdev, sh->sector,\n\t\t\t\t\t\t     RAID5_STRIPE_SECTORS(conf), 0);\n\t\t\t\trdev_dec_pending(rdev, conf->mddev);\n\t\t\t}\n\t\t}\n\n\tif (s.ops_request)\n\t\traid_run_ops(sh, s.ops_request);\n\n\tops_run_io(sh, &s);\n\n\tif (s.dec_preread_active) {\n\t\t/* We delay this until after ops_run_io so that if make_request\n\t\t * is waiting on a flush, it won't continue until the writes\n\t\t * have actually been submitted.\n\t\t */\n\t\tatomic_dec(&conf->preread_active_stripes);\n\t\tif (atomic_read(&conf->preread_active_stripes) <\n\t\t    IO_THRESHOLD)\n\t\t\tmd_wakeup_thread(conf->mddev->thread);\n\t}\n\n\tclear_bit_unlock(STRIPE_ACTIVE, &sh->state);\n}\n\nstatic void raid5_activate_delayed(struct r5conf *conf)\n{\n\tif (atomic_read(&conf->preread_active_stripes) < IO_THRESHOLD) {\n\t\twhile (!list_empty(&conf->delayed_list)) {\n\t\t\tstruct list_head *l = conf->delayed_list.next;\n\t\t\tstruct stripe_head *sh;\n\t\t\tsh = list_entry(l, struct stripe_head, lru);\n\t\t\tlist_del_init(l);\n\t\t\tclear_bit(STRIPE_DELAYED, &sh->state);\n\t\t\tif (!test_and_set_bit(STRIPE_PREREAD_ACTIVE, &sh->state))\n\t\t\t\tatomic_inc(&conf->preread_active_stripes);\n\t\t\tlist_add_tail(&sh->lru, &conf->hold_list);\n\t\t\traid5_wakeup_stripe_thread(sh);\n\t\t}\n\t}\n}\n\nstatic void activate_bit_delay(struct r5conf *conf,\n\tstruct list_head *temp_inactive_list)\n{\n\t/* device_lock is held */\n\tstruct list_head head;\n\tlist_add(&head, &conf->bitmap_list);\n\tlist_del_init(&conf->bitmap_list);\n\twhile (!list_empty(&head)) {\n\t\tstruct stripe_head *sh = list_entry(head.next, struct stripe_head, lru);\n\t\tint hash;\n\t\tlist_del_init(&sh->lru);\n\t\tatomic_inc(&sh->count);\n\t\thash = sh->hash_lock_index;\n\t\t__release_stripe(conf, sh, &temp_inactive_list[hash]);\n\t}\n}\n\nstatic int in_chunk_boundary(struct mddev *mddev, struct bio *bio)\n{\n\tstruct r5conf *conf = mddev->private;\n\tsector_t sector = bio->bi_iter.bi_sector;\n\tunsigned int chunk_sectors;\n\tunsigned int bio_sectors = bio_sectors(bio);\n\n\tWARN_ON_ONCE(bio->bi_bdev->bd_partno);\n\n\tchunk_sectors = min(conf->chunk_sectors, conf->prev_chunk_sectors);\n\treturn  chunk_sectors >=\n\t\t((sector & (chunk_sectors - 1)) + bio_sectors);\n}\n\n/*\n *  add bio to the retry LIFO  ( in O(1) ... we are in interrupt )\n *  later sampled by raid5d.\n */\nstatic void add_bio_to_retry(struct bio *bi,struct r5conf *conf)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&conf->device_lock, flags);\n\n\tbi->bi_next = conf->retry_read_aligned_list;\n\tconf->retry_read_aligned_list = bi;\n\n\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\tmd_wakeup_thread(conf->mddev->thread);\n}\n\nstatic struct bio *remove_bio_from_retry(struct r5conf *conf,\n\t\t\t\t\t unsigned int *offset)\n{\n\tstruct bio *bi;\n\n\tbi = conf->retry_read_aligned;\n\tif (bi) {\n\t\t*offset = conf->retry_read_offset;\n\t\tconf->retry_read_aligned = NULL;\n\t\treturn bi;\n\t}\n\tbi = conf->retry_read_aligned_list;\n\tif(bi) {\n\t\tconf->retry_read_aligned_list = bi->bi_next;\n\t\tbi->bi_next = NULL;\n\t\t*offset = 0;\n\t}\n\n\treturn bi;\n}\n\n/*\n *  The \"raid5_align_endio\" should check if the read succeeded and if it\n *  did, call bio_endio on the original bio (having bio_put the new bio\n *  first).\n *  If the read failed..\n */\nstatic void raid5_align_endio(struct bio *bi)\n{\n\tstruct bio* raid_bi  = bi->bi_private;\n\tstruct mddev *mddev;\n\tstruct r5conf *conf;\n\tstruct md_rdev *rdev;\n\tblk_status_t error = bi->bi_status;\n\n\tbio_put(bi);\n\n\trdev = (void*)raid_bi->bi_next;\n\traid_bi->bi_next = NULL;\n\tmddev = rdev->mddev;\n\tconf = mddev->private;\n\n\trdev_dec_pending(rdev, conf->mddev);\n\n\tif (!error) {\n\t\tbio_endio(raid_bi);\n\t\tif (atomic_dec_and_test(&conf->active_aligned_reads))\n\t\t\twake_up(&conf->wait_for_quiescent);\n\t\treturn;\n\t}\n\n\tpr_debug(\"raid5_align_endio : io error...handing IO for a retry\\n\");\n\n\tadd_bio_to_retry(raid_bi, conf);\n}\n\nstatic int raid5_read_one_chunk(struct mddev *mddev, struct bio *raid_bio)\n{\n\tstruct r5conf *conf = mddev->private;\n\tstruct bio *align_bio;\n\tstruct md_rdev *rdev;\n\tsector_t sector, end_sector, first_bad;\n\tint bad_sectors, dd_idx;\n\n\tif (!in_chunk_boundary(mddev, raid_bio)) {\n\t\tpr_debug(\"%s: non aligned\\n\", __func__);\n\t\treturn 0;\n\t}\n\n\tsector = raid5_compute_sector(conf, raid_bio->bi_iter.bi_sector, 0,\n\t\t\t\t      &dd_idx, NULL);\n\tend_sector = bio_end_sector(raid_bio);\n\n\trcu_read_lock();\n\tif (r5c_big_stripe_cached(conf, sector))\n\t\tgoto out_rcu_unlock;\n\n\trdev = rcu_dereference(conf->disks[dd_idx].replacement);\n\tif (!rdev || test_bit(Faulty, &rdev->flags) ||\n\t    rdev->recovery_offset < end_sector) {\n\t\trdev = rcu_dereference(conf->disks[dd_idx].rdev);\n\t\tif (!rdev)\n\t\t\tgoto out_rcu_unlock;\n\t\tif (test_bit(Faulty, &rdev->flags) ||\n\t\t    !(test_bit(In_sync, &rdev->flags) ||\n\t\t      rdev->recovery_offset >= end_sector))\n\t\t\tgoto out_rcu_unlock;\n\t}\n\n\tatomic_inc(&rdev->nr_pending);\n\trcu_read_unlock();\n\n\talign_bio = bio_clone_fast(raid_bio, GFP_NOIO, &mddev->bio_set);\n\tbio_set_dev(align_bio, rdev->bdev);\n\talign_bio->bi_end_io = raid5_align_endio;\n\talign_bio->bi_private = raid_bio;\n\talign_bio->bi_iter.bi_sector = sector;\n\n\traid_bio->bi_next = (void *)rdev;\n\n\tif (is_badblock(rdev, sector, bio_sectors(align_bio), &first_bad,\n\t\t\t&bad_sectors)) {\n\t\tbio_put(align_bio);\n\t\trdev_dec_pending(rdev, mddev);\n\t\treturn 0;\n\t}\n\n\t/* No reshape active, so we can trust rdev->data_offset */\n\talign_bio->bi_iter.bi_sector += rdev->data_offset;\n\n\tspin_lock_irq(&conf->device_lock);\n\twait_event_lock_irq(conf->wait_for_quiescent, conf->quiesce == 0,\n\t\t\t    conf->device_lock);\n\tatomic_inc(&conf->active_aligned_reads);\n\tspin_unlock_irq(&conf->device_lock);\n\n\tif (mddev->gendisk)\n\t\ttrace_block_bio_remap(align_bio, disk_devt(mddev->gendisk),\n\t\t\t\t      raid_bio->bi_iter.bi_sector);\n\tsubmit_bio_noacct(align_bio);\n\treturn 1;\n\nout_rcu_unlock:\n\trcu_read_unlock();\n\treturn 0;\n}\n\nstatic struct bio *chunk_aligned_read(struct mddev *mddev, struct bio *raid_bio)\n{\n\tstruct bio *split;\n\tsector_t sector = raid_bio->bi_iter.bi_sector;\n\tunsigned chunk_sects = mddev->chunk_sectors;\n\tunsigned sectors = chunk_sects - (sector & (chunk_sects-1));\n\n\tif (sectors < bio_sectors(raid_bio)) {\n\t\tstruct r5conf *conf = mddev->private;\n\t\tsplit = bio_split(raid_bio, sectors, GFP_NOIO, &conf->bio_split);\n\t\tbio_chain(split, raid_bio);\n\t\tsubmit_bio_noacct(raid_bio);\n\t\traid_bio = split;\n\t}\n\n\tif (!raid5_read_one_chunk(mddev, raid_bio))\n\t\treturn raid_bio;\n\n\treturn NULL;\n}\n\n/* __get_priority_stripe - get the next stripe to process\n *\n * Full stripe writes are allowed to pass preread active stripes up until\n * the bypass_threshold is exceeded.  In general the bypass_count\n * increments when the handle_list is handled before the hold_list; however, it\n * will not be incremented when STRIPE_IO_STARTED is sampled set signifying a\n * stripe with in flight i/o.  The bypass_count will be reset when the\n * head of the hold_list has changed, i.e. the head was promoted to the\n * handle_list.\n */\nstatic struct stripe_head *__get_priority_stripe(struct r5conf *conf, int group)\n{\n\tstruct stripe_head *sh, *tmp;\n\tstruct list_head *handle_list = NULL;\n\tstruct r5worker_group *wg;\n\tbool second_try = !r5c_is_writeback(conf->log) &&\n\t\t!r5l_log_disk_error(conf);\n\tbool try_loprio = test_bit(R5C_LOG_TIGHT, &conf->cache_state) ||\n\t\tr5l_log_disk_error(conf);\n\nagain:\n\twg = NULL;\n\tsh = NULL;\n\tif (conf->worker_cnt_per_group == 0) {\n\t\thandle_list = try_loprio ? &conf->loprio_list :\n\t\t\t\t\t&conf->handle_list;\n\t} else if (group != ANY_GROUP) {\n\t\thandle_list = try_loprio ? &conf->worker_groups[group].loprio_list :\n\t\t\t\t&conf->worker_groups[group].handle_list;\n\t\twg = &conf->worker_groups[group];\n\t} else {\n\t\tint i;\n\t\tfor (i = 0; i < conf->group_cnt; i++) {\n\t\t\thandle_list = try_loprio ? &conf->worker_groups[i].loprio_list :\n\t\t\t\t&conf->worker_groups[i].handle_list;\n\t\t\twg = &conf->worker_groups[i];\n\t\t\tif (!list_empty(handle_list))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tpr_debug(\"%s: handle: %s hold: %s full_writes: %d bypass_count: %d\\n\",\n\t\t  __func__,\n\t\t  list_empty(handle_list) ? \"empty\" : \"busy\",\n\t\t  list_empty(&conf->hold_list) ? \"empty\" : \"busy\",\n\t\t  atomic_read(&conf->pending_full_writes), conf->bypass_count);\n\n\tif (!list_empty(handle_list)) {\n\t\tsh = list_entry(handle_list->next, typeof(*sh), lru);\n\n\t\tif (list_empty(&conf->hold_list))\n\t\t\tconf->bypass_count = 0;\n\t\telse if (!test_bit(STRIPE_IO_STARTED, &sh->state)) {\n\t\t\tif (conf->hold_list.next == conf->last_hold)\n\t\t\t\tconf->bypass_count++;\n\t\t\telse {\n\t\t\t\tconf->last_hold = conf->hold_list.next;\n\t\t\t\tconf->bypass_count -= conf->bypass_threshold;\n\t\t\t\tif (conf->bypass_count < 0)\n\t\t\t\t\tconf->bypass_count = 0;\n\t\t\t}\n\t\t}\n\t} else if (!list_empty(&conf->hold_list) &&\n\t\t   ((conf->bypass_threshold &&\n\t\t     conf->bypass_count > conf->bypass_threshold) ||\n\t\t    atomic_read(&conf->pending_full_writes) == 0)) {\n\n\t\tlist_for_each_entry(tmp, &conf->hold_list,  lru) {\n\t\t\tif (conf->worker_cnt_per_group == 0 ||\n\t\t\t    group == ANY_GROUP ||\n\t\t\t    !cpu_online(tmp->cpu) ||\n\t\t\t    cpu_to_group(tmp->cpu) == group) {\n\t\t\t\tsh = tmp;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (sh) {\n\t\t\tconf->bypass_count -= conf->bypass_threshold;\n\t\t\tif (conf->bypass_count < 0)\n\t\t\t\tconf->bypass_count = 0;\n\t\t}\n\t\twg = NULL;\n\t}\n\n\tif (!sh) {\n\t\tif (second_try)\n\t\t\treturn NULL;\n\t\tsecond_try = true;\n\t\ttry_loprio = !try_loprio;\n\t\tgoto again;\n\t}\n\n\tif (wg) {\n\t\twg->stripes_cnt--;\n\t\tsh->group = NULL;\n\t}\n\tlist_del_init(&sh->lru);\n\tBUG_ON(atomic_inc_return(&sh->count) != 1);\n\treturn sh;\n}\n\nstruct raid5_plug_cb {\n\tstruct blk_plug_cb\tcb;\n\tstruct list_head\tlist;\n\tstruct list_head\ttemp_inactive_list[NR_STRIPE_HASH_LOCKS];\n};\n\nstatic void raid5_unplug(struct blk_plug_cb *blk_cb, bool from_schedule)\n{\n\tstruct raid5_plug_cb *cb = container_of(\n\t\tblk_cb, struct raid5_plug_cb, cb);\n\tstruct stripe_head *sh;\n\tstruct mddev *mddev = cb->cb.data;\n\tstruct r5conf *conf = mddev->private;\n\tint cnt = 0;\n\tint hash;\n\n\tif (cb->list.next && !list_empty(&cb->list)) {\n\t\tspin_lock_irq(&conf->device_lock);\n\t\twhile (!list_empty(&cb->list)) {\n\t\t\tsh = list_first_entry(&cb->list, struct stripe_head, lru);\n\t\t\tlist_del_init(&sh->lru);\n\t\t\t/*\n\t\t\t * avoid race release_stripe_plug() sees\n\t\t\t * STRIPE_ON_UNPLUG_LIST clear but the stripe\n\t\t\t * is still in our list\n\t\t\t */\n\t\t\tsmp_mb__before_atomic();\n\t\t\tclear_bit(STRIPE_ON_UNPLUG_LIST, &sh->state);\n\t\t\t/*\n\t\t\t * STRIPE_ON_RELEASE_LIST could be set here. In that\n\t\t\t * case, the count is always > 1 here\n\t\t\t */\n\t\t\thash = sh->hash_lock_index;\n\t\t\t__release_stripe(conf, sh, &cb->temp_inactive_list[hash]);\n\t\t\tcnt++;\n\t\t}\n\t\tspin_unlock_irq(&conf->device_lock);\n\t}\n\trelease_inactive_stripe_list(conf, cb->temp_inactive_list,\n\t\t\t\t     NR_STRIPE_HASH_LOCKS);\n\tif (mddev->queue)\n\t\ttrace_block_unplug(mddev->queue, cnt, !from_schedule);\n\tkfree(cb);\n}\n\nstatic void release_stripe_plug(struct mddev *mddev,\n\t\t\t\tstruct stripe_head *sh)\n{\n\tstruct blk_plug_cb *blk_cb = blk_check_plugged(\n\t\traid5_unplug, mddev,\n\t\tsizeof(struct raid5_plug_cb));\n\tstruct raid5_plug_cb *cb;\n\n\tif (!blk_cb) {\n\t\traid5_release_stripe(sh);\n\t\treturn;\n\t}\n\n\tcb = container_of(blk_cb, struct raid5_plug_cb, cb);\n\n\tif (cb->list.next == NULL) {\n\t\tint i;\n\t\tINIT_LIST_HEAD(&cb->list);\n\t\tfor (i = 0; i < NR_STRIPE_HASH_LOCKS; i++)\n\t\t\tINIT_LIST_HEAD(cb->temp_inactive_list + i);\n\t}\n\n\tif (!test_and_set_bit(STRIPE_ON_UNPLUG_LIST, &sh->state))\n\t\tlist_add_tail(&sh->lru, &cb->list);\n\telse\n\t\traid5_release_stripe(sh);\n}\n\nstatic void make_discard_request(struct mddev *mddev, struct bio *bi)\n{\n\tstruct r5conf *conf = mddev->private;\n\tsector_t logical_sector, last_sector;\n\tstruct stripe_head *sh;\n\tint stripe_sectors;\n\n\tif (mddev->reshape_position != MaxSector)\n\t\t/* Skip discard while reshape is happening */\n\t\treturn;\n\n\tlogical_sector = bi->bi_iter.bi_sector & ~((sector_t)RAID5_STRIPE_SECTORS(conf)-1);\n\tlast_sector = bio_end_sector(bi);\n\n\tbi->bi_next = NULL;\n\n\tstripe_sectors = conf->chunk_sectors *\n\t\t(conf->raid_disks - conf->max_degraded);\n\tlogical_sector = DIV_ROUND_UP_SECTOR_T(logical_sector,\n\t\t\t\t\t       stripe_sectors);\n\tsector_div(last_sector, stripe_sectors);\n\n\tlogical_sector *= conf->chunk_sectors;\n\tlast_sector *= conf->chunk_sectors;\n\n\tfor (; logical_sector < last_sector;\n\t     logical_sector += RAID5_STRIPE_SECTORS(conf)) {\n\t\tDEFINE_WAIT(w);\n\t\tint d;\n\tagain:\n\t\tsh = raid5_get_active_stripe(conf, logical_sector, 0, 0, 0);\n\t\tprepare_to_wait(&conf->wait_for_overlap, &w,\n\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tset_bit(R5_Overlap, &sh->dev[sh->pd_idx].flags);\n\t\tif (test_bit(STRIPE_SYNCING, &sh->state)) {\n\t\t\traid5_release_stripe(sh);\n\t\t\tschedule();\n\t\t\tgoto again;\n\t\t}\n\t\tclear_bit(R5_Overlap, &sh->dev[sh->pd_idx].flags);\n\t\tspin_lock_irq(&sh->stripe_lock);\n\t\tfor (d = 0; d < conf->raid_disks; d++) {\n\t\t\tif (d == sh->pd_idx || d == sh->qd_idx)\n\t\t\t\tcontinue;\n\t\t\tif (sh->dev[d].towrite || sh->dev[d].toread) {\n\t\t\t\tset_bit(R5_Overlap, &sh->dev[d].flags);\n\t\t\t\tspin_unlock_irq(&sh->stripe_lock);\n\t\t\t\traid5_release_stripe(sh);\n\t\t\t\tschedule();\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t}\n\t\tset_bit(STRIPE_DISCARD, &sh->state);\n\t\tfinish_wait(&conf->wait_for_overlap, &w);\n\t\tsh->overwrite_disks = 0;\n\t\tfor (d = 0; d < conf->raid_disks; d++) {\n\t\t\tif (d == sh->pd_idx || d == sh->qd_idx)\n\t\t\t\tcontinue;\n\t\t\tsh->dev[d].towrite = bi;\n\t\t\tset_bit(R5_OVERWRITE, &sh->dev[d].flags);\n\t\t\tbio_inc_remaining(bi);\n\t\t\tmd_write_inc(mddev, bi);\n\t\t\tsh->overwrite_disks++;\n\t\t}\n\t\tspin_unlock_irq(&sh->stripe_lock);\n\t\tif (conf->mddev->bitmap) {\n\t\t\tfor (d = 0;\n\t\t\t     d < conf->raid_disks - conf->max_degraded;\n\t\t\t     d++)\n\t\t\t\tmd_bitmap_startwrite(mddev->bitmap,\n\t\t\t\t\t\t     sh->sector,\n\t\t\t\t\t\t     RAID5_STRIPE_SECTORS(conf),\n\t\t\t\t\t\t     0);\n\t\t\tsh->bm_seq = conf->seq_flush + 1;\n\t\t\tset_bit(STRIPE_BIT_DELAY, &sh->state);\n\t\t}\n\n\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\tclear_bit(STRIPE_DELAYED, &sh->state);\n\t\tif (!test_and_set_bit(STRIPE_PREREAD_ACTIVE, &sh->state))\n\t\t\tatomic_inc(&conf->preread_active_stripes);\n\t\trelease_stripe_plug(mddev, sh);\n\t}\n\n\tbio_endio(bi);\n}\n\nstatic bool raid5_make_request(struct mddev *mddev, struct bio * bi)\n{\n\tstruct r5conf *conf = mddev->private;\n\tint dd_idx;\n\tsector_t new_sector;\n\tsector_t logical_sector, last_sector;\n\tstruct stripe_head *sh;\n\tconst int rw = bio_data_dir(bi);\n\tDEFINE_WAIT(w);\n\tbool do_prepare;\n\tbool do_flush = false;\n\n\tif (unlikely(bi->bi_opf & REQ_PREFLUSH)) {\n\t\tint ret = log_handle_flush_request(conf, bi);\n\n\t\tif (ret == 0)\n\t\t\treturn true;\n\t\tif (ret == -ENODEV) {\n\t\t\tif (md_flush_request(mddev, bi))\n\t\t\t\treturn true;\n\t\t}\n\t\t/* ret == -EAGAIN, fallback */\n\t\t/*\n\t\t * if r5l_handle_flush_request() didn't clear REQ_PREFLUSH,\n\t\t * we need to flush journal device\n\t\t */\n\t\tdo_flush = bi->bi_opf & REQ_PREFLUSH;\n\t}\n\n\tif (!md_write_start(mddev, bi))\n\t\treturn false;\n\t/*\n\t * If array is degraded, better not do chunk aligned read because\n\t * later we might have to read it again in order to reconstruct\n\t * data on failed drives.\n\t */\n\tif (rw == READ && mddev->degraded == 0 &&\n\t    mddev->reshape_position == MaxSector) {\n\t\tbi = chunk_aligned_read(mddev, bi);\n\t\tif (!bi)\n\t\t\treturn true;\n\t}\n\n\tif (unlikely(bio_op(bi) == REQ_OP_DISCARD)) {\n\t\tmake_discard_request(mddev, bi);\n\t\tmd_write_end(mddev);\n\t\treturn true;\n\t}\n\n\tlogical_sector = bi->bi_iter.bi_sector & ~((sector_t)RAID5_STRIPE_SECTORS(conf)-1);\n\tlast_sector = bio_end_sector(bi);\n\tbi->bi_next = NULL;\n\n\tprepare_to_wait(&conf->wait_for_overlap, &w, TASK_UNINTERRUPTIBLE);\n\tfor (; logical_sector < last_sector; logical_sector += RAID5_STRIPE_SECTORS(conf)) {\n\t\tint previous;\n\t\tint seq;\n\n\t\tdo_prepare = false;\n\tretry:\n\t\tseq = read_seqcount_begin(&conf->gen_lock);\n\t\tprevious = 0;\n\t\tif (do_prepare)\n\t\t\tprepare_to_wait(&conf->wait_for_overlap, &w,\n\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tif (unlikely(conf->reshape_progress != MaxSector)) {\n\t\t\t/* spinlock is needed as reshape_progress may be\n\t\t\t * 64bit on a 32bit platform, and so it might be\n\t\t\t * possible to see a half-updated value\n\t\t\t * Of course reshape_progress could change after\n\t\t\t * the lock is dropped, so once we get a reference\n\t\t\t * to the stripe that we think it is, we will have\n\t\t\t * to check again.\n\t\t\t */\n\t\t\tspin_lock_irq(&conf->device_lock);\n\t\t\tif (mddev->reshape_backwards\n\t\t\t    ? logical_sector < conf->reshape_progress\n\t\t\t    : logical_sector >= conf->reshape_progress) {\n\t\t\t\tprevious = 1;\n\t\t\t} else {\n\t\t\t\tif (mddev->reshape_backwards\n\t\t\t\t    ? logical_sector < conf->reshape_safe\n\t\t\t\t    : logical_sector >= conf->reshape_safe) {\n\t\t\t\t\tspin_unlock_irq(&conf->device_lock);\n\t\t\t\t\tschedule();\n\t\t\t\t\tdo_prepare = true;\n\t\t\t\t\tgoto retry;\n\t\t\t\t}\n\t\t\t}\n\t\t\tspin_unlock_irq(&conf->device_lock);\n\t\t}\n\n\t\tnew_sector = raid5_compute_sector(conf, logical_sector,\n\t\t\t\t\t\t  previous,\n\t\t\t\t\t\t  &dd_idx, NULL);\n\t\tpr_debug(\"raid456: raid5_make_request, sector %llu logical %llu\\n\",\n\t\t\t(unsigned long long)new_sector,\n\t\t\t(unsigned long long)logical_sector);\n\n\t\tsh = raid5_get_active_stripe(conf, new_sector, previous,\n\t\t\t\t       (bi->bi_opf & REQ_RAHEAD), 0);\n\t\tif (sh) {\n\t\t\tif (unlikely(previous)) {\n\t\t\t\t/* expansion might have moved on while waiting for a\n\t\t\t\t * stripe, so we must do the range check again.\n\t\t\t\t * Expansion could still move past after this\n\t\t\t\t * test, but as we are holding a reference to\n\t\t\t\t * 'sh', we know that if that happens,\n\t\t\t\t *  STRIPE_EXPANDING will get set and the expansion\n\t\t\t\t * won't proceed until we finish with the stripe.\n\t\t\t\t */\n\t\t\t\tint must_retry = 0;\n\t\t\t\tspin_lock_irq(&conf->device_lock);\n\t\t\t\tif (mddev->reshape_backwards\n\t\t\t\t    ? logical_sector >= conf->reshape_progress\n\t\t\t\t    : logical_sector < conf->reshape_progress)\n\t\t\t\t\t/* mismatch, need to try again */\n\t\t\t\t\tmust_retry = 1;\n\t\t\t\tspin_unlock_irq(&conf->device_lock);\n\t\t\t\tif (must_retry) {\n\t\t\t\t\traid5_release_stripe(sh);\n\t\t\t\t\tschedule();\n\t\t\t\t\tdo_prepare = true;\n\t\t\t\t\tgoto retry;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (read_seqcount_retry(&conf->gen_lock, seq)) {\n\t\t\t\t/* Might have got the wrong stripe_head\n\t\t\t\t * by accident\n\t\t\t\t */\n\t\t\t\traid5_release_stripe(sh);\n\t\t\t\tgoto retry;\n\t\t\t}\n\n\t\t\tif (test_bit(STRIPE_EXPANDING, &sh->state) ||\n\t\t\t    !add_stripe_bio(sh, bi, dd_idx, rw, previous)) {\n\t\t\t\t/* Stripe is busy expanding or\n\t\t\t\t * add failed due to overlap.  Flush everything\n\t\t\t\t * and wait a while\n\t\t\t\t */\n\t\t\t\tmd_wakeup_thread(mddev->thread);\n\t\t\t\traid5_release_stripe(sh);\n\t\t\t\tschedule();\n\t\t\t\tdo_prepare = true;\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t\tif (do_flush) {\n\t\t\t\tset_bit(STRIPE_R5C_PREFLUSH, &sh->state);\n\t\t\t\t/* we only need flush for one stripe */\n\t\t\t\tdo_flush = false;\n\t\t\t}\n\n\t\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\t\tclear_bit(STRIPE_DELAYED, &sh->state);\n\t\t\tif ((!sh->batch_head || sh == sh->batch_head) &&\n\t\t\t    (bi->bi_opf & REQ_SYNC) &&\n\t\t\t    !test_and_set_bit(STRIPE_PREREAD_ACTIVE, &sh->state))\n\t\t\t\tatomic_inc(&conf->preread_active_stripes);\n\t\t\trelease_stripe_plug(mddev, sh);\n\t\t} else {\n\t\t\t/* cannot get stripe for read-ahead, just give-up */\n\t\t\tbi->bi_status = BLK_STS_IOERR;\n\t\t\tbreak;\n\t\t}\n\t}\n\tfinish_wait(&conf->wait_for_overlap, &w);\n\n\tif (rw == WRITE)\n\t\tmd_write_end(mddev);\n\tbio_endio(bi);\n\treturn true;\n}\n\nstatic sector_t raid5_size(struct mddev *mddev, sector_t sectors, int raid_disks);\n\nstatic sector_t reshape_request(struct mddev *mddev, sector_t sector_nr, int *skipped)\n{\n\t/* reshaping is quite different to recovery/resync so it is\n\t * handled quite separately ... here.\n\t *\n\t * On each call to sync_request, we gather one chunk worth of\n\t * destination stripes and flag them as expanding.\n\t * Then we find all the source stripes and request reads.\n\t * As the reads complete, handle_stripe will copy the data\n\t * into the destination stripe and release that stripe.\n\t */\n\tstruct r5conf *conf = mddev->private;\n\tstruct stripe_head *sh;\n\tstruct md_rdev *rdev;\n\tsector_t first_sector, last_sector;\n\tint raid_disks = conf->previous_raid_disks;\n\tint data_disks = raid_disks - conf->max_degraded;\n\tint new_data_disks = conf->raid_disks - conf->max_degraded;\n\tint i;\n\tint dd_idx;\n\tsector_t writepos, readpos, safepos;\n\tsector_t stripe_addr;\n\tint reshape_sectors;\n\tstruct list_head stripes;\n\tsector_t retn;\n\n\tif (sector_nr == 0) {\n\t\t/* If restarting in the middle, skip the initial sectors */\n\t\tif (mddev->reshape_backwards &&\n\t\t    conf->reshape_progress < raid5_size(mddev, 0, 0)) {\n\t\t\tsector_nr = raid5_size(mddev, 0, 0)\n\t\t\t\t- conf->reshape_progress;\n\t\t} else if (mddev->reshape_backwards &&\n\t\t\t   conf->reshape_progress == MaxSector) {\n\t\t\t/* shouldn't happen, but just in case, finish up.*/\n\t\t\tsector_nr = MaxSector;\n\t\t} else if (!mddev->reshape_backwards &&\n\t\t\t   conf->reshape_progress > 0)\n\t\t\tsector_nr = conf->reshape_progress;\n\t\tsector_div(sector_nr, new_data_disks);\n\t\tif (sector_nr) {\n\t\t\tmddev->curr_resync_completed = sector_nr;\n\t\t\tsysfs_notify_dirent_safe(mddev->sysfs_completed);\n\t\t\t*skipped = 1;\n\t\t\tretn = sector_nr;\n\t\t\tgoto finish;\n\t\t}\n\t}\n\n\t/* We need to process a full chunk at a time.\n\t * If old and new chunk sizes differ, we need to process the\n\t * largest of these\n\t */\n\n\treshape_sectors = max(conf->chunk_sectors, conf->prev_chunk_sectors);\n\n\t/* We update the metadata at least every 10 seconds, or when\n\t * the data about to be copied would over-write the source of\n\t * the data at the front of the range.  i.e. one new_stripe\n\t * along from reshape_progress new_maps to after where\n\t * reshape_safe old_maps to\n\t */\n\twritepos = conf->reshape_progress;\n\tsector_div(writepos, new_data_disks);\n\treadpos = conf->reshape_progress;\n\tsector_div(readpos, data_disks);\n\tsafepos = conf->reshape_safe;\n\tsector_div(safepos, data_disks);\n\tif (mddev->reshape_backwards) {\n\t\tBUG_ON(writepos < reshape_sectors);\n\t\twritepos -= reshape_sectors;\n\t\treadpos += reshape_sectors;\n\t\tsafepos += reshape_sectors;\n\t} else {\n\t\twritepos += reshape_sectors;\n\t\t/* readpos and safepos are worst-case calculations.\n\t\t * A negative number is overly pessimistic, and causes\n\t\t * obvious problems for unsigned storage.  So clip to 0.\n\t\t */\n\t\treadpos -= min_t(sector_t, reshape_sectors, readpos);\n\t\tsafepos -= min_t(sector_t, reshape_sectors, safepos);\n\t}\n\n\t/* Having calculated the 'writepos' possibly use it\n\t * to set 'stripe_addr' which is where we will write to.\n\t */\n\tif (mddev->reshape_backwards) {\n\t\tBUG_ON(conf->reshape_progress == 0);\n\t\tstripe_addr = writepos;\n\t\tBUG_ON((mddev->dev_sectors &\n\t\t\t~((sector_t)reshape_sectors - 1))\n\t\t       - reshape_sectors - stripe_addr\n\t\t       != sector_nr);\n\t} else {\n\t\tBUG_ON(writepos != sector_nr + reshape_sectors);\n\t\tstripe_addr = sector_nr;\n\t}\n\n\t/* 'writepos' is the most advanced device address we might write.\n\t * 'readpos' is the least advanced device address we might read.\n\t * 'safepos' is the least address recorded in the metadata as having\n\t *     been reshaped.\n\t * If there is a min_offset_diff, these are adjusted either by\n\t * increasing the safepos/readpos if diff is negative, or\n\t * increasing writepos if diff is positive.\n\t * If 'readpos' is then behind 'writepos', there is no way that we can\n\t * ensure safety in the face of a crash - that must be done by userspace\n\t * making a backup of the data.  So in that case there is no particular\n\t * rush to update metadata.\n\t * Otherwise if 'safepos' is behind 'writepos', then we really need to\n\t * update the metadata to advance 'safepos' to match 'readpos' so that\n\t * we can be safe in the event of a crash.\n\t * So we insist on updating metadata if safepos is behind writepos and\n\t * readpos is beyond writepos.\n\t * In any case, update the metadata every 10 seconds.\n\t * Maybe that number should be configurable, but I'm not sure it is\n\t * worth it.... maybe it could be a multiple of safemode_delay???\n\t */\n\tif (conf->min_offset_diff < 0) {\n\t\tsafepos += -conf->min_offset_diff;\n\t\treadpos += -conf->min_offset_diff;\n\t} else\n\t\twritepos += conf->min_offset_diff;\n\n\tif ((mddev->reshape_backwards\n\t     ? (safepos > writepos && readpos < writepos)\n\t     : (safepos < writepos && readpos > writepos)) ||\n\t    time_after(jiffies, conf->reshape_checkpoint + 10*HZ)) {\n\t\t/* Cannot proceed until we've updated the superblock... */\n\t\twait_event(conf->wait_for_overlap,\n\t\t\t   atomic_read(&conf->reshape_stripes)==0\n\t\t\t   || test_bit(MD_RECOVERY_INTR, &mddev->recovery));\n\t\tif (atomic_read(&conf->reshape_stripes) != 0)\n\t\t\treturn 0;\n\t\tmddev->reshape_position = conf->reshape_progress;\n\t\tmddev->curr_resync_completed = sector_nr;\n\t\tif (!mddev->reshape_backwards)\n\t\t\t/* Can update recovery_offset */\n\t\t\trdev_for_each(rdev, mddev)\n\t\t\t\tif (rdev->raid_disk >= 0 &&\n\t\t\t\t    !test_bit(Journal, &rdev->flags) &&\n\t\t\t\t    !test_bit(In_sync, &rdev->flags) &&\n\t\t\t\t    rdev->recovery_offset < sector_nr)\n\t\t\t\t\trdev->recovery_offset = sector_nr;\n\n\t\tconf->reshape_checkpoint = jiffies;\n\t\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\t\tmd_wakeup_thread(mddev->thread);\n\t\twait_event(mddev->sb_wait, mddev->sb_flags == 0 ||\n\t\t\t   test_bit(MD_RECOVERY_INTR, &mddev->recovery));\n\t\tif (test_bit(MD_RECOVERY_INTR, &mddev->recovery))\n\t\t\treturn 0;\n\t\tspin_lock_irq(&conf->device_lock);\n\t\tconf->reshape_safe = mddev->reshape_position;\n\t\tspin_unlock_irq(&conf->device_lock);\n\t\twake_up(&conf->wait_for_overlap);\n\t\tsysfs_notify_dirent_safe(mddev->sysfs_completed);\n\t}\n\n\tINIT_LIST_HEAD(&stripes);\n\tfor (i = 0; i < reshape_sectors; i += RAID5_STRIPE_SECTORS(conf)) {\n\t\tint j;\n\t\tint skipped_disk = 0;\n\t\tsh = raid5_get_active_stripe(conf, stripe_addr+i, 0, 0, 1);\n\t\tset_bit(STRIPE_EXPANDING, &sh->state);\n\t\tatomic_inc(&conf->reshape_stripes);\n\t\t/* If any of this stripe is beyond the end of the old\n\t\t * array, then we need to zero those blocks\n\t\t */\n\t\tfor (j=sh->disks; j--;) {\n\t\t\tsector_t s;\n\t\t\tif (j == sh->pd_idx)\n\t\t\t\tcontinue;\n\t\t\tif (conf->level == 6 &&\n\t\t\t    j == sh->qd_idx)\n\t\t\t\tcontinue;\n\t\t\ts = raid5_compute_blocknr(sh, j, 0);\n\t\t\tif (s < raid5_size(mddev, 0, 0)) {\n\t\t\t\tskipped_disk = 1;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tmemset(page_address(sh->dev[j].page), 0, RAID5_STRIPE_SIZE(conf));\n\t\t\tset_bit(R5_Expanded, &sh->dev[j].flags);\n\t\t\tset_bit(R5_UPTODATE, &sh->dev[j].flags);\n\t\t}\n\t\tif (!skipped_disk) {\n\t\t\tset_bit(STRIPE_EXPAND_READY, &sh->state);\n\t\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\t}\n\t\tlist_add(&sh->lru, &stripes);\n\t}\n\tspin_lock_irq(&conf->device_lock);\n\tif (mddev->reshape_backwards)\n\t\tconf->reshape_progress -= reshape_sectors * new_data_disks;\n\telse\n\t\tconf->reshape_progress += reshape_sectors * new_data_disks;\n\tspin_unlock_irq(&conf->device_lock);\n\t/* Ok, those stripe are ready. We can start scheduling\n\t * reads on the source stripes.\n\t * The source stripes are determined by mapping the first and last\n\t * block on the destination stripes.\n\t */\n\tfirst_sector =\n\t\traid5_compute_sector(conf, stripe_addr*(new_data_disks),\n\t\t\t\t     1, &dd_idx, NULL);\n\tlast_sector =\n\t\traid5_compute_sector(conf, ((stripe_addr+reshape_sectors)\n\t\t\t\t\t    * new_data_disks - 1),\n\t\t\t\t     1, &dd_idx, NULL);\n\tif (last_sector >= mddev->dev_sectors)\n\t\tlast_sector = mddev->dev_sectors - 1;\n\twhile (first_sector <= last_sector) {\n\t\tsh = raid5_get_active_stripe(conf, first_sector, 1, 0, 1);\n\t\tset_bit(STRIPE_EXPAND_SOURCE, &sh->state);\n\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\traid5_release_stripe(sh);\n\t\tfirst_sector += RAID5_STRIPE_SECTORS(conf);\n\t}\n\t/* Now that the sources are clearly marked, we can release\n\t * the destination stripes\n\t */\n\twhile (!list_empty(&stripes)) {\n\t\tsh = list_entry(stripes.next, struct stripe_head, lru);\n\t\tlist_del_init(&sh->lru);\n\t\traid5_release_stripe(sh);\n\t}\n\t/* If this takes us to the resync_max point where we have to pause,\n\t * then we need to write out the superblock.\n\t */\n\tsector_nr += reshape_sectors;\n\tretn = reshape_sectors;\nfinish:\n\tif (mddev->curr_resync_completed > mddev->resync_max ||\n\t    (sector_nr - mddev->curr_resync_completed) * 2\n\t    >= mddev->resync_max - mddev->curr_resync_completed) {\n\t\t/* Cannot proceed until we've updated the superblock... */\n\t\twait_event(conf->wait_for_overlap,\n\t\t\t   atomic_read(&conf->reshape_stripes) == 0\n\t\t\t   || test_bit(MD_RECOVERY_INTR, &mddev->recovery));\n\t\tif (atomic_read(&conf->reshape_stripes) != 0)\n\t\t\tgoto ret;\n\t\tmddev->reshape_position = conf->reshape_progress;\n\t\tmddev->curr_resync_completed = sector_nr;\n\t\tif (!mddev->reshape_backwards)\n\t\t\t/* Can update recovery_offset */\n\t\t\trdev_for_each(rdev, mddev)\n\t\t\t\tif (rdev->raid_disk >= 0 &&\n\t\t\t\t    !test_bit(Journal, &rdev->flags) &&\n\t\t\t\t    !test_bit(In_sync, &rdev->flags) &&\n\t\t\t\t    rdev->recovery_offset < sector_nr)\n\t\t\t\t\trdev->recovery_offset = sector_nr;\n\t\tconf->reshape_checkpoint = jiffies;\n\t\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\t\tmd_wakeup_thread(mddev->thread);\n\t\twait_event(mddev->sb_wait,\n\t\t\t   !test_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags)\n\t\t\t   || test_bit(MD_RECOVERY_INTR, &mddev->recovery));\n\t\tif (test_bit(MD_RECOVERY_INTR, &mddev->recovery))\n\t\t\tgoto ret;\n\t\tspin_lock_irq(&conf->device_lock);\n\t\tconf->reshape_safe = mddev->reshape_position;\n\t\tspin_unlock_irq(&conf->device_lock);\n\t\twake_up(&conf->wait_for_overlap);\n\t\tsysfs_notify_dirent_safe(mddev->sysfs_completed);\n\t}\nret:\n\treturn retn;\n}\n\nstatic inline sector_t raid5_sync_request(struct mddev *mddev, sector_t sector_nr,\n\t\t\t\t\t  int *skipped)\n{\n\tstruct r5conf *conf = mddev->private;\n\tstruct stripe_head *sh;\n\tsector_t max_sector = mddev->dev_sectors;\n\tsector_t sync_blocks;\n\tint still_degraded = 0;\n\tint i;\n\n\tif (sector_nr >= max_sector) {\n\t\t/* just being told to finish up .. nothing much to do */\n\n\t\tif (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery)) {\n\t\t\tend_reshape(conf);\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (mddev->curr_resync < max_sector) /* aborted */\n\t\t\tmd_bitmap_end_sync(mddev->bitmap, mddev->curr_resync,\n\t\t\t\t\t   &sync_blocks, 1);\n\t\telse /* completed sync */\n\t\t\tconf->fullsync = 0;\n\t\tmd_bitmap_close_sync(mddev->bitmap);\n\n\t\treturn 0;\n\t}\n\n\t/* Allow raid5_quiesce to complete */\n\twait_event(conf->wait_for_overlap, conf->quiesce != 2);\n\n\tif (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery))\n\t\treturn reshape_request(mddev, sector_nr, skipped);\n\n\t/* No need to check resync_max as we never do more than one\n\t * stripe, and as resync_max will always be on a chunk boundary,\n\t * if the check in md_do_sync didn't fire, there is no chance\n\t * of overstepping resync_max here\n\t */\n\n\t/* if there is too many failed drives and we are trying\n\t * to resync, then assert that we are finished, because there is\n\t * nothing we can do.\n\t */\n\tif (mddev->degraded >= conf->max_degraded &&\n\t    test_bit(MD_RECOVERY_SYNC, &mddev->recovery)) {\n\t\tsector_t rv = mddev->dev_sectors - sector_nr;\n\t\t*skipped = 1;\n\t\treturn rv;\n\t}\n\tif (!test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery) &&\n\t    !conf->fullsync &&\n\t    !md_bitmap_start_sync(mddev->bitmap, sector_nr, &sync_blocks, 1) &&\n\t    sync_blocks >= RAID5_STRIPE_SECTORS(conf)) {\n\t\t/* we can skip this block, and probably more */\n\t\tdo_div(sync_blocks, RAID5_STRIPE_SECTORS(conf));\n\t\t*skipped = 1;\n\t\t/* keep things rounded to whole stripes */\n\t\treturn sync_blocks * RAID5_STRIPE_SECTORS(conf);\n\t}\n\n\tmd_bitmap_cond_end_sync(mddev->bitmap, sector_nr, false);\n\n\tsh = raid5_get_active_stripe(conf, sector_nr, 0, 1, 0);\n\tif (sh == NULL) {\n\t\tsh = raid5_get_active_stripe(conf, sector_nr, 0, 0, 0);\n\t\t/* make sure we don't swamp the stripe cache if someone else\n\t\t * is trying to get access\n\t\t */\n\t\tschedule_timeout_uninterruptible(1);\n\t}\n\t/* Need to check if array will still be degraded after recovery/resync\n\t * Note in case of > 1 drive failures it's possible we're rebuilding\n\t * one drive while leaving another faulty drive in array.\n\t */\n\trcu_read_lock();\n\tfor (i = 0; i < conf->raid_disks; i++) {\n\t\tstruct md_rdev *rdev = READ_ONCE(conf->disks[i].rdev);\n\n\t\tif (rdev == NULL || test_bit(Faulty, &rdev->flags))\n\t\t\tstill_degraded = 1;\n\t}\n\trcu_read_unlock();\n\n\tmd_bitmap_start_sync(mddev->bitmap, sector_nr, &sync_blocks, still_degraded);\n\n\tset_bit(STRIPE_SYNC_REQUESTED, &sh->state);\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\n\traid5_release_stripe(sh);\n\n\treturn RAID5_STRIPE_SECTORS(conf);\n}\n\nstatic int  retry_aligned_read(struct r5conf *conf, struct bio *raid_bio,\n\t\t\t       unsigned int offset)\n{\n\t/* We may not be able to submit a whole bio at once as there\n\t * may not be enough stripe_heads available.\n\t * We cannot pre-allocate enough stripe_heads as we may need\n\t * more than exist in the cache (if we allow ever large chunks).\n\t * So we do one stripe head at a time and record in\n\t * ->bi_hw_segments how many have been done.\n\t *\n\t * We *know* that this entire raid_bio is in one chunk, so\n\t * it will be only one 'dd_idx' and only need one call to raid5_compute_sector.\n\t */\n\tstruct stripe_head *sh;\n\tint dd_idx;\n\tsector_t sector, logical_sector, last_sector;\n\tint scnt = 0;\n\tint handled = 0;\n\n\tlogical_sector = raid_bio->bi_iter.bi_sector &\n\t\t~((sector_t)RAID5_STRIPE_SECTORS(conf)-1);\n\tsector = raid5_compute_sector(conf, logical_sector,\n\t\t\t\t      0, &dd_idx, NULL);\n\tlast_sector = bio_end_sector(raid_bio);\n\n\tfor (; logical_sector < last_sector;\n\t     logical_sector += RAID5_STRIPE_SECTORS(conf),\n\t\t     sector += RAID5_STRIPE_SECTORS(conf),\n\t\t     scnt++) {\n\n\t\tif (scnt < offset)\n\t\t\t/* already done this stripe */\n\t\t\tcontinue;\n\n\t\tsh = raid5_get_active_stripe(conf, sector, 0, 1, 1);\n\n\t\tif (!sh) {\n\t\t\t/* failed to get a stripe - must wait */\n\t\t\tconf->retry_read_aligned = raid_bio;\n\t\t\tconf->retry_read_offset = scnt;\n\t\t\treturn handled;\n\t\t}\n\n\t\tif (!add_stripe_bio(sh, raid_bio, dd_idx, 0, 0)) {\n\t\t\traid5_release_stripe(sh);\n\t\t\tconf->retry_read_aligned = raid_bio;\n\t\t\tconf->retry_read_offset = scnt;\n\t\t\treturn handled;\n\t\t}\n\n\t\tset_bit(R5_ReadNoMerge, &sh->dev[dd_idx].flags);\n\t\thandle_stripe(sh);\n\t\traid5_release_stripe(sh);\n\t\thandled++;\n\t}\n\n\tbio_endio(raid_bio);\n\n\tif (atomic_dec_and_test(&conf->active_aligned_reads))\n\t\twake_up(&conf->wait_for_quiescent);\n\treturn handled;\n}\n\nstatic int handle_active_stripes(struct r5conf *conf, int group,\n\t\t\t\t struct r5worker *worker,\n\t\t\t\t struct list_head *temp_inactive_list)\n\t\t__releases(&conf->device_lock)\n\t\t__acquires(&conf->device_lock)\n{\n\tstruct stripe_head *batch[MAX_STRIPE_BATCH], *sh;\n\tint i, batch_size = 0, hash;\n\tbool release_inactive = false;\n\n\twhile (batch_size < MAX_STRIPE_BATCH &&\n\t\t\t(sh = __get_priority_stripe(conf, group)) != NULL)\n\t\tbatch[batch_size++] = sh;\n\n\tif (batch_size == 0) {\n\t\tfor (i = 0; i < NR_STRIPE_HASH_LOCKS; i++)\n\t\t\tif (!list_empty(temp_inactive_list + i))\n\t\t\t\tbreak;\n\t\tif (i == NR_STRIPE_HASH_LOCKS) {\n\t\t\tspin_unlock_irq(&conf->device_lock);\n\t\t\tlog_flush_stripe_to_raid(conf);\n\t\t\tspin_lock_irq(&conf->device_lock);\n\t\t\treturn batch_size;\n\t\t}\n\t\trelease_inactive = true;\n\t}\n\tspin_unlock_irq(&conf->device_lock);\n\n\trelease_inactive_stripe_list(conf, temp_inactive_list,\n\t\t\t\t     NR_STRIPE_HASH_LOCKS);\n\n\tr5l_flush_stripe_to_raid(conf->log);\n\tif (release_inactive) {\n\t\tspin_lock_irq(&conf->device_lock);\n\t\treturn 0;\n\t}\n\n\tfor (i = 0; i < batch_size; i++)\n\t\thandle_stripe(batch[i]);\n\tlog_write_stripe_run(conf);\n\n\tcond_resched();\n\n\tspin_lock_irq(&conf->device_lock);\n\tfor (i = 0; i < batch_size; i++) {\n\t\thash = batch[i]->hash_lock_index;\n\t\t__release_stripe(conf, batch[i], &temp_inactive_list[hash]);\n\t}\n\treturn batch_size;\n}\n\nstatic void raid5_do_work(struct work_struct *work)\n{\n\tstruct r5worker *worker = container_of(work, struct r5worker, work);\n\tstruct r5worker_group *group = worker->group;\n\tstruct r5conf *conf = group->conf;\n\tstruct mddev *mddev = conf->mddev;\n\tint group_id = group - conf->worker_groups;\n\tint handled;\n\tstruct blk_plug plug;\n\n\tpr_debug(\"+++ raid5worker active\\n\");\n\n\tblk_start_plug(&plug);\n\thandled = 0;\n\tspin_lock_irq(&conf->device_lock);\n\twhile (1) {\n\t\tint batch_size, released;\n\n\t\treleased = release_stripe_list(conf, worker->temp_inactive_list);\n\n\t\tbatch_size = handle_active_stripes(conf, group_id, worker,\n\t\t\t\t\t\t   worker->temp_inactive_list);\n\t\tworker->working = false;\n\t\tif (!batch_size && !released)\n\t\t\tbreak;\n\t\thandled += batch_size;\n\t\twait_event_lock_irq(mddev->sb_wait,\n\t\t\t!test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags),\n\t\t\tconf->device_lock);\n\t}\n\tpr_debug(\"%d stripes handled\\n\", handled);\n\n\tspin_unlock_irq(&conf->device_lock);\n\n\tflush_deferred_bios(conf);\n\n\tr5l_flush_stripe_to_raid(conf->log);\n\n\tasync_tx_issue_pending_all();\n\tblk_finish_plug(&plug);\n\n\tpr_debug(\"--- raid5worker inactive\\n\");\n}\n\n/*\n * This is our raid5 kernel thread.\n *\n * We scan the hash table for stripes which can be handled now.\n * During the scan, completed stripes are saved for us by the interrupt\n * handler, so that they will not have to wait for our next wakeup.\n */\nstatic void raid5d(struct md_thread *thread)\n{\n\tstruct mddev *mddev = thread->mddev;\n\tstruct r5conf *conf = mddev->private;\n\tint handled;\n\tstruct blk_plug plug;\n\n\tpr_debug(\"+++ raid5d active\\n\");\n\n\tmd_check_recovery(mddev);\n\n\tblk_start_plug(&plug);\n\thandled = 0;\n\tspin_lock_irq(&conf->device_lock);\n\twhile (1) {\n\t\tstruct bio *bio;\n\t\tint batch_size, released;\n\t\tunsigned int offset;\n\n\t\treleased = release_stripe_list(conf, conf->temp_inactive_list);\n\t\tif (released)\n\t\t\tclear_bit(R5_DID_ALLOC, &conf->cache_state);\n\n\t\tif (\n\t\t    !list_empty(&conf->bitmap_list)) {\n\t\t\t/* Now is a good time to flush some bitmap updates */\n\t\t\tconf->seq_flush++;\n\t\t\tspin_unlock_irq(&conf->device_lock);\n\t\t\tmd_bitmap_unplug(mddev->bitmap);\n\t\t\tspin_lock_irq(&conf->device_lock);\n\t\t\tconf->seq_write = conf->seq_flush;\n\t\t\tactivate_bit_delay(conf, conf->temp_inactive_list);\n\t\t}\n\t\traid5_activate_delayed(conf);\n\n\t\twhile ((bio = remove_bio_from_retry(conf, &offset))) {\n\t\t\tint ok;\n\t\t\tspin_unlock_irq(&conf->device_lock);\n\t\t\tok = retry_aligned_read(conf, bio, offset);\n\t\t\tspin_lock_irq(&conf->device_lock);\n\t\t\tif (!ok)\n\t\t\t\tbreak;\n\t\t\thandled++;\n\t\t}\n\n\t\tbatch_size = handle_active_stripes(conf, ANY_GROUP, NULL,\n\t\t\t\t\t\t   conf->temp_inactive_list);\n\t\tif (!batch_size && !released)\n\t\t\tbreak;\n\t\thandled += batch_size;\n\n\t\tif (mddev->sb_flags & ~(1 << MD_SB_CHANGE_PENDING)) {\n\t\t\tspin_unlock_irq(&conf->device_lock);\n\t\t\tmd_check_recovery(mddev);\n\t\t\tspin_lock_irq(&conf->device_lock);\n\t\t}\n\t}\n\tpr_debug(\"%d stripes handled\\n\", handled);\n\n\tspin_unlock_irq(&conf->device_lock);\n\tif (test_and_clear_bit(R5_ALLOC_MORE, &conf->cache_state) &&\n\t    mutex_trylock(&conf->cache_size_mutex)) {\n\t\tgrow_one_stripe(conf, __GFP_NOWARN);\n\t\t/* Set flag even if allocation failed.  This helps\n\t\t * slow down allocation requests when mem is short\n\t\t */\n\t\tset_bit(R5_DID_ALLOC, &conf->cache_state);\n\t\tmutex_unlock(&conf->cache_size_mutex);\n\t}\n\n\tflush_deferred_bios(conf);\n\n\tr5l_flush_stripe_to_raid(conf->log);\n\n\tasync_tx_issue_pending_all();\n\tblk_finish_plug(&plug);\n\n\tpr_debug(\"--- raid5d inactive\\n\");\n}\n\nstatic ssize_t\nraid5_show_stripe_cache_size(struct mddev *mddev, char *page)\n{\n\tstruct r5conf *conf;\n\tint ret = 0;\n\tspin_lock(&mddev->lock);\n\tconf = mddev->private;\n\tif (conf)\n\t\tret = sprintf(page, \"%d\\n\", conf->min_nr_stripes);\n\tspin_unlock(&mddev->lock);\n\treturn ret;\n}\n\nint\nraid5_set_cache_size(struct mddev *mddev, int size)\n{\n\tint result = 0;\n\tstruct r5conf *conf = mddev->private;\n\n\tif (size <= 16 || size > 32768)\n\t\treturn -EINVAL;\n\n\tconf->min_nr_stripes = size;\n\tmutex_lock(&conf->cache_size_mutex);\n\twhile (size < conf->max_nr_stripes &&\n\t       drop_one_stripe(conf))\n\t\t;\n\tmutex_unlock(&conf->cache_size_mutex);\n\n\tmd_allow_write(mddev);\n\n\tmutex_lock(&conf->cache_size_mutex);\n\twhile (size > conf->max_nr_stripes)\n\t\tif (!grow_one_stripe(conf, GFP_KERNEL)) {\n\t\t\tconf->min_nr_stripes = conf->max_nr_stripes;\n\t\t\tresult = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\tmutex_unlock(&conf->cache_size_mutex);\n\n\treturn result;\n}\nEXPORT_SYMBOL(raid5_set_cache_size);\n\nstatic ssize_t\nraid5_store_stripe_cache_size(struct mddev *mddev, const char *page, size_t len)\n{\n\tstruct r5conf *conf;\n\tunsigned long new;\n\tint err;\n\n\tif (len >= PAGE_SIZE)\n\t\treturn -EINVAL;\n\tif (kstrtoul(page, 10, &new))\n\t\treturn -EINVAL;\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\tconf = mddev->private;\n\tif (!conf)\n\t\terr = -ENODEV;\n\telse\n\t\terr = raid5_set_cache_size(mddev, new);\n\tmddev_unlock(mddev);\n\n\treturn err ?: len;\n}\n\nstatic struct md_sysfs_entry\nraid5_stripecache_size = __ATTR(stripe_cache_size, S_IRUGO | S_IWUSR,\n\t\t\t\traid5_show_stripe_cache_size,\n\t\t\t\traid5_store_stripe_cache_size);\n\nstatic ssize_t\nraid5_show_rmw_level(struct mddev  *mddev, char *page)\n{\n\tstruct r5conf *conf = mddev->private;\n\tif (conf)\n\t\treturn sprintf(page, \"%d\\n\", conf->rmw_level);\n\telse\n\t\treturn 0;\n}\n\nstatic ssize_t\nraid5_store_rmw_level(struct mddev  *mddev, const char *page, size_t len)\n{\n\tstruct r5conf *conf = mddev->private;\n\tunsigned long new;\n\n\tif (!conf)\n\t\treturn -ENODEV;\n\n\tif (len >= PAGE_SIZE)\n\t\treturn -EINVAL;\n\n\tif (kstrtoul(page, 10, &new))\n\t\treturn -EINVAL;\n\n\tif (new != PARITY_DISABLE_RMW && !raid6_call.xor_syndrome)\n\t\treturn -EINVAL;\n\n\tif (new != PARITY_DISABLE_RMW &&\n\t    new != PARITY_ENABLE_RMW &&\n\t    new != PARITY_PREFER_RMW)\n\t\treturn -EINVAL;\n\n\tconf->rmw_level = new;\n\treturn len;\n}\n\nstatic struct md_sysfs_entry\nraid5_rmw_level = __ATTR(rmw_level, S_IRUGO | S_IWUSR,\n\t\t\t raid5_show_rmw_level,\n\t\t\t raid5_store_rmw_level);\n\nstatic ssize_t\nraid5_show_stripe_size(struct mddev  *mddev, char *page)\n{\n\tstruct r5conf *conf;\n\tint ret = 0;\n\n\tspin_lock(&mddev->lock);\n\tconf = mddev->private;\n\tif (conf)\n\t\tret = sprintf(page, \"%lu\\n\", RAID5_STRIPE_SIZE(conf));\n\tspin_unlock(&mddev->lock);\n\treturn ret;\n}\n\n#if PAGE_SIZE != DEFAULT_STRIPE_SIZE\nstatic ssize_t\nraid5_store_stripe_size(struct mddev  *mddev, const char *page, size_t len)\n{\n\tstruct r5conf *conf;\n\tunsigned long new;\n\tint err;\n\tint size;\n\n\tif (len >= PAGE_SIZE)\n\t\treturn -EINVAL;\n\tif (kstrtoul(page, 10, &new))\n\t\treturn -EINVAL;\n\n\t/*\n\t * The value should not be bigger than PAGE_SIZE. It requires to\n\t * be multiple of DEFAULT_STRIPE_SIZE and the value should be power\n\t * of two.\n\t */\n\tif (new % DEFAULT_STRIPE_SIZE != 0 ||\n\t\t\tnew > PAGE_SIZE || new == 0 ||\n\t\t\tnew != roundup_pow_of_two(new))\n\t\treturn -EINVAL;\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\n\tconf = mddev->private;\n\tif (!conf) {\n\t\terr = -ENODEV;\n\t\tgoto out_unlock;\n\t}\n\n\tif (new == conf->stripe_size)\n\t\tgoto out_unlock;\n\n\tpr_debug(\"md/raid: change stripe_size from %lu to %lu\\n\",\n\t\t\tconf->stripe_size, new);\n\n\tif (mddev->sync_thread ||\n\t\ttest_bit(MD_RECOVERY_RUNNING, &mddev->recovery) ||\n\t\tmddev->reshape_position != MaxSector ||\n\t\tmddev->sysfs_active) {\n\t\terr = -EBUSY;\n\t\tgoto out_unlock;\n\t}\n\n\tmddev_suspend(mddev);\n\tmutex_lock(&conf->cache_size_mutex);\n\tsize = conf->max_nr_stripes;\n\n\tshrink_stripes(conf);\n\n\tconf->stripe_size = new;\n\tconf->stripe_shift = ilog2(new) - 9;\n\tconf->stripe_sectors = new >> 9;\n\tif (grow_stripes(conf, size)) {\n\t\tpr_warn(\"md/raid:%s: couldn't allocate buffers\\n\",\n\t\t\t\tmdname(mddev));\n\t\terr = -ENOMEM;\n\t}\n\tmutex_unlock(&conf->cache_size_mutex);\n\tmddev_resume(mddev);\n\nout_unlock:\n\tmddev_unlock(mddev);\n\treturn err ?: len;\n}\n\nstatic struct md_sysfs_entry\nraid5_stripe_size = __ATTR(stripe_size, 0644,\n\t\t\t raid5_show_stripe_size,\n\t\t\t raid5_store_stripe_size);\n#else\nstatic struct md_sysfs_entry\nraid5_stripe_size = __ATTR(stripe_size, 0444,\n\t\t\t raid5_show_stripe_size,\n\t\t\t NULL);\n#endif\n\nstatic ssize_t\nraid5_show_preread_threshold(struct mddev *mddev, char *page)\n{\n\tstruct r5conf *conf;\n\tint ret = 0;\n\tspin_lock(&mddev->lock);\n\tconf = mddev->private;\n\tif (conf)\n\t\tret = sprintf(page, \"%d\\n\", conf->bypass_threshold);\n\tspin_unlock(&mddev->lock);\n\treturn ret;\n}\n\nstatic ssize_t\nraid5_store_preread_threshold(struct mddev *mddev, const char *page, size_t len)\n{\n\tstruct r5conf *conf;\n\tunsigned long new;\n\tint err;\n\n\tif (len >= PAGE_SIZE)\n\t\treturn -EINVAL;\n\tif (kstrtoul(page, 10, &new))\n\t\treturn -EINVAL;\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\tconf = mddev->private;\n\tif (!conf)\n\t\terr = -ENODEV;\n\telse if (new > conf->min_nr_stripes)\n\t\terr = -EINVAL;\n\telse\n\t\tconf->bypass_threshold = new;\n\tmddev_unlock(mddev);\n\treturn err ?: len;\n}\n\nstatic struct md_sysfs_entry\nraid5_preread_bypass_threshold = __ATTR(preread_bypass_threshold,\n\t\t\t\t\tS_IRUGO | S_IWUSR,\n\t\t\t\t\traid5_show_preread_threshold,\n\t\t\t\t\traid5_store_preread_threshold);\n\nstatic ssize_t\nraid5_show_skip_copy(struct mddev *mddev, char *page)\n{\n\tstruct r5conf *conf;\n\tint ret = 0;\n\tspin_lock(&mddev->lock);\n\tconf = mddev->private;\n\tif (conf)\n\t\tret = sprintf(page, \"%d\\n\", conf->skip_copy);\n\tspin_unlock(&mddev->lock);\n\treturn ret;\n}\n\nstatic ssize_t\nraid5_store_skip_copy(struct mddev *mddev, const char *page, size_t len)\n{\n\tstruct r5conf *conf;\n\tunsigned long new;\n\tint err;\n\n\tif (len >= PAGE_SIZE)\n\t\treturn -EINVAL;\n\tif (kstrtoul(page, 10, &new))\n\t\treturn -EINVAL;\n\tnew = !!new;\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\tconf = mddev->private;\n\tif (!conf)\n\t\terr = -ENODEV;\n\telse if (new != conf->skip_copy) {\n\t\tstruct request_queue *q = mddev->queue;\n\n\t\tmddev_suspend(mddev);\n\t\tconf->skip_copy = new;\n\t\tif (new)\n\t\t\tblk_queue_flag_set(QUEUE_FLAG_STABLE_WRITES, q);\n\t\telse\n\t\t\tblk_queue_flag_clear(QUEUE_FLAG_STABLE_WRITES, q);\n\t\tmddev_resume(mddev);\n\t}\n\tmddev_unlock(mddev);\n\treturn err ?: len;\n}\n\nstatic struct md_sysfs_entry\nraid5_skip_copy = __ATTR(skip_copy, S_IRUGO | S_IWUSR,\n\t\t\t\t\traid5_show_skip_copy,\n\t\t\t\t\traid5_store_skip_copy);\n\nstatic ssize_t\nstripe_cache_active_show(struct mddev *mddev, char *page)\n{\n\tstruct r5conf *conf = mddev->private;\n\tif (conf)\n\t\treturn sprintf(page, \"%d\\n\", atomic_read(&conf->active_stripes));\n\telse\n\t\treturn 0;\n}\n\nstatic struct md_sysfs_entry\nraid5_stripecache_active = __ATTR_RO(stripe_cache_active);\n\nstatic ssize_t\nraid5_show_group_thread_cnt(struct mddev *mddev, char *page)\n{\n\tstruct r5conf *conf;\n\tint ret = 0;\n\tspin_lock(&mddev->lock);\n\tconf = mddev->private;\n\tif (conf)\n\t\tret = sprintf(page, \"%d\\n\", conf->worker_cnt_per_group);\n\tspin_unlock(&mddev->lock);\n\treturn ret;\n}\n\nstatic int alloc_thread_groups(struct r5conf *conf, int cnt,\n\t\t\t       int *group_cnt,\n\t\t\t       struct r5worker_group **worker_groups);\nstatic ssize_t\nraid5_store_group_thread_cnt(struct mddev *mddev, const char *page, size_t len)\n{\n\tstruct r5conf *conf;\n\tunsigned int new;\n\tint err;\n\tstruct r5worker_group *new_groups, *old_groups;\n\tint group_cnt;\n\n\tif (len >= PAGE_SIZE)\n\t\treturn -EINVAL;\n\tif (kstrtouint(page, 10, &new))\n\t\treturn -EINVAL;\n\t/* 8192 should be big enough */\n\tif (new > 8192)\n\t\treturn -EINVAL;\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\tconf = mddev->private;\n\tif (!conf)\n\t\terr = -ENODEV;\n\telse if (new != conf->worker_cnt_per_group) {\n\t\tmddev_suspend(mddev);\n\n\t\told_groups = conf->worker_groups;\n\t\tif (old_groups)\n\t\t\tflush_workqueue(raid5_wq);\n\n\t\terr = alloc_thread_groups(conf, new, &group_cnt, &new_groups);\n\t\tif (!err) {\n\t\t\tspin_lock_irq(&conf->device_lock);\n\t\t\tconf->group_cnt = group_cnt;\n\t\t\tconf->worker_cnt_per_group = new;\n\t\t\tconf->worker_groups = new_groups;\n\t\t\tspin_unlock_irq(&conf->device_lock);\n\n\t\t\tif (old_groups)\n\t\t\t\tkfree(old_groups[0].workers);\n\t\t\tkfree(old_groups);\n\t\t}\n\t\tmddev_resume(mddev);\n\t}\n\tmddev_unlock(mddev);\n\n\treturn err ?: len;\n}\n\nstatic struct md_sysfs_entry\nraid5_group_thread_cnt = __ATTR(group_thread_cnt, S_IRUGO | S_IWUSR,\n\t\t\t\traid5_show_group_thread_cnt,\n\t\t\t\traid5_store_group_thread_cnt);\n\nstatic struct attribute *raid5_attrs[] =  {\n\t&raid5_stripecache_size.attr,\n\t&raid5_stripecache_active.attr,\n\t&raid5_preread_bypass_threshold.attr,\n\t&raid5_group_thread_cnt.attr,\n\t&raid5_skip_copy.attr,\n\t&raid5_rmw_level.attr,\n\t&raid5_stripe_size.attr,\n\t&r5c_journal_mode.attr,\n\t&ppl_write_hint.attr,\n\tNULL,\n};\nstatic struct attribute_group raid5_attrs_group = {\n\t.name = NULL,\n\t.attrs = raid5_attrs,\n};\n\nstatic int alloc_thread_groups(struct r5conf *conf, int cnt, int *group_cnt,\n\t\t\t       struct r5worker_group **worker_groups)\n{\n\tint i, j, k;\n\tssize_t size;\n\tstruct r5worker *workers;\n\n\tif (cnt == 0) {\n\t\t*group_cnt = 0;\n\t\t*worker_groups = NULL;\n\t\treturn 0;\n\t}\n\t*group_cnt = num_possible_nodes();\n\tsize = sizeof(struct r5worker) * cnt;\n\tworkers = kcalloc(size, *group_cnt, GFP_NOIO);\n\t*worker_groups = kcalloc(*group_cnt, sizeof(struct r5worker_group),\n\t\t\t\t GFP_NOIO);\n\tif (!*worker_groups || !workers) {\n\t\tkfree(workers);\n\t\tkfree(*worker_groups);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < *group_cnt; i++) {\n\t\tstruct r5worker_group *group;\n\n\t\tgroup = &(*worker_groups)[i];\n\t\tINIT_LIST_HEAD(&group->handle_list);\n\t\tINIT_LIST_HEAD(&group->loprio_list);\n\t\tgroup->conf = conf;\n\t\tgroup->workers = workers + i * cnt;\n\n\t\tfor (j = 0; j < cnt; j++) {\n\t\t\tstruct r5worker *worker = group->workers + j;\n\t\t\tworker->group = group;\n\t\t\tINIT_WORK(&worker->work, raid5_do_work);\n\n\t\t\tfor (k = 0; k < NR_STRIPE_HASH_LOCKS; k++)\n\t\t\t\tINIT_LIST_HEAD(worker->temp_inactive_list + k);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void free_thread_groups(struct r5conf *conf)\n{\n\tif (conf->worker_groups)\n\t\tkfree(conf->worker_groups[0].workers);\n\tkfree(conf->worker_groups);\n\tconf->worker_groups = NULL;\n}\n\nstatic sector_t\nraid5_size(struct mddev *mddev, sector_t sectors, int raid_disks)\n{\n\tstruct r5conf *conf = mddev->private;\n\n\tif (!sectors)\n\t\tsectors = mddev->dev_sectors;\n\tif (!raid_disks)\n\t\t/* size is defined by the smallest of previous and new size */\n\t\traid_disks = min(conf->raid_disks, conf->previous_raid_disks);\n\n\tsectors &= ~((sector_t)conf->chunk_sectors - 1);\n\tsectors &= ~((sector_t)conf->prev_chunk_sectors - 1);\n\treturn sectors * (raid_disks - conf->max_degraded);\n}\n\nstatic void free_scratch_buffer(struct r5conf *conf, struct raid5_percpu *percpu)\n{\n\tsafe_put_page(percpu->spare_page);\n\tpercpu->spare_page = NULL;\n\tkvfree(percpu->scribble);\n\tpercpu->scribble = NULL;\n}\n\nstatic int alloc_scratch_buffer(struct r5conf *conf, struct raid5_percpu *percpu)\n{\n\tif (conf->level == 6 && !percpu->spare_page) {\n\t\tpercpu->spare_page = alloc_page(GFP_KERNEL);\n\t\tif (!percpu->spare_page)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tif (scribble_alloc(percpu,\n\t\t\t   max(conf->raid_disks,\n\t\t\t       conf->previous_raid_disks),\n\t\t\t   max(conf->chunk_sectors,\n\t\t\t       conf->prev_chunk_sectors)\n\t\t\t   / RAID5_STRIPE_SECTORS(conf))) {\n\t\tfree_scratch_buffer(conf, percpu);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic int raid456_cpu_dead(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct r5conf *conf = hlist_entry_safe(node, struct r5conf, node);\n\n\tfree_scratch_buffer(conf, per_cpu_ptr(conf->percpu, cpu));\n\treturn 0;\n}\n\nstatic void raid5_free_percpu(struct r5conf *conf)\n{\n\tif (!conf->percpu)\n\t\treturn;\n\n\tcpuhp_state_remove_instance(CPUHP_MD_RAID5_PREPARE, &conf->node);\n\tfree_percpu(conf->percpu);\n}\n\nstatic void free_conf(struct r5conf *conf)\n{\n\tint i;\n\n\tlog_exit(conf);\n\n\tunregister_shrinker(&conf->shrinker);\n\tfree_thread_groups(conf);\n\tshrink_stripes(conf);\n\traid5_free_percpu(conf);\n\tfor (i = 0; i < conf->pool_size; i++)\n\t\tif (conf->disks[i].extra_page)\n\t\t\tput_page(conf->disks[i].extra_page);\n\tkfree(conf->disks);\n\tbioset_exit(&conf->bio_split);\n\tkfree(conf->stripe_hashtbl);\n\tkfree(conf->pending_data);\n\tkfree(conf);\n}\n\nstatic int raid456_cpu_up_prepare(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct r5conf *conf = hlist_entry_safe(node, struct r5conf, node);\n\tstruct raid5_percpu *percpu = per_cpu_ptr(conf->percpu, cpu);\n\n\tif (alloc_scratch_buffer(conf, percpu)) {\n\t\tpr_warn(\"%s: failed memory allocation for cpu%u\\n\",\n\t\t\t__func__, cpu);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nstatic int raid5_alloc_percpu(struct r5conf *conf)\n{\n\tint err = 0;\n\n\tconf->percpu = alloc_percpu(struct raid5_percpu);\n\tif (!conf->percpu)\n\t\treturn -ENOMEM;\n\n\terr = cpuhp_state_add_instance(CPUHP_MD_RAID5_PREPARE, &conf->node);\n\tif (!err) {\n\t\tconf->scribble_disks = max(conf->raid_disks,\n\t\t\tconf->previous_raid_disks);\n\t\tconf->scribble_sectors = max(conf->chunk_sectors,\n\t\t\tconf->prev_chunk_sectors);\n\t}\n\treturn err;\n}\n\nstatic unsigned long raid5_cache_scan(struct shrinker *shrink,\n\t\t\t\t      struct shrink_control *sc)\n{\n\tstruct r5conf *conf = container_of(shrink, struct r5conf, shrinker);\n\tunsigned long ret = SHRINK_STOP;\n\n\tif (mutex_trylock(&conf->cache_size_mutex)) {\n\t\tret= 0;\n\t\twhile (ret < sc->nr_to_scan &&\n\t\t       conf->max_nr_stripes > conf->min_nr_stripes) {\n\t\t\tif (drop_one_stripe(conf) == 0) {\n\t\t\t\tret = SHRINK_STOP;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tret++;\n\t\t}\n\t\tmutex_unlock(&conf->cache_size_mutex);\n\t}\n\treturn ret;\n}\n\nstatic unsigned long raid5_cache_count(struct shrinker *shrink,\n\t\t\t\t       struct shrink_control *sc)\n{\n\tstruct r5conf *conf = container_of(shrink, struct r5conf, shrinker);\n\n\tif (conf->max_nr_stripes < conf->min_nr_stripes)\n\t\t/* unlikely, but not impossible */\n\t\treturn 0;\n\treturn conf->max_nr_stripes - conf->min_nr_stripes;\n}\n\nstatic struct r5conf *setup_conf(struct mddev *mddev)\n{\n\tstruct r5conf *conf;\n\tint raid_disk, memory, max_disks;\n\tstruct md_rdev *rdev;\n\tstruct disk_info *disk;\n\tchar pers_name[6];\n\tint i;\n\tint group_cnt;\n\tstruct r5worker_group *new_group;\n\tint ret;\n\n\tif (mddev->new_level != 5\n\t    && mddev->new_level != 4\n\t    && mddev->new_level != 6) {\n\t\tpr_warn(\"md/raid:%s: raid level not set to 4/5/6 (%d)\\n\",\n\t\t\tmdname(mddev), mddev->new_level);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\tif ((mddev->new_level == 5\n\t     && !algorithm_valid_raid5(mddev->new_layout)) ||\n\t    (mddev->new_level == 6\n\t     && !algorithm_valid_raid6(mddev->new_layout))) {\n\t\tpr_warn(\"md/raid:%s: layout %d not supported\\n\",\n\t\t\tmdname(mddev), mddev->new_layout);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\tif (mddev->new_level == 6 && mddev->raid_disks < 4) {\n\t\tpr_warn(\"md/raid:%s: not enough configured devices (%d, minimum 4)\\n\",\n\t\t\tmdname(mddev), mddev->raid_disks);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (!mddev->new_chunk_sectors ||\n\t    (mddev->new_chunk_sectors << 9) % PAGE_SIZE ||\n\t    !is_power_of_2(mddev->new_chunk_sectors)) {\n\t\tpr_warn(\"md/raid:%s: invalid chunk size %d\\n\",\n\t\t\tmdname(mddev), mddev->new_chunk_sectors << 9);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tconf = kzalloc(sizeof(struct r5conf), GFP_KERNEL);\n\tif (conf == NULL)\n\t\tgoto abort;\n\n#if PAGE_SIZE != DEFAULT_STRIPE_SIZE\n\tconf->stripe_size = DEFAULT_STRIPE_SIZE;\n\tconf->stripe_shift = ilog2(DEFAULT_STRIPE_SIZE) - 9;\n\tconf->stripe_sectors = DEFAULT_STRIPE_SIZE >> 9;\n#endif\n\tINIT_LIST_HEAD(&conf->free_list);\n\tINIT_LIST_HEAD(&conf->pending_list);\n\tconf->pending_data = kcalloc(PENDING_IO_MAX,\n\t\t\t\t     sizeof(struct r5pending_data),\n\t\t\t\t     GFP_KERNEL);\n\tif (!conf->pending_data)\n\t\tgoto abort;\n\tfor (i = 0; i < PENDING_IO_MAX; i++)\n\t\tlist_add(&conf->pending_data[i].sibling, &conf->free_list);\n\t/* Don't enable multi-threading by default*/\n\tif (!alloc_thread_groups(conf, 0, &group_cnt, &new_group)) {\n\t\tconf->group_cnt = group_cnt;\n\t\tconf->worker_cnt_per_group = 0;\n\t\tconf->worker_groups = new_group;\n\t} else\n\t\tgoto abort;\n\tspin_lock_init(&conf->device_lock);\n\tseqcount_spinlock_init(&conf->gen_lock, &conf->device_lock);\n\tmutex_init(&conf->cache_size_mutex);\n\tinit_waitqueue_head(&conf->wait_for_quiescent);\n\tinit_waitqueue_head(&conf->wait_for_stripe);\n\tinit_waitqueue_head(&conf->wait_for_overlap);\n\tINIT_LIST_HEAD(&conf->handle_list);\n\tINIT_LIST_HEAD(&conf->loprio_list);\n\tINIT_LIST_HEAD(&conf->hold_list);\n\tINIT_LIST_HEAD(&conf->delayed_list);\n\tINIT_LIST_HEAD(&conf->bitmap_list);\n\tinit_llist_head(&conf->released_stripes);\n\tatomic_set(&conf->active_stripes, 0);\n\tatomic_set(&conf->preread_active_stripes, 0);\n\tatomic_set(&conf->active_aligned_reads, 0);\n\tspin_lock_init(&conf->pending_bios_lock);\n\tconf->batch_bio_dispatch = true;\n\trdev_for_each(rdev, mddev) {\n\t\tif (test_bit(Journal, &rdev->flags))\n\t\t\tcontinue;\n\t\tif (blk_queue_nonrot(bdev_get_queue(rdev->bdev))) {\n\t\t\tconf->batch_bio_dispatch = false;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tconf->bypass_threshold = BYPASS_THRESHOLD;\n\tconf->recovery_disabled = mddev->recovery_disabled - 1;\n\n\tconf->raid_disks = mddev->raid_disks;\n\tif (mddev->reshape_position == MaxSector)\n\t\tconf->previous_raid_disks = mddev->raid_disks;\n\telse\n\t\tconf->previous_raid_disks = mddev->raid_disks - mddev->delta_disks;\n\tmax_disks = max(conf->raid_disks, conf->previous_raid_disks);\n\n\tconf->disks = kcalloc(max_disks, sizeof(struct disk_info),\n\t\t\t      GFP_KERNEL);\n\n\tif (!conf->disks)\n\t\tgoto abort;\n\n\tfor (i = 0; i < max_disks; i++) {\n\t\tconf->disks[i].extra_page = alloc_page(GFP_KERNEL);\n\t\tif (!conf->disks[i].extra_page)\n\t\t\tgoto abort;\n\t}\n\n\tret = bioset_init(&conf->bio_split, BIO_POOL_SIZE, 0, 0);\n\tif (ret)\n\t\tgoto abort;\n\tconf->mddev = mddev;\n\n\tif ((conf->stripe_hashtbl = kzalloc(PAGE_SIZE, GFP_KERNEL)) == NULL)\n\t\tgoto abort;\n\n\t/* We init hash_locks[0] separately to that it can be used\n\t * as the reference lock in the spin_lock_nest_lock() call\n\t * in lock_all_device_hash_locks_irq in order to convince\n\t * lockdep that we know what we are doing.\n\t */\n\tspin_lock_init(conf->hash_locks);\n\tfor (i = 1; i < NR_STRIPE_HASH_LOCKS; i++)\n\t\tspin_lock_init(conf->hash_locks + i);\n\n\tfor (i = 0; i < NR_STRIPE_HASH_LOCKS; i++)\n\t\tINIT_LIST_HEAD(conf->inactive_list + i);\n\n\tfor (i = 0; i < NR_STRIPE_HASH_LOCKS; i++)\n\t\tINIT_LIST_HEAD(conf->temp_inactive_list + i);\n\n\tatomic_set(&conf->r5c_cached_full_stripes, 0);\n\tINIT_LIST_HEAD(&conf->r5c_full_stripe_list);\n\tatomic_set(&conf->r5c_cached_partial_stripes, 0);\n\tINIT_LIST_HEAD(&conf->r5c_partial_stripe_list);\n\tatomic_set(&conf->r5c_flushing_full_stripes, 0);\n\tatomic_set(&conf->r5c_flushing_partial_stripes, 0);\n\n\tconf->level = mddev->new_level;\n\tconf->chunk_sectors = mddev->new_chunk_sectors;\n\tif (raid5_alloc_percpu(conf) != 0)\n\t\tgoto abort;\n\n\tpr_debug(\"raid456: run(%s) called.\\n\", mdname(mddev));\n\n\trdev_for_each(rdev, mddev) {\n\t\traid_disk = rdev->raid_disk;\n\t\tif (raid_disk >= max_disks\n\t\t    || raid_disk < 0 || test_bit(Journal, &rdev->flags))\n\t\t\tcontinue;\n\t\tdisk = conf->disks + raid_disk;\n\n\t\tif (test_bit(Replacement, &rdev->flags)) {\n\t\t\tif (disk->replacement)\n\t\t\t\tgoto abort;\n\t\t\tdisk->replacement = rdev;\n\t\t} else {\n\t\t\tif (disk->rdev)\n\t\t\t\tgoto abort;\n\t\t\tdisk->rdev = rdev;\n\t\t}\n\n\t\tif (test_bit(In_sync, &rdev->flags)) {\n\t\t\tchar b[BDEVNAME_SIZE];\n\t\t\tpr_info(\"md/raid:%s: device %s operational as raid disk %d\\n\",\n\t\t\t\tmdname(mddev), bdevname(rdev->bdev, b), raid_disk);\n\t\t} else if (rdev->saved_raid_disk != raid_disk)\n\t\t\t/* Cannot rely on bitmap to complete recovery */\n\t\t\tconf->fullsync = 1;\n\t}\n\n\tconf->level = mddev->new_level;\n\tif (conf->level == 6) {\n\t\tconf->max_degraded = 2;\n\t\tif (raid6_call.xor_syndrome)\n\t\t\tconf->rmw_level = PARITY_ENABLE_RMW;\n\t\telse\n\t\t\tconf->rmw_level = PARITY_DISABLE_RMW;\n\t} else {\n\t\tconf->max_degraded = 1;\n\t\tconf->rmw_level = PARITY_ENABLE_RMW;\n\t}\n\tconf->algorithm = mddev->new_layout;\n\tconf->reshape_progress = mddev->reshape_position;\n\tif (conf->reshape_progress != MaxSector) {\n\t\tconf->prev_chunk_sectors = mddev->chunk_sectors;\n\t\tconf->prev_algo = mddev->layout;\n\t} else {\n\t\tconf->prev_chunk_sectors = conf->chunk_sectors;\n\t\tconf->prev_algo = conf->algorithm;\n\t}\n\n\tconf->min_nr_stripes = NR_STRIPES;\n\tif (mddev->reshape_position != MaxSector) {\n\t\tint stripes = max_t(int,\n\t\t\t((mddev->chunk_sectors << 9) / RAID5_STRIPE_SIZE(conf)) * 4,\n\t\t\t((mddev->new_chunk_sectors << 9) / RAID5_STRIPE_SIZE(conf)) * 4);\n\t\tconf->min_nr_stripes = max(NR_STRIPES, stripes);\n\t\tif (conf->min_nr_stripes != NR_STRIPES)\n\t\t\tpr_info(\"md/raid:%s: force stripe size %d for reshape\\n\",\n\t\t\t\tmdname(mddev), conf->min_nr_stripes);\n\t}\n\tmemory = conf->min_nr_stripes * (sizeof(struct stripe_head) +\n\t\t max_disks * ((sizeof(struct bio) + PAGE_SIZE))) / 1024;\n\tatomic_set(&conf->empty_inactive_list_nr, NR_STRIPE_HASH_LOCKS);\n\tif (grow_stripes(conf, conf->min_nr_stripes)) {\n\t\tpr_warn(\"md/raid:%s: couldn't allocate %dkB for buffers\\n\",\n\t\t\tmdname(mddev), memory);\n\t\tgoto abort;\n\t} else\n\t\tpr_debug(\"md/raid:%s: allocated %dkB\\n\", mdname(mddev), memory);\n\t/*\n\t * Losing a stripe head costs more than the time to refill it,\n\t * it reduces the queue depth and so can hurt throughput.\n\t * So set it rather large, scaled by number of devices.\n\t */\n\tconf->shrinker.seeks = DEFAULT_SEEKS * conf->raid_disks * 4;\n\tconf->shrinker.scan_objects = raid5_cache_scan;\n\tconf->shrinker.count_objects = raid5_cache_count;\n\tconf->shrinker.batch = 128;\n\tconf->shrinker.flags = 0;\n\tif (register_shrinker(&conf->shrinker)) {\n\t\tpr_warn(\"md/raid:%s: couldn't register shrinker.\\n\",\n\t\t\tmdname(mddev));\n\t\tgoto abort;\n\t}\n\n\tsprintf(pers_name, \"raid%d\", mddev->new_level);\n\tconf->thread = md_register_thread(raid5d, mddev, pers_name);\n\tif (!conf->thread) {\n\t\tpr_warn(\"md/raid:%s: couldn't allocate thread.\\n\",\n\t\t\tmdname(mddev));\n\t\tgoto abort;\n\t}\n\n\treturn conf;\n\n abort:\n\tif (conf) {\n\t\tfree_conf(conf);\n\t\treturn ERR_PTR(-EIO);\n\t} else\n\t\treturn ERR_PTR(-ENOMEM);\n}\n\nstatic int only_parity(int raid_disk, int algo, int raid_disks, int max_degraded)\n{\n\tswitch (algo) {\n\tcase ALGORITHM_PARITY_0:\n\t\tif (raid_disk < max_degraded)\n\t\t\treturn 1;\n\t\tbreak;\n\tcase ALGORITHM_PARITY_N:\n\t\tif (raid_disk >= raid_disks - max_degraded)\n\t\t\treturn 1;\n\t\tbreak;\n\tcase ALGORITHM_PARITY_0_6:\n\t\tif (raid_disk == 0 ||\n\t\t    raid_disk == raid_disks - 1)\n\t\t\treturn 1;\n\t\tbreak;\n\tcase ALGORITHM_LEFT_ASYMMETRIC_6:\n\tcase ALGORITHM_RIGHT_ASYMMETRIC_6:\n\tcase ALGORITHM_LEFT_SYMMETRIC_6:\n\tcase ALGORITHM_RIGHT_SYMMETRIC_6:\n\t\tif (raid_disk == raid_disks - 1)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic void raid5_set_io_opt(struct r5conf *conf)\n{\n\tblk_queue_io_opt(conf->mddev->queue, (conf->chunk_sectors << 9) *\n\t\t\t (conf->raid_disks - conf->max_degraded));\n}\n\nstatic int raid5_run(struct mddev *mddev)\n{\n\tstruct r5conf *conf;\n\tint working_disks = 0;\n\tint dirty_parity_disks = 0;\n\tstruct md_rdev *rdev;\n\tstruct md_rdev *journal_dev = NULL;\n\tsector_t reshape_offset = 0;\n\tint i;\n\tlong long min_offset_diff = 0;\n\tint first = 1;\n\n\tif (mddev_init_writes_pending(mddev) < 0)\n\t\treturn -ENOMEM;\n\n\tif (mddev->recovery_cp != MaxSector)\n\t\tpr_notice(\"md/raid:%s: not clean -- starting background reconstruction\\n\",\n\t\t\t  mdname(mddev));\n\n\trdev_for_each(rdev, mddev) {\n\t\tlong long diff;\n\n\t\tif (test_bit(Journal, &rdev->flags)) {\n\t\t\tjournal_dev = rdev;\n\t\t\tcontinue;\n\t\t}\n\t\tif (rdev->raid_disk < 0)\n\t\t\tcontinue;\n\t\tdiff = (rdev->new_data_offset - rdev->data_offset);\n\t\tif (first) {\n\t\t\tmin_offset_diff = diff;\n\t\t\tfirst = 0;\n\t\t} else if (mddev->reshape_backwards &&\n\t\t\t diff < min_offset_diff)\n\t\t\tmin_offset_diff = diff;\n\t\telse if (!mddev->reshape_backwards &&\n\t\t\t diff > min_offset_diff)\n\t\t\tmin_offset_diff = diff;\n\t}\n\n\tif ((test_bit(MD_HAS_JOURNAL, &mddev->flags) || journal_dev) &&\n\t    (mddev->bitmap_info.offset || mddev->bitmap_info.file)) {\n\t\tpr_notice(\"md/raid:%s: array cannot have both journal and bitmap\\n\",\n\t\t\t  mdname(mddev));\n\t\treturn -EINVAL;\n\t}\n\n\tif (mddev->reshape_position != MaxSector) {\n\t\t/* Check that we can continue the reshape.\n\t\t * Difficulties arise if the stripe we would write to\n\t\t * next is at or after the stripe we would read from next.\n\t\t * For a reshape that changes the number of devices, this\n\t\t * is only possible for a very short time, and mdadm makes\n\t\t * sure that time appears to have past before assembling\n\t\t * the array.  So we fail if that time hasn't passed.\n\t\t * For a reshape that keeps the number of devices the same\n\t\t * mdadm must be monitoring the reshape can keeping the\n\t\t * critical areas read-only and backed up.  It will start\n\t\t * the array in read-only mode, so we check for that.\n\t\t */\n\t\tsector_t here_new, here_old;\n\t\tint old_disks;\n\t\tint max_degraded = (mddev->level == 6 ? 2 : 1);\n\t\tint chunk_sectors;\n\t\tint new_data_disks;\n\n\t\tif (journal_dev) {\n\t\t\tpr_warn(\"md/raid:%s: don't support reshape with journal - aborting.\\n\",\n\t\t\t\tmdname(mddev));\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (mddev->new_level != mddev->level) {\n\t\t\tpr_warn(\"md/raid:%s: unsupported reshape required - aborting.\\n\",\n\t\t\t\tmdname(mddev));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\told_disks = mddev->raid_disks - mddev->delta_disks;\n\t\t/* reshape_position must be on a new-stripe boundary, and one\n\t\t * further up in new geometry must map after here in old\n\t\t * geometry.\n\t\t * If the chunk sizes are different, then as we perform reshape\n\t\t * in units of the largest of the two, reshape_position needs\n\t\t * be a multiple of the largest chunk size times new data disks.\n\t\t */\n\t\there_new = mddev->reshape_position;\n\t\tchunk_sectors = max(mddev->chunk_sectors, mddev->new_chunk_sectors);\n\t\tnew_data_disks = mddev->raid_disks - max_degraded;\n\t\tif (sector_div(here_new, chunk_sectors * new_data_disks)) {\n\t\t\tpr_warn(\"md/raid:%s: reshape_position not on a stripe boundary\\n\",\n\t\t\t\tmdname(mddev));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treshape_offset = here_new * chunk_sectors;\n\t\t/* here_new is the stripe we will write to */\n\t\there_old = mddev->reshape_position;\n\t\tsector_div(here_old, chunk_sectors * (old_disks-max_degraded));\n\t\t/* here_old is the first stripe that we might need to read\n\t\t * from */\n\t\tif (mddev->delta_disks == 0) {\n\t\t\t/* We cannot be sure it is safe to start an in-place\n\t\t\t * reshape.  It is only safe if user-space is monitoring\n\t\t\t * and taking constant backups.\n\t\t\t * mdadm always starts a situation like this in\n\t\t\t * readonly mode so it can take control before\n\t\t\t * allowing any writes.  So just check for that.\n\t\t\t */\n\t\t\tif (abs(min_offset_diff) >= mddev->chunk_sectors &&\n\t\t\t    abs(min_offset_diff) >= mddev->new_chunk_sectors)\n\t\t\t\t/* not really in-place - so OK */;\n\t\t\telse if (mddev->ro == 0) {\n\t\t\t\tpr_warn(\"md/raid:%s: in-place reshape must be started in read-only mode - aborting\\n\",\n\t\t\t\t\tmdname(mddev));\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else if (mddev->reshape_backwards\n\t\t    ? (here_new * chunk_sectors + min_offset_diff <=\n\t\t       here_old * chunk_sectors)\n\t\t    : (here_new * chunk_sectors >=\n\t\t       here_old * chunk_sectors + (-min_offset_diff))) {\n\t\t\t/* Reading from the same stripe as writing to - bad */\n\t\t\tpr_warn(\"md/raid:%s: reshape_position too early for auto-recovery - aborting.\\n\",\n\t\t\t\tmdname(mddev));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tpr_debug(\"md/raid:%s: reshape will continue\\n\", mdname(mddev));\n\t\t/* OK, we should be able to continue; */\n\t} else {\n\t\tBUG_ON(mddev->level != mddev->new_level);\n\t\tBUG_ON(mddev->layout != mddev->new_layout);\n\t\tBUG_ON(mddev->chunk_sectors != mddev->new_chunk_sectors);\n\t\tBUG_ON(mddev->delta_disks != 0);\n\t}\n\n\tif (test_bit(MD_HAS_JOURNAL, &mddev->flags) &&\n\t    test_bit(MD_HAS_PPL, &mddev->flags)) {\n\t\tpr_warn(\"md/raid:%s: using journal device and PPL not allowed - disabling PPL\\n\",\n\t\t\tmdname(mddev));\n\t\tclear_bit(MD_HAS_PPL, &mddev->flags);\n\t\tclear_bit(MD_HAS_MULTIPLE_PPLS, &mddev->flags);\n\t}\n\n\tif (mddev->private == NULL)\n\t\tconf = setup_conf(mddev);\n\telse\n\t\tconf = mddev->private;\n\n\tif (IS_ERR(conf))\n\t\treturn PTR_ERR(conf);\n\n\tif (test_bit(MD_HAS_JOURNAL, &mddev->flags)) {\n\t\tif (!journal_dev) {\n\t\t\tpr_warn(\"md/raid:%s: journal disk is missing, force array readonly\\n\",\n\t\t\t\tmdname(mddev));\n\t\t\tmddev->ro = 1;\n\t\t\tset_disk_ro(mddev->gendisk, 1);\n\t\t} else if (mddev->recovery_cp == MaxSector)\n\t\t\tset_bit(MD_JOURNAL_CLEAN, &mddev->flags);\n\t}\n\n\tconf->min_offset_diff = min_offset_diff;\n\tmddev->thread = conf->thread;\n\tconf->thread = NULL;\n\tmddev->private = conf;\n\n\tfor (i = 0; i < conf->raid_disks && conf->previous_raid_disks;\n\t     i++) {\n\t\trdev = conf->disks[i].rdev;\n\t\tif (!rdev && conf->disks[i].replacement) {\n\t\t\t/* The replacement is all we have yet */\n\t\t\trdev = conf->disks[i].replacement;\n\t\t\tconf->disks[i].replacement = NULL;\n\t\t\tclear_bit(Replacement, &rdev->flags);\n\t\t\tconf->disks[i].rdev = rdev;\n\t\t}\n\t\tif (!rdev)\n\t\t\tcontinue;\n\t\tif (conf->disks[i].replacement &&\n\t\t    conf->reshape_progress != MaxSector) {\n\t\t\t/* replacements and reshape simply do not mix. */\n\t\t\tpr_warn(\"md: cannot handle concurrent replacement and reshape.\\n\");\n\t\t\tgoto abort;\n\t\t}\n\t\tif (test_bit(In_sync, &rdev->flags)) {\n\t\t\tworking_disks++;\n\t\t\tcontinue;\n\t\t}\n\t\t/* This disc is not fully in-sync.  However if it\n\t\t * just stored parity (beyond the recovery_offset),\n\t\t * when we don't need to be concerned about the\n\t\t * array being dirty.\n\t\t * When reshape goes 'backwards', we never have\n\t\t * partially completed devices, so we only need\n\t\t * to worry about reshape going forwards.\n\t\t */\n\t\t/* Hack because v0.91 doesn't store recovery_offset properly. */\n\t\tif (mddev->major_version == 0 &&\n\t\t    mddev->minor_version > 90)\n\t\t\trdev->recovery_offset = reshape_offset;\n\n\t\tif (rdev->recovery_offset < reshape_offset) {\n\t\t\t/* We need to check old and new layout */\n\t\t\tif (!only_parity(rdev->raid_disk,\n\t\t\t\t\t conf->algorithm,\n\t\t\t\t\t conf->raid_disks,\n\t\t\t\t\t conf->max_degraded))\n\t\t\t\tcontinue;\n\t\t}\n\t\tif (!only_parity(rdev->raid_disk,\n\t\t\t\t conf->prev_algo,\n\t\t\t\t conf->previous_raid_disks,\n\t\t\t\t conf->max_degraded))\n\t\t\tcontinue;\n\t\tdirty_parity_disks++;\n\t}\n\n\t/*\n\t * 0 for a fully functional array, 1 or 2 for a degraded array.\n\t */\n\tmddev->degraded = raid5_calc_degraded(conf);\n\n\tif (has_failed(conf)) {\n\t\tpr_crit(\"md/raid:%s: not enough operational devices (%d/%d failed)\\n\",\n\t\t\tmdname(mddev), mddev->degraded, conf->raid_disks);\n\t\tgoto abort;\n\t}\n\n\t/* device size must be a multiple of chunk size */\n\tmddev->dev_sectors &= ~((sector_t)mddev->chunk_sectors - 1);\n\tmddev->resync_max_sectors = mddev->dev_sectors;\n\n\tif (mddev->degraded > dirty_parity_disks &&\n\t    mddev->recovery_cp != MaxSector) {\n\t\tif (test_bit(MD_HAS_PPL, &mddev->flags))\n\t\t\tpr_crit(\"md/raid:%s: starting dirty degraded array with PPL.\\n\",\n\t\t\t\tmdname(mddev));\n\t\telse if (mddev->ok_start_degraded)\n\t\t\tpr_crit(\"md/raid:%s: starting dirty degraded array - data corruption possible.\\n\",\n\t\t\t\tmdname(mddev));\n\t\telse {\n\t\t\tpr_crit(\"md/raid:%s: cannot start dirty degraded array.\\n\",\n\t\t\t\tmdname(mddev));\n\t\t\tgoto abort;\n\t\t}\n\t}\n\n\tpr_info(\"md/raid:%s: raid level %d active with %d out of %d devices, algorithm %d\\n\",\n\t\tmdname(mddev), conf->level,\n\t\tmddev->raid_disks-mddev->degraded, mddev->raid_disks,\n\t\tmddev->new_layout);\n\n\tprint_raid5_conf(conf);\n\n\tif (conf->reshape_progress != MaxSector) {\n\t\tconf->reshape_safe = conf->reshape_progress;\n\t\tatomic_set(&conf->reshape_stripes, 0);\n\t\tclear_bit(MD_RECOVERY_SYNC, &mddev->recovery);\n\t\tclear_bit(MD_RECOVERY_CHECK, &mddev->recovery);\n\t\tset_bit(MD_RECOVERY_RESHAPE, &mddev->recovery);\n\t\tset_bit(MD_RECOVERY_RUNNING, &mddev->recovery);\n\t\tmddev->sync_thread = md_register_thread(md_do_sync, mddev,\n\t\t\t\t\t\t\t\"reshape\");\n\t\tif (!mddev->sync_thread)\n\t\t\tgoto abort;\n\t}\n\n\t/* Ok, everything is just fine now */\n\tif (mddev->to_remove == &raid5_attrs_group)\n\t\tmddev->to_remove = NULL;\n\telse if (mddev->kobj.sd &&\n\t    sysfs_create_group(&mddev->kobj, &raid5_attrs_group))\n\t\tpr_warn(\"raid5: failed to create sysfs attributes for %s\\n\",\n\t\t\tmdname(mddev));\n\tmd_set_array_sectors(mddev, raid5_size(mddev, 0, 0));\n\n\tif (mddev->queue) {\n\t\tint chunk_size;\n\t\t/* read-ahead size must cover two whole stripes, which\n\t\t * is 2 * (datadisks) * chunksize where 'n' is the\n\t\t * number of raid devices\n\t\t */\n\t\tint data_disks = conf->previous_raid_disks - conf->max_degraded;\n\t\tint stripe = data_disks *\n\t\t\t((mddev->chunk_sectors << 9) / PAGE_SIZE);\n\n\t\tchunk_size = mddev->chunk_sectors << 9;\n\t\tblk_queue_io_min(mddev->queue, chunk_size);\n\t\traid5_set_io_opt(conf);\n\t\tmddev->queue->limits.raid_partial_stripes_expensive = 1;\n\t\t/*\n\t\t * We can only discard a whole stripe. It doesn't make sense to\n\t\t * discard data disk but write parity disk\n\t\t */\n\t\tstripe = stripe * PAGE_SIZE;\n\t\t/* Round up to power of 2, as discard handling\n\t\t * currently assumes that */\n\t\twhile ((stripe-1) & stripe)\n\t\t\tstripe = (stripe | (stripe-1)) + 1;\n\t\tmddev->queue->limits.discard_alignment = stripe;\n\t\tmddev->queue->limits.discard_granularity = stripe;\n\n\t\tblk_queue_max_write_same_sectors(mddev->queue, 0);\n\t\tblk_queue_max_write_zeroes_sectors(mddev->queue, 0);\n\n\t\trdev_for_each(rdev, mddev) {\n\t\t\tdisk_stack_limits(mddev->gendisk, rdev->bdev,\n\t\t\t\t\t  rdev->data_offset << 9);\n\t\t\tdisk_stack_limits(mddev->gendisk, rdev->bdev,\n\t\t\t\t\t  rdev->new_data_offset << 9);\n\t\t}\n\n\t\t/*\n\t\t * zeroing is required, otherwise data\n\t\t * could be lost. Consider a scenario: discard a stripe\n\t\t * (the stripe could be inconsistent if\n\t\t * discard_zeroes_data is 0); write one disk of the\n\t\t * stripe (the stripe could be inconsistent again\n\t\t * depending on which disks are used to calculate\n\t\t * parity); the disk is broken; The stripe data of this\n\t\t * disk is lost.\n\t\t *\n\t\t * We only allow DISCARD if the sysadmin has confirmed that\n\t\t * only safe devices are in use by setting a module parameter.\n\t\t * A better idea might be to turn DISCARD into WRITE_ZEROES\n\t\t * requests, as that is required to be safe.\n\t\t */\n\t\tif (devices_handle_discard_safely &&\n\t\t    mddev->queue->limits.max_discard_sectors >= (stripe >> 9) &&\n\t\t    mddev->queue->limits.discard_granularity >= stripe)\n\t\t\tblk_queue_flag_set(QUEUE_FLAG_DISCARD,\n\t\t\t\t\t\tmddev->queue);\n\t\telse\n\t\t\tblk_queue_flag_clear(QUEUE_FLAG_DISCARD,\n\t\t\t\t\t\tmddev->queue);\n\n\t\tblk_queue_max_hw_sectors(mddev->queue, UINT_MAX);\n\t}\n\n\tif (log_init(conf, journal_dev, raid5_has_ppl(conf)))\n\t\tgoto abort;\n\n\treturn 0;\nabort:\n\tmd_unregister_thread(&mddev->thread);\n\tprint_raid5_conf(conf);\n\tfree_conf(conf);\n\tmddev->private = NULL;\n\tpr_warn(\"md/raid:%s: failed to run raid set.\\n\", mdname(mddev));\n\treturn -EIO;\n}\n\nstatic void raid5_free(struct mddev *mddev, void *priv)\n{\n\tstruct r5conf *conf = priv;\n\n\tfree_conf(conf);\n\tmddev->to_remove = &raid5_attrs_group;\n}\n\nstatic void raid5_status(struct seq_file *seq, struct mddev *mddev)\n{\n\tstruct r5conf *conf = mddev->private;\n\tint i;\n\n\tseq_printf(seq, \" level %d, %dk chunk, algorithm %d\", mddev->level,\n\t\tconf->chunk_sectors / 2, mddev->layout);\n\tseq_printf (seq, \" [%d/%d] [\", conf->raid_disks, conf->raid_disks - mddev->degraded);\n\trcu_read_lock();\n\tfor (i = 0; i < conf->raid_disks; i++) {\n\t\tstruct md_rdev *rdev = rcu_dereference(conf->disks[i].rdev);\n\t\tseq_printf (seq, \"%s\", rdev && test_bit(In_sync, &rdev->flags) ? \"U\" : \"_\");\n\t}\n\trcu_read_unlock();\n\tseq_printf (seq, \"]\");\n}\n\nstatic void print_raid5_conf (struct r5conf *conf)\n{\n\tint i;\n\tstruct disk_info *tmp;\n\n\tpr_debug(\"RAID conf printout:\\n\");\n\tif (!conf) {\n\t\tpr_debug(\"(conf==NULL)\\n\");\n\t\treturn;\n\t}\n\tpr_debug(\" --- level:%d rd:%d wd:%d\\n\", conf->level,\n\t       conf->raid_disks,\n\t       conf->raid_disks - conf->mddev->degraded);\n\n\tfor (i = 0; i < conf->raid_disks; i++) {\n\t\tchar b[BDEVNAME_SIZE];\n\t\ttmp = conf->disks + i;\n\t\tif (tmp->rdev)\n\t\t\tpr_debug(\" disk %d, o:%d, dev:%s\\n\",\n\t\t\t       i, !test_bit(Faulty, &tmp->rdev->flags),\n\t\t\t       bdevname(tmp->rdev->bdev, b));\n\t}\n}\n\nstatic int raid5_spare_active(struct mddev *mddev)\n{\n\tint i;\n\tstruct r5conf *conf = mddev->private;\n\tstruct disk_info *tmp;\n\tint count = 0;\n\tunsigned long flags;\n\n\tfor (i = 0; i < conf->raid_disks; i++) {\n\t\ttmp = conf->disks + i;\n\t\tif (tmp->replacement\n\t\t    && tmp->replacement->recovery_offset == MaxSector\n\t\t    && !test_bit(Faulty, &tmp->replacement->flags)\n\t\t    && !test_and_set_bit(In_sync, &tmp->replacement->flags)) {\n\t\t\t/* Replacement has just become active. */\n\t\t\tif (!tmp->rdev\n\t\t\t    || !test_and_clear_bit(In_sync, &tmp->rdev->flags))\n\t\t\t\tcount++;\n\t\t\tif (tmp->rdev) {\n\t\t\t\t/* Replaced device not technically faulty,\n\t\t\t\t * but we need to be sure it gets removed\n\t\t\t\t * and never re-added.\n\t\t\t\t */\n\t\t\t\tset_bit(Faulty, &tmp->rdev->flags);\n\t\t\t\tsysfs_notify_dirent_safe(\n\t\t\t\t\ttmp->rdev->sysfs_state);\n\t\t\t}\n\t\t\tsysfs_notify_dirent_safe(tmp->replacement->sysfs_state);\n\t\t} else if (tmp->rdev\n\t\t    && tmp->rdev->recovery_offset == MaxSector\n\t\t    && !test_bit(Faulty, &tmp->rdev->flags)\n\t\t    && !test_and_set_bit(In_sync, &tmp->rdev->flags)) {\n\t\t\tcount++;\n\t\t\tsysfs_notify_dirent_safe(tmp->rdev->sysfs_state);\n\t\t}\n\t}\n\tspin_lock_irqsave(&conf->device_lock, flags);\n\tmddev->degraded = raid5_calc_degraded(conf);\n\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\tprint_raid5_conf(conf);\n\treturn count;\n}\n\nstatic int raid5_remove_disk(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tstruct r5conf *conf = mddev->private;\n\tint err = 0;\n\tint number = rdev->raid_disk;\n\tstruct md_rdev **rdevp;\n\tstruct disk_info *p = conf->disks + number;\n\n\tprint_raid5_conf(conf);\n\tif (test_bit(Journal, &rdev->flags) && conf->log) {\n\t\t/*\n\t\t * we can't wait pending write here, as this is called in\n\t\t * raid5d, wait will deadlock.\n\t\t * neilb: there is no locking about new writes here,\n\t\t * so this cannot be safe.\n\t\t */\n\t\tif (atomic_read(&conf->active_stripes) ||\n\t\t    atomic_read(&conf->r5c_cached_full_stripes) ||\n\t\t    atomic_read(&conf->r5c_cached_partial_stripes)) {\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tlog_exit(conf);\n\t\treturn 0;\n\t}\n\tif (rdev == p->rdev)\n\t\trdevp = &p->rdev;\n\telse if (rdev == p->replacement)\n\t\trdevp = &p->replacement;\n\telse\n\t\treturn 0;\n\n\tif (number >= conf->raid_disks &&\n\t    conf->reshape_progress == MaxSector)\n\t\tclear_bit(In_sync, &rdev->flags);\n\n\tif (test_bit(In_sync, &rdev->flags) ||\n\t    atomic_read(&rdev->nr_pending)) {\n\t\terr = -EBUSY;\n\t\tgoto abort;\n\t}\n\t/* Only remove non-faulty devices if recovery\n\t * isn't possible.\n\t */\n\tif (!test_bit(Faulty, &rdev->flags) &&\n\t    mddev->recovery_disabled != conf->recovery_disabled &&\n\t    !has_failed(conf) &&\n\t    (!p->replacement || p->replacement == rdev) &&\n\t    number < conf->raid_disks) {\n\t\terr = -EBUSY;\n\t\tgoto abort;\n\t}\n\t*rdevp = NULL;\n\tif (!test_bit(RemoveSynchronized, &rdev->flags)) {\n\t\tsynchronize_rcu();\n\t\tif (atomic_read(&rdev->nr_pending)) {\n\t\t\t/* lost the race, try later */\n\t\t\terr = -EBUSY;\n\t\t\t*rdevp = rdev;\n\t\t}\n\t}\n\tif (!err) {\n\t\terr = log_modify(conf, rdev, false);\n\t\tif (err)\n\t\t\tgoto abort;\n\t}\n\tif (p->replacement) {\n\t\t/* We must have just cleared 'rdev' */\n\t\tp->rdev = p->replacement;\n\t\tclear_bit(Replacement, &p->replacement->flags);\n\t\tsmp_mb(); /* Make sure other CPUs may see both as identical\n\t\t\t   * but will never see neither - if they are careful\n\t\t\t   */\n\t\tp->replacement = NULL;\n\n\t\tif (!err)\n\t\t\terr = log_modify(conf, p->rdev, true);\n\t}\n\n\tclear_bit(WantReplacement, &rdev->flags);\nabort:\n\n\tprint_raid5_conf(conf);\n\treturn err;\n}\n\nstatic int raid5_add_disk(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tstruct r5conf *conf = mddev->private;\n\tint ret, err = -EEXIST;\n\tint disk;\n\tstruct disk_info *p;\n\tint first = 0;\n\tint last = conf->raid_disks - 1;\n\n\tif (test_bit(Journal, &rdev->flags)) {\n\t\tif (conf->log)\n\t\t\treturn -EBUSY;\n\n\t\trdev->raid_disk = 0;\n\t\t/*\n\t\t * The array is in readonly mode if journal is missing, so no\n\t\t * write requests running. We should be safe\n\t\t */\n\t\tret = log_init(conf, rdev, false);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = r5l_start(conf->log);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\treturn 0;\n\t}\n\tif (mddev->recovery_disabled == conf->recovery_disabled)\n\t\treturn -EBUSY;\n\n\tif (rdev->saved_raid_disk < 0 && has_failed(conf))\n\t\t/* no point adding a device */\n\t\treturn -EINVAL;\n\n\tif (rdev->raid_disk >= 0)\n\t\tfirst = last = rdev->raid_disk;\n\n\t/*\n\t * find the disk ... but prefer rdev->saved_raid_disk\n\t * if possible.\n\t */\n\tif (rdev->saved_raid_disk >= 0 &&\n\t    rdev->saved_raid_disk >= first &&\n\t    conf->disks[rdev->saved_raid_disk].rdev == NULL)\n\t\tfirst = rdev->saved_raid_disk;\n\n\tfor (disk = first; disk <= last; disk++) {\n\t\tp = conf->disks + disk;\n\t\tif (p->rdev == NULL) {\n\t\t\tclear_bit(In_sync, &rdev->flags);\n\t\t\trdev->raid_disk = disk;\n\t\t\tif (rdev->saved_raid_disk != disk)\n\t\t\t\tconf->fullsync = 1;\n\t\t\trcu_assign_pointer(p->rdev, rdev);\n\n\t\t\terr = log_modify(conf, rdev, true);\n\n\t\t\tgoto out;\n\t\t}\n\t}\n\tfor (disk = first; disk <= last; disk++) {\n\t\tp = conf->disks + disk;\n\t\tif (test_bit(WantReplacement, &p->rdev->flags) &&\n\t\t    p->replacement == NULL) {\n\t\t\tclear_bit(In_sync, &rdev->flags);\n\t\t\tset_bit(Replacement, &rdev->flags);\n\t\t\trdev->raid_disk = disk;\n\t\t\terr = 0;\n\t\t\tconf->fullsync = 1;\n\t\t\trcu_assign_pointer(p->replacement, rdev);\n\t\t\tbreak;\n\t\t}\n\t}\nout:\n\tprint_raid5_conf(conf);\n\treturn err;\n}\n\nstatic int raid5_resize(struct mddev *mddev, sector_t sectors)\n{\n\t/* no resync is happening, and there is enough space\n\t * on all devices, so we can resize.\n\t * We need to make sure resync covers any new space.\n\t * If the array is shrinking we should possibly wait until\n\t * any io in the removed space completes, but it hardly seems\n\t * worth it.\n\t */\n\tsector_t newsize;\n\tstruct r5conf *conf = mddev->private;\n\n\tif (raid5_has_log(conf) || raid5_has_ppl(conf))\n\t\treturn -EINVAL;\n\tsectors &= ~((sector_t)conf->chunk_sectors - 1);\n\tnewsize = raid5_size(mddev, sectors, mddev->raid_disks);\n\tif (mddev->external_size &&\n\t    mddev->array_sectors > newsize)\n\t\treturn -EINVAL;\n\tif (mddev->bitmap) {\n\t\tint ret = md_bitmap_resize(mddev->bitmap, sectors, 0, 0);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\tmd_set_array_sectors(mddev, newsize);\n\tif (sectors > mddev->dev_sectors &&\n\t    mddev->recovery_cp > mddev->dev_sectors) {\n\t\tmddev->recovery_cp = mddev->dev_sectors;\n\t\tset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\t}\n\tmddev->dev_sectors = sectors;\n\tmddev->resync_max_sectors = sectors;\n\treturn 0;\n}\n\nstatic int check_stripe_cache(struct mddev *mddev)\n{\n\t/* Can only proceed if there are plenty of stripe_heads.\n\t * We need a minimum of one full stripe,, and for sensible progress\n\t * it is best to have about 4 times that.\n\t * If we require 4 times, then the default 256 4K stripe_heads will\n\t * allow for chunk sizes up to 256K, which is probably OK.\n\t * If the chunk size is greater, user-space should request more\n\t * stripe_heads first.\n\t */\n\tstruct r5conf *conf = mddev->private;\n\tif (((mddev->chunk_sectors << 9) / RAID5_STRIPE_SIZE(conf)) * 4\n\t    > conf->min_nr_stripes ||\n\t    ((mddev->new_chunk_sectors << 9) / RAID5_STRIPE_SIZE(conf)) * 4\n\t    > conf->min_nr_stripes) {\n\t\tpr_warn(\"md/raid:%s: reshape: not enough stripes.  Needed %lu\\n\",\n\t\t\tmdname(mddev),\n\t\t\t((max(mddev->chunk_sectors, mddev->new_chunk_sectors) << 9)\n\t\t\t / RAID5_STRIPE_SIZE(conf))*4);\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\nstatic int check_reshape(struct mddev *mddev)\n{\n\tstruct r5conf *conf = mddev->private;\n\n\tif (raid5_has_log(conf) || raid5_has_ppl(conf))\n\t\treturn -EINVAL;\n\tif (mddev->delta_disks == 0 &&\n\t    mddev->new_layout == mddev->layout &&\n\t    mddev->new_chunk_sectors == mddev->chunk_sectors)\n\t\treturn 0; /* nothing to do */\n\tif (has_failed(conf))\n\t\treturn -EINVAL;\n\tif (mddev->delta_disks < 0 && mddev->reshape_position == MaxSector) {\n\t\t/* We might be able to shrink, but the devices must\n\t\t * be made bigger first.\n\t\t * For raid6, 4 is the minimum size.\n\t\t * Otherwise 2 is the minimum\n\t\t */\n\t\tint min = 2;\n\t\tif (mddev->level == 6)\n\t\t\tmin = 4;\n\t\tif (mddev->raid_disks + mddev->delta_disks < min)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!check_stripe_cache(mddev))\n\t\treturn -ENOSPC;\n\n\tif (mddev->new_chunk_sectors > mddev->chunk_sectors ||\n\t    mddev->delta_disks > 0)\n\t\tif (resize_chunks(conf,\n\t\t\t\t  conf->previous_raid_disks\n\t\t\t\t  + max(0, mddev->delta_disks),\n\t\t\t\t  max(mddev->new_chunk_sectors,\n\t\t\t\t      mddev->chunk_sectors)\n\t\t\t    ) < 0)\n\t\t\treturn -ENOMEM;\n\n\tif (conf->previous_raid_disks + mddev->delta_disks <= conf->pool_size)\n\t\treturn 0; /* never bother to shrink */\n\treturn resize_stripes(conf, (conf->previous_raid_disks\n\t\t\t\t     + mddev->delta_disks));\n}\n\nstatic int raid5_start_reshape(struct mddev *mddev)\n{\n\tstruct r5conf *conf = mddev->private;\n\tstruct md_rdev *rdev;\n\tint spares = 0;\n\tunsigned long flags;\n\n\tif (test_bit(MD_RECOVERY_RUNNING, &mddev->recovery))\n\t\treturn -EBUSY;\n\n\tif (!check_stripe_cache(mddev))\n\t\treturn -ENOSPC;\n\n\tif (has_failed(conf))\n\t\treturn -EINVAL;\n\n\trdev_for_each(rdev, mddev) {\n\t\tif (!test_bit(In_sync, &rdev->flags)\n\t\t    && !test_bit(Faulty, &rdev->flags))\n\t\t\tspares++;\n\t}\n\n\tif (spares - mddev->degraded < mddev->delta_disks - conf->max_degraded)\n\t\t/* Not enough devices even to make a degraded array\n\t\t * of that size\n\t\t */\n\t\treturn -EINVAL;\n\n\t/* Refuse to reduce size of the array.  Any reductions in\n\t * array size must be through explicit setting of array_size\n\t * attribute.\n\t */\n\tif (raid5_size(mddev, 0, conf->raid_disks + mddev->delta_disks)\n\t    < mddev->array_sectors) {\n\t\tpr_warn(\"md/raid:%s: array size must be reduced before number of disks\\n\",\n\t\t\tmdname(mddev));\n\t\treturn -EINVAL;\n\t}\n\n\tatomic_set(&conf->reshape_stripes, 0);\n\tspin_lock_irq(&conf->device_lock);\n\twrite_seqcount_begin(&conf->gen_lock);\n\tconf->previous_raid_disks = conf->raid_disks;\n\tconf->raid_disks += mddev->delta_disks;\n\tconf->prev_chunk_sectors = conf->chunk_sectors;\n\tconf->chunk_sectors = mddev->new_chunk_sectors;\n\tconf->prev_algo = conf->algorithm;\n\tconf->algorithm = mddev->new_layout;\n\tconf->generation++;\n\t/* Code that selects data_offset needs to see the generation update\n\t * if reshape_progress has been set - so a memory barrier needed.\n\t */\n\tsmp_mb();\n\tif (mddev->reshape_backwards)\n\t\tconf->reshape_progress = raid5_size(mddev, 0, 0);\n\telse\n\t\tconf->reshape_progress = 0;\n\tconf->reshape_safe = conf->reshape_progress;\n\twrite_seqcount_end(&conf->gen_lock);\n\tspin_unlock_irq(&conf->device_lock);\n\n\t/* Now make sure any requests that proceeded on the assumption\n\t * the reshape wasn't running - like Discard or Read - have\n\t * completed.\n\t */\n\tmddev_suspend(mddev);\n\tmddev_resume(mddev);\n\n\t/* Add some new drives, as many as will fit.\n\t * We know there are enough to make the newly sized array work.\n\t * Don't add devices if we are reducing the number of\n\t * devices in the array.  This is because it is not possible\n\t * to correctly record the \"partially reconstructed\" state of\n\t * such devices during the reshape and confusion could result.\n\t */\n\tif (mddev->delta_disks >= 0) {\n\t\trdev_for_each(rdev, mddev)\n\t\t\tif (rdev->raid_disk < 0 &&\n\t\t\t    !test_bit(Faulty, &rdev->flags)) {\n\t\t\t\tif (raid5_add_disk(mddev, rdev) == 0) {\n\t\t\t\t\tif (rdev->raid_disk\n\t\t\t\t\t    >= conf->previous_raid_disks)\n\t\t\t\t\t\tset_bit(In_sync, &rdev->flags);\n\t\t\t\t\telse\n\t\t\t\t\t\trdev->recovery_offset = 0;\n\n\t\t\t\t\t/* Failure here is OK */\n\t\t\t\t\tsysfs_link_rdev(mddev, rdev);\n\t\t\t\t}\n\t\t\t} else if (rdev->raid_disk >= conf->previous_raid_disks\n\t\t\t\t   && !test_bit(Faulty, &rdev->flags)) {\n\t\t\t\t/* This is a spare that was manually added */\n\t\t\t\tset_bit(In_sync, &rdev->flags);\n\t\t\t}\n\n\t\t/* When a reshape changes the number of devices,\n\t\t * ->degraded is measured against the larger of the\n\t\t * pre and post number of devices.\n\t\t */\n\t\tspin_lock_irqsave(&conf->device_lock, flags);\n\t\tmddev->degraded = raid5_calc_degraded(conf);\n\t\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\t}\n\tmddev->raid_disks = conf->raid_disks;\n\tmddev->reshape_position = conf->reshape_progress;\n\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\n\tclear_bit(MD_RECOVERY_SYNC, &mddev->recovery);\n\tclear_bit(MD_RECOVERY_CHECK, &mddev->recovery);\n\tclear_bit(MD_RECOVERY_DONE, &mddev->recovery);\n\tset_bit(MD_RECOVERY_RESHAPE, &mddev->recovery);\n\tset_bit(MD_RECOVERY_RUNNING, &mddev->recovery);\n\tmddev->sync_thread = md_register_thread(md_do_sync, mddev,\n\t\t\t\t\t\t\"reshape\");\n\tif (!mddev->sync_thread) {\n\t\tmddev->recovery = 0;\n\t\tspin_lock_irq(&conf->device_lock);\n\t\twrite_seqcount_begin(&conf->gen_lock);\n\t\tmddev->raid_disks = conf->raid_disks = conf->previous_raid_disks;\n\t\tmddev->new_chunk_sectors =\n\t\t\tconf->chunk_sectors = conf->prev_chunk_sectors;\n\t\tmddev->new_layout = conf->algorithm = conf->prev_algo;\n\t\trdev_for_each(rdev, mddev)\n\t\t\trdev->new_data_offset = rdev->data_offset;\n\t\tsmp_wmb();\n\t\tconf->generation --;\n\t\tconf->reshape_progress = MaxSector;\n\t\tmddev->reshape_position = MaxSector;\n\t\twrite_seqcount_end(&conf->gen_lock);\n\t\tspin_unlock_irq(&conf->device_lock);\n\t\treturn -EAGAIN;\n\t}\n\tconf->reshape_checkpoint = jiffies;\n\tmd_wakeup_thread(mddev->sync_thread);\n\tmd_new_event(mddev);\n\treturn 0;\n}\n\n/* This is called from the reshape thread and should make any\n * changes needed in 'conf'\n */\nstatic void end_reshape(struct r5conf *conf)\n{\n\n\tif (!test_bit(MD_RECOVERY_INTR, &conf->mddev->recovery)) {\n\t\tstruct md_rdev *rdev;\n\n\t\tspin_lock_irq(&conf->device_lock);\n\t\tconf->previous_raid_disks = conf->raid_disks;\n\t\tmd_finish_reshape(conf->mddev);\n\t\tsmp_wmb();\n\t\tconf->reshape_progress = MaxSector;\n\t\tconf->mddev->reshape_position = MaxSector;\n\t\trdev_for_each(rdev, conf->mddev)\n\t\t\tif (rdev->raid_disk >= 0 &&\n\t\t\t    !test_bit(Journal, &rdev->flags) &&\n\t\t\t    !test_bit(In_sync, &rdev->flags))\n\t\t\t\trdev->recovery_offset = MaxSector;\n\t\tspin_unlock_irq(&conf->device_lock);\n\t\twake_up(&conf->wait_for_overlap);\n\n\t\tif (conf->mddev->queue)\n\t\t\traid5_set_io_opt(conf);\n\t}\n}\n\n/* This is called from the raid5d thread with mddev_lock held.\n * It makes config changes to the device.\n */\nstatic void raid5_finish_reshape(struct mddev *mddev)\n{\n\tstruct r5conf *conf = mddev->private;\n\n\tif (!test_bit(MD_RECOVERY_INTR, &mddev->recovery)) {\n\n\t\tif (mddev->delta_disks <= 0) {\n\t\t\tint d;\n\t\t\tspin_lock_irq(&conf->device_lock);\n\t\t\tmddev->degraded = raid5_calc_degraded(conf);\n\t\t\tspin_unlock_irq(&conf->device_lock);\n\t\t\tfor (d = conf->raid_disks ;\n\t\t\t     d < conf->raid_disks - mddev->delta_disks;\n\t\t\t     d++) {\n\t\t\t\tstruct md_rdev *rdev = conf->disks[d].rdev;\n\t\t\t\tif (rdev)\n\t\t\t\t\tclear_bit(In_sync, &rdev->flags);\n\t\t\t\trdev = conf->disks[d].replacement;\n\t\t\t\tif (rdev)\n\t\t\t\t\tclear_bit(In_sync, &rdev->flags);\n\t\t\t}\n\t\t}\n\t\tmddev->layout = conf->algorithm;\n\t\tmddev->chunk_sectors = conf->chunk_sectors;\n\t\tmddev->reshape_position = MaxSector;\n\t\tmddev->delta_disks = 0;\n\t\tmddev->reshape_backwards = 0;\n\t}\n}\n\nstatic void raid5_quiesce(struct mddev *mddev, int quiesce)\n{\n\tstruct r5conf *conf = mddev->private;\n\n\tif (quiesce) {\n\t\t/* stop all writes */\n\t\tlock_all_device_hash_locks_irq(conf);\n\t\t/* '2' tells resync/reshape to pause so that all\n\t\t * active stripes can drain\n\t\t */\n\t\tr5c_flush_cache(conf, INT_MAX);\n\t\tconf->quiesce = 2;\n\t\twait_event_cmd(conf->wait_for_quiescent,\n\t\t\t\t    atomic_read(&conf->active_stripes) == 0 &&\n\t\t\t\t    atomic_read(&conf->active_aligned_reads) == 0,\n\t\t\t\t    unlock_all_device_hash_locks_irq(conf),\n\t\t\t\t    lock_all_device_hash_locks_irq(conf));\n\t\tconf->quiesce = 1;\n\t\tunlock_all_device_hash_locks_irq(conf);\n\t\t/* allow reshape to continue */\n\t\twake_up(&conf->wait_for_overlap);\n\t} else {\n\t\t/* re-enable writes */\n\t\tlock_all_device_hash_locks_irq(conf);\n\t\tconf->quiesce = 0;\n\t\twake_up(&conf->wait_for_quiescent);\n\t\twake_up(&conf->wait_for_overlap);\n\t\tunlock_all_device_hash_locks_irq(conf);\n\t}\n\tlog_quiesce(conf, quiesce);\n}\n\nstatic void *raid45_takeover_raid0(struct mddev *mddev, int level)\n{\n\tstruct r0conf *raid0_conf = mddev->private;\n\tsector_t sectors;\n\n\t/* for raid0 takeover only one zone is supported */\n\tif (raid0_conf->nr_strip_zones > 1) {\n\t\tpr_warn(\"md/raid:%s: cannot takeover raid0 with more than one zone.\\n\",\n\t\t\tmdname(mddev));\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tsectors = raid0_conf->strip_zone[0].zone_end;\n\tsector_div(sectors, raid0_conf->strip_zone[0].nb_dev);\n\tmddev->dev_sectors = sectors;\n\tmddev->new_level = level;\n\tmddev->new_layout = ALGORITHM_PARITY_N;\n\tmddev->new_chunk_sectors = mddev->chunk_sectors;\n\tmddev->raid_disks += 1;\n\tmddev->delta_disks = 1;\n\t/* make sure it will be not marked as dirty */\n\tmddev->recovery_cp = MaxSector;\n\n\treturn setup_conf(mddev);\n}\n\nstatic void *raid5_takeover_raid1(struct mddev *mddev)\n{\n\tint chunksect;\n\tvoid *ret;\n\n\tif (mddev->raid_disks != 2 ||\n\t    mddev->degraded > 1)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/* Should check if there are write-behind devices? */\n\n\tchunksect = 64*2; /* 64K by default */\n\n\t/* The array must be an exact multiple of chunksize */\n\twhile (chunksect && (mddev->array_sectors & (chunksect-1)))\n\t\tchunksect >>= 1;\n\n\tif ((chunksect<<9) < RAID5_STRIPE_SIZE((struct r5conf *)mddev->private))\n\t\t/* array size does not allow a suitable chunk size */\n\t\treturn ERR_PTR(-EINVAL);\n\n\tmddev->new_level = 5;\n\tmddev->new_layout = ALGORITHM_LEFT_SYMMETRIC;\n\tmddev->new_chunk_sectors = chunksect;\n\n\tret = setup_conf(mddev);\n\tif (!IS_ERR(ret))\n\t\tmddev_clear_unsupported_flags(mddev,\n\t\t\tUNSUPPORTED_MDDEV_FLAGS);\n\treturn ret;\n}\n\nstatic void *raid5_takeover_raid6(struct mddev *mddev)\n{\n\tint new_layout;\n\n\tswitch (mddev->layout) {\n\tcase ALGORITHM_LEFT_ASYMMETRIC_6:\n\t\tnew_layout = ALGORITHM_LEFT_ASYMMETRIC;\n\t\tbreak;\n\tcase ALGORITHM_RIGHT_ASYMMETRIC_6:\n\t\tnew_layout = ALGORITHM_RIGHT_ASYMMETRIC;\n\t\tbreak;\n\tcase ALGORITHM_LEFT_SYMMETRIC_6:\n\t\tnew_layout = ALGORITHM_LEFT_SYMMETRIC;\n\t\tbreak;\n\tcase ALGORITHM_RIGHT_SYMMETRIC_6:\n\t\tnew_layout = ALGORITHM_RIGHT_SYMMETRIC;\n\t\tbreak;\n\tcase ALGORITHM_PARITY_0_6:\n\t\tnew_layout = ALGORITHM_PARITY_0;\n\t\tbreak;\n\tcase ALGORITHM_PARITY_N:\n\t\tnew_layout = ALGORITHM_PARITY_N;\n\t\tbreak;\n\tdefault:\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tmddev->new_level = 5;\n\tmddev->new_layout = new_layout;\n\tmddev->delta_disks = -1;\n\tmddev->raid_disks -= 1;\n\treturn setup_conf(mddev);\n}\n\nstatic int raid5_check_reshape(struct mddev *mddev)\n{\n\t/* For a 2-drive array, the layout and chunk size can be changed\n\t * immediately as not restriping is needed.\n\t * For larger arrays we record the new value - after validation\n\t * to be used by a reshape pass.\n\t */\n\tstruct r5conf *conf = mddev->private;\n\tint new_chunk = mddev->new_chunk_sectors;\n\n\tif (mddev->new_layout >= 0 && !algorithm_valid_raid5(mddev->new_layout))\n\t\treturn -EINVAL;\n\tif (new_chunk > 0) {\n\t\tif (!is_power_of_2(new_chunk))\n\t\t\treturn -EINVAL;\n\t\tif (new_chunk < (PAGE_SIZE>>9))\n\t\t\treturn -EINVAL;\n\t\tif (mddev->array_sectors & (new_chunk-1))\n\t\t\t/* not factor of array size */\n\t\t\treturn -EINVAL;\n\t}\n\n\t/* They look valid */\n\n\tif (mddev->raid_disks == 2) {\n\t\t/* can make the change immediately */\n\t\tif (mddev->new_layout >= 0) {\n\t\t\tconf->algorithm = mddev->new_layout;\n\t\t\tmddev->layout = mddev->new_layout;\n\t\t}\n\t\tif (new_chunk > 0) {\n\t\t\tconf->chunk_sectors = new_chunk ;\n\t\t\tmddev->chunk_sectors = new_chunk;\n\t\t}\n\t\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\t\tmd_wakeup_thread(mddev->thread);\n\t}\n\treturn check_reshape(mddev);\n}\n\nstatic int raid6_check_reshape(struct mddev *mddev)\n{\n\tint new_chunk = mddev->new_chunk_sectors;\n\n\tif (mddev->new_layout >= 0 && !algorithm_valid_raid6(mddev->new_layout))\n\t\treturn -EINVAL;\n\tif (new_chunk > 0) {\n\t\tif (!is_power_of_2(new_chunk))\n\t\t\treturn -EINVAL;\n\t\tif (new_chunk < (PAGE_SIZE >> 9))\n\t\t\treturn -EINVAL;\n\t\tif (mddev->array_sectors & (new_chunk-1))\n\t\t\t/* not factor of array size */\n\t\t\treturn -EINVAL;\n\t}\n\n\t/* They look valid */\n\treturn check_reshape(mddev);\n}\n\nstatic void *raid5_takeover(struct mddev *mddev)\n{\n\t/* raid5 can take over:\n\t *  raid0 - if there is only one strip zone - make it a raid4 layout\n\t *  raid1 - if there are two drives.  We need to know the chunk size\n\t *  raid4 - trivial - just use a raid4 layout.\n\t *  raid6 - Providing it is a *_6 layout\n\t */\n\tif (mddev->level == 0)\n\t\treturn raid45_takeover_raid0(mddev, 5);\n\tif (mddev->level == 1)\n\t\treturn raid5_takeover_raid1(mddev);\n\tif (mddev->level == 4) {\n\t\tmddev->new_layout = ALGORITHM_PARITY_N;\n\t\tmddev->new_level = 5;\n\t\treturn setup_conf(mddev);\n\t}\n\tif (mddev->level == 6)\n\t\treturn raid5_takeover_raid6(mddev);\n\n\treturn ERR_PTR(-EINVAL);\n}\n\nstatic void *raid4_takeover(struct mddev *mddev)\n{\n\t/* raid4 can take over:\n\t *  raid0 - if there is only one strip zone\n\t *  raid5 - if layout is right\n\t */\n\tif (mddev->level == 0)\n\t\treturn raid45_takeover_raid0(mddev, 4);\n\tif (mddev->level == 5 &&\n\t    mddev->layout == ALGORITHM_PARITY_N) {\n\t\tmddev->new_layout = 0;\n\t\tmddev->new_level = 4;\n\t\treturn setup_conf(mddev);\n\t}\n\treturn ERR_PTR(-EINVAL);\n}\n\nstatic struct md_personality raid5_personality;\n\nstatic void *raid6_takeover(struct mddev *mddev)\n{\n\t/* Currently can only take over a raid5.  We map the\n\t * personality to an equivalent raid6 personality\n\t * with the Q block at the end.\n\t */\n\tint new_layout;\n\n\tif (mddev->pers != &raid5_personality)\n\t\treturn ERR_PTR(-EINVAL);\n\tif (mddev->degraded > 1)\n\t\treturn ERR_PTR(-EINVAL);\n\tif (mddev->raid_disks > 253)\n\t\treturn ERR_PTR(-EINVAL);\n\tif (mddev->raid_disks < 3)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tswitch (mddev->layout) {\n\tcase ALGORITHM_LEFT_ASYMMETRIC:\n\t\tnew_layout = ALGORITHM_LEFT_ASYMMETRIC_6;\n\t\tbreak;\n\tcase ALGORITHM_RIGHT_ASYMMETRIC:\n\t\tnew_layout = ALGORITHM_RIGHT_ASYMMETRIC_6;\n\t\tbreak;\n\tcase ALGORITHM_LEFT_SYMMETRIC:\n\t\tnew_layout = ALGORITHM_LEFT_SYMMETRIC_6;\n\t\tbreak;\n\tcase ALGORITHM_RIGHT_SYMMETRIC:\n\t\tnew_layout = ALGORITHM_RIGHT_SYMMETRIC_6;\n\t\tbreak;\n\tcase ALGORITHM_PARITY_0:\n\t\tnew_layout = ALGORITHM_PARITY_0_6;\n\t\tbreak;\n\tcase ALGORITHM_PARITY_N:\n\t\tnew_layout = ALGORITHM_PARITY_N;\n\t\tbreak;\n\tdefault:\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tmddev->new_level = 6;\n\tmddev->new_layout = new_layout;\n\tmddev->delta_disks = 1;\n\tmddev->raid_disks += 1;\n\treturn setup_conf(mddev);\n}\n\nstatic int raid5_change_consistency_policy(struct mddev *mddev, const char *buf)\n{\n\tstruct r5conf *conf;\n\tint err;\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\tconf = mddev->private;\n\tif (!conf) {\n\t\tmddev_unlock(mddev);\n\t\treturn -ENODEV;\n\t}\n\n\tif (strncmp(buf, \"ppl\", 3) == 0) {\n\t\t/* ppl only works with RAID 5 */\n\t\tif (!raid5_has_ppl(conf) && conf->level == 5) {\n\t\t\terr = log_init(conf, NULL, true);\n\t\t\tif (!err) {\n\t\t\t\terr = resize_stripes(conf, conf->pool_size);\n\t\t\t\tif (err)\n\t\t\t\t\tlog_exit(conf);\n\t\t\t}\n\t\t} else\n\t\t\terr = -EINVAL;\n\t} else if (strncmp(buf, \"resync\", 6) == 0) {\n\t\tif (raid5_has_ppl(conf)) {\n\t\t\tmddev_suspend(mddev);\n\t\t\tlog_exit(conf);\n\t\t\tmddev_resume(mddev);\n\t\t\terr = resize_stripes(conf, conf->pool_size);\n\t\t} else if (test_bit(MD_HAS_JOURNAL, &conf->mddev->flags) &&\n\t\t\t   r5l_log_disk_error(conf)) {\n\t\t\tbool journal_dev_exists = false;\n\t\t\tstruct md_rdev *rdev;\n\n\t\t\trdev_for_each(rdev, mddev)\n\t\t\t\tif (test_bit(Journal, &rdev->flags)) {\n\t\t\t\t\tjournal_dev_exists = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\tif (!journal_dev_exists) {\n\t\t\t\tmddev_suspend(mddev);\n\t\t\t\tclear_bit(MD_HAS_JOURNAL, &mddev->flags);\n\t\t\t\tmddev_resume(mddev);\n\t\t\t} else  /* need remove journal device first */\n\t\t\t\terr = -EBUSY;\n\t\t} else\n\t\t\terr = -EINVAL;\n\t} else {\n\t\terr = -EINVAL;\n\t}\n\n\tif (!err)\n\t\tmd_update_sb(mddev, 1);\n\n\tmddev_unlock(mddev);\n\n\treturn err;\n}\n\nstatic int raid5_start(struct mddev *mddev)\n{\n\tstruct r5conf *conf = mddev->private;\n\n\treturn r5l_start(conf->log);\n}\n\nstatic struct md_personality raid6_personality =\n{\n\t.name\t\t= \"raid6\",\n\t.level\t\t= 6,\n\t.owner\t\t= THIS_MODULE,\n\t.make_request\t= raid5_make_request,\n\t.run\t\t= raid5_run,\n\t.start\t\t= raid5_start,\n\t.free\t\t= raid5_free,\n\t.status\t\t= raid5_status,\n\t.error_handler\t= raid5_error,\n\t.hot_add_disk\t= raid5_add_disk,\n\t.hot_remove_disk= raid5_remove_disk,\n\t.spare_active\t= raid5_spare_active,\n\t.sync_request\t= raid5_sync_request,\n\t.resize\t\t= raid5_resize,\n\t.size\t\t= raid5_size,\n\t.check_reshape\t= raid6_check_reshape,\n\t.start_reshape  = raid5_start_reshape,\n\t.finish_reshape = raid5_finish_reshape,\n\t.quiesce\t= raid5_quiesce,\n\t.takeover\t= raid6_takeover,\n\t.change_consistency_policy = raid5_change_consistency_policy,\n};\nstatic struct md_personality raid5_personality =\n{\n\t.name\t\t= \"raid5\",\n\t.level\t\t= 5,\n\t.owner\t\t= THIS_MODULE,\n\t.make_request\t= raid5_make_request,\n\t.run\t\t= raid5_run,\n\t.start\t\t= raid5_start,\n\t.free\t\t= raid5_free,\n\t.status\t\t= raid5_status,\n\t.error_handler\t= raid5_error,\n\t.hot_add_disk\t= raid5_add_disk,\n\t.hot_remove_disk= raid5_remove_disk,\n\t.spare_active\t= raid5_spare_active,\n\t.sync_request\t= raid5_sync_request,\n\t.resize\t\t= raid5_resize,\n\t.size\t\t= raid5_size,\n\t.check_reshape\t= raid5_check_reshape,\n\t.start_reshape  = raid5_start_reshape,\n\t.finish_reshape = raid5_finish_reshape,\n\t.quiesce\t= raid5_quiesce,\n\t.takeover\t= raid5_takeover,\n\t.change_consistency_policy = raid5_change_consistency_policy,\n};\n\nstatic struct md_personality raid4_personality =\n{\n\t.name\t\t= \"raid4\",\n\t.level\t\t= 4,\n\t.owner\t\t= THIS_MODULE,\n\t.make_request\t= raid5_make_request,\n\t.run\t\t= raid5_run,\n\t.start\t\t= raid5_start,\n\t.free\t\t= raid5_free,\n\t.status\t\t= raid5_status,\n\t.error_handler\t= raid5_error,\n\t.hot_add_disk\t= raid5_add_disk,\n\t.hot_remove_disk= raid5_remove_disk,\n\t.spare_active\t= raid5_spare_active,\n\t.sync_request\t= raid5_sync_request,\n\t.resize\t\t= raid5_resize,\n\t.size\t\t= raid5_size,\n\t.check_reshape\t= raid5_check_reshape,\n\t.start_reshape  = raid5_start_reshape,\n\t.finish_reshape = raid5_finish_reshape,\n\t.quiesce\t= raid5_quiesce,\n\t.takeover\t= raid4_takeover,\n\t.change_consistency_policy = raid5_change_consistency_policy,\n};\n\nstatic int __init raid5_init(void)\n{\n\tint ret;\n\n\traid5_wq = alloc_workqueue(\"raid5wq\",\n\t\tWQ_UNBOUND|WQ_MEM_RECLAIM|WQ_CPU_INTENSIVE|WQ_SYSFS, 0);\n\tif (!raid5_wq)\n\t\treturn -ENOMEM;\n\n\tret = cpuhp_setup_state_multi(CPUHP_MD_RAID5_PREPARE,\n\t\t\t\t      \"md/raid5:prepare\",\n\t\t\t\t      raid456_cpu_up_prepare,\n\t\t\t\t      raid456_cpu_dead);\n\tif (ret) {\n\t\tdestroy_workqueue(raid5_wq);\n\t\treturn ret;\n\t}\n\tregister_md_personality(&raid6_personality);\n\tregister_md_personality(&raid5_personality);\n\tregister_md_personality(&raid4_personality);\n\treturn 0;\n}\n\nstatic void raid5_exit(void)\n{\n\tunregister_md_personality(&raid6_personality);\n\tunregister_md_personality(&raid5_personality);\n\tunregister_md_personality(&raid4_personality);\n\tcpuhp_remove_multi_state(CPUHP_MD_RAID5_PREPARE);\n\tdestroy_workqueue(raid5_wq);\n}\n\nmodule_init(raid5_init);\nmodule_exit(raid5_exit);\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"RAID4/5/6 (striping with parity) personality for MD\");\nMODULE_ALIAS(\"md-personality-4\"); /* RAID5 */\nMODULE_ALIAS(\"md-raid5\");\nMODULE_ALIAS(\"md-raid4\");\nMODULE_ALIAS(\"md-level-5\");\nMODULE_ALIAS(\"md-level-4\");\nMODULE_ALIAS(\"md-personality-8\"); /* RAID6 */\nMODULE_ALIAS(\"md-raid6\");\nMODULE_ALIAS(\"md-level-6\");\n\n/* This used to be two separate modules, they were: */\nMODULE_ALIAS(\"raid5\");\nMODULE_ALIAS(\"raid6\");\n"}}, "reports": [{"events": [{"location": {"col": 40, "file": 0, "line": 392}, "message": "atomic_add_unless"}], "macros": [], "notes": [], "path": "/src/drivers/md/raid5.c", "reportHash": "bbd5d4d98f4f69c87ce77cc827f8a9d2", "checkerName": "coccinelle", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
