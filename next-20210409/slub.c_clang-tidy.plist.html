<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"0": {"id": 0, "path": "/src/mm/slub.c", "content": "// SPDX-License-Identifier: GPL-2.0\n/*\n * SLUB: A slab allocator that limits cache line use instead of queuing\n * objects in per cpu and per node lists.\n *\n * The allocator synchronizes using per slab locks or atomic operations\n * and only uses a centralized lock to manage a pool of partial slabs.\n *\n * (C) 2007 SGI, Christoph Lameter\n * (C) 2011 Linux Foundation, Christoph Lameter\n */\n\n#include <linux/mm.h>\n#include <linux/swap.h> /* struct reclaim_state */\n#include <linux/module.h>\n#include <linux/bit_spinlock.h>\n#include <linux/interrupt.h>\n#include <linux/bitops.h>\n#include <linux/slab.h>\n#include \"slab.h\"\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/kasan.h>\n#include <linux/cpu.h>\n#include <linux/cpuset.h>\n#include <linux/mempolicy.h>\n#include <linux/ctype.h>\n#include <linux/debugobjects.h>\n#include <linux/kallsyms.h>\n#include <linux/kfence.h>\n#include <linux/memory.h>\n#include <linux/math64.h>\n#include <linux/fault-inject.h>\n#include <linux/stacktrace.h>\n#include <linux/prefetch.h>\n#include <linux/memcontrol.h>\n#include <linux/random.h>\n\n#include <trace/events/kmem.h>\n\n#include \"internal.h\"\n\n/*\n * Lock order:\n *   1. slab_mutex (Global Mutex)\n *   2. node->list_lock\n *   3. slab_lock(page) (Only on some arches and for debugging)\n *\n *   slab_mutex\n *\n *   The role of the slab_mutex is to protect the list of all the slabs\n *   and to synchronize major metadata changes to slab cache structures.\n *\n *   The slab_lock is only used for debugging and on arches that do not\n *   have the ability to do a cmpxchg_double. It only protects:\n *\tA. page->freelist\t-> List of object free in a page\n *\tB. page->inuse\t\t-> Number of objects in use\n *\tC. page->objects\t-> Number of objects in page\n *\tD. page->frozen\t\t-> frozen state\n *\n *   If a slab is frozen then it is exempt from list management. It is not\n *   on any list except per cpu partial list. The processor that froze the\n *   slab is the one who can perform list operations on the page. Other\n *   processors may put objects onto the freelist but the processor that\n *   froze the slab is the only one that can retrieve the objects from the\n *   page's freelist.\n *\n *   The list_lock protects the partial and full list on each node and\n *   the partial slab counter. If taken then no new slabs may be added or\n *   removed from the lists nor make the number of partial slabs be modified.\n *   (Note that the total number of slabs is an atomic value that may be\n *   modified without taking the list lock).\n *\n *   The list_lock is a centralized lock and thus we avoid taking it as\n *   much as possible. As long as SLUB does not have to handle partial\n *   slabs, operations can continue without any centralized lock. F.e.\n *   allocating a long series of objects that fill up slabs does not require\n *   the list lock.\n *   Interrupts are disabled during allocation and deallocation in order to\n *   make the slab allocator safe to use in the context of an irq. In addition\n *   interrupts are disabled to ensure that the processor does not change\n *   while handling per_cpu slabs, due to kernel preemption.\n *\n * SLUB assigns one slab for allocation to each processor.\n * Allocations only occur from these slabs called cpu slabs.\n *\n * Slabs with free elements are kept on a partial list and during regular\n * operations no list for full slabs is used. If an object in a full slab is\n * freed then the slab will show up again on the partial lists.\n * We track full slabs for debugging purposes though because otherwise we\n * cannot scan all objects.\n *\n * Slabs are freed when they become empty. Teardown and setup is\n * minimal so we rely on the page allocators per cpu caches for\n * fast frees and allocs.\n *\n * page->frozen\t\tThe slab is frozen and exempt from list processing.\n * \t\t\tThis means that the slab is dedicated to a purpose\n * \t\t\tsuch as satisfying allocations for a specific\n * \t\t\tprocessor. Objects may be freed in the slab while\n * \t\t\tit is frozen but slab_free will then skip the usual\n * \t\t\tlist operations. It is up to the processor holding\n * \t\t\tthe slab to integrate the slab into the slab lists\n * \t\t\twhen the slab is no longer needed.\n *\n * \t\t\tOne use of this flag is to mark slabs that are\n * \t\t\tused for allocations. Then such a slab becomes a cpu\n * \t\t\tslab. The cpu slab may be equipped with an additional\n * \t\t\tfreelist that allows lockless access to\n * \t\t\tfree objects in addition to the regular freelist\n * \t\t\tthat requires the slab lock.\n *\n * SLAB_DEBUG_FLAGS\tSlab requires special handling due to debug\n * \t\t\toptions set. This moves\tslab handling out of\n * \t\t\tthe fast path and disables lockless freelists.\n */\n\n#ifdef CONFIG_SLUB_DEBUG\n#ifdef CONFIG_SLUB_DEBUG_ON\nDEFINE_STATIC_KEY_TRUE(slub_debug_enabled);\n#else\nDEFINE_STATIC_KEY_FALSE(slub_debug_enabled);\n#endif\n#endif\n\nstatic inline bool kmem_cache_debug(struct kmem_cache *s)\n{\n\treturn kmem_cache_debug_flags(s, SLAB_DEBUG_FLAGS);\n}\n\nvoid *fixup_red_left(struct kmem_cache *s, void *p)\n{\n\tif (kmem_cache_debug_flags(s, SLAB_RED_ZONE))\n\t\tp += s->red_left_pad;\n\n\treturn p;\n}\n\nstatic inline bool kmem_cache_has_cpu_partial(struct kmem_cache *s)\n{\n#ifdef CONFIG_SLUB_CPU_PARTIAL\n\treturn !kmem_cache_debug(s);\n#else\n\treturn false;\n#endif\n}\n\n/*\n * Issues still to be resolved:\n *\n * - Support PAGE_ALLOC_DEBUG. Should be easy to do.\n *\n * - Variable sizing of the per node arrays\n */\n\n/* Enable to log cmpxchg failures */\n#undef SLUB_DEBUG_CMPXCHG\n\n/*\n * Minimum number of partial slabs. These will be left on the partial\n * lists even if they are empty. kmem_cache_shrink may reclaim them.\n */\n#define MIN_PARTIAL 5\n\n/*\n * Maximum number of desirable partial slabs.\n * The existence of more partial slabs makes kmem_cache_shrink\n * sort the partial list by the number of objects in use.\n */\n#define MAX_PARTIAL 10\n\n#define DEBUG_DEFAULT_FLAGS (SLAB_CONSISTENCY_CHECKS | SLAB_RED_ZONE | \\\n\t\t\t\tSLAB_POISON | SLAB_STORE_USER)\n\n/*\n * These debug flags cannot use CMPXCHG because there might be consistency\n * issues when checking or reading debug information\n */\n#define SLAB_NO_CMPXCHG (SLAB_CONSISTENCY_CHECKS | SLAB_STORE_USER | \\\n\t\t\t\tSLAB_TRACE)\n\n\n/*\n * Debugging flags that require metadata to be stored in the slab.  These get\n * disabled when slub_debug=O is used and a cache's min order increases with\n * metadata.\n */\n#define DEBUG_METADATA_FLAGS (SLAB_RED_ZONE | SLAB_POISON | SLAB_STORE_USER)\n\n#define OO_SHIFT\t16\n#define OO_MASK\t\t((1 << OO_SHIFT) - 1)\n#define MAX_OBJS_PER_PAGE\t32767 /* since page.objects is u15 */\n\n/* Internal SLUB flags */\n/* Poison object */\n#define __OBJECT_POISON\t\t((slab_flags_t __force)0x80000000U)\n/* Use cmpxchg_double */\n#define __CMPXCHG_DOUBLE\t((slab_flags_t __force)0x40000000U)\n\n/*\n * Tracking user of a slab.\n */\n#define TRACK_ADDRS_COUNT 16\nstruct track {\n\tunsigned long addr;\t/* Called from address */\n#ifdef CONFIG_STACKTRACE\n\tunsigned long addrs[TRACK_ADDRS_COUNT];\t/* Called from address */\n#endif\n\tint cpu;\t\t/* Was running on cpu */\n\tint pid;\t\t/* Pid context */\n\tunsigned long when;\t/* When did the operation occur */\n};\n\nenum track_item { TRACK_ALLOC, TRACK_FREE };\n\n#ifdef CONFIG_SYSFS\nstatic int sysfs_slab_add(struct kmem_cache *);\nstatic int sysfs_slab_alias(struct kmem_cache *, const char *);\n#else\nstatic inline int sysfs_slab_add(struct kmem_cache *s) { return 0; }\nstatic inline int sysfs_slab_alias(struct kmem_cache *s, const char *p)\n\t\t\t\t\t\t\t{ return 0; }\n#endif\n\nstatic inline void stat(const struct kmem_cache *s, enum stat_item si)\n{\n#ifdef CONFIG_SLUB_STATS\n\t/*\n\t * The rmw is racy on a preemptible kernel but this is acceptable, so\n\t * avoid this_cpu_add()'s irq-disable overhead.\n\t */\n\traw_cpu_inc(s->cpu_slab->stat[si]);\n#endif\n}\n\n/*\n * Tracks for which NUMA nodes we have kmem_cache_nodes allocated.\n * Corresponds to node_state[N_NORMAL_MEMORY], but can temporarily\n * differ during memory hotplug/hotremove operations.\n * Protected by slab_mutex.\n */\nstatic nodemask_t slab_nodes;\n\n/********************************************************************\n * \t\t\tCore slab cache functions\n *******************************************************************/\n\n/*\n * Returns freelist pointer (ptr). With hardening, this is obfuscated\n * with an XOR of the address where the pointer is held and a per-cache\n * random number.\n */\nstatic inline void *freelist_ptr(const struct kmem_cache *s, void *ptr,\n\t\t\t\t unsigned long ptr_addr)\n{\n#ifdef CONFIG_SLAB_FREELIST_HARDENED\n\t/*\n\t * When CONFIG_KASAN_SW/HW_TAGS is enabled, ptr_addr might be tagged.\n\t * Normally, this doesn't cause any issues, as both set_freepointer()\n\t * and get_freepointer() are called with a pointer with the same tag.\n\t * However, there are some issues with CONFIG_SLUB_DEBUG code. For\n\t * example, when __free_slub() iterates over objects in a cache, it\n\t * passes untagged pointers to check_object(). check_object() in turns\n\t * calls get_freepointer() with an untagged pointer, which causes the\n\t * freepointer to be restored incorrectly.\n\t */\n\treturn (void *)((unsigned long)ptr ^ s->random ^\n\t\t\tswab((unsigned long)kasan_reset_tag((void *)ptr_addr)));\n#else\n\treturn ptr;\n#endif\n}\n\n/* Returns the freelist pointer recorded at location ptr_addr. */\nstatic inline void *freelist_dereference(const struct kmem_cache *s,\n\t\t\t\t\t void *ptr_addr)\n{\n\treturn freelist_ptr(s, (void *)*(unsigned long *)(ptr_addr),\n\t\t\t    (unsigned long)ptr_addr);\n}\n\nstatic inline void *get_freepointer(struct kmem_cache *s, void *object)\n{\n\tobject = kasan_reset_tag(object);\n\treturn freelist_dereference(s, object + s->offset);\n}\n\nstatic void prefetch_freepointer(const struct kmem_cache *s, void *object)\n{\n\tprefetch(object + s->offset);\n}\n\nstatic inline void *get_freepointer_safe(struct kmem_cache *s, void *object)\n{\n\tunsigned long freepointer_addr;\n\tvoid *p;\n\n\tif (!debug_pagealloc_enabled_static())\n\t\treturn get_freepointer(s, object);\n\n\tfreepointer_addr = (unsigned long)object + s->offset;\n\tcopy_from_kernel_nofault(&p, (void **)freepointer_addr, sizeof(p));\n\treturn freelist_ptr(s, p, freepointer_addr);\n}\n\nstatic inline void set_freepointer(struct kmem_cache *s, void *object, void *fp)\n{\n\tunsigned long freeptr_addr = (unsigned long)object + s->offset;\n\n#ifdef CONFIG_SLAB_FREELIST_HARDENED\n\tBUG_ON(object == fp); /* naive detection of double free or corruption */\n#endif\n\n\tfreeptr_addr = (unsigned long)kasan_reset_tag((void *)freeptr_addr);\n\t*(void **)freeptr_addr = freelist_ptr(s, fp, freeptr_addr);\n}\n\n/* Loop over all objects in a slab */\n#define for_each_object(__p, __s, __addr, __objects) \\\n\tfor (__p = fixup_red_left(__s, __addr); \\\n\t\t__p < (__addr) + (__objects) * (__s)->size; \\\n\t\t__p += (__s)->size)\n\nstatic inline unsigned int order_objects(unsigned int order, unsigned int size)\n{\n\treturn ((unsigned int)PAGE_SIZE << order) / size;\n}\n\nstatic inline struct kmem_cache_order_objects oo_make(unsigned int order,\n\t\tunsigned int size)\n{\n\tstruct kmem_cache_order_objects x = {\n\t\t(order << OO_SHIFT) + order_objects(order, size)\n\t};\n\n\treturn x;\n}\n\nstatic inline unsigned int oo_order(struct kmem_cache_order_objects x)\n{\n\treturn x.x >> OO_SHIFT;\n}\n\nstatic inline unsigned int oo_objects(struct kmem_cache_order_objects x)\n{\n\treturn x.x & OO_MASK;\n}\n\n/*\n * Per slab locking using the pagelock\n */\nstatic __always_inline void slab_lock(struct page *page)\n{\n\tVM_BUG_ON_PAGE(PageTail(page), page);\n\tbit_spin_lock(PG_locked, &page->flags);\n}\n\nstatic __always_inline void slab_unlock(struct page *page)\n{\n\tVM_BUG_ON_PAGE(PageTail(page), page);\n\t__bit_spin_unlock(PG_locked, &page->flags);\n}\n\n/* Interrupts must be disabled (for the fallback code to work right) */\nstatic inline bool __cmpxchg_double_slab(struct kmem_cache *s, struct page *page,\n\t\tvoid *freelist_old, unsigned long counters_old,\n\t\tvoid *freelist_new, unsigned long counters_new,\n\t\tconst char *n)\n{\n\tVM_BUG_ON(!irqs_disabled());\n#if defined(CONFIG_HAVE_CMPXCHG_DOUBLE) && \\\n    defined(CONFIG_HAVE_ALIGNED_STRUCT_PAGE)\n\tif (s->flags & __CMPXCHG_DOUBLE) {\n\t\tif (cmpxchg_double(&page->freelist, &page->counters,\n\t\t\t\t   freelist_old, counters_old,\n\t\t\t\t   freelist_new, counters_new))\n\t\t\treturn true;\n\t} else\n#endif\n\t{\n\t\tslab_lock(page);\n\t\tif (page->freelist == freelist_old &&\n\t\t\t\t\tpage->counters == counters_old) {\n\t\t\tpage->freelist = freelist_new;\n\t\t\tpage->counters = counters_new;\n\t\t\tslab_unlock(page);\n\t\t\treturn true;\n\t\t}\n\t\tslab_unlock(page);\n\t}\n\n\tcpu_relax();\n\tstat(s, CMPXCHG_DOUBLE_FAIL);\n\n#ifdef SLUB_DEBUG_CMPXCHG\n\tpr_info(\"%s %s: cmpxchg double redo \", n, s->name);\n#endif\n\n\treturn false;\n}\n\nstatic inline bool cmpxchg_double_slab(struct kmem_cache *s, struct page *page,\n\t\tvoid *freelist_old, unsigned long counters_old,\n\t\tvoid *freelist_new, unsigned long counters_new,\n\t\tconst char *n)\n{\n#if defined(CONFIG_HAVE_CMPXCHG_DOUBLE) && \\\n    defined(CONFIG_HAVE_ALIGNED_STRUCT_PAGE)\n\tif (s->flags & __CMPXCHG_DOUBLE) {\n\t\tif (cmpxchg_double(&page->freelist, &page->counters,\n\t\t\t\t   freelist_old, counters_old,\n\t\t\t\t   freelist_new, counters_new))\n\t\t\treturn true;\n\t} else\n#endif\n\t{\n\t\tunsigned long flags;\n\n\t\tlocal_irq_save(flags);\n\t\tslab_lock(page);\n\t\tif (page->freelist == freelist_old &&\n\t\t\t\t\tpage->counters == counters_old) {\n\t\t\tpage->freelist = freelist_new;\n\t\t\tpage->counters = counters_new;\n\t\t\tslab_unlock(page);\n\t\t\tlocal_irq_restore(flags);\n\t\t\treturn true;\n\t\t}\n\t\tslab_unlock(page);\n\t\tlocal_irq_restore(flags);\n\t}\n\n\tcpu_relax();\n\tstat(s, CMPXCHG_DOUBLE_FAIL);\n\n#ifdef SLUB_DEBUG_CMPXCHG\n\tpr_info(\"%s %s: cmpxchg double redo \", n, s->name);\n#endif\n\n\treturn false;\n}\n\n#ifdef CONFIG_SLUB_DEBUG\nstatic unsigned long object_map[BITS_TO_LONGS(MAX_OBJS_PER_PAGE)];\nstatic DEFINE_SPINLOCK(object_map_lock);\n\n/*\n * Determine a map of object in use on a page.\n *\n * Node listlock must be held to guarantee that the page does\n * not vanish from under us.\n */\nstatic unsigned long *get_map(struct kmem_cache *s, struct page *page)\n\t__acquires(&object_map_lock)\n{\n\tvoid *p;\n\tvoid *addr = page_address(page);\n\n\tVM_BUG_ON(!irqs_disabled());\n\n\tspin_lock(&object_map_lock);\n\n\tbitmap_zero(object_map, page->objects);\n\n\tfor (p = page->freelist; p; p = get_freepointer(s, p))\n\t\tset_bit(__obj_to_index(s, addr, p), object_map);\n\n\treturn object_map;\n}\n\nstatic void put_map(unsigned long *map) __releases(&object_map_lock)\n{\n\tVM_BUG_ON(map != object_map);\n\tspin_unlock(&object_map_lock);\n}\n\nstatic inline unsigned int size_from_object(struct kmem_cache *s)\n{\n\tif (s->flags & SLAB_RED_ZONE)\n\t\treturn s->size - s->red_left_pad;\n\n\treturn s->size;\n}\n\nstatic inline void *restore_red_left(struct kmem_cache *s, void *p)\n{\n\tif (s->flags & SLAB_RED_ZONE)\n\t\tp -= s->red_left_pad;\n\n\treturn p;\n}\n\n/*\n * Debug settings:\n */\n#if defined(CONFIG_SLUB_DEBUG_ON)\nstatic slab_flags_t slub_debug = DEBUG_DEFAULT_FLAGS;\n#else\nstatic slab_flags_t slub_debug;\n#endif\n\nstatic char *slub_debug_string;\nstatic int disable_higher_order_debug;\n\n/*\n * slub is about to manipulate internal object metadata.  This memory lies\n * outside the range of the allocated object, so accessing it would normally\n * be reported by kasan as a bounds error.  metadata_access_enable() is used\n * to tell kasan that these accesses are OK.\n */\nstatic inline void metadata_access_enable(void)\n{\n\tkasan_disable_current();\n}\n\nstatic inline void metadata_access_disable(void)\n{\n\tkasan_enable_current();\n}\n\n/*\n * Object debugging\n */\n\n/* Verify that a pointer has an address that is valid within a slab page */\nstatic inline int check_valid_pointer(struct kmem_cache *s,\n\t\t\t\tstruct page *page, void *object)\n{\n\tvoid *base;\n\n\tif (!object)\n\t\treturn 1;\n\n\tbase = page_address(page);\n\tobject = kasan_reset_tag(object);\n\tobject = restore_red_left(s, object);\n\tif (object < base || object >= base + page->objects * s->size ||\n\t\t(object - base) % s->size) {\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic void print_section(char *level, char *text, u8 *addr,\n\t\t\t  unsigned int length)\n{\n\tmetadata_access_enable();\n\tprint_hex_dump(level, kasan_reset_tag(text), DUMP_PREFIX_ADDRESS,\n\t\t\t16, 1, addr, length, 1);\n\tmetadata_access_disable();\n}\n\n/*\n * See comment in calculate_sizes().\n */\nstatic inline bool freeptr_outside_object(struct kmem_cache *s)\n{\n\treturn s->offset >= s->inuse;\n}\n\n/*\n * Return offset of the end of info block which is inuse + free pointer if\n * not overlapping with object.\n */\nstatic inline unsigned int get_info_end(struct kmem_cache *s)\n{\n\tif (freeptr_outside_object(s))\n\t\treturn s->inuse + sizeof(void *);\n\telse\n\t\treturn s->inuse;\n}\n\nstatic struct track *get_track(struct kmem_cache *s, void *object,\n\tenum track_item alloc)\n{\n\tstruct track *p;\n\n\tp = object + get_info_end(s);\n\n\treturn kasan_reset_tag(p + alloc);\n}\n\nstatic void set_track(struct kmem_cache *s, void *object,\n\t\t\tenum track_item alloc, unsigned long addr)\n{\n\tstruct track *p = get_track(s, object, alloc);\n\n\tif (addr) {\n#ifdef CONFIG_STACKTRACE\n\t\tunsigned int nr_entries;\n\n\t\tmetadata_access_enable();\n\t\tnr_entries = stack_trace_save(kasan_reset_tag(p->addrs),\n\t\t\t\t\t      TRACK_ADDRS_COUNT, 3);\n\t\tmetadata_access_disable();\n\n\t\tif (nr_entries < TRACK_ADDRS_COUNT)\n\t\t\tp->addrs[nr_entries] = 0;\n#endif\n\t\tp->addr = addr;\n\t\tp->cpu = smp_processor_id();\n\t\tp->pid = current->pid;\n\t\tp->when = jiffies;\n\t} else {\n\t\tmemset(p, 0, sizeof(struct track));\n\t}\n}\n\nstatic void init_tracking(struct kmem_cache *s, void *object)\n{\n\tif (!(s->flags & SLAB_STORE_USER))\n\t\treturn;\n\n\tset_track(s, object, TRACK_FREE, 0UL);\n\tset_track(s, object, TRACK_ALLOC, 0UL);\n}\n\nstatic void print_track(const char *s, struct track *t, unsigned long pr_time)\n{\n\tif (!t->addr)\n\t\treturn;\n\n\tpr_err(\"%s in %pS age=%lu cpu=%u pid=%d\\n\",\n\t       s, (void *)t->addr, pr_time - t->when, t->cpu, t->pid);\n#ifdef CONFIG_STACKTRACE\n\t{\n\t\tint i;\n\t\tfor (i = 0; i < TRACK_ADDRS_COUNT; i++)\n\t\t\tif (t->addrs[i])\n\t\t\t\tpr_err(\"\\t%pS\\n\", (void *)t->addrs[i]);\n\t\t\telse\n\t\t\t\tbreak;\n\t}\n#endif\n}\n\nvoid print_tracking(struct kmem_cache *s, void *object)\n{\n\tunsigned long pr_time = jiffies;\n\tif (!(s->flags & SLAB_STORE_USER))\n\t\treturn;\n\n\tprint_track(\"Allocated\", get_track(s, object, TRACK_ALLOC), pr_time);\n\tprint_track(\"Freed\", get_track(s, object, TRACK_FREE), pr_time);\n}\n\nstatic void print_page_info(struct page *page)\n{\n\tpr_err(\"Slab 0x%p objects=%u used=%u fp=0x%p flags=%#lx(%pGp)\\n\",\n\t       page, page->objects, page->inuse, page->freelist,\n\t       page->flags, &page->flags);\n\n}\n\nstatic void slab_bug(struct kmem_cache *s, char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tva_start(args, fmt);\n\tvaf.fmt = fmt;\n\tvaf.va = &args;\n\tpr_err(\"=============================================================================\\n\");\n\tpr_err(\"BUG %s (%s): %pV\\n\", s->name, print_tainted(), &vaf);\n\tpr_err(\"-----------------------------------------------------------------------------\\n\\n\");\n\n\tadd_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);\n\tva_end(args);\n}\n\nstatic void slab_fix(struct kmem_cache *s, char *fmt, ...)\n{\n\tif (!(s->flags & SLAB_SILENT_ERRORS)) {\n\t\tstruct va_format vaf;\n\t\tva_list args;\n\n\t\tva_start(args, fmt);\n\t\tvaf.fmt = fmt;\n\t\tvaf.va = &args;\n\t\tpr_err(\"FIX %s: %pV\\n\", s->name, &vaf);\n\t\tva_end(args);\n\t}\n}\n\nstatic bool freelist_corrupted(struct kmem_cache *s, struct page *page,\n\t\t\t       void **freelist, void *nextfree)\n{\n\tif ((s->flags & SLAB_CONSISTENCY_CHECKS) &&\n\t    !check_valid_pointer(s, page, nextfree) && freelist) {\n\t\tobject_err(s, page, *freelist, \"Freechain corrupt\");\n\t\t*freelist = NULL;\n\t\tslab_fix(s, \"Isolate corrupted freechain\");\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void print_trailer(struct kmem_cache *s, struct page *page, u8 *p)\n{\n\tunsigned int off;\t/* Offset of last byte */\n\tu8 *addr = page_address(page);\n\n\tprint_tracking(s, p);\n\n\tprint_page_info(page);\n\n\tpr_err(\"Object 0x%p @offset=%tu fp=0x%p\\n\\n\",\n\t       p, p - addr, get_freepointer(s, p));\n\n\tif (s->flags & SLAB_RED_ZONE)\n\t\tprint_section(KERN_ERR, \"Redzone \", p - s->red_left_pad,\n\t\t\t      s->red_left_pad);\n\telse if (p > addr + 16)\n\t\tprint_section(KERN_ERR, \"Bytes b4 \", p - 16, 16);\n\n\tprint_section(KERN_ERR, \"Object \", p,\n\t\t      min_t(unsigned int, s->object_size, PAGE_SIZE));\n\tif (s->flags & SLAB_RED_ZONE)\n\t\tprint_section(KERN_ERR, \"Redzone \", p + s->object_size,\n\t\t\ts->inuse - s->object_size);\n\n\toff = get_info_end(s);\n\n\tif (s->flags & SLAB_STORE_USER)\n\t\toff += 2 * sizeof(struct track);\n\n\toff += kasan_metadata_size(s);\n\n\tif (off != size_from_object(s))\n\t\t/* Beginning of the filler is the free pointer */\n\t\tprint_section(KERN_ERR, \"Padding \", p + off,\n\t\t\t      size_from_object(s) - off);\n\n\tdump_stack();\n}\n\nvoid object_err(struct kmem_cache *s, struct page *page,\n\t\t\tu8 *object, char *reason)\n{\n\tif (!(s->flags & SLAB_SILENT_ERRORS)) {\n\t\tslab_bug(s, \"%s\", reason);\n\t\tprint_trailer(s, page, object);\n\t}\n}\n\nstatic __printf(3, 4) void slab_err(struct kmem_cache *s, struct page *page,\n\t\t\tconst char *fmt, ...)\n{\n\tva_list args;\n\tchar buf[100];\n\n\tva_start(args, fmt);\n\tvsnprintf(buf, sizeof(buf), fmt, args);\n\tva_end(args);\n\tif (!(s->flags & SLAB_SILENT_ERRORS)) {\n\t\tslab_bug(s, \"%s\", buf);\n\t\tprint_page_info(page);\n\t\tdump_stack();\n\t}\n}\n\nstatic void init_object(struct kmem_cache *s, void *object, u8 val)\n{\n\tu8 *p = kasan_reset_tag(object);\n\n\tif (s->flags & SLAB_RED_ZONE)\n\t\tmemset(p - s->red_left_pad, val, s->red_left_pad);\n\n\tif (s->flags & __OBJECT_POISON) {\n\t\tmemset(p, POISON_FREE, s->object_size - 1);\n\t\tp[s->object_size - 1] = POISON_END;\n\t}\n\n\tif (s->flags & SLAB_RED_ZONE)\n\t\tmemset(p + s->object_size, val, s->inuse - s->object_size);\n}\n\nstatic void restore_bytes(struct kmem_cache *s, char *message, u8 data,\n\t\t\t\t\t\tvoid *from, void *to)\n{\n\tslab_fix(s, \"Restoring 0x%p-0x%p=0x%x\\n\", from, to - 1, data);\n\tmemset(from, data, to - from);\n}\n\nstatic int check_bytes_and_report(struct kmem_cache *s, struct page *page,\n\t\t\tu8 *object, char *what,\n\t\t\tu8 *start, unsigned int value, unsigned int bytes)\n{\n\tu8 *fault;\n\tu8 *end;\n\tu8 *addr = page_address(page);\n\n\tmetadata_access_enable();\n\tfault = memchr_inv(kasan_reset_tag(start), value, bytes);\n\tmetadata_access_disable();\n\tif (!fault)\n\t\treturn 1;\n\n\tend = start + bytes;\n\twhile (end > fault && end[-1] == value)\n\t\tend--;\n\n\tif (!(s->flags & SLAB_SILENT_ERRORS)) {\n\t\tslab_bug(s, \"%s overwritten\", what);\n\t\tpr_err(\"INFO: 0x%p-0x%p @offset=%tu. First byte 0x%x instead of 0x%x\\n\",\n\t\t\t\t\tfault, end - 1, fault - addr,\n\t\t\t\t\tfault[0], value);\n\t\tprint_trailer(s, page, object);\n\t}\n\n\trestore_bytes(s, what, value, fault, end);\n\treturn 0;\n}\n\n/*\n * Object layout:\n *\n * object address\n * \tBytes of the object to be managed.\n * \tIf the freepointer may overlay the object then the free\n *\tpointer is at the middle of the object.\n *\n * \tPoisoning uses 0x6b (POISON_FREE) and the last byte is\n * \t0xa5 (POISON_END)\n *\n * object + s->object_size\n * \tPadding to reach word boundary. This is also used for Redzoning.\n * \tPadding is extended by another word if Redzoning is enabled and\n * \tobject_size == inuse.\n *\n * \tWe fill with 0xbb (RED_INACTIVE) for inactive objects and with\n * \t0xcc (RED_ACTIVE) for objects in use.\n *\n * object + s->inuse\n * \tMeta data starts here.\n *\n * \tA. Free pointer (if we cannot overwrite object on free)\n * \tB. Tracking data for SLAB_STORE_USER\n *\tC. Padding to reach required alignment boundary or at minimum\n * \t\tone word if debugging is on to be able to detect writes\n * \t\tbefore the word boundary.\n *\n *\tPadding is done using 0x5a (POISON_INUSE)\n *\n * object + s->size\n * \tNothing is used beyond s->size.\n *\n * If slabcaches are merged then the object_size and inuse boundaries are mostly\n * ignored. And therefore no slab options that rely on these boundaries\n * may be used with merged slabcaches.\n */\n\nstatic int check_pad_bytes(struct kmem_cache *s, struct page *page, u8 *p)\n{\n\tunsigned long off = get_info_end(s);\t/* The end of info */\n\n\tif (s->flags & SLAB_STORE_USER)\n\t\t/* We also have user information there */\n\t\toff += 2 * sizeof(struct track);\n\n\toff += kasan_metadata_size(s);\n\n\tif (size_from_object(s) == off)\n\t\treturn 1;\n\n\treturn check_bytes_and_report(s, page, p, \"Object padding\",\n\t\t\tp + off, POISON_INUSE, size_from_object(s) - off);\n}\n\n/* Check the pad bytes at the end of a slab page */\nstatic int slab_pad_check(struct kmem_cache *s, struct page *page)\n{\n\tu8 *start;\n\tu8 *fault;\n\tu8 *end;\n\tu8 *pad;\n\tint length;\n\tint remainder;\n\n\tif (!(s->flags & SLAB_POISON))\n\t\treturn 1;\n\n\tstart = page_address(page);\n\tlength = page_size(page);\n\tend = start + length;\n\tremainder = length % s->size;\n\tif (!remainder)\n\t\treturn 1;\n\n\tpad = end - remainder;\n\tmetadata_access_enable();\n\tfault = memchr_inv(kasan_reset_tag(pad), POISON_INUSE, remainder);\n\tmetadata_access_disable();\n\tif (!fault)\n\t\treturn 1;\n\twhile (end > fault && end[-1] == POISON_INUSE)\n\t\tend--;\n\n\tslab_err(s, page, \"Padding overwritten. 0x%p-0x%p @offset=%tu\",\n\t\t\tfault, end - 1, fault - start);\n\tprint_section(KERN_ERR, \"Padding \", pad, remainder);\n\n\trestore_bytes(s, \"slab padding\", POISON_INUSE, fault, end);\n\treturn 0;\n}\n\nstatic int check_object(struct kmem_cache *s, struct page *page,\n\t\t\t\t\tvoid *object, u8 val)\n{\n\tu8 *p = object;\n\tu8 *endobject = object + s->object_size;\n\n\tif (s->flags & SLAB_RED_ZONE) {\n\t\tif (!check_bytes_and_report(s, page, object, \"Redzone\",\n\t\t\tobject - s->red_left_pad, val, s->red_left_pad))\n\t\t\treturn 0;\n\n\t\tif (!check_bytes_and_report(s, page, object, \"Redzone\",\n\t\t\tendobject, val, s->inuse - s->object_size))\n\t\t\treturn 0;\n\t} else {\n\t\tif ((s->flags & SLAB_POISON) && s->object_size < s->inuse) {\n\t\t\tcheck_bytes_and_report(s, page, p, \"Alignment padding\",\n\t\t\t\tendobject, POISON_INUSE,\n\t\t\t\ts->inuse - s->object_size);\n\t\t}\n\t}\n\n\tif (s->flags & SLAB_POISON) {\n\t\tif (val != SLUB_RED_ACTIVE && (s->flags & __OBJECT_POISON) &&\n\t\t\t(!check_bytes_and_report(s, page, p, \"Poison\", p,\n\t\t\t\t\tPOISON_FREE, s->object_size - 1) ||\n\t\t\t !check_bytes_and_report(s, page, p, \"Poison\",\n\t\t\t\tp + s->object_size - 1, POISON_END, 1)))\n\t\t\treturn 0;\n\t\t/*\n\t\t * check_pad_bytes cleans up on its own.\n\t\t */\n\t\tcheck_pad_bytes(s, page, p);\n\t}\n\n\tif (!freeptr_outside_object(s) && val == SLUB_RED_ACTIVE)\n\t\t/*\n\t\t * Object and freepointer overlap. Cannot check\n\t\t * freepointer while object is allocated.\n\t\t */\n\t\treturn 1;\n\n\t/* Check free pointer validity */\n\tif (!check_valid_pointer(s, page, get_freepointer(s, p))) {\n\t\tobject_err(s, page, p, \"Freepointer corrupt\");\n\t\t/*\n\t\t * No choice but to zap it and thus lose the remainder\n\t\t * of the free objects in this slab. May cause\n\t\t * another error because the object count is now wrong.\n\t\t */\n\t\tset_freepointer(s, p, NULL);\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\nstatic int check_slab(struct kmem_cache *s, struct page *page)\n{\n\tint maxobj;\n\n\tVM_BUG_ON(!irqs_disabled());\n\n\tif (!PageSlab(page)) {\n\t\tslab_err(s, page, \"Not a valid slab page\");\n\t\ts->errors += 1;\n\t\treturn 0;\n\t}\n\n\tmaxobj = order_objects(compound_order(page), s->size);\n\tif (page->objects > maxobj) {\n\t\tslab_err(s, page, \"objects %u > max %u\",\n\t\t\tpage->objects, maxobj);\n\t\ts->errors += 1;\n\t\treturn 0;\n\t}\n\tif (page->inuse > page->objects) {\n\t\tslab_err(s, page, \"inuse %u > max %u\",\n\t\t\tpage->inuse, page->objects);\n\t\ts->errors += 1;\n\t\treturn 0;\n\t}\n\t/* Slab_pad_check fixes things up after itself */\n\tslab_pad_check(s, page);\n\treturn 1;\n}\n\n/*\n * Determine if a certain object on a page is on the freelist. Must hold the\n * slab lock to guarantee that the chains are in a consistent state.\n */\nstatic int on_freelist(struct kmem_cache *s, struct page *page, void *search)\n{\n\tint nr = 0;\n\tvoid *fp;\n\tvoid *object = NULL;\n\tint max_objects;\n\n\tfp = page->freelist;\n\twhile (fp && nr <= page->objects) {\n\t\tif (fp == search)\n\t\t\treturn 1;\n\t\tif (!check_valid_pointer(s, page, fp)) {\n\t\t\tif (object) {\n\t\t\t\tobject_err(s, page, object,\n\t\t\t\t\t\"Freechain corrupt\");\n\t\t\t\tset_freepointer(s, object, NULL);\n\t\t\t} else {\n\t\t\t\tslab_err(s, page, \"Freepointer corrupt\");\n\t\t\t\tpage->freelist = NULL;\n\t\t\t\tpage->inuse = page->objects;\n\t\t\t\tslab_fix(s, \"Freelist cleared\");\n\t\t\t\ts->errors += 1;\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\ts->errors += 1;\n\t\t\tbreak;\n\t\t}\n\t\tobject = fp;\n\t\tfp = get_freepointer(s, object);\n\t\tnr++;\n\t}\n\n\tmax_objects = order_objects(compound_order(page), s->size);\n\tif (max_objects > MAX_OBJS_PER_PAGE)\n\t\tmax_objects = MAX_OBJS_PER_PAGE;\n\n\tif (page->objects != max_objects) {\n\t\tslab_err(s, page, \"Wrong number of objects. Found %d but should be %d\",\n\t\t\t page->objects, max_objects);\n\t\tpage->objects = max_objects;\n\t\tslab_fix(s, \"Number of objects adjusted.\");\n\t\ts->errors += 1;\n\t}\n\tif (page->inuse != page->objects - nr) {\n\t\tslab_err(s, page, \"Wrong object count. Counter is %d but counted were %d\",\n\t\t\t page->inuse, page->objects - nr);\n\t\tpage->inuse = page->objects - nr;\n\t\tslab_fix(s, \"Object count adjusted.\");\n\t\ts->errors += 1;\n\t}\n\treturn search == NULL;\n}\n\nstatic void trace(struct kmem_cache *s, struct page *page, void *object,\n\t\t\t\t\t\t\t\tint alloc)\n{\n\tif (s->flags & SLAB_TRACE) {\n\t\tpr_info(\"TRACE %s %s 0x%p inuse=%d fp=0x%p\\n\",\n\t\t\ts->name,\n\t\t\talloc ? \"alloc\" : \"free\",\n\t\t\tobject, page->inuse,\n\t\t\tpage->freelist);\n\n\t\tif (!alloc)\n\t\t\tprint_section(KERN_INFO, \"Object \", (void *)object,\n\t\t\t\t\ts->object_size);\n\n\t\tdump_stack();\n\t}\n}\n\n/*\n * Tracking of fully allocated slabs for debugging purposes.\n */\nstatic void add_full(struct kmem_cache *s,\n\tstruct kmem_cache_node *n, struct page *page)\n{\n\tif (!(s->flags & SLAB_STORE_USER))\n\t\treturn;\n\n\tlockdep_assert_held(&n->list_lock);\n\tlist_add(&page->slab_list, &n->full);\n}\n\nstatic void remove_full(struct kmem_cache *s, struct kmem_cache_node *n, struct page *page)\n{\n\tif (!(s->flags & SLAB_STORE_USER))\n\t\treturn;\n\n\tlockdep_assert_held(&n->list_lock);\n\tlist_del(&page->slab_list);\n}\n\n/* Tracking of the number of slabs for debugging purposes */\nstatic inline unsigned long slabs_node(struct kmem_cache *s, int node)\n{\n\tstruct kmem_cache_node *n = get_node(s, node);\n\n\treturn atomic_long_read(&n->nr_slabs);\n}\n\nstatic inline unsigned long node_nr_slabs(struct kmem_cache_node *n)\n{\n\treturn atomic_long_read(&n->nr_slabs);\n}\n\nstatic inline void inc_slabs_node(struct kmem_cache *s, int node, int objects)\n{\n\tstruct kmem_cache_node *n = get_node(s, node);\n\n\t/*\n\t * May be called early in order to allocate a slab for the\n\t * kmem_cache_node structure. Solve the chicken-egg\n\t * dilemma by deferring the increment of the count during\n\t * bootstrap (see early_kmem_cache_node_alloc).\n\t */\n\tif (likely(n)) {\n\t\tatomic_long_inc(&n->nr_slabs);\n\t\tatomic_long_add(objects, &n->total_objects);\n\t}\n}\nstatic inline void dec_slabs_node(struct kmem_cache *s, int node, int objects)\n{\n\tstruct kmem_cache_node *n = get_node(s, node);\n\n\tatomic_long_dec(&n->nr_slabs);\n\tatomic_long_sub(objects, &n->total_objects);\n}\n\n/* Object debug checks for alloc/free paths */\nstatic void setup_object_debug(struct kmem_cache *s, struct page *page,\n\t\t\t\t\t\t\t\tvoid *object)\n{\n\tif (!kmem_cache_debug_flags(s, SLAB_STORE_USER|SLAB_RED_ZONE|__OBJECT_POISON))\n\t\treturn;\n\n\tinit_object(s, object, SLUB_RED_INACTIVE);\n\tinit_tracking(s, object);\n}\n\nstatic\nvoid setup_page_debug(struct kmem_cache *s, struct page *page, void *addr)\n{\n\tif (!kmem_cache_debug_flags(s, SLAB_POISON))\n\t\treturn;\n\n\tmetadata_access_enable();\n\tmemset(kasan_reset_tag(addr), POISON_INUSE, page_size(page));\n\tmetadata_access_disable();\n}\n\nstatic inline int alloc_consistency_checks(struct kmem_cache *s,\n\t\t\t\t\tstruct page *page, void *object)\n{\n\tif (!check_slab(s, page))\n\t\treturn 0;\n\n\tif (!check_valid_pointer(s, page, object)) {\n\t\tobject_err(s, page, object, \"Freelist Pointer check fails\");\n\t\treturn 0;\n\t}\n\n\tif (!check_object(s, page, object, SLUB_RED_INACTIVE))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic noinline int alloc_debug_processing(struct kmem_cache *s,\n\t\t\t\t\tstruct page *page,\n\t\t\t\t\tvoid *object, unsigned long addr)\n{\n\tif (s->flags & SLAB_CONSISTENCY_CHECKS) {\n\t\tif (!alloc_consistency_checks(s, page, object))\n\t\t\tgoto bad;\n\t}\n\n\t/* Success perform special debug activities for allocs */\n\tif (s->flags & SLAB_STORE_USER)\n\t\tset_track(s, object, TRACK_ALLOC, addr);\n\ttrace(s, page, object, 1);\n\tinit_object(s, object, SLUB_RED_ACTIVE);\n\treturn 1;\n\nbad:\n\tif (PageSlab(page)) {\n\t\t/*\n\t\t * If this is a slab page then lets do the best we can\n\t\t * to avoid issues in the future. Marking all objects\n\t\t * as used avoids touching the remaining objects.\n\t\t */\n\t\tslab_fix(s, \"Marking all objects used\");\n\t\tpage->inuse = page->objects;\n\t\tpage->freelist = NULL;\n\t}\n\treturn 0;\n}\n\nstatic inline int free_consistency_checks(struct kmem_cache *s,\n\t\tstruct page *page, void *object, unsigned long addr)\n{\n\tif (!check_valid_pointer(s, page, object)) {\n\t\tslab_err(s, page, \"Invalid object pointer 0x%p\", object);\n\t\treturn 0;\n\t}\n\n\tif (on_freelist(s, page, object)) {\n\t\tobject_err(s, page, object, \"Object already free\");\n\t\treturn 0;\n\t}\n\n\tif (!check_object(s, page, object, SLUB_RED_ACTIVE))\n\t\treturn 0;\n\n\tif (unlikely(s != page->slab_cache)) {\n\t\tif (!PageSlab(page)) {\n\t\t\tslab_err(s, page, \"Attempt to free object(0x%p) outside of slab\",\n\t\t\t\t object);\n\t\t} else if (!page->slab_cache) {\n\t\t\tpr_err(\"SLUB <none>: no slab for object 0x%p.\\n\",\n\t\t\t       object);\n\t\t\tdump_stack();\n\t\t} else\n\t\t\tobject_err(s, page, object,\n\t\t\t\t\t\"page slab pointer corrupt.\");\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\n/* Supports checking bulk free of a constructed freelist */\nstatic noinline int free_debug_processing(\n\tstruct kmem_cache *s, struct page *page,\n\tvoid *head, void *tail, int bulk_cnt,\n\tunsigned long addr)\n{\n\tstruct kmem_cache_node *n = get_node(s, page_to_nid(page));\n\tvoid *object = head;\n\tint cnt = 0;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tspin_lock_irqsave(&n->list_lock, flags);\n\tslab_lock(page);\n\n\tif (s->flags & SLAB_CONSISTENCY_CHECKS) {\n\t\tif (!check_slab(s, page))\n\t\t\tgoto out;\n\t}\n\nnext_object:\n\tcnt++;\n\n\tif (s->flags & SLAB_CONSISTENCY_CHECKS) {\n\t\tif (!free_consistency_checks(s, page, object, addr))\n\t\t\tgoto out;\n\t}\n\n\tif (s->flags & SLAB_STORE_USER)\n\t\tset_track(s, object, TRACK_FREE, addr);\n\ttrace(s, page, object, 0);\n\t/* Freepointer not overwritten by init_object(), SLAB_POISON moved it */\n\tinit_object(s, object, SLUB_RED_INACTIVE);\n\n\t/* Reached end of constructed freelist yet? */\n\tif (object != tail) {\n\t\tobject = get_freepointer(s, object);\n\t\tgoto next_object;\n\t}\n\tret = 1;\n\nout:\n\tif (cnt != bulk_cnt)\n\t\tslab_err(s, page, \"Bulk freelist count(%d) invalid(%d)\\n\",\n\t\t\t bulk_cnt, cnt);\n\n\tslab_unlock(page);\n\tspin_unlock_irqrestore(&n->list_lock, flags);\n\tif (!ret)\n\t\tslab_fix(s, \"Object at 0x%p not freed\", object);\n\treturn ret;\n}\n\n/*\n * Parse a block of slub_debug options. Blocks are delimited by ';'\n *\n * @str:    start of block\n * @flags:  returns parsed flags, or DEBUG_DEFAULT_FLAGS if none specified\n * @slabs:  return start of list of slabs, or NULL when there's no list\n * @init:   assume this is initial parsing and not per-kmem-create parsing\n *\n * returns the start of next block if there's any, or NULL\n */\nstatic char *\nparse_slub_debug_flags(char *str, slab_flags_t *flags, char **slabs, bool init)\n{\n\tbool higher_order_disable = false;\n\n\t/* Skip any completely empty blocks */\n\twhile (*str && *str == ';')\n\t\tstr++;\n\n\tif (*str == ',') {\n\t\t/*\n\t\t * No options but restriction on slabs. This means full\n\t\t * debugging for slabs matching a pattern.\n\t\t */\n\t\t*flags = DEBUG_DEFAULT_FLAGS;\n\t\tgoto check_slabs;\n\t}\n\t*flags = 0;\n\n\t/* Determine which debug features should be switched on */\n\tfor (; *str && *str != ',' && *str != ';'; str++) {\n\t\tswitch (tolower(*str)) {\n\t\tcase '-':\n\t\t\t*flags = 0;\n\t\t\tbreak;\n\t\tcase 'f':\n\t\t\t*flags |= SLAB_CONSISTENCY_CHECKS;\n\t\t\tbreak;\n\t\tcase 'z':\n\t\t\t*flags |= SLAB_RED_ZONE;\n\t\t\tbreak;\n\t\tcase 'p':\n\t\t\t*flags |= SLAB_POISON;\n\t\t\tbreak;\n\t\tcase 'u':\n\t\t\t*flags |= SLAB_STORE_USER;\n\t\t\tbreak;\n\t\tcase 't':\n\t\t\t*flags |= SLAB_TRACE;\n\t\t\tbreak;\n\t\tcase 'a':\n\t\t\t*flags |= SLAB_FAILSLAB;\n\t\t\tbreak;\n\t\tcase 'o':\n\t\t\t/*\n\t\t\t * Avoid enabling debugging on caches if its minimum\n\t\t\t * order would increase as a result.\n\t\t\t */\n\t\t\thigher_order_disable = true;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (init)\n\t\t\t\tpr_err(\"slub_debug option '%c' unknown. skipped\\n\", *str);\n\t\t}\n\t}\ncheck_slabs:\n\tif (*str == ',')\n\t\t*slabs = ++str;\n\telse\n\t\t*slabs = NULL;\n\n\t/* Skip over the slab list */\n\twhile (*str && *str != ';')\n\t\tstr++;\n\n\t/* Skip any completely empty blocks */\n\twhile (*str && *str == ';')\n\t\tstr++;\n\n\tif (init && higher_order_disable)\n\t\tdisable_higher_order_debug = 1;\n\n\tif (*str)\n\t\treturn str;\n\telse\n\t\treturn NULL;\n}\n\nstatic int __init setup_slub_debug(char *str)\n{\n\tslab_flags_t flags;\n\tchar *saved_str;\n\tchar *slab_list;\n\tbool global_slub_debug_changed = false;\n\tbool slab_list_specified = false;\n\n\tslub_debug = DEBUG_DEFAULT_FLAGS;\n\tif (*str++ != '=' || !*str)\n\t\t/*\n\t\t * No options specified. Switch on full debugging.\n\t\t */\n\t\tgoto out;\n\n\tsaved_str = str;\n\twhile (str) {\n\t\tstr = parse_slub_debug_flags(str, &flags, &slab_list, true);\n\n\t\tif (!slab_list) {\n\t\t\tslub_debug = flags;\n\t\t\tglobal_slub_debug_changed = true;\n\t\t} else {\n\t\t\tslab_list_specified = true;\n\t\t}\n\t}\n\n\t/*\n\t * For backwards compatibility, a single list of flags with list of\n\t * slabs means debugging is only enabled for those slabs, so the global\n\t * slub_debug should be 0. We can extended that to multiple lists as\n\t * long as there is no option specifying flags without a slab list.\n\t */\n\tif (slab_list_specified) {\n\t\tif (!global_slub_debug_changed)\n\t\t\tslub_debug = 0;\n\t\tslub_debug_string = saved_str;\n\t}\nout:\n\tif (slub_debug != 0 || slub_debug_string)\n\t\tstatic_branch_enable(&slub_debug_enabled);\n\tif ((static_branch_unlikely(&init_on_alloc) ||\n\t     static_branch_unlikely(&init_on_free)) &&\n\t    (slub_debug & SLAB_POISON))\n\t\tpr_info(\"mem auto-init: SLAB_POISON will take precedence over init_on_alloc/init_on_free\\n\");\n\treturn 1;\n}\n\n__setup(\"slub_debug\", setup_slub_debug);\n\n/*\n * kmem_cache_flags - apply debugging options to the cache\n * @object_size:\tthe size of an object without meta data\n * @flags:\t\tflags to set\n * @name:\t\tname of the cache\n *\n * Debug option(s) are applied to @flags. In addition to the debug\n * option(s), if a slab name (or multiple) is specified i.e.\n * slub_debug=<Debug-Options>,<slab name1>,<slab name2> ...\n * then only the select slabs will receive the debug option(s).\n */\nslab_flags_t kmem_cache_flags(unsigned int object_size,\n\tslab_flags_t flags, const char *name)\n{\n\tchar *iter;\n\tsize_t len;\n\tchar *next_block;\n\tslab_flags_t block_flags;\n\tslab_flags_t slub_debug_local = slub_debug;\n\n\t/*\n\t * If the slab cache is for debugging (e.g. kmemleak) then\n\t * don't store user (stack trace) information by default,\n\t * but let the user enable it via the command line below.\n\t */\n\tif (flags & SLAB_NOLEAKTRACE)\n\t\tslub_debug_local &= ~SLAB_STORE_USER;\n\n\tlen = strlen(name);\n\tnext_block = slub_debug_string;\n\t/* Go through all blocks of debug options, see if any matches our slab's name */\n\twhile (next_block) {\n\t\tnext_block = parse_slub_debug_flags(next_block, &block_flags, &iter, false);\n\t\tif (!iter)\n\t\t\tcontinue;\n\t\t/* Found a block that has a slab list, search it */\n\t\twhile (*iter) {\n\t\t\tchar *end, *glob;\n\t\t\tsize_t cmplen;\n\n\t\t\tend = strchrnul(iter, ',');\n\t\t\tif (next_block && next_block < end)\n\t\t\t\tend = next_block - 1;\n\n\t\t\tglob = strnchr(iter, end - iter, '*');\n\t\t\tif (glob)\n\t\t\t\tcmplen = glob - iter;\n\t\t\telse\n\t\t\t\tcmplen = max_t(size_t, len, (end - iter));\n\n\t\t\tif (!strncmp(name, iter, cmplen)) {\n\t\t\t\tflags |= block_flags;\n\t\t\t\treturn flags;\n\t\t\t}\n\n\t\t\tif (!*end || *end == ';')\n\t\t\t\tbreak;\n\t\t\titer = end + 1;\n\t\t}\n\t}\n\n\treturn flags | slub_debug_local;\n}\n#else /* !CONFIG_SLUB_DEBUG */\nstatic inline void setup_object_debug(struct kmem_cache *s,\n\t\t\tstruct page *page, void *object) {}\nstatic inline\nvoid setup_page_debug(struct kmem_cache *s, struct page *page, void *addr) {}\n\nstatic inline int alloc_debug_processing(struct kmem_cache *s,\n\tstruct page *page, void *object, unsigned long addr) { return 0; }\n\nstatic inline int free_debug_processing(\n\tstruct kmem_cache *s, struct page *page,\n\tvoid *head, void *tail, int bulk_cnt,\n\tunsigned long addr) { return 0; }\n\nstatic inline int slab_pad_check(struct kmem_cache *s, struct page *page)\n\t\t\t{ return 1; }\nstatic inline int check_object(struct kmem_cache *s, struct page *page,\n\t\t\tvoid *object, u8 val) { return 1; }\nstatic inline void add_full(struct kmem_cache *s, struct kmem_cache_node *n,\n\t\t\t\t\tstruct page *page) {}\nstatic inline void remove_full(struct kmem_cache *s, struct kmem_cache_node *n,\n\t\t\t\t\tstruct page *page) {}\nslab_flags_t kmem_cache_flags(unsigned int object_size,\n\tslab_flags_t flags, const char *name)\n{\n\treturn flags;\n}\n#define slub_debug 0\n\n#define disable_higher_order_debug 0\n\nstatic inline unsigned long slabs_node(struct kmem_cache *s, int node)\n\t\t\t\t\t\t\t{ return 0; }\nstatic inline unsigned long node_nr_slabs(struct kmem_cache_node *n)\n\t\t\t\t\t\t\t{ return 0; }\nstatic inline void inc_slabs_node(struct kmem_cache *s, int node,\n\t\t\t\t\t\t\tint objects) {}\nstatic inline void dec_slabs_node(struct kmem_cache *s, int node,\n\t\t\t\t\t\t\tint objects) {}\n\nstatic bool freelist_corrupted(struct kmem_cache *s, struct page *page,\n\t\t\t       void **freelist, void *nextfree)\n{\n\treturn false;\n}\n#endif /* CONFIG_SLUB_DEBUG */\n\n/*\n * Hooks for other subsystems that check memory allocations. In a typical\n * production configuration these hooks all should produce no code at all.\n */\nstatic inline void *kmalloc_large_node_hook(void *ptr, size_t size, gfp_t flags)\n{\n\tptr = kasan_kmalloc_large(ptr, size, flags);\n\t/* As ptr might get tagged, call kmemleak hook after KASAN. */\n\tkmemleak_alloc(ptr, size, 1, flags);\n\treturn ptr;\n}\n\nstatic __always_inline void kfree_hook(void *x)\n{\n\tkmemleak_free(x);\n\tkasan_kfree_large(x);\n}\n\nstatic __always_inline bool slab_free_hook(struct kmem_cache *s,\n\t\t\t\t\t\tvoid *x, bool init)\n{\n\tkmemleak_free_recursive(x, s->flags);\n\n\t/*\n\t * Trouble is that we may no longer disable interrupts in the fast path\n\t * So in order to make the debug calls that expect irqs to be\n\t * disabled we need to disable interrupts temporarily.\n\t */\n#ifdef CONFIG_LOCKDEP\n\t{\n\t\tunsigned long flags;\n\n\t\tlocal_irq_save(flags);\n\t\tdebug_check_no_locks_freed(x, s->object_size);\n\t\tlocal_irq_restore(flags);\n\t}\n#endif\n\tif (!(s->flags & SLAB_DEBUG_OBJECTS))\n\t\tdebug_check_no_obj_freed(x, s->object_size);\n\n\t/* Use KCSAN to help debug racy use-after-free. */\n\tif (!(s->flags & SLAB_TYPESAFE_BY_RCU))\n\t\t__kcsan_check_access(x, s->object_size,\n\t\t\t\t     KCSAN_ACCESS_WRITE | KCSAN_ACCESS_ASSERT);\n\n\t/*\n\t * As memory initialization might be integrated into KASAN,\n\t * kasan_slab_free and initialization memset's must be\n\t * kept together to avoid discrepancies in behavior.\n\t *\n\t * The initialization memset's clear the object and the metadata,\n\t * but don't touch the SLAB redzone.\n\t */\n\tif (init) {\n\t\tint rsize;\n\n\t\tif (!kasan_has_integrated_init())\n\t\t\tmemset(kasan_reset_tag(x), 0, s->object_size);\n\t\trsize = (s->flags & SLAB_RED_ZONE) ? s->red_left_pad : 0;\n\t\tmemset((char *)kasan_reset_tag(x) + s->inuse, 0,\n\t\t       s->size - s->inuse - rsize);\n\t}\n\t/* KASAN might put x into memory quarantine, delaying its reuse. */\n\treturn kasan_slab_free(s, x, init);\n}\n\nstatic inline bool slab_free_freelist_hook(struct kmem_cache *s,\n\t\t\t\t\t   void **head, void **tail)\n{\n\n\tvoid *object;\n\tvoid *next = *head;\n\tvoid *old_tail = *tail ? *tail : *head;\n\n\tif (is_kfence_address(next)) {\n\t\tslab_free_hook(s, next, false);\n\t\treturn true;\n\t}\n\n\t/* Head and tail of the reconstructed freelist */\n\t*head = NULL;\n\t*tail = NULL;\n\n\tdo {\n\t\tobject = next;\n\t\tnext = get_freepointer(s, object);\n\n\t\t/* If object's reuse doesn't have to be delayed */\n\t\tif (!slab_free_hook(s, object, slab_want_init_on_free(s))) {\n\t\t\t/* Move object to the new freelist */\n\t\t\tset_freepointer(s, object, *head);\n\t\t\t*head = object;\n\t\t\tif (!*tail)\n\t\t\t\t*tail = object;\n\t\t}\n\t} while (object != old_tail);\n\n\tif (*head == *tail)\n\t\t*tail = NULL;\n\n\treturn *head != NULL;\n}\n\nstatic void *setup_object(struct kmem_cache *s, struct page *page,\n\t\t\t\tvoid *object)\n{\n\tsetup_object_debug(s, page, object);\n\tobject = kasan_init_slab_obj(s, object);\n\tif (unlikely(s->ctor)) {\n\t\tkasan_unpoison_object_data(s, object);\n\t\ts->ctor(object);\n\t\tkasan_poison_object_data(s, object);\n\t}\n\treturn object;\n}\n\n/*\n * Slab allocation and freeing\n */\nstatic inline struct page *alloc_slab_page(struct kmem_cache *s,\n\t\tgfp_t flags, int node, struct kmem_cache_order_objects oo)\n{\n\tstruct page *page;\n\tunsigned int order = oo_order(oo);\n\n\tif (node == NUMA_NO_NODE)\n\t\tpage = alloc_pages(flags, order);\n\telse\n\t\tpage = __alloc_pages_node(node, flags, order);\n\n\treturn page;\n}\n\n#ifdef CONFIG_SLAB_FREELIST_RANDOM\n/* Pre-initialize the random sequence cache */\nstatic int init_cache_random_seq(struct kmem_cache *s)\n{\n\tunsigned int count = oo_objects(s->oo);\n\tint err;\n\n\t/* Bailout if already initialised */\n\tif (s->random_seq)\n\t\treturn 0;\n\n\terr = cache_random_seq_create(s, count, GFP_KERNEL);\n\tif (err) {\n\t\tpr_err(\"SLUB: Unable to initialize free list for %s\\n\",\n\t\t\ts->name);\n\t\treturn err;\n\t}\n\n\t/* Transform to an offset on the set of pages */\n\tif (s->random_seq) {\n\t\tunsigned int i;\n\n\t\tfor (i = 0; i < count; i++)\n\t\t\ts->random_seq[i] *= s->size;\n\t}\n\treturn 0;\n}\n\n/* Initialize each random sequence freelist per cache */\nstatic void __init init_freelist_randomization(void)\n{\n\tstruct kmem_cache *s;\n\n\tmutex_lock(&slab_mutex);\n\n\tlist_for_each_entry(s, &slab_caches, list)\n\t\tinit_cache_random_seq(s);\n\n\tmutex_unlock(&slab_mutex);\n}\n\n/* Get the next entry on the pre-computed freelist randomized */\nstatic void *next_freelist_entry(struct kmem_cache *s, struct page *page,\n\t\t\t\tunsigned long *pos, void *start,\n\t\t\t\tunsigned long page_limit,\n\t\t\t\tunsigned long freelist_count)\n{\n\tunsigned int idx;\n\n\t/*\n\t * If the target page allocation failed, the number of objects on the\n\t * page might be smaller than the usual size defined by the cache.\n\t */\n\tdo {\n\t\tidx = s->random_seq[*pos];\n\t\t*pos += 1;\n\t\tif (*pos >= freelist_count)\n\t\t\t*pos = 0;\n\t} while (unlikely(idx >= page_limit));\n\n\treturn (char *)start + idx;\n}\n\n/* Shuffle the single linked freelist based on a random pre-computed sequence */\nstatic bool shuffle_freelist(struct kmem_cache *s, struct page *page)\n{\n\tvoid *start;\n\tvoid *cur;\n\tvoid *next;\n\tunsigned long idx, pos, page_limit, freelist_count;\n\n\tif (page->objects < 2 || !s->random_seq)\n\t\treturn false;\n\n\tfreelist_count = oo_objects(s->oo);\n\tpos = get_random_int() % freelist_count;\n\n\tpage_limit = page->objects * s->size;\n\tstart = fixup_red_left(s, page_address(page));\n\n\t/* First entry is used as the base of the freelist */\n\tcur = next_freelist_entry(s, page, &pos, start, page_limit,\n\t\t\t\tfreelist_count);\n\tcur = setup_object(s, page, cur);\n\tpage->freelist = cur;\n\n\tfor (idx = 1; idx < page->objects; idx++) {\n\t\tnext = next_freelist_entry(s, page, &pos, start, page_limit,\n\t\t\tfreelist_count);\n\t\tnext = setup_object(s, page, next);\n\t\tset_freepointer(s, cur, next);\n\t\tcur = next;\n\t}\n\tset_freepointer(s, cur, NULL);\n\n\treturn true;\n}\n#else\nstatic inline int init_cache_random_seq(struct kmem_cache *s)\n{\n\treturn 0;\n}\nstatic inline void init_freelist_randomization(void) { }\nstatic inline bool shuffle_freelist(struct kmem_cache *s, struct page *page)\n{\n\treturn false;\n}\n#endif /* CONFIG_SLAB_FREELIST_RANDOM */\n\nstatic struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)\n{\n\tstruct page *page;\n\tstruct kmem_cache_order_objects oo = s->oo;\n\tgfp_t alloc_gfp;\n\tvoid *start, *p, *next;\n\tint idx;\n\tbool shuffle;\n\n\tflags &= gfp_allowed_mask;\n\n\tif (gfpflags_allow_blocking(flags))\n\t\tlocal_irq_enable();\n\n\tflags |= s->allocflags;\n\n\t/*\n\t * Let the initial higher-order allocation fail under memory pressure\n\t * so we fall-back to the minimum order allocation.\n\t */\n\talloc_gfp = (flags | __GFP_NOWARN | __GFP_NORETRY) & ~__GFP_NOFAIL;\n\tif ((alloc_gfp & __GFP_DIRECT_RECLAIM) && oo_order(oo) > oo_order(s->min))\n\t\talloc_gfp = (alloc_gfp | __GFP_NOMEMALLOC) & ~(__GFP_RECLAIM|__GFP_NOFAIL);\n\n\tpage = alloc_slab_page(s, alloc_gfp, node, oo);\n\tif (unlikely(!page)) {\n\t\too = s->min;\n\t\talloc_gfp = flags;\n\t\t/*\n\t\t * Allocation may have failed due to fragmentation.\n\t\t * Try a lower order alloc if possible\n\t\t */\n\t\tpage = alloc_slab_page(s, alloc_gfp, node, oo);\n\t\tif (unlikely(!page))\n\t\t\tgoto out;\n\t\tstat(s, ORDER_FALLBACK);\n\t}\n\n\tpage->objects = oo_objects(oo);\n\n\taccount_slab_page(page, oo_order(oo), s, flags);\n\n\tpage->slab_cache = s;\n\t__SetPageSlab(page);\n\tif (page_is_pfmemalloc(page))\n\t\tSetPageSlabPfmemalloc(page);\n\n\tkasan_poison_slab(page);\n\n\tstart = page_address(page);\n\n\tsetup_page_debug(s, page, start);\n\n\tshuffle = shuffle_freelist(s, page);\n\n\tif (!shuffle) {\n\t\tstart = fixup_red_left(s, start);\n\t\tstart = setup_object(s, page, start);\n\t\tpage->freelist = start;\n\t\tfor (idx = 0, p = start; idx < page->objects - 1; idx++) {\n\t\t\tnext = p + s->size;\n\t\t\tnext = setup_object(s, page, next);\n\t\t\tset_freepointer(s, p, next);\n\t\t\tp = next;\n\t\t}\n\t\tset_freepointer(s, p, NULL);\n\t}\n\n\tpage->inuse = page->objects;\n\tpage->frozen = 1;\n\nout:\n\tif (gfpflags_allow_blocking(flags))\n\t\tlocal_irq_disable();\n\tif (!page)\n\t\treturn NULL;\n\n\tinc_slabs_node(s, page_to_nid(page), page->objects);\n\n\treturn page;\n}\n\nstatic struct page *new_slab(struct kmem_cache *s, gfp_t flags, int node)\n{\n\tif (unlikely(flags & GFP_SLAB_BUG_MASK))\n\t\tflags = kmalloc_fix_flags(flags);\n\n\treturn allocate_slab(s,\n\t\tflags & (GFP_RECLAIM_MASK | GFP_CONSTRAINT_MASK), node);\n}\n\nstatic void __free_slab(struct kmem_cache *s, struct page *page)\n{\n\tint order = compound_order(page);\n\tint pages = 1 << order;\n\n\tif (kmem_cache_debug_flags(s, SLAB_CONSISTENCY_CHECKS)) {\n\t\tvoid *p;\n\n\t\tslab_pad_check(s, page);\n\t\tfor_each_object(p, s, page_address(page),\n\t\t\t\t\t\tpage->objects)\n\t\t\tcheck_object(s, page, p, SLUB_RED_INACTIVE);\n\t}\n\n\t__ClearPageSlabPfmemalloc(page);\n\t__ClearPageSlab(page);\n\t/* In union with page->mapping where page allocator expects NULL */\n\tpage->slab_cache = NULL;\n\tif (current->reclaim_state)\n\t\tcurrent->reclaim_state->reclaimed_slab += pages;\n\tunaccount_slab_page(page, order, s);\n\t__free_pages(page, order);\n}\n\nstatic void rcu_free_slab(struct rcu_head *h)\n{\n\tstruct page *page = container_of(h, struct page, rcu_head);\n\n\t__free_slab(page->slab_cache, page);\n}\n\nstatic void free_slab(struct kmem_cache *s, struct page *page)\n{\n\tif (unlikely(s->flags & SLAB_TYPESAFE_BY_RCU)) {\n\t\tcall_rcu(&page->rcu_head, rcu_free_slab);\n\t} else\n\t\t__free_slab(s, page);\n}\n\nstatic void discard_slab(struct kmem_cache *s, struct page *page)\n{\n\tdec_slabs_node(s, page_to_nid(page), page->objects);\n\tfree_slab(s, page);\n}\n\n/*\n * Management of partially allocated slabs.\n */\nstatic inline void\n__add_partial(struct kmem_cache_node *n, struct page *page, int tail)\n{\n\tn->nr_partial++;\n\tif (tail == DEACTIVATE_TO_TAIL)\n\t\tlist_add_tail(&page->slab_list, &n->partial);\n\telse\n\t\tlist_add(&page->slab_list, &n->partial);\n}\n\nstatic inline void add_partial(struct kmem_cache_node *n,\n\t\t\t\tstruct page *page, int tail)\n{\n\tlockdep_assert_held(&n->list_lock);\n\t__add_partial(n, page, tail);\n}\n\nstatic inline void remove_partial(struct kmem_cache_node *n,\n\t\t\t\t\tstruct page *page)\n{\n\tlockdep_assert_held(&n->list_lock);\n\tlist_del(&page->slab_list);\n\tn->nr_partial--;\n}\n\n/*\n * Remove slab from the partial list, freeze it and\n * return the pointer to the freelist.\n *\n * Returns a list of objects or NULL if it fails.\n */\nstatic inline void *acquire_slab(struct kmem_cache *s,\n\t\tstruct kmem_cache_node *n, struct page *page,\n\t\tint mode, int *objects)\n{\n\tvoid *freelist;\n\tunsigned long counters;\n\tstruct page new;\n\n\tlockdep_assert_held(&n->list_lock);\n\n\t/*\n\t * Zap the freelist and set the frozen bit.\n\t * The old freelist is the list of objects for the\n\t * per cpu allocation list.\n\t */\n\tfreelist = page->freelist;\n\tcounters = page->counters;\n\tnew.counters = counters;\n\t*objects = new.objects - new.inuse;\n\tif (mode) {\n\t\tnew.inuse = page->objects;\n\t\tnew.freelist = NULL;\n\t} else {\n\t\tnew.freelist = freelist;\n\t}\n\n\tVM_BUG_ON(new.frozen);\n\tnew.frozen = 1;\n\n\tif (!__cmpxchg_double_slab(s, page,\n\t\t\tfreelist, counters,\n\t\t\tnew.freelist, new.counters,\n\t\t\t\"acquire_slab\"))\n\t\treturn NULL;\n\n\tremove_partial(n, page);\n\tWARN_ON(!freelist);\n\treturn freelist;\n}\n\nstatic void put_cpu_partial(struct kmem_cache *s, struct page *page, int drain);\nstatic inline bool pfmemalloc_match(struct page *page, gfp_t gfpflags);\n\n/*\n * Try to allocate a partial slab from a specific node.\n */\nstatic void *get_partial_node(struct kmem_cache *s, struct kmem_cache_node *n,\n\t\t\t\tstruct kmem_cache_cpu *c, gfp_t flags)\n{\n\tstruct page *page, *page2;\n\tvoid *object = NULL;\n\tunsigned int available = 0;\n\tint objects;\n\n\t/*\n\t * Racy check. If we mistakenly see no partial slabs then we\n\t * just allocate an empty slab. If we mistakenly try to get a\n\t * partial slab and there is none available then get_partial()\n\t * will return NULL.\n\t */\n\tif (!n || !n->nr_partial)\n\t\treturn NULL;\n\n\tspin_lock(&n->list_lock);\n\tlist_for_each_entry_safe(page, page2, &n->partial, slab_list) {\n\t\tvoid *t;\n\n\t\tif (!pfmemalloc_match(page, flags))\n\t\t\tcontinue;\n\n\t\tt = acquire_slab(s, n, page, object == NULL, &objects);\n\t\tif (!t)\n\t\t\tbreak;\n\n\t\tavailable += objects;\n\t\tif (!object) {\n\t\t\tc->page = page;\n\t\t\tstat(s, ALLOC_FROM_PARTIAL);\n\t\t\tobject = t;\n\t\t} else {\n\t\t\tput_cpu_partial(s, page, 0);\n\t\t\tstat(s, CPU_PARTIAL_NODE);\n\t\t}\n\t\tif (!kmem_cache_has_cpu_partial(s)\n\t\t\t|| available > slub_cpu_partial(s) / 2)\n\t\t\tbreak;\n\n\t}\n\tspin_unlock(&n->list_lock);\n\treturn object;\n}\n\n/*\n * Get a page from somewhere. Search in increasing NUMA distances.\n */\nstatic void *get_any_partial(struct kmem_cache *s, gfp_t flags,\n\t\tstruct kmem_cache_cpu *c)\n{\n#ifdef CONFIG_NUMA\n\tstruct zonelist *zonelist;\n\tstruct zoneref *z;\n\tstruct zone *zone;\n\tenum zone_type highest_zoneidx = gfp_zone(flags);\n\tvoid *object;\n\tunsigned int cpuset_mems_cookie;\n\n\t/*\n\t * The defrag ratio allows a configuration of the tradeoffs between\n\t * inter node defragmentation and node local allocations. A lower\n\t * defrag_ratio increases the tendency to do local allocations\n\t * instead of attempting to obtain partial slabs from other nodes.\n\t *\n\t * If the defrag_ratio is set to 0 then kmalloc() always\n\t * returns node local objects. If the ratio is higher then kmalloc()\n\t * may return off node objects because partial slabs are obtained\n\t * from other nodes and filled up.\n\t *\n\t * If /sys/kernel/slab/xx/remote_node_defrag_ratio is set to 100\n\t * (which makes defrag_ratio = 1000) then every (well almost)\n\t * allocation will first attempt to defrag slab caches on other nodes.\n\t * This means scanning over all nodes to look for partial slabs which\n\t * may be expensive if we do it every time we are trying to find a slab\n\t * with available objects.\n\t */\n\tif (!s->remote_node_defrag_ratio ||\n\t\t\tget_cycles() % 1024 > s->remote_node_defrag_ratio)\n\t\treturn NULL;\n\n\tdo {\n\t\tcpuset_mems_cookie = read_mems_allowed_begin();\n\t\tzonelist = node_zonelist(mempolicy_slab_node(), flags);\n\t\tfor_each_zone_zonelist(zone, z, zonelist, highest_zoneidx) {\n\t\t\tstruct kmem_cache_node *n;\n\n\t\t\tn = get_node(s, zone_to_nid(zone));\n\n\t\t\tif (n && cpuset_zone_allowed(zone, flags) &&\n\t\t\t\t\tn->nr_partial > s->min_partial) {\n\t\t\t\tobject = get_partial_node(s, n, c, flags);\n\t\t\t\tif (object) {\n\t\t\t\t\t/*\n\t\t\t\t\t * Don't check read_mems_allowed_retry()\n\t\t\t\t\t * here - if mems_allowed was updated in\n\t\t\t\t\t * parallel, that was a harmless race\n\t\t\t\t\t * between allocation and the cpuset\n\t\t\t\t\t * update\n\t\t\t\t\t */\n\t\t\t\t\treturn object;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} while (read_mems_allowed_retry(cpuset_mems_cookie));\n#endif\t/* CONFIG_NUMA */\n\treturn NULL;\n}\n\n/*\n * Get a partial page, lock it and return it.\n */\nstatic void *get_partial(struct kmem_cache *s, gfp_t flags, int node,\n\t\tstruct kmem_cache_cpu *c)\n{\n\tvoid *object;\n\tint searchnode = node;\n\n\tif (node == NUMA_NO_NODE)\n\t\tsearchnode = numa_mem_id();\n\n\tobject = get_partial_node(s, get_node(s, searchnode), c, flags);\n\tif (object || node != NUMA_NO_NODE)\n\t\treturn object;\n\n\treturn get_any_partial(s, flags, c);\n}\n\n#ifdef CONFIG_PREEMPTION\n/*\n * Calculate the next globally unique transaction for disambiguation\n * during cmpxchg. The transactions start with the cpu number and are then\n * incremented by CONFIG_NR_CPUS.\n */\n#define TID_STEP  roundup_pow_of_two(CONFIG_NR_CPUS)\n#else\n/*\n * No preemption supported therefore also no need to check for\n * different cpus.\n */\n#define TID_STEP 1\n#endif\n\nstatic inline unsigned long next_tid(unsigned long tid)\n{\n\treturn tid + TID_STEP;\n}\n\n#ifdef SLUB_DEBUG_CMPXCHG\nstatic inline unsigned int tid_to_cpu(unsigned long tid)\n{\n\treturn tid % TID_STEP;\n}\n\nstatic inline unsigned long tid_to_event(unsigned long tid)\n{\n\treturn tid / TID_STEP;\n}\n#endif\n\nstatic inline unsigned int init_tid(int cpu)\n{\n\treturn cpu;\n}\n\nstatic inline void note_cmpxchg_failure(const char *n,\n\t\tconst struct kmem_cache *s, unsigned long tid)\n{\n#ifdef SLUB_DEBUG_CMPXCHG\n\tunsigned long actual_tid = __this_cpu_read(s->cpu_slab->tid);\n\n\tpr_info(\"%s %s: cmpxchg redo \", n, s->name);\n\n#ifdef CONFIG_PREEMPTION\n\tif (tid_to_cpu(tid) != tid_to_cpu(actual_tid))\n\t\tpr_warn(\"due to cpu change %d -> %d\\n\",\n\t\t\ttid_to_cpu(tid), tid_to_cpu(actual_tid));\n\telse\n#endif\n\tif (tid_to_event(tid) != tid_to_event(actual_tid))\n\t\tpr_warn(\"due to cpu running other code. Event %ld->%ld\\n\",\n\t\t\ttid_to_event(tid), tid_to_event(actual_tid));\n\telse\n\t\tpr_warn(\"for unknown reason: actual=%lx was=%lx target=%lx\\n\",\n\t\t\tactual_tid, tid, next_tid(tid));\n#endif\n\tstat(s, CMPXCHG_DOUBLE_CPU_FAIL);\n}\n\nstatic void init_kmem_cache_cpus(struct kmem_cache *s)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tper_cpu_ptr(s->cpu_slab, cpu)->tid = init_tid(cpu);\n}\n\n/*\n * Remove the cpu slab\n */\nstatic void deactivate_slab(struct kmem_cache *s, struct page *page,\n\t\t\t\tvoid *freelist, struct kmem_cache_cpu *c)\n{\n\tenum slab_modes { M_NONE, M_PARTIAL, M_FULL, M_FREE };\n\tstruct kmem_cache_node *n = get_node(s, page_to_nid(page));\n\tint lock = 0, free_delta = 0;\n\tenum slab_modes l = M_NONE, m = M_NONE;\n\tvoid *nextfree, *freelist_iter, *freelist_tail;\n\tint tail = DEACTIVATE_TO_HEAD;\n\tstruct page new;\n\tstruct page old;\n\n\tif (page->freelist) {\n\t\tstat(s, DEACTIVATE_REMOTE_FREES);\n\t\ttail = DEACTIVATE_TO_TAIL;\n\t}\n\n\t/*\n\t * Stage one: Count the objects on cpu's freelist as free_delta and\n\t * remember the last object in freelist_tail for later splicing.\n\t */\n\tfreelist_tail = NULL;\n\tfreelist_iter = freelist;\n\twhile (freelist_iter) {\n\t\tnextfree = get_freepointer(s, freelist_iter);\n\n\t\t/*\n\t\t * If 'nextfree' is invalid, it is possible that the object at\n\t\t * 'freelist_iter' is already corrupted.  So isolate all objects\n\t\t * starting at 'freelist_iter' by skipping them.\n\t\t */\n\t\tif (freelist_corrupted(s, page, &freelist_iter, nextfree))\n\t\t\tbreak;\n\n\t\tfreelist_tail = freelist_iter;\n\t\tfree_delta++;\n\n\t\tfreelist_iter = nextfree;\n\t}\n\n\t/*\n\t * Stage two: Unfreeze the page while splicing the per-cpu\n\t * freelist to the head of page's freelist.\n\t *\n\t * Ensure that the page is unfrozen while the list presence\n\t * reflects the actual number of objects during unfreeze.\n\t *\n\t * We setup the list membership and then perform a cmpxchg\n\t * with the count. If there is a mismatch then the page\n\t * is not unfrozen but the page is on the wrong list.\n\t *\n\t * Then we restart the process which may have to remove\n\t * the page from the list that we just put it on again\n\t * because the number of objects in the slab may have\n\t * changed.\n\t */\nredo:\n\n\told.freelist = READ_ONCE(page->freelist);\n\told.counters = READ_ONCE(page->counters);\n\tVM_BUG_ON(!old.frozen);\n\n\t/* Determine target state of the slab */\n\tnew.counters = old.counters;\n\tif (freelist_tail) {\n\t\tnew.inuse -= free_delta;\n\t\tset_freepointer(s, freelist_tail, old.freelist);\n\t\tnew.freelist = freelist;\n\t} else\n\t\tnew.freelist = old.freelist;\n\n\tnew.frozen = 0;\n\n\tif (!new.inuse && n->nr_partial >= s->min_partial)\n\t\tm = M_FREE;\n\telse if (new.freelist) {\n\t\tm = M_PARTIAL;\n\t\tif (!lock) {\n\t\t\tlock = 1;\n\t\t\t/*\n\t\t\t * Taking the spinlock removes the possibility\n\t\t\t * that acquire_slab() will see a slab page that\n\t\t\t * is frozen\n\t\t\t */\n\t\t\tspin_lock(&n->list_lock);\n\t\t}\n\t} else {\n\t\tm = M_FULL;\n\t\tif (kmem_cache_debug_flags(s, SLAB_STORE_USER) && !lock) {\n\t\t\tlock = 1;\n\t\t\t/*\n\t\t\t * This also ensures that the scanning of full\n\t\t\t * slabs from diagnostic functions will not see\n\t\t\t * any frozen slabs.\n\t\t\t */\n\t\t\tspin_lock(&n->list_lock);\n\t\t}\n\t}\n\n\tif (l != m) {\n\t\tif (l == M_PARTIAL)\n\t\t\tremove_partial(n, page);\n\t\telse if (l == M_FULL)\n\t\t\tremove_full(s, n, page);\n\n\t\tif (m == M_PARTIAL)\n\t\t\tadd_partial(n, page, tail);\n\t\telse if (m == M_FULL)\n\t\t\tadd_full(s, n, page);\n\t}\n\n\tl = m;\n\tif (!__cmpxchg_double_slab(s, page,\n\t\t\t\told.freelist, old.counters,\n\t\t\t\tnew.freelist, new.counters,\n\t\t\t\t\"unfreezing slab\"))\n\t\tgoto redo;\n\n\tif (lock)\n\t\tspin_unlock(&n->list_lock);\n\n\tif (m == M_PARTIAL)\n\t\tstat(s, tail);\n\telse if (m == M_FULL)\n\t\tstat(s, DEACTIVATE_FULL);\n\telse if (m == M_FREE) {\n\t\tstat(s, DEACTIVATE_EMPTY);\n\t\tdiscard_slab(s, page);\n\t\tstat(s, FREE_SLAB);\n\t}\n\n\tc->page = NULL;\n\tc->freelist = NULL;\n}\n\n/*\n * Unfreeze all the cpu partial slabs.\n *\n * This function must be called with interrupts disabled\n * for the cpu using c (or some other guarantee must be there\n * to guarantee no concurrent accesses).\n */\nstatic void unfreeze_partials(struct kmem_cache *s,\n\t\tstruct kmem_cache_cpu *c)\n{\n#ifdef CONFIG_SLUB_CPU_PARTIAL\n\tstruct kmem_cache_node *n = NULL, *n2 = NULL;\n\tstruct page *page, *discard_page = NULL;\n\n\twhile ((page = slub_percpu_partial(c))) {\n\t\tstruct page new;\n\t\tstruct page old;\n\n\t\tslub_set_percpu_partial(c, page);\n\n\t\tn2 = get_node(s, page_to_nid(page));\n\t\tif (n != n2) {\n\t\t\tif (n)\n\t\t\t\tspin_unlock(&n->list_lock);\n\n\t\t\tn = n2;\n\t\t\tspin_lock(&n->list_lock);\n\t\t}\n\n\t\tdo {\n\n\t\t\told.freelist = page->freelist;\n\t\t\told.counters = page->counters;\n\t\t\tVM_BUG_ON(!old.frozen);\n\n\t\t\tnew.counters = old.counters;\n\t\t\tnew.freelist = old.freelist;\n\n\t\t\tnew.frozen = 0;\n\n\t\t} while (!__cmpxchg_double_slab(s, page,\n\t\t\t\told.freelist, old.counters,\n\t\t\t\tnew.freelist, new.counters,\n\t\t\t\t\"unfreezing slab\"));\n\n\t\tif (unlikely(!new.inuse && n->nr_partial >= s->min_partial)) {\n\t\t\tpage->next = discard_page;\n\t\t\tdiscard_page = page;\n\t\t} else {\n\t\t\tadd_partial(n, page, DEACTIVATE_TO_TAIL);\n\t\t\tstat(s, FREE_ADD_PARTIAL);\n\t\t}\n\t}\n\n\tif (n)\n\t\tspin_unlock(&n->list_lock);\n\n\twhile (discard_page) {\n\t\tpage = discard_page;\n\t\tdiscard_page = discard_page->next;\n\n\t\tstat(s, DEACTIVATE_EMPTY);\n\t\tdiscard_slab(s, page);\n\t\tstat(s, FREE_SLAB);\n\t}\n#endif\t/* CONFIG_SLUB_CPU_PARTIAL */\n}\n\n/*\n * Put a page that was just frozen (in __slab_free|get_partial_node) into a\n * partial page slot if available.\n *\n * If we did not find a slot then simply move all the partials to the\n * per node partial list.\n */\nstatic void put_cpu_partial(struct kmem_cache *s, struct page *page, int drain)\n{\n#ifdef CONFIG_SLUB_CPU_PARTIAL\n\tstruct page *oldpage;\n\tint pages;\n\tint pobjects;\n\n\tpreempt_disable();\n\tdo {\n\t\tpages = 0;\n\t\tpobjects = 0;\n\t\toldpage = this_cpu_read(s->cpu_slab->partial);\n\n\t\tif (oldpage) {\n\t\t\tpobjects = oldpage->pobjects;\n\t\t\tpages = oldpage->pages;\n\t\t\tif (drain && pobjects > slub_cpu_partial(s)) {\n\t\t\t\tunsigned long flags;\n\t\t\t\t/*\n\t\t\t\t * partial array is full. Move the existing\n\t\t\t\t * set to the per node partial list.\n\t\t\t\t */\n\t\t\t\tlocal_irq_save(flags);\n\t\t\t\tunfreeze_partials(s, this_cpu_ptr(s->cpu_slab));\n\t\t\t\tlocal_irq_restore(flags);\n\t\t\t\toldpage = NULL;\n\t\t\t\tpobjects = 0;\n\t\t\t\tpages = 0;\n\t\t\t\tstat(s, CPU_PARTIAL_DRAIN);\n\t\t\t}\n\t\t}\n\n\t\tpages++;\n\t\tpobjects += page->objects - page->inuse;\n\n\t\tpage->pages = pages;\n\t\tpage->pobjects = pobjects;\n\t\tpage->next = oldpage;\n\n\t} while (this_cpu_cmpxchg(s->cpu_slab->partial, oldpage, page)\n\t\t\t\t\t\t\t\t!= oldpage);\n\tif (unlikely(!slub_cpu_partial(s))) {\n\t\tunsigned long flags;\n\n\t\tlocal_irq_save(flags);\n\t\tunfreeze_partials(s, this_cpu_ptr(s->cpu_slab));\n\t\tlocal_irq_restore(flags);\n\t}\n\tpreempt_enable();\n#endif\t/* CONFIG_SLUB_CPU_PARTIAL */\n}\n\nstatic inline void flush_slab(struct kmem_cache *s, struct kmem_cache_cpu *c)\n{\n\tstat(s, CPUSLAB_FLUSH);\n\tdeactivate_slab(s, c->page, c->freelist, c);\n\n\tc->tid = next_tid(c->tid);\n}\n\n/*\n * Flush cpu slab.\n *\n * Called from IPI handler with interrupts disabled.\n */\nstatic inline void __flush_cpu_slab(struct kmem_cache *s, int cpu)\n{\n\tstruct kmem_cache_cpu *c = per_cpu_ptr(s->cpu_slab, cpu);\n\n\tif (c->page)\n\t\tflush_slab(s, c);\n\n\tunfreeze_partials(s, c);\n}\n\nstatic void flush_cpu_slab(void *d)\n{\n\tstruct kmem_cache *s = d;\n\n\t__flush_cpu_slab(s, smp_processor_id());\n}\n\nstatic bool has_cpu_slab(int cpu, void *info)\n{\n\tstruct kmem_cache *s = info;\n\tstruct kmem_cache_cpu *c = per_cpu_ptr(s->cpu_slab, cpu);\n\n\treturn c->page || slub_percpu_partial(c);\n}\n\nstatic void flush_all(struct kmem_cache *s)\n{\n\ton_each_cpu_cond(has_cpu_slab, flush_cpu_slab, s, 1);\n}\n\n/*\n * Use the cpu notifier to insure that the cpu slabs are flushed when\n * necessary.\n */\nstatic int slub_cpu_dead(unsigned int cpu)\n{\n\tstruct kmem_cache *s;\n\tunsigned long flags;\n\n\tmutex_lock(&slab_mutex);\n\tlist_for_each_entry(s, &slab_caches, list) {\n\t\tlocal_irq_save(flags);\n\t\t__flush_cpu_slab(s, cpu);\n\t\tlocal_irq_restore(flags);\n\t}\n\tmutex_unlock(&slab_mutex);\n\treturn 0;\n}\n\n/*\n * Check if the objects in a per cpu structure fit numa\n * locality expectations.\n */\nstatic inline int node_match(struct page *page, int node)\n{\n#ifdef CONFIG_NUMA\n\tif (node != NUMA_NO_NODE && page_to_nid(page) != node)\n\t\treturn 0;\n#endif\n\treturn 1;\n}\n\n#ifdef CONFIG_SLUB_DEBUG\nstatic int count_free(struct page *page)\n{\n\treturn page->objects - page->inuse;\n}\n\nstatic inline unsigned long node_nr_objs(struct kmem_cache_node *n)\n{\n\treturn atomic_long_read(&n->total_objects);\n}\n#endif /* CONFIG_SLUB_DEBUG */\n\n#if defined(CONFIG_SLUB_DEBUG) || defined(CONFIG_SYSFS)\nstatic unsigned long count_partial(struct kmem_cache_node *n,\n\t\t\t\t\tint (*get_count)(struct page *))\n{\n\tunsigned long flags;\n\tunsigned long x = 0;\n\tstruct page *page;\n\n\tspin_lock_irqsave(&n->list_lock, flags);\n\tlist_for_each_entry(page, &n->partial, slab_list)\n\t\tx += get_count(page);\n\tspin_unlock_irqrestore(&n->list_lock, flags);\n\treturn x;\n}\n#endif /* CONFIG_SLUB_DEBUG || CONFIG_SYSFS */\n\nstatic noinline void\nslab_out_of_memory(struct kmem_cache *s, gfp_t gfpflags, int nid)\n{\n#ifdef CONFIG_SLUB_DEBUG\n\tstatic DEFINE_RATELIMIT_STATE(slub_oom_rs, DEFAULT_RATELIMIT_INTERVAL,\n\t\t\t\t      DEFAULT_RATELIMIT_BURST);\n\tint node;\n\tstruct kmem_cache_node *n;\n\n\tif ((gfpflags & __GFP_NOWARN) || !__ratelimit(&slub_oom_rs))\n\t\treturn;\n\n\tpr_warn(\"SLUB: Unable to allocate memory on node %d, gfp=%#x(%pGg)\\n\",\n\t\tnid, gfpflags, &gfpflags);\n\tpr_warn(\"  cache: %s, object size: %u, buffer size: %u, default order: %u, min order: %u\\n\",\n\t\ts->name, s->object_size, s->size, oo_order(s->oo),\n\t\too_order(s->min));\n\n\tif (oo_order(s->min) > get_order(s->object_size))\n\t\tpr_warn(\"  %s debugging increased min order, use slub_debug=O to disable.\\n\",\n\t\t\ts->name);\n\n\tfor_each_kmem_cache_node(s, node, n) {\n\t\tunsigned long nr_slabs;\n\t\tunsigned long nr_objs;\n\t\tunsigned long nr_free;\n\n\t\tnr_free  = count_partial(n, count_free);\n\t\tnr_slabs = node_nr_slabs(n);\n\t\tnr_objs  = node_nr_objs(n);\n\n\t\tpr_warn(\"  node %d: slabs: %ld, objs: %ld, free: %ld\\n\",\n\t\t\tnode, nr_slabs, nr_objs, nr_free);\n\t}\n#endif\n}\n\nstatic inline void *new_slab_objects(struct kmem_cache *s, gfp_t flags,\n\t\t\tint node, struct kmem_cache_cpu **pc)\n{\n\tvoid *freelist;\n\tstruct kmem_cache_cpu *c = *pc;\n\tstruct page *page;\n\n\tWARN_ON_ONCE(s->ctor && (flags & __GFP_ZERO));\n\n\tfreelist = get_partial(s, flags, node, c);\n\n\tif (freelist)\n\t\treturn freelist;\n\n\tpage = new_slab(s, flags, node);\n\tif (page) {\n\t\tc = raw_cpu_ptr(s->cpu_slab);\n\t\tif (c->page)\n\t\t\tflush_slab(s, c);\n\n\t\t/*\n\t\t * No other reference to the page yet so we can\n\t\t * muck around with it freely without cmpxchg\n\t\t */\n\t\tfreelist = page->freelist;\n\t\tpage->freelist = NULL;\n\n\t\tstat(s, ALLOC_SLAB);\n\t\tc->page = page;\n\t\t*pc = c;\n\t}\n\n\treturn freelist;\n}\n\nstatic inline bool pfmemalloc_match(struct page *page, gfp_t gfpflags)\n{\n\tif (unlikely(PageSlabPfmemalloc(page)))\n\t\treturn gfp_pfmemalloc_allowed(gfpflags);\n\n\treturn true;\n}\n\n/*\n * Check the page->freelist of a page and either transfer the freelist to the\n * per cpu freelist or deactivate the page.\n *\n * The page is still frozen if the return value is not NULL.\n *\n * If this function returns NULL then the page has been unfrozen.\n *\n * This function must be called with interrupt disabled.\n */\nstatic inline void *get_freelist(struct kmem_cache *s, struct page *page)\n{\n\tstruct page new;\n\tunsigned long counters;\n\tvoid *freelist;\n\n\tdo {\n\t\tfreelist = page->freelist;\n\t\tcounters = page->counters;\n\n\t\tnew.counters = counters;\n\t\tVM_BUG_ON(!new.frozen);\n\n\t\tnew.inuse = page->objects;\n\t\tnew.frozen = freelist != NULL;\n\n\t} while (!__cmpxchg_double_slab(s, page,\n\t\tfreelist, counters,\n\t\tNULL, new.counters,\n\t\t\"get_freelist\"));\n\n\treturn freelist;\n}\n\n/*\n * Slow path. The lockless freelist is empty or we need to perform\n * debugging duties.\n *\n * Processing is still very fast if new objects have been freed to the\n * regular freelist. In that case we simply take over the regular freelist\n * as the lockless freelist and zap the regular freelist.\n *\n * If that is not working then we fall back to the partial lists. We take the\n * first element of the freelist as the object to allocate now and move the\n * rest of the freelist to the lockless freelist.\n *\n * And if we were unable to get a new slab from the partial slab lists then\n * we need to allocate a new slab. This is the slowest path since it involves\n * a call to the page allocator and the setup of a new slab.\n *\n * Version of __slab_alloc to use when we know that interrupts are\n * already disabled (which is the case for bulk allocation).\n */\nstatic void *___slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,\n\t\t\t  unsigned long addr, struct kmem_cache_cpu *c)\n{\n\tvoid *freelist;\n\tstruct page *page;\n\n\tstat(s, ALLOC_SLOWPATH);\n\n\tpage = c->page;\n\tif (!page) {\n\t\t/*\n\t\t * if the node is not online or has no normal memory, just\n\t\t * ignore the node constraint\n\t\t */\n\t\tif (unlikely(node != NUMA_NO_NODE &&\n\t\t\t     !node_isset(node, slab_nodes)))\n\t\t\tnode = NUMA_NO_NODE;\n\t\tgoto new_slab;\n\t}\nredo:\n\n\tif (unlikely(!node_match(page, node))) {\n\t\t/*\n\t\t * same as above but node_match() being false already\n\t\t * implies node != NUMA_NO_NODE\n\t\t */\n\t\tif (!node_isset(node, slab_nodes)) {\n\t\t\tnode = NUMA_NO_NODE;\n\t\t\tgoto redo;\n\t\t} else {\n\t\t\tstat(s, ALLOC_NODE_MISMATCH);\n\t\t\tdeactivate_slab(s, page, c->freelist, c);\n\t\t\tgoto new_slab;\n\t\t}\n\t}\n\n\t/*\n\t * By rights, we should be searching for a slab page that was\n\t * PFMEMALLOC but right now, we are losing the pfmemalloc\n\t * information when the page leaves the per-cpu allocator\n\t */\n\tif (unlikely(!pfmemalloc_match(page, gfpflags))) {\n\t\tdeactivate_slab(s, page, c->freelist, c);\n\t\tgoto new_slab;\n\t}\n\n\t/* must check again c->freelist in case of cpu migration or IRQ */\n\tfreelist = c->freelist;\n\tif (freelist)\n\t\tgoto load_freelist;\n\n\tfreelist = get_freelist(s, page);\n\n\tif (!freelist) {\n\t\tc->page = NULL;\n\t\tstat(s, DEACTIVATE_BYPASS);\n\t\tgoto new_slab;\n\t}\n\n\tstat(s, ALLOC_REFILL);\n\nload_freelist:\n\t/*\n\t * freelist is pointing to the list of objects to be used.\n\t * page is pointing to the page from which the objects are obtained.\n\t * That page must be frozen for per cpu allocations to work.\n\t */\n\tVM_BUG_ON(!c->page->frozen);\n\tc->freelist = get_freepointer(s, freelist);\n\tc->tid = next_tid(c->tid);\n\treturn freelist;\n\nnew_slab:\n\n\tif (slub_percpu_partial(c)) {\n\t\tpage = c->page = slub_percpu_partial(c);\n\t\tslub_set_percpu_partial(c, page);\n\t\tstat(s, CPU_PARTIAL_ALLOC);\n\t\tgoto redo;\n\t}\n\n\tfreelist = new_slab_objects(s, gfpflags, node, &c);\n\n\tif (unlikely(!freelist)) {\n\t\tslab_out_of_memory(s, gfpflags, node);\n\t\treturn NULL;\n\t}\n\n\tpage = c->page;\n\tif (likely(!kmem_cache_debug(s) && pfmemalloc_match(page, gfpflags)))\n\t\tgoto load_freelist;\n\n\t/* Only entered in the debug case */\n\tif (kmem_cache_debug(s) &&\n\t\t\t!alloc_debug_processing(s, page, freelist, addr))\n\t\tgoto new_slab;\t/* Slab failed checks. Next slab needed */\n\n\tdeactivate_slab(s, page, get_freepointer(s, freelist), c);\n\treturn freelist;\n}\n\n/*\n * Another one that disabled interrupt and compensates for possible\n * cpu changes by refetching the per cpu area pointer.\n */\nstatic void *__slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,\n\t\t\t  unsigned long addr, struct kmem_cache_cpu *c)\n{\n\tvoid *p;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n#ifdef CONFIG_PREEMPTION\n\t/*\n\t * We may have been preempted and rescheduled on a different\n\t * cpu before disabling interrupts. Need to reload cpu area\n\t * pointer.\n\t */\n\tc = this_cpu_ptr(s->cpu_slab);\n#endif\n\n\tp = ___slab_alloc(s, gfpflags, node, addr, c);\n\tlocal_irq_restore(flags);\n\treturn p;\n}\n\n/*\n * If the object has been wiped upon free, make sure it's fully initialized by\n * zeroing out freelist pointer.\n */\nstatic __always_inline void maybe_wipe_obj_freeptr(struct kmem_cache *s,\n\t\t\t\t\t\t   void *obj)\n{\n\tif (unlikely(slab_want_init_on_free(s)) && obj)\n\t\tmemset((void *)((char *)kasan_reset_tag(obj) + s->offset),\n\t\t\t0, sizeof(void *));\n}\n\n/*\n * Inlined fastpath so that allocation functions (kmalloc, kmem_cache_alloc)\n * have the fastpath folded into their functions. So no function call\n * overhead for requests that can be satisfied on the fastpath.\n *\n * The fastpath works by first checking if the lockless freelist can be used.\n * If not then __slab_alloc is called for slow processing.\n *\n * Otherwise we can simply pick the next object from the lockless free list.\n */\nstatic __always_inline void *slab_alloc_node(struct kmem_cache *s,\n\t\tgfp_t gfpflags, int node, unsigned long addr, size_t orig_size)\n{\n\tvoid *object;\n\tstruct kmem_cache_cpu *c;\n\tstruct page *page;\n\tunsigned long tid;\n\tstruct obj_cgroup *objcg = NULL;\n\tbool init = false;\n\n\ts = slab_pre_alloc_hook(s, &objcg, 1, gfpflags);\n\tif (!s)\n\t\treturn NULL;\n\n\tobject = kfence_alloc(s, orig_size, gfpflags);\n\tif (unlikely(object))\n\t\tgoto out;\n\nredo:\n\t/*\n\t * Must read kmem_cache cpu data via this cpu ptr. Preemption is\n\t * enabled. We may switch back and forth between cpus while\n\t * reading from one cpu area. That does not matter as long\n\t * as we end up on the original cpu again when doing the cmpxchg.\n\t *\n\t * We should guarantee that tid and kmem_cache are retrieved on\n\t * the same cpu. It could be different if CONFIG_PREEMPTION so we need\n\t * to check if it is matched or not.\n\t */\n\tdo {\n\t\ttid = this_cpu_read(s->cpu_slab->tid);\n\t\tc = raw_cpu_ptr(s->cpu_slab);\n\t} while (IS_ENABLED(CONFIG_PREEMPTION) &&\n\t\t unlikely(tid != READ_ONCE(c->tid)));\n\n\t/*\n\t * Irqless object alloc/free algorithm used here depends on sequence\n\t * of fetching cpu_slab's data. tid should be fetched before anything\n\t * on c to guarantee that object and page associated with previous tid\n\t * won't be used with current tid. If we fetch tid first, object and\n\t * page could be one associated with next tid and our alloc/free\n\t * request will be failed. In this case, we will retry. So, no problem.\n\t */\n\tbarrier();\n\n\t/*\n\t * The transaction ids are globally unique per cpu and per operation on\n\t * a per cpu queue. Thus they can be guarantee that the cmpxchg_double\n\t * occurs on the right processor and that there was no operation on the\n\t * linked list in between.\n\t */\n\n\tobject = c->freelist;\n\tpage = c->page;\n\tif (unlikely(!object || !page || !node_match(page, node))) {\n\t\tobject = __slab_alloc(s, gfpflags, node, addr, c);\n\t} else {\n\t\tvoid *next_object = get_freepointer_safe(s, object);\n\n\t\t/*\n\t\t * The cmpxchg will only match if there was no additional\n\t\t * operation and if we are on the right processor.\n\t\t *\n\t\t * The cmpxchg does the following atomically (without lock\n\t\t * semantics!)\n\t\t * 1. Relocate first pointer to the current per cpu area.\n\t\t * 2. Verify that tid and freelist have not been changed\n\t\t * 3. If they were not changed replace tid and freelist\n\t\t *\n\t\t * Since this is without lock semantics the protection is only\n\t\t * against code executing on this cpu *not* from access by\n\t\t * other cpus.\n\t\t */\n\t\tif (unlikely(!this_cpu_cmpxchg_double(\n\t\t\t\ts->cpu_slab->freelist, s->cpu_slab->tid,\n\t\t\t\tobject, tid,\n\t\t\t\tnext_object, next_tid(tid)))) {\n\n\t\t\tnote_cmpxchg_failure(\"slab_alloc\", s, tid);\n\t\t\tgoto redo;\n\t\t}\n\t\tprefetch_freepointer(s, next_object);\n\t\tstat(s, ALLOC_FASTPATH);\n\t}\n\n\tmaybe_wipe_obj_freeptr(s, object);\n\tinit = slab_want_init_on_alloc(gfpflags, s);\n\nout:\n\tslab_post_alloc_hook(s, objcg, gfpflags, 1, &object, init);\n\n\treturn object;\n}\n\nstatic __always_inline void *slab_alloc(struct kmem_cache *s,\n\t\tgfp_t gfpflags, unsigned long addr, size_t orig_size)\n{\n\treturn slab_alloc_node(s, gfpflags, NUMA_NO_NODE, addr, orig_size);\n}\n\nvoid *kmem_cache_alloc(struct kmem_cache *s, gfp_t gfpflags)\n{\n\tvoid *ret = slab_alloc(s, gfpflags, _RET_IP_, s->object_size);\n\n\ttrace_kmem_cache_alloc(_RET_IP_, ret, s->object_size,\n\t\t\t\ts->size, gfpflags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(kmem_cache_alloc);\n\n#ifdef CONFIG_TRACING\nvoid *kmem_cache_alloc_trace(struct kmem_cache *s, gfp_t gfpflags, size_t size)\n{\n\tvoid *ret = slab_alloc(s, gfpflags, _RET_IP_, size);\n\ttrace_kmalloc(_RET_IP_, ret, size, s->size, gfpflags);\n\tret = kasan_kmalloc(s, ret, size, gfpflags);\n\treturn ret;\n}\nEXPORT_SYMBOL(kmem_cache_alloc_trace);\n#endif\n\n#ifdef CONFIG_NUMA\nvoid *kmem_cache_alloc_node(struct kmem_cache *s, gfp_t gfpflags, int node)\n{\n\tvoid *ret = slab_alloc_node(s, gfpflags, node, _RET_IP_, s->object_size);\n\n\ttrace_kmem_cache_alloc_node(_RET_IP_, ret,\n\t\t\t\t    s->object_size, s->size, gfpflags, node);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(kmem_cache_alloc_node);\n\n#ifdef CONFIG_TRACING\nvoid *kmem_cache_alloc_node_trace(struct kmem_cache *s,\n\t\t\t\t    gfp_t gfpflags,\n\t\t\t\t    int node, size_t size)\n{\n\tvoid *ret = slab_alloc_node(s, gfpflags, node, _RET_IP_, size);\n\n\ttrace_kmalloc_node(_RET_IP_, ret,\n\t\t\t   size, s->size, gfpflags, node);\n\n\tret = kasan_kmalloc(s, ret, size, gfpflags);\n\treturn ret;\n}\nEXPORT_SYMBOL(kmem_cache_alloc_node_trace);\n#endif\n#endif\t/* CONFIG_NUMA */\n\n/*\n * Slow path handling. This may still be called frequently since objects\n * have a longer lifetime than the cpu slabs in most processing loads.\n *\n * So we still attempt to reduce cache line usage. Just take the slab\n * lock and free the item. If there is no additional partial page\n * handling required then we can return immediately.\n */\nstatic void __slab_free(struct kmem_cache *s, struct page *page,\n\t\t\tvoid *head, void *tail, int cnt,\n\t\t\tunsigned long addr)\n\n{\n\tvoid *prior;\n\tint was_frozen;\n\tstruct page new;\n\tunsigned long counters;\n\tstruct kmem_cache_node *n = NULL;\n\tunsigned long flags;\n\n\tstat(s, FREE_SLOWPATH);\n\n\tif (kfence_free(head))\n\t\treturn;\n\n\tif (kmem_cache_debug(s) &&\n\t    !free_debug_processing(s, page, head, tail, cnt, addr))\n\t\treturn;\n\n\tdo {\n\t\tif (unlikely(n)) {\n\t\t\tspin_unlock_irqrestore(&n->list_lock, flags);\n\t\t\tn = NULL;\n\t\t}\n\t\tprior = page->freelist;\n\t\tcounters = page->counters;\n\t\tset_freepointer(s, tail, prior);\n\t\tnew.counters = counters;\n\t\twas_frozen = new.frozen;\n\t\tnew.inuse -= cnt;\n\t\tif ((!new.inuse || !prior) && !was_frozen) {\n\n\t\t\tif (kmem_cache_has_cpu_partial(s) && !prior) {\n\n\t\t\t\t/*\n\t\t\t\t * Slab was on no list before and will be\n\t\t\t\t * partially empty\n\t\t\t\t * We can defer the list move and instead\n\t\t\t\t * freeze it.\n\t\t\t\t */\n\t\t\t\tnew.frozen = 1;\n\n\t\t\t} else { /* Needs to be taken off a list */\n\n\t\t\t\tn = get_node(s, page_to_nid(page));\n\t\t\t\t/*\n\t\t\t\t * Speculatively acquire the list_lock.\n\t\t\t\t * If the cmpxchg does not succeed then we may\n\t\t\t\t * drop the list_lock without any processing.\n\t\t\t\t *\n\t\t\t\t * Otherwise the list_lock will synchronize with\n\t\t\t\t * other processors updating the list of slabs.\n\t\t\t\t */\n\t\t\t\tspin_lock_irqsave(&n->list_lock, flags);\n\n\t\t\t}\n\t\t}\n\n\t} while (!cmpxchg_double_slab(s, page,\n\t\tprior, counters,\n\t\thead, new.counters,\n\t\t\"__slab_free\"));\n\n\tif (likely(!n)) {\n\n\t\tif (likely(was_frozen)) {\n\t\t\t/*\n\t\t\t * The list lock was not taken therefore no list\n\t\t\t * activity can be necessary.\n\t\t\t */\n\t\t\tstat(s, FREE_FROZEN);\n\t\t} else if (new.frozen) {\n\t\t\t/*\n\t\t\t * If we just froze the page then put it onto the\n\t\t\t * per cpu partial list.\n\t\t\t */\n\t\t\tput_cpu_partial(s, page, 1);\n\t\t\tstat(s, CPU_PARTIAL_FREE);\n\t\t}\n\n\t\treturn;\n\t}\n\n\tif (unlikely(!new.inuse && n->nr_partial >= s->min_partial))\n\t\tgoto slab_empty;\n\n\t/*\n\t * Objects left in the slab. If it was not on the partial list before\n\t * then add it.\n\t */\n\tif (!kmem_cache_has_cpu_partial(s) && unlikely(!prior)) {\n\t\tremove_full(s, n, page);\n\t\tadd_partial(n, page, DEACTIVATE_TO_TAIL);\n\t\tstat(s, FREE_ADD_PARTIAL);\n\t}\n\tspin_unlock_irqrestore(&n->list_lock, flags);\n\treturn;\n\nslab_empty:\n\tif (prior) {\n\t\t/*\n\t\t * Slab on the partial list.\n\t\t */\n\t\tremove_partial(n, page);\n\t\tstat(s, FREE_REMOVE_PARTIAL);\n\t} else {\n\t\t/* Slab must be on the full list */\n\t\tremove_full(s, n, page);\n\t}\n\n\tspin_unlock_irqrestore(&n->list_lock, flags);\n\tstat(s, FREE_SLAB);\n\tdiscard_slab(s, page);\n}\n\n/*\n * Fastpath with forced inlining to produce a kfree and kmem_cache_free that\n * can perform fastpath freeing without additional function calls.\n *\n * The fastpath is only possible if we are freeing to the current cpu slab\n * of this processor. This typically the case if we have just allocated\n * the item before.\n *\n * If fastpath is not possible then fall back to __slab_free where we deal\n * with all sorts of special processing.\n *\n * Bulk free of a freelist with several objects (all pointing to the\n * same page) possible by specifying head and tail ptr, plus objects\n * count (cnt). Bulk free indicated by tail pointer being set.\n */\nstatic __always_inline void do_slab_free(struct kmem_cache *s,\n\t\t\t\tstruct page *page, void *head, void *tail,\n\t\t\t\tint cnt, unsigned long addr)\n{\n\tvoid *tail_obj = tail ? : head;\n\tstruct kmem_cache_cpu *c;\n\tunsigned long tid;\n\n\tmemcg_slab_free_hook(s, &head, 1);\nredo:\n\t/*\n\t * Determine the currently cpus per cpu slab.\n\t * The cpu may change afterward. However that does not matter since\n\t * data is retrieved via this pointer. If we are on the same cpu\n\t * during the cmpxchg then the free will succeed.\n\t */\n\tdo {\n\t\ttid = this_cpu_read(s->cpu_slab->tid);\n\t\tc = raw_cpu_ptr(s->cpu_slab);\n\t} while (IS_ENABLED(CONFIG_PREEMPTION) &&\n\t\t unlikely(tid != READ_ONCE(c->tid)));\n\n\t/* Same with comment on barrier() in slab_alloc_node() */\n\tbarrier();\n\n\tif (likely(page == c->page)) {\n\t\tvoid **freelist = READ_ONCE(c->freelist);\n\n\t\tset_freepointer(s, tail_obj, freelist);\n\n\t\tif (unlikely(!this_cpu_cmpxchg_double(\n\t\t\t\ts->cpu_slab->freelist, s->cpu_slab->tid,\n\t\t\t\tfreelist, tid,\n\t\t\t\thead, next_tid(tid)))) {\n\n\t\t\tnote_cmpxchg_failure(\"slab_free\", s, tid);\n\t\t\tgoto redo;\n\t\t}\n\t\tstat(s, FREE_FASTPATH);\n\t} else\n\t\t__slab_free(s, page, head, tail_obj, cnt, addr);\n\n}\n\nstatic __always_inline void slab_free(struct kmem_cache *s, struct page *page,\n\t\t\t\t      void *head, void *tail, int cnt,\n\t\t\t\t      unsigned long addr)\n{\n\t/*\n\t * With KASAN enabled slab_free_freelist_hook modifies the freelist\n\t * to remove objects, whose reuse must be delayed.\n\t */\n\tif (slab_free_freelist_hook(s, &head, &tail))\n\t\tdo_slab_free(s, page, head, tail, cnt, addr);\n}\n\n#ifdef CONFIG_KASAN_GENERIC\nvoid ___cache_free(struct kmem_cache *cache, void *x, unsigned long addr)\n{\n\tdo_slab_free(cache, virt_to_head_page(x), x, NULL, 1, addr);\n}\n#endif\n\nvoid kmem_cache_free(struct kmem_cache *s, void *x)\n{\n\ts = cache_from_obj(s, x);\n\tif (!s)\n\t\treturn;\n\tslab_free(s, virt_to_head_page(x), x, NULL, 1, _RET_IP_);\n\ttrace_kmem_cache_free(_RET_IP_, x, s->name);\n}\nEXPORT_SYMBOL(kmem_cache_free);\n\nstruct detached_freelist {\n\tstruct page *page;\n\tvoid *tail;\n\tvoid *freelist;\n\tint cnt;\n\tstruct kmem_cache *s;\n};\n\n/*\n * This function progressively scans the array with free objects (with\n * a limited look ahead) and extract objects belonging to the same\n * page.  It builds a detached freelist directly within the given\n * page/objects.  This can happen without any need for\n * synchronization, because the objects are owned by running process.\n * The freelist is build up as a single linked list in the objects.\n * The idea is, that this detached freelist can then be bulk\n * transferred to the real freelist(s), but only requiring a single\n * synchronization primitive.  Look ahead in the array is limited due\n * to performance reasons.\n */\nstatic inline\nint build_detached_freelist(struct kmem_cache *s, size_t size,\n\t\t\t    void **p, struct detached_freelist *df)\n{\n\tsize_t first_skipped_index = 0;\n\tint lookahead = 3;\n\tvoid *object;\n\tstruct page *page;\n\n\t/* Always re-init detached_freelist */\n\tdf->page = NULL;\n\n\tdo {\n\t\tobject = p[--size];\n\t\t/* Do we need !ZERO_OR_NULL_PTR(object) here? (for kfree) */\n\t} while (!object && size);\n\n\tif (!object)\n\t\treturn 0;\n\n\tpage = virt_to_head_page(object);\n\tif (!s) {\n\t\t/* Handle kalloc'ed objects */\n\t\tif (unlikely(!PageSlab(page))) {\n\t\t\tBUG_ON(!PageCompound(page));\n\t\t\tkfree_hook(object);\n\t\t\t__free_pages(page, compound_order(page));\n\t\t\tp[size] = NULL; /* mark object processed */\n\t\t\treturn size;\n\t\t}\n\t\t/* Derive kmem_cache from object */\n\t\tdf->s = page->slab_cache;\n\t} else {\n\t\tdf->s = cache_from_obj(s, object); /* Support for memcg */\n\t}\n\n\tif (is_kfence_address(object)) {\n\t\tslab_free_hook(df->s, object, false);\n\t\t__kfence_free(object);\n\t\tp[size] = NULL; /* mark object processed */\n\t\treturn size;\n\t}\n\n\t/* Start new detached freelist */\n\tdf->page = page;\n\tset_freepointer(df->s, object, NULL);\n\tdf->tail = object;\n\tdf->freelist = object;\n\tp[size] = NULL; /* mark object processed */\n\tdf->cnt = 1;\n\n\twhile (size) {\n\t\tobject = p[--size];\n\t\tif (!object)\n\t\t\tcontinue; /* Skip processed objects */\n\n\t\t/* df->page is always set at this point */\n\t\tif (df->page == virt_to_head_page(object)) {\n\t\t\t/* Opportunity build freelist */\n\t\t\tset_freepointer(df->s, object, df->freelist);\n\t\t\tdf->freelist = object;\n\t\t\tdf->cnt++;\n\t\t\tp[size] = NULL; /* mark object processed */\n\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Limit look ahead search */\n\t\tif (!--lookahead)\n\t\t\tbreak;\n\n\t\tif (!first_skipped_index)\n\t\t\tfirst_skipped_index = size + 1;\n\t}\n\n\treturn first_skipped_index;\n}\n\n/* Note that interrupts must be enabled when calling this function. */\nvoid kmem_cache_free_bulk(struct kmem_cache *s, size_t size, void **p)\n{\n\tif (WARN_ON(!size))\n\t\treturn;\n\n\tmemcg_slab_free_hook(s, p, size);\n\tdo {\n\t\tstruct detached_freelist df;\n\n\t\tsize = build_detached_freelist(s, size, p, &df);\n\t\tif (!df.page)\n\t\t\tcontinue;\n\n\t\tslab_free(df.s, df.page, df.freelist, df.tail, df.cnt, _RET_IP_);\n\t} while (likely(size));\n}\nEXPORT_SYMBOL(kmem_cache_free_bulk);\n\n/* Note that interrupts must be enabled when calling this function. */\nint kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,\n\t\t\t  void **p)\n{\n\tstruct kmem_cache_cpu *c;\n\tint i;\n\tstruct obj_cgroup *objcg = NULL;\n\n\t/* memcg and kmem_cache debug support */\n\ts = slab_pre_alloc_hook(s, &objcg, size, flags);\n\tif (unlikely(!s))\n\t\treturn false;\n\t/*\n\t * Drain objects in the per cpu slab, while disabling local\n\t * IRQs, which protects against PREEMPT and interrupts\n\t * handlers invoking normal fastpath.\n\t */\n\tlocal_irq_disable();\n\tc = this_cpu_ptr(s->cpu_slab);\n\n\tfor (i = 0; i < size; i++) {\n\t\tvoid *object = kfence_alloc(s, s->object_size, flags);\n\n\t\tif (unlikely(object)) {\n\t\t\tp[i] = object;\n\t\t\tcontinue;\n\t\t}\n\n\t\tobject = c->freelist;\n\t\tif (unlikely(!object)) {\n\t\t\t/*\n\t\t\t * We may have removed an object from c->freelist using\n\t\t\t * the fastpath in the previous iteration; in that case,\n\t\t\t * c->tid has not been bumped yet.\n\t\t\t * Since ___slab_alloc() may reenable interrupts while\n\t\t\t * allocating memory, we should bump c->tid now.\n\t\t\t */\n\t\t\tc->tid = next_tid(c->tid);\n\n\t\t\t/*\n\t\t\t * Invoking slow path likely have side-effect\n\t\t\t * of re-populating per CPU c->freelist\n\t\t\t */\n\t\t\tp[i] = ___slab_alloc(s, flags, NUMA_NO_NODE,\n\t\t\t\t\t    _RET_IP_, c);\n\t\t\tif (unlikely(!p[i]))\n\t\t\t\tgoto error;\n\n\t\t\tc = this_cpu_ptr(s->cpu_slab);\n\t\t\tmaybe_wipe_obj_freeptr(s, p[i]);\n\n\t\t\tcontinue; /* goto for-loop */\n\t\t}\n\t\tc->freelist = get_freepointer(s, object);\n\t\tp[i] = object;\n\t\tmaybe_wipe_obj_freeptr(s, p[i]);\n\t}\n\tc->tid = next_tid(c->tid);\n\tlocal_irq_enable();\n\n\t/*\n\t * memcg and kmem_cache debug support and memory initialization.\n\t * Done outside of the IRQ disabled fastpath loop.\n\t */\n\tslab_post_alloc_hook(s, objcg, flags, size, p,\n\t\t\t\tslab_want_init_on_alloc(flags, s));\n\treturn i;\nerror:\n\tlocal_irq_enable();\n\tslab_post_alloc_hook(s, objcg, flags, i, p, false);\n\t__kmem_cache_free_bulk(s, i, p);\n\treturn 0;\n}\nEXPORT_SYMBOL(kmem_cache_alloc_bulk);\n\n\n/*\n * Object placement in a slab is made very easy because we always start at\n * offset 0. If we tune the size of the object to the alignment then we can\n * get the required alignment by putting one properly sized object after\n * another.\n *\n * Notice that the allocation order determines the sizes of the per cpu\n * caches. Each processor has always one slab available for allocations.\n * Increasing the allocation order reduces the number of times that slabs\n * must be moved on and off the partial lists and is therefore a factor in\n * locking overhead.\n */\n\n/*\n * Minimum / Maximum order of slab pages. This influences locking overhead\n * and slab fragmentation. A higher order reduces the number of partial slabs\n * and increases the number of allocations possible without having to\n * take the list_lock.\n */\nstatic unsigned int slub_min_order;\nstatic unsigned int slub_max_order = PAGE_ALLOC_COSTLY_ORDER;\nstatic unsigned int slub_min_objects;\n\n/*\n * Calculate the order of allocation given an slab object size.\n *\n * The order of allocation has significant impact on performance and other\n * system components. Generally order 0 allocations should be preferred since\n * order 0 does not cause fragmentation in the page allocator. Larger objects\n * be problematic to put into order 0 slabs because there may be too much\n * unused space left. We go to a higher order if more than 1/16th of the slab\n * would be wasted.\n *\n * In order to reach satisfactory performance we must ensure that a minimum\n * number of objects is in one slab. Otherwise we may generate too much\n * activity on the partial lists which requires taking the list_lock. This is\n * less a concern for large slabs though which are rarely used.\n *\n * slub_max_order specifies the order where we begin to stop considering the\n * number of objects in a slab as critical. If we reach slub_max_order then\n * we try to keep the page order as low as possible. So we accept more waste\n * of space in favor of a small page order.\n *\n * Higher order allocations also allow the placement of more objects in a\n * slab and thereby reduce object handling overhead. If the user has\n * requested a higher minimum order then we start with that one instead of\n * the smallest order which will fit the object.\n */\nstatic inline unsigned int slab_order(unsigned int size,\n\t\tunsigned int min_objects, unsigned int max_order,\n\t\tunsigned int fract_leftover)\n{\n\tunsigned int min_order = slub_min_order;\n\tunsigned int order;\n\n\tif (order_objects(min_order, size) > MAX_OBJS_PER_PAGE)\n\t\treturn get_order(size * MAX_OBJS_PER_PAGE) - 1;\n\n\tfor (order = max(min_order, (unsigned int)get_order(min_objects * size));\n\t\t\torder <= max_order; order++) {\n\n\t\tunsigned int slab_size = (unsigned int)PAGE_SIZE << order;\n\t\tunsigned int rem;\n\n\t\trem = slab_size % size;\n\n\t\tif (rem <= slab_size / fract_leftover)\n\t\t\tbreak;\n\t}\n\n\treturn order;\n}\n\nstatic inline int calculate_order(unsigned int size)\n{\n\tunsigned int order;\n\tunsigned int min_objects;\n\tunsigned int max_objects;\n\tunsigned int nr_cpus;\n\n\t/*\n\t * Attempt to find best configuration for a slab. This\n\t * works by first attempting to generate a layout with\n\t * the best configuration and backing off gradually.\n\t *\n\t * First we increase the acceptable waste in a slab. Then\n\t * we reduce the minimum objects required in a slab.\n\t */\n\tmin_objects = slub_min_objects;\n\tif (!min_objects) {\n\t\t/*\n\t\t * Some architectures will only update present cpus when\n\t\t * onlining them, so don't trust the number if it's just 1. But\n\t\t * we also don't want to use nr_cpu_ids always, as on some other\n\t\t * architectures, there can be many possible cpus, but never\n\t\t * onlined. Here we compromise between trying to avoid too high\n\t\t * order on systems that appear larger than they are, and too\n\t\t * low order on systems that appear smaller than they are.\n\t\t */\n\t\tnr_cpus = num_present_cpus();\n\t\tif (nr_cpus <= 1)\n\t\t\tnr_cpus = nr_cpu_ids;\n\t\tmin_objects = 4 * (fls(nr_cpus) + 1);\n\t}\n\tmax_objects = order_objects(slub_max_order, size);\n\tmin_objects = min(min_objects, max_objects);\n\n\twhile (min_objects > 1) {\n\t\tunsigned int fraction;\n\n\t\tfraction = 16;\n\t\twhile (fraction >= 4) {\n\t\t\torder = slab_order(size, min_objects,\n\t\t\t\t\tslub_max_order, fraction);\n\t\t\tif (order <= slub_max_order)\n\t\t\t\treturn order;\n\t\t\tfraction /= 2;\n\t\t}\n\t\tmin_objects--;\n\t}\n\n\t/*\n\t * We were unable to place multiple objects in a slab. Now\n\t * lets see if we can place a single object there.\n\t */\n\torder = slab_order(size, 1, slub_max_order, 1);\n\tif (order <= slub_max_order)\n\t\treturn order;\n\n\t/*\n\t * Doh this slab cannot be placed using slub_max_order.\n\t */\n\torder = slab_order(size, 1, MAX_ORDER, 1);\n\tif (order < MAX_ORDER)\n\t\treturn order;\n\treturn -ENOSYS;\n}\n\nstatic void\ninit_kmem_cache_node(struct kmem_cache_node *n)\n{\n\tn->nr_partial = 0;\n\tspin_lock_init(&n->list_lock);\n\tINIT_LIST_HEAD(&n->partial);\n#ifdef CONFIG_SLUB_DEBUG\n\tatomic_long_set(&n->nr_slabs, 0);\n\tatomic_long_set(&n->total_objects, 0);\n\tINIT_LIST_HEAD(&n->full);\n#endif\n}\n\nstatic inline int alloc_kmem_cache_cpus(struct kmem_cache *s)\n{\n\tBUILD_BUG_ON(PERCPU_DYNAMIC_EARLY_SIZE <\n\t\t\tKMALLOC_SHIFT_HIGH * sizeof(struct kmem_cache_cpu));\n\n\t/*\n\t * Must align to double word boundary for the double cmpxchg\n\t * instructions to work; see __pcpu_double_call_return_bool().\n\t */\n\ts->cpu_slab = __alloc_percpu(sizeof(struct kmem_cache_cpu),\n\t\t\t\t     2 * sizeof(void *));\n\n\tif (!s->cpu_slab)\n\t\treturn 0;\n\n\tinit_kmem_cache_cpus(s);\n\n\treturn 1;\n}\n\nstatic struct kmem_cache *kmem_cache_node;\n\n/*\n * No kmalloc_node yet so do it by hand. We know that this is the first\n * slab on the node for this slabcache. There are no concurrent accesses\n * possible.\n *\n * Note that this function only works on the kmem_cache_node\n * when allocating for the kmem_cache_node. This is used for bootstrapping\n * memory on a fresh node that has no slab structures yet.\n */\nstatic void early_kmem_cache_node_alloc(int node)\n{\n\tstruct page *page;\n\tstruct kmem_cache_node *n;\n\n\tBUG_ON(kmem_cache_node->size < sizeof(struct kmem_cache_node));\n\n\tpage = new_slab(kmem_cache_node, GFP_NOWAIT, node);\n\n\tBUG_ON(!page);\n\tif (page_to_nid(page) != node) {\n\t\tpr_err(\"SLUB: Unable to allocate memory from node %d\\n\", node);\n\t\tpr_err(\"SLUB: Allocating a useless per node structure in order to be able to continue\\n\");\n\t}\n\n\tn = page->freelist;\n\tBUG_ON(!n);\n#ifdef CONFIG_SLUB_DEBUG\n\tinit_object(kmem_cache_node, n, SLUB_RED_ACTIVE);\n\tinit_tracking(kmem_cache_node, n);\n#endif\n\tn = kasan_slab_alloc(kmem_cache_node, n, GFP_KERNEL, false);\n\tpage->freelist = get_freepointer(kmem_cache_node, n);\n\tpage->inuse = 1;\n\tpage->frozen = 0;\n\tkmem_cache_node->node[node] = n;\n\tinit_kmem_cache_node(n);\n\tinc_slabs_node(kmem_cache_node, node, page->objects);\n\n\t/*\n\t * No locks need to be taken here as it has just been\n\t * initialized and there is no concurrent access.\n\t */\n\t__add_partial(n, page, DEACTIVATE_TO_HEAD);\n}\n\nstatic void free_kmem_cache_nodes(struct kmem_cache *s)\n{\n\tint node;\n\tstruct kmem_cache_node *n;\n\n\tfor_each_kmem_cache_node(s, node, n) {\n\t\ts->node[node] = NULL;\n\t\tkmem_cache_free(kmem_cache_node, n);\n\t}\n}\n\nvoid __kmem_cache_release(struct kmem_cache *s)\n{\n\tcache_random_seq_destroy(s);\n\tfree_percpu(s->cpu_slab);\n\tfree_kmem_cache_nodes(s);\n}\n\nstatic int init_kmem_cache_nodes(struct kmem_cache *s)\n{\n\tint node;\n\n\tfor_each_node_mask(node, slab_nodes) {\n\t\tstruct kmem_cache_node *n;\n\n\t\tif (slab_state == DOWN) {\n\t\t\tearly_kmem_cache_node_alloc(node);\n\t\t\tcontinue;\n\t\t}\n\t\tn = kmem_cache_alloc_node(kmem_cache_node,\n\t\t\t\t\t\tGFP_KERNEL, node);\n\n\t\tif (!n) {\n\t\t\tfree_kmem_cache_nodes(s);\n\t\t\treturn 0;\n\t\t}\n\n\t\tinit_kmem_cache_node(n);\n\t\ts->node[node] = n;\n\t}\n\treturn 1;\n}\n\nstatic void set_min_partial(struct kmem_cache *s, unsigned long min)\n{\n\tif (min < MIN_PARTIAL)\n\t\tmin = MIN_PARTIAL;\n\telse if (min > MAX_PARTIAL)\n\t\tmin = MAX_PARTIAL;\n\ts->min_partial = min;\n}\n\nstatic void set_cpu_partial(struct kmem_cache *s)\n{\n#ifdef CONFIG_SLUB_CPU_PARTIAL\n\t/*\n\t * cpu_partial determined the maximum number of objects kept in the\n\t * per cpu partial lists of a processor.\n\t *\n\t * Per cpu partial lists mainly contain slabs that just have one\n\t * object freed. If they are used for allocation then they can be\n\t * filled up again with minimal effort. The slab will never hit the\n\t * per node partial lists and therefore no locking will be required.\n\t *\n\t * This setting also determines\n\t *\n\t * A) The number of objects from per cpu partial slabs dumped to the\n\t *    per node list when we reach the limit.\n\t * B) The number of objects in cpu partial slabs to extract from the\n\t *    per node list when we run out of per cpu objects. We only fetch\n\t *    50% to keep some capacity around for frees.\n\t */\n\tif (!kmem_cache_has_cpu_partial(s))\n\t\tslub_set_cpu_partial(s, 0);\n\telse if (s->size >= PAGE_SIZE)\n\t\tslub_set_cpu_partial(s, 2);\n\telse if (s->size >= 1024)\n\t\tslub_set_cpu_partial(s, 6);\n\telse if (s->size >= 256)\n\t\tslub_set_cpu_partial(s, 13);\n\telse\n\t\tslub_set_cpu_partial(s, 30);\n#endif\n}\n\n/*\n * calculate_sizes() determines the order and the distribution of data within\n * a slab object.\n */\nstatic int calculate_sizes(struct kmem_cache *s, int forced_order)\n{\n\tslab_flags_t flags = s->flags;\n\tunsigned int size = s->object_size;\n\tunsigned int freepointer_area;\n\tunsigned int order;\n\n\t/*\n\t * Round up object size to the next word boundary. We can only\n\t * place the free pointer at word boundaries and this determines\n\t * the possible location of the free pointer.\n\t */\n\tsize = ALIGN(size, sizeof(void *));\n\t/*\n\t * This is the area of the object where a freepointer can be\n\t * safely written. If redzoning adds more to the inuse size, we\n\t * can't use that portion for writing the freepointer, so\n\t * s->offset must be limited within this for the general case.\n\t */\n\tfreepointer_area = size;\n\n#ifdef CONFIG_SLUB_DEBUG\n\t/*\n\t * Determine if we can poison the object itself. If the user of\n\t * the slab may touch the object after free or before allocation\n\t * then we should never poison the object itself.\n\t */\n\tif ((flags & SLAB_POISON) && !(flags & SLAB_TYPESAFE_BY_RCU) &&\n\t\t\t!s->ctor)\n\t\ts->flags |= __OBJECT_POISON;\n\telse\n\t\ts->flags &= ~__OBJECT_POISON;\n\n\n\t/*\n\t * If we are Redzoning then check if there is some space between the\n\t * end of the object and the free pointer. If not then add an\n\t * additional word to have some bytes to store Redzone information.\n\t */\n\tif ((flags & SLAB_RED_ZONE) && size == s->object_size)\n\t\tsize += sizeof(void *);\n#endif\n\n\t/*\n\t * With that we have determined the number of bytes in actual use\n\t * by the object. This is the potential offset to the free pointer.\n\t */\n\ts->inuse = size;\n\n\tif (((flags & (SLAB_TYPESAFE_BY_RCU | SLAB_POISON)) ||\n\t\ts->ctor)) {\n\t\t/*\n\t\t * Relocate free pointer after the object if it is not\n\t\t * permitted to overwrite the first word of the object on\n\t\t * kmem_cache_free.\n\t\t *\n\t\t * This is the case if we do RCU, have a constructor or\n\t\t * destructor or are poisoning the objects.\n\t\t *\n\t\t * The assumption that s->offset >= s->inuse means free\n\t\t * pointer is outside of the object is used in the\n\t\t * freeptr_outside_object() function. If that is no\n\t\t * longer true, the function needs to be modified.\n\t\t */\n\t\ts->offset = size;\n\t\tsize += sizeof(void *);\n\t} else if (freepointer_area > sizeof(void *)) {\n\t\t/*\n\t\t * Store freelist pointer near middle of object to keep\n\t\t * it away from the edges of the object to avoid small\n\t\t * sized over/underflows from neighboring allocations.\n\t\t */\n\t\ts->offset = ALIGN(freepointer_area / 2, sizeof(void *));\n\t}\n\n#ifdef CONFIG_SLUB_DEBUG\n\tif (flags & SLAB_STORE_USER)\n\t\t/*\n\t\t * Need to store information about allocs and frees after\n\t\t * the object.\n\t\t */\n\t\tsize += 2 * sizeof(struct track);\n#endif\n\n\tkasan_cache_create(s, &size, &s->flags);\n#ifdef CONFIG_SLUB_DEBUG\n\tif (flags & SLAB_RED_ZONE) {\n\t\t/*\n\t\t * Add some empty padding so that we can catch\n\t\t * overwrites from earlier objects rather than let\n\t\t * tracking information or the free pointer be\n\t\t * corrupted if a user writes before the start\n\t\t * of the object.\n\t\t */\n\t\tsize += sizeof(void *);\n\n\t\ts->red_left_pad = sizeof(void *);\n\t\ts->red_left_pad = ALIGN(s->red_left_pad, s->align);\n\t\tsize += s->red_left_pad;\n\t}\n#endif\n\n\t/*\n\t * SLUB stores one object immediately after another beginning from\n\t * offset 0. In order to align the objects we have to simply size\n\t * each object to conform to the alignment.\n\t */\n\tsize = ALIGN(size, s->align);\n\ts->size = size;\n\ts->reciprocal_size = reciprocal_value(size);\n\tif (forced_order >= 0)\n\t\torder = forced_order;\n\telse\n\t\torder = calculate_order(size);\n\n\tif ((int)order < 0)\n\t\treturn 0;\n\n\ts->allocflags = 0;\n\tif (order)\n\t\ts->allocflags |= __GFP_COMP;\n\n\tif (s->flags & SLAB_CACHE_DMA)\n\t\ts->allocflags |= GFP_DMA;\n\n\tif (s->flags & SLAB_CACHE_DMA32)\n\t\ts->allocflags |= GFP_DMA32;\n\n\tif (s->flags & SLAB_RECLAIM_ACCOUNT)\n\t\ts->allocflags |= __GFP_RECLAIMABLE;\n\n\t/*\n\t * Determine the number of objects per slab\n\t */\n\ts->oo = oo_make(order, size);\n\ts->min = oo_make(get_order(size), size);\n\tif (oo_objects(s->oo) > oo_objects(s->max))\n\t\ts->max = s->oo;\n\n\treturn !!oo_objects(s->oo);\n}\n\nstatic int kmem_cache_open(struct kmem_cache *s, slab_flags_t flags)\n{\n#ifdef CONFIG_SLUB_DEBUG\n\t/*\n\t * If no slub_debug was enabled globally, the static key is not yet\n\t * enabled by setup_slub_debug(). Enable it if the cache is being\n\t * created with any of the debugging flags passed explicitly.\n\t */\n\tif (flags & SLAB_DEBUG_FLAGS)\n\t\tstatic_branch_enable(&slub_debug_enabled);\n#endif\n\ts->flags = kmem_cache_flags(s->size, flags, s->name);\n#ifdef CONFIG_SLAB_FREELIST_HARDENED\n\ts->random = get_random_long();\n#endif\n\n\tif (!calculate_sizes(s, -1))\n\t\tgoto error;\n\tif (disable_higher_order_debug) {\n\t\t/*\n\t\t * Disable debugging flags that store metadata if the min slab\n\t\t * order increased.\n\t\t */\n\t\tif (get_order(s->size) > get_order(s->object_size)) {\n\t\t\ts->flags &= ~DEBUG_METADATA_FLAGS;\n\t\t\ts->offset = 0;\n\t\t\tif (!calculate_sizes(s, -1))\n\t\t\t\tgoto error;\n\t\t}\n\t}\n\n#if defined(CONFIG_HAVE_CMPXCHG_DOUBLE) && \\\n    defined(CONFIG_HAVE_ALIGNED_STRUCT_PAGE)\n\tif (system_has_cmpxchg_double() && (s->flags & SLAB_NO_CMPXCHG) == 0)\n\t\t/* Enable fast mode */\n\t\ts->flags |= __CMPXCHG_DOUBLE;\n#endif\n\n\t/*\n\t * The larger the object size is, the more pages we want on the partial\n\t * list to avoid pounding the page allocator excessively.\n\t */\n\tset_min_partial(s, ilog2(s->size) / 2);\n\n\tset_cpu_partial(s);\n\n#ifdef CONFIG_NUMA\n\ts->remote_node_defrag_ratio = 1000;\n#endif\n\n\t/* Initialize the pre-computed randomized freelist if slab is up */\n\tif (slab_state >= UP) {\n\t\tif (init_cache_random_seq(s))\n\t\t\tgoto error;\n\t}\n\n\tif (!init_kmem_cache_nodes(s))\n\t\tgoto error;\n\n\tif (alloc_kmem_cache_cpus(s))\n\t\treturn 0;\n\n\tfree_kmem_cache_nodes(s);\nerror:\n\treturn -EINVAL;\n}\n\nstatic void list_slab_objects(struct kmem_cache *s, struct page *page,\n\t\t\t      const char *text)\n{\n#ifdef CONFIG_SLUB_DEBUG\n\tvoid *addr = page_address(page);\n\tunsigned long *map;\n\tvoid *p;\n\n\tslab_err(s, page, text, s->name);\n\tslab_lock(page);\n\n\tmap = get_map(s, page);\n\tfor_each_object(p, s, addr, page->objects) {\n\n\t\tif (!test_bit(__obj_to_index(s, addr, p), map)) {\n\t\t\tpr_err(\"Object 0x%p @offset=%tu\\n\", p, p - addr);\n\t\t\tprint_tracking(s, p);\n\t\t}\n\t}\n\tput_map(map);\n\tslab_unlock(page);\n#endif\n}\n\n/*\n * Attempt to free all partial slabs on a node.\n * This is called from __kmem_cache_shutdown(). We must take list_lock\n * because sysfs file might still access partial list after the shutdowning.\n */\nstatic void free_partial(struct kmem_cache *s, struct kmem_cache_node *n)\n{\n\tLIST_HEAD(discard);\n\tstruct page *page, *h;\n\n\tBUG_ON(irqs_disabled());\n\tspin_lock_irq(&n->list_lock);\n\tlist_for_each_entry_safe(page, h, &n->partial, slab_list) {\n\t\tif (!page->inuse) {\n\t\t\tremove_partial(n, page);\n\t\t\tlist_add(&page->slab_list, &discard);\n\t\t} else {\n\t\t\tlist_slab_objects(s, page,\n\t\t\t  \"Objects remaining in %s on __kmem_cache_shutdown()\");\n\t\t}\n\t}\n\tspin_unlock_irq(&n->list_lock);\n\n\tlist_for_each_entry_safe(page, h, &discard, slab_list)\n\t\tdiscard_slab(s, page);\n}\n\nbool __kmem_cache_empty(struct kmem_cache *s)\n{\n\tint node;\n\tstruct kmem_cache_node *n;\n\n\tfor_each_kmem_cache_node(s, node, n)\n\t\tif (n->nr_partial || slabs_node(s, node))\n\t\t\treturn false;\n\treturn true;\n}\n\n/*\n * Release all resources used by a slab cache.\n */\nint __kmem_cache_shutdown(struct kmem_cache *s)\n{\n\tint node;\n\tstruct kmem_cache_node *n;\n\n\tflush_all(s);\n\t/* Attempt to free all objects */\n\tfor_each_kmem_cache_node(s, node, n) {\n\t\tfree_partial(s, n);\n\t\tif (n->nr_partial || slabs_node(s, node))\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_PRINTK\nvoid kmem_obj_info(struct kmem_obj_info *kpp, void *object, struct page *page)\n{\n\tvoid *base;\n\tint __maybe_unused i;\n\tunsigned int objnr;\n\tvoid *objp;\n\tvoid *objp0;\n\tstruct kmem_cache *s = page->slab_cache;\n\tstruct track __maybe_unused *trackp;\n\n\tkpp->kp_ptr = object;\n\tkpp->kp_page = page;\n\tkpp->kp_slab_cache = s;\n\tbase = page_address(page);\n\tobjp0 = kasan_reset_tag(object);\n#ifdef CONFIG_SLUB_DEBUG\n\tobjp = restore_red_left(s, objp0);\n#else\n\tobjp = objp0;\n#endif\n\tobjnr = obj_to_index(s, page, objp);\n\tkpp->kp_data_offset = (unsigned long)((char *)objp0 - (char *)objp);\n\tobjp = base + s->size * objnr;\n\tkpp->kp_objp = objp;\n\tif (WARN_ON_ONCE(objp < base || objp >= base + page->objects * s->size || (objp - base) % s->size) ||\n\t    !(s->flags & SLAB_STORE_USER))\n\t\treturn;\n#ifdef CONFIG_SLUB_DEBUG\n\tobjp = fixup_red_left(s, objp);\n\ttrackp = get_track(s, objp, TRACK_ALLOC);\n\tkpp->kp_ret = (void *)trackp->addr;\n#ifdef CONFIG_STACKTRACE\n\tfor (i = 0; i < KS_ADDRS_COUNT && i < TRACK_ADDRS_COUNT; i++) {\n\t\tkpp->kp_stack[i] = (void *)trackp->addrs[i];\n\t\tif (!kpp->kp_stack[i])\n\t\t\tbreak;\n\t}\n\n\ttrackp = get_track(s, objp, TRACK_FREE);\n\tfor (i = 0; i < KS_ADDRS_COUNT && i < TRACK_ADDRS_COUNT; i++) {\n\t\tkpp->kp_free_stack[i] = (void *)trackp->addrs[i];\n\t\tif (!kpp->kp_free_stack[i])\n\t\t\tbreak;\n\t}\n#endif\n#endif\n}\n#endif\n\n/********************************************************************\n *\t\tKmalloc subsystem\n *******************************************************************/\n\nstatic int __init setup_slub_min_order(char *str)\n{\n\tget_option(&str, (int *)&slub_min_order);\n\n\treturn 1;\n}\n\n__setup(\"slub_min_order=\", setup_slub_min_order);\n\nstatic int __init setup_slub_max_order(char *str)\n{\n\tget_option(&str, (int *)&slub_max_order);\n\tslub_max_order = min(slub_max_order, (unsigned int)MAX_ORDER - 1);\n\n\treturn 1;\n}\n\n__setup(\"slub_max_order=\", setup_slub_max_order);\n\nstatic int __init setup_slub_min_objects(char *str)\n{\n\tget_option(&str, (int *)&slub_min_objects);\n\n\treturn 1;\n}\n\n__setup(\"slub_min_objects=\", setup_slub_min_objects);\n\nvoid *__kmalloc(size_t size, gfp_t flags)\n{\n\tstruct kmem_cache *s;\n\tvoid *ret;\n\n\tif (unlikely(size > KMALLOC_MAX_CACHE_SIZE))\n\t\treturn kmalloc_large(size, flags);\n\n\ts = kmalloc_slab(size, flags);\n\n\tif (unlikely(ZERO_OR_NULL_PTR(s)))\n\t\treturn s;\n\n\tret = slab_alloc(s, flags, _RET_IP_, size);\n\n\ttrace_kmalloc(_RET_IP_, ret, size, s->size, flags);\n\n\tret = kasan_kmalloc(s, ret, size, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(__kmalloc);\n\n#ifdef CONFIG_NUMA\nstatic void *kmalloc_large_node(size_t size, gfp_t flags, int node)\n{\n\tstruct page *page;\n\tvoid *ptr = NULL;\n\tunsigned int order = get_order(size);\n\n\tflags |= __GFP_COMP;\n\tpage = alloc_pages_node(node, flags, order);\n\tif (page) {\n\t\tptr = page_address(page);\n\t\tmod_lruvec_page_state(page, NR_SLAB_UNRECLAIMABLE_B,\n\t\t\t\t      PAGE_SIZE << order);\n\t}\n\n\treturn kmalloc_large_node_hook(ptr, size, flags);\n}\n\nvoid *__kmalloc_node(size_t size, gfp_t flags, int node)\n{\n\tstruct kmem_cache *s;\n\tvoid *ret;\n\n\tif (unlikely(size > KMALLOC_MAX_CACHE_SIZE)) {\n\t\tret = kmalloc_large_node(size, flags, node);\n\n\t\ttrace_kmalloc_node(_RET_IP_, ret,\n\t\t\t\t   size, PAGE_SIZE << get_order(size),\n\t\t\t\t   flags, node);\n\n\t\treturn ret;\n\t}\n\n\ts = kmalloc_slab(size, flags);\n\n\tif (unlikely(ZERO_OR_NULL_PTR(s)))\n\t\treturn s;\n\n\tret = slab_alloc_node(s, flags, node, _RET_IP_, size);\n\n\ttrace_kmalloc_node(_RET_IP_, ret, size, s->size, flags, node);\n\n\tret = kasan_kmalloc(s, ret, size, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(__kmalloc_node);\n#endif\t/* CONFIG_NUMA */\n\n#ifdef CONFIG_HARDENED_USERCOPY\n/*\n * Rejects incorrectly sized objects and objects that are to be copied\n * to/from userspace but do not fall entirely within the containing slab\n * cache's usercopy region.\n *\n * Returns NULL if check passes, otherwise const char * to name of cache\n * to indicate an error.\n */\nvoid __check_heap_object(const void *ptr, unsigned long n, struct page *page,\n\t\t\t bool to_user)\n{\n\tstruct kmem_cache *s;\n\tunsigned int offset;\n\tsize_t object_size;\n\tbool is_kfence = is_kfence_address(ptr);\n\n\tptr = kasan_reset_tag(ptr);\n\n\t/* Find object and usable object size. */\n\ts = page->slab_cache;\n\n\t/* Reject impossible pointers. */\n\tif (ptr < page_address(page))\n\t\tusercopy_abort(\"SLUB object not in SLUB page?!\", NULL,\n\t\t\t       to_user, 0, n);\n\n\t/* Find offset within object. */\n\tif (is_kfence)\n\t\toffset = ptr - kfence_object_start(ptr);\n\telse\n\t\toffset = (ptr - page_address(page)) % s->size;\n\n\t/* Adjust for redzone and reject if within the redzone. */\n\tif (!is_kfence && kmem_cache_debug_flags(s, SLAB_RED_ZONE)) {\n\t\tif (offset < s->red_left_pad)\n\t\t\tusercopy_abort(\"SLUB object in left red zone\",\n\t\t\t\t       s->name, to_user, offset, n);\n\t\toffset -= s->red_left_pad;\n\t}\n\n\t/* Allow address range falling entirely within usercopy region. */\n\tif (offset >= s->useroffset &&\n\t    offset - s->useroffset <= s->usersize &&\n\t    n <= s->useroffset - offset + s->usersize)\n\t\treturn;\n\n\t/*\n\t * If the copy is still within the allocated object, produce\n\t * a warning instead of rejecting the copy. This is intended\n\t * to be a temporary method to find any missing usercopy\n\t * whitelists.\n\t */\n\tobject_size = slab_ksize(s);\n\tif (usercopy_fallback &&\n\t    offset <= object_size && n <= object_size - offset) {\n\t\tusercopy_warn(\"SLUB object\", s->name, to_user, offset, n);\n\t\treturn;\n\t}\n\n\tusercopy_abort(\"SLUB object\", s->name, to_user, offset, n);\n}\n#endif /* CONFIG_HARDENED_USERCOPY */\n\nsize_t __ksize(const void *object)\n{\n\tstruct page *page;\n\n\tif (unlikely(object == ZERO_SIZE_PTR))\n\t\treturn 0;\n\n\tpage = virt_to_head_page(object);\n\n\tif (unlikely(!PageSlab(page))) {\n\t\tWARN_ON(!PageCompound(page));\n\t\treturn page_size(page);\n\t}\n\n\treturn slab_ksize(page->slab_cache);\n}\nEXPORT_SYMBOL(__ksize);\n\nvoid kfree(const void *x)\n{\n\tstruct page *page;\n\tvoid *object = (void *)x;\n\n\ttrace_kfree(_RET_IP_, x);\n\n\tif (unlikely(ZERO_OR_NULL_PTR(x)))\n\t\treturn;\n\n\tpage = virt_to_head_page(x);\n\tif (unlikely(!PageSlab(page))) {\n\t\tunsigned int order = compound_order(page);\n\n\t\tBUG_ON(!PageCompound(page));\n\t\tkfree_hook(object);\n\t\tmod_lruvec_page_state(page, NR_SLAB_UNRECLAIMABLE_B,\n\t\t\t\t      -(PAGE_SIZE << order));\n\t\t__free_pages(page, order);\n\t\treturn;\n\t}\n\tslab_free(page->slab_cache, page, object, NULL, 1, _RET_IP_);\n}\nEXPORT_SYMBOL(kfree);\n\n#define SHRINK_PROMOTE_MAX 32\n\n/*\n * kmem_cache_shrink discards empty slabs and promotes the slabs filled\n * up most to the head of the partial lists. New allocations will then\n * fill those up and thus they can be removed from the partial lists.\n *\n * The slabs with the least items are placed last. This results in them\n * being allocated from last increasing the chance that the last objects\n * are freed in them.\n */\nint __kmem_cache_shrink(struct kmem_cache *s)\n{\n\tint node;\n\tint i;\n\tstruct kmem_cache_node *n;\n\tstruct page *page;\n\tstruct page *t;\n\tstruct list_head discard;\n\tstruct list_head promote[SHRINK_PROMOTE_MAX];\n\tunsigned long flags;\n\tint ret = 0;\n\n\tflush_all(s);\n\tfor_each_kmem_cache_node(s, node, n) {\n\t\tINIT_LIST_HEAD(&discard);\n\t\tfor (i = 0; i < SHRINK_PROMOTE_MAX; i++)\n\t\t\tINIT_LIST_HEAD(promote + i);\n\n\t\tspin_lock_irqsave(&n->list_lock, flags);\n\n\t\t/*\n\t\t * Build lists of slabs to discard or promote.\n\t\t *\n\t\t * Note that concurrent frees may occur while we hold the\n\t\t * list_lock. page->inuse here is the upper limit.\n\t\t */\n\t\tlist_for_each_entry_safe(page, t, &n->partial, slab_list) {\n\t\t\tint free = page->objects - page->inuse;\n\n\t\t\t/* Do not reread page->inuse */\n\t\t\tbarrier();\n\n\t\t\t/* We do not keep full slabs on the list */\n\t\t\tBUG_ON(free <= 0);\n\n\t\t\tif (free == page->objects) {\n\t\t\t\tlist_move(&page->slab_list, &discard);\n\t\t\t\tn->nr_partial--;\n\t\t\t} else if (free <= SHRINK_PROMOTE_MAX)\n\t\t\t\tlist_move(&page->slab_list, promote + free - 1);\n\t\t}\n\n\t\t/*\n\t\t * Promote the slabs filled up most to the head of the\n\t\t * partial list.\n\t\t */\n\t\tfor (i = SHRINK_PROMOTE_MAX - 1; i >= 0; i--)\n\t\t\tlist_splice(promote + i, &n->partial);\n\n\t\tspin_unlock_irqrestore(&n->list_lock, flags);\n\n\t\t/* Release empty slabs */\n\t\tlist_for_each_entry_safe(page, t, &discard, slab_list)\n\t\t\tdiscard_slab(s, page);\n\n\t\tif (slabs_node(s, node))\n\t\t\tret = 1;\n\t}\n\n\treturn ret;\n}\n\nstatic int slab_mem_going_offline_callback(void *arg)\n{\n\tstruct kmem_cache *s;\n\n\tmutex_lock(&slab_mutex);\n\tlist_for_each_entry(s, &slab_caches, list)\n\t\t__kmem_cache_shrink(s);\n\tmutex_unlock(&slab_mutex);\n\n\treturn 0;\n}\n\nstatic void slab_mem_offline_callback(void *arg)\n{\n\tstruct memory_notify *marg = arg;\n\tint offline_node;\n\n\toffline_node = marg->status_change_nid_normal;\n\n\t/*\n\t * If the node still has available memory. we need kmem_cache_node\n\t * for it yet.\n\t */\n\tif (offline_node < 0)\n\t\treturn;\n\n\tmutex_lock(&slab_mutex);\n\tnode_clear(offline_node, slab_nodes);\n\t/*\n\t * We no longer free kmem_cache_node structures here, as it would be\n\t * racy with all get_node() users, and infeasible to protect them with\n\t * slab_mutex.\n\t */\n\tmutex_unlock(&slab_mutex);\n}\n\nstatic int slab_mem_going_online_callback(void *arg)\n{\n\tstruct kmem_cache_node *n;\n\tstruct kmem_cache *s;\n\tstruct memory_notify *marg = arg;\n\tint nid = marg->status_change_nid_normal;\n\tint ret = 0;\n\n\t/*\n\t * If the node's memory is already available, then kmem_cache_node is\n\t * already created. Nothing to do.\n\t */\n\tif (nid < 0)\n\t\treturn 0;\n\n\t/*\n\t * We are bringing a node online. No memory is available yet. We must\n\t * allocate a kmem_cache_node structure in order to bring the node\n\t * online.\n\t */\n\tmutex_lock(&slab_mutex);\n\tlist_for_each_entry(s, &slab_caches, list) {\n\t\t/*\n\t\t * The structure may already exist if the node was previously\n\t\t * onlined and offlined.\n\t\t */\n\t\tif (get_node(s, nid))\n\t\t\tcontinue;\n\t\t/*\n\t\t * XXX: kmem_cache_alloc_node will fallback to other nodes\n\t\t *      since memory is not yet available from the node that\n\t\t *      is brought up.\n\t\t */\n\t\tn = kmem_cache_alloc(kmem_cache_node, GFP_KERNEL);\n\t\tif (!n) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tinit_kmem_cache_node(n);\n\t\ts->node[nid] = n;\n\t}\n\t/*\n\t * Any cache created after this point will also have kmem_cache_node\n\t * initialized for the new node.\n\t */\n\tnode_set(nid, slab_nodes);\nout:\n\tmutex_unlock(&slab_mutex);\n\treturn ret;\n}\n\nstatic int slab_memory_callback(struct notifier_block *self,\n\t\t\t\tunsigned long action, void *arg)\n{\n\tint ret = 0;\n\n\tswitch (action) {\n\tcase MEM_GOING_ONLINE:\n\t\tret = slab_mem_going_online_callback(arg);\n\t\tbreak;\n\tcase MEM_GOING_OFFLINE:\n\t\tret = slab_mem_going_offline_callback(arg);\n\t\tbreak;\n\tcase MEM_OFFLINE:\n\tcase MEM_CANCEL_ONLINE:\n\t\tslab_mem_offline_callback(arg);\n\t\tbreak;\n\tcase MEM_ONLINE:\n\tcase MEM_CANCEL_OFFLINE:\n\t\tbreak;\n\t}\n\tif (ret)\n\t\tret = notifier_from_errno(ret);\n\telse\n\t\tret = NOTIFY_OK;\n\treturn ret;\n}\n\nstatic struct notifier_block slab_memory_callback_nb = {\n\t.notifier_call = slab_memory_callback,\n\t.priority = SLAB_CALLBACK_PRI,\n};\n\n/********************************************************************\n *\t\t\tBasic setup of slabs\n *******************************************************************/\n\n/*\n * Used for early kmem_cache structures that were allocated using\n * the page allocator. Allocate them properly then fix up the pointers\n * that may be pointing to the wrong kmem_cache structure.\n */\n\nstatic struct kmem_cache * __init bootstrap(struct kmem_cache *static_cache)\n{\n\tint node;\n\tstruct kmem_cache *s = kmem_cache_zalloc(kmem_cache, GFP_NOWAIT);\n\tstruct kmem_cache_node *n;\n\n\tmemcpy(s, static_cache, kmem_cache->object_size);\n\n\t/*\n\t * This runs very early, and only the boot processor is supposed to be\n\t * up.  Even if it weren't true, IRQs are not up so we couldn't fire\n\t * IPIs around.\n\t */\n\t__flush_cpu_slab(s, smp_processor_id());\n\tfor_each_kmem_cache_node(s, node, n) {\n\t\tstruct page *p;\n\n\t\tlist_for_each_entry(p, &n->partial, slab_list)\n\t\t\tp->slab_cache = s;\n\n#ifdef CONFIG_SLUB_DEBUG\n\t\tlist_for_each_entry(p, &n->full, slab_list)\n\t\t\tp->slab_cache = s;\n#endif\n\t}\n\tlist_add(&s->list, &slab_caches);\n\treturn s;\n}\n\nvoid __init kmem_cache_init(void)\n{\n\tstatic __initdata struct kmem_cache boot_kmem_cache,\n\t\tboot_kmem_cache_node;\n\tint node;\n\n\tif (debug_guardpage_minorder())\n\t\tslub_max_order = 0;\n\n\tkmem_cache_node = &boot_kmem_cache_node;\n\tkmem_cache = &boot_kmem_cache;\n\n\t/*\n\t * Initialize the nodemask for which we will allocate per node\n\t * structures. Here we don't need taking slab_mutex yet.\n\t */\n\tfor_each_node_state(node, N_NORMAL_MEMORY)\n\t\tnode_set(node, slab_nodes);\n\n\tcreate_boot_cache(kmem_cache_node, \"kmem_cache_node\",\n\t\tsizeof(struct kmem_cache_node), SLAB_HWCACHE_ALIGN, 0, 0);\n\n\tregister_hotmemory_notifier(&slab_memory_callback_nb);\n\n\t/* Able to allocate the per node structures */\n\tslab_state = PARTIAL;\n\n\tcreate_boot_cache(kmem_cache, \"kmem_cache\",\n\t\t\toffsetof(struct kmem_cache, node) +\n\t\t\t\tnr_node_ids * sizeof(struct kmem_cache_node *),\n\t\t       SLAB_HWCACHE_ALIGN, 0, 0);\n\n\tkmem_cache = bootstrap(&boot_kmem_cache);\n\tkmem_cache_node = bootstrap(&boot_kmem_cache_node);\n\n\t/* Now we can use the kmem_cache to allocate kmalloc slabs */\n\tsetup_kmalloc_cache_index_table();\n\tcreate_kmalloc_caches(0);\n\n\t/* Setup random freelists for each cache */\n\tinit_freelist_randomization();\n\n\tcpuhp_setup_state_nocalls(CPUHP_SLUB_DEAD, \"slub:dead\", NULL,\n\t\t\t\t  slub_cpu_dead);\n\n\tpr_info(\"SLUB: HWalign=%d, Order=%u-%u, MinObjects=%u, CPUs=%u, Nodes=%u\\n\",\n\t\tcache_line_size(),\n\t\tslub_min_order, slub_max_order, slub_min_objects,\n\t\tnr_cpu_ids, nr_node_ids);\n}\n\nvoid __init kmem_cache_init_late(void)\n{\n}\n\nstruct kmem_cache *\n__kmem_cache_alias(const char *name, unsigned int size, unsigned int align,\n\t\t   slab_flags_t flags, void (*ctor)(void *))\n{\n\tstruct kmem_cache *s;\n\n\ts = find_mergeable(size, align, flags, name, ctor);\n\tif (s) {\n\t\ts->refcount++;\n\n\t\t/*\n\t\t * Adjust the object sizes so that we clear\n\t\t * the complete object on kzalloc.\n\t\t */\n\t\ts->object_size = max(s->object_size, size);\n\t\ts->inuse = max(s->inuse, ALIGN(size, sizeof(void *)));\n\n\t\tif (sysfs_slab_alias(s, name)) {\n\t\t\ts->refcount--;\n\t\t\ts = NULL;\n\t\t}\n\t}\n\n\treturn s;\n}\n\nint __kmem_cache_create(struct kmem_cache *s, slab_flags_t flags)\n{\n\tint err;\n\n\terr = kmem_cache_open(s, flags);\n\tif (err)\n\t\treturn err;\n\n\t/* Mutex is not taken during early boot */\n\tif (slab_state <= UP)\n\t\treturn 0;\n\n\terr = sysfs_slab_add(s);\n\tif (err)\n\t\t__kmem_cache_release(s);\n\n\treturn err;\n}\n\nvoid *__kmalloc_track_caller(size_t size, gfp_t gfpflags, unsigned long caller)\n{\n\tstruct kmem_cache *s;\n\tvoid *ret;\n\n\tif (unlikely(size > KMALLOC_MAX_CACHE_SIZE))\n\t\treturn kmalloc_large(size, gfpflags);\n\n\ts = kmalloc_slab(size, gfpflags);\n\n\tif (unlikely(ZERO_OR_NULL_PTR(s)))\n\t\treturn s;\n\n\tret = slab_alloc(s, gfpflags, caller, size);\n\n\t/* Honor the call site pointer we received. */\n\ttrace_kmalloc(caller, ret, size, s->size, gfpflags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(__kmalloc_track_caller);\n\n#ifdef CONFIG_NUMA\nvoid *__kmalloc_node_track_caller(size_t size, gfp_t gfpflags,\n\t\t\t\t\tint node, unsigned long caller)\n{\n\tstruct kmem_cache *s;\n\tvoid *ret;\n\n\tif (unlikely(size > KMALLOC_MAX_CACHE_SIZE)) {\n\t\tret = kmalloc_large_node(size, gfpflags, node);\n\n\t\ttrace_kmalloc_node(caller, ret,\n\t\t\t\t   size, PAGE_SIZE << get_order(size),\n\t\t\t\t   gfpflags, node);\n\n\t\treturn ret;\n\t}\n\n\ts = kmalloc_slab(size, gfpflags);\n\n\tif (unlikely(ZERO_OR_NULL_PTR(s)))\n\t\treturn s;\n\n\tret = slab_alloc_node(s, gfpflags, node, caller, size);\n\n\t/* Honor the call site pointer we received. */\n\ttrace_kmalloc_node(caller, ret, size, s->size, gfpflags, node);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(__kmalloc_node_track_caller);\n#endif\n\n#ifdef CONFIG_SYSFS\nstatic int count_inuse(struct page *page)\n{\n\treturn page->inuse;\n}\n\nstatic int count_total(struct page *page)\n{\n\treturn page->objects;\n}\n#endif\n\n#ifdef CONFIG_SLUB_DEBUG\nstatic void validate_slab(struct kmem_cache *s, struct page *page)\n{\n\tvoid *p;\n\tvoid *addr = page_address(page);\n\tunsigned long *map;\n\n\tslab_lock(page);\n\n\tif (!check_slab(s, page) || !on_freelist(s, page, NULL))\n\t\tgoto unlock;\n\n\t/* Now we know that a valid freelist exists */\n\tmap = get_map(s, page);\n\tfor_each_object(p, s, addr, page->objects) {\n\t\tu8 val = test_bit(__obj_to_index(s, addr, p), map) ?\n\t\t\t SLUB_RED_INACTIVE : SLUB_RED_ACTIVE;\n\n\t\tif (!check_object(s, page, p, val)) {\n\t\t\ts->errors += 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tput_map(map);\nunlock:\n\tslab_unlock(page);\n}\n\nstatic int validate_slab_node(struct kmem_cache *s,\n\t\tstruct kmem_cache_node *n)\n{\n\tunsigned long count = 0;\n\tstruct page *page;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&n->list_lock, flags);\n\n\tlist_for_each_entry(page, &n->partial, slab_list) {\n\t\tvalidate_slab(s, page);\n\t\tcount++;\n\t}\n\tif (count != n->nr_partial) {\n\t\tpr_err(\"SLUB %s: %ld partial slabs counted but counter=%ld\\n\",\n\t\t       s->name, count, n->nr_partial);\n\t\ts->errors += 1;\n\t}\n\n\tif (!(s->flags & SLAB_STORE_USER))\n\t\tgoto out;\n\n\tlist_for_each_entry(page, &n->full, slab_list) {\n\t\tvalidate_slab(s, page);\n\t\tcount++;\n\t}\n\tif (count != atomic_long_read(&n->nr_slabs)) {\n\t\tpr_err(\"SLUB: %s %ld slabs counted but counter=%ld\\n\",\n\t\t       s->name, count, atomic_long_read(&n->nr_slabs));\n\t\ts->errors += 1;\n\t}\n\nout:\n\tspin_unlock_irqrestore(&n->list_lock, flags);\n\treturn count;\n}\n\nlong validate_slab_cache(struct kmem_cache *s)\n{\n\tint node;\n\tunsigned long count = 0;\n\tstruct kmem_cache_node *n;\n\ts->errors = 0;\n\n\tflush_all(s);\n\tfor_each_kmem_cache_node(s, node, n)\n\t\tcount += validate_slab_node(s, n);\n\n\treturn count;\n}\nEXPORT_SYMBOL(validate_slab_cache);\n\n/*\n * Generate lists of code addresses where slabcache objects are allocated\n * and freed.\n */\n\nstruct location {\n\tunsigned long count;\n\tunsigned long addr;\n\tlong long sum_time;\n\tlong min_time;\n\tlong max_time;\n\tlong min_pid;\n\tlong max_pid;\n\tDECLARE_BITMAP(cpus, NR_CPUS);\n\tnodemask_t nodes;\n};\n\nstruct loc_track {\n\tunsigned long max;\n\tunsigned long count;\n\tstruct location *loc;\n};\n\nstatic void free_loc_track(struct loc_track *t)\n{\n\tif (t->max)\n\t\tfree_pages((unsigned long)t->loc,\n\t\t\tget_order(sizeof(struct location) * t->max));\n}\n\nstatic int alloc_loc_track(struct loc_track *t, unsigned long max, gfp_t flags)\n{\n\tstruct location *l;\n\tint order;\n\n\torder = get_order(sizeof(struct location) * max);\n\n\tl = (void *)__get_free_pages(flags, order);\n\tif (!l)\n\t\treturn 0;\n\n\tif (t->count) {\n\t\tmemcpy(l, t->loc, sizeof(struct location) * t->count);\n\t\tfree_loc_track(t);\n\t}\n\tt->max = max;\n\tt->loc = l;\n\treturn 1;\n}\n\nstatic int add_location(struct loc_track *t, struct kmem_cache *s,\n\t\t\t\tconst struct track *track)\n{\n\tlong start, end, pos;\n\tstruct location *l;\n\tunsigned long caddr;\n\tunsigned long age = jiffies - track->when;\n\n\tstart = -1;\n\tend = t->count;\n\n\tfor ( ; ; ) {\n\t\tpos = start + (end - start + 1) / 2;\n\n\t\t/*\n\t\t * There is nothing at \"end\". If we end up there\n\t\t * we need to add something to before end.\n\t\t */\n\t\tif (pos == end)\n\t\t\tbreak;\n\n\t\tcaddr = t->loc[pos].addr;\n\t\tif (track->addr == caddr) {\n\n\t\t\tl = &t->loc[pos];\n\t\t\tl->count++;\n\t\t\tif (track->when) {\n\t\t\t\tl->sum_time += age;\n\t\t\t\tif (age < l->min_time)\n\t\t\t\t\tl->min_time = age;\n\t\t\t\tif (age > l->max_time)\n\t\t\t\t\tl->max_time = age;\n\n\t\t\t\tif (track->pid < l->min_pid)\n\t\t\t\t\tl->min_pid = track->pid;\n\t\t\t\tif (track->pid > l->max_pid)\n\t\t\t\t\tl->max_pid = track->pid;\n\n\t\t\t\tcpumask_set_cpu(track->cpu,\n\t\t\t\t\t\tto_cpumask(l->cpus));\n\t\t\t}\n\t\t\tnode_set(page_to_nid(virt_to_page(track)), l->nodes);\n\t\t\treturn 1;\n\t\t}\n\n\t\tif (track->addr < caddr)\n\t\t\tend = pos;\n\t\telse\n\t\t\tstart = pos;\n\t}\n\n\t/*\n\t * Not found. Insert new tracking element.\n\t */\n\tif (t->count >= t->max && !alloc_loc_track(t, 2 * t->max, GFP_ATOMIC))\n\t\treturn 0;\n\n\tl = t->loc + pos;\n\tif (pos < t->count)\n\t\tmemmove(l + 1, l,\n\t\t\t(t->count - pos) * sizeof(struct location));\n\tt->count++;\n\tl->count = 1;\n\tl->addr = track->addr;\n\tl->sum_time = age;\n\tl->min_time = age;\n\tl->max_time = age;\n\tl->min_pid = track->pid;\n\tl->max_pid = track->pid;\n\tcpumask_clear(to_cpumask(l->cpus));\n\tcpumask_set_cpu(track->cpu, to_cpumask(l->cpus));\n\tnodes_clear(l->nodes);\n\tnode_set(page_to_nid(virt_to_page(track)), l->nodes);\n\treturn 1;\n}\n\nstatic void process_slab(struct loc_track *t, struct kmem_cache *s,\n\t\tstruct page *page, enum track_item alloc)\n{\n\tvoid *addr = page_address(page);\n\tvoid *p;\n\tunsigned long *map;\n\n\tmap = get_map(s, page);\n\tfor_each_object(p, s, addr, page->objects)\n\t\tif (!test_bit(__obj_to_index(s, addr, p), map))\n\t\t\tadd_location(t, s, get_track(s, p, alloc));\n\tput_map(map);\n}\n\nstatic int list_locations(struct kmem_cache *s, char *buf,\n\t\t\t  enum track_item alloc)\n{\n\tint len = 0;\n\tunsigned long i;\n\tstruct loc_track t = { 0, 0, NULL };\n\tint node;\n\tstruct kmem_cache_node *n;\n\n\tif (!alloc_loc_track(&t, PAGE_SIZE / sizeof(struct location),\n\t\t\t     GFP_KERNEL)) {\n\t\treturn sysfs_emit(buf, \"Out of memory\\n\");\n\t}\n\t/* Push back cpu slabs */\n\tflush_all(s);\n\n\tfor_each_kmem_cache_node(s, node, n) {\n\t\tunsigned long flags;\n\t\tstruct page *page;\n\n\t\tif (!atomic_long_read(&n->nr_slabs))\n\t\t\tcontinue;\n\n\t\tspin_lock_irqsave(&n->list_lock, flags);\n\t\tlist_for_each_entry(page, &n->partial, slab_list)\n\t\t\tprocess_slab(&t, s, page, alloc);\n\t\tlist_for_each_entry(page, &n->full, slab_list)\n\t\t\tprocess_slab(&t, s, page, alloc);\n\t\tspin_unlock_irqrestore(&n->list_lock, flags);\n\t}\n\n\tfor (i = 0; i < t.count; i++) {\n\t\tstruct location *l = &t.loc[i];\n\n\t\tlen += sysfs_emit_at(buf, len, \"%7ld \", l->count);\n\n\t\tif (l->addr)\n\t\t\tlen += sysfs_emit_at(buf, len, \"%pS\", (void *)l->addr);\n\t\telse\n\t\t\tlen += sysfs_emit_at(buf, len, \"<not-available>\");\n\n\t\tif (l->sum_time != l->min_time)\n\t\t\tlen += sysfs_emit_at(buf, len, \" age=%ld/%ld/%ld\",\n\t\t\t\t\t     l->min_time,\n\t\t\t\t\t     (long)div_u64(l->sum_time,\n\t\t\t\t\t\t\t   l->count),\n\t\t\t\t\t     l->max_time);\n\t\telse\n\t\t\tlen += sysfs_emit_at(buf, len, \" age=%ld\", l->min_time);\n\n\t\tif (l->min_pid != l->max_pid)\n\t\t\tlen += sysfs_emit_at(buf, len, \" pid=%ld-%ld\",\n\t\t\t\t\t     l->min_pid, l->max_pid);\n\t\telse\n\t\t\tlen += sysfs_emit_at(buf, len, \" pid=%ld\",\n\t\t\t\t\t     l->min_pid);\n\n\t\tif (num_online_cpus() > 1 &&\n\t\t    !cpumask_empty(to_cpumask(l->cpus)))\n\t\t\tlen += sysfs_emit_at(buf, len, \" cpus=%*pbl\",\n\t\t\t\t\t     cpumask_pr_args(to_cpumask(l->cpus)));\n\n\t\tif (nr_online_nodes > 1 && !nodes_empty(l->nodes))\n\t\t\tlen += sysfs_emit_at(buf, len, \" nodes=%*pbl\",\n\t\t\t\t\t     nodemask_pr_args(&l->nodes));\n\n\t\tlen += sysfs_emit_at(buf, len, \"\\n\");\n\t}\n\n\tfree_loc_track(&t);\n\tif (!t.count)\n\t\tlen += sysfs_emit_at(buf, len, \"No data\\n\");\n\n\treturn len;\n}\n#endif\t/* CONFIG_SLUB_DEBUG */\n\n#ifdef CONFIG_SYSFS\nenum slab_stat_type {\n\tSL_ALL,\t\t\t/* All slabs */\n\tSL_PARTIAL,\t\t/* Only partially allocated slabs */\n\tSL_CPU,\t\t\t/* Only slabs used for cpu caches */\n\tSL_OBJECTS,\t\t/* Determine allocated objects not slabs */\n\tSL_TOTAL\t\t/* Determine object capacity not slabs */\n};\n\n#define SO_ALL\t\t(1 << SL_ALL)\n#define SO_PARTIAL\t(1 << SL_PARTIAL)\n#define SO_CPU\t\t(1 << SL_CPU)\n#define SO_OBJECTS\t(1 << SL_OBJECTS)\n#define SO_TOTAL\t(1 << SL_TOTAL)\n\nstatic ssize_t show_slab_objects(struct kmem_cache *s,\n\t\t\t\t char *buf, unsigned long flags)\n{\n\tunsigned long total = 0;\n\tint node;\n\tint x;\n\tunsigned long *nodes;\n\tint len = 0;\n\n\tnodes = kcalloc(nr_node_ids, sizeof(unsigned long), GFP_KERNEL);\n\tif (!nodes)\n\t\treturn -ENOMEM;\n\n\tif (flags & SO_CPU) {\n\t\tint cpu;\n\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tstruct kmem_cache_cpu *c = per_cpu_ptr(s->cpu_slab,\n\t\t\t\t\t\t\t       cpu);\n\t\t\tint node;\n\t\t\tstruct page *page;\n\n\t\t\tpage = READ_ONCE(c->page);\n\t\t\tif (!page)\n\t\t\t\tcontinue;\n\n\t\t\tnode = page_to_nid(page);\n\t\t\tif (flags & SO_TOTAL)\n\t\t\t\tx = page->objects;\n\t\t\telse if (flags & SO_OBJECTS)\n\t\t\t\tx = page->inuse;\n\t\t\telse\n\t\t\t\tx = 1;\n\n\t\t\ttotal += x;\n\t\t\tnodes[node] += x;\n\n\t\t\tpage = slub_percpu_partial_read_once(c);\n\t\t\tif (page) {\n\t\t\t\tnode = page_to_nid(page);\n\t\t\t\tif (flags & SO_TOTAL)\n\t\t\t\t\tWARN_ON_ONCE(1);\n\t\t\t\telse if (flags & SO_OBJECTS)\n\t\t\t\t\tWARN_ON_ONCE(1);\n\t\t\t\telse\n\t\t\t\t\tx = page->pages;\n\t\t\t\ttotal += x;\n\t\t\t\tnodes[node] += x;\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * It is impossible to take \"mem_hotplug_lock\" here with \"kernfs_mutex\"\n\t * already held which will conflict with an existing lock order:\n\t *\n\t * mem_hotplug_lock->slab_mutex->kernfs_mutex\n\t *\n\t * We don't really need mem_hotplug_lock (to hold off\n\t * slab_mem_going_offline_callback) here because slab's memory hot\n\t * unplug code doesn't destroy the kmem_cache->node[] data.\n\t */\n\n#ifdef CONFIG_SLUB_DEBUG\n\tif (flags & SO_ALL) {\n\t\tstruct kmem_cache_node *n;\n\n\t\tfor_each_kmem_cache_node(s, node, n) {\n\n\t\t\tif (flags & SO_TOTAL)\n\t\t\t\tx = atomic_long_read(&n->total_objects);\n\t\t\telse if (flags & SO_OBJECTS)\n\t\t\t\tx = atomic_long_read(&n->total_objects) -\n\t\t\t\t\tcount_partial(n, count_free);\n\t\t\telse\n\t\t\t\tx = atomic_long_read(&n->nr_slabs);\n\t\t\ttotal += x;\n\t\t\tnodes[node] += x;\n\t\t}\n\n\t} else\n#endif\n\tif (flags & SO_PARTIAL) {\n\t\tstruct kmem_cache_node *n;\n\n\t\tfor_each_kmem_cache_node(s, node, n) {\n\t\t\tif (flags & SO_TOTAL)\n\t\t\t\tx = count_partial(n, count_total);\n\t\t\telse if (flags & SO_OBJECTS)\n\t\t\t\tx = count_partial(n, count_inuse);\n\t\t\telse\n\t\t\t\tx = n->nr_partial;\n\t\t\ttotal += x;\n\t\t\tnodes[node] += x;\n\t\t}\n\t}\n\n\tlen += sysfs_emit_at(buf, len, \"%lu\", total);\n#ifdef CONFIG_NUMA\n\tfor (node = 0; node < nr_node_ids; node++) {\n\t\tif (nodes[node])\n\t\t\tlen += sysfs_emit_at(buf, len, \" N%d=%lu\",\n\t\t\t\t\t     node, nodes[node]);\n\t}\n#endif\n\tlen += sysfs_emit_at(buf, len, \"\\n\");\n\tkfree(nodes);\n\n\treturn len;\n}\n\n#define to_slab_attr(n) container_of(n, struct slab_attribute, attr)\n#define to_slab(n) container_of(n, struct kmem_cache, kobj)\n\nstruct slab_attribute {\n\tstruct attribute attr;\n\tssize_t (*show)(struct kmem_cache *s, char *buf);\n\tssize_t (*store)(struct kmem_cache *s, const char *x, size_t count);\n};\n\n#define SLAB_ATTR_RO(_name) \\\n\tstatic struct slab_attribute _name##_attr = \\\n\t__ATTR(_name, 0400, _name##_show, NULL)\n\n#define SLAB_ATTR(_name) \\\n\tstatic struct slab_attribute _name##_attr =  \\\n\t__ATTR(_name, 0600, _name##_show, _name##_store)\n\nstatic ssize_t slab_size_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", s->size);\n}\nSLAB_ATTR_RO(slab_size);\n\nstatic ssize_t align_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", s->align);\n}\nSLAB_ATTR_RO(align);\n\nstatic ssize_t object_size_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", s->object_size);\n}\nSLAB_ATTR_RO(object_size);\n\nstatic ssize_t objs_per_slab_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", oo_objects(s->oo));\n}\nSLAB_ATTR_RO(objs_per_slab);\n\nstatic ssize_t order_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", oo_order(s->oo));\n}\nSLAB_ATTR_RO(order);\n\nstatic ssize_t min_partial_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%lu\\n\", s->min_partial);\n}\n\nstatic ssize_t min_partial_store(struct kmem_cache *s, const char *buf,\n\t\t\t\t size_t length)\n{\n\tunsigned long min;\n\tint err;\n\n\terr = kstrtoul(buf, 10, &min);\n\tif (err)\n\t\treturn err;\n\n\tset_min_partial(s, min);\n\treturn length;\n}\nSLAB_ATTR(min_partial);\n\nstatic ssize_t cpu_partial_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", slub_cpu_partial(s));\n}\n\nstatic ssize_t cpu_partial_store(struct kmem_cache *s, const char *buf,\n\t\t\t\t size_t length)\n{\n\tunsigned int objects;\n\tint err;\n\n\terr = kstrtouint(buf, 10, &objects);\n\tif (err)\n\t\treturn err;\n\tif (objects && !kmem_cache_has_cpu_partial(s))\n\t\treturn -EINVAL;\n\n\tslub_set_cpu_partial(s, objects);\n\tflush_all(s);\n\treturn length;\n}\nSLAB_ATTR(cpu_partial);\n\nstatic ssize_t ctor_show(struct kmem_cache *s, char *buf)\n{\n\tif (!s->ctor)\n\t\treturn 0;\n\treturn sysfs_emit(buf, \"%pS\\n\", s->ctor);\n}\nSLAB_ATTR_RO(ctor);\n\nstatic ssize_t aliases_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", s->refcount < 0 ? 0 : s->refcount - 1);\n}\nSLAB_ATTR_RO(aliases);\n\nstatic ssize_t partial_show(struct kmem_cache *s, char *buf)\n{\n\treturn show_slab_objects(s, buf, SO_PARTIAL);\n}\nSLAB_ATTR_RO(partial);\n\nstatic ssize_t cpu_slabs_show(struct kmem_cache *s, char *buf)\n{\n\treturn show_slab_objects(s, buf, SO_CPU);\n}\nSLAB_ATTR_RO(cpu_slabs);\n\nstatic ssize_t objects_show(struct kmem_cache *s, char *buf)\n{\n\treturn show_slab_objects(s, buf, SO_ALL|SO_OBJECTS);\n}\nSLAB_ATTR_RO(objects);\n\nstatic ssize_t objects_partial_show(struct kmem_cache *s, char *buf)\n{\n\treturn show_slab_objects(s, buf, SO_PARTIAL|SO_OBJECTS);\n}\nSLAB_ATTR_RO(objects_partial);\n\nstatic ssize_t slabs_cpu_partial_show(struct kmem_cache *s, char *buf)\n{\n\tint objects = 0;\n\tint pages = 0;\n\tint cpu;\n\tint len = 0;\n\n\tfor_each_online_cpu(cpu) {\n\t\tstruct page *page;\n\n\t\tpage = slub_percpu_partial(per_cpu_ptr(s->cpu_slab, cpu));\n\n\t\tif (page) {\n\t\t\tpages += page->pages;\n\t\t\tobjects += page->pobjects;\n\t\t}\n\t}\n\n\tlen += sysfs_emit_at(buf, len, \"%d(%d)\", objects, pages);\n\n#ifdef CONFIG_SMP\n\tfor_each_online_cpu(cpu) {\n\t\tstruct page *page;\n\n\t\tpage = slub_percpu_partial(per_cpu_ptr(s->cpu_slab, cpu));\n\t\tif (page)\n\t\t\tlen += sysfs_emit_at(buf, len, \" C%d=%d(%d)\",\n\t\t\t\t\t     cpu, page->pobjects, page->pages);\n\t}\n#endif\n\tlen += sysfs_emit_at(buf, len, \"\\n\");\n\n\treturn len;\n}\nSLAB_ATTR_RO(slabs_cpu_partial);\n\nstatic ssize_t reclaim_account_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", !!(s->flags & SLAB_RECLAIM_ACCOUNT));\n}\nSLAB_ATTR_RO(reclaim_account);\n\nstatic ssize_t hwcache_align_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", !!(s->flags & SLAB_HWCACHE_ALIGN));\n}\nSLAB_ATTR_RO(hwcache_align);\n\n#ifdef CONFIG_ZONE_DMA\nstatic ssize_t cache_dma_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", !!(s->flags & SLAB_CACHE_DMA));\n}\nSLAB_ATTR_RO(cache_dma);\n#endif\n\nstatic ssize_t usersize_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", s->usersize);\n}\nSLAB_ATTR_RO(usersize);\n\nstatic ssize_t destroy_by_rcu_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", !!(s->flags & SLAB_TYPESAFE_BY_RCU));\n}\nSLAB_ATTR_RO(destroy_by_rcu);\n\n#ifdef CONFIG_SLUB_DEBUG\nstatic ssize_t slabs_show(struct kmem_cache *s, char *buf)\n{\n\treturn show_slab_objects(s, buf, SO_ALL);\n}\nSLAB_ATTR_RO(slabs);\n\nstatic ssize_t total_objects_show(struct kmem_cache *s, char *buf)\n{\n\treturn show_slab_objects(s, buf, SO_ALL|SO_TOTAL);\n}\nSLAB_ATTR_RO(total_objects);\n\nstatic ssize_t sanity_checks_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", !!(s->flags & SLAB_CONSISTENCY_CHECKS));\n}\nSLAB_ATTR_RO(sanity_checks);\n\nstatic ssize_t trace_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", !!(s->flags & SLAB_TRACE));\n}\nSLAB_ATTR_RO(trace);\n\nstatic ssize_t red_zone_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", !!(s->flags & SLAB_RED_ZONE));\n}\n\nSLAB_ATTR_RO(red_zone);\n\nstatic ssize_t poison_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", !!(s->flags & SLAB_POISON));\n}\n\nSLAB_ATTR_RO(poison);\n\nstatic ssize_t store_user_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", !!(s->flags & SLAB_STORE_USER));\n}\n\nSLAB_ATTR_RO(store_user);\n\nstatic ssize_t validate_show(struct kmem_cache *s, char *buf)\n{\n\treturn 0;\n}\n\nstatic ssize_t validate_store(struct kmem_cache *s,\n\t\t\tconst char *buf, size_t length)\n{\n\tint ret = -EINVAL;\n\n\tif (buf[0] == '1') {\n\t\tret = validate_slab_cache(s);\n\t\tif (ret >= 0)\n\t\t\tret = length;\n\t}\n\treturn ret;\n}\nSLAB_ATTR(validate);\n\nstatic ssize_t alloc_calls_show(struct kmem_cache *s, char *buf)\n{\n\tif (!(s->flags & SLAB_STORE_USER))\n\t\treturn -ENOSYS;\n\treturn list_locations(s, buf, TRACK_ALLOC);\n}\nSLAB_ATTR_RO(alloc_calls);\n\nstatic ssize_t free_calls_show(struct kmem_cache *s, char *buf)\n{\n\tif (!(s->flags & SLAB_STORE_USER))\n\t\treturn -ENOSYS;\n\treturn list_locations(s, buf, TRACK_FREE);\n}\nSLAB_ATTR_RO(free_calls);\n#endif /* CONFIG_SLUB_DEBUG */\n\n#ifdef CONFIG_FAILSLAB\nstatic ssize_t failslab_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", !!(s->flags & SLAB_FAILSLAB));\n}\nSLAB_ATTR_RO(failslab);\n#endif\n\nstatic ssize_t shrink_show(struct kmem_cache *s, char *buf)\n{\n\treturn 0;\n}\n\nstatic ssize_t shrink_store(struct kmem_cache *s,\n\t\t\tconst char *buf, size_t length)\n{\n\tif (buf[0] == '1')\n\t\tkmem_cache_shrink(s);\n\telse\n\t\treturn -EINVAL;\n\treturn length;\n}\nSLAB_ATTR(shrink);\n\n#ifdef CONFIG_NUMA\nstatic ssize_t remote_node_defrag_ratio_show(struct kmem_cache *s, char *buf)\n{\n\treturn sysfs_emit(buf, \"%u\\n\", s->remote_node_defrag_ratio / 10);\n}\n\nstatic ssize_t remote_node_defrag_ratio_store(struct kmem_cache *s,\n\t\t\t\tconst char *buf, size_t length)\n{\n\tunsigned int ratio;\n\tint err;\n\n\terr = kstrtouint(buf, 10, &ratio);\n\tif (err)\n\t\treturn err;\n\tif (ratio > 100)\n\t\treturn -ERANGE;\n\n\ts->remote_node_defrag_ratio = ratio * 10;\n\n\treturn length;\n}\nSLAB_ATTR(remote_node_defrag_ratio);\n#endif\n\n#ifdef CONFIG_SLUB_STATS\nstatic int show_stat(struct kmem_cache *s, char *buf, enum stat_item si)\n{\n\tunsigned long sum  = 0;\n\tint cpu;\n\tint len = 0;\n\tint *data = kmalloc_array(nr_cpu_ids, sizeof(int), GFP_KERNEL);\n\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tfor_each_online_cpu(cpu) {\n\t\tunsigned x = per_cpu_ptr(s->cpu_slab, cpu)->stat[si];\n\n\t\tdata[cpu] = x;\n\t\tsum += x;\n\t}\n\n\tlen += sysfs_emit_at(buf, len, \"%lu\", sum);\n\n#ifdef CONFIG_SMP\n\tfor_each_online_cpu(cpu) {\n\t\tif (data[cpu])\n\t\t\tlen += sysfs_emit_at(buf, len, \" C%d=%u\",\n\t\t\t\t\t     cpu, data[cpu]);\n\t}\n#endif\n\tkfree(data);\n\tlen += sysfs_emit_at(buf, len, \"\\n\");\n\n\treturn len;\n}\n\nstatic void clear_stat(struct kmem_cache *s, enum stat_item si)\n{\n\tint cpu;\n\n\tfor_each_online_cpu(cpu)\n\t\tper_cpu_ptr(s->cpu_slab, cpu)->stat[si] = 0;\n}\n\n#define STAT_ATTR(si, text) \t\t\t\t\t\\\nstatic ssize_t text##_show(struct kmem_cache *s, char *buf)\t\\\n{\t\t\t\t\t\t\t\t\\\n\treturn show_stat(s, buf, si);\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\\\nstatic ssize_t text##_store(struct kmem_cache *s,\t\t\\\n\t\t\t\tconst char *buf, size_t length)\t\\\n{\t\t\t\t\t\t\t\t\\\n\tif (buf[0] != '0')\t\t\t\t\t\\\n\t\treturn -EINVAL;\t\t\t\t\t\\\n\tclear_stat(s, si);\t\t\t\t\t\\\n\treturn length;\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\\\nSLAB_ATTR(text);\t\t\t\t\t\t\\\n\nSTAT_ATTR(ALLOC_FASTPATH, alloc_fastpath);\nSTAT_ATTR(ALLOC_SLOWPATH, alloc_slowpath);\nSTAT_ATTR(FREE_FASTPATH, free_fastpath);\nSTAT_ATTR(FREE_SLOWPATH, free_slowpath);\nSTAT_ATTR(FREE_FROZEN, free_frozen);\nSTAT_ATTR(FREE_ADD_PARTIAL, free_add_partial);\nSTAT_ATTR(FREE_REMOVE_PARTIAL, free_remove_partial);\nSTAT_ATTR(ALLOC_FROM_PARTIAL, alloc_from_partial);\nSTAT_ATTR(ALLOC_SLAB, alloc_slab);\nSTAT_ATTR(ALLOC_REFILL, alloc_refill);\nSTAT_ATTR(ALLOC_NODE_MISMATCH, alloc_node_mismatch);\nSTAT_ATTR(FREE_SLAB, free_slab);\nSTAT_ATTR(CPUSLAB_FLUSH, cpuslab_flush);\nSTAT_ATTR(DEACTIVATE_FULL, deactivate_full);\nSTAT_ATTR(DEACTIVATE_EMPTY, deactivate_empty);\nSTAT_ATTR(DEACTIVATE_TO_HEAD, deactivate_to_head);\nSTAT_ATTR(DEACTIVATE_TO_TAIL, deactivate_to_tail);\nSTAT_ATTR(DEACTIVATE_REMOTE_FREES, deactivate_remote_frees);\nSTAT_ATTR(DEACTIVATE_BYPASS, deactivate_bypass);\nSTAT_ATTR(ORDER_FALLBACK, order_fallback);\nSTAT_ATTR(CMPXCHG_DOUBLE_CPU_FAIL, cmpxchg_double_cpu_fail);\nSTAT_ATTR(CMPXCHG_DOUBLE_FAIL, cmpxchg_double_fail);\nSTAT_ATTR(CPU_PARTIAL_ALLOC, cpu_partial_alloc);\nSTAT_ATTR(CPU_PARTIAL_FREE, cpu_partial_free);\nSTAT_ATTR(CPU_PARTIAL_NODE, cpu_partial_node);\nSTAT_ATTR(CPU_PARTIAL_DRAIN, cpu_partial_drain);\n#endif\t/* CONFIG_SLUB_STATS */\n\nstatic struct attribute *slab_attrs[] = {\n\t&slab_size_attr.attr,\n\t&object_size_attr.attr,\n\t&objs_per_slab_attr.attr,\n\t&order_attr.attr,\n\t&min_partial_attr.attr,\n\t&cpu_partial_attr.attr,\n\t&objects_attr.attr,\n\t&objects_partial_attr.attr,\n\t&partial_attr.attr,\n\t&cpu_slabs_attr.attr,\n\t&ctor_attr.attr,\n\t&aliases_attr.attr,\n\t&align_attr.attr,\n\t&hwcache_align_attr.attr,\n\t&reclaim_account_attr.attr,\n\t&destroy_by_rcu_attr.attr,\n\t&shrink_attr.attr,\n\t&slabs_cpu_partial_attr.attr,\n#ifdef CONFIG_SLUB_DEBUG\n\t&total_objects_attr.attr,\n\t&slabs_attr.attr,\n\t&sanity_checks_attr.attr,\n\t&trace_attr.attr,\n\t&red_zone_attr.attr,\n\t&poison_attr.attr,\n\t&store_user_attr.attr,\n\t&validate_attr.attr,\n\t&alloc_calls_attr.attr,\n\t&free_calls_attr.attr,\n#endif\n#ifdef CONFIG_ZONE_DMA\n\t&cache_dma_attr.attr,\n#endif\n#ifdef CONFIG_NUMA\n\t&remote_node_defrag_ratio_attr.attr,\n#endif\n#ifdef CONFIG_SLUB_STATS\n\t&alloc_fastpath_attr.attr,\n\t&alloc_slowpath_attr.attr,\n\t&free_fastpath_attr.attr,\n\t&free_slowpath_attr.attr,\n\t&free_frozen_attr.attr,\n\t&free_add_partial_attr.attr,\n\t&free_remove_partial_attr.attr,\n\t&alloc_from_partial_attr.attr,\n\t&alloc_slab_attr.attr,\n\t&alloc_refill_attr.attr,\n\t&alloc_node_mismatch_attr.attr,\n\t&free_slab_attr.attr,\n\t&cpuslab_flush_attr.attr,\n\t&deactivate_full_attr.attr,\n\t&deactivate_empty_attr.attr,\n\t&deactivate_to_head_attr.attr,\n\t&deactivate_to_tail_attr.attr,\n\t&deactivate_remote_frees_attr.attr,\n\t&deactivate_bypass_attr.attr,\n\t&order_fallback_attr.attr,\n\t&cmpxchg_double_fail_attr.attr,\n\t&cmpxchg_double_cpu_fail_attr.attr,\n\t&cpu_partial_alloc_attr.attr,\n\t&cpu_partial_free_attr.attr,\n\t&cpu_partial_node_attr.attr,\n\t&cpu_partial_drain_attr.attr,\n#endif\n#ifdef CONFIG_FAILSLAB\n\t&failslab_attr.attr,\n#endif\n\t&usersize_attr.attr,\n\n\tNULL\n};\n\nstatic const struct attribute_group slab_attr_group = {\n\t.attrs = slab_attrs,\n};\n\nstatic ssize_t slab_attr_show(struct kobject *kobj,\n\t\t\t\tstruct attribute *attr,\n\t\t\t\tchar *buf)\n{\n\tstruct slab_attribute *attribute;\n\tstruct kmem_cache *s;\n\tint err;\n\n\tattribute = to_slab_attr(attr);\n\ts = to_slab(kobj);\n\n\tif (!attribute->show)\n\t\treturn -EIO;\n\n\terr = attribute->show(s, buf);\n\n\treturn err;\n}\n\nstatic ssize_t slab_attr_store(struct kobject *kobj,\n\t\t\t\tstruct attribute *attr,\n\t\t\t\tconst char *buf, size_t len)\n{\n\tstruct slab_attribute *attribute;\n\tstruct kmem_cache *s;\n\tint err;\n\n\tattribute = to_slab_attr(attr);\n\ts = to_slab(kobj);\n\n\tif (!attribute->store)\n\t\treturn -EIO;\n\n\terr = attribute->store(s, buf, len);\n\treturn err;\n}\n\nstatic void kmem_cache_release(struct kobject *k)\n{\n\tslab_kmem_cache_release(to_slab(k));\n}\n\nstatic const struct sysfs_ops slab_sysfs_ops = {\n\t.show = slab_attr_show,\n\t.store = slab_attr_store,\n};\n\nstatic struct kobj_type slab_ktype = {\n\t.sysfs_ops = &slab_sysfs_ops,\n\t.release = kmem_cache_release,\n};\n\nstatic struct kset *slab_kset;\n\nstatic inline struct kset *cache_kset(struct kmem_cache *s)\n{\n\treturn slab_kset;\n}\n\n#define ID_STR_LENGTH 64\n\n/* Create a unique string id for a slab cache:\n *\n * Format\t:[flags-]size\n */\nstatic char *create_unique_id(struct kmem_cache *s)\n{\n\tchar *name = kmalloc(ID_STR_LENGTH, GFP_KERNEL);\n\tchar *p = name;\n\n\tBUG_ON(!name);\n\n\t*p++ = ':';\n\t/*\n\t * First flags affecting slabcache operations. We will only\n\t * get here for aliasable slabs so we do not need to support\n\t * too many flags. The flags here must cover all flags that\n\t * are matched during merging to guarantee that the id is\n\t * unique.\n\t */\n\tif (s->flags & SLAB_CACHE_DMA)\n\t\t*p++ = 'd';\n\tif (s->flags & SLAB_CACHE_DMA32)\n\t\t*p++ = 'D';\n\tif (s->flags & SLAB_RECLAIM_ACCOUNT)\n\t\t*p++ = 'a';\n\tif (s->flags & SLAB_CONSISTENCY_CHECKS)\n\t\t*p++ = 'F';\n\tif (s->flags & SLAB_ACCOUNT)\n\t\t*p++ = 'A';\n\tif (p != name + 1)\n\t\t*p++ = '-';\n\tp += sprintf(p, \"%07u\", s->size);\n\n\tBUG_ON(p > name + ID_STR_LENGTH - 1);\n\treturn name;\n}\n\nstatic int sysfs_slab_add(struct kmem_cache *s)\n{\n\tint err;\n\tconst char *name;\n\tstruct kset *kset = cache_kset(s);\n\tint unmergeable = slab_unmergeable(s);\n\n\tif (!kset) {\n\t\tkobject_init(&s->kobj, &slab_ktype);\n\t\treturn 0;\n\t}\n\n\tif (!unmergeable && disable_higher_order_debug &&\n\t\t\t(slub_debug & DEBUG_METADATA_FLAGS))\n\t\tunmergeable = 1;\n\n\tif (unmergeable) {\n\t\t/*\n\t\t * Slabcache can never be merged so we can use the name proper.\n\t\t * This is typically the case for debug situations. In that\n\t\t * case we can catch duplicate names easily.\n\t\t */\n\t\tsysfs_remove_link(&slab_kset->kobj, s->name);\n\t\tname = s->name;\n\t} else {\n\t\t/*\n\t\t * Create a unique name for the slab as a target\n\t\t * for the symlinks.\n\t\t */\n\t\tname = create_unique_id(s);\n\t}\n\n\ts->kobj.kset = kset;\n\terr = kobject_init_and_add(&s->kobj, &slab_ktype, NULL, \"%s\", name);\n\tif (err)\n\t\tgoto out;\n\n\terr = sysfs_create_group(&s->kobj, &slab_attr_group);\n\tif (err)\n\t\tgoto out_del_kobj;\n\n\tif (!unmergeable) {\n\t\t/* Setup first alias */\n\t\tsysfs_slab_alias(s, s->name);\n\t}\nout:\n\tif (!unmergeable)\n\t\tkfree(name);\n\treturn err;\nout_del_kobj:\n\tkobject_del(&s->kobj);\n\tgoto out;\n}\n\nvoid sysfs_slab_unlink(struct kmem_cache *s)\n{\n\tif (slab_state >= FULL)\n\t\tkobject_del(&s->kobj);\n}\n\nvoid sysfs_slab_release(struct kmem_cache *s)\n{\n\tif (slab_state >= FULL)\n\t\tkobject_put(&s->kobj);\n}\n\n/*\n * Need to buffer aliases during bootup until sysfs becomes\n * available lest we lose that information.\n */\nstruct saved_alias {\n\tstruct kmem_cache *s;\n\tconst char *name;\n\tstruct saved_alias *next;\n};\n\nstatic struct saved_alias *alias_list;\n\nstatic int sysfs_slab_alias(struct kmem_cache *s, const char *name)\n{\n\tstruct saved_alias *al;\n\n\tif (slab_state == FULL) {\n\t\t/*\n\t\t * If we have a leftover link then remove it.\n\t\t */\n\t\tsysfs_remove_link(&slab_kset->kobj, name);\n\t\treturn sysfs_create_link(&slab_kset->kobj, &s->kobj, name);\n\t}\n\n\tal = kmalloc(sizeof(struct saved_alias), GFP_KERNEL);\n\tif (!al)\n\t\treturn -ENOMEM;\n\n\tal->s = s;\n\tal->name = name;\n\tal->next = alias_list;\n\talias_list = al;\n\treturn 0;\n}\n\nstatic int __init slab_sysfs_init(void)\n{\n\tstruct kmem_cache *s;\n\tint err;\n\n\tmutex_lock(&slab_mutex);\n\n\tslab_kset = kset_create_and_add(\"slab\", NULL, kernel_kobj);\n\tif (!slab_kset) {\n\t\tmutex_unlock(&slab_mutex);\n\t\tpr_err(\"Cannot register slab subsystem.\\n\");\n\t\treturn -ENOSYS;\n\t}\n\n\tslab_state = FULL;\n\n\tlist_for_each_entry(s, &slab_caches, list) {\n\t\terr = sysfs_slab_add(s);\n\t\tif (err)\n\t\t\tpr_err(\"SLUB: Unable to add boot slab %s to sysfs\\n\",\n\t\t\t       s->name);\n\t}\n\n\twhile (alias_list) {\n\t\tstruct saved_alias *al = alias_list;\n\n\t\talias_list = alias_list->next;\n\t\terr = sysfs_slab_alias(al->s, al->name);\n\t\tif (err)\n\t\t\tpr_err(\"SLUB: Unable to add boot slab alias %s to sysfs\\n\",\n\t\t\t       al->name);\n\t\tkfree(al);\n\t}\n\n\tmutex_unlock(&slab_mutex);\n\treturn 0;\n}\n\n__initcall(slab_sysfs_init);\n#endif /* CONFIG_SYSFS */\n\n/*\n * The /proc/slabinfo ABI\n */\n#ifdef CONFIG_SLUB_DEBUG\nvoid get_slabinfo(struct kmem_cache *s, struct slabinfo *sinfo)\n{\n\tunsigned long nr_slabs = 0;\n\tunsigned long nr_objs = 0;\n\tunsigned long nr_free = 0;\n\tint node;\n\tstruct kmem_cache_node *n;\n\n\tfor_each_kmem_cache_node(s, node, n) {\n\t\tnr_slabs += node_nr_slabs(n);\n\t\tnr_objs += node_nr_objs(n);\n\t\tnr_free += count_partial(n, count_free);\n\t}\n\n\tsinfo->active_objs = nr_objs - nr_free;\n\tsinfo->num_objs = nr_objs;\n\tsinfo->active_slabs = nr_slabs;\n\tsinfo->num_slabs = nr_slabs;\n\tsinfo->objects_per_slab = oo_objects(s->oo);\n\tsinfo->cache_order = oo_order(s->oo);\n}\n\nvoid slabinfo_show_stats(struct seq_file *m, struct kmem_cache *s)\n{\n}\n\nssize_t slabinfo_write(struct file *file, const char __user *buffer,\n\t\t       size_t count, loff_t *ppos)\n{\n\treturn -EIO;\n}\n#endif /* CONFIG_SLUB_DEBUG */\n"}, "1": {"id": 1, "path": "/src/include/linux/list.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_LIST_H\n#define _LINUX_LIST_H\n\n#include <linux/types.h>\n#include <linux/stddef.h>\n#include <linux/poison.h>\n#include <linux/const.h>\n#include <linux/kernel.h>\n\n/*\n * Circular doubly linked list implementation.\n *\n * Some of the internal functions (\"__xxx\") are useful when\n * manipulating whole lists rather than single entries, as\n * sometimes we already know the next/prev entries and we can\n * generate better code by using them directly rather than\n * using the generic single-entry routines.\n */\n\n#define LIST_HEAD_INIT(name) { &(name), &(name) }\n\n#define LIST_HEAD(name) \\\n\tstruct list_head name = LIST_HEAD_INIT(name)\n\n/**\n * INIT_LIST_HEAD - Initialize a list_head structure\n * @list: list_head structure to be initialized.\n *\n * Initializes the list_head to point to itself.  If it is a list header,\n * the result is an empty list.\n */\nstatic inline void INIT_LIST_HEAD(struct list_head *list)\n{\n\tWRITE_ONCE(list->next, list);\n\tlist->prev = list;\n}\n\n#ifdef CONFIG_DEBUG_LIST\nextern bool __list_add_valid(struct list_head *new,\n\t\t\t      struct list_head *prev,\n\t\t\t      struct list_head *next);\nextern bool __list_del_entry_valid(struct list_head *entry);\n#else\nstatic inline bool __list_add_valid(struct list_head *new,\n\t\t\t\tstruct list_head *prev,\n\t\t\t\tstruct list_head *next)\n{\n\treturn true;\n}\nstatic inline bool __list_del_entry_valid(struct list_head *entry)\n{\n\treturn true;\n}\n#endif\n\n/*\n * Insert a new entry between two known consecutive entries.\n *\n * This is only for internal list manipulation where we know\n * the prev/next entries already!\n */\nstatic inline void __list_add(struct list_head *new,\n\t\t\t      struct list_head *prev,\n\t\t\t      struct list_head *next)\n{\n\tif (!__list_add_valid(new, prev, next))\n\t\treturn;\n\n\tnext->prev = new;\n\tnew->next = next;\n\tnew->prev = prev;\n\tWRITE_ONCE(prev->next, new);\n}\n\n/**\n * list_add - add a new entry\n * @new: new entry to be added\n * @head: list head to add it after\n *\n * Insert a new entry after the specified head.\n * This is good for implementing stacks.\n */\nstatic inline void list_add(struct list_head *new, struct list_head *head)\n{\n\t__list_add(new, head, head->next);\n}\n\n\n/**\n * list_add_tail - add a new entry\n * @new: new entry to be added\n * @head: list head to add it before\n *\n * Insert a new entry before the specified head.\n * This is useful for implementing queues.\n */\nstatic inline void list_add_tail(struct list_head *new, struct list_head *head)\n{\n\t__list_add(new, head->prev, head);\n}\n\n/*\n * Delete a list entry by making the prev/next entries\n * point to each other.\n *\n * This is only for internal list manipulation where we know\n * the prev/next entries already!\n */\nstatic inline void __list_del(struct list_head * prev, struct list_head * next)\n{\n\tnext->prev = prev;\n\tWRITE_ONCE(prev->next, next);\n}\n\n/*\n * Delete a list entry and clear the 'prev' pointer.\n *\n * This is a special-purpose list clearing method used in the networking code\n * for lists allocated as per-cpu, where we don't want to incur the extra\n * WRITE_ONCE() overhead of a regular list_del_init(). The code that uses this\n * needs to check the node 'prev' pointer instead of calling list_empty().\n */\nstatic inline void __list_del_clearprev(struct list_head *entry)\n{\n\t__list_del(entry->prev, entry->next);\n\tentry->prev = NULL;\n}\n\nstatic inline void __list_del_entry(struct list_head *entry)\n{\n\tif (!__list_del_entry_valid(entry))\n\t\treturn;\n\n\t__list_del(entry->prev, entry->next);\n}\n\n/**\n * list_del - deletes entry from list.\n * @entry: the element to delete from the list.\n * Note: list_empty() on entry does not return true after this, the entry is\n * in an undefined state.\n */\nstatic inline void list_del(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tentry->next = LIST_POISON1;\n\tentry->prev = LIST_POISON2;\n}\n\n/**\n * list_replace - replace old entry by new one\n * @old : the element to be replaced\n * @new : the new element to insert\n *\n * If @old was empty, it will be overwritten.\n */\nstatic inline void list_replace(struct list_head *old,\n\t\t\t\tstruct list_head *new)\n{\n\tnew->next = old->next;\n\tnew->next->prev = new;\n\tnew->prev = old->prev;\n\tnew->prev->next = new;\n}\n\n/**\n * list_replace_init - replace old entry by new one and initialize the old one\n * @old : the element to be replaced\n * @new : the new element to insert\n *\n * If @old was empty, it will be overwritten.\n */\nstatic inline void list_replace_init(struct list_head *old,\n\t\t\t\t     struct list_head *new)\n{\n\tlist_replace(old, new);\n\tINIT_LIST_HEAD(old);\n}\n\n/**\n * list_swap - replace entry1 with entry2 and re-add entry1 at entry2's position\n * @entry1: the location to place entry2\n * @entry2: the location to place entry1\n */\nstatic inline void list_swap(struct list_head *entry1,\n\t\t\t     struct list_head *entry2)\n{\n\tstruct list_head *pos = entry2->prev;\n\n\tlist_del(entry2);\n\tlist_replace(entry1, entry2);\n\tif (pos == entry1)\n\t\tpos = entry2;\n\tlist_add(entry1, pos);\n}\n\n/**\n * list_del_init - deletes entry from list and reinitialize it.\n * @entry: the element to delete from the list.\n */\nstatic inline void list_del_init(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tINIT_LIST_HEAD(entry);\n}\n\n/**\n * list_move - delete from one list and add as another's head\n * @list: the entry to move\n * @head: the head that will precede our entry\n */\nstatic inline void list_move(struct list_head *list, struct list_head *head)\n{\n\t__list_del_entry(list);\n\tlist_add(list, head);\n}\n\n/**\n * list_move_tail - delete from one list and add as another's tail\n * @list: the entry to move\n * @head: the head that will follow our entry\n */\nstatic inline void list_move_tail(struct list_head *list,\n\t\t\t\t  struct list_head *head)\n{\n\t__list_del_entry(list);\n\tlist_add_tail(list, head);\n}\n\n/**\n * list_bulk_move_tail - move a subsection of a list to its tail\n * @head: the head that will follow our entry\n * @first: first entry to move\n * @last: last entry to move, can be the same as first\n *\n * Move all entries between @first and including @last before @head.\n * All three entries must belong to the same linked list.\n */\nstatic inline void list_bulk_move_tail(struct list_head *head,\n\t\t\t\t       struct list_head *first,\n\t\t\t\t       struct list_head *last)\n{\n\tfirst->prev->next = last->next;\n\tlast->next->prev = first->prev;\n\n\thead->prev->next = first;\n\tfirst->prev = head->prev;\n\n\tlast->next = head;\n\thead->prev = last;\n}\n\n/**\n * list_is_first -- tests whether @list is the first entry in list @head\n * @list: the entry to test\n * @head: the head of the list\n */\nstatic inline int list_is_first(const struct list_head *list,\n\t\t\t\t\tconst struct list_head *head)\n{\n\treturn list->prev == head;\n}\n\n/**\n * list_is_last - tests whether @list is the last entry in list @head\n * @list: the entry to test\n * @head: the head of the list\n */\nstatic inline int list_is_last(const struct list_head *list,\n\t\t\t\tconst struct list_head *head)\n{\n\treturn list->next == head;\n}\n\n/**\n * list_empty - tests whether a list is empty\n * @head: the list to test.\n */\nstatic inline int list_empty(const struct list_head *head)\n{\n\treturn READ_ONCE(head->next) == head;\n}\n\n/**\n * list_del_init_careful - deletes entry from list and reinitialize it.\n * @entry: the element to delete from the list.\n *\n * This is the same as list_del_init(), except designed to be used\n * together with list_empty_careful() in a way to guarantee ordering\n * of other memory operations.\n *\n * Any memory operations done before a list_del_init_careful() are\n * guaranteed to be visible after a list_empty_careful() test.\n */\nstatic inline void list_del_init_careful(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tentry->prev = entry;\n\tsmp_store_release(&entry->next, entry);\n}\n\n/**\n * list_empty_careful - tests whether a list is empty and not being modified\n * @head: the list to test\n *\n * Description:\n * tests whether a list is empty _and_ checks that no other CPU might be\n * in the process of modifying either member (next or prev)\n *\n * NOTE: using list_empty_careful() without synchronization\n * can only be safe if the only activity that can happen\n * to the list entry is list_del_init(). Eg. it cannot be used\n * if another CPU could re-list_add() it.\n */\nstatic inline int list_empty_careful(const struct list_head *head)\n{\n\tstruct list_head *next = smp_load_acquire(&head->next);\n\treturn (next == head) && (next == head->prev);\n}\n\n/**\n * list_rotate_left - rotate the list to the left\n * @head: the head of the list\n */\nstatic inline void list_rotate_left(struct list_head *head)\n{\n\tstruct list_head *first;\n\n\tif (!list_empty(head)) {\n\t\tfirst = head->next;\n\t\tlist_move_tail(first, head);\n\t}\n}\n\n/**\n * list_rotate_to_front() - Rotate list to specific item.\n * @list: The desired new front of the list.\n * @head: The head of the list.\n *\n * Rotates list so that @list becomes the new front of the list.\n */\nstatic inline void list_rotate_to_front(struct list_head *list,\n\t\t\t\t\tstruct list_head *head)\n{\n\t/*\n\t * Deletes the list head from the list denoted by @head and\n\t * places it as the tail of @list, this effectively rotates the\n\t * list so that @list is at the front.\n\t */\n\tlist_move_tail(head, list);\n}\n\n/**\n * list_is_singular - tests whether a list has just one entry.\n * @head: the list to test.\n */\nstatic inline int list_is_singular(const struct list_head *head)\n{\n\treturn !list_empty(head) && (head->next == head->prev);\n}\n\nstatic inline void __list_cut_position(struct list_head *list,\n\t\tstruct list_head *head, struct list_head *entry)\n{\n\tstruct list_head *new_first = entry->next;\n\tlist->next = head->next;\n\tlist->next->prev = list;\n\tlist->prev = entry;\n\tentry->next = list;\n\thead->next = new_first;\n\tnew_first->prev = head;\n}\n\n/**\n * list_cut_position - cut a list into two\n * @list: a new list to add all removed entries\n * @head: a list with entries\n * @entry: an entry within head, could be the head itself\n *\tand if so we won't cut the list\n *\n * This helper moves the initial part of @head, up to and\n * including @entry, from @head to @list. You should\n * pass on @entry an element you know is on @head. @list\n * should be an empty list or a list you do not care about\n * losing its data.\n *\n */\nstatic inline void list_cut_position(struct list_head *list,\n\t\tstruct list_head *head, struct list_head *entry)\n{\n\tif (list_empty(head))\n\t\treturn;\n\tif (list_is_singular(head) &&\n\t\t(head->next != entry && head != entry))\n\t\treturn;\n\tif (entry == head)\n\t\tINIT_LIST_HEAD(list);\n\telse\n\t\t__list_cut_position(list, head, entry);\n}\n\n/**\n * list_cut_before - cut a list into two, before given entry\n * @list: a new list to add all removed entries\n * @head: a list with entries\n * @entry: an entry within head, could be the head itself\n *\n * This helper moves the initial part of @head, up to but\n * excluding @entry, from @head to @list.  You should pass\n * in @entry an element you know is on @head.  @list should\n * be an empty list or a list you do not care about losing\n * its data.\n * If @entry == @head, all entries on @head are moved to\n * @list.\n */\nstatic inline void list_cut_before(struct list_head *list,\n\t\t\t\t   struct list_head *head,\n\t\t\t\t   struct list_head *entry)\n{\n\tif (head->next == entry) {\n\t\tINIT_LIST_HEAD(list);\n\t\treturn;\n\t}\n\tlist->next = head->next;\n\tlist->next->prev = list;\n\tlist->prev = entry->prev;\n\tlist->prev->next = list;\n\thead->next = entry;\n\tentry->prev = head;\n}\n\nstatic inline void __list_splice(const struct list_head *list,\n\t\t\t\t struct list_head *prev,\n\t\t\t\t struct list_head *next)\n{\n\tstruct list_head *first = list->next;\n\tstruct list_head *last = list->prev;\n\n\tfirst->prev = prev;\n\tprev->next = first;\n\n\tlast->next = next;\n\tnext->prev = last;\n}\n\n/**\n * list_splice - join two lists, this is designed for stacks\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n */\nstatic inline void list_splice(const struct list_head *list,\n\t\t\t\tstruct list_head *head)\n{\n\tif (!list_empty(list))\n\t\t__list_splice(list, head, head->next);\n}\n\n/**\n * list_splice_tail - join two lists, each list being a queue\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n */\nstatic inline void list_splice_tail(struct list_head *list,\n\t\t\t\tstruct list_head *head)\n{\n\tif (!list_empty(list))\n\t\t__list_splice(list, head->prev, head);\n}\n\n/**\n * list_splice_init - join two lists and reinitialise the emptied list.\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n *\n * The list at @list is reinitialised\n */\nstatic inline void list_splice_init(struct list_head *list,\n\t\t\t\t    struct list_head *head)\n{\n\tif (!list_empty(list)) {\n\t\t__list_splice(list, head, head->next);\n\t\tINIT_LIST_HEAD(list);\n\t}\n}\n\n/**\n * list_splice_tail_init - join two lists and reinitialise the emptied list\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n *\n * Each of the lists is a queue.\n * The list at @list is reinitialised\n */\nstatic inline void list_splice_tail_init(struct list_head *list,\n\t\t\t\t\t struct list_head *head)\n{\n\tif (!list_empty(list)) {\n\t\t__list_splice(list, head->prev, head);\n\t\tINIT_LIST_HEAD(list);\n\t}\n}\n\n/**\n * list_entry - get the struct for this entry\n * @ptr:\tthe &struct list_head pointer.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_entry(ptr, type, member) \\\n\tcontainer_of(ptr, type, member)\n\n/**\n * list_first_entry - get the first element from a list\n * @ptr:\tthe list head to take the element from.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n *\n * Note, that list is expected to be not empty.\n */\n#define list_first_entry(ptr, type, member) \\\n\tlist_entry((ptr)->next, type, member)\n\n/**\n * list_last_entry - get the last element from a list\n * @ptr:\tthe list head to take the element from.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n *\n * Note, that list is expected to be not empty.\n */\n#define list_last_entry(ptr, type, member) \\\n\tlist_entry((ptr)->prev, type, member)\n\n/**\n * list_first_entry_or_null - get the first element from a list\n * @ptr:\tthe list head to take the element from.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n *\n * Note that if the list is empty, it returns NULL.\n */\n#define list_first_entry_or_null(ptr, type, member) ({ \\\n\tstruct list_head *head__ = (ptr); \\\n\tstruct list_head *pos__ = READ_ONCE(head__->next); \\\n\tpos__ != head__ ? list_entry(pos__, type, member) : NULL; \\\n})\n\n/**\n * list_next_entry - get the next element in list\n * @pos:\tthe type * to cursor\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_next_entry(pos, member) \\\n\tlist_entry((pos)->member.next, typeof(*(pos)), member)\n\n/**\n * list_prev_entry - get the prev element in list\n * @pos:\tthe type * to cursor\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_prev_entry(pos, member) \\\n\tlist_entry((pos)->member.prev, typeof(*(pos)), member)\n\n/**\n * list_for_each\t-\titerate over a list\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @head:\tthe head for your list.\n */\n#define list_for_each(pos, head) \\\n\tfor (pos = (head)->next; pos != (head); pos = pos->next)\n\n/**\n * list_for_each_continue - continue iteration over a list\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @head:\tthe head for your list.\n *\n * Continue to iterate over a list, continuing after the current position.\n */\n#define list_for_each_continue(pos, head) \\\n\tfor (pos = pos->next; pos != (head); pos = pos->next)\n\n/**\n * list_for_each_prev\t-\titerate over a list backwards\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @head:\tthe head for your list.\n */\n#define list_for_each_prev(pos, head) \\\n\tfor (pos = (head)->prev; pos != (head); pos = pos->prev)\n\n/**\n * list_for_each_safe - iterate over a list safe against removal of list entry\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @n:\t\tanother &struct list_head to use as temporary storage\n * @head:\tthe head for your list.\n */\n#define list_for_each_safe(pos, n, head) \\\n\tfor (pos = (head)->next, n = pos->next; pos != (head); \\\n\t\tpos = n, n = pos->next)\n\n/**\n * list_for_each_prev_safe - iterate over a list backwards safe against removal of list entry\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @n:\t\tanother &struct list_head to use as temporary storage\n * @head:\tthe head for your list.\n */\n#define list_for_each_prev_safe(pos, n, head) \\\n\tfor (pos = (head)->prev, n = pos->prev; \\\n\t     pos != (head); \\\n\t     pos = n, n = pos->prev)\n\n/**\n * list_entry_is_head - test if the entry points to the head of the list\n * @pos:\tthe type * to cursor\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_entry_is_head(pos, head, member)\t\t\t\t\\\n\t(&pos->member == (head))\n\n/**\n * list_for_each_entry\t-\titerate over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_for_each_entry(pos, head, member)\t\t\t\t\\\n\tfor (pos = list_first_entry(head, typeof(*pos), member);\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_next_entry(pos, member))\n\n/**\n * list_for_each_entry_reverse - iterate backwards over list of given type.\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_for_each_entry_reverse(pos, head, member)\t\t\t\\\n\tfor (pos = list_last_entry(head, typeof(*pos), member);\t\t\\\n\t     !list_entry_is_head(pos, head, member); \t\t\t\\\n\t     pos = list_prev_entry(pos, member))\n\n/**\n * list_prepare_entry - prepare a pos entry for use in list_for_each_entry_continue()\n * @pos:\tthe type * to use as a start point\n * @head:\tthe head of the list\n * @member:\tthe name of the list_head within the struct.\n *\n * Prepares a pos entry for use as a start point in list_for_each_entry_continue().\n */\n#define list_prepare_entry(pos, head, member) \\\n\t((pos) ? : list_entry(head, typeof(*pos), member))\n\n/**\n * list_for_each_entry_continue - continue iteration over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Continue to iterate over list of given type, continuing after\n * the current position.\n */\n#define list_for_each_entry_continue(pos, head, member) \t\t\\\n\tfor (pos = list_next_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_next_entry(pos, member))\n\n/**\n * list_for_each_entry_continue_reverse - iterate backwards from the given point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Start to iterate over list of given type backwards, continuing after\n * the current position.\n */\n#define list_for_each_entry_continue_reverse(pos, head, member)\t\t\\\n\tfor (pos = list_prev_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_prev_entry(pos, member))\n\n/**\n * list_for_each_entry_from - iterate over list of given type from the current point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate over list of given type, continuing from current position.\n */\n#define list_for_each_entry_from(pos, head, member) \t\t\t\\\n\tfor (; !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_next_entry(pos, member))\n\n/**\n * list_for_each_entry_from_reverse - iterate backwards over list of given type\n *                                    from the current point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate backwards over list of given type, continuing from current position.\n */\n#define list_for_each_entry_from_reverse(pos, head, member)\t\t\\\n\tfor (; !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_prev_entry(pos, member))\n\n/**\n * list_for_each_entry_safe - iterate over list of given type safe against removal of list entry\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_for_each_entry_safe(pos, n, head, member)\t\t\t\\\n\tfor (pos = list_first_entry(head, typeof(*pos), member),\t\\\n\t\tn = list_next_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member); \t\t\t\\\n\t     pos = n, n = list_next_entry(n, member))\n\n/**\n * list_for_each_entry_safe_continue - continue list iteration safe against removal\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate over list of given type, continuing after current point,\n * safe against removal of list entry.\n */\n#define list_for_each_entry_safe_continue(pos, n, head, member) \t\t\\\n\tfor (pos = list_next_entry(pos, member), \t\t\t\t\\\n\t\tn = list_next_entry(pos, member);\t\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\t\\\n\t     pos = n, n = list_next_entry(n, member))\n\n/**\n * list_for_each_entry_safe_from - iterate over list from current point safe against removal\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate over list of given type from current point, safe against\n * removal of list entry.\n */\n#define list_for_each_entry_safe_from(pos, n, head, member) \t\t\t\\\n\tfor (n = list_next_entry(pos, member);\t\t\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\t\\\n\t     pos = n, n = list_next_entry(n, member))\n\n/**\n * list_for_each_entry_safe_reverse - iterate backwards over list safe against removal\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate backwards over list of given type, safe against removal\n * of list entry.\n */\n#define list_for_each_entry_safe_reverse(pos, n, head, member)\t\t\\\n\tfor (pos = list_last_entry(head, typeof(*pos), member),\t\t\\\n\t\tn = list_prev_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member); \t\t\t\\\n\t     pos = n, n = list_prev_entry(n, member))\n\n/**\n * list_safe_reset_next - reset a stale list_for_each_entry_safe loop\n * @pos:\tthe loop cursor used in the list_for_each_entry_safe loop\n * @n:\t\ttemporary storage used in list_for_each_entry_safe\n * @member:\tthe name of the list_head within the struct.\n *\n * list_safe_reset_next is not safe to use in general if the list may be\n * modified concurrently (eg. the lock is dropped in the loop body). An\n * exception to this is if the cursor element (pos) is pinned in the list,\n * and list_safe_reset_next is called after re-taking the lock and before\n * completing the current iteration of the loop body.\n */\n#define list_safe_reset_next(pos, n, member)\t\t\t\t\\\n\tn = list_next_entry(pos, member)\n\n/*\n * Double linked lists with a single pointer list head.\n * Mostly useful for hash tables where the two pointer list head is\n * too wasteful.\n * You lose the ability to access the tail in O(1).\n */\n\n#define HLIST_HEAD_INIT { .first = NULL }\n#define HLIST_HEAD(name) struct hlist_head name = {  .first = NULL }\n#define INIT_HLIST_HEAD(ptr) ((ptr)->first = NULL)\nstatic inline void INIT_HLIST_NODE(struct hlist_node *h)\n{\n\th->next = NULL;\n\th->pprev = NULL;\n}\n\n/**\n * hlist_unhashed - Has node been removed from list and reinitialized?\n * @h: Node to be checked\n *\n * Not that not all removal functions will leave a node in unhashed\n * state.  For example, hlist_nulls_del_init_rcu() does leave the\n * node in unhashed state, but hlist_nulls_del() does not.\n */\nstatic inline int hlist_unhashed(const struct hlist_node *h)\n{\n\treturn !h->pprev;\n}\n\n/**\n * hlist_unhashed_lockless - Version of hlist_unhashed for lockless use\n * @h: Node to be checked\n *\n * This variant of hlist_unhashed() must be used in lockless contexts\n * to avoid potential load-tearing.  The READ_ONCE() is paired with the\n * various WRITE_ONCE() in hlist helpers that are defined below.\n */\nstatic inline int hlist_unhashed_lockless(const struct hlist_node *h)\n{\n\treturn !READ_ONCE(h->pprev);\n}\n\n/**\n * hlist_empty - Is the specified hlist_head structure an empty hlist?\n * @h: Structure to check.\n */\nstatic inline int hlist_empty(const struct hlist_head *h)\n{\n\treturn !READ_ONCE(h->first);\n}\n\nstatic inline void __hlist_del(struct hlist_node *n)\n{\n\tstruct hlist_node *next = n->next;\n\tstruct hlist_node **pprev = n->pprev;\n\n\tWRITE_ONCE(*pprev, next);\n\tif (next)\n\t\tWRITE_ONCE(next->pprev, pprev);\n}\n\n/**\n * hlist_del - Delete the specified hlist_node from its list\n * @n: Node to delete.\n *\n * Note that this function leaves the node in hashed state.  Use\n * hlist_del_init() or similar instead to unhash @n.\n */\nstatic inline void hlist_del(struct hlist_node *n)\n{\n\t__hlist_del(n);\n\tn->next = LIST_POISON1;\n\tn->pprev = LIST_POISON2;\n}\n\n/**\n * hlist_del_init - Delete the specified hlist_node from its list and initialize\n * @n: Node to delete.\n *\n * Note that this function leaves the node in unhashed state.\n */\nstatic inline void hlist_del_init(struct hlist_node *n)\n{\n\tif (!hlist_unhashed(n)) {\n\t\t__hlist_del(n);\n\t\tINIT_HLIST_NODE(n);\n\t}\n}\n\n/**\n * hlist_add_head - add a new entry at the beginning of the hlist\n * @n: new entry to be added\n * @h: hlist head to add it after\n *\n * Insert a new entry after the specified head.\n * This is good for implementing stacks.\n */\nstatic inline void hlist_add_head(struct hlist_node *n, struct hlist_head *h)\n{\n\tstruct hlist_node *first = h->first;\n\tWRITE_ONCE(n->next, first);\n\tif (first)\n\t\tWRITE_ONCE(first->pprev, &n->next);\n\tWRITE_ONCE(h->first, n);\n\tWRITE_ONCE(n->pprev, &h->first);\n}\n\n/**\n * hlist_add_before - add a new entry before the one specified\n * @n: new entry to be added\n * @next: hlist node to add it before, which must be non-NULL\n */\nstatic inline void hlist_add_before(struct hlist_node *n,\n\t\t\t\t    struct hlist_node *next)\n{\n\tWRITE_ONCE(n->pprev, next->pprev);\n\tWRITE_ONCE(n->next, next);\n\tWRITE_ONCE(next->pprev, &n->next);\n\tWRITE_ONCE(*(n->pprev), n);\n}\n\n/**\n * hlist_add_behind - add a new entry after the one specified\n * @n: new entry to be added\n * @prev: hlist node to add it after, which must be non-NULL\n */\nstatic inline void hlist_add_behind(struct hlist_node *n,\n\t\t\t\t    struct hlist_node *prev)\n{\n\tWRITE_ONCE(n->next, prev->next);\n\tWRITE_ONCE(prev->next, n);\n\tWRITE_ONCE(n->pprev, &prev->next);\n\n\tif (n->next)\n\t\tWRITE_ONCE(n->next->pprev, &n->next);\n}\n\n/**\n * hlist_add_fake - create a fake hlist consisting of a single headless node\n * @n: Node to make a fake list out of\n *\n * This makes @n appear to be its own predecessor on a headless hlist.\n * The point of this is to allow things like hlist_del() to work correctly\n * in cases where there is no list.\n */\nstatic inline void hlist_add_fake(struct hlist_node *n)\n{\n\tn->pprev = &n->next;\n}\n\n/**\n * hlist_fake: Is this node a fake hlist?\n * @h: Node to check for being a self-referential fake hlist.\n */\nstatic inline bool hlist_fake(struct hlist_node *h)\n{\n\treturn h->pprev == &h->next;\n}\n\n/**\n * hlist_is_singular_node - is node the only element of the specified hlist?\n * @n: Node to check for singularity.\n * @h: Header for potentially singular list.\n *\n * Check whether the node is the only node of the head without\n * accessing head, thus avoiding unnecessary cache misses.\n */\nstatic inline bool\nhlist_is_singular_node(struct hlist_node *n, struct hlist_head *h)\n{\n\treturn !n->next && n->pprev == &h->first;\n}\n\n/**\n * hlist_move_list - Move an hlist\n * @old: hlist_head for old list.\n * @new: hlist_head for new list.\n *\n * Move a list from one list head to another. Fixup the pprev\n * reference of the first entry if it exists.\n */\nstatic inline void hlist_move_list(struct hlist_head *old,\n\t\t\t\t   struct hlist_head *new)\n{\n\tnew->first = old->first;\n\tif (new->first)\n\t\tnew->first->pprev = &new->first;\n\told->first = NULL;\n}\n\n#define hlist_entry(ptr, type, member) container_of(ptr,type,member)\n\n#define hlist_for_each(pos, head) \\\n\tfor (pos = (head)->first; pos ; pos = pos->next)\n\n#define hlist_for_each_safe(pos, n, head) \\\n\tfor (pos = (head)->first; pos && ({ n = pos->next; 1; }); \\\n\t     pos = n)\n\n#define hlist_entry_safe(ptr, type, member) \\\n\t({ typeof(ptr) ____ptr = (ptr); \\\n\t   ____ptr ? hlist_entry(____ptr, type, member) : NULL; \\\n\t})\n\n/**\n * hlist_for_each_entry\t- iterate over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry(pos, head, member)\t\t\t\t\\\n\tfor (pos = hlist_entry_safe((head)->first, typeof(*(pos)), member);\\\n\t     pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_continue - iterate over a hlist continuing after current point\n * @pos:\tthe type * to use as a loop cursor.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_continue(pos, member)\t\t\t\\\n\tfor (pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member);\\\n\t     pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_from - iterate over a hlist continuing from current point\n * @pos:\tthe type * to use as a loop cursor.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_from(pos, member)\t\t\t\t\\\n\tfor (; pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_safe - iterate over list of given type safe against removal of list entry\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\ta &struct hlist_node to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_safe(pos, n, head, member) \t\t\\\n\tfor (pos = hlist_entry_safe((head)->first, typeof(*pos), member);\\\n\t     pos && ({ n = pos->member.next; 1; });\t\t\t\\\n\t     pos = hlist_entry_safe(n, typeof(*pos), member))\n\n#endif\n"}, "2": {"id": 2, "path": "/src/include/linux/kernel.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_KERNEL_H\n#define _LINUX_KERNEL_H\n\n#include <stdarg.h>\n#include <linux/align.h>\n#include <linux/limits.h>\n#include <linux/linkage.h>\n#include <linux/stddef.h>\n#include <linux/types.h>\n#include <linux/compiler.h>\n#include <linux/bitops.h>\n#include <linux/log2.h>\n#include <linux/math.h>\n#include <linux/minmax.h>\n#include <linux/typecheck.h>\n#include <linux/printk.h>\n#include <linux/build_bug.h>\n#include <linux/static_call_types.h>\n#include <asm/byteorder.h>\n\n#include <uapi/linux/kernel.h>\n\n#define STACK_MAGIC\t0xdeadbeef\n\n/**\n * REPEAT_BYTE - repeat the value @x multiple times as an unsigned long value\n * @x: value to repeat\n *\n * NOTE: @x is not checked for > 0xff; larger values produce odd results.\n */\n#define REPEAT_BYTE(x)\t((~0ul / 0xff) * (x))\n\n/* generic data direction definitions */\n#define READ\t\t\t0\n#define WRITE\t\t\t1\n\n/**\n * ARRAY_SIZE - get the number of elements in array @arr\n * @arr: array to be sized\n */\n#define ARRAY_SIZE(arr) (sizeof(arr) / sizeof((arr)[0]) + __must_be_array(arr))\n\n#define u64_to_user_ptr(x) (\t\t\\\n{\t\t\t\t\t\\\n\ttypecheck(u64, (x));\t\t\\\n\t(void __user *)(uintptr_t)(x);\t\\\n}\t\t\t\t\t\\\n)\n\n#define typeof_member(T, m)\ttypeof(((T*)0)->m)\n\n#define _RET_IP_\t\t(unsigned long)__builtin_return_address(0)\n#define _THIS_IP_  ({ __label__ __here; __here: (unsigned long)&&__here; })\n\n/**\n * upper_32_bits - return bits 32-63 of a number\n * @n: the number we're accessing\n *\n * A basic shift-right of a 64- or 32-bit quantity.  Use this to suppress\n * the \"right shift count >= width of type\" warning when that quantity is\n * 32-bits.\n */\n#define upper_32_bits(n) ((u32)(((n) >> 16) >> 16))\n\n/**\n * lower_32_bits - return bits 0-31 of a number\n * @n: the number we're accessing\n */\n#define lower_32_bits(n) ((u32)((n) & 0xffffffff))\n\nstruct completion;\nstruct pt_regs;\nstruct user;\n\n#ifdef CONFIG_PREEMPT_VOLUNTARY\n\nextern int __cond_resched(void);\n# define might_resched() __cond_resched()\n\n#elif defined(CONFIG_PREEMPT_DYNAMIC)\n\nextern int __cond_resched(void);\n\nDECLARE_STATIC_CALL(might_resched, __cond_resched);\n\nstatic __always_inline void might_resched(void)\n{\n\tstatic_call_mod(might_resched)();\n}\n\n#else\n\n# define might_resched() do { } while (0)\n\n#endif /* CONFIG_PREEMPT_* */\n\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\nextern void ___might_sleep(const char *file, int line, int preempt_offset);\nextern void __might_sleep(const char *file, int line, int preempt_offset);\nextern void __cant_sleep(const char *file, int line, int preempt_offset);\nextern void __cant_migrate(const char *file, int line);\n\n/**\n * might_sleep - annotation for functions that can sleep\n *\n * this macro will print a stack trace if it is executed in an atomic\n * context (spinlock, irq-handler, ...). Additional sections where blocking is\n * not allowed can be annotated with non_block_start() and non_block_end()\n * pairs.\n *\n * This is a useful debugging help to be able to catch problems early and not\n * be bitten later when the calling function happens to sleep when it is not\n * supposed to.\n */\n# define might_sleep() \\\n\tdo { __might_sleep(__FILE__, __LINE__, 0); might_resched(); } while (0)\n/**\n * cant_sleep - annotation for functions that cannot sleep\n *\n * this macro will print a stack trace if it is executed with preemption enabled\n */\n# define cant_sleep() \\\n\tdo { __cant_sleep(__FILE__, __LINE__, 0); } while (0)\n# define sched_annotate_sleep()\t(current->task_state_change = 0)\n\n/**\n * cant_migrate - annotation for functions that cannot migrate\n *\n * Will print a stack trace if executed in code which is migratable\n */\n# define cant_migrate()\t\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (IS_ENABLED(CONFIG_SMP))\t\t\t\t\\\n\t\t\t__cant_migrate(__FILE__, __LINE__);\t\t\\\n\t} while (0)\n\n/**\n * non_block_start - annotate the start of section where sleeping is prohibited\n *\n * This is on behalf of the oom reaper, specifically when it is calling the mmu\n * notifiers. The problem is that if the notifier were to block on, for example,\n * mutex_lock() and if the process which holds that mutex were to perform a\n * sleeping memory allocation, the oom reaper is now blocked on completion of\n * that memory allocation. Other blocking calls like wait_event() pose similar\n * issues.\n */\n# define non_block_start() (current->non_block_count++)\n/**\n * non_block_end - annotate the end of section where sleeping is prohibited\n *\n * Closes a section opened by non_block_start().\n */\n# define non_block_end() WARN_ON(current->non_block_count-- == 0)\n#else\n  static inline void ___might_sleep(const char *file, int line,\n\t\t\t\t   int preempt_offset) { }\n  static inline void __might_sleep(const char *file, int line,\n\t\t\t\t   int preempt_offset) { }\n# define might_sleep() do { might_resched(); } while (0)\n# define cant_sleep() do { } while (0)\n# define cant_migrate()\t\tdo { } while (0)\n# define sched_annotate_sleep() do { } while (0)\n# define non_block_start() do { } while (0)\n# define non_block_end() do { } while (0)\n#endif\n\n#define might_sleep_if(cond) do { if (cond) might_sleep(); } while (0)\n\n#if defined(CONFIG_MMU) && \\\n\t(defined(CONFIG_PROVE_LOCKING) || defined(CONFIG_DEBUG_ATOMIC_SLEEP))\n#define might_fault() __might_fault(__FILE__, __LINE__)\nvoid __might_fault(const char *file, int line);\n#else\nstatic inline void might_fault(void) { }\n#endif\n\nextern struct atomic_notifier_head panic_notifier_list;\nextern long (*panic_blink)(int state);\n__printf(1, 2)\nvoid panic(const char *fmt, ...) __noreturn __cold;\nvoid nmi_panic(struct pt_regs *regs, const char *msg);\nextern void oops_enter(void);\nextern void oops_exit(void);\nextern bool oops_may_print(void);\nvoid do_exit(long error_code) __noreturn;\nvoid complete_and_exit(struct completion *, long) __noreturn;\n\n/* Internal, do not use. */\nint __must_check _kstrtoul(const char *s, unsigned int base, unsigned long *res);\nint __must_check _kstrtol(const char *s, unsigned int base, long *res);\n\nint __must_check kstrtoull(const char *s, unsigned int base, unsigned long long *res);\nint __must_check kstrtoll(const char *s, unsigned int base, long long *res);\n\n/**\n * kstrtoul - convert a string to an unsigned long\n * @s: The start of the string. The string must be null-terminated, and may also\n *  include a single newline before its terminating null. The first character\n *  may also be a plus sign, but not a minus sign.\n * @base: The number base to use. The maximum supported base is 16. If base is\n *  given as 0, then the base of the string is automatically detected with the\n *  conventional semantics - If it begins with 0x the number will be parsed as a\n *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be\n *  parsed as an octal number. Otherwise it will be parsed as a decimal.\n * @res: Where to write the result of the conversion on success.\n *\n * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.\n * Preferred over simple_strtoul(). Return code must be checked.\n*/\nstatic inline int __must_check kstrtoul(const char *s, unsigned int base, unsigned long *res)\n{\n\t/*\n\t * We want to shortcut function call, but\n\t * __builtin_types_compatible_p(unsigned long, unsigned long long) = 0.\n\t */\n\tif (sizeof(unsigned long) == sizeof(unsigned long long) &&\n\t    __alignof__(unsigned long) == __alignof__(unsigned long long))\n\t\treturn kstrtoull(s, base, (unsigned long long *)res);\n\telse\n\t\treturn _kstrtoul(s, base, res);\n}\n\n/**\n * kstrtol - convert a string to a long\n * @s: The start of the string. The string must be null-terminated, and may also\n *  include a single newline before its terminating null. The first character\n *  may also be a plus sign or a minus sign.\n * @base: The number base to use. The maximum supported base is 16. If base is\n *  given as 0, then the base of the string is automatically detected with the\n *  conventional semantics - If it begins with 0x the number will be parsed as a\n *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be\n *  parsed as an octal number. Otherwise it will be parsed as a decimal.\n * @res: Where to write the result of the conversion on success.\n *\n * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.\n * Preferred over simple_strtol(). Return code must be checked.\n */\nstatic inline int __must_check kstrtol(const char *s, unsigned int base, long *res)\n{\n\t/*\n\t * We want to shortcut function call, but\n\t * __builtin_types_compatible_p(long, long long) = 0.\n\t */\n\tif (sizeof(long) == sizeof(long long) &&\n\t    __alignof__(long) == __alignof__(long long))\n\t\treturn kstrtoll(s, base, (long long *)res);\n\telse\n\t\treturn _kstrtol(s, base, res);\n}\n\nint __must_check kstrtouint(const char *s, unsigned int base, unsigned int *res);\nint __must_check kstrtoint(const char *s, unsigned int base, int *res);\n\nstatic inline int __must_check kstrtou64(const char *s, unsigned int base, u64 *res)\n{\n\treturn kstrtoull(s, base, res);\n}\n\nstatic inline int __must_check kstrtos64(const char *s, unsigned int base, s64 *res)\n{\n\treturn kstrtoll(s, base, res);\n}\n\nstatic inline int __must_check kstrtou32(const char *s, unsigned int base, u32 *res)\n{\n\treturn kstrtouint(s, base, res);\n}\n\nstatic inline int __must_check kstrtos32(const char *s, unsigned int base, s32 *res)\n{\n\treturn kstrtoint(s, base, res);\n}\n\nint __must_check kstrtou16(const char *s, unsigned int base, u16 *res);\nint __must_check kstrtos16(const char *s, unsigned int base, s16 *res);\nint __must_check kstrtou8(const char *s, unsigned int base, u8 *res);\nint __must_check kstrtos8(const char *s, unsigned int base, s8 *res);\nint __must_check kstrtobool(const char *s, bool *res);\n\nint __must_check kstrtoull_from_user(const char __user *s, size_t count, unsigned int base, unsigned long long *res);\nint __must_check kstrtoll_from_user(const char __user *s, size_t count, unsigned int base, long long *res);\nint __must_check kstrtoul_from_user(const char __user *s, size_t count, unsigned int base, unsigned long *res);\nint __must_check kstrtol_from_user(const char __user *s, size_t count, unsigned int base, long *res);\nint __must_check kstrtouint_from_user(const char __user *s, size_t count, unsigned int base, unsigned int *res);\nint __must_check kstrtoint_from_user(const char __user *s, size_t count, unsigned int base, int *res);\nint __must_check kstrtou16_from_user(const char __user *s, size_t count, unsigned int base, u16 *res);\nint __must_check kstrtos16_from_user(const char __user *s, size_t count, unsigned int base, s16 *res);\nint __must_check kstrtou8_from_user(const char __user *s, size_t count, unsigned int base, u8 *res);\nint __must_check kstrtos8_from_user(const char __user *s, size_t count, unsigned int base, s8 *res);\nint __must_check kstrtobool_from_user(const char __user *s, size_t count, bool *res);\n\nstatic inline int __must_check kstrtou64_from_user(const char __user *s, size_t count, unsigned int base, u64 *res)\n{\n\treturn kstrtoull_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtos64_from_user(const char __user *s, size_t count, unsigned int base, s64 *res)\n{\n\treturn kstrtoll_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtou32_from_user(const char __user *s, size_t count, unsigned int base, u32 *res)\n{\n\treturn kstrtouint_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtos32_from_user(const char __user *s, size_t count, unsigned int base, s32 *res)\n{\n\treturn kstrtoint_from_user(s, count, base, res);\n}\n\n/*\n * Use kstrto<foo> instead.\n *\n * NOTE: simple_strto<foo> does not check for the range overflow and,\n *\t depending on the input, may give interesting results.\n *\n * Use these functions if and only if you cannot use kstrto<foo>, because\n * the conversion ends on the first non-digit character, which may be far\n * beyond the supported range. It might be useful to parse the strings like\n * 10x50 or 12:21 without altering original string or temporary buffer in use.\n * Keep in mind above caveat.\n */\n\nextern unsigned long simple_strtoul(const char *,char **,unsigned int);\nextern long simple_strtol(const char *,char **,unsigned int);\nextern unsigned long long simple_strtoull(const char *,char **,unsigned int);\nextern long long simple_strtoll(const char *,char **,unsigned int);\n\nextern int num_to_str(char *buf, int size,\n\t\t      unsigned long long num, unsigned int width);\n\n/* lib/printf utilities */\n\nextern __printf(2, 3) int sprintf(char *buf, const char * fmt, ...);\nextern __printf(2, 0) int vsprintf(char *buf, const char *, va_list);\nextern __printf(3, 4)\nint snprintf(char *buf, size_t size, const char *fmt, ...);\nextern __printf(3, 0)\nint vsnprintf(char *buf, size_t size, const char *fmt, va_list args);\nextern __printf(3, 4)\nint scnprintf(char *buf, size_t size, const char *fmt, ...);\nextern __printf(3, 0)\nint vscnprintf(char *buf, size_t size, const char *fmt, va_list args);\nextern __printf(2, 3) __malloc\nchar *kasprintf(gfp_t gfp, const char *fmt, ...);\nextern __printf(2, 0) __malloc\nchar *kvasprintf(gfp_t gfp, const char *fmt, va_list args);\nextern __printf(2, 0)\nconst char *kvasprintf_const(gfp_t gfp, const char *fmt, va_list args);\n\nextern __scanf(2, 3)\nint sscanf(const char *, const char *, ...);\nextern __scanf(2, 0)\nint vsscanf(const char *, const char *, va_list);\n\nextern int get_option(char **str, int *pint);\nextern char *get_options(const char *str, int nints, int *ints);\nextern unsigned long long memparse(const char *ptr, char **retptr);\nextern bool parse_option_str(const char *str, const char *option);\nextern char *next_arg(char *args, char **param, char **val);\n\nextern int core_kernel_text(unsigned long addr);\nextern int init_kernel_text(unsigned long addr);\nextern int core_kernel_data(unsigned long addr);\nextern int __kernel_text_address(unsigned long addr);\nextern int kernel_text_address(unsigned long addr);\nextern int func_ptr_is_kernel_text(void *ptr);\n\n#ifdef CONFIG_SMP\nextern unsigned int sysctl_oops_all_cpu_backtrace;\n#else\n#define sysctl_oops_all_cpu_backtrace 0\n#endif /* CONFIG_SMP */\n\nextern void bust_spinlocks(int yes);\nextern int panic_timeout;\nextern unsigned long panic_print;\nextern int panic_on_oops;\nextern int panic_on_unrecovered_nmi;\nextern int panic_on_io_nmi;\nextern int panic_on_warn;\nextern unsigned long panic_on_taint;\nextern bool panic_on_taint_nousertaint;\nextern int sysctl_panic_on_rcu_stall;\nextern int sysctl_max_rcu_stall_to_panic;\nextern int sysctl_panic_on_stackoverflow;\n\nextern bool crash_kexec_post_notifiers;\n\n/*\n * panic_cpu is used for synchronizing panic() and crash_kexec() execution. It\n * holds a CPU number which is executing panic() currently. A value of\n * PANIC_CPU_INVALID means no CPU has entered panic() or crash_kexec().\n */\nextern atomic_t panic_cpu;\n#define PANIC_CPU_INVALID\t-1\n\n/*\n * Only to be used by arch init code. If the user over-wrote the default\n * CONFIG_PANIC_TIMEOUT, honor it.\n */\nstatic inline void set_arch_panic_timeout(int timeout, int arch_default_timeout)\n{\n\tif (panic_timeout == arch_default_timeout)\n\t\tpanic_timeout = timeout;\n}\nextern const char *print_tainted(void);\nenum lockdep_ok {\n\tLOCKDEP_STILL_OK,\n\tLOCKDEP_NOW_UNRELIABLE\n};\nextern void add_taint(unsigned flag, enum lockdep_ok);\nextern int test_taint(unsigned flag);\nextern unsigned long get_taint(void);\nextern int root_mountflags;\n\nextern bool early_boot_irqs_disabled;\n\n/*\n * Values used for system_state. Ordering of the states must not be changed\n * as code checks for <, <=, >, >= STATE.\n */\nextern enum system_states {\n\tSYSTEM_BOOTING,\n\tSYSTEM_SCHEDULING,\n\tSYSTEM_RUNNING,\n\tSYSTEM_HALT,\n\tSYSTEM_POWER_OFF,\n\tSYSTEM_RESTART,\n\tSYSTEM_SUSPEND,\n} system_state;\n\n/* This cannot be an enum because some may be used in assembly source. */\n#define TAINT_PROPRIETARY_MODULE\t0\n#define TAINT_FORCED_MODULE\t\t1\n#define TAINT_CPU_OUT_OF_SPEC\t\t2\n#define TAINT_FORCED_RMMOD\t\t3\n#define TAINT_MACHINE_CHECK\t\t4\n#define TAINT_BAD_PAGE\t\t\t5\n#define TAINT_USER\t\t\t6\n#define TAINT_DIE\t\t\t7\n#define TAINT_OVERRIDDEN_ACPI_TABLE\t8\n#define TAINT_WARN\t\t\t9\n#define TAINT_CRAP\t\t\t10\n#define TAINT_FIRMWARE_WORKAROUND\t11\n#define TAINT_OOT_MODULE\t\t12\n#define TAINT_UNSIGNED_MODULE\t\t13\n#define TAINT_SOFTLOCKUP\t\t14\n#define TAINT_LIVEPATCH\t\t\t15\n#define TAINT_AUX\t\t\t16\n#define TAINT_RANDSTRUCT\t\t17\n#define TAINT_FLAGS_COUNT\t\t18\n#define TAINT_FLAGS_MAX\t\t\t((1UL << TAINT_FLAGS_COUNT) - 1)\n\nstruct taint_flag {\n\tchar c_true;\t/* character printed when tainted */\n\tchar c_false;\t/* character printed when not tainted */\n\tbool module;\t/* also show as a per-module taint flag */\n};\n\nextern const struct taint_flag taint_flags[TAINT_FLAGS_COUNT];\n\nextern const char hex_asc[];\n#define hex_asc_lo(x)\thex_asc[((x) & 0x0f)]\n#define hex_asc_hi(x)\thex_asc[((x) & 0xf0) >> 4]\n\nstatic inline char *hex_byte_pack(char *buf, u8 byte)\n{\n\t*buf++ = hex_asc_hi(byte);\n\t*buf++ = hex_asc_lo(byte);\n\treturn buf;\n}\n\nextern const char hex_asc_upper[];\n#define hex_asc_upper_lo(x)\thex_asc_upper[((x) & 0x0f)]\n#define hex_asc_upper_hi(x)\thex_asc_upper[((x) & 0xf0) >> 4]\n\nstatic inline char *hex_byte_pack_upper(char *buf, u8 byte)\n{\n\t*buf++ = hex_asc_upper_hi(byte);\n\t*buf++ = hex_asc_upper_lo(byte);\n\treturn buf;\n}\n\nextern int hex_to_bin(char ch);\nextern int __must_check hex2bin(u8 *dst, const char *src, size_t count);\nextern char *bin2hex(char *dst, const void *src, size_t count);\n\nbool mac_pton(const char *s, u8 *mac);\n\n/*\n * General tracing related utility functions - trace_printk(),\n * tracing_on/tracing_off and tracing_start()/tracing_stop\n *\n * Use tracing_on/tracing_off when you want to quickly turn on or off\n * tracing. It simply enables or disables the recording of the trace events.\n * This also corresponds to the user space /sys/kernel/debug/tracing/tracing_on\n * file, which gives a means for the kernel and userspace to interact.\n * Place a tracing_off() in the kernel where you want tracing to end.\n * From user space, examine the trace, and then echo 1 > tracing_on\n * to continue tracing.\n *\n * tracing_stop/tracing_start has slightly more overhead. It is used\n * by things like suspend to ram where disabling the recording of the\n * trace is not enough, but tracing must actually stop because things\n * like calling smp_processor_id() may crash the system.\n *\n * Most likely, you want to use tracing_on/tracing_off.\n */\n\nenum ftrace_dump_mode {\n\tDUMP_NONE,\n\tDUMP_ALL,\n\tDUMP_ORIG,\n};\n\n#ifdef CONFIG_TRACING\nvoid tracing_on(void);\nvoid tracing_off(void);\nint tracing_is_on(void);\nvoid tracing_snapshot(void);\nvoid tracing_snapshot_alloc(void);\n\nextern void tracing_start(void);\nextern void tracing_stop(void);\n\nstatic inline __printf(1, 2)\nvoid ____trace_printk_check_format(const char *fmt, ...)\n{\n}\n#define __trace_printk_check_format(fmt, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\t\\\n\t\t____trace_printk_check_format(fmt, ##args);\t\t\\\n} while (0)\n\n/**\n * trace_printk - printf formatting in the ftrace buffer\n * @fmt: the printf format for printing\n *\n * Note: __trace_printk is an internal function for trace_printk() and\n *       the @ip is passed in via the trace_printk() macro.\n *\n * This function allows a kernel developer to debug fast path sections\n * that printk is not appropriate for. By scattering in various\n * printk like tracing in the code, a developer can quickly see\n * where problems are occurring.\n *\n * This is intended as a debugging tool for the developer only.\n * Please refrain from leaving trace_printks scattered around in\n * your code. (Extra memory is used for special buffers that are\n * allocated when trace_printk() is used.)\n *\n * A little optimization trick is done here. If there's only one\n * argument, there's no need to scan the string for printf formats.\n * The trace_puts() will suffice. But how can we take advantage of\n * using trace_puts() when trace_printk() has only one argument?\n * By stringifying the args and checking the size we can tell\n * whether or not there are args. __stringify((__VA_ARGS__)) will\n * turn into \"()\\0\" with a size of 3 when there are no args, anything\n * else will be bigger. All we need to do is define a string to this,\n * and then take its size and compare to 3. If it's bigger, use\n * do_trace_printk() otherwise, optimize it to trace_puts(). Then just\n * let gcc optimize the rest.\n */\n\n#define trace_printk(fmt, ...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\\\n\tchar _______STR[] = __stringify((__VA_ARGS__));\t\\\n\tif (sizeof(_______STR) > 3)\t\t\t\\\n\t\tdo_trace_printk(fmt, ##__VA_ARGS__);\t\\\n\telse\t\t\t\t\t\t\\\n\t\ttrace_puts(fmt);\t\t\t\\\n} while (0)\n\n#define do_trace_printk(fmt, args...)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tstatic const char *trace_printk_fmt __used\t\t\t\\\n\t\t__section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t__builtin_constant_p(fmt) ? fmt : NULL;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t__trace_printk_check_format(fmt, ##args);\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(fmt))\t\t\t\t\t\\\n\t\t__trace_bprintk(_THIS_IP_, trace_printk_fmt, ##args);\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t__trace_printk(_THIS_IP_, fmt, ##args);\t\t\t\\\n} while (0)\n\nextern __printf(2, 3)\nint __trace_bprintk(unsigned long ip, const char *fmt, ...);\n\nextern __printf(2, 3)\nint __trace_printk(unsigned long ip, const char *fmt, ...);\n\n/**\n * trace_puts - write a string into the ftrace buffer\n * @str: the string to record\n *\n * Note: __trace_bputs is an internal function for trace_puts and\n *       the @ip is passed in via the trace_puts macro.\n *\n * This is similar to trace_printk() but is made for those really fast\n * paths that a developer wants the least amount of \"Heisenbug\" effects,\n * where the processing of the print format is still too much.\n *\n * This function allows a kernel developer to debug fast path sections\n * that printk is not appropriate for. By scattering in various\n * printk like tracing in the code, a developer can quickly see\n * where problems are occurring.\n *\n * This is intended as a debugging tool for the developer only.\n * Please refrain from leaving trace_puts scattered around in\n * your code. (Extra memory is used for special buffers that are\n * allocated when trace_puts() is used.)\n *\n * Returns: 0 if nothing was written, positive # if string was.\n *  (1 when __trace_bputs is used, strlen(str) when __trace_puts is used)\n */\n\n#define trace_puts(str) ({\t\t\t\t\t\t\\\n\tstatic const char *trace_printk_fmt __used\t\t\t\\\n\t\t__section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t__builtin_constant_p(str) ? str : NULL;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(str))\t\t\t\t\t\\\n\t\t__trace_bputs(_THIS_IP_, trace_printk_fmt);\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t__trace_puts(_THIS_IP_, str, strlen(str));\t\t\\\n})\nextern int __trace_bputs(unsigned long ip, const char *str);\nextern int __trace_puts(unsigned long ip, const char *str, int size);\n\nextern void trace_dump_stack(int skip);\n\n/*\n * The double __builtin_constant_p is because gcc will give us an error\n * if we try to allocate the static variable to fmt if it is not a\n * constant. Even with the outer if statement.\n */\n#define ftrace_vprintk(fmt, vargs)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(fmt)) {\t\t\t\t\\\n\t\tstatic const char *trace_printk_fmt __used\t\t\\\n\t\t  __section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t\t__builtin_constant_p(fmt) ? fmt : NULL;\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t__ftrace_vbprintk(_THIS_IP_, trace_printk_fmt, vargs);\t\\\n\t} else\t\t\t\t\t\t\t\t\\\n\t\t__ftrace_vprintk(_THIS_IP_, fmt, vargs);\t\t\\\n} while (0)\n\nextern __printf(2, 0) int\n__ftrace_vbprintk(unsigned long ip, const char *fmt, va_list ap);\n\nextern __printf(2, 0) int\n__ftrace_vprintk(unsigned long ip, const char *fmt, va_list ap);\n\nextern void ftrace_dump(enum ftrace_dump_mode oops_dump_mode);\n#else\nstatic inline void tracing_start(void) { }\nstatic inline void tracing_stop(void) { }\nstatic inline void trace_dump_stack(int skip) { }\n\nstatic inline void tracing_on(void) { }\nstatic inline void tracing_off(void) { }\nstatic inline int tracing_is_on(void) { return 0; }\nstatic inline void tracing_snapshot(void) { }\nstatic inline void tracing_snapshot_alloc(void) { }\n\nstatic inline __printf(1, 2)\nint trace_printk(const char *fmt, ...)\n{\n\treturn 0;\n}\nstatic __printf(1, 0) inline int\nftrace_vprintk(const char *fmt, va_list ap)\n{\n\treturn 0;\n}\nstatic inline void ftrace_dump(enum ftrace_dump_mode oops_dump_mode) { }\n#endif /* CONFIG_TRACING */\n\n/* This counts to 12. Any more, it will return 13th argument. */\n#define __COUNT_ARGS(_0, _1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _n, X...) _n\n#define COUNT_ARGS(X...) __COUNT_ARGS(, ##X, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0)\n\n#define __CONCAT(a, b) a ## b\n#define CONCATENATE(a, b) __CONCAT(a, b)\n\n/**\n * container_of - cast a member of a structure out to the containing structure\n * @ptr:\tthe pointer to the member.\n * @type:\tthe type of the container struct this is embedded in.\n * @member:\tthe name of the member within the struct.\n *\n */\n#define container_of(ptr, type, member) ({\t\t\t\t\\\n\tvoid *__mptr = (void *)(ptr);\t\t\t\t\t\\\n\tBUILD_BUG_ON_MSG(!__same_type(*(ptr), ((type *)0)->member) &&\t\\\n\t\t\t !__same_type(*(ptr), void),\t\t\t\\\n\t\t\t \"pointer type mismatch in container_of()\");\t\\\n\t((type *)(__mptr - offsetof(type, member))); })\n\n/**\n * container_of_safe - cast a member of a structure out to the containing structure\n * @ptr:\tthe pointer to the member.\n * @type:\tthe type of the container struct this is embedded in.\n * @member:\tthe name of the member within the struct.\n *\n * If IS_ERR_OR_NULL(ptr), ptr is returned unchanged.\n */\n#define container_of_safe(ptr, type, member) ({\t\t\t\t\\\n\tvoid *__mptr = (void *)(ptr);\t\t\t\t\t\\\n\tBUILD_BUG_ON_MSG(!__same_type(*(ptr), ((type *)0)->member) &&\t\\\n\t\t\t !__same_type(*(ptr), void),\t\t\t\\\n\t\t\t \"pointer type mismatch in container_of()\");\t\\\n\tIS_ERR_OR_NULL(__mptr) ? ERR_CAST(__mptr) :\t\t\t\\\n\t\t((type *)(__mptr - offsetof(type, member))); })\n\n/* Rebuild everything on CONFIG_FTRACE_MCOUNT_RECORD */\n#ifdef CONFIG_FTRACE_MCOUNT_RECORD\n# define REBUILD_DUE_TO_FTRACE_MCOUNT_RECORD\n#endif\n\n/* Permissions on a sysfs file: you didn't miss the 0 prefix did you? */\n#define VERIFY_OCTAL_PERMISSIONS(perms)\t\t\t\t\t\t\\\n\t(BUILD_BUG_ON_ZERO((perms) < 0) +\t\t\t\t\t\\\n\t BUILD_BUG_ON_ZERO((perms) > 0777) +\t\t\t\t\t\\\n\t /* USER_READABLE >= GROUP_READABLE >= OTHER_READABLE */\t\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 6) & 4) < (((perms) >> 3) & 4)) +\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 3) & 4) < ((perms) & 4)) +\t\t\\\n\t /* USER_WRITABLE >= GROUP_WRITABLE */\t\t\t\t\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 6) & 2) < (((perms) >> 3) & 2)) +\t\\\n\t /* OTHER_WRITABLE?  Generally considered a bad idea. */\t\t\\\n\t BUILD_BUG_ON_ZERO((perms) & 2) +\t\t\t\t\t\\\n\t (perms))\n#endif\n"}, "3": {"id": 3, "path": "/src/mm/slab.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef MM_SLAB_H\n#define MM_SLAB_H\n/*\n * Internal slab definitions\n */\n\n#ifdef CONFIG_SLOB\n/*\n * Common fields provided in kmem_cache by all slab allocators\n * This struct is either used directly by the allocator (SLOB)\n * or the allocator must include definitions for all fields\n * provided in kmem_cache_common in their definition of kmem_cache.\n *\n * Once we can do anonymous structs (C11 standard) we could put a\n * anonymous struct definition in these allocators so that the\n * separate allocations in the kmem_cache structure of SLAB and\n * SLUB is no longer needed.\n */\nstruct kmem_cache {\n\tunsigned int object_size;/* The original size of the object */\n\tunsigned int size;\t/* The aligned/padded/added on size  */\n\tunsigned int align;\t/* Alignment as calculated */\n\tslab_flags_t flags;\t/* Active flags on the slab */\n\tunsigned int useroffset;/* Usercopy region offset */\n\tunsigned int usersize;\t/* Usercopy region size */\n\tconst char *name;\t/* Slab name for sysfs */\n\tint refcount;\t\t/* Use counter */\n\tvoid (*ctor)(void *);\t/* Called on object slot creation */\n\tstruct list_head list;\t/* List of all slab caches on the system */\n};\n\n#endif /* CONFIG_SLOB */\n\n#ifdef CONFIG_SLAB\n#include <linux/slab_def.h>\n#endif\n\n#ifdef CONFIG_SLUB\n#include <linux/slub_def.h>\n#endif\n\n#include <linux/memcontrol.h>\n#include <linux/fault-inject.h>\n#include <linux/kasan.h>\n#include <linux/kmemleak.h>\n#include <linux/random.h>\n#include <linux/sched/mm.h>\n\n/*\n * State of the slab allocator.\n *\n * This is used to describe the states of the allocator during bootup.\n * Allocators use this to gradually bootstrap themselves. Most allocators\n * have the problem that the structures used for managing slab caches are\n * allocated from slab caches themselves.\n */\nenum slab_state {\n\tDOWN,\t\t\t/* No slab functionality yet */\n\tPARTIAL,\t\t/* SLUB: kmem_cache_node available */\n\tPARTIAL_NODE,\t\t/* SLAB: kmalloc size for node struct available */\n\tUP,\t\t\t/* Slab caches usable but not all extras yet */\n\tFULL\t\t\t/* Everything is working */\n};\n\nextern enum slab_state slab_state;\n\n/* The slab cache mutex protects the management structures during changes */\nextern struct mutex slab_mutex;\n\n/* The list of all slab caches on the system */\nextern struct list_head slab_caches;\n\n/* The slab cache that manages slab cache information */\nextern struct kmem_cache *kmem_cache;\n\n/* A table of kmalloc cache names and sizes */\nextern const struct kmalloc_info_struct {\n\tconst char *name[NR_KMALLOC_TYPES];\n\tunsigned int size;\n} kmalloc_info[];\n\n#ifndef CONFIG_SLOB\n/* Kmalloc array related functions */\nvoid setup_kmalloc_cache_index_table(void);\nvoid create_kmalloc_caches(slab_flags_t);\n\n/* Find the kmalloc slab corresponding for a certain size */\nstruct kmem_cache *kmalloc_slab(size_t, gfp_t);\n#endif\n\ngfp_t kmalloc_fix_flags(gfp_t flags);\n\n/* Functions provided by the slab allocators */\nint __kmem_cache_create(struct kmem_cache *, slab_flags_t flags);\n\nstruct kmem_cache *create_kmalloc_cache(const char *name, unsigned int size,\n\t\t\tslab_flags_t flags, unsigned int useroffset,\n\t\t\tunsigned int usersize);\nextern void create_boot_cache(struct kmem_cache *, const char *name,\n\t\t\tunsigned int size, slab_flags_t flags,\n\t\t\tunsigned int useroffset, unsigned int usersize);\n\nint slab_unmergeable(struct kmem_cache *s);\nstruct kmem_cache *find_mergeable(unsigned size, unsigned align,\n\t\tslab_flags_t flags, const char *name, void (*ctor)(void *));\n#ifndef CONFIG_SLOB\nstruct kmem_cache *\n__kmem_cache_alias(const char *name, unsigned int size, unsigned int align,\n\t\t   slab_flags_t flags, void (*ctor)(void *));\n\nslab_flags_t kmem_cache_flags(unsigned int object_size,\n\tslab_flags_t flags, const char *name);\n#else\nstatic inline struct kmem_cache *\n__kmem_cache_alias(const char *name, unsigned int size, unsigned int align,\n\t\t   slab_flags_t flags, void (*ctor)(void *))\n{ return NULL; }\n\nstatic inline slab_flags_t kmem_cache_flags(unsigned int object_size,\n\tslab_flags_t flags, const char *name)\n{\n\treturn flags;\n}\n#endif\n\n\n/* Legal flag mask for kmem_cache_create(), for various configurations */\n#define SLAB_CORE_FLAGS (SLAB_HWCACHE_ALIGN | SLAB_CACHE_DMA | \\\n\t\t\t SLAB_CACHE_DMA32 | SLAB_PANIC | \\\n\t\t\t SLAB_TYPESAFE_BY_RCU | SLAB_DEBUG_OBJECTS )\n\n#if defined(CONFIG_DEBUG_SLAB)\n#define SLAB_DEBUG_FLAGS (SLAB_RED_ZONE | SLAB_POISON | SLAB_STORE_USER)\n#elif defined(CONFIG_SLUB_DEBUG)\n#define SLAB_DEBUG_FLAGS (SLAB_RED_ZONE | SLAB_POISON | SLAB_STORE_USER | \\\n\t\t\t  SLAB_TRACE | SLAB_CONSISTENCY_CHECKS | \\\n\t\t\t  SLAB_SILENT_ERRORS)\n#else\n#define SLAB_DEBUG_FLAGS (0)\n#endif\n\n#if defined(CONFIG_SLAB)\n#define SLAB_CACHE_FLAGS (SLAB_MEM_SPREAD | SLAB_NOLEAKTRACE | \\\n\t\t\t  SLAB_RECLAIM_ACCOUNT | SLAB_TEMPORARY | \\\n\t\t\t  SLAB_ACCOUNT)\n#elif defined(CONFIG_SLUB)\n#define SLAB_CACHE_FLAGS (SLAB_NOLEAKTRACE | SLAB_RECLAIM_ACCOUNT | \\\n\t\t\t  SLAB_TEMPORARY | SLAB_ACCOUNT)\n#else\n#define SLAB_CACHE_FLAGS (0)\n#endif\n\n/* Common flags available with current configuration */\n#define CACHE_CREATE_MASK (SLAB_CORE_FLAGS | SLAB_DEBUG_FLAGS | SLAB_CACHE_FLAGS)\n\n/* Common flags permitted for kmem_cache_create */\n#define SLAB_FLAGS_PERMITTED (SLAB_CORE_FLAGS | \\\n\t\t\t      SLAB_RED_ZONE | \\\n\t\t\t      SLAB_POISON | \\\n\t\t\t      SLAB_STORE_USER | \\\n\t\t\t      SLAB_TRACE | \\\n\t\t\t      SLAB_CONSISTENCY_CHECKS | \\\n\t\t\t      SLAB_MEM_SPREAD | \\\n\t\t\t      SLAB_NOLEAKTRACE | \\\n\t\t\t      SLAB_RECLAIM_ACCOUNT | \\\n\t\t\t      SLAB_TEMPORARY | \\\n\t\t\t      SLAB_ACCOUNT | \\\n\t\t\t      SLAB_SILENT_ERRORS)\n\nbool __kmem_cache_empty(struct kmem_cache *);\nint __kmem_cache_shutdown(struct kmem_cache *);\nvoid __kmem_cache_release(struct kmem_cache *);\nint __kmem_cache_shrink(struct kmem_cache *);\nvoid slab_kmem_cache_release(struct kmem_cache *);\n\nstruct seq_file;\nstruct file;\n\nstruct slabinfo {\n\tunsigned long active_objs;\n\tunsigned long num_objs;\n\tunsigned long active_slabs;\n\tunsigned long num_slabs;\n\tunsigned long shared_avail;\n\tunsigned int limit;\n\tunsigned int batchcount;\n\tunsigned int shared;\n\tunsigned int objects_per_slab;\n\tunsigned int cache_order;\n};\n\nvoid get_slabinfo(struct kmem_cache *s, struct slabinfo *sinfo);\nvoid slabinfo_show_stats(struct seq_file *m, struct kmem_cache *s);\nssize_t slabinfo_write(struct file *file, const char __user *buffer,\n\t\t       size_t count, loff_t *ppos);\n\n/*\n * Generic implementation of bulk operations\n * These are useful for situations in which the allocator cannot\n * perform optimizations. In that case segments of the object listed\n * may be allocated or freed using these operations.\n */\nvoid __kmem_cache_free_bulk(struct kmem_cache *, size_t, void **);\nint __kmem_cache_alloc_bulk(struct kmem_cache *, gfp_t, size_t, void **);\n\nstatic inline enum node_stat_item cache_vmstat_idx(struct kmem_cache *s)\n{\n\treturn (s->flags & SLAB_RECLAIM_ACCOUNT) ?\n\t\tNR_SLAB_RECLAIMABLE_B : NR_SLAB_UNRECLAIMABLE_B;\n}\n\n#ifdef CONFIG_SLUB_DEBUG\n#ifdef CONFIG_SLUB_DEBUG_ON\nDECLARE_STATIC_KEY_TRUE(slub_debug_enabled);\n#else\nDECLARE_STATIC_KEY_FALSE(slub_debug_enabled);\n#endif\nextern void print_tracking(struct kmem_cache *s, void *object);\nlong validate_slab_cache(struct kmem_cache *s);\n#else\nstatic inline void print_tracking(struct kmem_cache *s, void *object)\n{\n}\n#endif\n\n/*\n * Returns true if any of the specified slub_debug flags is enabled for the\n * cache. Use only for flags parsed by setup_slub_debug() as it also enables\n * the static key.\n */\nstatic inline bool kmem_cache_debug_flags(struct kmem_cache *s, slab_flags_t flags)\n{\n#ifdef CONFIG_SLUB_DEBUG\n\tVM_WARN_ON_ONCE(!(flags & SLAB_DEBUG_FLAGS));\n\tif (static_branch_unlikely(&slub_debug_enabled))\n\t\treturn s->flags & flags;\n#endif\n\treturn false;\n}\n\n#ifdef CONFIG_MEMCG_KMEM\nint memcg_alloc_page_obj_cgroups(struct page *page, struct kmem_cache *s,\n\t\t\t\t gfp_t gfp, bool new_page);\n\nstatic inline void memcg_free_page_obj_cgroups(struct page *page)\n{\n\tkfree(page_objcgs(page));\n\tpage->memcg_data = 0;\n}\n\nstatic inline size_t obj_full_size(struct kmem_cache *s)\n{\n\t/*\n\t * For each accounted object there is an extra space which is used\n\t * to store obj_cgroup membership. Charge it too.\n\t */\n\treturn s->size + sizeof(struct obj_cgroup *);\n}\n\n/*\n * Returns false if the allocation should fail.\n */\nstatic inline bool memcg_slab_pre_alloc_hook(struct kmem_cache *s,\n\t\t\t\t\t     struct obj_cgroup **objcgp,\n\t\t\t\t\t     size_t objects, gfp_t flags)\n{\n\tstruct obj_cgroup *objcg;\n\n\tif (!memcg_kmem_enabled())\n\t\treturn true;\n\n\tif (!(flags & __GFP_ACCOUNT) && !(s->flags & SLAB_ACCOUNT))\n\t\treturn true;\n\n\tobjcg = get_obj_cgroup_from_current();\n\tif (!objcg)\n\t\treturn true;\n\n\tif (obj_cgroup_charge(objcg, flags, objects * obj_full_size(s))) {\n\t\tobj_cgroup_put(objcg);\n\t\treturn false;\n\t}\n\n\t*objcgp = objcg;\n\treturn true;\n}\n\nstatic inline void mod_objcg_state(struct obj_cgroup *objcg,\n\t\t\t\t   struct pglist_data *pgdat,\n\t\t\t\t   enum node_stat_item idx, int nr)\n{\n\tstruct mem_cgroup *memcg;\n\tstruct lruvec *lruvec;\n\n\trcu_read_lock();\n\tmemcg = obj_cgroup_memcg(objcg);\n\tlruvec = mem_cgroup_lruvec(memcg, pgdat);\n\tmod_memcg_lruvec_state(lruvec, idx, nr);\n\trcu_read_unlock();\n}\n\nstatic inline void memcg_slab_post_alloc_hook(struct kmem_cache *s,\n\t\t\t\t\t      struct obj_cgroup *objcg,\n\t\t\t\t\t      gfp_t flags, size_t size,\n\t\t\t\t\t      void **p)\n{\n\tstruct page *page;\n\tunsigned long off;\n\tsize_t i;\n\n\tif (!memcg_kmem_enabled() || !objcg)\n\t\treturn;\n\n\tflags &= ~__GFP_ACCOUNT;\n\tfor (i = 0; i < size; i++) {\n\t\tif (likely(p[i])) {\n\t\t\tpage = virt_to_head_page(p[i]);\n\n\t\t\tif (!page_objcgs(page) &&\n\t\t\t    memcg_alloc_page_obj_cgroups(page, s, flags,\n\t\t\t\t\t\t\t false)) {\n\t\t\t\tobj_cgroup_uncharge(objcg, obj_full_size(s));\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\toff = obj_to_index(s, page, p[i]);\n\t\t\tobj_cgroup_get(objcg);\n\t\t\tpage_objcgs(page)[off] = objcg;\n\t\t\tmod_objcg_state(objcg, page_pgdat(page),\n\t\t\t\t\tcache_vmstat_idx(s), obj_full_size(s));\n\t\t} else {\n\t\t\tobj_cgroup_uncharge(objcg, obj_full_size(s));\n\t\t}\n\t}\n\tobj_cgroup_put(objcg);\n}\n\nstatic inline void memcg_slab_free_hook(struct kmem_cache *s_orig,\n\t\t\t\t\tvoid **p, int objects)\n{\n\tstruct kmem_cache *s;\n\tstruct obj_cgroup **objcgs;\n\tstruct obj_cgroup *objcg;\n\tstruct page *page;\n\tunsigned int off;\n\tint i;\n\n\tif (!memcg_kmem_enabled())\n\t\treturn;\n\n\tfor (i = 0; i < objects; i++) {\n\t\tif (unlikely(!p[i]))\n\t\t\tcontinue;\n\n\t\tpage = virt_to_head_page(p[i]);\n\t\tobjcgs = page_objcgs(page);\n\t\tif (!objcgs)\n\t\t\tcontinue;\n\n\t\tif (!s_orig)\n\t\t\ts = page->slab_cache;\n\t\telse\n\t\t\ts = s_orig;\n\n\t\toff = obj_to_index(s, page, p[i]);\n\t\tobjcg = objcgs[off];\n\t\tif (!objcg)\n\t\t\tcontinue;\n\n\t\tobjcgs[off] = NULL;\n\t\tobj_cgroup_uncharge(objcg, obj_full_size(s));\n\t\tmod_objcg_state(objcg, page_pgdat(page), cache_vmstat_idx(s),\n\t\t\t\t-obj_full_size(s));\n\t\tobj_cgroup_put(objcg);\n\t}\n}\n\n#else /* CONFIG_MEMCG_KMEM */\nstatic inline struct mem_cgroup *memcg_from_slab_obj(void *ptr)\n{\n\treturn NULL;\n}\n\nstatic inline int memcg_alloc_page_obj_cgroups(struct page *page,\n\t\t\t\t\t       struct kmem_cache *s, gfp_t gfp,\n\t\t\t\t\t       bool new_page)\n{\n\treturn 0;\n}\n\nstatic inline void memcg_free_page_obj_cgroups(struct page *page)\n{\n}\n\nstatic inline bool memcg_slab_pre_alloc_hook(struct kmem_cache *s,\n\t\t\t\t\t     struct obj_cgroup **objcgp,\n\t\t\t\t\t     size_t objects, gfp_t flags)\n{\n\treturn true;\n}\n\nstatic inline void memcg_slab_post_alloc_hook(struct kmem_cache *s,\n\t\t\t\t\t      struct obj_cgroup *objcg,\n\t\t\t\t\t      gfp_t flags, size_t size,\n\t\t\t\t\t      void **p)\n{\n}\n\nstatic inline void memcg_slab_free_hook(struct kmem_cache *s,\n\t\t\t\t\tvoid **p, int objects)\n{\n}\n#endif /* CONFIG_MEMCG_KMEM */\n\nstatic inline struct kmem_cache *virt_to_cache(const void *obj)\n{\n\tstruct page *page;\n\n\tpage = virt_to_head_page(obj);\n\tif (WARN_ONCE(!PageSlab(page), \"%s: Object is not a Slab page!\\n\",\n\t\t\t\t\t__func__))\n\t\treturn NULL;\n\treturn page->slab_cache;\n}\n\nstatic __always_inline void account_slab_page(struct page *page, int order,\n\t\t\t\t\t      struct kmem_cache *s,\n\t\t\t\t\t      gfp_t gfp)\n{\n\tif (memcg_kmem_enabled() && (s->flags & SLAB_ACCOUNT))\n\t\tmemcg_alloc_page_obj_cgroups(page, s, gfp, true);\n\n\tmod_node_page_state(page_pgdat(page), cache_vmstat_idx(s),\n\t\t\t    PAGE_SIZE << order);\n}\n\nstatic __always_inline void unaccount_slab_page(struct page *page, int order,\n\t\t\t\t\t\tstruct kmem_cache *s)\n{\n\tif (memcg_kmem_enabled())\n\t\tmemcg_free_page_obj_cgroups(page);\n\n\tmod_node_page_state(page_pgdat(page), cache_vmstat_idx(s),\n\t\t\t    -(PAGE_SIZE << order));\n}\n\nstatic inline struct kmem_cache *cache_from_obj(struct kmem_cache *s, void *x)\n{\n\tstruct kmem_cache *cachep;\n\n\tif (!IS_ENABLED(CONFIG_SLAB_FREELIST_HARDENED) &&\n\t    !kmem_cache_debug_flags(s, SLAB_CONSISTENCY_CHECKS))\n\t\treturn s;\n\n\tcachep = virt_to_cache(x);\n\tif (WARN(cachep && cachep != s,\n\t\t  \"%s: Wrong slab cache. %s but object is from %s\\n\",\n\t\t  __func__, s->name, cachep->name))\n\t\tprint_tracking(cachep, x);\n\treturn cachep;\n}\n\nstatic inline size_t slab_ksize(const struct kmem_cache *s)\n{\n#ifndef CONFIG_SLUB\n\treturn s->object_size;\n\n#else /* CONFIG_SLUB */\n# ifdef CONFIG_SLUB_DEBUG\n\t/*\n\t * Debugging requires use of the padding between object\n\t * and whatever may come after it.\n\t */\n\tif (s->flags & (SLAB_RED_ZONE | SLAB_POISON))\n\t\treturn s->object_size;\n# endif\n\tif (s->flags & SLAB_KASAN)\n\t\treturn s->object_size;\n\t/*\n\t * If we have the need to store the freelist pointer\n\t * back there or track user information then we can\n\t * only use the space before that information.\n\t */\n\tif (s->flags & (SLAB_TYPESAFE_BY_RCU | SLAB_STORE_USER))\n\t\treturn s->inuse;\n\t/*\n\t * Else we can use all the padding etc for the allocation\n\t */\n\treturn s->size;\n#endif\n}\n\nstatic inline struct kmem_cache *slab_pre_alloc_hook(struct kmem_cache *s,\n\t\t\t\t\t\t     struct obj_cgroup **objcgp,\n\t\t\t\t\t\t     size_t size, gfp_t flags)\n{\n\tflags &= gfp_allowed_mask;\n\n\tmight_alloc(flags);\n\n\tif (should_failslab(s, flags))\n\t\treturn NULL;\n\n\tif (!memcg_slab_pre_alloc_hook(s, objcgp, size, flags))\n\t\treturn NULL;\n\n\treturn s;\n}\n\nstatic inline void slab_post_alloc_hook(struct kmem_cache *s,\n\t\t\t\t\tstruct obj_cgroup *objcg, gfp_t flags,\n\t\t\t\t\tsize_t size, void **p, bool init)\n{\n\tsize_t i;\n\n\tflags &= gfp_allowed_mask;\n\n\t/*\n\t * As memory initialization might be integrated into KASAN,\n\t * kasan_slab_alloc and initialization memset must be\n\t * kept together to avoid discrepancies in behavior.\n\t *\n\t * As p[i] might get tagged, memset and kmemleak hook come after KASAN.\n\t */\n\tfor (i = 0; i < size; i++) {\n\t\tp[i] = kasan_slab_alloc(s, p[i], flags, init);\n\t\tif (p[i] && init && !kasan_has_integrated_init())\n\t\t\tmemset(p[i], 0, s->object_size);\n\t\tkmemleak_alloc_recursive(p[i], s->object_size, 1,\n\t\t\t\t\t s->flags, flags);\n\t}\n\n\tmemcg_slab_post_alloc_hook(s, objcg, flags, size, p);\n}\n\n#ifndef CONFIG_SLOB\n/*\n * The slab lists for all objects.\n */\nstruct kmem_cache_node {\n\tspinlock_t list_lock;\n\n#ifdef CONFIG_SLAB\n\tstruct list_head slabs_partial;\t/* partial list first, better asm code */\n\tstruct list_head slabs_full;\n\tstruct list_head slabs_free;\n\tunsigned long total_slabs;\t/* length of all slab lists */\n\tunsigned long free_slabs;\t/* length of free slab list only */\n\tunsigned long free_objects;\n\tunsigned int free_limit;\n\tunsigned int colour_next;\t/* Per-node cache coloring */\n\tstruct array_cache *shared;\t/* shared per node */\n\tstruct alien_cache **alien;\t/* on other nodes */\n\tunsigned long next_reap;\t/* updated without locking */\n\tint free_touched;\t\t/* updated without locking */\n#endif\n\n#ifdef CONFIG_SLUB\n\tunsigned long nr_partial;\n\tstruct list_head partial;\n#ifdef CONFIG_SLUB_DEBUG\n\tatomic_long_t nr_slabs;\n\tatomic_long_t total_objects;\n\tstruct list_head full;\n#endif\n#endif\n\n};\n\nstatic inline struct kmem_cache_node *get_node(struct kmem_cache *s, int node)\n{\n\treturn s->node[node];\n}\n\n/*\n * Iterator over all nodes. The body will be executed for each node that has\n * a kmem_cache_node structure allocated (which is true for all online nodes)\n */\n#define for_each_kmem_cache_node(__s, __node, __n) \\\n\tfor (__node = 0; __node < nr_node_ids; __node++) \\\n\t\t if ((__n = get_node(__s, __node)))\n\n#endif\n\nvoid *slab_start(struct seq_file *m, loff_t *pos);\nvoid *slab_next(struct seq_file *m, void *p, loff_t *pos);\nvoid slab_stop(struct seq_file *m, void *p);\nint memcg_slab_show(struct seq_file *m, void *p);\n\n#if defined(CONFIG_SLAB) || defined(CONFIG_SLUB_DEBUG)\nvoid dump_unreclaimable_slab(void);\n#else\nstatic inline void dump_unreclaimable_slab(void)\n{\n}\n#endif\n\nvoid ___cache_free(struct kmem_cache *cache, void *x, unsigned long addr);\n\n#ifdef CONFIG_SLAB_FREELIST_RANDOM\nint cache_random_seq_create(struct kmem_cache *cachep, unsigned int count,\n\t\t\tgfp_t gfp);\nvoid cache_random_seq_destroy(struct kmem_cache *cachep);\n#else\nstatic inline int cache_random_seq_create(struct kmem_cache *cachep,\n\t\t\t\t\tunsigned int count, gfp_t gfp)\n{\n\treturn 0;\n}\nstatic inline void cache_random_seq_destroy(struct kmem_cache *cachep) { }\n#endif /* CONFIG_SLAB_FREELIST_RANDOM */\n\nstatic inline bool slab_want_init_on_alloc(gfp_t flags, struct kmem_cache *c)\n{\n\tif (static_branch_unlikely(&init_on_alloc)) {\n\t\tif (c->ctor)\n\t\t\treturn false;\n\t\tif (c->flags & (SLAB_TYPESAFE_BY_RCU | SLAB_POISON))\n\t\t\treturn flags & __GFP_ZERO;\n\t\treturn true;\n\t}\n\treturn flags & __GFP_ZERO;\n}\n\nstatic inline bool slab_want_init_on_free(struct kmem_cache *c)\n{\n\tif (static_branch_unlikely(&init_on_free))\n\t\treturn !(c->ctor ||\n\t\t\t (c->flags & (SLAB_TYPESAFE_BY_RCU | SLAB_POISON)));\n\treturn false;\n}\n\n#ifdef CONFIG_PRINTK\n#define KS_ADDRS_COUNT 16\nstruct kmem_obj_info {\n\tvoid *kp_ptr;\n\tstruct page *kp_page;\n\tvoid *kp_objp;\n\tunsigned long kp_data_offset;\n\tstruct kmem_cache *kp_slab_cache;\n\tvoid *kp_ret;\n\tvoid *kp_stack[KS_ADDRS_COUNT];\n\tvoid *kp_free_stack[KS_ADDRS_COUNT];\n};\nvoid kmem_obj_info(struct kmem_obj_info *kpp, void *object, struct page *page);\n#endif\n\n#endif /* MM_SLAB_H */\n"}, "4": {"id": 4, "path": "/src/include/linux/irqflags.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n/*\n * include/linux/irqflags.h\n *\n * IRQ flags tracing: follow the state of the hardirq and softirq flags and\n * provide callbacks for transitions between ON and OFF states.\n *\n * This file gets included from lowlevel asm headers too, to provide\n * wrapped versions of the local_irq_*() APIs, based on the\n * raw_local_irq_*() macros from the lowlevel headers.\n */\n#ifndef _LINUX_TRACE_IRQFLAGS_H\n#define _LINUX_TRACE_IRQFLAGS_H\n\n#include <linux/typecheck.h>\n#include <asm/irqflags.h>\n#include <asm/percpu.h>\n\n/* Currently lockdep_softirqs_on/off is used only by lockdep */\n#ifdef CONFIG_PROVE_LOCKING\n  extern void lockdep_softirqs_on(unsigned long ip);\n  extern void lockdep_softirqs_off(unsigned long ip);\n  extern void lockdep_hardirqs_on_prepare(unsigned long ip);\n  extern void lockdep_hardirqs_on(unsigned long ip);\n  extern void lockdep_hardirqs_off(unsigned long ip);\n#else\n  static inline void lockdep_softirqs_on(unsigned long ip) { }\n  static inline void lockdep_softirqs_off(unsigned long ip) { }\n  static inline void lockdep_hardirqs_on_prepare(unsigned long ip) { }\n  static inline void lockdep_hardirqs_on(unsigned long ip) { }\n  static inline void lockdep_hardirqs_off(unsigned long ip) { }\n#endif\n\n#ifdef CONFIG_TRACE_IRQFLAGS\n\n/* Per-task IRQ trace events information. */\nstruct irqtrace_events {\n\tunsigned int\tirq_events;\n\tunsigned long\thardirq_enable_ip;\n\tunsigned long\thardirq_disable_ip;\n\tunsigned int\thardirq_enable_event;\n\tunsigned int\thardirq_disable_event;\n\tunsigned long\tsoftirq_disable_ip;\n\tunsigned long\tsoftirq_enable_ip;\n\tunsigned int\tsoftirq_disable_event;\n\tunsigned int\tsoftirq_enable_event;\n};\n\nDECLARE_PER_CPU(int, hardirqs_enabled);\nDECLARE_PER_CPU(int, hardirq_context);\n\nextern void trace_hardirqs_on_prepare(void);\nextern void trace_hardirqs_off_finish(void);\nextern void trace_hardirqs_on(void);\nextern void trace_hardirqs_off(void);\n\n# define lockdep_hardirq_context()\t(raw_cpu_read(hardirq_context))\n# define lockdep_softirq_context(p)\t((p)->softirq_context)\n# define lockdep_hardirqs_enabled()\t(this_cpu_read(hardirqs_enabled))\n# define lockdep_softirqs_enabled(p)\t((p)->softirqs_enabled)\n# define lockdep_hardirq_enter()\t\t\t\\\ndo {\t\t\t\t\t\t\t\\\n\tif (__this_cpu_inc_return(hardirq_context) == 1)\\\n\t\tcurrent->hardirq_threaded = 0;\t\t\\\n} while (0)\n# define lockdep_hardirq_threaded()\t\t\\\ndo {\t\t\t\t\t\t\\\n\tcurrent->hardirq_threaded = 1;\t\t\\\n} while (0)\n# define lockdep_hardirq_exit()\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\t__this_cpu_dec(hardirq_context);\t\\\n} while (0)\n# define lockdep_softirq_enter()\t\t\\\ndo {\t\t\t\t\t\t\\\n\tcurrent->softirq_context++;\t\t\\\n} while (0)\n# define lockdep_softirq_exit()\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tcurrent->softirq_context--;\t\t\\\n} while (0)\n\n# define lockdep_hrtimer_enter(__hrtimer)\t\t\\\n({\t\t\t\t\t\t\t\\\n\tbool __expires_hardirq = true;\t\t\t\\\n\t\t\t\t\t\t\t\\\n\tif (!__hrtimer->is_hard) {\t\t\t\\\n\t\tcurrent->irq_config = 1;\t\t\\\n\t\t__expires_hardirq = false;\t\t\\\n\t}\t\t\t\t\t\t\\\n\t__expires_hardirq;\t\t\t\t\\\n})\n\n# define lockdep_hrtimer_exit(__expires_hardirq)\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\tif (!__expires_hardirq)\t\t\t\\\n\t\t\tcurrent->irq_config = 0;\t\\\n\t} while (0)\n\n# define lockdep_posixtimer_enter()\t\t\t\t\\\n\t  do {\t\t\t\t\t\t\t\\\n\t\t  current->irq_config = 1;\t\t\t\\\n\t  } while (0)\n\n# define lockdep_posixtimer_exit()\t\t\t\t\\\n\t  do {\t\t\t\t\t\t\t\\\n\t\t  current->irq_config = 0;\t\t\t\\\n\t  } while (0)\n\n# define lockdep_irq_work_enter(_flags)\t\t\t\t\t\\\n\t  do {\t\t\t\t\t\t\t\t\\\n\t\t  if (!((_flags) & IRQ_WORK_HARD_IRQ))\t\t\t\\\n\t\t\tcurrent->irq_config = 1;\t\t\t\\\n\t  } while (0)\n# define lockdep_irq_work_exit(_flags)\t\t\t\t\t\\\n\t  do {\t\t\t\t\t\t\t\t\\\n\t\t  if (!((_flags) & IRQ_WORK_HARD_IRQ))\t\t\t\\\n\t\t\tcurrent->irq_config = 0;\t\t\t\\\n\t  } while (0)\n\n#else\n# define trace_hardirqs_on_prepare()\t\tdo { } while (0)\n# define trace_hardirqs_off_finish()\t\tdo { } while (0)\n# define trace_hardirqs_on()\t\t\tdo { } while (0)\n# define trace_hardirqs_off()\t\t\tdo { } while (0)\n# define lockdep_hardirq_context()\t\t0\n# define lockdep_softirq_context(p)\t\t0\n# define lockdep_hardirqs_enabled()\t\t0\n# define lockdep_softirqs_enabled(p)\t\t0\n# define lockdep_hardirq_enter()\t\tdo { } while (0)\n# define lockdep_hardirq_threaded()\t\tdo { } while (0)\n# define lockdep_hardirq_exit()\t\t\tdo { } while (0)\n# define lockdep_softirq_enter()\t\tdo { } while (0)\n# define lockdep_softirq_exit()\t\t\tdo { } while (0)\n# define lockdep_hrtimer_enter(__hrtimer)\tfalse\n# define lockdep_hrtimer_exit(__context)\tdo { } while (0)\n# define lockdep_posixtimer_enter()\t\tdo { } while (0)\n# define lockdep_posixtimer_exit()\t\tdo { } while (0)\n# define lockdep_irq_work_enter(__work)\t\tdo { } while (0)\n# define lockdep_irq_work_exit(__work)\t\tdo { } while (0)\n#endif\n\n#if defined(CONFIG_IRQSOFF_TRACER) || \\\n\tdefined(CONFIG_PREEMPT_TRACER)\n extern void stop_critical_timings(void);\n extern void start_critical_timings(void);\n#else\n# define stop_critical_timings() do { } while (0)\n# define start_critical_timings() do { } while (0)\n#endif\n\n#ifdef CONFIG_DEBUG_IRQFLAGS\nextern void warn_bogus_irq_restore(void);\n#define raw_check_bogus_irq_restore()\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\tif (unlikely(!arch_irqs_disabled()))\t\\\n\t\t\twarn_bogus_irq_restore();\t\\\n\t} while (0)\n#else\n#define raw_check_bogus_irq_restore() do { } while (0)\n#endif\n\n/*\n * Wrap the arch provided IRQ routines to provide appropriate checks.\n */\n#define raw_local_irq_disable()\t\tarch_local_irq_disable()\n#define raw_local_irq_enable()\t\tarch_local_irq_enable()\n#define raw_local_irq_save(flags)\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\\\n\t\tflags = arch_local_irq_save();\t\t\\\n\t} while (0)\n#define raw_local_irq_restore(flags)\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\\\n\t\traw_check_bogus_irq_restore();\t\t\\\n\t\tarch_local_irq_restore(flags);\t\t\\\n\t} while (0)\n#define raw_local_save_flags(flags)\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\\\n\t\tflags = arch_local_save_flags();\t\\\n\t} while (0)\n#define raw_irqs_disabled_flags(flags)\t\t\t\\\n\t({\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\\\n\t\tarch_irqs_disabled_flags(flags);\t\\\n\t})\n#define raw_irqs_disabled()\t\t(arch_irqs_disabled())\n#define raw_safe_halt()\t\t\tarch_safe_halt()\n\n/*\n * The local_irq_*() APIs are equal to the raw_local_irq*()\n * if !TRACE_IRQFLAGS.\n */\n#ifdef CONFIG_TRACE_IRQFLAGS\n\n#define local_irq_enable()\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\ttrace_hardirqs_on();\t\t\t\\\n\t\traw_local_irq_enable();\t\t\t\\\n\t} while (0)\n\n#define local_irq_disable()\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\tbool was_disabled = raw_irqs_disabled();\\\n\t\traw_local_irq_disable();\t\t\\\n\t\tif (!was_disabled)\t\t\t\\\n\t\t\ttrace_hardirqs_off();\t\t\\\n\t} while (0)\n\n#define local_irq_save(flags)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\traw_local_irq_save(flags);\t\t\\\n\t\tif (!raw_irqs_disabled_flags(flags))\t\\\n\t\t\ttrace_hardirqs_off();\t\t\\\n\t} while (0)\n\n#define local_irq_restore(flags)\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\tif (!raw_irqs_disabled_flags(flags))\t\\\n\t\t\ttrace_hardirqs_on();\t\t\\\n\t\traw_local_irq_restore(flags);\t\t\\\n\t} while (0)\n\n#define safe_halt()\t\t\t\t\\\n\tdo {\t\t\t\t\t\\\n\t\ttrace_hardirqs_on();\t\t\\\n\t\traw_safe_halt();\t\t\\\n\t} while (0)\n\n\n#else /* !CONFIG_TRACE_IRQFLAGS */\n\n#define local_irq_enable()\tdo { raw_local_irq_enable(); } while (0)\n#define local_irq_disable()\tdo { raw_local_irq_disable(); } while (0)\n#define local_irq_save(flags)\tdo { raw_local_irq_save(flags); } while (0)\n#define local_irq_restore(flags) do { raw_local_irq_restore(flags); } while (0)\n#define safe_halt()\t\tdo { raw_safe_halt(); } while (0)\n\n#endif /* CONFIG_TRACE_IRQFLAGS */\n\n#define local_save_flags(flags)\traw_local_save_flags(flags)\n\n/*\n * Some architectures don't define arch_irqs_disabled(), so even if either\n * definition would be fine we need to use different ones for the time being\n * to avoid build issues.\n */\n#ifdef CONFIG_TRACE_IRQFLAGS_SUPPORT\n#define irqs_disabled()\t\t\t\t\t\\\n\t({\t\t\t\t\t\t\\\n\t\tunsigned long _flags;\t\t\t\\\n\t\traw_local_save_flags(_flags);\t\t\\\n\t\traw_irqs_disabled_flags(_flags);\t\\\n\t})\n#else /* !CONFIG_TRACE_IRQFLAGS_SUPPORT */\n#define irqs_disabled()\traw_irqs_disabled()\n#endif /* CONFIG_TRACE_IRQFLAGS_SUPPORT */\n\n#define irqs_disabled_flags(flags) raw_irqs_disabled_flags(flags)\n\n#endif\n"}, "5": {"id": 5, "path": "/src/include/asm-generic/bug.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_GENERIC_BUG_H\n#define _ASM_GENERIC_BUG_H\n\n#include <linux/compiler.h>\n#include <linux/instrumentation.h>\n\n#define CUT_HERE\t\t\"------------[ cut here ]------------\\n\"\n\n#ifdef CONFIG_GENERIC_BUG\n#define BUGFLAG_WARNING\t\t(1 << 0)\n#define BUGFLAG_ONCE\t\t(1 << 1)\n#define BUGFLAG_DONE\t\t(1 << 2)\n#define BUGFLAG_NO_CUT_HERE\t(1 << 3)\t/* CUT_HERE already sent */\n#define BUGFLAG_TAINT(taint)\t((taint) << 8)\n#define BUG_GET_TAINT(bug)\t((bug)->flags >> 8)\n#endif\n\n#ifndef __ASSEMBLY__\n#include <linux/kernel.h>\n\n#ifdef CONFIG_BUG\n\n#ifdef CONFIG_GENERIC_BUG\nstruct bug_entry {\n#ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS\n\tunsigned long\tbug_addr;\n#else\n\tsigned int\tbug_addr_disp;\n#endif\n#ifdef CONFIG_DEBUG_BUGVERBOSE\n#ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS\n\tconst char\t*file;\n#else\n\tsigned int\tfile_disp;\n#endif\n\tunsigned short\tline;\n#endif\n\tunsigned short\tflags;\n};\n#endif\t/* CONFIG_GENERIC_BUG */\n\n/*\n * Don't use BUG() or BUG_ON() unless there's really no way out; one\n * example might be detecting data structure corruption in the middle\n * of an operation that can't be backed out of.  If the (sub)system\n * can somehow continue operating, perhaps with reduced functionality,\n * it's probably not BUG-worthy.\n *\n * If you're tempted to BUG(), think again:  is completely giving up\n * really the *only* solution?  There are usually better options, where\n * users don't need to reboot ASAP and can mostly shut down cleanly.\n */\n#ifndef HAVE_ARCH_BUG\n#define BUG() do { \\\n\tprintk(\"BUG: failure at %s:%d/%s()!\\n\", __FILE__, __LINE__, __func__); \\\n\tbarrier_before_unreachable(); \\\n\tpanic(\"BUG!\"); \\\n} while (0)\n#endif\n\n#ifndef HAVE_ARCH_BUG_ON\n#define BUG_ON(condition) do { if (unlikely(condition)) BUG(); } while (0)\n#endif\n\n/*\n * WARN(), WARN_ON(), WARN_ON_ONCE, and so on can be used to report\n * significant kernel issues that need prompt attention if they should ever\n * appear at runtime.\n *\n * Do not use these macros when checking for invalid external inputs\n * (e.g. invalid system call arguments, or invalid data coming from\n * network/devices), and on transient conditions like ENOMEM or EAGAIN.\n * These macros should be used for recoverable kernel issues only.\n * For invalid external inputs, transient conditions, etc use\n * pr_err[_once/_ratelimited]() followed by dump_stack(), if necessary.\n * Do not include \"BUG\"/\"WARNING\" in format strings manually to make these\n * conditions distinguishable from kernel issues.\n *\n * Use the versions with printk format strings to provide better diagnostics.\n */\n#ifndef __WARN_FLAGS\nextern __printf(4, 5)\nvoid warn_slowpath_fmt(const char *file, const int line, unsigned taint,\n\t\t       const char *fmt, ...);\n#define __WARN()\t\t__WARN_printf(TAINT_WARN, NULL)\n#define __WARN_printf(taint, arg...) do {\t\t\t\t\\\n\t\tinstrumentation_begin();\t\t\t\t\\\n\t\twarn_slowpath_fmt(__FILE__, __LINE__, taint, arg);\t\\\n\t\tinstrumentation_end();\t\t\t\t\t\\\n\t} while (0)\n#else\nextern __printf(1, 2) void __warn_printk(const char *fmt, ...);\n#define __WARN()\t\t__WARN_FLAGS(BUGFLAG_TAINT(TAINT_WARN))\n#define __WARN_printf(taint, arg...) do {\t\t\t\t\\\n\t\tinstrumentation_begin();\t\t\t\t\\\n\t\t__warn_printk(arg);\t\t\t\t\t\\\n\t\t__WARN_FLAGS(BUGFLAG_NO_CUT_HERE | BUGFLAG_TAINT(taint));\\\n\t\tinstrumentation_end();\t\t\t\t\t\\\n\t} while (0)\n#define WARN_ON_ONCE(condition) ({\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\\\n\t\t__WARN_FLAGS(BUGFLAG_ONCE |\t\t\t\\\n\t\t\t     BUGFLAG_TAINT(TAINT_WARN));\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\\\n})\n#endif\n\n/* used internally by panic.c */\nstruct warn_args;\nstruct pt_regs;\n\nvoid __warn(const char *file, int line, void *caller, unsigned taint,\n\t    struct pt_regs *regs, struct warn_args *args);\n\n#ifndef WARN_ON\n#define WARN_ON(condition) ({\t\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\t\\\n\t\t__WARN();\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#ifndef WARN\n#define WARN(condition, format...) ({\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\t\\\n\t\t__WARN_printf(TAINT_WARN, format);\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#define WARN_TAINT(condition, taint, format...) ({\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\t\\\n\t\t__WARN_printf(taint, format);\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n\n#ifndef WARN_ON_ONCE\n#define WARN_ON_ONCE(condition)\t({\t\t\t\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\\\n\tint __ret_warn_once = !!(condition);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (unlikely(__ret_warn_once && !__warned)) {\t\t\\\n\t\t__warned = true;\t\t\t\t\\\n\t\tWARN_ON(1);\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\\\n})\n#endif\n\n#define WARN_ONCE(condition, format...)\t({\t\t\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\\\n\tint __ret_warn_once = !!(condition);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (unlikely(__ret_warn_once && !__warned)) {\t\t\\\n\t\t__warned = true;\t\t\t\t\\\n\t\tWARN(1, format);\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\\\n})\n\n#define WARN_TAINT_ONCE(condition, taint, format...)\t({\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\\\n\tint __ret_warn_once = !!(condition);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (unlikely(__ret_warn_once && !__warned)) {\t\t\\\n\t\t__warned = true;\t\t\t\t\\\n\t\tWARN_TAINT(1, taint, format);\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\\\n})\n\n#else /* !CONFIG_BUG */\n#ifndef HAVE_ARCH_BUG\n#define BUG() do {} while (1)\n#endif\n\n#ifndef HAVE_ARCH_BUG_ON\n#define BUG_ON(condition) do { if (unlikely(condition)) BUG(); } while (0)\n#endif\n\n#ifndef HAVE_ARCH_WARN_ON\n#define WARN_ON(condition) ({\t\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#ifndef WARN\n#define WARN(condition, format...) ({\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tno_printk(format);\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#define WARN_ON_ONCE(condition) WARN_ON(condition)\n#define WARN_ONCE(condition, format...) WARN(condition, format)\n#define WARN_TAINT(condition, taint, format...) WARN(condition, format)\n#define WARN_TAINT_ONCE(condition, taint, format...) WARN(condition, format)\n\n#endif\n\n/*\n * WARN_ON_SMP() is for cases that the warning is either\n * meaningless for !SMP or may even cause failures.\n * It can also be used with values that are only defined\n * on SMP:\n *\n * struct foo {\n *  [...]\n * #ifdef CONFIG_SMP\n *\tint bar;\n * #endif\n * };\n *\n * void func(struct foo *zoot)\n * {\n *\tWARN_ON_SMP(!zoot->bar);\n *\n * For CONFIG_SMP, WARN_ON_SMP() should act the same as WARN_ON(),\n * and should be a nop and return false for uniprocessor.\n *\n * if (WARN_ON_SMP(x)) returns true only when CONFIG_SMP is set\n * and x is true.\n */\n#ifdef CONFIG_SMP\n# define WARN_ON_SMP(x)\t\t\tWARN_ON(x)\n#else\n/*\n * Use of ({0;}) because WARN_ON_SMP(x) may be used either as\n * a stand alone line statement or as a condition in an if ()\n * statement.\n * A simple \"0\" would cause gcc to give a \"statement has no effect\"\n * warning.\n */\n# define WARN_ON_SMP(x)\t\t\t({0;})\n#endif\n\n/*\n * WARN_ON_FUNCTION_MISMATCH() warns if a value doesn't match a\n * function address, and can be useful for catching issues with\n * callback functions, for example.\n *\n * With CONFIG_CFI_CLANG, the warning is disabled because the\n * compiler replaces function addresses taken in C code with\n * local jump table addresses, which breaks cross-module function\n * address equality.\n */\n#if defined(CONFIG_CFI_CLANG) && defined(CONFIG_MODULES)\n# define WARN_ON_FUNCTION_MISMATCH(x, fn) ({ 0; })\n#else\n# define WARN_ON_FUNCTION_MISMATCH(x, fn) WARN_ON_ONCE((x) != (fn))\n#endif\n\n#endif /* __ASSEMBLY__ */\n\n#endif\n"}}, "reports": [{"events": [{"location": {"col": 2, "file": 0, "line": 2504}, "message": "Left side of '&&' is false"}, {"location": {"col": 13, "file": 1, "line": 628}, "message": "expanded from macro 'list_for_each_entry'"}, {"location": {"col": 2, "file": 1, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 1, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 61, "file": 2, "line": 702}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 2, "file": 0, "line": 2504}, "message": "Taking false branch"}, {"location": {"col": 13, "file": 1, "line": 628}, "message": "expanded from macro 'list_for_each_entry'"}, {"location": {"col": 2, "file": 1, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 1, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 2, "file": 0, "line": 1914}, "message": "Access to field 'nr_partial' results in a dereference of a null pointer (loaded from variable 'n')"}], "macros": [], "notes": [], "path": "/src/mm/slub.c", "reportHash": "9b2f0a128b6ce25b14b8ac22afa4a686", "checkerName": "clang-analyzer-core.NullDereference", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 2, "file": 0, "line": 2504}, "message": "Left side of '&&' is false"}, {"location": {"col": 13, "file": 1, "line": 628}, "message": "expanded from macro 'list_for_each_entry'"}, {"location": {"col": 2, "file": 1, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 1, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 61, "file": 2, "line": 702}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 2, "file": 0, "line": 2504}, "message": "Taking false branch"}, {"location": {"col": 13, "file": 1, "line": 628}, "message": "expanded from macro 'list_for_each_entry'"}, {"location": {"col": 2, "file": 1, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 1, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 30, "file": 0, "line": 2369}, "message": "Access to field 'nr_partial' results in a dereference of a null pointer (loaded from variable 'n')"}], "macros": [], "notes": [], "path": "/src/mm/slub.c", "reportHash": "ef64d2864c5cd3b492b9490664932a4d", "checkerName": "clang-analyzer-core.NullDereference", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 7, "file": 1, "line": 716}, "message": "expanded from macro 'list_for_each_entry_safe'"}, {"location": {"col": 2, "file": 1, "line": 555}, "message": "expanded from macro 'list_next_entry'"}, {"location": {"col": 2, "file": 1, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 2, "file": 2, "line": 701}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 30, "file": 0, "line": 3980}, "message": "Assuming 'node' is < 'nr_node_ids'"}, {"location": {"col": 19, "file": 3, "line": 581}, "message": "expanded from macro 'for_each_kmem_cache_node'"}, {"location": {"col": 2, "file": 0, "line": 3980}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 2, "file": 3, "line": 581}, "message": "expanded from macro 'for_each_kmem_cache_node'"}, {"location": {"col": 36, "file": 0, "line": 3980}, "message": "Assuming 'n' is non-null"}, {"location": {"col": 9, "file": 3, "line": 582}, "message": "expanded from macro 'for_each_kmem_cache_node'"}, {"location": {"col": 2, "file": 0, "line": 3980}, "message": "Taking true branch"}, {"location": {"col": 4, "file": 3, "line": 582}, "message": "expanded from macro 'for_each_kmem_cache_node'"}, {"location": {"col": 3, "file": 0, "line": 3981}, "message": "Calling 'free_partial'"}, {"location": {"col": 9, "file": 0, "line": 3942}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 3, "file": 4, "line": 254}, "message": "expanded from macro 'irqs_disabled'"}, {"location": {"col": 2, "file": 4, "line": 180}, "message": "expanded from macro 'raw_local_save_flags'"}, {"location": {"col": 2, "file": 0, "line": 3942}, "message": "Taking false branch"}, {"location": {"col": 32, "file": 5, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 2, "file": 0, "line": 3942}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 27, "file": 5, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 2, "file": 0, "line": 3944}, "message": "Left side of '&&' is false"}, {"location": {"col": 13, "file": 1, "line": 715}, "message": "expanded from macro 'list_for_each_entry_safe'"}, {"location": {"col": 2, "file": 1, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 1, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 61, "file": 2, "line": 702}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 2, "file": 0, "line": 3944}, "message": "Taking false branch"}, {"location": {"col": 13, "file": 1, "line": 715}, "message": "expanded from macro 'list_for_each_entry_safe'"}, {"location": {"col": 2, "file": 1, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 1, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 2, "file": 0, "line": 3955}, "message": "Assigned value is garbage or undefined"}], "macros": [], "notes": [], "path": "/src/mm/slub.c", "reportHash": "831e6fd8518117da70d3a2b454a04734", "checkerName": "clang-analyzer-core.uninitialized.Assign", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
