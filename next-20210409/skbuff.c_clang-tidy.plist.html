<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"1": {"id": 1, "path": "/src/include/linux/mm_types.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_MM_TYPES_H\n#define _LINUX_MM_TYPES_H\n\n#include <linux/mm_types_task.h>\n\n#include <linux/auxvec.h>\n#include <linux/list.h>\n#include <linux/spinlock.h>\n#include <linux/rbtree.h>\n#include <linux/rwsem.h>\n#include <linux/completion.h>\n#include <linux/cpumask.h>\n#include <linux/uprobes.h>\n#include <linux/page-flags-layout.h>\n#include <linux/workqueue.h>\n#include <linux/seqlock.h>\n\n#include <asm/mmu.h>\n\n#ifndef AT_VECTOR_SIZE_ARCH\n#define AT_VECTOR_SIZE_ARCH 0\n#endif\n#define AT_VECTOR_SIZE (2*(AT_VECTOR_SIZE_ARCH + AT_VECTOR_SIZE_BASE + 1))\n\n#define INIT_PASID\t0\n\nstruct address_space;\nstruct mem_cgroup;\n\n/*\n * Each physical page in the system has a struct page associated with\n * it to keep track of whatever it is we are using the page for at the\n * moment. Note that we have no way to track which tasks are using\n * a page, though if it is a pagecache page, rmap structures can tell us\n * who is mapping it.\n *\n * If you allocate the page using alloc_pages(), you can use some of the\n * space in struct page for your own purposes.  The five words in the main\n * union are available, except for bit 0 of the first word which must be\n * kept clear.  Many users use this word to store a pointer to an object\n * which is guaranteed to be aligned.  If you use the same storage as\n * page->mapping, you must restore it to NULL before freeing the page.\n *\n * If your page will not be mapped to userspace, you can also use the four\n * bytes in the mapcount union, but you must call page_mapcount_reset()\n * before freeing it.\n *\n * If you want to use the refcount field, it must be used in such a way\n * that other CPUs temporarily incrementing and then decrementing the\n * refcount does not cause problems.  On receiving the page from\n * alloc_pages(), the refcount will be positive.\n *\n * If you allocate pages of order > 0, you can use some of the fields\n * in each subpage, but you may need to restore some of their values\n * afterwards.\n *\n * SLUB uses cmpxchg_double() to atomically update its freelist and\n * counters.  That requires that freelist & counters be adjacent and\n * double-word aligned.  We align all struct pages to double-word\n * boundaries, and ensure that 'freelist' is aligned within the\n * struct.\n */\n#ifdef CONFIG_HAVE_ALIGNED_STRUCT_PAGE\n#define _struct_page_alignment\t__aligned(2 * sizeof(unsigned long))\n#else\n#define _struct_page_alignment\n#endif\n\nstruct page {\n\tunsigned long flags;\t\t/* Atomic flags, some possibly\n\t\t\t\t\t * updated asynchronously */\n\t/*\n\t * Five words (20/40 bytes) are available in this union.\n\t * WARNING: bit 0 of the first word is used for PageTail(). That\n\t * means the other users of this union MUST NOT use the bit to\n\t * avoid collision and false-positive PageTail().\n\t */\n\tunion {\n\t\tstruct {\t/* Page cache and anonymous pages */\n\t\t\t/**\n\t\t\t * @lru: Pageout list, eg. active_list protected by\n\t\t\t * lruvec->lru_lock.  Sometimes used as a generic list\n\t\t\t * by the page owner.\n\t\t\t */\n\t\t\tstruct list_head lru;\n\t\t\t/* See page-flags.h for PAGE_MAPPING_FLAGS */\n\t\t\tstruct address_space *mapping;\n\t\t\tpgoff_t index;\t\t/* Our offset within mapping. */\n\t\t\t/**\n\t\t\t * @private: Mapping-private opaque data.\n\t\t\t * Usually used for buffer_heads if PagePrivate.\n\t\t\t * Used for swp_entry_t if PageSwapCache.\n\t\t\t * Indicates order in the buddy system if PageBuddy.\n\t\t\t */\n\t\t\tunsigned long private;\n\t\t};\n\t\tstruct {\t/* page_pool used by netstack */\n\t\t\t/**\n\t\t\t * @dma_addr: might require a 64-bit value even on\n\t\t\t * 32-bit architectures.\n\t\t\t */\n\t\t\tdma_addr_t dma_addr;\n\t\t};\n\t\tstruct {\t/* slab, slob and slub */\n\t\t\tunion {\n\t\t\t\tstruct list_head slab_list;\n\t\t\t\tstruct {\t/* Partial pages */\n\t\t\t\t\tstruct page *next;\n#ifdef CONFIG_64BIT\n\t\t\t\t\tint pages;\t/* Nr of pages left */\n\t\t\t\t\tint pobjects;\t/* Approximate count */\n#else\n\t\t\t\t\tshort int pages;\n\t\t\t\t\tshort int pobjects;\n#endif\n\t\t\t\t};\n\t\t\t};\n\t\t\tstruct kmem_cache *slab_cache; /* not slob */\n\t\t\t/* Double-word boundary */\n\t\t\tvoid *freelist;\t\t/* first free object */\n\t\t\tunion {\n\t\t\t\tvoid *s_mem;\t/* slab: first object */\n\t\t\t\tunsigned long counters;\t\t/* SLUB */\n\t\t\t\tstruct {\t\t\t/* SLUB */\n\t\t\t\t\tunsigned inuse:16;\n\t\t\t\t\tunsigned objects:15;\n\t\t\t\t\tunsigned frozen:1;\n\t\t\t\t};\n\t\t\t};\n\t\t};\n\t\tstruct {\t/* Tail pages of compound page */\n\t\t\tunsigned long compound_head;\t/* Bit zero is set */\n\n\t\t\t/* First tail page only */\n\t\t\tunsigned char compound_dtor;\n\t\t\tunsigned char compound_order;\n\t\t\tatomic_t compound_mapcount;\n\t\t\tunsigned int compound_nr; /* 1 << compound_order */\n\t\t};\n\t\tstruct {\t/* Second tail page of compound page */\n\t\t\tunsigned long _compound_pad_1;\t/* compound_head */\n\t\t\tatomic_t hpage_pinned_refcount;\n\t\t\t/* For both global and memcg */\n\t\t\tstruct list_head deferred_list;\n\t\t};\n\t\tstruct {\t/* Page table pages */\n\t\t\tunsigned long _pt_pad_1;\t/* compound_head */\n\t\t\tpgtable_t pmd_huge_pte; /* protected by page->ptl */\n\t\t\tunsigned long _pt_pad_2;\t/* mapping */\n\t\t\tunion {\n\t\t\t\tstruct mm_struct *pt_mm; /* x86 pgds only */\n\t\t\t\tatomic_t pt_frag_refcount; /* powerpc */\n\t\t\t};\n#if ALLOC_SPLIT_PTLOCKS\n\t\t\tspinlock_t *ptl;\n#else\n\t\t\tspinlock_t ptl;\n#endif\n\t\t};\n\t\tstruct {\t/* ZONE_DEVICE pages */\n\t\t\t/** @pgmap: Points to the hosting device page map. */\n\t\t\tstruct dev_pagemap *pgmap;\n\t\t\tvoid *zone_device_data;\n\t\t\t/*\n\t\t\t * ZONE_DEVICE private pages are counted as being\n\t\t\t * mapped so the next 3 words hold the mapping, index,\n\t\t\t * and private fields from the source anonymous or\n\t\t\t * page cache page while the page is migrated to device\n\t\t\t * private memory.\n\t\t\t * ZONE_DEVICE MEMORY_DEVICE_FS_DAX pages also\n\t\t\t * use the mapping, index, and private fields when\n\t\t\t * pmem backed DAX files are mapped.\n\t\t\t */\n\t\t};\n\n\t\t/** @rcu_head: You can use this to free a page by RCU. */\n\t\tstruct rcu_head rcu_head;\n\t};\n\n\tunion {\t\t/* This union is 4 bytes in size. */\n\t\t/*\n\t\t * If the page can be mapped to userspace, encodes the number\n\t\t * of times this page is referenced by a page table.\n\t\t */\n\t\tatomic_t _mapcount;\n\n\t\t/*\n\t\t * If the page is neither PageSlab nor mappable to userspace,\n\t\t * the value stored here may help determine what this page\n\t\t * is used for.  See page-flags.h for a list of page types\n\t\t * which are currently stored here.\n\t\t */\n\t\tunsigned int page_type;\n\n\t\tunsigned int active;\t\t/* SLAB */\n\t\tint units;\t\t\t/* SLOB */\n\t};\n\n\t/* Usage count. *DO NOT USE DIRECTLY*. See page_ref.h */\n\tatomic_t _refcount;\n\n#ifdef CONFIG_MEMCG\n\tunsigned long memcg_data;\n#endif\n\n\t/*\n\t * On machines where all RAM is mapped into kernel address space,\n\t * we can simply calculate the virtual address. On machines with\n\t * highmem some memory is mapped into kernel virtual memory\n\t * dynamically, so we need a place to store that address.\n\t * Note that this field could be 16 bits on x86 ... ;)\n\t *\n\t * Architectures with slow multiplication can define\n\t * WANT_PAGE_VIRTUAL in asm/page.h\n\t */\n#if defined(WANT_PAGE_VIRTUAL)\n\tvoid *virtual;\t\t\t/* Kernel virtual address (NULL if\n\t\t\t\t\t   not kmapped, ie. highmem) */\n#endif /* WANT_PAGE_VIRTUAL */\n\n#ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS\n\tint _last_cpupid;\n#endif\n} _struct_page_alignment;\n\nstatic inline atomic_t *compound_mapcount_ptr(struct page *page)\n{\n\treturn &page[1].compound_mapcount;\n}\n\nstatic inline atomic_t *compound_pincount_ptr(struct page *page)\n{\n\treturn &page[2].hpage_pinned_refcount;\n}\n\n/*\n * Used for sizing the vmemmap region on some architectures\n */\n#define STRUCT_PAGE_MAX_SHIFT\t(order_base_2(sizeof(struct page)))\n\n#define PAGE_FRAG_CACHE_MAX_SIZE\t__ALIGN_MASK(32768, ~PAGE_MASK)\n#define PAGE_FRAG_CACHE_MAX_ORDER\tget_order(PAGE_FRAG_CACHE_MAX_SIZE)\n\n#define page_private(page)\t\t((page)->private)\n\nstatic inline void set_page_private(struct page *page, unsigned long private)\n{\n\tpage->private = private;\n}\n\nstruct page_frag_cache {\n\tvoid * va;\n#if (PAGE_SIZE < PAGE_FRAG_CACHE_MAX_SIZE)\n\t__u16 offset;\n\t__u16 size;\n#else\n\t__u32 offset;\n#endif\n\t/* we maintain a pagecount bias, so that we dont dirty cache line\n\t * containing page->_refcount every time we allocate a fragment.\n\t */\n\tunsigned int\t\tpagecnt_bias;\n\tbool pfmemalloc;\n};\n\ntypedef unsigned long vm_flags_t;\n\n/*\n * A region containing a mapping of a non-memory backed file under NOMMU\n * conditions.  These are held in a global tree and are pinned by the VMAs that\n * map parts of them.\n */\nstruct vm_region {\n\tstruct rb_node\tvm_rb;\t\t/* link in global region tree */\n\tvm_flags_t\tvm_flags;\t/* VMA vm_flags */\n\tunsigned long\tvm_start;\t/* start address of region */\n\tunsigned long\tvm_end;\t\t/* region initialised to here */\n\tunsigned long\tvm_top;\t\t/* region allocated to here */\n\tunsigned long\tvm_pgoff;\t/* the offset in vm_file corresponding to vm_start */\n\tstruct file\t*vm_file;\t/* the backing file or NULL */\n\n\tint\t\tvm_usage;\t/* region usage count (access under nommu_region_sem) */\n\tbool\t\tvm_icache_flushed : 1; /* true if the icache has been flushed for\n\t\t\t\t\t\t* this region */\n};\n\n#ifdef CONFIG_USERFAULTFD\n#define NULL_VM_UFFD_CTX ((struct vm_userfaultfd_ctx) { NULL, })\nstruct vm_userfaultfd_ctx {\n\tstruct userfaultfd_ctx *ctx;\n};\n#else /* CONFIG_USERFAULTFD */\n#define NULL_VM_UFFD_CTX ((struct vm_userfaultfd_ctx) {})\nstruct vm_userfaultfd_ctx {};\n#endif /* CONFIG_USERFAULTFD */\n\n/*\n * This struct describes a virtual memory area. There is one of these\n * per VM-area/task. A VM area is any part of the process virtual memory\n * space that has a special rule for the page-fault handlers (ie a shared\n * library, the executable area etc).\n */\nstruct vm_area_struct {\n\t/* The first cache line has the info for VMA tree walking. */\n\n\tunsigned long vm_start;\t\t/* Our start address within vm_mm. */\n\tunsigned long vm_end;\t\t/* The first byte after our end address\n\t\t\t\t\t   within vm_mm. */\n\n\t/* linked list of VM areas per task, sorted by address */\n\tstruct vm_area_struct *vm_next, *vm_prev;\n\n\tstruct rb_node vm_rb;\n\n\t/*\n\t * Largest free memory gap in bytes to the left of this VMA.\n\t * Either between this VMA and vma->vm_prev, or between one of the\n\t * VMAs below us in the VMA rbtree and its ->vm_prev. This helps\n\t * get_unmapped_area find a free area of the right size.\n\t */\n\tunsigned long rb_subtree_gap;\n\n\t/* Second cache line starts here. */\n\n\tstruct mm_struct *vm_mm;\t/* The address space we belong to. */\n\n\t/*\n\t * Access permissions of this VMA.\n\t * See vmf_insert_mixed_prot() for discussion.\n\t */\n\tpgprot_t vm_page_prot;\n\tunsigned long vm_flags;\t\t/* Flags, see mm.h. */\n\n\t/*\n\t * For areas with an address space and backing store,\n\t * linkage into the address_space->i_mmap interval tree.\n\t */\n\tstruct {\n\t\tstruct rb_node rb;\n\t\tunsigned long rb_subtree_last;\n\t} shared;\n\n\t/*\n\t * A file's MAP_PRIVATE vma can be in both i_mmap tree and anon_vma\n\t * list, after a COW of one of the file pages.\tA MAP_SHARED vma\n\t * can only be in the i_mmap tree.  An anonymous MAP_PRIVATE, stack\n\t * or brk vma (with NULL file) can only be in an anon_vma list.\n\t */\n\tstruct list_head anon_vma_chain; /* Serialized by mmap_lock &\n\t\t\t\t\t  * page_table_lock */\n\tstruct anon_vma *anon_vma;\t/* Serialized by page_table_lock */\n\n\t/* Function pointers to deal with this struct. */\n\tconst struct vm_operations_struct *vm_ops;\n\n\t/* Information about our backing store: */\n\tunsigned long vm_pgoff;\t\t/* Offset (within vm_file) in PAGE_SIZE\n\t\t\t\t\t   units */\n\tstruct file * vm_file;\t\t/* File we map to (can be NULL). */\n\tvoid * vm_private_data;\t\t/* was vm_pte (shared mem) */\n\n#ifdef CONFIG_SWAP\n\tatomic_long_t swap_readahead_info;\n#endif\n#ifndef CONFIG_MMU\n\tstruct vm_region *vm_region;\t/* NOMMU mapping region */\n#endif\n#ifdef CONFIG_NUMA\n\tstruct mempolicy *vm_policy;\t/* NUMA policy for the VMA */\n#endif\n\tstruct vm_userfaultfd_ctx vm_userfaultfd_ctx;\n} __randomize_layout;\n\nstruct core_thread {\n\tstruct task_struct *task;\n\tstruct core_thread *next;\n};\n\nstruct core_state {\n\tatomic_t nr_threads;\n\tstruct core_thread dumper;\n\tstruct completion startup;\n};\n\nstruct kioctx_table;\nstruct mm_struct {\n\tstruct {\n\t\tstruct vm_area_struct *mmap;\t\t/* list of VMAs */\n\t\tstruct rb_root mm_rb;\n\t\tu64 vmacache_seqnum;                   /* per-thread vmacache */\n#ifdef CONFIG_MMU\n\t\tunsigned long (*get_unmapped_area) (struct file *filp,\n\t\t\t\tunsigned long addr, unsigned long len,\n\t\t\t\tunsigned long pgoff, unsigned long flags);\n#endif\n\t\tunsigned long mmap_base;\t/* base of mmap area */\n\t\tunsigned long mmap_legacy_base;\t/* base of mmap area in bottom-up allocations */\n#ifdef CONFIG_HAVE_ARCH_COMPAT_MMAP_BASES\n\t\t/* Base adresses for compatible mmap() */\n\t\tunsigned long mmap_compat_base;\n\t\tunsigned long mmap_compat_legacy_base;\n#endif\n\t\tunsigned long task_size;\t/* size of task vm space */\n\t\tunsigned long highest_vm_end;\t/* highest vma end address */\n\t\tpgd_t * pgd;\n\n#ifdef CONFIG_MEMBARRIER\n\t\t/**\n\t\t * @membarrier_state: Flags controlling membarrier behavior.\n\t\t *\n\t\t * This field is close to @pgd to hopefully fit in the same\n\t\t * cache-line, which needs to be touched by switch_mm().\n\t\t */\n\t\tatomic_t membarrier_state;\n#endif\n\n\t\t/**\n\t\t * @mm_users: The number of users including userspace.\n\t\t *\n\t\t * Use mmget()/mmget_not_zero()/mmput() to modify. When this\n\t\t * drops to 0 (i.e. when the task exits and there are no other\n\t\t * temporary reference holders), we also release a reference on\n\t\t * @mm_count (which may then free the &struct mm_struct if\n\t\t * @mm_count also drops to 0).\n\t\t */\n\t\tatomic_t mm_users;\n\n\t\t/**\n\t\t * @mm_count: The number of references to &struct mm_struct\n\t\t * (@mm_users count as 1).\n\t\t *\n\t\t * Use mmgrab()/mmdrop() to modify. When this drops to 0, the\n\t\t * &struct mm_struct is freed.\n\t\t */\n\t\tatomic_t mm_count;\n\n\t\t/**\n\t\t * @has_pinned: Whether this mm has pinned any pages.  This can\n\t\t * be either replaced in the future by @pinned_vm when it\n\t\t * becomes stable, or grow into a counter on its own. We're\n\t\t * aggresive on this bit now - even if the pinned pages were\n\t\t * unpinned later on, we'll still keep this bit set for the\n\t\t * lifecycle of this mm just for simplicity.\n\t\t */\n\t\tatomic_t has_pinned;\n\n\t\t/**\n\t\t * @write_protect_seq: Locked when any thread is write\n\t\t * protecting pages mapped by this mm to enforce a later COW,\n\t\t * for instance during page table copying for fork().\n\t\t */\n\t\tseqcount_t write_protect_seq;\n\n#ifdef CONFIG_MMU\n\t\tatomic_long_t pgtables_bytes;\t/* PTE page table pages */\n#endif\n\t\tint map_count;\t\t\t/* number of VMAs */\n\n\t\tspinlock_t page_table_lock; /* Protects page tables and some\n\t\t\t\t\t     * counters\n\t\t\t\t\t     */\n\t\tstruct rw_semaphore mmap_lock;\n\n\t\tstruct list_head mmlist; /* List of maybe swapped mm's.\tThese\n\t\t\t\t\t  * are globally strung together off\n\t\t\t\t\t  * init_mm.mmlist, and are protected\n\t\t\t\t\t  * by mmlist_lock\n\t\t\t\t\t  */\n\n\n\t\tunsigned long hiwater_rss; /* High-watermark of RSS usage */\n\t\tunsigned long hiwater_vm;  /* High-water virtual memory usage */\n\n\t\tunsigned long total_vm;\t   /* Total pages mapped */\n\t\tunsigned long locked_vm;   /* Pages that have PG_mlocked set */\n\t\tatomic64_t    pinned_vm;   /* Refcount permanently increased */\n\t\tunsigned long data_vm;\t   /* VM_WRITE & ~VM_SHARED & ~VM_STACK */\n\t\tunsigned long exec_vm;\t   /* VM_EXEC & ~VM_WRITE & ~VM_STACK */\n\t\tunsigned long stack_vm;\t   /* VM_STACK */\n\t\tunsigned long def_flags;\n\n\t\tspinlock_t arg_lock; /* protect the below fields */\n\t\tunsigned long start_code, end_code, start_data, end_data;\n\t\tunsigned long start_brk, brk, start_stack;\n\t\tunsigned long arg_start, arg_end, env_start, env_end;\n\n\t\tunsigned long saved_auxv[AT_VECTOR_SIZE]; /* for /proc/PID/auxv */\n\n\t\t/*\n\t\t * Special counters, in some configurations protected by the\n\t\t * page_table_lock, in other configurations by being atomic.\n\t\t */\n\t\tstruct mm_rss_stat rss_stat;\n\n\t\tstruct linux_binfmt *binfmt;\n\n\t\t/* Architecture-specific MM context */\n\t\tmm_context_t context;\n\n\t\tunsigned long flags; /* Must use atomic bitops to access */\n\n\t\tstruct core_state *core_state; /* coredumping support */\n\n#ifdef CONFIG_AIO\n\t\tspinlock_t\t\t\tioctx_lock;\n\t\tstruct kioctx_table __rcu\t*ioctx_table;\n#endif\n#ifdef CONFIG_MEMCG\n\t\t/*\n\t\t * \"owner\" points to a task that is regarded as the canonical\n\t\t * user/owner of this mm. All of the following must be true in\n\t\t * order for it to be changed:\n\t\t *\n\t\t * current == mm->owner\n\t\t * current->mm != mm\n\t\t * new_owner->mm == mm\n\t\t * new_owner->alloc_lock is held\n\t\t */\n\t\tstruct task_struct __rcu *owner;\n#endif\n\t\tstruct user_namespace *user_ns;\n\n\t\t/* store ref to file /proc/<pid>/exe symlink points to */\n\t\tstruct file __rcu *exe_file;\n#ifdef CONFIG_MMU_NOTIFIER\n\t\tstruct mmu_notifier_subscriptions *notifier_subscriptions;\n#endif\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS\n\t\tpgtable_t pmd_huge_pte; /* protected by page_table_lock */\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\t\t/*\n\t\t * numa_next_scan is the next time that the PTEs will be marked\n\t\t * pte_numa. NUMA hinting faults will gather statistics and\n\t\t * migrate pages to new nodes if necessary.\n\t\t */\n\t\tunsigned long numa_next_scan;\n\n\t\t/* Restart point for scanning and setting pte_numa */\n\t\tunsigned long numa_scan_offset;\n\n\t\t/* numa_scan_seq prevents two threads setting pte_numa */\n\t\tint numa_scan_seq;\n#endif\n\t\t/*\n\t\t * An operation with batched TLB flushing is going on. Anything\n\t\t * that can move process memory needs to flush the TLB when\n\t\t * moving a PROT_NONE or PROT_NUMA mapped page.\n\t\t */\n\t\tatomic_t tlb_flush_pending;\n#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH\n\t\t/* See flush_tlb_batched_pending() */\n\t\tbool tlb_flush_batched;\n#endif\n\t\tstruct uprobes_state uprobes_state;\n#ifdef CONFIG_HUGETLB_PAGE\n\t\tatomic_long_t hugetlb_usage;\n#endif\n\t\tstruct work_struct async_put_work;\n\n#ifdef CONFIG_IOMMU_SUPPORT\n\t\tu32 pasid;\n#endif\n\t} __randomize_layout;\n\n\t/*\n\t * The mm_cpumask needs to be at the end of mm_struct, because it\n\t * is dynamically sized based on nr_cpu_ids.\n\t */\n\tunsigned long cpu_bitmap[];\n};\n\nextern struct mm_struct init_mm;\n\n/* Pointer magic because the dynamic array size confuses some compilers. */\nstatic inline void mm_init_cpumask(struct mm_struct *mm)\n{\n\tunsigned long cpu_bitmap = (unsigned long)mm;\n\n\tcpu_bitmap += offsetof(struct mm_struct, cpu_bitmap);\n\tcpumask_clear((struct cpumask *)cpu_bitmap);\n}\n\n/* Future-safe accessor for struct mm_struct's cpu_vm_mask. */\nstatic inline cpumask_t *mm_cpumask(struct mm_struct *mm)\n{\n\treturn (struct cpumask *)&mm->cpu_bitmap;\n}\n\nstruct mmu_gather;\nextern void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm);\nextern void tlb_gather_mmu_fullmm(struct mmu_gather *tlb, struct mm_struct *mm);\nextern void tlb_finish_mmu(struct mmu_gather *tlb);\n\nstatic inline void init_tlb_flush_pending(struct mm_struct *mm)\n{\n\tatomic_set(&mm->tlb_flush_pending, 0);\n}\n\nstatic inline void inc_tlb_flush_pending(struct mm_struct *mm)\n{\n\tatomic_inc(&mm->tlb_flush_pending);\n\t/*\n\t * The only time this value is relevant is when there are indeed pages\n\t * to flush. And we'll only flush pages after changing them, which\n\t * requires the PTL.\n\t *\n\t * So the ordering here is:\n\t *\n\t *\tatomic_inc(&mm->tlb_flush_pending);\n\t *\tspin_lock(&ptl);\n\t *\t...\n\t *\tset_pte_at();\n\t *\tspin_unlock(&ptl);\n\t *\n\t *\t\t\t\tspin_lock(&ptl)\n\t *\t\t\t\tmm_tlb_flush_pending();\n\t *\t\t\t\t....\n\t *\t\t\t\tspin_unlock(&ptl);\n\t *\n\t *\tflush_tlb_range();\n\t *\tatomic_dec(&mm->tlb_flush_pending);\n\t *\n\t * Where the increment if constrained by the PTL unlock, it thus\n\t * ensures that the increment is visible if the PTE modification is\n\t * visible. After all, if there is no PTE modification, nobody cares\n\t * about TLB flushes either.\n\t *\n\t * This very much relies on users (mm_tlb_flush_pending() and\n\t * mm_tlb_flush_nested()) only caring about _specific_ PTEs (and\n\t * therefore specific PTLs), because with SPLIT_PTE_PTLOCKS and RCpc\n\t * locks (PPC) the unlock of one doesn't order against the lock of\n\t * another PTL.\n\t *\n\t * The decrement is ordered by the flush_tlb_range(), such that\n\t * mm_tlb_flush_pending() will not return false unless all flushes have\n\t * completed.\n\t */\n}\n\nstatic inline void dec_tlb_flush_pending(struct mm_struct *mm)\n{\n\t/*\n\t * See inc_tlb_flush_pending().\n\t *\n\t * This cannot be smp_mb__before_atomic() because smp_mb() simply does\n\t * not order against TLB invalidate completion, which is what we need.\n\t *\n\t * Therefore we must rely on tlb_flush_*() to guarantee order.\n\t */\n\tatomic_dec(&mm->tlb_flush_pending);\n}\n\nstatic inline bool mm_tlb_flush_pending(struct mm_struct *mm)\n{\n\t/*\n\t * Must be called after having acquired the PTL; orders against that\n\t * PTLs release and therefore ensures that if we observe the modified\n\t * PTE we must also observe the increment from inc_tlb_flush_pending().\n\t *\n\t * That is, it only guarantees to return true if there is a flush\n\t * pending for _this_ PTL.\n\t */\n\treturn atomic_read(&mm->tlb_flush_pending);\n}\n\nstatic inline bool mm_tlb_flush_nested(struct mm_struct *mm)\n{\n\t/*\n\t * Similar to mm_tlb_flush_pending(), we must have acquired the PTL\n\t * for which there is a TLB flush pending in order to guarantee\n\t * we've seen both that PTE modification and the increment.\n\t *\n\t * (no requirement on actually still holding the PTL, that is irrelevant)\n\t */\n\treturn atomic_read(&mm->tlb_flush_pending) > 1;\n}\n\nstruct vm_fault;\n\n/**\n * typedef vm_fault_t - Return type for page fault handlers.\n *\n * Page fault handlers return a bitmask of %VM_FAULT values.\n */\ntypedef __bitwise unsigned int vm_fault_t;\n\n/**\n * enum vm_fault_reason - Page fault handlers return a bitmask of\n * these values to tell the core VM what happened when handling the\n * fault. Used to decide whether a process gets delivered SIGBUS or\n * just gets major/minor fault counters bumped up.\n *\n * @VM_FAULT_OOM:\t\tOut Of Memory\n * @VM_FAULT_SIGBUS:\t\tBad access\n * @VM_FAULT_MAJOR:\t\tPage read from storage\n * @VM_FAULT_WRITE:\t\tSpecial case for get_user_pages\n * @VM_FAULT_HWPOISON:\t\tHit poisoned small page\n * @VM_FAULT_HWPOISON_LARGE:\tHit poisoned large page. Index encoded\n *\t\t\t\tin upper bits\n * @VM_FAULT_SIGSEGV:\t\tsegmentation fault\n * @VM_FAULT_NOPAGE:\t\t->fault installed the pte, not return page\n * @VM_FAULT_LOCKED:\t\t->fault locked the returned page\n * @VM_FAULT_RETRY:\t\t->fault blocked, must retry\n * @VM_FAULT_FALLBACK:\t\thuge page fault failed, fall back to small\n * @VM_FAULT_DONE_COW:\t\t->fault has fully handled COW\n * @VM_FAULT_NEEDDSYNC:\t\t->fault did not modify page tables and needs\n *\t\t\t\tfsync() to complete (for synchronous page faults\n *\t\t\t\tin DAX)\n * @VM_FAULT_HINDEX_MASK:\tmask HINDEX value\n *\n */\nenum vm_fault_reason {\n\tVM_FAULT_OOM            = (__force vm_fault_t)0x000001,\n\tVM_FAULT_SIGBUS         = (__force vm_fault_t)0x000002,\n\tVM_FAULT_MAJOR          = (__force vm_fault_t)0x000004,\n\tVM_FAULT_WRITE          = (__force vm_fault_t)0x000008,\n\tVM_FAULT_HWPOISON       = (__force vm_fault_t)0x000010,\n\tVM_FAULT_HWPOISON_LARGE = (__force vm_fault_t)0x000020,\n\tVM_FAULT_SIGSEGV        = (__force vm_fault_t)0x000040,\n\tVM_FAULT_NOPAGE         = (__force vm_fault_t)0x000100,\n\tVM_FAULT_LOCKED         = (__force vm_fault_t)0x000200,\n\tVM_FAULT_RETRY          = (__force vm_fault_t)0x000400,\n\tVM_FAULT_FALLBACK       = (__force vm_fault_t)0x000800,\n\tVM_FAULT_DONE_COW       = (__force vm_fault_t)0x001000,\n\tVM_FAULT_NEEDDSYNC      = (__force vm_fault_t)0x002000,\n\tVM_FAULT_HINDEX_MASK    = (__force vm_fault_t)0x0f0000,\n};\n\n/* Encode hstate index for a hwpoisoned large page */\n#define VM_FAULT_SET_HINDEX(x) ((__force vm_fault_t)((x) << 16))\n#define VM_FAULT_GET_HINDEX(x) (((__force unsigned int)(x) >> 16) & 0xf)\n\n#define VM_FAULT_ERROR (VM_FAULT_OOM | VM_FAULT_SIGBUS |\t\\\n\t\t\tVM_FAULT_SIGSEGV | VM_FAULT_HWPOISON |\t\\\n\t\t\tVM_FAULT_HWPOISON_LARGE | VM_FAULT_FALLBACK)\n\n#define VM_FAULT_RESULT_TRACE \\\n\t{ VM_FAULT_OOM,                 \"OOM\" },\t\\\n\t{ VM_FAULT_SIGBUS,              \"SIGBUS\" },\t\\\n\t{ VM_FAULT_MAJOR,               \"MAJOR\" },\t\\\n\t{ VM_FAULT_WRITE,               \"WRITE\" },\t\\\n\t{ VM_FAULT_HWPOISON,            \"HWPOISON\" },\t\\\n\t{ VM_FAULT_HWPOISON_LARGE,      \"HWPOISON_LARGE\" },\t\\\n\t{ VM_FAULT_SIGSEGV,             \"SIGSEGV\" },\t\\\n\t{ VM_FAULT_NOPAGE,              \"NOPAGE\" },\t\\\n\t{ VM_FAULT_LOCKED,              \"LOCKED\" },\t\\\n\t{ VM_FAULT_RETRY,               \"RETRY\" },\t\\\n\t{ VM_FAULT_FALLBACK,            \"FALLBACK\" },\t\\\n\t{ VM_FAULT_DONE_COW,            \"DONE_COW\" },\t\\\n\t{ VM_FAULT_NEEDDSYNC,           \"NEEDDSYNC\" }\n\nstruct vm_special_mapping {\n\tconst char *name;\t/* The name, e.g. \"[vdso]\". */\n\n\t/*\n\t * If .fault is not provided, this points to a\n\t * NULL-terminated array of pages that back the special mapping.\n\t *\n\t * This must not be NULL unless .fault is provided.\n\t */\n\tstruct page **pages;\n\n\t/*\n\t * If non-NULL, then this is called to resolve page faults\n\t * on the special mapping.  If used, .pages is not checked.\n\t */\n\tvm_fault_t (*fault)(const struct vm_special_mapping *sm,\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tstruct vm_fault *vmf);\n\n\tint (*mremap)(const struct vm_special_mapping *sm,\n\t\t     struct vm_area_struct *new_vma);\n};\n\nenum tlb_flush_reason {\n\tTLB_FLUSH_ON_TASK_SWITCH,\n\tTLB_REMOTE_SHOOTDOWN,\n\tTLB_LOCAL_SHOOTDOWN,\n\tTLB_LOCAL_MM_SHOOTDOWN,\n\tTLB_REMOTE_SEND_IPI,\n\tNR_TLB_FLUSH_REASONS,\n};\n\n /*\n  * A swap entry has to fit into a \"unsigned long\", as the entry is hidden\n  * in the \"index\" field of the swapper address space.\n  */\ntypedef struct {\n\tunsigned long val;\n} swp_entry_t;\n\n#endif /* _LINUX_MM_TYPES_H */\n"}, "0": {"id": 0, "path": "/src/net/core/skbuff.c", "content": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n *\tRoutines having to do with the 'struct sk_buff' memory handlers.\n *\n *\tAuthors:\tAlan Cox <alan@lxorguk.ukuu.org.uk>\n *\t\t\tFlorian La Roche <rzsfl@rz.uni-sb.de>\n *\n *\tFixes:\n *\t\tAlan Cox\t:\tFixed the worst of the load\n *\t\t\t\t\tbalancer bugs.\n *\t\tDave Platt\t:\tInterrupt stacking fix.\n *\tRichard Kooijman\t:\tTimestamp fixes.\n *\t\tAlan Cox\t:\tChanged buffer format.\n *\t\tAlan Cox\t:\tdestructor hook for AF_UNIX etc.\n *\t\tLinus Torvalds\t:\tBetter skb_clone.\n *\t\tAlan Cox\t:\tAdded skb_copy.\n *\t\tAlan Cox\t:\tAdded all the changed routines Linus\n *\t\t\t\t\tonly put in the headers\n *\t\tRay VanTassle\t:\tFixed --skb->lock in free\n *\t\tAlan Cox\t:\tskb_copy copy arp field\n *\t\tAndi Kleen\t:\tslabified it.\n *\t\tRobert Olsson\t:\tRemoved skb_head_pool\n *\n *\tNOTE:\n *\t\tThe __skb_ routines should be called with interrupts\n *\tdisabled, or you better be *real* sure that the operation is atomic\n *\twith respect to whatever list is being frobbed (e.g. via lock_sock()\n *\tor via disabling bottom half handlers, etc).\n */\n\n/*\n *\tThe functions in this file will not compile correctly with gcc 2.4.x\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/slab.h>\n#include <linux/tcp.h>\n#include <linux/udp.h>\n#include <linux/sctp.h>\n#include <linux/netdevice.h>\n#ifdef CONFIG_NET_CLS_ACT\n#include <net/pkt_sched.h>\n#endif\n#include <linux/string.h>\n#include <linux/skbuff.h>\n#include <linux/splice.h>\n#include <linux/cache.h>\n#include <linux/rtnetlink.h>\n#include <linux/init.h>\n#include <linux/scatterlist.h>\n#include <linux/errqueue.h>\n#include <linux/prefetch.h>\n#include <linux/if_vlan.h>\n#include <linux/mpls.h>\n#include <linux/kcov.h>\n\n#include <net/protocol.h>\n#include <net/dst.h>\n#include <net/sock.h>\n#include <net/checksum.h>\n#include <net/ip6_checksum.h>\n#include <net/xfrm.h>\n#include <net/mpls.h>\n#include <net/mptcp.h>\n\n#include <linux/uaccess.h>\n#include <trace/events/skb.h>\n#include <linux/highmem.h>\n#include <linux/capability.h>\n#include <linux/user_namespace.h>\n#include <linux/indirect_call_wrapper.h>\n\n#include \"datagram.h\"\n\nstruct kmem_cache *skbuff_head_cache __ro_after_init;\nstatic struct kmem_cache *skbuff_fclone_cache __ro_after_init;\n#ifdef CONFIG_SKB_EXTENSIONS\nstatic struct kmem_cache *skbuff_ext_cache __ro_after_init;\n#endif\nint sysctl_max_skb_frags __read_mostly = MAX_SKB_FRAGS;\nEXPORT_SYMBOL(sysctl_max_skb_frags);\n\n/**\n *\tskb_panic - private function for out-of-line support\n *\t@skb:\tbuffer\n *\t@sz:\tsize\n *\t@addr:\taddress\n *\t@msg:\tskb_over_panic or skb_under_panic\n *\n *\tOut-of-line support for skb_put() and skb_push().\n *\tCalled via the wrapper skb_over_panic() or skb_under_panic().\n *\tKeep out of line to prevent kernel bloat.\n *\t__builtin_return_address is not used because it is not always reliable.\n */\nstatic void skb_panic(struct sk_buff *skb, unsigned int sz, void *addr,\n\t\t      const char msg[])\n{\n\tpr_emerg(\"%s: text:%px len:%d put:%d head:%px data:%px tail:%#lx end:%#lx dev:%s\\n\",\n\t\t msg, addr, skb->len, sz, skb->head, skb->data,\n\t\t (unsigned long)skb->tail, (unsigned long)skb->end,\n\t\t skb->dev ? skb->dev->name : \"<NULL>\");\n\tBUG();\n}\n\nstatic void skb_over_panic(struct sk_buff *skb, unsigned int sz, void *addr)\n{\n\tskb_panic(skb, sz, addr, __func__);\n}\n\nstatic void skb_under_panic(struct sk_buff *skb, unsigned int sz, void *addr)\n{\n\tskb_panic(skb, sz, addr, __func__);\n}\n\n#define NAPI_SKB_CACHE_SIZE\t64\n#define NAPI_SKB_CACHE_BULK\t16\n#define NAPI_SKB_CACHE_HALF\t(NAPI_SKB_CACHE_SIZE / 2)\n\nstruct napi_alloc_cache {\n\tstruct page_frag_cache page;\n\tunsigned int skb_count;\n\tvoid *skb_cache[NAPI_SKB_CACHE_SIZE];\n};\n\nstatic DEFINE_PER_CPU(struct page_frag_cache, netdev_alloc_cache);\nstatic DEFINE_PER_CPU(struct napi_alloc_cache, napi_alloc_cache);\n\nstatic void *__alloc_frag_align(unsigned int fragsz, gfp_t gfp_mask,\n\t\t\t\tunsigned int align_mask)\n{\n\tstruct napi_alloc_cache *nc = this_cpu_ptr(&napi_alloc_cache);\n\n\treturn page_frag_alloc_align(&nc->page, fragsz, gfp_mask, align_mask);\n}\n\nvoid *__napi_alloc_frag_align(unsigned int fragsz, unsigned int align_mask)\n{\n\tfragsz = SKB_DATA_ALIGN(fragsz);\n\n\treturn __alloc_frag_align(fragsz, GFP_ATOMIC, align_mask);\n}\nEXPORT_SYMBOL(__napi_alloc_frag_align);\n\nvoid *__netdev_alloc_frag_align(unsigned int fragsz, unsigned int align_mask)\n{\n\tstruct page_frag_cache *nc;\n\tvoid *data;\n\n\tfragsz = SKB_DATA_ALIGN(fragsz);\n\tif (in_irq() || irqs_disabled()) {\n\t\tnc = this_cpu_ptr(&netdev_alloc_cache);\n\t\tdata = page_frag_alloc_align(nc, fragsz, GFP_ATOMIC, align_mask);\n\t} else {\n\t\tlocal_bh_disable();\n\t\tdata = __alloc_frag_align(fragsz, GFP_ATOMIC, align_mask);\n\t\tlocal_bh_enable();\n\t}\n\treturn data;\n}\nEXPORT_SYMBOL(__netdev_alloc_frag_align);\n\nstatic struct sk_buff *napi_skb_cache_get(void)\n{\n\tstruct napi_alloc_cache *nc = this_cpu_ptr(&napi_alloc_cache);\n\tstruct sk_buff *skb;\n\n\tif (unlikely(!nc->skb_count))\n\t\tnc->skb_count = kmem_cache_alloc_bulk(skbuff_head_cache,\n\t\t\t\t\t\t      GFP_ATOMIC,\n\t\t\t\t\t\t      NAPI_SKB_CACHE_BULK,\n\t\t\t\t\t\t      nc->skb_cache);\n\tif (unlikely(!nc->skb_count))\n\t\treturn NULL;\n\n\tskb = nc->skb_cache[--nc->skb_count];\n\tkasan_unpoison_object_data(skbuff_head_cache, skb);\n\n\treturn skb;\n}\n\n/* Caller must provide SKB that is memset cleared */\nstatic void __build_skb_around(struct sk_buff *skb, void *data,\n\t\t\t       unsigned int frag_size)\n{\n\tstruct skb_shared_info *shinfo;\n\tunsigned int size = frag_size ? : ksize(data);\n\n\tsize -= SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\t/* Assumes caller memset cleared SKB */\n\tskb->truesize = SKB_TRUESIZE(size);\n\trefcount_set(&skb->users, 1);\n\tskb->head = data;\n\tskb->data = data;\n\tskb_reset_tail_pointer(skb);\n\tskb->end = skb->tail + size;\n\tskb->mac_header = (typeof(skb->mac_header))~0U;\n\tskb->transport_header = (typeof(skb->transport_header))~0U;\n\n\t/* make sure we initialize shinfo sequentially */\n\tshinfo = skb_shinfo(skb);\n\tmemset(shinfo, 0, offsetof(struct skb_shared_info, dataref));\n\tatomic_set(&shinfo->dataref, 1);\n\n\tskb_set_kcov_handle(skb, kcov_common_handle());\n}\n\n/**\n * __build_skb - build a network buffer\n * @data: data buffer provided by caller\n * @frag_size: size of data, or 0 if head was kmalloced\n *\n * Allocate a new &sk_buff. Caller provides space holding head and\n * skb_shared_info. @data must have been allocated by kmalloc() only if\n * @frag_size is 0, otherwise data should come from the page allocator\n *  or vmalloc()\n * The return is the new skb buffer.\n * On a failure the return is %NULL, and @data is not freed.\n * Notes :\n *  Before IO, driver allocates only data buffer where NIC put incoming frame\n *  Driver should add room at head (NET_SKB_PAD) and\n *  MUST add room at tail (SKB_DATA_ALIGN(skb_shared_info))\n *  After IO, driver calls build_skb(), to allocate sk_buff and populate it\n *  before giving packet to stack.\n *  RX rings only contains data buffers, not full skbs.\n */\nstruct sk_buff *__build_skb(void *data, unsigned int frag_size)\n{\n\tstruct sk_buff *skb;\n\n\tskb = kmem_cache_alloc(skbuff_head_cache, GFP_ATOMIC);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\t__build_skb_around(skb, data, frag_size);\n\n\treturn skb;\n}\n\n/* build_skb() is wrapper over __build_skb(), that specifically\n * takes care of skb->head and skb->pfmemalloc\n * This means that if @frag_size is not zero, then @data must be backed\n * by a page fragment, not kmalloc() or vmalloc()\n */\nstruct sk_buff *build_skb(void *data, unsigned int frag_size)\n{\n\tstruct sk_buff *skb = __build_skb(data, frag_size);\n\n\tif (skb && frag_size) {\n\t\tskb->head_frag = 1;\n\t\tif (page_is_pfmemalloc(virt_to_head_page(data)))\n\t\t\tskb->pfmemalloc = 1;\n\t}\n\treturn skb;\n}\nEXPORT_SYMBOL(build_skb);\n\n/**\n * build_skb_around - build a network buffer around provided skb\n * @skb: sk_buff provide by caller, must be memset cleared\n * @data: data buffer provided by caller\n * @frag_size: size of data, or 0 if head was kmalloced\n */\nstruct sk_buff *build_skb_around(struct sk_buff *skb,\n\t\t\t\t void *data, unsigned int frag_size)\n{\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\t__build_skb_around(skb, data, frag_size);\n\n\tif (frag_size) {\n\t\tskb->head_frag = 1;\n\t\tif (page_is_pfmemalloc(virt_to_head_page(data)))\n\t\t\tskb->pfmemalloc = 1;\n\t}\n\treturn skb;\n}\nEXPORT_SYMBOL(build_skb_around);\n\n/**\n * __napi_build_skb - build a network buffer\n * @data: data buffer provided by caller\n * @frag_size: size of data, or 0 if head was kmalloced\n *\n * Version of __build_skb() that uses NAPI percpu caches to obtain\n * skbuff_head instead of inplace allocation.\n *\n * Returns a new &sk_buff on success, %NULL on allocation failure.\n */\nstatic struct sk_buff *__napi_build_skb(void *data, unsigned int frag_size)\n{\n\tstruct sk_buff *skb;\n\n\tskb = napi_skb_cache_get();\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\t__build_skb_around(skb, data, frag_size);\n\n\treturn skb;\n}\n\n/**\n * napi_build_skb - build a network buffer\n * @data: data buffer provided by caller\n * @frag_size: size of data, or 0 if head was kmalloced\n *\n * Version of __napi_build_skb() that takes care of skb->head_frag\n * and skb->pfmemalloc when the data is a page or page fragment.\n *\n * Returns a new &sk_buff on success, %NULL on allocation failure.\n */\nstruct sk_buff *napi_build_skb(void *data, unsigned int frag_size)\n{\n\tstruct sk_buff *skb = __napi_build_skb(data, frag_size);\n\n\tif (likely(skb) && frag_size) {\n\t\tskb->head_frag = 1;\n\t\tskb_propagate_pfmemalloc(virt_to_head_page(data), skb);\n\t}\n\n\treturn skb;\n}\nEXPORT_SYMBOL(napi_build_skb);\n\n/*\n * kmalloc_reserve is a wrapper around kmalloc_node_track_caller that tells\n * the caller if emergency pfmemalloc reserves are being used. If it is and\n * the socket is later found to be SOCK_MEMALLOC then PFMEMALLOC reserves\n * may be used. Otherwise, the packet data may be discarded until enough\n * memory is free\n */\nstatic void *kmalloc_reserve(size_t size, gfp_t flags, int node,\n\t\t\t     bool *pfmemalloc)\n{\n\tvoid *obj;\n\tbool ret_pfmemalloc = false;\n\n\t/*\n\t * Try a regular allocation, when that fails and we're not entitled\n\t * to the reserves, fail.\n\t */\n\tobj = kmalloc_node_track_caller(size,\n\t\t\t\t\tflags | __GFP_NOMEMALLOC | __GFP_NOWARN,\n\t\t\t\t\tnode);\n\tif (obj || !(gfp_pfmemalloc_allowed(flags)))\n\t\tgoto out;\n\n\t/* Try again but now we are using pfmemalloc reserves */\n\tret_pfmemalloc = true;\n\tobj = kmalloc_node_track_caller(size, flags, node);\n\nout:\n\tif (pfmemalloc)\n\t\t*pfmemalloc = ret_pfmemalloc;\n\n\treturn obj;\n}\n\n/* \tAllocate a new skbuff. We do this ourselves so we can fill in a few\n *\t'private' fields and also do memory statistics to find all the\n *\t[BEEP] leaks.\n *\n */\n\n/**\n *\t__alloc_skb\t-\tallocate a network buffer\n *\t@size: size to allocate\n *\t@gfp_mask: allocation mask\n *\t@flags: If SKB_ALLOC_FCLONE is set, allocate from fclone cache\n *\t\tinstead of head cache and allocate a cloned (child) skb.\n *\t\tIf SKB_ALLOC_RX is set, __GFP_MEMALLOC will be used for\n *\t\tallocations in case the data is required for writeback\n *\t@node: numa node to allocate memory on\n *\n *\tAllocate a new &sk_buff. The returned buffer has no headroom and a\n *\ttail room of at least size bytes. The object has a reference count\n *\tof one. The return is the buffer. On a failure the return is %NULL.\n *\n *\tBuffers may only be allocated from interrupts using a @gfp_mask of\n *\t%GFP_ATOMIC.\n */\nstruct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,\n\t\t\t    int flags, int node)\n{\n\tstruct kmem_cache *cache;\n\tstruct sk_buff *skb;\n\tu8 *data;\n\tbool pfmemalloc;\n\n\tcache = (flags & SKB_ALLOC_FCLONE)\n\t\t? skbuff_fclone_cache : skbuff_head_cache;\n\n\tif (sk_memalloc_socks() && (flags & SKB_ALLOC_RX))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\t/* Get the HEAD */\n\tif ((flags & (SKB_ALLOC_FCLONE | SKB_ALLOC_NAPI)) == SKB_ALLOC_NAPI &&\n\t    likely(node == NUMA_NO_NODE || node == numa_mem_id()))\n\t\tskb = napi_skb_cache_get();\n\telse\n\t\tskb = kmem_cache_alloc_node(cache, gfp_mask & ~GFP_DMA, node);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\tprefetchw(skb);\n\n\t/* We do our best to align skb_shared_info on a separate cache\n\t * line. It usually works because kmalloc(X > SMP_CACHE_BYTES) gives\n\t * aligned memory blocks, unless SLUB/SLAB debug is enabled.\n\t * Both skb->head and skb_shared_info are cache line aligned.\n\t */\n\tsize = SKB_DATA_ALIGN(size);\n\tsize += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\tdata = kmalloc_reserve(size, gfp_mask, node, &pfmemalloc);\n\tif (unlikely(!data))\n\t\tgoto nodata;\n\t/* kmalloc(size) might give us more room than requested.\n\t * Put skb_shared_info exactly at the end of allocated zone,\n\t * to allow max possible filling before reallocation.\n\t */\n\tsize = SKB_WITH_OVERHEAD(ksize(data));\n\tprefetchw(data + size);\n\n\t/*\n\t * Only clear those fields we need to clear, not those that we will\n\t * actually initialise below. Hence, don't put any more fields after\n\t * the tail pointer in struct sk_buff!\n\t */\n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\t__build_skb_around(skb, data, 0);\n\tskb->pfmemalloc = pfmemalloc;\n\n\tif (flags & SKB_ALLOC_FCLONE) {\n\t\tstruct sk_buff_fclones *fclones;\n\n\t\tfclones = container_of(skb, struct sk_buff_fclones, skb1);\n\n\t\tskb->fclone = SKB_FCLONE_ORIG;\n\t\trefcount_set(&fclones->fclone_ref, 1);\n\n\t\tfclones->skb2.fclone = SKB_FCLONE_CLONE;\n\t}\n\n\treturn skb;\n\nnodata:\n\tkmem_cache_free(cache, skb);\n\treturn NULL;\n}\nEXPORT_SYMBOL(__alloc_skb);\n\n/**\n *\t__netdev_alloc_skb - allocate an skbuff for rx on a specific device\n *\t@dev: network device to receive on\n *\t@len: length to allocate\n *\t@gfp_mask: get_free_pages mask, passed to alloc_skb\n *\n *\tAllocate a new &sk_buff and assign it a usage count of one. The\n *\tbuffer has NET_SKB_PAD headroom built in. Users should allocate\n *\tthe headroom they think they need without accounting for the\n *\tbuilt in space. The built in space is used for optimisations.\n *\n *\t%NULL is returned if there is no free memory.\n */\nstruct sk_buff *__netdev_alloc_skb(struct net_device *dev, unsigned int len,\n\t\t\t\t   gfp_t gfp_mask)\n{\n\tstruct page_frag_cache *nc;\n\tstruct sk_buff *skb;\n\tbool pfmemalloc;\n\tvoid *data;\n\n\tlen += NET_SKB_PAD;\n\n\t/* If requested length is either too small or too big,\n\t * we use kmalloc() for skb->head allocation.\n\t */\n\tif (len <= SKB_WITH_OVERHEAD(1024) ||\n\t    len > SKB_WITH_OVERHEAD(PAGE_SIZE) ||\n\t    (gfp_mask & (__GFP_DIRECT_RECLAIM | GFP_DMA))) {\n\t\tskb = __alloc_skb(len, gfp_mask, SKB_ALLOC_RX, NUMA_NO_NODE);\n\t\tif (!skb)\n\t\t\tgoto skb_fail;\n\t\tgoto skb_success;\n\t}\n\n\tlen += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\tlen = SKB_DATA_ALIGN(len);\n\n\tif (sk_memalloc_socks())\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\tif (in_irq() || irqs_disabled()) {\n\t\tnc = this_cpu_ptr(&netdev_alloc_cache);\n\t\tdata = page_frag_alloc(nc, len, gfp_mask);\n\t\tpfmemalloc = nc->pfmemalloc;\n\t} else {\n\t\tlocal_bh_disable();\n\t\tnc = this_cpu_ptr(&napi_alloc_cache.page);\n\t\tdata = page_frag_alloc(nc, len, gfp_mask);\n\t\tpfmemalloc = nc->pfmemalloc;\n\t\tlocal_bh_enable();\n\t}\n\n\tif (unlikely(!data))\n\t\treturn NULL;\n\n\tskb = __build_skb(data, len);\n\tif (unlikely(!skb)) {\n\t\tskb_free_frag(data);\n\t\treturn NULL;\n\t}\n\n\tif (pfmemalloc)\n\t\tskb->pfmemalloc = 1;\n\tskb->head_frag = 1;\n\nskb_success:\n\tskb_reserve(skb, NET_SKB_PAD);\n\tskb->dev = dev;\n\nskb_fail:\n\treturn skb;\n}\nEXPORT_SYMBOL(__netdev_alloc_skb);\n\n/**\n *\t__napi_alloc_skb - allocate skbuff for rx in a specific NAPI instance\n *\t@napi: napi instance this buffer was allocated for\n *\t@len: length to allocate\n *\t@gfp_mask: get_free_pages mask, passed to alloc_skb and alloc_pages\n *\n *\tAllocate a new sk_buff for use in NAPI receive.  This buffer will\n *\tattempt to allocate the head from a special reserved region used\n *\tonly for NAPI Rx allocation.  By doing this we can save several\n *\tCPU cycles by avoiding having to disable and re-enable IRQs.\n *\n *\t%NULL is returned if there is no free memory.\n */\nstruct sk_buff *__napi_alloc_skb(struct napi_struct *napi, unsigned int len,\n\t\t\t\t gfp_t gfp_mask)\n{\n\tstruct napi_alloc_cache *nc;\n\tstruct sk_buff *skb;\n\tvoid *data;\n\n\tlen += NET_SKB_PAD + NET_IP_ALIGN;\n\n\t/* If requested length is either too small or too big,\n\t * we use kmalloc() for skb->head allocation.\n\t */\n\tif (len <= SKB_WITH_OVERHEAD(1024) ||\n\t    len > SKB_WITH_OVERHEAD(PAGE_SIZE) ||\n\t    (gfp_mask & (__GFP_DIRECT_RECLAIM | GFP_DMA))) {\n\t\tskb = __alloc_skb(len, gfp_mask, SKB_ALLOC_RX | SKB_ALLOC_NAPI,\n\t\t\t\t  NUMA_NO_NODE);\n\t\tif (!skb)\n\t\t\tgoto skb_fail;\n\t\tgoto skb_success;\n\t}\n\n\tnc = this_cpu_ptr(&napi_alloc_cache);\n\tlen += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\tlen = SKB_DATA_ALIGN(len);\n\n\tif (sk_memalloc_socks())\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\tdata = page_frag_alloc(&nc->page, len, gfp_mask);\n\tif (unlikely(!data))\n\t\treturn NULL;\n\n\tskb = __napi_build_skb(data, len);\n\tif (unlikely(!skb)) {\n\t\tskb_free_frag(data);\n\t\treturn NULL;\n\t}\n\n\tif (nc->page.pfmemalloc)\n\t\tskb->pfmemalloc = 1;\n\tskb->head_frag = 1;\n\nskb_success:\n\tskb_reserve(skb, NET_SKB_PAD + NET_IP_ALIGN);\n\tskb->dev = napi->dev;\n\nskb_fail:\n\treturn skb;\n}\nEXPORT_SYMBOL(__napi_alloc_skb);\n\nvoid skb_add_rx_frag(struct sk_buff *skb, int i, struct page *page, int off,\n\t\t     int size, unsigned int truesize)\n{\n\tskb_fill_page_desc(skb, i, page, off, size);\n\tskb->len += size;\n\tskb->data_len += size;\n\tskb->truesize += truesize;\n}\nEXPORT_SYMBOL(skb_add_rx_frag);\n\nvoid skb_coalesce_rx_frag(struct sk_buff *skb, int i, int size,\n\t\t\t  unsigned int truesize)\n{\n\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\tskb_frag_size_add(frag, size);\n\tskb->len += size;\n\tskb->data_len += size;\n\tskb->truesize += truesize;\n}\nEXPORT_SYMBOL(skb_coalesce_rx_frag);\n\nstatic void skb_drop_list(struct sk_buff **listp)\n{\n\tkfree_skb_list(*listp);\n\t*listp = NULL;\n}\n\nstatic inline void skb_drop_fraglist(struct sk_buff *skb)\n{\n\tskb_drop_list(&skb_shinfo(skb)->frag_list);\n}\n\nstatic void skb_clone_fraglist(struct sk_buff *skb)\n{\n\tstruct sk_buff *list;\n\n\tskb_walk_frags(skb, list)\n\t\tskb_get(list);\n}\n\nstatic void skb_free_head(struct sk_buff *skb)\n{\n\tunsigned char *head = skb->head;\n\n\tif (skb->head_frag)\n\t\tskb_free_frag(head);\n\telse\n\t\tkfree(head);\n}\n\nstatic void skb_release_data(struct sk_buff *skb)\n{\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\tint i;\n\n\tif (skb->cloned &&\n\t    atomic_sub_return(skb->nohdr ? (1 << SKB_DATAREF_SHIFT) + 1 : 1,\n\t\t\t      &shinfo->dataref))\n\t\treturn;\n\n\tskb_zcopy_clear(skb, true);\n\n\tfor (i = 0; i < shinfo->nr_frags; i++)\n\t\t__skb_frag_unref(&shinfo->frags[i]);\n\n\tif (shinfo->frag_list)\n\t\tkfree_skb_list(shinfo->frag_list);\n\n\tskb_free_head(skb);\n}\n\n/*\n *\tFree an skbuff by memory without cleaning the state.\n */\nstatic void kfree_skbmem(struct sk_buff *skb)\n{\n\tstruct sk_buff_fclones *fclones;\n\n\tswitch (skb->fclone) {\n\tcase SKB_FCLONE_UNAVAILABLE:\n\t\tkmem_cache_free(skbuff_head_cache, skb);\n\t\treturn;\n\n\tcase SKB_FCLONE_ORIG:\n\t\tfclones = container_of(skb, struct sk_buff_fclones, skb1);\n\n\t\t/* We usually free the clone (TX completion) before original skb\n\t\t * This test would have no chance to be true for the clone,\n\t\t * while here, branch prediction will be good.\n\t\t */\n\t\tif (refcount_read(&fclones->fclone_ref) == 1)\n\t\t\tgoto fastpath;\n\t\tbreak;\n\n\tdefault: /* SKB_FCLONE_CLONE */\n\t\tfclones = container_of(skb, struct sk_buff_fclones, skb2);\n\t\tbreak;\n\t}\n\tif (!refcount_dec_and_test(&fclones->fclone_ref))\n\t\treturn;\nfastpath:\n\tkmem_cache_free(skbuff_fclone_cache, fclones);\n}\n\nvoid skb_release_head_state(struct sk_buff *skb)\n{\n\tskb_dst_drop(skb);\n\tif (skb->destructor) {\n\t\tWARN_ON(in_irq());\n\t\tskb->destructor(skb);\n\t}\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\tnf_conntrack_put(skb_nfct(skb));\n#endif\n\tskb_ext_put(skb);\n}\n\n/* Free everything but the sk_buff shell. */\nstatic void skb_release_all(struct sk_buff *skb)\n{\n\tskb_release_head_state(skb);\n\tif (likely(skb->head))\n\t\tskb_release_data(skb);\n}\n\n/**\n *\t__kfree_skb - private function\n *\t@skb: buffer\n *\n *\tFree an sk_buff. Release anything attached to the buffer.\n *\tClean the state. This is an internal helper function. Users should\n *\talways call kfree_skb\n */\n\nvoid __kfree_skb(struct sk_buff *skb)\n{\n\tskb_release_all(skb);\n\tkfree_skbmem(skb);\n}\nEXPORT_SYMBOL(__kfree_skb);\n\n/**\n *\tkfree_skb - free an sk_buff\n *\t@skb: buffer to free\n *\n *\tDrop a reference to the buffer and free it if the usage count has\n *\thit zero.\n */\nvoid kfree_skb(struct sk_buff *skb)\n{\n\tif (!skb_unref(skb))\n\t\treturn;\n\n\ttrace_kfree_skb(skb, __builtin_return_address(0));\n\t__kfree_skb(skb);\n}\nEXPORT_SYMBOL(kfree_skb);\n\nvoid kfree_skb_list(struct sk_buff *segs)\n{\n\twhile (segs) {\n\t\tstruct sk_buff *next = segs->next;\n\n\t\tkfree_skb(segs);\n\t\tsegs = next;\n\t}\n}\nEXPORT_SYMBOL(kfree_skb_list);\n\n/* Dump skb information and contents.\n *\n * Must only be called from net_ratelimit()-ed paths.\n *\n * Dumps whole packets if full_pkt, only headers otherwise.\n */\nvoid skb_dump(const char *level, const struct sk_buff *skb, bool full_pkt)\n{\n\tstruct skb_shared_info *sh = skb_shinfo(skb);\n\tstruct net_device *dev = skb->dev;\n\tstruct sock *sk = skb->sk;\n\tstruct sk_buff *list_skb;\n\tbool has_mac, has_trans;\n\tint headroom, tailroom;\n\tint i, len, seg_len;\n\n\tif (full_pkt)\n\t\tlen = skb->len;\n\telse\n\t\tlen = min_t(int, skb->len, MAX_HEADER + 128);\n\n\theadroom = skb_headroom(skb);\n\ttailroom = skb_tailroom(skb);\n\n\thas_mac = skb_mac_header_was_set(skb);\n\thas_trans = skb_transport_header_was_set(skb);\n\n\tprintk(\"%sskb len=%u headroom=%u headlen=%u tailroom=%u\\n\"\n\t       \"mac=(%d,%d) net=(%d,%d) trans=%d\\n\"\n\t       \"shinfo(txflags=%u nr_frags=%u gso(size=%hu type=%u segs=%hu))\\n\"\n\t       \"csum(0x%x ip_summed=%u complete_sw=%u valid=%u level=%u)\\n\"\n\t       \"hash(0x%x sw=%u l4=%u) proto=0x%04x pkttype=%u iif=%d\\n\",\n\t       level, skb->len, headroom, skb_headlen(skb), tailroom,\n\t       has_mac ? skb->mac_header : -1,\n\t       has_mac ? skb_mac_header_len(skb) : -1,\n\t       skb->network_header,\n\t       has_trans ? skb_network_header_len(skb) : -1,\n\t       has_trans ? skb->transport_header : -1,\n\t       sh->tx_flags, sh->nr_frags,\n\t       sh->gso_size, sh->gso_type, sh->gso_segs,\n\t       skb->csum, skb->ip_summed, skb->csum_complete_sw,\n\t       skb->csum_valid, skb->csum_level,\n\t       skb->hash, skb->sw_hash, skb->l4_hash,\n\t       ntohs(skb->protocol), skb->pkt_type, skb->skb_iif);\n\n\tif (dev)\n\t\tprintk(\"%sdev name=%s feat=0x%pNF\\n\",\n\t\t       level, dev->name, &dev->features);\n\tif (sk)\n\t\tprintk(\"%ssk family=%hu type=%u proto=%u\\n\",\n\t\t       level, sk->sk_family, sk->sk_type, sk->sk_protocol);\n\n\tif (full_pkt && headroom)\n\t\tprint_hex_dump(level, \"skb headroom: \", DUMP_PREFIX_OFFSET,\n\t\t\t       16, 1, skb->head, headroom, false);\n\n\tseg_len = min_t(int, skb_headlen(skb), len);\n\tif (seg_len)\n\t\tprint_hex_dump(level, \"skb linear:   \", DUMP_PREFIX_OFFSET,\n\t\t\t       16, 1, skb->data, seg_len, false);\n\tlen -= seg_len;\n\n\tif (full_pkt && tailroom)\n\t\tprint_hex_dump(level, \"skb tailroom: \", DUMP_PREFIX_OFFSET,\n\t\t\t       16, 1, skb_tail_pointer(skb), tailroom, false);\n\n\tfor (i = 0; len && i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\tu32 p_off, p_len, copied;\n\t\tstruct page *p;\n\t\tu8 *vaddr;\n\n\t\tskb_frag_foreach_page(frag, skb_frag_off(frag),\n\t\t\t\t      skb_frag_size(frag), p, p_off, p_len,\n\t\t\t\t      copied) {\n\t\t\tseg_len = min_t(int, p_len, len);\n\t\t\tvaddr = kmap_atomic(p);\n\t\t\tprint_hex_dump(level, \"skb frag:     \",\n\t\t\t\t       DUMP_PREFIX_OFFSET,\n\t\t\t\t       16, 1, vaddr + p_off, seg_len, false);\n\t\t\tkunmap_atomic(vaddr);\n\t\t\tlen -= seg_len;\n\t\t\tif (!len)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (full_pkt && skb_has_frag_list(skb)) {\n\t\tprintk(\"skb fraglist:\\n\");\n\t\tskb_walk_frags(skb, list_skb)\n\t\t\tskb_dump(level, list_skb, true);\n\t}\n}\nEXPORT_SYMBOL(skb_dump);\n\n/**\n *\tskb_tx_error - report an sk_buff xmit error\n *\t@skb: buffer that triggered an error\n *\n *\tReport xmit error if a device callback is tracking this skb.\n *\tskb must be freed afterwards.\n */\nvoid skb_tx_error(struct sk_buff *skb)\n{\n\tskb_zcopy_clear(skb, true);\n}\nEXPORT_SYMBOL(skb_tx_error);\n\n#ifdef CONFIG_TRACEPOINTS\n/**\n *\tconsume_skb - free an skbuff\n *\t@skb: buffer to free\n *\n *\tDrop a ref to the buffer and free it if the usage count has hit zero\n *\tFunctions identically to kfree_skb, but kfree_skb assumes that the frame\n *\tis being dropped after a failure and notes that\n */\nvoid consume_skb(struct sk_buff *skb)\n{\n\tif (!skb_unref(skb))\n\t\treturn;\n\n\ttrace_consume_skb(skb);\n\t__kfree_skb(skb);\n}\nEXPORT_SYMBOL(consume_skb);\n#endif\n\n/**\n *\t__consume_stateless_skb - free an skbuff, assuming it is stateless\n *\t@skb: buffer to free\n *\n *\tAlike consume_skb(), but this variant assumes that this is the last\n *\tskb reference and all the head states have been already dropped\n */\nvoid __consume_stateless_skb(struct sk_buff *skb)\n{\n\ttrace_consume_skb(skb);\n\tskb_release_data(skb);\n\tkfree_skbmem(skb);\n}\n\nstatic void napi_skb_cache_put(struct sk_buff *skb)\n{\n\tstruct napi_alloc_cache *nc = this_cpu_ptr(&napi_alloc_cache);\n\tu32 i;\n\n\tkasan_poison_object_data(skbuff_head_cache, skb);\n\tnc->skb_cache[nc->skb_count++] = skb;\n\n\tif (unlikely(nc->skb_count == NAPI_SKB_CACHE_SIZE)) {\n\t\tfor (i = NAPI_SKB_CACHE_HALF; i < NAPI_SKB_CACHE_SIZE; i++)\n\t\t\tkasan_unpoison_object_data(skbuff_head_cache,\n\t\t\t\t\t\t   nc->skb_cache[i]);\n\n\t\tkmem_cache_free_bulk(skbuff_head_cache, NAPI_SKB_CACHE_HALF,\n\t\t\t\t     nc->skb_cache + NAPI_SKB_CACHE_HALF);\n\t\tnc->skb_count = NAPI_SKB_CACHE_HALF;\n\t}\n}\n\nvoid __kfree_skb_defer(struct sk_buff *skb)\n{\n\tskb_release_all(skb);\n\tnapi_skb_cache_put(skb);\n}\n\nvoid napi_skb_free_stolen_head(struct sk_buff *skb)\n{\n\tskb_dst_drop(skb);\n\tskb_ext_put(skb);\n\tnapi_skb_cache_put(skb);\n}\n\nvoid napi_consume_skb(struct sk_buff *skb, int budget)\n{\n\t/* Zero budget indicate non-NAPI context called us, like netpoll */\n\tif (unlikely(!budget)) {\n\t\tdev_consume_skb_any(skb);\n\t\treturn;\n\t}\n\n\tlockdep_assert_in_softirq();\n\n\tif (!skb_unref(skb))\n\t\treturn;\n\n\t/* if reaching here SKB is ready to free */\n\ttrace_consume_skb(skb);\n\n\t/* if SKB is a clone, don't handle this case */\n\tif (skb->fclone != SKB_FCLONE_UNAVAILABLE) {\n\t\t__kfree_skb(skb);\n\t\treturn;\n\t}\n\n\tskb_release_all(skb);\n\tnapi_skb_cache_put(skb);\n}\nEXPORT_SYMBOL(napi_consume_skb);\n\n/* Make sure a field is enclosed inside headers_start/headers_end section */\n#define CHECK_SKB_FIELD(field) \\\n\tBUILD_BUG_ON(offsetof(struct sk_buff, field) <\t\t\\\n\t\t     offsetof(struct sk_buff, headers_start));\t\\\n\tBUILD_BUG_ON(offsetof(struct sk_buff, field) >\t\t\\\n\t\t     offsetof(struct sk_buff, headers_end));\t\\\n\nstatic void __copy_skb_header(struct sk_buff *new, const struct sk_buff *old)\n{\n\tnew->tstamp\t\t= old->tstamp;\n\t/* We do not copy old->sk */\n\tnew->dev\t\t= old->dev;\n\tmemcpy(new->cb, old->cb, sizeof(old->cb));\n\tskb_dst_copy(new, old);\n\t__skb_ext_copy(new, old);\n\t__nf_copy(new, old, false);\n\n\t/* Note : this field could be in headers_start/headers_end section\n\t * It is not yet because we do not want to have a 16 bit hole\n\t */\n\tnew->queue_mapping = old->queue_mapping;\n\n\tmemcpy(&new->headers_start, &old->headers_start,\n\t       offsetof(struct sk_buff, headers_end) -\n\t       offsetof(struct sk_buff, headers_start));\n\tCHECK_SKB_FIELD(protocol);\n\tCHECK_SKB_FIELD(csum);\n\tCHECK_SKB_FIELD(hash);\n\tCHECK_SKB_FIELD(priority);\n\tCHECK_SKB_FIELD(skb_iif);\n\tCHECK_SKB_FIELD(vlan_proto);\n\tCHECK_SKB_FIELD(vlan_tci);\n\tCHECK_SKB_FIELD(transport_header);\n\tCHECK_SKB_FIELD(network_header);\n\tCHECK_SKB_FIELD(mac_header);\n\tCHECK_SKB_FIELD(inner_protocol);\n\tCHECK_SKB_FIELD(inner_transport_header);\n\tCHECK_SKB_FIELD(inner_network_header);\n\tCHECK_SKB_FIELD(inner_mac_header);\n\tCHECK_SKB_FIELD(mark);\n#ifdef CONFIG_NETWORK_SECMARK\n\tCHECK_SKB_FIELD(secmark);\n#endif\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tCHECK_SKB_FIELD(napi_id);\n#endif\n#ifdef CONFIG_XPS\n\tCHECK_SKB_FIELD(sender_cpu);\n#endif\n#ifdef CONFIG_NET_SCHED\n\tCHECK_SKB_FIELD(tc_index);\n#endif\n\n}\n\n/*\n * You should not add any new code to this function.  Add it to\n * __copy_skb_header above instead.\n */\nstatic struct sk_buff *__skb_clone(struct sk_buff *n, struct sk_buff *skb)\n{\n#define C(x) n->x = skb->x\n\n\tn->next = n->prev = NULL;\n\tn->sk = NULL;\n\t__copy_skb_header(n, skb);\n\n\tC(len);\n\tC(data_len);\n\tC(mac_len);\n\tn->hdr_len = skb->nohdr ? skb_headroom(skb) : skb->hdr_len;\n\tn->cloned = 1;\n\tn->nohdr = 0;\n\tn->peeked = 0;\n\tC(pfmemalloc);\n\tn->destructor = NULL;\n\tC(tail);\n\tC(end);\n\tC(head);\n\tC(head_frag);\n\tC(data);\n\tC(truesize);\n\trefcount_set(&n->users, 1);\n\n\tatomic_inc(&(skb_shinfo(skb)->dataref));\n\tskb->cloned = 1;\n\n\treturn n;\n#undef C\n}\n\n/**\n * alloc_skb_for_msg() - allocate sk_buff to wrap frag list forming a msg\n * @first: first sk_buff of the msg\n */\nstruct sk_buff *alloc_skb_for_msg(struct sk_buff *first)\n{\n\tstruct sk_buff *n;\n\n\tn = alloc_skb(0, GFP_ATOMIC);\n\tif (!n)\n\t\treturn NULL;\n\n\tn->len = first->len;\n\tn->data_len = first->len;\n\tn->truesize = first->truesize;\n\n\tskb_shinfo(n)->frag_list = first;\n\n\t__copy_skb_header(n, first);\n\tn->destructor = NULL;\n\n\treturn n;\n}\nEXPORT_SYMBOL_GPL(alloc_skb_for_msg);\n\n/**\n *\tskb_morph\t-\tmorph one skb into another\n *\t@dst: the skb to receive the contents\n *\t@src: the skb to supply the contents\n *\n *\tThis is identical to skb_clone except that the target skb is\n *\tsupplied by the user.\n *\n *\tThe target skb is returned upon exit.\n */\nstruct sk_buff *skb_morph(struct sk_buff *dst, struct sk_buff *src)\n{\n\tskb_release_all(dst);\n\treturn __skb_clone(dst, src);\n}\nEXPORT_SYMBOL_GPL(skb_morph);\n\nint mm_account_pinned_pages(struct mmpin *mmp, size_t size)\n{\n\tunsigned long max_pg, num_pg, new_pg, old_pg;\n\tstruct user_struct *user;\n\n\tif (capable(CAP_IPC_LOCK) || !size)\n\t\treturn 0;\n\n\tnum_pg = (size >> PAGE_SHIFT) + 2;\t/* worst case */\n\tmax_pg = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\n\tuser = mmp->user ? : current_user();\n\n\tdo {\n\t\told_pg = atomic_long_read(&user->locked_vm);\n\t\tnew_pg = old_pg + num_pg;\n\t\tif (new_pg > max_pg)\n\t\t\treturn -ENOBUFS;\n\t} while (atomic_long_cmpxchg(&user->locked_vm, old_pg, new_pg) !=\n\t\t old_pg);\n\n\tif (!mmp->user) {\n\t\tmmp->user = get_uid(user);\n\t\tmmp->num_pg = num_pg;\n\t} else {\n\t\tmmp->num_pg += num_pg;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(mm_account_pinned_pages);\n\nvoid mm_unaccount_pinned_pages(struct mmpin *mmp)\n{\n\tif (mmp->user) {\n\t\tatomic_long_sub(mmp->num_pg, &mmp->user->locked_vm);\n\t\tfree_uid(mmp->user);\n\t}\n}\nEXPORT_SYMBOL_GPL(mm_unaccount_pinned_pages);\n\nstruct ubuf_info *msg_zerocopy_alloc(struct sock *sk, size_t size)\n{\n\tstruct ubuf_info *uarg;\n\tstruct sk_buff *skb;\n\n\tWARN_ON_ONCE(!in_task());\n\n\tskb = sock_omalloc(sk, 0, GFP_KERNEL);\n\tif (!skb)\n\t\treturn NULL;\n\n\tBUILD_BUG_ON(sizeof(*uarg) > sizeof(skb->cb));\n\tuarg = (void *)skb->cb;\n\tuarg->mmp.user = NULL;\n\n\tif (mm_account_pinned_pages(&uarg->mmp, size)) {\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\t}\n\n\tuarg->callback = msg_zerocopy_callback;\n\tuarg->id = ((u32)atomic_inc_return(&sk->sk_zckey)) - 1;\n\tuarg->len = 1;\n\tuarg->bytelen = size;\n\tuarg->zerocopy = 1;\n\tuarg->flags = SKBFL_ZEROCOPY_FRAG;\n\trefcount_set(&uarg->refcnt, 1);\n\tsock_hold(sk);\n\n\treturn uarg;\n}\nEXPORT_SYMBOL_GPL(msg_zerocopy_alloc);\n\nstatic inline struct sk_buff *skb_from_uarg(struct ubuf_info *uarg)\n{\n\treturn container_of((void *)uarg, struct sk_buff, cb);\n}\n\nstruct ubuf_info *msg_zerocopy_realloc(struct sock *sk, size_t size,\n\t\t\t\t       struct ubuf_info *uarg)\n{\n\tif (uarg) {\n\t\tconst u32 byte_limit = 1 << 19;\t\t/* limit to a few TSO */\n\t\tu32 bytelen, next;\n\n\t\t/* realloc only when socket is locked (TCP, UDP cork),\n\t\t * so uarg->len and sk_zckey access is serialized\n\t\t */\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\tWARN_ON_ONCE(1);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tbytelen = uarg->bytelen + size;\n\t\tif (uarg->len == USHRT_MAX - 1 || bytelen > byte_limit) {\n\t\t\t/* TCP can create new skb to attach new uarg */\n\t\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\t\tgoto new_alloc;\n\t\t\treturn NULL;\n\t\t}\n\n\t\tnext = (u32)atomic_read(&sk->sk_zckey);\n\t\tif ((u32)(uarg->id + uarg->len) == next) {\n\t\t\tif (mm_account_pinned_pages(&uarg->mmp, size))\n\t\t\t\treturn NULL;\n\t\t\tuarg->len++;\n\t\t\tuarg->bytelen = bytelen;\n\t\t\tatomic_set(&sk->sk_zckey, ++next);\n\n\t\t\t/* no extra ref when appending to datagram (MSG_MORE) */\n\t\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\t\tnet_zcopy_get(uarg);\n\n\t\t\treturn uarg;\n\t\t}\n\t}\n\nnew_alloc:\n\treturn msg_zerocopy_alloc(sk, size);\n}\nEXPORT_SYMBOL_GPL(msg_zerocopy_realloc);\n\nstatic bool skb_zerocopy_notify_extend(struct sk_buff *skb, u32 lo, u16 len)\n{\n\tstruct sock_exterr_skb *serr = SKB_EXT_ERR(skb);\n\tu32 old_lo, old_hi;\n\tu64 sum_len;\n\n\told_lo = serr->ee.ee_info;\n\told_hi = serr->ee.ee_data;\n\tsum_len = old_hi - old_lo + 1ULL + len;\n\n\tif (sum_len >= (1ULL << 32))\n\t\treturn false;\n\n\tif (lo != old_hi + 1)\n\t\treturn false;\n\n\tserr->ee.ee_data += len;\n\treturn true;\n}\n\nstatic void __msg_zerocopy_callback(struct ubuf_info *uarg)\n{\n\tstruct sk_buff *tail, *skb = skb_from_uarg(uarg);\n\tstruct sock_exterr_skb *serr;\n\tstruct sock *sk = skb->sk;\n\tstruct sk_buff_head *q;\n\tunsigned long flags;\n\tu32 lo, hi;\n\tu16 len;\n\n\tmm_unaccount_pinned_pages(&uarg->mmp);\n\n\t/* if !len, there was only 1 call, and it was aborted\n\t * so do not queue a completion notification\n\t */\n\tif (!uarg->len || sock_flag(sk, SOCK_DEAD))\n\t\tgoto release;\n\n\tlen = uarg->len;\n\tlo = uarg->id;\n\thi = uarg->id + len - 1;\n\n\tserr = SKB_EXT_ERR(skb);\n\tmemset(serr, 0, sizeof(*serr));\n\tserr->ee.ee_errno = 0;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_ZEROCOPY;\n\tserr->ee.ee_data = hi;\n\tserr->ee.ee_info = lo;\n\tif (!uarg->zerocopy)\n\t\tserr->ee.ee_code |= SO_EE_CODE_ZEROCOPY_COPIED;\n\n\tq = &sk->sk_error_queue;\n\tspin_lock_irqsave(&q->lock, flags);\n\ttail = skb_peek_tail(q);\n\tif (!tail || SKB_EXT_ERR(tail)->ee.ee_origin != SO_EE_ORIGIN_ZEROCOPY ||\n\t    !skb_zerocopy_notify_extend(tail, lo, len)) {\n\t\t__skb_queue_tail(q, skb);\n\t\tskb = NULL;\n\t}\n\tspin_unlock_irqrestore(&q->lock, flags);\n\n\tsk->sk_error_report(sk);\n\nrelease:\n\tconsume_skb(skb);\n\tsock_put(sk);\n}\n\nvoid msg_zerocopy_callback(struct sk_buff *skb, struct ubuf_info *uarg,\n\t\t\t   bool success)\n{\n\tuarg->zerocopy = uarg->zerocopy & success;\n\n\tif (refcount_dec_and_test(&uarg->refcnt))\n\t\t__msg_zerocopy_callback(uarg);\n}\nEXPORT_SYMBOL_GPL(msg_zerocopy_callback);\n\nvoid msg_zerocopy_put_abort(struct ubuf_info *uarg, bool have_uref)\n{\n\tstruct sock *sk = skb_from_uarg(uarg)->sk;\n\n\tatomic_dec(&sk->sk_zckey);\n\tuarg->len--;\n\n\tif (have_uref)\n\t\tmsg_zerocopy_callback(NULL, uarg, true);\n}\nEXPORT_SYMBOL_GPL(msg_zerocopy_put_abort);\n\nint skb_zerocopy_iter_dgram(struct sk_buff *skb, struct msghdr *msg, int len)\n{\n\treturn __zerocopy_sg_from_iter(skb->sk, skb, &msg->msg_iter, len);\n}\nEXPORT_SYMBOL_GPL(skb_zerocopy_iter_dgram);\n\nint skb_zerocopy_iter_stream(struct sock *sk, struct sk_buff *skb,\n\t\t\t     struct msghdr *msg, int len,\n\t\t\t     struct ubuf_info *uarg)\n{\n\tstruct ubuf_info *orig_uarg = skb_zcopy(skb);\n\tstruct iov_iter orig_iter = msg->msg_iter;\n\tint err, orig_len = skb->len;\n\n\t/* An skb can only point to one uarg. This edge case happens when\n\t * TCP appends to an skb, but zerocopy_realloc triggered a new alloc.\n\t */\n\tif (orig_uarg && uarg != orig_uarg)\n\t\treturn -EEXIST;\n\n\terr = __zerocopy_sg_from_iter(sk, skb, &msg->msg_iter, len);\n\tif (err == -EFAULT || (err == -EMSGSIZE && skb->len == orig_len)) {\n\t\tstruct sock *save_sk = skb->sk;\n\n\t\t/* Streams do not free skb on error. Reset to prev state. */\n\t\tmsg->msg_iter = orig_iter;\n\t\tskb->sk = sk;\n\t\t___pskb_trim(skb, orig_len);\n\t\tskb->sk = save_sk;\n\t\treturn err;\n\t}\n\n\tskb_zcopy_set(skb, uarg, NULL);\n\treturn skb->len - orig_len;\n}\nEXPORT_SYMBOL_GPL(skb_zerocopy_iter_stream);\n\nstatic int skb_zerocopy_clone(struct sk_buff *nskb, struct sk_buff *orig,\n\t\t\t      gfp_t gfp_mask)\n{\n\tif (skb_zcopy(orig)) {\n\t\tif (skb_zcopy(nskb)) {\n\t\t\t/* !gfp_mask callers are verified to !skb_zcopy(nskb) */\n\t\t\tif (!gfp_mask) {\n\t\t\t\tWARN_ON_ONCE(1);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\tif (skb_uarg(nskb) == skb_uarg(orig))\n\t\t\t\treturn 0;\n\t\t\tif (skb_copy_ubufs(nskb, GFP_ATOMIC))\n\t\t\t\treturn -EIO;\n\t\t}\n\t\tskb_zcopy_set(nskb, skb_uarg(orig), NULL);\n\t}\n\treturn 0;\n}\n\n/**\n *\tskb_copy_ubufs\t-\tcopy userspace skb frags buffers to kernel\n *\t@skb: the skb to modify\n *\t@gfp_mask: allocation priority\n *\n *\tThis must be called on skb with SKBFL_ZEROCOPY_ENABLE.\n *\tIt will copy all frags into kernel and drop the reference\n *\tto userspace pages.\n *\n *\tIf this function is called from an interrupt gfp_mask() must be\n *\t%GFP_ATOMIC.\n *\n *\tReturns 0 on success or a negative error code on failure\n *\tto allocate kernel memory to copy to.\n */\nint skb_copy_ubufs(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tint num_frags = skb_shinfo(skb)->nr_frags;\n\tstruct page *page, *head = NULL;\n\tint i, new_frags;\n\tu32 d_off;\n\n\tif (skb_shared(skb) || skb_unclone(skb, gfp_mask))\n\t\treturn -EINVAL;\n\n\tif (!num_frags)\n\t\tgoto release;\n\n\tnew_frags = (__skb_pagelen(skb) + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tfor (i = 0; i < new_frags; i++) {\n\t\tpage = alloc_page(gfp_mask);\n\t\tif (!page) {\n\t\t\twhile (head) {\n\t\t\t\tstruct page *next = (struct page *)page_private(head);\n\t\t\t\tput_page(head);\n\t\t\t\thead = next;\n\t\t\t}\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tset_page_private(page, (unsigned long)head);\n\t\thead = page;\n\t}\n\n\tpage = head;\n\td_off = 0;\n\tfor (i = 0; i < num_frags; i++) {\n\t\tskb_frag_t *f = &skb_shinfo(skb)->frags[i];\n\t\tu32 p_off, p_len, copied;\n\t\tstruct page *p;\n\t\tu8 *vaddr;\n\n\t\tskb_frag_foreach_page(f, skb_frag_off(f), skb_frag_size(f),\n\t\t\t\t      p, p_off, p_len, copied) {\n\t\t\tu32 copy, done = 0;\n\t\t\tvaddr = kmap_atomic(p);\n\n\t\t\twhile (done < p_len) {\n\t\t\t\tif (d_off == PAGE_SIZE) {\n\t\t\t\t\td_off = 0;\n\t\t\t\t\tpage = (struct page *)page_private(page);\n\t\t\t\t}\n\t\t\t\tcopy = min_t(u32, PAGE_SIZE - d_off, p_len - done);\n\t\t\t\tmemcpy(page_address(page) + d_off,\n\t\t\t\t       vaddr + p_off + done, copy);\n\t\t\t\tdone += copy;\n\t\t\t\td_off += copy;\n\t\t\t}\n\t\t\tkunmap_atomic(vaddr);\n\t\t}\n\t}\n\n\t/* skb frags release userspace buffers */\n\tfor (i = 0; i < num_frags; i++)\n\t\tskb_frag_unref(skb, i);\n\n\t/* skb frags point to kernel buffers */\n\tfor (i = 0; i < new_frags - 1; i++) {\n\t\t__skb_fill_page_desc(skb, i, head, 0, PAGE_SIZE);\n\t\thead = (struct page *)page_private(head);\n\t}\n\t__skb_fill_page_desc(skb, new_frags - 1, head, 0, d_off);\n\tskb_shinfo(skb)->nr_frags = new_frags;\n\nrelease:\n\tskb_zcopy_clear(skb, false);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_copy_ubufs);\n\n/**\n *\tskb_clone\t-\tduplicate an sk_buff\n *\t@skb: buffer to clone\n *\t@gfp_mask: allocation priority\n *\n *\tDuplicate an &sk_buff. The new one is not owned by a socket. Both\n *\tcopies share the same packet data but not structure. The new\n *\tbuffer has a reference count of 1. If the allocation fails the\n *\tfunction returns %NULL otherwise the new buffer is returned.\n *\n *\tIf this function is called from an interrupt gfp_mask() must be\n *\t%GFP_ATOMIC.\n */\n\nstruct sk_buff *skb_clone(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tstruct sk_buff_fclones *fclones = container_of(skb,\n\t\t\t\t\t\t       struct sk_buff_fclones,\n\t\t\t\t\t\t       skb1);\n\tstruct sk_buff *n;\n\n\tif (skb_orphan_frags(skb, gfp_mask))\n\t\treturn NULL;\n\n\tif (skb->fclone == SKB_FCLONE_ORIG &&\n\t    refcount_read(&fclones->fclone_ref) == 1) {\n\t\tn = &fclones->skb2;\n\t\trefcount_set(&fclones->fclone_ref, 2);\n\t} else {\n\t\tif (skb_pfmemalloc(skb))\n\t\t\tgfp_mask |= __GFP_MEMALLOC;\n\n\t\tn = kmem_cache_alloc(skbuff_head_cache, gfp_mask);\n\t\tif (!n)\n\t\t\treturn NULL;\n\n\t\tn->fclone = SKB_FCLONE_UNAVAILABLE;\n\t}\n\n\treturn __skb_clone(n, skb);\n}\nEXPORT_SYMBOL(skb_clone);\n\nvoid skb_headers_offset_update(struct sk_buff *skb, int off)\n{\n\t/* Only adjust this if it actually is csum_start rather than csum */\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tskb->csum_start += off;\n\t/* {transport,network,mac}_header and tail are relative to skb->head */\n\tskb->transport_header += off;\n\tskb->network_header   += off;\n\tif (skb_mac_header_was_set(skb))\n\t\tskb->mac_header += off;\n\tskb->inner_transport_header += off;\n\tskb->inner_network_header += off;\n\tskb->inner_mac_header += off;\n}\nEXPORT_SYMBOL(skb_headers_offset_update);\n\nvoid skb_copy_header(struct sk_buff *new, const struct sk_buff *old)\n{\n\t__copy_skb_header(new, old);\n\n\tskb_shinfo(new)->gso_size = skb_shinfo(old)->gso_size;\n\tskb_shinfo(new)->gso_segs = skb_shinfo(old)->gso_segs;\n\tskb_shinfo(new)->gso_type = skb_shinfo(old)->gso_type;\n}\nEXPORT_SYMBOL(skb_copy_header);\n\nstatic inline int skb_alloc_rx_flag(const struct sk_buff *skb)\n{\n\tif (skb_pfmemalloc(skb))\n\t\treturn SKB_ALLOC_RX;\n\treturn 0;\n}\n\n/**\n *\tskb_copy\t-\tcreate private copy of an sk_buff\n *\t@skb: buffer to copy\n *\t@gfp_mask: allocation priority\n *\n *\tMake a copy of both an &sk_buff and its data. This is used when the\n *\tcaller wishes to modify the data and needs a private copy of the\n *\tdata to alter. Returns %NULL on failure or the pointer to the buffer\n *\ton success. The returned buffer has a reference count of 1.\n *\n *\tAs by-product this function converts non-linear &sk_buff to linear\n *\tone, so that &sk_buff becomes completely private and caller is allowed\n *\tto modify all the data of returned buffer. This means that this\n *\tfunction is not recommended for use in circumstances when only\n *\theader is going to be modified. Use pskb_copy() instead.\n */\n\nstruct sk_buff *skb_copy(const struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tint headerlen = skb_headroom(skb);\n\tunsigned int size = skb_end_offset(skb) + skb->data_len;\n\tstruct sk_buff *n = __alloc_skb(size, gfp_mask,\n\t\t\t\t\tskb_alloc_rx_flag(skb), NUMA_NO_NODE);\n\n\tif (!n)\n\t\treturn NULL;\n\n\t/* Set the data pointer */\n\tskb_reserve(n, headerlen);\n\t/* Set the tail pointer and length */\n\tskb_put(n, skb->len);\n\n\tBUG_ON(skb_copy_bits(skb, -headerlen, n->head, headerlen + skb->len));\n\n\tskb_copy_header(n, skb);\n\treturn n;\n}\nEXPORT_SYMBOL(skb_copy);\n\n/**\n *\t__pskb_copy_fclone\t-  create copy of an sk_buff with private head.\n *\t@skb: buffer to copy\n *\t@headroom: headroom of new skb\n *\t@gfp_mask: allocation priority\n *\t@fclone: if true allocate the copy of the skb from the fclone\n *\tcache instead of the head cache; it is recommended to set this\n *\tto true for the cases where the copy will likely be cloned\n *\n *\tMake a copy of both an &sk_buff and part of its data, located\n *\tin header. Fragmented data remain shared. This is used when\n *\tthe caller wishes to modify only header of &sk_buff and needs\n *\tprivate copy of the header to alter. Returns %NULL on failure\n *\tor the pointer to the buffer on success.\n *\tThe returned buffer has a reference count of 1.\n */\n\nstruct sk_buff *__pskb_copy_fclone(struct sk_buff *skb, int headroom,\n\t\t\t\t   gfp_t gfp_mask, bool fclone)\n{\n\tunsigned int size = skb_headlen(skb) + headroom;\n\tint flags = skb_alloc_rx_flag(skb) | (fclone ? SKB_ALLOC_FCLONE : 0);\n\tstruct sk_buff *n = __alloc_skb(size, gfp_mask, flags, NUMA_NO_NODE);\n\n\tif (!n)\n\t\tgoto out;\n\n\t/* Set the data pointer */\n\tskb_reserve(n, headroom);\n\t/* Set the tail pointer and length */\n\tskb_put(n, skb_headlen(skb));\n\t/* Copy the bytes */\n\tskb_copy_from_linear_data(skb, n->data, n->len);\n\n\tn->truesize += skb->data_len;\n\tn->data_len  = skb->data_len;\n\tn->len\t     = skb->len;\n\n\tif (skb_shinfo(skb)->nr_frags) {\n\t\tint i;\n\n\t\tif (skb_orphan_frags(skb, gfp_mask) ||\n\t\t    skb_zerocopy_clone(n, skb, gfp_mask)) {\n\t\t\tkfree_skb(n);\n\t\t\tn = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_shinfo(n)->frags[i] = skb_shinfo(skb)->frags[i];\n\t\t\tskb_frag_ref(skb, i);\n\t\t}\n\t\tskb_shinfo(n)->nr_frags = i;\n\t}\n\n\tif (skb_has_frag_list(skb)) {\n\t\tskb_shinfo(n)->frag_list = skb_shinfo(skb)->frag_list;\n\t\tskb_clone_fraglist(n);\n\t}\n\n\tskb_copy_header(n, skb);\nout:\n\treturn n;\n}\nEXPORT_SYMBOL(__pskb_copy_fclone);\n\n/**\n *\tpskb_expand_head - reallocate header of &sk_buff\n *\t@skb: buffer to reallocate\n *\t@nhead: room to add at head\n *\t@ntail: room to add at tail\n *\t@gfp_mask: allocation priority\n *\n *\tExpands (or creates identical copy, if @nhead and @ntail are zero)\n *\theader of @skb. &sk_buff itself is not changed. &sk_buff MUST have\n *\treference count of 1. Returns zero in the case of success or error,\n *\tif expansion failed. In the last case, &sk_buff is not changed.\n *\n *\tAll the pointers pointing into skb header may change and must be\n *\treloaded after call to this function.\n */\n\nint pskb_expand_head(struct sk_buff *skb, int nhead, int ntail,\n\t\t     gfp_t gfp_mask)\n{\n\tint i, osize = skb_end_offset(skb);\n\tint size = osize + nhead + ntail;\n\tlong off;\n\tu8 *data;\n\n\tBUG_ON(nhead < 0);\n\n\tBUG_ON(skb_shared(skb));\n\n\tsize = SKB_DATA_ALIGN(size);\n\n\tif (skb_pfmemalloc(skb))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\tdata = kmalloc_reserve(size + SKB_DATA_ALIGN(sizeof(struct skb_shared_info)),\n\t\t\t       gfp_mask, NUMA_NO_NODE, NULL);\n\tif (!data)\n\t\tgoto nodata;\n\tsize = SKB_WITH_OVERHEAD(ksize(data));\n\n\t/* Copy only real data... and, alas, header. This should be\n\t * optimized for the cases when header is void.\n\t */\n\tmemcpy(data + nhead, skb->head, skb_tail_pointer(skb) - skb->head);\n\n\tmemcpy((struct skb_shared_info *)(data + size),\n\t       skb_shinfo(skb),\n\t       offsetof(struct skb_shared_info, frags[skb_shinfo(skb)->nr_frags]));\n\n\t/*\n\t * if shinfo is shared we must drop the old head gracefully, but if it\n\t * is not we can just drop the old head and let the existing refcount\n\t * be since all we did is relocate the values\n\t */\n\tif (skb_cloned(skb)) {\n\t\tif (skb_orphan_frags(skb, gfp_mask))\n\t\t\tgoto nofrags;\n\t\tif (skb_zcopy(skb))\n\t\t\trefcount_inc(&skb_uarg(skb)->refcnt);\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++)\n\t\t\tskb_frag_ref(skb, i);\n\n\t\tif (skb_has_frag_list(skb))\n\t\t\tskb_clone_fraglist(skb);\n\n\t\tskb_release_data(skb);\n\t} else {\n\t\tskb_free_head(skb);\n\t}\n\toff = (data + nhead) - skb->head;\n\n\tskb->head     = data;\n\tskb->head_frag = 0;\n\tskb->data    += off;\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\n\tskb->end      = size;\n\toff           = nhead;\n#else\n\tskb->end      = skb->head + size;\n#endif\n\tskb->tail\t      += off;\n\tskb_headers_offset_update(skb, nhead);\n\tskb->cloned   = 0;\n\tskb->hdr_len  = 0;\n\tskb->nohdr    = 0;\n\tatomic_set(&skb_shinfo(skb)->dataref, 1);\n\n\tskb_metadata_clear(skb);\n\n\t/* It is not generally safe to change skb->truesize.\n\t * For the moment, we really care of rx path, or\n\t * when skb is orphaned (not attached to a socket).\n\t */\n\tif (!skb->sk || skb->destructor == sock_edemux)\n\t\tskb->truesize += size - osize;\n\n\treturn 0;\n\nnofrags:\n\tkfree(data);\nnodata:\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL(pskb_expand_head);\n\n/* Make private copy of skb with writable head and some headroom */\n\nstruct sk_buff *skb_realloc_headroom(struct sk_buff *skb, unsigned int headroom)\n{\n\tstruct sk_buff *skb2;\n\tint delta = headroom - skb_headroom(skb);\n\n\tif (delta <= 0)\n\t\tskb2 = pskb_copy(skb, GFP_ATOMIC);\n\telse {\n\t\tskb2 = skb_clone(skb, GFP_ATOMIC);\n\t\tif (skb2 && pskb_expand_head(skb2, SKB_DATA_ALIGN(delta), 0,\n\t\t\t\t\t     GFP_ATOMIC)) {\n\t\t\tkfree_skb(skb2);\n\t\t\tskb2 = NULL;\n\t\t}\n\t}\n\treturn skb2;\n}\nEXPORT_SYMBOL(skb_realloc_headroom);\n\n/**\n *\tskb_copy_expand\t-\tcopy and expand sk_buff\n *\t@skb: buffer to copy\n *\t@newheadroom: new free bytes at head\n *\t@newtailroom: new free bytes at tail\n *\t@gfp_mask: allocation priority\n *\n *\tMake a copy of both an &sk_buff and its data and while doing so\n *\tallocate additional space.\n *\n *\tThis is used when the caller wishes to modify the data and needs a\n *\tprivate copy of the data to alter as well as more space for new fields.\n *\tReturns %NULL on failure or the pointer to the buffer\n *\ton success. The returned buffer has a reference count of 1.\n *\n *\tYou must pass %GFP_ATOMIC as the allocation priority if this function\n *\tis called from an interrupt.\n */\nstruct sk_buff *skb_copy_expand(const struct sk_buff *skb,\n\t\t\t\tint newheadroom, int newtailroom,\n\t\t\t\tgfp_t gfp_mask)\n{\n\t/*\n\t *\tAllocate the copy buffer\n\t */\n\tstruct sk_buff *n = __alloc_skb(newheadroom + skb->len + newtailroom,\n\t\t\t\t\tgfp_mask, skb_alloc_rx_flag(skb),\n\t\t\t\t\tNUMA_NO_NODE);\n\tint oldheadroom = skb_headroom(skb);\n\tint head_copy_len, head_copy_off;\n\n\tif (!n)\n\t\treturn NULL;\n\n\tskb_reserve(n, newheadroom);\n\n\t/* Set the tail pointer and length */\n\tskb_put(n, skb->len);\n\n\thead_copy_len = oldheadroom;\n\thead_copy_off = 0;\n\tif (newheadroom <= head_copy_len)\n\t\thead_copy_len = newheadroom;\n\telse\n\t\thead_copy_off = newheadroom - head_copy_len;\n\n\t/* Copy the linear header and data. */\n\tBUG_ON(skb_copy_bits(skb, -head_copy_len, n->head + head_copy_off,\n\t\t\t     skb->len + head_copy_len));\n\n\tskb_copy_header(n, skb);\n\n\tskb_headers_offset_update(n, newheadroom - oldheadroom);\n\n\treturn n;\n}\nEXPORT_SYMBOL(skb_copy_expand);\n\n/**\n *\t__skb_pad\t\t-\tzero pad the tail of an skb\n *\t@skb: buffer to pad\n *\t@pad: space to pad\n *\t@free_on_error: free buffer on error\n *\n *\tEnsure that a buffer is followed by a padding area that is zero\n *\tfilled. Used by network drivers which may DMA or transfer data\n *\tbeyond the buffer end onto the wire.\n *\n *\tMay return error in out of memory cases. The skb is freed on error\n *\tif @free_on_error is true.\n */\n\nint __skb_pad(struct sk_buff *skb, int pad, bool free_on_error)\n{\n\tint err;\n\tint ntail;\n\n\t/* If the skbuff is non linear tailroom is always zero.. */\n\tif (!skb_cloned(skb) && skb_tailroom(skb) >= pad) {\n\t\tmemset(skb->data+skb->len, 0, pad);\n\t\treturn 0;\n\t}\n\n\tntail = skb->data_len + pad - (skb->end - skb->tail);\n\tif (likely(skb_cloned(skb) || ntail > 0)) {\n\t\terr = pskb_expand_head(skb, 0, ntail, GFP_ATOMIC);\n\t\tif (unlikely(err))\n\t\t\tgoto free_skb;\n\t}\n\n\t/* FIXME: The use of this function with non-linear skb's really needs\n\t * to be audited.\n\t */\n\terr = skb_linearize(skb);\n\tif (unlikely(err))\n\t\tgoto free_skb;\n\n\tmemset(skb->data + skb->len, 0, pad);\n\treturn 0;\n\nfree_skb:\n\tif (free_on_error)\n\t\tkfree_skb(skb);\n\treturn err;\n}\nEXPORT_SYMBOL(__skb_pad);\n\n/**\n *\tpskb_put - add data to the tail of a potentially fragmented buffer\n *\t@skb: start of the buffer to use\n *\t@tail: tail fragment of the buffer to use\n *\t@len: amount of data to add\n *\n *\tThis function extends the used data area of the potentially\n *\tfragmented buffer. @tail must be the last fragment of @skb -- or\n *\t@skb itself. If this would exceed the total buffer size the kernel\n *\twill panic. A pointer to the first byte of the extra data is\n *\treturned.\n */\n\nvoid *pskb_put(struct sk_buff *skb, struct sk_buff *tail, int len)\n{\n\tif (tail != skb) {\n\t\tskb->data_len += len;\n\t\tskb->len += len;\n\t}\n\treturn skb_put(tail, len);\n}\nEXPORT_SYMBOL_GPL(pskb_put);\n\n/**\n *\tskb_put - add data to a buffer\n *\t@skb: buffer to use\n *\t@len: amount of data to add\n *\n *\tThis function extends the used data area of the buffer. If this would\n *\texceed the total buffer size the kernel will panic. A pointer to the\n *\tfirst byte of the extra data is returned.\n */\nvoid *skb_put(struct sk_buff *skb, unsigned int len)\n{\n\tvoid *tmp = skb_tail_pointer(skb);\n\tSKB_LINEAR_ASSERT(skb);\n\tskb->tail += len;\n\tskb->len  += len;\n\tif (unlikely(skb->tail > skb->end))\n\t\tskb_over_panic(skb, len, __builtin_return_address(0));\n\treturn tmp;\n}\nEXPORT_SYMBOL(skb_put);\n\n/**\n *\tskb_push - add data to the start of a buffer\n *\t@skb: buffer to use\n *\t@len: amount of data to add\n *\n *\tThis function extends the used data area of the buffer at the buffer\n *\tstart. If this would exceed the total buffer headroom the kernel will\n *\tpanic. A pointer to the first byte of the extra data is returned.\n */\nvoid *skb_push(struct sk_buff *skb, unsigned int len)\n{\n\tskb->data -= len;\n\tskb->len  += len;\n\tif (unlikely(skb->data < skb->head))\n\t\tskb_under_panic(skb, len, __builtin_return_address(0));\n\treturn skb->data;\n}\nEXPORT_SYMBOL(skb_push);\n\n/**\n *\tskb_pull - remove data from the start of a buffer\n *\t@skb: buffer to use\n *\t@len: amount of data to remove\n *\n *\tThis function removes data from the start of a buffer, returning\n *\tthe memory to the headroom. A pointer to the next data in the buffer\n *\tis returned. Once the data has been pulled future pushes will overwrite\n *\tthe old data.\n */\nvoid *skb_pull(struct sk_buff *skb, unsigned int len)\n{\n\treturn skb_pull_inline(skb, len);\n}\nEXPORT_SYMBOL(skb_pull);\n\n/**\n *\tskb_trim - remove end from a buffer\n *\t@skb: buffer to alter\n *\t@len: new length\n *\n *\tCut the length of a buffer down by removing data from the tail. If\n *\tthe buffer is already under the length specified it is not modified.\n *\tThe skb must be linear.\n */\nvoid skb_trim(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->len > len)\n\t\t__skb_trim(skb, len);\n}\nEXPORT_SYMBOL(skb_trim);\n\n/* Trims skb to length len. It can change skb pointers.\n */\n\nint ___pskb_trim(struct sk_buff *skb, unsigned int len)\n{\n\tstruct sk_buff **fragp;\n\tstruct sk_buff *frag;\n\tint offset = skb_headlen(skb);\n\tint nfrags = skb_shinfo(skb)->nr_frags;\n\tint i;\n\tint err;\n\n\tif (skb_cloned(skb) &&\n\t    unlikely((err = pskb_expand_head(skb, 0, 0, GFP_ATOMIC))))\n\t\treturn err;\n\n\ti = 0;\n\tif (offset >= len)\n\t\tgoto drop_pages;\n\n\tfor (; i < nfrags; i++) {\n\t\tint end = offset + skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (end < len) {\n\t\t\toffset = end;\n\t\t\tcontinue;\n\t\t}\n\n\t\tskb_frag_size_set(&skb_shinfo(skb)->frags[i++], len - offset);\n\ndrop_pages:\n\t\tskb_shinfo(skb)->nr_frags = i;\n\n\t\tfor (; i < nfrags; i++)\n\t\t\tskb_frag_unref(skb, i);\n\n\t\tif (skb_has_frag_list(skb))\n\t\t\tskb_drop_fraglist(skb);\n\t\tgoto done;\n\t}\n\n\tfor (fragp = &skb_shinfo(skb)->frag_list; (frag = *fragp);\n\t     fragp = &frag->next) {\n\t\tint end = offset + frag->len;\n\n\t\tif (skb_shared(frag)) {\n\t\t\tstruct sk_buff *nfrag;\n\n\t\t\tnfrag = skb_clone(frag, GFP_ATOMIC);\n\t\t\tif (unlikely(!nfrag))\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tnfrag->next = frag->next;\n\t\t\tconsume_skb(frag);\n\t\t\tfrag = nfrag;\n\t\t\t*fragp = frag;\n\t\t}\n\n\t\tif (end < len) {\n\t\t\toffset = end;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (end > len &&\n\t\t    unlikely((err = pskb_trim(frag, len - offset))))\n\t\t\treturn err;\n\n\t\tif (frag->next)\n\t\t\tskb_drop_list(&frag->next);\n\t\tbreak;\n\t}\n\ndone:\n\tif (len > skb_headlen(skb)) {\n\t\tskb->data_len -= skb->len - len;\n\t\tskb->len       = len;\n\t} else {\n\t\tskb->len       = len;\n\t\tskb->data_len  = 0;\n\t\tskb_set_tail_pointer(skb, len);\n\t}\n\n\tif (!skb->sk || skb->destructor == sock_edemux)\n\t\tskb_condense(skb);\n\treturn 0;\n}\nEXPORT_SYMBOL(___pskb_trim);\n\n/* Note : use pskb_trim_rcsum() instead of calling this directly\n */\nint pskb_trim_rcsum_slow(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\tint delta = skb->len - len;\n\n\t\tskb->csum = csum_block_sub(skb->csum,\n\t\t\t\t\t   skb_checksum(skb, len, delta, 0),\n\t\t\t\t\t   len);\n\t} else if (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tint hdlen = (len > skb_headlen(skb)) ? skb_headlen(skb) : len;\n\t\tint offset = skb_checksum_start_offset(skb) + skb->csum_offset;\n\n\t\tif (offset + sizeof(__sum16) > hdlen)\n\t\t\treturn -EINVAL;\n\t}\n\treturn __pskb_trim(skb, len);\n}\nEXPORT_SYMBOL(pskb_trim_rcsum_slow);\n\n/**\n *\t__pskb_pull_tail - advance tail of skb header\n *\t@skb: buffer to reallocate\n *\t@delta: number of bytes to advance tail\n *\n *\tThe function makes a sense only on a fragmented &sk_buff,\n *\tit expands header moving its tail forward and copying necessary\n *\tdata from fragmented part.\n *\n *\t&sk_buff MUST have reference count of 1.\n *\n *\tReturns %NULL (and &sk_buff does not change) if pull failed\n *\tor value of new tail of skb in the case of success.\n *\n *\tAll the pointers pointing into skb header may change and must be\n *\treloaded after call to this function.\n */\n\n/* Moves tail of skb head forward, copying data from fragmented part,\n * when it is necessary.\n * 1. It may fail due to malloc failure.\n * 2. It may change skb pointers.\n *\n * It is pretty complicated. Luckily, it is called only in exceptional cases.\n */\nvoid *__pskb_pull_tail(struct sk_buff *skb, int delta)\n{\n\t/* If skb has not enough free space at tail, get new one\n\t * plus 128 bytes for future expansions. If we have enough\n\t * room at tail, reallocate without expansion only if skb is cloned.\n\t */\n\tint i, k, eat = (skb->tail + delta) - skb->end;\n\n\tif (eat > 0 || skb_cloned(skb)) {\n\t\tif (pskb_expand_head(skb, 0, eat > 0 ? eat + 128 : 0,\n\t\t\t\t     GFP_ATOMIC))\n\t\t\treturn NULL;\n\t}\n\n\tBUG_ON(skb_copy_bits(skb, skb_headlen(skb),\n\t\t\t     skb_tail_pointer(skb), delta));\n\n\t/* Optimization: no fragments, no reasons to preestimate\n\t * size of pulled pages. Superb.\n\t */\n\tif (!skb_has_frag_list(skb))\n\t\tgoto pull_pages;\n\n\t/* Estimate size of pulled pages. */\n\teat = delta;\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (size >= eat)\n\t\t\tgoto pull_pages;\n\t\teat -= size;\n\t}\n\n\t/* If we need update frag list, we are in troubles.\n\t * Certainly, it is possible to add an offset to skb data,\n\t * but taking into account that pulling is expected to\n\t * be very rare operation, it is worth to fight against\n\t * further bloating skb head and crucify ourselves here instead.\n\t * Pure masohism, indeed. 8)8)\n\t */\n\tif (eat) {\n\t\tstruct sk_buff *list = skb_shinfo(skb)->frag_list;\n\t\tstruct sk_buff *clone = NULL;\n\t\tstruct sk_buff *insp = NULL;\n\n\t\tdo {\n\t\t\tif (list->len <= eat) {\n\t\t\t\t/* Eaten as whole. */\n\t\t\t\teat -= list->len;\n\t\t\t\tlist = list->next;\n\t\t\t\tinsp = list;\n\t\t\t} else {\n\t\t\t\t/* Eaten partially. */\n\n\t\t\t\tif (skb_shared(list)) {\n\t\t\t\t\t/* Sucks! We need to fork list. :-( */\n\t\t\t\t\tclone = skb_clone(list, GFP_ATOMIC);\n\t\t\t\t\tif (!clone)\n\t\t\t\t\t\treturn NULL;\n\t\t\t\t\tinsp = list->next;\n\t\t\t\t\tlist = clone;\n\t\t\t\t} else {\n\t\t\t\t\t/* This may be pulled without\n\t\t\t\t\t * problems. */\n\t\t\t\t\tinsp = list;\n\t\t\t\t}\n\t\t\t\tif (!pskb_pull(list, eat)) {\n\t\t\t\t\tkfree_skb(clone);\n\t\t\t\t\treturn NULL;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} while (eat);\n\n\t\t/* Free pulled out fragments. */\n\t\twhile ((list = skb_shinfo(skb)->frag_list) != insp) {\n\t\t\tskb_shinfo(skb)->frag_list = list->next;\n\t\t\tkfree_skb(list);\n\t\t}\n\t\t/* And insert new clone at head. */\n\t\tif (clone) {\n\t\t\tclone->next = list;\n\t\t\tskb_shinfo(skb)->frag_list = clone;\n\t\t}\n\t}\n\t/* Success! Now we may commit changes to skb data. */\n\npull_pages:\n\teat = delta;\n\tk = 0;\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (size <= eat) {\n\t\t\tskb_frag_unref(skb, i);\n\t\t\teat -= size;\n\t\t} else {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[k];\n\n\t\t\t*frag = skb_shinfo(skb)->frags[i];\n\t\t\tif (eat) {\n\t\t\t\tskb_frag_off_add(frag, eat);\n\t\t\t\tskb_frag_size_sub(frag, eat);\n\t\t\t\tif (!i)\n\t\t\t\t\tgoto end;\n\t\t\t\teat = 0;\n\t\t\t}\n\t\t\tk++;\n\t\t}\n\t}\n\tskb_shinfo(skb)->nr_frags = k;\n\nend:\n\tskb->tail     += delta;\n\tskb->data_len -= delta;\n\n\tif (!skb->data_len)\n\t\tskb_zcopy_clear(skb, false);\n\n\treturn skb_tail_pointer(skb);\n}\nEXPORT_SYMBOL(__pskb_pull_tail);\n\n/**\n *\tskb_copy_bits - copy bits from skb to kernel buffer\n *\t@skb: source skb\n *\t@offset: offset in source\n *\t@to: destination buffer\n *\t@len: number of bytes to copy\n *\n *\tCopy the specified number of bytes from the source skb to the\n *\tdestination buffer.\n *\n *\tCAUTION ! :\n *\t\tIf its prototype is ever changed,\n *\t\tcheck arch/{*}/net/{*}.S files,\n *\t\tsince it is called from BPF assembly code.\n */\nint skb_copy_bits(const struct sk_buff *skb, int offset, void *to, int len)\n{\n\tint start = skb_headlen(skb);\n\tstruct sk_buff *frag_iter;\n\tint i, copy;\n\n\tif (offset > (int)skb->len - len)\n\t\tgoto fault;\n\n\t/* Copy header. */\n\tif ((copy = start - offset) > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tskb_copy_from_linear_data_offset(skb, offset, to, copy);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn 0;\n\t\toffset += copy;\n\t\tto     += copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\t\tskb_frag_t *f = &skb_shinfo(skb)->frags[i];\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(f);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tu32 p_off, p_len, copied;\n\t\t\tstruct page *p;\n\t\t\tu8 *vaddr;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\n\t\t\tskb_frag_foreach_page(f,\n\t\t\t\t\t      skb_frag_off(f) + offset - start,\n\t\t\t\t\t      copy, p, p_off, p_len, copied) {\n\t\t\t\tvaddr = kmap_atomic(p);\n\t\t\t\tmemcpy(to + copied, vaddr + p_off, p_len);\n\t\t\t\tkunmap_atomic(vaddr);\n\t\t\t}\n\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tif (skb_copy_bits(frag_iter, offset - start, to, copy))\n\t\t\t\tgoto fault;\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tif (!len)\n\t\treturn 0;\n\nfault:\n\treturn -EFAULT;\n}\nEXPORT_SYMBOL(skb_copy_bits);\n\n/*\n * Callback from splice_to_pipe(), if we need to release some pages\n * at the end of the spd in case we error'ed out in filling the pipe.\n */\nstatic void sock_spd_release(struct splice_pipe_desc *spd, unsigned int i)\n{\n\tput_page(spd->pages[i]);\n}\n\nstatic struct page *linear_to_page(struct page *page, unsigned int *len,\n\t\t\t\t   unsigned int *offset,\n\t\t\t\t   struct sock *sk)\n{\n\tstruct page_frag *pfrag = sk_page_frag(sk);\n\n\tif (!sk_page_frag_refill(sk, pfrag))\n\t\treturn NULL;\n\n\t*len = min_t(unsigned int, *len, pfrag->size - pfrag->offset);\n\n\tmemcpy(page_address(pfrag->page) + pfrag->offset,\n\t       page_address(page) + *offset, *len);\n\t*offset = pfrag->offset;\n\tpfrag->offset += *len;\n\n\treturn pfrag->page;\n}\n\nstatic bool spd_can_coalesce(const struct splice_pipe_desc *spd,\n\t\t\t     struct page *page,\n\t\t\t     unsigned int offset)\n{\n\treturn\tspd->nr_pages &&\n\t\tspd->pages[spd->nr_pages - 1] == page &&\n\t\t(spd->partial[spd->nr_pages - 1].offset +\n\t\t spd->partial[spd->nr_pages - 1].len == offset);\n}\n\n/*\n * Fill page/offset/length into spd, if it can hold more pages.\n */\nstatic bool spd_fill_page(struct splice_pipe_desc *spd,\n\t\t\t  struct pipe_inode_info *pipe, struct page *page,\n\t\t\t  unsigned int *len, unsigned int offset,\n\t\t\t  bool linear,\n\t\t\t  struct sock *sk)\n{\n\tif (unlikely(spd->nr_pages == MAX_SKB_FRAGS))\n\t\treturn true;\n\n\tif (linear) {\n\t\tpage = linear_to_page(page, len, &offset, sk);\n\t\tif (!page)\n\t\t\treturn true;\n\t}\n\tif (spd_can_coalesce(spd, page, offset)) {\n\t\tspd->partial[spd->nr_pages - 1].len += *len;\n\t\treturn false;\n\t}\n\tget_page(page);\n\tspd->pages[spd->nr_pages] = page;\n\tspd->partial[spd->nr_pages].len = *len;\n\tspd->partial[spd->nr_pages].offset = offset;\n\tspd->nr_pages++;\n\n\treturn false;\n}\n\nstatic bool __splice_segment(struct page *page, unsigned int poff,\n\t\t\t     unsigned int plen, unsigned int *off,\n\t\t\t     unsigned int *len,\n\t\t\t     struct splice_pipe_desc *spd, bool linear,\n\t\t\t     struct sock *sk,\n\t\t\t     struct pipe_inode_info *pipe)\n{\n\tif (!*len)\n\t\treturn true;\n\n\t/* skip this segment if already processed */\n\tif (*off >= plen) {\n\t\t*off -= plen;\n\t\treturn false;\n\t}\n\n\t/* ignore any bits we already processed */\n\tpoff += *off;\n\tplen -= *off;\n\t*off = 0;\n\n\tdo {\n\t\tunsigned int flen = min(*len, plen);\n\n\t\tif (spd_fill_page(spd, pipe, page, &flen, poff,\n\t\t\t\t  linear, sk))\n\t\t\treturn true;\n\t\tpoff += flen;\n\t\tplen -= flen;\n\t\t*len -= flen;\n\t} while (*len && plen);\n\n\treturn false;\n}\n\n/*\n * Map linear and fragment data from the skb to spd. It reports true if the\n * pipe is full or if we already spliced the requested length.\n */\nstatic bool __skb_splice_bits(struct sk_buff *skb, struct pipe_inode_info *pipe,\n\t\t\t      unsigned int *offset, unsigned int *len,\n\t\t\t      struct splice_pipe_desc *spd, struct sock *sk)\n{\n\tint seg;\n\tstruct sk_buff *iter;\n\n\t/* map the linear part :\n\t * If skb->head_frag is set, this 'linear' part is backed by a\n\t * fragment, and if the head is not shared with any clones then\n\t * we can avoid a copy since we own the head portion of this page.\n\t */\n\tif (__splice_segment(virt_to_page(skb->data),\n\t\t\t     (unsigned long) skb->data & (PAGE_SIZE - 1),\n\t\t\t     skb_headlen(skb),\n\t\t\t     offset, len, spd,\n\t\t\t     skb_head_is_locked(skb),\n\t\t\t     sk, pipe))\n\t\treturn true;\n\n\t/*\n\t * then map the fragments\n\t */\n\tfor (seg = 0; seg < skb_shinfo(skb)->nr_frags; seg++) {\n\t\tconst skb_frag_t *f = &skb_shinfo(skb)->frags[seg];\n\n\t\tif (__splice_segment(skb_frag_page(f),\n\t\t\t\t     skb_frag_off(f), skb_frag_size(f),\n\t\t\t\t     offset, len, spd, false, sk, pipe))\n\t\t\treturn true;\n\t}\n\n\tskb_walk_frags(skb, iter) {\n\t\tif (*offset >= iter->len) {\n\t\t\t*offset -= iter->len;\n\t\t\tcontinue;\n\t\t}\n\t\t/* __skb_splice_bits() only fails if the output has no room\n\t\t * left, so no point in going over the frag_list for the error\n\t\t * case.\n\t\t */\n\t\tif (__skb_splice_bits(iter, pipe, offset, len, spd, sk))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * Map data from the skb to a pipe. Should handle both the linear part,\n * the fragments, and the frag list.\n */\nint skb_splice_bits(struct sk_buff *skb, struct sock *sk, unsigned int offset,\n\t\t    struct pipe_inode_info *pipe, unsigned int tlen,\n\t\t    unsigned int flags)\n{\n\tstruct partial_page partial[MAX_SKB_FRAGS];\n\tstruct page *pages[MAX_SKB_FRAGS];\n\tstruct splice_pipe_desc spd = {\n\t\t.pages = pages,\n\t\t.partial = partial,\n\t\t.nr_pages_max = MAX_SKB_FRAGS,\n\t\t.ops = &nosteal_pipe_buf_ops,\n\t\t.spd_release = sock_spd_release,\n\t};\n\tint ret = 0;\n\n\t__skb_splice_bits(skb, pipe, &offset, &tlen, &spd, sk);\n\n\tif (spd.nr_pages)\n\t\tret = splice_to_pipe(pipe, &spd);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(skb_splice_bits);\n\nstatic int sendmsg_unlocked(struct sock *sk, struct msghdr *msg,\n\t\t\t    struct kvec *vec, size_t num, size_t size)\n{\n\tstruct socket *sock = sk->sk_socket;\n\n\tif (!sock)\n\t\treturn -EINVAL;\n\treturn kernel_sendmsg(sock, msg, vec, num, size);\n}\n\nstatic int sendpage_unlocked(struct sock *sk, struct page *page, int offset,\n\t\t\t     size_t size, int flags)\n{\n\tstruct socket *sock = sk->sk_socket;\n\n\tif (!sock)\n\t\treturn -EINVAL;\n\treturn kernel_sendpage(sock, page, offset, size, flags);\n}\n\ntypedef int (*sendmsg_func)(struct sock *sk, struct msghdr *msg,\n\t\t\t    struct kvec *vec, size_t num, size_t size);\ntypedef int (*sendpage_func)(struct sock *sk, struct page *page, int offset,\n\t\t\t     size_t size, int flags);\nstatic int __skb_send_sock(struct sock *sk, struct sk_buff *skb, int offset,\n\t\t\t   int len, sendmsg_func sendmsg, sendpage_func sendpage)\n{\n\tunsigned int orig_len = len;\n\tstruct sk_buff *head = skb;\n\tunsigned short fragidx;\n\tint slen, ret;\n\ndo_frag_list:\n\n\t/* Deal with head data */\n\twhile (offset < skb_headlen(skb) && len) {\n\t\tstruct kvec kv;\n\t\tstruct msghdr msg;\n\n\t\tslen = min_t(int, len, skb_headlen(skb) - offset);\n\t\tkv.iov_base = skb->data + offset;\n\t\tkv.iov_len = slen;\n\t\tmemset(&msg, 0, sizeof(msg));\n\t\tmsg.msg_flags = MSG_DONTWAIT;\n\n\t\tret = INDIRECT_CALL_2(sendmsg, kernel_sendmsg_locked,\n\t\t\t\t      sendmsg_unlocked, sk, &msg, &kv, 1, slen);\n\t\tif (ret <= 0)\n\t\t\tgoto error;\n\n\t\toffset += ret;\n\t\tlen -= ret;\n\t}\n\n\t/* All the data was skb head? */\n\tif (!len)\n\t\tgoto out;\n\n\t/* Make offset relative to start of frags */\n\toffset -= skb_headlen(skb);\n\n\t/* Find where we are in frag list */\n\tfor (fragidx = 0; fragidx < skb_shinfo(skb)->nr_frags; fragidx++) {\n\t\tskb_frag_t *frag  = &skb_shinfo(skb)->frags[fragidx];\n\n\t\tif (offset < skb_frag_size(frag))\n\t\t\tbreak;\n\n\t\toffset -= skb_frag_size(frag);\n\t}\n\n\tfor (; len && fragidx < skb_shinfo(skb)->nr_frags; fragidx++) {\n\t\tskb_frag_t *frag  = &skb_shinfo(skb)->frags[fragidx];\n\n\t\tslen = min_t(size_t, len, skb_frag_size(frag) - offset);\n\n\t\twhile (slen) {\n\t\t\tret = INDIRECT_CALL_2(sendpage, kernel_sendpage_locked,\n\t\t\t\t\t      sendpage_unlocked, sk,\n\t\t\t\t\t      skb_frag_page(frag),\n\t\t\t\t\t      skb_frag_off(frag) + offset,\n\t\t\t\t\t      slen, MSG_DONTWAIT);\n\t\t\tif (ret <= 0)\n\t\t\t\tgoto error;\n\n\t\t\tlen -= ret;\n\t\t\toffset += ret;\n\t\t\tslen -= ret;\n\t\t}\n\n\t\toffset = 0;\n\t}\n\n\tif (len) {\n\t\t/* Process any frag lists */\n\n\t\tif (skb == head) {\n\t\t\tif (skb_has_frag_list(skb)) {\n\t\t\t\tskb = skb_shinfo(skb)->frag_list;\n\t\t\t\tgoto do_frag_list;\n\t\t\t}\n\t\t} else if (skb->next) {\n\t\t\tskb = skb->next;\n\t\t\tgoto do_frag_list;\n\t\t}\n\t}\n\nout:\n\treturn orig_len - len;\n\nerror:\n\treturn orig_len == len ? ret : orig_len - len;\n}\n\n/* Send skb data on a socket. Socket must be locked. */\nint skb_send_sock_locked(struct sock *sk, struct sk_buff *skb, int offset,\n\t\t\t int len)\n{\n\treturn __skb_send_sock(sk, skb, offset, len, kernel_sendmsg_locked,\n\t\t\t       kernel_sendpage_locked);\n}\nEXPORT_SYMBOL_GPL(skb_send_sock_locked);\n\n/* Send skb data on a socket. Socket must be unlocked. */\nint skb_send_sock(struct sock *sk, struct sk_buff *skb, int offset, int len)\n{\n\treturn __skb_send_sock(sk, skb, offset, len, sendmsg_unlocked,\n\t\t\t       sendpage_unlocked);\n}\n\n/**\n *\tskb_store_bits - store bits from kernel buffer to skb\n *\t@skb: destination buffer\n *\t@offset: offset in destination\n *\t@from: source buffer\n *\t@len: number of bytes to copy\n *\n *\tCopy the specified number of bytes from the source buffer to the\n *\tdestination skb.  This function handles all the messy bits of\n *\ttraversing fragment lists and such.\n */\n\nint skb_store_bits(struct sk_buff *skb, int offset, const void *from, int len)\n{\n\tint start = skb_headlen(skb);\n\tstruct sk_buff *frag_iter;\n\tint i, copy;\n\n\tif (offset > (int)skb->len - len)\n\t\tgoto fault;\n\n\tif ((copy = start - offset) > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tskb_copy_to_linear_data_offset(skb, offset, from, copy);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn 0;\n\t\toffset += copy;\n\t\tfrom += copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(frag);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tu32 p_off, p_len, copied;\n\t\t\tstruct page *p;\n\t\t\tu8 *vaddr;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\n\t\t\tskb_frag_foreach_page(frag,\n\t\t\t\t\t      skb_frag_off(frag) + offset - start,\n\t\t\t\t\t      copy, p, p_off, p_len, copied) {\n\t\t\t\tvaddr = kmap_atomic(p);\n\t\t\t\tmemcpy(vaddr + p_off, from + copied, p_len);\n\t\t\t\tkunmap_atomic(vaddr);\n\t\t\t}\n\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tfrom += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tif (skb_store_bits(frag_iter, offset - start,\n\t\t\t\t\t   from, copy))\n\t\t\t\tgoto fault;\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn 0;\n\t\t\toffset += copy;\n\t\t\tfrom += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tif (!len)\n\t\treturn 0;\n\nfault:\n\treturn -EFAULT;\n}\nEXPORT_SYMBOL(skb_store_bits);\n\n/* Checksum skb data. */\n__wsum __skb_checksum(const struct sk_buff *skb, int offset, int len,\n\t\t      __wsum csum, const struct skb_checksum_ops *ops)\n{\n\tint start = skb_headlen(skb);\n\tint i, copy = start - offset;\n\tstruct sk_buff *frag_iter;\n\tint pos = 0;\n\n\t/* Checksum header. */\n\tif (copy > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tcsum = INDIRECT_CALL_1(ops->update, csum_partial_ext,\n\t\t\t\t       skb->data + offset, copy, csum);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn csum;\n\t\toffset += copy;\n\t\tpos\t= copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(frag);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tu32 p_off, p_len, copied;\n\t\t\tstruct page *p;\n\t\t\t__wsum csum2;\n\t\t\tu8 *vaddr;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\n\t\t\tskb_frag_foreach_page(frag,\n\t\t\t\t\t      skb_frag_off(frag) + offset - start,\n\t\t\t\t\t      copy, p, p_off, p_len, copied) {\n\t\t\t\tvaddr = kmap_atomic(p);\n\t\t\t\tcsum2 = INDIRECT_CALL_1(ops->update,\n\t\t\t\t\t\t\tcsum_partial_ext,\n\t\t\t\t\t\t\tvaddr + p_off, p_len, 0);\n\t\t\t\tkunmap_atomic(vaddr);\n\t\t\t\tcsum = INDIRECT_CALL_1(ops->combine,\n\t\t\t\t\t\t       csum_block_add_ext, csum,\n\t\t\t\t\t\t       csum2, pos, p_len);\n\t\t\t\tpos += p_len;\n\t\t\t}\n\n\t\t\tif (!(len -= copy))\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\t__wsum csum2;\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tcsum2 = __skb_checksum(frag_iter, offset - start,\n\t\t\t\t\t       copy, 0, ops);\n\t\t\tcsum = INDIRECT_CALL_1(ops->combine, csum_block_add_ext,\n\t\t\t\t\t       csum, csum2, pos, copy);\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tpos    += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tBUG_ON(len);\n\n\treturn csum;\n}\nEXPORT_SYMBOL(__skb_checksum);\n\n__wsum skb_checksum(const struct sk_buff *skb, int offset,\n\t\t    int len, __wsum csum)\n{\n\tconst struct skb_checksum_ops ops = {\n\t\t.update  = csum_partial_ext,\n\t\t.combine = csum_block_add_ext,\n\t};\n\n\treturn __skb_checksum(skb, offset, len, csum, &ops);\n}\nEXPORT_SYMBOL(skb_checksum);\n\n/* Both of above in one bottle. */\n\n__wsum skb_copy_and_csum_bits(const struct sk_buff *skb, int offset,\n\t\t\t\t    u8 *to, int len)\n{\n\tint start = skb_headlen(skb);\n\tint i, copy = start - offset;\n\tstruct sk_buff *frag_iter;\n\tint pos = 0;\n\t__wsum csum = 0;\n\n\t/* Copy header. */\n\tif (copy > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tcsum = csum_partial_copy_nocheck(skb->data + offset, to,\n\t\t\t\t\t\t copy);\n\t\tif ((len -= copy) == 0)\n\t\t\treturn csum;\n\t\toffset += copy;\n\t\tto     += copy;\n\t\tpos\t= copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\t\tu32 p_off, p_len, copied;\n\t\t\tstruct page *p;\n\t\t\t__wsum csum2;\n\t\t\tu8 *vaddr;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\n\t\t\tskb_frag_foreach_page(frag,\n\t\t\t\t\t      skb_frag_off(frag) + offset - start,\n\t\t\t\t\t      copy, p, p_off, p_len, copied) {\n\t\t\t\tvaddr = kmap_atomic(p);\n\t\t\t\tcsum2 = csum_partial_copy_nocheck(vaddr + p_off,\n\t\t\t\t\t\t\t\t  to + copied,\n\t\t\t\t\t\t\t\t  p_len);\n\t\t\t\tkunmap_atomic(vaddr);\n\t\t\t\tcsum = csum_block_add(csum, csum2, pos);\n\t\t\t\tpos += p_len;\n\t\t\t}\n\n\t\t\tif (!(len -= copy))\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\t__wsum csum2;\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tcsum2 = skb_copy_and_csum_bits(frag_iter,\n\t\t\t\t\t\t       offset - start,\n\t\t\t\t\t\t       to, copy);\n\t\t\tcsum = csum_block_add(csum, csum2, pos);\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn csum;\n\t\t\toffset += copy;\n\t\t\tto     += copy;\n\t\t\tpos    += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tBUG_ON(len);\n\treturn csum;\n}\nEXPORT_SYMBOL(skb_copy_and_csum_bits);\n\n__sum16 __skb_checksum_complete_head(struct sk_buff *skb, int len)\n{\n\t__sum16 sum;\n\n\tsum = csum_fold(skb_checksum(skb, 0, len, skb->csum));\n\t/* See comments in __skb_checksum_complete(). */\n\tif (likely(!sum)) {\n\t\tif (unlikely(skb->ip_summed == CHECKSUM_COMPLETE) &&\n\t\t    !skb->csum_complete_sw)\n\t\t\tnetdev_rx_csum_fault(skb->dev, skb);\n\t}\n\tif (!skb_shared(skb))\n\t\tskb->csum_valid = !sum;\n\treturn sum;\n}\nEXPORT_SYMBOL(__skb_checksum_complete_head);\n\n/* This function assumes skb->csum already holds pseudo header's checksum,\n * which has been changed from the hardware checksum, for example, by\n * __skb_checksum_validate_complete(). And, the original skb->csum must\n * have been validated unsuccessfully for CHECKSUM_COMPLETE case.\n *\n * It returns non-zero if the recomputed checksum is still invalid, otherwise\n * zero. The new checksum is stored back into skb->csum unless the skb is\n * shared.\n */\n__sum16 __skb_checksum_complete(struct sk_buff *skb)\n{\n\t__wsum csum;\n\t__sum16 sum;\n\n\tcsum = skb_checksum(skb, 0, skb->len, 0);\n\n\tsum = csum_fold(csum_add(skb->csum, csum));\n\t/* This check is inverted, because we already knew the hardware\n\t * checksum is invalid before calling this function. So, if the\n\t * re-computed checksum is valid instead, then we have a mismatch\n\t * between the original skb->csum and skb_checksum(). This means either\n\t * the original hardware checksum is incorrect or we screw up skb->csum\n\t * when moving skb->data around.\n\t */\n\tif (likely(!sum)) {\n\t\tif (unlikely(skb->ip_summed == CHECKSUM_COMPLETE) &&\n\t\t    !skb->csum_complete_sw)\n\t\t\tnetdev_rx_csum_fault(skb->dev, skb);\n\t}\n\n\tif (!skb_shared(skb)) {\n\t\t/* Save full packet checksum */\n\t\tskb->csum = csum;\n\t\tskb->ip_summed = CHECKSUM_COMPLETE;\n\t\tskb->csum_complete_sw = 1;\n\t\tskb->csum_valid = !sum;\n\t}\n\n\treturn sum;\n}\nEXPORT_SYMBOL(__skb_checksum_complete);\n\nstatic __wsum warn_crc32c_csum_update(const void *buff, int len, __wsum sum)\n{\n\tnet_warn_ratelimited(\n\t\t\"%s: attempt to compute crc32c without libcrc32c.ko\\n\",\n\t\t__func__);\n\treturn 0;\n}\n\nstatic __wsum warn_crc32c_csum_combine(__wsum csum, __wsum csum2,\n\t\t\t\t       int offset, int len)\n{\n\tnet_warn_ratelimited(\n\t\t\"%s: attempt to compute crc32c without libcrc32c.ko\\n\",\n\t\t__func__);\n\treturn 0;\n}\n\nstatic const struct skb_checksum_ops default_crc32c_ops = {\n\t.update  = warn_crc32c_csum_update,\n\t.combine = warn_crc32c_csum_combine,\n};\n\nconst struct skb_checksum_ops *crc32c_csum_stub __read_mostly =\n\t&default_crc32c_ops;\nEXPORT_SYMBOL(crc32c_csum_stub);\n\n /**\n *\tskb_zerocopy_headlen - Calculate headroom needed for skb_zerocopy()\n *\t@from: source buffer\n *\n *\tCalculates the amount of linear headroom needed in the 'to' skb passed\n *\tinto skb_zerocopy().\n */\nunsigned int\nskb_zerocopy_headlen(const struct sk_buff *from)\n{\n\tunsigned int hlen = 0;\n\n\tif (!from->head_frag ||\n\t    skb_headlen(from) < L1_CACHE_BYTES ||\n\t    skb_shinfo(from)->nr_frags >= MAX_SKB_FRAGS)\n\t\thlen = skb_headlen(from);\n\n\tif (skb_has_frag_list(from))\n\t\thlen = from->len;\n\n\treturn hlen;\n}\nEXPORT_SYMBOL_GPL(skb_zerocopy_headlen);\n\n/**\n *\tskb_zerocopy - Zero copy skb to skb\n *\t@to: destination buffer\n *\t@from: source buffer\n *\t@len: number of bytes to copy from source buffer\n *\t@hlen: size of linear headroom in destination buffer\n *\n *\tCopies up to `len` bytes from `from` to `to` by creating references\n *\tto the frags in the source buffer.\n *\n *\tThe `hlen` as calculated by skb_zerocopy_headlen() specifies the\n *\theadroom in the `to` buffer.\n *\n *\tReturn value:\n *\t0: everything is OK\n *\t-ENOMEM: couldn't orphan frags of @from due to lack of memory\n *\t-EFAULT: skb_copy_bits() found some problem with skb geometry\n */\nint\nskb_zerocopy(struct sk_buff *to, struct sk_buff *from, int len, int hlen)\n{\n\tint i, j = 0;\n\tint plen = 0; /* length of skb->head fragment */\n\tint ret;\n\tstruct page *page;\n\tunsigned int offset;\n\n\tBUG_ON(!from->head_frag && !hlen);\n\n\t/* dont bother with small payloads */\n\tif (len <= skb_tailroom(to))\n\t\treturn skb_copy_bits(from, 0, skb_put(to, len), len);\n\n\tif (hlen) {\n\t\tret = skb_copy_bits(from, 0, skb_put(to, hlen), hlen);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t\tlen -= hlen;\n\t} else {\n\t\tplen = min_t(int, skb_headlen(from), len);\n\t\tif (plen) {\n\t\t\tpage = virt_to_head_page(from->head);\n\t\t\toffset = from->data - (unsigned char *)page_address(page);\n\t\t\t__skb_fill_page_desc(to, 0, page, offset, plen);\n\t\t\tget_page(page);\n\t\t\tj = 1;\n\t\t\tlen -= plen;\n\t\t}\n\t}\n\n\tto->truesize += len + plen;\n\tto->len += len + plen;\n\tto->data_len += len + plen;\n\n\tif (unlikely(skb_orphan_frags(from, GFP_ATOMIC))) {\n\t\tskb_tx_error(from);\n\t\treturn -ENOMEM;\n\t}\n\tskb_zerocopy_clone(to, from, GFP_ATOMIC);\n\n\tfor (i = 0; i < skb_shinfo(from)->nr_frags; i++) {\n\t\tint size;\n\n\t\tif (!len)\n\t\t\tbreak;\n\t\tskb_shinfo(to)->frags[j] = skb_shinfo(from)->frags[i];\n\t\tsize = min_t(int, skb_frag_size(&skb_shinfo(to)->frags[j]),\n\t\t\t\t\tlen);\n\t\tskb_frag_size_set(&skb_shinfo(to)->frags[j], size);\n\t\tlen -= size;\n\t\tskb_frag_ref(to, j);\n\t\tj++;\n\t}\n\tskb_shinfo(to)->nr_frags = j;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_zerocopy);\n\nvoid skb_copy_and_csum_dev(const struct sk_buff *skb, u8 *to)\n{\n\t__wsum csum;\n\tlong csstart;\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tcsstart = skb_checksum_start_offset(skb);\n\telse\n\t\tcsstart = skb_headlen(skb);\n\n\tBUG_ON(csstart > skb_headlen(skb));\n\n\tskb_copy_from_linear_data(skb, to, csstart);\n\n\tcsum = 0;\n\tif (csstart != skb->len)\n\t\tcsum = skb_copy_and_csum_bits(skb, csstart, to + csstart,\n\t\t\t\t\t      skb->len - csstart);\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tlong csstuff = csstart + skb->csum_offset;\n\n\t\t*((__sum16 *)(to + csstuff)) = csum_fold(csum);\n\t}\n}\nEXPORT_SYMBOL(skb_copy_and_csum_dev);\n\n/**\n *\tskb_dequeue - remove from the head of the queue\n *\t@list: list to dequeue from\n *\n *\tRemove the head of the list. The list lock is taken so the function\n *\tmay be used safely with other locking list functions. The head item is\n *\treturned or %NULL if the list is empty.\n */\n\nstruct sk_buff *skb_dequeue(struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\tstruct sk_buff *result;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\tresult = __skb_dequeue(list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n\treturn result;\n}\nEXPORT_SYMBOL(skb_dequeue);\n\n/**\n *\tskb_dequeue_tail - remove from the tail of the queue\n *\t@list: list to dequeue from\n *\n *\tRemove the tail of the list. The list lock is taken so the function\n *\tmay be used safely with other locking list functions. The tail item is\n *\treturned or %NULL if the list is empty.\n */\nstruct sk_buff *skb_dequeue_tail(struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\tstruct sk_buff *result;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\tresult = __skb_dequeue_tail(list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n\treturn result;\n}\nEXPORT_SYMBOL(skb_dequeue_tail);\n\n/**\n *\tskb_queue_purge - empty a list\n *\t@list: list to empty\n *\n *\tDelete all buffers on an &sk_buff list. Each buffer is removed from\n *\tthe list and one reference dropped. This function takes the list\n *\tlock and is atomic with respect to other list locking functions.\n */\nvoid skb_queue_purge(struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb;\n\twhile ((skb = skb_dequeue(list)) != NULL)\n\t\tkfree_skb(skb);\n}\nEXPORT_SYMBOL(skb_queue_purge);\n\n/**\n *\tskb_rbtree_purge - empty a skb rbtree\n *\t@root: root of the rbtree to empty\n *\tReturn value: the sum of truesizes of all purged skbs.\n *\n *\tDelete all buffers on an &sk_buff rbtree. Each buffer is removed from\n *\tthe list and one reference dropped. This function does not take\n *\tany lock. Synchronization should be handled by the caller (e.g., TCP\n *\tout-of-order queue is protected by the socket lock).\n */\nunsigned int skb_rbtree_purge(struct rb_root *root)\n{\n\tstruct rb_node *p = rb_first(root);\n\tunsigned int sum = 0;\n\n\twhile (p) {\n\t\tstruct sk_buff *skb = rb_entry(p, struct sk_buff, rbnode);\n\n\t\tp = rb_next(p);\n\t\trb_erase(&skb->rbnode, root);\n\t\tsum += skb->truesize;\n\t\tkfree_skb(skb);\n\t}\n\treturn sum;\n}\n\n/**\n *\tskb_queue_head - queue a buffer at the list head\n *\t@list: list to use\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer at the start of the list. This function takes the\n *\tlist lock and can be used safely with other locking &sk_buff functions\n *\tsafely.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nvoid skb_queue_head(struct sk_buff_head *list, struct sk_buff *newsk)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_queue_head(list, newsk);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_queue_head);\n\n/**\n *\tskb_queue_tail - queue a buffer at the list tail\n *\t@list: list to use\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer at the tail of the list. This function takes the\n *\tlist lock and can be used safely with other locking &sk_buff functions\n *\tsafely.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nvoid skb_queue_tail(struct sk_buff_head *list, struct sk_buff *newsk)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_queue_tail(list, newsk);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_queue_tail);\n\n/**\n *\tskb_unlink\t-\tremove a buffer from a list\n *\t@skb: buffer to remove\n *\t@list: list to use\n *\n *\tRemove a packet from a list. The list locks are taken and this\n *\tfunction is atomic with respect to other list locked calls\n *\n *\tYou must know what list the SKB is on.\n */\nvoid skb_unlink(struct sk_buff *skb, struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_unlink(skb, list);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_unlink);\n\n/**\n *\tskb_append\t-\tappend a buffer\n *\t@old: buffer to insert after\n *\t@newsk: buffer to insert\n *\t@list: list to use\n *\n *\tPlace a packet after a given packet in a list. The list locks are taken\n *\tand this function is atomic with respect to other list locked calls.\n *\tA buffer cannot be placed on two lists at the same time.\n */\nvoid skb_append(struct sk_buff *old, struct sk_buff *newsk, struct sk_buff_head *list)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list->lock, flags);\n\t__skb_queue_after(list, old, newsk);\n\tspin_unlock_irqrestore(&list->lock, flags);\n}\nEXPORT_SYMBOL(skb_append);\n\nstatic inline void skb_split_inside_header(struct sk_buff *skb,\n\t\t\t\t\t   struct sk_buff* skb1,\n\t\t\t\t\t   const u32 len, const int pos)\n{\n\tint i;\n\n\tskb_copy_from_linear_data_offset(skb, len, skb_put(skb1, pos - len),\n\t\t\t\t\t pos - len);\n\t/* And move data appendix as is. */\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++)\n\t\tskb_shinfo(skb1)->frags[i] = skb_shinfo(skb)->frags[i];\n\n\tskb_shinfo(skb1)->nr_frags = skb_shinfo(skb)->nr_frags;\n\tskb_shinfo(skb)->nr_frags  = 0;\n\tskb1->data_len\t\t   = skb->data_len;\n\tskb1->len\t\t   += skb1->data_len;\n\tskb->data_len\t\t   = 0;\n\tskb->len\t\t   = len;\n\tskb_set_tail_pointer(skb, len);\n}\n\nstatic inline void skb_split_no_header(struct sk_buff *skb,\n\t\t\t\t       struct sk_buff* skb1,\n\t\t\t\t       const u32 len, int pos)\n{\n\tint i, k = 0;\n\tconst int nfrags = skb_shinfo(skb)->nr_frags;\n\n\tskb_shinfo(skb)->nr_frags = 0;\n\tskb1->len\t\t  = skb1->data_len = skb->len - len;\n\tskb->len\t\t  = len;\n\tskb->data_len\t\t  = len - pos;\n\n\tfor (i = 0; i < nfrags; i++) {\n\t\tint size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (pos + size > len) {\n\t\t\tskb_shinfo(skb1)->frags[k] = skb_shinfo(skb)->frags[i];\n\n\t\t\tif (pos < len) {\n\t\t\t\t/* Split frag.\n\t\t\t\t * We have two variants in this case:\n\t\t\t\t * 1. Move all the frag to the second\n\t\t\t\t *    part, if it is possible. F.e.\n\t\t\t\t *    this approach is mandatory for TUX,\n\t\t\t\t *    where splitting is expensive.\n\t\t\t\t * 2. Split is accurately. We make this.\n\t\t\t\t */\n\t\t\t\tskb_frag_ref(skb, i);\n\t\t\t\tskb_frag_off_add(&skb_shinfo(skb1)->frags[0], len - pos);\n\t\t\t\tskb_frag_size_sub(&skb_shinfo(skb1)->frags[0], len - pos);\n\t\t\t\tskb_frag_size_set(&skb_shinfo(skb)->frags[i], len - pos);\n\t\t\t\tskb_shinfo(skb)->nr_frags++;\n\t\t\t}\n\t\t\tk++;\n\t\t} else\n\t\t\tskb_shinfo(skb)->nr_frags++;\n\t\tpos += size;\n\t}\n\tskb_shinfo(skb1)->nr_frags = k;\n}\n\n/**\n * skb_split - Split fragmented skb to two parts at length len.\n * @skb: the buffer to split\n * @skb1: the buffer to receive the second part\n * @len: new length for skb\n */\nvoid skb_split(struct sk_buff *skb, struct sk_buff *skb1, const u32 len)\n{\n\tint pos = skb_headlen(skb);\n\n\tskb_shinfo(skb1)->flags |= skb_shinfo(skb)->flags & SKBFL_SHARED_FRAG;\n\tskb_zerocopy_clone(skb1, skb, 0);\n\tif (len < pos)\t/* Split line is inside header. */\n\t\tskb_split_inside_header(skb, skb1, len, pos);\n\telse\t\t/* Second chunk has no header, nothing to copy. */\n\t\tskb_split_no_header(skb, skb1, len, pos);\n}\nEXPORT_SYMBOL(skb_split);\n\n/* Shifting from/to a cloned skb is a no-go.\n *\n * Caller cannot keep skb_shinfo related pointers past calling here!\n */\nstatic int skb_prepare_for_shift(struct sk_buff *skb)\n{\n\tint ret = 0;\n\n\tif (skb_cloned(skb)) {\n\t\t/* Save and restore truesize: pskb_expand_head() may reallocate\n\t\t * memory where ksize(kmalloc(S)) != ksize(kmalloc(S)), but we\n\t\t * cannot change truesize at this point.\n\t\t */\n\t\tunsigned int save_truesize = skb->truesize;\n\n\t\tret = pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n\t\tskb->truesize = save_truesize;\n\t}\n\treturn ret;\n}\n\n/**\n * skb_shift - Shifts paged data partially from skb to another\n * @tgt: buffer into which tail data gets added\n * @skb: buffer from which the paged data comes from\n * @shiftlen: shift up to this many bytes\n *\n * Attempts to shift up to shiftlen worth of bytes, which may be less than\n * the length of the skb, from skb to tgt. Returns number bytes shifted.\n * It's up to caller to free skb if everything was shifted.\n *\n * If @tgt runs out of frags, the whole operation is aborted.\n *\n * Skb cannot include anything else but paged data while tgt is allowed\n * to have non-paged data as well.\n *\n * TODO: full sized shift could be optimized but that would need\n * specialized skb free'er to handle frags without up-to-date nr_frags.\n */\nint skb_shift(struct sk_buff *tgt, struct sk_buff *skb, int shiftlen)\n{\n\tint from, to, merge, todo;\n\tskb_frag_t *fragfrom, *fragto;\n\n\tBUG_ON(shiftlen > skb->len);\n\n\tif (skb_headlen(skb))\n\t\treturn 0;\n\tif (skb_zcopy(tgt) || skb_zcopy(skb))\n\t\treturn 0;\n\n\ttodo = shiftlen;\n\tfrom = 0;\n\tto = skb_shinfo(tgt)->nr_frags;\n\tfragfrom = &skb_shinfo(skb)->frags[from];\n\n\t/* Actual merge is delayed until the point when we know we can\n\t * commit all, so that we don't have to undo partial changes\n\t */\n\tif (!to ||\n\t    !skb_can_coalesce(tgt, to, skb_frag_page(fragfrom),\n\t\t\t      skb_frag_off(fragfrom))) {\n\t\tmerge = -1;\n\t} else {\n\t\tmerge = to - 1;\n\n\t\ttodo -= skb_frag_size(fragfrom);\n\t\tif (todo < 0) {\n\t\t\tif (skb_prepare_for_shift(skb) ||\n\t\t\t    skb_prepare_for_shift(tgt))\n\t\t\t\treturn 0;\n\n\t\t\t/* All previous frag pointers might be stale! */\n\t\t\tfragfrom = &skb_shinfo(skb)->frags[from];\n\t\t\tfragto = &skb_shinfo(tgt)->frags[merge];\n\n\t\t\tskb_frag_size_add(fragto, shiftlen);\n\t\t\tskb_frag_size_sub(fragfrom, shiftlen);\n\t\t\tskb_frag_off_add(fragfrom, shiftlen);\n\n\t\t\tgoto onlymerged;\n\t\t}\n\n\t\tfrom++;\n\t}\n\n\t/* Skip full, not-fitting skb to avoid expensive operations */\n\tif ((shiftlen == skb->len) &&\n\t    (skb_shinfo(skb)->nr_frags - from) > (MAX_SKB_FRAGS - to))\n\t\treturn 0;\n\n\tif (skb_prepare_for_shift(skb) || skb_prepare_for_shift(tgt))\n\t\treturn 0;\n\n\twhile ((todo > 0) && (from < skb_shinfo(skb)->nr_frags)) {\n\t\tif (to == MAX_SKB_FRAGS)\n\t\t\treturn 0;\n\n\t\tfragfrom = &skb_shinfo(skb)->frags[from];\n\t\tfragto = &skb_shinfo(tgt)->frags[to];\n\n\t\tif (todo >= skb_frag_size(fragfrom)) {\n\t\t\t*fragto = *fragfrom;\n\t\t\ttodo -= skb_frag_size(fragfrom);\n\t\t\tfrom++;\n\t\t\tto++;\n\n\t\t} else {\n\t\t\t__skb_frag_ref(fragfrom);\n\t\t\tskb_frag_page_copy(fragto, fragfrom);\n\t\t\tskb_frag_off_copy(fragto, fragfrom);\n\t\t\tskb_frag_size_set(fragto, todo);\n\n\t\t\tskb_frag_off_add(fragfrom, todo);\n\t\t\tskb_frag_size_sub(fragfrom, todo);\n\t\t\ttodo = 0;\n\n\t\t\tto++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Ready to \"commit\" this state change to tgt */\n\tskb_shinfo(tgt)->nr_frags = to;\n\n\tif (merge >= 0) {\n\t\tfragfrom = &skb_shinfo(skb)->frags[0];\n\t\tfragto = &skb_shinfo(tgt)->frags[merge];\n\n\t\tskb_frag_size_add(fragto, skb_frag_size(fragfrom));\n\t\t__skb_frag_unref(fragfrom);\n\t}\n\n\t/* Reposition in the original skb */\n\tto = 0;\n\twhile (from < skb_shinfo(skb)->nr_frags)\n\t\tskb_shinfo(skb)->frags[to++] = skb_shinfo(skb)->frags[from++];\n\tskb_shinfo(skb)->nr_frags = to;\n\n\tBUG_ON(todo > 0 && !skb_shinfo(skb)->nr_frags);\n\nonlymerged:\n\t/* Most likely the tgt won't ever need its checksum anymore, skb on\n\t * the other hand might need it if it needs to be resent\n\t */\n\ttgt->ip_summed = CHECKSUM_PARTIAL;\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\n\t/* Yak, is it really working this way? Some helper please? */\n\tskb->len -= shiftlen;\n\tskb->data_len -= shiftlen;\n\tskb->truesize -= shiftlen;\n\ttgt->len += shiftlen;\n\ttgt->data_len += shiftlen;\n\ttgt->truesize += shiftlen;\n\n\treturn shiftlen;\n}\n\n/**\n * skb_prepare_seq_read - Prepare a sequential read of skb data\n * @skb: the buffer to read\n * @from: lower offset of data to be read\n * @to: upper offset of data to be read\n * @st: state variable\n *\n * Initializes the specified state variable. Must be called before\n * invoking skb_seq_read() for the first time.\n */\nvoid skb_prepare_seq_read(struct sk_buff *skb, unsigned int from,\n\t\t\t  unsigned int to, struct skb_seq_state *st)\n{\n\tst->lower_offset = from;\n\tst->upper_offset = to;\n\tst->root_skb = st->cur_skb = skb;\n\tst->frag_idx = st->stepped_offset = 0;\n\tst->frag_data = NULL;\n\tst->frag_off = 0;\n}\nEXPORT_SYMBOL(skb_prepare_seq_read);\n\n/**\n * skb_seq_read - Sequentially read skb data\n * @consumed: number of bytes consumed by the caller so far\n * @data: destination pointer for data to be returned\n * @st: state variable\n *\n * Reads a block of skb data at @consumed relative to the\n * lower offset specified to skb_prepare_seq_read(). Assigns\n * the head of the data block to @data and returns the length\n * of the block or 0 if the end of the skb data or the upper\n * offset has been reached.\n *\n * The caller is not required to consume all of the data\n * returned, i.e. @consumed is typically set to the number\n * of bytes already consumed and the next call to\n * skb_seq_read() will return the remaining part of the block.\n *\n * Note 1: The size of each block of data returned can be arbitrary,\n *       this limitation is the cost for zerocopy sequential\n *       reads of potentially non linear data.\n *\n * Note 2: Fragment lists within fragments are not implemented\n *       at the moment, state->root_skb could be replaced with\n *       a stack for this purpose.\n */\nunsigned int skb_seq_read(unsigned int consumed, const u8 **data,\n\t\t\t  struct skb_seq_state *st)\n{\n\tunsigned int block_limit, abs_offset = consumed + st->lower_offset;\n\tskb_frag_t *frag;\n\n\tif (unlikely(abs_offset >= st->upper_offset)) {\n\t\tif (st->frag_data) {\n\t\t\tkunmap_atomic(st->frag_data);\n\t\t\tst->frag_data = NULL;\n\t\t}\n\t\treturn 0;\n\t}\n\nnext_skb:\n\tblock_limit = skb_headlen(st->cur_skb) + st->stepped_offset;\n\n\tif (abs_offset < block_limit && !st->frag_data) {\n\t\t*data = st->cur_skb->data + (abs_offset - st->stepped_offset);\n\t\treturn block_limit - abs_offset;\n\t}\n\n\tif (st->frag_idx == 0 && !st->frag_data)\n\t\tst->stepped_offset += skb_headlen(st->cur_skb);\n\n\twhile (st->frag_idx < skb_shinfo(st->cur_skb)->nr_frags) {\n\t\tunsigned int pg_idx, pg_off, pg_sz;\n\n\t\tfrag = &skb_shinfo(st->cur_skb)->frags[st->frag_idx];\n\n\t\tpg_idx = 0;\n\t\tpg_off = skb_frag_off(frag);\n\t\tpg_sz = skb_frag_size(frag);\n\n\t\tif (skb_frag_must_loop(skb_frag_page(frag))) {\n\t\t\tpg_idx = (pg_off + st->frag_off) >> PAGE_SHIFT;\n\t\t\tpg_off = offset_in_page(pg_off + st->frag_off);\n\t\t\tpg_sz = min_t(unsigned int, pg_sz - st->frag_off,\n\t\t\t\t\t\t    PAGE_SIZE - pg_off);\n\t\t}\n\n\t\tblock_limit = pg_sz + st->stepped_offset;\n\t\tif (abs_offset < block_limit) {\n\t\t\tif (!st->frag_data)\n\t\t\t\tst->frag_data = kmap_atomic(skb_frag_page(frag) + pg_idx);\n\n\t\t\t*data = (u8 *)st->frag_data + pg_off +\n\t\t\t\t(abs_offset - st->stepped_offset);\n\n\t\t\treturn block_limit - abs_offset;\n\t\t}\n\n\t\tif (st->frag_data) {\n\t\t\tkunmap_atomic(st->frag_data);\n\t\t\tst->frag_data = NULL;\n\t\t}\n\n\t\tst->stepped_offset += pg_sz;\n\t\tst->frag_off += pg_sz;\n\t\tif (st->frag_off == skb_frag_size(frag)) {\n\t\t\tst->frag_off = 0;\n\t\t\tst->frag_idx++;\n\t\t}\n\t}\n\n\tif (st->frag_data) {\n\t\tkunmap_atomic(st->frag_data);\n\t\tst->frag_data = NULL;\n\t}\n\n\tif (st->root_skb == st->cur_skb && skb_has_frag_list(st->root_skb)) {\n\t\tst->cur_skb = skb_shinfo(st->root_skb)->frag_list;\n\t\tst->frag_idx = 0;\n\t\tgoto next_skb;\n\t} else if (st->cur_skb->next) {\n\t\tst->cur_skb = st->cur_skb->next;\n\t\tst->frag_idx = 0;\n\t\tgoto next_skb;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_seq_read);\n\n/**\n * skb_abort_seq_read - Abort a sequential read of skb data\n * @st: state variable\n *\n * Must be called if skb_seq_read() was not called until it\n * returned 0.\n */\nvoid skb_abort_seq_read(struct skb_seq_state *st)\n{\n\tif (st->frag_data)\n\t\tkunmap_atomic(st->frag_data);\n}\nEXPORT_SYMBOL(skb_abort_seq_read);\n\n#define TS_SKB_CB(state)\t((struct skb_seq_state *) &((state)->cb))\n\nstatic unsigned int skb_ts_get_next_block(unsigned int offset, const u8 **text,\n\t\t\t\t\t  struct ts_config *conf,\n\t\t\t\t\t  struct ts_state *state)\n{\n\treturn skb_seq_read(offset, text, TS_SKB_CB(state));\n}\n\nstatic void skb_ts_finish(struct ts_config *conf, struct ts_state *state)\n{\n\tskb_abort_seq_read(TS_SKB_CB(state));\n}\n\n/**\n * skb_find_text - Find a text pattern in skb data\n * @skb: the buffer to look in\n * @from: search offset\n * @to: search limit\n * @config: textsearch configuration\n *\n * Finds a pattern in the skb data according to the specified\n * textsearch configuration. Use textsearch_next() to retrieve\n * subsequent occurrences of the pattern. Returns the offset\n * to the first occurrence or UINT_MAX if no match was found.\n */\nunsigned int skb_find_text(struct sk_buff *skb, unsigned int from,\n\t\t\t   unsigned int to, struct ts_config *config)\n{\n\tstruct ts_state state;\n\tunsigned int ret;\n\n\tBUILD_BUG_ON(sizeof(struct skb_seq_state) > sizeof(state.cb));\n\n\tconfig->get_next_block = skb_ts_get_next_block;\n\tconfig->finish = skb_ts_finish;\n\n\tskb_prepare_seq_read(skb, from, to, TS_SKB_CB(&state));\n\n\tret = textsearch_find(config, &state);\n\treturn (ret <= to - from ? ret : UINT_MAX);\n}\nEXPORT_SYMBOL(skb_find_text);\n\nint skb_append_pagefrags(struct sk_buff *skb, struct page *page,\n\t\t\t int offset, size_t size)\n{\n\tint i = skb_shinfo(skb)->nr_frags;\n\n\tif (skb_can_coalesce(skb, i, page, offset)) {\n\t\tskb_frag_size_add(&skb_shinfo(skb)->frags[i - 1], size);\n\t} else if (i < MAX_SKB_FRAGS) {\n\t\tget_page(page);\n\t\tskb_fill_page_desc(skb, i, page, offset, size);\n\t} else {\n\t\treturn -EMSGSIZE;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_append_pagefrags);\n\n/**\n *\tskb_pull_rcsum - pull skb and update receive checksum\n *\t@skb: buffer to update\n *\t@len: length of data pulled\n *\n *\tThis function performs an skb_pull on the packet and updates\n *\tthe CHECKSUM_COMPLETE checksum.  It should be used on\n *\treceive path processing instead of skb_pull unless you know\n *\tthat the checksum difference is zero (e.g., a valid IP header)\n *\tor you are setting ip_summed to CHECKSUM_NONE.\n */\nvoid *skb_pull_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tunsigned char *data = skb->data;\n\n\tBUG_ON(len > skb->len);\n\t__skb_pull(skb, len);\n\tskb_postpull_rcsum(skb, data, len);\n\treturn skb->data;\n}\nEXPORT_SYMBOL_GPL(skb_pull_rcsum);\n\nstatic inline skb_frag_t skb_head_frag_to_page_desc(struct sk_buff *frag_skb)\n{\n\tskb_frag_t head_frag;\n\tstruct page *page;\n\n\tpage = virt_to_head_page(frag_skb->head);\n\t__skb_frag_set_page(&head_frag, page);\n\tskb_frag_off_set(&head_frag, frag_skb->data -\n\t\t\t (unsigned char *)page_address(page));\n\tskb_frag_size_set(&head_frag, skb_headlen(frag_skb));\n\treturn head_frag;\n}\n\nstruct sk_buff *skb_segment_list(struct sk_buff *skb,\n\t\t\t\t netdev_features_t features,\n\t\t\t\t unsigned int offset)\n{\n\tstruct sk_buff *list_skb = skb_shinfo(skb)->frag_list;\n\tunsigned int tnl_hlen = skb_tnl_header_len(skb);\n\tunsigned int delta_truesize = 0;\n\tunsigned int delta_len = 0;\n\tstruct sk_buff *nskb, *tmp;\n\tint err;\n\n\tskb_push(skb, -skb_network_offset(skb) + offset);\n\n\tskb_shinfo(skb)->frag_list = NULL;\n\tskb->next = list_skb;\n\n\tdo {\n\t\tnskb = list_skb;\n\t\tlist_skb = list_skb->next;\n\n\t\terr = 0;\n\t\tif (skb_shared(nskb)) {\n\t\t\ttmp = skb_clone(nskb, GFP_ATOMIC);\n\t\t\tif (tmp) {\n\t\t\t\tconsume_skb(nskb);\n\t\t\t\tnskb = tmp;\n\t\t\t\terr = skb_unclone(nskb, GFP_ATOMIC);\n\t\t\t} else {\n\t\t\t\terr = -ENOMEM;\n\t\t\t}\n\t\t}\n\n\t\tif (unlikely(err))\n\t\t\tgoto err_linearize;\n\n\t\tdelta_len += nskb->len;\n\t\tdelta_truesize += nskb->truesize;\n\n\t\tskb_push(nskb, -skb_network_offset(nskb) + offset);\n\n\t\tskb_release_head_state(nskb);\n\t\t __copy_skb_header(nskb, skb);\n\n\t\tskb_headers_offset_update(nskb, skb_headroom(nskb) - skb_headroom(skb));\n\t\tskb_copy_from_linear_data_offset(skb, -tnl_hlen,\n\t\t\t\t\t\t nskb->data - tnl_hlen,\n\t\t\t\t\t\t offset + tnl_hlen);\n\n\t\tif (skb_needs_linearize(nskb, features) &&\n\t\t    __skb_linearize(nskb))\n\t\t\tgoto err_linearize;\n\n\t} while (list_skb);\n\n\tskb->truesize = skb->truesize - delta_truesize;\n\tskb->data_len = skb->data_len - delta_len;\n\tskb->len = skb->len - delta_len;\n\n\tskb_gso_reset(skb);\n\n\tskb->prev = nskb;\n\n\tif (skb_needs_linearize(skb, features) &&\n\t    __skb_linearize(skb))\n\t\tgoto err_linearize;\n\n\tskb_get(skb);\n\n\treturn skb;\n\nerr_linearize:\n\tkfree_skb_list(skb->next);\n\tskb->next = NULL;\n\treturn ERR_PTR(-ENOMEM);\n}\nEXPORT_SYMBOL_GPL(skb_segment_list);\n\nint skb_gro_receive_list(struct sk_buff *p, struct sk_buff *skb)\n{\n\tif (unlikely(p->len + skb->len >= 65536))\n\t\treturn -E2BIG;\n\n\tif (NAPI_GRO_CB(p)->last == p)\n\t\tskb_shinfo(p)->frag_list = skb;\n\telse\n\t\tNAPI_GRO_CB(p)->last->next = skb;\n\n\tskb_pull(skb, skb_gro_offset(skb));\n\n\tNAPI_GRO_CB(p)->last = skb;\n\tNAPI_GRO_CB(p)->count++;\n\tp->data_len += skb->len;\n\tp->truesize += skb->truesize;\n\tp->len += skb->len;\n\n\tNAPI_GRO_CB(skb)->same_flow = 1;\n\n\treturn 0;\n}\n\n/**\n *\tskb_segment - Perform protocol segmentation on skb.\n *\t@head_skb: buffer to segment\n *\t@features: features for the output path (see dev->features)\n *\n *\tThis function performs segmentation on the given skb.  It returns\n *\ta pointer to the first in a list of new skbs for the segments.\n *\tIn case of error it returns ERR_PTR(err).\n */\nstruct sk_buff *skb_segment(struct sk_buff *head_skb,\n\t\t\t    netdev_features_t features)\n{\n\tstruct sk_buff *segs = NULL;\n\tstruct sk_buff *tail = NULL;\n\tstruct sk_buff *list_skb = skb_shinfo(head_skb)->frag_list;\n\tskb_frag_t *frag = skb_shinfo(head_skb)->frags;\n\tunsigned int mss = skb_shinfo(head_skb)->gso_size;\n\tunsigned int doffset = head_skb->data - skb_mac_header(head_skb);\n\tstruct sk_buff *frag_skb = head_skb;\n\tunsigned int offset = doffset;\n\tunsigned int tnl_hlen = skb_tnl_header_len(head_skb);\n\tunsigned int partial_segs = 0;\n\tunsigned int headroom;\n\tunsigned int len = head_skb->len;\n\t__be16 proto;\n\tbool csum, sg;\n\tint nfrags = skb_shinfo(head_skb)->nr_frags;\n\tint err = -ENOMEM;\n\tint i = 0;\n\tint pos;\n\n\tif (list_skb && !list_skb->head_frag && skb_headlen(list_skb) &&\n\t    (skb_shinfo(head_skb)->gso_type & SKB_GSO_DODGY)) {\n\t\t/* gso_size is untrusted, and we have a frag_list with a linear\n\t\t * non head_frag head.\n\t\t *\n\t\t * (we assume checking the first list_skb member suffices;\n\t\t * i.e if either of the list_skb members have non head_frag\n\t\t * head, then the first one has too).\n\t\t *\n\t\t * If head_skb's headlen does not fit requested gso_size, it\n\t\t * means that the frag_list members do NOT terminate on exact\n\t\t * gso_size boundaries. Hence we cannot perform skb_frag_t page\n\t\t * sharing. Therefore we must fallback to copying the frag_list\n\t\t * skbs; we do so by disabling SG.\n\t\t */\n\t\tif (mss != GSO_BY_FRAGS && mss != skb_headlen(head_skb))\n\t\t\tfeatures &= ~NETIF_F_SG;\n\t}\n\n\t__skb_push(head_skb, doffset);\n\tproto = skb_network_protocol(head_skb, NULL);\n\tif (unlikely(!proto))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tsg = !!(features & NETIF_F_SG);\n\tcsum = !!can_checksum_protocol(features, proto);\n\n\tif (sg && csum && (mss != GSO_BY_FRAGS))  {\n\t\tif (!(features & NETIF_F_GSO_PARTIAL)) {\n\t\t\tstruct sk_buff *iter;\n\t\t\tunsigned int frag_len;\n\n\t\t\tif (!list_skb ||\n\t\t\t    !net_gso_ok(features, skb_shinfo(head_skb)->gso_type))\n\t\t\t\tgoto normal;\n\n\t\t\t/* If we get here then all the required\n\t\t\t * GSO features except frag_list are supported.\n\t\t\t * Try to split the SKB to multiple GSO SKBs\n\t\t\t * with no frag_list.\n\t\t\t * Currently we can do that only when the buffers don't\n\t\t\t * have a linear part and all the buffers except\n\t\t\t * the last are of the same length.\n\t\t\t */\n\t\t\tfrag_len = list_skb->len;\n\t\t\tskb_walk_frags(head_skb, iter) {\n\t\t\t\tif (frag_len != iter->len && iter->next)\n\t\t\t\t\tgoto normal;\n\t\t\t\tif (skb_headlen(iter) && !iter->head_frag)\n\t\t\t\t\tgoto normal;\n\n\t\t\t\tlen -= iter->len;\n\t\t\t}\n\n\t\t\tif (len != frag_len)\n\t\t\t\tgoto normal;\n\t\t}\n\n\t\t/* GSO partial only requires that we trim off any excess that\n\t\t * doesn't fit into an MSS sized block, so take care of that\n\t\t * now.\n\t\t */\n\t\tpartial_segs = len / mss;\n\t\tif (partial_segs > 1)\n\t\t\tmss *= partial_segs;\n\t\telse\n\t\t\tpartial_segs = 0;\n\t}\n\nnormal:\n\theadroom = skb_headroom(head_skb);\n\tpos = skb_headlen(head_skb);\n\n\tdo {\n\t\tstruct sk_buff *nskb;\n\t\tskb_frag_t *nskb_frag;\n\t\tint hsize;\n\t\tint size;\n\n\t\tif (unlikely(mss == GSO_BY_FRAGS)) {\n\t\t\tlen = list_skb->len;\n\t\t} else {\n\t\t\tlen = head_skb->len - offset;\n\t\t\tif (len > mss)\n\t\t\t\tlen = mss;\n\t\t}\n\n\t\thsize = skb_headlen(head_skb) - offset;\n\n\t\tif (hsize <= 0 && i >= nfrags && skb_headlen(list_skb) &&\n\t\t    (skb_headlen(list_skb) == len || sg)) {\n\t\t\tBUG_ON(skb_headlen(list_skb) > len);\n\n\t\t\ti = 0;\n\t\t\tnfrags = skb_shinfo(list_skb)->nr_frags;\n\t\t\tfrag = skb_shinfo(list_skb)->frags;\n\t\t\tfrag_skb = list_skb;\n\t\t\tpos += skb_headlen(list_skb);\n\n\t\t\twhile (pos < offset + len) {\n\t\t\t\tBUG_ON(i >= nfrags);\n\n\t\t\t\tsize = skb_frag_size(frag);\n\t\t\t\tif (pos + size > offset + len)\n\t\t\t\t\tbreak;\n\n\t\t\t\ti++;\n\t\t\t\tpos += size;\n\t\t\t\tfrag++;\n\t\t\t}\n\n\t\t\tnskb = skb_clone(list_skb, GFP_ATOMIC);\n\t\t\tlist_skb = list_skb->next;\n\n\t\t\tif (unlikely(!nskb))\n\t\t\t\tgoto err;\n\n\t\t\tif (unlikely(pskb_trim(nskb, len))) {\n\t\t\t\tkfree_skb(nskb);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\thsize = skb_end_offset(nskb);\n\t\t\tif (skb_cow_head(nskb, doffset + headroom)) {\n\t\t\t\tkfree_skb(nskb);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tnskb->truesize += skb_end_offset(nskb) - hsize;\n\t\t\tskb_release_head_state(nskb);\n\t\t\t__skb_push(nskb, doffset);\n\t\t} else {\n\t\t\tif (hsize < 0)\n\t\t\t\thsize = 0;\n\t\t\tif (hsize > len || !sg)\n\t\t\t\thsize = len;\n\n\t\t\tnskb = __alloc_skb(hsize + doffset + headroom,\n\t\t\t\t\t   GFP_ATOMIC, skb_alloc_rx_flag(head_skb),\n\t\t\t\t\t   NUMA_NO_NODE);\n\n\t\t\tif (unlikely(!nskb))\n\t\t\t\tgoto err;\n\n\t\t\tskb_reserve(nskb, headroom);\n\t\t\t__skb_put(nskb, doffset);\n\t\t}\n\n\t\tif (segs)\n\t\t\ttail->next = nskb;\n\t\telse\n\t\t\tsegs = nskb;\n\t\ttail = nskb;\n\n\t\t__copy_skb_header(nskb, head_skb);\n\n\t\tskb_headers_offset_update(nskb, skb_headroom(nskb) - headroom);\n\t\tskb_reset_mac_len(nskb);\n\n\t\tskb_copy_from_linear_data_offset(head_skb, -tnl_hlen,\n\t\t\t\t\t\t nskb->data - tnl_hlen,\n\t\t\t\t\t\t doffset + tnl_hlen);\n\n\t\tif (nskb->len == len + doffset)\n\t\t\tgoto perform_csum_check;\n\n\t\tif (!sg) {\n\t\t\tif (!csum) {\n\t\t\t\tif (!nskb->remcsum_offload)\n\t\t\t\t\tnskb->ip_summed = CHECKSUM_NONE;\n\t\t\t\tSKB_GSO_CB(nskb)->csum =\n\t\t\t\t\tskb_copy_and_csum_bits(head_skb, offset,\n\t\t\t\t\t\t\t       skb_put(nskb,\n\t\t\t\t\t\t\t\t       len),\n\t\t\t\t\t\t\t       len);\n\t\t\t\tSKB_GSO_CB(nskb)->csum_start =\n\t\t\t\t\tskb_headroom(nskb) + doffset;\n\t\t\t} else {\n\t\t\t\tskb_copy_bits(head_skb, offset,\n\t\t\t\t\t      skb_put(nskb, len),\n\t\t\t\t\t      len);\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tnskb_frag = skb_shinfo(nskb)->frags;\n\n\t\tskb_copy_from_linear_data_offset(head_skb, offset,\n\t\t\t\t\t\t skb_put(nskb, hsize), hsize);\n\n\t\tskb_shinfo(nskb)->flags |= skb_shinfo(head_skb)->flags &\n\t\t\t\t\t   SKBFL_SHARED_FRAG;\n\n\t\tif (skb_orphan_frags(frag_skb, GFP_ATOMIC) ||\n\t\t    skb_zerocopy_clone(nskb, frag_skb, GFP_ATOMIC))\n\t\t\tgoto err;\n\n\t\twhile (pos < offset + len) {\n\t\t\tif (i >= nfrags) {\n\t\t\t\ti = 0;\n\t\t\t\tnfrags = skb_shinfo(list_skb)->nr_frags;\n\t\t\t\tfrag = skb_shinfo(list_skb)->frags;\n\t\t\t\tfrag_skb = list_skb;\n\t\t\t\tif (!skb_headlen(list_skb)) {\n\t\t\t\t\tBUG_ON(!nfrags);\n\t\t\t\t} else {\n\t\t\t\t\tBUG_ON(!list_skb->head_frag);\n\n\t\t\t\t\t/* to make room for head_frag. */\n\t\t\t\t\ti--;\n\t\t\t\t\tfrag--;\n\t\t\t\t}\n\t\t\t\tif (skb_orphan_frags(frag_skb, GFP_ATOMIC) ||\n\t\t\t\t    skb_zerocopy_clone(nskb, frag_skb,\n\t\t\t\t\t\t       GFP_ATOMIC))\n\t\t\t\t\tgoto err;\n\n\t\t\t\tlist_skb = list_skb->next;\n\t\t\t}\n\n\t\t\tif (unlikely(skb_shinfo(nskb)->nr_frags >=\n\t\t\t\t     MAX_SKB_FRAGS)) {\n\t\t\t\tnet_warn_ratelimited(\n\t\t\t\t\t\"skb_segment: too many frags: %u %u\\n\",\n\t\t\t\t\tpos, mss);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\t*nskb_frag = (i < 0) ? skb_head_frag_to_page_desc(frag_skb) : *frag;\n\t\t\t__skb_frag_ref(nskb_frag);\n\t\t\tsize = skb_frag_size(nskb_frag);\n\n\t\t\tif (pos < offset) {\n\t\t\t\tskb_frag_off_add(nskb_frag, offset - pos);\n\t\t\t\tskb_frag_size_sub(nskb_frag, offset - pos);\n\t\t\t}\n\n\t\t\tskb_shinfo(nskb)->nr_frags++;\n\n\t\t\tif (pos + size <= offset + len) {\n\t\t\t\ti++;\n\t\t\t\tfrag++;\n\t\t\t\tpos += size;\n\t\t\t} else {\n\t\t\t\tskb_frag_size_sub(nskb_frag, pos + size - (offset + len));\n\t\t\t\tgoto skip_fraglist;\n\t\t\t}\n\n\t\t\tnskb_frag++;\n\t\t}\n\nskip_fraglist:\n\t\tnskb->data_len = len - hsize;\n\t\tnskb->len += nskb->data_len;\n\t\tnskb->truesize += nskb->data_len;\n\nperform_csum_check:\n\t\tif (!csum) {\n\t\t\tif (skb_has_shared_frag(nskb) &&\n\t\t\t    __skb_linearize(nskb))\n\t\t\t\tgoto err;\n\n\t\t\tif (!nskb->remcsum_offload)\n\t\t\t\tnskb->ip_summed = CHECKSUM_NONE;\n\t\t\tSKB_GSO_CB(nskb)->csum =\n\t\t\t\tskb_checksum(nskb, doffset,\n\t\t\t\t\t     nskb->len - doffset, 0);\n\t\t\tSKB_GSO_CB(nskb)->csum_start =\n\t\t\t\tskb_headroom(nskb) + doffset;\n\t\t}\n\t} while ((offset += len) < head_skb->len);\n\n\t/* Some callers want to get the end of the list.\n\t * Put it in segs->prev to avoid walking the list.\n\t * (see validate_xmit_skb_list() for example)\n\t */\n\tsegs->prev = tail;\n\n\tif (partial_segs) {\n\t\tstruct sk_buff *iter;\n\t\tint type = skb_shinfo(head_skb)->gso_type;\n\t\tunsigned short gso_size = skb_shinfo(head_skb)->gso_size;\n\n\t\t/* Update type to add partial and then remove dodgy if set */\n\t\ttype |= (features & NETIF_F_GSO_PARTIAL) / NETIF_F_GSO_PARTIAL * SKB_GSO_PARTIAL;\n\t\ttype &= ~SKB_GSO_DODGY;\n\n\t\t/* Update GSO info and prepare to start updating headers on\n\t\t * our way back down the stack of protocols.\n\t\t */\n\t\tfor (iter = segs; iter; iter = iter->next) {\n\t\t\tskb_shinfo(iter)->gso_size = gso_size;\n\t\t\tskb_shinfo(iter)->gso_segs = partial_segs;\n\t\t\tskb_shinfo(iter)->gso_type = type;\n\t\t\tSKB_GSO_CB(iter)->data_offset = skb_headroom(iter) + doffset;\n\t\t}\n\n\t\tif (tail->len - doffset <= gso_size)\n\t\t\tskb_shinfo(tail)->gso_size = 0;\n\t\telse if (tail != segs)\n\t\t\tskb_shinfo(tail)->gso_segs = DIV_ROUND_UP(tail->len - doffset, gso_size);\n\t}\n\n\t/* Following permits correct backpressure, for protocols\n\t * using skb_set_owner_w().\n\t * Idea is to tranfert ownership from head_skb to last segment.\n\t */\n\tif (head_skb->destructor == sock_wfree) {\n\t\tswap(tail->truesize, head_skb->truesize);\n\t\tswap(tail->destructor, head_skb->destructor);\n\t\tswap(tail->sk, head_skb->sk);\n\t}\n\treturn segs;\n\nerr:\n\tkfree_skb_list(segs);\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(skb_segment);\n\nint skb_gro_receive(struct sk_buff *p, struct sk_buff *skb)\n{\n\tstruct skb_shared_info *pinfo, *skbinfo = skb_shinfo(skb);\n\tunsigned int offset = skb_gro_offset(skb);\n\tunsigned int headlen = skb_headlen(skb);\n\tunsigned int len = skb_gro_len(skb);\n\tunsigned int delta_truesize;\n\tstruct sk_buff *lp;\n\n\tif (unlikely(p->len + len >= 65536 || NAPI_GRO_CB(skb)->flush))\n\t\treturn -E2BIG;\n\n\tlp = NAPI_GRO_CB(p)->last;\n\tpinfo = skb_shinfo(lp);\n\n\tif (headlen <= offset) {\n\t\tskb_frag_t *frag;\n\t\tskb_frag_t *frag2;\n\t\tint i = skbinfo->nr_frags;\n\t\tint nr_frags = pinfo->nr_frags + i;\n\n\t\tif (nr_frags > MAX_SKB_FRAGS)\n\t\t\tgoto merge;\n\n\t\toffset -= headlen;\n\t\tpinfo->nr_frags = nr_frags;\n\t\tskbinfo->nr_frags = 0;\n\n\t\tfrag = pinfo->frags + nr_frags;\n\t\tfrag2 = skbinfo->frags + i;\n\t\tdo {\n\t\t\t*--frag = *--frag2;\n\t\t} while (--i);\n\n\t\tskb_frag_off_add(frag, offset);\n\t\tskb_frag_size_sub(frag, offset);\n\n\t\t/* all fragments truesize : remove (head size + sk_buff) */\n\t\tdelta_truesize = skb->truesize -\n\t\t\t\t SKB_TRUESIZE(skb_end_offset(skb));\n\n\t\tskb->truesize -= skb->data_len;\n\t\tskb->len -= skb->data_len;\n\t\tskb->data_len = 0;\n\n\t\tNAPI_GRO_CB(skb)->free = NAPI_GRO_FREE;\n\t\tgoto done;\n\t} else if (skb->head_frag) {\n\t\tint nr_frags = pinfo->nr_frags;\n\t\tskb_frag_t *frag = pinfo->frags + nr_frags;\n\t\tstruct page *page = virt_to_head_page(skb->head);\n\t\tunsigned int first_size = headlen - offset;\n\t\tunsigned int first_offset;\n\n\t\tif (nr_frags + 1 + skbinfo->nr_frags > MAX_SKB_FRAGS)\n\t\t\tgoto merge;\n\n\t\tfirst_offset = skb->data -\n\t\t\t       (unsigned char *)page_address(page) +\n\t\t\t       offset;\n\n\t\tpinfo->nr_frags = nr_frags + 1 + skbinfo->nr_frags;\n\n\t\t__skb_frag_set_page(frag, page);\n\t\tskb_frag_off_set(frag, first_offset);\n\t\tskb_frag_size_set(frag, first_size);\n\n\t\tmemcpy(frag + 1, skbinfo->frags, sizeof(*frag) * skbinfo->nr_frags);\n\t\t/* We dont need to clear skbinfo->nr_frags here */\n\n\t\tdelta_truesize = skb->truesize - SKB_DATA_ALIGN(sizeof(struct sk_buff));\n\t\tNAPI_GRO_CB(skb)->free = NAPI_GRO_FREE_STOLEN_HEAD;\n\t\tgoto done;\n\t}\n\nmerge:\n\tdelta_truesize = skb->truesize;\n\tif (offset > headlen) {\n\t\tunsigned int eat = offset - headlen;\n\n\t\tskb_frag_off_add(&skbinfo->frags[0], eat);\n\t\tskb_frag_size_sub(&skbinfo->frags[0], eat);\n\t\tskb->data_len -= eat;\n\t\tskb->len -= eat;\n\t\toffset = headlen;\n\t}\n\n\t__skb_pull(skb, offset);\n\n\tif (NAPI_GRO_CB(p)->last == p)\n\t\tskb_shinfo(p)->frag_list = skb;\n\telse\n\t\tNAPI_GRO_CB(p)->last->next = skb;\n\tNAPI_GRO_CB(p)->last = skb;\n\t__skb_header_release(skb);\n\tlp = p;\n\ndone:\n\tNAPI_GRO_CB(p)->count++;\n\tp->data_len += len;\n\tp->truesize += delta_truesize;\n\tp->len += len;\n\tif (lp != p) {\n\t\tlp->data_len += len;\n\t\tlp->truesize += delta_truesize;\n\t\tlp->len += len;\n\t}\n\tNAPI_GRO_CB(skb)->same_flow = 1;\n\treturn 0;\n}\n\n#ifdef CONFIG_SKB_EXTENSIONS\n#define SKB_EXT_ALIGN_VALUE\t8\n#define SKB_EXT_CHUNKSIZEOF(x)\t(ALIGN((sizeof(x)), SKB_EXT_ALIGN_VALUE) / SKB_EXT_ALIGN_VALUE)\n\nstatic const u8 skb_ext_type_len[] = {\n#if IS_ENABLED(CONFIG_BRIDGE_NETFILTER)\n\t[SKB_EXT_BRIDGE_NF] = SKB_EXT_CHUNKSIZEOF(struct nf_bridge_info),\n#endif\n#ifdef CONFIG_XFRM\n\t[SKB_EXT_SEC_PATH] = SKB_EXT_CHUNKSIZEOF(struct sec_path),\n#endif\n#if IS_ENABLED(CONFIG_NET_TC_SKB_EXT)\n\t[TC_SKB_EXT] = SKB_EXT_CHUNKSIZEOF(struct tc_skb_ext),\n#endif\n#if IS_ENABLED(CONFIG_MPTCP)\n\t[SKB_EXT_MPTCP] = SKB_EXT_CHUNKSIZEOF(struct mptcp_ext),\n#endif\n};\n\nstatic __always_inline unsigned int skb_ext_total_length(void)\n{\n\treturn SKB_EXT_CHUNKSIZEOF(struct skb_ext) +\n#if IS_ENABLED(CONFIG_BRIDGE_NETFILTER)\n\t\tskb_ext_type_len[SKB_EXT_BRIDGE_NF] +\n#endif\n#ifdef CONFIG_XFRM\n\t\tskb_ext_type_len[SKB_EXT_SEC_PATH] +\n#endif\n#if IS_ENABLED(CONFIG_NET_TC_SKB_EXT)\n\t\tskb_ext_type_len[TC_SKB_EXT] +\n#endif\n#if IS_ENABLED(CONFIG_MPTCP)\n\t\tskb_ext_type_len[SKB_EXT_MPTCP] +\n#endif\n\t\t0;\n}\n\nstatic void skb_extensions_init(void)\n{\n\tBUILD_BUG_ON(SKB_EXT_NUM >= 8);\n\tBUILD_BUG_ON(skb_ext_total_length() > 255);\n\n\tskbuff_ext_cache = kmem_cache_create(\"skbuff_ext_cache\",\n\t\t\t\t\t     SKB_EXT_ALIGN_VALUE * skb_ext_total_length(),\n\t\t\t\t\t     0,\n\t\t\t\t\t     SLAB_HWCACHE_ALIGN|SLAB_PANIC,\n\t\t\t\t\t     NULL);\n}\n#else\nstatic void skb_extensions_init(void) {}\n#endif\n\nvoid __init skb_init(void)\n{\n\tskbuff_head_cache = kmem_cache_create_usercopy(\"skbuff_head_cache\",\n\t\t\t\t\t      sizeof(struct sk_buff),\n\t\t\t\t\t      0,\n\t\t\t\t\t      SLAB_HWCACHE_ALIGN|SLAB_PANIC,\n\t\t\t\t\t      offsetof(struct sk_buff, cb),\n\t\t\t\t\t      sizeof_field(struct sk_buff, cb),\n\t\t\t\t\t      NULL);\n\tskbuff_fclone_cache = kmem_cache_create(\"skbuff_fclone_cache\",\n\t\t\t\t\t\tsizeof(struct sk_buff_fclones),\n\t\t\t\t\t\t0,\n\t\t\t\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC,\n\t\t\t\t\t\tNULL);\n\tskb_extensions_init();\n}\n\nstatic int\n__skb_to_sgvec(struct sk_buff *skb, struct scatterlist *sg, int offset, int len,\n\t       unsigned int recursion_level)\n{\n\tint start = skb_headlen(skb);\n\tint i, copy = start - offset;\n\tstruct sk_buff *frag_iter;\n\tint elt = 0;\n\n\tif (unlikely(recursion_level >= 24))\n\t\treturn -EMSGSIZE;\n\n\tif (copy > 0) {\n\t\tif (copy > len)\n\t\t\tcopy = len;\n\t\tsg_set_buf(sg, skb->data + offset, copy);\n\t\telt++;\n\t\tif ((len -= copy) == 0)\n\t\t\treturn elt;\n\t\toffset += copy;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tint end;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\t\tif (unlikely(elt && sg_is_last(&sg[elt - 1])))\n\t\t\t\treturn -EMSGSIZE;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tsg_set_page(&sg[elt], skb_frag_page(frag), copy,\n\t\t\t\t    skb_frag_off(frag) + offset - start);\n\t\t\telt++;\n\t\t\tif (!(len -= copy))\n\t\t\t\treturn elt;\n\t\t\toffset += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\n\tskb_walk_frags(skb, frag_iter) {\n\t\tint end, ret;\n\n\t\tWARN_ON(start > offset + len);\n\n\t\tend = start + frag_iter->len;\n\t\tif ((copy = end - offset) > 0) {\n\t\t\tif (unlikely(elt && sg_is_last(&sg[elt - 1])))\n\t\t\t\treturn -EMSGSIZE;\n\n\t\t\tif (copy > len)\n\t\t\t\tcopy = len;\n\t\t\tret = __skb_to_sgvec(frag_iter, sg+elt, offset - start,\n\t\t\t\t\t      copy, recursion_level + 1);\n\t\t\tif (unlikely(ret < 0))\n\t\t\t\treturn ret;\n\t\t\telt += ret;\n\t\t\tif ((len -= copy) == 0)\n\t\t\t\treturn elt;\n\t\t\toffset += copy;\n\t\t}\n\t\tstart = end;\n\t}\n\tBUG_ON(len);\n\treturn elt;\n}\n\n/**\n *\tskb_to_sgvec - Fill a scatter-gather list from a socket buffer\n *\t@skb: Socket buffer containing the buffers to be mapped\n *\t@sg: The scatter-gather list to map into\n *\t@offset: The offset into the buffer's contents to start mapping\n *\t@len: Length of buffer space to be mapped\n *\n *\tFill the specified scatter-gather list with mappings/pointers into a\n *\tregion of the buffer space attached to a socket buffer. Returns either\n *\tthe number of scatterlist items used, or -EMSGSIZE if the contents\n *\tcould not fit.\n */\nint skb_to_sgvec(struct sk_buff *skb, struct scatterlist *sg, int offset, int len)\n{\n\tint nsg = __skb_to_sgvec(skb, sg, offset, len, 0);\n\n\tif (nsg <= 0)\n\t\treturn nsg;\n\n\tsg_mark_end(&sg[nsg - 1]);\n\n\treturn nsg;\n}\nEXPORT_SYMBOL_GPL(skb_to_sgvec);\n\n/* As compared with skb_to_sgvec, skb_to_sgvec_nomark only map skb to given\n * sglist without mark the sg which contain last skb data as the end.\n * So the caller can mannipulate sg list as will when padding new data after\n * the first call without calling sg_unmark_end to expend sg list.\n *\n * Scenario to use skb_to_sgvec_nomark:\n * 1. sg_init_table\n * 2. skb_to_sgvec_nomark(payload1)\n * 3. skb_to_sgvec_nomark(payload2)\n *\n * This is equivalent to:\n * 1. sg_init_table\n * 2. skb_to_sgvec(payload1)\n * 3. sg_unmark_end\n * 4. skb_to_sgvec(payload2)\n *\n * When mapping mutilple payload conditionally, skb_to_sgvec_nomark\n * is more preferable.\n */\nint skb_to_sgvec_nomark(struct sk_buff *skb, struct scatterlist *sg,\n\t\t\tint offset, int len)\n{\n\treturn __skb_to_sgvec(skb, sg, offset, len, 0);\n}\nEXPORT_SYMBOL_GPL(skb_to_sgvec_nomark);\n\n\n\n/**\n *\tskb_cow_data - Check that a socket buffer's data buffers are writable\n *\t@skb: The socket buffer to check.\n *\t@tailbits: Amount of trailing space to be added\n *\t@trailer: Returned pointer to the skb where the @tailbits space begins\n *\n *\tMake sure that the data buffers attached to a socket buffer are\n *\twritable. If they are not, private copies are made of the data buffers\n *\tand the socket buffer is set to use these instead.\n *\n *\tIf @tailbits is given, make sure that there is space to write @tailbits\n *\tbytes of data beyond current end of socket buffer.  @trailer will be\n *\tset to point to the skb in which this space begins.\n *\n *\tThe number of scatterlist elements required to completely map the\n *\tCOW'd and extended socket buffer will be returned.\n */\nint skb_cow_data(struct sk_buff *skb, int tailbits, struct sk_buff **trailer)\n{\n\tint copyflag;\n\tint elt;\n\tstruct sk_buff *skb1, **skb_p;\n\n\t/* If skb is cloned or its head is paged, reallocate\n\t * head pulling out all the pages (pages are considered not writable\n\t * at the moment even if they are anonymous).\n\t */\n\tif ((skb_cloned(skb) || skb_shinfo(skb)->nr_frags) &&\n\t    !__pskb_pull_tail(skb, __skb_pagelen(skb)))\n\t\treturn -ENOMEM;\n\n\t/* Easy case. Most of packets will go this way. */\n\tif (!skb_has_frag_list(skb)) {\n\t\t/* A little of trouble, not enough of space for trailer.\n\t\t * This should not happen, when stack is tuned to generate\n\t\t * good frames. OK, on miss we reallocate and reserve even more\n\t\t * space, 128 bytes is fair. */\n\n\t\tif (skb_tailroom(skb) < tailbits &&\n\t\t    pskb_expand_head(skb, 0, tailbits-skb_tailroom(skb)+128, GFP_ATOMIC))\n\t\t\treturn -ENOMEM;\n\n\t\t/* Voila! */\n\t\t*trailer = skb;\n\t\treturn 1;\n\t}\n\n\t/* Misery. We are in troubles, going to mincer fragments... */\n\n\telt = 1;\n\tskb_p = &skb_shinfo(skb)->frag_list;\n\tcopyflag = 0;\n\n\twhile ((skb1 = *skb_p) != NULL) {\n\t\tint ntail = 0;\n\n\t\t/* The fragment is partially pulled by someone,\n\t\t * this can happen on input. Copy it and everything\n\t\t * after it. */\n\n\t\tif (skb_shared(skb1))\n\t\t\tcopyflag = 1;\n\n\t\t/* If the skb is the last, worry about trailer. */\n\n\t\tif (skb1->next == NULL && tailbits) {\n\t\t\tif (skb_shinfo(skb1)->nr_frags ||\n\t\t\t    skb_has_frag_list(skb1) ||\n\t\t\t    skb_tailroom(skb1) < tailbits)\n\t\t\t\tntail = tailbits + 128;\n\t\t}\n\n\t\tif (copyflag ||\n\t\t    skb_cloned(skb1) ||\n\t\t    ntail ||\n\t\t    skb_shinfo(skb1)->nr_frags ||\n\t\t    skb_has_frag_list(skb1)) {\n\t\t\tstruct sk_buff *skb2;\n\n\t\t\t/* Fuck, we are miserable poor guys... */\n\t\t\tif (ntail == 0)\n\t\t\t\tskb2 = skb_copy(skb1, GFP_ATOMIC);\n\t\t\telse\n\t\t\t\tskb2 = skb_copy_expand(skb1,\n\t\t\t\t\t\t       skb_headroom(skb1),\n\t\t\t\t\t\t       ntail,\n\t\t\t\t\t\t       GFP_ATOMIC);\n\t\t\tif (unlikely(skb2 == NULL))\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tif (skb1->sk)\n\t\t\t\tskb_set_owner_w(skb2, skb1->sk);\n\n\t\t\t/* Looking around. Are we still alive?\n\t\t\t * OK, link new skb, drop old one */\n\n\t\t\tskb2->next = skb1->next;\n\t\t\t*skb_p = skb2;\n\t\t\tkfree_skb(skb1);\n\t\t\tskb1 = skb2;\n\t\t}\n\t\telt++;\n\t\t*trailer = skb1;\n\t\tskb_p = &skb1->next;\n\t}\n\n\treturn elt;\n}\nEXPORT_SYMBOL_GPL(skb_cow_data);\n\nstatic void sock_rmem_free(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\n\tatomic_sub(skb->truesize, &sk->sk_rmem_alloc);\n}\n\nstatic void skb_set_err_queue(struct sk_buff *skb)\n{\n\t/* pkt_type of skbs received on local sockets is never PACKET_OUTGOING.\n\t * So, it is safe to (mis)use it to mark skbs on the error queue.\n\t */\n\tskb->pkt_type = PACKET_OUTGOING;\n\tBUILD_BUG_ON(PACKET_OUTGOING == 0);\n}\n\n/*\n * Note: We dont mem charge error packets (no sk_forward_alloc changes)\n */\nint sock_queue_err_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tif (atomic_read(&sk->sk_rmem_alloc) + skb->truesize >=\n\t    (unsigned int)READ_ONCE(sk->sk_rcvbuf))\n\t\treturn -ENOMEM;\n\n\tskb_orphan(skb);\n\tskb->sk = sk;\n\tskb->destructor = sock_rmem_free;\n\tatomic_add(skb->truesize, &sk->sk_rmem_alloc);\n\tskb_set_err_queue(skb);\n\n\t/* before exiting rcu section, make sure dst is refcounted */\n\tskb_dst_force(skb);\n\n\tskb_queue_tail(&sk->sk_error_queue, skb);\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk->sk_error_report(sk);\n\treturn 0;\n}\nEXPORT_SYMBOL(sock_queue_err_skb);\n\nstatic bool is_icmp_err_skb(const struct sk_buff *skb)\n{\n\treturn skb && (SKB_EXT_ERR(skb)->ee.ee_origin == SO_EE_ORIGIN_ICMP ||\n\t\t       SKB_EXT_ERR(skb)->ee.ee_origin == SO_EE_ORIGIN_ICMP6);\n}\n\nstruct sk_buff *sock_dequeue_err_skb(struct sock *sk)\n{\n\tstruct sk_buff_head *q = &sk->sk_error_queue;\n\tstruct sk_buff *skb, *skb_next = NULL;\n\tbool icmp_next = false;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&q->lock, flags);\n\tskb = __skb_dequeue(q);\n\tif (skb && (skb_next = skb_peek(q))) {\n\t\ticmp_next = is_icmp_err_skb(skb_next);\n\t\tif (icmp_next)\n\t\t\tsk->sk_err = SKB_EXT_ERR(skb_next)->ee.ee_errno;\n\t}\n\tspin_unlock_irqrestore(&q->lock, flags);\n\n\tif (is_icmp_err_skb(skb) && !icmp_next)\n\t\tsk->sk_err = 0;\n\n\tif (skb_next)\n\t\tsk->sk_error_report(sk);\n\n\treturn skb;\n}\nEXPORT_SYMBOL(sock_dequeue_err_skb);\n\n/**\n * skb_clone_sk - create clone of skb, and take reference to socket\n * @skb: the skb to clone\n *\n * This function creates a clone of a buffer that holds a reference on\n * sk_refcnt.  Buffers created via this function are meant to be\n * returned using sock_queue_err_skb, or free via kfree_skb.\n *\n * When passing buffers allocated with this function to sock_queue_err_skb\n * it is necessary to wrap the call with sock_hold/sock_put in order to\n * prevent the socket from being released prior to being enqueued on\n * the sk_error_queue.\n */\nstruct sk_buff *skb_clone_sk(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct sk_buff *clone;\n\n\tif (!sk || !refcount_inc_not_zero(&sk->sk_refcnt))\n\t\treturn NULL;\n\n\tclone = skb_clone(skb, GFP_ATOMIC);\n\tif (!clone) {\n\t\tsock_put(sk);\n\t\treturn NULL;\n\t}\n\n\tclone->sk = sk;\n\tclone->destructor = sock_efree;\n\n\treturn clone;\n}\nEXPORT_SYMBOL(skb_clone_sk);\n\nstatic void __skb_complete_tx_timestamp(struct sk_buff *skb,\n\t\t\t\t\tstruct sock *sk,\n\t\t\t\t\tint tstype,\n\t\t\t\t\tbool opt_stats)\n{\n\tstruct sock_exterr_skb *serr;\n\tint err;\n\n\tBUILD_BUG_ON(sizeof(struct sock_exterr_skb) > sizeof(skb->cb));\n\n\tserr = SKB_EXT_ERR(skb);\n\tmemset(serr, 0, sizeof(*serr));\n\tserr->ee.ee_errno = ENOMSG;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_TIMESTAMPING;\n\tserr->ee.ee_info = tstype;\n\tserr->opt_stats = opt_stats;\n\tserr->header.h4.iif = skb->dev ? skb->dev->ifindex : 0;\n\tif (sk->sk_tsflags & SOF_TIMESTAMPING_OPT_ID) {\n\t\tserr->ee.ee_data = skb_shinfo(skb)->tskey;\n\t\tif (sk->sk_protocol == IPPROTO_TCP &&\n\t\t    sk->sk_type == SOCK_STREAM)\n\t\t\tserr->ee.ee_data -= sk->sk_tskey;\n\t}\n\n\terr = sock_queue_err_skb(sk, skb);\n\n\tif (err)\n\t\tkfree_skb(skb);\n}\n\nstatic bool skb_may_tx_timestamp(struct sock *sk, bool tsonly)\n{\n\tbool ret;\n\n\tif (likely(sysctl_tstamp_allow_data || tsonly))\n\t\treturn true;\n\n\tread_lock_bh(&sk->sk_callback_lock);\n\tret = sk->sk_socket && sk->sk_socket->file &&\n\t      file_ns_capable(sk->sk_socket->file, &init_user_ns, CAP_NET_RAW);\n\tread_unlock_bh(&sk->sk_callback_lock);\n\treturn ret;\n}\n\nvoid skb_complete_tx_timestamp(struct sk_buff *skb,\n\t\t\t       struct skb_shared_hwtstamps *hwtstamps)\n{\n\tstruct sock *sk = skb->sk;\n\n\tif (!skb_may_tx_timestamp(sk, false))\n\t\tgoto err;\n\n\t/* Take a reference to prevent skb_orphan() from freeing the socket,\n\t * but only if the socket refcount is not zero.\n\t */\n\tif (likely(refcount_inc_not_zero(&sk->sk_refcnt))) {\n\t\t*skb_hwtstamps(skb) = *hwtstamps;\n\t\t__skb_complete_tx_timestamp(skb, sk, SCM_TSTAMP_SND, false);\n\t\tsock_put(sk);\n\t\treturn;\n\t}\n\nerr:\n\tkfree_skb(skb);\n}\nEXPORT_SYMBOL_GPL(skb_complete_tx_timestamp);\n\nvoid __skb_tstamp_tx(struct sk_buff *orig_skb,\n\t\t     const struct sk_buff *ack_skb,\n\t\t     struct skb_shared_hwtstamps *hwtstamps,\n\t\t     struct sock *sk, int tstype)\n{\n\tstruct sk_buff *skb;\n\tbool tsonly, opt_stats = false;\n\n\tif (!sk)\n\t\treturn;\n\n\tif (!hwtstamps && !(sk->sk_tsflags & SOF_TIMESTAMPING_OPT_TX_SWHW) &&\n\t    skb_shinfo(orig_skb)->tx_flags & SKBTX_IN_PROGRESS)\n\t\treturn;\n\n\ttsonly = sk->sk_tsflags & SOF_TIMESTAMPING_OPT_TSONLY;\n\tif (!skb_may_tx_timestamp(sk, tsonly))\n\t\treturn;\n\n\tif (tsonly) {\n#ifdef CONFIG_INET\n\t\tif ((sk->sk_tsflags & SOF_TIMESTAMPING_OPT_STATS) &&\n\t\t    sk->sk_protocol == IPPROTO_TCP &&\n\t\t    sk->sk_type == SOCK_STREAM) {\n\t\t\tskb = tcp_get_timestamping_opt_stats(sk, orig_skb,\n\t\t\t\t\t\t\t     ack_skb);\n\t\t\topt_stats = true;\n\t\t} else\n#endif\n\t\t\tskb = alloc_skb(0, GFP_ATOMIC);\n\t} else {\n\t\tskb = skb_clone(orig_skb, GFP_ATOMIC);\n\t}\n\tif (!skb)\n\t\treturn;\n\n\tif (tsonly) {\n\t\tskb_shinfo(skb)->tx_flags |= skb_shinfo(orig_skb)->tx_flags &\n\t\t\t\t\t     SKBTX_ANY_TSTAMP;\n\t\tskb_shinfo(skb)->tskey = skb_shinfo(orig_skb)->tskey;\n\t}\n\n\tif (hwtstamps)\n\t\t*skb_hwtstamps(skb) = *hwtstamps;\n\telse\n\t\tskb->tstamp = ktime_get_real();\n\n\t__skb_complete_tx_timestamp(skb, sk, tstype, opt_stats);\n}\nEXPORT_SYMBOL_GPL(__skb_tstamp_tx);\n\nvoid skb_tstamp_tx(struct sk_buff *orig_skb,\n\t\t   struct skb_shared_hwtstamps *hwtstamps)\n{\n\treturn __skb_tstamp_tx(orig_skb, NULL, hwtstamps, orig_skb->sk,\n\t\t\t       SCM_TSTAMP_SND);\n}\nEXPORT_SYMBOL_GPL(skb_tstamp_tx);\n\nvoid skb_complete_wifi_ack(struct sk_buff *skb, bool acked)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct sock_exterr_skb *serr;\n\tint err = 1;\n\n\tskb->wifi_acked_valid = 1;\n\tskb->wifi_acked = acked;\n\n\tserr = SKB_EXT_ERR(skb);\n\tmemset(serr, 0, sizeof(*serr));\n\tserr->ee.ee_errno = ENOMSG;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_TXSTATUS;\n\n\t/* Take a reference to prevent skb_orphan() from freeing the socket,\n\t * but only if the socket refcount is not zero.\n\t */\n\tif (likely(refcount_inc_not_zero(&sk->sk_refcnt))) {\n\t\terr = sock_queue_err_skb(sk, skb);\n\t\tsock_put(sk);\n\t}\n\tif (err)\n\t\tkfree_skb(skb);\n}\nEXPORT_SYMBOL_GPL(skb_complete_wifi_ack);\n\n/**\n * skb_partial_csum_set - set up and verify partial csum values for packet\n * @skb: the skb to set\n * @start: the number of bytes after skb->data to start checksumming.\n * @off: the offset from start to place the checksum.\n *\n * For untrusted partially-checksummed packets, we need to make sure the values\n * for skb->csum_start and skb->csum_offset are valid so we don't oops.\n *\n * This function checks and sets those values and skb->ip_summed: if this\n * returns false you should drop the packet.\n */\nbool skb_partial_csum_set(struct sk_buff *skb, u16 start, u16 off)\n{\n\tu32 csum_end = (u32)start + (u32)off + sizeof(__sum16);\n\tu32 csum_start = skb_headroom(skb) + (u32)start;\n\n\tif (unlikely(csum_start > U16_MAX || csum_end > skb_headlen(skb))) {\n\t\tnet_warn_ratelimited(\"bad partial csum: csum=%u/%u headroom=%u headlen=%u\\n\",\n\t\t\t\t     start, off, skb_headroom(skb), skb_headlen(skb));\n\t\treturn false;\n\t}\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\tskb->csum_start = csum_start;\n\tskb->csum_offset = off;\n\tskb_set_transport_header(skb, start);\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(skb_partial_csum_set);\n\nstatic int skb_maybe_pull_tail(struct sk_buff *skb, unsigned int len,\n\t\t\t       unsigned int max)\n{\n\tif (skb_headlen(skb) >= len)\n\t\treturn 0;\n\n\t/* If we need to pullup then pullup to the max, so we\n\t * won't need to do it again.\n\t */\n\tif (max > skb->len)\n\t\tmax = skb->len;\n\n\tif (__pskb_pull_tail(skb, max - skb_headlen(skb)) == NULL)\n\t\treturn -ENOMEM;\n\n\tif (skb_headlen(skb) < len)\n\t\treturn -EPROTO;\n\n\treturn 0;\n}\n\n#define MAX_TCP_HDR_LEN (15 * 4)\n\nstatic __sum16 *skb_checksum_setup_ip(struct sk_buff *skb,\n\t\t\t\t      typeof(IPPROTO_IP) proto,\n\t\t\t\t      unsigned int off)\n{\n\tint err;\n\n\tswitch (proto) {\n\tcase IPPROTO_TCP:\n\t\terr = skb_maybe_pull_tail(skb, off + sizeof(struct tcphdr),\n\t\t\t\t\t  off + MAX_TCP_HDR_LEN);\n\t\tif (!err && !skb_partial_csum_set(skb, off,\n\t\t\t\t\t\t  offsetof(struct tcphdr,\n\t\t\t\t\t\t\t   check)))\n\t\t\terr = -EPROTO;\n\t\treturn err ? ERR_PTR(err) : &tcp_hdr(skb)->check;\n\n\tcase IPPROTO_UDP:\n\t\terr = skb_maybe_pull_tail(skb, off + sizeof(struct udphdr),\n\t\t\t\t\t  off + sizeof(struct udphdr));\n\t\tif (!err && !skb_partial_csum_set(skb, off,\n\t\t\t\t\t\t  offsetof(struct udphdr,\n\t\t\t\t\t\t\t   check)))\n\t\t\terr = -EPROTO;\n\t\treturn err ? ERR_PTR(err) : &udp_hdr(skb)->check;\n\t}\n\n\treturn ERR_PTR(-EPROTO);\n}\n\n/* This value should be large enough to cover a tagged ethernet header plus\n * maximally sized IP and TCP or UDP headers.\n */\n#define MAX_IP_HDR_LEN 128\n\nstatic int skb_checksum_setup_ipv4(struct sk_buff *skb, bool recalculate)\n{\n\tunsigned int off;\n\tbool fragment;\n\t__sum16 *csum;\n\tint err;\n\n\tfragment = false;\n\n\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t  sizeof(struct iphdr),\n\t\t\t\t  MAX_IP_HDR_LEN);\n\tif (err < 0)\n\t\tgoto out;\n\n\tif (ip_is_fragment(ip_hdr(skb)))\n\t\tfragment = true;\n\n\toff = ip_hdrlen(skb);\n\n\terr = -EPROTO;\n\n\tif (fragment)\n\t\tgoto out;\n\n\tcsum = skb_checksum_setup_ip(skb, ip_hdr(skb)->protocol, off);\n\tif (IS_ERR(csum))\n\t\treturn PTR_ERR(csum);\n\n\tif (recalculate)\n\t\t*csum = ~csum_tcpudp_magic(ip_hdr(skb)->saddr,\n\t\t\t\t\t   ip_hdr(skb)->daddr,\n\t\t\t\t\t   skb->len - off,\n\t\t\t\t\t   ip_hdr(skb)->protocol, 0);\n\terr = 0;\n\nout:\n\treturn err;\n}\n\n/* This value should be large enough to cover a tagged ethernet header plus\n * an IPv6 header, all options, and a maximal TCP or UDP header.\n */\n#define MAX_IPV6_HDR_LEN 256\n\n#define OPT_HDR(type, skb, off) \\\n\t(type *)(skb_network_header(skb) + (off))\n\nstatic int skb_checksum_setup_ipv6(struct sk_buff *skb, bool recalculate)\n{\n\tint err;\n\tu8 nexthdr;\n\tunsigned int off;\n\tunsigned int len;\n\tbool fragment;\n\tbool done;\n\t__sum16 *csum;\n\n\tfragment = false;\n\tdone = false;\n\n\toff = sizeof(struct ipv6hdr);\n\n\terr = skb_maybe_pull_tail(skb, off, MAX_IPV6_HDR_LEN);\n\tif (err < 0)\n\t\tgoto out;\n\n\tnexthdr = ipv6_hdr(skb)->nexthdr;\n\n\tlen = sizeof(struct ipv6hdr) + ntohs(ipv6_hdr(skb)->payload_len);\n\twhile (off <= len && !done) {\n\t\tswitch (nexthdr) {\n\t\tcase IPPROTO_DSTOPTS:\n\t\tcase IPPROTO_HOPOPTS:\n\t\tcase IPPROTO_ROUTING: {\n\t\t\tstruct ipv6_opt_hdr *hp;\n\n\t\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t\t  off +\n\t\t\t\t\t\t  sizeof(struct ipv6_opt_hdr),\n\t\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t\thp = OPT_HDR(struct ipv6_opt_hdr, skb, off);\n\t\t\tnexthdr = hp->nexthdr;\n\t\t\toff += ipv6_optlen(hp);\n\t\t\tbreak;\n\t\t}\n\t\tcase IPPROTO_AH: {\n\t\t\tstruct ip_auth_hdr *hp;\n\n\t\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t\t  off +\n\t\t\t\t\t\t  sizeof(struct ip_auth_hdr),\n\t\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t\thp = OPT_HDR(struct ip_auth_hdr, skb, off);\n\t\t\tnexthdr = hp->nexthdr;\n\t\t\toff += ipv6_authlen(hp);\n\t\t\tbreak;\n\t\t}\n\t\tcase IPPROTO_FRAGMENT: {\n\t\t\tstruct frag_hdr *hp;\n\n\t\t\terr = skb_maybe_pull_tail(skb,\n\t\t\t\t\t\t  off +\n\t\t\t\t\t\t  sizeof(struct frag_hdr),\n\t\t\t\t\t\t  MAX_IPV6_HDR_LEN);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t\thp = OPT_HDR(struct frag_hdr, skb, off);\n\n\t\t\tif (hp->frag_off & htons(IP6_OFFSET | IP6_MF))\n\t\t\t\tfragment = true;\n\n\t\t\tnexthdr = hp->nexthdr;\n\t\t\toff += sizeof(struct frag_hdr);\n\t\t\tbreak;\n\t\t}\n\t\tdefault:\n\t\t\tdone = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\terr = -EPROTO;\n\n\tif (!done || fragment)\n\t\tgoto out;\n\n\tcsum = skb_checksum_setup_ip(skb, nexthdr, off);\n\tif (IS_ERR(csum))\n\t\treturn PTR_ERR(csum);\n\n\tif (recalculate)\n\t\t*csum = ~csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t\t &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t skb->len - off, nexthdr, 0);\n\terr = 0;\n\nout:\n\treturn err;\n}\n\n/**\n * skb_checksum_setup - set up partial checksum offset\n * @skb: the skb to set up\n * @recalculate: if true the pseudo-header checksum will be recalculated\n */\nint skb_checksum_setup(struct sk_buff *skb, bool recalculate)\n{\n\tint err;\n\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_IP):\n\t\terr = skb_checksum_setup_ipv4(skb, recalculate);\n\t\tbreak;\n\n\tcase htons(ETH_P_IPV6):\n\t\terr = skb_checksum_setup_ipv6(skb, recalculate);\n\t\tbreak;\n\n\tdefault:\n\t\terr = -EPROTO;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(skb_checksum_setup);\n\n/**\n * skb_checksum_maybe_trim - maybe trims the given skb\n * @skb: the skb to check\n * @transport_len: the data length beyond the network header\n *\n * Checks whether the given skb has data beyond the given transport length.\n * If so, returns a cloned skb trimmed to this transport length.\n * Otherwise returns the provided skb. Returns NULL in error cases\n * (e.g. transport_len exceeds skb length or out-of-memory).\n *\n * Caller needs to set the skb transport header and free any returned skb if it\n * differs from the provided skb.\n */\nstatic struct sk_buff *skb_checksum_maybe_trim(struct sk_buff *skb,\n\t\t\t\t\t       unsigned int transport_len)\n{\n\tstruct sk_buff *skb_chk;\n\tunsigned int len = skb_transport_offset(skb) + transport_len;\n\tint ret;\n\n\tif (skb->len < len)\n\t\treturn NULL;\n\telse if (skb->len == len)\n\t\treturn skb;\n\n\tskb_chk = skb_clone(skb, GFP_ATOMIC);\n\tif (!skb_chk)\n\t\treturn NULL;\n\n\tret = pskb_trim_rcsum(skb_chk, len);\n\tif (ret) {\n\t\tkfree_skb(skb_chk);\n\t\treturn NULL;\n\t}\n\n\treturn skb_chk;\n}\n\n/**\n * skb_checksum_trimmed - validate checksum of an skb\n * @skb: the skb to check\n * @transport_len: the data length beyond the network header\n * @skb_chkf: checksum function to use\n *\n * Applies the given checksum function skb_chkf to the provided skb.\n * Returns a checked and maybe trimmed skb. Returns NULL on error.\n *\n * If the skb has data beyond the given transport length, then a\n * trimmed & cloned skb is checked and returned.\n *\n * Caller needs to set the skb transport header and free any returned skb if it\n * differs from the provided skb.\n */\nstruct sk_buff *skb_checksum_trimmed(struct sk_buff *skb,\n\t\t\t\t     unsigned int transport_len,\n\t\t\t\t     __sum16(*skb_chkf)(struct sk_buff *skb))\n{\n\tstruct sk_buff *skb_chk;\n\tunsigned int offset = skb_transport_offset(skb);\n\t__sum16 ret;\n\n\tskb_chk = skb_checksum_maybe_trim(skb, transport_len);\n\tif (!skb_chk)\n\t\tgoto err;\n\n\tif (!pskb_may_pull(skb_chk, offset))\n\t\tgoto err;\n\n\tskb_pull_rcsum(skb_chk, offset);\n\tret = skb_chkf(skb_chk);\n\tskb_push_rcsum(skb_chk, offset);\n\n\tif (ret)\n\t\tgoto err;\n\n\treturn skb_chk;\n\nerr:\n\tif (skb_chk && skb_chk != skb)\n\t\tkfree_skb(skb_chk);\n\n\treturn NULL;\n\n}\nEXPORT_SYMBOL(skb_checksum_trimmed);\n\nvoid __skb_warn_lro_forwarding(const struct sk_buff *skb)\n{\n\tnet_warn_ratelimited(\"%s: received packets cannot be forwarded while LRO is enabled\\n\",\n\t\t\t     skb->dev->name);\n}\nEXPORT_SYMBOL(__skb_warn_lro_forwarding);\n\nvoid kfree_skb_partial(struct sk_buff *skb, bool head_stolen)\n{\n\tif (head_stolen) {\n\t\tskb_release_head_state(skb);\n\t\tkmem_cache_free(skbuff_head_cache, skb);\n\t} else {\n\t\t__kfree_skb(skb);\n\t}\n}\nEXPORT_SYMBOL(kfree_skb_partial);\n\n/**\n * skb_try_coalesce - try to merge skb to prior one\n * @to: prior buffer\n * @from: buffer to add\n * @fragstolen: pointer to boolean\n * @delta_truesize: how much more was allocated than was requested\n */\nbool skb_try_coalesce(struct sk_buff *to, struct sk_buff *from,\n\t\t      bool *fragstolen, int *delta_truesize)\n{\n\tstruct skb_shared_info *to_shinfo, *from_shinfo;\n\tint i, delta, len = from->len;\n\n\t*fragstolen = false;\n\n\tif (skb_cloned(to))\n\t\treturn false;\n\n\tif (len <= skb_tailroom(to)) {\n\t\tif (len)\n\t\t\tBUG_ON(skb_copy_bits(from, 0, skb_put(to, len), len));\n\t\t*delta_truesize = 0;\n\t\treturn true;\n\t}\n\n\tto_shinfo = skb_shinfo(to);\n\tfrom_shinfo = skb_shinfo(from);\n\tif (to_shinfo->frag_list || from_shinfo->frag_list)\n\t\treturn false;\n\tif (skb_zcopy(to) || skb_zcopy(from))\n\t\treturn false;\n\n\tif (skb_headlen(from) != 0) {\n\t\tstruct page *page;\n\t\tunsigned int offset;\n\n\t\tif (to_shinfo->nr_frags +\n\t\t    from_shinfo->nr_frags >= MAX_SKB_FRAGS)\n\t\t\treturn false;\n\n\t\tif (skb_head_is_locked(from))\n\t\t\treturn false;\n\n\t\tdelta = from->truesize - SKB_DATA_ALIGN(sizeof(struct sk_buff));\n\n\t\tpage = virt_to_head_page(from->head);\n\t\toffset = from->data - (unsigned char *)page_address(page);\n\n\t\tskb_fill_page_desc(to, to_shinfo->nr_frags,\n\t\t\t\t   page, offset, skb_headlen(from));\n\t\t*fragstolen = true;\n\t} else {\n\t\tif (to_shinfo->nr_frags +\n\t\t    from_shinfo->nr_frags > MAX_SKB_FRAGS)\n\t\t\treturn false;\n\n\t\tdelta = from->truesize - SKB_TRUESIZE(skb_end_offset(from));\n\t}\n\n\tWARN_ON_ONCE(delta < len);\n\n\tmemcpy(to_shinfo->frags + to_shinfo->nr_frags,\n\t       from_shinfo->frags,\n\t       from_shinfo->nr_frags * sizeof(skb_frag_t));\n\tto_shinfo->nr_frags += from_shinfo->nr_frags;\n\n\tif (!skb_cloned(from))\n\t\tfrom_shinfo->nr_frags = 0;\n\n\t/* if the skb is not cloned this does nothing\n\t * since we set nr_frags to 0.\n\t */\n\tfor (i = 0; i < from_shinfo->nr_frags; i++)\n\t\t__skb_frag_ref(&from_shinfo->frags[i]);\n\n\tto->truesize += delta;\n\tto->len += len;\n\tto->data_len += len;\n\n\t*delta_truesize = delta;\n\treturn true;\n}\nEXPORT_SYMBOL(skb_try_coalesce);\n\n/**\n * skb_scrub_packet - scrub an skb\n *\n * @skb: buffer to clean\n * @xnet: packet is crossing netns\n *\n * skb_scrub_packet can be used after encapsulating or decapsulting a packet\n * into/from a tunnel. Some information have to be cleared during these\n * operations.\n * skb_scrub_packet can also be used to clean a skb before injecting it in\n * another namespace (@xnet == true). We have to clear all information in the\n * skb that could impact namespace isolation.\n */\nvoid skb_scrub_packet(struct sk_buff *skb, bool xnet)\n{\n\tskb->pkt_type = PACKET_HOST;\n\tskb->skb_iif = 0;\n\tskb->ignore_df = 0;\n\tskb_dst_drop(skb);\n\tskb_ext_reset(skb);\n\tnf_reset_ct(skb);\n\tnf_reset_trace(skb);\n\n#ifdef CONFIG_NET_SWITCHDEV\n\tskb->offload_fwd_mark = 0;\n\tskb->offload_l3_fwd_mark = 0;\n#endif\n\n\tif (!xnet)\n\t\treturn;\n\n\tipvs_reset(skb);\n\tskb->mark = 0;\n\tskb->tstamp = 0;\n}\nEXPORT_SYMBOL_GPL(skb_scrub_packet);\n\n/**\n * skb_gso_transport_seglen - Return length of individual segments of a gso packet\n *\n * @skb: GSO skb\n *\n * skb_gso_transport_seglen is used to determine the real size of the\n * individual segments, including Layer4 headers (TCP/UDP).\n *\n * The MAC/L2 or network (IP, IPv6) headers are not accounted for.\n */\nstatic unsigned int skb_gso_transport_seglen(const struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\tunsigned int thlen = 0;\n\n\tif (skb->encapsulation) {\n\t\tthlen = skb_inner_transport_header(skb) -\n\t\t\tskb_transport_header(skb);\n\n\t\tif (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6)))\n\t\t\tthlen += inner_tcp_hdrlen(skb);\n\t} else if (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6))) {\n\t\tthlen = tcp_hdrlen(skb);\n\t} else if (unlikely(skb_is_gso_sctp(skb))) {\n\t\tthlen = sizeof(struct sctphdr);\n\t} else if (shinfo->gso_type & SKB_GSO_UDP_L4) {\n\t\tthlen = sizeof(struct udphdr);\n\t}\n\t/* UFO sets gso_size to the size of the fragmentation\n\t * payload, i.e. the size of the L4 (UDP) header is already\n\t * accounted for.\n\t */\n\treturn thlen + shinfo->gso_size;\n}\n\n/**\n * skb_gso_network_seglen - Return length of individual segments of a gso packet\n *\n * @skb: GSO skb\n *\n * skb_gso_network_seglen is used to determine the real size of the\n * individual segments, including Layer3 (IP, IPv6) and L4 headers (TCP/UDP).\n *\n * The MAC/L2 header is not accounted for.\n */\nstatic unsigned int skb_gso_network_seglen(const struct sk_buff *skb)\n{\n\tunsigned int hdr_len = skb_transport_header(skb) -\n\t\t\t       skb_network_header(skb);\n\n\treturn hdr_len + skb_gso_transport_seglen(skb);\n}\n\n/**\n * skb_gso_mac_seglen - Return length of individual segments of a gso packet\n *\n * @skb: GSO skb\n *\n * skb_gso_mac_seglen is used to determine the real size of the\n * individual segments, including MAC/L2, Layer3 (IP, IPv6) and L4\n * headers (TCP/UDP).\n */\nstatic unsigned int skb_gso_mac_seglen(const struct sk_buff *skb)\n{\n\tunsigned int hdr_len = skb_transport_header(skb) - skb_mac_header(skb);\n\n\treturn hdr_len + skb_gso_transport_seglen(skb);\n}\n\n/**\n * skb_gso_size_check - check the skb size, considering GSO_BY_FRAGS\n *\n * There are a couple of instances where we have a GSO skb, and we\n * want to determine what size it would be after it is segmented.\n *\n * We might want to check:\n * -    L3+L4+payload size (e.g. IP forwarding)\n * - L2+L3+L4+payload size (e.g. sanity check before passing to driver)\n *\n * This is a helper to do that correctly considering GSO_BY_FRAGS.\n *\n * @skb: GSO skb\n *\n * @seg_len: The segmented length (from skb_gso_*_seglen). In the\n *           GSO_BY_FRAGS case this will be [header sizes + GSO_BY_FRAGS].\n *\n * @max_len: The maximum permissible length.\n *\n * Returns true if the segmented length <= max length.\n */\nstatic inline bool skb_gso_size_check(const struct sk_buff *skb,\n\t\t\t\t      unsigned int seg_len,\n\t\t\t\t      unsigned int max_len) {\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\tconst struct sk_buff *iter;\n\n\tif (shinfo->gso_size != GSO_BY_FRAGS)\n\t\treturn seg_len <= max_len;\n\n\t/* Undo this so we can re-use header sizes */\n\tseg_len -= GSO_BY_FRAGS;\n\n\tskb_walk_frags(skb, iter) {\n\t\tif (seg_len + skb_headlen(iter) > max_len)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n/**\n * skb_gso_validate_network_len - Will a split GSO skb fit into a given MTU?\n *\n * @skb: GSO skb\n * @mtu: MTU to validate against\n *\n * skb_gso_validate_network_len validates if a given skb will fit a\n * wanted MTU once split. It considers L3 headers, L4 headers, and the\n * payload.\n */\nbool skb_gso_validate_network_len(const struct sk_buff *skb, unsigned int mtu)\n{\n\treturn skb_gso_size_check(skb, skb_gso_network_seglen(skb), mtu);\n}\nEXPORT_SYMBOL_GPL(skb_gso_validate_network_len);\n\n/**\n * skb_gso_validate_mac_len - Will a split GSO skb fit in a given length?\n *\n * @skb: GSO skb\n * @len: length to validate against\n *\n * skb_gso_validate_mac_len validates if a given skb will fit a wanted\n * length once split, including L2, L3 and L4 headers and the payload.\n */\nbool skb_gso_validate_mac_len(const struct sk_buff *skb, unsigned int len)\n{\n\treturn skb_gso_size_check(skb, skb_gso_mac_seglen(skb), len);\n}\nEXPORT_SYMBOL_GPL(skb_gso_validate_mac_len);\n\nstatic struct sk_buff *skb_reorder_vlan_header(struct sk_buff *skb)\n{\n\tint mac_len, meta_len;\n\tvoid *meta;\n\n\tif (skb_cow(skb, skb_headroom(skb)) < 0) {\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\t}\n\n\tmac_len = skb->data - skb_mac_header(skb);\n\tif (likely(mac_len > VLAN_HLEN + ETH_TLEN)) {\n\t\tmemmove(skb_mac_header(skb) + VLAN_HLEN, skb_mac_header(skb),\n\t\t\tmac_len - VLAN_HLEN - ETH_TLEN);\n\t}\n\n\tmeta_len = skb_metadata_len(skb);\n\tif (meta_len) {\n\t\tmeta = skb_metadata_end(skb) - meta_len;\n\t\tmemmove(meta + VLAN_HLEN, meta, meta_len);\n\t}\n\n\tskb->mac_header += VLAN_HLEN;\n\treturn skb;\n}\n\nstruct sk_buff *skb_vlan_untag(struct sk_buff *skb)\n{\n\tstruct vlan_hdr *vhdr;\n\tu16 vlan_tci;\n\n\tif (unlikely(skb_vlan_tag_present(skb))) {\n\t\t/* vlan_tci is already set-up so leave this for another time */\n\t\treturn skb;\n\t}\n\n\tskb = skb_share_check(skb, GFP_ATOMIC);\n\tif (unlikely(!skb))\n\t\tgoto err_free;\n\t/* We may access the two bytes after vlan_hdr in vlan_set_encap_proto(). */\n\tif (unlikely(!pskb_may_pull(skb, VLAN_HLEN + sizeof(unsigned short))))\n\t\tgoto err_free;\n\n\tvhdr = (struct vlan_hdr *)skb->data;\n\tvlan_tci = ntohs(vhdr->h_vlan_TCI);\n\t__vlan_hwaccel_put_tag(skb, skb->protocol, vlan_tci);\n\n\tskb_pull_rcsum(skb, VLAN_HLEN);\n\tvlan_set_encap_proto(skb, vhdr);\n\n\tskb = skb_reorder_vlan_header(skb);\n\tif (unlikely(!skb))\n\t\tgoto err_free;\n\n\tskb_reset_network_header(skb);\n\tif (!skb_transport_header_was_set(skb))\n\t\tskb_reset_transport_header(skb);\n\tskb_reset_mac_len(skb);\n\n\treturn skb;\n\nerr_free:\n\tkfree_skb(skb);\n\treturn NULL;\n}\nEXPORT_SYMBOL(skb_vlan_untag);\n\nint skb_ensure_writable(struct sk_buff *skb, int write_len)\n{\n\tif (!pskb_may_pull(skb, write_len))\n\t\treturn -ENOMEM;\n\n\tif (!skb_cloned(skb) || skb_clone_writable(skb, write_len))\n\t\treturn 0;\n\n\treturn pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n}\nEXPORT_SYMBOL(skb_ensure_writable);\n\n/* remove VLAN header from packet and update csum accordingly.\n * expects a non skb_vlan_tag_present skb with a vlan tag payload\n */\nint __skb_vlan_pop(struct sk_buff *skb, u16 *vlan_tci)\n{\n\tstruct vlan_hdr *vhdr;\n\tint offset = skb->data - skb_mac_header(skb);\n\tint err;\n\n\tif (WARN_ONCE(offset,\n\t\t      \"__skb_vlan_pop got skb with skb->data not at mac header (offset %d)\\n\",\n\t\t      offset)) {\n\t\treturn -EINVAL;\n\t}\n\n\terr = skb_ensure_writable(skb, VLAN_ETH_HLEN);\n\tif (unlikely(err))\n\t\treturn err;\n\n\tskb_postpull_rcsum(skb, skb->data + (2 * ETH_ALEN), VLAN_HLEN);\n\n\tvhdr = (struct vlan_hdr *)(skb->data + ETH_HLEN);\n\t*vlan_tci = ntohs(vhdr->h_vlan_TCI);\n\n\tmemmove(skb->data + VLAN_HLEN, skb->data, 2 * ETH_ALEN);\n\t__skb_pull(skb, VLAN_HLEN);\n\n\tvlan_set_encap_proto(skb, vhdr);\n\tskb->mac_header += VLAN_HLEN;\n\n\tif (skb_network_offset(skb) < ETH_HLEN)\n\t\tskb_set_network_header(skb, ETH_HLEN);\n\n\tskb_reset_mac_len(skb);\n\n\treturn err;\n}\nEXPORT_SYMBOL(__skb_vlan_pop);\n\n/* Pop a vlan tag either from hwaccel or from payload.\n * Expects skb->data at mac header.\n */\nint skb_vlan_pop(struct sk_buff *skb)\n{\n\tu16 vlan_tci;\n\t__be16 vlan_proto;\n\tint err;\n\n\tif (likely(skb_vlan_tag_present(skb))) {\n\t\t__vlan_hwaccel_clear_tag(skb);\n\t} else {\n\t\tif (unlikely(!eth_type_vlan(skb->protocol)))\n\t\t\treturn 0;\n\n\t\terr = __skb_vlan_pop(skb, &vlan_tci);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\t/* move next vlan tag to hw accel tag */\n\tif (likely(!eth_type_vlan(skb->protocol)))\n\t\treturn 0;\n\n\tvlan_proto = skb->protocol;\n\terr = __skb_vlan_pop(skb, &vlan_tci);\n\tif (unlikely(err))\n\t\treturn err;\n\n\t__vlan_hwaccel_put_tag(skb, vlan_proto, vlan_tci);\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_vlan_pop);\n\n/* Push a vlan tag either into hwaccel or into payload (if hwaccel tag present).\n * Expects skb->data at mac header.\n */\nint skb_vlan_push(struct sk_buff *skb, __be16 vlan_proto, u16 vlan_tci)\n{\n\tif (skb_vlan_tag_present(skb)) {\n\t\tint offset = skb->data - skb_mac_header(skb);\n\t\tint err;\n\n\t\tif (WARN_ONCE(offset,\n\t\t\t      \"skb_vlan_push got skb with skb->data not at mac header (offset %d)\\n\",\n\t\t\t      offset)) {\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\terr = __vlan_insert_tag(skb, skb->vlan_proto,\n\t\t\t\t\tskb_vlan_tag_get(skb));\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tskb->protocol = skb->vlan_proto;\n\t\tskb->mac_len += VLAN_HLEN;\n\n\t\tskb_postpush_rcsum(skb, skb->data + (2 * ETH_ALEN), VLAN_HLEN);\n\t}\n\t__vlan_hwaccel_put_tag(skb, vlan_proto, vlan_tci);\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_vlan_push);\n\n/**\n * skb_eth_pop() - Drop the Ethernet header at the head of a packet\n *\n * @skb: Socket buffer to modify\n *\n * Drop the Ethernet header of @skb.\n *\n * Expects that skb->data points to the mac header and that no VLAN tags are\n * present.\n *\n * Returns 0 on success, -errno otherwise.\n */\nint skb_eth_pop(struct sk_buff *skb)\n{\n\tif (!pskb_may_pull(skb, ETH_HLEN) || skb_vlan_tagged(skb) ||\n\t    skb_network_offset(skb) < ETH_HLEN)\n\t\treturn -EPROTO;\n\n\tskb_pull_rcsum(skb, ETH_HLEN);\n\tskb_reset_mac_header(skb);\n\tskb_reset_mac_len(skb);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_eth_pop);\n\n/**\n * skb_eth_push() - Add a new Ethernet header at the head of a packet\n *\n * @skb: Socket buffer to modify\n * @dst: Destination MAC address of the new header\n * @src: Source MAC address of the new header\n *\n * Prepend @skb with a new Ethernet header.\n *\n * Expects that skb->data points to the mac header, which must be empty.\n *\n * Returns 0 on success, -errno otherwise.\n */\nint skb_eth_push(struct sk_buff *skb, const unsigned char *dst,\n\t\t const unsigned char *src)\n{\n\tstruct ethhdr *eth;\n\tint err;\n\n\tif (skb_network_offset(skb) || skb_vlan_tag_present(skb))\n\t\treturn -EPROTO;\n\n\terr = skb_cow_head(skb, sizeof(*eth));\n\tif (err < 0)\n\t\treturn err;\n\n\tskb_push(skb, sizeof(*eth));\n\tskb_reset_mac_header(skb);\n\tskb_reset_mac_len(skb);\n\n\teth = eth_hdr(skb);\n\tether_addr_copy(eth->h_dest, dst);\n\tether_addr_copy(eth->h_source, src);\n\teth->h_proto = skb->protocol;\n\n\tskb_postpush_rcsum(skb, eth, sizeof(*eth));\n\n\treturn 0;\n}\nEXPORT_SYMBOL(skb_eth_push);\n\n/* Update the ethertype of hdr and the skb csum value if required. */\nstatic void skb_mod_eth_type(struct sk_buff *skb, struct ethhdr *hdr,\n\t\t\t     __be16 ethertype)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\t__be16 diff[] = { ~hdr->h_proto, ethertype };\n\n\t\tskb->csum = csum_partial((char *)diff, sizeof(diff), skb->csum);\n\t}\n\n\thdr->h_proto = ethertype;\n}\n\n/**\n * skb_mpls_push() - push a new MPLS header after mac_len bytes from start of\n *                   the packet\n *\n * @skb: buffer\n * @mpls_lse: MPLS label stack entry to push\n * @mpls_proto: ethertype of the new MPLS header (expects 0x8847 or 0x8848)\n * @mac_len: length of the MAC header\n * @ethernet: flag to indicate if the resulting packet after skb_mpls_push is\n *            ethernet\n *\n * Expects skb->data at mac header.\n *\n * Returns 0 on success, -errno otherwise.\n */\nint skb_mpls_push(struct sk_buff *skb, __be32 mpls_lse, __be16 mpls_proto,\n\t\t  int mac_len, bool ethernet)\n{\n\tstruct mpls_shim_hdr *lse;\n\tint err;\n\n\tif (unlikely(!eth_p_mpls(mpls_proto)))\n\t\treturn -EINVAL;\n\n\t/* Networking stack does not allow simultaneous Tunnel and MPLS GSO. */\n\tif (skb->encapsulation)\n\t\treturn -EINVAL;\n\n\terr = skb_cow_head(skb, MPLS_HLEN);\n\tif (unlikely(err))\n\t\treturn err;\n\n\tif (!skb->inner_protocol) {\n\t\tskb_set_inner_network_header(skb, skb_network_offset(skb));\n\t\tskb_set_inner_protocol(skb, skb->protocol);\n\t}\n\n\tskb_push(skb, MPLS_HLEN);\n\tmemmove(skb_mac_header(skb) - MPLS_HLEN, skb_mac_header(skb),\n\t\tmac_len);\n\tskb_reset_mac_header(skb);\n\tskb_set_network_header(skb, mac_len);\n\tskb_reset_mac_len(skb);\n\n\tlse = mpls_hdr(skb);\n\tlse->label_stack_entry = mpls_lse;\n\tskb_postpush_rcsum(skb, lse, MPLS_HLEN);\n\n\tif (ethernet && mac_len >= ETH_HLEN)\n\t\tskb_mod_eth_type(skb, eth_hdr(skb), mpls_proto);\n\tskb->protocol = mpls_proto;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_mpls_push);\n\n/**\n * skb_mpls_pop() - pop the outermost MPLS header\n *\n * @skb: buffer\n * @next_proto: ethertype of header after popped MPLS header\n * @mac_len: length of the MAC header\n * @ethernet: flag to indicate if the packet is ethernet\n *\n * Expects skb->data at mac header.\n *\n * Returns 0 on success, -errno otherwise.\n */\nint skb_mpls_pop(struct sk_buff *skb, __be16 next_proto, int mac_len,\n\t\t bool ethernet)\n{\n\tint err;\n\n\tif (unlikely(!eth_p_mpls(skb->protocol)))\n\t\treturn 0;\n\n\terr = skb_ensure_writable(skb, mac_len + MPLS_HLEN);\n\tif (unlikely(err))\n\t\treturn err;\n\n\tskb_postpull_rcsum(skb, mpls_hdr(skb), MPLS_HLEN);\n\tmemmove(skb_mac_header(skb) + MPLS_HLEN, skb_mac_header(skb),\n\t\tmac_len);\n\n\t__skb_pull(skb, MPLS_HLEN);\n\tskb_reset_mac_header(skb);\n\tskb_set_network_header(skb, mac_len);\n\n\tif (ethernet && mac_len >= ETH_HLEN) {\n\t\tstruct ethhdr *hdr;\n\n\t\t/* use mpls_hdr() to get ethertype to account for VLANs. */\n\t\thdr = (struct ethhdr *)((void *)mpls_hdr(skb) - ETH_HLEN);\n\t\tskb_mod_eth_type(skb, hdr, next_proto);\n\t}\n\tskb->protocol = next_proto;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_mpls_pop);\n\n/**\n * skb_mpls_update_lse() - modify outermost MPLS header and update csum\n *\n * @skb: buffer\n * @mpls_lse: new MPLS label stack entry to update to\n *\n * Expects skb->data at mac header.\n *\n * Returns 0 on success, -errno otherwise.\n */\nint skb_mpls_update_lse(struct sk_buff *skb, __be32 mpls_lse)\n{\n\tint err;\n\n\tif (unlikely(!eth_p_mpls(skb->protocol)))\n\t\treturn -EINVAL;\n\n\terr = skb_ensure_writable(skb, skb->mac_len + MPLS_HLEN);\n\tif (unlikely(err))\n\t\treturn err;\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\t__be32 diff[] = { ~mpls_hdr(skb)->label_stack_entry, mpls_lse };\n\n\t\tskb->csum = csum_partial((char *)diff, sizeof(diff), skb->csum);\n\t}\n\n\tmpls_hdr(skb)->label_stack_entry = mpls_lse;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(skb_mpls_update_lse);\n\n/**\n * skb_mpls_dec_ttl() - decrement the TTL of the outermost MPLS header\n *\n * @skb: buffer\n *\n * Expects skb->data at mac header.\n *\n * Returns 0 on success, -errno otherwise.\n */\nint skb_mpls_dec_ttl(struct sk_buff *skb)\n{\n\tu32 lse;\n\tu8 ttl;\n\n\tif (unlikely(!eth_p_mpls(skb->protocol)))\n\t\treturn -EINVAL;\n\n\tif (!pskb_may_pull(skb, skb_network_offset(skb) + MPLS_HLEN))\n\t\treturn -ENOMEM;\n\n\tlse = be32_to_cpu(mpls_hdr(skb)->label_stack_entry);\n\tttl = (lse & MPLS_LS_TTL_MASK) >> MPLS_LS_TTL_SHIFT;\n\tif (!--ttl)\n\t\treturn -EINVAL;\n\n\tlse &= ~MPLS_LS_TTL_MASK;\n\tlse |= ttl << MPLS_LS_TTL_SHIFT;\n\n\treturn skb_mpls_update_lse(skb, cpu_to_be32(lse));\n}\nEXPORT_SYMBOL_GPL(skb_mpls_dec_ttl);\n\n/**\n * alloc_skb_with_frags - allocate skb with page frags\n *\n * @header_len: size of linear part\n * @data_len: needed length in frags\n * @max_page_order: max page order desired.\n * @errcode: pointer to error code if any\n * @gfp_mask: allocation mask\n *\n * This can be used to allocate a paged skb, given a maximal order for frags.\n */\nstruct sk_buff *alloc_skb_with_frags(unsigned long header_len,\n\t\t\t\t     unsigned long data_len,\n\t\t\t\t     int max_page_order,\n\t\t\t\t     int *errcode,\n\t\t\t\t     gfp_t gfp_mask)\n{\n\tint npages = (data_len + (PAGE_SIZE - 1)) >> PAGE_SHIFT;\n\tunsigned long chunk;\n\tstruct sk_buff *skb;\n\tstruct page *page;\n\tint i;\n\n\t*errcode = -EMSGSIZE;\n\t/* Note this test could be relaxed, if we succeed to allocate\n\t * high order pages...\n\t */\n\tif (npages > MAX_SKB_FRAGS)\n\t\treturn NULL;\n\n\t*errcode = -ENOBUFS;\n\tskb = alloc_skb(header_len, gfp_mask);\n\tif (!skb)\n\t\treturn NULL;\n\n\tskb->truesize += npages << PAGE_SHIFT;\n\n\tfor (i = 0; npages > 0; i++) {\n\t\tint order = max_page_order;\n\n\t\twhile (order) {\n\t\t\tif (npages >= 1 << order) {\n\t\t\t\tpage = alloc_pages((gfp_mask & ~__GFP_DIRECT_RECLAIM) |\n\t\t\t\t\t\t   __GFP_COMP |\n\t\t\t\t\t\t   __GFP_NOWARN,\n\t\t\t\t\t\t   order);\n\t\t\t\tif (page)\n\t\t\t\t\tgoto fill_page;\n\t\t\t\t/* Do not retry other high order allocations */\n\t\t\t\torder = 1;\n\t\t\t\tmax_page_order = 0;\n\t\t\t}\n\t\t\torder--;\n\t\t}\n\t\tpage = alloc_page(gfp_mask);\n\t\tif (!page)\n\t\t\tgoto failure;\nfill_page:\n\t\tchunk = min_t(unsigned long, data_len,\n\t\t\t      PAGE_SIZE << order);\n\t\tskb_fill_page_desc(skb, i, page, 0, chunk);\n\t\tdata_len -= chunk;\n\t\tnpages -= 1 << order;\n\t}\n\treturn skb;\n\nfailure:\n\tkfree_skb(skb);\n\treturn NULL;\n}\nEXPORT_SYMBOL(alloc_skb_with_frags);\n\n/* carve out the first off bytes from skb when off < headlen */\nstatic int pskb_carve_inside_header(struct sk_buff *skb, const u32 off,\n\t\t\t\t    const int headlen, gfp_t gfp_mask)\n{\n\tint i;\n\tint size = skb_end_offset(skb);\n\tint new_hlen = headlen - off;\n\tu8 *data;\n\n\tsize = SKB_DATA_ALIGN(size);\n\n\tif (skb_pfmemalloc(skb))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\tdata = kmalloc_reserve(size +\n\t\t\t       SKB_DATA_ALIGN(sizeof(struct skb_shared_info)),\n\t\t\t       gfp_mask, NUMA_NO_NODE, NULL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tsize = SKB_WITH_OVERHEAD(ksize(data));\n\n\t/* Copy real data, and all frags */\n\tskb_copy_from_linear_data_offset(skb, off, data, new_hlen);\n\tskb->len -= off;\n\n\tmemcpy((struct skb_shared_info *)(data + size),\n\t       skb_shinfo(skb),\n\t       offsetof(struct skb_shared_info,\n\t\t\tfrags[skb_shinfo(skb)->nr_frags]));\n\tif (skb_cloned(skb)) {\n\t\t/* drop the old head gracefully */\n\t\tif (skb_orphan_frags(skb, gfp_mask)) {\n\t\t\tkfree(data);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++)\n\t\t\tskb_frag_ref(skb, i);\n\t\tif (skb_has_frag_list(skb))\n\t\t\tskb_clone_fraglist(skb);\n\t\tskb_release_data(skb);\n\t} else {\n\t\t/* we can reuse existing recount- all we did was\n\t\t * relocate values\n\t\t */\n\t\tskb_free_head(skb);\n\t}\n\n\tskb->head = data;\n\tskb->data = data;\n\tskb->head_frag = 0;\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\n\tskb->end = size;\n#else\n\tskb->end = skb->head + size;\n#endif\n\tskb_set_tail_pointer(skb, skb_headlen(skb));\n\tskb_headers_offset_update(skb, 0);\n\tskb->cloned = 0;\n\tskb->hdr_len = 0;\n\tskb->nohdr = 0;\n\tatomic_set(&skb_shinfo(skb)->dataref, 1);\n\n\treturn 0;\n}\n\nstatic int pskb_carve(struct sk_buff *skb, const u32 off, gfp_t gfp);\n\n/* carve out the first eat bytes from skb's frag_list. May recurse into\n * pskb_carve()\n */\nstatic int pskb_carve_frag_list(struct sk_buff *skb,\n\t\t\t\tstruct skb_shared_info *shinfo, int eat,\n\t\t\t\tgfp_t gfp_mask)\n{\n\tstruct sk_buff *list = shinfo->frag_list;\n\tstruct sk_buff *clone = NULL;\n\tstruct sk_buff *insp = NULL;\n\n\tdo {\n\t\tif (!list) {\n\t\t\tpr_err(\"Not enough bytes to eat. Want %d\\n\", eat);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tif (list->len <= eat) {\n\t\t\t/* Eaten as whole. */\n\t\t\teat -= list->len;\n\t\t\tlist = list->next;\n\t\t\tinsp = list;\n\t\t} else {\n\t\t\t/* Eaten partially. */\n\t\t\tif (skb_shared(list)) {\n\t\t\t\tclone = skb_clone(list, gfp_mask);\n\t\t\t\tif (!clone)\n\t\t\t\t\treturn -ENOMEM;\n\t\t\t\tinsp = list->next;\n\t\t\t\tlist = clone;\n\t\t\t} else {\n\t\t\t\t/* This may be pulled without problems. */\n\t\t\t\tinsp = list;\n\t\t\t}\n\t\t\tif (pskb_carve(list, eat, gfp_mask) < 0) {\n\t\t\t\tkfree_skb(clone);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t} while (eat);\n\n\t/* Free pulled out fragments. */\n\twhile ((list = shinfo->frag_list) != insp) {\n\t\tshinfo->frag_list = list->next;\n\t\tkfree_skb(list);\n\t}\n\t/* And insert new clone at head. */\n\tif (clone) {\n\t\tclone->next = list;\n\t\tshinfo->frag_list = clone;\n\t}\n\treturn 0;\n}\n\n/* carve off first len bytes from skb. Split line (off) is in the\n * non-linear part of skb\n */\nstatic int pskb_carve_inside_nonlinear(struct sk_buff *skb, const u32 off,\n\t\t\t\t       int pos, gfp_t gfp_mask)\n{\n\tint i, k = 0;\n\tint size = skb_end_offset(skb);\n\tu8 *data;\n\tconst int nfrags = skb_shinfo(skb)->nr_frags;\n\tstruct skb_shared_info *shinfo;\n\n\tsize = SKB_DATA_ALIGN(size);\n\n\tif (skb_pfmemalloc(skb))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\tdata = kmalloc_reserve(size +\n\t\t\t       SKB_DATA_ALIGN(sizeof(struct skb_shared_info)),\n\t\t\t       gfp_mask, NUMA_NO_NODE, NULL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tsize = SKB_WITH_OVERHEAD(ksize(data));\n\n\tmemcpy((struct skb_shared_info *)(data + size),\n\t       skb_shinfo(skb), offsetof(struct skb_shared_info, frags[0]));\n\tif (skb_orphan_frags(skb, gfp_mask)) {\n\t\tkfree(data);\n\t\treturn -ENOMEM;\n\t}\n\tshinfo = (struct skb_shared_info *)(data + size);\n\tfor (i = 0; i < nfrags; i++) {\n\t\tint fsize = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (pos + fsize > off) {\n\t\t\tshinfo->frags[k] = skb_shinfo(skb)->frags[i];\n\n\t\t\tif (pos < off) {\n\t\t\t\t/* Split frag.\n\t\t\t\t * We have two variants in this case:\n\t\t\t\t * 1. Move all the frag to the second\n\t\t\t\t *    part, if it is possible. F.e.\n\t\t\t\t *    this approach is mandatory for TUX,\n\t\t\t\t *    where splitting is expensive.\n\t\t\t\t * 2. Split is accurately. We make this.\n\t\t\t\t */\n\t\t\t\tskb_frag_off_add(&shinfo->frags[0], off - pos);\n\t\t\t\tskb_frag_size_sub(&shinfo->frags[0], off - pos);\n\t\t\t}\n\t\t\tskb_frag_ref(skb, i);\n\t\t\tk++;\n\t\t}\n\t\tpos += fsize;\n\t}\n\tshinfo->nr_frags = k;\n\tif (skb_has_frag_list(skb))\n\t\tskb_clone_fraglist(skb);\n\n\t/* split line is in frag list */\n\tif (k == 0 && pskb_carve_frag_list(skb, shinfo, off - pos, gfp_mask)) {\n\t\t/* skb_frag_unref() is not needed here as shinfo->nr_frags = 0. */\n\t\tif (skb_has_frag_list(skb))\n\t\t\tkfree_skb_list(skb_shinfo(skb)->frag_list);\n\t\tkfree(data);\n\t\treturn -ENOMEM;\n\t}\n\tskb_release_data(skb);\n\n\tskb->head = data;\n\tskb->head_frag = 0;\n\tskb->data = data;\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\n\tskb->end = size;\n#else\n\tskb->end = skb->head + size;\n#endif\n\tskb_reset_tail_pointer(skb);\n\tskb_headers_offset_update(skb, 0);\n\tskb->cloned   = 0;\n\tskb->hdr_len  = 0;\n\tskb->nohdr    = 0;\n\tskb->len -= off;\n\tskb->data_len = skb->len;\n\tatomic_set(&skb_shinfo(skb)->dataref, 1);\n\treturn 0;\n}\n\n/* remove len bytes from the beginning of the skb */\nstatic int pskb_carve(struct sk_buff *skb, const u32 len, gfp_t gfp)\n{\n\tint headlen = skb_headlen(skb);\n\n\tif (len < headlen)\n\t\treturn pskb_carve_inside_header(skb, len, headlen, gfp);\n\telse\n\t\treturn pskb_carve_inside_nonlinear(skb, len, headlen, gfp);\n}\n\n/* Extract to_copy bytes starting at off from skb, and return this in\n * a new skb\n */\nstruct sk_buff *pskb_extract(struct sk_buff *skb, int off,\n\t\t\t     int to_copy, gfp_t gfp)\n{\n\tstruct sk_buff  *clone = skb_clone(skb, gfp);\n\n\tif (!clone)\n\t\treturn NULL;\n\n\tif (pskb_carve(clone, off, gfp) < 0 ||\n\t    pskb_trim(clone, to_copy)) {\n\t\tkfree_skb(clone);\n\t\treturn NULL;\n\t}\n\treturn clone;\n}\nEXPORT_SYMBOL(pskb_extract);\n\n/**\n * skb_condense - try to get rid of fragments/frag_list if possible\n * @skb: buffer\n *\n * Can be used to save memory before skb is added to a busy queue.\n * If packet has bytes in frags and enough tail room in skb->head,\n * pull all of them, so that we can free the frags right now and adjust\n * truesize.\n * Notes:\n *\tWe do not reallocate skb->head thus can not fail.\n *\tCaller must re-evaluate skb->truesize if needed.\n */\nvoid skb_condense(struct sk_buff *skb)\n{\n\tif (skb->data_len) {\n\t\tif (skb->data_len > skb->end - skb->tail ||\n\t\t    skb_cloned(skb))\n\t\t\treturn;\n\n\t\t/* Nice, we can free page frag(s) right now */\n\t\t__pskb_pull_tail(skb, skb->data_len);\n\t}\n\t/* At this point, skb->truesize might be over estimated,\n\t * because skb had a fragment, and fragments do not tell\n\t * their truesize.\n\t * When we pulled its content into skb->head, fragment\n\t * was freed, but __pskb_pull_tail() could not possibly\n\t * adjust skb->truesize, not knowing the frag truesize.\n\t */\n\tskb->truesize = SKB_TRUESIZE(skb_end_offset(skb));\n}\n\n#ifdef CONFIG_SKB_EXTENSIONS\nstatic void *skb_ext_get_ptr(struct skb_ext *ext, enum skb_ext_id id)\n{\n\treturn (void *)ext + (ext->offset[id] * SKB_EXT_ALIGN_VALUE);\n}\n\n/**\n * __skb_ext_alloc - allocate a new skb extensions storage\n *\n * @flags: See kmalloc().\n *\n * Returns the newly allocated pointer. The pointer can later attached to a\n * skb via __skb_ext_set().\n * Note: caller must handle the skb_ext as an opaque data.\n */\nstruct skb_ext *__skb_ext_alloc(gfp_t flags)\n{\n\tstruct skb_ext *new = kmem_cache_alloc(skbuff_ext_cache, flags);\n\n\tif (new) {\n\t\tmemset(new->offset, 0, sizeof(new->offset));\n\t\trefcount_set(&new->refcnt, 1);\n\t}\n\n\treturn new;\n}\n\nstatic struct skb_ext *skb_ext_maybe_cow(struct skb_ext *old,\n\t\t\t\t\t unsigned int old_active)\n{\n\tstruct skb_ext *new;\n\n\tif (refcount_read(&old->refcnt) == 1)\n\t\treturn old;\n\n\tnew = kmem_cache_alloc(skbuff_ext_cache, GFP_ATOMIC);\n\tif (!new)\n\t\treturn NULL;\n\n\tmemcpy(new, old, old->chunks * SKB_EXT_ALIGN_VALUE);\n\trefcount_set(&new->refcnt, 1);\n\n#ifdef CONFIG_XFRM\n\tif (old_active & (1 << SKB_EXT_SEC_PATH)) {\n\t\tstruct sec_path *sp = skb_ext_get_ptr(old, SKB_EXT_SEC_PATH);\n\t\tunsigned int i;\n\n\t\tfor (i = 0; i < sp->len; i++)\n\t\t\txfrm_state_hold(sp->xvec[i]);\n\t}\n#endif\n\t__skb_ext_put(old);\n\treturn new;\n}\n\n/**\n * __skb_ext_set - attach the specified extension storage to this skb\n * @skb: buffer\n * @id: extension id\n * @ext: extension storage previously allocated via __skb_ext_alloc()\n *\n * Existing extensions, if any, are cleared.\n *\n * Returns the pointer to the extension.\n */\nvoid *__skb_ext_set(struct sk_buff *skb, enum skb_ext_id id,\n\t\t    struct skb_ext *ext)\n{\n\tunsigned int newlen, newoff = SKB_EXT_CHUNKSIZEOF(*ext);\n\n\tskb_ext_put(skb);\n\tnewlen = newoff + skb_ext_type_len[id];\n\text->chunks = newlen;\n\text->offset[id] = newoff;\n\tskb->extensions = ext;\n\tskb->active_extensions = 1 << id;\n\treturn skb_ext_get_ptr(ext, id);\n}\n\n/**\n * skb_ext_add - allocate space for given extension, COW if needed\n * @skb: buffer\n * @id: extension to allocate space for\n *\n * Allocates enough space for the given extension.\n * If the extension is already present, a pointer to that extension\n * is returned.\n *\n * If the skb was cloned, COW applies and the returned memory can be\n * modified without changing the extension space of clones buffers.\n *\n * Returns pointer to the extension or NULL on allocation failure.\n */\nvoid *skb_ext_add(struct sk_buff *skb, enum skb_ext_id id)\n{\n\tstruct skb_ext *new, *old = NULL;\n\tunsigned int newlen, newoff;\n\n\tif (skb->active_extensions) {\n\t\told = skb->extensions;\n\n\t\tnew = skb_ext_maybe_cow(old, skb->active_extensions);\n\t\tif (!new)\n\t\t\treturn NULL;\n\n\t\tif (__skb_ext_exist(new, id))\n\t\t\tgoto set_active;\n\n\t\tnewoff = new->chunks;\n\t} else {\n\t\tnewoff = SKB_EXT_CHUNKSIZEOF(*new);\n\n\t\tnew = __skb_ext_alloc(GFP_ATOMIC);\n\t\tif (!new)\n\t\t\treturn NULL;\n\t}\n\n\tnewlen = newoff + skb_ext_type_len[id];\n\tnew->chunks = newlen;\n\tnew->offset[id] = newoff;\nset_active:\n\tskb->extensions = new;\n\tskb->active_extensions |= 1 << id;\n\treturn skb_ext_get_ptr(new, id);\n}\nEXPORT_SYMBOL(skb_ext_add);\n\n#ifdef CONFIG_XFRM\nstatic void skb_ext_put_sp(struct sec_path *sp)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < sp->len; i++)\n\t\txfrm_state_put(sp->xvec[i]);\n}\n#endif\n\nvoid __skb_ext_del(struct sk_buff *skb, enum skb_ext_id id)\n{\n\tstruct skb_ext *ext = skb->extensions;\n\n\tskb->active_extensions &= ~(1 << id);\n\tif (skb->active_extensions == 0) {\n\t\tskb->extensions = NULL;\n\t\t__skb_ext_put(ext);\n#ifdef CONFIG_XFRM\n\t} else if (id == SKB_EXT_SEC_PATH &&\n\t\t   refcount_read(&ext->refcnt) == 1) {\n\t\tstruct sec_path *sp = skb_ext_get_ptr(ext, SKB_EXT_SEC_PATH);\n\n\t\tskb_ext_put_sp(sp);\n\t\tsp->len = 0;\n#endif\n\t}\n}\nEXPORT_SYMBOL(__skb_ext_del);\n\nvoid __skb_ext_put(struct skb_ext *ext)\n{\n\t/* If this is last clone, nothing can increment\n\t * it after check passes.  Avoids one atomic op.\n\t */\n\tif (refcount_read(&ext->refcnt) == 1)\n\t\tgoto free_now;\n\n\tif (!refcount_dec_and_test(&ext->refcnt))\n\t\treturn;\nfree_now:\n#ifdef CONFIG_XFRM\n\tif (__skb_ext_exist(ext, SKB_EXT_SEC_PATH))\n\t\tskb_ext_put_sp(skb_ext_get_ptr(ext, SKB_EXT_SEC_PATH));\n#endif\n\n\tkmem_cache_free(skbuff_ext_cache, ext);\n}\nEXPORT_SYMBOL(__skb_ext_put);\n#endif /* CONFIG_SKB_EXTENSIONS */\n"}, "2": {"id": 2, "path": "/src/include/linux/kernel.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_KERNEL_H\n#define _LINUX_KERNEL_H\n\n#include <stdarg.h>\n#include <linux/align.h>\n#include <linux/limits.h>\n#include <linux/linkage.h>\n#include <linux/stddef.h>\n#include <linux/types.h>\n#include <linux/compiler.h>\n#include <linux/bitops.h>\n#include <linux/log2.h>\n#include <linux/math.h>\n#include <linux/minmax.h>\n#include <linux/typecheck.h>\n#include <linux/printk.h>\n#include <linux/build_bug.h>\n#include <linux/static_call_types.h>\n#include <asm/byteorder.h>\n\n#include <uapi/linux/kernel.h>\n\n#define STACK_MAGIC\t0xdeadbeef\n\n/**\n * REPEAT_BYTE - repeat the value @x multiple times as an unsigned long value\n * @x: value to repeat\n *\n * NOTE: @x is not checked for > 0xff; larger values produce odd results.\n */\n#define REPEAT_BYTE(x)\t((~0ul / 0xff) * (x))\n\n/* generic data direction definitions */\n#define READ\t\t\t0\n#define WRITE\t\t\t1\n\n/**\n * ARRAY_SIZE - get the number of elements in array @arr\n * @arr: array to be sized\n */\n#define ARRAY_SIZE(arr) (sizeof(arr) / sizeof((arr)[0]) + __must_be_array(arr))\n\n#define u64_to_user_ptr(x) (\t\t\\\n{\t\t\t\t\t\\\n\ttypecheck(u64, (x));\t\t\\\n\t(void __user *)(uintptr_t)(x);\t\\\n}\t\t\t\t\t\\\n)\n\n#define typeof_member(T, m)\ttypeof(((T*)0)->m)\n\n#define _RET_IP_\t\t(unsigned long)__builtin_return_address(0)\n#define _THIS_IP_  ({ __label__ __here; __here: (unsigned long)&&__here; })\n\n/**\n * upper_32_bits - return bits 32-63 of a number\n * @n: the number we're accessing\n *\n * A basic shift-right of a 64- or 32-bit quantity.  Use this to suppress\n * the \"right shift count >= width of type\" warning when that quantity is\n * 32-bits.\n */\n#define upper_32_bits(n) ((u32)(((n) >> 16) >> 16))\n\n/**\n * lower_32_bits - return bits 0-31 of a number\n * @n: the number we're accessing\n */\n#define lower_32_bits(n) ((u32)((n) & 0xffffffff))\n\nstruct completion;\nstruct pt_regs;\nstruct user;\n\n#ifdef CONFIG_PREEMPT_VOLUNTARY\n\nextern int __cond_resched(void);\n# define might_resched() __cond_resched()\n\n#elif defined(CONFIG_PREEMPT_DYNAMIC)\n\nextern int __cond_resched(void);\n\nDECLARE_STATIC_CALL(might_resched, __cond_resched);\n\nstatic __always_inline void might_resched(void)\n{\n\tstatic_call_mod(might_resched)();\n}\n\n#else\n\n# define might_resched() do { } while (0)\n\n#endif /* CONFIG_PREEMPT_* */\n\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\nextern void ___might_sleep(const char *file, int line, int preempt_offset);\nextern void __might_sleep(const char *file, int line, int preempt_offset);\nextern void __cant_sleep(const char *file, int line, int preempt_offset);\nextern void __cant_migrate(const char *file, int line);\n\n/**\n * might_sleep - annotation for functions that can sleep\n *\n * this macro will print a stack trace if it is executed in an atomic\n * context (spinlock, irq-handler, ...). Additional sections where blocking is\n * not allowed can be annotated with non_block_start() and non_block_end()\n * pairs.\n *\n * This is a useful debugging help to be able to catch problems early and not\n * be bitten later when the calling function happens to sleep when it is not\n * supposed to.\n */\n# define might_sleep() \\\n\tdo { __might_sleep(__FILE__, __LINE__, 0); might_resched(); } while (0)\n/**\n * cant_sleep - annotation for functions that cannot sleep\n *\n * this macro will print a stack trace if it is executed with preemption enabled\n */\n# define cant_sleep() \\\n\tdo { __cant_sleep(__FILE__, __LINE__, 0); } while (0)\n# define sched_annotate_sleep()\t(current->task_state_change = 0)\n\n/**\n * cant_migrate - annotation for functions that cannot migrate\n *\n * Will print a stack trace if executed in code which is migratable\n */\n# define cant_migrate()\t\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (IS_ENABLED(CONFIG_SMP))\t\t\t\t\\\n\t\t\t__cant_migrate(__FILE__, __LINE__);\t\t\\\n\t} while (0)\n\n/**\n * non_block_start - annotate the start of section where sleeping is prohibited\n *\n * This is on behalf of the oom reaper, specifically when it is calling the mmu\n * notifiers. The problem is that if the notifier were to block on, for example,\n * mutex_lock() and if the process which holds that mutex were to perform a\n * sleeping memory allocation, the oom reaper is now blocked on completion of\n * that memory allocation. Other blocking calls like wait_event() pose similar\n * issues.\n */\n# define non_block_start() (current->non_block_count++)\n/**\n * non_block_end - annotate the end of section where sleeping is prohibited\n *\n * Closes a section opened by non_block_start().\n */\n# define non_block_end() WARN_ON(current->non_block_count-- == 0)\n#else\n  static inline void ___might_sleep(const char *file, int line,\n\t\t\t\t   int preempt_offset) { }\n  static inline void __might_sleep(const char *file, int line,\n\t\t\t\t   int preempt_offset) { }\n# define might_sleep() do { might_resched(); } while (0)\n# define cant_sleep() do { } while (0)\n# define cant_migrate()\t\tdo { } while (0)\n# define sched_annotate_sleep() do { } while (0)\n# define non_block_start() do { } while (0)\n# define non_block_end() do { } while (0)\n#endif\n\n#define might_sleep_if(cond) do { if (cond) might_sleep(); } while (0)\n\n#if defined(CONFIG_MMU) && \\\n\t(defined(CONFIG_PROVE_LOCKING) || defined(CONFIG_DEBUG_ATOMIC_SLEEP))\n#define might_fault() __might_fault(__FILE__, __LINE__)\nvoid __might_fault(const char *file, int line);\n#else\nstatic inline void might_fault(void) { }\n#endif\n\nextern struct atomic_notifier_head panic_notifier_list;\nextern long (*panic_blink)(int state);\n__printf(1, 2)\nvoid panic(const char *fmt, ...) __noreturn __cold;\nvoid nmi_panic(struct pt_regs *regs, const char *msg);\nextern void oops_enter(void);\nextern void oops_exit(void);\nextern bool oops_may_print(void);\nvoid do_exit(long error_code) __noreturn;\nvoid complete_and_exit(struct completion *, long) __noreturn;\n\n/* Internal, do not use. */\nint __must_check _kstrtoul(const char *s, unsigned int base, unsigned long *res);\nint __must_check _kstrtol(const char *s, unsigned int base, long *res);\n\nint __must_check kstrtoull(const char *s, unsigned int base, unsigned long long *res);\nint __must_check kstrtoll(const char *s, unsigned int base, long long *res);\n\n/**\n * kstrtoul - convert a string to an unsigned long\n * @s: The start of the string. The string must be null-terminated, and may also\n *  include a single newline before its terminating null. The first character\n *  may also be a plus sign, but not a minus sign.\n * @base: The number base to use. The maximum supported base is 16. If base is\n *  given as 0, then the base of the string is automatically detected with the\n *  conventional semantics - If it begins with 0x the number will be parsed as a\n *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be\n *  parsed as an octal number. Otherwise it will be parsed as a decimal.\n * @res: Where to write the result of the conversion on success.\n *\n * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.\n * Preferred over simple_strtoul(). Return code must be checked.\n*/\nstatic inline int __must_check kstrtoul(const char *s, unsigned int base, unsigned long *res)\n{\n\t/*\n\t * We want to shortcut function call, but\n\t * __builtin_types_compatible_p(unsigned long, unsigned long long) = 0.\n\t */\n\tif (sizeof(unsigned long) == sizeof(unsigned long long) &&\n\t    __alignof__(unsigned long) == __alignof__(unsigned long long))\n\t\treturn kstrtoull(s, base, (unsigned long long *)res);\n\telse\n\t\treturn _kstrtoul(s, base, res);\n}\n\n/**\n * kstrtol - convert a string to a long\n * @s: The start of the string. The string must be null-terminated, and may also\n *  include a single newline before its terminating null. The first character\n *  may also be a plus sign or a minus sign.\n * @base: The number base to use. The maximum supported base is 16. If base is\n *  given as 0, then the base of the string is automatically detected with the\n *  conventional semantics - If it begins with 0x the number will be parsed as a\n *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be\n *  parsed as an octal number. Otherwise it will be parsed as a decimal.\n * @res: Where to write the result of the conversion on success.\n *\n * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.\n * Preferred over simple_strtol(). Return code must be checked.\n */\nstatic inline int __must_check kstrtol(const char *s, unsigned int base, long *res)\n{\n\t/*\n\t * We want to shortcut function call, but\n\t * __builtin_types_compatible_p(long, long long) = 0.\n\t */\n\tif (sizeof(long) == sizeof(long long) &&\n\t    __alignof__(long) == __alignof__(long long))\n\t\treturn kstrtoll(s, base, (long long *)res);\n\telse\n\t\treturn _kstrtol(s, base, res);\n}\n\nint __must_check kstrtouint(const char *s, unsigned int base, unsigned int *res);\nint __must_check kstrtoint(const char *s, unsigned int base, int *res);\n\nstatic inline int __must_check kstrtou64(const char *s, unsigned int base, u64 *res)\n{\n\treturn kstrtoull(s, base, res);\n}\n\nstatic inline int __must_check kstrtos64(const char *s, unsigned int base, s64 *res)\n{\n\treturn kstrtoll(s, base, res);\n}\n\nstatic inline int __must_check kstrtou32(const char *s, unsigned int base, u32 *res)\n{\n\treturn kstrtouint(s, base, res);\n}\n\nstatic inline int __must_check kstrtos32(const char *s, unsigned int base, s32 *res)\n{\n\treturn kstrtoint(s, base, res);\n}\n\nint __must_check kstrtou16(const char *s, unsigned int base, u16 *res);\nint __must_check kstrtos16(const char *s, unsigned int base, s16 *res);\nint __must_check kstrtou8(const char *s, unsigned int base, u8 *res);\nint __must_check kstrtos8(const char *s, unsigned int base, s8 *res);\nint __must_check kstrtobool(const char *s, bool *res);\n\nint __must_check kstrtoull_from_user(const char __user *s, size_t count, unsigned int base, unsigned long long *res);\nint __must_check kstrtoll_from_user(const char __user *s, size_t count, unsigned int base, long long *res);\nint __must_check kstrtoul_from_user(const char __user *s, size_t count, unsigned int base, unsigned long *res);\nint __must_check kstrtol_from_user(const char __user *s, size_t count, unsigned int base, long *res);\nint __must_check kstrtouint_from_user(const char __user *s, size_t count, unsigned int base, unsigned int *res);\nint __must_check kstrtoint_from_user(const char __user *s, size_t count, unsigned int base, int *res);\nint __must_check kstrtou16_from_user(const char __user *s, size_t count, unsigned int base, u16 *res);\nint __must_check kstrtos16_from_user(const char __user *s, size_t count, unsigned int base, s16 *res);\nint __must_check kstrtou8_from_user(const char __user *s, size_t count, unsigned int base, u8 *res);\nint __must_check kstrtos8_from_user(const char __user *s, size_t count, unsigned int base, s8 *res);\nint __must_check kstrtobool_from_user(const char __user *s, size_t count, bool *res);\n\nstatic inline int __must_check kstrtou64_from_user(const char __user *s, size_t count, unsigned int base, u64 *res)\n{\n\treturn kstrtoull_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtos64_from_user(const char __user *s, size_t count, unsigned int base, s64 *res)\n{\n\treturn kstrtoll_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtou32_from_user(const char __user *s, size_t count, unsigned int base, u32 *res)\n{\n\treturn kstrtouint_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtos32_from_user(const char __user *s, size_t count, unsigned int base, s32 *res)\n{\n\treturn kstrtoint_from_user(s, count, base, res);\n}\n\n/*\n * Use kstrto<foo> instead.\n *\n * NOTE: simple_strto<foo> does not check for the range overflow and,\n *\t depending on the input, may give interesting results.\n *\n * Use these functions if and only if you cannot use kstrto<foo>, because\n * the conversion ends on the first non-digit character, which may be far\n * beyond the supported range. It might be useful to parse the strings like\n * 10x50 or 12:21 without altering original string or temporary buffer in use.\n * Keep in mind above caveat.\n */\n\nextern unsigned long simple_strtoul(const char *,char **,unsigned int);\nextern long simple_strtol(const char *,char **,unsigned int);\nextern unsigned long long simple_strtoull(const char *,char **,unsigned int);\nextern long long simple_strtoll(const char *,char **,unsigned int);\n\nextern int num_to_str(char *buf, int size,\n\t\t      unsigned long long num, unsigned int width);\n\n/* lib/printf utilities */\n\nextern __printf(2, 3) int sprintf(char *buf, const char * fmt, ...);\nextern __printf(2, 0) int vsprintf(char *buf, const char *, va_list);\nextern __printf(3, 4)\nint snprintf(char *buf, size_t size, const char *fmt, ...);\nextern __printf(3, 0)\nint vsnprintf(char *buf, size_t size, const char *fmt, va_list args);\nextern __printf(3, 4)\nint scnprintf(char *buf, size_t size, const char *fmt, ...);\nextern __printf(3, 0)\nint vscnprintf(char *buf, size_t size, const char *fmt, va_list args);\nextern __printf(2, 3) __malloc\nchar *kasprintf(gfp_t gfp, const char *fmt, ...);\nextern __printf(2, 0) __malloc\nchar *kvasprintf(gfp_t gfp, const char *fmt, va_list args);\nextern __printf(2, 0)\nconst char *kvasprintf_const(gfp_t gfp, const char *fmt, va_list args);\n\nextern __scanf(2, 3)\nint sscanf(const char *, const char *, ...);\nextern __scanf(2, 0)\nint vsscanf(const char *, const char *, va_list);\n\nextern int get_option(char **str, int *pint);\nextern char *get_options(const char *str, int nints, int *ints);\nextern unsigned long long memparse(const char *ptr, char **retptr);\nextern bool parse_option_str(const char *str, const char *option);\nextern char *next_arg(char *args, char **param, char **val);\n\nextern int core_kernel_text(unsigned long addr);\nextern int init_kernel_text(unsigned long addr);\nextern int core_kernel_data(unsigned long addr);\nextern int __kernel_text_address(unsigned long addr);\nextern int kernel_text_address(unsigned long addr);\nextern int func_ptr_is_kernel_text(void *ptr);\n\n#ifdef CONFIG_SMP\nextern unsigned int sysctl_oops_all_cpu_backtrace;\n#else\n#define sysctl_oops_all_cpu_backtrace 0\n#endif /* CONFIG_SMP */\n\nextern void bust_spinlocks(int yes);\nextern int panic_timeout;\nextern unsigned long panic_print;\nextern int panic_on_oops;\nextern int panic_on_unrecovered_nmi;\nextern int panic_on_io_nmi;\nextern int panic_on_warn;\nextern unsigned long panic_on_taint;\nextern bool panic_on_taint_nousertaint;\nextern int sysctl_panic_on_rcu_stall;\nextern int sysctl_max_rcu_stall_to_panic;\nextern int sysctl_panic_on_stackoverflow;\n\nextern bool crash_kexec_post_notifiers;\n\n/*\n * panic_cpu is used for synchronizing panic() and crash_kexec() execution. It\n * holds a CPU number which is executing panic() currently. A value of\n * PANIC_CPU_INVALID means no CPU has entered panic() or crash_kexec().\n */\nextern atomic_t panic_cpu;\n#define PANIC_CPU_INVALID\t-1\n\n/*\n * Only to be used by arch init code. If the user over-wrote the default\n * CONFIG_PANIC_TIMEOUT, honor it.\n */\nstatic inline void set_arch_panic_timeout(int timeout, int arch_default_timeout)\n{\n\tif (panic_timeout == arch_default_timeout)\n\t\tpanic_timeout = timeout;\n}\nextern const char *print_tainted(void);\nenum lockdep_ok {\n\tLOCKDEP_STILL_OK,\n\tLOCKDEP_NOW_UNRELIABLE\n};\nextern void add_taint(unsigned flag, enum lockdep_ok);\nextern int test_taint(unsigned flag);\nextern unsigned long get_taint(void);\nextern int root_mountflags;\n\nextern bool early_boot_irqs_disabled;\n\n/*\n * Values used for system_state. Ordering of the states must not be changed\n * as code checks for <, <=, >, >= STATE.\n */\nextern enum system_states {\n\tSYSTEM_BOOTING,\n\tSYSTEM_SCHEDULING,\n\tSYSTEM_RUNNING,\n\tSYSTEM_HALT,\n\tSYSTEM_POWER_OFF,\n\tSYSTEM_RESTART,\n\tSYSTEM_SUSPEND,\n} system_state;\n\n/* This cannot be an enum because some may be used in assembly source. */\n#define TAINT_PROPRIETARY_MODULE\t0\n#define TAINT_FORCED_MODULE\t\t1\n#define TAINT_CPU_OUT_OF_SPEC\t\t2\n#define TAINT_FORCED_RMMOD\t\t3\n#define TAINT_MACHINE_CHECK\t\t4\n#define TAINT_BAD_PAGE\t\t\t5\n#define TAINT_USER\t\t\t6\n#define TAINT_DIE\t\t\t7\n#define TAINT_OVERRIDDEN_ACPI_TABLE\t8\n#define TAINT_WARN\t\t\t9\n#define TAINT_CRAP\t\t\t10\n#define TAINT_FIRMWARE_WORKAROUND\t11\n#define TAINT_OOT_MODULE\t\t12\n#define TAINT_UNSIGNED_MODULE\t\t13\n#define TAINT_SOFTLOCKUP\t\t14\n#define TAINT_LIVEPATCH\t\t\t15\n#define TAINT_AUX\t\t\t16\n#define TAINT_RANDSTRUCT\t\t17\n#define TAINT_FLAGS_COUNT\t\t18\n#define TAINT_FLAGS_MAX\t\t\t((1UL << TAINT_FLAGS_COUNT) - 1)\n\nstruct taint_flag {\n\tchar c_true;\t/* character printed when tainted */\n\tchar c_false;\t/* character printed when not tainted */\n\tbool module;\t/* also show as a per-module taint flag */\n};\n\nextern const struct taint_flag taint_flags[TAINT_FLAGS_COUNT];\n\nextern const char hex_asc[];\n#define hex_asc_lo(x)\thex_asc[((x) & 0x0f)]\n#define hex_asc_hi(x)\thex_asc[((x) & 0xf0) >> 4]\n\nstatic inline char *hex_byte_pack(char *buf, u8 byte)\n{\n\t*buf++ = hex_asc_hi(byte);\n\t*buf++ = hex_asc_lo(byte);\n\treturn buf;\n}\n\nextern const char hex_asc_upper[];\n#define hex_asc_upper_lo(x)\thex_asc_upper[((x) & 0x0f)]\n#define hex_asc_upper_hi(x)\thex_asc_upper[((x) & 0xf0) >> 4]\n\nstatic inline char *hex_byte_pack_upper(char *buf, u8 byte)\n{\n\t*buf++ = hex_asc_upper_hi(byte);\n\t*buf++ = hex_asc_upper_lo(byte);\n\treturn buf;\n}\n\nextern int hex_to_bin(char ch);\nextern int __must_check hex2bin(u8 *dst, const char *src, size_t count);\nextern char *bin2hex(char *dst, const void *src, size_t count);\n\nbool mac_pton(const char *s, u8 *mac);\n\n/*\n * General tracing related utility functions - trace_printk(),\n * tracing_on/tracing_off and tracing_start()/tracing_stop\n *\n * Use tracing_on/tracing_off when you want to quickly turn on or off\n * tracing. It simply enables or disables the recording of the trace events.\n * This also corresponds to the user space /sys/kernel/debug/tracing/tracing_on\n * file, which gives a means for the kernel and userspace to interact.\n * Place a tracing_off() in the kernel where you want tracing to end.\n * From user space, examine the trace, and then echo 1 > tracing_on\n * to continue tracing.\n *\n * tracing_stop/tracing_start has slightly more overhead. It is used\n * by things like suspend to ram where disabling the recording of the\n * trace is not enough, but tracing must actually stop because things\n * like calling smp_processor_id() may crash the system.\n *\n * Most likely, you want to use tracing_on/tracing_off.\n */\n\nenum ftrace_dump_mode {\n\tDUMP_NONE,\n\tDUMP_ALL,\n\tDUMP_ORIG,\n};\n\n#ifdef CONFIG_TRACING\nvoid tracing_on(void);\nvoid tracing_off(void);\nint tracing_is_on(void);\nvoid tracing_snapshot(void);\nvoid tracing_snapshot_alloc(void);\n\nextern void tracing_start(void);\nextern void tracing_stop(void);\n\nstatic inline __printf(1, 2)\nvoid ____trace_printk_check_format(const char *fmt, ...)\n{\n}\n#define __trace_printk_check_format(fmt, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\t\\\n\t\t____trace_printk_check_format(fmt, ##args);\t\t\\\n} while (0)\n\n/**\n * trace_printk - printf formatting in the ftrace buffer\n * @fmt: the printf format for printing\n *\n * Note: __trace_printk is an internal function for trace_printk() and\n *       the @ip is passed in via the trace_printk() macro.\n *\n * This function allows a kernel developer to debug fast path sections\n * that printk is not appropriate for. By scattering in various\n * printk like tracing in the code, a developer can quickly see\n * where problems are occurring.\n *\n * This is intended as a debugging tool for the developer only.\n * Please refrain from leaving trace_printks scattered around in\n * your code. (Extra memory is used for special buffers that are\n * allocated when trace_printk() is used.)\n *\n * A little optimization trick is done here. If there's only one\n * argument, there's no need to scan the string for printf formats.\n * The trace_puts() will suffice. But how can we take advantage of\n * using trace_puts() when trace_printk() has only one argument?\n * By stringifying the args and checking the size we can tell\n * whether or not there are args. __stringify((__VA_ARGS__)) will\n * turn into \"()\\0\" with a size of 3 when there are no args, anything\n * else will be bigger. All we need to do is define a string to this,\n * and then take its size and compare to 3. If it's bigger, use\n * do_trace_printk() otherwise, optimize it to trace_puts(). Then just\n * let gcc optimize the rest.\n */\n\n#define trace_printk(fmt, ...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\\\n\tchar _______STR[] = __stringify((__VA_ARGS__));\t\\\n\tif (sizeof(_______STR) > 3)\t\t\t\\\n\t\tdo_trace_printk(fmt, ##__VA_ARGS__);\t\\\n\telse\t\t\t\t\t\t\\\n\t\ttrace_puts(fmt);\t\t\t\\\n} while (0)\n\n#define do_trace_printk(fmt, args...)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tstatic const char *trace_printk_fmt __used\t\t\t\\\n\t\t__section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t__builtin_constant_p(fmt) ? fmt : NULL;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t__trace_printk_check_format(fmt, ##args);\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(fmt))\t\t\t\t\t\\\n\t\t__trace_bprintk(_THIS_IP_, trace_printk_fmt, ##args);\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t__trace_printk(_THIS_IP_, fmt, ##args);\t\t\t\\\n} while (0)\n\nextern __printf(2, 3)\nint __trace_bprintk(unsigned long ip, const char *fmt, ...);\n\nextern __printf(2, 3)\nint __trace_printk(unsigned long ip, const char *fmt, ...);\n\n/**\n * trace_puts - write a string into the ftrace buffer\n * @str: the string to record\n *\n * Note: __trace_bputs is an internal function for trace_puts and\n *       the @ip is passed in via the trace_puts macro.\n *\n * This is similar to trace_printk() but is made for those really fast\n * paths that a developer wants the least amount of \"Heisenbug\" effects,\n * where the processing of the print format is still too much.\n *\n * This function allows a kernel developer to debug fast path sections\n * that printk is not appropriate for. By scattering in various\n * printk like tracing in the code, a developer can quickly see\n * where problems are occurring.\n *\n * This is intended as a debugging tool for the developer only.\n * Please refrain from leaving trace_puts scattered around in\n * your code. (Extra memory is used for special buffers that are\n * allocated when trace_puts() is used.)\n *\n * Returns: 0 if nothing was written, positive # if string was.\n *  (1 when __trace_bputs is used, strlen(str) when __trace_puts is used)\n */\n\n#define trace_puts(str) ({\t\t\t\t\t\t\\\n\tstatic const char *trace_printk_fmt __used\t\t\t\\\n\t\t__section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t__builtin_constant_p(str) ? str : NULL;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(str))\t\t\t\t\t\\\n\t\t__trace_bputs(_THIS_IP_, trace_printk_fmt);\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t__trace_puts(_THIS_IP_, str, strlen(str));\t\t\\\n})\nextern int __trace_bputs(unsigned long ip, const char *str);\nextern int __trace_puts(unsigned long ip, const char *str, int size);\n\nextern void trace_dump_stack(int skip);\n\n/*\n * The double __builtin_constant_p is because gcc will give us an error\n * if we try to allocate the static variable to fmt if it is not a\n * constant. Even with the outer if statement.\n */\n#define ftrace_vprintk(fmt, vargs)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(fmt)) {\t\t\t\t\\\n\t\tstatic const char *trace_printk_fmt __used\t\t\\\n\t\t  __section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t\t__builtin_constant_p(fmt) ? fmt : NULL;\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t__ftrace_vbprintk(_THIS_IP_, trace_printk_fmt, vargs);\t\\\n\t} else\t\t\t\t\t\t\t\t\\\n\t\t__ftrace_vprintk(_THIS_IP_, fmt, vargs);\t\t\\\n} while (0)\n\nextern __printf(2, 0) int\n__ftrace_vbprintk(unsigned long ip, const char *fmt, va_list ap);\n\nextern __printf(2, 0) int\n__ftrace_vprintk(unsigned long ip, const char *fmt, va_list ap);\n\nextern void ftrace_dump(enum ftrace_dump_mode oops_dump_mode);\n#else\nstatic inline void tracing_start(void) { }\nstatic inline void tracing_stop(void) { }\nstatic inline void trace_dump_stack(int skip) { }\n\nstatic inline void tracing_on(void) { }\nstatic inline void tracing_off(void) { }\nstatic inline int tracing_is_on(void) { return 0; }\nstatic inline void tracing_snapshot(void) { }\nstatic inline void tracing_snapshot_alloc(void) { }\n\nstatic inline __printf(1, 2)\nint trace_printk(const char *fmt, ...)\n{\n\treturn 0;\n}\nstatic __printf(1, 0) inline int\nftrace_vprintk(const char *fmt, va_list ap)\n{\n\treturn 0;\n}\nstatic inline void ftrace_dump(enum ftrace_dump_mode oops_dump_mode) { }\n#endif /* CONFIG_TRACING */\n\n/* This counts to 12. Any more, it will return 13th argument. */\n#define __COUNT_ARGS(_0, _1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _n, X...) _n\n#define COUNT_ARGS(X...) __COUNT_ARGS(, ##X, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0)\n\n#define __CONCAT(a, b) a ## b\n#define CONCATENATE(a, b) __CONCAT(a, b)\n\n/**\n * container_of - cast a member of a structure out to the containing structure\n * @ptr:\tthe pointer to the member.\n * @type:\tthe type of the container struct this is embedded in.\n * @member:\tthe name of the member within the struct.\n *\n */\n#define container_of(ptr, type, member) ({\t\t\t\t\\\n\tvoid *__mptr = (void *)(ptr);\t\t\t\t\t\\\n\tBUILD_BUG_ON_MSG(!__same_type(*(ptr), ((type *)0)->member) &&\t\\\n\t\t\t !__same_type(*(ptr), void),\t\t\t\\\n\t\t\t \"pointer type mismatch in container_of()\");\t\\\n\t((type *)(__mptr - offsetof(type, member))); })\n\n/**\n * container_of_safe - cast a member of a structure out to the containing structure\n * @ptr:\tthe pointer to the member.\n * @type:\tthe type of the container struct this is embedded in.\n * @member:\tthe name of the member within the struct.\n *\n * If IS_ERR_OR_NULL(ptr), ptr is returned unchanged.\n */\n#define container_of_safe(ptr, type, member) ({\t\t\t\t\\\n\tvoid *__mptr = (void *)(ptr);\t\t\t\t\t\\\n\tBUILD_BUG_ON_MSG(!__same_type(*(ptr), ((type *)0)->member) &&\t\\\n\t\t\t !__same_type(*(ptr), void),\t\t\t\\\n\t\t\t \"pointer type mismatch in container_of()\");\t\\\n\tIS_ERR_OR_NULL(__mptr) ? ERR_CAST(__mptr) :\t\t\t\\\n\t\t((type *)(__mptr - offsetof(type, member))); })\n\n/* Rebuild everything on CONFIG_FTRACE_MCOUNT_RECORD */\n#ifdef CONFIG_FTRACE_MCOUNT_RECORD\n# define REBUILD_DUE_TO_FTRACE_MCOUNT_RECORD\n#endif\n\n/* Permissions on a sysfs file: you didn't miss the 0 prefix did you? */\n#define VERIFY_OCTAL_PERMISSIONS(perms)\t\t\t\t\t\t\\\n\t(BUILD_BUG_ON_ZERO((perms) < 0) +\t\t\t\t\t\\\n\t BUILD_BUG_ON_ZERO((perms) > 0777) +\t\t\t\t\t\\\n\t /* USER_READABLE >= GROUP_READABLE >= OTHER_READABLE */\t\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 6) & 4) < (((perms) >> 3) & 4)) +\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 3) & 4) < ((perms) & 4)) +\t\t\\\n\t /* USER_WRITABLE >= GROUP_WRITABLE */\t\t\t\t\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 6) & 2) < (((perms) >> 3) & 2)) +\t\\\n\t /* OTHER_WRITABLE?  Generally considered a bad idea. */\t\t\\\n\t BUILD_BUG_ON_ZERO((perms) & 2) +\t\t\t\t\t\\\n\t (perms))\n#endif\n"}, "3": {"id": 3, "path": "/src/include/linux/build_bug.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_BUILD_BUG_H\n#define _LINUX_BUILD_BUG_H\n\n#include <linux/compiler.h>\n\n#ifdef __CHECKER__\n#define BUILD_BUG_ON_ZERO(e) (0)\n#else /* __CHECKER__ */\n/*\n * Force a compilation error if condition is true, but also produce a\n * result (of value 0 and type int), so the expression can be used\n * e.g. in a structure initializer (or where-ever else comma expressions\n * aren't permitted).\n */\n#define BUILD_BUG_ON_ZERO(e) ((int)(sizeof(struct { int:(-!!(e)); })))\n#endif /* __CHECKER__ */\n\n/* Force a compilation error if a constant expression is not a power of 2 */\n#define __BUILD_BUG_ON_NOT_POWER_OF_2(n)\t\\\n\tBUILD_BUG_ON(((n) & ((n) - 1)) != 0)\n#define BUILD_BUG_ON_NOT_POWER_OF_2(n)\t\t\t\\\n\tBUILD_BUG_ON((n) == 0 || (((n) & ((n) - 1)) != 0))\n\n/*\n * BUILD_BUG_ON_INVALID() permits the compiler to check the validity of the\n * expression but avoids the generation of any code, even if that expression\n * has side-effects.\n */\n#define BUILD_BUG_ON_INVALID(e) ((void)(sizeof((__force long)(e))))\n\n/**\n * BUILD_BUG_ON_MSG - break compile if a condition is true & emit supplied\n *\t\t      error message.\n * @condition: the condition which the compiler should know is false.\n *\n * See BUILD_BUG_ON for description.\n */\n#define BUILD_BUG_ON_MSG(cond, msg) compiletime_assert(!(cond), msg)\n\n/**\n * BUILD_BUG_ON - break compile if a condition is true.\n * @condition: the condition which the compiler should know is false.\n *\n * If you have some code which relies on certain constants being equal, or\n * some other compile-time-evaluated condition, you should use BUILD_BUG_ON to\n * detect if someone changes it.\n */\n#define BUILD_BUG_ON(condition) \\\n\tBUILD_BUG_ON_MSG(condition, \"BUILD_BUG_ON failed: \" #condition)\n\n/**\n * BUILD_BUG - break compile if used.\n *\n * If you have some code that you expect the compiler to eliminate at\n * build time, you should use BUILD_BUG to detect if it is\n * unexpectedly used.\n */\n#define BUILD_BUG() BUILD_BUG_ON_MSG(1, \"BUILD_BUG failed\")\n\n/**\n * static_assert - check integer constant expression at build time\n *\n * static_assert() is a wrapper for the C11 _Static_assert, with a\n * little macro magic to make the message optional (defaulting to the\n * stringification of the tested expression).\n *\n * Contrary to BUILD_BUG_ON(), static_assert() can be used at global\n * scope, but requires the expression to be an integer constant\n * expression (i.e., it is not enough that __builtin_constant_p() is\n * true for expr).\n *\n * Also note that BUILD_BUG_ON() fails the build if the condition is\n * true, while static_assert() fails the build if the expression is\n * false.\n */\n#define static_assert(expr, ...) __static_assert(expr, ##__VA_ARGS__, #expr)\n#define __static_assert(expr, msg, ...) _Static_assert(expr, msg)\n\n#endif\t/* _LINUX_BUILD_BUG_H */\n"}, "4": {"id": 4, "path": "/src/include/linux/compiler_types.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_COMPILER_TYPES_H\n#define __LINUX_COMPILER_TYPES_H\n\n#ifndef __ASSEMBLY__\n\n#ifdef __CHECKER__\n/* address spaces */\n# define __kernel\t__attribute__((address_space(0)))\n# define __user\t\t__attribute__((noderef, address_space(__user)))\n# define __iomem\t__attribute__((noderef, address_space(__iomem)))\n# define __percpu\t__attribute__((noderef, address_space(__percpu)))\n# define __rcu\t\t__attribute__((noderef, address_space(__rcu)))\nstatic inline void __chk_user_ptr(const volatile void __user *ptr) { }\nstatic inline void __chk_io_ptr(const volatile void __iomem *ptr) { }\n/* context/locking */\n# define __must_hold(x)\t__attribute__((context(x,1,1)))\n# define __acquires(x)\t__attribute__((context(x,0,1)))\n# define __releases(x)\t__attribute__((context(x,1,0)))\n# define __acquire(x)\t__context__(x,1)\n# define __release(x)\t__context__(x,-1)\n# define __cond_lock(x,c)\t((c) ? ({ __acquire(x); 1; }) : 0)\n/* other */\n# define __force\t__attribute__((force))\n# define __nocast\t__attribute__((nocast))\n# define __safe\t\t__attribute__((safe))\n# define __private\t__attribute__((noderef))\n# define ACCESS_PRIVATE(p, member) (*((typeof((p)->member) __force *) &(p)->member))\n#else /* __CHECKER__ */\n/* address spaces */\n# define __kernel\n# ifdef STRUCTLEAK_PLUGIN\n#  define __user\t__attribute__((user))\n# else\n#  define __user\n# endif\n# define __iomem\n# define __percpu\n# define __rcu\n# define __chk_user_ptr(x)\t(void)0\n# define __chk_io_ptr(x)\t(void)0\n/* context/locking */\n# define __must_hold(x)\n# define __acquires(x)\n# define __releases(x)\n# define __acquire(x)\t(void)0\n# define __release(x)\t(void)0\n# define __cond_lock(x,c) (c)\n/* other */\n# define __force\n# define __nocast\n# define __safe\n# define __private\n# define ACCESS_PRIVATE(p, member) ((p)->member)\n# define __builtin_warning(x, y...) (1)\n#endif /* __CHECKER__ */\n\n/* Indirect macros required for expanded argument pasting, eg. __LINE__. */\n#define ___PASTE(a,b) a##b\n#define __PASTE(a,b) ___PASTE(a,b)\n\n#ifdef __KERNEL__\n\n/* Attributes */\n#include <linux/compiler_attributes.h>\n\n/* Builtins */\n\n/*\n * __has_builtin is supported on gcc >= 10, clang >= 3 and icc >= 21.\n * In the meantime, to support gcc < 10, we implement __has_builtin\n * by hand.\n */\n#ifndef __has_builtin\n#define __has_builtin(x) (0)\n#endif\n\n/* Compiler specific macros. */\n#ifdef __clang__\n#include <linux/compiler-clang.h>\n#elif defined(__INTEL_COMPILER)\n#include <linux/compiler-intel.h>\n#elif defined(__GNUC__)\n/* The above compilers also define __GNUC__, so order is important here. */\n#include <linux/compiler-gcc.h>\n#else\n#error \"Unknown compiler\"\n#endif\n\n/*\n * Some architectures need to provide custom definitions of macros provided\n * by linux/compiler-*.h, and can do so using asm/compiler.h. We include that\n * conditionally rather than using an asm-generic wrapper in order to avoid\n * build failures if any C compilation, which will include this file via an\n * -include argument in c_flags, occurs prior to the asm-generic wrappers being\n * generated.\n */\n#ifdef CONFIG_HAVE_ARCH_COMPILER_H\n#include <asm/compiler.h>\n#endif\n\nstruct ftrace_branch_data {\n\tconst char *func;\n\tconst char *file;\n\tunsigned line;\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long correct;\n\t\t\tunsigned long incorrect;\n\t\t};\n\t\tstruct {\n\t\t\tunsigned long miss;\n\t\t\tunsigned long hit;\n\t\t};\n\t\tunsigned long miss_hit[2];\n\t};\n};\n\nstruct ftrace_likely_data {\n\tstruct ftrace_branch_data\tdata;\n\tunsigned long\t\t\tconstant;\n};\n\n#if defined(CC_USING_HOTPATCH)\n#define notrace\t\t\t__attribute__((hotpatch(0, 0)))\n#elif defined(CC_USING_PATCHABLE_FUNCTION_ENTRY)\n#define notrace\t\t\t__attribute__((patchable_function_entry(0, 0)))\n#else\n#define notrace\t\t\t__attribute__((__no_instrument_function__))\n#endif\n\n/*\n * it doesn't make sense on ARM (currently the only user of __naked)\n * to trace naked functions because then mcount is called without\n * stack and frame pointer being set up and there is no chance to\n * restore the lr register to the value before mcount was called.\n */\n#define __naked\t\t\t__attribute__((__naked__)) notrace\n\n#define __compiler_offsetof(a, b)\t__builtin_offsetof(a, b)\n\n/*\n * Prefer gnu_inline, so that extern inline functions do not emit an\n * externally visible function. This makes extern inline behave as per gnu89\n * semantics rather than c99. This prevents multiple symbol definition errors\n * of extern inline functions at link time.\n * A lot of inline functions can cause havoc with function tracing.\n */\n#define inline inline __gnu_inline __inline_maybe_unused notrace\n\n/*\n * gcc provides both __inline__ and __inline as alternate spellings of\n * the inline keyword, though the latter is undocumented. New kernel\n * code should only use the inline spelling, but some existing code\n * uses __inline__. Since we #define inline above, to ensure\n * __inline__ has the same semantics, we need this #define.\n *\n * However, the spelling __inline is strictly reserved for referring\n * to the bare keyword.\n */\n#define __inline__ inline\n\n/*\n * GCC does not warn about unused static inline functions for -Wunused-function.\n * Suppress the warning in clang as well by using __maybe_unused, but enable it\n * for W=1 build. This will allow clang to find unused functions. Remove the\n * __inline_maybe_unused entirely after fixing most of -Wunused-function warnings.\n */\n#ifdef KBUILD_EXTRA_WARN1\n#define __inline_maybe_unused\n#else\n#define __inline_maybe_unused __maybe_unused\n#endif\n\n/*\n * Rather then using noinline to prevent stack consumption, use\n * noinline_for_stack instead.  For documentation reasons.\n */\n#define noinline_for_stack noinline\n\n/*\n * Sanitizer helper attributes: Because using __always_inline and\n * __no_sanitize_* conflict, provide helper attributes that will either expand\n * to __no_sanitize_* in compilation units where instrumentation is enabled\n * (__SANITIZE_*__), or __always_inline in compilation units without\n * instrumentation (__SANITIZE_*__ undefined).\n */\n#ifdef __SANITIZE_ADDRESS__\n/*\n * We can't declare function 'inline' because __no_sanitize_address conflicts\n * with inlining. Attempt to inline it may cause a build failure.\n *     https://gcc.gnu.org/bugzilla/show_bug.cgi?id=67368\n * '__maybe_unused' allows us to avoid defined-but-not-used warnings.\n */\n# define __no_kasan_or_inline __no_sanitize_address notrace __maybe_unused\n# define __no_sanitize_or_inline __no_kasan_or_inline\n#else\n# define __no_kasan_or_inline __always_inline\n#endif\n\n#define __no_kcsan __no_sanitize_thread\n#ifdef __SANITIZE_THREAD__\n# define __no_sanitize_or_inline __no_kcsan notrace __maybe_unused\n#endif\n\n#ifndef __no_sanitize_or_inline\n#define __no_sanitize_or_inline __always_inline\n#endif\n\n/* Section for code which can't be instrumented at all */\n#define noinstr\t\t\t\t\t\t\t\t\\\n\tnoinline notrace __attribute((__section__(\".noinstr.text\")))\t\\\n\t__no_kcsan __no_sanitize_address\n\n#endif /* __KERNEL__ */\n\n#endif /* __ASSEMBLY__ */\n\n/*\n * The below symbols may be defined for one or more, but not ALL, of the above\n * compilers. We don't consider that to be an error, so set them to nothing.\n * For example, some of them are for compiler specific plugins.\n */\n#ifndef __latent_entropy\n# define __latent_entropy\n#endif\n\n#ifndef __randomize_layout\n# define __randomize_layout __designated_init\n#endif\n\n#ifndef __no_randomize_layout\n# define __no_randomize_layout\n#endif\n\n#ifndef randomized_struct_fields_start\n# define randomized_struct_fields_start\n# define randomized_struct_fields_end\n#endif\n\n#ifndef __noscs\n# define __noscs\n#endif\n\n#ifndef __nocfi\n# define __nocfi\n#endif\n\n#ifndef __cficanonical\n# define __cficanonical\n#endif\n\n#ifndef asm_volatile_goto\n#define asm_volatile_goto(x...) asm goto(x)\n#endif\n\n#ifdef CONFIG_CC_HAS_ASM_INLINE\n#define asm_inline asm __inline\n#else\n#define asm_inline asm\n#endif\n\n/* Are two types/vars the same type (ignoring qualifiers)? */\n#define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b))\n\n/*\n * __unqual_scalar_typeof(x) - Declare an unqualified scalar type, leaving\n *\t\t\t       non-scalar types unchanged.\n */\n/*\n * Prefer C11 _Generic for better compile-times and simpler code. Note: 'char'\n * is not type-compatible with 'signed char', and we define a separate case.\n */\n#define __scalar_type_to_expr_cases(type)\t\t\t\t\\\n\t\tunsigned type:\t(unsigned type)0,\t\t\t\\\n\t\tsigned type:\t(signed type)0\n\n#define __unqual_scalar_typeof(x) typeof(\t\t\t\t\\\n\t\t_Generic((x),\t\t\t\t\t\t\\\n\t\t\t char:\t(char)0,\t\t\t\t\\\n\t\t\t __scalar_type_to_expr_cases(char),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(short),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(int),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(long),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(long long),\t\\\n\t\t\t default: (x)))\n\n/* Is this type a native word size -- useful for atomic operations */\n#define __native_word(t) \\\n\t(sizeof(t) == sizeof(char) || sizeof(t) == sizeof(short) || \\\n\t sizeof(t) == sizeof(int) || sizeof(t) == sizeof(long))\n\n/* Compile time object size, -1 for unknown */\n#ifndef __compiletime_object_size\n# define __compiletime_object_size(obj) -1\n#endif\n#ifndef __compiletime_warning\n# define __compiletime_warning(message)\n#endif\n#ifndef __compiletime_error\n# define __compiletime_error(message)\n#endif\n\n#ifdef __OPTIMIZE__\n# define __compiletime_assert(condition, msg, prefix, suffix)\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\textern void prefix ## suffix(void) __compiletime_error(msg); \\\n\t\tif (!(condition))\t\t\t\t\t\\\n\t\t\tprefix ## suffix();\t\t\t\t\\\n\t} while (0)\n#else\n# define __compiletime_assert(condition, msg, prefix, suffix) do { } while (0)\n#endif\n\n#define _compiletime_assert(condition, msg, prefix, suffix) \\\n\t__compiletime_assert(condition, msg, prefix, suffix)\n\n/**\n * compiletime_assert - break build and emit msg if condition is false\n * @condition: a compile-time constant condition to check\n * @msg:       a message to emit if condition is false\n *\n * In tradition of POSIX assert, this macro will break the build if the\n * supplied condition is *false*, emitting the supplied error message if the\n * compiler has support to do so.\n */\n#define compiletime_assert(condition, msg) \\\n\t_compiletime_assert(condition, msg, __compiletime_assert_, __COUNTER__)\n\n#define compiletime_assert_atomic_type(t)\t\t\t\t\\\n\tcompiletime_assert(__native_word(t),\t\t\t\t\\\n\t\t\"Need native word sized stores/loads for atomicity.\")\n\n/* Helpers for emitting diagnostics in pragmas. */\n#ifndef __diag\n#define __diag(string)\n#endif\n\n#ifndef __diag_GCC\n#define __diag_GCC(version, severity, string)\n#endif\n\n#define __diag_push()\t__diag(push)\n#define __diag_pop()\t__diag(pop)\n\n#define __diag_ignore(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, ignore, option)\n#define __diag_warn(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, warn, option)\n#define __diag_error(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, error, option)\n\n#endif /* __LINUX_COMPILER_TYPES_H */\n"}, "5": {"id": 5, "path": "/src/include/linux/skbuff.h", "content": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/*\n *\tDefinitions for the 'struct sk_buff' memory handlers.\n *\n *\tAuthors:\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\t\tFlorian La Roche, <rzsfl@rz.uni-sb.de>\n */\n\n#ifndef _LINUX_SKBUFF_H\n#define _LINUX_SKBUFF_H\n\n#include <linux/kernel.h>\n#include <linux/compiler.h>\n#include <linux/time.h>\n#include <linux/bug.h>\n#include <linux/bvec.h>\n#include <linux/cache.h>\n#include <linux/rbtree.h>\n#include <linux/socket.h>\n#include <linux/refcount.h>\n\n#include <linux/atomic.h>\n#include <asm/types.h>\n#include <linux/spinlock.h>\n#include <linux/net.h>\n#include <linux/textsearch.h>\n#include <net/checksum.h>\n#include <linux/rcupdate.h>\n#include <linux/hrtimer.h>\n#include <linux/dma-mapping.h>\n#include <linux/netdev_features.h>\n#include <linux/sched.h>\n#include <linux/sched/clock.h>\n#include <net/flow_dissector.h>\n#include <linux/splice.h>\n#include <linux/in6.h>\n#include <linux/if_packet.h>\n#include <net/flow.h>\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n#include <linux/netfilter/nf_conntrack_common.h>\n#endif\n\n/* The interface for checksum offload between the stack and networking drivers\n * is as follows...\n *\n * A. IP checksum related features\n *\n * Drivers advertise checksum offload capabilities in the features of a device.\n * From the stack's point of view these are capabilities offered by the driver.\n * A driver typically only advertises features that it is capable of offloading\n * to its device.\n *\n * The checksum related features are:\n *\n *\tNETIF_F_HW_CSUM\t- The driver (or its device) is able to compute one\n *\t\t\t  IP (one's complement) checksum for any combination\n *\t\t\t  of protocols or protocol layering. The checksum is\n *\t\t\t  computed and set in a packet per the CHECKSUM_PARTIAL\n *\t\t\t  interface (see below).\n *\n *\tNETIF_F_IP_CSUM - Driver (device) is only able to checksum plain\n *\t\t\t  TCP or UDP packets over IPv4. These are specifically\n *\t\t\t  unencapsulated packets of the form IPv4|TCP or\n *\t\t\t  IPv4|UDP where the Protocol field in the IPv4 header\n *\t\t\t  is TCP or UDP. The IPv4 header may contain IP options.\n *\t\t\t  This feature cannot be set in features for a device\n *\t\t\t  with NETIF_F_HW_CSUM also set. This feature is being\n *\t\t\t  DEPRECATED (see below).\n *\n *\tNETIF_F_IPV6_CSUM - Driver (device) is only able to checksum plain\n *\t\t\t  TCP or UDP packets over IPv6. These are specifically\n *\t\t\t  unencapsulated packets of the form IPv6|TCP or\n *\t\t\t  IPv6|UDP where the Next Header field in the IPv6\n *\t\t\t  header is either TCP or UDP. IPv6 extension headers\n *\t\t\t  are not supported with this feature. This feature\n *\t\t\t  cannot be set in features for a device with\n *\t\t\t  NETIF_F_HW_CSUM also set. This feature is being\n *\t\t\t  DEPRECATED (see below).\n *\n *\tNETIF_F_RXCSUM - Driver (device) performs receive checksum offload.\n *\t\t\t This flag is only used to disable the RX checksum\n *\t\t\t feature for a device. The stack will accept receive\n *\t\t\t checksum indication in packets received on a device\n *\t\t\t regardless of whether NETIF_F_RXCSUM is set.\n *\n * B. Checksumming of received packets by device. Indication of checksum\n *    verification is set in skb->ip_summed. Possible values are:\n *\n * CHECKSUM_NONE:\n *\n *   Device did not checksum this packet e.g. due to lack of capabilities.\n *   The packet contains full (though not verified) checksum in packet but\n *   not in skb->csum. Thus, skb->csum is undefined in this case.\n *\n * CHECKSUM_UNNECESSARY:\n *\n *   The hardware you're dealing with doesn't calculate the full checksum\n *   (as in CHECKSUM_COMPLETE), but it does parse headers and verify checksums\n *   for specific protocols. For such packets it will set CHECKSUM_UNNECESSARY\n *   if their checksums are okay. skb->csum is still undefined in this case\n *   though. A driver or device must never modify the checksum field in the\n *   packet even if checksum is verified.\n *\n *   CHECKSUM_UNNECESSARY is applicable to following protocols:\n *     TCP: IPv6 and IPv4.\n *     UDP: IPv4 and IPv6. A device may apply CHECKSUM_UNNECESSARY to a\n *       zero UDP checksum for either IPv4 or IPv6, the networking stack\n *       may perform further validation in this case.\n *     GRE: only if the checksum is present in the header.\n *     SCTP: indicates the CRC in SCTP header has been validated.\n *     FCOE: indicates the CRC in FC frame has been validated.\n *\n *   skb->csum_level indicates the number of consecutive checksums found in\n *   the packet minus one that have been verified as CHECKSUM_UNNECESSARY.\n *   For instance if a device receives an IPv6->UDP->GRE->IPv4->TCP packet\n *   and a device is able to verify the checksums for UDP (possibly zero),\n *   GRE (checksum flag is set) and TCP, skb->csum_level would be set to\n *   two. If the device were only able to verify the UDP checksum and not\n *   GRE, either because it doesn't support GRE checksum or because GRE\n *   checksum is bad, skb->csum_level would be set to zero (TCP checksum is\n *   not considered in this case).\n *\n * CHECKSUM_COMPLETE:\n *\n *   This is the most generic way. The device supplied checksum of the _whole_\n *   packet as seen by netif_rx() and fills in skb->csum. This means the\n *   hardware doesn't need to parse L3/L4 headers to implement this.\n *\n *   Notes:\n *   - Even if device supports only some protocols, but is able to produce\n *     skb->csum, it MUST use CHECKSUM_COMPLETE, not CHECKSUM_UNNECESSARY.\n *   - CHECKSUM_COMPLETE is not applicable to SCTP and FCoE protocols.\n *\n * CHECKSUM_PARTIAL:\n *\n *   A checksum is set up to be offloaded to a device as described in the\n *   output description for CHECKSUM_PARTIAL. This may occur on a packet\n *   received directly from another Linux OS, e.g., a virtualized Linux kernel\n *   on the same host, or it may be set in the input path in GRO or remote\n *   checksum offload. For the purposes of checksum verification, the checksum\n *   referred to by skb->csum_start + skb->csum_offset and any preceding\n *   checksums in the packet are considered verified. Any checksums in the\n *   packet that are after the checksum being offloaded are not considered to\n *   be verified.\n *\n * C. Checksumming on transmit for non-GSO. The stack requests checksum offload\n *    in the skb->ip_summed for a packet. Values are:\n *\n * CHECKSUM_PARTIAL:\n *\n *   The driver is required to checksum the packet as seen by hard_start_xmit()\n *   from skb->csum_start up to the end, and to record/write the checksum at\n *   offset skb->csum_start + skb->csum_offset. A driver may verify that the\n *   csum_start and csum_offset values are valid values given the length and\n *   offset of the packet, but it should not attempt to validate that the\n *   checksum refers to a legitimate transport layer checksum -- it is the\n *   purview of the stack to validate that csum_start and csum_offset are set\n *   correctly.\n *\n *   When the stack requests checksum offload for a packet, the driver MUST\n *   ensure that the checksum is set correctly. A driver can either offload the\n *   checksum calculation to the device, or call skb_checksum_help (in the case\n *   that the device does not support offload for a particular checksum).\n *\n *   NETIF_F_IP_CSUM and NETIF_F_IPV6_CSUM are being deprecated in favor of\n *   NETIF_F_HW_CSUM. New devices should use NETIF_F_HW_CSUM to indicate\n *   checksum offload capability.\n *   skb_csum_hwoffload_help() can be called to resolve CHECKSUM_PARTIAL based\n *   on network device checksumming capabilities: if a packet does not match\n *   them, skb_checksum_help or skb_crc32c_help (depending on the value of\n *   csum_not_inet, see item D.) is called to resolve the checksum.\n *\n * CHECKSUM_NONE:\n *\n *   The skb was already checksummed by the protocol, or a checksum is not\n *   required.\n *\n * CHECKSUM_UNNECESSARY:\n *\n *   This has the same meaning as CHECKSUM_NONE for checksum offload on\n *   output.\n *\n * CHECKSUM_COMPLETE:\n *   Not used in checksum output. If a driver observes a packet with this value\n *   set in skbuff, it should treat the packet as if CHECKSUM_NONE were set.\n *\n * D. Non-IP checksum (CRC) offloads\n *\n *   NETIF_F_SCTP_CRC - This feature indicates that a device is capable of\n *     offloading the SCTP CRC in a packet. To perform this offload the stack\n *     will set csum_start and csum_offset accordingly, set ip_summed to\n *     CHECKSUM_PARTIAL and set csum_not_inet to 1, to provide an indication in\n *     the skbuff that the CHECKSUM_PARTIAL refers to CRC32c.\n *     A driver that supports both IP checksum offload and SCTP CRC32c offload\n *     must verify which offload is configured for a packet by testing the\n *     value of skb->csum_not_inet; skb_crc32c_csum_help is provided to resolve\n *     CHECKSUM_PARTIAL on skbs where csum_not_inet is set to 1.\n *\n *   NETIF_F_FCOE_CRC - This feature indicates that a device is capable of\n *     offloading the FCOE CRC in a packet. To perform this offload the stack\n *     will set ip_summed to CHECKSUM_PARTIAL and set csum_start and csum_offset\n *     accordingly. Note that there is no indication in the skbuff that the\n *     CHECKSUM_PARTIAL refers to an FCOE checksum, so a driver that supports\n *     both IP checksum offload and FCOE CRC offload must verify which offload\n *     is configured for a packet, presumably by inspecting packet headers.\n *\n * E. Checksumming on output with GSO.\n *\n * In the case of a GSO packet (skb_is_gso(skb) is true), checksum offload\n * is implied by the SKB_GSO_* flags in gso_type. Most obviously, if the\n * gso_type is SKB_GSO_TCPV4 or SKB_GSO_TCPV6, TCP checksum offload as\n * part of the GSO operation is implied. If a checksum is being offloaded\n * with GSO then ip_summed is CHECKSUM_PARTIAL, and both csum_start and\n * csum_offset are set to refer to the outermost checksum being offloaded\n * (two offloaded checksums are possible with UDP encapsulation).\n */\n\n/* Don't change this without changing skb_csum_unnecessary! */\n#define CHECKSUM_NONE\t\t0\n#define CHECKSUM_UNNECESSARY\t1\n#define CHECKSUM_COMPLETE\t2\n#define CHECKSUM_PARTIAL\t3\n\n/* Maximum value in skb->csum_level */\n#define SKB_MAX_CSUM_LEVEL\t3\n\n#define SKB_DATA_ALIGN(X)\tALIGN(X, SMP_CACHE_BYTES)\n#define SKB_WITH_OVERHEAD(X)\t\\\n\t((X) - SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))\n#define SKB_MAX_ORDER(X, ORDER) \\\n\tSKB_WITH_OVERHEAD((PAGE_SIZE << (ORDER)) - (X))\n#define SKB_MAX_HEAD(X)\t\t(SKB_MAX_ORDER((X), 0))\n#define SKB_MAX_ALLOC\t\t(SKB_MAX_ORDER(0, 2))\n\n/* return minimum truesize of one skb containing X bytes of data */\n#define SKB_TRUESIZE(X) ((X) +\t\t\t\t\t\t\\\n\t\t\t SKB_DATA_ALIGN(sizeof(struct sk_buff)) +\t\\\n\t\t\t SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))\n\nstruct ahash_request;\nstruct net_device;\nstruct scatterlist;\nstruct pipe_inode_info;\nstruct iov_iter;\nstruct napi_struct;\nstruct bpf_prog;\nunion bpf_attr;\nstruct skb_ext;\n\n#if IS_ENABLED(CONFIG_BRIDGE_NETFILTER)\nstruct nf_bridge_info {\n\tenum {\n\t\tBRNF_PROTO_UNCHANGED,\n\t\tBRNF_PROTO_8021Q,\n\t\tBRNF_PROTO_PPPOE\n\t} orig_proto:8;\n\tu8\t\t\tpkt_otherhost:1;\n\tu8\t\t\tin_prerouting:1;\n\tu8\t\t\tbridged_dnat:1;\n\t__u16\t\t\tfrag_max_size;\n\tstruct net_device\t*physindev;\n\n\t/* always valid & non-NULL from FORWARD on, for physdev match */\n\tstruct net_device\t*physoutdev;\n\tunion {\n\t\t/* prerouting: detect dnat in orig/reply direction */\n\t\t__be32          ipv4_daddr;\n\t\tstruct in6_addr ipv6_daddr;\n\n\t\t/* after prerouting + nat detected: store original source\n\t\t * mac since neigh resolution overwrites it, only used while\n\t\t * skb is out in neigh layer.\n\t\t */\n\t\tchar neigh_header[8];\n\t};\n};\n#endif\n\n#if IS_ENABLED(CONFIG_NET_TC_SKB_EXT)\n/* Chain in tc_skb_ext will be used to share the tc chain with\n * ovs recirc_id. It will be set to the current chain by tc\n * and read by ovs to recirc_id.\n */\nstruct tc_skb_ext {\n\t__u32 chain;\n\t__u16 mru;\n\tbool post_ct;\n};\n#endif\n\nstruct sk_buff_head {\n\t/* These two members must be first. */\n\tstruct sk_buff\t*next;\n\tstruct sk_buff\t*prev;\n\n\t__u32\t\tqlen;\n\tspinlock_t\tlock;\n};\n\nstruct sk_buff;\n\n/* To allow 64K frame to be packed as single skb without frag_list we\n * require 64K/PAGE_SIZE pages plus 1 additional page to allow for\n * buffers which do not start on a page boundary.\n *\n * Since GRO uses frags we allocate at least 16 regardless of page\n * size.\n */\n#if (65536/PAGE_SIZE + 1) < 16\n#define MAX_SKB_FRAGS 16UL\n#else\n#define MAX_SKB_FRAGS (65536/PAGE_SIZE + 1)\n#endif\nextern int sysctl_max_skb_frags;\n\n/* Set skb_shinfo(skb)->gso_size to this in case you want skb_segment to\n * segment using its current segmentation instead.\n */\n#define GSO_BY_FRAGS\t0xFFFF\n\ntypedef struct bio_vec skb_frag_t;\n\n/**\n * skb_frag_size() - Returns the size of a skb fragment\n * @frag: skb fragment\n */\nstatic inline unsigned int skb_frag_size(const skb_frag_t *frag)\n{\n\treturn frag->bv_len;\n}\n\n/**\n * skb_frag_size_set() - Sets the size of a skb fragment\n * @frag: skb fragment\n * @size: size of fragment\n */\nstatic inline void skb_frag_size_set(skb_frag_t *frag, unsigned int size)\n{\n\tfrag->bv_len = size;\n}\n\n/**\n * skb_frag_size_add() - Increments the size of a skb fragment by @delta\n * @frag: skb fragment\n * @delta: value to add\n */\nstatic inline void skb_frag_size_add(skb_frag_t *frag, int delta)\n{\n\tfrag->bv_len += delta;\n}\n\n/**\n * skb_frag_size_sub() - Decrements the size of a skb fragment by @delta\n * @frag: skb fragment\n * @delta: value to subtract\n */\nstatic inline void skb_frag_size_sub(skb_frag_t *frag, int delta)\n{\n\tfrag->bv_len -= delta;\n}\n\n/**\n * skb_frag_must_loop - Test if %p is a high memory page\n * @p: fragment's page\n */\nstatic inline bool skb_frag_must_loop(struct page *p)\n{\n#if defined(CONFIG_HIGHMEM)\n\tif (IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL_FORCE_MAP) || PageHighMem(p))\n\t\treturn true;\n#endif\n\treturn false;\n}\n\n/**\n *\tskb_frag_foreach_page - loop over pages in a fragment\n *\n *\t@f:\t\tskb frag to operate on\n *\t@f_off:\t\toffset from start of f->bv_page\n *\t@f_len:\t\tlength from f_off to loop over\n *\t@p:\t\t(temp var) current page\n *\t@p_off:\t\t(temp var) offset from start of current page,\n *\t                           non-zero only on first page.\n *\t@p_len:\t\t(temp var) length in current page,\n *\t\t\t\t   < PAGE_SIZE only on first and last page.\n *\t@copied:\t(temp var) length so far, excluding current p_len.\n *\n *\tA fragment can hold a compound page, in which case per-page\n *\toperations, notably kmap_atomic, must be called for each\n *\tregular page.\n */\n#define skb_frag_foreach_page(f, f_off, f_len, p, p_off, p_len, copied)\t\\\n\tfor (p = skb_frag_page(f) + ((f_off) >> PAGE_SHIFT),\t\t\\\n\t     p_off = (f_off) & (PAGE_SIZE - 1),\t\t\t\t\\\n\t     p_len = skb_frag_must_loop(p) ?\t\t\t\t\\\n\t     min_t(u32, f_len, PAGE_SIZE - p_off) : f_len,\t\t\\\n\t     copied = 0;\t\t\t\t\t\t\\\n\t     copied < f_len;\t\t\t\t\t\t\\\n\t     copied += p_len, p++, p_off = 0,\t\t\t\t\\\n\t     p_len = min_t(u32, f_len - copied, PAGE_SIZE))\t\t\\\n\n#define HAVE_HW_TIME_STAMP\n\n/**\n * struct skb_shared_hwtstamps - hardware time stamps\n * @hwtstamp:\thardware time stamp transformed into duration\n *\t\tsince arbitrary point in time\n *\n * Software time stamps generated by ktime_get_real() are stored in\n * skb->tstamp.\n *\n * hwtstamps can only be compared against other hwtstamps from\n * the same device.\n *\n * This structure is attached to packets as part of the\n * &skb_shared_info. Use skb_hwtstamps() to get a pointer.\n */\nstruct skb_shared_hwtstamps {\n\tktime_t\thwtstamp;\n};\n\n/* Definitions for tx_flags in struct skb_shared_info */\nenum {\n\t/* generate hardware time stamp */\n\tSKBTX_HW_TSTAMP = 1 << 0,\n\n\t/* generate software time stamp when queueing packet to NIC */\n\tSKBTX_SW_TSTAMP = 1 << 1,\n\n\t/* device driver is going to provide hardware time stamp */\n\tSKBTX_IN_PROGRESS = 1 << 2,\n\n\t/* generate wifi status information (where possible) */\n\tSKBTX_WIFI_STATUS = 1 << 4,\n\n\t/* generate software time stamp when entering packet scheduling */\n\tSKBTX_SCHED_TSTAMP = 1 << 6,\n};\n\n#define SKBTX_ANY_SW_TSTAMP\t(SKBTX_SW_TSTAMP    | \\\n\t\t\t\t SKBTX_SCHED_TSTAMP)\n#define SKBTX_ANY_TSTAMP\t(SKBTX_HW_TSTAMP | SKBTX_ANY_SW_TSTAMP)\n\n/* Definitions for flags in struct skb_shared_info */\nenum {\n\t/* use zcopy routines */\n\tSKBFL_ZEROCOPY_ENABLE = BIT(0),\n\n\t/* This indicates at least one fragment might be overwritten\n\t * (as in vmsplice(), sendfile() ...)\n\t * If we need to compute a TX checksum, we'll need to copy\n\t * all frags to avoid possible bad checksum\n\t */\n\tSKBFL_SHARED_FRAG = BIT(1),\n};\n\n#define SKBFL_ZEROCOPY_FRAG\t(SKBFL_ZEROCOPY_ENABLE | SKBFL_SHARED_FRAG)\n\n/*\n * The callback notifies userspace to release buffers when skb DMA is done in\n * lower device, the skb last reference should be 0 when calling this.\n * The zerocopy_success argument is true if zero copy transmit occurred,\n * false on data copy or out of memory error caused by data copy attempt.\n * The ctx field is used to track device context.\n * The desc field is used to track userspace buffer index.\n */\nstruct ubuf_info {\n\tvoid (*callback)(struct sk_buff *, struct ubuf_info *,\n\t\t\t bool zerocopy_success);\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long desc;\n\t\t\tvoid *ctx;\n\t\t};\n\t\tstruct {\n\t\t\tu32 id;\n\t\t\tu16 len;\n\t\t\tu16 zerocopy:1;\n\t\t\tu32 bytelen;\n\t\t};\n\t};\n\trefcount_t refcnt;\n\tu8 flags;\n\n\tstruct mmpin {\n\t\tstruct user_struct *user;\n\t\tunsigned int num_pg;\n\t} mmp;\n};\n\n#define skb_uarg(SKB)\t((struct ubuf_info *)(skb_shinfo(SKB)->destructor_arg))\n\nint mm_account_pinned_pages(struct mmpin *mmp, size_t size);\nvoid mm_unaccount_pinned_pages(struct mmpin *mmp);\n\nstruct ubuf_info *msg_zerocopy_alloc(struct sock *sk, size_t size);\nstruct ubuf_info *msg_zerocopy_realloc(struct sock *sk, size_t size,\n\t\t\t\t       struct ubuf_info *uarg);\n\nvoid msg_zerocopy_put_abort(struct ubuf_info *uarg, bool have_uref);\n\nvoid msg_zerocopy_callback(struct sk_buff *skb, struct ubuf_info *uarg,\n\t\t\t   bool success);\n\nint skb_zerocopy_iter_dgram(struct sk_buff *skb, struct msghdr *msg, int len);\nint skb_zerocopy_iter_stream(struct sock *sk, struct sk_buff *skb,\n\t\t\t     struct msghdr *msg, int len,\n\t\t\t     struct ubuf_info *uarg);\n\n/* This data is invariant across clones and lives at\n * the end of the header data, ie. at skb->end.\n */\nstruct skb_shared_info {\n\t__u8\t\tflags;\n\t__u8\t\tmeta_len;\n\t__u8\t\tnr_frags;\n\t__u8\t\ttx_flags;\n\tunsigned short\tgso_size;\n\t/* Warning: this field is not always filled in (UFO)! */\n\tunsigned short\tgso_segs;\n\tstruct sk_buff\t*frag_list;\n\tstruct skb_shared_hwtstamps hwtstamps;\n\tunsigned int\tgso_type;\n\tu32\t\ttskey;\n\n\t/*\n\t * Warning : all fields before dataref are cleared in __alloc_skb()\n\t */\n\tatomic_t\tdataref;\n\n\t/* Intermediate layers must ensure that destructor_arg\n\t * remains valid until skb destructor */\n\tvoid *\t\tdestructor_arg;\n\n\t/* must be last field, see pskb_expand_head() */\n\tskb_frag_t\tfrags[MAX_SKB_FRAGS];\n};\n\n/* We divide dataref into two halves.  The higher 16 bits hold references\n * to the payload part of skb->data.  The lower 16 bits hold references to\n * the entire skb->data.  A clone of a headerless skb holds the length of\n * the header in skb->hdr_len.\n *\n * All users must obey the rule that the skb->data reference count must be\n * greater than or equal to the payload reference count.\n *\n * Holding a reference to the payload part means that the user does not\n * care about modifications to the header part of skb->data.\n */\n#define SKB_DATAREF_SHIFT 16\n#define SKB_DATAREF_MASK ((1 << SKB_DATAREF_SHIFT) - 1)\n\n\nenum {\n\tSKB_FCLONE_UNAVAILABLE,\t/* skb has no fclone (from head_cache) */\n\tSKB_FCLONE_ORIG,\t/* orig skb (from fclone_cache) */\n\tSKB_FCLONE_CLONE,\t/* companion fclone skb (from fclone_cache) */\n};\n\nenum {\n\tSKB_GSO_TCPV4 = 1 << 0,\n\n\t/* This indicates the skb is from an untrusted source. */\n\tSKB_GSO_DODGY = 1 << 1,\n\n\t/* This indicates the tcp segment has CWR set. */\n\tSKB_GSO_TCP_ECN = 1 << 2,\n\n\tSKB_GSO_TCP_FIXEDID = 1 << 3,\n\n\tSKB_GSO_TCPV6 = 1 << 4,\n\n\tSKB_GSO_FCOE = 1 << 5,\n\n\tSKB_GSO_GRE = 1 << 6,\n\n\tSKB_GSO_GRE_CSUM = 1 << 7,\n\n\tSKB_GSO_IPXIP4 = 1 << 8,\n\n\tSKB_GSO_IPXIP6 = 1 << 9,\n\n\tSKB_GSO_UDP_TUNNEL = 1 << 10,\n\n\tSKB_GSO_UDP_TUNNEL_CSUM = 1 << 11,\n\n\tSKB_GSO_PARTIAL = 1 << 12,\n\n\tSKB_GSO_TUNNEL_REMCSUM = 1 << 13,\n\n\tSKB_GSO_SCTP = 1 << 14,\n\n\tSKB_GSO_ESP = 1 << 15,\n\n\tSKB_GSO_UDP = 1 << 16,\n\n\tSKB_GSO_UDP_L4 = 1 << 17,\n\n\tSKB_GSO_FRAGLIST = 1 << 18,\n};\n\n#if BITS_PER_LONG > 32\n#define NET_SKBUFF_DATA_USES_OFFSET 1\n#endif\n\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\ntypedef unsigned int sk_buff_data_t;\n#else\ntypedef unsigned char *sk_buff_data_t;\n#endif\n\n/**\n *\tstruct sk_buff - socket buffer\n *\t@next: Next buffer in list\n *\t@prev: Previous buffer in list\n *\t@tstamp: Time we arrived/left\n *\t@skb_mstamp_ns: (aka @tstamp) earliest departure time; start point\n *\t\tfor retransmit timer\n *\t@rbnode: RB tree node, alternative to next/prev for netem/tcp\n *\t@list: queue head\n *\t@sk: Socket we are owned by\n *\t@ip_defrag_offset: (aka @sk) alternate use of @sk, used in\n *\t\tfragmentation management\n *\t@dev: Device we arrived on/are leaving by\n *\t@dev_scratch: (aka @dev) alternate use of @dev when @dev would be %NULL\n *\t@cb: Control buffer. Free for use by every layer. Put private vars here\n *\t@_skb_refdst: destination entry (with norefcount bit)\n *\t@sp: the security path, used for xfrm\n *\t@len: Length of actual data\n *\t@data_len: Data length\n *\t@mac_len: Length of link layer header\n *\t@hdr_len: writable header length of cloned skb\n *\t@csum: Checksum (must include start/offset pair)\n *\t@csum_start: Offset from skb->head where checksumming should start\n *\t@csum_offset: Offset from csum_start where checksum should be stored\n *\t@priority: Packet queueing priority\n *\t@ignore_df: allow local fragmentation\n *\t@cloned: Head may be cloned (check refcnt to be sure)\n *\t@ip_summed: Driver fed us an IP checksum\n *\t@nohdr: Payload reference only, must not modify header\n *\t@pkt_type: Packet class\n *\t@fclone: skbuff clone status\n *\t@ipvs_property: skbuff is owned by ipvs\n *\t@inner_protocol_type: whether the inner protocol is\n *\t\tENCAP_TYPE_ETHER or ENCAP_TYPE_IPPROTO\n *\t@remcsum_offload: remote checksum offload is enabled\n *\t@offload_fwd_mark: Packet was L2-forwarded in hardware\n *\t@offload_l3_fwd_mark: Packet was L3-forwarded in hardware\n *\t@tc_skip_classify: do not classify packet. set by IFB device\n *\t@tc_at_ingress: used within tc_classify to distinguish in/egress\n *\t@redirected: packet was redirected by packet classifier\n *\t@from_ingress: packet was redirected from the ingress path\n *\t@peeked: this packet has been seen already, so stats have been\n *\t\tdone for it, don't do them again\n *\t@nf_trace: netfilter packet trace flag\n *\t@protocol: Packet protocol from driver\n *\t@destructor: Destruct function\n *\t@tcp_tsorted_anchor: list structure for TCP (tp->tsorted_sent_queue)\n *\t@_sk_redir: socket redirection information for skmsg\n *\t@_nfct: Associated connection, if any (with nfctinfo bits)\n *\t@nf_bridge: Saved data about a bridged frame - see br_netfilter.c\n *\t@skb_iif: ifindex of device we arrived on\n *\t@tc_index: Traffic control index\n *\t@hash: the packet hash\n *\t@queue_mapping: Queue mapping for multiqueue devices\n *\t@head_frag: skb was allocated from page fragments,\n *\t\tnot allocated by kmalloc() or vmalloc().\n *\t@pfmemalloc: skbuff was allocated from PFMEMALLOC reserves\n *\t@active_extensions: active extensions (skb_ext_id types)\n *\t@ndisc_nodetype: router type (from link layer)\n *\t@ooo_okay: allow the mapping of a socket to a queue to be changed\n *\t@l4_hash: indicate hash is a canonical 4-tuple hash over transport\n *\t\tports.\n *\t@sw_hash: indicates hash was computed in software stack\n *\t@wifi_acked_valid: wifi_acked was set\n *\t@wifi_acked: whether frame was acked on wifi or not\n *\t@no_fcs:  Request NIC to treat last 4 bytes as Ethernet FCS\n *\t@encapsulation: indicates the inner headers in the skbuff are valid\n *\t@encap_hdr_csum: software checksum is needed\n *\t@csum_valid: checksum is already valid\n *\t@csum_not_inet: use CRC32c to resolve CHECKSUM_PARTIAL\n *\t@csum_complete_sw: checksum was completed by software\n *\t@csum_level: indicates the number of consecutive checksums found in\n *\t\tthe packet minus one that have been verified as\n *\t\tCHECKSUM_UNNECESSARY (max 3)\n *\t@dst_pending_confirm: need to confirm neighbour\n *\t@decrypted: Decrypted SKB\n *\t@napi_id: id of the NAPI struct this skb came from\n *\t@sender_cpu: (aka @napi_id) source CPU in XPS\n *\t@secmark: security marking\n *\t@mark: Generic packet mark\n *\t@reserved_tailroom: (aka @mark) number of bytes of free space available\n *\t\tat the tail of an sk_buff\n *\t@vlan_present: VLAN tag is present\n *\t@vlan_proto: vlan encapsulation protocol\n *\t@vlan_tci: vlan tag control information\n *\t@inner_protocol: Protocol (encapsulation)\n *\t@inner_ipproto: (aka @inner_protocol) stores ipproto when\n *\t\tskb->inner_protocol_type == ENCAP_TYPE_IPPROTO;\n *\t@inner_transport_header: Inner transport layer header (encapsulation)\n *\t@inner_network_header: Network layer header (encapsulation)\n *\t@inner_mac_header: Link layer header (encapsulation)\n *\t@transport_header: Transport layer header\n *\t@network_header: Network layer header\n *\t@mac_header: Link layer header\n *\t@kcov_handle: KCOV remote handle for remote coverage collection\n *\t@tail: Tail pointer\n *\t@end: End pointer\n *\t@head: Head of buffer\n *\t@data: Data head pointer\n *\t@truesize: Buffer size\n *\t@users: User count - see {datagram,tcp}.c\n *\t@extensions: allocated extensions, valid if active_extensions is nonzero\n */\n\nstruct sk_buff {\n\tunion {\n\t\tstruct {\n\t\t\t/* These two members must be first. */\n\t\t\tstruct sk_buff\t\t*next;\n\t\t\tstruct sk_buff\t\t*prev;\n\n\t\t\tunion {\n\t\t\t\tstruct net_device\t*dev;\n\t\t\t\t/* Some protocols might use this space to store information,\n\t\t\t\t * while device pointer would be NULL.\n\t\t\t\t * UDP receive path is one user.\n\t\t\t\t */\n\t\t\t\tunsigned long\t\tdev_scratch;\n\t\t\t};\n\t\t};\n\t\tstruct rb_node\t\trbnode; /* used in netem, ip4 defrag, and tcp stack */\n\t\tstruct list_head\tlist;\n\t};\n\n\tunion {\n\t\tstruct sock\t\t*sk;\n\t\tint\t\t\tip_defrag_offset;\n\t};\n\n\tunion {\n\t\tktime_t\t\ttstamp;\n\t\tu64\t\tskb_mstamp_ns; /* earliest departure time */\n\t};\n\t/*\n\t * This is the control buffer. It is free to use for every\n\t * layer. Please put your private variables there. If you\n\t * want to keep them across layers you have to do a skb_clone()\n\t * first. This is owned by whoever has the skb queued ATM.\n\t */\n\tchar\t\t\tcb[48] __aligned(8);\n\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long\t_skb_refdst;\n\t\t\tvoid\t\t(*destructor)(struct sk_buff *skb);\n\t\t};\n\t\tstruct list_head\ttcp_tsorted_anchor;\n#ifdef CONFIG_NET_SOCK_MSG\n\t\tunsigned long\t\t_sk_redir;\n#endif\n\t};\n\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tunsigned long\t\t _nfct;\n#endif\n\tunsigned int\t\tlen,\n\t\t\t\tdata_len;\n\t__u16\t\t\tmac_len,\n\t\t\t\thdr_len;\n\n\t/* Following fields are _not_ copied in __copy_skb_header()\n\t * Note that queue_mapping is here mostly to fill a hole.\n\t */\n\t__u16\t\t\tqueue_mapping;\n\n/* if you move cloned around you also must adapt those constants */\n#ifdef __BIG_ENDIAN_BITFIELD\n#define CLONED_MASK\t(1 << 7)\n#else\n#define CLONED_MASK\t1\n#endif\n#define CLONED_OFFSET()\t\toffsetof(struct sk_buff, __cloned_offset)\n\n\t/* private: */\n\t__u8\t\t\t__cloned_offset[0];\n\t/* public: */\n\t__u8\t\t\tcloned:1,\n\t\t\t\tnohdr:1,\n\t\t\t\tfclone:2,\n\t\t\t\tpeeked:1,\n\t\t\t\thead_frag:1,\n\t\t\t\tpfmemalloc:1;\n#ifdef CONFIG_SKB_EXTENSIONS\n\t__u8\t\t\tactive_extensions;\n#endif\n\t/* fields enclosed in headers_start/headers_end are copied\n\t * using a single memcpy() in __copy_skb_header()\n\t */\n\t/* private: */\n\t__u32\t\t\theaders_start[0];\n\t/* public: */\n\n/* if you move pkt_type around you also must adapt those constants */\n#ifdef __BIG_ENDIAN_BITFIELD\n#define PKT_TYPE_MAX\t(7 << 5)\n#else\n#define PKT_TYPE_MAX\t7\n#endif\n#define PKT_TYPE_OFFSET()\toffsetof(struct sk_buff, __pkt_type_offset)\n\n\t/* private: */\n\t__u8\t\t\t__pkt_type_offset[0];\n\t/* public: */\n\t__u8\t\t\tpkt_type:3;\n\t__u8\t\t\tignore_df:1;\n\t__u8\t\t\tnf_trace:1;\n\t__u8\t\t\tip_summed:2;\n\t__u8\t\t\tooo_okay:1;\n\n\t__u8\t\t\tl4_hash:1;\n\t__u8\t\t\tsw_hash:1;\n\t__u8\t\t\twifi_acked_valid:1;\n\t__u8\t\t\twifi_acked:1;\n\t__u8\t\t\tno_fcs:1;\n\t/* Indicates the inner headers are valid in the skbuff. */\n\t__u8\t\t\tencapsulation:1;\n\t__u8\t\t\tencap_hdr_csum:1;\n\t__u8\t\t\tcsum_valid:1;\n\n#ifdef __BIG_ENDIAN_BITFIELD\n#define PKT_VLAN_PRESENT_BIT\t7\n#else\n#define PKT_VLAN_PRESENT_BIT\t0\n#endif\n#define PKT_VLAN_PRESENT_OFFSET()\toffsetof(struct sk_buff, __pkt_vlan_present_offset)\n\t/* private: */\n\t__u8\t\t\t__pkt_vlan_present_offset[0];\n\t/* public: */\n\t__u8\t\t\tvlan_present:1;\n\t__u8\t\t\tcsum_complete_sw:1;\n\t__u8\t\t\tcsum_level:2;\n\t__u8\t\t\tcsum_not_inet:1;\n\t__u8\t\t\tdst_pending_confirm:1;\n#ifdef CONFIG_IPV6_NDISC_NODETYPE\n\t__u8\t\t\tndisc_nodetype:2;\n#endif\n\n\t__u8\t\t\tipvs_property:1;\n\t__u8\t\t\tinner_protocol_type:1;\n\t__u8\t\t\tremcsum_offload:1;\n#ifdef CONFIG_NET_SWITCHDEV\n\t__u8\t\t\toffload_fwd_mark:1;\n\t__u8\t\t\toffload_l3_fwd_mark:1;\n#endif\n#ifdef CONFIG_NET_CLS_ACT\n\t__u8\t\t\ttc_skip_classify:1;\n\t__u8\t\t\ttc_at_ingress:1;\n#endif\n#ifdef CONFIG_NET_REDIRECT\n\t__u8\t\t\tredirected:1;\n\t__u8\t\t\tfrom_ingress:1;\n#endif\n#ifdef CONFIG_TLS_DEVICE\n\t__u8\t\t\tdecrypted:1;\n#endif\n\n#ifdef CONFIG_NET_SCHED\n\t__u16\t\t\ttc_index;\t/* traffic control index */\n#endif\n\n\tunion {\n\t\t__wsum\t\tcsum;\n\t\tstruct {\n\t\t\t__u16\tcsum_start;\n\t\t\t__u16\tcsum_offset;\n\t\t};\n\t};\n\t__u32\t\t\tpriority;\n\tint\t\t\tskb_iif;\n\t__u32\t\t\thash;\n\t__be16\t\t\tvlan_proto;\n\t__u16\t\t\tvlan_tci;\n#if defined(CONFIG_NET_RX_BUSY_POLL) || defined(CONFIG_XPS)\n\tunion {\n\t\tunsigned int\tnapi_id;\n\t\tunsigned int\tsender_cpu;\n\t};\n#endif\n#ifdef CONFIG_NETWORK_SECMARK\n\t__u32\t\tsecmark;\n#endif\n\n\tunion {\n\t\t__u32\t\tmark;\n\t\t__u32\t\treserved_tailroom;\n\t};\n\n\tunion {\n\t\t__be16\t\tinner_protocol;\n\t\t__u8\t\tinner_ipproto;\n\t};\n\n\t__u16\t\t\tinner_transport_header;\n\t__u16\t\t\tinner_network_header;\n\t__u16\t\t\tinner_mac_header;\n\n\t__be16\t\t\tprotocol;\n\t__u16\t\t\ttransport_header;\n\t__u16\t\t\tnetwork_header;\n\t__u16\t\t\tmac_header;\n\n#ifdef CONFIG_KCOV\n\tu64\t\t\tkcov_handle;\n#endif\n\n\t/* private: */\n\t__u32\t\t\theaders_end[0];\n\t/* public: */\n\n\t/* These elements must be at the end, see alloc_skb() for details.  */\n\tsk_buff_data_t\t\ttail;\n\tsk_buff_data_t\t\tend;\n\tunsigned char\t\t*head,\n\t\t\t\t*data;\n\tunsigned int\t\ttruesize;\n\trefcount_t\t\tusers;\n\n#ifdef CONFIG_SKB_EXTENSIONS\n\t/* only useable after checking ->active_extensions != 0 */\n\tstruct skb_ext\t\t*extensions;\n#endif\n};\n\n#ifdef __KERNEL__\n/*\n *\tHandling routines are only of interest to the kernel\n */\n\n#define SKB_ALLOC_FCLONE\t0x01\n#define SKB_ALLOC_RX\t\t0x02\n#define SKB_ALLOC_NAPI\t\t0x04\n\n/**\n * skb_pfmemalloc - Test if the skb was allocated from PFMEMALLOC reserves\n * @skb: buffer\n */\nstatic inline bool skb_pfmemalloc(const struct sk_buff *skb)\n{\n\treturn unlikely(skb->pfmemalloc);\n}\n\n/*\n * skb might have a dst pointer attached, refcounted or not.\n * _skb_refdst low order bit is set if refcount was _not_ taken\n */\n#define SKB_DST_NOREF\t1UL\n#define SKB_DST_PTRMASK\t~(SKB_DST_NOREF)\n\n/**\n * skb_dst - returns skb dst_entry\n * @skb: buffer\n *\n * Returns skb dst_entry, regardless of reference taken or not.\n */\nstatic inline struct dst_entry *skb_dst(const struct sk_buff *skb)\n{\n\t/* If refdst was not refcounted, check we still are in a\n\t * rcu_read_lock section\n\t */\n\tWARN_ON((skb->_skb_refdst & SKB_DST_NOREF) &&\n\t\t!rcu_read_lock_held() &&\n\t\t!rcu_read_lock_bh_held());\n\treturn (struct dst_entry *)(skb->_skb_refdst & SKB_DST_PTRMASK);\n}\n\n/**\n * skb_dst_set - sets skb dst\n * @skb: buffer\n * @dst: dst entry\n *\n * Sets skb dst, assuming a reference was taken on dst and should\n * be released by skb_dst_drop()\n */\nstatic inline void skb_dst_set(struct sk_buff *skb, struct dst_entry *dst)\n{\n\tskb->_skb_refdst = (unsigned long)dst;\n}\n\n/**\n * skb_dst_set_noref - sets skb dst, hopefully, without taking reference\n * @skb: buffer\n * @dst: dst entry\n *\n * Sets skb dst, assuming a reference was not taken on dst.\n * If dst entry is cached, we do not take reference and dst_release\n * will be avoided by refdst_drop. If dst entry is not cached, we take\n * reference, so that last dst_release can destroy the dst immediately.\n */\nstatic inline void skb_dst_set_noref(struct sk_buff *skb, struct dst_entry *dst)\n{\n\tWARN_ON(!rcu_read_lock_held() && !rcu_read_lock_bh_held());\n\tskb->_skb_refdst = (unsigned long)dst | SKB_DST_NOREF;\n}\n\n/**\n * skb_dst_is_noref - Test if skb dst isn't refcounted\n * @skb: buffer\n */\nstatic inline bool skb_dst_is_noref(const struct sk_buff *skb)\n{\n\treturn (skb->_skb_refdst & SKB_DST_NOREF) && skb_dst(skb);\n}\n\n/**\n * skb_rtable - Returns the skb &rtable\n * @skb: buffer\n */\nstatic inline struct rtable *skb_rtable(const struct sk_buff *skb)\n{\n\treturn (struct rtable *)skb_dst(skb);\n}\n\n/* For mangling skb->pkt_type from user space side from applications\n * such as nft, tc, etc, we only allow a conservative subset of\n * possible pkt_types to be set.\n*/\nstatic inline bool skb_pkt_type_ok(u32 ptype)\n{\n\treturn ptype <= PACKET_OTHERHOST;\n}\n\n/**\n * skb_napi_id - Returns the skb's NAPI id\n * @skb: buffer\n */\nstatic inline unsigned int skb_napi_id(const struct sk_buff *skb)\n{\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\treturn skb->napi_id;\n#else\n\treturn 0;\n#endif\n}\n\n/**\n * skb_unref - decrement the skb's reference count\n * @skb: buffer\n *\n * Returns true if we can free the skb.\n */\nstatic inline bool skb_unref(struct sk_buff *skb)\n{\n\tif (unlikely(!skb))\n\t\treturn false;\n\tif (likely(refcount_read(&skb->users) == 1))\n\t\tsmp_rmb();\n\telse if (likely(!refcount_dec_and_test(&skb->users)))\n\t\treturn false;\n\n\treturn true;\n}\n\nvoid skb_release_head_state(struct sk_buff *skb);\nvoid kfree_skb(struct sk_buff *skb);\nvoid kfree_skb_list(struct sk_buff *segs);\nvoid skb_dump(const char *level, const struct sk_buff *skb, bool full_pkt);\nvoid skb_tx_error(struct sk_buff *skb);\n\n#ifdef CONFIG_TRACEPOINTS\nvoid consume_skb(struct sk_buff *skb);\n#else\nstatic inline void consume_skb(struct sk_buff *skb)\n{\n\treturn kfree_skb(skb);\n}\n#endif\n\nvoid __consume_stateless_skb(struct sk_buff *skb);\nvoid  __kfree_skb(struct sk_buff *skb);\nextern struct kmem_cache *skbuff_head_cache;\n\nvoid kfree_skb_partial(struct sk_buff *skb, bool head_stolen);\nbool skb_try_coalesce(struct sk_buff *to, struct sk_buff *from,\n\t\t      bool *fragstolen, int *delta_truesize);\n\nstruct sk_buff *__alloc_skb(unsigned int size, gfp_t priority, int flags,\n\t\t\t    int node);\nstruct sk_buff *__build_skb(void *data, unsigned int frag_size);\nstruct sk_buff *build_skb(void *data, unsigned int frag_size);\nstruct sk_buff *build_skb_around(struct sk_buff *skb,\n\t\t\t\t void *data, unsigned int frag_size);\n\nstruct sk_buff *napi_build_skb(void *data, unsigned int frag_size);\n\n/**\n * alloc_skb - allocate a network buffer\n * @size: size to allocate\n * @priority: allocation mask\n *\n * This function is a convenient wrapper around __alloc_skb().\n */\nstatic inline struct sk_buff *alloc_skb(unsigned int size,\n\t\t\t\t\tgfp_t priority)\n{\n\treturn __alloc_skb(size, priority, 0, NUMA_NO_NODE);\n}\n\nstruct sk_buff *alloc_skb_with_frags(unsigned long header_len,\n\t\t\t\t     unsigned long data_len,\n\t\t\t\t     int max_page_order,\n\t\t\t\t     int *errcode,\n\t\t\t\t     gfp_t gfp_mask);\nstruct sk_buff *alloc_skb_for_msg(struct sk_buff *first);\n\n/* Layout of fast clones : [skb1][skb2][fclone_ref] */\nstruct sk_buff_fclones {\n\tstruct sk_buff\tskb1;\n\n\tstruct sk_buff\tskb2;\n\n\trefcount_t\tfclone_ref;\n};\n\n/**\n *\tskb_fclone_busy - check if fclone is busy\n *\t@sk: socket\n *\t@skb: buffer\n *\n * Returns true if skb is a fast clone, and its clone is not freed.\n * Some drivers call skb_orphan() in their ndo_start_xmit(),\n * so we also check that this didnt happen.\n */\nstatic inline bool skb_fclone_busy(const struct sock *sk,\n\t\t\t\t   const struct sk_buff *skb)\n{\n\tconst struct sk_buff_fclones *fclones;\n\n\tfclones = container_of(skb, struct sk_buff_fclones, skb1);\n\n\treturn skb->fclone == SKB_FCLONE_ORIG &&\n\t       refcount_read(&fclones->fclone_ref) > 1 &&\n\t       READ_ONCE(fclones->skb2.sk) == sk;\n}\n\n/**\n * alloc_skb_fclone - allocate a network buffer from fclone cache\n * @size: size to allocate\n * @priority: allocation mask\n *\n * This function is a convenient wrapper around __alloc_skb().\n */\nstatic inline struct sk_buff *alloc_skb_fclone(unsigned int size,\n\t\t\t\t\t       gfp_t priority)\n{\n\treturn __alloc_skb(size, priority, SKB_ALLOC_FCLONE, NUMA_NO_NODE);\n}\n\nstruct sk_buff *skb_morph(struct sk_buff *dst, struct sk_buff *src);\nvoid skb_headers_offset_update(struct sk_buff *skb, int off);\nint skb_copy_ubufs(struct sk_buff *skb, gfp_t gfp_mask);\nstruct sk_buff *skb_clone(struct sk_buff *skb, gfp_t priority);\nvoid skb_copy_header(struct sk_buff *new, const struct sk_buff *old);\nstruct sk_buff *skb_copy(const struct sk_buff *skb, gfp_t priority);\nstruct sk_buff *__pskb_copy_fclone(struct sk_buff *skb, int headroom,\n\t\t\t\t   gfp_t gfp_mask, bool fclone);\nstatic inline struct sk_buff *__pskb_copy(struct sk_buff *skb, int headroom,\n\t\t\t\t\t  gfp_t gfp_mask)\n{\n\treturn __pskb_copy_fclone(skb, headroom, gfp_mask, false);\n}\n\nint pskb_expand_head(struct sk_buff *skb, int nhead, int ntail, gfp_t gfp_mask);\nstruct sk_buff *skb_realloc_headroom(struct sk_buff *skb,\n\t\t\t\t     unsigned int headroom);\nstruct sk_buff *skb_copy_expand(const struct sk_buff *skb, int newheadroom,\n\t\t\t\tint newtailroom, gfp_t priority);\nint __must_check skb_to_sgvec_nomark(struct sk_buff *skb, struct scatterlist *sg,\n\t\t\t\t     int offset, int len);\nint __must_check skb_to_sgvec(struct sk_buff *skb, struct scatterlist *sg,\n\t\t\t      int offset, int len);\nint skb_cow_data(struct sk_buff *skb, int tailbits, struct sk_buff **trailer);\nint __skb_pad(struct sk_buff *skb, int pad, bool free_on_error);\n\n/**\n *\tskb_pad\t\t\t-\tzero pad the tail of an skb\n *\t@skb: buffer to pad\n *\t@pad: space to pad\n *\n *\tEnsure that a buffer is followed by a padding area that is zero\n *\tfilled. Used by network drivers which may DMA or transfer data\n *\tbeyond the buffer end onto the wire.\n *\n *\tMay return error in out of memory cases. The skb is freed on error.\n */\nstatic inline int skb_pad(struct sk_buff *skb, int pad)\n{\n\treturn __skb_pad(skb, pad, true);\n}\n#define dev_kfree_skb(a)\tconsume_skb(a)\n\nint skb_append_pagefrags(struct sk_buff *skb, struct page *page,\n\t\t\t int offset, size_t size);\n\nstruct skb_seq_state {\n\t__u32\t\tlower_offset;\n\t__u32\t\tupper_offset;\n\t__u32\t\tfrag_idx;\n\t__u32\t\tstepped_offset;\n\tstruct sk_buff\t*root_skb;\n\tstruct sk_buff\t*cur_skb;\n\t__u8\t\t*frag_data;\n\t__u32\t\tfrag_off;\n};\n\nvoid skb_prepare_seq_read(struct sk_buff *skb, unsigned int from,\n\t\t\t  unsigned int to, struct skb_seq_state *st);\nunsigned int skb_seq_read(unsigned int consumed, const u8 **data,\n\t\t\t  struct skb_seq_state *st);\nvoid skb_abort_seq_read(struct skb_seq_state *st);\n\nunsigned int skb_find_text(struct sk_buff *skb, unsigned int from,\n\t\t\t   unsigned int to, struct ts_config *config);\n\n/*\n * Packet hash types specify the type of hash in skb_set_hash.\n *\n * Hash types refer to the protocol layer addresses which are used to\n * construct a packet's hash. The hashes are used to differentiate or identify\n * flows of the protocol layer for the hash type. Hash types are either\n * layer-2 (L2), layer-3 (L3), or layer-4 (L4).\n *\n * Properties of hashes:\n *\n * 1) Two packets in different flows have different hash values\n * 2) Two packets in the same flow should have the same hash value\n *\n * A hash at a higher layer is considered to be more specific. A driver should\n * set the most specific hash possible.\n *\n * A driver cannot indicate a more specific hash than the layer at which a hash\n * was computed. For instance an L3 hash cannot be set as an L4 hash.\n *\n * A driver may indicate a hash level which is less specific than the\n * actual layer the hash was computed on. For instance, a hash computed\n * at L4 may be considered an L3 hash. This should only be done if the\n * driver can't unambiguously determine that the HW computed the hash at\n * the higher layer. Note that the \"should\" in the second property above\n * permits this.\n */\nenum pkt_hash_types {\n\tPKT_HASH_TYPE_NONE,\t/* Undefined type */\n\tPKT_HASH_TYPE_L2,\t/* Input: src_MAC, dest_MAC */\n\tPKT_HASH_TYPE_L3,\t/* Input: src_IP, dst_IP */\n\tPKT_HASH_TYPE_L4,\t/* Input: src_IP, dst_IP, src_port, dst_port */\n};\n\nstatic inline void skb_clear_hash(struct sk_buff *skb)\n{\n\tskb->hash = 0;\n\tskb->sw_hash = 0;\n\tskb->l4_hash = 0;\n}\n\nstatic inline void skb_clear_hash_if_not_l4(struct sk_buff *skb)\n{\n\tif (!skb->l4_hash)\n\t\tskb_clear_hash(skb);\n}\n\nstatic inline void\n__skb_set_hash(struct sk_buff *skb, __u32 hash, bool is_sw, bool is_l4)\n{\n\tskb->l4_hash = is_l4;\n\tskb->sw_hash = is_sw;\n\tskb->hash = hash;\n}\n\nstatic inline void\nskb_set_hash(struct sk_buff *skb, __u32 hash, enum pkt_hash_types type)\n{\n\t/* Used by drivers to set hash from HW */\n\t__skb_set_hash(skb, hash, false, type == PKT_HASH_TYPE_L4);\n}\n\nstatic inline void\n__skb_set_sw_hash(struct sk_buff *skb, __u32 hash, bool is_l4)\n{\n\t__skb_set_hash(skb, hash, true, is_l4);\n}\n\nvoid __skb_get_hash(struct sk_buff *skb);\nu32 __skb_get_hash_symmetric(const struct sk_buff *skb);\nu32 skb_get_poff(const struct sk_buff *skb);\nu32 __skb_get_poff(const struct sk_buff *skb, const void *data,\n\t\t   const struct flow_keys_basic *keys, int hlen);\n__be32 __skb_flow_get_ports(const struct sk_buff *skb, int thoff, u8 ip_proto,\n\t\t\t    const void *data, int hlen_proto);\n\nstatic inline __be32 skb_flow_get_ports(const struct sk_buff *skb,\n\t\t\t\t\tint thoff, u8 ip_proto)\n{\n\treturn __skb_flow_get_ports(skb, thoff, ip_proto, NULL, 0);\n}\n\nvoid skb_flow_dissector_init(struct flow_dissector *flow_dissector,\n\t\t\t     const struct flow_dissector_key *key,\n\t\t\t     unsigned int key_count);\n\nstruct bpf_flow_dissector;\nbool bpf_flow_dissect(struct bpf_prog *prog, struct bpf_flow_dissector *ctx,\n\t\t      __be16 proto, int nhoff, int hlen, unsigned int flags);\n\nbool __skb_flow_dissect(const struct net *net,\n\t\t\tconst struct sk_buff *skb,\n\t\t\tstruct flow_dissector *flow_dissector,\n\t\t\tvoid *target_container, const void *data,\n\t\t\t__be16 proto, int nhoff, int hlen, unsigned int flags);\n\nstatic inline bool skb_flow_dissect(const struct sk_buff *skb,\n\t\t\t\t    struct flow_dissector *flow_dissector,\n\t\t\t\t    void *target_container, unsigned int flags)\n{\n\treturn __skb_flow_dissect(NULL, skb, flow_dissector,\n\t\t\t\t  target_container, NULL, 0, 0, 0, flags);\n}\n\nstatic inline bool skb_flow_dissect_flow_keys(const struct sk_buff *skb,\n\t\t\t\t\t      struct flow_keys *flow,\n\t\t\t\t\t      unsigned int flags)\n{\n\tmemset(flow, 0, sizeof(*flow));\n\treturn __skb_flow_dissect(NULL, skb, &flow_keys_dissector,\n\t\t\t\t  flow, NULL, 0, 0, 0, flags);\n}\n\nstatic inline bool\nskb_flow_dissect_flow_keys_basic(const struct net *net,\n\t\t\t\t const struct sk_buff *skb,\n\t\t\t\t struct flow_keys_basic *flow,\n\t\t\t\t const void *data, __be16 proto,\n\t\t\t\t int nhoff, int hlen, unsigned int flags)\n{\n\tmemset(flow, 0, sizeof(*flow));\n\treturn __skb_flow_dissect(net, skb, &flow_keys_basic_dissector, flow,\n\t\t\t\t  data, proto, nhoff, hlen, flags);\n}\n\nvoid skb_flow_dissect_meta(const struct sk_buff *skb,\n\t\t\t   struct flow_dissector *flow_dissector,\n\t\t\t   void *target_container);\n\n/* Gets a skb connection tracking info, ctinfo map should be a\n * map of mapsize to translate enum ip_conntrack_info states\n * to user states.\n */\nvoid\nskb_flow_dissect_ct(const struct sk_buff *skb,\n\t\t    struct flow_dissector *flow_dissector,\n\t\t    void *target_container,\n\t\t    u16 *ctinfo_map, size_t mapsize,\n\t\t    bool post_ct);\nvoid\nskb_flow_dissect_tunnel_info(const struct sk_buff *skb,\n\t\t\t     struct flow_dissector *flow_dissector,\n\t\t\t     void *target_container);\n\nvoid skb_flow_dissect_hash(const struct sk_buff *skb,\n\t\t\t   struct flow_dissector *flow_dissector,\n\t\t\t   void *target_container);\n\nstatic inline __u32 skb_get_hash(struct sk_buff *skb)\n{\n\tif (!skb->l4_hash && !skb->sw_hash)\n\t\t__skb_get_hash(skb);\n\n\treturn skb->hash;\n}\n\nstatic inline __u32 skb_get_hash_flowi6(struct sk_buff *skb, const struct flowi6 *fl6)\n{\n\tif (!skb->l4_hash && !skb->sw_hash) {\n\t\tstruct flow_keys keys;\n\t\t__u32 hash = __get_hash_from_flowi6(fl6, &keys);\n\n\t\t__skb_set_sw_hash(skb, hash, flow_keys_have_l4(&keys));\n\t}\n\n\treturn skb->hash;\n}\n\n__u32 skb_get_hash_perturb(const struct sk_buff *skb,\n\t\t\t   const siphash_key_t *perturb);\n\nstatic inline __u32 skb_get_hash_raw(const struct sk_buff *skb)\n{\n\treturn skb->hash;\n}\n\nstatic inline void skb_copy_hash(struct sk_buff *to, const struct sk_buff *from)\n{\n\tto->hash = from->hash;\n\tto->sw_hash = from->sw_hash;\n\tto->l4_hash = from->l4_hash;\n};\n\nstatic inline void skb_copy_decrypted(struct sk_buff *to,\n\t\t\t\t      const struct sk_buff *from)\n{\n#ifdef CONFIG_TLS_DEVICE\n\tto->decrypted = from->decrypted;\n#endif\n}\n\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\nstatic inline unsigned char *skb_end_pointer(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->end;\n}\n\nstatic inline unsigned int skb_end_offset(const struct sk_buff *skb)\n{\n\treturn skb->end;\n}\n#else\nstatic inline unsigned char *skb_end_pointer(const struct sk_buff *skb)\n{\n\treturn skb->end;\n}\n\nstatic inline unsigned int skb_end_offset(const struct sk_buff *skb)\n{\n\treturn skb->end - skb->head;\n}\n#endif\n\n/* Internal */\n#define skb_shinfo(SKB)\t((struct skb_shared_info *)(skb_end_pointer(SKB)))\n\nstatic inline struct skb_shared_hwtstamps *skb_hwtstamps(struct sk_buff *skb)\n{\n\treturn &skb_shinfo(skb)->hwtstamps;\n}\n\nstatic inline struct ubuf_info *skb_zcopy(struct sk_buff *skb)\n{\n\tbool is_zcopy = skb && skb_shinfo(skb)->flags & SKBFL_ZEROCOPY_ENABLE;\n\n\treturn is_zcopy ? skb_uarg(skb) : NULL;\n}\n\nstatic inline void net_zcopy_get(struct ubuf_info *uarg)\n{\n\trefcount_inc(&uarg->refcnt);\n}\n\nstatic inline void skb_zcopy_init(struct sk_buff *skb, struct ubuf_info *uarg)\n{\n\tskb_shinfo(skb)->destructor_arg = uarg;\n\tskb_shinfo(skb)->flags |= uarg->flags;\n}\n\nstatic inline void skb_zcopy_set(struct sk_buff *skb, struct ubuf_info *uarg,\n\t\t\t\t bool *have_ref)\n{\n\tif (skb && uarg && !skb_zcopy(skb)) {\n\t\tif (unlikely(have_ref && *have_ref))\n\t\t\t*have_ref = false;\n\t\telse\n\t\t\tnet_zcopy_get(uarg);\n\t\tskb_zcopy_init(skb, uarg);\n\t}\n}\n\nstatic inline void skb_zcopy_set_nouarg(struct sk_buff *skb, void *val)\n{\n\tskb_shinfo(skb)->destructor_arg = (void *)((uintptr_t) val | 0x1UL);\n\tskb_shinfo(skb)->flags |= SKBFL_ZEROCOPY_FRAG;\n}\n\nstatic inline bool skb_zcopy_is_nouarg(struct sk_buff *skb)\n{\n\treturn (uintptr_t) skb_shinfo(skb)->destructor_arg & 0x1UL;\n}\n\nstatic inline void *skb_zcopy_get_nouarg(struct sk_buff *skb)\n{\n\treturn (void *)((uintptr_t) skb_shinfo(skb)->destructor_arg & ~0x1UL);\n}\n\nstatic inline void net_zcopy_put(struct ubuf_info *uarg)\n{\n\tif (uarg)\n\t\tuarg->callback(NULL, uarg, true);\n}\n\nstatic inline void net_zcopy_put_abort(struct ubuf_info *uarg, bool have_uref)\n{\n\tif (uarg) {\n\t\tif (uarg->callback == msg_zerocopy_callback)\n\t\t\tmsg_zerocopy_put_abort(uarg, have_uref);\n\t\telse if (have_uref)\n\t\t\tnet_zcopy_put(uarg);\n\t}\n}\n\n/* Release a reference on a zerocopy structure */\nstatic inline void skb_zcopy_clear(struct sk_buff *skb, bool zerocopy_success)\n{\n\tstruct ubuf_info *uarg = skb_zcopy(skb);\n\n\tif (uarg) {\n\t\tif (!skb_zcopy_is_nouarg(skb))\n\t\t\tuarg->callback(skb, uarg, zerocopy_success);\n\n\t\tskb_shinfo(skb)->flags &= ~SKBFL_ZEROCOPY_FRAG;\n\t}\n}\n\nstatic inline void skb_mark_not_on_list(struct sk_buff *skb)\n{\n\tskb->next = NULL;\n}\n\n/* Iterate through singly-linked GSO fragments of an skb. */\n#define skb_list_walk_safe(first, skb, next_skb)                               \\\n\tfor ((skb) = (first), (next_skb) = (skb) ? (skb)->next : NULL; (skb);  \\\n\t     (skb) = (next_skb), (next_skb) = (skb) ? (skb)->next : NULL)\n\nstatic inline void skb_list_del_init(struct sk_buff *skb)\n{\n\t__list_del_entry(&skb->list);\n\tskb_mark_not_on_list(skb);\n}\n\n/**\n *\tskb_queue_empty - check if a queue is empty\n *\t@list: queue head\n *\n *\tReturns true if the queue is empty, false otherwise.\n */\nstatic inline int skb_queue_empty(const struct sk_buff_head *list)\n{\n\treturn list->next == (const struct sk_buff *) list;\n}\n\n/**\n *\tskb_queue_empty_lockless - check if a queue is empty\n *\t@list: queue head\n *\n *\tReturns true if the queue is empty, false otherwise.\n *\tThis variant can be used in lockless contexts.\n */\nstatic inline bool skb_queue_empty_lockless(const struct sk_buff_head *list)\n{\n\treturn READ_ONCE(list->next) == (const struct sk_buff *) list;\n}\n\n\n/**\n *\tskb_queue_is_last - check if skb is the last entry in the queue\n *\t@list: queue head\n *\t@skb: buffer\n *\n *\tReturns true if @skb is the last buffer on the list.\n */\nstatic inline bool skb_queue_is_last(const struct sk_buff_head *list,\n\t\t\t\t     const struct sk_buff *skb)\n{\n\treturn skb->next == (const struct sk_buff *) list;\n}\n\n/**\n *\tskb_queue_is_first - check if skb is the first entry in the queue\n *\t@list: queue head\n *\t@skb: buffer\n *\n *\tReturns true if @skb is the first buffer on the list.\n */\nstatic inline bool skb_queue_is_first(const struct sk_buff_head *list,\n\t\t\t\t      const struct sk_buff *skb)\n{\n\treturn skb->prev == (const struct sk_buff *) list;\n}\n\n/**\n *\tskb_queue_next - return the next packet in the queue\n *\t@list: queue head\n *\t@skb: current buffer\n *\n *\tReturn the next packet in @list after @skb.  It is only valid to\n *\tcall this if skb_queue_is_last() evaluates to false.\n */\nstatic inline struct sk_buff *skb_queue_next(const struct sk_buff_head *list,\n\t\t\t\t\t     const struct sk_buff *skb)\n{\n\t/* This BUG_ON may seem severe, but if we just return then we\n\t * are going to dereference garbage.\n\t */\n\tBUG_ON(skb_queue_is_last(list, skb));\n\treturn skb->next;\n}\n\n/**\n *\tskb_queue_prev - return the prev packet in the queue\n *\t@list: queue head\n *\t@skb: current buffer\n *\n *\tReturn the prev packet in @list before @skb.  It is only valid to\n *\tcall this if skb_queue_is_first() evaluates to false.\n */\nstatic inline struct sk_buff *skb_queue_prev(const struct sk_buff_head *list,\n\t\t\t\t\t     const struct sk_buff *skb)\n{\n\t/* This BUG_ON may seem severe, but if we just return then we\n\t * are going to dereference garbage.\n\t */\n\tBUG_ON(skb_queue_is_first(list, skb));\n\treturn skb->prev;\n}\n\n/**\n *\tskb_get - reference buffer\n *\t@skb: buffer to reference\n *\n *\tMakes another reference to a socket buffer and returns a pointer\n *\tto the buffer.\n */\nstatic inline struct sk_buff *skb_get(struct sk_buff *skb)\n{\n\trefcount_inc(&skb->users);\n\treturn skb;\n}\n\n/*\n * If users == 1, we are the only owner and can avoid redundant atomic changes.\n */\n\n/**\n *\tskb_cloned - is the buffer a clone\n *\t@skb: buffer to check\n *\n *\tReturns true if the buffer was generated with skb_clone() and is\n *\tone of multiple shared copies of the buffer. Cloned buffers are\n *\tshared data so must not be written to under normal circumstances.\n */\nstatic inline int skb_cloned(const struct sk_buff *skb)\n{\n\treturn skb->cloned &&\n\t       (atomic_read(&skb_shinfo(skb)->dataref) & SKB_DATAREF_MASK) != 1;\n}\n\nstatic inline int skb_unclone(struct sk_buff *skb, gfp_t pri)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(pri));\n\n\tif (skb_cloned(skb))\n\t\treturn pskb_expand_head(skb, 0, 0, pri);\n\n\treturn 0;\n}\n\n/**\n *\tskb_header_cloned - is the header a clone\n *\t@skb: buffer to check\n *\n *\tReturns true if modifying the header part of the buffer requires\n *\tthe data to be copied.\n */\nstatic inline int skb_header_cloned(const struct sk_buff *skb)\n{\n\tint dataref;\n\n\tif (!skb->cloned)\n\t\treturn 0;\n\n\tdataref = atomic_read(&skb_shinfo(skb)->dataref);\n\tdataref = (dataref & SKB_DATAREF_MASK) - (dataref >> SKB_DATAREF_SHIFT);\n\treturn dataref != 1;\n}\n\nstatic inline int skb_header_unclone(struct sk_buff *skb, gfp_t pri)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(pri));\n\n\tif (skb_header_cloned(skb))\n\t\treturn pskb_expand_head(skb, 0, 0, pri);\n\n\treturn 0;\n}\n\n/**\n *\t__skb_header_release - release reference to header\n *\t@skb: buffer to operate on\n */\nstatic inline void __skb_header_release(struct sk_buff *skb)\n{\n\tskb->nohdr = 1;\n\tatomic_set(&skb_shinfo(skb)->dataref, 1 + (1 << SKB_DATAREF_SHIFT));\n}\n\n\n/**\n *\tskb_shared - is the buffer shared\n *\t@skb: buffer to check\n *\n *\tReturns true if more than one person has a reference to this\n *\tbuffer.\n */\nstatic inline int skb_shared(const struct sk_buff *skb)\n{\n\treturn refcount_read(&skb->users) != 1;\n}\n\n/**\n *\tskb_share_check - check if buffer is shared and if so clone it\n *\t@skb: buffer to check\n *\t@pri: priority for memory allocation\n *\n *\tIf the buffer is shared the buffer is cloned and the old copy\n *\tdrops a reference. A new clone with a single reference is returned.\n *\tIf the buffer is not shared the original buffer is returned. When\n *\tbeing called from interrupt status or with spinlocks held pri must\n *\tbe GFP_ATOMIC.\n *\n *\tNULL is returned on a memory allocation failure.\n */\nstatic inline struct sk_buff *skb_share_check(struct sk_buff *skb, gfp_t pri)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(pri));\n\tif (skb_shared(skb)) {\n\t\tstruct sk_buff *nskb = skb_clone(skb, pri);\n\n\t\tif (likely(nskb))\n\t\t\tconsume_skb(skb);\n\t\telse\n\t\t\tkfree_skb(skb);\n\t\tskb = nskb;\n\t}\n\treturn skb;\n}\n\n/*\n *\tCopy shared buffers into a new sk_buff. We effectively do COW on\n *\tpackets to handle cases where we have a local reader and forward\n *\tand a couple of other messy ones. The normal one is tcpdumping\n *\ta packet thats being forwarded.\n */\n\n/**\n *\tskb_unshare - make a copy of a shared buffer\n *\t@skb: buffer to check\n *\t@pri: priority for memory allocation\n *\n *\tIf the socket buffer is a clone then this function creates a new\n *\tcopy of the data, drops a reference count on the old copy and returns\n *\tthe new copy with the reference count at 1. If the buffer is not a clone\n *\tthe original buffer is returned. When called with a spinlock held or\n *\tfrom interrupt state @pri must be %GFP_ATOMIC\n *\n *\t%NULL is returned on a memory allocation failure.\n */\nstatic inline struct sk_buff *skb_unshare(struct sk_buff *skb,\n\t\t\t\t\t  gfp_t pri)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(pri));\n\tif (skb_cloned(skb)) {\n\t\tstruct sk_buff *nskb = skb_copy(skb, pri);\n\n\t\t/* Free our shared copy */\n\t\tif (likely(nskb))\n\t\t\tconsume_skb(skb);\n\t\telse\n\t\t\tkfree_skb(skb);\n\t\tskb = nskb;\n\t}\n\treturn skb;\n}\n\n/**\n *\tskb_peek - peek at the head of an &sk_buff_head\n *\t@list_: list to peek at\n *\n *\tPeek an &sk_buff. Unlike most other operations you _MUST_\n *\tbe careful with this one. A peek leaves the buffer on the\n *\tlist and someone else may run off with it. You must hold\n *\tthe appropriate locks or have a private queue to do this.\n *\n *\tReturns %NULL for an empty list or a pointer to the head element.\n *\tThe reference count is not incremented and the reference is therefore\n *\tvolatile. Use with caution.\n */\nstatic inline struct sk_buff *skb_peek(const struct sk_buff_head *list_)\n{\n\tstruct sk_buff *skb = list_->next;\n\n\tif (skb == (struct sk_buff *)list_)\n\t\tskb = NULL;\n\treturn skb;\n}\n\n/**\n *\t__skb_peek - peek at the head of a non-empty &sk_buff_head\n *\t@list_: list to peek at\n *\n *\tLike skb_peek(), but the caller knows that the list is not empty.\n */\nstatic inline struct sk_buff *__skb_peek(const struct sk_buff_head *list_)\n{\n\treturn list_->next;\n}\n\n/**\n *\tskb_peek_next - peek skb following the given one from a queue\n *\t@skb: skb to start from\n *\t@list_: list to peek at\n *\n *\tReturns %NULL when the end of the list is met or a pointer to the\n *\tnext element. The reference count is not incremented and the\n *\treference is therefore volatile. Use with caution.\n */\nstatic inline struct sk_buff *skb_peek_next(struct sk_buff *skb,\n\t\tconst struct sk_buff_head *list_)\n{\n\tstruct sk_buff *next = skb->next;\n\n\tif (next == (struct sk_buff *)list_)\n\t\tnext = NULL;\n\treturn next;\n}\n\n/**\n *\tskb_peek_tail - peek at the tail of an &sk_buff_head\n *\t@list_: list to peek at\n *\n *\tPeek an &sk_buff. Unlike most other operations you _MUST_\n *\tbe careful with this one. A peek leaves the buffer on the\n *\tlist and someone else may run off with it. You must hold\n *\tthe appropriate locks or have a private queue to do this.\n *\n *\tReturns %NULL for an empty list or a pointer to the tail element.\n *\tThe reference count is not incremented and the reference is therefore\n *\tvolatile. Use with caution.\n */\nstatic inline struct sk_buff *skb_peek_tail(const struct sk_buff_head *list_)\n{\n\tstruct sk_buff *skb = READ_ONCE(list_->prev);\n\n\tif (skb == (struct sk_buff *)list_)\n\t\tskb = NULL;\n\treturn skb;\n\n}\n\n/**\n *\tskb_queue_len\t- get queue length\n *\t@list_: list to measure\n *\n *\tReturn the length of an &sk_buff queue.\n */\nstatic inline __u32 skb_queue_len(const struct sk_buff_head *list_)\n{\n\treturn list_->qlen;\n}\n\n/**\n *\tskb_queue_len_lockless\t- get queue length\n *\t@list_: list to measure\n *\n *\tReturn the length of an &sk_buff queue.\n *\tThis variant can be used in lockless contexts.\n */\nstatic inline __u32 skb_queue_len_lockless(const struct sk_buff_head *list_)\n{\n\treturn READ_ONCE(list_->qlen);\n}\n\n/**\n *\t__skb_queue_head_init - initialize non-spinlock portions of sk_buff_head\n *\t@list: queue to initialize\n *\n *\tThis initializes only the list and queue length aspects of\n *\tan sk_buff_head object.  This allows to initialize the list\n *\taspects of an sk_buff_head without reinitializing things like\n *\tthe spinlock.  It can also be used for on-stack sk_buff_head\n *\tobjects where the spinlock is known to not be used.\n */\nstatic inline void __skb_queue_head_init(struct sk_buff_head *list)\n{\n\tlist->prev = list->next = (struct sk_buff *)list;\n\tlist->qlen = 0;\n}\n\n/*\n * This function creates a split out lock class for each invocation;\n * this is needed for now since a whole lot of users of the skb-queue\n * infrastructure in drivers have different locking usage (in hardirq)\n * than the networking core (in softirq only). In the long run either the\n * network layer or drivers should need annotation to consolidate the\n * main types of usage into 3 classes.\n */\nstatic inline void skb_queue_head_init(struct sk_buff_head *list)\n{\n\tspin_lock_init(&list->lock);\n\t__skb_queue_head_init(list);\n}\n\nstatic inline void skb_queue_head_init_class(struct sk_buff_head *list,\n\t\tstruct lock_class_key *class)\n{\n\tskb_queue_head_init(list);\n\tlockdep_set_class(&list->lock, class);\n}\n\n/*\n *\tInsert an sk_buff on a list.\n *\n *\tThe \"__skb_xxxx()\" functions are the non-atomic ones that\n *\tcan only be called with interrupts disabled.\n */\nstatic inline void __skb_insert(struct sk_buff *newsk,\n\t\t\t\tstruct sk_buff *prev, struct sk_buff *next,\n\t\t\t\tstruct sk_buff_head *list)\n{\n\t/* See skb_queue_empty_lockless() and skb_peek_tail()\n\t * for the opposite READ_ONCE()\n\t */\n\tWRITE_ONCE(newsk->next, next);\n\tWRITE_ONCE(newsk->prev, prev);\n\tWRITE_ONCE(next->prev, newsk);\n\tWRITE_ONCE(prev->next, newsk);\n\tlist->qlen++;\n}\n\nstatic inline void __skb_queue_splice(const struct sk_buff_head *list,\n\t\t\t\t      struct sk_buff *prev,\n\t\t\t\t      struct sk_buff *next)\n{\n\tstruct sk_buff *first = list->next;\n\tstruct sk_buff *last = list->prev;\n\n\tWRITE_ONCE(first->prev, prev);\n\tWRITE_ONCE(prev->next, first);\n\n\tWRITE_ONCE(last->next, next);\n\tWRITE_ONCE(next->prev, last);\n}\n\n/**\n *\tskb_queue_splice - join two skb lists, this is designed for stacks\n *\t@list: the new list to add\n *\t@head: the place to add it in the first list\n */\nstatic inline void skb_queue_splice(const struct sk_buff_head *list,\n\t\t\t\t    struct sk_buff_head *head)\n{\n\tif (!skb_queue_empty(list)) {\n\t\t__skb_queue_splice(list, (struct sk_buff *) head, head->next);\n\t\thead->qlen += list->qlen;\n\t}\n}\n\n/**\n *\tskb_queue_splice_init - join two skb lists and reinitialise the emptied list\n *\t@list: the new list to add\n *\t@head: the place to add it in the first list\n *\n *\tThe list at @list is reinitialised\n */\nstatic inline void skb_queue_splice_init(struct sk_buff_head *list,\n\t\t\t\t\t struct sk_buff_head *head)\n{\n\tif (!skb_queue_empty(list)) {\n\t\t__skb_queue_splice(list, (struct sk_buff *) head, head->next);\n\t\thead->qlen += list->qlen;\n\t\t__skb_queue_head_init(list);\n\t}\n}\n\n/**\n *\tskb_queue_splice_tail - join two skb lists, each list being a queue\n *\t@list: the new list to add\n *\t@head: the place to add it in the first list\n */\nstatic inline void skb_queue_splice_tail(const struct sk_buff_head *list,\n\t\t\t\t\t struct sk_buff_head *head)\n{\n\tif (!skb_queue_empty(list)) {\n\t\t__skb_queue_splice(list, head->prev, (struct sk_buff *) head);\n\t\thead->qlen += list->qlen;\n\t}\n}\n\n/**\n *\tskb_queue_splice_tail_init - join two skb lists and reinitialise the emptied list\n *\t@list: the new list to add\n *\t@head: the place to add it in the first list\n *\n *\tEach of the lists is a queue.\n *\tThe list at @list is reinitialised\n */\nstatic inline void skb_queue_splice_tail_init(struct sk_buff_head *list,\n\t\t\t\t\t      struct sk_buff_head *head)\n{\n\tif (!skb_queue_empty(list)) {\n\t\t__skb_queue_splice(list, head->prev, (struct sk_buff *) head);\n\t\thead->qlen += list->qlen;\n\t\t__skb_queue_head_init(list);\n\t}\n}\n\n/**\n *\t__skb_queue_after - queue a buffer at the list head\n *\t@list: list to use\n *\t@prev: place after this buffer\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer int the middle of a list. This function takes no locks\n *\tand you must therefore hold required locks before calling it.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nstatic inline void __skb_queue_after(struct sk_buff_head *list,\n\t\t\t\t     struct sk_buff *prev,\n\t\t\t\t     struct sk_buff *newsk)\n{\n\t__skb_insert(newsk, prev, prev->next, list);\n}\n\nvoid skb_append(struct sk_buff *old, struct sk_buff *newsk,\n\t\tstruct sk_buff_head *list);\n\nstatic inline void __skb_queue_before(struct sk_buff_head *list,\n\t\t\t\t      struct sk_buff *next,\n\t\t\t\t      struct sk_buff *newsk)\n{\n\t__skb_insert(newsk, next->prev, next, list);\n}\n\n/**\n *\t__skb_queue_head - queue a buffer at the list head\n *\t@list: list to use\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer at the start of a list. This function takes no locks\n *\tand you must therefore hold required locks before calling it.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nstatic inline void __skb_queue_head(struct sk_buff_head *list,\n\t\t\t\t    struct sk_buff *newsk)\n{\n\t__skb_queue_after(list, (struct sk_buff *)list, newsk);\n}\nvoid skb_queue_head(struct sk_buff_head *list, struct sk_buff *newsk);\n\n/**\n *\t__skb_queue_tail - queue a buffer at the list tail\n *\t@list: list to use\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer at the end of a list. This function takes no locks\n *\tand you must therefore hold required locks before calling it.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nstatic inline void __skb_queue_tail(struct sk_buff_head *list,\n\t\t\t\t   struct sk_buff *newsk)\n{\n\t__skb_queue_before(list, (struct sk_buff *)list, newsk);\n}\nvoid skb_queue_tail(struct sk_buff_head *list, struct sk_buff *newsk);\n\n/*\n * remove sk_buff from list. _Must_ be called atomically, and with\n * the list known..\n */\nvoid skb_unlink(struct sk_buff *skb, struct sk_buff_head *list);\nstatic inline void __skb_unlink(struct sk_buff *skb, struct sk_buff_head *list)\n{\n\tstruct sk_buff *next, *prev;\n\n\tWRITE_ONCE(list->qlen, list->qlen - 1);\n\tnext\t   = skb->next;\n\tprev\t   = skb->prev;\n\tskb->next  = skb->prev = NULL;\n\tWRITE_ONCE(next->prev, prev);\n\tWRITE_ONCE(prev->next, next);\n}\n\n/**\n *\t__skb_dequeue - remove from the head of the queue\n *\t@list: list to dequeue from\n *\n *\tRemove the head of the list. This function does not take any locks\n *\tso must be used with appropriate locks held only. The head item is\n *\treturned or %NULL if the list is empty.\n */\nstatic inline struct sk_buff *__skb_dequeue(struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb = skb_peek(list);\n\tif (skb)\n\t\t__skb_unlink(skb, list);\n\treturn skb;\n}\nstruct sk_buff *skb_dequeue(struct sk_buff_head *list);\n\n/**\n *\t__skb_dequeue_tail - remove from the tail of the queue\n *\t@list: list to dequeue from\n *\n *\tRemove the tail of the list. This function does not take any locks\n *\tso must be used with appropriate locks held only. The tail item is\n *\treturned or %NULL if the list is empty.\n */\nstatic inline struct sk_buff *__skb_dequeue_tail(struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb = skb_peek_tail(list);\n\tif (skb)\n\t\t__skb_unlink(skb, list);\n\treturn skb;\n}\nstruct sk_buff *skb_dequeue_tail(struct sk_buff_head *list);\n\n\nstatic inline bool skb_is_nonlinear(const struct sk_buff *skb)\n{\n\treturn skb->data_len;\n}\n\nstatic inline unsigned int skb_headlen(const struct sk_buff *skb)\n{\n\treturn skb->len - skb->data_len;\n}\n\nstatic inline unsigned int __skb_pagelen(const struct sk_buff *skb)\n{\n\tunsigned int i, len = 0;\n\n\tfor (i = skb_shinfo(skb)->nr_frags - 1; (int)i >= 0; i--)\n\t\tlen += skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\treturn len;\n}\n\nstatic inline unsigned int skb_pagelen(const struct sk_buff *skb)\n{\n\treturn skb_headlen(skb) + __skb_pagelen(skb);\n}\n\n/**\n * __skb_fill_page_desc - initialise a paged fragment in an skb\n * @skb: buffer containing fragment to be initialised\n * @i: paged fragment index to initialise\n * @page: the page to use for this fragment\n * @off: the offset to the data with @page\n * @size: the length of the data\n *\n * Initialises the @i'th fragment of @skb to point to &size bytes at\n * offset @off within @page.\n *\n * Does not take any additional reference on the fragment.\n */\nstatic inline void __skb_fill_page_desc(struct sk_buff *skb, int i,\n\t\t\t\t\tstruct page *page, int off, int size)\n{\n\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t/*\n\t * Propagate page pfmemalloc to the skb if we can. The problem is\n\t * that not all callers have unique ownership of the page but rely\n\t * on page_is_pfmemalloc doing the right thing(tm).\n\t */\n\tfrag->bv_page\t\t  = page;\n\tfrag->bv_offset\t\t  = off;\n\tskb_frag_size_set(frag, size);\n\n\tpage = compound_head(page);\n\tif (page_is_pfmemalloc(page))\n\t\tskb->pfmemalloc\t= true;\n}\n\n/**\n * skb_fill_page_desc - initialise a paged fragment in an skb\n * @skb: buffer containing fragment to be initialised\n * @i: paged fragment index to initialise\n * @page: the page to use for this fragment\n * @off: the offset to the data with @page\n * @size: the length of the data\n *\n * As per __skb_fill_page_desc() -- initialises the @i'th fragment of\n * @skb to point to @size bytes at offset @off within @page. In\n * addition updates @skb such that @i is the last fragment.\n *\n * Does not take any additional reference on the fragment.\n */\nstatic inline void skb_fill_page_desc(struct sk_buff *skb, int i,\n\t\t\t\t      struct page *page, int off, int size)\n{\n\t__skb_fill_page_desc(skb, i, page, off, size);\n\tskb_shinfo(skb)->nr_frags = i + 1;\n}\n\nvoid skb_add_rx_frag(struct sk_buff *skb, int i, struct page *page, int off,\n\t\t     int size, unsigned int truesize);\n\nvoid skb_coalesce_rx_frag(struct sk_buff *skb, int i, int size,\n\t\t\t  unsigned int truesize);\n\n#define SKB_LINEAR_ASSERT(skb)  BUG_ON(skb_is_nonlinear(skb))\n\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\nstatic inline unsigned char *skb_tail_pointer(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->tail;\n}\n\nstatic inline void skb_reset_tail_pointer(struct sk_buff *skb)\n{\n\tskb->tail = skb->data - skb->head;\n}\n\nstatic inline void skb_set_tail_pointer(struct sk_buff *skb, const int offset)\n{\n\tskb_reset_tail_pointer(skb);\n\tskb->tail += offset;\n}\n\n#else /* NET_SKBUFF_DATA_USES_OFFSET */\nstatic inline unsigned char *skb_tail_pointer(const struct sk_buff *skb)\n{\n\treturn skb->tail;\n}\n\nstatic inline void skb_reset_tail_pointer(struct sk_buff *skb)\n{\n\tskb->tail = skb->data;\n}\n\nstatic inline void skb_set_tail_pointer(struct sk_buff *skb, const int offset)\n{\n\tskb->tail = skb->data + offset;\n}\n\n#endif /* NET_SKBUFF_DATA_USES_OFFSET */\n\n/*\n *\tAdd data to an sk_buff\n */\nvoid *pskb_put(struct sk_buff *skb, struct sk_buff *tail, int len);\nvoid *skb_put(struct sk_buff *skb, unsigned int len);\nstatic inline void *__skb_put(struct sk_buff *skb, unsigned int len)\n{\n\tvoid *tmp = skb_tail_pointer(skb);\n\tSKB_LINEAR_ASSERT(skb);\n\tskb->tail += len;\n\tskb->len  += len;\n\treturn tmp;\n}\n\nstatic inline void *__skb_put_zero(struct sk_buff *skb, unsigned int len)\n{\n\tvoid *tmp = __skb_put(skb, len);\n\n\tmemset(tmp, 0, len);\n\treturn tmp;\n}\n\nstatic inline void *__skb_put_data(struct sk_buff *skb, const void *data,\n\t\t\t\t   unsigned int len)\n{\n\tvoid *tmp = __skb_put(skb, len);\n\n\tmemcpy(tmp, data, len);\n\treturn tmp;\n}\n\nstatic inline void __skb_put_u8(struct sk_buff *skb, u8 val)\n{\n\t*(u8 *)__skb_put(skb, 1) = val;\n}\n\nstatic inline void *skb_put_zero(struct sk_buff *skb, unsigned int len)\n{\n\tvoid *tmp = skb_put(skb, len);\n\n\tmemset(tmp, 0, len);\n\n\treturn tmp;\n}\n\nstatic inline void *skb_put_data(struct sk_buff *skb, const void *data,\n\t\t\t\t unsigned int len)\n{\n\tvoid *tmp = skb_put(skb, len);\n\n\tmemcpy(tmp, data, len);\n\n\treturn tmp;\n}\n\nstatic inline void skb_put_u8(struct sk_buff *skb, u8 val)\n{\n\t*(u8 *)skb_put(skb, 1) = val;\n}\n\nvoid *skb_push(struct sk_buff *skb, unsigned int len);\nstatic inline void *__skb_push(struct sk_buff *skb, unsigned int len)\n{\n\tskb->data -= len;\n\tskb->len  += len;\n\treturn skb->data;\n}\n\nvoid *skb_pull(struct sk_buff *skb, unsigned int len);\nstatic inline void *__skb_pull(struct sk_buff *skb, unsigned int len)\n{\n\tskb->len -= len;\n\tBUG_ON(skb->len < skb->data_len);\n\treturn skb->data += len;\n}\n\nstatic inline void *skb_pull_inline(struct sk_buff *skb, unsigned int len)\n{\n\treturn unlikely(len > skb->len) ? NULL : __skb_pull(skb, len);\n}\n\nvoid *__pskb_pull_tail(struct sk_buff *skb, int delta);\n\nstatic inline void *__pskb_pull(struct sk_buff *skb, unsigned int len)\n{\n\tif (len > skb_headlen(skb) &&\n\t    !__pskb_pull_tail(skb, len - skb_headlen(skb)))\n\t\treturn NULL;\n\tskb->len -= len;\n\treturn skb->data += len;\n}\n\nstatic inline void *pskb_pull(struct sk_buff *skb, unsigned int len)\n{\n\treturn unlikely(len > skb->len) ? NULL : __pskb_pull(skb, len);\n}\n\nstatic inline bool pskb_may_pull(struct sk_buff *skb, unsigned int len)\n{\n\tif (likely(len <= skb_headlen(skb)))\n\t\treturn true;\n\tif (unlikely(len > skb->len))\n\t\treturn false;\n\treturn __pskb_pull_tail(skb, len - skb_headlen(skb)) != NULL;\n}\n\nvoid skb_condense(struct sk_buff *skb);\n\n/**\n *\tskb_headroom - bytes at buffer head\n *\t@skb: buffer to check\n *\n *\tReturn the number of bytes of free space at the head of an &sk_buff.\n */\nstatic inline unsigned int skb_headroom(const struct sk_buff *skb)\n{\n\treturn skb->data - skb->head;\n}\n\n/**\n *\tskb_tailroom - bytes at buffer end\n *\t@skb: buffer to check\n *\n *\tReturn the number of bytes of free space at the tail of an sk_buff\n */\nstatic inline int skb_tailroom(const struct sk_buff *skb)\n{\n\treturn skb_is_nonlinear(skb) ? 0 : skb->end - skb->tail;\n}\n\n/**\n *\tskb_availroom - bytes at buffer end\n *\t@skb: buffer to check\n *\n *\tReturn the number of bytes of free space at the tail of an sk_buff\n *\tallocated by sk_stream_alloc()\n */\nstatic inline int skb_availroom(const struct sk_buff *skb)\n{\n\tif (skb_is_nonlinear(skb))\n\t\treturn 0;\n\n\treturn skb->end - skb->tail - skb->reserved_tailroom;\n}\n\n/**\n *\tskb_reserve - adjust headroom\n *\t@skb: buffer to alter\n *\t@len: bytes to move\n *\n *\tIncrease the headroom of an empty &sk_buff by reducing the tail\n *\troom. This is only allowed for an empty buffer.\n */\nstatic inline void skb_reserve(struct sk_buff *skb, int len)\n{\n\tskb->data += len;\n\tskb->tail += len;\n}\n\n/**\n *\tskb_tailroom_reserve - adjust reserved_tailroom\n *\t@skb: buffer to alter\n *\t@mtu: maximum amount of headlen permitted\n *\t@needed_tailroom: minimum amount of reserved_tailroom\n *\n *\tSet reserved_tailroom so that headlen can be as large as possible but\n *\tnot larger than mtu and tailroom cannot be smaller than\n *\tneeded_tailroom.\n *\tThe required headroom should already have been reserved before using\n *\tthis function.\n */\nstatic inline void skb_tailroom_reserve(struct sk_buff *skb, unsigned int mtu,\n\t\t\t\t\tunsigned int needed_tailroom)\n{\n\tSKB_LINEAR_ASSERT(skb);\n\tif (mtu < skb_tailroom(skb) - needed_tailroom)\n\t\t/* use at most mtu */\n\t\tskb->reserved_tailroom = skb_tailroom(skb) - mtu;\n\telse\n\t\t/* use up to all available space */\n\t\tskb->reserved_tailroom = needed_tailroom;\n}\n\n#define ENCAP_TYPE_ETHER\t0\n#define ENCAP_TYPE_IPPROTO\t1\n\nstatic inline void skb_set_inner_protocol(struct sk_buff *skb,\n\t\t\t\t\t  __be16 protocol)\n{\n\tskb->inner_protocol = protocol;\n\tskb->inner_protocol_type = ENCAP_TYPE_ETHER;\n}\n\nstatic inline void skb_set_inner_ipproto(struct sk_buff *skb,\n\t\t\t\t\t __u8 ipproto)\n{\n\tskb->inner_ipproto = ipproto;\n\tskb->inner_protocol_type = ENCAP_TYPE_IPPROTO;\n}\n\nstatic inline void skb_reset_inner_headers(struct sk_buff *skb)\n{\n\tskb->inner_mac_header = skb->mac_header;\n\tskb->inner_network_header = skb->network_header;\n\tskb->inner_transport_header = skb->transport_header;\n}\n\nstatic inline void skb_reset_mac_len(struct sk_buff *skb)\n{\n\tskb->mac_len = skb->network_header - skb->mac_header;\n}\n\nstatic inline unsigned char *skb_inner_transport_header(const struct sk_buff\n\t\t\t\t\t\t\t*skb)\n{\n\treturn skb->head + skb->inner_transport_header;\n}\n\nstatic inline int skb_inner_transport_offset(const struct sk_buff *skb)\n{\n\treturn skb_inner_transport_header(skb) - skb->data;\n}\n\nstatic inline void skb_reset_inner_transport_header(struct sk_buff *skb)\n{\n\tskb->inner_transport_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_inner_transport_header(struct sk_buff *skb,\n\t\t\t\t\t\t   const int offset)\n{\n\tskb_reset_inner_transport_header(skb);\n\tskb->inner_transport_header += offset;\n}\n\nstatic inline unsigned char *skb_inner_network_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->inner_network_header;\n}\n\nstatic inline void skb_reset_inner_network_header(struct sk_buff *skb)\n{\n\tskb->inner_network_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_inner_network_header(struct sk_buff *skb,\n\t\t\t\t\t\tconst int offset)\n{\n\tskb_reset_inner_network_header(skb);\n\tskb->inner_network_header += offset;\n}\n\nstatic inline unsigned char *skb_inner_mac_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->inner_mac_header;\n}\n\nstatic inline void skb_reset_inner_mac_header(struct sk_buff *skb)\n{\n\tskb->inner_mac_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_inner_mac_header(struct sk_buff *skb,\n\t\t\t\t\t    const int offset)\n{\n\tskb_reset_inner_mac_header(skb);\n\tskb->inner_mac_header += offset;\n}\nstatic inline bool skb_transport_header_was_set(const struct sk_buff *skb)\n{\n\treturn skb->transport_header != (typeof(skb->transport_header))~0U;\n}\n\nstatic inline unsigned char *skb_transport_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->transport_header;\n}\n\nstatic inline void skb_reset_transport_header(struct sk_buff *skb)\n{\n\tskb->transport_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_transport_header(struct sk_buff *skb,\n\t\t\t\t\t    const int offset)\n{\n\tskb_reset_transport_header(skb);\n\tskb->transport_header += offset;\n}\n\nstatic inline unsigned char *skb_network_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->network_header;\n}\n\nstatic inline void skb_reset_network_header(struct sk_buff *skb)\n{\n\tskb->network_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_network_header(struct sk_buff *skb, const int offset)\n{\n\tskb_reset_network_header(skb);\n\tskb->network_header += offset;\n}\n\nstatic inline unsigned char *skb_mac_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->mac_header;\n}\n\nstatic inline int skb_mac_offset(const struct sk_buff *skb)\n{\n\treturn skb_mac_header(skb) - skb->data;\n}\n\nstatic inline u32 skb_mac_header_len(const struct sk_buff *skb)\n{\n\treturn skb->network_header - skb->mac_header;\n}\n\nstatic inline int skb_mac_header_was_set(const struct sk_buff *skb)\n{\n\treturn skb->mac_header != (typeof(skb->mac_header))~0U;\n}\n\nstatic inline void skb_unset_mac_header(struct sk_buff *skb)\n{\n\tskb->mac_header = (typeof(skb->mac_header))~0U;\n}\n\nstatic inline void skb_reset_mac_header(struct sk_buff *skb)\n{\n\tskb->mac_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_mac_header(struct sk_buff *skb, const int offset)\n{\n\tskb_reset_mac_header(skb);\n\tskb->mac_header += offset;\n}\n\nstatic inline void skb_pop_mac_header(struct sk_buff *skb)\n{\n\tskb->mac_header = skb->network_header;\n}\n\nstatic inline void skb_probe_transport_header(struct sk_buff *skb)\n{\n\tstruct flow_keys_basic keys;\n\n\tif (skb_transport_header_was_set(skb))\n\t\treturn;\n\n\tif (skb_flow_dissect_flow_keys_basic(NULL, skb, &keys,\n\t\t\t\t\t     NULL, 0, 0, 0, 0))\n\t\tskb_set_transport_header(skb, keys.control.thoff);\n}\n\nstatic inline void skb_mac_header_rebuild(struct sk_buff *skb)\n{\n\tif (skb_mac_header_was_set(skb)) {\n\t\tconst unsigned char *old_mac = skb_mac_header(skb);\n\n\t\tskb_set_mac_header(skb, -skb->mac_len);\n\t\tmemmove(skb_mac_header(skb), old_mac, skb->mac_len);\n\t}\n}\n\nstatic inline int skb_checksum_start_offset(const struct sk_buff *skb)\n{\n\treturn skb->csum_start - skb_headroom(skb);\n}\n\nstatic inline unsigned char *skb_checksum_start(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->csum_start;\n}\n\nstatic inline int skb_transport_offset(const struct sk_buff *skb)\n{\n\treturn skb_transport_header(skb) - skb->data;\n}\n\nstatic inline u32 skb_network_header_len(const struct sk_buff *skb)\n{\n\treturn skb->transport_header - skb->network_header;\n}\n\nstatic inline u32 skb_inner_network_header_len(const struct sk_buff *skb)\n{\n\treturn skb->inner_transport_header - skb->inner_network_header;\n}\n\nstatic inline int skb_network_offset(const struct sk_buff *skb)\n{\n\treturn skb_network_header(skb) - skb->data;\n}\n\nstatic inline int skb_inner_network_offset(const struct sk_buff *skb)\n{\n\treturn skb_inner_network_header(skb) - skb->data;\n}\n\nstatic inline int pskb_network_may_pull(struct sk_buff *skb, unsigned int len)\n{\n\treturn pskb_may_pull(skb, skb_network_offset(skb) + len);\n}\n\n/*\n * CPUs often take a performance hit when accessing unaligned memory\n * locations. The actual performance hit varies, it can be small if the\n * hardware handles it or large if we have to take an exception and fix it\n * in software.\n *\n * Since an ethernet header is 14 bytes network drivers often end up with\n * the IP header at an unaligned offset. The IP header can be aligned by\n * shifting the start of the packet by 2 bytes. Drivers should do this\n * with:\n *\n * skb_reserve(skb, NET_IP_ALIGN);\n *\n * The downside to this alignment of the IP header is that the DMA is now\n * unaligned. On some architectures the cost of an unaligned DMA is high\n * and this cost outweighs the gains made by aligning the IP header.\n *\n * Since this trade off varies between architectures, we allow NET_IP_ALIGN\n * to be overridden.\n */\n#ifndef NET_IP_ALIGN\n#define NET_IP_ALIGN\t2\n#endif\n\n/*\n * The networking layer reserves some headroom in skb data (via\n * dev_alloc_skb). This is used to avoid having to reallocate skb data when\n * the header has to grow. In the default case, if the header has to grow\n * 32 bytes or less we avoid the reallocation.\n *\n * Unfortunately this headroom changes the DMA alignment of the resulting\n * network packet. As for NET_IP_ALIGN, this unaligned DMA is expensive\n * on some architectures. An architecture can override this value,\n * perhaps setting it to a cacheline in size (since that will maintain\n * cacheline alignment of the DMA). It must be a power of 2.\n *\n * Various parts of the networking layer expect at least 32 bytes of\n * headroom, you should not reduce this.\n *\n * Using max(32, L1_CACHE_BYTES) makes sense (especially with RPS)\n * to reduce average number of cache lines per packet.\n * get_rps_cpu() for example only access one 64 bytes aligned block :\n * NET_IP_ALIGN(2) + ethernet_header(14) + IP_header(20/40) + ports(8)\n */\n#ifndef NET_SKB_PAD\n#define NET_SKB_PAD\tmax(32, L1_CACHE_BYTES)\n#endif\n\nint ___pskb_trim(struct sk_buff *skb, unsigned int len);\n\nstatic inline void __skb_set_length(struct sk_buff *skb, unsigned int len)\n{\n\tif (WARN_ON(skb_is_nonlinear(skb)))\n\t\treturn;\n\tskb->len = len;\n\tskb_set_tail_pointer(skb, len);\n}\n\nstatic inline void __skb_trim(struct sk_buff *skb, unsigned int len)\n{\n\t__skb_set_length(skb, len);\n}\n\nvoid skb_trim(struct sk_buff *skb, unsigned int len);\n\nstatic inline int __pskb_trim(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->data_len)\n\t\treturn ___pskb_trim(skb, len);\n\t__skb_trim(skb, len);\n\treturn 0;\n}\n\nstatic inline int pskb_trim(struct sk_buff *skb, unsigned int len)\n{\n\treturn (len < skb->len) ? __pskb_trim(skb, len) : 0;\n}\n\n/**\n *\tpskb_trim_unique - remove end from a paged unique (not cloned) buffer\n *\t@skb: buffer to alter\n *\t@len: new length\n *\n *\tThis is identical to pskb_trim except that the caller knows that\n *\tthe skb is not cloned so we should never get an error due to out-\n *\tof-memory.\n */\nstatic inline void pskb_trim_unique(struct sk_buff *skb, unsigned int len)\n{\n\tint err = pskb_trim(skb, len);\n\tBUG_ON(err);\n}\n\nstatic inline int __skb_grow(struct sk_buff *skb, unsigned int len)\n{\n\tunsigned int diff = len - skb->len;\n\n\tif (skb_tailroom(skb) < diff) {\n\t\tint ret = pskb_expand_head(skb, 0, diff - skb_tailroom(skb),\n\t\t\t\t\t   GFP_ATOMIC);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\t__skb_set_length(skb, len);\n\treturn 0;\n}\n\n/**\n *\tskb_orphan - orphan a buffer\n *\t@skb: buffer to orphan\n *\n *\tIf a buffer currently has an owner then we call the owner's\n *\tdestructor function and make the @skb unowned. The buffer continues\n *\tto exist but is no longer charged to its former owner.\n */\nstatic inline void skb_orphan(struct sk_buff *skb)\n{\n\tif (skb->destructor) {\n\t\tskb->destructor(skb);\n\t\tskb->destructor = NULL;\n\t\tskb->sk\t\t= NULL;\n\t} else {\n\t\tBUG_ON(skb->sk);\n\t}\n}\n\n/**\n *\tskb_orphan_frags - orphan the frags contained in a buffer\n *\t@skb: buffer to orphan frags from\n *\t@gfp_mask: allocation mask for replacement pages\n *\n *\tFor each frag in the SKB which needs a destructor (i.e. has an\n *\towner) create a copy of that frag and release the original\n *\tpage by calling the destructor.\n */\nstatic inline int skb_orphan_frags(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tif (likely(!skb_zcopy(skb)))\n\t\treturn 0;\n\tif (!skb_zcopy_is_nouarg(skb) &&\n\t    skb_uarg(skb)->callback == msg_zerocopy_callback)\n\t\treturn 0;\n\treturn skb_copy_ubufs(skb, gfp_mask);\n}\n\n/* Frags must be orphaned, even if refcounted, if skb might loop to rx path */\nstatic inline int skb_orphan_frags_rx(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tif (likely(!skb_zcopy(skb)))\n\t\treturn 0;\n\treturn skb_copy_ubufs(skb, gfp_mask);\n}\n\n/**\n *\t__skb_queue_purge - empty a list\n *\t@list: list to empty\n *\n *\tDelete all buffers on an &sk_buff list. Each buffer is removed from\n *\tthe list and one reference dropped. This function does not take the\n *\tlist lock and the caller must hold the relevant locks to use it.\n */\nstatic inline void __skb_queue_purge(struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb;\n\twhile ((skb = __skb_dequeue(list)) != NULL)\n\t\tkfree_skb(skb);\n}\nvoid skb_queue_purge(struct sk_buff_head *list);\n\nunsigned int skb_rbtree_purge(struct rb_root *root);\n\nvoid *__netdev_alloc_frag_align(unsigned int fragsz, unsigned int align_mask);\n\n/**\n * netdev_alloc_frag - allocate a page fragment\n * @fragsz: fragment size\n *\n * Allocates a frag from a page for receive buffer.\n * Uses GFP_ATOMIC allocations.\n */\nstatic inline void *netdev_alloc_frag(unsigned int fragsz)\n{\n\treturn __netdev_alloc_frag_align(fragsz, ~0u);\n}\n\nstatic inline void *netdev_alloc_frag_align(unsigned int fragsz,\n\t\t\t\t\t    unsigned int align)\n{\n\tWARN_ON_ONCE(!is_power_of_2(align));\n\treturn __netdev_alloc_frag_align(fragsz, -align);\n}\n\nstruct sk_buff *__netdev_alloc_skb(struct net_device *dev, unsigned int length,\n\t\t\t\t   gfp_t gfp_mask);\n\n/**\n *\tnetdev_alloc_skb - allocate an skbuff for rx on a specific device\n *\t@dev: network device to receive on\n *\t@length: length to allocate\n *\n *\tAllocate a new &sk_buff and assign it a usage count of one. The\n *\tbuffer has unspecified headroom built in. Users should allocate\n *\tthe headroom they think they need without accounting for the\n *\tbuilt in space. The built in space is used for optimisations.\n *\n *\t%NULL is returned if there is no free memory. Although this function\n *\tallocates memory it can be called from an interrupt.\n */\nstatic inline struct sk_buff *netdev_alloc_skb(struct net_device *dev,\n\t\t\t\t\t       unsigned int length)\n{\n\treturn __netdev_alloc_skb(dev, length, GFP_ATOMIC);\n}\n\n/* legacy helper around __netdev_alloc_skb() */\nstatic inline struct sk_buff *__dev_alloc_skb(unsigned int length,\n\t\t\t\t\t      gfp_t gfp_mask)\n{\n\treturn __netdev_alloc_skb(NULL, length, gfp_mask);\n}\n\n/* legacy helper around netdev_alloc_skb() */\nstatic inline struct sk_buff *dev_alloc_skb(unsigned int length)\n{\n\treturn netdev_alloc_skb(NULL, length);\n}\n\n\nstatic inline struct sk_buff *__netdev_alloc_skb_ip_align(struct net_device *dev,\n\t\tunsigned int length, gfp_t gfp)\n{\n\tstruct sk_buff *skb = __netdev_alloc_skb(dev, length + NET_IP_ALIGN, gfp);\n\n\tif (NET_IP_ALIGN && skb)\n\t\tskb_reserve(skb, NET_IP_ALIGN);\n\treturn skb;\n}\n\nstatic inline struct sk_buff *netdev_alloc_skb_ip_align(struct net_device *dev,\n\t\tunsigned int length)\n{\n\treturn __netdev_alloc_skb_ip_align(dev, length, GFP_ATOMIC);\n}\n\nstatic inline void skb_free_frag(void *addr)\n{\n\tpage_frag_free(addr);\n}\n\nvoid *__napi_alloc_frag_align(unsigned int fragsz, unsigned int align_mask);\n\nstatic inline void *napi_alloc_frag(unsigned int fragsz)\n{\n\treturn __napi_alloc_frag_align(fragsz, ~0u);\n}\n\nstatic inline void *napi_alloc_frag_align(unsigned int fragsz,\n\t\t\t\t\t  unsigned int align)\n{\n\tWARN_ON_ONCE(!is_power_of_2(align));\n\treturn __napi_alloc_frag_align(fragsz, -align);\n}\n\nstruct sk_buff *__napi_alloc_skb(struct napi_struct *napi,\n\t\t\t\t unsigned int length, gfp_t gfp_mask);\nstatic inline struct sk_buff *napi_alloc_skb(struct napi_struct *napi,\n\t\t\t\t\t     unsigned int length)\n{\n\treturn __napi_alloc_skb(napi, length, GFP_ATOMIC);\n}\nvoid napi_consume_skb(struct sk_buff *skb, int budget);\n\nvoid napi_skb_free_stolen_head(struct sk_buff *skb);\nvoid __kfree_skb_defer(struct sk_buff *skb);\n\n/**\n * __dev_alloc_pages - allocate page for network Rx\n * @gfp_mask: allocation priority. Set __GFP_NOMEMALLOC if not for network Rx\n * @order: size of the allocation\n *\n * Allocate a new page.\n *\n * %NULL is returned if there is no free memory.\n*/\nstatic inline struct page *__dev_alloc_pages(gfp_t gfp_mask,\n\t\t\t\t\t     unsigned int order)\n{\n\t/* This piece of code contains several assumptions.\n\t * 1.  This is for device Rx, therefor a cold page is preferred.\n\t * 2.  The expectation is the user wants a compound page.\n\t * 3.  If requesting a order 0 page it will not be compound\n\t *     due to the check to see if order has a value in prep_new_page\n\t * 4.  __GFP_MEMALLOC is ignored if __GFP_NOMEMALLOC is set due to\n\t *     code in gfp_to_alloc_flags that should be enforcing this.\n\t */\n\tgfp_mask |= __GFP_COMP | __GFP_MEMALLOC;\n\n\treturn alloc_pages_node(NUMA_NO_NODE, gfp_mask, order);\n}\n\nstatic inline struct page *dev_alloc_pages(unsigned int order)\n{\n\treturn __dev_alloc_pages(GFP_ATOMIC | __GFP_NOWARN, order);\n}\n\n/**\n * __dev_alloc_page - allocate a page for network Rx\n * @gfp_mask: allocation priority. Set __GFP_NOMEMALLOC if not for network Rx\n *\n * Allocate a new page.\n *\n * %NULL is returned if there is no free memory.\n */\nstatic inline struct page *__dev_alloc_page(gfp_t gfp_mask)\n{\n\treturn __dev_alloc_pages(gfp_mask, 0);\n}\n\nstatic inline struct page *dev_alloc_page(void)\n{\n\treturn dev_alloc_pages(0);\n}\n\n/**\n * dev_page_is_reusable - check whether a page can be reused for network Rx\n * @page: the page to test\n *\n * A page shouldn't be considered for reusing/recycling if it was allocated\n * under memory pressure or at a distant memory node.\n *\n * Returns false if this page should be returned to page allocator, true\n * otherwise.\n */\nstatic inline bool dev_page_is_reusable(const struct page *page)\n{\n\treturn likely(page_to_nid(page) == numa_mem_id() &&\n\t\t      !page_is_pfmemalloc(page));\n}\n\n/**\n *\tskb_propagate_pfmemalloc - Propagate pfmemalloc if skb is allocated after RX page\n *\t@page: The page that was allocated from skb_alloc_page\n *\t@skb: The skb that may need pfmemalloc set\n */\nstatic inline void skb_propagate_pfmemalloc(const struct page *page,\n\t\t\t\t\t    struct sk_buff *skb)\n{\n\tif (page_is_pfmemalloc(page))\n\t\tskb->pfmemalloc = true;\n}\n\n/**\n * skb_frag_off() - Returns the offset of a skb fragment\n * @frag: the paged fragment\n */\nstatic inline unsigned int skb_frag_off(const skb_frag_t *frag)\n{\n\treturn frag->bv_offset;\n}\n\n/**\n * skb_frag_off_add() - Increments the offset of a skb fragment by @delta\n * @frag: skb fragment\n * @delta: value to add\n */\nstatic inline void skb_frag_off_add(skb_frag_t *frag, int delta)\n{\n\tfrag->bv_offset += delta;\n}\n\n/**\n * skb_frag_off_set() - Sets the offset of a skb fragment\n * @frag: skb fragment\n * @offset: offset of fragment\n */\nstatic inline void skb_frag_off_set(skb_frag_t *frag, unsigned int offset)\n{\n\tfrag->bv_offset = offset;\n}\n\n/**\n * skb_frag_off_copy() - Sets the offset of a skb fragment from another fragment\n * @fragto: skb fragment where offset is set\n * @fragfrom: skb fragment offset is copied from\n */\nstatic inline void skb_frag_off_copy(skb_frag_t *fragto,\n\t\t\t\t     const skb_frag_t *fragfrom)\n{\n\tfragto->bv_offset = fragfrom->bv_offset;\n}\n\n/**\n * skb_frag_page - retrieve the page referred to by a paged fragment\n * @frag: the paged fragment\n *\n * Returns the &struct page associated with @frag.\n */\nstatic inline struct page *skb_frag_page(const skb_frag_t *frag)\n{\n\treturn frag->bv_page;\n}\n\n/**\n * __skb_frag_ref - take an addition reference on a paged fragment.\n * @frag: the paged fragment\n *\n * Takes an additional reference on the paged fragment @frag.\n */\nstatic inline void __skb_frag_ref(skb_frag_t *frag)\n{\n\tget_page(skb_frag_page(frag));\n}\n\n/**\n * skb_frag_ref - take an addition reference on a paged fragment of an skb.\n * @skb: the buffer\n * @f: the fragment offset.\n *\n * Takes an additional reference on the @f'th paged fragment of @skb.\n */\nstatic inline void skb_frag_ref(struct sk_buff *skb, int f)\n{\n\t__skb_frag_ref(&skb_shinfo(skb)->frags[f]);\n}\n\n/**\n * __skb_frag_unref - release a reference on a paged fragment.\n * @frag: the paged fragment\n *\n * Releases a reference on the paged fragment @frag.\n */\nstatic inline void __skb_frag_unref(skb_frag_t *frag)\n{\n\tput_page(skb_frag_page(frag));\n}\n\n/**\n * skb_frag_unref - release a reference on a paged fragment of an skb.\n * @skb: the buffer\n * @f: the fragment offset\n *\n * Releases a reference on the @f'th paged fragment of @skb.\n */\nstatic inline void skb_frag_unref(struct sk_buff *skb, int f)\n{\n\t__skb_frag_unref(&skb_shinfo(skb)->frags[f]);\n}\n\n/**\n * skb_frag_address - gets the address of the data contained in a paged fragment\n * @frag: the paged fragment buffer\n *\n * Returns the address of the data within @frag. The page must already\n * be mapped.\n */\nstatic inline void *skb_frag_address(const skb_frag_t *frag)\n{\n\treturn page_address(skb_frag_page(frag)) + skb_frag_off(frag);\n}\n\n/**\n * skb_frag_address_safe - gets the address of the data contained in a paged fragment\n * @frag: the paged fragment buffer\n *\n * Returns the address of the data within @frag. Checks that the page\n * is mapped and returns %NULL otherwise.\n */\nstatic inline void *skb_frag_address_safe(const skb_frag_t *frag)\n{\n\tvoid *ptr = page_address(skb_frag_page(frag));\n\tif (unlikely(!ptr))\n\t\treturn NULL;\n\n\treturn ptr + skb_frag_off(frag);\n}\n\n/**\n * skb_frag_page_copy() - sets the page in a fragment from another fragment\n * @fragto: skb fragment where page is set\n * @fragfrom: skb fragment page is copied from\n */\nstatic inline void skb_frag_page_copy(skb_frag_t *fragto,\n\t\t\t\t      const skb_frag_t *fragfrom)\n{\n\tfragto->bv_page = fragfrom->bv_page;\n}\n\n/**\n * __skb_frag_set_page - sets the page contained in a paged fragment\n * @frag: the paged fragment\n * @page: the page to set\n *\n * Sets the fragment @frag to contain @page.\n */\nstatic inline void __skb_frag_set_page(skb_frag_t *frag, struct page *page)\n{\n\tfrag->bv_page = page;\n}\n\n/**\n * skb_frag_set_page - sets the page contained in a paged fragment of an skb\n * @skb: the buffer\n * @f: the fragment offset\n * @page: the page to set\n *\n * Sets the @f'th fragment of @skb to contain @page.\n */\nstatic inline void skb_frag_set_page(struct sk_buff *skb, int f,\n\t\t\t\t     struct page *page)\n{\n\t__skb_frag_set_page(&skb_shinfo(skb)->frags[f], page);\n}\n\nbool skb_page_frag_refill(unsigned int sz, struct page_frag *pfrag, gfp_t prio);\n\n/**\n * skb_frag_dma_map - maps a paged fragment via the DMA API\n * @dev: the device to map the fragment to\n * @frag: the paged fragment to map\n * @offset: the offset within the fragment (starting at the\n *          fragment's own offset)\n * @size: the number of bytes to map\n * @dir: the direction of the mapping (``PCI_DMA_*``)\n *\n * Maps the page associated with @frag to @device.\n */\nstatic inline dma_addr_t skb_frag_dma_map(struct device *dev,\n\t\t\t\t\t  const skb_frag_t *frag,\n\t\t\t\t\t  size_t offset, size_t size,\n\t\t\t\t\t  enum dma_data_direction dir)\n{\n\treturn dma_map_page(dev, skb_frag_page(frag),\n\t\t\t    skb_frag_off(frag) + offset, size, dir);\n}\n\nstatic inline struct sk_buff *pskb_copy(struct sk_buff *skb,\n\t\t\t\t\tgfp_t gfp_mask)\n{\n\treturn __pskb_copy(skb, skb_headroom(skb), gfp_mask);\n}\n\n\nstatic inline struct sk_buff *pskb_copy_for_clone(struct sk_buff *skb,\n\t\t\t\t\t\t  gfp_t gfp_mask)\n{\n\treturn __pskb_copy_fclone(skb, skb_headroom(skb), gfp_mask, true);\n}\n\n\n/**\n *\tskb_clone_writable - is the header of a clone writable\n *\t@skb: buffer to check\n *\t@len: length up to which to write\n *\n *\tReturns true if modifying the header part of the cloned buffer\n *\tdoes not requires the data to be copied.\n */\nstatic inline int skb_clone_writable(const struct sk_buff *skb, unsigned int len)\n{\n\treturn !skb_header_cloned(skb) &&\n\t       skb_headroom(skb) + len <= skb->hdr_len;\n}\n\nstatic inline int skb_try_make_writable(struct sk_buff *skb,\n\t\t\t\t\tunsigned int write_len)\n{\n\treturn skb_cloned(skb) && !skb_clone_writable(skb, write_len) &&\n\t       pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n}\n\nstatic inline int __skb_cow(struct sk_buff *skb, unsigned int headroom,\n\t\t\t    int cloned)\n{\n\tint delta = 0;\n\n\tif (headroom > skb_headroom(skb))\n\t\tdelta = headroom - skb_headroom(skb);\n\n\tif (delta || cloned)\n\t\treturn pskb_expand_head(skb, ALIGN(delta, NET_SKB_PAD), 0,\n\t\t\t\t\tGFP_ATOMIC);\n\treturn 0;\n}\n\n/**\n *\tskb_cow - copy header of skb when it is required\n *\t@skb: buffer to cow\n *\t@headroom: needed headroom\n *\n *\tIf the skb passed lacks sufficient headroom or its data part\n *\tis shared, data is reallocated. If reallocation fails, an error\n *\tis returned and original skb is not changed.\n *\n *\tThe result is skb with writable area skb->head...skb->tail\n *\tand at least @headroom of space at head.\n */\nstatic inline int skb_cow(struct sk_buff *skb, unsigned int headroom)\n{\n\treturn __skb_cow(skb, headroom, skb_cloned(skb));\n}\n\n/**\n *\tskb_cow_head - skb_cow but only making the head writable\n *\t@skb: buffer to cow\n *\t@headroom: needed headroom\n *\n *\tThis function is identical to skb_cow except that we replace the\n *\tskb_cloned check by skb_header_cloned.  It should be used when\n *\tyou only need to push on some header and do not need to modify\n *\tthe data.\n */\nstatic inline int skb_cow_head(struct sk_buff *skb, unsigned int headroom)\n{\n\treturn __skb_cow(skb, headroom, skb_header_cloned(skb));\n}\n\n/**\n *\tskb_padto\t- pad an skbuff up to a minimal size\n *\t@skb: buffer to pad\n *\t@len: minimal length\n *\n *\tPads up a buffer to ensure the trailing bytes exist and are\n *\tblanked. If the buffer already contains sufficient data it\n *\tis untouched. Otherwise it is extended. Returns zero on\n *\tsuccess. The skb is freed on error.\n */\nstatic inline int skb_padto(struct sk_buff *skb, unsigned int len)\n{\n\tunsigned int size = skb->len;\n\tif (likely(size >= len))\n\t\treturn 0;\n\treturn skb_pad(skb, len - size);\n}\n\n/**\n *\t__skb_put_padto - increase size and pad an skbuff up to a minimal size\n *\t@skb: buffer to pad\n *\t@len: minimal length\n *\t@free_on_error: free buffer on error\n *\n *\tPads up a buffer to ensure the trailing bytes exist and are\n *\tblanked. If the buffer already contains sufficient data it\n *\tis untouched. Otherwise it is extended. Returns zero on\n *\tsuccess. The skb is freed on error if @free_on_error is true.\n */\nstatic inline int __must_check __skb_put_padto(struct sk_buff *skb,\n\t\t\t\t\t       unsigned int len,\n\t\t\t\t\t       bool free_on_error)\n{\n\tunsigned int size = skb->len;\n\n\tif (unlikely(size < len)) {\n\t\tlen -= size;\n\t\tif (__skb_pad(skb, len, free_on_error))\n\t\t\treturn -ENOMEM;\n\t\t__skb_put(skb, len);\n\t}\n\treturn 0;\n}\n\n/**\n *\tskb_put_padto - increase size and pad an skbuff up to a minimal size\n *\t@skb: buffer to pad\n *\t@len: minimal length\n *\n *\tPads up a buffer to ensure the trailing bytes exist and are\n *\tblanked. If the buffer already contains sufficient data it\n *\tis untouched. Otherwise it is extended. Returns zero on\n *\tsuccess. The skb is freed on error.\n */\nstatic inline int __must_check skb_put_padto(struct sk_buff *skb, unsigned int len)\n{\n\treturn __skb_put_padto(skb, len, true);\n}\n\nstatic inline int skb_add_data(struct sk_buff *skb,\n\t\t\t       struct iov_iter *from, int copy)\n{\n\tconst int off = skb->len;\n\n\tif (skb->ip_summed == CHECKSUM_NONE) {\n\t\t__wsum csum = 0;\n\t\tif (csum_and_copy_from_iter_full(skb_put(skb, copy), copy,\n\t\t\t\t\t         &csum, from)) {\n\t\t\tskb->csum = csum_block_add(skb->csum, csum, off);\n\t\t\treturn 0;\n\t\t}\n\t} else if (copy_from_iter_full(skb_put(skb, copy), copy, from))\n\t\treturn 0;\n\n\t__skb_trim(skb, off);\n\treturn -EFAULT;\n}\n\nstatic inline bool skb_can_coalesce(struct sk_buff *skb, int i,\n\t\t\t\t    const struct page *page, int off)\n{\n\tif (skb_zcopy(skb))\n\t\treturn false;\n\tif (i) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i - 1];\n\n\t\treturn page == skb_frag_page(frag) &&\n\t\t       off == skb_frag_off(frag) + skb_frag_size(frag);\n\t}\n\treturn false;\n}\n\nstatic inline int __skb_linearize(struct sk_buff *skb)\n{\n\treturn __pskb_pull_tail(skb, skb->data_len) ? 0 : -ENOMEM;\n}\n\n/**\n *\tskb_linearize - convert paged skb to linear one\n *\t@skb: buffer to linarize\n *\n *\tIf there is no free memory -ENOMEM is returned, otherwise zero\n *\tis returned and the old skb data released.\n */\nstatic inline int skb_linearize(struct sk_buff *skb)\n{\n\treturn skb_is_nonlinear(skb) ? __skb_linearize(skb) : 0;\n}\n\n/**\n * skb_has_shared_frag - can any frag be overwritten\n * @skb: buffer to test\n *\n * Return true if the skb has at least one frag that might be modified\n * by an external entity (as in vmsplice()/sendfile())\n */\nstatic inline bool skb_has_shared_frag(const struct sk_buff *skb)\n{\n\treturn skb_is_nonlinear(skb) &&\n\t       skb_shinfo(skb)->flags & SKBFL_SHARED_FRAG;\n}\n\n/**\n *\tskb_linearize_cow - make sure skb is linear and writable\n *\t@skb: buffer to process\n *\n *\tIf there is no free memory -ENOMEM is returned, otherwise zero\n *\tis returned and the old skb data released.\n */\nstatic inline int skb_linearize_cow(struct sk_buff *skb)\n{\n\treturn skb_is_nonlinear(skb) || skb_cloned(skb) ?\n\t       __skb_linearize(skb) : 0;\n}\n\nstatic __always_inline void\n__skb_postpull_rcsum(struct sk_buff *skb, const void *start, unsigned int len,\n\t\t     unsigned int off)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->csum = csum_block_sub(skb->csum,\n\t\t\t\t\t   csum_partial(start, len, 0), off);\n\telse if (skb->ip_summed == CHECKSUM_PARTIAL &&\n\t\t skb_checksum_start_offset(skb) < 0)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n}\n\n/**\n *\tskb_postpull_rcsum - update checksum for received skb after pull\n *\t@skb: buffer to update\n *\t@start: start of data before pull\n *\t@len: length of data pulled\n *\n *\tAfter doing a pull on a received packet, you need to call this to\n *\tupdate the CHECKSUM_COMPLETE checksum, or set ip_summed to\n *\tCHECKSUM_NONE so that it can be recomputed from scratch.\n */\nstatic inline void skb_postpull_rcsum(struct sk_buff *skb,\n\t\t\t\t      const void *start, unsigned int len)\n{\n\t__skb_postpull_rcsum(skb, start, len, 0);\n}\n\nstatic __always_inline void\n__skb_postpush_rcsum(struct sk_buff *skb, const void *start, unsigned int len,\n\t\t     unsigned int off)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->csum = csum_block_add(skb->csum,\n\t\t\t\t\t   csum_partial(start, len, 0), off);\n}\n\n/**\n *\tskb_postpush_rcsum - update checksum for received skb after push\n *\t@skb: buffer to update\n *\t@start: start of data after push\n *\t@len: length of data pushed\n *\n *\tAfter doing a push on a received packet, you need to call this to\n *\tupdate the CHECKSUM_COMPLETE checksum.\n */\nstatic inline void skb_postpush_rcsum(struct sk_buff *skb,\n\t\t\t\t      const void *start, unsigned int len)\n{\n\t__skb_postpush_rcsum(skb, start, len, 0);\n}\n\nvoid *skb_pull_rcsum(struct sk_buff *skb, unsigned int len);\n\n/**\n *\tskb_push_rcsum - push skb and update receive checksum\n *\t@skb: buffer to update\n *\t@len: length of data pulled\n *\n *\tThis function performs an skb_push on the packet and updates\n *\tthe CHECKSUM_COMPLETE checksum.  It should be used on\n *\treceive path processing instead of skb_push unless you know\n *\tthat the checksum difference is zero (e.g., a valid IP header)\n *\tor you are setting ip_summed to CHECKSUM_NONE.\n */\nstatic inline void *skb_push_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tskb_push(skb, len);\n\tskb_postpush_rcsum(skb, skb->data, len);\n\treturn skb->data;\n}\n\nint pskb_trim_rcsum_slow(struct sk_buff *skb, unsigned int len);\n/**\n *\tpskb_trim_rcsum - trim received skb and update checksum\n *\t@skb: buffer to trim\n *\t@len: new length\n *\n *\tThis is exactly the same as pskb_trim except that it ensures the\n *\tchecksum of received packets are still valid after the operation.\n *\tIt can change skb pointers.\n */\n\nstatic inline int pskb_trim_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tif (likely(len >= skb->len))\n\t\treturn 0;\n\treturn pskb_trim_rcsum_slow(skb, len);\n}\n\nstatic inline int __skb_trim_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t__skb_trim(skb, len);\n\treturn 0;\n}\n\nstatic inline int __skb_grow_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\treturn __skb_grow(skb, len);\n}\n\n#define rb_to_skb(rb) rb_entry_safe(rb, struct sk_buff, rbnode)\n#define skb_rb_first(root) rb_to_skb(rb_first(root))\n#define skb_rb_last(root)  rb_to_skb(rb_last(root))\n#define skb_rb_next(skb)   rb_to_skb(rb_next(&(skb)->rbnode))\n#define skb_rb_prev(skb)   rb_to_skb(rb_prev(&(skb)->rbnode))\n\n#define skb_queue_walk(queue, skb) \\\n\t\tfor (skb = (queue)->next;\t\t\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = skb->next)\n\n#define skb_queue_walk_safe(queue, skb, tmp)\t\t\t\t\t\\\n\t\tfor (skb = (queue)->next, tmp = skb->next;\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = tmp, tmp = skb->next)\n\n#define skb_queue_walk_from(queue, skb)\t\t\t\t\t\t\\\n\t\tfor (; skb != (struct sk_buff *)(queue);\t\t\t\\\n\t\t     skb = skb->next)\n\n#define skb_rbtree_walk(skb, root)\t\t\t\t\t\t\\\n\t\tfor (skb = skb_rb_first(root); skb != NULL;\t\t\t\\\n\t\t     skb = skb_rb_next(skb))\n\n#define skb_rbtree_walk_from(skb)\t\t\t\t\t\t\\\n\t\tfor (; skb != NULL;\t\t\t\t\t\t\\\n\t\t     skb = skb_rb_next(skb))\n\n#define skb_rbtree_walk_from_safe(skb, tmp)\t\t\t\t\t\\\n\t\tfor (; tmp = skb ? skb_rb_next(skb) : NULL, (skb != NULL);\t\\\n\t\t     skb = tmp)\n\n#define skb_queue_walk_from_safe(queue, skb, tmp)\t\t\t\t\\\n\t\tfor (tmp = skb->next;\t\t\t\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = tmp, tmp = skb->next)\n\n#define skb_queue_reverse_walk(queue, skb) \\\n\t\tfor (skb = (queue)->prev;\t\t\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = skb->prev)\n\n#define skb_queue_reverse_walk_safe(queue, skb, tmp)\t\t\t\t\\\n\t\tfor (skb = (queue)->prev, tmp = skb->prev;\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = tmp, tmp = skb->prev)\n\n#define skb_queue_reverse_walk_from_safe(queue, skb, tmp)\t\t\t\\\n\t\tfor (tmp = skb->prev;\t\t\t\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = tmp, tmp = skb->prev)\n\nstatic inline bool skb_has_frag_list(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->frag_list != NULL;\n}\n\nstatic inline void skb_frag_list_init(struct sk_buff *skb)\n{\n\tskb_shinfo(skb)->frag_list = NULL;\n}\n\n#define skb_walk_frags(skb, iter)\t\\\n\tfor (iter = skb_shinfo(skb)->frag_list; iter; iter = iter->next)\n\n\nint __skb_wait_for_more_packets(struct sock *sk, struct sk_buff_head *queue,\n\t\t\t\tint *err, long *timeo_p,\n\t\t\t\tconst struct sk_buff *skb);\nstruct sk_buff *__skb_try_recv_from_queue(struct sock *sk,\n\t\t\t\t\t  struct sk_buff_head *queue,\n\t\t\t\t\t  unsigned int flags,\n\t\t\t\t\t  int *off, int *err,\n\t\t\t\t\t  struct sk_buff **last);\nstruct sk_buff *__skb_try_recv_datagram(struct sock *sk,\n\t\t\t\t\tstruct sk_buff_head *queue,\n\t\t\t\t\tunsigned int flags, int *off, int *err,\n\t\t\t\t\tstruct sk_buff **last);\nstruct sk_buff *__skb_recv_datagram(struct sock *sk,\n\t\t\t\t    struct sk_buff_head *sk_queue,\n\t\t\t\t    unsigned int flags, int *off, int *err);\nstruct sk_buff *skb_recv_datagram(struct sock *sk, unsigned flags, int noblock,\n\t\t\t\t  int *err);\n__poll_t datagram_poll(struct file *file, struct socket *sock,\n\t\t\t   struct poll_table_struct *wait);\nint skb_copy_datagram_iter(const struct sk_buff *from, int offset,\n\t\t\t   struct iov_iter *to, int size);\nstatic inline int skb_copy_datagram_msg(const struct sk_buff *from, int offset,\n\t\t\t\t\tstruct msghdr *msg, int size)\n{\n\treturn skb_copy_datagram_iter(from, offset, &msg->msg_iter, size);\n}\nint skb_copy_and_csum_datagram_msg(struct sk_buff *skb, int hlen,\n\t\t\t\t   struct msghdr *msg);\nint skb_copy_and_hash_datagram_iter(const struct sk_buff *skb, int offset,\n\t\t\t   struct iov_iter *to, int len,\n\t\t\t   struct ahash_request *hash);\nint skb_copy_datagram_from_iter(struct sk_buff *skb, int offset,\n\t\t\t\t struct iov_iter *from, int len);\nint zerocopy_sg_from_iter(struct sk_buff *skb, struct iov_iter *frm);\nvoid skb_free_datagram(struct sock *sk, struct sk_buff *skb);\nvoid __skb_free_datagram_locked(struct sock *sk, struct sk_buff *skb, int len);\nstatic inline void skb_free_datagram_locked(struct sock *sk,\n\t\t\t\t\t    struct sk_buff *skb)\n{\n\t__skb_free_datagram_locked(sk, skb, 0);\n}\nint skb_kill_datagram(struct sock *sk, struct sk_buff *skb, unsigned int flags);\nint skb_copy_bits(const struct sk_buff *skb, int offset, void *to, int len);\nint skb_store_bits(struct sk_buff *skb, int offset, const void *from, int len);\n__wsum skb_copy_and_csum_bits(const struct sk_buff *skb, int offset, u8 *to,\n\t\t\t      int len);\nint skb_splice_bits(struct sk_buff *skb, struct sock *sk, unsigned int offset,\n\t\t    struct pipe_inode_info *pipe, unsigned int len,\n\t\t    unsigned int flags);\nint skb_send_sock_locked(struct sock *sk, struct sk_buff *skb, int offset,\n\t\t\t int len);\nint skb_send_sock(struct sock *sk, struct sk_buff *skb, int offset, int len);\nvoid skb_copy_and_csum_dev(const struct sk_buff *skb, u8 *to);\nunsigned int skb_zerocopy_headlen(const struct sk_buff *from);\nint skb_zerocopy(struct sk_buff *to, struct sk_buff *from,\n\t\t int len, int hlen);\nvoid skb_split(struct sk_buff *skb, struct sk_buff *skb1, const u32 len);\nint skb_shift(struct sk_buff *tgt, struct sk_buff *skb, int shiftlen);\nvoid skb_scrub_packet(struct sk_buff *skb, bool xnet);\nbool skb_gso_validate_network_len(const struct sk_buff *skb, unsigned int mtu);\nbool skb_gso_validate_mac_len(const struct sk_buff *skb, unsigned int len);\nstruct sk_buff *skb_segment(struct sk_buff *skb, netdev_features_t features);\nstruct sk_buff *skb_segment_list(struct sk_buff *skb, netdev_features_t features,\n\t\t\t\t unsigned int offset);\nstruct sk_buff *skb_vlan_untag(struct sk_buff *skb);\nint skb_ensure_writable(struct sk_buff *skb, int write_len);\nint __skb_vlan_pop(struct sk_buff *skb, u16 *vlan_tci);\nint skb_vlan_pop(struct sk_buff *skb);\nint skb_vlan_push(struct sk_buff *skb, __be16 vlan_proto, u16 vlan_tci);\nint skb_eth_pop(struct sk_buff *skb);\nint skb_eth_push(struct sk_buff *skb, const unsigned char *dst,\n\t\t const unsigned char *src);\nint skb_mpls_push(struct sk_buff *skb, __be32 mpls_lse, __be16 mpls_proto,\n\t\t  int mac_len, bool ethernet);\nint skb_mpls_pop(struct sk_buff *skb, __be16 next_proto, int mac_len,\n\t\t bool ethernet);\nint skb_mpls_update_lse(struct sk_buff *skb, __be32 mpls_lse);\nint skb_mpls_dec_ttl(struct sk_buff *skb);\nstruct sk_buff *pskb_extract(struct sk_buff *skb, int off, int to_copy,\n\t\t\t     gfp_t gfp);\n\nstatic inline int memcpy_from_msg(void *data, struct msghdr *msg, int len)\n{\n\treturn copy_from_iter_full(data, len, &msg->msg_iter) ? 0 : -EFAULT;\n}\n\nstatic inline int memcpy_to_msg(struct msghdr *msg, void *data, int len)\n{\n\treturn copy_to_iter(data, len, &msg->msg_iter) == len ? 0 : -EFAULT;\n}\n\nstruct skb_checksum_ops {\n\t__wsum (*update)(const void *mem, int len, __wsum wsum);\n\t__wsum (*combine)(__wsum csum, __wsum csum2, int offset, int len);\n};\n\nextern const struct skb_checksum_ops *crc32c_csum_stub __read_mostly;\n\n__wsum __skb_checksum(const struct sk_buff *skb, int offset, int len,\n\t\t      __wsum csum, const struct skb_checksum_ops *ops);\n__wsum skb_checksum(const struct sk_buff *skb, int offset, int len,\n\t\t    __wsum csum);\n\nstatic inline void * __must_check\n__skb_header_pointer(const struct sk_buff *skb, int offset, int len,\n\t\t     const void *data, int hlen, void *buffer)\n{\n\tif (likely(hlen - offset >= len))\n\t\treturn (void *)data + offset;\n\n\tif (!skb || unlikely(skb_copy_bits(skb, offset, buffer, len) < 0))\n\t\treturn NULL;\n\n\treturn buffer;\n}\n\nstatic inline void * __must_check\nskb_header_pointer(const struct sk_buff *skb, int offset, int len, void *buffer)\n{\n\treturn __skb_header_pointer(skb, offset, len, skb->data,\n\t\t\t\t    skb_headlen(skb), buffer);\n}\n\n/**\n *\tskb_needs_linearize - check if we need to linearize a given skb\n *\t\t\t      depending on the given device features.\n *\t@skb: socket buffer to check\n *\t@features: net device features\n *\n *\tReturns true if either:\n *\t1. skb has frag_list and the device doesn't support FRAGLIST, or\n *\t2. skb is fragmented and the device does not support SG.\n */\nstatic inline bool skb_needs_linearize(struct sk_buff *skb,\n\t\t\t\t       netdev_features_t features)\n{\n\treturn skb_is_nonlinear(skb) &&\n\t       ((skb_has_frag_list(skb) && !(features & NETIF_F_FRAGLIST)) ||\n\t\t(skb_shinfo(skb)->nr_frags && !(features & NETIF_F_SG)));\n}\n\nstatic inline void skb_copy_from_linear_data(const struct sk_buff *skb,\n\t\t\t\t\t     void *to,\n\t\t\t\t\t     const unsigned int len)\n{\n\tmemcpy(to, skb->data, len);\n}\n\nstatic inline void skb_copy_from_linear_data_offset(const struct sk_buff *skb,\n\t\t\t\t\t\t    const int offset, void *to,\n\t\t\t\t\t\t    const unsigned int len)\n{\n\tmemcpy(to, skb->data + offset, len);\n}\n\nstatic inline void skb_copy_to_linear_data(struct sk_buff *skb,\n\t\t\t\t\t   const void *from,\n\t\t\t\t\t   const unsigned int len)\n{\n\tmemcpy(skb->data, from, len);\n}\n\nstatic inline void skb_copy_to_linear_data_offset(struct sk_buff *skb,\n\t\t\t\t\t\t  const int offset,\n\t\t\t\t\t\t  const void *from,\n\t\t\t\t\t\t  const unsigned int len)\n{\n\tmemcpy(skb->data + offset, from, len);\n}\n\nvoid skb_init(void);\n\nstatic inline ktime_t skb_get_ktime(const struct sk_buff *skb)\n{\n\treturn skb->tstamp;\n}\n\n/**\n *\tskb_get_timestamp - get timestamp from a skb\n *\t@skb: skb to get stamp from\n *\t@stamp: pointer to struct __kernel_old_timeval to store stamp in\n *\n *\tTimestamps are stored in the skb as offsets to a base timestamp.\n *\tThis function converts the offset back to a struct timeval and stores\n *\tit in stamp.\n */\nstatic inline void skb_get_timestamp(const struct sk_buff *skb,\n\t\t\t\t     struct __kernel_old_timeval *stamp)\n{\n\t*stamp = ns_to_kernel_old_timeval(skb->tstamp);\n}\n\nstatic inline void skb_get_new_timestamp(const struct sk_buff *skb,\n\t\t\t\t\t struct __kernel_sock_timeval *stamp)\n{\n\tstruct timespec64 ts = ktime_to_timespec64(skb->tstamp);\n\n\tstamp->tv_sec = ts.tv_sec;\n\tstamp->tv_usec = ts.tv_nsec / 1000;\n}\n\nstatic inline void skb_get_timestampns(const struct sk_buff *skb,\n\t\t\t\t       struct __kernel_old_timespec *stamp)\n{\n\tstruct timespec64 ts = ktime_to_timespec64(skb->tstamp);\n\n\tstamp->tv_sec = ts.tv_sec;\n\tstamp->tv_nsec = ts.tv_nsec;\n}\n\nstatic inline void skb_get_new_timestampns(const struct sk_buff *skb,\n\t\t\t\t\t   struct __kernel_timespec *stamp)\n{\n\tstruct timespec64 ts = ktime_to_timespec64(skb->tstamp);\n\n\tstamp->tv_sec = ts.tv_sec;\n\tstamp->tv_nsec = ts.tv_nsec;\n}\n\nstatic inline void __net_timestamp(struct sk_buff *skb)\n{\n\tskb->tstamp = ktime_get_real();\n}\n\nstatic inline ktime_t net_timedelta(ktime_t t)\n{\n\treturn ktime_sub(ktime_get_real(), t);\n}\n\nstatic inline ktime_t net_invalid_timestamp(void)\n{\n\treturn 0;\n}\n\nstatic inline u8 skb_metadata_len(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->meta_len;\n}\n\nstatic inline void *skb_metadata_end(const struct sk_buff *skb)\n{\n\treturn skb_mac_header(skb);\n}\n\nstatic inline bool __skb_metadata_differs(const struct sk_buff *skb_a,\n\t\t\t\t\t  const struct sk_buff *skb_b,\n\t\t\t\t\t  u8 meta_len)\n{\n\tconst void *a = skb_metadata_end(skb_a);\n\tconst void *b = skb_metadata_end(skb_b);\n\t/* Using more efficient varaiant than plain call to memcmp(). */\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n\tu64 diffs = 0;\n\n\tswitch (meta_len) {\n#define __it(x, op) (x -= sizeof(u##op))\n#define __it_diff(a, b, op) (*(u##op *)__it(a, op)) ^ (*(u##op *)__it(b, op))\n\tcase 32: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase 24: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase 16: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase  8: diffs |= __it_diff(a, b, 64);\n\t\tbreak;\n\tcase 28: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase 20: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase 12: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase  4: diffs |= __it_diff(a, b, 32);\n\t\tbreak;\n\t}\n\treturn diffs;\n#else\n\treturn memcmp(a - meta_len, b - meta_len, meta_len);\n#endif\n}\n\nstatic inline bool skb_metadata_differs(const struct sk_buff *skb_a,\n\t\t\t\t\tconst struct sk_buff *skb_b)\n{\n\tu8 len_a = skb_metadata_len(skb_a);\n\tu8 len_b = skb_metadata_len(skb_b);\n\n\tif (!(len_a | len_b))\n\t\treturn false;\n\n\treturn len_a != len_b ?\n\t       true : __skb_metadata_differs(skb_a, skb_b, len_a);\n}\n\nstatic inline void skb_metadata_set(struct sk_buff *skb, u8 meta_len)\n{\n\tskb_shinfo(skb)->meta_len = meta_len;\n}\n\nstatic inline void skb_metadata_clear(struct sk_buff *skb)\n{\n\tskb_metadata_set(skb, 0);\n}\n\nstruct sk_buff *skb_clone_sk(struct sk_buff *skb);\n\n#ifdef CONFIG_NETWORK_PHY_TIMESTAMPING\n\nvoid skb_clone_tx_timestamp(struct sk_buff *skb);\nbool skb_defer_rx_timestamp(struct sk_buff *skb);\n\n#else /* CONFIG_NETWORK_PHY_TIMESTAMPING */\n\nstatic inline void skb_clone_tx_timestamp(struct sk_buff *skb)\n{\n}\n\nstatic inline bool skb_defer_rx_timestamp(struct sk_buff *skb)\n{\n\treturn false;\n}\n\n#endif /* !CONFIG_NETWORK_PHY_TIMESTAMPING */\n\n/**\n * skb_complete_tx_timestamp() - deliver cloned skb with tx timestamps\n *\n * PHY drivers may accept clones of transmitted packets for\n * timestamping via their phy_driver.txtstamp method. These drivers\n * must call this function to return the skb back to the stack with a\n * timestamp.\n *\n * @skb: clone of the original outgoing packet\n * @hwtstamps: hardware time stamps\n *\n */\nvoid skb_complete_tx_timestamp(struct sk_buff *skb,\n\t\t\t       struct skb_shared_hwtstamps *hwtstamps);\n\nvoid __skb_tstamp_tx(struct sk_buff *orig_skb, const struct sk_buff *ack_skb,\n\t\t     struct skb_shared_hwtstamps *hwtstamps,\n\t\t     struct sock *sk, int tstype);\n\n/**\n * skb_tstamp_tx - queue clone of skb with send time stamps\n * @orig_skb:\tthe original outgoing packet\n * @hwtstamps:\thardware time stamps, may be NULL if not available\n *\n * If the skb has a socket associated, then this function clones the\n * skb (thus sharing the actual data and optional structures), stores\n * the optional hardware time stamping information (if non NULL) or\n * generates a software time stamp (otherwise), then queues the clone\n * to the error queue of the socket.  Errors are silently ignored.\n */\nvoid skb_tstamp_tx(struct sk_buff *orig_skb,\n\t\t   struct skb_shared_hwtstamps *hwtstamps);\n\n/**\n * skb_tx_timestamp() - Driver hook for transmit timestamping\n *\n * Ethernet MAC Drivers should call this function in their hard_xmit()\n * function immediately before giving the sk_buff to the MAC hardware.\n *\n * Specifically, one should make absolutely sure that this function is\n * called before TX completion of this packet can trigger.  Otherwise\n * the packet could potentially already be freed.\n *\n * @skb: A socket buffer.\n */\nstatic inline void skb_tx_timestamp(struct sk_buff *skb)\n{\n\tskb_clone_tx_timestamp(skb);\n\tif (skb_shinfo(skb)->tx_flags & SKBTX_SW_TSTAMP)\n\t\tskb_tstamp_tx(skb, NULL);\n}\n\n/**\n * skb_complete_wifi_ack - deliver skb with wifi status\n *\n * @skb: the original outgoing packet\n * @acked: ack status\n *\n */\nvoid skb_complete_wifi_ack(struct sk_buff *skb, bool acked);\n\n__sum16 __skb_checksum_complete_head(struct sk_buff *skb, int len);\n__sum16 __skb_checksum_complete(struct sk_buff *skb);\n\nstatic inline int skb_csum_unnecessary(const struct sk_buff *skb)\n{\n\treturn ((skb->ip_summed == CHECKSUM_UNNECESSARY) ||\n\t\tskb->csum_valid ||\n\t\t(skb->ip_summed == CHECKSUM_PARTIAL &&\n\t\t skb_checksum_start_offset(skb) >= 0));\n}\n\n/**\n *\tskb_checksum_complete - Calculate checksum of an entire packet\n *\t@skb: packet to process\n *\n *\tThis function calculates the checksum over the entire packet plus\n *\tthe value of skb->csum.  The latter can be used to supply the\n *\tchecksum of a pseudo header as used by TCP/UDP.  It returns the\n *\tchecksum.\n *\n *\tFor protocols that contain complete checksums such as ICMP/TCP/UDP,\n *\tthis function can be used to verify that checksum on received\n *\tpackets.  In that case the function should return zero if the\n *\tchecksum is correct.  In particular, this function will return zero\n *\tif skb->ip_summed is CHECKSUM_UNNECESSARY which indicates that the\n *\thardware has already verified the correctness of the checksum.\n */\nstatic inline __sum16 skb_checksum_complete(struct sk_buff *skb)\n{\n\treturn skb_csum_unnecessary(skb) ?\n\t       0 : __skb_checksum_complete(skb);\n}\n\nstatic inline void __skb_decr_checksum_unnecessary(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\tif (skb->csum_level == 0)\n\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\telse\n\t\t\tskb->csum_level--;\n\t}\n}\n\nstatic inline void __skb_incr_checksum_unnecessary(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\tif (skb->csum_level < SKB_MAX_CSUM_LEVEL)\n\t\t\tskb->csum_level++;\n\t} else if (skb->ip_summed == CHECKSUM_NONE) {\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\tskb->csum_level = 0;\n\t}\n}\n\nstatic inline void __skb_reset_checksum_unnecessary(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tskb->csum_level = 0;\n\t}\n}\n\n/* Check if we need to perform checksum complete validation.\n *\n * Returns true if checksum complete is needed, false otherwise\n * (either checksum is unnecessary or zero checksum is allowed).\n */\nstatic inline bool __skb_checksum_validate_needed(struct sk_buff *skb,\n\t\t\t\t\t\t  bool zero_okay,\n\t\t\t\t\t\t  __sum16 check)\n{\n\tif (skb_csum_unnecessary(skb) || (zero_okay && !check)) {\n\t\tskb->csum_valid = 1;\n\t\t__skb_decr_checksum_unnecessary(skb);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n/* For small packets <= CHECKSUM_BREAK perform checksum complete directly\n * in checksum_init.\n */\n#define CHECKSUM_BREAK 76\n\n/* Unset checksum-complete\n *\n * Unset checksum complete can be done when packet is being modified\n * (uncompressed for instance) and checksum-complete value is\n * invalidated.\n */\nstatic inline void skb_checksum_complete_unset(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n}\n\n/* Validate (init) checksum based on checksum complete.\n *\n * Return values:\n *   0: checksum is validated or try to in skb_checksum_complete. In the latter\n *\tcase the ip_summed will not be CHECKSUM_UNNECESSARY and the pseudo\n *\tchecksum is stored in skb->csum for use in __skb_checksum_complete\n *   non-zero: value of invalid checksum\n *\n */\nstatic inline __sum16 __skb_checksum_validate_complete(struct sk_buff *skb,\n\t\t\t\t\t\t       bool complete,\n\t\t\t\t\t\t       __wsum psum)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\tif (!csum_fold(csum_add(psum, skb->csum))) {\n\t\t\tskb->csum_valid = 1;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tskb->csum = psum;\n\n\tif (complete || skb->len <= CHECKSUM_BREAK) {\n\t\t__sum16 csum;\n\n\t\tcsum = __skb_checksum_complete(skb);\n\t\tskb->csum_valid = !csum;\n\t\treturn csum;\n\t}\n\n\treturn 0;\n}\n\nstatic inline __wsum null_compute_pseudo(struct sk_buff *skb, int proto)\n{\n\treturn 0;\n}\n\n/* Perform checksum validate (init). Note that this is a macro since we only\n * want to calculate the pseudo header which is an input function if necessary.\n * First we try to validate without any computation (checksum unnecessary) and\n * then calculate based on checksum complete calling the function to compute\n * pseudo header.\n *\n * Return values:\n *   0: checksum is validated or try to in skb_checksum_complete\n *   non-zero: value of invalid checksum\n */\n#define __skb_checksum_validate(skb, proto, complete,\t\t\t\\\n\t\t\t\tzero_okay, check, compute_pseudo)\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__sum16 __ret = 0;\t\t\t\t\t\t\\\n\tskb->csum_valid = 0;\t\t\t\t\t\t\\\n\tif (__skb_checksum_validate_needed(skb, zero_okay, check))\t\\\n\t\t__ret = __skb_checksum_validate_complete(skb,\t\t\\\n\t\t\t\tcomplete, compute_pseudo(skb, proto));\t\\\n\t__ret;\t\t\t\t\t\t\t\t\\\n})\n\n#define skb_checksum_init(skb, proto, compute_pseudo)\t\t\t\\\n\t__skb_checksum_validate(skb, proto, false, false, 0, compute_pseudo)\n\n#define skb_checksum_init_zero_check(skb, proto, check, compute_pseudo)\t\\\n\t__skb_checksum_validate(skb, proto, false, true, check, compute_pseudo)\n\n#define skb_checksum_validate(skb, proto, compute_pseudo)\t\t\\\n\t__skb_checksum_validate(skb, proto, true, false, 0, compute_pseudo)\n\n#define skb_checksum_validate_zero_check(skb, proto, check,\t\t\\\n\t\t\t\t\t compute_pseudo)\t\t\\\n\t__skb_checksum_validate(skb, proto, true, true, check, compute_pseudo)\n\n#define skb_checksum_simple_validate(skb)\t\t\t\t\\\n\t__skb_checksum_validate(skb, 0, true, false, 0, null_compute_pseudo)\n\nstatic inline bool __skb_checksum_convert_check(struct sk_buff *skb)\n{\n\treturn (skb->ip_summed == CHECKSUM_NONE && skb->csum_valid);\n}\n\nstatic inline void __skb_checksum_convert(struct sk_buff *skb, __wsum pseudo)\n{\n\tskb->csum = ~pseudo;\n\tskb->ip_summed = CHECKSUM_COMPLETE;\n}\n\n#define skb_checksum_try_convert(skb, proto, compute_pseudo)\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (__skb_checksum_convert_check(skb))\t\t\t\t\\\n\t\t__skb_checksum_convert(skb, compute_pseudo(skb, proto)); \\\n} while (0)\n\nstatic inline void skb_remcsum_adjust_partial(struct sk_buff *skb, void *ptr,\n\t\t\t\t\t      u16 start, u16 offset)\n{\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\tskb->csum_start = ((unsigned char *)ptr + start) - skb->head;\n\tskb->csum_offset = offset - start;\n}\n\n/* Update skbuf and packet to reflect the remote checksum offload operation.\n * When called, ptr indicates the starting point for skb->csum when\n * ip_summed is CHECKSUM_COMPLETE. If we need create checksum complete\n * here, skb_postpull_rcsum is done so skb->csum start is ptr.\n */\nstatic inline void skb_remcsum_process(struct sk_buff *skb, void *ptr,\n\t\t\t\t       int start, int offset, bool nopartial)\n{\n\t__wsum delta;\n\n\tif (!nopartial) {\n\t\tskb_remcsum_adjust_partial(skb, ptr, start, offset);\n\t\treturn;\n\t}\n\n\t if (unlikely(skb->ip_summed != CHECKSUM_COMPLETE)) {\n\t\t__skb_checksum_complete(skb);\n\t\tskb_postpull_rcsum(skb, skb->data, ptr - (void *)skb->data);\n\t}\n\n\tdelta = remcsum_adjust(ptr, skb->csum, start, offset);\n\n\t/* Adjust skb->csum since we changed the packet */\n\tskb->csum = csum_add(skb->csum, delta);\n}\n\nstatic inline struct nf_conntrack *skb_nfct(const struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\treturn (void *)(skb->_nfct & NFCT_PTRMASK);\n#else\n\treturn NULL;\n#endif\n}\n\nstatic inline unsigned long skb_get_nfct(const struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\treturn skb->_nfct;\n#else\n\treturn 0UL;\n#endif\n}\n\nstatic inline void skb_set_nfct(struct sk_buff *skb, unsigned long nfct)\n{\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\tskb->_nfct = nfct;\n#endif\n}\n\n#ifdef CONFIG_SKB_EXTENSIONS\nenum skb_ext_id {\n#if IS_ENABLED(CONFIG_BRIDGE_NETFILTER)\n\tSKB_EXT_BRIDGE_NF,\n#endif\n#ifdef CONFIG_XFRM\n\tSKB_EXT_SEC_PATH,\n#endif\n#if IS_ENABLED(CONFIG_NET_TC_SKB_EXT)\n\tTC_SKB_EXT,\n#endif\n#if IS_ENABLED(CONFIG_MPTCP)\n\tSKB_EXT_MPTCP,\n#endif\n\tSKB_EXT_NUM, /* must be last */\n};\n\n/**\n *\tstruct skb_ext - sk_buff extensions\n *\t@refcnt: 1 on allocation, deallocated on 0\n *\t@offset: offset to add to @data to obtain extension address\n *\t@chunks: size currently allocated, stored in SKB_EXT_ALIGN_SHIFT units\n *\t@data: start of extension data, variable sized\n *\n *\tNote: offsets/lengths are stored in chunks of 8 bytes, this allows\n *\tto use 'u8' types while allowing up to 2kb worth of extension data.\n */\nstruct skb_ext {\n\trefcount_t refcnt;\n\tu8 offset[SKB_EXT_NUM]; /* in chunks of 8 bytes */\n\tu8 chunks;\t\t/* same */\n\tchar data[] __aligned(8);\n};\n\nstruct skb_ext *__skb_ext_alloc(gfp_t flags);\nvoid *__skb_ext_set(struct sk_buff *skb, enum skb_ext_id id,\n\t\t    struct skb_ext *ext);\nvoid *skb_ext_add(struct sk_buff *skb, enum skb_ext_id id);\nvoid __skb_ext_del(struct sk_buff *skb, enum skb_ext_id id);\nvoid __skb_ext_put(struct skb_ext *ext);\n\nstatic inline void skb_ext_put(struct sk_buff *skb)\n{\n\tif (skb->active_extensions)\n\t\t__skb_ext_put(skb->extensions);\n}\n\nstatic inline void __skb_ext_copy(struct sk_buff *dst,\n\t\t\t\t  const struct sk_buff *src)\n{\n\tdst->active_extensions = src->active_extensions;\n\n\tif (src->active_extensions) {\n\t\tstruct skb_ext *ext = src->extensions;\n\n\t\trefcount_inc(&ext->refcnt);\n\t\tdst->extensions = ext;\n\t}\n}\n\nstatic inline void skb_ext_copy(struct sk_buff *dst, const struct sk_buff *src)\n{\n\tskb_ext_put(dst);\n\t__skb_ext_copy(dst, src);\n}\n\nstatic inline bool __skb_ext_exist(const struct skb_ext *ext, enum skb_ext_id i)\n{\n\treturn !!ext->offset[i];\n}\n\nstatic inline bool skb_ext_exist(const struct sk_buff *skb, enum skb_ext_id id)\n{\n\treturn skb->active_extensions & (1 << id);\n}\n\nstatic inline void skb_ext_del(struct sk_buff *skb, enum skb_ext_id id)\n{\n\tif (skb_ext_exist(skb, id))\n\t\t__skb_ext_del(skb, id);\n}\n\nstatic inline void *skb_ext_find(const struct sk_buff *skb, enum skb_ext_id id)\n{\n\tif (skb_ext_exist(skb, id)) {\n\t\tstruct skb_ext *ext = skb->extensions;\n\n\t\treturn (void *)ext + (ext->offset[id] << 3);\n\t}\n\n\treturn NULL;\n}\n\nstatic inline void skb_ext_reset(struct sk_buff *skb)\n{\n\tif (unlikely(skb->active_extensions)) {\n\t\t__skb_ext_put(skb->extensions);\n\t\tskb->active_extensions = 0;\n\t}\n}\n\nstatic inline bool skb_has_extensions(struct sk_buff *skb)\n{\n\treturn unlikely(skb->active_extensions);\n}\n#else\nstatic inline void skb_ext_put(struct sk_buff *skb) {}\nstatic inline void skb_ext_reset(struct sk_buff *skb) {}\nstatic inline void skb_ext_del(struct sk_buff *skb, int unused) {}\nstatic inline void __skb_ext_copy(struct sk_buff *d, const struct sk_buff *s) {}\nstatic inline void skb_ext_copy(struct sk_buff *dst, const struct sk_buff *s) {}\nstatic inline bool skb_has_extensions(struct sk_buff *skb) { return false; }\n#endif /* CONFIG_SKB_EXTENSIONS */\n\nstatic inline void nf_reset_ct(struct sk_buff *skb)\n{\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tnf_conntrack_put(skb_nfct(skb));\n\tskb->_nfct = 0;\n#endif\n}\n\nstatic inline void nf_reset_trace(struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE) || defined(CONFIG_NF_TABLES)\n\tskb->nf_trace = 0;\n#endif\n}\n\nstatic inline void ipvs_reset(struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_IP_VS)\n\tskb->ipvs_property = 0;\n#endif\n}\n\n/* Note: This doesn't put any conntrack info in dst. */\nstatic inline void __nf_copy(struct sk_buff *dst, const struct sk_buff *src,\n\t\t\t     bool copy)\n{\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tdst->_nfct = src->_nfct;\n\tnf_conntrack_get(skb_nfct(src));\n#endif\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE) || defined(CONFIG_NF_TABLES)\n\tif (copy)\n\t\tdst->nf_trace = src->nf_trace;\n#endif\n}\n\nstatic inline void nf_copy(struct sk_buff *dst, const struct sk_buff *src)\n{\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tnf_conntrack_put(skb_nfct(dst));\n#endif\n\t__nf_copy(dst, src, true);\n}\n\n#ifdef CONFIG_NETWORK_SECMARK\nstatic inline void skb_copy_secmark(struct sk_buff *to, const struct sk_buff *from)\n{\n\tto->secmark = from->secmark;\n}\n\nstatic inline void skb_init_secmark(struct sk_buff *skb)\n{\n\tskb->secmark = 0;\n}\n#else\nstatic inline void skb_copy_secmark(struct sk_buff *to, const struct sk_buff *from)\n{ }\n\nstatic inline void skb_init_secmark(struct sk_buff *skb)\n{ }\n#endif\n\nstatic inline int secpath_exists(const struct sk_buff *skb)\n{\n#ifdef CONFIG_XFRM\n\treturn skb_ext_exist(skb, SKB_EXT_SEC_PATH);\n#else\n\treturn 0;\n#endif\n}\n\nstatic inline bool skb_irq_freeable(const struct sk_buff *skb)\n{\n\treturn !skb->destructor &&\n\t\t!secpath_exists(skb) &&\n\t\t!skb_nfct(skb) &&\n\t\t!skb->_skb_refdst &&\n\t\t!skb_has_frag_list(skb);\n}\n\nstatic inline void skb_set_queue_mapping(struct sk_buff *skb, u16 queue_mapping)\n{\n\tskb->queue_mapping = queue_mapping;\n}\n\nstatic inline u16 skb_get_queue_mapping(const struct sk_buff *skb)\n{\n\treturn skb->queue_mapping;\n}\n\nstatic inline void skb_copy_queue_mapping(struct sk_buff *to, const struct sk_buff *from)\n{\n\tto->queue_mapping = from->queue_mapping;\n}\n\nstatic inline void skb_record_rx_queue(struct sk_buff *skb, u16 rx_queue)\n{\n\tskb->queue_mapping = rx_queue + 1;\n}\n\nstatic inline u16 skb_get_rx_queue(const struct sk_buff *skb)\n{\n\treturn skb->queue_mapping - 1;\n}\n\nstatic inline bool skb_rx_queue_recorded(const struct sk_buff *skb)\n{\n\treturn skb->queue_mapping != 0;\n}\n\nstatic inline void skb_set_dst_pending_confirm(struct sk_buff *skb, u32 val)\n{\n\tskb->dst_pending_confirm = val;\n}\n\nstatic inline bool skb_get_dst_pending_confirm(const struct sk_buff *skb)\n{\n\treturn skb->dst_pending_confirm != 0;\n}\n\nstatic inline struct sec_path *skb_sec_path(const struct sk_buff *skb)\n{\n#ifdef CONFIG_XFRM\n\treturn skb_ext_find(skb, SKB_EXT_SEC_PATH);\n#else\n\treturn NULL;\n#endif\n}\n\n/* Keeps track of mac header offset relative to skb->head.\n * It is useful for TSO of Tunneling protocol. e.g. GRE.\n * For non-tunnel skb it points to skb_mac_header() and for\n * tunnel skb it points to outer mac header.\n * Keeps track of level of encapsulation of network headers.\n */\nstruct skb_gso_cb {\n\tunion {\n\t\tint\tmac_offset;\n\t\tint\tdata_offset;\n\t};\n\tint\tencap_level;\n\t__wsum\tcsum;\n\t__u16\tcsum_start;\n};\n#define SKB_GSO_CB_OFFSET\t32\n#define SKB_GSO_CB(skb) ((struct skb_gso_cb *)((skb)->cb + SKB_GSO_CB_OFFSET))\n\nstatic inline int skb_tnl_header_len(const struct sk_buff *inner_skb)\n{\n\treturn (skb_mac_header(inner_skb) - inner_skb->head) -\n\t\tSKB_GSO_CB(inner_skb)->mac_offset;\n}\n\nstatic inline int gso_pskb_expand_head(struct sk_buff *skb, int extra)\n{\n\tint new_headroom, headroom;\n\tint ret;\n\n\theadroom = skb_headroom(skb);\n\tret = pskb_expand_head(skb, extra, 0, GFP_ATOMIC);\n\tif (ret)\n\t\treturn ret;\n\n\tnew_headroom = skb_headroom(skb);\n\tSKB_GSO_CB(skb)->mac_offset += (new_headroom - headroom);\n\treturn 0;\n}\n\nstatic inline void gso_reset_checksum(struct sk_buff *skb, __wsum res)\n{\n\t/* Do not update partial checksums if remote checksum is enabled. */\n\tif (skb->remcsum_offload)\n\t\treturn;\n\n\tSKB_GSO_CB(skb)->csum = res;\n\tSKB_GSO_CB(skb)->csum_start = skb_checksum_start(skb) - skb->head;\n}\n\n/* Compute the checksum for a gso segment. First compute the checksum value\n * from the start of transport header to SKB_GSO_CB(skb)->csum_start, and\n * then add in skb->csum (checksum from csum_start to end of packet).\n * skb->csum and csum_start are then updated to reflect the checksum of the\n * resultant packet starting from the transport header-- the resultant checksum\n * is in the res argument (i.e. normally zero or ~ of checksum of a pseudo\n * header.\n */\nstatic inline __sum16 gso_make_checksum(struct sk_buff *skb, __wsum res)\n{\n\tunsigned char *csum_start = skb_transport_header(skb);\n\tint plen = (skb->head + SKB_GSO_CB(skb)->csum_start) - csum_start;\n\t__wsum partial = SKB_GSO_CB(skb)->csum;\n\n\tSKB_GSO_CB(skb)->csum = res;\n\tSKB_GSO_CB(skb)->csum_start = csum_start - skb->head;\n\n\treturn csum_fold(csum_partial(csum_start, plen, partial));\n}\n\nstatic inline bool skb_is_gso(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->gso_size;\n}\n\n/* Note: Should be called only if skb_is_gso(skb) is true */\nstatic inline bool skb_is_gso_v6(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->gso_type & SKB_GSO_TCPV6;\n}\n\n/* Note: Should be called only if skb_is_gso(skb) is true */\nstatic inline bool skb_is_gso_sctp(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->gso_type & SKB_GSO_SCTP;\n}\n\n/* Note: Should be called only if skb_is_gso(skb) is true */\nstatic inline bool skb_is_gso_tcp(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6);\n}\n\nstatic inline void skb_gso_reset(struct sk_buff *skb)\n{\n\tskb_shinfo(skb)->gso_size = 0;\n\tskb_shinfo(skb)->gso_segs = 0;\n\tskb_shinfo(skb)->gso_type = 0;\n}\n\nstatic inline void skb_increase_gso_size(struct skb_shared_info *shinfo,\n\t\t\t\t\t u16 increment)\n{\n\tif (WARN_ON_ONCE(shinfo->gso_size == GSO_BY_FRAGS))\n\t\treturn;\n\tshinfo->gso_size += increment;\n}\n\nstatic inline void skb_decrease_gso_size(struct skb_shared_info *shinfo,\n\t\t\t\t\t u16 decrement)\n{\n\tif (WARN_ON_ONCE(shinfo->gso_size == GSO_BY_FRAGS))\n\t\treturn;\n\tshinfo->gso_size -= decrement;\n}\n\nvoid __skb_warn_lro_forwarding(const struct sk_buff *skb);\n\nstatic inline bool skb_warn_if_lro(const struct sk_buff *skb)\n{\n\t/* LRO sets gso_size but not gso_type, whereas if GSO is really\n\t * wanted then gso_type will be set. */\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\tif (skb_is_nonlinear(skb) && shinfo->gso_size != 0 &&\n\t    unlikely(shinfo->gso_type == 0)) {\n\t\t__skb_warn_lro_forwarding(skb);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline void skb_forward_csum(struct sk_buff *skb)\n{\n\t/* Unfortunately we don't support this one.  Any brave souls? */\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n}\n\n/**\n * skb_checksum_none_assert - make sure skb ip_summed is CHECKSUM_NONE\n * @skb: skb to check\n *\n * fresh skbs have their ip_summed set to CHECKSUM_NONE.\n * Instead of forcing ip_summed to CHECKSUM_NONE, we can\n * use this helper, to document places where we make this assertion.\n */\nstatic inline void skb_checksum_none_assert(const struct sk_buff *skb)\n{\n#ifdef DEBUG\n\tBUG_ON(skb->ip_summed != CHECKSUM_NONE);\n#endif\n}\n\nbool skb_partial_csum_set(struct sk_buff *skb, u16 start, u16 off);\n\nint skb_checksum_setup(struct sk_buff *skb, bool recalculate);\nstruct sk_buff *skb_checksum_trimmed(struct sk_buff *skb,\n\t\t\t\t     unsigned int transport_len,\n\t\t\t\t     __sum16(*skb_chkf)(struct sk_buff *skb));\n\n/**\n * skb_head_is_locked - Determine if the skb->head is locked down\n * @skb: skb to check\n *\n * The head on skbs build around a head frag can be removed if they are\n * not cloned.  This function returns true if the skb head is locked down\n * due to either being allocated via kmalloc, or by being a clone with\n * multiple references to the head.\n */\nstatic inline bool skb_head_is_locked(const struct sk_buff *skb)\n{\n\treturn !skb->head_frag || skb_cloned(skb);\n}\n\n/* Local Checksum Offload.\n * Compute outer checksum based on the assumption that the\n * inner checksum will be offloaded later.\n * See Documentation/networking/checksum-offloads.rst for\n * explanation of how this works.\n * Fill in outer checksum adjustment (e.g. with sum of outer\n * pseudo-header) before calling.\n * Also ensure that inner checksum is in linear data area.\n */\nstatic inline __wsum lco_csum(struct sk_buff *skb)\n{\n\tunsigned char *csum_start = skb_checksum_start(skb);\n\tunsigned char *l4_hdr = skb_transport_header(skb);\n\t__wsum partial;\n\n\t/* Start with complement of inner checksum adjustment */\n\tpartial = ~csum_unfold(*(__force __sum16 *)(csum_start +\n\t\t\t\t\t\t    skb->csum_offset));\n\n\t/* Add in checksum of our headers (incl. outer checksum\n\t * adjustment filled in by caller) and return result.\n\t */\n\treturn csum_partial(l4_hdr, csum_start - l4_hdr, partial);\n}\n\nstatic inline bool skb_is_redirected(const struct sk_buff *skb)\n{\n#ifdef CONFIG_NET_REDIRECT\n\treturn skb->redirected;\n#else\n\treturn false;\n#endif\n}\n\nstatic inline void skb_set_redirected(struct sk_buff *skb, bool from_ingress)\n{\n#ifdef CONFIG_NET_REDIRECT\n\tskb->redirected = 1;\n\tskb->from_ingress = from_ingress;\n\tif (skb->from_ingress)\n\t\tskb->tstamp = 0;\n#endif\n}\n\nstatic inline void skb_reset_redirect(struct sk_buff *skb)\n{\n#ifdef CONFIG_NET_REDIRECT\n\tskb->redirected = 0;\n#endif\n}\n\nstatic inline bool skb_csum_is_sctp(struct sk_buff *skb)\n{\n\treturn skb->csum_not_inet;\n}\n\nstatic inline void skb_set_kcov_handle(struct sk_buff *skb,\n\t\t\t\t       const u64 kcov_handle)\n{\n#ifdef CONFIG_KCOV\n\tskb->kcov_handle = kcov_handle;\n#endif\n}\n\nstatic inline u64 skb_get_kcov_handle(struct sk_buff *skb)\n{\n#ifdef CONFIG_KCOV\n\treturn skb->kcov_handle;\n#else\n\treturn 0;\n#endif\n}\n\n#endif\t/* __KERNEL__ */\n#endif\t/* _LINUX_SKBUFF_H */\n"}, "6": {"id": 6, "path": "/src/include/linux/compiler.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_COMPILER_H\n#define __LINUX_COMPILER_H\n\n#include <linux/compiler_types.h>\n\n#ifndef __ASSEMBLY__\n\n#ifdef __KERNEL__\n\n/*\n * Note: DISABLE_BRANCH_PROFILING can be used by special lowlevel code\n * to disable branch tracing on a per file basis.\n */\n#if defined(CONFIG_TRACE_BRANCH_PROFILING) \\\n    && !defined(DISABLE_BRANCH_PROFILING) && !defined(__CHECKER__)\nvoid ftrace_likely_update(struct ftrace_likely_data *f, int val,\n\t\t\t  int expect, int is_constant);\n\n#define likely_notrace(x)\t__builtin_expect(!!(x), 1)\n#define unlikely_notrace(x)\t__builtin_expect(!!(x), 0)\n\n#define __branch_check__(x, expect, is_constant) ({\t\t\t\\\n\t\t\tlong ______r;\t\t\t\t\t\\\n\t\t\tstatic struct ftrace_likely_data\t\t\\\n\t\t\t\t__aligned(4)\t\t\t\t\\\n\t\t\t\t__section(\"_ftrace_annotated_branch\")\t\\\n\t\t\t\t______f = {\t\t\t\t\\\n\t\t\t\t.data.func = __func__,\t\t\t\\\n\t\t\t\t.data.file = __FILE__,\t\t\t\\\n\t\t\t\t.data.line = __LINE__,\t\t\t\\\n\t\t\t};\t\t\t\t\t\t\\\n\t\t\t______r = __builtin_expect(!!(x), expect);\t\\\n\t\t\tftrace_likely_update(&______f, ______r,\t\t\\\n\t\t\t\t\t     expect, is_constant);\t\\\n\t\t\t______r;\t\t\t\t\t\\\n\t\t})\n\n/*\n * Using __builtin_constant_p(x) to ignore cases where the return\n * value is always the same.  This idea is taken from a similar patch\n * written by Daniel Walker.\n */\n# ifndef likely\n#  define likely(x)\t(__branch_check__(x, 1, __builtin_constant_p(x)))\n# endif\n# ifndef unlikely\n#  define unlikely(x)\t(__branch_check__(x, 0, __builtin_constant_p(x)))\n# endif\n\n#ifdef CONFIG_PROFILE_ALL_BRANCHES\n/*\n * \"Define 'is'\", Bill Clinton\n * \"Define 'if'\", Steven Rostedt\n */\n#define if(cond, ...) if ( __trace_if_var( !!(cond , ## __VA_ARGS__) ) )\n\n#define __trace_if_var(cond) (__builtin_constant_p(cond) ? (cond) : __trace_if_value(cond))\n\n#define __trace_if_value(cond) ({\t\t\t\\\n\tstatic struct ftrace_branch_data\t\t\\\n\t\t__aligned(4)\t\t\t\t\\\n\t\t__section(\"_ftrace_branch\")\t\t\\\n\t\t__if_trace = {\t\t\t\t\\\n\t\t\t.func = __func__,\t\t\\\n\t\t\t.file = __FILE__,\t\t\\\n\t\t\t.line = __LINE__,\t\t\\\n\t\t};\t\t\t\t\t\\\n\t(cond) ?\t\t\t\t\t\\\n\t\t(__if_trace.miss_hit[1]++,1) :\t\t\\\n\t\t(__if_trace.miss_hit[0]++,0);\t\t\\\n})\n\n#endif /* CONFIG_PROFILE_ALL_BRANCHES */\n\n#else\n# define likely(x)\t__builtin_expect(!!(x), 1)\n# define unlikely(x)\t__builtin_expect(!!(x), 0)\n# define likely_notrace(x)\tlikely(x)\n# define unlikely_notrace(x)\tunlikely(x)\n#endif\n\n/* Optimization barrier */\n#ifndef barrier\n/* The \"volatile\" is due to gcc bugs */\n# define barrier() __asm__ __volatile__(\"\": : :\"memory\")\n#endif\n\n#ifndef barrier_data\n/*\n * This version is i.e. to prevent dead stores elimination on @ptr\n * where gcc and llvm may behave differently when otherwise using\n * normal barrier(): while gcc behavior gets along with a normal\n * barrier(), llvm needs an explicit input variable to be assumed\n * clobbered. The issue is as follows: while the inline asm might\n * access any memory it wants, the compiler could have fit all of\n * @ptr into memory registers instead, and since @ptr never escaped\n * from that, it proved that the inline asm wasn't touching any of\n * it. This version works well with both compilers, i.e. we're telling\n * the compiler that the inline asm absolutely may see the contents\n * of @ptr. See also: https://llvm.org/bugs/show_bug.cgi?id=15495\n */\n# define barrier_data(ptr) __asm__ __volatile__(\"\": :\"r\"(ptr) :\"memory\")\n#endif\n\n/* workaround for GCC PR82365 if needed */\n#ifndef barrier_before_unreachable\n# define barrier_before_unreachable() do { } while (0)\n#endif\n\n/* Unreachable code */\n#ifdef CONFIG_STACK_VALIDATION\n/*\n * These macros help objtool understand GCC code flow for unreachable code.\n * The __COUNTER__ based labels are a hack to make each instance of the macros\n * unique, to convince GCC not to merge duplicate inline asm statements.\n */\n#define annotate_reachable() ({\t\t\t\t\t\t\\\n\tasm volatile(\"%c0:\\n\\t\"\t\t\t\t\t\t\\\n\t\t     \".pushsection .discard.reachable\\n\\t\"\t\t\\\n\t\t     \".long %c0b - .\\n\\t\"\t\t\t\t\\\n\t\t     \".popsection\\n\\t\" : : \"i\" (__COUNTER__));\t\t\\\n})\n#define annotate_unreachable() ({\t\t\t\t\t\\\n\tasm volatile(\"%c0:\\n\\t\"\t\t\t\t\t\t\\\n\t\t     \".pushsection .discard.unreachable\\n\\t\"\t\t\\\n\t\t     \".long %c0b - .\\n\\t\"\t\t\t\t\\\n\t\t     \".popsection\\n\\t\" : : \"i\" (__COUNTER__));\t\t\\\n})\n#define ASM_UNREACHABLE\t\t\t\t\t\t\t\\\n\t\"999:\\n\\t\"\t\t\t\t\t\t\t\\\n\t\".pushsection .discard.unreachable\\n\\t\"\t\t\t\t\\\n\t\".long 999b - .\\n\\t\"\t\t\t\t\t\t\\\n\t\".popsection\\n\\t\"\n\n/* Annotate a C jump table to allow objtool to follow the code flow */\n#define __annotate_jump_table __section(\".rodata..c_jump_table\")\n\n#else\n#define annotate_reachable()\n#define annotate_unreachable()\n#define __annotate_jump_table\n#endif\n\n#ifndef ASM_UNREACHABLE\n# define ASM_UNREACHABLE\n#endif\n#ifndef unreachable\n# define unreachable() do {\t\t\\\n\tannotate_unreachable();\t\t\\\n\t__builtin_unreachable();\t\\\n} while (0)\n#endif\n\n/*\n * KENTRY - kernel entry point\n * This can be used to annotate symbols (functions or data) that are used\n * without their linker symbol being referenced explicitly. For example,\n * interrupt vector handlers, or functions in the kernel image that are found\n * programatically.\n *\n * Not required for symbols exported with EXPORT_SYMBOL, or initcalls. Those\n * are handled in their own way (with KEEP() in linker scripts).\n *\n * KENTRY can be avoided if the symbols in question are marked as KEEP() in the\n * linker script. For example an architecture could KEEP() its entire\n * boot/exception vector code rather than annotate each function and data.\n */\n#ifndef KENTRY\n# define KENTRY(sym)\t\t\t\t\t\t\\\n\textern typeof(sym) sym;\t\t\t\t\t\\\n\tstatic const unsigned long __kentry_##sym\t\t\\\n\t__used\t\t\t\t\t\t\t\\\n\t__attribute__((__section__(\"___kentry+\" #sym)))\t\t\\\n\t= (unsigned long)&sym;\n#endif\n\n#ifndef RELOC_HIDE\n# define RELOC_HIDE(ptr, off)\t\t\t\t\t\\\n  ({ unsigned long __ptr;\t\t\t\t\t\\\n     __ptr = (unsigned long) (ptr);\t\t\t\t\\\n    (typeof(ptr)) (__ptr + (off)); })\n#endif\n\n#ifndef OPTIMIZER_HIDE_VAR\n/* Make the optimizer believe the variable can be manipulated arbitrarily. */\n#define OPTIMIZER_HIDE_VAR(var)\t\t\t\t\t\t\\\n\t__asm__ (\"\" : \"=r\" (var) : \"0\" (var))\n#endif\n\n/* Not-quite-unique ID. */\n#ifndef __UNIQUE_ID\n# define __UNIQUE_ID(prefix) __PASTE(__PASTE(__UNIQUE_ID_, prefix), __LINE__)\n#endif\n\n/**\n * data_race - mark an expression as containing intentional data races\n *\n * This data_race() macro is useful for situations in which data races\n * should be forgiven.  One example is diagnostic code that accesses\n * shared variables but is not a part of the core synchronization design.\n *\n * This macro *does not* affect normal code generation, but is a hint\n * to tooling that data races here are to be ignored.\n */\n#define data_race(expr)\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__unqual_scalar_typeof(({ expr; })) __v = ({\t\t\t\\\n\t\t__kcsan_disable_current();\t\t\t\t\\\n\t\texpr;\t\t\t\t\t\t\t\\\n\t});\t\t\t\t\t\t\t\t\\\n\t__kcsan_enable_current();\t\t\t\t\t\\\n\t__v;\t\t\t\t\t\t\t\t\\\n})\n\n#endif /* __KERNEL__ */\n\n/*\n * Force the compiler to emit 'sym' as a symbol, so that we can reference\n * it from inline assembler. Necessary in case 'sym' could be inlined\n * otherwise, or eliminated entirely due to lack of references that are\n * visible to the compiler.\n */\n#define __ADDRESSABLE(sym) \\\n\tstatic void * __section(\".discard.addressable\") __used \\\n\t\t__UNIQUE_ID(__PASTE(__addressable_,sym)) = (void *)&sym;\n\n/**\n * offset_to_ptr - convert a relative memory offset to an absolute pointer\n * @off:\tthe address of the 32-bit offset value\n */\nstatic inline void *offset_to_ptr(const int *off)\n{\n\treturn (void *)((unsigned long)off + *off);\n}\n\n#endif /* __ASSEMBLY__ */\n\n/* &a[0] degrades to a pointer: a different type from an array */\n#define __must_be_array(a)\tBUILD_BUG_ON_ZERO(__same_type((a), &(a)[0]))\n\n/*\n * This is needed in functions which generate the stack canary, see\n * arch/x86/kernel/smpboot.c::start_secondary() for an example.\n */\n#define prevent_tail_call_optimization()\tmb()\n\n#include <asm/rwonce.h>\n\n#endif /* __LINUX_COMPILER_H */\n"}, "7": {"id": 7, "path": "/src/include/linux/minmax.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_MINMAX_H\n#define _LINUX_MINMAX_H\n\n/*\n * min()/max()/clamp() macros must accomplish three things:\n *\n * - avoid multiple evaluations of the arguments (so side-effects like\n *   \"x++\" happen only once) when non-constant.\n * - perform strict type-checking (to generate warnings instead of\n *   nasty runtime surprises). See the \"unnecessary\" pointer comparison\n *   in __typecheck().\n * - retain result as a constant expressions when called with only\n *   constant expressions (to avoid tripping VLA warnings in stack\n *   allocation usage).\n */\n#define __typecheck(x, y) \\\n\t(!!(sizeof((typeof(x) *)1 == (typeof(y) *)1)))\n\n/*\n * This returns a constant expression while determining if an argument is\n * a constant expression, most importantly without evaluating the argument.\n * Glory to Martin Uecker <Martin.Uecker@med.uni-goettingen.de>\n */\n#define __is_constexpr(x) \\\n\t(sizeof(int) == sizeof(*(8 ? ((void *)((long)(x) * 0l)) : (int *)8)))\n\n#define __no_side_effects(x, y) \\\n\t\t(__is_constexpr(x) && __is_constexpr(y))\n\n#define __safe_cmp(x, y) \\\n\t\t(__typecheck(x, y) && __no_side_effects(x, y))\n\n#define __cmp(x, y, op)\t((x) op (y) ? (x) : (y))\n\n#define __cmp_once(x, y, unique_x, unique_y, op) ({\t\\\n\t\ttypeof(x) unique_x = (x);\t\t\\\n\t\ttypeof(y) unique_y = (y);\t\t\\\n\t\t__cmp(unique_x, unique_y, op); })\n\n#define __careful_cmp(x, y, op) \\\n\t__builtin_choose_expr(__safe_cmp(x, y), \\\n\t\t__cmp(x, y, op), \\\n\t\t__cmp_once(x, y, __UNIQUE_ID(__x), __UNIQUE_ID(__y), op))\n\n/**\n * min - return minimum of two values of the same or compatible types\n * @x: first value\n * @y: second value\n */\n#define min(x, y)\t__careful_cmp(x, y, <)\n\n/**\n * max - return maximum of two values of the same or compatible types\n * @x: first value\n * @y: second value\n */\n#define max(x, y)\t__careful_cmp(x, y, >)\n\n/**\n * min3 - return minimum of three values\n * @x: first value\n * @y: second value\n * @z: third value\n */\n#define min3(x, y, z) min((typeof(x))min(x, y), z)\n\n/**\n * max3 - return maximum of three values\n * @x: first value\n * @y: second value\n * @z: third value\n */\n#define max3(x, y, z) max((typeof(x))max(x, y), z)\n\n/**\n * min_not_zero - return the minimum that is _not_ zero, unless both are zero\n * @x: value1\n * @y: value2\n */\n#define min_not_zero(x, y) ({\t\t\t\\\n\ttypeof(x) __x = (x);\t\t\t\\\n\ttypeof(y) __y = (y);\t\t\t\\\n\t__x == 0 ? __y : ((__y == 0) ? __x : min(__x, __y)); })\n\n/**\n * clamp - return a value clamped to a given range with strict typechecking\n * @val: current value\n * @lo: lowest allowable value\n * @hi: highest allowable value\n *\n * This macro does strict typechecking of @lo/@hi to make sure they are of the\n * same type as @val.  See the unnecessary pointer comparisons.\n */\n#define clamp(val, lo, hi) min((typeof(val))max(val, lo), hi)\n\n/*\n * ..and if you can't take the strict\n * types, you can specify one yourself.\n *\n * Or not use min/max/clamp at all, of course.\n */\n\n/**\n * min_t - return minimum of two values, using the specified type\n * @type: data type to use\n * @x: first value\n * @y: second value\n */\n#define min_t(type, x, y)\t__careful_cmp((type)(x), (type)(y), <)\n\n/**\n * max_t - return maximum of two values, using the specified type\n * @type: data type to use\n * @x: first value\n * @y: second value\n */\n#define max_t(type, x, y)\t__careful_cmp((type)(x), (type)(y), >)\n\n/**\n * clamp_t - return a value clamped to a given range using a given type\n * @type: the type of variable to use\n * @val: current value\n * @lo: minimum allowable value\n * @hi: maximum allowable value\n *\n * This macro does no typechecking and uses temporary variables of type\n * @type to make all the comparisons.\n */\n#define clamp_t(type, val, lo, hi) min_t(type, max_t(type, val, lo), hi)\n\n/**\n * clamp_val - return a value clamped to a given range using val's type\n * @val: current value\n * @lo: minimum allowable value\n * @hi: maximum allowable value\n *\n * This macro does no typechecking and uses temporary variables of whatever\n * type the input argument @val is.  This is useful when @val is an unsigned\n * type and @lo and @hi are literals that will otherwise be assigned a signed\n * integer type.\n */\n#define clamp_val(val, lo, hi) clamp_t(typeof(val), val, lo, hi)\n\n/**\n * swap - swap values of @a and @b\n * @a: first value\n * @b: second value\n */\n#define swap(a, b) \\\n\tdo { typeof(a) __tmp = (a); (a) = (b); (b) = __tmp; } while (0)\n\n#endif\t/* _LINUX_MINMAX_H */\n"}}, "reports": [{"events": [{"location": {"col": 29, "file": 1, "line": 245}, "message": "expanded from macro 'page_private'"}, {"location": {"col": 27, "file": 0, "line": 6247}, "message": "Calling 'skb_clone'"}, {"location": {"col": 36, "file": 0, "line": 1481}, "message": "Left side of '&&' is false"}, {"location": {"col": 61, "file": 2, "line": 702}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 36, "file": 0, "line": 1481}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 2, "line": 702}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 37, "file": 3, "line": 39}, "message": "expanded from macro 'BUILD_BUG_ON_MSG'"}, {"location": {"col": 2, "file": 4, "line": 328}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 4, "line": 316}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 4, "line": 308}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 36, "file": 0, "line": 1481}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 2, "line": 702}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 37, "file": 3, "line": 39}, "message": "expanded from macro 'BUILD_BUG_ON_MSG'"}, {"location": {"col": 2, "file": 4, "line": 328}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 4, "line": 316}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 4, "line": 306}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 6, "file": 0, "line": 1486}, "message": "Calling 'skb_orphan_frags'"}, {"location": {"col": 6, "file": 5, "line": 2793}, "message": "Assuming the condition is false"}, {"location": {"col": 20, "file": 6, "line": 77}, "message": "expanded from macro 'likely'"}, {"location": {"col": 2, "file": 5, "line": 2793}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 5, "line": 2795}, "message": "Assuming the condition is false"}, {"location": {"col": 32, "file": 5, "line": 2795}, "message": "Left side of '&&' is false"}, {"location": {"col": 9, "file": 5, "line": 2798}, "message": "Calling 'skb_copy_ubufs'"}, {"location": {"col": 22, "file": 0, "line": 1394}, "message": "'head' initialized to a null pointer value"}, {"location": {"col": 6, "file": 0, "line": 1398}, "message": "Calling 'skb_shared'"}, {"location": {"col": 9, "file": 5, "line": 1713}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 5, "line": 1713}, "message": "Returning zero, which participates in a condition later"}, {"location": {"col": 6, "file": 0, "line": 1398}, "message": "Returning from 'skb_shared'"}, {"location": {"col": 6, "file": 0, "line": 1398}, "message": "Left side of '||' is false"}, {"location": {"col": 25, "file": 0, "line": 1398}, "message": "Calling 'skb_unclone'"}, {"location": {"col": 2, "file": 5, "line": 1656}, "message": "Taking true branch"}, {"location": {"col": 35, "file": 2, "line": 168}, "message": "expanded from macro 'might_sleep_if'"}, {"location": {"col": 2, "file": 5, "line": 1656}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 45, "file": 2, "line": 168}, "message": "expanded from macro 'might_sleep_if'"}, {"location": {"col": 24, "file": 2, "line": 160}, "message": "expanded from macro 'might_sleep'"}, {"location": {"col": 2, "file": 5, "line": 1656}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 30, "file": 2, "line": 168}, "message": "expanded from macro 'might_sleep_if'"}, {"location": {"col": 6, "file": 5, "line": 1658}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 5, "line": 1658}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 5, "line": 1661}, "message": "Returning zero, which participates in a condition later"}, {"location": {"col": 25, "file": 0, "line": 1398}, "message": "Returning from 'skb_unclone'"}, {"location": {"col": 2, "file": 0, "line": 1398}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 0, "line": 1401}, "message": "Assuming 'num_frags' is not equal to 0"}, {"location": {"col": 2, "file": 0, "line": 1401}, "message": "Taking false branch"}, {"location": {"col": 14, "file": 0, "line": 1405}, "message": "Assuming 'i' is >= 'new_frags'"}, {"location": {"col": 2, "file": 0, "line": 1405}, "message": "Loop condition is false. Execution continues on line 1419"}, {"location": {"col": 2, "file": 0, "line": 1419}, "message": "Null pointer value stored to 'page'"}, {"location": {"col": 14, "file": 0, "line": 1421}, "message": "Assuming 'i' is < 'num_frags'"}, {"location": {"col": 2, "file": 0, "line": 1421}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 3, "file": 0, "line": 1427}, "message": "'?' condition is false"}, {"location": {"col": 15, "file": 5, "line": 396}, "message": "expanded from macro 'skb_frag_foreach_page'"}, {"location": {"col": 28, "file": 0, "line": 1428}, "message": "Assuming the condition is true"}, {"location": {"col": 7, "file": 5, "line": 399}, "message": "expanded from macro 'skb_frag_foreach_page'"}, {"location": {"col": 3, "file": 0, "line": 1427}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 2, "file": 5, "line": 394}, "message": "expanded from macro 'skb_frag_foreach_page'"}, {"location": {"col": 11, "file": 0, "line": 1432}, "message": "'done' is < 'p_len'"}, {"location": {"col": 4, "file": 0, "line": 1432}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 5, "file": 0, "line": 1433}, "message": "Taking false branch"}, {"location": {"col": 12, "file": 0, "line": 1437}, "message": "Assuming '__UNIQUE_ID___x701' is < '__UNIQUE_ID___y702'"}, {"location": {"col": 27, "file": 7, "line": 110}, "message": "expanded from macro 'min_t'"}, {"location": {"col": 3, "file": 7, "line": 44}, "message": "expanded from macro '__careful_cmp'"}, {"location": {"col": 3, "file": 7, "line": 39}, "message": "expanded from macro '__cmp_once'"}, {"location": {"col": 26, "file": 7, "line": 34}, "message": "expanded from macro '__cmp'"}, {"location": {"col": 12, "file": 0, "line": 1437}, "message": "'?' condition is true"}, {"location": {"col": 27, "file": 7, "line": 110}, "message": "expanded from macro 'min_t'"}, {"location": {"col": 3, "file": 7, "line": 44}, "message": "expanded from macro '__careful_cmp'"}, {"location": {"col": 3, "file": 7, "line": 39}, "message": "expanded from macro '__cmp_once'"}, {"location": {"col": 26, "file": 7, "line": 34}, "message": "expanded from macro '__cmp'"}, {"location": {"col": 11, "file": 0, "line": 1432}, "message": "'done' is < 'p_len'"}, {"location": {"col": 4, "file": 0, "line": 1432}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 5, "file": 0, "line": 1433}, "message": "Taking true branch"}, {"location": {"col": 28, "file": 0, "line": 1435}, "message": "Dereference of null pointer"}, {"location": {"col": 29, "file": 1, "line": 245}, "message": "expanded from macro 'page_private'"}, {"location": {"col": 28, "file": 0, "line": 1435}, "message": "Dereference of null pointer"}], "macros": [], "notes": [], "path": "/src/net/core/skbuff.c", "reportHash": "e6bf03e1d63f975642b9ea2527933401", "checkerName": "clang-analyzer-core.NullDereference", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
