<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"0": {"id": 0, "path": "/src/block/blk-mq.c", "content": "// SPDX-License-Identifier: GPL-2.0\n/*\n * Block multiqueue core code\n *\n * Copyright (C) 2013-2014 Jens Axboe\n * Copyright (C) 2013-2014 Christoph Hellwig\n */\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/backing-dev.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/kmemleak.h>\n#include <linux/mm.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <linux/workqueue.h>\n#include <linux/smp.h>\n#include <linux/llist.h>\n#include <linux/list_sort.h>\n#include <linux/cpu.h>\n#include <linux/cache.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/signal.h>\n#include <linux/delay.h>\n#include <linux/crash_dump.h>\n#include <linux/prefetch.h>\n#include <linux/blk-crypto.h>\n\n#include <trace/events/block.h>\n\n#include <linux/blk-mq.h>\n#include <linux/t10-pi.h>\n#include \"blk.h\"\n#include \"blk-mq.h\"\n#include \"blk-mq-debugfs.h\"\n#include \"blk-mq-tag.h\"\n#include \"blk-pm.h\"\n#include \"blk-stat.h\"\n#include \"blk-mq-sched.h\"\n#include \"blk-rq-qos.h\"\n\nstatic DEFINE_PER_CPU(struct list_head, blk_cpu_done);\n\nstatic void blk_mq_poll_stats_start(struct request_queue *q);\nstatic void blk_mq_poll_stats_fn(struct blk_stat_callback *cb);\n\nstatic int blk_mq_poll_stats_bkt(const struct request *rq)\n{\n\tint ddir, sectors, bucket;\n\n\tddir = rq_data_dir(rq);\n\tsectors = blk_rq_stats_sectors(rq);\n\n\tbucket = ddir + 2 * ilog2(sectors);\n\n\tif (bucket < 0)\n\t\treturn -1;\n\telse if (bucket >= BLK_MQ_POLL_STATS_BKTS)\n\t\treturn ddir + BLK_MQ_POLL_STATS_BKTS - 2;\n\n\treturn bucket;\n}\n\n/*\n * Check if any of the ctx, dispatch list or elevator\n * have pending work in this hardware queue.\n */\nstatic bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)\n{\n\treturn !list_empty_careful(&hctx->dispatch) ||\n\t\tsbitmap_any_bit_set(&hctx->ctx_map) ||\n\t\t\tblk_mq_sched_has_work(hctx);\n}\n\n/*\n * Mark this ctx as having pending work in this hardware queue\n */\nstatic void blk_mq_hctx_mark_pending(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t     struct blk_mq_ctx *ctx)\n{\n\tconst int bit = ctx->index_hw[hctx->type];\n\n\tif (!sbitmap_test_bit(&hctx->ctx_map, bit))\n\t\tsbitmap_set_bit(&hctx->ctx_map, bit);\n}\n\nstatic void blk_mq_hctx_clear_pending(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t      struct blk_mq_ctx *ctx)\n{\n\tconst int bit = ctx->index_hw[hctx->type];\n\n\tsbitmap_clear_bit(&hctx->ctx_map, bit);\n}\n\nstruct mq_inflight {\n\tstruct hd_struct *part;\n\tunsigned int inflight[2];\n};\n\nstatic bool blk_mq_check_inflight(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t  struct request *rq, void *priv,\n\t\t\t\t  bool reserved)\n{\n\tstruct mq_inflight *mi = priv;\n\n\tif (rq->part == mi->part && blk_mq_rq_state(rq) == MQ_RQ_IN_FLIGHT)\n\t\tmi->inflight[rq_data_dir(rq)]++;\n\n\treturn true;\n}\n\nunsigned int blk_mq_in_flight(struct request_queue *q, struct hd_struct *part)\n{\n\tstruct mq_inflight mi = { .part = part };\n\n\tblk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);\n\n\treturn mi.inflight[0] + mi.inflight[1];\n}\n\nvoid blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,\n\t\t\t unsigned int inflight[2])\n{\n\tstruct mq_inflight mi = { .part = part };\n\n\tblk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);\n\tinflight[0] = mi.inflight[0];\n\tinflight[1] = mi.inflight[1];\n}\n\nvoid blk_freeze_queue_start(struct request_queue *q)\n{\n\tmutex_lock(&q->mq_freeze_lock);\n\tif (++q->mq_freeze_depth == 1) {\n\t\tpercpu_ref_kill(&q->q_usage_counter);\n\t\tmutex_unlock(&q->mq_freeze_lock);\n\t\tif (queue_is_mq(q))\n\t\t\tblk_mq_run_hw_queues(q, false);\n\t} else {\n\t\tmutex_unlock(&q->mq_freeze_lock);\n\t}\n}\nEXPORT_SYMBOL_GPL(blk_freeze_queue_start);\n\nvoid blk_mq_freeze_queue_wait(struct request_queue *q)\n{\n\twait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));\n}\nEXPORT_SYMBOL_GPL(blk_mq_freeze_queue_wait);\n\nint blk_mq_freeze_queue_wait_timeout(struct request_queue *q,\n\t\t\t\t     unsigned long timeout)\n{\n\treturn wait_event_timeout(q->mq_freeze_wq,\n\t\t\t\t\tpercpu_ref_is_zero(&q->q_usage_counter),\n\t\t\t\t\ttimeout);\n}\nEXPORT_SYMBOL_GPL(blk_mq_freeze_queue_wait_timeout);\n\n/*\n * Guarantee no request is in use, so we can change any data structure of\n * the queue afterward.\n */\nvoid blk_freeze_queue(struct request_queue *q)\n{\n\t/*\n\t * In the !blk_mq case we are only calling this to kill the\n\t * q_usage_counter, otherwise this increases the freeze depth\n\t * and waits for it to return to zero.  For this reason there is\n\t * no blk_unfreeze_queue(), and blk_freeze_queue() is not\n\t * exported to drivers as the only user for unfreeze is blk_mq.\n\t */\n\tblk_freeze_queue_start(q);\n\tblk_mq_freeze_queue_wait(q);\n}\n\nvoid blk_mq_freeze_queue(struct request_queue *q)\n{\n\t/*\n\t * ...just an alias to keep freeze and unfreeze actions balanced\n\t * in the blk_mq_* namespace\n\t */\n\tblk_freeze_queue(q);\n}\nEXPORT_SYMBOL_GPL(blk_mq_freeze_queue);\n\nvoid blk_mq_unfreeze_queue(struct request_queue *q)\n{\n\tmutex_lock(&q->mq_freeze_lock);\n\tq->mq_freeze_depth--;\n\tWARN_ON_ONCE(q->mq_freeze_depth < 0);\n\tif (!q->mq_freeze_depth) {\n\t\tpercpu_ref_resurrect(&q->q_usage_counter);\n\t\twake_up_all(&q->mq_freeze_wq);\n\t}\n\tmutex_unlock(&q->mq_freeze_lock);\n}\nEXPORT_SYMBOL_GPL(blk_mq_unfreeze_queue);\n\n/*\n * FIXME: replace the scsi_internal_device_*block_nowait() calls in the\n * mpt3sas driver such that this function can be removed.\n */\nvoid blk_mq_quiesce_queue_nowait(struct request_queue *q)\n{\n\tblk_queue_flag_set(QUEUE_FLAG_QUIESCED, q);\n}\nEXPORT_SYMBOL_GPL(blk_mq_quiesce_queue_nowait);\n\n/**\n * blk_mq_quiesce_queue() - wait until all ongoing dispatches have finished\n * @q: request queue.\n *\n * Note: this function does not prevent that the struct request end_io()\n * callback function is invoked. Once this function is returned, we make\n * sure no dispatch can happen until the queue is unquiesced via\n * blk_mq_unquiesce_queue().\n */\nvoid blk_mq_quiesce_queue(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned int i;\n\tbool rcu = false;\n\n\tblk_mq_quiesce_queue_nowait(q);\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (hctx->flags & BLK_MQ_F_BLOCKING)\n\t\t\tsynchronize_srcu(hctx->srcu);\n\t\telse\n\t\t\trcu = true;\n\t}\n\tif (rcu)\n\t\tsynchronize_rcu();\n}\nEXPORT_SYMBOL_GPL(blk_mq_quiesce_queue);\n\n/*\n * blk_mq_unquiesce_queue() - counterpart of blk_mq_quiesce_queue()\n * @q: request queue.\n *\n * This function recovers queue into the state before quiescing\n * which is done by blk_mq_quiesce_queue.\n */\nvoid blk_mq_unquiesce_queue(struct request_queue *q)\n{\n\tblk_queue_flag_clear(QUEUE_FLAG_QUIESCED, q);\n\n\t/* dispatch requests which are inserted during quiescing */\n\tblk_mq_run_hw_queues(q, true);\n}\nEXPORT_SYMBOL_GPL(blk_mq_unquiesce_queue);\n\nvoid blk_mq_wake_waiters(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned int i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tif (blk_mq_hw_queue_mapped(hctx))\n\t\t\tblk_mq_tag_wakeup_all(hctx->tags, true);\n}\n\n/*\n * Only need start/end time stamping if we have iostat or\n * blk stats enabled, or using an IO scheduler.\n */\nstatic inline bool blk_mq_need_time_stamp(struct request *rq)\n{\n\treturn (rq->rq_flags & (RQF_IO_STAT | RQF_STATS)) || rq->q->elevator;\n}\n\nstatic struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,\n\t\tunsigned int tag, u64 alloc_time_ns)\n{\n\tstruct blk_mq_tags *tags = blk_mq_tags_from_data(data);\n\tstruct request *rq = tags->static_rqs[tag];\n\n\tif (data->q->elevator) {\n\t\trq->tag = BLK_MQ_NO_TAG;\n\t\trq->internal_tag = tag;\n\t} else {\n\t\trq->tag = tag;\n\t\trq->internal_tag = BLK_MQ_NO_TAG;\n\t}\n\n\t/* csd/requeue_work/fifo_time is initialized before use */\n\trq->q = data->q;\n\trq->mq_ctx = data->ctx;\n\trq->mq_hctx = data->hctx;\n\trq->rq_flags = 0;\n\trq->cmd_flags = data->cmd_flags;\n\tif (data->flags & BLK_MQ_REQ_PREEMPT)\n\t\trq->rq_flags |= RQF_PREEMPT;\n\tif (blk_queue_io_stat(data->q))\n\t\trq->rq_flags |= RQF_IO_STAT;\n\tINIT_LIST_HEAD(&rq->queuelist);\n\tINIT_HLIST_NODE(&rq->hash);\n\tRB_CLEAR_NODE(&rq->rb_node);\n\trq->rq_disk = NULL;\n\trq->part = NULL;\n#ifdef CONFIG_BLK_RQ_ALLOC_TIME\n\trq->alloc_time_ns = alloc_time_ns;\n#endif\n\tif (blk_mq_need_time_stamp(rq))\n\t\trq->start_time_ns = ktime_get_ns();\n\telse\n\t\trq->start_time_ns = 0;\n\trq->io_start_time_ns = 0;\n\trq->stats_sectors = 0;\n\trq->nr_phys_segments = 0;\n#if defined(CONFIG_BLK_DEV_INTEGRITY)\n\trq->nr_integrity_segments = 0;\n#endif\n\tblk_crypto_rq_set_defaults(rq);\n\t/* tag was already set */\n\tWRITE_ONCE(rq->deadline, 0);\n\n\trq->timeout = 0;\n\n\trq->end_io = NULL;\n\trq->end_io_data = NULL;\n\n\tdata->ctx->rq_dispatched[op_is_sync(data->cmd_flags)]++;\n\trefcount_set(&rq->ref, 1);\n\n\tif (!op_is_flush(data->cmd_flags)) {\n\t\tstruct elevator_queue *e = data->q->elevator;\n\n\t\trq->elv.icq = NULL;\n\t\tif (e && e->type->ops.prepare_request) {\n\t\t\tif (e->type->icq_cache)\n\t\t\t\tblk_mq_sched_assign_ioc(rq);\n\n\t\t\te->type->ops.prepare_request(rq);\n\t\t\trq->rq_flags |= RQF_ELVPRIV;\n\t\t}\n\t}\n\n\tdata->hctx->queued++;\n\treturn rq;\n}\n\nstatic struct request *__blk_mq_alloc_request(struct blk_mq_alloc_data *data)\n{\n\tstruct request_queue *q = data->q;\n\tstruct elevator_queue *e = q->elevator;\n\tu64 alloc_time_ns = 0;\n\tunsigned int tag;\n\n\t/* alloc_time includes depth and tag waits */\n\tif (blk_queue_rq_alloc_time(q))\n\t\talloc_time_ns = ktime_get_ns();\n\n\tif (data->cmd_flags & REQ_NOWAIT)\n\t\tdata->flags |= BLK_MQ_REQ_NOWAIT;\n\n\tif (e) {\n\t\t/*\n\t\t * Flush requests are special and go directly to the\n\t\t * dispatch list. Don't include reserved tags in the\n\t\t * limiting, as it isn't useful.\n\t\t */\n\t\tif (!op_is_flush(data->cmd_flags) &&\n\t\t    e->type->ops.limit_depth &&\n\t\t    !(data->flags & BLK_MQ_REQ_RESERVED))\n\t\t\te->type->ops.limit_depth(data->cmd_flags, data);\n\t}\n\nretry:\n\tdata->ctx = blk_mq_get_ctx(q);\n\tdata->hctx = blk_mq_map_queue(q, data->cmd_flags, data->ctx);\n\tif (!e)\n\t\tblk_mq_tag_busy(data->hctx);\n\n\t/*\n\t * Waiting allocations only fail because of an inactive hctx.  In that\n\t * case just retry the hctx assignment and tag allocation as CPU hotplug\n\t * should have migrated us to an online CPU by now.\n\t */\n\ttag = blk_mq_get_tag(data);\n\tif (tag == BLK_MQ_NO_TAG) {\n\t\tif (data->flags & BLK_MQ_REQ_NOWAIT)\n\t\t\treturn NULL;\n\n\t\t/*\n\t\t * Give up the CPU and sleep for a random short time to ensure\n\t\t * that thread using a realtime scheduling class are migrated\n\t\t * off the CPU, and thus off the hctx that is going away.\n\t\t */\n\t\tmsleep(3);\n\t\tgoto retry;\n\t}\n\treturn blk_mq_rq_ctx_init(data, tag, alloc_time_ns);\n}\n\nstruct request *blk_mq_alloc_request(struct request_queue *q, unsigned int op,\n\t\tblk_mq_req_flags_t flags)\n{\n\tstruct blk_mq_alloc_data data = {\n\t\t.q\t\t= q,\n\t\t.flags\t\t= flags,\n\t\t.cmd_flags\t= op,\n\t};\n\tstruct request *rq;\n\tint ret;\n\n\tret = blk_queue_enter(q, flags);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\trq = __blk_mq_alloc_request(&data);\n\tif (!rq)\n\t\tgoto out_queue_exit;\n\trq->__data_len = 0;\n\trq->__sector = (sector_t) -1;\n\trq->bio = rq->biotail = NULL;\n\treturn rq;\nout_queue_exit:\n\tblk_queue_exit(q);\n\treturn ERR_PTR(-EWOULDBLOCK);\n}\nEXPORT_SYMBOL(blk_mq_alloc_request);\n\nstruct request *blk_mq_alloc_request_hctx(struct request_queue *q,\n\tunsigned int op, blk_mq_req_flags_t flags, unsigned int hctx_idx)\n{\n\tstruct blk_mq_alloc_data data = {\n\t\t.q\t\t= q,\n\t\t.flags\t\t= flags,\n\t\t.cmd_flags\t= op,\n\t};\n\tu64 alloc_time_ns = 0;\n\tunsigned int cpu;\n\tunsigned int tag;\n\tint ret;\n\n\t/* alloc_time includes depth and tag waits */\n\tif (blk_queue_rq_alloc_time(q))\n\t\talloc_time_ns = ktime_get_ns();\n\n\t/*\n\t * If the tag allocator sleeps we could get an allocation for a\n\t * different hardware context.  No need to complicate the low level\n\t * allocator for this for the rare use case of a command tied to\n\t * a specific queue.\n\t */\n\tif (WARN_ON_ONCE(!(flags & (BLK_MQ_REQ_NOWAIT | BLK_MQ_REQ_RESERVED))))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (hctx_idx >= q->nr_hw_queues)\n\t\treturn ERR_PTR(-EIO);\n\n\tret = blk_queue_enter(q, flags);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\t/*\n\t * Check if the hardware context is actually mapped to anything.\n\t * If not tell the caller that it should skip this queue.\n\t */\n\tret = -EXDEV;\n\tdata.hctx = q->queue_hw_ctx[hctx_idx];\n\tif (!blk_mq_hw_queue_mapped(data.hctx))\n\t\tgoto out_queue_exit;\n\tcpu = cpumask_first_and(data.hctx->cpumask, cpu_online_mask);\n\tdata.ctx = __blk_mq_get_ctx(q, cpu);\n\n\tif (!q->elevator)\n\t\tblk_mq_tag_busy(data.hctx);\n\n\tret = -EWOULDBLOCK;\n\ttag = blk_mq_get_tag(&data);\n\tif (tag == BLK_MQ_NO_TAG)\n\t\tgoto out_queue_exit;\n\treturn blk_mq_rq_ctx_init(&data, tag, alloc_time_ns);\n\nout_queue_exit:\n\tblk_queue_exit(q);\n\treturn ERR_PTR(ret);\n}\nEXPORT_SYMBOL_GPL(blk_mq_alloc_request_hctx);\n\nstatic void __blk_mq_free_request(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct blk_mq_ctx *ctx = rq->mq_ctx;\n\tstruct blk_mq_hw_ctx *hctx = rq->mq_hctx;\n\tconst int sched_tag = rq->internal_tag;\n\n\tblk_crypto_free_request(rq);\n\tblk_pm_mark_last_busy(rq);\n\trq->mq_hctx = NULL;\n\tif (rq->tag != BLK_MQ_NO_TAG)\n\t\tblk_mq_put_tag(hctx->tags, ctx, rq->tag);\n\tif (sched_tag != BLK_MQ_NO_TAG)\n\t\tblk_mq_put_tag(hctx->sched_tags, ctx, sched_tag);\n\tblk_mq_sched_restart(hctx);\n\tblk_queue_exit(q);\n}\n\nvoid blk_mq_free_request(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct elevator_queue *e = q->elevator;\n\tstruct blk_mq_ctx *ctx = rq->mq_ctx;\n\tstruct blk_mq_hw_ctx *hctx = rq->mq_hctx;\n\n\tif (rq->rq_flags & RQF_ELVPRIV) {\n\t\tif (e && e->type->ops.finish_request)\n\t\t\te->type->ops.finish_request(rq);\n\t\tif (rq->elv.icq) {\n\t\t\tput_io_context(rq->elv.icq->ioc);\n\t\t\trq->elv.icq = NULL;\n\t\t}\n\t}\n\n\tctx->rq_completed[rq_is_sync(rq)]++;\n\tif (rq->rq_flags & RQF_MQ_INFLIGHT)\n\t\t__blk_mq_dec_active_requests(hctx);\n\n\tif (unlikely(laptop_mode && !blk_rq_is_passthrough(rq)))\n\t\tlaptop_io_completion(q->backing_dev_info);\n\n\trq_qos_done(q, rq);\n\n\tWRITE_ONCE(rq->state, MQ_RQ_IDLE);\n\tif (refcount_dec_and_test(&rq->ref))\n\t\t__blk_mq_free_request(rq);\n}\nEXPORT_SYMBOL_GPL(blk_mq_free_request);\n\ninline void __blk_mq_end_request(struct request *rq, blk_status_t error)\n{\n\tu64 now = 0;\n\n\tif (blk_mq_need_time_stamp(rq))\n\t\tnow = ktime_get_ns();\n\n\tif (rq->rq_flags & RQF_STATS) {\n\t\tblk_mq_poll_stats_start(rq->q);\n\t\tblk_stat_add(rq, now);\n\t}\n\n\tblk_mq_sched_completed_request(rq, now);\n\n\tblk_account_io_done(rq, now);\n\n\tif (rq->end_io) {\n\t\trq_qos_done(rq->q, rq);\n\t\trq->end_io(rq, error);\n\t} else {\n\t\tblk_mq_free_request(rq);\n\t}\n}\nEXPORT_SYMBOL(__blk_mq_end_request);\n\nvoid blk_mq_end_request(struct request *rq, blk_status_t error)\n{\n\tif (blk_update_request(rq, error, blk_rq_bytes(rq)))\n\t\tBUG();\n\t__blk_mq_end_request(rq, error);\n}\nEXPORT_SYMBOL(blk_mq_end_request);\n\n/*\n * Softirq action handler - move entries to local list and loop over them\n * while passing them to the queue registered handler.\n */\nstatic __latent_entropy void blk_done_softirq(struct softirq_action *h)\n{\n\tstruct list_head *cpu_list, local_list;\n\n\tlocal_irq_disable();\n\tcpu_list = this_cpu_ptr(&blk_cpu_done);\n\tlist_replace_init(cpu_list, &local_list);\n\tlocal_irq_enable();\n\n\twhile (!list_empty(&local_list)) {\n\t\tstruct request *rq;\n\n\t\trq = list_entry(local_list.next, struct request, ipi_list);\n\t\tlist_del_init(&rq->ipi_list);\n\t\trq->q->mq_ops->complete(rq);\n\t}\n}\n\nstatic void blk_mq_trigger_softirq(struct request *rq)\n{\n\tstruct list_head *list;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tlist = this_cpu_ptr(&blk_cpu_done);\n\tlist_add_tail(&rq->ipi_list, list);\n\n\t/*\n\t * If the list only contains our just added request, signal a raise of\n\t * the softirq.  If there are already entries there, someone already\n\t * raised the irq but it hasn't run yet.\n\t */\n\tif (list->next == &rq->ipi_list)\n\t\traise_softirq_irqoff(BLOCK_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\n\nstatic int blk_softirq_cpu_dead(unsigned int cpu)\n{\n\t/*\n\t * If a CPU goes away, splice its entries to the current CPU\n\t * and trigger a run of the softirq\n\t */\n\tlocal_irq_disable();\n\tlist_splice_init(&per_cpu(blk_cpu_done, cpu),\n\t\t\t this_cpu_ptr(&blk_cpu_done));\n\traise_softirq_irqoff(BLOCK_SOFTIRQ);\n\tlocal_irq_enable();\n\n\treturn 0;\n}\n\n\nstatic void __blk_mq_complete_request_remote(void *data)\n{\n\tstruct request *rq = data;\n\n\t/*\n\t * For most of single queue controllers, there is only one irq vector\n\t * for handling I/O completion, and the only irq's affinity is set\n\t * to all possible CPUs.  On most of ARCHs, this affinity means the irq\n\t * is handled on one specific CPU.\n\t *\n\t * So complete I/O requests in softirq context in case of single queue\n\t * devices to avoid degrading I/O performance due to irqsoff latency.\n\t */\n\tif (rq->q->nr_hw_queues == 1)\n\t\tblk_mq_trigger_softirq(rq);\n\telse\n\t\trq->q->mq_ops->complete(rq);\n}\n\nstatic inline bool blk_mq_complete_need_ipi(struct request *rq)\n{\n\tint cpu = raw_smp_processor_id();\n\n\tif (!IS_ENABLED(CONFIG_SMP) ||\n\t    !test_bit(QUEUE_FLAG_SAME_COMP, &rq->q->queue_flags))\n\t\treturn false;\n\n\t/* same CPU or cache domain?  Complete locally */\n\tif (cpu == rq->mq_ctx->cpu ||\n\t    (!test_bit(QUEUE_FLAG_SAME_FORCE, &rq->q->queue_flags) &&\n\t     cpus_share_cache(cpu, rq->mq_ctx->cpu)))\n\t\treturn false;\n\n\t/* don't try to IPI to an offline CPU */\n\treturn cpu_online(rq->mq_ctx->cpu);\n}\n\nbool blk_mq_complete_request_remote(struct request *rq)\n{\n\tWRITE_ONCE(rq->state, MQ_RQ_COMPLETE);\n\n\t/*\n\t * For a polled request, always complete locallly, it's pointless\n\t * to redirect the completion.\n\t */\n\tif (rq->cmd_flags & REQ_HIPRI)\n\t\treturn false;\n\n\tif (blk_mq_complete_need_ipi(rq)) {\n\t\trq->csd.func = __blk_mq_complete_request_remote;\n\t\trq->csd.info = rq;\n\t\trq->csd.flags = 0;\n\t\tsmp_call_function_single_async(rq->mq_ctx->cpu, &rq->csd);\n\t} else {\n\t\tif (rq->q->nr_hw_queues > 1)\n\t\t\treturn false;\n\t\tblk_mq_trigger_softirq(rq);\n\t}\n\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(blk_mq_complete_request_remote);\n\n/**\n * blk_mq_complete_request - end I/O on a request\n * @rq:\t\tthe request being processed\n *\n * Description:\n *\tComplete a request by scheduling the ->complete_rq operation.\n **/\nvoid blk_mq_complete_request(struct request *rq)\n{\n\tif (!blk_mq_complete_request_remote(rq))\n\t\trq->q->mq_ops->complete(rq);\n}\nEXPORT_SYMBOL(blk_mq_complete_request);\n\nstatic void hctx_unlock(struct blk_mq_hw_ctx *hctx, int srcu_idx)\n\t__releases(hctx->srcu)\n{\n\tif (!(hctx->flags & BLK_MQ_F_BLOCKING))\n\t\trcu_read_unlock();\n\telse\n\t\tsrcu_read_unlock(hctx->srcu, srcu_idx);\n}\n\nstatic void hctx_lock(struct blk_mq_hw_ctx *hctx, int *srcu_idx)\n\t__acquires(hctx->srcu)\n{\n\tif (!(hctx->flags & BLK_MQ_F_BLOCKING)) {\n\t\t/* shut up gcc false positive */\n\t\t*srcu_idx = 0;\n\t\trcu_read_lock();\n\t} else\n\t\t*srcu_idx = srcu_read_lock(hctx->srcu);\n}\n\n/**\n * blk_mq_start_request - Start processing a request\n * @rq: Pointer to request to be started\n *\n * Function used by device drivers to notify the block layer that a request\n * is going to be processed now, so blk layer can do proper initializations\n * such as starting the timeout timer.\n */\nvoid blk_mq_start_request(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\n\ttrace_block_rq_issue(q, rq);\n\n\tif (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {\n\t\trq->io_start_time_ns = ktime_get_ns();\n\t\trq->stats_sectors = blk_rq_sectors(rq);\n\t\trq->rq_flags |= RQF_STATS;\n\t\trq_qos_issue(q, rq);\n\t}\n\n\tWARN_ON_ONCE(blk_mq_rq_state(rq) != MQ_RQ_IDLE);\n\n\tblk_add_timer(rq);\n\tWRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT);\n\n#ifdef CONFIG_BLK_DEV_INTEGRITY\n\tif (blk_integrity_rq(rq) && req_op(rq) == REQ_OP_WRITE)\n\t\tq->integrity.profile->prepare_fn(rq);\n#endif\n}\nEXPORT_SYMBOL(blk_mq_start_request);\n\nstatic void __blk_mq_requeue_request(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\n\tblk_mq_put_driver_tag(rq);\n\n\ttrace_block_rq_requeue(q, rq);\n\trq_qos_requeue(q, rq);\n\n\tif (blk_mq_request_started(rq)) {\n\t\tWRITE_ONCE(rq->state, MQ_RQ_IDLE);\n\t\trq->rq_flags &= ~RQF_TIMED_OUT;\n\t}\n}\n\nvoid blk_mq_requeue_request(struct request *rq, bool kick_requeue_list)\n{\n\t__blk_mq_requeue_request(rq);\n\n\t/* this request will be re-inserted to io scheduler queue */\n\tblk_mq_sched_requeue_request(rq);\n\n\tBUG_ON(!list_empty(&rq->queuelist));\n\tblk_mq_add_to_requeue_list(rq, true, kick_requeue_list);\n}\nEXPORT_SYMBOL(blk_mq_requeue_request);\n\nstatic void blk_mq_requeue_work(struct work_struct *work)\n{\n\tstruct request_queue *q =\n\t\tcontainer_of(work, struct request_queue, requeue_work.work);\n\tLIST_HEAD(rq_list);\n\tstruct request *rq, *next;\n\n\tspin_lock_irq(&q->requeue_lock);\n\tlist_splice_init(&q->requeue_list, &rq_list);\n\tspin_unlock_irq(&q->requeue_lock);\n\n\tlist_for_each_entry_safe(rq, next, &rq_list, queuelist) {\n\t\tif (!(rq->rq_flags & (RQF_SOFTBARRIER | RQF_DONTPREP)))\n\t\t\tcontinue;\n\n\t\trq->rq_flags &= ~RQF_SOFTBARRIER;\n\t\tlist_del_init(&rq->queuelist);\n\t\t/*\n\t\t * If RQF_DONTPREP, rq has contained some driver specific\n\t\t * data, so insert it to hctx dispatch list to avoid any\n\t\t * merge.\n\t\t */\n\t\tif (rq->rq_flags & RQF_DONTPREP)\n\t\t\tblk_mq_request_bypass_insert(rq, false, false);\n\t\telse\n\t\t\tblk_mq_sched_insert_request(rq, true, false, false);\n\t}\n\n\twhile (!list_empty(&rq_list)) {\n\t\trq = list_entry(rq_list.next, struct request, queuelist);\n\t\tlist_del_init(&rq->queuelist);\n\t\tblk_mq_sched_insert_request(rq, false, false, false);\n\t}\n\n\tblk_mq_run_hw_queues(q, false);\n}\n\nvoid blk_mq_add_to_requeue_list(struct request *rq, bool at_head,\n\t\t\t\tbool kick_requeue_list)\n{\n\tstruct request_queue *q = rq->q;\n\tunsigned long flags;\n\n\t/*\n\t * We abuse this flag that is otherwise used by the I/O scheduler to\n\t * request head insertion from the workqueue.\n\t */\n\tBUG_ON(rq->rq_flags & RQF_SOFTBARRIER);\n\n\tspin_lock_irqsave(&q->requeue_lock, flags);\n\tif (at_head) {\n\t\trq->rq_flags |= RQF_SOFTBARRIER;\n\t\tlist_add(&rq->queuelist, &q->requeue_list);\n\t} else {\n\t\tlist_add_tail(&rq->queuelist, &q->requeue_list);\n\t}\n\tspin_unlock_irqrestore(&q->requeue_lock, flags);\n\n\tif (kick_requeue_list)\n\t\tblk_mq_kick_requeue_list(q);\n}\n\nvoid blk_mq_kick_requeue_list(struct request_queue *q)\n{\n\tkblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work, 0);\n}\nEXPORT_SYMBOL(blk_mq_kick_requeue_list);\n\nvoid blk_mq_delay_kick_requeue_list(struct request_queue *q,\n\t\t\t\t    unsigned long msecs)\n{\n\tkblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work,\n\t\t\t\t    msecs_to_jiffies(msecs));\n}\nEXPORT_SYMBOL(blk_mq_delay_kick_requeue_list);\n\nstruct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n{\n\tif (tag < tags->nr_tags) {\n\t\tprefetch(tags->rqs[tag]);\n\t\treturn tags->rqs[tag];\n\t}\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(blk_mq_tag_to_rq);\n\nstatic bool blk_mq_rq_inflight(struct blk_mq_hw_ctx *hctx, struct request *rq,\n\t\t\t       void *priv, bool reserved)\n{\n\t/*\n\t * If we find a request that isn't idle and the queue matches,\n\t * we know the queue is busy. Return false to stop the iteration.\n\t */\n\tif (blk_mq_request_started(rq) && rq->q == hctx->queue) {\n\t\tbool *busy = priv;\n\n\t\t*busy = true;\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nbool blk_mq_queue_inflight(struct request_queue *q)\n{\n\tbool busy = false;\n\n\tblk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);\n\treturn busy;\n}\nEXPORT_SYMBOL_GPL(blk_mq_queue_inflight);\n\nstatic void blk_mq_rq_timed_out(struct request *req, bool reserved)\n{\n\treq->rq_flags |= RQF_TIMED_OUT;\n\tif (req->q->mq_ops->timeout) {\n\t\tenum blk_eh_timer_return ret;\n\n\t\tret = req->q->mq_ops->timeout(req, reserved);\n\t\tif (ret == BLK_EH_DONE)\n\t\t\treturn;\n\t\tWARN_ON_ONCE(ret != BLK_EH_RESET_TIMER);\n\t}\n\n\tblk_add_timer(req);\n}\n\nstatic bool blk_mq_req_expired(struct request *rq, unsigned long *next)\n{\n\tunsigned long deadline;\n\n\tif (blk_mq_rq_state(rq) != MQ_RQ_IN_FLIGHT)\n\t\treturn false;\n\tif (rq->rq_flags & RQF_TIMED_OUT)\n\t\treturn false;\n\n\tdeadline = READ_ONCE(rq->deadline);\n\tif (time_after_eq(jiffies, deadline))\n\t\treturn true;\n\n\tif (*next == 0)\n\t\t*next = deadline;\n\telse if (time_after(*next, deadline))\n\t\t*next = deadline;\n\treturn false;\n}\n\nstatic bool blk_mq_check_expired(struct blk_mq_hw_ctx *hctx,\n\t\tstruct request *rq, void *priv, bool reserved)\n{\n\tunsigned long *next = priv;\n\n\t/*\n\t * Just do a quick check if it is expired before locking the request in\n\t * so we're not unnecessarilly synchronizing across CPUs.\n\t */\n\tif (!blk_mq_req_expired(rq, next))\n\t\treturn true;\n\n\t/*\n\t * We have reason to believe the request may be expired. Take a\n\t * reference on the request to lock this request lifetime into its\n\t * currently allocated context to prevent it from being reallocated in\n\t * the event the completion by-passes this timeout handler.\n\t *\n\t * If the reference was already released, then the driver beat the\n\t * timeout handler to posting a natural completion.\n\t */\n\tif (!refcount_inc_not_zero(&rq->ref))\n\t\treturn true;\n\n\t/*\n\t * The request is now locked and cannot be reallocated underneath the\n\t * timeout handler's processing. Re-verify this exact request is truly\n\t * expired; if it is not expired, then the request was completed and\n\t * reallocated as a new request.\n\t */\n\tif (blk_mq_req_expired(rq, next))\n\t\tblk_mq_rq_timed_out(rq, reserved);\n\n\tif (is_flush_rq(rq, hctx))\n\t\trq->end_io(rq, 0);\n\telse if (refcount_dec_and_test(&rq->ref))\n\t\t__blk_mq_free_request(rq);\n\n\treturn true;\n}\n\nstatic void blk_mq_timeout_work(struct work_struct *work)\n{\n\tstruct request_queue *q =\n\t\tcontainer_of(work, struct request_queue, timeout_work);\n\tunsigned long next = 0;\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\t/* A deadlock might occur if a request is stuck requiring a\n\t * timeout at the same time a queue freeze is waiting\n\t * completion, since the timeout code would not be able to\n\t * acquire the queue reference here.\n\t *\n\t * That's why we don't use blk_queue_enter here; instead, we use\n\t * percpu_ref_tryget directly, because we need to be able to\n\t * obtain a reference even in the short window between the queue\n\t * starting to freeze, by dropping the first reference in\n\t * blk_freeze_queue_start, and the moment the last request is\n\t * consumed, marked by the instant q_usage_counter reaches\n\t * zero.\n\t */\n\tif (!percpu_ref_tryget(&q->q_usage_counter))\n\t\treturn;\n\n\tblk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);\n\n\tif (next != 0) {\n\t\tmod_timer(&q->timeout, next);\n\t} else {\n\t\t/*\n\t\t * Request timeouts are handled as a forward rolling timer. If\n\t\t * we end up here it means that no requests are pending and\n\t\t * also that no request has been pending for a while. Mark\n\t\t * each hctx as idle.\n\t\t */\n\t\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\t\t/* the hctx may be unmapped, so check it here */\n\t\t\tif (blk_mq_hw_queue_mapped(hctx))\n\t\t\t\tblk_mq_tag_idle(hctx);\n\t\t}\n\t}\n\tblk_queue_exit(q);\n}\n\nstruct flush_busy_ctx_data {\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct list_head *list;\n};\n\nstatic bool flush_busy_ctx(struct sbitmap *sb, unsigned int bitnr, void *data)\n{\n\tstruct flush_busy_ctx_data *flush_data = data;\n\tstruct blk_mq_hw_ctx *hctx = flush_data->hctx;\n\tstruct blk_mq_ctx *ctx = hctx->ctxs[bitnr];\n\tenum hctx_type type = hctx->type;\n\n\tspin_lock(&ctx->lock);\n\tlist_splice_tail_init(&ctx->rq_lists[type], flush_data->list);\n\tsbitmap_clear_bit(sb, bitnr);\n\tspin_unlock(&ctx->lock);\n\treturn true;\n}\n\n/*\n * Process software queues that have been marked busy, splicing them\n * to the for-dispatch\n */\nvoid blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list)\n{\n\tstruct flush_busy_ctx_data data = {\n\t\t.hctx = hctx,\n\t\t.list = list,\n\t};\n\n\tsbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);\n}\nEXPORT_SYMBOL_GPL(blk_mq_flush_busy_ctxs);\n\nstruct dispatch_rq_data {\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct request *rq;\n};\n\nstatic bool dispatch_rq_from_ctx(struct sbitmap *sb, unsigned int bitnr,\n\t\tvoid *data)\n{\n\tstruct dispatch_rq_data *dispatch_data = data;\n\tstruct blk_mq_hw_ctx *hctx = dispatch_data->hctx;\n\tstruct blk_mq_ctx *ctx = hctx->ctxs[bitnr];\n\tenum hctx_type type = hctx->type;\n\n\tspin_lock(&ctx->lock);\n\tif (!list_empty(&ctx->rq_lists[type])) {\n\t\tdispatch_data->rq = list_entry_rq(ctx->rq_lists[type].next);\n\t\tlist_del_init(&dispatch_data->rq->queuelist);\n\t\tif (list_empty(&ctx->rq_lists[type]))\n\t\t\tsbitmap_clear_bit(sb, bitnr);\n\t}\n\tspin_unlock(&ctx->lock);\n\n\treturn !dispatch_data->rq;\n}\n\nstruct request *blk_mq_dequeue_from_ctx(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t\tstruct blk_mq_ctx *start)\n{\n\tunsigned off = start ? start->index_hw[hctx->type] : 0;\n\tstruct dispatch_rq_data data = {\n\t\t.hctx = hctx,\n\t\t.rq   = NULL,\n\t};\n\n\t__sbitmap_for_each_set(&hctx->ctx_map, off,\n\t\t\t       dispatch_rq_from_ctx, &data);\n\n\treturn data.rq;\n}\n\nstatic inline unsigned int queued_to_index(unsigned int queued)\n{\n\tif (!queued)\n\t\treturn 0;\n\n\treturn min(BLK_MQ_MAX_DISPATCH_ORDER - 1, ilog2(queued) + 1);\n}\n\nstatic bool __blk_mq_get_driver_tag(struct request *rq)\n{\n\tstruct sbitmap_queue *bt = rq->mq_hctx->tags->bitmap_tags;\n\tunsigned int tag_offset = rq->mq_hctx->tags->nr_reserved_tags;\n\tint tag;\n\n\tblk_mq_tag_busy(rq->mq_hctx);\n\n\tif (blk_mq_tag_is_reserved(rq->mq_hctx->sched_tags, rq->internal_tag)) {\n\t\tbt = rq->mq_hctx->tags->breserved_tags;\n\t\ttag_offset = 0;\n\t} else {\n\t\tif (!hctx_may_queue(rq->mq_hctx, bt))\n\t\t\treturn false;\n\t}\n\n\ttag = __sbitmap_queue_get(bt);\n\tif (tag == BLK_MQ_NO_TAG)\n\t\treturn false;\n\n\trq->tag = tag + tag_offset;\n\treturn true;\n}\n\nstatic bool blk_mq_get_driver_tag(struct request *rq)\n{\n\tstruct blk_mq_hw_ctx *hctx = rq->mq_hctx;\n\n\tif (rq->tag == BLK_MQ_NO_TAG && !__blk_mq_get_driver_tag(rq))\n\t\treturn false;\n\n\tif ((hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED) &&\n\t\t\t!(rq->rq_flags & RQF_MQ_INFLIGHT)) {\n\t\trq->rq_flags |= RQF_MQ_INFLIGHT;\n\t\t__blk_mq_inc_active_requests(hctx);\n\t}\n\thctx->tags->rqs[rq->tag] = rq;\n\treturn true;\n}\n\nstatic int blk_mq_dispatch_wake(wait_queue_entry_t *wait, unsigned mode,\n\t\t\t\tint flags, void *key)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\n\thctx = container_of(wait, struct blk_mq_hw_ctx, dispatch_wait);\n\n\tspin_lock(&hctx->dispatch_wait_lock);\n\tif (!list_empty(&wait->entry)) {\n\t\tstruct sbitmap_queue *sbq;\n\n\t\tlist_del_init(&wait->entry);\n\t\tsbq = hctx->tags->bitmap_tags;\n\t\tatomic_dec(&sbq->ws_active);\n\t}\n\tspin_unlock(&hctx->dispatch_wait_lock);\n\n\tblk_mq_run_hw_queue(hctx, true);\n\treturn 1;\n}\n\n/*\n * Mark us waiting for a tag. For shared tags, this involves hooking us into\n * the tag wakeups. For non-shared tags, we can simply mark us needing a\n * restart. For both cases, take care to check the condition again after\n * marking us as waiting.\n */\nstatic bool blk_mq_mark_tag_wait(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t struct request *rq)\n{\n\tstruct sbitmap_queue *sbq = hctx->tags->bitmap_tags;\n\tstruct wait_queue_head *wq;\n\twait_queue_entry_t *wait;\n\tbool ret;\n\n\tif (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {\n\t\tblk_mq_sched_mark_restart_hctx(hctx);\n\n\t\t/*\n\t\t * It's possible that a tag was freed in the window between the\n\t\t * allocation failure and adding the hardware queue to the wait\n\t\t * queue.\n\t\t *\n\t\t * Don't clear RESTART here, someone else could have set it.\n\t\t * At most this will cost an extra queue run.\n\t\t */\n\t\treturn blk_mq_get_driver_tag(rq);\n\t}\n\n\twait = &hctx->dispatch_wait;\n\tif (!list_empty_careful(&wait->entry))\n\t\treturn false;\n\n\twq = &bt_wait_ptr(sbq, hctx)->wait;\n\n\tspin_lock_irq(&wq->lock);\n\tspin_lock(&hctx->dispatch_wait_lock);\n\tif (!list_empty(&wait->entry)) {\n\t\tspin_unlock(&hctx->dispatch_wait_lock);\n\t\tspin_unlock_irq(&wq->lock);\n\t\treturn false;\n\t}\n\n\tatomic_inc(&sbq->ws_active);\n\twait->flags &= ~WQ_FLAG_EXCLUSIVE;\n\t__add_wait_queue(wq, wait);\n\n\t/*\n\t * It's possible that a tag was freed in the window between the\n\t * allocation failure and adding the hardware queue to the wait\n\t * queue.\n\t */\n\tret = blk_mq_get_driver_tag(rq);\n\tif (!ret) {\n\t\tspin_unlock(&hctx->dispatch_wait_lock);\n\t\tspin_unlock_irq(&wq->lock);\n\t\treturn false;\n\t}\n\n\t/*\n\t * We got a tag, remove ourselves from the wait queue to ensure\n\t * someone else gets the wakeup.\n\t */\n\tlist_del_init(&wait->entry);\n\tatomic_dec(&sbq->ws_active);\n\tspin_unlock(&hctx->dispatch_wait_lock);\n\tspin_unlock_irq(&wq->lock);\n\n\treturn true;\n}\n\n#define BLK_MQ_DISPATCH_BUSY_EWMA_WEIGHT  8\n#define BLK_MQ_DISPATCH_BUSY_EWMA_FACTOR  4\n/*\n * Update dispatch busy with the Exponential Weighted Moving Average(EWMA):\n * - EWMA is one simple way to compute running average value\n * - weight(7/8 and 1/8) is applied so that it can decrease exponentially\n * - take 4 as factor for avoiding to get too small(0) result, and this\n *   factor doesn't matter because EWMA decreases exponentially\n */\nstatic void blk_mq_update_dispatch_busy(struct blk_mq_hw_ctx *hctx, bool busy)\n{\n\tunsigned int ewma;\n\n\tif (hctx->queue->elevator)\n\t\treturn;\n\n\tewma = hctx->dispatch_busy;\n\n\tif (!ewma && !busy)\n\t\treturn;\n\n\tewma *= BLK_MQ_DISPATCH_BUSY_EWMA_WEIGHT - 1;\n\tif (busy)\n\t\tewma += 1 << BLK_MQ_DISPATCH_BUSY_EWMA_FACTOR;\n\tewma /= BLK_MQ_DISPATCH_BUSY_EWMA_WEIGHT;\n\n\thctx->dispatch_busy = ewma;\n}\n\n#define BLK_MQ_RESOURCE_DELAY\t3\t\t/* ms units */\n\nstatic void blk_mq_handle_dev_resource(struct request *rq,\n\t\t\t\t       struct list_head *list)\n{\n\tstruct request *next =\n\t\tlist_first_entry_or_null(list, struct request, queuelist);\n\n\t/*\n\t * If an I/O scheduler has been configured and we got a driver tag for\n\t * the next request already, free it.\n\t */\n\tif (next)\n\t\tblk_mq_put_driver_tag(next);\n\n\tlist_add(&rq->queuelist, list);\n\t__blk_mq_requeue_request(rq);\n}\n\nstatic void blk_mq_handle_zone_resource(struct request *rq,\n\t\t\t\t\tstruct list_head *zone_list)\n{\n\t/*\n\t * If we end up here it is because we cannot dispatch a request to a\n\t * specific zone due to LLD level zone-write locking or other zone\n\t * related resource not being available. In this case, set the request\n\t * aside in zone_list for retrying it later.\n\t */\n\tlist_add(&rq->queuelist, zone_list);\n\t__blk_mq_requeue_request(rq);\n}\n\nenum prep_dispatch {\n\tPREP_DISPATCH_OK,\n\tPREP_DISPATCH_NO_TAG,\n\tPREP_DISPATCH_NO_BUDGET,\n};\n\nstatic enum prep_dispatch blk_mq_prep_dispatch_rq(struct request *rq,\n\t\t\t\t\t\t  bool need_budget)\n{\n\tstruct blk_mq_hw_ctx *hctx = rq->mq_hctx;\n\n\tif (need_budget && !blk_mq_get_dispatch_budget(rq->q)) {\n\t\tblk_mq_put_driver_tag(rq);\n\t\treturn PREP_DISPATCH_NO_BUDGET;\n\t}\n\n\tif (!blk_mq_get_driver_tag(rq)) {\n\t\t/*\n\t\t * The initial allocation attempt failed, so we need to\n\t\t * rerun the hardware queue when a tag is freed. The\n\t\t * waitqueue takes care of that. If the queue is run\n\t\t * before we add this entry back on the dispatch list,\n\t\t * we'll re-run it below.\n\t\t */\n\t\tif (!blk_mq_mark_tag_wait(hctx, rq)) {\n\t\t\t/*\n\t\t\t * All budgets not got from this function will be put\n\t\t\t * together during handling partial dispatch\n\t\t\t */\n\t\t\tif (need_budget)\n\t\t\t\tblk_mq_put_dispatch_budget(rq->q);\n\t\t\treturn PREP_DISPATCH_NO_TAG;\n\t\t}\n\t}\n\n\treturn PREP_DISPATCH_OK;\n}\n\n/* release all allocated budgets before calling to blk_mq_dispatch_rq_list */\nstatic void blk_mq_release_budgets(struct request_queue *q,\n\t\tunsigned int nr_budgets)\n{\n\tint i;\n\n\tfor (i = 0; i < nr_budgets; i++)\n\t\tblk_mq_put_dispatch_budget(q);\n}\n\n/*\n * Returns true if we did some work AND can potentially do more.\n */\nbool blk_mq_dispatch_rq_list(struct blk_mq_hw_ctx *hctx, struct list_head *list,\n\t\t\t     unsigned int nr_budgets)\n{\n\tenum prep_dispatch prep;\n\tstruct request_queue *q = hctx->queue;\n\tstruct request *rq, *nxt;\n\tint errors, queued;\n\tblk_status_t ret = BLK_STS_OK;\n\tLIST_HEAD(zone_list);\n\n\tif (list_empty(list))\n\t\treturn false;\n\n\t/*\n\t * Now process all the entries, sending them to the driver.\n\t */\n\terrors = queued = 0;\n\tdo {\n\t\tstruct blk_mq_queue_data bd;\n\n\t\trq = list_first_entry(list, struct request, queuelist);\n\n\t\tWARN_ON_ONCE(hctx != rq->mq_hctx);\n\t\tprep = blk_mq_prep_dispatch_rq(rq, !nr_budgets);\n\t\tif (prep != PREP_DISPATCH_OK)\n\t\t\tbreak;\n\n\t\tlist_del_init(&rq->queuelist);\n\n\t\tbd.rq = rq;\n\n\t\t/*\n\t\t * Flag last if we have no more requests, or if we have more\n\t\t * but can't assign a driver tag to it.\n\t\t */\n\t\tif (list_empty(list))\n\t\t\tbd.last = true;\n\t\telse {\n\t\t\tnxt = list_first_entry(list, struct request, queuelist);\n\t\t\tbd.last = !blk_mq_get_driver_tag(nxt);\n\t\t}\n\n\t\t/*\n\t\t * once the request is queued to lld, no need to cover the\n\t\t * budget any more\n\t\t */\n\t\tif (nr_budgets)\n\t\t\tnr_budgets--;\n\t\tret = q->mq_ops->queue_rq(hctx, &bd);\n\t\tswitch (ret) {\n\t\tcase BLK_STS_OK:\n\t\t\tqueued++;\n\t\t\tbreak;\n\t\tcase BLK_STS_RESOURCE:\n\t\tcase BLK_STS_DEV_RESOURCE:\n\t\t\tblk_mq_handle_dev_resource(rq, list);\n\t\t\tgoto out;\n\t\tcase BLK_STS_ZONE_RESOURCE:\n\t\t\t/*\n\t\t\t * Move the request to zone_list and keep going through\n\t\t\t * the dispatch list to find more requests the drive can\n\t\t\t * accept.\n\t\t\t */\n\t\t\tblk_mq_handle_zone_resource(rq, &zone_list);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terrors++;\n\t\t\tblk_mq_end_request(rq, BLK_STS_IOERR);\n\t\t}\n\t} while (!list_empty(list));\nout:\n\tif (!list_empty(&zone_list))\n\t\tlist_splice_tail_init(&zone_list, list);\n\n\thctx->dispatched[queued_to_index(queued)]++;\n\n\t/* If we didn't flush the entire list, we could have told the driver\n\t * there was more coming, but that turned out to be a lie.\n\t */\n\tif ((!list_empty(list) || errors) && q->mq_ops->commit_rqs && queued)\n\t\tq->mq_ops->commit_rqs(hctx);\n\t/*\n\t * Any items that need requeuing? Stuff them into hctx->dispatch,\n\t * that is where we will continue on next queue run.\n\t */\n\tif (!list_empty(list)) {\n\t\tbool needs_restart;\n\t\t/* For non-shared tags, the RESTART check will suffice */\n\t\tbool no_tag = prep == PREP_DISPATCH_NO_TAG &&\n\t\t\t(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED);\n\t\tbool no_budget_avail = prep == PREP_DISPATCH_NO_BUDGET;\n\n\t\tblk_mq_release_budgets(q, nr_budgets);\n\n\t\tspin_lock(&hctx->lock);\n\t\tlist_splice_tail_init(list, &hctx->dispatch);\n\t\tspin_unlock(&hctx->lock);\n\n\t\t/*\n\t\t * Order adding requests to hctx->dispatch and checking\n\t\t * SCHED_RESTART flag. The pair of this smp_mb() is the one\n\t\t * in blk_mq_sched_restart(). Avoid restart code path to\n\t\t * miss the new added requests to hctx->dispatch, meantime\n\t\t * SCHED_RESTART is observed here.\n\t\t */\n\t\tsmp_mb();\n\n\t\t/*\n\t\t * If SCHED_RESTART was set by the caller of this function and\n\t\t * it is no longer set that means that it was cleared by another\n\t\t * thread and hence that a queue rerun is needed.\n\t\t *\n\t\t * If 'no_tag' is set, that means that we failed getting\n\t\t * a driver tag with an I/O scheduler attached. If our dispatch\n\t\t * waitqueue is no longer active, ensure that we run the queue\n\t\t * AFTER adding our entries back to the list.\n\t\t *\n\t\t * If no I/O scheduler has been configured it is possible that\n\t\t * the hardware queue got stopped and restarted before requests\n\t\t * were pushed back onto the dispatch list. Rerun the queue to\n\t\t * avoid starvation. Notes:\n\t\t * - blk_mq_run_hw_queue() checks whether or not a queue has\n\t\t *   been stopped before rerunning a queue.\n\t\t * - Some but not all block drivers stop a queue before\n\t\t *   returning BLK_STS_RESOURCE. Two exceptions are scsi-mq\n\t\t *   and dm-rq.\n\t\t *\n\t\t * If driver returns BLK_STS_RESOURCE and SCHED_RESTART\n\t\t * bit is set, run queue after a delay to avoid IO stalls\n\t\t * that could otherwise occur if the queue is idle.  We'll do\n\t\t * similar if we couldn't get budget and SCHED_RESTART is set.\n\t\t */\n\t\tneeds_restart = blk_mq_sched_needs_restart(hctx);\n\t\tif (!needs_restart ||\n\t\t    (no_tag && list_empty_careful(&hctx->dispatch_wait.entry)))\n\t\t\tblk_mq_run_hw_queue(hctx, true);\n\t\telse if (needs_restart && (ret == BLK_STS_RESOURCE ||\n\t\t\t\t\t   no_budget_avail))\n\t\t\tblk_mq_delay_run_hw_queue(hctx, BLK_MQ_RESOURCE_DELAY);\n\n\t\tblk_mq_update_dispatch_busy(hctx, true);\n\t\treturn false;\n\t} else\n\t\tblk_mq_update_dispatch_busy(hctx, false);\n\n\treturn (queued + errors) != 0;\n}\n\n/**\n * __blk_mq_run_hw_queue - Run a hardware queue.\n * @hctx: Pointer to the hardware queue to run.\n *\n * Send pending requests to the hardware.\n */\nstatic void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)\n{\n\tint srcu_idx;\n\n\t/*\n\t * We should be running this queue from one of the CPUs that\n\t * are mapped to it.\n\t *\n\t * There are at least two related races now between setting\n\t * hctx->next_cpu from blk_mq_hctx_next_cpu() and running\n\t * __blk_mq_run_hw_queue():\n\t *\n\t * - hctx->next_cpu is found offline in blk_mq_hctx_next_cpu(),\n\t *   but later it becomes online, then this warning is harmless\n\t *   at all\n\t *\n\t * - hctx->next_cpu is found online in blk_mq_hctx_next_cpu(),\n\t *   but later it becomes offline, then the warning can't be\n\t *   triggered, and we depend on blk-mq timeout handler to\n\t *   handle dispatched requests to this hctx\n\t */\n\tif (!cpumask_test_cpu(raw_smp_processor_id(), hctx->cpumask) &&\n\t\tcpu_online(hctx->next_cpu)) {\n\t\tprintk(KERN_WARNING \"run queue from wrong CPU %d, hctx %s\\n\",\n\t\t\traw_smp_processor_id(),\n\t\t\tcpumask_empty(hctx->cpumask) ? \"inactive\": \"active\");\n\t\tdump_stack();\n\t}\n\n\t/*\n\t * We can't run the queue inline with ints disabled. Ensure that\n\t * we catch bad users of this early.\n\t */\n\tWARN_ON_ONCE(in_interrupt());\n\n\tmight_sleep_if(hctx->flags & BLK_MQ_F_BLOCKING);\n\n\thctx_lock(hctx, &srcu_idx);\n\tblk_mq_sched_dispatch_requests(hctx);\n\thctx_unlock(hctx, srcu_idx);\n}\n\nstatic inline int blk_mq_first_mapped_cpu(struct blk_mq_hw_ctx *hctx)\n{\n\tint cpu = cpumask_first_and(hctx->cpumask, cpu_online_mask);\n\n\tif (cpu >= nr_cpu_ids)\n\t\tcpu = cpumask_first(hctx->cpumask);\n\treturn cpu;\n}\n\n/*\n * It'd be great if the workqueue API had a way to pass\n * in a mask and had some smarts for more clever placement.\n * For now we just round-robin here, switching for every\n * BLK_MQ_CPU_WORK_BATCH queued items.\n */\nstatic int blk_mq_hctx_next_cpu(struct blk_mq_hw_ctx *hctx)\n{\n\tbool tried = false;\n\tint next_cpu = hctx->next_cpu;\n\n\tif (hctx->queue->nr_hw_queues == 1)\n\t\treturn WORK_CPU_UNBOUND;\n\n\tif (--hctx->next_cpu_batch <= 0) {\nselect_cpu:\n\t\tnext_cpu = cpumask_next_and(next_cpu, hctx->cpumask,\n\t\t\t\tcpu_online_mask);\n\t\tif (next_cpu >= nr_cpu_ids)\n\t\t\tnext_cpu = blk_mq_first_mapped_cpu(hctx);\n\t\thctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;\n\t}\n\n\t/*\n\t * Do unbound schedule if we can't find a online CPU for this hctx,\n\t * and it should only happen in the path of handling CPU DEAD.\n\t */\n\tif (!cpu_online(next_cpu)) {\n\t\tif (!tried) {\n\t\t\ttried = true;\n\t\t\tgoto select_cpu;\n\t\t}\n\n\t\t/*\n\t\t * Make sure to re-select CPU next time once after CPUs\n\t\t * in hctx->cpumask become online again.\n\t\t */\n\t\thctx->next_cpu = next_cpu;\n\t\thctx->next_cpu_batch = 1;\n\t\treturn WORK_CPU_UNBOUND;\n\t}\n\n\thctx->next_cpu = next_cpu;\n\treturn next_cpu;\n}\n\n/**\n * __blk_mq_delay_run_hw_queue - Run (or schedule to run) a hardware queue.\n * @hctx: Pointer to the hardware queue to run.\n * @async: If we want to run the queue asynchronously.\n * @msecs: Microseconds of delay to wait before running the queue.\n *\n * If !@async, try to run the queue now. Else, run the queue asynchronously and\n * with a delay of @msecs.\n */\nstatic void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,\n\t\t\t\t\tunsigned long msecs)\n{\n\tif (unlikely(blk_mq_hctx_stopped(hctx)))\n\t\treturn;\n\n\tif (!async && !(hctx->flags & BLK_MQ_F_BLOCKING)) {\n\t\tint cpu = get_cpu();\n\t\tif (cpumask_test_cpu(cpu, hctx->cpumask)) {\n\t\t\t__blk_mq_run_hw_queue(hctx);\n\t\t\tput_cpu();\n\t\t\treturn;\n\t\t}\n\n\t\tput_cpu();\n\t}\n\n\tkblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,\n\t\t\t\t    msecs_to_jiffies(msecs));\n}\n\n/**\n * blk_mq_delay_run_hw_queue - Run a hardware queue asynchronously.\n * @hctx: Pointer to the hardware queue to run.\n * @msecs: Microseconds of delay to wait before running the queue.\n *\n * Run a hardware queue asynchronously with a delay of @msecs.\n */\nvoid blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs)\n{\n\t__blk_mq_delay_run_hw_queue(hctx, true, msecs);\n}\nEXPORT_SYMBOL(blk_mq_delay_run_hw_queue);\n\n/**\n * blk_mq_run_hw_queue - Start to run a hardware queue.\n * @hctx: Pointer to the hardware queue to run.\n * @async: If we want to run the queue asynchronously.\n *\n * Check if the request queue is not in a quiesced state and if there are\n * pending requests to be sent. If this is true, run the queue to send requests\n * to hardware.\n */\nvoid blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)\n{\n\tint srcu_idx;\n\tbool need_run;\n\n\t/*\n\t * When queue is quiesced, we may be switching io scheduler, or\n\t * updating nr_hw_queues, or other things, and we can't run queue\n\t * any more, even __blk_mq_hctx_has_pending() can't be called safely.\n\t *\n\t * And queue will be rerun in blk_mq_unquiesce_queue() if it is\n\t * quiesced.\n\t */\n\thctx_lock(hctx, &srcu_idx);\n\tneed_run = !blk_queue_quiesced(hctx->queue) &&\n\t\tblk_mq_hctx_has_pending(hctx);\n\thctx_unlock(hctx, srcu_idx);\n\n\tif (need_run)\n\t\t__blk_mq_delay_run_hw_queue(hctx, async, 0);\n}\nEXPORT_SYMBOL(blk_mq_run_hw_queue);\n\n/**\n * blk_mq_run_hw_queue - Run all hardware queues in a request queue.\n * @q: Pointer to the request queue to run.\n * @async: If we want to run the queue asynchronously.\n */\nvoid blk_mq_run_hw_queues(struct request_queue *q, bool async)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (blk_mq_hctx_stopped(hctx))\n\t\t\tcontinue;\n\n\t\tblk_mq_run_hw_queue(hctx, async);\n\t}\n}\nEXPORT_SYMBOL(blk_mq_run_hw_queues);\n\n/**\n * blk_mq_delay_run_hw_queues - Run all hardware queues asynchronously.\n * @q: Pointer to the request queue to run.\n * @msecs: Microseconds of delay to wait before running the queues.\n */\nvoid blk_mq_delay_run_hw_queues(struct request_queue *q, unsigned long msecs)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (blk_mq_hctx_stopped(hctx))\n\t\t\tcontinue;\n\n\t\tblk_mq_delay_run_hw_queue(hctx, msecs);\n\t}\n}\nEXPORT_SYMBOL(blk_mq_delay_run_hw_queues);\n\n/**\n * blk_mq_queue_stopped() - check whether one or more hctxs have been stopped\n * @q: request queue.\n *\n * The caller is responsible for serializing this function against\n * blk_mq_{start,stop}_hw_queue().\n */\nbool blk_mq_queue_stopped(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tif (blk_mq_hctx_stopped(hctx))\n\t\t\treturn true;\n\n\treturn false;\n}\nEXPORT_SYMBOL(blk_mq_queue_stopped);\n\n/*\n * This function is often used for pausing .queue_rq() by driver when\n * there isn't enough resource or some conditions aren't satisfied, and\n * BLK_STS_RESOURCE is usually returned.\n *\n * We do not guarantee that dispatch can be drained or blocked\n * after blk_mq_stop_hw_queue() returns. Please use\n * blk_mq_quiesce_queue() for that requirement.\n */\nvoid blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx)\n{\n\tcancel_delayed_work(&hctx->run_work);\n\n\tset_bit(BLK_MQ_S_STOPPED, &hctx->state);\n}\nEXPORT_SYMBOL(blk_mq_stop_hw_queue);\n\n/*\n * This function is often used for pausing .queue_rq() by driver when\n * there isn't enough resource or some conditions aren't satisfied, and\n * BLK_STS_RESOURCE is usually returned.\n *\n * We do not guarantee that dispatch can be drained or blocked\n * after blk_mq_stop_hw_queues() returns. Please use\n * blk_mq_quiesce_queue() for that requirement.\n */\nvoid blk_mq_stop_hw_queues(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tblk_mq_stop_hw_queue(hctx);\n}\nEXPORT_SYMBOL(blk_mq_stop_hw_queues);\n\nvoid blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx)\n{\n\tclear_bit(BLK_MQ_S_STOPPED, &hctx->state);\n\n\tblk_mq_run_hw_queue(hctx, false);\n}\nEXPORT_SYMBOL(blk_mq_start_hw_queue);\n\nvoid blk_mq_start_hw_queues(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tblk_mq_start_hw_queue(hctx);\n}\nEXPORT_SYMBOL(blk_mq_start_hw_queues);\n\nvoid blk_mq_start_stopped_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)\n{\n\tif (!blk_mq_hctx_stopped(hctx))\n\t\treturn;\n\n\tclear_bit(BLK_MQ_S_STOPPED, &hctx->state);\n\tblk_mq_run_hw_queue(hctx, async);\n}\nEXPORT_SYMBOL_GPL(blk_mq_start_stopped_hw_queue);\n\nvoid blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tblk_mq_start_stopped_hw_queue(hctx, async);\n}\nEXPORT_SYMBOL(blk_mq_start_stopped_hw_queues);\n\nstatic void blk_mq_run_work_fn(struct work_struct *work)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\n\thctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);\n\n\t/*\n\t * If we are stopped, don't run the queue.\n\t */\n\tif (blk_mq_hctx_stopped(hctx))\n\t\treturn;\n\n\t__blk_mq_run_hw_queue(hctx);\n}\n\nstatic inline void __blk_mq_insert_req_list(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t\t    struct request *rq,\n\t\t\t\t\t    bool at_head)\n{\n\tstruct blk_mq_ctx *ctx = rq->mq_ctx;\n\tenum hctx_type type = hctx->type;\n\n\tlockdep_assert_held(&ctx->lock);\n\n\ttrace_block_rq_insert(hctx->queue, rq);\n\n\tif (at_head)\n\t\tlist_add(&rq->queuelist, &ctx->rq_lists[type]);\n\telse\n\t\tlist_add_tail(&rq->queuelist, &ctx->rq_lists[type]);\n}\n\nvoid __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,\n\t\t\t     bool at_head)\n{\n\tstruct blk_mq_ctx *ctx = rq->mq_ctx;\n\n\tlockdep_assert_held(&ctx->lock);\n\n\t__blk_mq_insert_req_list(hctx, rq, at_head);\n\tblk_mq_hctx_mark_pending(hctx, ctx);\n}\n\n/**\n * blk_mq_request_bypass_insert - Insert a request at dispatch list.\n * @rq: Pointer to request to be inserted.\n * @at_head: true if the request should be inserted at the head of the list.\n * @run_queue: If we should run the hardware queue after inserting the request.\n *\n * Should only be used carefully, when the caller knows we want to\n * bypass a potential IO scheduler on the target device.\n */\nvoid blk_mq_request_bypass_insert(struct request *rq, bool at_head,\n\t\t\t\t  bool run_queue)\n{\n\tstruct blk_mq_hw_ctx *hctx = rq->mq_hctx;\n\n\tspin_lock(&hctx->lock);\n\tif (at_head)\n\t\tlist_add(&rq->queuelist, &hctx->dispatch);\n\telse\n\t\tlist_add_tail(&rq->queuelist, &hctx->dispatch);\n\tspin_unlock(&hctx->lock);\n\n\tif (run_queue)\n\t\tblk_mq_run_hw_queue(hctx, false);\n}\n\nvoid blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,\n\t\t\t    struct list_head *list)\n\n{\n\tstruct request *rq;\n\tenum hctx_type type = hctx->type;\n\n\t/*\n\t * preemption doesn't flush plug list, so it's possible ctx->cpu is\n\t * offline now\n\t */\n\tlist_for_each_entry(rq, list, queuelist) {\n\t\tBUG_ON(rq->mq_ctx != ctx);\n\t\ttrace_block_rq_insert(hctx->queue, rq);\n\t}\n\n\tspin_lock(&ctx->lock);\n\tlist_splice_tail_init(list, &ctx->rq_lists[type]);\n\tblk_mq_hctx_mark_pending(hctx, ctx);\n\tspin_unlock(&ctx->lock);\n}\n\nstatic int plug_rq_cmp(void *priv, struct list_head *a, struct list_head *b)\n{\n\tstruct request *rqa = container_of(a, struct request, queuelist);\n\tstruct request *rqb = container_of(b, struct request, queuelist);\n\n\tif (rqa->mq_ctx != rqb->mq_ctx)\n\t\treturn rqa->mq_ctx > rqb->mq_ctx;\n\tif (rqa->mq_hctx != rqb->mq_hctx)\n\t\treturn rqa->mq_hctx > rqb->mq_hctx;\n\n\treturn blk_rq_pos(rqa) > blk_rq_pos(rqb);\n}\n\nvoid blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule)\n{\n\tLIST_HEAD(list);\n\n\tif (list_empty(&plug->mq_list))\n\t\treturn;\n\tlist_splice_init(&plug->mq_list, &list);\n\n\tif (plug->rq_count > 2 && plug->multiple_queues)\n\t\tlist_sort(NULL, &list, plug_rq_cmp);\n\n\tplug->rq_count = 0;\n\n\tdo {\n\t\tstruct list_head rq_list;\n\t\tstruct request *rq, *head_rq = list_entry_rq(list.next);\n\t\tstruct list_head *pos = &head_rq->queuelist; /* skip first */\n\t\tstruct blk_mq_hw_ctx *this_hctx = head_rq->mq_hctx;\n\t\tstruct blk_mq_ctx *this_ctx = head_rq->mq_ctx;\n\t\tunsigned int depth = 1;\n\n\t\tlist_for_each_continue(pos, &list) {\n\t\t\trq = list_entry_rq(pos);\n\t\t\tBUG_ON(!rq->q);\n\t\t\tif (rq->mq_hctx != this_hctx || rq->mq_ctx != this_ctx)\n\t\t\t\tbreak;\n\t\t\tdepth++;\n\t\t}\n\n\t\tlist_cut_before(&rq_list, &list, pos);\n\t\ttrace_block_unplug(head_rq->q, depth, !from_schedule);\n\t\tblk_mq_sched_insert_requests(this_hctx, this_ctx, &rq_list,\n\t\t\t\t\t\tfrom_schedule);\n\t} while(!list_empty(&list));\n}\n\nstatic void blk_mq_bio_to_request(struct request *rq, struct bio *bio,\n\t\tunsigned int nr_segs)\n{\n\tint err;\n\n\tif (bio->bi_opf & REQ_RAHEAD)\n\t\trq->cmd_flags |= REQ_FAILFAST_MASK;\n\n\trq->__sector = bio->bi_iter.bi_sector;\n\trq->write_hint = bio->bi_write_hint;\n\tblk_rq_bio_prep(rq, bio, nr_segs);\n\n\t/* This can't fail, since GFP_NOIO includes __GFP_DIRECT_RECLAIM. */\n\terr = blk_crypto_rq_bio_prep(rq, bio, GFP_NOIO);\n\tWARN_ON_ONCE(err);\n\n\tblk_account_io_start(rq);\n}\n\nstatic blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t\t    struct request *rq,\n\t\t\t\t\t    blk_qc_t *cookie, bool last)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct blk_mq_queue_data bd = {\n\t\t.rq = rq,\n\t\t.last = last,\n\t};\n\tblk_qc_t new_cookie;\n\tblk_status_t ret;\n\n\tnew_cookie = request_to_qc_t(hctx, rq);\n\n\t/*\n\t * For OK queue, we are done. For error, caller may kill it.\n\t * Any other error (busy), just add it to our list as we\n\t * previously would have done.\n\t */\n\tret = q->mq_ops->queue_rq(hctx, &bd);\n\tswitch (ret) {\n\tcase BLK_STS_OK:\n\t\tblk_mq_update_dispatch_busy(hctx, false);\n\t\t*cookie = new_cookie;\n\t\tbreak;\n\tcase BLK_STS_RESOURCE:\n\tcase BLK_STS_DEV_RESOURCE:\n\t\tblk_mq_update_dispatch_busy(hctx, true);\n\t\t__blk_mq_requeue_request(rq);\n\t\tbreak;\n\tdefault:\n\t\tblk_mq_update_dispatch_busy(hctx, false);\n\t\t*cookie = BLK_QC_T_NONE;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic blk_status_t __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t\t\tstruct request *rq,\n\t\t\t\t\t\tblk_qc_t *cookie,\n\t\t\t\t\t\tbool bypass_insert, bool last)\n{\n\tstruct request_queue *q = rq->q;\n\tbool run_queue = true;\n\n\t/*\n\t * RCU or SRCU read lock is needed before checking quiesced flag.\n\t *\n\t * When queue is stopped or quiesced, ignore 'bypass_insert' from\n\t * blk_mq_request_issue_directly(), and return BLK_STS_OK to caller,\n\t * and avoid driver to try to dispatch again.\n\t */\n\tif (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {\n\t\trun_queue = false;\n\t\tbypass_insert = false;\n\t\tgoto insert;\n\t}\n\n\tif (q->elevator && !bypass_insert)\n\t\tgoto insert;\n\n\tif (!blk_mq_get_dispatch_budget(q))\n\t\tgoto insert;\n\n\tif (!blk_mq_get_driver_tag(rq)) {\n\t\tblk_mq_put_dispatch_budget(q);\n\t\tgoto insert;\n\t}\n\n\treturn __blk_mq_issue_directly(hctx, rq, cookie, last);\ninsert:\n\tif (bypass_insert)\n\t\treturn BLK_STS_RESOURCE;\n\n\tblk_mq_sched_insert_request(rq, false, run_queue, false);\n\n\treturn BLK_STS_OK;\n}\n\n/**\n * blk_mq_try_issue_directly - Try to send a request directly to device driver.\n * @hctx: Pointer of the associated hardware queue.\n * @rq: Pointer to request to be sent.\n * @cookie: Request queue cookie.\n *\n * If the device has enough resources to accept a new request now, send the\n * request directly to device driver. Else, insert at hctx->dispatch queue, so\n * we can try send it another time in the future. Requests inserted at this\n * queue have higher priority.\n */\nstatic void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,\n\t\tstruct request *rq, blk_qc_t *cookie)\n{\n\tblk_status_t ret;\n\tint srcu_idx;\n\n\tmight_sleep_if(hctx->flags & BLK_MQ_F_BLOCKING);\n\n\thctx_lock(hctx, &srcu_idx);\n\n\tret = __blk_mq_try_issue_directly(hctx, rq, cookie, false, true);\n\tif (ret == BLK_STS_RESOURCE || ret == BLK_STS_DEV_RESOURCE)\n\t\tblk_mq_request_bypass_insert(rq, false, true);\n\telse if (ret != BLK_STS_OK)\n\t\tblk_mq_end_request(rq, ret);\n\n\thctx_unlock(hctx, srcu_idx);\n}\n\nblk_status_t blk_mq_request_issue_directly(struct request *rq, bool last)\n{\n\tblk_status_t ret;\n\tint srcu_idx;\n\tblk_qc_t unused_cookie;\n\tstruct blk_mq_hw_ctx *hctx = rq->mq_hctx;\n\n\thctx_lock(hctx, &srcu_idx);\n\tret = __blk_mq_try_issue_directly(hctx, rq, &unused_cookie, true, last);\n\thctx_unlock(hctx, srcu_idx);\n\n\treturn ret;\n}\n\nvoid blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,\n\t\tstruct list_head *list)\n{\n\tint queued = 0;\n\tint errors = 0;\n\n\twhile (!list_empty(list)) {\n\t\tblk_status_t ret;\n\t\tstruct request *rq = list_first_entry(list, struct request,\n\t\t\t\tqueuelist);\n\n\t\tlist_del_init(&rq->queuelist);\n\t\tret = blk_mq_request_issue_directly(rq, list_empty(list));\n\t\tif (ret != BLK_STS_OK) {\n\t\t\tif (ret == BLK_STS_RESOURCE ||\n\t\t\t\t\tret == BLK_STS_DEV_RESOURCE) {\n\t\t\t\tblk_mq_request_bypass_insert(rq, false,\n\t\t\t\t\t\t\tlist_empty(list));\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tblk_mq_end_request(rq, ret);\n\t\t\terrors++;\n\t\t} else\n\t\t\tqueued++;\n\t}\n\n\t/*\n\t * If we didn't flush the entire list, we could have told\n\t * the driver there was more coming, but that turned out to\n\t * be a lie.\n\t */\n\tif ((!list_empty(list) || errors) &&\n\t     hctx->queue->mq_ops->commit_rqs && queued)\n\t\thctx->queue->mq_ops->commit_rqs(hctx);\n}\n\nstatic void blk_add_rq_to_plug(struct blk_plug *plug, struct request *rq)\n{\n\tlist_add_tail(&rq->queuelist, &plug->mq_list);\n\tplug->rq_count++;\n\tif (!plug->multiple_queues && !list_is_singular(&plug->mq_list)) {\n\t\tstruct request *tmp;\n\n\t\ttmp = list_first_entry(&plug->mq_list, struct request,\n\t\t\t\t\t\tqueuelist);\n\t\tif (tmp->q != rq->q)\n\t\t\tplug->multiple_queues = true;\n\t}\n}\n\n/**\n * blk_mq_submit_bio - Create and send a request to block device.\n * @bio: Bio pointer.\n *\n * Builds up a request structure from @q and @bio and send to the device. The\n * request may not be queued directly to hardware if:\n * * This request can be merged with another one\n * * We want to place request at plug queue for possible future merging\n * * There is an IO scheduler active at this queue\n *\n * It will not queue the request if there is an error with the bio, or at the\n * request creation.\n *\n * Returns: Request queue cookie.\n */\nblk_qc_t blk_mq_submit_bio(struct bio *bio)\n{\n\tstruct request_queue *q = bio->bi_disk->queue;\n\tconst int is_sync = op_is_sync(bio->bi_opf);\n\tconst int is_flush_fua = op_is_flush(bio->bi_opf);\n\tstruct blk_mq_alloc_data data = {\n\t\t.q\t\t= q,\n\t};\n\tstruct request *rq;\n\tstruct blk_plug *plug;\n\tstruct request *same_queue_rq = NULL;\n\tunsigned int nr_segs;\n\tblk_qc_t cookie;\n\tblk_status_t ret;\n\n\tblk_queue_bounce(q, &bio);\n\t__blk_queue_split(&bio, &nr_segs);\n\n\tif (!bio_integrity_prep(bio))\n\t\tgoto queue_exit;\n\n\tif (!is_flush_fua && !blk_queue_nomerges(q) &&\n\t    blk_attempt_plug_merge(q, bio, nr_segs, &same_queue_rq))\n\t\tgoto queue_exit;\n\n\tif (blk_mq_sched_bio_merge(q, bio, nr_segs))\n\t\tgoto queue_exit;\n\n\trq_qos_throttle(q, bio);\n\n\tdata.cmd_flags = bio->bi_opf;\n\trq = __blk_mq_alloc_request(&data);\n\tif (unlikely(!rq)) {\n\t\trq_qos_cleanup(q, bio);\n\t\tif (bio->bi_opf & REQ_NOWAIT)\n\t\t\tbio_wouldblock_error(bio);\n\t\tgoto queue_exit;\n\t}\n\n\ttrace_block_getrq(q, bio, bio->bi_opf);\n\n\trq_qos_track(q, rq, bio);\n\n\tcookie = request_to_qc_t(data.hctx, rq);\n\n\tblk_mq_bio_to_request(rq, bio, nr_segs);\n\n\tret = blk_crypto_init_request(rq);\n\tif (ret != BLK_STS_OK) {\n\t\tbio->bi_status = ret;\n\t\tbio_endio(bio);\n\t\tblk_mq_free_request(rq);\n\t\treturn BLK_QC_T_NONE;\n\t}\n\n\tplug = blk_mq_plug(q, bio);\n\tif (unlikely(is_flush_fua)) {\n\t\t/* Bypass scheduler for flush requests */\n\t\tblk_insert_flush(rq);\n\t\tblk_mq_run_hw_queue(data.hctx, true);\n\t} else if (plug && (q->nr_hw_queues == 1 || q->mq_ops->commit_rqs ||\n\t\t\t\t!blk_queue_nonrot(q))) {\n\t\t/*\n\t\t * Use plugging if we have a ->commit_rqs() hook as well, as\n\t\t * we know the driver uses bd->last in a smart fashion.\n\t\t *\n\t\t * Use normal plugging if this disk is slow HDD, as sequential\n\t\t * IO may benefit a lot from plug merging.\n\t\t */\n\t\tunsigned int request_count = plug->rq_count;\n\t\tstruct request *last = NULL;\n\n\t\tif (!request_count)\n\t\t\ttrace_block_plug(q);\n\t\telse\n\t\t\tlast = list_entry_rq(plug->mq_list.prev);\n\n\t\tif (request_count >= BLK_MAX_REQUEST_COUNT || (last &&\n\t\t    blk_rq_bytes(last) >= BLK_PLUG_FLUSH_SIZE)) {\n\t\t\tblk_flush_plug_list(plug, false);\n\t\t\ttrace_block_plug(q);\n\t\t}\n\n\t\tblk_add_rq_to_plug(plug, rq);\n\t} else if (q->elevator) {\n\t\t/* Insert the request at the IO scheduler queue */\n\t\tblk_mq_sched_insert_request(rq, false, true, true);\n\t} else if (plug && !blk_queue_nomerges(q)) {\n\t\t/*\n\t\t * We do limited plugging. If the bio can be merged, do that.\n\t\t * Otherwise the existing request in the plug list will be\n\t\t * issued. So the plug list will have one request at most\n\t\t * The plug list might get flushed before this. If that happens,\n\t\t * the plug list is empty, and same_queue_rq is invalid.\n\t\t */\n\t\tif (list_empty(&plug->mq_list))\n\t\t\tsame_queue_rq = NULL;\n\t\tif (same_queue_rq) {\n\t\t\tlist_del_init(&same_queue_rq->queuelist);\n\t\t\tplug->rq_count--;\n\t\t}\n\t\tblk_add_rq_to_plug(plug, rq);\n\t\ttrace_block_plug(q);\n\n\t\tif (same_queue_rq) {\n\t\t\tdata.hctx = same_queue_rq->mq_hctx;\n\t\t\ttrace_block_unplug(q, 1, true);\n\t\t\tblk_mq_try_issue_directly(data.hctx, same_queue_rq,\n\t\t\t\t\t&cookie);\n\t\t}\n\t} else if ((q->nr_hw_queues > 1 && is_sync) ||\n\t\t\t!data.hctx->dispatch_busy) {\n\t\t/*\n\t\t * There is no scheduler and we can try to send directly\n\t\t * to the hardware.\n\t\t */\n\t\tblk_mq_try_issue_directly(data.hctx, rq, &cookie);\n\t} else {\n\t\t/* Default case. */\n\t\tblk_mq_sched_insert_request(rq, false, true, true);\n\t}\n\n\treturn cookie;\nqueue_exit:\n\tblk_queue_exit(q);\n\treturn BLK_QC_T_NONE;\n}\n\nvoid blk_mq_free_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,\n\t\t     unsigned int hctx_idx)\n{\n\tstruct page *page;\n\n\tif (tags->rqs && set->ops->exit_request) {\n\t\tint i;\n\n\t\tfor (i = 0; i < tags->nr_tags; i++) {\n\t\t\tstruct request *rq = tags->static_rqs[i];\n\n\t\t\tif (!rq)\n\t\t\t\tcontinue;\n\t\t\tset->ops->exit_request(set, rq, hctx_idx);\n\t\t\ttags->static_rqs[i] = NULL;\n\t\t}\n\t}\n\n\twhile (!list_empty(&tags->page_list)) {\n\t\tpage = list_first_entry(&tags->page_list, struct page, lru);\n\t\tlist_del_init(&page->lru);\n\t\t/*\n\t\t * Remove kmemleak object previously allocated in\n\t\t * blk_mq_alloc_rqs().\n\t\t */\n\t\tkmemleak_free(page_address(page));\n\t\t__free_pages(page, page->private);\n\t}\n}\n\nvoid blk_mq_free_rq_map(struct blk_mq_tags *tags, unsigned int flags)\n{\n\tkfree(tags->rqs);\n\ttags->rqs = NULL;\n\tkfree(tags->static_rqs);\n\ttags->static_rqs = NULL;\n\n\tblk_mq_free_tags(tags, flags);\n}\n\nstruct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set,\n\t\t\t\t\tunsigned int hctx_idx,\n\t\t\t\t\tunsigned int nr_tags,\n\t\t\t\t\tunsigned int reserved_tags,\n\t\t\t\t\tunsigned int flags)\n{\n\tstruct blk_mq_tags *tags;\n\tint node;\n\n\tnode = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], hctx_idx);\n\tif (node == NUMA_NO_NODE)\n\t\tnode = set->numa_node;\n\n\ttags = blk_mq_init_tags(nr_tags, reserved_tags, node, flags);\n\tif (!tags)\n\t\treturn NULL;\n\n\ttags->rqs = kcalloc_node(nr_tags, sizeof(struct request *),\n\t\t\t\t GFP_NOIO | __GFP_NOWARN | __GFP_NORETRY,\n\t\t\t\t node);\n\tif (!tags->rqs) {\n\t\tblk_mq_free_tags(tags, flags);\n\t\treturn NULL;\n\t}\n\n\ttags->static_rqs = kcalloc_node(nr_tags, sizeof(struct request *),\n\t\t\t\t\tGFP_NOIO | __GFP_NOWARN | __GFP_NORETRY,\n\t\t\t\t\tnode);\n\tif (!tags->static_rqs) {\n\t\tkfree(tags->rqs);\n\t\tblk_mq_free_tags(tags, flags);\n\t\treturn NULL;\n\t}\n\n\treturn tags;\n}\n\nstatic size_t order_to_size(unsigned int order)\n{\n\treturn (size_t)PAGE_SIZE << order;\n}\n\nstatic int blk_mq_init_request(struct blk_mq_tag_set *set, struct request *rq,\n\t\t\t       unsigned int hctx_idx, int node)\n{\n\tint ret;\n\n\tif (set->ops->init_request) {\n\t\tret = set->ops->init_request(set, rq, hctx_idx, node);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tWRITE_ONCE(rq->state, MQ_RQ_IDLE);\n\treturn 0;\n}\n\nint blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,\n\t\t     unsigned int hctx_idx, unsigned int depth)\n{\n\tunsigned int i, j, entries_per_page, max_order = 4;\n\tsize_t rq_size, left;\n\tint node;\n\n\tnode = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], hctx_idx);\n\tif (node == NUMA_NO_NODE)\n\t\tnode = set->numa_node;\n\n\tINIT_LIST_HEAD(&tags->page_list);\n\n\t/*\n\t * rq_size is the size of the request plus driver payload, rounded\n\t * to the cacheline size\n\t */\n\trq_size = round_up(sizeof(struct request) + set->cmd_size,\n\t\t\t\tcache_line_size());\n\tleft = rq_size * depth;\n\n\tfor (i = 0; i < depth; ) {\n\t\tint this_order = max_order;\n\t\tstruct page *page;\n\t\tint to_do;\n\t\tvoid *p;\n\n\t\twhile (this_order && left < order_to_size(this_order - 1))\n\t\t\tthis_order--;\n\n\t\tdo {\n\t\t\tpage = alloc_pages_node(node,\n\t\t\t\tGFP_NOIO | __GFP_NOWARN | __GFP_NORETRY | __GFP_ZERO,\n\t\t\t\tthis_order);\n\t\t\tif (page)\n\t\t\t\tbreak;\n\t\t\tif (!this_order--)\n\t\t\t\tbreak;\n\t\t\tif (order_to_size(this_order) < rq_size)\n\t\t\t\tbreak;\n\t\t} while (1);\n\n\t\tif (!page)\n\t\t\tgoto fail;\n\n\t\tpage->private = this_order;\n\t\tlist_add_tail(&page->lru, &tags->page_list);\n\n\t\tp = page_address(page);\n\t\t/*\n\t\t * Allow kmemleak to scan these pages as they contain pointers\n\t\t * to additional allocations like via ops->init_request().\n\t\t */\n\t\tkmemleak_alloc(p, order_to_size(this_order), 1, GFP_NOIO);\n\t\tentries_per_page = order_to_size(this_order) / rq_size;\n\t\tto_do = min(entries_per_page, depth - i);\n\t\tleft -= to_do * rq_size;\n\t\tfor (j = 0; j < to_do; j++) {\n\t\t\tstruct request *rq = p;\n\n\t\t\ttags->static_rqs[i] = rq;\n\t\t\tif (blk_mq_init_request(set, rq, hctx_idx, node)) {\n\t\t\t\ttags->static_rqs[i] = NULL;\n\t\t\t\tgoto fail;\n\t\t\t}\n\n\t\t\tp += rq_size;\n\t\t\ti++;\n\t\t}\n\t}\n\treturn 0;\n\nfail:\n\tblk_mq_free_rqs(set, tags, hctx_idx);\n\treturn -ENOMEM;\n}\n\nstruct rq_iter_data {\n\tstruct blk_mq_hw_ctx *hctx;\n\tbool has_rq;\n};\n\nstatic bool blk_mq_has_request(struct request *rq, void *data, bool reserved)\n{\n\tstruct rq_iter_data *iter_data = data;\n\n\tif (rq->mq_hctx != iter_data->hctx)\n\t\treturn true;\n\titer_data->has_rq = true;\n\treturn false;\n}\n\nstatic bool blk_mq_hctx_has_requests(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct blk_mq_tags *tags = hctx->sched_tags ?\n\t\t\thctx->sched_tags : hctx->tags;\n\tstruct rq_iter_data data = {\n\t\t.hctx\t= hctx,\n\t};\n\n\tblk_mq_all_tag_iter(tags, blk_mq_has_request, &data);\n\treturn data.has_rq;\n}\n\nstatic inline bool blk_mq_last_cpu_in_hctx(unsigned int cpu,\n\t\tstruct blk_mq_hw_ctx *hctx)\n{\n\tif (cpumask_next_and(-1, hctx->cpumask, cpu_online_mask) != cpu)\n\t\treturn false;\n\tif (cpumask_next_and(cpu, hctx->cpumask, cpu_online_mask) < nr_cpu_ids)\n\t\treturn false;\n\treturn true;\n}\n\nstatic int blk_mq_hctx_notify_offline(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct blk_mq_hw_ctx *hctx = hlist_entry_safe(node,\n\t\t\tstruct blk_mq_hw_ctx, cpuhp_online);\n\n\tif (!cpumask_test_cpu(cpu, hctx->cpumask) ||\n\t    !blk_mq_last_cpu_in_hctx(cpu, hctx))\n\t\treturn 0;\n\n\t/*\n\t * Prevent new request from being allocated on the current hctx.\n\t *\n\t * The smp_mb__after_atomic() Pairs with the implied barrier in\n\t * test_and_set_bit_lock in sbitmap_get().  Ensures the inactive flag is\n\t * seen once we return from the tag allocator.\n\t */\n\tset_bit(BLK_MQ_S_INACTIVE, &hctx->state);\n\tsmp_mb__after_atomic();\n\n\t/*\n\t * Try to grab a reference to the queue and wait for any outstanding\n\t * requests.  If we could not grab a reference the queue has been\n\t * frozen and there are no requests.\n\t */\n\tif (percpu_ref_tryget(&hctx->queue->q_usage_counter)) {\n\t\twhile (blk_mq_hctx_has_requests(hctx))\n\t\t\tmsleep(5);\n\t\tpercpu_ref_put(&hctx->queue->q_usage_counter);\n\t}\n\n\treturn 0;\n}\n\nstatic int blk_mq_hctx_notify_online(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct blk_mq_hw_ctx *hctx = hlist_entry_safe(node,\n\t\t\tstruct blk_mq_hw_ctx, cpuhp_online);\n\n\tif (cpumask_test_cpu(cpu, hctx->cpumask))\n\t\tclear_bit(BLK_MQ_S_INACTIVE, &hctx->state);\n\treturn 0;\n}\n\n/*\n * 'cpu' is going away. splice any existing rq_list entries from this\n * software queue to the hw queue dispatch list, and ensure that it\n * gets run.\n */\nstatic int blk_mq_hctx_notify_dead(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct blk_mq_ctx *ctx;\n\tLIST_HEAD(tmp);\n\tenum hctx_type type;\n\n\thctx = hlist_entry_safe(node, struct blk_mq_hw_ctx, cpuhp_dead);\n\tif (!cpumask_test_cpu(cpu, hctx->cpumask))\n\t\treturn 0;\n\n\tctx = __blk_mq_get_ctx(hctx->queue, cpu);\n\ttype = hctx->type;\n\n\tspin_lock(&ctx->lock);\n\tif (!list_empty(&ctx->rq_lists[type])) {\n\t\tlist_splice_init(&ctx->rq_lists[type], &tmp);\n\t\tblk_mq_hctx_clear_pending(hctx, ctx);\n\t}\n\tspin_unlock(&ctx->lock);\n\n\tif (list_empty(&tmp))\n\t\treturn 0;\n\n\tspin_lock(&hctx->lock);\n\tlist_splice_tail_init(&tmp, &hctx->dispatch);\n\tspin_unlock(&hctx->lock);\n\n\tblk_mq_run_hw_queue(hctx, true);\n\treturn 0;\n}\n\nstatic void blk_mq_remove_cpuhp(struct blk_mq_hw_ctx *hctx)\n{\n\tif (!(hctx->flags & BLK_MQ_F_STACKING))\n\t\tcpuhp_state_remove_instance_nocalls(CPUHP_AP_BLK_MQ_ONLINE,\n\t\t\t\t\t\t    &hctx->cpuhp_online);\n\tcpuhp_state_remove_instance_nocalls(CPUHP_BLK_MQ_DEAD,\n\t\t\t\t\t    &hctx->cpuhp_dead);\n}\n\n/* hctx->ctxs will be freed in queue's release handler */\nstatic void blk_mq_exit_hctx(struct request_queue *q,\n\t\tstruct blk_mq_tag_set *set,\n\t\tstruct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)\n{\n\tif (blk_mq_hw_queue_mapped(hctx))\n\t\tblk_mq_tag_idle(hctx);\n\n\tif (set->ops->exit_request)\n\t\tset->ops->exit_request(set, hctx->fq->flush_rq, hctx_idx);\n\n\tif (set->ops->exit_hctx)\n\t\tset->ops->exit_hctx(hctx, hctx_idx);\n\n\tblk_mq_remove_cpuhp(hctx);\n\n\tspin_lock(&q->unused_hctx_lock);\n\tlist_add(&hctx->hctx_list, &q->unused_hctx_list);\n\tspin_unlock(&q->unused_hctx_lock);\n}\n\nstatic void blk_mq_exit_hw_queues(struct request_queue *q,\n\t\tstruct blk_mq_tag_set *set, int nr_queue)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned int i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (i == nr_queue)\n\t\t\tbreak;\n\t\tblk_mq_debugfs_unregister_hctx(hctx);\n\t\tblk_mq_exit_hctx(q, set, hctx, i);\n\t}\n}\n\nstatic int blk_mq_hw_ctx_size(struct blk_mq_tag_set *tag_set)\n{\n\tint hw_ctx_size = sizeof(struct blk_mq_hw_ctx);\n\n\tBUILD_BUG_ON(ALIGN(offsetof(struct blk_mq_hw_ctx, srcu),\n\t\t\t   __alignof__(struct blk_mq_hw_ctx)) !=\n\t\t     sizeof(struct blk_mq_hw_ctx));\n\n\tif (tag_set->flags & BLK_MQ_F_BLOCKING)\n\t\thw_ctx_size += sizeof(struct srcu_struct);\n\n\treturn hw_ctx_size;\n}\n\nstatic int blk_mq_init_hctx(struct request_queue *q,\n\t\tstruct blk_mq_tag_set *set,\n\t\tstruct blk_mq_hw_ctx *hctx, unsigned hctx_idx)\n{\n\thctx->queue_num = hctx_idx;\n\n\tif (!(hctx->flags & BLK_MQ_F_STACKING))\n\t\tcpuhp_state_add_instance_nocalls(CPUHP_AP_BLK_MQ_ONLINE,\n\t\t\t\t&hctx->cpuhp_online);\n\tcpuhp_state_add_instance_nocalls(CPUHP_BLK_MQ_DEAD, &hctx->cpuhp_dead);\n\n\thctx->tags = set->tags[hctx_idx];\n\n\tif (set->ops->init_hctx &&\n\t    set->ops->init_hctx(hctx, set->driver_data, hctx_idx))\n\t\tgoto unregister_cpu_notifier;\n\n\tif (blk_mq_init_request(set, hctx->fq->flush_rq, hctx_idx,\n\t\t\t\thctx->numa_node))\n\t\tgoto exit_hctx;\n\treturn 0;\n\n exit_hctx:\n\tif (set->ops->exit_hctx)\n\t\tset->ops->exit_hctx(hctx, hctx_idx);\n unregister_cpu_notifier:\n\tblk_mq_remove_cpuhp(hctx);\n\treturn -1;\n}\n\nstatic struct blk_mq_hw_ctx *\nblk_mq_alloc_hctx(struct request_queue *q, struct blk_mq_tag_set *set,\n\t\tint node)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tgfp_t gfp = GFP_NOIO | __GFP_NOWARN | __GFP_NORETRY;\n\n\thctx = kzalloc_node(blk_mq_hw_ctx_size(set), gfp, node);\n\tif (!hctx)\n\t\tgoto fail_alloc_hctx;\n\n\tif (!zalloc_cpumask_var_node(&hctx->cpumask, gfp, node))\n\t\tgoto free_hctx;\n\n\tatomic_set(&hctx->nr_active, 0);\n\tatomic_set(&hctx->elevator_queued, 0);\n\tif (node == NUMA_NO_NODE)\n\t\tnode = set->numa_node;\n\thctx->numa_node = node;\n\n\tINIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);\n\tspin_lock_init(&hctx->lock);\n\tINIT_LIST_HEAD(&hctx->dispatch);\n\thctx->queue = q;\n\thctx->flags = set->flags & ~BLK_MQ_F_TAG_QUEUE_SHARED;\n\n\tINIT_LIST_HEAD(&hctx->hctx_list);\n\n\t/*\n\t * Allocate space for all possible cpus to avoid allocation at\n\t * runtime\n\t */\n\thctx->ctxs = kmalloc_array_node(nr_cpu_ids, sizeof(void *),\n\t\t\tgfp, node);\n\tif (!hctx->ctxs)\n\t\tgoto free_cpumask;\n\n\tif (sbitmap_init_node(&hctx->ctx_map, nr_cpu_ids, ilog2(8),\n\t\t\t\tgfp, node))\n\t\tgoto free_ctxs;\n\thctx->nr_ctx = 0;\n\n\tspin_lock_init(&hctx->dispatch_wait_lock);\n\tinit_waitqueue_func_entry(&hctx->dispatch_wait, blk_mq_dispatch_wake);\n\tINIT_LIST_HEAD(&hctx->dispatch_wait.entry);\n\n\thctx->fq = blk_alloc_flush_queue(hctx->numa_node, set->cmd_size, gfp);\n\tif (!hctx->fq)\n\t\tgoto free_bitmap;\n\n\tif (hctx->flags & BLK_MQ_F_BLOCKING)\n\t\tinit_srcu_struct(hctx->srcu);\n\tblk_mq_hctx_kobj_init(hctx);\n\n\treturn hctx;\n\n free_bitmap:\n\tsbitmap_free(&hctx->ctx_map);\n free_ctxs:\n\tkfree(hctx->ctxs);\n free_cpumask:\n\tfree_cpumask_var(hctx->cpumask);\n free_hctx:\n\tkfree(hctx);\n fail_alloc_hctx:\n\treturn NULL;\n}\n\nstatic void blk_mq_init_cpu_queues(struct request_queue *q,\n\t\t\t\t   unsigned int nr_hw_queues)\n{\n\tstruct blk_mq_tag_set *set = q->tag_set;\n\tunsigned int i, j;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct blk_mq_ctx *__ctx = per_cpu_ptr(q->queue_ctx, i);\n\t\tstruct blk_mq_hw_ctx *hctx;\n\t\tint k;\n\n\t\t__ctx->cpu = i;\n\t\tspin_lock_init(&__ctx->lock);\n\t\tfor (k = HCTX_TYPE_DEFAULT; k < HCTX_MAX_TYPES; k++)\n\t\t\tINIT_LIST_HEAD(&__ctx->rq_lists[k]);\n\n\t\t__ctx->queue = q;\n\n\t\t/*\n\t\t * Set local node, IFF we have more than one hw queue. If\n\t\t * not, we remain on the home node of the device\n\t\t */\n\t\tfor (j = 0; j < set->nr_maps; j++) {\n\t\t\thctx = blk_mq_map_queue_type(q, j, i);\n\t\t\tif (nr_hw_queues > 1 && hctx->numa_node == NUMA_NO_NODE)\n\t\t\t\thctx->numa_node = cpu_to_node(i);\n\t\t}\n\t}\n}\n\nstatic bool __blk_mq_alloc_map_and_request(struct blk_mq_tag_set *set,\n\t\t\t\t\tint hctx_idx)\n{\n\tunsigned int flags = set->flags;\n\tint ret = 0;\n\n\tset->tags[hctx_idx] = blk_mq_alloc_rq_map(set, hctx_idx,\n\t\t\t\t\tset->queue_depth, set->reserved_tags, flags);\n\tif (!set->tags[hctx_idx])\n\t\treturn false;\n\n\tret = blk_mq_alloc_rqs(set, set->tags[hctx_idx], hctx_idx,\n\t\t\t\tset->queue_depth);\n\tif (!ret)\n\t\treturn true;\n\n\tblk_mq_free_rq_map(set->tags[hctx_idx], flags);\n\tset->tags[hctx_idx] = NULL;\n\treturn false;\n}\n\nstatic void blk_mq_free_map_and_requests(struct blk_mq_tag_set *set,\n\t\t\t\t\t unsigned int hctx_idx)\n{\n\tunsigned int flags = set->flags;\n\n\tif (set->tags && set->tags[hctx_idx]) {\n\t\tblk_mq_free_rqs(set, set->tags[hctx_idx], hctx_idx);\n\t\tblk_mq_free_rq_map(set->tags[hctx_idx], flags);\n\t\tset->tags[hctx_idx] = NULL;\n\t}\n}\n\nstatic void blk_mq_map_swqueue(struct request_queue *q)\n{\n\tunsigned int i, j, hctx_idx;\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct blk_mq_ctx *ctx;\n\tstruct blk_mq_tag_set *set = q->tag_set;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tcpumask_clear(hctx->cpumask);\n\t\thctx->nr_ctx = 0;\n\t\thctx->dispatch_from = NULL;\n\t}\n\n\t/*\n\t * Map software to hardware queues.\n\t *\n\t * If the cpu isn't present, the cpu is mapped to first hctx.\n\t */\n\tfor_each_possible_cpu(i) {\n\n\t\tctx = per_cpu_ptr(q->queue_ctx, i);\n\t\tfor (j = 0; j < set->nr_maps; j++) {\n\t\t\tif (!set->map[j].nr_queues) {\n\t\t\t\tctx->hctxs[j] = blk_mq_map_queue_type(q,\n\t\t\t\t\t\tHCTX_TYPE_DEFAULT, i);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\thctx_idx = set->map[j].mq_map[i];\n\t\t\t/* unmapped hw queue can be remapped after CPU topo changed */\n\t\t\tif (!set->tags[hctx_idx] &&\n\t\t\t    !__blk_mq_alloc_map_and_request(set, hctx_idx)) {\n\t\t\t\t/*\n\t\t\t\t * If tags initialization fail for some hctx,\n\t\t\t\t * that hctx won't be brought online.  In this\n\t\t\t\t * case, remap the current ctx to hctx[0] which\n\t\t\t\t * is guaranteed to always have tags allocated\n\t\t\t\t */\n\t\t\t\tset->map[j].mq_map[i] = 0;\n\t\t\t}\n\n\t\t\thctx = blk_mq_map_queue_type(q, j, i);\n\t\t\tctx->hctxs[j] = hctx;\n\t\t\t/*\n\t\t\t * If the CPU is already set in the mask, then we've\n\t\t\t * mapped this one already. This can happen if\n\t\t\t * devices share queues across queue maps.\n\t\t\t */\n\t\t\tif (cpumask_test_cpu(i, hctx->cpumask))\n\t\t\t\tcontinue;\n\n\t\t\tcpumask_set_cpu(i, hctx->cpumask);\n\t\t\thctx->type = j;\n\t\t\tctx->index_hw[hctx->type] = hctx->nr_ctx;\n\t\t\thctx->ctxs[hctx->nr_ctx++] = ctx;\n\n\t\t\t/*\n\t\t\t * If the nr_ctx type overflows, we have exceeded the\n\t\t\t * amount of sw queues we can support.\n\t\t\t */\n\t\t\tBUG_ON(!hctx->nr_ctx);\n\t\t}\n\n\t\tfor (; j < HCTX_MAX_TYPES; j++)\n\t\t\tctx->hctxs[j] = blk_mq_map_queue_type(q,\n\t\t\t\t\tHCTX_TYPE_DEFAULT, i);\n\t}\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\t/*\n\t\t * If no software queues are mapped to this hardware queue,\n\t\t * disable it and free the request entries.\n\t\t */\n\t\tif (!hctx->nr_ctx) {\n\t\t\t/* Never unmap queue 0.  We need it as a\n\t\t\t * fallback in case of a new remap fails\n\t\t\t * allocation\n\t\t\t */\n\t\t\tif (i && set->tags[i])\n\t\t\t\tblk_mq_free_map_and_requests(set, i);\n\n\t\t\thctx->tags = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\thctx->tags = set->tags[i];\n\t\tWARN_ON(!hctx->tags);\n\n\t\t/*\n\t\t * Set the map size to the number of mapped software queues.\n\t\t * This is more accurate and more efficient than looping\n\t\t * over all possibly mapped software queues.\n\t\t */\n\t\tsbitmap_resize(&hctx->ctx_map, hctx->nr_ctx);\n\n\t\t/*\n\t\t * Initialize batch roundrobin counts\n\t\t */\n\t\thctx->next_cpu = blk_mq_first_mapped_cpu(hctx);\n\t\thctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;\n\t}\n}\n\n/*\n * Caller needs to ensure that we're either frozen/quiesced, or that\n * the queue isn't live yet.\n */\nstatic void queue_set_hctx_shared(struct request_queue *q, bool shared)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (shared)\n\t\t\thctx->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;\n\t\telse\n\t\t\thctx->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;\n\t}\n}\n\nstatic void blk_mq_update_tag_set_shared(struct blk_mq_tag_set *set,\n\t\t\t\t\t bool shared)\n{\n\tstruct request_queue *q;\n\n\tlockdep_assert_held(&set->tag_list_lock);\n\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list) {\n\t\tblk_mq_freeze_queue(q);\n\t\tqueue_set_hctx_shared(q, shared);\n\t\tblk_mq_unfreeze_queue(q);\n\t}\n}\n\nstatic void blk_mq_del_queue_tag_set(struct request_queue *q)\n{\n\tstruct blk_mq_tag_set *set = q->tag_set;\n\n\tmutex_lock(&set->tag_list_lock);\n\tlist_del(&q->tag_set_list);\n\tif (list_is_singular(&set->tag_list)) {\n\t\t/* just transitioned to unshared */\n\t\tset->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;\n\t\t/* update existing queue */\n\t\tblk_mq_update_tag_set_shared(set, false);\n\t}\n\tmutex_unlock(&set->tag_list_lock);\n\tINIT_LIST_HEAD(&q->tag_set_list);\n}\n\nstatic void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,\n\t\t\t\t     struct request_queue *q)\n{\n\tmutex_lock(&set->tag_list_lock);\n\n\t/*\n\t * Check to see if we're transitioning to shared (from 1 to 2 queues).\n\t */\n\tif (!list_empty(&set->tag_list) &&\n\t    !(set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {\n\t\tset->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;\n\t\t/* update existing queue */\n\t\tblk_mq_update_tag_set_shared(set, true);\n\t}\n\tif (set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)\n\t\tqueue_set_hctx_shared(q, true);\n\tlist_add_tail(&q->tag_set_list, &set->tag_list);\n\n\tmutex_unlock(&set->tag_list_lock);\n}\n\n/* All allocations will be freed in release handler of q->mq_kobj */\nstatic int blk_mq_alloc_ctxs(struct request_queue *q)\n{\n\tstruct blk_mq_ctxs *ctxs;\n\tint cpu;\n\n\tctxs = kzalloc(sizeof(*ctxs), GFP_KERNEL);\n\tif (!ctxs)\n\t\treturn -ENOMEM;\n\n\tctxs->queue_ctx = alloc_percpu(struct blk_mq_ctx);\n\tif (!ctxs->queue_ctx)\n\t\tgoto fail;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct blk_mq_ctx *ctx = per_cpu_ptr(ctxs->queue_ctx, cpu);\n\t\tctx->ctxs = ctxs;\n\t}\n\n\tq->mq_kobj = &ctxs->kobj;\n\tq->queue_ctx = ctxs->queue_ctx;\n\n\treturn 0;\n fail:\n\tkfree(ctxs);\n\treturn -ENOMEM;\n}\n\n/*\n * It is the actual release handler for mq, but we do it from\n * request queue's release handler for avoiding use-after-free\n * and headache because q->mq_kobj shouldn't have been introduced,\n * but we can't group ctx/kctx kobj without it.\n */\nvoid blk_mq_release(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx, *next;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tWARN_ON_ONCE(hctx && list_empty(&hctx->hctx_list));\n\n\t/* all hctx are in .unused_hctx_list now */\n\tlist_for_each_entry_safe(hctx, next, &q->unused_hctx_list, hctx_list) {\n\t\tlist_del_init(&hctx->hctx_list);\n\t\tkobject_put(&hctx->kobj);\n\t}\n\n\tkfree(q->queue_hw_ctx);\n\n\t/*\n\t * release .mq_kobj and sw queue's kobject now because\n\t * both share lifetime with request queue.\n\t */\n\tblk_mq_sysfs_deinit(q);\n}\n\nstruct request_queue *blk_mq_init_queue_data(struct blk_mq_tag_set *set,\n\t\tvoid *queuedata)\n{\n\tstruct request_queue *uninit_q, *q;\n\n\tuninit_q = blk_alloc_queue(set->numa_node);\n\tif (!uninit_q)\n\t\treturn ERR_PTR(-ENOMEM);\n\tuninit_q->queuedata = queuedata;\n\n\t/*\n\t * Initialize the queue without an elevator. device_add_disk() will do\n\t * the initialization.\n\t */\n\tq = blk_mq_init_allocated_queue(set, uninit_q, false);\n\tif (IS_ERR(q))\n\t\tblk_cleanup_queue(uninit_q);\n\n\treturn q;\n}\nEXPORT_SYMBOL_GPL(blk_mq_init_queue_data);\n\nstruct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *set)\n{\n\treturn blk_mq_init_queue_data(set, NULL);\n}\nEXPORT_SYMBOL(blk_mq_init_queue);\n\n/*\n * Helper for setting up a queue with mq ops, given queue depth, and\n * the passed in mq ops flags.\n */\nstruct request_queue *blk_mq_init_sq_queue(struct blk_mq_tag_set *set,\n\t\t\t\t\t   const struct blk_mq_ops *ops,\n\t\t\t\t\t   unsigned int queue_depth,\n\t\t\t\t\t   unsigned int set_flags)\n{\n\tstruct request_queue *q;\n\tint ret;\n\n\tmemset(set, 0, sizeof(*set));\n\tset->ops = ops;\n\tset->nr_hw_queues = 1;\n\tset->nr_maps = 1;\n\tset->queue_depth = queue_depth;\n\tset->numa_node = NUMA_NO_NODE;\n\tset->flags = set_flags;\n\n\tret = blk_mq_alloc_tag_set(set);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\tq = blk_mq_init_queue(set);\n\tif (IS_ERR(q)) {\n\t\tblk_mq_free_tag_set(set);\n\t\treturn q;\n\t}\n\n\treturn q;\n}\nEXPORT_SYMBOL(blk_mq_init_sq_queue);\n\nstatic struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(\n\t\tstruct blk_mq_tag_set *set, struct request_queue *q,\n\t\tint hctx_idx, int node)\n{\n\tstruct blk_mq_hw_ctx *hctx = NULL, *tmp;\n\n\t/* reuse dead hctx first */\n\tspin_lock(&q->unused_hctx_lock);\n\tlist_for_each_entry(tmp, &q->unused_hctx_list, hctx_list) {\n\t\tif (tmp->numa_node == node) {\n\t\t\thctx = tmp;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (hctx)\n\t\tlist_del_init(&hctx->hctx_list);\n\tspin_unlock(&q->unused_hctx_lock);\n\n\tif (!hctx)\n\t\thctx = blk_mq_alloc_hctx(q, set, node);\n\tif (!hctx)\n\t\tgoto fail;\n\n\tif (blk_mq_init_hctx(q, set, hctx, hctx_idx))\n\t\tgoto free_hctx;\n\n\treturn hctx;\n\n free_hctx:\n\tkobject_put(&hctx->kobj);\n fail:\n\treturn NULL;\n}\n\nstatic void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,\n\t\t\t\t\t\tstruct request_queue *q)\n{\n\tint i, j, end;\n\tstruct blk_mq_hw_ctx **hctxs = q->queue_hw_ctx;\n\n\tif (q->nr_hw_queues < set->nr_hw_queues) {\n\t\tstruct blk_mq_hw_ctx **new_hctxs;\n\n\t\tnew_hctxs = kcalloc_node(set->nr_hw_queues,\n\t\t\t\t       sizeof(*new_hctxs), GFP_KERNEL,\n\t\t\t\t       set->numa_node);\n\t\tif (!new_hctxs)\n\t\t\treturn;\n\t\tif (hctxs)\n\t\t\tmemcpy(new_hctxs, hctxs, q->nr_hw_queues *\n\t\t\t       sizeof(*hctxs));\n\t\tq->queue_hw_ctx = new_hctxs;\n\t\tkfree(hctxs);\n\t\thctxs = new_hctxs;\n\t}\n\n\t/* protect against switching io scheduler  */\n\tmutex_lock(&q->sysfs_lock);\n\tfor (i = 0; i < set->nr_hw_queues; i++) {\n\t\tint node;\n\t\tstruct blk_mq_hw_ctx *hctx;\n\n\t\tnode = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], i);\n\t\t/*\n\t\t * If the hw queue has been mapped to another numa node,\n\t\t * we need to realloc the hctx. If allocation fails, fallback\n\t\t * to use the previous one.\n\t\t */\n\t\tif (hctxs[i] && (hctxs[i]->numa_node == node))\n\t\t\tcontinue;\n\n\t\thctx = blk_mq_alloc_and_init_hctx(set, q, i, node);\n\t\tif (hctx) {\n\t\t\tif (hctxs[i])\n\t\t\t\tblk_mq_exit_hctx(q, set, hctxs[i], i);\n\t\t\thctxs[i] = hctx;\n\t\t} else {\n\t\t\tif (hctxs[i])\n\t\t\t\tpr_warn(\"Allocate new hctx on node %d fails,\\\n\t\t\t\t\t\tfallback to previous one on node %d\\n\",\n\t\t\t\t\t\tnode, hctxs[i]->numa_node);\n\t\t\telse\n\t\t\t\tbreak;\n\t\t}\n\t}\n\t/*\n\t * Increasing nr_hw_queues fails. Free the newly allocated\n\t * hctxs and keep the previous q->nr_hw_queues.\n\t */\n\tif (i != set->nr_hw_queues) {\n\t\tj = q->nr_hw_queues;\n\t\tend = i;\n\t} else {\n\t\tj = i;\n\t\tend = q->nr_hw_queues;\n\t\tq->nr_hw_queues = set->nr_hw_queues;\n\t}\n\n\tfor (; j < end; j++) {\n\t\tstruct blk_mq_hw_ctx *hctx = hctxs[j];\n\n\t\tif (hctx) {\n\t\t\tif (hctx->tags)\n\t\t\t\tblk_mq_free_map_and_requests(set, j);\n\t\t\tblk_mq_exit_hctx(q, set, hctx, j);\n\t\t\thctxs[j] = NULL;\n\t\t}\n\t}\n\tmutex_unlock(&q->sysfs_lock);\n}\n\nstruct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,\n\t\t\t\t\t\t  struct request_queue *q,\n\t\t\t\t\t\t  bool elevator_init)\n{\n\t/* mark the queue as mq asap */\n\tq->mq_ops = set->ops;\n\n\tq->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,\n\t\t\t\t\t     blk_mq_poll_stats_bkt,\n\t\t\t\t\t     BLK_MQ_POLL_STATS_BKTS, q);\n\tif (!q->poll_cb)\n\t\tgoto err_exit;\n\n\tif (blk_mq_alloc_ctxs(q))\n\t\tgoto err_poll;\n\n\t/* init q->mq_kobj and sw queues' kobjects */\n\tblk_mq_sysfs_init(q);\n\n\tINIT_LIST_HEAD(&q->unused_hctx_list);\n\tspin_lock_init(&q->unused_hctx_lock);\n\n\tblk_mq_realloc_hw_ctxs(set, q);\n\tif (!q->nr_hw_queues)\n\t\tgoto err_hctxs;\n\n\tINIT_WORK(&q->timeout_work, blk_mq_timeout_work);\n\tblk_queue_rq_timeout(q, set->timeout ? set->timeout : 30 * HZ);\n\n\tq->tag_set = set;\n\n\tq->queue_flags |= QUEUE_FLAG_MQ_DEFAULT;\n\tif (set->nr_maps > HCTX_TYPE_POLL &&\n\t    set->map[HCTX_TYPE_POLL].nr_queues)\n\t\tblk_queue_flag_set(QUEUE_FLAG_POLL, q);\n\n\tq->sg_reserved_size = INT_MAX;\n\n\tINIT_DELAYED_WORK(&q->requeue_work, blk_mq_requeue_work);\n\tINIT_LIST_HEAD(&q->requeue_list);\n\tspin_lock_init(&q->requeue_lock);\n\n\tq->nr_requests = set->queue_depth;\n\n\t/*\n\t * Default to classic polling\n\t */\n\tq->poll_nsec = BLK_MQ_POLL_CLASSIC;\n\n\tblk_mq_init_cpu_queues(q, set->nr_hw_queues);\n\tblk_mq_add_queue_tag_set(set, q);\n\tblk_mq_map_swqueue(q);\n\n\tif (elevator_init)\n\t\televator_init_mq(q);\n\n\treturn q;\n\nerr_hctxs:\n\tkfree(q->queue_hw_ctx);\n\tq->nr_hw_queues = 0;\n\tblk_mq_sysfs_deinit(q);\nerr_poll:\n\tblk_stat_free_callback(q->poll_cb);\n\tq->poll_cb = NULL;\nerr_exit:\n\tq->mq_ops = NULL;\n\treturn ERR_PTR(-ENOMEM);\n}\nEXPORT_SYMBOL(blk_mq_init_allocated_queue);\n\n/* tags can _not_ be used after returning from blk_mq_exit_queue */\nvoid blk_mq_exit_queue(struct request_queue *q)\n{\n\tstruct blk_mq_tag_set\t*set = q->tag_set;\n\n\tblk_mq_del_queue_tag_set(q);\n\tblk_mq_exit_hw_queues(q, set, set->nr_hw_queues);\n}\n\nstatic int __blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)\n{\n\tint i;\n\n\tfor (i = 0; i < set->nr_hw_queues; i++) {\n\t\tif (!__blk_mq_alloc_map_and_request(set, i))\n\t\t\tgoto out_unwind;\n\t\tcond_resched();\n\t}\n\n\treturn 0;\n\nout_unwind:\n\twhile (--i >= 0)\n\t\tblk_mq_free_map_and_requests(set, i);\n\n\treturn -ENOMEM;\n}\n\n/*\n * Allocate the request maps associated with this tag_set. Note that this\n * may reduce the depth asked for, if memory is tight. set->queue_depth\n * will be updated to reflect the allocated depth.\n */\nstatic int blk_mq_alloc_map_and_requests(struct blk_mq_tag_set *set)\n{\n\tunsigned int depth;\n\tint err;\n\n\tdepth = set->queue_depth;\n\tdo {\n\t\terr = __blk_mq_alloc_rq_maps(set);\n\t\tif (!err)\n\t\t\tbreak;\n\n\t\tset->queue_depth >>= 1;\n\t\tif (set->queue_depth < set->reserved_tags + BLK_MQ_TAG_MIN) {\n\t\t\terr = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t} while (set->queue_depth);\n\n\tif (!set->queue_depth || err) {\n\t\tpr_err(\"blk-mq: failed to allocate request map\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (depth != set->queue_depth)\n\t\tpr_info(\"blk-mq: reduced tag depth (%u -> %u)\\n\",\n\t\t\t\t\t\tdepth, set->queue_depth);\n\n\treturn 0;\n}\n\nstatic int blk_mq_update_queue_map(struct blk_mq_tag_set *set)\n{\n\t/*\n\t * blk_mq_map_queues() and multiple .map_queues() implementations\n\t * expect that set->map[HCTX_TYPE_DEFAULT].nr_queues is set to the\n\t * number of hardware queues.\n\t */\n\tif (set->nr_maps == 1)\n\t\tset->map[HCTX_TYPE_DEFAULT].nr_queues = set->nr_hw_queues;\n\n\tif (set->ops->map_queues && !is_kdump_kernel()) {\n\t\tint i;\n\n\t\t/*\n\t\t * transport .map_queues is usually done in the following\n\t\t * way:\n\t\t *\n\t\t * for (queue = 0; queue < set->nr_hw_queues; queue++) {\n\t\t * \tmask = get_cpu_mask(queue)\n\t\t * \tfor_each_cpu(cpu, mask)\n\t\t * \t\tset->map[x].mq_map[cpu] = queue;\n\t\t * }\n\t\t *\n\t\t * When we need to remap, the table has to be cleared for\n\t\t * killing stale mapping since one CPU may not be mapped\n\t\t * to any hw queue.\n\t\t */\n\t\tfor (i = 0; i < set->nr_maps; i++)\n\t\t\tblk_mq_clear_mq_map(&set->map[i]);\n\n\t\treturn set->ops->map_queues(set);\n\t} else {\n\t\tBUG_ON(set->nr_maps > 1);\n\t\treturn blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);\n\t}\n}\n\nstatic int blk_mq_realloc_tag_set_tags(struct blk_mq_tag_set *set,\n\t\t\t\t  int cur_nr_hw_queues, int new_nr_hw_queues)\n{\n\tstruct blk_mq_tags **new_tags;\n\n\tif (cur_nr_hw_queues >= new_nr_hw_queues)\n\t\treturn 0;\n\n\tnew_tags = kcalloc_node(new_nr_hw_queues, sizeof(struct blk_mq_tags *),\n\t\t\t\tGFP_KERNEL, set->numa_node);\n\tif (!new_tags)\n\t\treturn -ENOMEM;\n\n\tif (set->tags)\n\t\tmemcpy(new_tags, set->tags, cur_nr_hw_queues *\n\t\t       sizeof(*set->tags));\n\tkfree(set->tags);\n\tset->tags = new_tags;\n\tset->nr_hw_queues = new_nr_hw_queues;\n\n\treturn 0;\n}\n\n/*\n * Alloc a tag set to be associated with one or more request queues.\n * May fail with EINVAL for various error conditions. May adjust the\n * requested depth down, if it's too large. In that case, the set\n * value will be stored in set->queue_depth.\n */\nint blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)\n{\n\tint i, ret;\n\n\tBUILD_BUG_ON(BLK_MQ_MAX_DEPTH > 1 << BLK_MQ_UNIQUE_TAG_BITS);\n\n\tif (!set->nr_hw_queues)\n\t\treturn -EINVAL;\n\tif (!set->queue_depth)\n\t\treturn -EINVAL;\n\tif (set->queue_depth < set->reserved_tags + BLK_MQ_TAG_MIN)\n\t\treturn -EINVAL;\n\n\tif (!set->ops->queue_rq)\n\t\treturn -EINVAL;\n\n\tif (!set->ops->get_budget ^ !set->ops->put_budget)\n\t\treturn -EINVAL;\n\n\tif (set->queue_depth > BLK_MQ_MAX_DEPTH) {\n\t\tpr_info(\"blk-mq: reduced tag depth to %u\\n\",\n\t\t\tBLK_MQ_MAX_DEPTH);\n\t\tset->queue_depth = BLK_MQ_MAX_DEPTH;\n\t}\n\n\tif (!set->nr_maps)\n\t\tset->nr_maps = 1;\n\telse if (set->nr_maps > HCTX_MAX_TYPES)\n\t\treturn -EINVAL;\n\n\t/*\n\t * If a crashdump is active, then we are potentially in a very\n\t * memory constrained environment. Limit us to 1 queue and\n\t * 64 tags to prevent using too much memory.\n\t */\n\tif (is_kdump_kernel()) {\n\t\tset->nr_hw_queues = 1;\n\t\tset->nr_maps = 1;\n\t\tset->queue_depth = min(64U, set->queue_depth);\n\t}\n\t/*\n\t * There is no use for more h/w queues than cpus if we just have\n\t * a single map\n\t */\n\tif (set->nr_maps == 1 && set->nr_hw_queues > nr_cpu_ids)\n\t\tset->nr_hw_queues = nr_cpu_ids;\n\n\tif (blk_mq_realloc_tag_set_tags(set, 0, set->nr_hw_queues) < 0)\n\t\treturn -ENOMEM;\n\n\tret = -ENOMEM;\n\tfor (i = 0; i < set->nr_maps; i++) {\n\t\tset->map[i].mq_map = kcalloc_node(nr_cpu_ids,\n\t\t\t\t\t\t  sizeof(set->map[i].mq_map[0]),\n\t\t\t\t\t\t  GFP_KERNEL, set->numa_node);\n\t\tif (!set->map[i].mq_map)\n\t\t\tgoto out_free_mq_map;\n\t\tset->map[i].nr_queues = is_kdump_kernel() ? 1 : set->nr_hw_queues;\n\t}\n\n\tret = blk_mq_update_queue_map(set);\n\tif (ret)\n\t\tgoto out_free_mq_map;\n\n\tret = blk_mq_alloc_map_and_requests(set);\n\tif (ret)\n\t\tgoto out_free_mq_map;\n\n\tif (blk_mq_is_sbitmap_shared(set->flags)) {\n\t\tatomic_set(&set->active_queues_shared_sbitmap, 0);\n\n\t\tif (blk_mq_init_shared_sbitmap(set, set->flags)) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out_free_mq_rq_maps;\n\t\t}\n\t}\n\n\tmutex_init(&set->tag_list_lock);\n\tINIT_LIST_HEAD(&set->tag_list);\n\n\treturn 0;\n\nout_free_mq_rq_maps:\n\tfor (i = 0; i < set->nr_hw_queues; i++)\n\t\tblk_mq_free_map_and_requests(set, i);\nout_free_mq_map:\n\tfor (i = 0; i < set->nr_maps; i++) {\n\t\tkfree(set->map[i].mq_map);\n\t\tset->map[i].mq_map = NULL;\n\t}\n\tkfree(set->tags);\n\tset->tags = NULL;\n\treturn ret;\n}\nEXPORT_SYMBOL(blk_mq_alloc_tag_set);\n\nvoid blk_mq_free_tag_set(struct blk_mq_tag_set *set)\n{\n\tint i, j;\n\n\tfor (i = 0; i < set->nr_hw_queues; i++)\n\t\tblk_mq_free_map_and_requests(set, i);\n\n\tif (blk_mq_is_sbitmap_shared(set->flags))\n\t\tblk_mq_exit_shared_sbitmap(set);\n\n\tfor (j = 0; j < set->nr_maps; j++) {\n\t\tkfree(set->map[j].mq_map);\n\t\tset->map[j].mq_map = NULL;\n\t}\n\n\tkfree(set->tags);\n\tset->tags = NULL;\n}\nEXPORT_SYMBOL(blk_mq_free_tag_set);\n\nint blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)\n{\n\tstruct blk_mq_tag_set *set = q->tag_set;\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i, ret;\n\n\tif (!set)\n\t\treturn -EINVAL;\n\n\tif (q->nr_requests == nr)\n\t\treturn 0;\n\n\tblk_mq_freeze_queue(q);\n\tblk_mq_quiesce_queue(q);\n\n\tret = 0;\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (!hctx->tags)\n\t\t\tcontinue;\n\t\t/*\n\t\t * If we're using an MQ scheduler, just update the scheduler\n\t\t * queue depth. This is similar to what the old code would do.\n\t\t */\n\t\tif (!hctx->sched_tags) {\n\t\t\tret = blk_mq_tag_update_depth(hctx, &hctx->tags, nr,\n\t\t\t\t\t\t\tfalse);\n\t\t\tif (!ret && blk_mq_is_sbitmap_shared(set->flags))\n\t\t\t\tblk_mq_tag_resize_shared_sbitmap(set, nr);\n\t\t} else {\n\t\t\tret = blk_mq_tag_update_depth(hctx, &hctx->sched_tags,\n\t\t\t\t\t\t\tnr, true);\n\t\t}\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (q->elevator && q->elevator->type->ops.depth_updated)\n\t\t\tq->elevator->type->ops.depth_updated(hctx);\n\t}\n\n\tif (!ret)\n\t\tq->nr_requests = nr;\n\n\tblk_mq_unquiesce_queue(q);\n\tblk_mq_unfreeze_queue(q);\n\n\treturn ret;\n}\n\n/*\n * request_queue and elevator_type pair.\n * It is just used by __blk_mq_update_nr_hw_queues to cache\n * the elevator_type associated with a request_queue.\n */\nstruct blk_mq_qe_pair {\n\tstruct list_head node;\n\tstruct request_queue *q;\n\tstruct elevator_type *type;\n};\n\n/*\n * Cache the elevator_type in qe pair list and switch the\n * io scheduler to 'none'\n */\nstatic bool blk_mq_elv_switch_none(struct list_head *head,\n\t\tstruct request_queue *q)\n{\n\tstruct blk_mq_qe_pair *qe;\n\n\tif (!q->elevator)\n\t\treturn true;\n\n\tqe = kmalloc(sizeof(*qe), GFP_NOIO | __GFP_NOWARN | __GFP_NORETRY);\n\tif (!qe)\n\t\treturn false;\n\n\tINIT_LIST_HEAD(&qe->node);\n\tqe->q = q;\n\tqe->type = q->elevator->type;\n\tlist_add(&qe->node, head);\n\n\tmutex_lock(&q->sysfs_lock);\n\t/*\n\t * After elevator_switch_mq, the previous elevator_queue will be\n\t * released by elevator_release. The reference of the io scheduler\n\t * module get by elevator_get will also be put. So we need to get\n\t * a reference of the io scheduler module here to prevent it to be\n\t * removed.\n\t */\n\t__module_get(qe->type->elevator_owner);\n\televator_switch_mq(q, NULL);\n\tmutex_unlock(&q->sysfs_lock);\n\n\treturn true;\n}\n\nstatic void blk_mq_elv_switch_back(struct list_head *head,\n\t\tstruct request_queue *q)\n{\n\tstruct blk_mq_qe_pair *qe;\n\tstruct elevator_type *t = NULL;\n\n\tlist_for_each_entry(qe, head, node)\n\t\tif (qe->q == q) {\n\t\t\tt = qe->type;\n\t\t\tbreak;\n\t\t}\n\n\tif (!t)\n\t\treturn;\n\n\tlist_del(&qe->node);\n\tkfree(qe);\n\n\tmutex_lock(&q->sysfs_lock);\n\televator_switch_mq(q, t);\n\tmutex_unlock(&q->sysfs_lock);\n}\n\nstatic void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,\n\t\t\t\t\t\t\tint nr_hw_queues)\n{\n\tstruct request_queue *q;\n\tLIST_HEAD(head);\n\tint prev_nr_hw_queues;\n\n\tlockdep_assert_held(&set->tag_list_lock);\n\n\tif (set->nr_maps == 1 && nr_hw_queues > nr_cpu_ids)\n\t\tnr_hw_queues = nr_cpu_ids;\n\tif (nr_hw_queues < 1)\n\t\treturn;\n\tif (set->nr_maps == 1 && nr_hw_queues == set->nr_hw_queues)\n\t\treturn;\n\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tblk_mq_freeze_queue(q);\n\t/*\n\t * Switch IO scheduler to 'none', cleaning up the data associated\n\t * with the previous scheduler. We will switch back once we are done\n\t * updating the new sw to hw queue mappings.\n\t */\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tif (!blk_mq_elv_switch_none(&head, q))\n\t\t\tgoto switch_back;\n\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list) {\n\t\tblk_mq_debugfs_unregister_hctxs(q);\n\t\tblk_mq_sysfs_unregister(q);\n\t}\n\n\tprev_nr_hw_queues = set->nr_hw_queues;\n\tif (blk_mq_realloc_tag_set_tags(set, set->nr_hw_queues, nr_hw_queues) <\n\t    0)\n\t\tgoto reregister;\n\n\tset->nr_hw_queues = nr_hw_queues;\nfallback:\n\tblk_mq_update_queue_map(set);\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list) {\n\t\tblk_mq_realloc_hw_ctxs(set, q);\n\t\tif (q->nr_hw_queues != set->nr_hw_queues) {\n\t\t\tpr_warn(\"Increasing nr_hw_queues to %d fails, fallback to %d\\n\",\n\t\t\t\t\tnr_hw_queues, prev_nr_hw_queues);\n\t\t\tset->nr_hw_queues = prev_nr_hw_queues;\n\t\t\tblk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);\n\t\t\tgoto fallback;\n\t\t}\n\t\tblk_mq_map_swqueue(q);\n\t}\n\nreregister:\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list) {\n\t\tblk_mq_sysfs_register(q);\n\t\tblk_mq_debugfs_register_hctxs(q);\n\t}\n\nswitch_back:\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tblk_mq_elv_switch_back(&head, q);\n\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tblk_mq_unfreeze_queue(q);\n}\n\nvoid blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)\n{\n\tmutex_lock(&set->tag_list_lock);\n\t__blk_mq_update_nr_hw_queues(set, nr_hw_queues);\n\tmutex_unlock(&set->tag_list_lock);\n}\nEXPORT_SYMBOL_GPL(blk_mq_update_nr_hw_queues);\n\n/* Enable polling stats and return whether they were already enabled. */\nstatic bool blk_poll_stats_enable(struct request_queue *q)\n{\n\tif (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||\n\t    blk_queue_flag_test_and_set(QUEUE_FLAG_POLL_STATS, q))\n\t\treturn true;\n\tblk_stat_add_callback(q, q->poll_cb);\n\treturn false;\n}\n\nstatic void blk_mq_poll_stats_start(struct request_queue *q)\n{\n\t/*\n\t * We don't arm the callback if polling stats are not enabled or the\n\t * callback is already active.\n\t */\n\tif (!test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||\n\t    blk_stat_is_active(q->poll_cb))\n\t\treturn;\n\n\tblk_stat_activate_msecs(q->poll_cb, 100);\n}\n\nstatic void blk_mq_poll_stats_fn(struct blk_stat_callback *cb)\n{\n\tstruct request_queue *q = cb->data;\n\tint bucket;\n\n\tfor (bucket = 0; bucket < BLK_MQ_POLL_STATS_BKTS; bucket++) {\n\t\tif (cb->stat[bucket].nr_samples)\n\t\t\tq->poll_stat[bucket] = cb->stat[bucket];\n\t}\n}\n\nstatic unsigned long blk_mq_poll_nsecs(struct request_queue *q,\n\t\t\t\t       struct request *rq)\n{\n\tunsigned long ret = 0;\n\tint bucket;\n\n\t/*\n\t * If stats collection isn't on, don't sleep but turn it on for\n\t * future users\n\t */\n\tif (!blk_poll_stats_enable(q))\n\t\treturn 0;\n\n\t/*\n\t * As an optimistic guess, use half of the mean service time\n\t * for this type of request. We can (and should) make this smarter.\n\t * For instance, if the completion latencies are tight, we can\n\t * get closer than just half the mean. This is especially\n\t * important on devices where the completion latencies are longer\n\t * than ~10 usec. We do use the stats for the relevant IO size\n\t * if available which does lead to better estimates.\n\t */\n\tbucket = blk_mq_poll_stats_bkt(rq);\n\tif (bucket < 0)\n\t\treturn ret;\n\n\tif (q->poll_stat[bucket].nr_samples)\n\t\tret = (q->poll_stat[bucket].mean + 1) / 2;\n\n\treturn ret;\n}\n\nstatic bool blk_mq_poll_hybrid_sleep(struct request_queue *q,\n\t\t\t\t     struct request *rq)\n{\n\tstruct hrtimer_sleeper hs;\n\tenum hrtimer_mode mode;\n\tunsigned int nsecs;\n\tktime_t kt;\n\n\tif (rq->rq_flags & RQF_MQ_POLL_SLEPT)\n\t\treturn false;\n\n\t/*\n\t * If we get here, hybrid polling is enabled. Hence poll_nsec can be:\n\t *\n\t *  0:\tuse half of prev avg\n\t * >0:\tuse this specific value\n\t */\n\tif (q->poll_nsec > 0)\n\t\tnsecs = q->poll_nsec;\n\telse\n\t\tnsecs = blk_mq_poll_nsecs(q, rq);\n\n\tif (!nsecs)\n\t\treturn false;\n\n\trq->rq_flags |= RQF_MQ_POLL_SLEPT;\n\n\t/*\n\t * This will be replaced with the stats tracking code, using\n\t * 'avg_completion_time / 2' as the pre-sleep target.\n\t */\n\tkt = nsecs;\n\n\tmode = HRTIMER_MODE_REL;\n\thrtimer_init_sleeper_on_stack(&hs, CLOCK_MONOTONIC, mode);\n\thrtimer_set_expires(&hs.timer, kt);\n\n\tdo {\n\t\tif (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)\n\t\t\tbreak;\n\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\thrtimer_sleeper_start_expires(&hs, mode);\n\t\tif (hs.task)\n\t\t\tio_schedule();\n\t\thrtimer_cancel(&hs.timer);\n\t\tmode = HRTIMER_MODE_ABS;\n\t} while (hs.task && !signal_pending(current));\n\n\t__set_current_state(TASK_RUNNING);\n\tdestroy_hrtimer_on_stack(&hs.timer);\n\treturn true;\n}\n\nstatic bool blk_mq_poll_hybrid(struct request_queue *q,\n\t\t\t       struct blk_mq_hw_ctx *hctx, blk_qc_t cookie)\n{\n\tstruct request *rq;\n\n\tif (q->poll_nsec == BLK_MQ_POLL_CLASSIC)\n\t\treturn false;\n\n\tif (!blk_qc_t_is_internal(cookie))\n\t\trq = blk_mq_tag_to_rq(hctx->tags, blk_qc_t_to_tag(cookie));\n\telse {\n\t\trq = blk_mq_tag_to_rq(hctx->sched_tags, blk_qc_t_to_tag(cookie));\n\t\t/*\n\t\t * With scheduling, if the request has completed, we'll\n\t\t * get a NULL return here, as we clear the sched tag when\n\t\t * that happens. The request still remains valid, like always,\n\t\t * so we should be safe with just the NULL check.\n\t\t */\n\t\tif (!rq)\n\t\t\treturn false;\n\t}\n\n\treturn blk_mq_poll_hybrid_sleep(q, rq);\n}\n\n/**\n * blk_poll - poll for IO completions\n * @q:  the queue\n * @cookie: cookie passed back at IO submission time\n * @spin: whether to spin for completions\n *\n * Description:\n *    Poll for completions on the passed in queue. Returns number of\n *    completed entries found. If @spin is true, then blk_poll will continue\n *    looping until at least one completion is found, unless the task is\n *    otherwise marked running (or we need to reschedule).\n */\nint blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tlong state;\n\n\tif (!blk_qc_t_valid(cookie) ||\n\t    !test_bit(QUEUE_FLAG_POLL, &q->queue_flags))\n\t\treturn 0;\n\n\tif (current->plug)\n\t\tblk_flush_plug_list(current->plug, false);\n\n\thctx = q->queue_hw_ctx[blk_qc_t_to_queue_num(cookie)];\n\n\t/*\n\t * If we sleep, have the caller restart the poll loop to reset\n\t * the state. Like for the other success return cases, the\n\t * caller is responsible for checking if the IO completed. If\n\t * the IO isn't complete, we'll get called again and will go\n\t * straight to the busy poll loop.\n\t */\n\tif (blk_mq_poll_hybrid(q, hctx, cookie))\n\t\treturn 1;\n\n\thctx->poll_considered++;\n\n\tstate = current->state;\n\tdo {\n\t\tint ret;\n\n\t\thctx->poll_invoked++;\n\n\t\tret = q->mq_ops->poll(hctx);\n\t\tif (ret > 0) {\n\t\t\thctx->poll_success++;\n\t\t\t__set_current_state(TASK_RUNNING);\n\t\t\treturn ret;\n\t\t}\n\n\t\tif (signal_pending_state(state, current))\n\t\t\t__set_current_state(TASK_RUNNING);\n\n\t\tif (current->state == TASK_RUNNING)\n\t\t\treturn 1;\n\t\tif (ret < 0 || !spin)\n\t\t\tbreak;\n\t\tcpu_relax();\n\t} while (!need_resched());\n\n\t__set_current_state(TASK_RUNNING);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(blk_poll);\n\nunsigned int blk_mq_rq_cpu(struct request *rq)\n{\n\treturn rq->mq_ctx->cpu;\n}\nEXPORT_SYMBOL(blk_mq_rq_cpu);\n\nstatic int __init blk_mq_init(void)\n{\n\tint i;\n\n\tfor_each_possible_cpu(i)\n\t\tINIT_LIST_HEAD(&per_cpu(blk_cpu_done, i));\n\topen_softirq(BLOCK_SOFTIRQ, blk_done_softirq);\n\n\tcpuhp_setup_state_nocalls(CPUHP_BLOCK_SOFTIRQ_DEAD,\n\t\t\t\t  \"block/softirq:dead\", NULL,\n\t\t\t\t  blk_softirq_cpu_dead);\n\tcpuhp_setup_state_multi(CPUHP_BLK_MQ_DEAD, \"block/mq:dead\", NULL,\n\t\t\t\tblk_mq_hctx_notify_dead);\n\tcpuhp_setup_state_multi(CPUHP_AP_BLK_MQ_ONLINE, \"block/mq:online\",\n\t\t\t\tblk_mq_hctx_notify_online,\n\t\t\t\tblk_mq_hctx_notify_offline);\n\treturn 0;\n}\nsubsys_initcall(blk_mq_init);\n"}, "1": {"id": 1, "path": "/src/include/linux/list.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_LIST_H\n#define _LINUX_LIST_H\n\n#include <linux/types.h>\n#include <linux/stddef.h>\n#include <linux/poison.h>\n#include <linux/const.h>\n#include <linux/kernel.h>\n\n/*\n * Simple doubly linked list implementation.\n *\n * Some of the internal functions (\"__xxx\") are useful when\n * manipulating whole lists rather than single entries, as\n * sometimes we already know the next/prev entries and we can\n * generate better code by using them directly rather than\n * using the generic single-entry routines.\n */\n\n#define LIST_HEAD_INIT(name) { &(name), &(name) }\n\n#define LIST_HEAD(name) \\\n\tstruct list_head name = LIST_HEAD_INIT(name)\n\n/**\n * INIT_LIST_HEAD - Initialize a list_head structure\n * @list: list_head structure to be initialized.\n *\n * Initializes the list_head to point to itself.  If it is a list header,\n * the result is an empty list.\n */\nstatic inline void INIT_LIST_HEAD(struct list_head *list)\n{\n\tWRITE_ONCE(list->next, list);\n\tlist->prev = list;\n}\n\n#ifdef CONFIG_DEBUG_LIST\nextern bool __list_add_valid(struct list_head *new,\n\t\t\t      struct list_head *prev,\n\t\t\t      struct list_head *next);\nextern bool __list_del_entry_valid(struct list_head *entry);\n#else\nstatic inline bool __list_add_valid(struct list_head *new,\n\t\t\t\tstruct list_head *prev,\n\t\t\t\tstruct list_head *next)\n{\n\treturn true;\n}\nstatic inline bool __list_del_entry_valid(struct list_head *entry)\n{\n\treturn true;\n}\n#endif\n\n/*\n * Insert a new entry between two known consecutive entries.\n *\n * This is only for internal list manipulation where we know\n * the prev/next entries already!\n */\nstatic inline void __list_add(struct list_head *new,\n\t\t\t      struct list_head *prev,\n\t\t\t      struct list_head *next)\n{\n\tif (!__list_add_valid(new, prev, next))\n\t\treturn;\n\n\tnext->prev = new;\n\tnew->next = next;\n\tnew->prev = prev;\n\tWRITE_ONCE(prev->next, new);\n}\n\n/**\n * list_add - add a new entry\n * @new: new entry to be added\n * @head: list head to add it after\n *\n * Insert a new entry after the specified head.\n * This is good for implementing stacks.\n */\nstatic inline void list_add(struct list_head *new, struct list_head *head)\n{\n\t__list_add(new, head, head->next);\n}\n\n\n/**\n * list_add_tail - add a new entry\n * @new: new entry to be added\n * @head: list head to add it before\n *\n * Insert a new entry before the specified head.\n * This is useful for implementing queues.\n */\nstatic inline void list_add_tail(struct list_head *new, struct list_head *head)\n{\n\t__list_add(new, head->prev, head);\n}\n\n/*\n * Delete a list entry by making the prev/next entries\n * point to each other.\n *\n * This is only for internal list manipulation where we know\n * the prev/next entries already!\n */\nstatic inline void __list_del(struct list_head * prev, struct list_head * next)\n{\n\tnext->prev = prev;\n\tWRITE_ONCE(prev->next, next);\n}\n\n/*\n * Delete a list entry and clear the 'prev' pointer.\n *\n * This is a special-purpose list clearing method used in the networking code\n * for lists allocated as per-cpu, where we don't want to incur the extra\n * WRITE_ONCE() overhead of a regular list_del_init(). The code that uses this\n * needs to check the node 'prev' pointer instead of calling list_empty().\n */\nstatic inline void __list_del_clearprev(struct list_head *entry)\n{\n\t__list_del(entry->prev, entry->next);\n\tentry->prev = NULL;\n}\n\nstatic inline void __list_del_entry(struct list_head *entry)\n{\n\tif (!__list_del_entry_valid(entry))\n\t\treturn;\n\n\t__list_del(entry->prev, entry->next);\n}\n\n/**\n * list_del - deletes entry from list.\n * @entry: the element to delete from the list.\n * Note: list_empty() on entry does not return true after this, the entry is\n * in an undefined state.\n */\nstatic inline void list_del(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tentry->next = LIST_POISON1;\n\tentry->prev = LIST_POISON2;\n}\n\n/**\n * list_replace - replace old entry by new one\n * @old : the element to be replaced\n * @new : the new element to insert\n *\n * If @old was empty, it will be overwritten.\n */\nstatic inline void list_replace(struct list_head *old,\n\t\t\t\tstruct list_head *new)\n{\n\tnew->next = old->next;\n\tnew->next->prev = new;\n\tnew->prev = old->prev;\n\tnew->prev->next = new;\n}\n\n/**\n * list_replace_init - replace old entry by new one and initialize the old one\n * @old : the element to be replaced\n * @new : the new element to insert\n *\n * If @old was empty, it will be overwritten.\n */\nstatic inline void list_replace_init(struct list_head *old,\n\t\t\t\t     struct list_head *new)\n{\n\tlist_replace(old, new);\n\tINIT_LIST_HEAD(old);\n}\n\n/**\n * list_swap - replace entry1 with entry2 and re-add entry1 at entry2's position\n * @entry1: the location to place entry2\n * @entry2: the location to place entry1\n */\nstatic inline void list_swap(struct list_head *entry1,\n\t\t\t     struct list_head *entry2)\n{\n\tstruct list_head *pos = entry2->prev;\n\n\tlist_del(entry2);\n\tlist_replace(entry1, entry2);\n\tif (pos == entry1)\n\t\tpos = entry2;\n\tlist_add(entry1, pos);\n}\n\n/**\n * list_del_init - deletes entry from list and reinitialize it.\n * @entry: the element to delete from the list.\n */\nstatic inline void list_del_init(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tINIT_LIST_HEAD(entry);\n}\n\n/**\n * list_move - delete from one list and add as another's head\n * @list: the entry to move\n * @head: the head that will precede our entry\n */\nstatic inline void list_move(struct list_head *list, struct list_head *head)\n{\n\t__list_del_entry(list);\n\tlist_add(list, head);\n}\n\n/**\n * list_move_tail - delete from one list and add as another's tail\n * @list: the entry to move\n * @head: the head that will follow our entry\n */\nstatic inline void list_move_tail(struct list_head *list,\n\t\t\t\t  struct list_head *head)\n{\n\t__list_del_entry(list);\n\tlist_add_tail(list, head);\n}\n\n/**\n * list_bulk_move_tail - move a subsection of a list to its tail\n * @head: the head that will follow our entry\n * @first: first entry to move\n * @last: last entry to move, can be the same as first\n *\n * Move all entries between @first and including @last before @head.\n * All three entries must belong to the same linked list.\n */\nstatic inline void list_bulk_move_tail(struct list_head *head,\n\t\t\t\t       struct list_head *first,\n\t\t\t\t       struct list_head *last)\n{\n\tfirst->prev->next = last->next;\n\tlast->next->prev = first->prev;\n\n\thead->prev->next = first;\n\tfirst->prev = head->prev;\n\n\tlast->next = head;\n\thead->prev = last;\n}\n\n/**\n * list_is_first -- tests whether @list is the first entry in list @head\n * @list: the entry to test\n * @head: the head of the list\n */\nstatic inline int list_is_first(const struct list_head *list,\n\t\t\t\t\tconst struct list_head *head)\n{\n\treturn list->prev == head;\n}\n\n/**\n * list_is_last - tests whether @list is the last entry in list @head\n * @list: the entry to test\n * @head: the head of the list\n */\nstatic inline int list_is_last(const struct list_head *list,\n\t\t\t\tconst struct list_head *head)\n{\n\treturn list->next == head;\n}\n\n/**\n * list_empty - tests whether a list is empty\n * @head: the list to test.\n */\nstatic inline int list_empty(const struct list_head *head)\n{\n\treturn READ_ONCE(head->next) == head;\n}\n\n/**\n * list_del_init_careful - deletes entry from list and reinitialize it.\n * @entry: the element to delete from the list.\n *\n * This is the same as list_del_init(), except designed to be used\n * together with list_empty_careful() in a way to guarantee ordering\n * of other memory operations.\n *\n * Any memory operations done before a list_del_init_careful() are\n * guaranteed to be visible after a list_empty_careful() test.\n */\nstatic inline void list_del_init_careful(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tentry->prev = entry;\n\tsmp_store_release(&entry->next, entry);\n}\n\n/**\n * list_empty_careful - tests whether a list is empty and not being modified\n * @head: the list to test\n *\n * Description:\n * tests whether a list is empty _and_ checks that no other CPU might be\n * in the process of modifying either member (next or prev)\n *\n * NOTE: using list_empty_careful() without synchronization\n * can only be safe if the only activity that can happen\n * to the list entry is list_del_init(). Eg. it cannot be used\n * if another CPU could re-list_add() it.\n */\nstatic inline int list_empty_careful(const struct list_head *head)\n{\n\tstruct list_head *next = smp_load_acquire(&head->next);\n\treturn (next == head) && (next == head->prev);\n}\n\n/**\n * list_rotate_left - rotate the list to the left\n * @head: the head of the list\n */\nstatic inline void list_rotate_left(struct list_head *head)\n{\n\tstruct list_head *first;\n\n\tif (!list_empty(head)) {\n\t\tfirst = head->next;\n\t\tlist_move_tail(first, head);\n\t}\n}\n\n/**\n * list_rotate_to_front() - Rotate list to specific item.\n * @list: The desired new front of the list.\n * @head: The head of the list.\n *\n * Rotates list so that @list becomes the new front of the list.\n */\nstatic inline void list_rotate_to_front(struct list_head *list,\n\t\t\t\t\tstruct list_head *head)\n{\n\t/*\n\t * Deletes the list head from the list denoted by @head and\n\t * places it as the tail of @list, this effectively rotates the\n\t * list so that @list is at the front.\n\t */\n\tlist_move_tail(head, list);\n}\n\n/**\n * list_is_singular - tests whether a list has just one entry.\n * @head: the list to test.\n */\nstatic inline int list_is_singular(const struct list_head *head)\n{\n\treturn !list_empty(head) && (head->next == head->prev);\n}\n\nstatic inline void __list_cut_position(struct list_head *list,\n\t\tstruct list_head *head, struct list_head *entry)\n{\n\tstruct list_head *new_first = entry->next;\n\tlist->next = head->next;\n\tlist->next->prev = list;\n\tlist->prev = entry;\n\tentry->next = list;\n\thead->next = new_first;\n\tnew_first->prev = head;\n}\n\n/**\n * list_cut_position - cut a list into two\n * @list: a new list to add all removed entries\n * @head: a list with entries\n * @entry: an entry within head, could be the head itself\n *\tand if so we won't cut the list\n *\n * This helper moves the initial part of @head, up to and\n * including @entry, from @head to @list. You should\n * pass on @entry an element you know is on @head. @list\n * should be an empty list or a list you do not care about\n * losing its data.\n *\n */\nstatic inline void list_cut_position(struct list_head *list,\n\t\tstruct list_head *head, struct list_head *entry)\n{\n\tif (list_empty(head))\n\t\treturn;\n\tif (list_is_singular(head) &&\n\t\t(head->next != entry && head != entry))\n\t\treturn;\n\tif (entry == head)\n\t\tINIT_LIST_HEAD(list);\n\telse\n\t\t__list_cut_position(list, head, entry);\n}\n\n/**\n * list_cut_before - cut a list into two, before given entry\n * @list: a new list to add all removed entries\n * @head: a list with entries\n * @entry: an entry within head, could be the head itself\n *\n * This helper moves the initial part of @head, up to but\n * excluding @entry, from @head to @list.  You should pass\n * in @entry an element you know is on @head.  @list should\n * be an empty list or a list you do not care about losing\n * its data.\n * If @entry == @head, all entries on @head are moved to\n * @list.\n */\nstatic inline void list_cut_before(struct list_head *list,\n\t\t\t\t   struct list_head *head,\n\t\t\t\t   struct list_head *entry)\n{\n\tif (head->next == entry) {\n\t\tINIT_LIST_HEAD(list);\n\t\treturn;\n\t}\n\tlist->next = head->next;\n\tlist->next->prev = list;\n\tlist->prev = entry->prev;\n\tlist->prev->next = list;\n\thead->next = entry;\n\tentry->prev = head;\n}\n\nstatic inline void __list_splice(const struct list_head *list,\n\t\t\t\t struct list_head *prev,\n\t\t\t\t struct list_head *next)\n{\n\tstruct list_head *first = list->next;\n\tstruct list_head *last = list->prev;\n\n\tfirst->prev = prev;\n\tprev->next = first;\n\n\tlast->next = next;\n\tnext->prev = last;\n}\n\n/**\n * list_splice - join two lists, this is designed for stacks\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n */\nstatic inline void list_splice(const struct list_head *list,\n\t\t\t\tstruct list_head *head)\n{\n\tif (!list_empty(list))\n\t\t__list_splice(list, head, head->next);\n}\n\n/**\n * list_splice_tail - join two lists, each list being a queue\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n */\nstatic inline void list_splice_tail(struct list_head *list,\n\t\t\t\tstruct list_head *head)\n{\n\tif (!list_empty(list))\n\t\t__list_splice(list, head->prev, head);\n}\n\n/**\n * list_splice_init - join two lists and reinitialise the emptied list.\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n *\n * The list at @list is reinitialised\n */\nstatic inline void list_splice_init(struct list_head *list,\n\t\t\t\t    struct list_head *head)\n{\n\tif (!list_empty(list)) {\n\t\t__list_splice(list, head, head->next);\n\t\tINIT_LIST_HEAD(list);\n\t}\n}\n\n/**\n * list_splice_tail_init - join two lists and reinitialise the emptied list\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n *\n * Each of the lists is a queue.\n * The list at @list is reinitialised\n */\nstatic inline void list_splice_tail_init(struct list_head *list,\n\t\t\t\t\t struct list_head *head)\n{\n\tif (!list_empty(list)) {\n\t\t__list_splice(list, head->prev, head);\n\t\tINIT_LIST_HEAD(list);\n\t}\n}\n\n/**\n * list_entry - get the struct for this entry\n * @ptr:\tthe &struct list_head pointer.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_entry(ptr, type, member) \\\n\tcontainer_of(ptr, type, member)\n\n/**\n * list_first_entry - get the first element from a list\n * @ptr:\tthe list head to take the element from.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n *\n * Note, that list is expected to be not empty.\n */\n#define list_first_entry(ptr, type, member) \\\n\tlist_entry((ptr)->next, type, member)\n\n/**\n * list_last_entry - get the last element from a list\n * @ptr:\tthe list head to take the element from.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n *\n * Note, that list is expected to be not empty.\n */\n#define list_last_entry(ptr, type, member) \\\n\tlist_entry((ptr)->prev, type, member)\n\n/**\n * list_first_entry_or_null - get the first element from a list\n * @ptr:\tthe list head to take the element from.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n *\n * Note that if the list is empty, it returns NULL.\n */\n#define list_first_entry_or_null(ptr, type, member) ({ \\\n\tstruct list_head *head__ = (ptr); \\\n\tstruct list_head *pos__ = READ_ONCE(head__->next); \\\n\tpos__ != head__ ? list_entry(pos__, type, member) : NULL; \\\n})\n\n/**\n * list_next_entry - get the next element in list\n * @pos:\tthe type * to cursor\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_next_entry(pos, member) \\\n\tlist_entry((pos)->member.next, typeof(*(pos)), member)\n\n/**\n * list_prev_entry - get the prev element in list\n * @pos:\tthe type * to cursor\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_prev_entry(pos, member) \\\n\tlist_entry((pos)->member.prev, typeof(*(pos)), member)\n\n/**\n * list_for_each\t-\titerate over a list\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @head:\tthe head for your list.\n */\n#define list_for_each(pos, head) \\\n\tfor (pos = (head)->next; pos != (head); pos = pos->next)\n\n/**\n * list_for_each_continue - continue iteration over a list\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @head:\tthe head for your list.\n *\n * Continue to iterate over a list, continuing after the current position.\n */\n#define list_for_each_continue(pos, head) \\\n\tfor (pos = pos->next; pos != (head); pos = pos->next)\n\n/**\n * list_for_each_prev\t-\titerate over a list backwards\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @head:\tthe head for your list.\n */\n#define list_for_each_prev(pos, head) \\\n\tfor (pos = (head)->prev; pos != (head); pos = pos->prev)\n\n/**\n * list_for_each_safe - iterate over a list safe against removal of list entry\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @n:\t\tanother &struct list_head to use as temporary storage\n * @head:\tthe head for your list.\n */\n#define list_for_each_safe(pos, n, head) \\\n\tfor (pos = (head)->next, n = pos->next; pos != (head); \\\n\t\tpos = n, n = pos->next)\n\n/**\n * list_for_each_prev_safe - iterate over a list backwards safe against removal of list entry\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @n:\t\tanother &struct list_head to use as temporary storage\n * @head:\tthe head for your list.\n */\n#define list_for_each_prev_safe(pos, n, head) \\\n\tfor (pos = (head)->prev, n = pos->prev; \\\n\t     pos != (head); \\\n\t     pos = n, n = pos->prev)\n\n/**\n * list_entry_is_head - test if the entry points to the head of the list\n * @pos:\tthe type * to cursor\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_entry_is_head(pos, head, member)\t\t\t\t\\\n\t(&pos->member == (head))\n\n/**\n * list_for_each_entry\t-\titerate over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_for_each_entry(pos, head, member)\t\t\t\t\\\n\tfor (pos = list_first_entry(head, typeof(*pos), member);\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_next_entry(pos, member))\n\n/**\n * list_for_each_entry_reverse - iterate backwards over list of given type.\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_for_each_entry_reverse(pos, head, member)\t\t\t\\\n\tfor (pos = list_last_entry(head, typeof(*pos), member);\t\t\\\n\t     !list_entry_is_head(pos, head, member); \t\t\t\\\n\t     pos = list_prev_entry(pos, member))\n\n/**\n * list_prepare_entry - prepare a pos entry for use in list_for_each_entry_continue()\n * @pos:\tthe type * to use as a start point\n * @head:\tthe head of the list\n * @member:\tthe name of the list_head within the struct.\n *\n * Prepares a pos entry for use as a start point in list_for_each_entry_continue().\n */\n#define list_prepare_entry(pos, head, member) \\\n\t((pos) ? : list_entry(head, typeof(*pos), member))\n\n/**\n * list_for_each_entry_continue - continue iteration over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Continue to iterate over list of given type, continuing after\n * the current position.\n */\n#define list_for_each_entry_continue(pos, head, member) \t\t\\\n\tfor (pos = list_next_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_next_entry(pos, member))\n\n/**\n * list_for_each_entry_continue_reverse - iterate backwards from the given point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Start to iterate over list of given type backwards, continuing after\n * the current position.\n */\n#define list_for_each_entry_continue_reverse(pos, head, member)\t\t\\\n\tfor (pos = list_prev_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_prev_entry(pos, member))\n\n/**\n * list_for_each_entry_from - iterate over list of given type from the current point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate over list of given type, continuing from current position.\n */\n#define list_for_each_entry_from(pos, head, member) \t\t\t\\\n\tfor (; !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_next_entry(pos, member))\n\n/**\n * list_for_each_entry_from_reverse - iterate backwards over list of given type\n *                                    from the current point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate backwards over list of given type, continuing from current position.\n */\n#define list_for_each_entry_from_reverse(pos, head, member)\t\t\\\n\tfor (; !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_prev_entry(pos, member))\n\n/**\n * list_for_each_entry_safe - iterate over list of given type safe against removal of list entry\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_for_each_entry_safe(pos, n, head, member)\t\t\t\\\n\tfor (pos = list_first_entry(head, typeof(*pos), member),\t\\\n\t\tn = list_next_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member); \t\t\t\\\n\t     pos = n, n = list_next_entry(n, member))\n\n/**\n * list_for_each_entry_safe_continue - continue list iteration safe against removal\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate over list of given type, continuing after current point,\n * safe against removal of list entry.\n */\n#define list_for_each_entry_safe_continue(pos, n, head, member) \t\t\\\n\tfor (pos = list_next_entry(pos, member), \t\t\t\t\\\n\t\tn = list_next_entry(pos, member);\t\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\t\\\n\t     pos = n, n = list_next_entry(n, member))\n\n/**\n * list_for_each_entry_safe_from - iterate over list from current point safe against removal\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate over list of given type from current point, safe against\n * removal of list entry.\n */\n#define list_for_each_entry_safe_from(pos, n, head, member) \t\t\t\\\n\tfor (n = list_next_entry(pos, member);\t\t\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\t\\\n\t     pos = n, n = list_next_entry(n, member))\n\n/**\n * list_for_each_entry_safe_reverse - iterate backwards over list safe against removal\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate backwards over list of given type, safe against removal\n * of list entry.\n */\n#define list_for_each_entry_safe_reverse(pos, n, head, member)\t\t\\\n\tfor (pos = list_last_entry(head, typeof(*pos), member),\t\t\\\n\t\tn = list_prev_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member); \t\t\t\\\n\t     pos = n, n = list_prev_entry(n, member))\n\n/**\n * list_safe_reset_next - reset a stale list_for_each_entry_safe loop\n * @pos:\tthe loop cursor used in the list_for_each_entry_safe loop\n * @n:\t\ttemporary storage used in list_for_each_entry_safe\n * @member:\tthe name of the list_head within the struct.\n *\n * list_safe_reset_next is not safe to use in general if the list may be\n * modified concurrently (eg. the lock is dropped in the loop body). An\n * exception to this is if the cursor element (pos) is pinned in the list,\n * and list_safe_reset_next is called after re-taking the lock and before\n * completing the current iteration of the loop body.\n */\n#define list_safe_reset_next(pos, n, member)\t\t\t\t\\\n\tn = list_next_entry(pos, member)\n\n/*\n * Double linked lists with a single pointer list head.\n * Mostly useful for hash tables where the two pointer list head is\n * too wasteful.\n * You lose the ability to access the tail in O(1).\n */\n\n#define HLIST_HEAD_INIT { .first = NULL }\n#define HLIST_HEAD(name) struct hlist_head name = {  .first = NULL }\n#define INIT_HLIST_HEAD(ptr) ((ptr)->first = NULL)\nstatic inline void INIT_HLIST_NODE(struct hlist_node *h)\n{\n\th->next = NULL;\n\th->pprev = NULL;\n}\n\n/**\n * hlist_unhashed - Has node been removed from list and reinitialized?\n * @h: Node to be checked\n *\n * Not that not all removal functions will leave a node in unhashed\n * state.  For example, hlist_nulls_del_init_rcu() does leave the\n * node in unhashed state, but hlist_nulls_del() does not.\n */\nstatic inline int hlist_unhashed(const struct hlist_node *h)\n{\n\treturn !h->pprev;\n}\n\n/**\n * hlist_unhashed_lockless - Version of hlist_unhashed for lockless use\n * @h: Node to be checked\n *\n * This variant of hlist_unhashed() must be used in lockless contexts\n * to avoid potential load-tearing.  The READ_ONCE() is paired with the\n * various WRITE_ONCE() in hlist helpers that are defined below.\n */\nstatic inline int hlist_unhashed_lockless(const struct hlist_node *h)\n{\n\treturn !READ_ONCE(h->pprev);\n}\n\n/**\n * hlist_empty - Is the specified hlist_head structure an empty hlist?\n * @h: Structure to check.\n */\nstatic inline int hlist_empty(const struct hlist_head *h)\n{\n\treturn !READ_ONCE(h->first);\n}\n\nstatic inline void __hlist_del(struct hlist_node *n)\n{\n\tstruct hlist_node *next = n->next;\n\tstruct hlist_node **pprev = n->pprev;\n\n\tWRITE_ONCE(*pprev, next);\n\tif (next)\n\t\tWRITE_ONCE(next->pprev, pprev);\n}\n\n/**\n * hlist_del - Delete the specified hlist_node from its list\n * @n: Node to delete.\n *\n * Note that this function leaves the node in hashed state.  Use\n * hlist_del_init() or similar instead to unhash @n.\n */\nstatic inline void hlist_del(struct hlist_node *n)\n{\n\t__hlist_del(n);\n\tn->next = LIST_POISON1;\n\tn->pprev = LIST_POISON2;\n}\n\n/**\n * hlist_del_init - Delete the specified hlist_node from its list and initialize\n * @n: Node to delete.\n *\n * Note that this function leaves the node in unhashed state.\n */\nstatic inline void hlist_del_init(struct hlist_node *n)\n{\n\tif (!hlist_unhashed(n)) {\n\t\t__hlist_del(n);\n\t\tINIT_HLIST_NODE(n);\n\t}\n}\n\n/**\n * hlist_add_head - add a new entry at the beginning of the hlist\n * @n: new entry to be added\n * @h: hlist head to add it after\n *\n * Insert a new entry after the specified head.\n * This is good for implementing stacks.\n */\nstatic inline void hlist_add_head(struct hlist_node *n, struct hlist_head *h)\n{\n\tstruct hlist_node *first = h->first;\n\tWRITE_ONCE(n->next, first);\n\tif (first)\n\t\tWRITE_ONCE(first->pprev, &n->next);\n\tWRITE_ONCE(h->first, n);\n\tWRITE_ONCE(n->pprev, &h->first);\n}\n\n/**\n * hlist_add_before - add a new entry before the one specified\n * @n: new entry to be added\n * @next: hlist node to add it before, which must be non-NULL\n */\nstatic inline void hlist_add_before(struct hlist_node *n,\n\t\t\t\t    struct hlist_node *next)\n{\n\tWRITE_ONCE(n->pprev, next->pprev);\n\tWRITE_ONCE(n->next, next);\n\tWRITE_ONCE(next->pprev, &n->next);\n\tWRITE_ONCE(*(n->pprev), n);\n}\n\n/**\n * hlist_add_behing - add a new entry after the one specified\n * @n: new entry to be added\n * @prev: hlist node to add it after, which must be non-NULL\n */\nstatic inline void hlist_add_behind(struct hlist_node *n,\n\t\t\t\t    struct hlist_node *prev)\n{\n\tWRITE_ONCE(n->next, prev->next);\n\tWRITE_ONCE(prev->next, n);\n\tWRITE_ONCE(n->pprev, &prev->next);\n\n\tif (n->next)\n\t\tWRITE_ONCE(n->next->pprev, &n->next);\n}\n\n/**\n * hlist_add_fake - create a fake hlist consisting of a single headless node\n * @n: Node to make a fake list out of\n *\n * This makes @n appear to be its own predecessor on a headless hlist.\n * The point of this is to allow things like hlist_del() to work correctly\n * in cases where there is no list.\n */\nstatic inline void hlist_add_fake(struct hlist_node *n)\n{\n\tn->pprev = &n->next;\n}\n\n/**\n * hlist_fake: Is this node a fake hlist?\n * @h: Node to check for being a self-referential fake hlist.\n */\nstatic inline bool hlist_fake(struct hlist_node *h)\n{\n\treturn h->pprev == &h->next;\n}\n\n/**\n * hlist_is_singular_node - is node the only element of the specified hlist?\n * @n: Node to check for singularity.\n * @h: Header for potentially singular list.\n *\n * Check whether the node is the only node of the head without\n * accessing head, thus avoiding unnecessary cache misses.\n */\nstatic inline bool\nhlist_is_singular_node(struct hlist_node *n, struct hlist_head *h)\n{\n\treturn !n->next && n->pprev == &h->first;\n}\n\n/**\n * hlist_move_list - Move an hlist\n * @old: hlist_head for old list.\n * @new: hlist_head for new list.\n *\n * Move a list from one list head to another. Fixup the pprev\n * reference of the first entry if it exists.\n */\nstatic inline void hlist_move_list(struct hlist_head *old,\n\t\t\t\t   struct hlist_head *new)\n{\n\tnew->first = old->first;\n\tif (new->first)\n\t\tnew->first->pprev = &new->first;\n\told->first = NULL;\n}\n\n#define hlist_entry(ptr, type, member) container_of(ptr,type,member)\n\n#define hlist_for_each(pos, head) \\\n\tfor (pos = (head)->first; pos ; pos = pos->next)\n\n#define hlist_for_each_safe(pos, n, head) \\\n\tfor (pos = (head)->first; pos && ({ n = pos->next; 1; }); \\\n\t     pos = n)\n\n#define hlist_entry_safe(ptr, type, member) \\\n\t({ typeof(ptr) ____ptr = (ptr); \\\n\t   ____ptr ? hlist_entry(____ptr, type, member) : NULL; \\\n\t})\n\n/**\n * hlist_for_each_entry\t- iterate over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry(pos, head, member)\t\t\t\t\\\n\tfor (pos = hlist_entry_safe((head)->first, typeof(*(pos)), member);\\\n\t     pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_continue - iterate over a hlist continuing after current point\n * @pos:\tthe type * to use as a loop cursor.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_continue(pos, member)\t\t\t\\\n\tfor (pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member);\\\n\t     pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_from - iterate over a hlist continuing from current point\n * @pos:\tthe type * to use as a loop cursor.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_from(pos, member)\t\t\t\t\\\n\tfor (; pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_safe - iterate over list of given type safe against removal of list entry\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\ta &struct hlist_node to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_safe(pos, n, head, member) \t\t\\\n\tfor (pos = hlist_entry_safe((head)->first, typeof(*pos), member);\\\n\t     pos && ({ n = pos->member.next; 1; });\t\t\t\\\n\t     pos = hlist_entry_safe(n, typeof(*pos), member))\n\n#endif\n"}, "2": {"id": 2, "path": "/src/include/linux/blkdev.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_BLKDEV_H\n#define _LINUX_BLKDEV_H\n\n#include <linux/sched.h>\n#include <linux/sched/clock.h>\n#include <linux/major.h>\n#include <linux/genhd.h>\n#include <linux/list.h>\n#include <linux/llist.h>\n#include <linux/minmax.h>\n#include <linux/timer.h>\n#include <linux/workqueue.h>\n#include <linux/pagemap.h>\n#include <linux/backing-dev-defs.h>\n#include <linux/wait.h>\n#include <linux/mempool.h>\n#include <linux/pfn.h>\n#include <linux/bio.h>\n#include <linux/stringify.h>\n#include <linux/gfp.h>\n#include <linux/bsg.h>\n#include <linux/smp.h>\n#include <linux/rcupdate.h>\n#include <linux/percpu-refcount.h>\n#include <linux/scatterlist.h>\n#include <linux/blkzoned.h>\n#include <linux/pm.h>\n\nstruct module;\nstruct scsi_ioctl_command;\n\nstruct request_queue;\nstruct elevator_queue;\nstruct blk_trace;\nstruct request;\nstruct sg_io_hdr;\nstruct bsg_job;\nstruct blkcg_gq;\nstruct blk_flush_queue;\nstruct pr_ops;\nstruct rq_qos;\nstruct blk_queue_stats;\nstruct blk_stat_callback;\nstruct blk_keyslot_manager;\n\n#define BLKDEV_MIN_RQ\t4\n#define BLKDEV_MAX_RQ\t128\t/* Default maximum */\n\n/* Must be consistent with blk_mq_poll_stats_bkt() */\n#define BLK_MQ_POLL_STATS_BKTS 16\n\n/* Doing classic polling */\n#define BLK_MQ_POLL_CLASSIC -1\n\n/*\n * Maximum number of blkcg policies allowed to be registered concurrently.\n * Defined here to simplify include dependency.\n */\n#define BLKCG_MAX_POLS\t\t5\n\ntypedef void (rq_end_io_fn)(struct request *, blk_status_t);\n\n/*\n * request flags */\ntypedef __u32 __bitwise req_flags_t;\n\n/* elevator knows about this request */\n#define RQF_SORTED\t\t((__force req_flags_t)(1 << 0))\n/* drive already may have started this one */\n#define RQF_STARTED\t\t((__force req_flags_t)(1 << 1))\n/* may not be passed by ioscheduler */\n#define RQF_SOFTBARRIER\t\t((__force req_flags_t)(1 << 3))\n/* request for flush sequence */\n#define RQF_FLUSH_SEQ\t\t((__force req_flags_t)(1 << 4))\n/* merge of different types, fail separately */\n#define RQF_MIXED_MERGE\t\t((__force req_flags_t)(1 << 5))\n/* track inflight for MQ */\n#define RQF_MQ_INFLIGHT\t\t((__force req_flags_t)(1 << 6))\n/* don't call prep for this one */\n#define RQF_DONTPREP\t\t((__force req_flags_t)(1 << 7))\n/* set for \"ide_preempt\" requests and also for requests for which the SCSI\n   \"quiesce\" state must be ignored. */\n#define RQF_PREEMPT\t\t((__force req_flags_t)(1 << 8))\n/* vaguely specified driver internal error.  Ignored by the block layer */\n#define RQF_FAILED\t\t((__force req_flags_t)(1 << 10))\n/* don't warn about errors */\n#define RQF_QUIET\t\t((__force req_flags_t)(1 << 11))\n/* elevator private data attached */\n#define RQF_ELVPRIV\t\t((__force req_flags_t)(1 << 12))\n/* account into disk and partition IO statistics */\n#define RQF_IO_STAT\t\t((__force req_flags_t)(1 << 13))\n/* request came from our alloc pool */\n#define RQF_ALLOCED\t\t((__force req_flags_t)(1 << 14))\n/* runtime pm request */\n#define RQF_PM\t\t\t((__force req_flags_t)(1 << 15))\n/* on IO scheduler merge hash */\n#define RQF_HASHED\t\t((__force req_flags_t)(1 << 16))\n/* track IO completion time */\n#define RQF_STATS\t\t((__force req_flags_t)(1 << 17))\n/* Look at ->special_vec for the actual data payload instead of the\n   bio chain. */\n#define RQF_SPECIAL_PAYLOAD\t((__force req_flags_t)(1 << 18))\n/* The per-zone write lock is held for this request */\n#define RQF_ZONE_WRITE_LOCKED\t((__force req_flags_t)(1 << 19))\n/* already slept for hybrid poll */\n#define RQF_MQ_POLL_SLEPT\t((__force req_flags_t)(1 << 20))\n/* ->timeout has been called, don't expire again */\n#define RQF_TIMED_OUT\t\t((__force req_flags_t)(1 << 21))\n\n/* flags that prevent us from merging requests: */\n#define RQF_NOMERGE_FLAGS \\\n\t(RQF_STARTED | RQF_SOFTBARRIER | RQF_FLUSH_SEQ | RQF_SPECIAL_PAYLOAD)\n\n/*\n * Request state for blk-mq.\n */\nenum mq_rq_state {\n\tMQ_RQ_IDLE\t\t= 0,\n\tMQ_RQ_IN_FLIGHT\t\t= 1,\n\tMQ_RQ_COMPLETE\t\t= 2,\n};\n\n/*\n * Try to put the fields that are referenced together in the same cacheline.\n *\n * If you modify this structure, make sure to update blk_rq_init() and\n * especially blk_mq_rq_ctx_init() to take care of the added fields.\n */\nstruct request {\n\tstruct request_queue *q;\n\tstruct blk_mq_ctx *mq_ctx;\n\tstruct blk_mq_hw_ctx *mq_hctx;\n\n\tunsigned int cmd_flags;\t\t/* op and common flags */\n\treq_flags_t rq_flags;\n\n\tint tag;\n\tint internal_tag;\n\n\t/* the following two fields are internal, NEVER access directly */\n\tunsigned int __data_len;\t/* total data len */\n\tsector_t __sector;\t\t/* sector cursor */\n\n\tstruct bio *bio;\n\tstruct bio *biotail;\n\n\tstruct list_head queuelist;\n\n\t/*\n\t * The hash is used inside the scheduler, and killed once the\n\t * request reaches the dispatch list. The ipi_list is only used\n\t * to queue the request for softirq completion, which is long\n\t * after the request has been unhashed (and even removed from\n\t * the dispatch list).\n\t */\n\tunion {\n\t\tstruct hlist_node hash;\t/* merge hash */\n\t\tstruct list_head ipi_list;\n\t};\n\n\t/*\n\t * The rb_node is only used inside the io scheduler, requests\n\t * are pruned when moved to the dispatch queue. So let the\n\t * completion_data share space with the rb_node.\n\t */\n\tunion {\n\t\tstruct rb_node rb_node;\t/* sort/lookup */\n\t\tstruct bio_vec special_vec;\n\t\tvoid *completion_data;\n\t\tint error_count; /* for legacy drivers, don't use */\n\t};\n\n\t/*\n\t * Three pointers are available for the IO schedulers, if they need\n\t * more they have to dynamically allocate it.  Flush requests are\n\t * never put on the IO scheduler. So let the flush fields share\n\t * space with the elevator data.\n\t */\n\tunion {\n\t\tstruct {\n\t\t\tstruct io_cq\t\t*icq;\n\t\t\tvoid\t\t\t*priv[2];\n\t\t} elv;\n\n\t\tstruct {\n\t\t\tunsigned int\t\tseq;\n\t\t\tstruct list_head\tlist;\n\t\t\trq_end_io_fn\t\t*saved_end_io;\n\t\t} flush;\n\t};\n\n\tstruct gendisk *rq_disk;\n\tstruct hd_struct *part;\n#ifdef CONFIG_BLK_RQ_ALLOC_TIME\n\t/* Time that the first bio started allocating this request. */\n\tu64 alloc_time_ns;\n#endif\n\t/* Time that this request was allocated for this IO. */\n\tu64 start_time_ns;\n\t/* Time that I/O was submitted to the device. */\n\tu64 io_start_time_ns;\n\n#ifdef CONFIG_BLK_WBT\n\tunsigned short wbt_flags;\n#endif\n\t/*\n\t * rq sectors used for blk stats. It has the same value\n\t * with blk_rq_sectors(rq), except that it never be zeroed\n\t * by completion.\n\t */\n\tunsigned short stats_sectors;\n\n\t/*\n\t * Number of scatter-gather DMA addr+len pairs after\n\t * physical address coalescing is performed.\n\t */\n\tunsigned short nr_phys_segments;\n\n#if defined(CONFIG_BLK_DEV_INTEGRITY)\n\tunsigned short nr_integrity_segments;\n#endif\n\n#ifdef CONFIG_BLK_INLINE_ENCRYPTION\n\tstruct bio_crypt_ctx *crypt_ctx;\n\tstruct blk_ksm_keyslot *crypt_keyslot;\n#endif\n\n\tunsigned short write_hint;\n\tunsigned short ioprio;\n\n\tenum mq_rq_state state;\n\trefcount_t ref;\n\n\tunsigned int timeout;\n\tunsigned long deadline;\n\n\tunion {\n\t\tstruct __call_single_data csd;\n\t\tu64 fifo_time;\n\t};\n\n\t/*\n\t * completion callback.\n\t */\n\trq_end_io_fn *end_io;\n\tvoid *end_io_data;\n};\n\nstatic inline bool blk_op_is_scsi(unsigned int op)\n{\n\treturn op == REQ_OP_SCSI_IN || op == REQ_OP_SCSI_OUT;\n}\n\nstatic inline bool blk_op_is_private(unsigned int op)\n{\n\treturn op == REQ_OP_DRV_IN || op == REQ_OP_DRV_OUT;\n}\n\nstatic inline bool blk_rq_is_scsi(struct request *rq)\n{\n\treturn blk_op_is_scsi(req_op(rq));\n}\n\nstatic inline bool blk_rq_is_private(struct request *rq)\n{\n\treturn blk_op_is_private(req_op(rq));\n}\n\nstatic inline bool blk_rq_is_passthrough(struct request *rq)\n{\n\treturn blk_rq_is_scsi(rq) || blk_rq_is_private(rq);\n}\n\nstatic inline bool bio_is_passthrough(struct bio *bio)\n{\n\tunsigned op = bio_op(bio);\n\n\treturn blk_op_is_scsi(op) || blk_op_is_private(op);\n}\n\nstatic inline unsigned short req_get_ioprio(struct request *req)\n{\n\treturn req->ioprio;\n}\n\n#include <linux/elevator.h>\n\nstruct blk_queue_ctx;\n\nstruct bio_vec;\n\nenum blk_eh_timer_return {\n\tBLK_EH_DONE,\t\t/* drivers has completed the command */\n\tBLK_EH_RESET_TIMER,\t/* reset timer and try again */\n};\n\nenum blk_queue_state {\n\tQueue_down,\n\tQueue_up,\n};\n\n#define BLK_TAG_ALLOC_FIFO 0 /* allocate starting from 0 */\n#define BLK_TAG_ALLOC_RR 1 /* allocate starting from last allocated tag */\n\n#define BLK_SCSI_MAX_CMDS\t(256)\n#define BLK_SCSI_CMD_PER_LONG\t(BLK_SCSI_MAX_CMDS / (sizeof(long) * 8))\n\n/*\n * Zoned block device models (zoned limit).\n *\n * Note: This needs to be ordered from the least to the most severe\n * restrictions for the inheritance in blk_stack_limits() to work.\n */\nenum blk_zoned_model {\n\tBLK_ZONED_NONE = 0,\t/* Regular block device */\n\tBLK_ZONED_HA,\t\t/* Host-aware zoned block device */\n\tBLK_ZONED_HM,\t\t/* Host-managed zoned block device */\n};\n\nstruct queue_limits {\n\tunsigned long\t\tbounce_pfn;\n\tunsigned long\t\tseg_boundary_mask;\n\tunsigned long\t\tvirt_boundary_mask;\n\n\tunsigned int\t\tmax_hw_sectors;\n\tunsigned int\t\tmax_dev_sectors;\n\tunsigned int\t\tchunk_sectors;\n\tunsigned int\t\tmax_sectors;\n\tunsigned int\t\tmax_segment_size;\n\tunsigned int\t\tphysical_block_size;\n\tunsigned int\t\tlogical_block_size;\n\tunsigned int\t\talignment_offset;\n\tunsigned int\t\tio_min;\n\tunsigned int\t\tio_opt;\n\tunsigned int\t\tmax_discard_sectors;\n\tunsigned int\t\tmax_hw_discard_sectors;\n\tunsigned int\t\tmax_write_same_sectors;\n\tunsigned int\t\tmax_write_zeroes_sectors;\n\tunsigned int\t\tmax_zone_append_sectors;\n\tunsigned int\t\tdiscard_granularity;\n\tunsigned int\t\tdiscard_alignment;\n\n\tunsigned short\t\tmax_segments;\n\tunsigned short\t\tmax_integrity_segments;\n\tunsigned short\t\tmax_discard_segments;\n\n\tunsigned char\t\tmisaligned;\n\tunsigned char\t\tdiscard_misaligned;\n\tunsigned char\t\traid_partial_stripes_expensive;\n\tenum blk_zoned_model\tzoned;\n};\n\ntypedef int (*report_zones_cb)(struct blk_zone *zone, unsigned int idx,\n\t\t\t       void *data);\n\nvoid blk_queue_set_zoned(struct gendisk *disk, enum blk_zoned_model model);\n\n#ifdef CONFIG_BLK_DEV_ZONED\n\n#define BLK_ALL_ZONES  ((unsigned int)-1)\nint blkdev_report_zones(struct block_device *bdev, sector_t sector,\n\t\t\tunsigned int nr_zones, report_zones_cb cb, void *data);\nunsigned int blkdev_nr_zones(struct gendisk *disk);\nextern int blkdev_zone_mgmt(struct block_device *bdev, enum req_opf op,\n\t\t\t    sector_t sectors, sector_t nr_sectors,\n\t\t\t    gfp_t gfp_mask);\nint blk_revalidate_disk_zones(struct gendisk *disk,\n\t\t\t      void (*update_driver_data)(struct gendisk *disk));\n\nextern int blkdev_report_zones_ioctl(struct block_device *bdev, fmode_t mode,\n\t\t\t\t     unsigned int cmd, unsigned long arg);\nextern int blkdev_zone_mgmt_ioctl(struct block_device *bdev, fmode_t mode,\n\t\t\t\t  unsigned int cmd, unsigned long arg);\n\n#else /* CONFIG_BLK_DEV_ZONED */\n\nstatic inline unsigned int blkdev_nr_zones(struct gendisk *disk)\n{\n\treturn 0;\n}\n\nstatic inline int blkdev_report_zones_ioctl(struct block_device *bdev,\n\t\t\t\t\t    fmode_t mode, unsigned int cmd,\n\t\t\t\t\t    unsigned long arg)\n{\n\treturn -ENOTTY;\n}\n\nstatic inline int blkdev_zone_mgmt_ioctl(struct block_device *bdev,\n\t\t\t\t\t fmode_t mode, unsigned int cmd,\n\t\t\t\t\t unsigned long arg)\n{\n\treturn -ENOTTY;\n}\n\n#endif /* CONFIG_BLK_DEV_ZONED */\n\nstruct request_queue {\n\tstruct request\t\t*last_merge;\n\tstruct elevator_queue\t*elevator;\n\n\tstruct percpu_ref\tq_usage_counter;\n\n\tstruct blk_queue_stats\t*stats;\n\tstruct rq_qos\t\t*rq_qos;\n\n\tconst struct blk_mq_ops\t*mq_ops;\n\n\t/* sw queues */\n\tstruct blk_mq_ctx __percpu\t*queue_ctx;\n\n\tunsigned int\t\tqueue_depth;\n\n\t/* hw dispatch queues */\n\tstruct blk_mq_hw_ctx\t**queue_hw_ctx;\n\tunsigned int\t\tnr_hw_queues;\n\n\tstruct backing_dev_info\t*backing_dev_info;\n\n\t/*\n\t * The queue owner gets to use this for whatever they like.\n\t * ll_rw_blk doesn't touch it.\n\t */\n\tvoid\t\t\t*queuedata;\n\n\t/*\n\t * various queue flags, see QUEUE_* below\n\t */\n\tunsigned long\t\tqueue_flags;\n\t/*\n\t * Number of contexts that have called blk_set_pm_only(). If this\n\t * counter is above zero then only RQF_PM and RQF_PREEMPT requests are\n\t * processed.\n\t */\n\tatomic_t\t\tpm_only;\n\n\t/*\n\t * ida allocated id for this queue.  Used to index queues from\n\t * ioctx.\n\t */\n\tint\t\t\tid;\n\n\t/*\n\t * queue needs bounce pages for pages above this limit\n\t */\n\tgfp_t\t\t\tbounce_gfp;\n\n\tspinlock_t\t\tqueue_lock;\n\n\t/*\n\t * queue kobject\n\t */\n\tstruct kobject kobj;\n\n\t/*\n\t * mq queue kobject\n\t */\n\tstruct kobject *mq_kobj;\n\n#ifdef  CONFIG_BLK_DEV_INTEGRITY\n\tstruct blk_integrity integrity;\n#endif\t/* CONFIG_BLK_DEV_INTEGRITY */\n\n#ifdef CONFIG_PM\n\tstruct device\t\t*dev;\n\tenum rpm_status\t\trpm_status;\n\tunsigned int\t\tnr_pending;\n#endif\n\n\t/*\n\t * queue settings\n\t */\n\tunsigned long\t\tnr_requests;\t/* Max # of requests */\n\n\tunsigned int\t\tdma_pad_mask;\n\tunsigned int\t\tdma_alignment;\n\n#ifdef CONFIG_BLK_INLINE_ENCRYPTION\n\t/* Inline crypto capabilities */\n\tstruct blk_keyslot_manager *ksm;\n#endif\n\n\tunsigned int\t\trq_timeout;\n\tint\t\t\tpoll_nsec;\n\n\tstruct blk_stat_callback\t*poll_cb;\n\tstruct blk_rq_stat\tpoll_stat[BLK_MQ_POLL_STATS_BKTS];\n\n\tstruct timer_list\ttimeout;\n\tstruct work_struct\ttimeout_work;\n\n\tatomic_t\t\tnr_active_requests_shared_sbitmap;\n\n\tstruct list_head\ticq_list;\n#ifdef CONFIG_BLK_CGROUP\n\tDECLARE_BITMAP\t\t(blkcg_pols, BLKCG_MAX_POLS);\n\tstruct blkcg_gq\t\t*root_blkg;\n\tstruct list_head\tblkg_list;\n#endif\n\n\tstruct queue_limits\tlimits;\n\n\tunsigned int\t\trequired_elevator_features;\n\n#ifdef CONFIG_BLK_DEV_ZONED\n\t/*\n\t * Zoned block device information for request dispatch control.\n\t * nr_zones is the total number of zones of the device. This is always\n\t * 0 for regular block devices. conv_zones_bitmap is a bitmap of nr_zones\n\t * bits which indicates if a zone is conventional (bit set) or\n\t * sequential (bit clear). seq_zones_wlock is a bitmap of nr_zones\n\t * bits which indicates if a zone is write locked, that is, if a write\n\t * request targeting the zone was dispatched. All three fields are\n\t * initialized by the low level device driver (e.g. scsi/sd.c).\n\t * Stacking drivers (device mappers) may or may not initialize\n\t * these fields.\n\t *\n\t * Reads of this information must be protected with blk_queue_enter() /\n\t * blk_queue_exit(). Modifying this information is only allowed while\n\t * no requests are being processed. See also blk_mq_freeze_queue() and\n\t * blk_mq_unfreeze_queue().\n\t */\n\tunsigned int\t\tnr_zones;\n\tunsigned long\t\t*conv_zones_bitmap;\n\tunsigned long\t\t*seq_zones_wlock;\n\tunsigned int\t\tmax_open_zones;\n\tunsigned int\t\tmax_active_zones;\n#endif /* CONFIG_BLK_DEV_ZONED */\n\n\t/*\n\t * sg stuff\n\t */\n\tunsigned int\t\tsg_timeout;\n\tunsigned int\t\tsg_reserved_size;\n\tint\t\t\tnode;\n\tstruct mutex\t\tdebugfs_mutex;\n#ifdef CONFIG_BLK_DEV_IO_TRACE\n\tstruct blk_trace __rcu\t*blk_trace;\n#endif\n\t/*\n\t * for flush operations\n\t */\n\tstruct blk_flush_queue\t*fq;\n\n\tstruct list_head\trequeue_list;\n\tspinlock_t\t\trequeue_lock;\n\tstruct delayed_work\trequeue_work;\n\n\tstruct mutex\t\tsysfs_lock;\n\tstruct mutex\t\tsysfs_dir_lock;\n\n\t/*\n\t * for reusing dead hctx instance in case of updating\n\t * nr_hw_queues\n\t */\n\tstruct list_head\tunused_hctx_list;\n\tspinlock_t\t\tunused_hctx_lock;\n\n\tint\t\t\tmq_freeze_depth;\n\n#if defined(CONFIG_BLK_DEV_BSG)\n\tstruct bsg_class_device bsg_dev;\n#endif\n\n#ifdef CONFIG_BLK_DEV_THROTTLING\n\t/* Throttle data */\n\tstruct throtl_data *td;\n#endif\n\tstruct rcu_head\t\trcu_head;\n\twait_queue_head_t\tmq_freeze_wq;\n\t/*\n\t * Protect concurrent access to q_usage_counter by\n\t * percpu_ref_kill() and percpu_ref_reinit().\n\t */\n\tstruct mutex\t\tmq_freeze_lock;\n\n\tstruct blk_mq_tag_set\t*tag_set;\n\tstruct list_head\ttag_set_list;\n\tstruct bio_set\t\tbio_split;\n\n\tstruct dentry\t\t*debugfs_dir;\n\n#ifdef CONFIG_BLK_DEBUG_FS\n\tstruct dentry\t\t*sched_debugfs_dir;\n\tstruct dentry\t\t*rqos_debugfs_dir;\n#endif\n\n\tbool\t\t\tmq_sysfs_init_done;\n\n\tsize_t\t\t\tcmd_size;\n\n#define BLK_MAX_WRITE_HINTS\t5\n\tu64\t\t\twrite_hints[BLK_MAX_WRITE_HINTS];\n};\n\n/* Keep blk_queue_flag_name[] in sync with the definitions below */\n#define QUEUE_FLAG_STOPPED\t0\t/* queue is stopped */\n#define QUEUE_FLAG_DYING\t1\t/* queue being torn down */\n#define QUEUE_FLAG_NOMERGES     3\t/* disable merge attempts */\n#define QUEUE_FLAG_SAME_COMP\t4\t/* complete on same CPU-group */\n#define QUEUE_FLAG_FAIL_IO\t5\t/* fake timeout */\n#define QUEUE_FLAG_NONROT\t6\t/* non-rotational device (SSD) */\n#define QUEUE_FLAG_VIRT\t\tQUEUE_FLAG_NONROT /* paravirt device */\n#define QUEUE_FLAG_IO_STAT\t7\t/* do disk/partitions IO accounting */\n#define QUEUE_FLAG_DISCARD\t8\t/* supports DISCARD */\n#define QUEUE_FLAG_NOXMERGES\t9\t/* No extended merges */\n#define QUEUE_FLAG_ADD_RANDOM\t10\t/* Contributes to random pool */\n#define QUEUE_FLAG_SECERASE\t11\t/* supports secure erase */\n#define QUEUE_FLAG_SAME_FORCE\t12\t/* force complete on same CPU */\n#define QUEUE_FLAG_DEAD\t\t13\t/* queue tear-down finished */\n#define QUEUE_FLAG_INIT_DONE\t14\t/* queue is initialized */\n#define QUEUE_FLAG_STABLE_WRITES 15\t/* don't modify blks until WB is done */\n#define QUEUE_FLAG_POLL\t\t16\t/* IO polling enabled if set */\n#define QUEUE_FLAG_WC\t\t17\t/* Write back caching */\n#define QUEUE_FLAG_FUA\t\t18\t/* device supports FUA writes */\n#define QUEUE_FLAG_DAX\t\t19\t/* device supports DAX */\n#define QUEUE_FLAG_STATS\t20\t/* track IO start and completion times */\n#define QUEUE_FLAG_POLL_STATS\t21\t/* collecting stats for hybrid polling */\n#define QUEUE_FLAG_REGISTERED\t22\t/* queue has been registered to a disk */\n#define QUEUE_FLAG_SCSI_PASSTHROUGH 23\t/* queue supports SCSI commands */\n#define QUEUE_FLAG_QUIESCED\t24\t/* queue has been quiesced */\n#define QUEUE_FLAG_PCI_P2PDMA\t25\t/* device supports PCI p2p requests */\n#define QUEUE_FLAG_ZONE_RESETALL 26\t/* supports Zone Reset All */\n#define QUEUE_FLAG_RQ_ALLOC_TIME 27\t/* record rq->alloc_time_ns */\n#define QUEUE_FLAG_HCTX_ACTIVE\t28\t/* at least one blk-mq hctx is active */\n#define QUEUE_FLAG_NOWAIT       29\t/* device supports NOWAIT */\n\n#define QUEUE_FLAG_MQ_DEFAULT\t((1 << QUEUE_FLAG_IO_STAT) |\t\t\\\n\t\t\t\t (1 << QUEUE_FLAG_SAME_COMP) |\t\t\\\n\t\t\t\t (1 << QUEUE_FLAG_NOWAIT))\n\nvoid blk_queue_flag_set(unsigned int flag, struct request_queue *q);\nvoid blk_queue_flag_clear(unsigned int flag, struct request_queue *q);\nbool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q);\n\n#define blk_queue_stopped(q)\ttest_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)\n#define blk_queue_dying(q)\ttest_bit(QUEUE_FLAG_DYING, &(q)->queue_flags)\n#define blk_queue_dead(q)\ttest_bit(QUEUE_FLAG_DEAD, &(q)->queue_flags)\n#define blk_queue_init_done(q)\ttest_bit(QUEUE_FLAG_INIT_DONE, &(q)->queue_flags)\n#define blk_queue_nomerges(q)\ttest_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)\n#define blk_queue_noxmerges(q)\t\\\n\ttest_bit(QUEUE_FLAG_NOXMERGES, &(q)->queue_flags)\n#define blk_queue_nonrot(q)\ttest_bit(QUEUE_FLAG_NONROT, &(q)->queue_flags)\n#define blk_queue_stable_writes(q) \\\n\ttest_bit(QUEUE_FLAG_STABLE_WRITES, &(q)->queue_flags)\n#define blk_queue_io_stat(q)\ttest_bit(QUEUE_FLAG_IO_STAT, &(q)->queue_flags)\n#define blk_queue_add_random(q)\ttest_bit(QUEUE_FLAG_ADD_RANDOM, &(q)->queue_flags)\n#define blk_queue_discard(q)\ttest_bit(QUEUE_FLAG_DISCARD, &(q)->queue_flags)\n#define blk_queue_zone_resetall(q)\t\\\n\ttest_bit(QUEUE_FLAG_ZONE_RESETALL, &(q)->queue_flags)\n#define blk_queue_secure_erase(q) \\\n\t(test_bit(QUEUE_FLAG_SECERASE, &(q)->queue_flags))\n#define blk_queue_dax(q)\ttest_bit(QUEUE_FLAG_DAX, &(q)->queue_flags)\n#define blk_queue_scsi_passthrough(q)\t\\\n\ttest_bit(QUEUE_FLAG_SCSI_PASSTHROUGH, &(q)->queue_flags)\n#define blk_queue_pci_p2pdma(q)\t\\\n\ttest_bit(QUEUE_FLAG_PCI_P2PDMA, &(q)->queue_flags)\n#ifdef CONFIG_BLK_RQ_ALLOC_TIME\n#define blk_queue_rq_alloc_time(q)\t\\\n\ttest_bit(QUEUE_FLAG_RQ_ALLOC_TIME, &(q)->queue_flags)\n#else\n#define blk_queue_rq_alloc_time(q)\tfalse\n#endif\n\n#define blk_noretry_request(rq) \\\n\t((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \\\n\t\t\t     REQ_FAILFAST_DRIVER))\n#define blk_queue_quiesced(q)\ttest_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)\n#define blk_queue_pm_only(q)\tatomic_read(&(q)->pm_only)\n#define blk_queue_fua(q)\ttest_bit(QUEUE_FLAG_FUA, &(q)->queue_flags)\n#define blk_queue_registered(q)\ttest_bit(QUEUE_FLAG_REGISTERED, &(q)->queue_flags)\n#define blk_queue_nowait(q)\ttest_bit(QUEUE_FLAG_NOWAIT, &(q)->queue_flags)\n\nextern void blk_set_pm_only(struct request_queue *q);\nextern void blk_clear_pm_only(struct request_queue *q);\n\nstatic inline bool blk_account_rq(struct request *rq)\n{\n\treturn (rq->rq_flags & RQF_STARTED) && !blk_rq_is_passthrough(rq);\n}\n\n#define list_entry_rq(ptr)\tlist_entry((ptr), struct request, queuelist)\n\n#define rq_data_dir(rq)\t\t(op_is_write(req_op(rq)) ? WRITE : READ)\n\n#define rq_dma_dir(rq) \\\n\t(op_is_write(req_op(rq)) ? DMA_TO_DEVICE : DMA_FROM_DEVICE)\n\n#define dma_map_bvec(dev, bv, dir, attrs) \\\n\tdma_map_page_attrs(dev, (bv)->bv_page, (bv)->bv_offset, (bv)->bv_len, \\\n\t(dir), (attrs))\n\nstatic inline bool queue_is_mq(struct request_queue *q)\n{\n\treturn q->mq_ops;\n}\n\nstatic inline enum blk_zoned_model\nblk_queue_zoned_model(struct request_queue *q)\n{\n\tif (IS_ENABLED(CONFIG_BLK_DEV_ZONED))\n\t\treturn q->limits.zoned;\n\treturn BLK_ZONED_NONE;\n}\n\nstatic inline bool blk_queue_is_zoned(struct request_queue *q)\n{\n\tswitch (blk_queue_zoned_model(q)) {\n\tcase BLK_ZONED_HA:\n\tcase BLK_ZONED_HM:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic inline sector_t blk_queue_zone_sectors(struct request_queue *q)\n{\n\treturn blk_queue_is_zoned(q) ? q->limits.chunk_sectors : 0;\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic inline unsigned int blk_queue_nr_zones(struct request_queue *q)\n{\n\treturn blk_queue_is_zoned(q) ? q->nr_zones : 0;\n}\n\nstatic inline unsigned int blk_queue_zone_no(struct request_queue *q,\n\t\t\t\t\t     sector_t sector)\n{\n\tif (!blk_queue_is_zoned(q))\n\t\treturn 0;\n\treturn sector >> ilog2(q->limits.chunk_sectors);\n}\n\nstatic inline bool blk_queue_zone_is_seq(struct request_queue *q,\n\t\t\t\t\t sector_t sector)\n{\n\tif (!blk_queue_is_zoned(q))\n\t\treturn false;\n\tif (!q->conv_zones_bitmap)\n\t\treturn true;\n\treturn !test_bit(blk_queue_zone_no(q, sector), q->conv_zones_bitmap);\n}\n\nstatic inline void blk_queue_max_open_zones(struct request_queue *q,\n\t\tunsigned int max_open_zones)\n{\n\tq->max_open_zones = max_open_zones;\n}\n\nstatic inline unsigned int queue_max_open_zones(const struct request_queue *q)\n{\n\treturn q->max_open_zones;\n}\n\nstatic inline void blk_queue_max_active_zones(struct request_queue *q,\n\t\tunsigned int max_active_zones)\n{\n\tq->max_active_zones = max_active_zones;\n}\n\nstatic inline unsigned int queue_max_active_zones(const struct request_queue *q)\n{\n\treturn q->max_active_zones;\n}\n#else /* CONFIG_BLK_DEV_ZONED */\nstatic inline unsigned int blk_queue_nr_zones(struct request_queue *q)\n{\n\treturn 0;\n}\nstatic inline bool blk_queue_zone_is_seq(struct request_queue *q,\n\t\t\t\t\t sector_t sector)\n{\n\treturn false;\n}\nstatic inline unsigned int blk_queue_zone_no(struct request_queue *q,\n\t\t\t\t\t     sector_t sector)\n{\n\treturn 0;\n}\nstatic inline unsigned int queue_max_open_zones(const struct request_queue *q)\n{\n\treturn 0;\n}\nstatic inline unsigned int queue_max_active_zones(const struct request_queue *q)\n{\n\treturn 0;\n}\n#endif /* CONFIG_BLK_DEV_ZONED */\n\nstatic inline bool rq_is_sync(struct request *rq)\n{\n\treturn op_is_sync(rq->cmd_flags);\n}\n\nstatic inline bool rq_mergeable(struct request *rq)\n{\n\tif (blk_rq_is_passthrough(rq))\n\t\treturn false;\n\n\tif (req_op(rq) == REQ_OP_FLUSH)\n\t\treturn false;\n\n\tif (req_op(rq) == REQ_OP_WRITE_ZEROES)\n\t\treturn false;\n\n\tif (req_op(rq) == REQ_OP_ZONE_APPEND)\n\t\treturn false;\n\n\tif (rq->cmd_flags & REQ_NOMERGE_FLAGS)\n\t\treturn false;\n\tif (rq->rq_flags & RQF_NOMERGE_FLAGS)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic inline bool blk_write_same_mergeable(struct bio *a, struct bio *b)\n{\n\tif (bio_page(a) == bio_page(b) &&\n\t    bio_offset(a) == bio_offset(b))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline unsigned int blk_queue_depth(struct request_queue *q)\n{\n\tif (q->queue_depth)\n\t\treturn q->queue_depth;\n\n\treturn q->nr_requests;\n}\n\nextern unsigned long blk_max_low_pfn, blk_max_pfn;\n\n/*\n * standard bounce addresses:\n *\n * BLK_BOUNCE_HIGH\t: bounce all highmem pages\n * BLK_BOUNCE_ANY\t: don't bounce anything\n * BLK_BOUNCE_ISA\t: bounce pages above ISA DMA boundary\n */\n\n#if BITS_PER_LONG == 32\n#define BLK_BOUNCE_HIGH\t\t((u64)blk_max_low_pfn << PAGE_SHIFT)\n#else\n#define BLK_BOUNCE_HIGH\t\t-1ULL\n#endif\n#define BLK_BOUNCE_ANY\t\t(-1ULL)\n#define BLK_BOUNCE_ISA\t\t(DMA_BIT_MASK(24))\n\n/*\n * default timeout for SG_IO if none specified\n */\n#define BLK_DEFAULT_SG_TIMEOUT\t(60 * HZ)\n#define BLK_MIN_SG_TIMEOUT\t(7 * HZ)\n\nstruct rq_map_data {\n\tstruct page **pages;\n\tint page_order;\n\tint nr_entries;\n\tunsigned long offset;\n\tint null_mapped;\n\tint from_user;\n};\n\nstruct req_iterator {\n\tstruct bvec_iter iter;\n\tstruct bio *bio;\n};\n\n/* This should not be used directly - use rq_for_each_segment */\n#define for_each_bio(_bio)\t\t\\\n\tfor (; _bio; _bio = _bio->bi_next)\n#define __rq_for_each_bio(_bio, rq)\t\\\n\tif ((rq->bio))\t\t\t\\\n\t\tfor (_bio = (rq)->bio; _bio; _bio = _bio->bi_next)\n\n#define rq_for_each_segment(bvl, _rq, _iter)\t\t\t\\\n\t__rq_for_each_bio(_iter.bio, _rq)\t\t\t\\\n\t\tbio_for_each_segment(bvl, _iter.bio, _iter.iter)\n\n#define rq_for_each_bvec(bvl, _rq, _iter)\t\t\t\\\n\t__rq_for_each_bio(_iter.bio, _rq)\t\t\t\\\n\t\tbio_for_each_bvec(bvl, _iter.bio, _iter.iter)\n\n#define rq_iter_last(bvec, _iter)\t\t\t\t\\\n\t\t(_iter.bio->bi_next == NULL &&\t\t\t\\\n\t\t bio_iter_last(bvec, _iter.iter))\n\n#ifndef ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE\n# error\t\"You should define ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE for your platform\"\n#endif\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE\nextern void rq_flush_dcache_pages(struct request *rq);\n#else\nstatic inline void rq_flush_dcache_pages(struct request *rq)\n{\n}\n#endif\n\nextern int blk_register_queue(struct gendisk *disk);\nextern void blk_unregister_queue(struct gendisk *disk);\nblk_qc_t submit_bio_noacct(struct bio *bio);\nextern void blk_rq_init(struct request_queue *q, struct request *rq);\nextern void blk_put_request(struct request *);\nextern struct request *blk_get_request(struct request_queue *, unsigned int op,\n\t\t\t\t       blk_mq_req_flags_t flags);\nextern int blk_lld_busy(struct request_queue *q);\nextern int blk_rq_prep_clone(struct request *rq, struct request *rq_src,\n\t\t\t     struct bio_set *bs, gfp_t gfp_mask,\n\t\t\t     int (*bio_ctr)(struct bio *, struct bio *, void *),\n\t\t\t     void *data);\nextern void blk_rq_unprep_clone(struct request *rq);\nextern blk_status_t blk_insert_cloned_request(struct request_queue *q,\n\t\t\t\t     struct request *rq);\nextern int blk_rq_append_bio(struct request *rq, struct bio **bio);\nextern void blk_queue_split(struct bio **);\nextern int scsi_verify_blk_ioctl(struct block_device *, unsigned int);\nextern int scsi_cmd_blk_ioctl(struct block_device *, fmode_t,\n\t\t\t      unsigned int, void __user *);\nextern int scsi_cmd_ioctl(struct request_queue *, struct gendisk *, fmode_t,\n\t\t\t  unsigned int, void __user *);\nextern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,\n\t\t\t struct scsi_ioctl_command __user *);\nextern int get_sg_io_hdr(struct sg_io_hdr *hdr, const void __user *argp);\nextern int put_sg_io_hdr(const struct sg_io_hdr *hdr, void __user *argp);\n\nextern int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags);\nextern void blk_queue_exit(struct request_queue *q);\nextern void blk_sync_queue(struct request_queue *q);\nextern int blk_rq_map_user(struct request_queue *, struct request *,\n\t\t\t   struct rq_map_data *, void __user *, unsigned long,\n\t\t\t   gfp_t);\nextern int blk_rq_unmap_user(struct bio *);\nextern int blk_rq_map_kern(struct request_queue *, struct request *, void *, unsigned int, gfp_t);\nextern int blk_rq_map_user_iov(struct request_queue *, struct request *,\n\t\t\t       struct rq_map_data *, const struct iov_iter *,\n\t\t\t       gfp_t);\nextern void blk_execute_rq(struct request_queue *, struct gendisk *,\n\t\t\t  struct request *, int);\nextern void blk_execute_rq_nowait(struct request_queue *, struct gendisk *,\n\t\t\t\t  struct request *, int, rq_end_io_fn *);\n\n/* Helper to convert REQ_OP_XXX to its string format XXX */\nextern const char *blk_op_str(unsigned int op);\n\nint blk_status_to_errno(blk_status_t status);\nblk_status_t errno_to_blk_status(int errno);\n\nint blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin);\n\nstatic inline struct request_queue *bdev_get_queue(struct block_device *bdev)\n{\n\treturn bdev->bd_disk->queue;\t/* this is never NULL */\n}\n\n/*\n * The basic unit of block I/O is a sector. It is used in a number of contexts\n * in Linux (blk, bio, genhd). The size of one sector is 512 = 2**9\n * bytes. Variables of type sector_t represent an offset or size that is a\n * multiple of 512 bytes. Hence these two constants.\n */\n#ifndef SECTOR_SHIFT\n#define SECTOR_SHIFT 9\n#endif\n#ifndef SECTOR_SIZE\n#define SECTOR_SIZE (1 << SECTOR_SHIFT)\n#endif\n\n/*\n * blk_rq_pos()\t\t\t: the current sector\n * blk_rq_bytes()\t\t: bytes left in the entire request\n * blk_rq_cur_bytes()\t\t: bytes left in the current segment\n * blk_rq_err_bytes()\t\t: bytes left till the next error boundary\n * blk_rq_sectors()\t\t: sectors left in the entire request\n * blk_rq_cur_sectors()\t\t: sectors left in the current segment\n * blk_rq_stats_sectors()\t: sectors of the entire request used for stats\n */\nstatic inline sector_t blk_rq_pos(const struct request *rq)\n{\n\treturn rq->__sector;\n}\n\nstatic inline unsigned int blk_rq_bytes(const struct request *rq)\n{\n\treturn rq->__data_len;\n}\n\nstatic inline int blk_rq_cur_bytes(const struct request *rq)\n{\n\treturn rq->bio ? bio_cur_bytes(rq->bio) : 0;\n}\n\nextern unsigned int blk_rq_err_bytes(const struct request *rq);\n\nstatic inline unsigned int blk_rq_sectors(const struct request *rq)\n{\n\treturn blk_rq_bytes(rq) >> SECTOR_SHIFT;\n}\n\nstatic inline unsigned int blk_rq_cur_sectors(const struct request *rq)\n{\n\treturn blk_rq_cur_bytes(rq) >> SECTOR_SHIFT;\n}\n\nstatic inline unsigned int blk_rq_stats_sectors(const struct request *rq)\n{\n\treturn rq->stats_sectors;\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\n\n/* Helper to convert BLK_ZONE_ZONE_XXX to its string format XXX */\nconst char *blk_zone_cond_str(enum blk_zone_cond zone_cond);\n\nstatic inline unsigned int blk_rq_zone_no(struct request *rq)\n{\n\treturn blk_queue_zone_no(rq->q, blk_rq_pos(rq));\n}\n\nstatic inline unsigned int blk_rq_zone_is_seq(struct request *rq)\n{\n\treturn blk_queue_zone_is_seq(rq->q, blk_rq_pos(rq));\n}\n#endif /* CONFIG_BLK_DEV_ZONED */\n\n/*\n * Some commands like WRITE SAME have a payload or data transfer size which\n * is different from the size of the request.  Any driver that supports such\n * commands using the RQF_SPECIAL_PAYLOAD flag needs to use this helper to\n * calculate the data transfer size.\n */\nstatic inline unsigned int blk_rq_payload_bytes(struct request *rq)\n{\n\tif (rq->rq_flags & RQF_SPECIAL_PAYLOAD)\n\t\treturn rq->special_vec.bv_len;\n\treturn blk_rq_bytes(rq);\n}\n\n/*\n * Return the first full biovec in the request.  The caller needs to check that\n * there are any bvecs before calling this helper.\n */\nstatic inline struct bio_vec req_bvec(struct request *rq)\n{\n\tif (rq->rq_flags & RQF_SPECIAL_PAYLOAD)\n\t\treturn rq->special_vec;\n\treturn mp_bvec_iter_bvec(rq->bio->bi_io_vec, rq->bio->bi_iter);\n}\n\nstatic inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,\n\t\t\t\t\t\t     int op)\n{\n\tif (unlikely(op == REQ_OP_DISCARD || op == REQ_OP_SECURE_ERASE))\n\t\treturn min(q->limits.max_discard_sectors,\n\t\t\t   UINT_MAX >> SECTOR_SHIFT);\n\n\tif (unlikely(op == REQ_OP_WRITE_SAME))\n\t\treturn q->limits.max_write_same_sectors;\n\n\tif (unlikely(op == REQ_OP_WRITE_ZEROES))\n\t\treturn q->limits.max_write_zeroes_sectors;\n\n\treturn q->limits.max_sectors;\n}\n\n/*\n * Return maximum size of a request at given offset. Only valid for\n * file system requests.\n */\nstatic inline unsigned int blk_max_size_offset(struct request_queue *q,\n\t\t\t\t\t       sector_t offset)\n{\n\tunsigned int chunk_sectors = q->limits.chunk_sectors;\n\n\tif (!chunk_sectors)\n\t\treturn q->limits.max_sectors;\n\n\tif (likely(is_power_of_2(chunk_sectors)))\n\t\tchunk_sectors -= offset & (chunk_sectors - 1);\n\telse\n\t\tchunk_sectors -= sector_div(offset, chunk_sectors);\n\n\treturn min(q->limits.max_sectors, chunk_sectors);\n}\n\nstatic inline unsigned int blk_rq_get_max_sectors(struct request *rq,\n\t\t\t\t\t\t  sector_t offset)\n{\n\tstruct request_queue *q = rq->q;\n\n\tif (blk_rq_is_passthrough(rq))\n\t\treturn q->limits.max_hw_sectors;\n\n\tif (!q->limits.chunk_sectors ||\n\t    req_op(rq) == REQ_OP_DISCARD ||\n\t    req_op(rq) == REQ_OP_SECURE_ERASE)\n\t\treturn blk_queue_get_max_sectors(q, req_op(rq));\n\n\treturn min(blk_max_size_offset(q, offset),\n\t\t\tblk_queue_get_max_sectors(q, req_op(rq)));\n}\n\nstatic inline unsigned int blk_rq_count_bios(struct request *rq)\n{\n\tunsigned int nr_bios = 0;\n\tstruct bio *bio;\n\n\t__rq_for_each_bio(bio, rq)\n\t\tnr_bios++;\n\n\treturn nr_bios;\n}\n\nvoid blk_steal_bios(struct bio_list *list, struct request *rq);\n\n/*\n * Request completion related functions.\n *\n * blk_update_request() completes given number of bytes and updates\n * the request without completing it.\n */\nextern bool blk_update_request(struct request *rq, blk_status_t error,\n\t\t\t       unsigned int nr_bytes);\n\nextern void blk_abort_request(struct request *);\n\n/*\n * Access functions for manipulating queue properties\n */\nextern void blk_cleanup_queue(struct request_queue *);\nextern void blk_queue_bounce_limit(struct request_queue *, u64);\nextern void blk_queue_max_hw_sectors(struct request_queue *, unsigned int);\nextern void blk_queue_chunk_sectors(struct request_queue *, unsigned int);\nextern void blk_queue_max_segments(struct request_queue *, unsigned short);\nextern void blk_queue_max_discard_segments(struct request_queue *,\n\t\tunsigned short);\nextern void blk_queue_max_segment_size(struct request_queue *, unsigned int);\nextern void blk_queue_max_discard_sectors(struct request_queue *q,\n\t\tunsigned int max_discard_sectors);\nextern void blk_queue_max_write_same_sectors(struct request_queue *q,\n\t\tunsigned int max_write_same_sectors);\nextern void blk_queue_max_write_zeroes_sectors(struct request_queue *q,\n\t\tunsigned int max_write_same_sectors);\nextern void blk_queue_logical_block_size(struct request_queue *, unsigned int);\nextern void blk_queue_max_zone_append_sectors(struct request_queue *q,\n\t\tunsigned int max_zone_append_sectors);\nextern void blk_queue_physical_block_size(struct request_queue *, unsigned int);\nextern void blk_queue_alignment_offset(struct request_queue *q,\n\t\t\t\t       unsigned int alignment);\nvoid blk_queue_update_readahead(struct request_queue *q);\nextern void blk_limits_io_min(struct queue_limits *limits, unsigned int min);\nextern void blk_queue_io_min(struct request_queue *q, unsigned int min);\nextern void blk_limits_io_opt(struct queue_limits *limits, unsigned int opt);\nextern void blk_queue_io_opt(struct request_queue *q, unsigned int opt);\nextern void blk_set_queue_depth(struct request_queue *q, unsigned int depth);\nextern void blk_set_default_limits(struct queue_limits *lim);\nextern void blk_set_stacking_limits(struct queue_limits *lim);\nextern int blk_stack_limits(struct queue_limits *t, struct queue_limits *b,\n\t\t\t    sector_t offset);\nextern void disk_stack_limits(struct gendisk *disk, struct block_device *bdev,\n\t\t\t      sector_t offset);\nextern void blk_queue_update_dma_pad(struct request_queue *, unsigned int);\nextern void blk_queue_segment_boundary(struct request_queue *, unsigned long);\nextern void blk_queue_virt_boundary(struct request_queue *, unsigned long);\nextern void blk_queue_dma_alignment(struct request_queue *, int);\nextern void blk_queue_update_dma_alignment(struct request_queue *, int);\nextern void blk_queue_rq_timeout(struct request_queue *, unsigned int);\nextern void blk_queue_write_cache(struct request_queue *q, bool enabled, bool fua);\nextern void blk_queue_required_elevator_features(struct request_queue *q,\n\t\t\t\t\t\t unsigned int features);\nextern bool blk_queue_can_use_dma_map_merging(struct request_queue *q,\n\t\t\t\t\t      struct device *dev);\n\n/*\n * Number of physical segments as sent to the device.\n *\n * Normally this is the number of discontiguous data segments sent by the\n * submitter.  But for data-less command like discard we might have no\n * actual data segments submitted, but the driver might have to add it's\n * own special payload.  In that case we still return 1 here so that this\n * special payload will be mapped.\n */\nstatic inline unsigned short blk_rq_nr_phys_segments(struct request *rq)\n{\n\tif (rq->rq_flags & RQF_SPECIAL_PAYLOAD)\n\t\treturn 1;\n\treturn rq->nr_phys_segments;\n}\n\n/*\n * Number of discard segments (or ranges) the driver needs to fill in.\n * Each discard bio merged into a request is counted as one segment.\n */\nstatic inline unsigned short blk_rq_nr_discard_segments(struct request *rq)\n{\n\treturn max_t(unsigned short, rq->nr_phys_segments, 1);\n}\n\nint __blk_rq_map_sg(struct request_queue *q, struct request *rq,\n\t\tstruct scatterlist *sglist, struct scatterlist **last_sg);\nstatic inline int blk_rq_map_sg(struct request_queue *q, struct request *rq,\n\t\tstruct scatterlist *sglist)\n{\n\tstruct scatterlist *last_sg = NULL;\n\n\treturn __blk_rq_map_sg(q, rq, sglist, &last_sg);\n}\nextern void blk_dump_rq_flags(struct request *, char *);\n\nbool __must_check blk_get_queue(struct request_queue *);\nstruct request_queue *blk_alloc_queue(int node_id);\nextern void blk_put_queue(struct request_queue *);\nextern void blk_set_queue_dying(struct request_queue *);\n\n#ifdef CONFIG_BLOCK\n/*\n * blk_plug permits building a queue of related requests by holding the I/O\n * fragments for a short period. This allows merging of sequential requests\n * into single larger request. As the requests are moved from a per-task list to\n * the device's request_queue in a batch, this results in improved scalability\n * as the lock contention for request_queue lock is reduced.\n *\n * It is ok not to disable preemption when adding the request to the plug list\n * or when attempting a merge, because blk_schedule_flush_list() will only flush\n * the plug list when the task sleeps by itself. For details, please see\n * schedule() where blk_schedule_flush_plug() is called.\n */\nstruct blk_plug {\n\tstruct list_head mq_list; /* blk-mq requests */\n\tstruct list_head cb_list; /* md requires an unplug callback */\n\tunsigned short rq_count;\n\tbool multiple_queues;\n\tbool nowait;\n};\n#define BLK_MAX_REQUEST_COUNT 16\n#define BLK_PLUG_FLUSH_SIZE (128 * 1024)\n\nstruct blk_plug_cb;\ntypedef void (*blk_plug_cb_fn)(struct blk_plug_cb *, bool);\nstruct blk_plug_cb {\n\tstruct list_head list;\n\tblk_plug_cb_fn callback;\n\tvoid *data;\n};\nextern struct blk_plug_cb *blk_check_plugged(blk_plug_cb_fn unplug,\n\t\t\t\t\t     void *data, int size);\nextern void blk_start_plug(struct blk_plug *);\nextern void blk_finish_plug(struct blk_plug *);\nextern void blk_flush_plug_list(struct blk_plug *, bool);\n\nstatic inline void blk_flush_plug(struct task_struct *tsk)\n{\n\tstruct blk_plug *plug = tsk->plug;\n\n\tif (plug)\n\t\tblk_flush_plug_list(plug, false);\n}\n\nstatic inline void blk_schedule_flush_plug(struct task_struct *tsk)\n{\n\tstruct blk_plug *plug = tsk->plug;\n\n\tif (plug)\n\t\tblk_flush_plug_list(plug, true);\n}\n\nstatic inline bool blk_needs_flush_plug(struct task_struct *tsk)\n{\n\tstruct blk_plug *plug = tsk->plug;\n\n\treturn plug &&\n\t\t (!list_empty(&plug->mq_list) ||\n\t\t !list_empty(&plug->cb_list));\n}\n\nint blkdev_issue_flush(struct block_device *, gfp_t);\nlong nr_blockdev_pages(void);\n#else /* CONFIG_BLOCK */\nstruct blk_plug {\n};\n\nstatic inline void blk_start_plug(struct blk_plug *plug)\n{\n}\n\nstatic inline void blk_finish_plug(struct blk_plug *plug)\n{\n}\n\nstatic inline void blk_flush_plug(struct task_struct *task)\n{\n}\n\nstatic inline void blk_schedule_flush_plug(struct task_struct *task)\n{\n}\n\n\nstatic inline bool blk_needs_flush_plug(struct task_struct *tsk)\n{\n\treturn false;\n}\n\nstatic inline int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask)\n{\n\treturn 0;\n}\n\nstatic inline long nr_blockdev_pages(void)\n{\n\treturn 0;\n}\n#endif /* CONFIG_BLOCK */\n\nextern void blk_io_schedule(void);\n\nextern int blkdev_issue_write_same(struct block_device *bdev, sector_t sector,\n\t\tsector_t nr_sects, gfp_t gfp_mask, struct page *page);\n\n#define BLKDEV_DISCARD_SECURE\t(1 << 0)\t/* issue a secure erase */\n\nextern int blkdev_issue_discard(struct block_device *bdev, sector_t sector,\n\t\tsector_t nr_sects, gfp_t gfp_mask, unsigned long flags);\nextern int __blkdev_issue_discard(struct block_device *bdev, sector_t sector,\n\t\tsector_t nr_sects, gfp_t gfp_mask, int flags,\n\t\tstruct bio **biop);\n\n#define BLKDEV_ZERO_NOUNMAP\t(1 << 0)  /* do not free blocks */\n#define BLKDEV_ZERO_NOFALLBACK\t(1 << 1)  /* don't write explicit zeroes */\n\nextern int __blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,\n\t\tsector_t nr_sects, gfp_t gfp_mask, struct bio **biop,\n\t\tunsigned flags);\nextern int blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,\n\t\tsector_t nr_sects, gfp_t gfp_mask, unsigned flags);\n\nstatic inline int sb_issue_discard(struct super_block *sb, sector_t block,\n\t\tsector_t nr_blocks, gfp_t gfp_mask, unsigned long flags)\n{\n\treturn blkdev_issue_discard(sb->s_bdev,\n\t\t\t\t    block << (sb->s_blocksize_bits -\n\t\t\t\t\t      SECTOR_SHIFT),\n\t\t\t\t    nr_blocks << (sb->s_blocksize_bits -\n\t\t\t\t\t\t  SECTOR_SHIFT),\n\t\t\t\t    gfp_mask, flags);\n}\nstatic inline int sb_issue_zeroout(struct super_block *sb, sector_t block,\n\t\tsector_t nr_blocks, gfp_t gfp_mask)\n{\n\treturn blkdev_issue_zeroout(sb->s_bdev,\n\t\t\t\t    block << (sb->s_blocksize_bits -\n\t\t\t\t\t      SECTOR_SHIFT),\n\t\t\t\t    nr_blocks << (sb->s_blocksize_bits -\n\t\t\t\t\t\t  SECTOR_SHIFT),\n\t\t\t\t    gfp_mask, 0);\n}\n\nextern int blk_verify_command(unsigned char *cmd, fmode_t mode);\n\nstatic inline bool bdev_is_partition(struct block_device *bdev)\n{\n\treturn bdev->bd_partno;\n}\n\nenum blk_default_limits {\n\tBLK_MAX_SEGMENTS\t= 128,\n\tBLK_SAFE_MAX_SECTORS\t= 255,\n\tBLK_DEF_MAX_SECTORS\t= 2560,\n\tBLK_MAX_SEGMENT_SIZE\t= 65536,\n\tBLK_SEG_BOUNDARY_MASK\t= 0xFFFFFFFFUL,\n};\n\nstatic inline unsigned long queue_segment_boundary(const struct request_queue *q)\n{\n\treturn q->limits.seg_boundary_mask;\n}\n\nstatic inline unsigned long queue_virt_boundary(const struct request_queue *q)\n{\n\treturn q->limits.virt_boundary_mask;\n}\n\nstatic inline unsigned int queue_max_sectors(const struct request_queue *q)\n{\n\treturn q->limits.max_sectors;\n}\n\nstatic inline unsigned int queue_max_hw_sectors(const struct request_queue *q)\n{\n\treturn q->limits.max_hw_sectors;\n}\n\nstatic inline unsigned short queue_max_segments(const struct request_queue *q)\n{\n\treturn q->limits.max_segments;\n}\n\nstatic inline unsigned short queue_max_discard_segments(const struct request_queue *q)\n{\n\treturn q->limits.max_discard_segments;\n}\n\nstatic inline unsigned int queue_max_segment_size(const struct request_queue *q)\n{\n\treturn q->limits.max_segment_size;\n}\n\nstatic inline unsigned int queue_max_zone_append_sectors(const struct request_queue *q)\n{\n\n\tconst struct queue_limits *l = &q->limits;\n\n\treturn min(l->max_zone_append_sectors, l->max_sectors);\n}\n\nstatic inline unsigned queue_logical_block_size(const struct request_queue *q)\n{\n\tint retval = 512;\n\n\tif (q && q->limits.logical_block_size)\n\t\tretval = q->limits.logical_block_size;\n\n\treturn retval;\n}\n\nstatic inline unsigned int bdev_logical_block_size(struct block_device *bdev)\n{\n\treturn queue_logical_block_size(bdev_get_queue(bdev));\n}\n\nstatic inline unsigned int queue_physical_block_size(const struct request_queue *q)\n{\n\treturn q->limits.physical_block_size;\n}\n\nstatic inline unsigned int bdev_physical_block_size(struct block_device *bdev)\n{\n\treturn queue_physical_block_size(bdev_get_queue(bdev));\n}\n\nstatic inline unsigned int queue_io_min(const struct request_queue *q)\n{\n\treturn q->limits.io_min;\n}\n\nstatic inline int bdev_io_min(struct block_device *bdev)\n{\n\treturn queue_io_min(bdev_get_queue(bdev));\n}\n\nstatic inline unsigned int queue_io_opt(const struct request_queue *q)\n{\n\treturn q->limits.io_opt;\n}\n\nstatic inline int bdev_io_opt(struct block_device *bdev)\n{\n\treturn queue_io_opt(bdev_get_queue(bdev));\n}\n\nstatic inline int queue_alignment_offset(const struct request_queue *q)\n{\n\tif (q->limits.misaligned)\n\t\treturn -1;\n\n\treturn q->limits.alignment_offset;\n}\n\nstatic inline int queue_limit_alignment_offset(struct queue_limits *lim, sector_t sector)\n{\n\tunsigned int granularity = max(lim->physical_block_size, lim->io_min);\n\tunsigned int alignment = sector_div(sector, granularity >> SECTOR_SHIFT)\n\t\t<< SECTOR_SHIFT;\n\n\treturn (granularity + lim->alignment_offset - alignment) % granularity;\n}\n\nstatic inline int bdev_alignment_offset(struct block_device *bdev)\n{\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\n\tif (q->limits.misaligned)\n\t\treturn -1;\n\tif (bdev_is_partition(bdev))\n\t\treturn queue_limit_alignment_offset(&q->limits,\n\t\t\t\tbdev->bd_part->start_sect);\n\treturn q->limits.alignment_offset;\n}\n\nstatic inline int queue_discard_alignment(const struct request_queue *q)\n{\n\tif (q->limits.discard_misaligned)\n\t\treturn -1;\n\n\treturn q->limits.discard_alignment;\n}\n\nstatic inline int queue_limit_discard_alignment(struct queue_limits *lim, sector_t sector)\n{\n\tunsigned int alignment, granularity, offset;\n\n\tif (!lim->max_discard_sectors)\n\t\treturn 0;\n\n\t/* Why are these in bytes, not sectors? */\n\talignment = lim->discard_alignment >> SECTOR_SHIFT;\n\tgranularity = lim->discard_granularity >> SECTOR_SHIFT;\n\tif (!granularity)\n\t\treturn 0;\n\n\t/* Offset of the partition start in 'granularity' sectors */\n\toffset = sector_div(sector, granularity);\n\n\t/* And why do we do this modulus *again* in blkdev_issue_discard()? */\n\toffset = (granularity + alignment - offset) % granularity;\n\n\t/* Turn it back into bytes, gaah */\n\treturn offset << SECTOR_SHIFT;\n}\n\nstatic inline int bdev_discard_alignment(struct block_device *bdev)\n{\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\n\tif (bdev_is_partition(bdev))\n\t\treturn queue_limit_discard_alignment(&q->limits,\n\t\t\t\tbdev->bd_part->start_sect);\n\treturn q->limits.discard_alignment;\n}\n\nstatic inline unsigned int bdev_write_same(struct block_device *bdev)\n{\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\n\tif (q)\n\t\treturn q->limits.max_write_same_sectors;\n\n\treturn 0;\n}\n\nstatic inline unsigned int bdev_write_zeroes_sectors(struct block_device *bdev)\n{\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\n\tif (q)\n\t\treturn q->limits.max_write_zeroes_sectors;\n\n\treturn 0;\n}\n\nstatic inline enum blk_zoned_model bdev_zoned_model(struct block_device *bdev)\n{\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\n\tif (q)\n\t\treturn blk_queue_zoned_model(q);\n\n\treturn BLK_ZONED_NONE;\n}\n\nstatic inline bool bdev_is_zoned(struct block_device *bdev)\n{\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\n\tif (q)\n\t\treturn blk_queue_is_zoned(q);\n\n\treturn false;\n}\n\nstatic inline sector_t bdev_zone_sectors(struct block_device *bdev)\n{\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\n\tif (q)\n\t\treturn blk_queue_zone_sectors(q);\n\treturn 0;\n}\n\nstatic inline unsigned int bdev_max_open_zones(struct block_device *bdev)\n{\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\n\tif (q)\n\t\treturn queue_max_open_zones(q);\n\treturn 0;\n}\n\nstatic inline unsigned int bdev_max_active_zones(struct block_device *bdev)\n{\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\n\tif (q)\n\t\treturn queue_max_active_zones(q);\n\treturn 0;\n}\n\nstatic inline int queue_dma_alignment(const struct request_queue *q)\n{\n\treturn q ? q->dma_alignment : 511;\n}\n\nstatic inline int blk_rq_aligned(struct request_queue *q, unsigned long addr,\n\t\t\t\t unsigned int len)\n{\n\tunsigned int alignment = queue_dma_alignment(q) | q->dma_pad_mask;\n\treturn !(addr & alignment) && !(len & alignment);\n}\n\n/* assumes size > 256 */\nstatic inline unsigned int blksize_bits(unsigned int size)\n{\n\tunsigned int bits = 8;\n\tdo {\n\t\tbits++;\n\t\tsize >>= 1;\n\t} while (size > 256);\n\treturn bits;\n}\n\nstatic inline unsigned int block_size(struct block_device *bdev)\n{\n\treturn 1 << bdev->bd_inode->i_blkbits;\n}\n\nint kblockd_schedule_work(struct work_struct *work);\nint kblockd_mod_delayed_work_on(int cpu, struct delayed_work *dwork, unsigned long delay);\n\n#define MODULE_ALIAS_BLOCKDEV(major,minor) \\\n\tMODULE_ALIAS(\"block-major-\" __stringify(major) \"-\" __stringify(minor))\n#define MODULE_ALIAS_BLOCKDEV_MAJOR(major) \\\n\tMODULE_ALIAS(\"block-major-\" __stringify(major) \"-*\")\n\n#if defined(CONFIG_BLK_DEV_INTEGRITY)\n\nenum blk_integrity_flags {\n\tBLK_INTEGRITY_VERIFY\t\t= 1 << 0,\n\tBLK_INTEGRITY_GENERATE\t\t= 1 << 1,\n\tBLK_INTEGRITY_DEVICE_CAPABLE\t= 1 << 2,\n\tBLK_INTEGRITY_IP_CHECKSUM\t= 1 << 3,\n};\n\nstruct blk_integrity_iter {\n\tvoid\t\t\t*prot_buf;\n\tvoid\t\t\t*data_buf;\n\tsector_t\t\tseed;\n\tunsigned int\t\tdata_size;\n\tunsigned short\t\tinterval;\n\tconst char\t\t*disk_name;\n};\n\ntypedef blk_status_t (integrity_processing_fn) (struct blk_integrity_iter *);\ntypedef void (integrity_prepare_fn) (struct request *);\ntypedef void (integrity_complete_fn) (struct request *, unsigned int);\n\nstruct blk_integrity_profile {\n\tintegrity_processing_fn\t\t*generate_fn;\n\tintegrity_processing_fn\t\t*verify_fn;\n\tintegrity_prepare_fn\t\t*prepare_fn;\n\tintegrity_complete_fn\t\t*complete_fn;\n\tconst char\t\t\t*name;\n};\n\nextern void blk_integrity_register(struct gendisk *, struct blk_integrity *);\nextern void blk_integrity_unregister(struct gendisk *);\nextern int blk_integrity_compare(struct gendisk *, struct gendisk *);\nextern int blk_rq_map_integrity_sg(struct request_queue *, struct bio *,\n\t\t\t\t   struct scatterlist *);\nextern int blk_rq_count_integrity_sg(struct request_queue *, struct bio *);\n\nstatic inline struct blk_integrity *blk_get_integrity(struct gendisk *disk)\n{\n\tstruct blk_integrity *bi = &disk->queue->integrity;\n\n\tif (!bi->profile)\n\t\treturn NULL;\n\n\treturn bi;\n}\n\nstatic inline\nstruct blk_integrity *bdev_get_integrity(struct block_device *bdev)\n{\n\treturn blk_get_integrity(bdev->bd_disk);\n}\n\nstatic inline bool\nblk_integrity_queue_supports_integrity(struct request_queue *q)\n{\n\treturn q->integrity.profile;\n}\n\nstatic inline bool blk_integrity_rq(struct request *rq)\n{\n\treturn rq->cmd_flags & REQ_INTEGRITY;\n}\n\nstatic inline void blk_queue_max_integrity_segments(struct request_queue *q,\n\t\t\t\t\t\t    unsigned int segs)\n{\n\tq->limits.max_integrity_segments = segs;\n}\n\nstatic inline unsigned short\nqueue_max_integrity_segments(const struct request_queue *q)\n{\n\treturn q->limits.max_integrity_segments;\n}\n\n/**\n * bio_integrity_intervals - Return number of integrity intervals for a bio\n * @bi:\t\tblk_integrity profile for device\n * @sectors:\tSize of the bio in 512-byte sectors\n *\n * Description: The block layer calculates everything in 512 byte\n * sectors but integrity metadata is done in terms of the data integrity\n * interval size of the storage device.  Convert the block layer sectors\n * to the appropriate number of integrity intervals.\n */\nstatic inline unsigned int bio_integrity_intervals(struct blk_integrity *bi,\n\t\t\t\t\t\t   unsigned int sectors)\n{\n\treturn sectors >> (bi->interval_exp - 9);\n}\n\nstatic inline unsigned int bio_integrity_bytes(struct blk_integrity *bi,\n\t\t\t\t\t       unsigned int sectors)\n{\n\treturn bio_integrity_intervals(bi, sectors) * bi->tuple_size;\n}\n\n/*\n * Return the first bvec that contains integrity data.  Only drivers that are\n * limited to a single integrity segment should use this helper.\n */\nstatic inline struct bio_vec *rq_integrity_vec(struct request *rq)\n{\n\tif (WARN_ON_ONCE(queue_max_integrity_segments(rq->q) > 1))\n\t\treturn NULL;\n\treturn rq->bio->bi_integrity->bip_vec;\n}\n\n#else /* CONFIG_BLK_DEV_INTEGRITY */\n\nstruct bio;\nstruct block_device;\nstruct gendisk;\nstruct blk_integrity;\n\nstatic inline int blk_integrity_rq(struct request *rq)\n{\n\treturn 0;\n}\nstatic inline int blk_rq_count_integrity_sg(struct request_queue *q,\n\t\t\t\t\t    struct bio *b)\n{\n\treturn 0;\n}\nstatic inline int blk_rq_map_integrity_sg(struct request_queue *q,\n\t\t\t\t\t  struct bio *b,\n\t\t\t\t\t  struct scatterlist *s)\n{\n\treturn 0;\n}\nstatic inline struct blk_integrity *bdev_get_integrity(struct block_device *b)\n{\n\treturn NULL;\n}\nstatic inline struct blk_integrity *blk_get_integrity(struct gendisk *disk)\n{\n\treturn NULL;\n}\nstatic inline bool\nblk_integrity_queue_supports_integrity(struct request_queue *q)\n{\n\treturn false;\n}\nstatic inline int blk_integrity_compare(struct gendisk *a, struct gendisk *b)\n{\n\treturn 0;\n}\nstatic inline void blk_integrity_register(struct gendisk *d,\n\t\t\t\t\t struct blk_integrity *b)\n{\n}\nstatic inline void blk_integrity_unregister(struct gendisk *d)\n{\n}\nstatic inline void blk_queue_max_integrity_segments(struct request_queue *q,\n\t\t\t\t\t\t    unsigned int segs)\n{\n}\nstatic inline unsigned short queue_max_integrity_segments(const struct request_queue *q)\n{\n\treturn 0;\n}\n\nstatic inline unsigned int bio_integrity_intervals(struct blk_integrity *bi,\n\t\t\t\t\t\t   unsigned int sectors)\n{\n\treturn 0;\n}\n\nstatic inline unsigned int bio_integrity_bytes(struct blk_integrity *bi,\n\t\t\t\t\t       unsigned int sectors)\n{\n\treturn 0;\n}\n\nstatic inline struct bio_vec *rq_integrity_vec(struct request *rq)\n{\n\treturn NULL;\n}\n\n#endif /* CONFIG_BLK_DEV_INTEGRITY */\n\n#ifdef CONFIG_BLK_INLINE_ENCRYPTION\n\nbool blk_ksm_register(struct blk_keyslot_manager *ksm, struct request_queue *q);\n\nvoid blk_ksm_unregister(struct request_queue *q);\n\n#else /* CONFIG_BLK_INLINE_ENCRYPTION */\n\nstatic inline bool blk_ksm_register(struct blk_keyslot_manager *ksm,\n\t\t\t\t    struct request_queue *q)\n{\n\treturn true;\n}\n\nstatic inline void blk_ksm_unregister(struct request_queue *q) { }\n\n#endif /* CONFIG_BLK_INLINE_ENCRYPTION */\n\n\nstruct block_device_operations {\n\tblk_qc_t (*submit_bio) (struct bio *bio);\n\tint (*open) (struct block_device *, fmode_t);\n\tvoid (*release) (struct gendisk *, fmode_t);\n\tint (*rw_page)(struct block_device *, sector_t, struct page *, unsigned int);\n\tint (*ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);\n\tint (*compat_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);\n\tunsigned int (*check_events) (struct gendisk *disk,\n\t\t\t\t      unsigned int clearing);\n\tvoid (*unlock_native_capacity) (struct gendisk *);\n\tint (*revalidate_disk) (struct gendisk *);\n\tint (*getgeo)(struct block_device *, struct hd_geometry *);\n\t/* this callback is with swap_lock and sometimes page table lock held */\n\tvoid (*swap_slot_free_notify) (struct block_device *, unsigned long);\n\tint (*report_zones)(struct gendisk *, sector_t sector,\n\t\t\tunsigned int nr_zones, report_zones_cb cb, void *data);\n\tchar *(*devnode)(struct gendisk *disk, umode_t *mode);\n\tstruct module *owner;\n\tconst struct pr_ops *pr_ops;\n};\n\n#ifdef CONFIG_COMPAT\nextern int blkdev_compat_ptr_ioctl(struct block_device *, fmode_t,\n\t\t\t\t      unsigned int, unsigned long);\n#else\n#define blkdev_compat_ptr_ioctl NULL\n#endif\n\nextern int __blkdev_driver_ioctl(struct block_device *, fmode_t, unsigned int,\n\t\t\t\t unsigned long);\nextern int bdev_read_page(struct block_device *, sector_t, struct page *);\nextern int bdev_write_page(struct block_device *, sector_t, struct page *,\n\t\t\t\t\t\tstruct writeback_control *);\n\n#ifdef CONFIG_BLK_DEV_ZONED\nbool blk_req_needs_zone_write_lock(struct request *rq);\nbool blk_req_zone_write_trylock(struct request *rq);\nvoid __blk_req_zone_write_lock(struct request *rq);\nvoid __blk_req_zone_write_unlock(struct request *rq);\n\nstatic inline void blk_req_zone_write_lock(struct request *rq)\n{\n\tif (blk_req_needs_zone_write_lock(rq))\n\t\t__blk_req_zone_write_lock(rq);\n}\n\nstatic inline void blk_req_zone_write_unlock(struct request *rq)\n{\n\tif (rq->rq_flags & RQF_ZONE_WRITE_LOCKED)\n\t\t__blk_req_zone_write_unlock(rq);\n}\n\nstatic inline bool blk_req_zone_is_write_locked(struct request *rq)\n{\n\treturn rq->q->seq_zones_wlock &&\n\t\ttest_bit(blk_rq_zone_no(rq), rq->q->seq_zones_wlock);\n}\n\nstatic inline bool blk_req_can_dispatch_to_zone(struct request *rq)\n{\n\tif (!blk_req_needs_zone_write_lock(rq))\n\t\treturn true;\n\treturn !blk_req_zone_is_write_locked(rq);\n}\n#else\nstatic inline bool blk_req_needs_zone_write_lock(struct request *rq)\n{\n\treturn false;\n}\n\nstatic inline void blk_req_zone_write_lock(struct request *rq)\n{\n}\n\nstatic inline void blk_req_zone_write_unlock(struct request *rq)\n{\n}\nstatic inline bool blk_req_zone_is_write_locked(struct request *rq)\n{\n\treturn false;\n}\n\nstatic inline bool blk_req_can_dispatch_to_zone(struct request *rq)\n{\n\treturn true;\n}\n#endif /* CONFIG_BLK_DEV_ZONED */\n\nstatic inline void blk_wake_io_task(struct task_struct *waiter)\n{\n\t/*\n\t * If we're polling, the task itself is doing the completions. For\n\t * that case, we don't need to signal a wakeup, it's enough to just\n\t * mark us as RUNNING.\n\t */\n\tif (waiter == current)\n\t\t__set_current_state(TASK_RUNNING);\n\telse\n\t\twake_up_process(waiter);\n}\n\nunsigned long disk_start_io_acct(struct gendisk *disk, unsigned int sectors,\n\t\tunsigned int op);\nvoid disk_end_io_acct(struct gendisk *disk, unsigned int op,\n\t\tunsigned long start_time);\n\nunsigned long part_start_io_acct(struct gendisk *disk, struct hd_struct **part,\n\t\t\t\t struct bio *bio);\nvoid part_end_io_acct(struct hd_struct *part, struct bio *bio,\n\t\t      unsigned long start_time);\n\n/**\n * bio_start_io_acct - start I/O accounting for bio based drivers\n * @bio:\tbio to start account for\n *\n * Returns the start time that should be passed back to bio_end_io_acct().\n */\nstatic inline unsigned long bio_start_io_acct(struct bio *bio)\n{\n\treturn disk_start_io_acct(bio->bi_disk, bio_sectors(bio), bio_op(bio));\n}\n\n/**\n * bio_end_io_acct - end I/O accounting for bio based drivers\n * @bio:\tbio to end account for\n * @start:\tstart time returned by bio_start_io_acct()\n */\nstatic inline void bio_end_io_acct(struct bio *bio, unsigned long start_time)\n{\n\treturn disk_end_io_acct(bio->bi_disk, bio_op(bio), start_time);\n}\n\nint bdev_read_only(struct block_device *bdev);\nint set_blocksize(struct block_device *bdev, int size);\n\nconst char *bdevname(struct block_device *bdev, char *buffer);\nstruct block_device *lookup_bdev(const char *);\n\nvoid blkdev_show(struct seq_file *seqf, off_t offset);\n\n#define BDEVNAME_SIZE\t32\t/* Largest string for a blockdev identifier */\n#define BDEVT_SIZE\t10\t/* Largest string for MAJ:MIN for blkdev */\n#ifdef CONFIG_BLOCK\n#define BLKDEV_MAJOR_MAX\t512\n#else\n#define BLKDEV_MAJOR_MAX\t0\n#endif\n\nstruct block_device *blkdev_get_by_path(const char *path, fmode_t mode,\n\t\tvoid *holder);\nstruct block_device *blkdev_get_by_dev(dev_t dev, fmode_t mode, void *holder);\nint bd_prepare_to_claim(struct block_device *bdev, struct block_device *whole,\n\t\tvoid *holder);\nvoid bd_abort_claiming(struct block_device *bdev, struct block_device *whole,\n\t\tvoid *holder);\nvoid blkdev_put(struct block_device *bdev, fmode_t mode);\n\nstruct block_device *I_BDEV(struct inode *inode);\nstruct block_device *bdget_part(struct hd_struct *part);\nstruct block_device *bdgrab(struct block_device *bdev);\nvoid bdput(struct block_device *);\n\n#ifdef CONFIG_BLOCK\nvoid invalidate_bdev(struct block_device *bdev);\nint truncate_bdev_range(struct block_device *bdev, fmode_t mode, loff_t lstart,\n\t\t\tloff_t lend);\nint sync_blockdev(struct block_device *bdev);\n#else\nstatic inline void invalidate_bdev(struct block_device *bdev)\n{\n}\nstatic inline int truncate_bdev_range(struct block_device *bdev, fmode_t mode,\n\t\t\t\t      loff_t lstart, loff_t lend)\n{\n\treturn 0;\n}\nstatic inline int sync_blockdev(struct block_device *bdev)\n{\n\treturn 0;\n}\n#endif\nint fsync_bdev(struct block_device *bdev);\n\nstruct super_block *freeze_bdev(struct block_device *bdev);\nint thaw_bdev(struct block_device *bdev, struct super_block *sb);\n\n#endif /* _LINUX_BLKDEV_H */\n"}, "3": {"id": 3, "path": "/src/include/linux/kernel.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_KERNEL_H\n#define _LINUX_KERNEL_H\n\n\n#include <stdarg.h>\n#include <linux/limits.h>\n#include <linux/linkage.h>\n#include <linux/stddef.h>\n#include <linux/types.h>\n#include <linux/compiler.h>\n#include <linux/bitops.h>\n#include <linux/log2.h>\n#include <linux/minmax.h>\n#include <linux/typecheck.h>\n#include <linux/printk.h>\n#include <linux/build_bug.h>\n#include <asm/byteorder.h>\n#include <asm/div64.h>\n#include <uapi/linux/kernel.h>\n\n#define STACK_MAGIC\t0xdeadbeef\n\n/**\n * REPEAT_BYTE - repeat the value @x multiple times as an unsigned long value\n * @x: value to repeat\n *\n * NOTE: @x is not checked for > 0xff; larger values produce odd results.\n */\n#define REPEAT_BYTE(x)\t((~0ul / 0xff) * (x))\n\n/* @a is a power of 2 value */\n#define ALIGN(x, a)\t\t__ALIGN_KERNEL((x), (a))\n#define ALIGN_DOWN(x, a)\t__ALIGN_KERNEL((x) - ((a) - 1), (a))\n#define __ALIGN_MASK(x, mask)\t__ALIGN_KERNEL_MASK((x), (mask))\n#define PTR_ALIGN(p, a)\t\t((typeof(p))ALIGN((unsigned long)(p), (a)))\n#define PTR_ALIGN_DOWN(p, a)\t((typeof(p))ALIGN_DOWN((unsigned long)(p), (a)))\n#define IS_ALIGNED(x, a)\t\t(((x) & ((typeof(x))(a) - 1)) == 0)\n\n/* generic data direction definitions */\n#define READ\t\t\t0\n#define WRITE\t\t\t1\n\n/**\n * ARRAY_SIZE - get the number of elements in array @arr\n * @arr: array to be sized\n */\n#define ARRAY_SIZE(arr) (sizeof(arr) / sizeof((arr)[0]) + __must_be_array(arr))\n\n#define u64_to_user_ptr(x) (\t\t\\\n{\t\t\t\t\t\\\n\ttypecheck(u64, (x));\t\t\\\n\t(void __user *)(uintptr_t)(x);\t\\\n}\t\t\t\t\t\\\n)\n\n/*\n * This looks more complex than it should be. But we need to\n * get the type for the ~ right in round_down (it needs to be\n * as wide as the result!), and we want to evaluate the macro\n * arguments just once each.\n */\n#define __round_mask(x, y) ((__typeof__(x))((y)-1))\n/**\n * round_up - round up to next specified power of 2\n * @x: the value to round\n * @y: multiple to round up to (must be a power of 2)\n *\n * Rounds @x up to next multiple of @y (which must be a power of 2).\n * To perform arbitrary rounding up, use roundup() below.\n */\n#define round_up(x, y) ((((x)-1) | __round_mask(x, y))+1)\n/**\n * round_down - round down to next specified power of 2\n * @x: the value to round\n * @y: multiple to round down to (must be a power of 2)\n *\n * Rounds @x down to next multiple of @y (which must be a power of 2).\n * To perform arbitrary rounding down, use rounddown() below.\n */\n#define round_down(x, y) ((x) & ~__round_mask(x, y))\n\n#define typeof_member(T, m)\ttypeof(((T*)0)->m)\n\n#define DIV_ROUND_UP __KERNEL_DIV_ROUND_UP\n\n#define DIV_ROUND_DOWN_ULL(ll, d) \\\n\t({ unsigned long long _tmp = (ll); do_div(_tmp, d); _tmp; })\n\n#define DIV_ROUND_UP_ULL(ll, d) \\\n\tDIV_ROUND_DOWN_ULL((unsigned long long)(ll) + (d) - 1, (d))\n\n#if BITS_PER_LONG == 32\n# define DIV_ROUND_UP_SECTOR_T(ll,d) DIV_ROUND_UP_ULL(ll, d)\n#else\n# define DIV_ROUND_UP_SECTOR_T(ll,d) DIV_ROUND_UP(ll,d)\n#endif\n\n/**\n * roundup - round up to the next specified multiple\n * @x: the value to up\n * @y: multiple to round up to\n *\n * Rounds @x up to next multiple of @y. If @y will always be a power\n * of 2, consider using the faster round_up().\n */\n#define roundup(x, y) (\t\t\t\t\t\\\n{\t\t\t\t\t\t\t\\\n\ttypeof(y) __y = y;\t\t\t\t\\\n\t(((x) + (__y - 1)) / __y) * __y;\t\t\\\n}\t\t\t\t\t\t\t\\\n)\n/**\n * rounddown - round down to next specified multiple\n * @x: the value to round\n * @y: multiple to round down to\n *\n * Rounds @x down to next multiple of @y. If @y will always be a power\n * of 2, consider using the faster round_down().\n */\n#define rounddown(x, y) (\t\t\t\t\\\n{\t\t\t\t\t\t\t\\\n\ttypeof(x) __x = (x);\t\t\t\t\\\n\t__x - (__x % (y));\t\t\t\t\\\n}\t\t\t\t\t\t\t\\\n)\n\n/*\n * Divide positive or negative dividend by positive or negative divisor\n * and round to closest integer. Result is undefined for negative\n * divisors if the dividend variable type is unsigned and for negative\n * dividends if the divisor variable type is unsigned.\n */\n#define DIV_ROUND_CLOSEST(x, divisor)(\t\t\t\\\n{\t\t\t\t\t\t\t\\\n\ttypeof(x) __x = x;\t\t\t\t\\\n\ttypeof(divisor) __d = divisor;\t\t\t\\\n\t(((typeof(x))-1) > 0 ||\t\t\t\t\\\n\t ((typeof(divisor))-1) > 0 ||\t\t\t\\\n\t (((__x) > 0) == ((__d) > 0))) ?\t\t\\\n\t\t(((__x) + ((__d) / 2)) / (__d)) :\t\\\n\t\t(((__x) - ((__d) / 2)) / (__d));\t\\\n}\t\t\t\t\t\t\t\\\n)\n/*\n * Same as above but for u64 dividends. divisor must be a 32-bit\n * number.\n */\n#define DIV_ROUND_CLOSEST_ULL(x, divisor)(\t\t\\\n{\t\t\t\t\t\t\t\\\n\ttypeof(divisor) __d = divisor;\t\t\t\\\n\tunsigned long long _tmp = (x) + (__d) / 2;\t\\\n\tdo_div(_tmp, __d);\t\t\t\t\\\n\t_tmp;\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\\\n)\n\n/*\n * Multiplies an integer by a fraction, while avoiding unnecessary\n * overflow or loss of precision.\n */\n#define mult_frac(x, numer, denom)(\t\t\t\\\n{\t\t\t\t\t\t\t\\\n\ttypeof(x) quot = (x) / (denom);\t\t\t\\\n\ttypeof(x) rem  = (x) % (denom);\t\t\t\\\n\t(quot * (numer)) + ((rem * (numer)) / (denom));\t\\\n}\t\t\t\t\t\t\t\\\n)\n\n\n#define _RET_IP_\t\t(unsigned long)__builtin_return_address(0)\n#define _THIS_IP_  ({ __label__ __here; __here: (unsigned long)&&__here; })\n\n#define sector_div(a, b) do_div(a, b)\n\n/**\n * upper_32_bits - return bits 32-63 of a number\n * @n: the number we're accessing\n *\n * A basic shift-right of a 64- or 32-bit quantity.  Use this to suppress\n * the \"right shift count >= width of type\" warning when that quantity is\n * 32-bits.\n */\n#define upper_32_bits(n) ((u32)(((n) >> 16) >> 16))\n\n/**\n * lower_32_bits - return bits 0-31 of a number\n * @n: the number we're accessing\n */\n#define lower_32_bits(n) ((u32)((n) & 0xffffffff))\n\nstruct completion;\nstruct pt_regs;\nstruct user;\n\n#ifdef CONFIG_PREEMPT_VOLUNTARY\nextern int _cond_resched(void);\n# define might_resched() _cond_resched()\n#else\n# define might_resched() do { } while (0)\n#endif\n\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\nextern void ___might_sleep(const char *file, int line, int preempt_offset);\nextern void __might_sleep(const char *file, int line, int preempt_offset);\nextern void __cant_sleep(const char *file, int line, int preempt_offset);\n\n/**\n * might_sleep - annotation for functions that can sleep\n *\n * this macro will print a stack trace if it is executed in an atomic\n * context (spinlock, irq-handler, ...). Additional sections where blocking is\n * not allowed can be annotated with non_block_start() and non_block_end()\n * pairs.\n *\n * This is a useful debugging help to be able to catch problems early and not\n * be bitten later when the calling function happens to sleep when it is not\n * supposed to.\n */\n# define might_sleep() \\\n\tdo { __might_sleep(__FILE__, __LINE__, 0); might_resched(); } while (0)\n/**\n * cant_sleep - annotation for functions that cannot sleep\n *\n * this macro will print a stack trace if it is executed with preemption enabled\n */\n# define cant_sleep() \\\n\tdo { __cant_sleep(__FILE__, __LINE__, 0); } while (0)\n# define sched_annotate_sleep()\t(current->task_state_change = 0)\n/**\n * non_block_start - annotate the start of section where sleeping is prohibited\n *\n * This is on behalf of the oom reaper, specifically when it is calling the mmu\n * notifiers. The problem is that if the notifier were to block on, for example,\n * mutex_lock() and if the process which holds that mutex were to perform a\n * sleeping memory allocation, the oom reaper is now blocked on completion of\n * that memory allocation. Other blocking calls like wait_event() pose similar\n * issues.\n */\n# define non_block_start() (current->non_block_count++)\n/**\n * non_block_end - annotate the end of section where sleeping is prohibited\n *\n * Closes a section opened by non_block_start().\n */\n# define non_block_end() WARN_ON(current->non_block_count-- == 0)\n#else\n  static inline void ___might_sleep(const char *file, int line,\n\t\t\t\t   int preempt_offset) { }\n  static inline void __might_sleep(const char *file, int line,\n\t\t\t\t   int preempt_offset) { }\n# define might_sleep() do { might_resched(); } while (0)\n# define cant_sleep() do { } while (0)\n# define sched_annotate_sleep() do { } while (0)\n# define non_block_start() do { } while (0)\n# define non_block_end() do { } while (0)\n#endif\n\n#define might_sleep_if(cond) do { if (cond) might_sleep(); } while (0)\n\n#ifndef CONFIG_PREEMPT_RT\n# define cant_migrate()\t\tcant_sleep()\n#else\n  /* Placeholder for now */\n# define cant_migrate()\t\tdo { } while (0)\n#endif\n\n/**\n * abs - return absolute value of an argument\n * @x: the value.  If it is unsigned type, it is converted to signed type first.\n *     char is treated as if it was signed (regardless of whether it really is)\n *     but the macro's return type is preserved as char.\n *\n * Return: an absolute value of x.\n */\n#define abs(x)\t__abs_choose_expr(x, long long,\t\t\t\t\\\n\t\t__abs_choose_expr(x, long,\t\t\t\t\\\n\t\t__abs_choose_expr(x, int,\t\t\t\t\\\n\t\t__abs_choose_expr(x, short,\t\t\t\t\\\n\t\t__abs_choose_expr(x, char,\t\t\t\t\\\n\t\t__builtin_choose_expr(\t\t\t\t\t\\\n\t\t\t__builtin_types_compatible_p(typeof(x), char),\t\\\n\t\t\t(char)({ signed char __x = (x); __x<0?-__x:__x; }), \\\n\t\t\t((void)0)))))))\n\n#define __abs_choose_expr(x, type, other) __builtin_choose_expr(\t\\\n\t__builtin_types_compatible_p(typeof(x),   signed type) ||\t\\\n\t__builtin_types_compatible_p(typeof(x), unsigned type),\t\t\\\n\t({ signed type __x = (x); __x < 0 ? -__x : __x; }), other)\n\n/**\n * reciprocal_scale - \"scale\" a value into range [0, ep_ro)\n * @val: value\n * @ep_ro: right open interval endpoint\n *\n * Perform a \"reciprocal multiplication\" in order to \"scale\" a value into\n * range [0, @ep_ro), where the upper interval endpoint is right-open.\n * This is useful, e.g. for accessing a index of an array containing\n * @ep_ro elements, for example. Think of it as sort of modulus, only that\n * the result isn't that of modulo. ;) Note that if initial input is a\n * small value, then result will return 0.\n *\n * Return: a result based on @val in interval [0, @ep_ro).\n */\nstatic inline u32 reciprocal_scale(u32 val, u32 ep_ro)\n{\n\treturn (u32)(((u64) val * ep_ro) >> 32);\n}\n\n#if defined(CONFIG_MMU) && \\\n\t(defined(CONFIG_PROVE_LOCKING) || defined(CONFIG_DEBUG_ATOMIC_SLEEP))\n#define might_fault() __might_fault(__FILE__, __LINE__)\nvoid __might_fault(const char *file, int line);\n#else\nstatic inline void might_fault(void) { }\n#endif\n\nextern struct atomic_notifier_head panic_notifier_list;\nextern long (*panic_blink)(int state);\n__printf(1, 2)\nvoid panic(const char *fmt, ...) __noreturn __cold;\nvoid nmi_panic(struct pt_regs *regs, const char *msg);\nextern void oops_enter(void);\nextern void oops_exit(void);\nextern bool oops_may_print(void);\nvoid do_exit(long error_code) __noreturn;\nvoid complete_and_exit(struct completion *, long) __noreturn;\n\n/* Internal, do not use. */\nint __must_check _kstrtoul(const char *s, unsigned int base, unsigned long *res);\nint __must_check _kstrtol(const char *s, unsigned int base, long *res);\n\nint __must_check kstrtoull(const char *s, unsigned int base, unsigned long long *res);\nint __must_check kstrtoll(const char *s, unsigned int base, long long *res);\n\n/**\n * kstrtoul - convert a string to an unsigned long\n * @s: The start of the string. The string must be null-terminated, and may also\n *  include a single newline before its terminating null. The first character\n *  may also be a plus sign, but not a minus sign.\n * @base: The number base to use. The maximum supported base is 16. If base is\n *  given as 0, then the base of the string is automatically detected with the\n *  conventional semantics - If it begins with 0x the number will be parsed as a\n *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be\n *  parsed as an octal number. Otherwise it will be parsed as a decimal.\n * @res: Where to write the result of the conversion on success.\n *\n * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.\n * Preferred over simple_strtoul(). Return code must be checked.\n*/\nstatic inline int __must_check kstrtoul(const char *s, unsigned int base, unsigned long *res)\n{\n\t/*\n\t * We want to shortcut function call, but\n\t * __builtin_types_compatible_p(unsigned long, unsigned long long) = 0.\n\t */\n\tif (sizeof(unsigned long) == sizeof(unsigned long long) &&\n\t    __alignof__(unsigned long) == __alignof__(unsigned long long))\n\t\treturn kstrtoull(s, base, (unsigned long long *)res);\n\telse\n\t\treturn _kstrtoul(s, base, res);\n}\n\n/**\n * kstrtol - convert a string to a long\n * @s: The start of the string. The string must be null-terminated, and may also\n *  include a single newline before its terminating null. The first character\n *  may also be a plus sign or a minus sign.\n * @base: The number base to use. The maximum supported base is 16. If base is\n *  given as 0, then the base of the string is automatically detected with the\n *  conventional semantics - If it begins with 0x the number will be parsed as a\n *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be\n *  parsed as an octal number. Otherwise it will be parsed as a decimal.\n * @res: Where to write the result of the conversion on success.\n *\n * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.\n * Preferred over simple_strtol(). Return code must be checked.\n */\nstatic inline int __must_check kstrtol(const char *s, unsigned int base, long *res)\n{\n\t/*\n\t * We want to shortcut function call, but\n\t * __builtin_types_compatible_p(long, long long) = 0.\n\t */\n\tif (sizeof(long) == sizeof(long long) &&\n\t    __alignof__(long) == __alignof__(long long))\n\t\treturn kstrtoll(s, base, (long long *)res);\n\telse\n\t\treturn _kstrtol(s, base, res);\n}\n\nint __must_check kstrtouint(const char *s, unsigned int base, unsigned int *res);\nint __must_check kstrtoint(const char *s, unsigned int base, int *res);\n\nstatic inline int __must_check kstrtou64(const char *s, unsigned int base, u64 *res)\n{\n\treturn kstrtoull(s, base, res);\n}\n\nstatic inline int __must_check kstrtos64(const char *s, unsigned int base, s64 *res)\n{\n\treturn kstrtoll(s, base, res);\n}\n\nstatic inline int __must_check kstrtou32(const char *s, unsigned int base, u32 *res)\n{\n\treturn kstrtouint(s, base, res);\n}\n\nstatic inline int __must_check kstrtos32(const char *s, unsigned int base, s32 *res)\n{\n\treturn kstrtoint(s, base, res);\n}\n\nint __must_check kstrtou16(const char *s, unsigned int base, u16 *res);\nint __must_check kstrtos16(const char *s, unsigned int base, s16 *res);\nint __must_check kstrtou8(const char *s, unsigned int base, u8 *res);\nint __must_check kstrtos8(const char *s, unsigned int base, s8 *res);\nint __must_check kstrtobool(const char *s, bool *res);\n\nint __must_check kstrtoull_from_user(const char __user *s, size_t count, unsigned int base, unsigned long long *res);\nint __must_check kstrtoll_from_user(const char __user *s, size_t count, unsigned int base, long long *res);\nint __must_check kstrtoul_from_user(const char __user *s, size_t count, unsigned int base, unsigned long *res);\nint __must_check kstrtol_from_user(const char __user *s, size_t count, unsigned int base, long *res);\nint __must_check kstrtouint_from_user(const char __user *s, size_t count, unsigned int base, unsigned int *res);\nint __must_check kstrtoint_from_user(const char __user *s, size_t count, unsigned int base, int *res);\nint __must_check kstrtou16_from_user(const char __user *s, size_t count, unsigned int base, u16 *res);\nint __must_check kstrtos16_from_user(const char __user *s, size_t count, unsigned int base, s16 *res);\nint __must_check kstrtou8_from_user(const char __user *s, size_t count, unsigned int base, u8 *res);\nint __must_check kstrtos8_from_user(const char __user *s, size_t count, unsigned int base, s8 *res);\nint __must_check kstrtobool_from_user(const char __user *s, size_t count, bool *res);\n\nstatic inline int __must_check kstrtou64_from_user(const char __user *s, size_t count, unsigned int base, u64 *res)\n{\n\treturn kstrtoull_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtos64_from_user(const char __user *s, size_t count, unsigned int base, s64 *res)\n{\n\treturn kstrtoll_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtou32_from_user(const char __user *s, size_t count, unsigned int base, u32 *res)\n{\n\treturn kstrtouint_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtos32_from_user(const char __user *s, size_t count, unsigned int base, s32 *res)\n{\n\treturn kstrtoint_from_user(s, count, base, res);\n}\n\n/*\n * Use kstrto<foo> instead.\n *\n * NOTE: simple_strto<foo> does not check for the range overflow and,\n *\t depending on the input, may give interesting results.\n *\n * Use these functions if and only if you cannot use kstrto<foo>, because\n * the conversion ends on the first non-digit character, which may be far\n * beyond the supported range. It might be useful to parse the strings like\n * 10x50 or 12:21 without altering original string or temporary buffer in use.\n * Keep in mind above caveat.\n */\n\nextern unsigned long simple_strtoul(const char *,char **,unsigned int);\nextern long simple_strtol(const char *,char **,unsigned int);\nextern unsigned long long simple_strtoull(const char *,char **,unsigned int);\nextern long long simple_strtoll(const char *,char **,unsigned int);\n\nextern int num_to_str(char *buf, int size,\n\t\t      unsigned long long num, unsigned int width);\n\n/* lib/printf utilities */\n\nextern __printf(2, 3) int sprintf(char *buf, const char * fmt, ...);\nextern __printf(2, 0) int vsprintf(char *buf, const char *, va_list);\nextern __printf(3, 4)\nint snprintf(char *buf, size_t size, const char *fmt, ...);\nextern __printf(3, 0)\nint vsnprintf(char *buf, size_t size, const char *fmt, va_list args);\nextern __printf(3, 4)\nint scnprintf(char *buf, size_t size, const char *fmt, ...);\nextern __printf(3, 0)\nint vscnprintf(char *buf, size_t size, const char *fmt, va_list args);\nextern __printf(2, 3) __malloc\nchar *kasprintf(gfp_t gfp, const char *fmt, ...);\nextern __printf(2, 0) __malloc\nchar *kvasprintf(gfp_t gfp, const char *fmt, va_list args);\nextern __printf(2, 0)\nconst char *kvasprintf_const(gfp_t gfp, const char *fmt, va_list args);\n\nextern __scanf(2, 3)\nint sscanf(const char *, const char *, ...);\nextern __scanf(2, 0)\nint vsscanf(const char *, const char *, va_list);\n\nextern int get_option(char **str, int *pint);\nextern char *get_options(const char *str, int nints, int *ints);\nextern unsigned long long memparse(const char *ptr, char **retptr);\nextern bool parse_option_str(const char *str, const char *option);\nextern char *next_arg(char *args, char **param, char **val);\n\nextern int core_kernel_text(unsigned long addr);\nextern int init_kernel_text(unsigned long addr);\nextern int core_kernel_data(unsigned long addr);\nextern int __kernel_text_address(unsigned long addr);\nextern int kernel_text_address(unsigned long addr);\nextern int func_ptr_is_kernel_text(void *ptr);\n\nu64 int_pow(u64 base, unsigned int exp);\nunsigned long int_sqrt(unsigned long);\n\n#if BITS_PER_LONG < 64\nu32 int_sqrt64(u64 x);\n#else\nstatic inline u32 int_sqrt64(u64 x)\n{\n\treturn (u32)int_sqrt(x);\n}\n#endif\n\n#ifdef CONFIG_SMP\nextern unsigned int sysctl_oops_all_cpu_backtrace;\n#else\n#define sysctl_oops_all_cpu_backtrace 0\n#endif /* CONFIG_SMP */\n\nextern void bust_spinlocks(int yes);\nextern int panic_timeout;\nextern unsigned long panic_print;\nextern int panic_on_oops;\nextern int panic_on_unrecovered_nmi;\nextern int panic_on_io_nmi;\nextern int panic_on_warn;\nextern unsigned long panic_on_taint;\nextern bool panic_on_taint_nousertaint;\nextern int sysctl_panic_on_rcu_stall;\nextern int sysctl_panic_on_stackoverflow;\n\nextern bool crash_kexec_post_notifiers;\n\n/*\n * panic_cpu is used for synchronizing panic() and crash_kexec() execution. It\n * holds a CPU number which is executing panic() currently. A value of\n * PANIC_CPU_INVALID means no CPU has entered panic() or crash_kexec().\n */\nextern atomic_t panic_cpu;\n#define PANIC_CPU_INVALID\t-1\n\n/*\n * Only to be used by arch init code. If the user over-wrote the default\n * CONFIG_PANIC_TIMEOUT, honor it.\n */\nstatic inline void set_arch_panic_timeout(int timeout, int arch_default_timeout)\n{\n\tif (panic_timeout == arch_default_timeout)\n\t\tpanic_timeout = timeout;\n}\nextern const char *print_tainted(void);\nenum lockdep_ok {\n\tLOCKDEP_STILL_OK,\n\tLOCKDEP_NOW_UNRELIABLE\n};\nextern void add_taint(unsigned flag, enum lockdep_ok);\nextern int test_taint(unsigned flag);\nextern unsigned long get_taint(void);\nextern int root_mountflags;\n\nextern bool early_boot_irqs_disabled;\n\n/*\n * Values used for system_state. Ordering of the states must not be changed\n * as code checks for <, <=, >, >= STATE.\n */\nextern enum system_states {\n\tSYSTEM_BOOTING,\n\tSYSTEM_SCHEDULING,\n\tSYSTEM_RUNNING,\n\tSYSTEM_HALT,\n\tSYSTEM_POWER_OFF,\n\tSYSTEM_RESTART,\n\tSYSTEM_SUSPEND,\n} system_state;\n\n/* This cannot be an enum because some may be used in assembly source. */\n#define TAINT_PROPRIETARY_MODULE\t0\n#define TAINT_FORCED_MODULE\t\t1\n#define TAINT_CPU_OUT_OF_SPEC\t\t2\n#define TAINT_FORCED_RMMOD\t\t3\n#define TAINT_MACHINE_CHECK\t\t4\n#define TAINT_BAD_PAGE\t\t\t5\n#define TAINT_USER\t\t\t6\n#define TAINT_DIE\t\t\t7\n#define TAINT_OVERRIDDEN_ACPI_TABLE\t8\n#define TAINT_WARN\t\t\t9\n#define TAINT_CRAP\t\t\t10\n#define TAINT_FIRMWARE_WORKAROUND\t11\n#define TAINT_OOT_MODULE\t\t12\n#define TAINT_UNSIGNED_MODULE\t\t13\n#define TAINT_SOFTLOCKUP\t\t14\n#define TAINT_LIVEPATCH\t\t\t15\n#define TAINT_AUX\t\t\t16\n#define TAINT_RANDSTRUCT\t\t17\n#define TAINT_FLAGS_COUNT\t\t18\n#define TAINT_FLAGS_MAX\t\t\t((1UL << TAINT_FLAGS_COUNT) - 1)\n\nstruct taint_flag {\n\tchar c_true;\t/* character printed when tainted */\n\tchar c_false;\t/* character printed when not tainted */\n\tbool module;\t/* also show as a per-module taint flag */\n};\n\nextern const struct taint_flag taint_flags[TAINT_FLAGS_COUNT];\n\nextern const char hex_asc[];\n#define hex_asc_lo(x)\thex_asc[((x) & 0x0f)]\n#define hex_asc_hi(x)\thex_asc[((x) & 0xf0) >> 4]\n\nstatic inline char *hex_byte_pack(char *buf, u8 byte)\n{\n\t*buf++ = hex_asc_hi(byte);\n\t*buf++ = hex_asc_lo(byte);\n\treturn buf;\n}\n\nextern const char hex_asc_upper[];\n#define hex_asc_upper_lo(x)\thex_asc_upper[((x) & 0x0f)]\n#define hex_asc_upper_hi(x)\thex_asc_upper[((x) & 0xf0) >> 4]\n\nstatic inline char *hex_byte_pack_upper(char *buf, u8 byte)\n{\n\t*buf++ = hex_asc_upper_hi(byte);\n\t*buf++ = hex_asc_upper_lo(byte);\n\treturn buf;\n}\n\nextern int hex_to_bin(char ch);\nextern int __must_check hex2bin(u8 *dst, const char *src, size_t count);\nextern char *bin2hex(char *dst, const void *src, size_t count);\n\nbool mac_pton(const char *s, u8 *mac);\n\n/*\n * General tracing related utility functions - trace_printk(),\n * tracing_on/tracing_off and tracing_start()/tracing_stop\n *\n * Use tracing_on/tracing_off when you want to quickly turn on or off\n * tracing. It simply enables or disables the recording of the trace events.\n * This also corresponds to the user space /sys/kernel/debug/tracing/tracing_on\n * file, which gives a means for the kernel and userspace to interact.\n * Place a tracing_off() in the kernel where you want tracing to end.\n * From user space, examine the trace, and then echo 1 > tracing_on\n * to continue tracing.\n *\n * tracing_stop/tracing_start has slightly more overhead. It is used\n * by things like suspend to ram where disabling the recording of the\n * trace is not enough, but tracing must actually stop because things\n * like calling smp_processor_id() may crash the system.\n *\n * Most likely, you want to use tracing_on/tracing_off.\n */\n\nenum ftrace_dump_mode {\n\tDUMP_NONE,\n\tDUMP_ALL,\n\tDUMP_ORIG,\n};\n\n#ifdef CONFIG_TRACING\nvoid tracing_on(void);\nvoid tracing_off(void);\nint tracing_is_on(void);\nvoid tracing_snapshot(void);\nvoid tracing_snapshot_alloc(void);\n\nextern void tracing_start(void);\nextern void tracing_stop(void);\n\nstatic inline __printf(1, 2)\nvoid ____trace_printk_check_format(const char *fmt, ...)\n{\n}\n#define __trace_printk_check_format(fmt, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\t\\\n\t\t____trace_printk_check_format(fmt, ##args);\t\t\\\n} while (0)\n\n/**\n * trace_printk - printf formatting in the ftrace buffer\n * @fmt: the printf format for printing\n *\n * Note: __trace_printk is an internal function for trace_printk() and\n *       the @ip is passed in via the trace_printk() macro.\n *\n * This function allows a kernel developer to debug fast path sections\n * that printk is not appropriate for. By scattering in various\n * printk like tracing in the code, a developer can quickly see\n * where problems are occurring.\n *\n * This is intended as a debugging tool for the developer only.\n * Please refrain from leaving trace_printks scattered around in\n * your code. (Extra memory is used for special buffers that are\n * allocated when trace_printk() is used.)\n *\n * A little optimization trick is done here. If there's only one\n * argument, there's no need to scan the string for printf formats.\n * The trace_puts() will suffice. But how can we take advantage of\n * using trace_puts() when trace_printk() has only one argument?\n * By stringifying the args and checking the size we can tell\n * whether or not there are args. __stringify((__VA_ARGS__)) will\n * turn into \"()\\0\" with a size of 3 when there are no args, anything\n * else will be bigger. All we need to do is define a string to this,\n * and then take its size and compare to 3. If it's bigger, use\n * do_trace_printk() otherwise, optimize it to trace_puts(). Then just\n * let gcc optimize the rest.\n */\n\n#define trace_printk(fmt, ...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\\\n\tchar _______STR[] = __stringify((__VA_ARGS__));\t\\\n\tif (sizeof(_______STR) > 3)\t\t\t\\\n\t\tdo_trace_printk(fmt, ##__VA_ARGS__);\t\\\n\telse\t\t\t\t\t\t\\\n\t\ttrace_puts(fmt);\t\t\t\\\n} while (0)\n\n#define do_trace_printk(fmt, args...)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tstatic const char *trace_printk_fmt __used\t\t\t\\\n\t\t__attribute__((section(\"__trace_printk_fmt\"))) =\t\\\n\t\t__builtin_constant_p(fmt) ? fmt : NULL;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t__trace_printk_check_format(fmt, ##args);\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(fmt))\t\t\t\t\t\\\n\t\t__trace_bprintk(_THIS_IP_, trace_printk_fmt, ##args);\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t__trace_printk(_THIS_IP_, fmt, ##args);\t\t\t\\\n} while (0)\n\nextern __printf(2, 3)\nint __trace_bprintk(unsigned long ip, const char *fmt, ...);\n\nextern __printf(2, 3)\nint __trace_printk(unsigned long ip, const char *fmt, ...);\n\n/**\n * trace_puts - write a string into the ftrace buffer\n * @str: the string to record\n *\n * Note: __trace_bputs is an internal function for trace_puts and\n *       the @ip is passed in via the trace_puts macro.\n *\n * This is similar to trace_printk() but is made for those really fast\n * paths that a developer wants the least amount of \"Heisenbug\" effects,\n * where the processing of the print format is still too much.\n *\n * This function allows a kernel developer to debug fast path sections\n * that printk is not appropriate for. By scattering in various\n * printk like tracing in the code, a developer can quickly see\n * where problems are occurring.\n *\n * This is intended as a debugging tool for the developer only.\n * Please refrain from leaving trace_puts scattered around in\n * your code. (Extra memory is used for special buffers that are\n * allocated when trace_puts() is used.)\n *\n * Returns: 0 if nothing was written, positive # if string was.\n *  (1 when __trace_bputs is used, strlen(str) when __trace_puts is used)\n */\n\n#define trace_puts(str) ({\t\t\t\t\t\t\\\n\tstatic const char *trace_printk_fmt __used\t\t\t\\\n\t\t__attribute__((section(\"__trace_printk_fmt\"))) =\t\\\n\t\t__builtin_constant_p(str) ? str : NULL;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(str))\t\t\t\t\t\\\n\t\t__trace_bputs(_THIS_IP_, trace_printk_fmt);\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t__trace_puts(_THIS_IP_, str, strlen(str));\t\t\\\n})\nextern int __trace_bputs(unsigned long ip, const char *str);\nextern int __trace_puts(unsigned long ip, const char *str, int size);\n\nextern void trace_dump_stack(int skip);\n\n/*\n * The double __builtin_constant_p is because gcc will give us an error\n * if we try to allocate the static variable to fmt if it is not a\n * constant. Even with the outer if statement.\n */\n#define ftrace_vprintk(fmt, vargs)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(fmt)) {\t\t\t\t\\\n\t\tstatic const char *trace_printk_fmt __used\t\t\\\n\t\t  __attribute__((section(\"__trace_printk_fmt\"))) =\t\\\n\t\t\t__builtin_constant_p(fmt) ? fmt : NULL;\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t__ftrace_vbprintk(_THIS_IP_, trace_printk_fmt, vargs);\t\\\n\t} else\t\t\t\t\t\t\t\t\\\n\t\t__ftrace_vprintk(_THIS_IP_, fmt, vargs);\t\t\\\n} while (0)\n\nextern __printf(2, 0) int\n__ftrace_vbprintk(unsigned long ip, const char *fmt, va_list ap);\n\nextern __printf(2, 0) int\n__ftrace_vprintk(unsigned long ip, const char *fmt, va_list ap);\n\nextern void ftrace_dump(enum ftrace_dump_mode oops_dump_mode);\n#else\nstatic inline void tracing_start(void) { }\nstatic inline void tracing_stop(void) { }\nstatic inline void trace_dump_stack(int skip) { }\n\nstatic inline void tracing_on(void) { }\nstatic inline void tracing_off(void) { }\nstatic inline int tracing_is_on(void) { return 0; }\nstatic inline void tracing_snapshot(void) { }\nstatic inline void tracing_snapshot_alloc(void) { }\n\nstatic inline __printf(1, 2)\nint trace_printk(const char *fmt, ...)\n{\n\treturn 0;\n}\nstatic __printf(1, 0) inline int\nftrace_vprintk(const char *fmt, va_list ap)\n{\n\treturn 0;\n}\nstatic inline void ftrace_dump(enum ftrace_dump_mode oops_dump_mode) { }\n#endif /* CONFIG_TRACING */\n\n/* This counts to 12. Any more, it will return 13th argument. */\n#define __COUNT_ARGS(_0, _1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _n, X...) _n\n#define COUNT_ARGS(X...) __COUNT_ARGS(, ##X, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0)\n\n#define __CONCAT(a, b) a ## b\n#define CONCATENATE(a, b) __CONCAT(a, b)\n\n/**\n * container_of - cast a member of a structure out to the containing structure\n * @ptr:\tthe pointer to the member.\n * @type:\tthe type of the container struct this is embedded in.\n * @member:\tthe name of the member within the struct.\n *\n */\n#define container_of(ptr, type, member) ({\t\t\t\t\\\n\tvoid *__mptr = (void *)(ptr);\t\t\t\t\t\\\n\tBUILD_BUG_ON_MSG(!__same_type(*(ptr), ((type *)0)->member) &&\t\\\n\t\t\t !__same_type(*(ptr), void),\t\t\t\\\n\t\t\t \"pointer type mismatch in container_of()\");\t\\\n\t((type *)(__mptr - offsetof(type, member))); })\n\n/**\n * container_of_safe - cast a member of a structure out to the containing structure\n * @ptr:\tthe pointer to the member.\n * @type:\tthe type of the container struct this is embedded in.\n * @member:\tthe name of the member within the struct.\n *\n * If IS_ERR_OR_NULL(ptr), ptr is returned unchanged.\n */\n#define container_of_safe(ptr, type, member) ({\t\t\t\t\\\n\tvoid *__mptr = (void *)(ptr);\t\t\t\t\t\\\n\tBUILD_BUG_ON_MSG(!__same_type(*(ptr), ((type *)0)->member) &&\t\\\n\t\t\t !__same_type(*(ptr), void),\t\t\t\\\n\t\t\t \"pointer type mismatch in container_of()\");\t\\\n\tIS_ERR_OR_NULL(__mptr) ? ERR_CAST(__mptr) :\t\t\t\\\n\t\t((type *)(__mptr - offsetof(type, member))); })\n\n/* Rebuild everything on CONFIG_FTRACE_MCOUNT_RECORD */\n#ifdef CONFIG_FTRACE_MCOUNT_RECORD\n# define REBUILD_DUE_TO_FTRACE_MCOUNT_RECORD\n#endif\n\n/* Permissions on a sysfs file: you didn't miss the 0 prefix did you? */\n#define VERIFY_OCTAL_PERMISSIONS(perms)\t\t\t\t\t\t\\\n\t(BUILD_BUG_ON_ZERO((perms) < 0) +\t\t\t\t\t\\\n\t BUILD_BUG_ON_ZERO((perms) > 0777) +\t\t\t\t\t\\\n\t /* USER_READABLE >= GROUP_READABLE >= OTHER_READABLE */\t\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 6) & 4) < (((perms) >> 3) & 4)) +\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 3) & 4) < ((perms) & 4)) +\t\t\\\n\t /* USER_WRITABLE >= GROUP_WRITABLE */\t\t\t\t\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 6) & 2) < (((perms) >> 3) & 2)) +\t\\\n\t /* OTHER_WRITABLE?  Generally considered a bad idea. */\t\t\\\n\t BUILD_BUG_ON_ZERO((perms) & 2) +\t\t\t\t\t\\\n\t (perms))\n#endif\n"}}, "reports": [{"events": [{"location": {"col": 6, "file": 0, "line": 1905}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 0, "line": 1905}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 0, "line": 1907}, "message": "Calling 'list_splice_init'"}, {"location": {"col": 6, "file": 1, "line": 481}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 1, "line": 481}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 0, "line": 1907}, "message": "Returning from 'list_splice_init'"}, {"location": {"col": 6, "file": 0, "line": 1909}, "message": "Assuming field 'rq_count' is <= 2"}, {"location": {"col": 25, "file": 0, "line": 1909}, "message": "Left side of '&&' is false"}, {"location": {"col": 34, "file": 0, "line": 1916}, "message": "Left side of '&&' is false"}, {"location": {"col": 28, "file": 2, "line": 683}, "message": "expanded from macro 'list_entry_rq'"}, {"location": {"col": 2, "file": 1, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 61, "file": 3, "line": 853}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 34, "file": 0, "line": 1916}, "message": "Taking false branch"}, {"location": {"col": 28, "file": 2, "line": 683}, "message": "expanded from macro 'list_entry_rq'"}, {"location": {"col": 2, "file": 1, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 2, "file": 3, "line": 853}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 3, "file": 0, "line": 1918}, "message": "Assigned value is garbage or undefined"}], "macros": [], "notes": [], "path": "/src/block/blk-mq.c", "reportHash": "7b6ff3c5a470b4c70ae730bdc03c3b89", "checkerName": "clang-analyzer-core.uninitialized.Assign", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
