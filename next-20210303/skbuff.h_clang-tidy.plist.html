<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"1": {"id": 1, "path": "/src/fs/io_uring.c", "content": "// SPDX-License-Identifier: GPL-2.0\n/*\n * Shared application/kernel submission and completion ring pairs, for\n * supporting fast/efficient IO.\n *\n * A note on the read/write ordering memory barriers that are matched between\n * the application and kernel side.\n *\n * After the application reads the CQ ring tail, it must use an\n * appropriate smp_rmb() to pair with the smp_wmb() the kernel uses\n * before writing the tail (using smp_load_acquire to read the tail will\n * do). It also needs a smp_mb() before updating CQ head (ordering the\n * entry load(s) with the head store), pairing with an implicit barrier\n * through a control-dependency in io_get_cqring (smp_store_release to\n * store head will do). Failure to do so could lead to reading invalid\n * CQ entries.\n *\n * Likewise, the application must use an appropriate smp_wmb() before\n * writing the SQ tail (ordering SQ entry stores with the tail store),\n * which pairs with smp_load_acquire in io_get_sqring (smp_store_release\n * to store the tail will do). And it needs a barrier ordering the SQ\n * head load before writing new SQ entries (smp_load_acquire to read\n * head will do).\n *\n * When using the SQ poll thread (IORING_SETUP_SQPOLL), the application\n * needs to check the SQ flags for IORING_SQ_NEED_WAKEUP *after*\n * updating the SQ tail; a full memory barrier smp_mb() is needed\n * between.\n *\n * Also see the examples in the liburing library:\n *\n *\tgit://git.kernel.dk/liburing\n *\n * io_uring also uses READ/WRITE_ONCE() for _any_ store or load that happens\n * from data shared between the kernel and application. This is done both\n * for ordering purposes, but also to ensure that once a value is loaded from\n * data that the application could potentially modify, it remains stable.\n *\n * Copyright (C) 2018-2019 Jens Axboe\n * Copyright (c) 2018-2019 Christoph Hellwig\n */\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/errno.h>\n#include <linux/syscalls.h>\n#include <linux/compat.h>\n#include <net/compat.h>\n#include <linux/refcount.h>\n#include <linux/uio.h>\n#include <linux/bits.h>\n\n#include <linux/sched/signal.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/fdtable.h>\n#include <linux/mm.h>\n#include <linux/mman.h>\n#include <linux/percpu.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/bvec.h>\n#include <linux/net.h>\n#include <net/sock.h>\n#include <net/af_unix.h>\n#include <net/scm.h>\n#include <linux/anon_inodes.h>\n#include <linux/sched/mm.h>\n#include <linux/uaccess.h>\n#include <linux/nospec.h>\n#include <linux/sizes.h>\n#include <linux/hugetlb.h>\n#include <linux/highmem.h>\n#include <linux/namei.h>\n#include <linux/fsnotify.h>\n#include <linux/fadvise.h>\n#include <linux/eventpoll.h>\n#include <linux/splice.h>\n#include <linux/task_work.h>\n#include <linux/pagemap.h>\n#include <linux/io_uring.h>\n#include <linux/freezer.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/io_uring.h>\n\n#include <uapi/linux/io_uring.h>\n\n#include \"internal.h\"\n#include \"io-wq.h\"\n\n#define IORING_MAX_ENTRIES\t32768\n#define IORING_MAX_CQ_ENTRIES\t(2 * IORING_MAX_ENTRIES)\n\n/*\n * Shift of 9 is 512 entries, or exactly one page on 64-bit archs\n */\n#define IORING_FILE_TABLE_SHIFT\t9\n#define IORING_MAX_FILES_TABLE\t(1U << IORING_FILE_TABLE_SHIFT)\n#define IORING_FILE_TABLE_MASK\t(IORING_MAX_FILES_TABLE - 1)\n#define IORING_MAX_FIXED_FILES\t(64 * IORING_MAX_FILES_TABLE)\n#define IORING_MAX_RESTRICTIONS\t(IORING_RESTRICTION_LAST + \\\n\t\t\t\t IORING_REGISTER_LAST + IORING_OP_LAST)\n\n#define SQE_VALID_FLAGS\t(IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK|\t\\\n\t\t\t\tIOSQE_IO_HARDLINK | IOSQE_ASYNC | \\\n\t\t\t\tIOSQE_BUFFER_SELECT)\n\nstruct io_uring {\n\tu32 head ____cacheline_aligned_in_smp;\n\tu32 tail ____cacheline_aligned_in_smp;\n};\n\n/*\n * This data is shared with the application through the mmap at offsets\n * IORING_OFF_SQ_RING and IORING_OFF_CQ_RING.\n *\n * The offsets to the member fields are published through struct\n * io_sqring_offsets when calling io_uring_setup.\n */\nstruct io_rings {\n\t/*\n\t * Head and tail offsets into the ring; the offsets need to be\n\t * masked to get valid indices.\n\t *\n\t * The kernel controls head of the sq ring and the tail of the cq ring,\n\t * and the application controls tail of the sq ring and the head of the\n\t * cq ring.\n\t */\n\tstruct io_uring\t\tsq, cq;\n\t/*\n\t * Bitmasks to apply to head and tail offsets (constant, equals\n\t * ring_entries - 1)\n\t */\n\tu32\t\t\tsq_ring_mask, cq_ring_mask;\n\t/* Ring sizes (constant, power of 2) */\n\tu32\t\t\tsq_ring_entries, cq_ring_entries;\n\t/*\n\t * Number of invalid entries dropped by the kernel due to\n\t * invalid index stored in array\n\t *\n\t * Written by the kernel, shouldn't be modified by the\n\t * application (i.e. get number of \"new events\" by comparing to\n\t * cached value).\n\t *\n\t * After a new SQ head value was read by the application this\n\t * counter includes all submissions that were dropped reaching\n\t * the new SQ head (and possibly more).\n\t */\n\tu32\t\t\tsq_dropped;\n\t/*\n\t * Runtime SQ flags\n\t *\n\t * Written by the kernel, shouldn't be modified by the\n\t * application.\n\t *\n\t * The application needs a full memory barrier before checking\n\t * for IORING_SQ_NEED_WAKEUP after updating the sq tail.\n\t */\n\tu32\t\t\tsq_flags;\n\t/*\n\t * Runtime CQ flags\n\t *\n\t * Written by the application, shouldn't be modified by the\n\t * kernel.\n\t */\n\tu32                     cq_flags;\n\t/*\n\t * Number of completion events lost because the queue was full;\n\t * this should be avoided by the application by making sure\n\t * there are not more requests pending than there is space in\n\t * the completion queue.\n\t *\n\t * Written by the kernel, shouldn't be modified by the\n\t * application (i.e. get number of \"new events\" by comparing to\n\t * cached value).\n\t *\n\t * As completion events come in out of order this counter is not\n\t * ordered with any other data.\n\t */\n\tu32\t\t\tcq_overflow;\n\t/*\n\t * Ring buffer of completion events.\n\t *\n\t * The kernel writes completion events fresh every time they are\n\t * produced, so the application is allowed to modify pending\n\t * entries.\n\t */\n\tstruct io_uring_cqe\tcqes[] ____cacheline_aligned_in_smp;\n};\n\nenum io_uring_cmd_flags {\n\tIO_URING_F_NONBLOCK\t\t= 1,\n\tIO_URING_F_COMPLETE_DEFER\t= 2,\n};\n\nstruct io_mapped_ubuf {\n\tu64\t\tubuf;\n\tsize_t\t\tlen;\n\tstruct\t\tbio_vec *bvec;\n\tunsigned int\tnr_bvecs;\n\tunsigned long\tacct_pages;\n};\n\nstruct io_ring_ctx;\n\nstruct io_rsrc_put {\n\tstruct list_head list;\n\tunion {\n\t\tvoid *rsrc;\n\t\tstruct file *file;\n\t};\n};\n\nstruct fixed_rsrc_table {\n\tstruct file\t\t**files;\n};\n\nstruct fixed_rsrc_ref_node {\n\tstruct percpu_ref\t\trefs;\n\tstruct list_head\t\tnode;\n\tstruct list_head\t\trsrc_list;\n\tstruct fixed_rsrc_data\t\t*rsrc_data;\n\tvoid\t\t\t\t(*rsrc_put)(struct io_ring_ctx *ctx,\n\t\t\t\t\t\t    struct io_rsrc_put *prsrc);\n\tstruct llist_node\t\tllist;\n\tbool\t\t\t\tdone;\n};\n\nstruct fixed_rsrc_data {\n\tstruct fixed_rsrc_table\t\t*table;\n\tstruct io_ring_ctx\t\t*ctx;\n\n\tstruct fixed_rsrc_ref_node\t*node;\n\tstruct percpu_ref\t\trefs;\n\tstruct completion\t\tdone;\n\tbool\t\t\t\tquiesce;\n};\n\nstruct io_buffer {\n\tstruct list_head list;\n\t__u64 addr;\n\t__s32 len;\n\t__u16 bid;\n};\n\nstruct io_restriction {\n\tDECLARE_BITMAP(register_op, IORING_REGISTER_LAST);\n\tDECLARE_BITMAP(sqe_op, IORING_OP_LAST);\n\tu8 sqe_flags_allowed;\n\tu8 sqe_flags_required;\n\tbool registered;\n};\n\nenum {\n\tIO_SQ_THREAD_SHOULD_STOP = 0,\n\tIO_SQ_THREAD_SHOULD_PARK,\n};\n\nstruct io_sq_data {\n\trefcount_t\t\trefs;\n\tstruct mutex\t\tlock;\n\n\t/* ctx's that are using this sqd */\n\tstruct list_head\tctx_list;\n\tstruct list_head\tctx_new_list;\n\tstruct mutex\t\tctx_lock;\n\n\tstruct task_struct\t*thread;\n\tstruct wait_queue_head\twait;\n\n\tunsigned\t\tsq_thread_idle;\n\tint\t\t\tsq_cpu;\n\tpid_t\t\t\ttask_pid;\n\n\tunsigned long\t\tstate;\n\tstruct completion\tstartup;\n\tstruct completion\tcompletion;\n\tstruct completion\texited;\n};\n\n#define IO_IOPOLL_BATCH\t\t\t8\n#define IO_COMPL_BATCH\t\t\t32\n#define IO_REQ_CACHE_SIZE\t\t32\n#define IO_REQ_ALLOC_BATCH\t\t8\n\nstruct io_comp_state {\n\tstruct io_kiocb\t\t*reqs[IO_COMPL_BATCH];\n\tunsigned int\t\tnr;\n\tunsigned int\t\tlocked_free_nr;\n\t/* inline/task_work completion list, under ->uring_lock */\n\tstruct list_head\tfree_list;\n\t/* IRQ completion list, under ->completion_lock */\n\tstruct list_head\tlocked_free_list;\n};\n\nstruct io_submit_link {\n\tstruct io_kiocb\t\t*head;\n\tstruct io_kiocb\t\t*last;\n};\n\nstruct io_submit_state {\n\tstruct blk_plug\t\tplug;\n\tstruct io_submit_link\tlink;\n\n\t/*\n\t * io_kiocb alloc cache\n\t */\n\tvoid\t\t\t*reqs[IO_REQ_CACHE_SIZE];\n\tunsigned int\t\tfree_reqs;\n\n\tbool\t\t\tplug_started;\n\n\t/*\n\t * Batch completion logic\n\t */\n\tstruct io_comp_state\tcomp;\n\n\t/*\n\t * File reference cache\n\t */\n\tstruct file\t\t*file;\n\tunsigned int\t\tfd;\n\tunsigned int\t\tfile_refs;\n\tunsigned int\t\tios_left;\n};\n\nstruct io_ring_ctx {\n\tstruct {\n\t\tstruct percpu_ref\trefs;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct {\n\t\tunsigned int\t\tflags;\n\t\tunsigned int\t\tcompat: 1;\n\t\tunsigned int\t\tcq_overflow_flushed: 1;\n\t\tunsigned int\t\tdrain_next: 1;\n\t\tunsigned int\t\teventfd_async: 1;\n\t\tunsigned int\t\trestricted: 1;\n\t\tunsigned int\t\tsqo_exec: 1;\n\n\t\t/*\n\t\t * Ring buffer of indices into array of io_uring_sqe, which is\n\t\t * mmapped by the application using the IORING_OFF_SQES offset.\n\t\t *\n\t\t * This indirection could e.g. be used to assign fixed\n\t\t * io_uring_sqe entries to operations and only submit them to\n\t\t * the queue when needed.\n\t\t *\n\t\t * The kernel modifies neither the indices array nor the entries\n\t\t * array.\n\t\t */\n\t\tu32\t\t\t*sq_array;\n\t\tunsigned\t\tcached_sq_head;\n\t\tunsigned\t\tsq_entries;\n\t\tunsigned\t\tsq_mask;\n\t\tunsigned\t\tsq_thread_idle;\n\t\tunsigned\t\tcached_sq_dropped;\n\t\tunsigned\t\tcached_cq_overflow;\n\t\tunsigned long\t\tsq_check_overflow;\n\n\t\t/* hashed buffered write serialization */\n\t\tstruct io_wq_hash\t*hash_map;\n\n\t\tstruct list_head\tdefer_list;\n\t\tstruct list_head\ttimeout_list;\n\t\tstruct list_head\tcq_overflow_list;\n\n\t\tstruct io_uring_sqe\t*sq_sqes;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct {\n\t\tstruct mutex\t\turing_lock;\n\t\twait_queue_head_t\twait;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct io_submit_state\t\tsubmit_state;\n\n\tstruct io_rings\t*rings;\n\n\t/* Only used for accounting purposes */\n\tstruct mm_struct\t*mm_account;\n\n\tstruct io_sq_data\t*sq_data;\t/* if using sq thread polling */\n\n\tstruct wait_queue_head\tsqo_sq_wait;\n\tstruct list_head\tsqd_list;\n\n\t/*\n\t * If used, fixed file set. Writers must ensure that ->refs is dead,\n\t * readers must ensure that ->refs is alive as long as the file* is\n\t * used. Only updated through io_uring_register(2).\n\t */\n\tstruct fixed_rsrc_data\t*file_data;\n\tunsigned\t\tnr_user_files;\n\n\t/* if used, fixed mapped user buffers */\n\tunsigned\t\tnr_user_bufs;\n\tstruct io_mapped_ubuf\t*user_bufs;\n\n\tstruct user_struct\t*user;\n\n\tstruct completion\tref_comp;\n\tstruct completion\tsq_thread_comp;\n\n#if defined(CONFIG_UNIX)\n\tstruct socket\t\t*ring_sock;\n#endif\n\n\tstruct idr\t\tio_buffer_idr;\n\n\tstruct idr\t\tpersonality_idr;\n\n\tstruct {\n\t\tunsigned\t\tcached_cq_tail;\n\t\tunsigned\t\tcq_entries;\n\t\tunsigned\t\tcq_mask;\n\t\tatomic_t\t\tcq_timeouts;\n\t\tunsigned\t\tcq_last_tm_flush;\n\t\tunsigned long\t\tcq_check_overflow;\n\t\tstruct wait_queue_head\tcq_wait;\n\t\tstruct fasync_struct\t*cq_fasync;\n\t\tstruct eventfd_ctx\t*cq_ev_fd;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct {\n\t\tspinlock_t\t\tcompletion_lock;\n\n\t\t/*\n\t\t * ->iopoll_list is protected by the ctx->uring_lock for\n\t\t * io_uring instances that don't use IORING_SETUP_SQPOLL.\n\t\t * For SQPOLL, only the single threaded io_sq_thread() will\n\t\t * manipulate the list, hence no extra locking is needed there.\n\t\t */\n\t\tstruct list_head\tiopoll_list;\n\t\tstruct hlist_head\t*cancel_hash;\n\t\tunsigned\t\tcancel_hash_bits;\n\t\tbool\t\t\tpoll_multi_file;\n\n\t\tspinlock_t\t\tinflight_lock;\n\t\tstruct list_head\tinflight_list;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct delayed_work\t\trsrc_put_work;\n\tstruct llist_head\t\trsrc_put_llist;\n\tstruct list_head\t\trsrc_ref_list;\n\tspinlock_t\t\t\trsrc_ref_lock;\n\n\tstruct io_restriction\t\trestrictions;\n\n\t/* exit task_work */\n\tstruct callback_head\t\t*exit_task_work;\n\n\tstruct wait_queue_head\t\thash_wait;\n\n\t/* Keep this last, we don't need it for the fast path */\n\tstruct work_struct\t\texit_work;\n};\n\n/*\n * First field must be the file pointer in all the\n * iocb unions! See also 'struct kiocb' in <linux/fs.h>\n */\nstruct io_poll_iocb {\n\tstruct file\t\t\t*file;\n\tstruct wait_queue_head\t\t*head;\n\t__poll_t\t\t\tevents;\n\tbool\t\t\t\tdone;\n\tbool\t\t\t\tcanceled;\n\tstruct wait_queue_entry\t\twait;\n};\n\nstruct io_poll_remove {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\taddr;\n};\n\nstruct io_close {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\tfd;\n};\n\nstruct io_timeout_data {\n\tstruct io_kiocb\t\t\t*req;\n\tstruct hrtimer\t\t\ttimer;\n\tstruct timespec64\t\tts;\n\tenum hrtimer_mode\t\tmode;\n};\n\nstruct io_accept {\n\tstruct file\t\t\t*file;\n\tstruct sockaddr __user\t\t*addr;\n\tint __user\t\t\t*addr_len;\n\tint\t\t\t\tflags;\n\tunsigned long\t\t\tnofile;\n};\n\nstruct io_sync {\n\tstruct file\t\t\t*file;\n\tloff_t\t\t\t\tlen;\n\tloff_t\t\t\t\toff;\n\tint\t\t\t\tflags;\n\tint\t\t\t\tmode;\n};\n\nstruct io_cancel {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\taddr;\n};\n\nstruct io_timeout {\n\tstruct file\t\t\t*file;\n\tu32\t\t\t\toff;\n\tu32\t\t\t\ttarget_seq;\n\tstruct list_head\t\tlist;\n\t/* head of the link, used by linked timeouts only */\n\tstruct io_kiocb\t\t\t*head;\n};\n\nstruct io_timeout_rem {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\taddr;\n\n\t/* timeout update */\n\tstruct timespec64\t\tts;\n\tu32\t\t\t\tflags;\n};\n\nstruct io_rw {\n\t/* NOTE: kiocb has the file as the first member, so don't do it here */\n\tstruct kiocb\t\t\tkiocb;\n\tu64\t\t\t\taddr;\n\tu64\t\t\t\tlen;\n};\n\nstruct io_connect {\n\tstruct file\t\t\t*file;\n\tstruct sockaddr __user\t\t*addr;\n\tint\t\t\t\taddr_len;\n};\n\nstruct io_sr_msg {\n\tstruct file\t\t\t*file;\n\tunion {\n\t\tstruct user_msghdr __user *umsg;\n\t\tvoid __user\t\t*buf;\n\t};\n\tint\t\t\t\tmsg_flags;\n\tint\t\t\t\tbgid;\n\tsize_t\t\t\t\tlen;\n\tstruct io_buffer\t\t*kbuf;\n};\n\nstruct io_open {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\tdfd;\n\tstruct filename\t\t\t*filename;\n\tstruct open_how\t\t\thow;\n\tunsigned long\t\t\tnofile;\n};\n\nstruct io_rsrc_update {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\targ;\n\tu32\t\t\t\tnr_args;\n\tu32\t\t\t\toffset;\n};\n\nstruct io_fadvise {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\toffset;\n\tu32\t\t\t\tlen;\n\tu32\t\t\t\tadvice;\n};\n\nstruct io_madvise {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\taddr;\n\tu32\t\t\t\tlen;\n\tu32\t\t\t\tadvice;\n};\n\nstruct io_epoll {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\tepfd;\n\tint\t\t\t\top;\n\tint\t\t\t\tfd;\n\tstruct epoll_event\t\tevent;\n};\n\nstruct io_splice {\n\tstruct file\t\t\t*file_out;\n\tstruct file\t\t\t*file_in;\n\tloff_t\t\t\t\toff_out;\n\tloff_t\t\t\t\toff_in;\n\tu64\t\t\t\tlen;\n\tunsigned int\t\t\tflags;\n};\n\nstruct io_provide_buf {\n\tstruct file\t\t\t*file;\n\t__u64\t\t\t\taddr;\n\t__s32\t\t\t\tlen;\n\t__u32\t\t\t\tbgid;\n\t__u16\t\t\t\tnbufs;\n\t__u16\t\t\t\tbid;\n};\n\nstruct io_statx {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\tdfd;\n\tunsigned int\t\t\tmask;\n\tunsigned int\t\t\tflags;\n\tconst char __user\t\t*filename;\n\tstruct statx __user\t\t*buffer;\n};\n\nstruct io_shutdown {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\thow;\n};\n\nstruct io_rename {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\told_dfd;\n\tint\t\t\t\tnew_dfd;\n\tstruct filename\t\t\t*oldpath;\n\tstruct filename\t\t\t*newpath;\n\tint\t\t\t\tflags;\n};\n\nstruct io_unlink {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\tdfd;\n\tint\t\t\t\tflags;\n\tstruct filename\t\t\t*filename;\n};\n\nstruct io_completion {\n\tstruct file\t\t\t*file;\n\tstruct list_head\t\tlist;\n\tint\t\t\t\tcflags;\n};\n\nstruct io_async_connect {\n\tstruct sockaddr_storage\t\taddress;\n};\n\nstruct io_async_msghdr {\n\tstruct iovec\t\t\tfast_iov[UIO_FASTIOV];\n\t/* points to an allocated iov, if NULL we use fast_iov instead */\n\tstruct iovec\t\t\t*free_iov;\n\tstruct sockaddr __user\t\t*uaddr;\n\tstruct msghdr\t\t\tmsg;\n\tstruct sockaddr_storage\t\taddr;\n};\n\nstruct io_async_rw {\n\tstruct iovec\t\t\tfast_iov[UIO_FASTIOV];\n\tconst struct iovec\t\t*free_iovec;\n\tstruct iov_iter\t\t\titer;\n\tsize_t\t\t\t\tbytes_done;\n\tstruct wait_page_queue\t\twpq;\n};\n\nenum {\n\tREQ_F_FIXED_FILE_BIT\t= IOSQE_FIXED_FILE_BIT,\n\tREQ_F_IO_DRAIN_BIT\t= IOSQE_IO_DRAIN_BIT,\n\tREQ_F_LINK_BIT\t\t= IOSQE_IO_LINK_BIT,\n\tREQ_F_HARDLINK_BIT\t= IOSQE_IO_HARDLINK_BIT,\n\tREQ_F_FORCE_ASYNC_BIT\t= IOSQE_ASYNC_BIT,\n\tREQ_F_BUFFER_SELECT_BIT\t= IOSQE_BUFFER_SELECT_BIT,\n\n\tREQ_F_FAIL_LINK_BIT,\n\tREQ_F_INFLIGHT_BIT,\n\tREQ_F_CUR_POS_BIT,\n\tREQ_F_NOWAIT_BIT,\n\tREQ_F_LINK_TIMEOUT_BIT,\n\tREQ_F_ISREG_BIT,\n\tREQ_F_NEED_CLEANUP_BIT,\n\tREQ_F_POLLED_BIT,\n\tREQ_F_BUFFER_SELECTED_BIT,\n\tREQ_F_NO_FILE_TABLE_BIT,\n\tREQ_F_LTIMEOUT_ACTIVE_BIT,\n\tREQ_F_COMPLETE_INLINE_BIT,\n\n\t/* not a real bit, just to check we're not overflowing the space */\n\t__REQ_F_LAST_BIT,\n};\n\nenum {\n\t/* ctx owns file */\n\tREQ_F_FIXED_FILE\t= BIT(REQ_F_FIXED_FILE_BIT),\n\t/* drain existing IO first */\n\tREQ_F_IO_DRAIN\t\t= BIT(REQ_F_IO_DRAIN_BIT),\n\t/* linked sqes */\n\tREQ_F_LINK\t\t= BIT(REQ_F_LINK_BIT),\n\t/* doesn't sever on completion < 0 */\n\tREQ_F_HARDLINK\t\t= BIT(REQ_F_HARDLINK_BIT),\n\t/* IOSQE_ASYNC */\n\tREQ_F_FORCE_ASYNC\t= BIT(REQ_F_FORCE_ASYNC_BIT),\n\t/* IOSQE_BUFFER_SELECT */\n\tREQ_F_BUFFER_SELECT\t= BIT(REQ_F_BUFFER_SELECT_BIT),\n\n\t/* fail rest of links */\n\tREQ_F_FAIL_LINK\t\t= BIT(REQ_F_FAIL_LINK_BIT),\n\t/* on inflight list */\n\tREQ_F_INFLIGHT\t\t= BIT(REQ_F_INFLIGHT_BIT),\n\t/* read/write uses file position */\n\tREQ_F_CUR_POS\t\t= BIT(REQ_F_CUR_POS_BIT),\n\t/* must not punt to workers */\n\tREQ_F_NOWAIT\t\t= BIT(REQ_F_NOWAIT_BIT),\n\t/* has or had linked timeout */\n\tREQ_F_LINK_TIMEOUT\t= BIT(REQ_F_LINK_TIMEOUT_BIT),\n\t/* regular file */\n\tREQ_F_ISREG\t\t= BIT(REQ_F_ISREG_BIT),\n\t/* needs cleanup */\n\tREQ_F_NEED_CLEANUP\t= BIT(REQ_F_NEED_CLEANUP_BIT),\n\t/* already went through poll handler */\n\tREQ_F_POLLED\t\t= BIT(REQ_F_POLLED_BIT),\n\t/* buffer already selected */\n\tREQ_F_BUFFER_SELECTED\t= BIT(REQ_F_BUFFER_SELECTED_BIT),\n\t/* doesn't need file table for this request */\n\tREQ_F_NO_FILE_TABLE\t= BIT(REQ_F_NO_FILE_TABLE_BIT),\n\t/* linked timeout is active, i.e. prepared by link's head */\n\tREQ_F_LTIMEOUT_ACTIVE\t= BIT(REQ_F_LTIMEOUT_ACTIVE_BIT),\n\t/* completion is deferred through io_comp_state */\n\tREQ_F_COMPLETE_INLINE\t= BIT(REQ_F_COMPLETE_INLINE_BIT),\n};\n\nstruct async_poll {\n\tstruct io_poll_iocb\tpoll;\n\tstruct io_poll_iocb\t*double_poll;\n};\n\nstruct io_task_work {\n\tstruct io_wq_work_node\tnode;\n\ttask_work_func_t\tfunc;\n};\n\n/*\n * NOTE! Each of the iocb union members has the file pointer\n * as the first entry in their struct definition. So you can\n * access the file pointer through any of the sub-structs,\n * or directly as just 'ki_filp' in this struct.\n */\nstruct io_kiocb {\n\tunion {\n\t\tstruct file\t\t*file;\n\t\tstruct io_rw\t\trw;\n\t\tstruct io_poll_iocb\tpoll;\n\t\tstruct io_poll_remove\tpoll_remove;\n\t\tstruct io_accept\taccept;\n\t\tstruct io_sync\t\tsync;\n\t\tstruct io_cancel\tcancel;\n\t\tstruct io_timeout\ttimeout;\n\t\tstruct io_timeout_rem\ttimeout_rem;\n\t\tstruct io_connect\tconnect;\n\t\tstruct io_sr_msg\tsr_msg;\n\t\tstruct io_open\t\topen;\n\t\tstruct io_close\t\tclose;\n\t\tstruct io_rsrc_update\trsrc_update;\n\t\tstruct io_fadvise\tfadvise;\n\t\tstruct io_madvise\tmadvise;\n\t\tstruct io_epoll\t\tepoll;\n\t\tstruct io_splice\tsplice;\n\t\tstruct io_provide_buf\tpbuf;\n\t\tstruct io_statx\t\tstatx;\n\t\tstruct io_shutdown\tshutdown;\n\t\tstruct io_rename\trename;\n\t\tstruct io_unlink\tunlink;\n\t\t/* use only after cleaning per-op data, see io_clean_op() */\n\t\tstruct io_completion\tcompl;\n\t};\n\n\t/* opcode allocated if it needs to store data for async defer */\n\tvoid\t\t\t\t*async_data;\n\tu8\t\t\t\topcode;\n\t/* polled IO has completed */\n\tu8\t\t\t\tiopoll_completed;\n\n\tu16\t\t\t\tbuf_index;\n\tu32\t\t\t\tresult;\n\n\tstruct io_ring_ctx\t\t*ctx;\n\tunsigned int\t\t\tflags;\n\trefcount_t\t\t\trefs;\n\tstruct task_struct\t\t*task;\n\tu64\t\t\t\tuser_data;\n\n\tstruct io_kiocb\t\t\t*link;\n\tstruct percpu_ref\t\t*fixed_rsrc_refs;\n\n\t/*\n\t * 1. used with ctx->iopoll_list with reads/writes\n\t * 2. to track reqs with ->files (see io_op_def::file_table)\n\t */\n\tstruct list_head\t\tinflight_entry;\n\tunion {\n\t\tstruct io_task_work\tio_task_work;\n\t\tstruct callback_head\ttask_work;\n\t};\n\t/* for polled requests, i.e. IORING_OP_POLL_ADD and async armed poll */\n\tstruct hlist_node\t\thash_node;\n\tstruct async_poll\t\t*apoll;\n\tstruct io_wq_work\t\twork;\n};\n\nstruct io_defer_entry {\n\tstruct list_head\tlist;\n\tstruct io_kiocb\t\t*req;\n\tu32\t\t\tseq;\n};\n\nstruct io_op_def {\n\t/* needs req->file assigned */\n\tunsigned\t\tneeds_file : 1;\n\t/* hash wq insertion if file is a regular file */\n\tunsigned\t\thash_reg_file : 1;\n\t/* unbound wq insertion if file is a non-regular file */\n\tunsigned\t\tunbound_nonreg_file : 1;\n\t/* opcode is not supported by this kernel */\n\tunsigned\t\tnot_supported : 1;\n\t/* set if opcode supports polled \"wait\" */\n\tunsigned\t\tpollin : 1;\n\tunsigned\t\tpollout : 1;\n\t/* op supports buffer selection */\n\tunsigned\t\tbuffer_select : 1;\n\t/* must always have async data allocated */\n\tunsigned\t\tneeds_async_data : 1;\n\t/* should block plug */\n\tunsigned\t\tplug : 1;\n\t/* size of async data needed, if any */\n\tunsigned short\t\tasync_size;\n};\n\nstatic const struct io_op_def io_op_defs[] = {\n\t[IORING_OP_NOP] = {},\n\t[IORING_OP_READV] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.buffer_select\t\t= 1,\n\t\t.needs_async_data\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t},\n\t[IORING_OP_WRITEV] = {\n\t\t.needs_file\t\t= 1,\n\t\t.hash_reg_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.needs_async_data\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t},\n\t[IORING_OP_FSYNC] = {\n\t\t.needs_file\t\t= 1,\n\t},\n\t[IORING_OP_READ_FIXED] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t},\n\t[IORING_OP_WRITE_FIXED] = {\n\t\t.needs_file\t\t= 1,\n\t\t.hash_reg_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t},\n\t[IORING_OP_POLL_ADD] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t},\n\t[IORING_OP_POLL_REMOVE] = {},\n\t[IORING_OP_SYNC_FILE_RANGE] = {\n\t\t.needs_file\t\t= 1,\n\t},\n\t[IORING_OP_SENDMSG] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.needs_async_data\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_msghdr),\n\t},\n\t[IORING_OP_RECVMSG] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.buffer_select\t\t= 1,\n\t\t.needs_async_data\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_msghdr),\n\t},\n\t[IORING_OP_TIMEOUT] = {\n\t\t.needs_async_data\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_timeout_data),\n\t},\n\t[IORING_OP_TIMEOUT_REMOVE] = {\n\t\t/* used by timeout updates' prep() */\n\t},\n\t[IORING_OP_ACCEPT] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t},\n\t[IORING_OP_ASYNC_CANCEL] = {},\n\t[IORING_OP_LINK_TIMEOUT] = {\n\t\t.needs_async_data\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_timeout_data),\n\t},\n\t[IORING_OP_CONNECT] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.needs_async_data\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_connect),\n\t},\n\t[IORING_OP_FALLOCATE] = {\n\t\t.needs_file\t\t= 1,\n\t},\n\t[IORING_OP_OPENAT] = {},\n\t[IORING_OP_CLOSE] = {},\n\t[IORING_OP_FILES_UPDATE] = {},\n\t[IORING_OP_STATX] = {},\n\t[IORING_OP_READ] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.buffer_select\t\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t},\n\t[IORING_OP_WRITE] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t},\n\t[IORING_OP_FADVISE] = {\n\t\t.needs_file\t\t= 1,\n\t},\n\t[IORING_OP_MADVISE] = {},\n\t[IORING_OP_SEND] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t},\n\t[IORING_OP_RECV] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.buffer_select\t\t= 1,\n\t},\n\t[IORING_OP_OPENAT2] = {\n\t},\n\t[IORING_OP_EPOLL_CTL] = {\n\t\t.unbound_nonreg_file\t= 1,\n\t},\n\t[IORING_OP_SPLICE] = {\n\t\t.needs_file\t\t= 1,\n\t\t.hash_reg_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t},\n\t[IORING_OP_PROVIDE_BUFFERS] = {},\n\t[IORING_OP_REMOVE_BUFFERS] = {},\n\t[IORING_OP_TEE] = {\n\t\t.needs_file\t\t= 1,\n\t\t.hash_reg_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t},\n\t[IORING_OP_SHUTDOWN] = {\n\t\t.needs_file\t\t= 1,\n\t},\n\t[IORING_OP_RENAMEAT] = {},\n\t[IORING_OP_UNLINKAT] = {},\n};\n\nstatic void io_uring_try_cancel_requests(struct io_ring_ctx *ctx,\n\t\t\t\t\t struct task_struct *task,\n\t\t\t\t\t struct files_struct *files);\nstatic void io_uring_cancel_sqpoll(struct io_ring_ctx *ctx);\nstatic void destroy_fixed_rsrc_ref_node(struct fixed_rsrc_ref_node *ref_node);\nstatic struct fixed_rsrc_ref_node *alloc_fixed_rsrc_ref_node(\n\t\t\tstruct io_ring_ctx *ctx);\nstatic void io_ring_file_put(struct io_ring_ctx *ctx, struct io_rsrc_put *prsrc);\n\nstatic bool io_rw_reissue(struct io_kiocb *req);\nstatic void io_cqring_fill_event(struct io_kiocb *req, long res);\nstatic void io_put_req(struct io_kiocb *req);\nstatic void io_put_req_deferred(struct io_kiocb *req, int nr);\nstatic void io_double_put_req(struct io_kiocb *req);\nstatic void io_dismantle_req(struct io_kiocb *req);\nstatic void io_put_task(struct task_struct *task, int nr);\nstatic void io_queue_next(struct io_kiocb *req);\nstatic struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req);\nstatic void __io_queue_linked_timeout(struct io_kiocb *req);\nstatic void io_queue_linked_timeout(struct io_kiocb *req);\nstatic int __io_sqe_files_update(struct io_ring_ctx *ctx,\n\t\t\t\t struct io_uring_rsrc_update *ip,\n\t\t\t\t unsigned nr_args);\nstatic void __io_clean_op(struct io_kiocb *req);\nstatic struct file *io_file_get(struct io_submit_state *state,\n\t\t\t\tstruct io_kiocb *req, int fd, bool fixed);\nstatic void __io_queue_sqe(struct io_kiocb *req);\nstatic void io_rsrc_put_work(struct work_struct *work);\n\nstatic int io_import_iovec(int rw, struct io_kiocb *req, struct iovec **iovec,\n\t\t\t   struct iov_iter *iter, bool needs_lock);\nstatic int io_setup_async_rw(struct io_kiocb *req, const struct iovec *iovec,\n\t\t\t     const struct iovec *fast_iov,\n\t\t\t     struct iov_iter *iter, bool force);\nstatic void io_req_task_queue(struct io_kiocb *req);\nstatic void io_submit_flush_completions(struct io_comp_state *cs,\n\t\t\t\t\tstruct io_ring_ctx *ctx);\n\nstatic struct kmem_cache *req_cachep;\n\nstatic const struct file_operations io_uring_fops;\n\nstruct sock *io_uring_get_socket(struct file *file)\n{\n#if defined(CONFIG_UNIX)\n\tif (file->f_op == &io_uring_fops) {\n\t\tstruct io_ring_ctx *ctx = file->private_data;\n\n\t\treturn ctx->ring_sock->sk;\n\t}\n#endif\n\treturn NULL;\n}\nEXPORT_SYMBOL(io_uring_get_socket);\n\n#define io_for_each_link(pos, head) \\\n\tfor (pos = (head); pos; pos = pos->link)\n\nstatic inline void io_clean_op(struct io_kiocb *req)\n{\n\tif (req->flags & (REQ_F_NEED_CLEANUP | REQ_F_BUFFER_SELECTED))\n\t\t__io_clean_op(req);\n}\n\nstatic inline void io_set_resource_node(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!req->fixed_rsrc_refs) {\n\t\treq->fixed_rsrc_refs = &ctx->file_data->node->refs;\n\t\tpercpu_ref_get(req->fixed_rsrc_refs);\n\t}\n}\n\nstatic bool io_match_task(struct io_kiocb *head,\n\t\t\t  struct task_struct *task,\n\t\t\t  struct files_struct *files)\n{\n\tstruct io_kiocb *req;\n\n\tif (task && head->task != task) {\n\t\t/* in terms of cancelation, always match if req task is dead */\n\t\tif (head->task->flags & PF_EXITING)\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\tif (!files)\n\t\treturn true;\n\n\tio_for_each_link(req, head) {\n\t\tif (req->file && req->file->f_op == &io_uring_fops)\n\t\t\treturn true;\n\t\tif (req->task->files == files)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline void req_set_fail_links(struct io_kiocb *req)\n{\n\tif ((req->flags & (REQ_F_LINK | REQ_F_HARDLINK)) == REQ_F_LINK)\n\t\treq->flags |= REQ_F_FAIL_LINK;\n}\n\nstatic void io_ring_ctx_ref_free(struct percpu_ref *ref)\n{\n\tstruct io_ring_ctx *ctx = container_of(ref, struct io_ring_ctx, refs);\n\n\tcomplete(&ctx->ref_comp);\n}\n\nstatic inline bool io_is_timeout_noseq(struct io_kiocb *req)\n{\n\treturn !req->timeout.off;\n}\n\nstatic struct io_ring_ctx *io_ring_ctx_alloc(struct io_uring_params *p)\n{\n\tstruct io_ring_ctx *ctx;\n\tint hash_bits;\n\n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn NULL;\n\n\t/*\n\t * Use 5 bits less than the max cq entries, that should give us around\n\t * 32 entries per hash list if totally full and uniformly spread.\n\t */\n\thash_bits = ilog2(p->cq_entries);\n\thash_bits -= 5;\n\tif (hash_bits <= 0)\n\t\thash_bits = 1;\n\tctx->cancel_hash_bits = hash_bits;\n\tctx->cancel_hash = kmalloc((1U << hash_bits) * sizeof(struct hlist_head),\n\t\t\t\t\tGFP_KERNEL);\n\tif (!ctx->cancel_hash)\n\t\tgoto err;\n\t__hash_init(ctx->cancel_hash, 1U << hash_bits);\n\n\tif (percpu_ref_init(&ctx->refs, io_ring_ctx_ref_free,\n\t\t\t    PERCPU_REF_ALLOW_REINIT, GFP_KERNEL))\n\t\tgoto err;\n\n\tctx->flags = p->flags;\n\tinit_waitqueue_head(&ctx->sqo_sq_wait);\n\tINIT_LIST_HEAD(&ctx->sqd_list);\n\tinit_waitqueue_head(&ctx->cq_wait);\n\tINIT_LIST_HEAD(&ctx->cq_overflow_list);\n\tinit_completion(&ctx->ref_comp);\n\tinit_completion(&ctx->sq_thread_comp);\n\tidr_init(&ctx->io_buffer_idr);\n\tidr_init(&ctx->personality_idr);\n\tmutex_init(&ctx->uring_lock);\n\tinit_waitqueue_head(&ctx->wait);\n\tspin_lock_init(&ctx->completion_lock);\n\tINIT_LIST_HEAD(&ctx->iopoll_list);\n\tINIT_LIST_HEAD(&ctx->defer_list);\n\tINIT_LIST_HEAD(&ctx->timeout_list);\n\tspin_lock_init(&ctx->inflight_lock);\n\tINIT_LIST_HEAD(&ctx->inflight_list);\n\tspin_lock_init(&ctx->rsrc_ref_lock);\n\tINIT_LIST_HEAD(&ctx->rsrc_ref_list);\n\tINIT_DELAYED_WORK(&ctx->rsrc_put_work, io_rsrc_put_work);\n\tinit_llist_head(&ctx->rsrc_put_llist);\n\tINIT_LIST_HEAD(&ctx->submit_state.comp.free_list);\n\tINIT_LIST_HEAD(&ctx->submit_state.comp.locked_free_list);\n\treturn ctx;\nerr:\n\tkfree(ctx->cancel_hash);\n\tkfree(ctx);\n\treturn NULL;\n}\n\nstatic bool req_need_defer(struct io_kiocb *req, u32 seq)\n{\n\tif (unlikely(req->flags & REQ_F_IO_DRAIN)) {\n\t\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t\treturn seq != ctx->cached_cq_tail\n\t\t\t\t+ READ_ONCE(ctx->cached_cq_overflow);\n\t}\n\n\treturn false;\n}\n\nstatic void io_req_track_inflight(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!(req->flags & REQ_F_INFLIGHT)) {\n\t\treq->flags |= REQ_F_INFLIGHT;\n\n\t\tspin_lock_irq(&ctx->inflight_lock);\n\t\tlist_add(&req->inflight_entry, &ctx->inflight_list);\n\t\tspin_unlock_irq(&ctx->inflight_lock);\n\t}\n}\n\nstatic void io_prep_async_work(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (req->flags & REQ_F_FORCE_ASYNC)\n\t\treq->work.flags |= IO_WQ_WORK_CONCURRENT;\n\n\tif (req->flags & REQ_F_ISREG) {\n\t\tif (def->hash_reg_file || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tio_wq_hash_work(&req->work, file_inode(req->file));\n\t} else {\n\t\tif (def->unbound_nonreg_file)\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n}\n\nstatic void io_prep_async_link(struct io_kiocb *req)\n{\n\tstruct io_kiocb *cur;\n\n\tio_for_each_link(cur, req)\n\t\tio_prep_async_work(cur);\n}\n\nstatic void io_queue_async_work(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_kiocb *link = io_prep_linked_timeout(req);\n\tstruct io_uring_task *tctx = req->task->io_uring;\n\n\tBUG_ON(!tctx);\n\tBUG_ON(!tctx->io_wq);\n\n\ttrace_io_uring_queue_async_work(ctx, io_wq_is_hashed(&req->work), req,\n\t\t\t\t\t&req->work, req->flags);\n\t/* init ->work of the whole link before punting */\n\tio_prep_async_link(req);\n\tio_wq_enqueue(tctx->io_wq, &req->work);\n\tif (link)\n\t\tio_queue_linked_timeout(link);\n}\n\nstatic void io_kill_timeout(struct io_kiocb *req)\n{\n\tstruct io_timeout_data *io = req->async_data;\n\tint ret;\n\n\tret = hrtimer_try_to_cancel(&io->timer);\n\tif (ret != -1) {\n\t\tatomic_set(&req->ctx->cq_timeouts,\n\t\t\tatomic_read(&req->ctx->cq_timeouts) + 1);\n\t\tlist_del_init(&req->timeout.list);\n\t\tio_cqring_fill_event(req, 0);\n\t\tio_put_req_deferred(req, 1);\n\t}\n}\n\n/*\n * Returns true if we found and killed one or more timeouts\n */\nstatic bool io_kill_timeouts(struct io_ring_ctx *ctx, struct task_struct *tsk,\n\t\t\t     struct files_struct *files)\n{\n\tstruct io_kiocb *req, *tmp;\n\tint canceled = 0;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tlist_for_each_entry_safe(req, tmp, &ctx->timeout_list, timeout.list) {\n\t\tif (io_match_task(req, tsk, files)) {\n\t\t\tio_kill_timeout(req);\n\t\t\tcanceled++;\n\t\t}\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\treturn canceled != 0;\n}\n\nstatic void __io_queue_deferred(struct io_ring_ctx *ctx)\n{\n\tdo {\n\t\tstruct io_defer_entry *de = list_first_entry(&ctx->defer_list,\n\t\t\t\t\t\tstruct io_defer_entry, list);\n\n\t\tif (req_need_defer(de->req, de->seq))\n\t\t\tbreak;\n\t\tlist_del_init(&de->list);\n\t\tio_req_task_queue(de->req);\n\t\tkfree(de);\n\t} while (!list_empty(&ctx->defer_list));\n}\n\nstatic void io_flush_timeouts(struct io_ring_ctx *ctx)\n{\n\tu32 seq;\n\n\tif (list_empty(&ctx->timeout_list))\n\t\treturn;\n\n\tseq = ctx->cached_cq_tail - atomic_read(&ctx->cq_timeouts);\n\n\tdo {\n\t\tu32 events_needed, events_got;\n\t\tstruct io_kiocb *req = list_first_entry(&ctx->timeout_list,\n\t\t\t\t\t\tstruct io_kiocb, timeout.list);\n\n\t\tif (io_is_timeout_noseq(req))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Since seq can easily wrap around over time, subtract\n\t\t * the last seq at which timeouts were flushed before comparing.\n\t\t * Assuming not more than 2^31-1 events have happened since,\n\t\t * these subtractions won't have wrapped, so we can check if\n\t\t * target is in [last_seq, current_seq] by comparing the two.\n\t\t */\n\t\tevents_needed = req->timeout.target_seq - ctx->cq_last_tm_flush;\n\t\tevents_got = seq - ctx->cq_last_tm_flush;\n\t\tif (events_got < events_needed)\n\t\t\tbreak;\n\n\t\tlist_del_init(&req->timeout.list);\n\t\tio_kill_timeout(req);\n\t} while (!list_empty(&ctx->timeout_list));\n\n\tctx->cq_last_tm_flush = seq;\n}\n\nstatic void io_commit_cqring(struct io_ring_ctx *ctx)\n{\n\tio_flush_timeouts(ctx);\n\n\t/* order cqe stores with ring update */\n\tsmp_store_release(&ctx->rings->cq.tail, ctx->cached_cq_tail);\n\n\tif (unlikely(!list_empty(&ctx->defer_list)))\n\t\t__io_queue_deferred(ctx);\n}\n\nstatic inline bool io_sqring_full(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *r = ctx->rings;\n\n\treturn READ_ONCE(r->sq.tail) - ctx->cached_sq_head == r->sq_ring_entries;\n}\n\nstatic inline unsigned int __io_cqring_events(struct io_ring_ctx *ctx)\n{\n\treturn ctx->cached_cq_tail - READ_ONCE(ctx->rings->cq.head);\n}\n\nstatic struct io_uring_cqe *io_get_cqring(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *rings = ctx->rings;\n\tunsigned tail;\n\n\t/*\n\t * writes to the cq entry need to come after reading head; the\n\t * control dependency is enough as we're using WRITE_ONCE to\n\t * fill the cq entry\n\t */\n\tif (__io_cqring_events(ctx) == rings->cq_ring_entries)\n\t\treturn NULL;\n\n\ttail = ctx->cached_cq_tail++;\n\treturn &rings->cqes[tail & ctx->cq_mask];\n}\n\nstatic inline bool io_should_trigger_evfd(struct io_ring_ctx *ctx)\n{\n\tif (!ctx->cq_ev_fd)\n\t\treturn false;\n\tif (READ_ONCE(ctx->rings->cq_flags) & IORING_CQ_EVENTFD_DISABLED)\n\t\treturn false;\n\tif (!ctx->eventfd_async)\n\t\treturn true;\n\treturn io_wq_current_is_worker();\n}\n\nstatic void io_cqring_ev_posted(struct io_ring_ctx *ctx)\n{\n\t/* see waitqueue_active() comment */\n\tsmp_mb();\n\n\tif (waitqueue_active(&ctx->wait))\n\t\twake_up(&ctx->wait);\n\tif (ctx->sq_data && waitqueue_active(&ctx->sq_data->wait))\n\t\twake_up(&ctx->sq_data->wait);\n\tif (io_should_trigger_evfd(ctx))\n\t\teventfd_signal(ctx->cq_ev_fd, 1);\n\tif (waitqueue_active(&ctx->cq_wait)) {\n\t\twake_up_interruptible(&ctx->cq_wait);\n\t\tkill_fasync(&ctx->cq_fasync, SIGIO, POLL_IN);\n\t}\n}\n\nstatic void io_cqring_ev_posted_iopoll(struct io_ring_ctx *ctx)\n{\n\t/* see waitqueue_active() comment */\n\tsmp_mb();\n\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\tif (waitqueue_active(&ctx->wait))\n\t\t\twake_up(&ctx->wait);\n\t}\n\tif (io_should_trigger_evfd(ctx))\n\t\teventfd_signal(ctx->cq_ev_fd, 1);\n\tif (waitqueue_active(&ctx->cq_wait)) {\n\t\twake_up_interruptible(&ctx->cq_wait);\n\t\tkill_fasync(&ctx->cq_fasync, SIGIO, POLL_IN);\n\t}\n}\n\n/* Returns true if there are no backlogged entries after the flush */\nstatic bool __io_cqring_overflow_flush(struct io_ring_ctx *ctx, bool force,\n\t\t\t\t       struct task_struct *tsk,\n\t\t\t\t       struct files_struct *files)\n{\n\tstruct io_rings *rings = ctx->rings;\n\tstruct io_kiocb *req, *tmp;\n\tstruct io_uring_cqe *cqe;\n\tunsigned long flags;\n\tbool all_flushed, posted;\n\tLIST_HEAD(list);\n\n\tif (!force && __io_cqring_events(ctx) == rings->cq_ring_entries)\n\t\treturn false;\n\n\tposted = false;\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tlist_for_each_entry_safe(req, tmp, &ctx->cq_overflow_list, compl.list) {\n\t\tif (!io_match_task(req, tsk, files))\n\t\t\tcontinue;\n\n\t\tcqe = io_get_cqring(ctx);\n\t\tif (!cqe && !force)\n\t\t\tbreak;\n\n\t\tlist_move(&req->compl.list, &list);\n\t\tif (cqe) {\n\t\t\tWRITE_ONCE(cqe->user_data, req->user_data);\n\t\t\tWRITE_ONCE(cqe->res, req->result);\n\t\t\tWRITE_ONCE(cqe->flags, req->compl.cflags);\n\t\t} else {\n\t\t\tctx->cached_cq_overflow++;\n\t\t\tWRITE_ONCE(ctx->rings->cq_overflow,\n\t\t\t\t   ctx->cached_cq_overflow);\n\t\t}\n\t\tposted = true;\n\t}\n\n\tall_flushed = list_empty(&ctx->cq_overflow_list);\n\tif (all_flushed) {\n\t\tclear_bit(0, &ctx->sq_check_overflow);\n\t\tclear_bit(0, &ctx->cq_check_overflow);\n\t\tctx->rings->sq_flags &= ~IORING_SQ_CQ_OVERFLOW;\n\t}\n\n\tif (posted)\n\t\tio_commit_cqring(ctx);\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\tif (posted)\n\t\tio_cqring_ev_posted(ctx);\n\n\twhile (!list_empty(&list)) {\n\t\treq = list_first_entry(&list, struct io_kiocb, compl.list);\n\t\tlist_del(&req->compl.list);\n\t\tio_put_req(req);\n\t}\n\n\treturn all_flushed;\n}\n\nstatic void io_cqring_overflow_flush(struct io_ring_ctx *ctx, bool force,\n\t\t\t\t     struct task_struct *tsk,\n\t\t\t\t     struct files_struct *files)\n{\n\tif (test_bit(0, &ctx->cq_check_overflow)) {\n\t\t/* iopoll syncs against uring_lock, not completion_lock */\n\t\tif (ctx->flags & IORING_SETUP_IOPOLL)\n\t\t\tmutex_lock(&ctx->uring_lock);\n\t\t__io_cqring_overflow_flush(ctx, force, tsk, files);\n\t\tif (ctx->flags & IORING_SETUP_IOPOLL)\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t}\n}\n\nstatic void __io_cqring_fill_event(struct io_kiocb *req, long res, long cflags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_uring_cqe *cqe;\n\n\ttrace_io_uring_complete(ctx, req->user_data, res);\n\n\t/*\n\t * If we can't get a cq entry, userspace overflowed the\n\t * submission (by quite a lot). Increment the overflow count in\n\t * the ring.\n\t */\n\tcqe = io_get_cqring(ctx);\n\tif (likely(cqe)) {\n\t\tWRITE_ONCE(cqe->user_data, req->user_data);\n\t\tWRITE_ONCE(cqe->res, res);\n\t\tWRITE_ONCE(cqe->flags, cflags);\n\t} else if (ctx->cq_overflow_flushed ||\n\t\t   atomic_read(&req->task->io_uring->in_idle)) {\n\t\t/*\n\t\t * If we're in ring overflow flush mode, or in task cancel mode,\n\t\t * then we cannot store the request for later flushing, we need\n\t\t * to drop it on the floor.\n\t\t */\n\t\tctx->cached_cq_overflow++;\n\t\tWRITE_ONCE(ctx->rings->cq_overflow, ctx->cached_cq_overflow);\n\t} else {\n\t\tif (list_empty(&ctx->cq_overflow_list)) {\n\t\t\tset_bit(0, &ctx->sq_check_overflow);\n\t\t\tset_bit(0, &ctx->cq_check_overflow);\n\t\t\tctx->rings->sq_flags |= IORING_SQ_CQ_OVERFLOW;\n\t\t}\n\t\tio_clean_op(req);\n\t\treq->result = res;\n\t\treq->compl.cflags = cflags;\n\t\trefcount_inc(&req->refs);\n\t\tlist_add_tail(&req->compl.list, &ctx->cq_overflow_list);\n\t}\n}\n\nstatic void io_cqring_fill_event(struct io_kiocb *req, long res)\n{\n\t__io_cqring_fill_event(req, res, 0);\n}\n\nstatic inline void io_req_complete_post(struct io_kiocb *req, long res,\n\t\t\t\t\tunsigned int cflags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\t__io_cqring_fill_event(req, res, cflags);\n\tio_commit_cqring(ctx);\n\t/*\n\t * If we're the last reference to this request, add to our locked\n\t * free_list cache.\n\t */\n\tif (refcount_dec_and_test(&req->refs)) {\n\t\tstruct io_comp_state *cs = &ctx->submit_state.comp;\n\n\t\tio_dismantle_req(req);\n\t\tio_put_task(req->task, 1);\n\t\tlist_add(&req->compl.list, &cs->locked_free_list);\n\t\tcs->locked_free_nr++;\n\t} else\n\t\treq = NULL;\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\tio_cqring_ev_posted(ctx);\n\tif (req) {\n\t\tio_queue_next(req);\n\t\tpercpu_ref_put(&ctx->refs);\n\t}\n}\n\nstatic void io_req_complete_state(struct io_kiocb *req, long res,\n\t\t\t\t  unsigned int cflags)\n{\n\tio_clean_op(req);\n\treq->result = res;\n\treq->compl.cflags = cflags;\n\treq->flags |= REQ_F_COMPLETE_INLINE;\n}\n\nstatic inline void __io_req_complete(struct io_kiocb *req, unsigned issue_flags,\n\t\t\t\t     long res, unsigned cflags)\n{\n\tif (issue_flags & IO_URING_F_COMPLETE_DEFER)\n\t\tio_req_complete_state(req, res, cflags);\n\telse\n\t\tio_req_complete_post(req, res, cflags);\n}\n\nstatic inline void io_req_complete(struct io_kiocb *req, long res)\n{\n\t__io_req_complete(req, 0, res, 0);\n}\n\nstatic bool io_flush_cached_reqs(struct io_ring_ctx *ctx)\n{\n\tstruct io_submit_state *state = &ctx->submit_state;\n\tstruct io_comp_state *cs = &state->comp;\n\tstruct io_kiocb *req = NULL;\n\n\t/*\n\t * If we have more than a batch's worth of requests in our IRQ side\n\t * locked cache, grab the lock and move them over to our submission\n\t * side cache.\n\t */\n\tif (READ_ONCE(cs->locked_free_nr) > IO_COMPL_BATCH) {\n\t\tspin_lock_irq(&ctx->completion_lock);\n\t\tlist_splice_init(&cs->locked_free_list, &cs->free_list);\n\t\tcs->locked_free_nr = 0;\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t}\n\n\twhile (!list_empty(&cs->free_list)) {\n\t\treq = list_first_entry(&cs->free_list, struct io_kiocb,\n\t\t\t\t\tcompl.list);\n\t\tlist_del(&req->compl.list);\n\t\tstate->reqs[state->free_reqs++] = req;\n\t\tif (state->free_reqs == ARRAY_SIZE(state->reqs))\n\t\t\tbreak;\n\t}\n\n\treturn req != NULL;\n}\n\nstatic struct io_kiocb *io_alloc_req(struct io_ring_ctx *ctx)\n{\n\tstruct io_submit_state *state = &ctx->submit_state;\n\n\tBUILD_BUG_ON(IO_REQ_ALLOC_BATCH > ARRAY_SIZE(state->reqs));\n\n\tif (!state->free_reqs) {\n\t\tgfp_t gfp = GFP_KERNEL | __GFP_NOWARN;\n\t\tint ret;\n\n\t\tif (io_flush_cached_reqs(ctx))\n\t\t\tgoto got_req;\n\n\t\tret = kmem_cache_alloc_bulk(req_cachep, gfp, IO_REQ_ALLOC_BATCH,\n\t\t\t\t\t    state->reqs);\n\n\t\t/*\n\t\t * Bulk alloc is all-or-nothing. If we fail to get a batch,\n\t\t * retry single alloc to be on the safe side.\n\t\t */\n\t\tif (unlikely(ret <= 0)) {\n\t\t\tstate->reqs[0] = kmem_cache_alloc(req_cachep, gfp);\n\t\t\tif (!state->reqs[0])\n\t\t\t\treturn NULL;\n\t\t\tret = 1;\n\t\t}\n\t\tstate->free_reqs = ret;\n\t}\ngot_req:\n\tstate->free_reqs--;\n\treturn state->reqs[state->free_reqs];\n}\n\nstatic inline void io_put_file(struct io_kiocb *req, struct file *file,\n\t\t\t  bool fixed)\n{\n\tif (!fixed)\n\t\tfput(file);\n}\n\nstatic void io_dismantle_req(struct io_kiocb *req)\n{\n\tio_clean_op(req);\n\n\tif (req->async_data)\n\t\tkfree(req->async_data);\n\tif (req->file)\n\t\tio_put_file(req, req->file, (req->flags & REQ_F_FIXED_FILE));\n\tif (req->fixed_rsrc_refs)\n\t\tpercpu_ref_put(req->fixed_rsrc_refs);\n\n\tif (req->flags & REQ_F_INFLIGHT) {\n\t\tstruct io_ring_ctx *ctx = req->ctx;\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&ctx->inflight_lock, flags);\n\t\tlist_del(&req->inflight_entry);\n\t\tspin_unlock_irqrestore(&ctx->inflight_lock, flags);\n\t\treq->flags &= ~REQ_F_INFLIGHT;\n\t}\n}\n\n/* must to be called somewhat shortly after putting a request */\nstatic inline void io_put_task(struct task_struct *task, int nr)\n{\n\tstruct io_uring_task *tctx = task->io_uring;\n\n\tpercpu_counter_sub(&tctx->inflight, nr);\n\tif (unlikely(atomic_read(&tctx->in_idle)))\n\t\twake_up(&tctx->wait);\n\tput_task_struct_many(task, nr);\n}\n\nstatic void __io_free_req(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_dismantle_req(req);\n\tio_put_task(req->task, 1);\n\n\tkmem_cache_free(req_cachep, req);\n\tpercpu_ref_put(&ctx->refs);\n}\n\nstatic inline void io_remove_next_linked(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt = req->link;\n\n\treq->link = nxt->link;\n\tnxt->link = NULL;\n}\n\nstatic void io_kill_linked_timeout(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_kiocb *link;\n\tbool cancelled = false;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tlink = req->link;\n\n\t/*\n\t * Can happen if a linked timeout fired and link had been like\n\t * req -> link t-out -> link t-out [-> ...]\n\t */\n\tif (link && (link->flags & REQ_F_LTIMEOUT_ACTIVE)) {\n\t\tstruct io_timeout_data *io = link->async_data;\n\t\tint ret;\n\n\t\tio_remove_next_linked(req);\n\t\tlink->timeout.head = NULL;\n\t\tret = hrtimer_try_to_cancel(&io->timer);\n\t\tif (ret != -1) {\n\t\t\tio_cqring_fill_event(link, -ECANCELED);\n\t\t\tio_commit_cqring(ctx);\n\t\t\tcancelled = true;\n\t\t}\n\t}\n\treq->flags &= ~REQ_F_LINK_TIMEOUT;\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\tif (cancelled) {\n\t\tio_cqring_ev_posted(ctx);\n\t\tio_put_req(link);\n\t}\n}\n\n\nstatic void io_fail_links(struct io_kiocb *req)\n{\n\tstruct io_kiocb *link, *nxt;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tlink = req->link;\n\treq->link = NULL;\n\n\twhile (link) {\n\t\tnxt = link->link;\n\t\tlink->link = NULL;\n\n\t\ttrace_io_uring_fail_link(req, link);\n\t\tio_cqring_fill_event(link, -ECANCELED);\n\n\t\tio_put_req_deferred(link, 2);\n\t\tlink = nxt;\n\t}\n\tio_commit_cqring(ctx);\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\tio_cqring_ev_posted(ctx);\n}\n\nstatic struct io_kiocb *__io_req_find_next(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_LINK_TIMEOUT)\n\t\tio_kill_linked_timeout(req);\n\n\t/*\n\t * If LINK is set, we have dependent requests in this chain. If we\n\t * didn't fail this request, queue the first one up, moving any other\n\t * dependencies to the next request. In case of failure, fail the rest\n\t * of the chain.\n\t */\n\tif (likely(!(req->flags & REQ_F_FAIL_LINK))) {\n\t\tstruct io_kiocb *nxt = req->link;\n\n\t\treq->link = NULL;\n\t\treturn nxt;\n\t}\n\tio_fail_links(req);\n\treturn NULL;\n}\n\nstatic inline struct io_kiocb *io_req_find_next(struct io_kiocb *req)\n{\n\tif (likely(!(req->flags & (REQ_F_LINK|REQ_F_HARDLINK))))\n\t\treturn NULL;\n\treturn __io_req_find_next(req);\n}\n\nstatic void ctx_flush_and_put(struct io_ring_ctx *ctx)\n{\n\tif (!ctx)\n\t\treturn;\n\tif (ctx->submit_state.comp.nr) {\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tio_submit_flush_completions(&ctx->submit_state.comp, ctx);\n\t\tmutex_unlock(&ctx->uring_lock);\n\t}\n\tpercpu_ref_put(&ctx->refs);\n}\n\nstatic bool __tctx_task_work(struct io_uring_task *tctx)\n{\n\tstruct io_ring_ctx *ctx = NULL;\n\tstruct io_wq_work_list list;\n\tstruct io_wq_work_node *node;\n\n\tif (wq_list_empty(&tctx->task_list))\n\t\treturn false;\n\n\tspin_lock_irq(&tctx->task_lock);\n\tlist = tctx->task_list;\n\tINIT_WQ_LIST(&tctx->task_list);\n\tspin_unlock_irq(&tctx->task_lock);\n\n\tnode = list.first;\n\twhile (node) {\n\t\tstruct io_wq_work_node *next = node->next;\n\t\tstruct io_kiocb *req;\n\n\t\treq = container_of(node, struct io_kiocb, io_task_work.node);\n\t\tif (req->ctx != ctx) {\n\t\t\tctx_flush_and_put(ctx);\n\t\t\tctx = req->ctx;\n\t\t\tpercpu_ref_get(&ctx->refs);\n\t\t}\n\n\t\treq->task_work.func(&req->task_work);\n\t\tnode = next;\n\t}\n\n\tctx_flush_and_put(ctx);\n\treturn list.first != NULL;\n}\n\nstatic void tctx_task_work(struct callback_head *cb)\n{\n\tstruct io_uring_task *tctx = container_of(cb, struct io_uring_task, task_work);\n\n\tclear_bit(0, &tctx->task_state);\n\n\twhile (__tctx_task_work(tctx))\n\t\tcond_resched();\n}\n\nstatic int io_task_work_add(struct task_struct *tsk, struct io_kiocb *req,\n\t\t\t    enum task_work_notify_mode notify)\n{\n\tstruct io_uring_task *tctx = tsk->io_uring;\n\tstruct io_wq_work_node *node, *prev;\n\tunsigned long flags;\n\tint ret;\n\n\tWARN_ON_ONCE(!tctx);\n\n\tspin_lock_irqsave(&tctx->task_lock, flags);\n\twq_list_add_tail(&req->io_task_work.node, &tctx->task_list);\n\tspin_unlock_irqrestore(&tctx->task_lock, flags);\n\n\t/* task_work already pending, we're done */\n\tif (test_bit(0, &tctx->task_state) ||\n\t    test_and_set_bit(0, &tctx->task_state))\n\t\treturn 0;\n\n\tif (!task_work_add(tsk, &tctx->task_work, notify))\n\t\treturn 0;\n\n\t/*\n\t * Slow path - we failed, find and delete work. if the work is not\n\t * in the list, it got run and we're fine.\n\t */\n\tret = 0;\n\tspin_lock_irqsave(&tctx->task_lock, flags);\n\twq_list_for_each(node, prev, &tctx->task_list) {\n\t\tif (&req->io_task_work.node == node) {\n\t\t\twq_list_del(&tctx->task_list, node, prev);\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&tctx->task_lock, flags);\n\tclear_bit(0, &tctx->task_state);\n\treturn ret;\n}\n\nstatic int io_req_task_work_add(struct io_kiocb *req)\n{\n\tstruct task_struct *tsk = req->task;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tenum task_work_notify_mode notify;\n\tint ret;\n\n\tif (tsk->flags & PF_EXITING)\n\t\treturn -ESRCH;\n\n\t/*\n\t * SQPOLL kernel thread doesn't need notification, just a wakeup. For\n\t * all other cases, use TWA_SIGNAL unconditionally to ensure we're\n\t * processing task_work. There's no reliable way to tell if TWA_RESUME\n\t * will do the job.\n\t */\n\tnotify = TWA_NONE;\n\tif (!(ctx->flags & IORING_SETUP_SQPOLL))\n\t\tnotify = TWA_SIGNAL;\n\n\tret = io_task_work_add(tsk, req, notify);\n\tif (!ret)\n\t\twake_up_process(tsk);\n\n\treturn ret;\n}\n\nstatic void io_req_task_work_add_fallback(struct io_kiocb *req,\n\t\t\t\t\t  task_work_func_t cb)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct callback_head *head;\n\n\tinit_task_work(&req->task_work, cb);\n\tdo {\n\t\thead = READ_ONCE(ctx->exit_task_work);\n\t\treq->task_work.next = head;\n\t} while (cmpxchg(&ctx->exit_task_work, head, &req->task_work) != head);\n}\n\nstatic void __io_req_task_cancel(struct io_kiocb *req, int error)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tio_cqring_fill_event(req, error);\n\tio_commit_cqring(ctx);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tio_cqring_ev_posted(ctx);\n\treq_set_fail_links(req);\n\tio_double_put_req(req);\n}\n\nstatic void io_req_task_cancel(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tmutex_lock(&ctx->uring_lock);\n\t__io_req_task_cancel(req, req->result);\n\tmutex_unlock(&ctx->uring_lock);\n\tpercpu_ref_put(&ctx->refs);\n}\n\nstatic void __io_req_task_submit(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t/* ctx stays valid until unlock, even if we drop all ours ctx->refs */\n\tmutex_lock(&ctx->uring_lock);\n\tif (!(current->flags & PF_EXITING) && !current->in_execve)\n\t\t__io_queue_sqe(req);\n\telse\n\t\t__io_req_task_cancel(req, -EFAULT);\n\tmutex_unlock(&ctx->uring_lock);\n}\n\nstatic void io_req_task_submit(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\n\t__io_req_task_submit(req);\n}\n\nstatic void io_req_task_queue(struct io_kiocb *req)\n{\n\tint ret;\n\n\treq->task_work.func = io_req_task_submit;\n\tret = io_req_task_work_add(req);\n\tif (unlikely(ret)) {\n\t\treq->result = -ECANCELED;\n\t\tpercpu_ref_get(&req->ctx->refs);\n\t\tio_req_task_work_add_fallback(req, io_req_task_cancel);\n\t}\n}\n\nstatic void io_req_task_queue_fail(struct io_kiocb *req, int ret)\n{\n\tpercpu_ref_get(&req->ctx->refs);\n\treq->result = ret;\n\treq->task_work.func = io_req_task_cancel;\n\n\tif (unlikely(io_req_task_work_add(req)))\n\t\tio_req_task_work_add_fallback(req, io_req_task_cancel);\n}\n\nstatic inline void io_queue_next(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt = io_req_find_next(req);\n\n\tif (nxt)\n\t\tio_req_task_queue(nxt);\n}\n\nstatic void io_free_req(struct io_kiocb *req)\n{\n\tio_queue_next(req);\n\t__io_free_req(req);\n}\n\nstruct req_batch {\n\tstruct task_struct\t*task;\n\tint\t\t\ttask_refs;\n\tint\t\t\tctx_refs;\n};\n\nstatic inline void io_init_req_batch(struct req_batch *rb)\n{\n\trb->task_refs = 0;\n\trb->ctx_refs = 0;\n\trb->task = NULL;\n}\n\nstatic void io_req_free_batch_finish(struct io_ring_ctx *ctx,\n\t\t\t\t     struct req_batch *rb)\n{\n\tif (rb->task)\n\t\tio_put_task(rb->task, rb->task_refs);\n\tif (rb->ctx_refs)\n\t\tpercpu_ref_put_many(&ctx->refs, rb->ctx_refs);\n}\n\nstatic void io_req_free_batch(struct req_batch *rb, struct io_kiocb *req,\n\t\t\t      struct io_submit_state *state)\n{\n\tio_queue_next(req);\n\n\tif (req->task != rb->task) {\n\t\tif (rb->task)\n\t\t\tio_put_task(rb->task, rb->task_refs);\n\t\trb->task = req->task;\n\t\trb->task_refs = 0;\n\t}\n\trb->task_refs++;\n\trb->ctx_refs++;\n\n\tio_dismantle_req(req);\n\tif (state->free_reqs != ARRAY_SIZE(state->reqs))\n\t\tstate->reqs[state->free_reqs++] = req;\n\telse\n\t\tlist_add(&req->compl.list, &state->comp.free_list);\n}\n\nstatic void io_submit_flush_completions(struct io_comp_state *cs,\n\t\t\t\t\tstruct io_ring_ctx *ctx)\n{\n\tint i, nr = cs->nr;\n\tstruct io_kiocb *req;\n\tstruct req_batch rb;\n\n\tio_init_req_batch(&rb);\n\tspin_lock_irq(&ctx->completion_lock);\n\tfor (i = 0; i < nr; i++) {\n\t\treq = cs->reqs[i];\n\t\t__io_cqring_fill_event(req, req->result, req->compl.cflags);\n\t}\n\tio_commit_cqring(ctx);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tio_cqring_ev_posted(ctx);\n\tfor (i = 0; i < nr; i++) {\n\t\treq = cs->reqs[i];\n\n\t\t/* submission and completion refs */\n\t\tif (refcount_sub_and_test(2, &req->refs))\n\t\t\tio_req_free_batch(&rb, req, &ctx->submit_state);\n\t}\n\n\tio_req_free_batch_finish(ctx, &rb);\n\tcs->nr = 0;\n}\n\n/*\n * Drop reference to request, return next in chain (if there is one) if this\n * was the last reference to this request.\n */\nstatic struct io_kiocb *io_put_req_find_next(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt = NULL;\n\n\tif (refcount_dec_and_test(&req->refs)) {\n\t\tnxt = io_req_find_next(req);\n\t\t__io_free_req(req);\n\t}\n\treturn nxt;\n}\n\nstatic void io_put_req(struct io_kiocb *req)\n{\n\tif (refcount_dec_and_test(&req->refs))\n\t\tio_free_req(req);\n}\n\nstatic void io_put_req_deferred_cb(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\n\tio_free_req(req);\n}\n\nstatic void io_free_req_deferred(struct io_kiocb *req)\n{\n\tint ret;\n\n\treq->task_work.func = io_put_req_deferred_cb;\n\tret = io_req_task_work_add(req);\n\tif (unlikely(ret))\n\t\tio_req_task_work_add_fallback(req, io_put_req_deferred_cb);\n}\n\nstatic inline void io_put_req_deferred(struct io_kiocb *req, int refs)\n{\n\tif (refcount_sub_and_test(refs, &req->refs))\n\t\tio_free_req_deferred(req);\n}\n\nstatic void io_double_put_req(struct io_kiocb *req)\n{\n\t/* drop both submit and complete references */\n\tif (refcount_sub_and_test(2, &req->refs))\n\t\tio_free_req(req);\n}\n\nstatic unsigned io_cqring_events(struct io_ring_ctx *ctx)\n{\n\t/* See comment at the top of this file */\n\tsmp_rmb();\n\treturn __io_cqring_events(ctx);\n}\n\nstatic inline unsigned int io_sqring_entries(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *rings = ctx->rings;\n\n\t/* make sure SQ entry isn't read before tail */\n\treturn smp_load_acquire(&rings->sq.tail) - ctx->cached_sq_head;\n}\n\nstatic unsigned int io_put_kbuf(struct io_kiocb *req, struct io_buffer *kbuf)\n{\n\tunsigned int cflags;\n\n\tcflags = kbuf->bid << IORING_CQE_BUFFER_SHIFT;\n\tcflags |= IORING_CQE_F_BUFFER;\n\treq->flags &= ~REQ_F_BUFFER_SELECTED;\n\tkfree(kbuf);\n\treturn cflags;\n}\n\nstatic inline unsigned int io_put_rw_kbuf(struct io_kiocb *req)\n{\n\tstruct io_buffer *kbuf;\n\n\tkbuf = (struct io_buffer *) (unsigned long) req->rw.addr;\n\treturn io_put_kbuf(req, kbuf);\n}\n\nstatic inline bool io_run_task_work(void)\n{\n\t/*\n\t * Not safe to run on exiting task, and the task_work handling will\n\t * not add work to such a task.\n\t */\n\tif (unlikely(current->flags & PF_EXITING))\n\t\treturn false;\n\tif (current->task_works) {\n\t\t__set_current_state(TASK_RUNNING);\n\t\ttask_work_run();\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * Find and free completed poll iocbs\n */\nstatic void io_iopoll_complete(struct io_ring_ctx *ctx, unsigned int *nr_events,\n\t\t\t       struct list_head *done)\n{\n\tstruct req_batch rb;\n\tstruct io_kiocb *req;\n\n\t/* order with ->result store in io_complete_rw_iopoll() */\n\tsmp_rmb();\n\n\tio_init_req_batch(&rb);\n\twhile (!list_empty(done)) {\n\t\tint cflags = 0;\n\n\t\treq = list_first_entry(done, struct io_kiocb, inflight_entry);\n\t\tlist_del(&req->inflight_entry);\n\n\t\tif (READ_ONCE(req->result) == -EAGAIN) {\n\t\t\treq->iopoll_completed = 0;\n\t\t\tif (io_rw_reissue(req))\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tif (req->flags & REQ_F_BUFFER_SELECTED)\n\t\t\tcflags = io_put_rw_kbuf(req);\n\n\t\t__io_cqring_fill_event(req, req->result, cflags);\n\t\t(*nr_events)++;\n\n\t\tif (refcount_dec_and_test(&req->refs))\n\t\t\tio_req_free_batch(&rb, req, &ctx->submit_state);\n\t}\n\n\tio_commit_cqring(ctx);\n\tio_cqring_ev_posted_iopoll(ctx);\n\tio_req_free_batch_finish(ctx, &rb);\n}\n\nstatic int io_do_iopoll(struct io_ring_ctx *ctx, unsigned int *nr_events,\n\t\t\tlong min)\n{\n\tstruct io_kiocb *req, *tmp;\n\tLIST_HEAD(done);\n\tbool spin;\n\tint ret;\n\n\t/*\n\t * Only spin for completions if we don't have multiple devices hanging\n\t * off our complete list, and we're under the requested amount.\n\t */\n\tspin = !ctx->poll_multi_file && *nr_events < min;\n\n\tret = 0;\n\tlist_for_each_entry_safe(req, tmp, &ctx->iopoll_list, inflight_entry) {\n\t\tstruct kiocb *kiocb = &req->rw.kiocb;\n\n\t\t/*\n\t\t * Move completed and retryable entries to our local lists.\n\t\t * If we find a request that requires polling, break out\n\t\t * and complete those lists first, if we have entries there.\n\t\t */\n\t\tif (READ_ONCE(req->iopoll_completed)) {\n\t\t\tlist_move_tail(&req->inflight_entry, &done);\n\t\t\tcontinue;\n\t\t}\n\t\tif (!list_empty(&done))\n\t\t\tbreak;\n\n\t\tret = kiocb->ki_filp->f_op->iopoll(kiocb, spin);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\n\t\t/* iopoll may have completed current req */\n\t\tif (READ_ONCE(req->iopoll_completed))\n\t\t\tlist_move_tail(&req->inflight_entry, &done);\n\n\t\tif (ret && spin)\n\t\t\tspin = false;\n\t\tret = 0;\n\t}\n\n\tif (!list_empty(&done))\n\t\tio_iopoll_complete(ctx, nr_events, &done);\n\n\treturn ret;\n}\n\n/*\n * Poll for a minimum of 'min' events. Note that if min == 0 we consider that a\n * non-spinning poll check - we'll still enter the driver poll loop, but only\n * as a non-spinning completion check.\n */\nstatic int io_iopoll_getevents(struct io_ring_ctx *ctx, unsigned int *nr_events,\n\t\t\t\tlong min)\n{\n\twhile (!list_empty(&ctx->iopoll_list) && !need_resched()) {\n\t\tint ret;\n\n\t\tret = io_do_iopoll(ctx, nr_events, min);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tif (*nr_events >= min)\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\n/*\n * We can't just wait for polled events to come to us, we have to actively\n * find and complete them.\n */\nstatic void io_iopoll_try_reap_events(struct io_ring_ctx *ctx)\n{\n\tif (!(ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn;\n\n\tmutex_lock(&ctx->uring_lock);\n\twhile (!list_empty(&ctx->iopoll_list)) {\n\t\tunsigned int nr_events = 0;\n\n\t\tio_do_iopoll(ctx, &nr_events, 0);\n\n\t\t/* let it sleep and repeat later if can't complete a request */\n\t\tif (nr_events == 0)\n\t\t\tbreak;\n\t\t/*\n\t\t * Ensure we allow local-to-the-cpu processing to take place,\n\t\t * in this case we need to ensure that we reap all events.\n\t\t * Also let task_work, etc. to progress by releasing the mutex\n\t\t */\n\t\tif (need_resched()) {\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t\t\tcond_resched();\n\t\t\tmutex_lock(&ctx->uring_lock);\n\t\t}\n\t}\n\tmutex_unlock(&ctx->uring_lock);\n}\n\nstatic int io_iopoll_check(struct io_ring_ctx *ctx, long min)\n{\n\tunsigned int nr_events = 0;\n\tint iters = 0, ret = 0;\n\n\t/*\n\t * We disallow the app entering submit/complete with polling, but we\n\t * still need to lock the ring to prevent racing with polled issue\n\t * that got punted to a workqueue.\n\t */\n\tmutex_lock(&ctx->uring_lock);\n\tdo {\n\t\t/*\n\t\t * Don't enter poll loop if we already have events pending.\n\t\t * If we do, we can potentially be spinning for commands that\n\t\t * already triggered a CQE (eg in error).\n\t\t */\n\t\tif (test_bit(0, &ctx->cq_check_overflow))\n\t\t\t__io_cqring_overflow_flush(ctx, false, NULL, NULL);\n\t\tif (io_cqring_events(ctx))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * If a submit got punted to a workqueue, we can have the\n\t\t * application entering polling for a command before it gets\n\t\t * issued. That app will hold the uring_lock for the duration\n\t\t * of the poll right here, so we need to take a breather every\n\t\t * now and then to ensure that the issue has a chance to add\n\t\t * the poll to the issued list. Otherwise we can spin here\n\t\t * forever, while the workqueue is stuck trying to acquire the\n\t\t * very same mutex.\n\t\t */\n\t\tif (!(++iters & 7)) {\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t\t\tio_run_task_work();\n\t\t\tmutex_lock(&ctx->uring_lock);\n\t\t}\n\n\t\tret = io_iopoll_getevents(ctx, &nr_events, min);\n\t\tif (ret <= 0)\n\t\t\tbreak;\n\t\tret = 0;\n\t} while (min && !nr_events && !need_resched());\n\n\tmutex_unlock(&ctx->uring_lock);\n\treturn ret;\n}\n\nstatic void kiocb_end_write(struct io_kiocb *req)\n{\n\t/*\n\t * Tell lockdep we inherited freeze protection from submission\n\t * thread.\n\t */\n\tif (req->flags & REQ_F_ISREG) {\n\t\tstruct inode *inode = file_inode(req->file);\n\n\t\t__sb_writers_acquired(inode->i_sb, SB_FREEZE_WRITE);\n\t}\n\tfile_end_write(req->file);\n}\n\n#ifdef CONFIG_BLOCK\nstatic bool io_resubmit_prep(struct io_kiocb *req)\n{\n\tstruct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;\n\tint rw, ret;\n\tstruct iov_iter iter;\n\n\t/* already prepared */\n\tif (req->async_data)\n\t\treturn true;\n\n\tswitch (req->opcode) {\n\tcase IORING_OP_READV:\n\tcase IORING_OP_READ_FIXED:\n\tcase IORING_OP_READ:\n\t\trw = READ;\n\t\tbreak;\n\tcase IORING_OP_WRITEV:\n\tcase IORING_OP_WRITE_FIXED:\n\tcase IORING_OP_WRITE:\n\t\trw = WRITE;\n\t\tbreak;\n\tdefault:\n\t\tprintk_once(KERN_WARNING \"io_uring: bad opcode in resubmit %d\\n\",\n\t\t\t\treq->opcode);\n\t\treturn false;\n\t}\n\n\tret = io_import_iovec(rw, req, &iovec, &iter, false);\n\tif (ret < 0)\n\t\treturn false;\n\treturn !io_setup_async_rw(req, iovec, inline_vecs, &iter, false);\n}\n\nstatic bool io_rw_should_reissue(struct io_kiocb *req)\n{\n\tumode_t mode = file_inode(req->file)->i_mode;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!S_ISBLK(mode) && !S_ISREG(mode))\n\t\treturn false;\n\tif ((req->flags & REQ_F_NOWAIT) || (io_wq_current_is_worker() &&\n\t    !(ctx->flags & IORING_SETUP_IOPOLL)))\n\t\treturn false;\n\t/*\n\t * If ref is dying, we might be running poll reap from the exit work.\n\t * Don't attempt to reissue from that path, just let it fail with\n\t * -EAGAIN.\n\t */\n\tif (percpu_ref_is_dying(&ctx->refs))\n\t\treturn false;\n\treturn true;\n}\n#endif\n\nstatic bool io_rw_reissue(struct io_kiocb *req)\n{\n#ifdef CONFIG_BLOCK\n\tif (!io_rw_should_reissue(req))\n\t\treturn false;\n\n\tlockdep_assert_held(&req->ctx->uring_lock);\n\n\tif (io_resubmit_prep(req)) {\n\t\trefcount_inc(&req->refs);\n\t\tio_queue_async_work(req);\n\t\treturn true;\n\t}\n\treq_set_fail_links(req);\n#endif\n\treturn false;\n}\n\nstatic void __io_complete_rw(struct io_kiocb *req, long res, long res2,\n\t\t\t     unsigned int issue_flags)\n{\n\tint cflags = 0;\n\n\tif ((res == -EAGAIN || res == -EOPNOTSUPP) && io_rw_reissue(req))\n\t\treturn;\n\tif (res != req->result)\n\t\treq_set_fail_links(req);\n\n\tif (req->rw.kiocb.ki_flags & IOCB_WRITE)\n\t\tkiocb_end_write(req);\n\tif (req->flags & REQ_F_BUFFER_SELECTED)\n\t\tcflags = io_put_rw_kbuf(req);\n\t__io_req_complete(req, issue_flags, res, cflags);\n}\n\nstatic void io_complete_rw(struct kiocb *kiocb, long res, long res2)\n{\n\tstruct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw.kiocb);\n\n\t__io_complete_rw(req, res, res2, 0);\n}\n\nstatic void io_complete_rw_iopoll(struct kiocb *kiocb, long res, long res2)\n{\n\tstruct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw.kiocb);\n\n#ifdef CONFIG_BLOCK\n\t/* Rewind iter, if we have one. iopoll path resubmits as usual */\n\tif (res == -EAGAIN && io_rw_should_reissue(req)) {\n\t\tstruct io_async_rw *rw = req->async_data;\n\n\t\tif (rw)\n\t\t\tiov_iter_revert(&rw->iter,\n\t\t\t\t\treq->result - iov_iter_count(&rw->iter));\n\t\telse if (!io_resubmit_prep(req))\n\t\t\tres = -EIO;\n\t}\n#endif\n\n\tif (kiocb->ki_flags & IOCB_WRITE)\n\t\tkiocb_end_write(req);\n\n\tif (res != -EAGAIN && res != req->result)\n\t\treq_set_fail_links(req);\n\n\tWRITE_ONCE(req->result, res);\n\t/* order with io_poll_complete() checking ->result */\n\tsmp_wmb();\n\tWRITE_ONCE(req->iopoll_completed, 1);\n}\n\n/*\n * After the iocb has been issued, it's safe to be found on the poll list.\n * Adding the kiocb to the list AFTER submission ensures that we don't\n * find it from a io_iopoll_getevents() thread before the issuer is done\n * accessing the kiocb cookie.\n */\nstatic void io_iopoll_req_issued(struct io_kiocb *req, bool in_async)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t/*\n\t * Track whether we have multiple files in our lists. This will impact\n\t * how we do polling eventually, not spinning if we're on potentially\n\t * different devices.\n\t */\n\tif (list_empty(&ctx->iopoll_list)) {\n\t\tctx->poll_multi_file = false;\n\t} else if (!ctx->poll_multi_file) {\n\t\tstruct io_kiocb *list_req;\n\n\t\tlist_req = list_first_entry(&ctx->iopoll_list, struct io_kiocb,\n\t\t\t\t\t\tinflight_entry);\n\t\tif (list_req->file != req->file)\n\t\t\tctx->poll_multi_file = true;\n\t}\n\n\t/*\n\t * For fast devices, IO may have already completed. If it has, add\n\t * it to the front so we find it first.\n\t */\n\tif (READ_ONCE(req->iopoll_completed))\n\t\tlist_add(&req->inflight_entry, &ctx->iopoll_list);\n\telse\n\t\tlist_add_tail(&req->inflight_entry, &ctx->iopoll_list);\n\n\t/*\n\t * If IORING_SETUP_SQPOLL is enabled, sqes are either handled in sq thread\n\t * task context or in io worker task context. If current task context is\n\t * sq thread, we don't need to check whether should wake up sq thread.\n\t */\n\tif (in_async && (ctx->flags & IORING_SETUP_SQPOLL) &&\n\t    wq_has_sleeper(&ctx->sq_data->wait))\n\t\twake_up(&ctx->sq_data->wait);\n}\n\nstatic inline void io_state_file_put(struct io_submit_state *state)\n{\n\tif (state->file_refs) {\n\t\tfput_many(state->file, state->file_refs);\n\t\tstate->file_refs = 0;\n\t}\n}\n\n/*\n * Get as many references to a file as we have IOs left in this submission,\n * assuming most submissions are for one file, or at least that each file\n * has more than one submission.\n */\nstatic struct file *__io_file_get(struct io_submit_state *state, int fd)\n{\n\tif (!state)\n\t\treturn fget(fd);\n\n\tif (state->file_refs) {\n\t\tif (state->fd == fd) {\n\t\t\tstate->file_refs--;\n\t\t\treturn state->file;\n\t\t}\n\t\tio_state_file_put(state);\n\t}\n\tstate->file = fget_many(fd, state->ios_left);\n\tif (unlikely(!state->file))\n\t\treturn NULL;\n\n\tstate->fd = fd;\n\tstate->file_refs = state->ios_left - 1;\n\treturn state->file;\n}\n\nstatic bool io_bdev_nowait(struct block_device *bdev)\n{\n\treturn !bdev || blk_queue_nowait(bdev_get_queue(bdev));\n}\n\n/*\n * If we tracked the file through the SCM inflight mechanism, we could support\n * any file. For now, just ensure that anything potentially problematic is done\n * inline.\n */\nstatic bool io_file_supports_async(struct file *file, int rw)\n{\n\tumode_t mode = file_inode(file)->i_mode;\n\n\tif (S_ISBLK(mode)) {\n\t\tif (IS_ENABLED(CONFIG_BLOCK) &&\n\t\t    io_bdev_nowait(I_BDEV(file->f_mapping->host)))\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\tif (S_ISCHR(mode) || S_ISSOCK(mode))\n\t\treturn true;\n\tif (S_ISREG(mode)) {\n\t\tif (IS_ENABLED(CONFIG_BLOCK) &&\n\t\t    io_bdev_nowait(file->f_inode->i_sb->s_bdev) &&\n\t\t    file->f_op != &io_uring_fops)\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\n\t/* any ->read/write should understand O_NONBLOCK */\n\tif (file->f_flags & O_NONBLOCK)\n\t\treturn true;\n\n\tif (!(file->f_mode & FMODE_NOWAIT))\n\t\treturn false;\n\n\tif (rw == READ)\n\t\treturn file->f_op->read_iter != NULL;\n\n\treturn file->f_op->write_iter != NULL;\n}\n\nstatic int io_prep_rw(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct kiocb *kiocb = &req->rw.kiocb;\n\tstruct file *file = req->file;\n\tunsigned ioprio;\n\tint ret;\n\n\tif (S_ISREG(file_inode(file)->i_mode))\n\t\treq->flags |= REQ_F_ISREG;\n\n\tkiocb->ki_pos = READ_ONCE(sqe->off);\n\tif (kiocb->ki_pos == -1 && !(file->f_mode & FMODE_STREAM)) {\n\t\treq->flags |= REQ_F_CUR_POS;\n\t\tkiocb->ki_pos = file->f_pos;\n\t}\n\tkiocb->ki_hint = ki_hint_validate(file_write_hint(kiocb->ki_filp));\n\tkiocb->ki_flags = iocb_flags(kiocb->ki_filp);\n\tret = kiocb_set_rw_flags(kiocb, READ_ONCE(sqe->rw_flags));\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\t/* don't allow async punt for O_NONBLOCK or RWF_NOWAIT */\n\tif ((kiocb->ki_flags & IOCB_NOWAIT) || (file->f_flags & O_NONBLOCK))\n\t\treq->flags |= REQ_F_NOWAIT;\n\n\tioprio = READ_ONCE(sqe->ioprio);\n\tif (ioprio) {\n\t\tret = ioprio_check_cap(ioprio);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tkiocb->ki_ioprio = ioprio;\n\t} else\n\t\tkiocb->ki_ioprio = get_current_ioprio();\n\n\tif (ctx->flags & IORING_SETUP_IOPOLL) {\n\t\tif (!(kiocb->ki_flags & IOCB_DIRECT) ||\n\t\t    !kiocb->ki_filp->f_op->iopoll)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tkiocb->ki_flags |= IOCB_HIPRI;\n\t\tkiocb->ki_complete = io_complete_rw_iopoll;\n\t\treq->iopoll_completed = 0;\n\t} else {\n\t\tif (kiocb->ki_flags & IOCB_HIPRI)\n\t\t\treturn -EINVAL;\n\t\tkiocb->ki_complete = io_complete_rw;\n\t}\n\n\treq->rw.addr = READ_ONCE(sqe->addr);\n\treq->rw.len = READ_ONCE(sqe->len);\n\treq->buf_index = READ_ONCE(sqe->buf_index);\n\treturn 0;\n}\n\nstatic inline void io_rw_done(struct kiocb *kiocb, ssize_t ret)\n{\n\tswitch (ret) {\n\tcase -EIOCBQUEUED:\n\t\tbreak;\n\tcase -ERESTARTSYS:\n\tcase -ERESTARTNOINTR:\n\tcase -ERESTARTNOHAND:\n\tcase -ERESTART_RESTARTBLOCK:\n\t\t/*\n\t\t * We can't just restart the syscall, since previously\n\t\t * submitted sqes may already be in progress. Just fail this\n\t\t * IO with EINTR.\n\t\t */\n\t\tret = -EINTR;\n\t\tfallthrough;\n\tdefault:\n\t\tkiocb->ki_complete(kiocb, ret, 0);\n\t}\n}\n\nstatic void kiocb_done(struct kiocb *kiocb, ssize_t ret,\n\t\t       unsigned int issue_flags)\n{\n\tstruct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw.kiocb);\n\tstruct io_async_rw *io = req->async_data;\n\n\t/* add previously done IO, if any */\n\tif (io && io->bytes_done > 0) {\n\t\tif (ret < 0)\n\t\t\tret = io->bytes_done;\n\t\telse\n\t\t\tret += io->bytes_done;\n\t}\n\n\tif (req->flags & REQ_F_CUR_POS)\n\t\treq->file->f_pos = kiocb->ki_pos;\n\tif (ret >= 0 && kiocb->ki_complete == io_complete_rw)\n\t\t__io_complete_rw(req, ret, 0, issue_flags);\n\telse\n\t\tio_rw_done(kiocb, ret);\n}\n\nstatic int io_import_fixed(struct io_kiocb *req, int rw, struct iov_iter *iter)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tsize_t len = req->rw.len;\n\tstruct io_mapped_ubuf *imu;\n\tu16 index, buf_index = req->buf_index;\n\tsize_t offset;\n\tu64 buf_addr;\n\n\tif (unlikely(buf_index >= ctx->nr_user_bufs))\n\t\treturn -EFAULT;\n\tindex = array_index_nospec(buf_index, ctx->nr_user_bufs);\n\timu = &ctx->user_bufs[index];\n\tbuf_addr = req->rw.addr;\n\n\t/* overflow */\n\tif (buf_addr + len < buf_addr)\n\t\treturn -EFAULT;\n\t/* not inside the mapped region */\n\tif (buf_addr < imu->ubuf || buf_addr + len > imu->ubuf + imu->len)\n\t\treturn -EFAULT;\n\n\t/*\n\t * May not be a start of buffer, set size appropriately\n\t * and advance us to the beginning.\n\t */\n\toffset = buf_addr - imu->ubuf;\n\tiov_iter_bvec(iter, rw, imu->bvec, imu->nr_bvecs, offset + len);\n\n\tif (offset) {\n\t\t/*\n\t\t * Don't use iov_iter_advance() here, as it's really slow for\n\t\t * using the latter parts of a big fixed buffer - it iterates\n\t\t * over each segment manually. We can cheat a bit here, because\n\t\t * we know that:\n\t\t *\n\t\t * 1) it's a BVEC iter, we set it up\n\t\t * 2) all bvecs are PAGE_SIZE in size, except potentially the\n\t\t *    first and last bvec\n\t\t *\n\t\t * So just find our index, and adjust the iterator afterwards.\n\t\t * If the offset is within the first bvec (or the whole first\n\t\t * bvec, just use iov_iter_advance(). This makes it easier\n\t\t * since we can just skip the first segment, which may not\n\t\t * be PAGE_SIZE aligned.\n\t\t */\n\t\tconst struct bio_vec *bvec = imu->bvec;\n\n\t\tif (offset <= bvec->bv_len) {\n\t\t\tiov_iter_advance(iter, offset);\n\t\t} else {\n\t\t\tunsigned long seg_skip;\n\n\t\t\t/* skip first vec */\n\t\t\toffset -= bvec->bv_len;\n\t\t\tseg_skip = 1 + (offset >> PAGE_SHIFT);\n\n\t\t\titer->bvec = bvec + seg_skip;\n\t\t\titer->nr_segs -= seg_skip;\n\t\t\titer->count -= bvec->bv_len + offset;\n\t\t\titer->iov_offset = offset & ~PAGE_MASK;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void io_ring_submit_unlock(struct io_ring_ctx *ctx, bool needs_lock)\n{\n\tif (needs_lock)\n\t\tmutex_unlock(&ctx->uring_lock);\n}\n\nstatic void io_ring_submit_lock(struct io_ring_ctx *ctx, bool needs_lock)\n{\n\t/*\n\t * \"Normal\" inline submissions always hold the uring_lock, since we\n\t * grab it from the system call. Same is true for the SQPOLL offload.\n\t * The only exception is when we've detached the request and issue it\n\t * from an async worker thread, grab the lock for that case.\n\t */\n\tif (needs_lock)\n\t\tmutex_lock(&ctx->uring_lock);\n}\n\nstatic struct io_buffer *io_buffer_select(struct io_kiocb *req, size_t *len,\n\t\t\t\t\t  int bgid, struct io_buffer *kbuf,\n\t\t\t\t\t  bool needs_lock)\n{\n\tstruct io_buffer *head;\n\n\tif (req->flags & REQ_F_BUFFER_SELECTED)\n\t\treturn kbuf;\n\n\tio_ring_submit_lock(req->ctx, needs_lock);\n\n\tlockdep_assert_held(&req->ctx->uring_lock);\n\n\thead = idr_find(&req->ctx->io_buffer_idr, bgid);\n\tif (head) {\n\t\tif (!list_empty(&head->list)) {\n\t\t\tkbuf = list_last_entry(&head->list, struct io_buffer,\n\t\t\t\t\t\t\tlist);\n\t\t\tlist_del(&kbuf->list);\n\t\t} else {\n\t\t\tkbuf = head;\n\t\t\tidr_remove(&req->ctx->io_buffer_idr, bgid);\n\t\t}\n\t\tif (*len > kbuf->len)\n\t\t\t*len = kbuf->len;\n\t} else {\n\t\tkbuf = ERR_PTR(-ENOBUFS);\n\t}\n\n\tio_ring_submit_unlock(req->ctx, needs_lock);\n\n\treturn kbuf;\n}\n\nstatic void __user *io_rw_buffer_select(struct io_kiocb *req, size_t *len,\n\t\t\t\t\tbool needs_lock)\n{\n\tstruct io_buffer *kbuf;\n\tu16 bgid;\n\n\tkbuf = (struct io_buffer *) (unsigned long) req->rw.addr;\n\tbgid = req->buf_index;\n\tkbuf = io_buffer_select(req, len, bgid, kbuf, needs_lock);\n\tif (IS_ERR(kbuf))\n\t\treturn kbuf;\n\treq->rw.addr = (u64) (unsigned long) kbuf;\n\treq->flags |= REQ_F_BUFFER_SELECTED;\n\treturn u64_to_user_ptr(kbuf->addr);\n}\n\n#ifdef CONFIG_COMPAT\nstatic ssize_t io_compat_import(struct io_kiocb *req, struct iovec *iov,\n\t\t\t\tbool needs_lock)\n{\n\tstruct compat_iovec __user *uiov;\n\tcompat_ssize_t clen;\n\tvoid __user *buf;\n\tssize_t len;\n\n\tuiov = u64_to_user_ptr(req->rw.addr);\n\tif (!access_ok(uiov, sizeof(*uiov)))\n\t\treturn -EFAULT;\n\tif (__get_user(clen, &uiov->iov_len))\n\t\treturn -EFAULT;\n\tif (clen < 0)\n\t\treturn -EINVAL;\n\n\tlen = clen;\n\tbuf = io_rw_buffer_select(req, &len, needs_lock);\n\tif (IS_ERR(buf))\n\t\treturn PTR_ERR(buf);\n\tiov[0].iov_base = buf;\n\tiov[0].iov_len = (compat_size_t) len;\n\treturn 0;\n}\n#endif\n\nstatic ssize_t __io_iov_buffer_select(struct io_kiocb *req, struct iovec *iov,\n\t\t\t\t      bool needs_lock)\n{\n\tstruct iovec __user *uiov = u64_to_user_ptr(req->rw.addr);\n\tvoid __user *buf;\n\tssize_t len;\n\n\tif (copy_from_user(iov, uiov, sizeof(*uiov)))\n\t\treturn -EFAULT;\n\n\tlen = iov[0].iov_len;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\tbuf = io_rw_buffer_select(req, &len, needs_lock);\n\tif (IS_ERR(buf))\n\t\treturn PTR_ERR(buf);\n\tiov[0].iov_base = buf;\n\tiov[0].iov_len = len;\n\treturn 0;\n}\n\nstatic ssize_t io_iov_buffer_select(struct io_kiocb *req, struct iovec *iov,\n\t\t\t\t    bool needs_lock)\n{\n\tif (req->flags & REQ_F_BUFFER_SELECTED) {\n\t\tstruct io_buffer *kbuf;\n\n\t\tkbuf = (struct io_buffer *) (unsigned long) req->rw.addr;\n\t\tiov[0].iov_base = u64_to_user_ptr(kbuf->addr);\n\t\tiov[0].iov_len = kbuf->len;\n\t\treturn 0;\n\t}\n\tif (req->rw.len != 1)\n\t\treturn -EINVAL;\n\n#ifdef CONFIG_COMPAT\n\tif (req->ctx->compat)\n\t\treturn io_compat_import(req, iov, needs_lock);\n#endif\n\n\treturn __io_iov_buffer_select(req, iov, needs_lock);\n}\n\nstatic int io_import_iovec(int rw, struct io_kiocb *req, struct iovec **iovec,\n\t\t\t   struct iov_iter *iter, bool needs_lock)\n{\n\tvoid __user *buf = u64_to_user_ptr(req->rw.addr);\n\tsize_t sqe_len = req->rw.len;\n\tu8 opcode = req->opcode;\n\tssize_t ret;\n\n\tif (opcode == IORING_OP_READ_FIXED || opcode == IORING_OP_WRITE_FIXED) {\n\t\t*iovec = NULL;\n\t\treturn io_import_fixed(req, rw, iter);\n\t}\n\n\t/* buffer index only valid with fixed read/write, or buffer select  */\n\tif (req->buf_index && !(req->flags & REQ_F_BUFFER_SELECT))\n\t\treturn -EINVAL;\n\n\tif (opcode == IORING_OP_READ || opcode == IORING_OP_WRITE) {\n\t\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\t\tbuf = io_rw_buffer_select(req, &sqe_len, needs_lock);\n\t\t\tif (IS_ERR(buf))\n\t\t\t\treturn PTR_ERR(buf);\n\t\t\treq->rw.len = sqe_len;\n\t\t}\n\n\t\tret = import_single_range(rw, buf, sqe_len, *iovec, iter);\n\t\t*iovec = NULL;\n\t\treturn ret;\n\t}\n\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tret = io_iov_buffer_select(req, *iovec, needs_lock);\n\t\tif (!ret)\n\t\t\tiov_iter_init(iter, rw, *iovec, 1, (*iovec)->iov_len);\n\t\t*iovec = NULL;\n\t\treturn ret;\n\t}\n\n\treturn __import_iovec(rw, buf, sqe_len, UIO_FASTIOV, iovec, iter,\n\t\t\t      req->ctx->compat);\n}\n\nstatic inline loff_t *io_kiocb_ppos(struct kiocb *kiocb)\n{\n\treturn (kiocb->ki_filp->f_mode & FMODE_STREAM) ? NULL : &kiocb->ki_pos;\n}\n\n/*\n * For files that don't have ->read_iter() and ->write_iter(), handle them\n * by looping over ->read() or ->write() manually.\n */\nstatic ssize_t loop_rw_iter(int rw, struct io_kiocb *req, struct iov_iter *iter)\n{\n\tstruct kiocb *kiocb = &req->rw.kiocb;\n\tstruct file *file = req->file;\n\tssize_t ret = 0;\n\n\t/*\n\t * Don't support polled IO through this interface, and we can't\n\t * support non-blocking either. For the latter, this just causes\n\t * the kiocb to be handled from an async context.\n\t */\n\tif (kiocb->ki_flags & IOCB_HIPRI)\n\t\treturn -EOPNOTSUPP;\n\tif (kiocb->ki_flags & IOCB_NOWAIT)\n\t\treturn -EAGAIN;\n\n\twhile (iov_iter_count(iter)) {\n\t\tstruct iovec iovec;\n\t\tssize_t nr;\n\n\t\tif (!iov_iter_is_bvec(iter)) {\n\t\t\tiovec = iov_iter_iovec(iter);\n\t\t} else {\n\t\t\tiovec.iov_base = u64_to_user_ptr(req->rw.addr);\n\t\t\tiovec.iov_len = req->rw.len;\n\t\t}\n\n\t\tif (rw == READ) {\n\t\t\tnr = file->f_op->read(file, iovec.iov_base,\n\t\t\t\t\t      iovec.iov_len, io_kiocb_ppos(kiocb));\n\t\t} else {\n\t\t\tnr = file->f_op->write(file, iovec.iov_base,\n\t\t\t\t\t       iovec.iov_len, io_kiocb_ppos(kiocb));\n\t\t}\n\n\t\tif (nr < 0) {\n\t\t\tif (!ret)\n\t\t\t\tret = nr;\n\t\t\tbreak;\n\t\t}\n\t\tret += nr;\n\t\tif (nr != iovec.iov_len)\n\t\t\tbreak;\n\t\treq->rw.len -= nr;\n\t\treq->rw.addr += nr;\n\t\tiov_iter_advance(iter, nr);\n\t}\n\n\treturn ret;\n}\n\nstatic void io_req_map_rw(struct io_kiocb *req, const struct iovec *iovec,\n\t\t\t  const struct iovec *fast_iov, struct iov_iter *iter)\n{\n\tstruct io_async_rw *rw = req->async_data;\n\n\tmemcpy(&rw->iter, iter, sizeof(*iter));\n\trw->free_iovec = iovec;\n\trw->bytes_done = 0;\n\t/* can only be fixed buffers, no need to do anything */\n\tif (iov_iter_is_bvec(iter))\n\t\treturn;\n\tif (!iovec) {\n\t\tunsigned iov_off = 0;\n\n\t\trw->iter.iov = rw->fast_iov;\n\t\tif (iter->iov != fast_iov) {\n\t\t\tiov_off = iter->iov - fast_iov;\n\t\t\trw->iter.iov += iov_off;\n\t\t}\n\t\tif (rw->fast_iov != fast_iov)\n\t\t\tmemcpy(rw->fast_iov + iov_off, fast_iov + iov_off,\n\t\t\t       sizeof(struct iovec) * iter->nr_segs);\n\t} else {\n\t\treq->flags |= REQ_F_NEED_CLEANUP;\n\t}\n}\n\nstatic inline int __io_alloc_async_data(struct io_kiocb *req)\n{\n\tWARN_ON_ONCE(!io_op_defs[req->opcode].async_size);\n\treq->async_data = kmalloc(io_op_defs[req->opcode].async_size, GFP_KERNEL);\n\treturn req->async_data == NULL;\n}\n\nstatic int io_alloc_async_data(struct io_kiocb *req)\n{\n\tif (!io_op_defs[req->opcode].needs_async_data)\n\t\treturn 0;\n\n\treturn  __io_alloc_async_data(req);\n}\n\nstatic int io_setup_async_rw(struct io_kiocb *req, const struct iovec *iovec,\n\t\t\t     const struct iovec *fast_iov,\n\t\t\t     struct iov_iter *iter, bool force)\n{\n\tif (!force && !io_op_defs[req->opcode].needs_async_data)\n\t\treturn 0;\n\tif (!req->async_data) {\n\t\tif (__io_alloc_async_data(req)) {\n\t\t\tkfree(iovec);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tio_req_map_rw(req, iovec, fast_iov, iter);\n\t}\n\treturn 0;\n}\n\nstatic inline int io_rw_prep_async(struct io_kiocb *req, int rw)\n{\n\tstruct io_async_rw *iorw = req->async_data;\n\tstruct iovec *iov = iorw->fast_iov;\n\tint ret;\n\n\tret = io_import_iovec(rw, req, &iov, &iorw->iter, false);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tiorw->bytes_done = 0;\n\tiorw->free_iovec = iov;\n\tif (iov)\n\t\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn 0;\n}\n\nstatic int io_read_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tif (unlikely(!(req->file->f_mode & FMODE_READ)))\n\t\treturn -EBADF;\n\treturn io_prep_rw(req, sqe);\n}\n\n/*\n * This is our waitqueue callback handler, registered through lock_page_async()\n * when we initially tried to do the IO with the iocb armed our waitqueue.\n * This gets called when the page is unlocked, and we generally expect that to\n * happen when the page IO is completed and the page is now uptodate. This will\n * queue a task_work based retry of the operation, attempting to copy the data\n * again. If the latter fails because the page was NOT uptodate, then we will\n * do a thread based blocking retry of the operation. That's the unexpected\n * slow path.\n */\nstatic int io_async_buf_func(struct wait_queue_entry *wait, unsigned mode,\n\t\t\t     int sync, void *arg)\n{\n\tstruct wait_page_queue *wpq;\n\tstruct io_kiocb *req = wait->private;\n\tstruct wait_page_key *key = arg;\n\n\twpq = container_of(wait, struct wait_page_queue, wait);\n\n\tif (!wake_page_match(wpq, key))\n\t\treturn 0;\n\n\treq->rw.kiocb.ki_flags &= ~IOCB_WAITQ;\n\tlist_del_init(&wait->entry);\n\n\t/* submit ref gets dropped, acquire a new one */\n\trefcount_inc(&req->refs);\n\tio_req_task_queue(req);\n\treturn 1;\n}\n\n/*\n * This controls whether a given IO request should be armed for async page\n * based retry. If we return false here, the request is handed to the async\n * worker threads for retry. If we're doing buffered reads on a regular file,\n * we prepare a private wait_page_queue entry and retry the operation. This\n * will either succeed because the page is now uptodate and unlocked, or it\n * will register a callback when the page is unlocked at IO completion. Through\n * that callback, io_uring uses task_work to setup a retry of the operation.\n * That retry will attempt the buffered read again. The retry will generally\n * succeed, or in rare cases where it fails, we then fall back to using the\n * async worker threads for a blocking retry.\n */\nstatic bool io_rw_should_retry(struct io_kiocb *req)\n{\n\tstruct io_async_rw *rw = req->async_data;\n\tstruct wait_page_queue *wait = &rw->wpq;\n\tstruct kiocb *kiocb = &req->rw.kiocb;\n\n\t/* never retry for NOWAIT, we just complete with -EAGAIN */\n\tif (req->flags & REQ_F_NOWAIT)\n\t\treturn false;\n\n\t/* Only for buffered IO */\n\tif (kiocb->ki_flags & (IOCB_DIRECT | IOCB_HIPRI))\n\t\treturn false;\n\n\t/*\n\t * just use poll if we can, and don't attempt if the fs doesn't\n\t * support callback based unlocks\n\t */\n\tif (file_can_poll(req->file) || !(req->file->f_mode & FMODE_BUF_RASYNC))\n\t\treturn false;\n\n\twait->wait.func = io_async_buf_func;\n\twait->wait.private = req;\n\twait->wait.flags = 0;\n\tINIT_LIST_HEAD(&wait->wait.entry);\n\tkiocb->ki_flags |= IOCB_WAITQ;\n\tkiocb->ki_flags &= ~IOCB_NOWAIT;\n\tkiocb->ki_waitq = wait;\n\treturn true;\n}\n\nstatic int io_iter_do_read(struct io_kiocb *req, struct iov_iter *iter)\n{\n\tif (req->file->f_op->read_iter)\n\t\treturn call_read_iter(req->file, &req->rw.kiocb, iter);\n\telse if (req->file->f_op->read)\n\t\treturn loop_rw_iter(READ, req, iter);\n\telse\n\t\treturn -EINVAL;\n}\n\nstatic int io_read(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;\n\tstruct kiocb *kiocb = &req->rw.kiocb;\n\tstruct iov_iter __iter, *iter = &__iter;\n\tstruct io_async_rw *rw = req->async_data;\n\tssize_t io_size, ret, ret2;\n\tbool force_nonblock = issue_flags & IO_URING_F_NONBLOCK;\n\n\tif (rw) {\n\t\titer = &rw->iter;\n\t\tiovec = NULL;\n\t} else {\n\t\tret = io_import_iovec(READ, req, &iovec, iter, !force_nonblock);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\tio_size = iov_iter_count(iter);\n\treq->result = io_size;\n\n\t/* Ensure we clear previously set non-block flag */\n\tif (!force_nonblock)\n\t\tkiocb->ki_flags &= ~IOCB_NOWAIT;\n\telse\n\t\tkiocb->ki_flags |= IOCB_NOWAIT;\n\n\t/* If the file doesn't support async, just async punt */\n\tif (force_nonblock && !io_file_supports_async(req->file, READ)) {\n\t\tret = io_setup_async_rw(req, iovec, inline_vecs, iter, true);\n\t\treturn ret ?: -EAGAIN;\n\t}\n\n\tret = rw_verify_area(READ, req->file, io_kiocb_ppos(kiocb), io_size);\n\tif (unlikely(ret)) {\n\t\tkfree(iovec);\n\t\treturn ret;\n\t}\n\n\tret = io_iter_do_read(req, iter);\n\n\tif (ret == -EIOCBQUEUED) {\n\t\tif (req->async_data)\n\t\t\tiov_iter_revert(iter, io_size - iov_iter_count(iter));\n\t\tgoto out_free;\n\t} else if (ret == -EAGAIN) {\n\t\t/* IOPOLL retry should happen for io-wq threads */\n\t\tif (!force_nonblock && !(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tgoto done;\n\t\t/* no retry on NONBLOCK nor RWF_NOWAIT */\n\t\tif (req->flags & REQ_F_NOWAIT)\n\t\t\tgoto done;\n\t\t/* some cases will consume bytes even on error returns */\n\t\tiov_iter_revert(iter, io_size - iov_iter_count(iter));\n\t\tret = 0;\n\t} else if (ret <= 0 || ret == io_size || !force_nonblock ||\n\t\t   (req->flags & REQ_F_NOWAIT) || !(req->flags & REQ_F_ISREG)) {\n\t\t/* read all, failed, already did sync or don't want to retry */\n\t\tgoto done;\n\t}\n\n\tret2 = io_setup_async_rw(req, iovec, inline_vecs, iter, true);\n\tif (ret2)\n\t\treturn ret2;\n\n\tiovec = NULL;\n\trw = req->async_data;\n\t/* now use our persistent iterator, if we aren't already */\n\titer = &rw->iter;\n\n\tdo {\n\t\tio_size -= ret;\n\t\trw->bytes_done += ret;\n\t\t/* if we can retry, do so with the callbacks armed */\n\t\tif (!io_rw_should_retry(req)) {\n\t\t\tkiocb->ki_flags &= ~IOCB_WAITQ;\n\t\t\treturn -EAGAIN;\n\t\t}\n\n\t\t/*\n\t\t * Now retry read with the IOCB_WAITQ parts set in the iocb. If\n\t\t * we get -EIOCBQUEUED, then we'll get a notification when the\n\t\t * desired page gets unlocked. We can also get a partial read\n\t\t * here, and if we do, then just retry at the new offset.\n\t\t */\n\t\tret = io_iter_do_read(req, iter);\n\t\tif (ret == -EIOCBQUEUED)\n\t\t\treturn 0;\n\t\t/* we got some bytes, but not all. retry. */\n\t} while (ret > 0 && ret < io_size);\ndone:\n\tkiocb_done(kiocb, ret, issue_flags);\nout_free:\n\t/* it's faster to check here then delegate to kfree */\n\tif (iovec)\n\t\tkfree(iovec);\n\treturn 0;\n}\n\nstatic int io_write_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tif (unlikely(!(req->file->f_mode & FMODE_WRITE)))\n\t\treturn -EBADF;\n\treturn io_prep_rw(req, sqe);\n}\n\nstatic int io_write(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;\n\tstruct kiocb *kiocb = &req->rw.kiocb;\n\tstruct iov_iter __iter, *iter = &__iter;\n\tstruct io_async_rw *rw = req->async_data;\n\tssize_t ret, ret2, io_size;\n\tbool force_nonblock = issue_flags & IO_URING_F_NONBLOCK;\n\n\tif (rw) {\n\t\titer = &rw->iter;\n\t\tiovec = NULL;\n\t} else {\n\t\tret = io_import_iovec(WRITE, req, &iovec, iter, !force_nonblock);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\tio_size = iov_iter_count(iter);\n\treq->result = io_size;\n\n\t/* Ensure we clear previously set non-block flag */\n\tif (!force_nonblock)\n\t\tkiocb->ki_flags &= ~IOCB_NOWAIT;\n\telse\n\t\tkiocb->ki_flags |= IOCB_NOWAIT;\n\n\t/* If the file doesn't support async, just async punt */\n\tif (force_nonblock && !io_file_supports_async(req->file, WRITE))\n\t\tgoto copy_iov;\n\n\t/* file path doesn't support NOWAIT for non-direct_IO */\n\tif (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT) &&\n\t    (req->flags & REQ_F_ISREG))\n\t\tgoto copy_iov;\n\n\tret = rw_verify_area(WRITE, req->file, io_kiocb_ppos(kiocb), io_size);\n\tif (unlikely(ret))\n\t\tgoto out_free;\n\n\t/*\n\t * Open-code file_start_write here to grab freeze protection,\n\t * which will be released by another thread in\n\t * io_complete_rw().  Fool lockdep by telling it the lock got\n\t * released so that it doesn't complain about the held lock when\n\t * we return to userspace.\n\t */\n\tif (req->flags & REQ_F_ISREG) {\n\t\tsb_start_write(file_inode(req->file)->i_sb);\n\t\t__sb_writers_release(file_inode(req->file)->i_sb,\n\t\t\t\t\tSB_FREEZE_WRITE);\n\t}\n\tkiocb->ki_flags |= IOCB_WRITE;\n\n\tif (req->file->f_op->write_iter)\n\t\tret2 = call_write_iter(req->file, kiocb, iter);\n\telse if (req->file->f_op->write)\n\t\tret2 = loop_rw_iter(WRITE, req, iter);\n\telse\n\t\tret2 = -EINVAL;\n\n\t/*\n\t * Raw bdev writes will return -EOPNOTSUPP for IOCB_NOWAIT. Just\n\t * retry them without IOCB_NOWAIT.\n\t */\n\tif (ret2 == -EOPNOTSUPP && (kiocb->ki_flags & IOCB_NOWAIT))\n\t\tret2 = -EAGAIN;\n\t/* no retry on NONBLOCK nor RWF_NOWAIT */\n\tif (ret2 == -EAGAIN && (req->flags & REQ_F_NOWAIT))\n\t\tgoto done;\n\tif (ret2 == -EIOCBQUEUED && req->async_data)\n\t\tiov_iter_revert(iter, io_size - iov_iter_count(iter));\n\tif (!force_nonblock || ret2 != -EAGAIN) {\n\t\t/* IOPOLL retry should happen for io-wq threads */\n\t\tif ((req->ctx->flags & IORING_SETUP_IOPOLL) && ret2 == -EAGAIN)\n\t\t\tgoto copy_iov;\ndone:\n\t\tkiocb_done(kiocb, ret2, issue_flags);\n\t} else {\ncopy_iov:\n\t\t/* some cases will consume bytes even on error returns */\n\t\tiov_iter_revert(iter, io_size - iov_iter_count(iter));\n\t\tret = io_setup_async_rw(req, iovec, inline_vecs, iter, false);\n\t\treturn ret ?: -EAGAIN;\n\t}\nout_free:\n\t/* it's reportedly faster than delegating the null check to kfree() */\n\tif (iovec)\n\t\tkfree(iovec);\n\treturn ret;\n}\n\nstatic int io_renameat_prep(struct io_kiocb *req,\n\t\t\t    const struct io_uring_sqe *sqe)\n{\n\tstruct io_rename *ren = &req->rename;\n\tconst char __user *oldf, *newf;\n\n\tif (unlikely(req->flags & REQ_F_FIXED_FILE))\n\t\treturn -EBADF;\n\n\tren->old_dfd = READ_ONCE(sqe->fd);\n\toldf = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\tnewf = u64_to_user_ptr(READ_ONCE(sqe->addr2));\n\tren->new_dfd = READ_ONCE(sqe->len);\n\tren->flags = READ_ONCE(sqe->rename_flags);\n\n\tren->oldpath = getname(oldf);\n\tif (IS_ERR(ren->oldpath))\n\t\treturn PTR_ERR(ren->oldpath);\n\n\tren->newpath = getname(newf);\n\tif (IS_ERR(ren->newpath)) {\n\t\tputname(ren->oldpath);\n\t\treturn PTR_ERR(ren->newpath);\n\t}\n\n\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn 0;\n}\n\nstatic int io_renameat(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_rename *ren = &req->rename;\n\tint ret;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\n\tret = do_renameat2(ren->old_dfd, ren->oldpath, ren->new_dfd,\n\t\t\t\tren->newpath, ren->flags);\n\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_unlinkat_prep(struct io_kiocb *req,\n\t\t\t    const struct io_uring_sqe *sqe)\n{\n\tstruct io_unlink *un = &req->unlink;\n\tconst char __user *fname;\n\n\tif (unlikely(req->flags & REQ_F_FIXED_FILE))\n\t\treturn -EBADF;\n\n\tun->dfd = READ_ONCE(sqe->fd);\n\n\tun->flags = READ_ONCE(sqe->unlink_flags);\n\tif (un->flags & ~AT_REMOVEDIR)\n\t\treturn -EINVAL;\n\n\tfname = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\tun->filename = getname(fname);\n\tif (IS_ERR(un->filename))\n\t\treturn PTR_ERR(un->filename);\n\n\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn 0;\n}\n\nstatic int io_unlinkat(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_unlink *un = &req->unlink;\n\tint ret;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\n\tif (un->flags & AT_REMOVEDIR)\n\t\tret = do_rmdir(un->dfd, un->filename);\n\telse\n\t\tret = do_unlinkat(un->dfd, un->filename);\n\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_shutdown_prep(struct io_kiocb *req,\n\t\t\t    const struct io_uring_sqe *sqe)\n{\n#if defined(CONFIG_NET)\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->off || sqe->addr || sqe->rw_flags ||\n\t    sqe->buf_index)\n\t\treturn -EINVAL;\n\n\treq->shutdown.how = READ_ONCE(sqe->len);\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int io_shutdown(struct io_kiocb *req, unsigned int issue_flags)\n{\n#if defined(CONFIG_NET)\n\tstruct socket *sock;\n\tint ret;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\n\tsock = sock_from_file(req->file);\n\tif (unlikely(!sock))\n\t\treturn -ENOTSOCK;\n\n\tret = __sys_shutdown_sock(sock, req->shutdown.how);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int __io_splice_prep(struct io_kiocb *req,\n\t\t\t    const struct io_uring_sqe *sqe)\n{\n\tstruct io_splice* sp = &req->splice;\n\tunsigned int valid_flags = SPLICE_F_FD_IN_FIXED | SPLICE_F_ALL;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\tsp->file_in = NULL;\n\tsp->len = READ_ONCE(sqe->len);\n\tsp->flags = READ_ONCE(sqe->splice_flags);\n\n\tif (unlikely(sp->flags & ~valid_flags))\n\t\treturn -EINVAL;\n\n\tsp->file_in = io_file_get(NULL, req, READ_ONCE(sqe->splice_fd_in),\n\t\t\t\t  (sp->flags & SPLICE_F_FD_IN_FIXED));\n\tif (!sp->file_in)\n\t\treturn -EBADF;\n\treq->flags |= REQ_F_NEED_CLEANUP;\n\n\tif (!S_ISREG(file_inode(sp->file_in)->i_mode)) {\n\t\t/*\n\t\t * Splice operation will be punted aync, and here need to\n\t\t * modify io_wq_work.flags, so initialize io_wq_work firstly.\n\t\t */\n\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n\n\treturn 0;\n}\n\nstatic int io_tee_prep(struct io_kiocb *req,\n\t\t       const struct io_uring_sqe *sqe)\n{\n\tif (READ_ONCE(sqe->splice_off_in) || READ_ONCE(sqe->off))\n\t\treturn -EINVAL;\n\treturn __io_splice_prep(req, sqe);\n}\n\nstatic int io_tee(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_splice *sp = &req->splice;\n\tstruct file *in = sp->file_in;\n\tstruct file *out = sp->file_out;\n\tunsigned int flags = sp->flags & ~SPLICE_F_FD_IN_FIXED;\n\tlong ret = 0;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\tif (sp->len)\n\t\tret = do_tee(in, out, sp->len, flags);\n\n\tio_put_file(req, in, (sp->flags & SPLICE_F_FD_IN_FIXED));\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\n\tif (ret != sp->len)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_splice_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_splice* sp = &req->splice;\n\n\tsp->off_in = READ_ONCE(sqe->splice_off_in);\n\tsp->off_out = READ_ONCE(sqe->off);\n\treturn __io_splice_prep(req, sqe);\n}\n\nstatic int io_splice(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_splice *sp = &req->splice;\n\tstruct file *in = sp->file_in;\n\tstruct file *out = sp->file_out;\n\tunsigned int flags = sp->flags & ~SPLICE_F_FD_IN_FIXED;\n\tloff_t *poff_in, *poff_out;\n\tlong ret = 0;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\n\tpoff_in = (sp->off_in == -1) ? NULL : &sp->off_in;\n\tpoff_out = (sp->off_out == -1) ? NULL : &sp->off_out;\n\n\tif (sp->len)\n\t\tret = do_splice(in, poff_in, out, poff_out, sp->len, flags);\n\n\tio_put_file(req, in, (sp->flags & SPLICE_F_FD_IN_FIXED));\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\n\tif (ret != sp->len)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\n/*\n * IORING_OP_NOP just posts a completion event, nothing else.\n */\nstatic int io_nop(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (unlikely(ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\t__io_req_complete(req, issue_flags, 0, 0);\n\treturn 0;\n}\n\nstatic int io_fsync_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!req->file)\n\t\treturn -EBADF;\n\n\tif (unlikely(ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (unlikely(sqe->addr || sqe->ioprio || sqe->buf_index))\n\t\treturn -EINVAL;\n\n\treq->sync.flags = READ_ONCE(sqe->fsync_flags);\n\tif (unlikely(req->sync.flags & ~IORING_FSYNC_DATASYNC))\n\t\treturn -EINVAL;\n\n\treq->sync.off = READ_ONCE(sqe->off);\n\treq->sync.len = READ_ONCE(sqe->len);\n\treturn 0;\n}\n\nstatic int io_fsync(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tloff_t end = req->sync.off + req->sync.len;\n\tint ret;\n\n\t/* fsync always requires a blocking context */\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\n\tret = vfs_fsync_range(req->file, req->sync.off,\n\t\t\t\tend > 0 ? end : LLONG_MAX,\n\t\t\t\treq->sync.flags & IORING_FSYNC_DATASYNC);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_fallocate_prep(struct io_kiocb *req,\n\t\t\t     const struct io_uring_sqe *sqe)\n{\n\tif (sqe->ioprio || sqe->buf_index || sqe->rw_flags)\n\t\treturn -EINVAL;\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\treq->sync.off = READ_ONCE(sqe->off);\n\treq->sync.len = READ_ONCE(sqe->addr);\n\treq->sync.mode = READ_ONCE(sqe->len);\n\treturn 0;\n}\n\nstatic int io_fallocate(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tint ret;\n\n\t/* fallocate always requiring blocking context */\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\tret = vfs_fallocate(req->file, req->sync.mode, req->sync.off,\n\t\t\t\treq->sync.len);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int __io_openat_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tconst char __user *fname;\n\tint ret;\n\n\tif (unlikely(sqe->ioprio || sqe->buf_index))\n\t\treturn -EINVAL;\n\tif (unlikely(req->flags & REQ_F_FIXED_FILE))\n\t\treturn -EBADF;\n\n\t/* open.how should be already initialised */\n\tif (!(req->open.how.flags & O_PATH) && force_o_largefile())\n\t\treq->open.how.flags |= O_LARGEFILE;\n\n\treq->open.dfd = READ_ONCE(sqe->fd);\n\tfname = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\treq->open.filename = getname(fname);\n\tif (IS_ERR(req->open.filename)) {\n\t\tret = PTR_ERR(req->open.filename);\n\t\treq->open.filename = NULL;\n\t\treturn ret;\n\t}\n\treq->open.nofile = rlimit(RLIMIT_NOFILE);\n\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn 0;\n}\n\nstatic int io_openat_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tu64 flags, mode;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tmode = READ_ONCE(sqe->len);\n\tflags = READ_ONCE(sqe->open_flags);\n\treq->open.how = build_open_how(flags, mode);\n\treturn __io_openat_prep(req, sqe);\n}\n\nstatic int io_openat2_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct open_how __user *how;\n\tsize_t len;\n\tint ret;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\thow = u64_to_user_ptr(READ_ONCE(sqe->addr2));\n\tlen = READ_ONCE(sqe->len);\n\tif (len < OPEN_HOW_SIZE_VER0)\n\t\treturn -EINVAL;\n\n\tret = copy_struct_from_user(&req->open.how, sizeof(req->open.how), how,\n\t\t\t\t\tlen);\n\tif (ret)\n\t\treturn ret;\n\n\treturn __io_openat_prep(req, sqe);\n}\n\nstatic int io_openat2(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct open_flags op;\n\tstruct file *file;\n\tbool nonblock_set;\n\tbool resolve_nonblock;\n\tint ret;\n\n\tret = build_open_flags(&req->open.how, &op);\n\tif (ret)\n\t\tgoto err;\n\tnonblock_set = op.open_flag & O_NONBLOCK;\n\tresolve_nonblock = req->open.how.resolve & RESOLVE_CACHED;\n\tif (issue_flags & IO_URING_F_NONBLOCK) {\n\t\t/*\n\t\t * Don't bother trying for O_TRUNC, O_CREAT, or O_TMPFILE open,\n\t\t * it'll always -EAGAIN\n\t\t */\n\t\tif (req->open.how.flags & (O_TRUNC | O_CREAT | O_TMPFILE))\n\t\t\treturn -EAGAIN;\n\t\top.lookup_flags |= LOOKUP_CACHED;\n\t\top.open_flag |= O_NONBLOCK;\n\t}\n\n\tret = __get_unused_fd_flags(req->open.how.flags, req->open.nofile);\n\tif (ret < 0)\n\t\tgoto err;\n\n\tfile = do_filp_open(req->open.dfd, req->open.filename, &op);\n\t/* only retry if RESOLVE_CACHED wasn't already set by application */\n\tif ((!resolve_nonblock && (issue_flags & IO_URING_F_NONBLOCK)) &&\n\t    file == ERR_PTR(-EAGAIN)) {\n\t\t/*\n\t\t * We could hang on to this 'fd', but seems like marginal\n\t\t * gain for something that is now known to be a slower path.\n\t\t * So just put it, and we'll get a new one when we retry.\n\t\t */\n\t\tput_unused_fd(ret);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (IS_ERR(file)) {\n\t\tput_unused_fd(ret);\n\t\tret = PTR_ERR(file);\n\t} else {\n\t\tif ((issue_flags & IO_URING_F_NONBLOCK) && !nonblock_set)\n\t\t\tfile->f_flags &= ~O_NONBLOCK;\n\t\tfsnotify_open(file);\n\t\tfd_install(ret, file);\n\t}\nerr:\n\tputname(req->open.filename);\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_openat(struct io_kiocb *req, unsigned int issue_flags)\n{\n\treturn io_openat2(req, issue_flags & IO_URING_F_NONBLOCK);\n}\n\nstatic int io_remove_buffers_prep(struct io_kiocb *req,\n\t\t\t\t  const struct io_uring_sqe *sqe)\n{\n\tstruct io_provide_buf *p = &req->pbuf;\n\tu64 tmp;\n\n\tif (sqe->ioprio || sqe->rw_flags || sqe->addr || sqe->len || sqe->off)\n\t\treturn -EINVAL;\n\n\ttmp = READ_ONCE(sqe->fd);\n\tif (!tmp || tmp > USHRT_MAX)\n\t\treturn -EINVAL;\n\n\tmemset(p, 0, sizeof(*p));\n\tp->nbufs = tmp;\n\tp->bgid = READ_ONCE(sqe->buf_group);\n\treturn 0;\n}\n\nstatic int __io_remove_buffers(struct io_ring_ctx *ctx, struct io_buffer *buf,\n\t\t\t       int bgid, unsigned nbufs)\n{\n\tunsigned i = 0;\n\n\t/* shouldn't happen */\n\tif (!nbufs)\n\t\treturn 0;\n\n\t/* the head kbuf is the list itself */\n\twhile (!list_empty(&buf->list)) {\n\t\tstruct io_buffer *nxt;\n\n\t\tnxt = list_first_entry(&buf->list, struct io_buffer, list);\n\t\tlist_del(&nxt->list);\n\t\tkfree(nxt);\n\t\tif (++i == nbufs)\n\t\t\treturn i;\n\t}\n\ti++;\n\tkfree(buf);\n\tidr_remove(&ctx->io_buffer_idr, bgid);\n\n\treturn i;\n}\n\nstatic int io_remove_buffers(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_provide_buf *p = &req->pbuf;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_buffer *head;\n\tint ret = 0;\n\tbool force_nonblock = issue_flags & IO_URING_F_NONBLOCK;\n\n\tio_ring_submit_lock(ctx, !force_nonblock);\n\n\tlockdep_assert_held(&ctx->uring_lock);\n\n\tret = -ENOENT;\n\thead = idr_find(&ctx->io_buffer_idr, p->bgid);\n\tif (head)\n\t\tret = __io_remove_buffers(ctx, head, p->bgid, p->nbufs);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\n\t/* need to hold the lock to complete IOPOLL requests */\n\tif (ctx->flags & IORING_SETUP_IOPOLL) {\n\t\t__io_req_complete(req, issue_flags, ret, 0);\n\t\tio_ring_submit_unlock(ctx, !force_nonblock);\n\t} else {\n\t\tio_ring_submit_unlock(ctx, !force_nonblock);\n\t\t__io_req_complete(req, issue_flags, ret, 0);\n\t}\n\treturn 0;\n}\n\nstatic int io_provide_buffers_prep(struct io_kiocb *req,\n\t\t\t\t   const struct io_uring_sqe *sqe)\n{\n\tstruct io_provide_buf *p = &req->pbuf;\n\tu64 tmp;\n\n\tif (sqe->ioprio || sqe->rw_flags)\n\t\treturn -EINVAL;\n\n\ttmp = READ_ONCE(sqe->fd);\n\tif (!tmp || tmp > USHRT_MAX)\n\t\treturn -E2BIG;\n\tp->nbufs = tmp;\n\tp->addr = READ_ONCE(sqe->addr);\n\tp->len = READ_ONCE(sqe->len);\n\n\tif (!access_ok(u64_to_user_ptr(p->addr), (p->len * p->nbufs)))\n\t\treturn -EFAULT;\n\n\tp->bgid = READ_ONCE(sqe->buf_group);\n\ttmp = READ_ONCE(sqe->off);\n\tif (tmp > USHRT_MAX)\n\t\treturn -E2BIG;\n\tp->bid = tmp;\n\treturn 0;\n}\n\nstatic int io_add_buffers(struct io_provide_buf *pbuf, struct io_buffer **head)\n{\n\tstruct io_buffer *buf;\n\tu64 addr = pbuf->addr;\n\tint i, bid = pbuf->bid;\n\n\tfor (i = 0; i < pbuf->nbufs; i++) {\n\t\tbuf = kmalloc(sizeof(*buf), GFP_KERNEL);\n\t\tif (!buf)\n\t\t\tbreak;\n\n\t\tbuf->addr = addr;\n\t\tbuf->len = pbuf->len;\n\t\tbuf->bid = bid;\n\t\taddr += pbuf->len;\n\t\tbid++;\n\t\tif (!*head) {\n\t\t\tINIT_LIST_HEAD(&buf->list);\n\t\t\t*head = buf;\n\t\t} else {\n\t\t\tlist_add_tail(&buf->list, &(*head)->list);\n\t\t}\n\t}\n\n\treturn i ? i : -ENOMEM;\n}\n\nstatic int io_provide_buffers(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_provide_buf *p = &req->pbuf;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_buffer *head, *list;\n\tint ret = 0;\n\tbool force_nonblock = issue_flags & IO_URING_F_NONBLOCK;\n\n\tio_ring_submit_lock(ctx, !force_nonblock);\n\n\tlockdep_assert_held(&ctx->uring_lock);\n\n\tlist = head = idr_find(&ctx->io_buffer_idr, p->bgid);\n\n\tret = io_add_buffers(p, &head);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (!list) {\n\t\tret = idr_alloc(&ctx->io_buffer_idr, head, p->bgid, p->bgid + 1,\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (ret < 0) {\n\t\t\t__io_remove_buffers(ctx, head, p->bgid, -1U);\n\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\n\t/* need to hold the lock to complete IOPOLL requests */\n\tif (ctx->flags & IORING_SETUP_IOPOLL) {\n\t\t__io_req_complete(req, issue_flags, ret, 0);\n\t\tio_ring_submit_unlock(ctx, !force_nonblock);\n\t} else {\n\t\tio_ring_submit_unlock(ctx, !force_nonblock);\n\t\t__io_req_complete(req, issue_flags, ret, 0);\n\t}\n\treturn 0;\n}\n\nstatic int io_epoll_ctl_prep(struct io_kiocb *req,\n\t\t\t     const struct io_uring_sqe *sqe)\n{\n#if defined(CONFIG_EPOLL)\n\tif (sqe->ioprio || sqe->buf_index)\n\t\treturn -EINVAL;\n\tif (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL)))\n\t\treturn -EINVAL;\n\n\treq->epoll.epfd = READ_ONCE(sqe->fd);\n\treq->epoll.op = READ_ONCE(sqe->len);\n\treq->epoll.fd = READ_ONCE(sqe->off);\n\n\tif (ep_op_has_event(req->epoll.op)) {\n\t\tstruct epoll_event __user *ev;\n\n\t\tev = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\t\tif (copy_from_user(&req->epoll.event, ev, sizeof(*ev)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int io_epoll_ctl(struct io_kiocb *req, unsigned int issue_flags)\n{\n#if defined(CONFIG_EPOLL)\n\tstruct io_epoll *ie = &req->epoll;\n\tint ret;\n\tbool force_nonblock = issue_flags & IO_URING_F_NONBLOCK;\n\n\tret = do_epoll_ctl(ie->epfd, ie->op, ie->fd, &ie->event, force_nonblock);\n\tif (force_nonblock && ret == -EAGAIN)\n\t\treturn -EAGAIN;\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, issue_flags, ret, 0);\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int io_madvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n#if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)\n\tif (sqe->ioprio || sqe->buf_index || sqe->off)\n\t\treturn -EINVAL;\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\treq->madvise.addr = READ_ONCE(sqe->addr);\n\treq->madvise.len = READ_ONCE(sqe->len);\n\treq->madvise.advice = READ_ONCE(sqe->fadvise_advice);\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int io_madvise(struct io_kiocb *req, unsigned int issue_flags)\n{\n#if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)\n\tstruct io_madvise *ma = &req->madvise;\n\tint ret;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\n\tret = do_madvise(current->mm, ma->addr, ma->len, ma->advice);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int io_fadvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tif (sqe->ioprio || sqe->buf_index || sqe->addr)\n\t\treturn -EINVAL;\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\treq->fadvise.offset = READ_ONCE(sqe->off);\n\treq->fadvise.len = READ_ONCE(sqe->len);\n\treq->fadvise.advice = READ_ONCE(sqe->fadvise_advice);\n\treturn 0;\n}\n\nstatic int io_fadvise(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_fadvise *fa = &req->fadvise;\n\tint ret;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK) {\n\t\tswitch (fa->advice) {\n\t\tcase POSIX_FADV_NORMAL:\n\t\tcase POSIX_FADV_RANDOM:\n\t\tcase POSIX_FADV_SEQUENTIAL:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\n\tret = vfs_fadvise(req->file, fa->offset, fa->len, fa->advice);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_statx_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tif (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL)))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->buf_index)\n\t\treturn -EINVAL;\n\tif (req->flags & REQ_F_FIXED_FILE)\n\t\treturn -EBADF;\n\n\treq->statx.dfd = READ_ONCE(sqe->fd);\n\treq->statx.mask = READ_ONCE(sqe->len);\n\treq->statx.filename = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\treq->statx.buffer = u64_to_user_ptr(READ_ONCE(sqe->addr2));\n\treq->statx.flags = READ_ONCE(sqe->statx_flags);\n\n\treturn 0;\n}\n\nstatic int io_statx(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_statx *ctx = &req->statx;\n\tint ret;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK) {\n\t\t/* only need file table for an actual valid fd */\n\t\tif (ctx->dfd == -1 || ctx->dfd == AT_FDCWD)\n\t\t\treq->flags |= REQ_F_NO_FILE_TABLE;\n\t\treturn -EAGAIN;\n\t}\n\n\tret = do_statx(ctx->dfd, ctx->filename, ctx->flags, ctx->mask,\n\t\t       ctx->buffer);\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_close_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||\n\t    sqe->rw_flags || sqe->buf_index)\n\t\treturn -EINVAL;\n\tif (req->flags & REQ_F_FIXED_FILE)\n\t\treturn -EBADF;\n\n\treq->close.fd = READ_ONCE(sqe->fd);\n\treturn 0;\n}\n\nstatic int io_close(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct files_struct *files = current->files;\n\tstruct io_close *close = &req->close;\n\tstruct fdtable *fdt;\n\tstruct file *file;\n\tint ret;\n\n\tfile = NULL;\n\tret = -EBADF;\n\tspin_lock(&files->file_lock);\n\tfdt = files_fdtable(files);\n\tif (close->fd >= fdt->max_fds) {\n\t\tspin_unlock(&files->file_lock);\n\t\tgoto err;\n\t}\n\tfile = fdt->fd[close->fd];\n\tif (!file) {\n\t\tspin_unlock(&files->file_lock);\n\t\tgoto err;\n\t}\n\n\tif (file->f_op == &io_uring_fops) {\n\t\tspin_unlock(&files->file_lock);\n\t\tfile = NULL;\n\t\tgoto err;\n\t}\n\n\t/* if the file has a flush method, be safe and punt to async */\n\tif (file->f_op->flush && (issue_flags & IO_URING_F_NONBLOCK)) {\n\t\tspin_unlock(&files->file_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tret = __close_fd_get_file(close->fd, &file);\n\tspin_unlock(&files->file_lock);\n\tif (ret < 0) {\n\t\tif (ret == -ENOENT)\n\t\t\tret = -EBADF;\n\t\tgoto err;\n\t}\n\n\t/* No ->flush() or already async, safely close from here */\n\tret = filp_close(file, current->files);\nerr:\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tif (file)\n\t\tfput(file);\n\t__io_req_complete(req, issue_flags, ret, 0);\n\treturn 0;\n}\n\nstatic int io_sfr_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (unlikely(ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (unlikely(sqe->addr || sqe->ioprio || sqe->buf_index))\n\t\treturn -EINVAL;\n\n\treq->sync.off = READ_ONCE(sqe->off);\n\treq->sync.len = READ_ONCE(sqe->len);\n\treq->sync.flags = READ_ONCE(sqe->sync_range_flags);\n\treturn 0;\n}\n\nstatic int io_sync_file_range(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tint ret;\n\n\t/* sync_file_range always requires a blocking context */\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\n\tret = sync_file_range(req->file, req->sync.off, req->sync.len,\n\t\t\t\treq->sync.flags);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\n#if defined(CONFIG_NET)\nstatic int io_setup_async_msg(struct io_kiocb *req,\n\t\t\t      struct io_async_msghdr *kmsg)\n{\n\tstruct io_async_msghdr *async_msg = req->async_data;\n\n\tif (async_msg)\n\t\treturn -EAGAIN;\n\tif (io_alloc_async_data(req)) {\n\t\tkfree(kmsg->free_iov);\n\t\treturn -ENOMEM;\n\t}\n\tasync_msg = req->async_data;\n\treq->flags |= REQ_F_NEED_CLEANUP;\n\tmemcpy(async_msg, kmsg, sizeof(*kmsg));\n\tasync_msg->msg.msg_name = &async_msg->addr;\n\t/* if were using fast_iov, set it to the new one */\n\tif (!async_msg->free_iov)\n\t\tasync_msg->msg.msg_iter.iov = async_msg->fast_iov;\n\n\treturn -EAGAIN;\n}\n\nstatic int io_sendmsg_copy_hdr(struct io_kiocb *req,\n\t\t\t       struct io_async_msghdr *iomsg)\n{\n\tiomsg->msg.msg_name = &iomsg->addr;\n\tiomsg->free_iov = iomsg->fast_iov;\n\treturn sendmsg_copy_msghdr(&iomsg->msg, req->sr_msg.umsg,\n\t\t\t\t   req->sr_msg.msg_flags, &iomsg->free_iov);\n}\n\nstatic int io_sendmsg_prep_async(struct io_kiocb *req)\n{\n\tint ret;\n\n\tif (!io_op_defs[req->opcode].needs_async_data)\n\t\treturn 0;\n\tret = io_sendmsg_copy_hdr(req, req->async_data);\n\tif (!ret)\n\t\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn ret;\n}\n\nstatic int io_sendmsg_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\tsr->msg_flags = READ_ONCE(sqe->msg_flags);\n\tsr->umsg = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\tsr->len = READ_ONCE(sqe->len);\n\n#ifdef CONFIG_COMPAT\n\tif (req->ctx->compat)\n\t\tsr->msg_flags |= MSG_CMSG_COMPAT;\n#endif\n\treturn 0;\n}\n\nstatic int io_sendmsg(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_async_msghdr iomsg, *kmsg;\n\tstruct socket *sock;\n\tunsigned flags;\n\tint ret;\n\n\tsock = sock_from_file(req->file);\n\tif (unlikely(!sock))\n\t\treturn -ENOTSOCK;\n\n\tkmsg = req->async_data;\n\tif (!kmsg) {\n\t\tret = io_sendmsg_copy_hdr(req, &iomsg);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tkmsg = &iomsg;\n\t}\n\n\tflags = req->sr_msg.msg_flags;\n\tif (flags & MSG_DONTWAIT)\n\t\treq->flags |= REQ_F_NOWAIT;\n\telse if (issue_flags & IO_URING_F_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\n\tret = __sys_sendmsg_sock(sock, &kmsg->msg, flags);\n\tif ((issue_flags & IO_URING_F_NONBLOCK) && ret == -EAGAIN)\n\t\treturn io_setup_async_msg(req, kmsg);\n\tif (ret == -ERESTARTSYS)\n\t\tret = -EINTR;\n\n\t/* fast path, check for non-NULL to avoid function call */\n\tif (kmsg->free_iov)\n\t\tkfree(kmsg->free_iov);\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, issue_flags, ret, 0);\n\treturn 0;\n}\n\nstatic int io_send(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tstruct msghdr msg;\n\tstruct iovec iov;\n\tstruct socket *sock;\n\tunsigned flags;\n\tint ret;\n\n\tsock = sock_from_file(req->file);\n\tif (unlikely(!sock))\n\t\treturn -ENOTSOCK;\n\n\tret = import_single_range(WRITE, sr->buf, sr->len, &iov, &msg.msg_iter);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tmsg.msg_name = NULL;\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\tmsg.msg_namelen = 0;\n\n\tflags = req->sr_msg.msg_flags;\n\tif (flags & MSG_DONTWAIT)\n\t\treq->flags |= REQ_F_NOWAIT;\n\telse if (issue_flags & IO_URING_F_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\n\tmsg.msg_flags = flags;\n\tret = sock_sendmsg(sock, &msg);\n\tif ((issue_flags & IO_URING_F_NONBLOCK) && ret == -EAGAIN)\n\t\treturn -EAGAIN;\n\tif (ret == -ERESTARTSYS)\n\t\tret = -EINTR;\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, issue_flags, ret, 0);\n\treturn 0;\n}\n\nstatic int __io_recvmsg_copy_hdr(struct io_kiocb *req,\n\t\t\t\t struct io_async_msghdr *iomsg)\n{\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tstruct iovec __user *uiov;\n\tsize_t iov_len;\n\tint ret;\n\n\tret = __copy_msghdr_from_user(&iomsg->msg, sr->umsg,\n\t\t\t\t\t&iomsg->uaddr, &uiov, &iov_len);\n\tif (ret)\n\t\treturn ret;\n\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tif (iov_len > 1)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(iomsg->fast_iov, uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\t\tsr->len = iomsg->fast_iov[0].iov_len;\n\t\tiomsg->free_iov = NULL;\n\t} else {\n\t\tiomsg->free_iov = iomsg->fast_iov;\n\t\tret = __import_iovec(READ, uiov, iov_len, UIO_FASTIOV,\n\t\t\t\t     &iomsg->free_iov, &iomsg->msg.msg_iter,\n\t\t\t\t     false);\n\t\tif (ret > 0)\n\t\t\tret = 0;\n\t}\n\n\treturn ret;\n}\n\n#ifdef CONFIG_COMPAT\nstatic int __io_compat_recvmsg_copy_hdr(struct io_kiocb *req,\n\t\t\t\t\tstruct io_async_msghdr *iomsg)\n{\n\tstruct compat_msghdr __user *msg_compat;\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tstruct compat_iovec __user *uiov;\n\tcompat_uptr_t ptr;\n\tcompat_size_t len;\n\tint ret;\n\n\tmsg_compat = (struct compat_msghdr __user *) sr->umsg;\n\tret = __get_compat_msghdr(&iomsg->msg, msg_compat, &iomsg->uaddr,\n\t\t\t\t\t&ptr, &len);\n\tif (ret)\n\t\treturn ret;\n\n\tuiov = compat_ptr(ptr);\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tcompat_ssize_t clen;\n\n\t\tif (len > 1)\n\t\t\treturn -EINVAL;\n\t\tif (!access_ok(uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\t\tif (__get_user(clen, &uiov->iov_len))\n\t\t\treturn -EFAULT;\n\t\tif (clen < 0)\n\t\t\treturn -EINVAL;\n\t\tsr->len = clen;\n\t\tiomsg->free_iov = NULL;\n\t} else {\n\t\tiomsg->free_iov = iomsg->fast_iov;\n\t\tret = __import_iovec(READ, (struct iovec __user *)uiov, len,\n\t\t\t\t   UIO_FASTIOV, &iomsg->free_iov,\n\t\t\t\t   &iomsg->msg.msg_iter, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n#endif\n\nstatic int io_recvmsg_copy_hdr(struct io_kiocb *req,\n\t\t\t       struct io_async_msghdr *iomsg)\n{\n\tiomsg->msg.msg_name = &iomsg->addr;\n\n#ifdef CONFIG_COMPAT\n\tif (req->ctx->compat)\n\t\treturn __io_compat_recvmsg_copy_hdr(req, iomsg);\n#endif\n\n\treturn __io_recvmsg_copy_hdr(req, iomsg);\n}\n\nstatic struct io_buffer *io_recv_buffer_select(struct io_kiocb *req,\n\t\t\t\t\t       bool needs_lock)\n{\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tstruct io_buffer *kbuf;\n\n\tkbuf = io_buffer_select(req, &sr->len, sr->bgid, sr->kbuf, needs_lock);\n\tif (IS_ERR(kbuf))\n\t\treturn kbuf;\n\n\tsr->kbuf = kbuf;\n\treq->flags |= REQ_F_BUFFER_SELECTED;\n\treturn kbuf;\n}\n\nstatic inline unsigned int io_put_recv_kbuf(struct io_kiocb *req)\n{\n\treturn io_put_kbuf(req, req->sr_msg.kbuf);\n}\n\nstatic int io_recvmsg_prep_async(struct io_kiocb *req)\n{\n\tint ret;\n\n\tif (!io_op_defs[req->opcode].needs_async_data)\n\t\treturn 0;\n\tret = io_recvmsg_copy_hdr(req, req->async_data);\n\tif (!ret)\n\t\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn ret;\n}\n\nstatic int io_recvmsg_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\tsr->msg_flags = READ_ONCE(sqe->msg_flags);\n\tsr->umsg = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\tsr->len = READ_ONCE(sqe->len);\n\tsr->bgid = READ_ONCE(sqe->buf_group);\n\n#ifdef CONFIG_COMPAT\n\tif (req->ctx->compat)\n\t\tsr->msg_flags |= MSG_CMSG_COMPAT;\n#endif\n\treturn 0;\n}\n\nstatic int io_recvmsg(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_async_msghdr iomsg, *kmsg;\n\tstruct socket *sock;\n\tstruct io_buffer *kbuf;\n\tunsigned flags;\n\tint ret, cflags = 0;\n\tbool force_nonblock = issue_flags & IO_URING_F_NONBLOCK;\n\n\tsock = sock_from_file(req->file);\n\tif (unlikely(!sock))\n\t\treturn -ENOTSOCK;\n\n\tkmsg = req->async_data;\n\tif (!kmsg) {\n\t\tret = io_recvmsg_copy_hdr(req, &iomsg);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tkmsg = &iomsg;\n\t}\n\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tkbuf = io_recv_buffer_select(req, !force_nonblock);\n\t\tif (IS_ERR(kbuf))\n\t\t\treturn PTR_ERR(kbuf);\n\t\tkmsg->fast_iov[0].iov_base = u64_to_user_ptr(kbuf->addr);\n\t\tkmsg->fast_iov[0].iov_len = req->sr_msg.len;\n\t\tiov_iter_init(&kmsg->msg.msg_iter, READ, kmsg->fast_iov,\n\t\t\t\t1, req->sr_msg.len);\n\t}\n\n\tflags = req->sr_msg.msg_flags;\n\tif (flags & MSG_DONTWAIT)\n\t\treq->flags |= REQ_F_NOWAIT;\n\telse if (force_nonblock)\n\t\tflags |= MSG_DONTWAIT;\n\n\tret = __sys_recvmsg_sock(sock, &kmsg->msg, req->sr_msg.umsg,\n\t\t\t\t\tkmsg->uaddr, flags);\n\tif (force_nonblock && ret == -EAGAIN)\n\t\treturn io_setup_async_msg(req, kmsg);\n\tif (ret == -ERESTARTSYS)\n\t\tret = -EINTR;\n\n\tif (req->flags & REQ_F_BUFFER_SELECTED)\n\t\tcflags = io_put_recv_kbuf(req);\n\t/* fast path, check for non-NULL to avoid function call */\n\tif (kmsg->free_iov)\n\t\tkfree(kmsg->free_iov);\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, issue_flags, ret, cflags);\n\treturn 0;\n}\n\nstatic int io_recv(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_buffer *kbuf;\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tstruct msghdr msg;\n\tvoid __user *buf = sr->buf;\n\tstruct socket *sock;\n\tstruct iovec iov;\n\tunsigned flags;\n\tint ret, cflags = 0;\n\tbool force_nonblock = issue_flags & IO_URING_F_NONBLOCK;\n\n\tsock = sock_from_file(req->file);\n\tif (unlikely(!sock))\n\t\treturn -ENOTSOCK;\n\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tkbuf = io_recv_buffer_select(req, !force_nonblock);\n\t\tif (IS_ERR(kbuf))\n\t\t\treturn PTR_ERR(kbuf);\n\t\tbuf = u64_to_user_ptr(kbuf->addr);\n\t}\n\n\tret = import_single_range(READ, buf, sr->len, &iov, &msg.msg_iter);\n\tif (unlikely(ret))\n\t\tgoto out_free;\n\n\tmsg.msg_name = NULL;\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\tmsg.msg_namelen = 0;\n\tmsg.msg_iocb = NULL;\n\tmsg.msg_flags = 0;\n\n\tflags = req->sr_msg.msg_flags;\n\tif (flags & MSG_DONTWAIT)\n\t\treq->flags |= REQ_F_NOWAIT;\n\telse if (force_nonblock)\n\t\tflags |= MSG_DONTWAIT;\n\n\tret = sock_recvmsg(sock, &msg, flags);\n\tif (force_nonblock && ret == -EAGAIN)\n\t\treturn -EAGAIN;\n\tif (ret == -ERESTARTSYS)\n\t\tret = -EINTR;\nout_free:\n\tif (req->flags & REQ_F_BUFFER_SELECTED)\n\t\tcflags = io_put_recv_kbuf(req);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, issue_flags, ret, cflags);\n\treturn 0;\n}\n\nstatic int io_accept_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_accept *accept = &req->accept;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->len || sqe->buf_index)\n\t\treturn -EINVAL;\n\n\taccept->addr = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\taccept->addr_len = u64_to_user_ptr(READ_ONCE(sqe->addr2));\n\taccept->flags = READ_ONCE(sqe->accept_flags);\n\taccept->nofile = rlimit(RLIMIT_NOFILE);\n\treturn 0;\n}\n\nstatic int io_accept(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_accept *accept = &req->accept;\n\tbool force_nonblock = issue_flags & IO_URING_F_NONBLOCK;\n\tunsigned int file_flags = force_nonblock ? O_NONBLOCK : 0;\n\tint ret;\n\n\tif (req->file->f_flags & O_NONBLOCK)\n\t\treq->flags |= REQ_F_NOWAIT;\n\n\tret = __sys_accept4_file(req->file, file_flags, accept->addr,\n\t\t\t\t\taccept->addr_len, accept->flags,\n\t\t\t\t\taccept->nofile);\n\tif (ret == -EAGAIN && force_nonblock)\n\t\treturn -EAGAIN;\n\tif (ret < 0) {\n\t\tif (ret == -ERESTARTSYS)\n\t\t\tret = -EINTR;\n\t\treq_set_fail_links(req);\n\t}\n\t__io_req_complete(req, issue_flags, ret, 0);\n\treturn 0;\n}\n\nstatic int io_connect_prep_async(struct io_kiocb *req)\n{\n\tstruct io_async_connect *io = req->async_data;\n\tstruct io_connect *conn = &req->connect;\n\n\treturn move_addr_to_kernel(conn->addr, conn->addr_len, &io->address);\n}\n\nstatic int io_connect_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_connect *conn = &req->connect;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->len || sqe->buf_index || sqe->rw_flags)\n\t\treturn -EINVAL;\n\n\tconn->addr = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\tconn->addr_len =  READ_ONCE(sqe->addr2);\n\treturn 0;\n}\n\nstatic int io_connect(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_async_connect __io, *io;\n\tunsigned file_flags;\n\tint ret;\n\tbool force_nonblock = issue_flags & IO_URING_F_NONBLOCK;\n\n\tif (req->async_data) {\n\t\tio = req->async_data;\n\t} else {\n\t\tret = move_addr_to_kernel(req->connect.addr,\n\t\t\t\t\t\treq->connect.addr_len,\n\t\t\t\t\t\t&__io.address);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tio = &__io;\n\t}\n\n\tfile_flags = force_nonblock ? O_NONBLOCK : 0;\n\n\tret = __sys_connect_file(req->file, &io->address,\n\t\t\t\t\treq->connect.addr_len, file_flags);\n\tif ((ret == -EAGAIN || ret == -EINPROGRESS) && force_nonblock) {\n\t\tif (req->async_data)\n\t\t\treturn -EAGAIN;\n\t\tif (io_alloc_async_data(req)) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tio = req->async_data;\n\t\tmemcpy(req->async_data, &__io, sizeof(__io));\n\t\treturn -EAGAIN;\n\t}\n\tif (ret == -ERESTARTSYS)\n\t\tret = -EINTR;\nout:\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, issue_flags, ret, 0);\n\treturn 0;\n}\n#else /* !CONFIG_NET */\n#define IO_NETOP_FN(op)\t\t\t\t\t\t\t\\\nstatic int io_##op(struct io_kiocb *req, unsigned int issue_flags)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn -EOPNOTSUPP;\t\t\t\t\t\t\\\n}\n\n#define IO_NETOP_PREP(op)\t\t\t\t\t\t\\\nIO_NETOP_FN(op)\t\t\t\t\t\t\t\t\\\nstatic int io_##op##_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe) \\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn -EOPNOTSUPP;\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\n#define IO_NETOP_PREP_ASYNC(op)\t\t\t\t\t\t\\\nIO_NETOP_PREP(op)\t\t\t\t\t\t\t\\\nstatic int io_##op##_prep_async(struct io_kiocb *req)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn -EOPNOTSUPP;\t\t\t\t\t\t\\\n}\n\nIO_NETOP_PREP_ASYNC(sendmsg);\nIO_NETOP_PREP_ASYNC(recvmsg);\nIO_NETOP_PREP_ASYNC(connect);\nIO_NETOP_PREP(accept);\nIO_NETOP_FN(send);\nIO_NETOP_FN(recv);\n#endif /* CONFIG_NET */\n\nstruct io_poll_table {\n\tstruct poll_table_struct pt;\n\tstruct io_kiocb *req;\n\tint error;\n};\n\nstatic int __io_async_wake(struct io_kiocb *req, struct io_poll_iocb *poll,\n\t\t\t   __poll_t mask, task_work_func_t func)\n{\n\tint ret;\n\n\t/* for instances that support it check for an event match first: */\n\tif (mask && !(mask & poll->events))\n\t\treturn 0;\n\n\ttrace_io_uring_task_add(req->ctx, req->opcode, req->user_data, mask);\n\n\tlist_del_init(&poll->wait.entry);\n\n\treq->result = mask;\n\treq->task_work.func = func;\n\tpercpu_ref_get(&req->ctx->refs);\n\n\t/*\n\t * If this fails, then the task is exiting. When a task exits, the\n\t * work gets canceled, so just cancel this request as well instead\n\t * of executing it. We can't safely execute it anyway, as we may not\n\t * have the needed state needed for it anyway.\n\t */\n\tret = io_req_task_work_add(req);\n\tif (unlikely(ret)) {\n\t\tWRITE_ONCE(poll->canceled, true);\n\t\tio_req_task_work_add_fallback(req, func);\n\t}\n\treturn 1;\n}\n\nstatic bool io_poll_rewait(struct io_kiocb *req, struct io_poll_iocb *poll)\n\t__acquires(&req->ctx->completion_lock)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!req->result && !READ_ONCE(poll->canceled)) {\n\t\tstruct poll_table_struct pt = { ._key = poll->events };\n\n\t\treq->result = vfs_poll(req->file, &pt) & poll->events;\n\t}\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tif (!req->result && !READ_ONCE(poll->canceled)) {\n\t\tadd_wait_queue(poll->head, &poll->wait);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic struct io_poll_iocb *io_poll_get_double(struct io_kiocb *req)\n{\n\t/* pure poll stashes this in ->async_data, poll driven retry elsewhere */\n\tif (req->opcode == IORING_OP_POLL_ADD)\n\t\treturn req->async_data;\n\treturn req->apoll->double_poll;\n}\n\nstatic struct io_poll_iocb *io_poll_get_single(struct io_kiocb *req)\n{\n\tif (req->opcode == IORING_OP_POLL_ADD)\n\t\treturn &req->poll;\n\treturn &req->apoll->poll;\n}\n\nstatic void io_poll_remove_double(struct io_kiocb *req)\n{\n\tstruct io_poll_iocb *poll = io_poll_get_double(req);\n\n\tlockdep_assert_held(&req->ctx->completion_lock);\n\n\tif (poll && poll->head) {\n\t\tstruct wait_queue_head *head = poll->head;\n\n\t\tspin_lock(&head->lock);\n\t\tlist_del_init(&poll->wait.entry);\n\t\tif (poll->wait.private)\n\t\t\trefcount_dec(&req->refs);\n\t\tpoll->head = NULL;\n\t\tspin_unlock(&head->lock);\n\t}\n}\n\nstatic void io_poll_complete(struct io_kiocb *req, __poll_t mask, int error)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_poll_remove_double(req);\n\treq->poll.done = true;\n\tio_cqring_fill_event(req, error ? error : mangle_poll(mask));\n\tio_commit_cqring(ctx);\n}\n\nstatic void io_poll_task_func(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_kiocb *nxt;\n\n\tif (io_poll_rewait(req, &req->poll)) {\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t} else {\n\t\thash_del(&req->hash_node);\n\t\tio_poll_complete(req, req->result, 0);\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\n\t\tnxt = io_put_req_find_next(req);\n\t\tio_cqring_ev_posted(ctx);\n\t\tif (nxt)\n\t\t\t__io_req_task_submit(nxt);\n\t}\n\n\tpercpu_ref_put(&ctx->refs);\n}\n\nstatic int io_poll_double_wake(struct wait_queue_entry *wait, unsigned mode,\n\t\t\t       int sync, void *key)\n{\n\tstruct io_kiocb *req = wait->private;\n\tstruct io_poll_iocb *poll = io_poll_get_single(req);\n\t__poll_t mask = key_to_poll(key);\n\n\t/* for instances that support it check for an event match first: */\n\tif (mask && !(mask & poll->events))\n\t\treturn 0;\n\n\tlist_del_init(&wait->entry);\n\n\tif (poll && poll->head) {\n\t\tbool done;\n\n\t\tspin_lock(&poll->head->lock);\n\t\tdone = list_empty(&poll->wait.entry);\n\t\tif (!done)\n\t\t\tlist_del_init(&poll->wait.entry);\n\t\t/* make sure double remove sees this as being gone */\n\t\twait->private = NULL;\n\t\tspin_unlock(&poll->head->lock);\n\t\tif (!done) {\n\t\t\t/* use wait func handler, so it matches the rq type */\n\t\t\tpoll->wait.func(&poll->wait, mode, sync, key);\n\t\t}\n\t}\n\trefcount_dec(&req->refs);\n\treturn 1;\n}\n\nstatic void io_init_poll_iocb(struct io_poll_iocb *poll, __poll_t events,\n\t\t\t      wait_queue_func_t wake_func)\n{\n\tpoll->head = NULL;\n\tpoll->done = false;\n\tpoll->canceled = false;\n\tpoll->events = events;\n\tINIT_LIST_HEAD(&poll->wait.entry);\n\tinit_waitqueue_func_entry(&poll->wait, wake_func);\n}\n\nstatic void __io_queue_proc(struct io_poll_iocb *poll, struct io_poll_table *pt,\n\t\t\t    struct wait_queue_head *head,\n\t\t\t    struct io_poll_iocb **poll_ptr)\n{\n\tstruct io_kiocb *req = pt->req;\n\n\t/*\n\t * If poll->head is already set, it's because the file being polled\n\t * uses multiple waitqueues for poll handling (eg one for read, one\n\t * for write). Setup a separate io_poll_iocb if this happens.\n\t */\n\tif (unlikely(poll->head)) {\n\t\tstruct io_poll_iocb *poll_one = poll;\n\n\t\t/* already have a 2nd entry, fail a third attempt */\n\t\tif (*poll_ptr) {\n\t\t\tpt->error = -EINVAL;\n\t\t\treturn;\n\t\t}\n\t\t/* double add on the same waitqueue head, ignore */\n\t\tif (poll->head == head)\n\t\t\treturn;\n\t\tpoll = kmalloc(sizeof(*poll), GFP_ATOMIC);\n\t\tif (!poll) {\n\t\t\tpt->error = -ENOMEM;\n\t\t\treturn;\n\t\t}\n\t\tio_init_poll_iocb(poll, poll_one->events, io_poll_double_wake);\n\t\trefcount_inc(&req->refs);\n\t\tpoll->wait.private = req;\n\t\t*poll_ptr = poll;\n\t}\n\n\tpt->error = 0;\n\tpoll->head = head;\n\n\tif (poll->events & EPOLLEXCLUSIVE)\n\t\tadd_wait_queue_exclusive(head, &poll->wait);\n\telse\n\t\tadd_wait_queue(head, &poll->wait);\n}\n\nstatic void io_async_queue_proc(struct file *file, struct wait_queue_head *head,\n\t\t\t       struct poll_table_struct *p)\n{\n\tstruct io_poll_table *pt = container_of(p, struct io_poll_table, pt);\n\tstruct async_poll *apoll = pt->req->apoll;\n\n\t__io_queue_proc(&apoll->poll, pt, head, &apoll->double_poll);\n}\n\nstatic void io_async_task_func(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct async_poll *apoll = req->apoll;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\ttrace_io_uring_task_run(req->ctx, req->opcode, req->user_data);\n\n\tif (io_poll_rewait(req, &apoll->poll)) {\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t\tpercpu_ref_put(&ctx->refs);\n\t\treturn;\n\t}\n\n\t/* If req is still hashed, it cannot have been canceled. Don't check. */\n\tif (hash_hashed(&req->hash_node))\n\t\thash_del(&req->hash_node);\n\n\tio_poll_remove_double(req);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tif (!READ_ONCE(apoll->poll.canceled))\n\t\t__io_req_task_submit(req);\n\telse\n\t\t__io_req_task_cancel(req, -ECANCELED);\n\n\tpercpu_ref_put(&ctx->refs);\n\tkfree(apoll->double_poll);\n\tkfree(apoll);\n}\n\nstatic int io_async_wake(struct wait_queue_entry *wait, unsigned mode, int sync,\n\t\t\tvoid *key)\n{\n\tstruct io_kiocb *req = wait->private;\n\tstruct io_poll_iocb *poll = &req->apoll->poll;\n\n\ttrace_io_uring_poll_wake(req->ctx, req->opcode, req->user_data,\n\t\t\t\t\tkey_to_poll(key));\n\n\treturn __io_async_wake(req, poll, key_to_poll(key), io_async_task_func);\n}\n\nstatic void io_poll_req_insert(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct hlist_head *list;\n\n\tlist = &ctx->cancel_hash[hash_long(req->user_data, ctx->cancel_hash_bits)];\n\thlist_add_head(&req->hash_node, list);\n}\n\nstatic __poll_t __io_arm_poll_handler(struct io_kiocb *req,\n\t\t\t\t      struct io_poll_iocb *poll,\n\t\t\t\t      struct io_poll_table *ipt, __poll_t mask,\n\t\t\t\t      wait_queue_func_t wake_func)\n\t__acquires(&ctx->completion_lock)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tbool cancel = false;\n\n\tINIT_HLIST_NODE(&req->hash_node);\n\tio_init_poll_iocb(poll, mask, wake_func);\n\tpoll->file = req->file;\n\tpoll->wait.private = req;\n\n\tipt->pt._key = mask;\n\tipt->req = req;\n\tipt->error = -EINVAL;\n\n\tmask = vfs_poll(req->file, &ipt->pt) & poll->events;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tif (likely(poll->head)) {\n\t\tspin_lock(&poll->head->lock);\n\t\tif (unlikely(list_empty(&poll->wait.entry))) {\n\t\t\tif (ipt->error)\n\t\t\t\tcancel = true;\n\t\t\tipt->error = 0;\n\t\t\tmask = 0;\n\t\t}\n\t\tif (mask || ipt->error)\n\t\t\tlist_del_init(&poll->wait.entry);\n\t\telse if (cancel)\n\t\t\tWRITE_ONCE(poll->canceled, true);\n\t\telse if (!poll->done) /* actually waiting for an event */\n\t\t\tio_poll_req_insert(req);\n\t\tspin_unlock(&poll->head->lock);\n\t}\n\n\treturn mask;\n}\n\nstatic bool io_arm_poll_handler(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct async_poll *apoll;\n\tstruct io_poll_table ipt;\n\t__poll_t mask, ret;\n\tint rw;\n\n\tif (!req->file || !file_can_poll(req->file))\n\t\treturn false;\n\tif (req->flags & REQ_F_POLLED)\n\t\treturn false;\n\tif (def->pollin)\n\t\trw = READ;\n\telse if (def->pollout)\n\t\trw = WRITE;\n\telse\n\t\treturn false;\n\t/* if we can't nonblock try, then no point in arming a poll handler */\n\tif (!io_file_supports_async(req->file, rw))\n\t\treturn false;\n\n\tapoll = kmalloc(sizeof(*apoll), GFP_ATOMIC);\n\tif (unlikely(!apoll))\n\t\treturn false;\n\tapoll->double_poll = NULL;\n\n\treq->flags |= REQ_F_POLLED;\n\treq->apoll = apoll;\n\n\tmask = 0;\n\tif (def->pollin)\n\t\tmask |= POLLIN | POLLRDNORM;\n\tif (def->pollout)\n\t\tmask |= POLLOUT | POLLWRNORM;\n\n\t/* If reading from MSG_ERRQUEUE using recvmsg, ignore POLLIN */\n\tif ((req->opcode == IORING_OP_RECVMSG) &&\n\t    (req->sr_msg.msg_flags & MSG_ERRQUEUE))\n\t\tmask &= ~POLLIN;\n\n\tmask |= POLLERR | POLLPRI;\n\n\tipt.pt._qproc = io_async_queue_proc;\n\n\tret = __io_arm_poll_handler(req, &apoll->poll, &ipt, mask,\n\t\t\t\t\tio_async_wake);\n\tif (ret || ipt.error) {\n\t\tio_poll_remove_double(req);\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t\tkfree(apoll->double_poll);\n\t\tkfree(apoll);\n\t\treturn false;\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\ttrace_io_uring_poll_arm(ctx, req->opcode, req->user_data, mask,\n\t\t\t\t\tapoll->poll.events);\n\treturn true;\n}\n\nstatic bool __io_poll_remove_one(struct io_kiocb *req,\n\t\t\t\t struct io_poll_iocb *poll)\n{\n\tbool do_complete = false;\n\n\tspin_lock(&poll->head->lock);\n\tWRITE_ONCE(poll->canceled, true);\n\tif (!list_empty(&poll->wait.entry)) {\n\t\tlist_del_init(&poll->wait.entry);\n\t\tdo_complete = true;\n\t}\n\tspin_unlock(&poll->head->lock);\n\thash_del(&req->hash_node);\n\treturn do_complete;\n}\n\nstatic bool io_poll_remove_one(struct io_kiocb *req)\n{\n\tbool do_complete;\n\n\tio_poll_remove_double(req);\n\n\tif (req->opcode == IORING_OP_POLL_ADD) {\n\t\tdo_complete = __io_poll_remove_one(req, &req->poll);\n\t} else {\n\t\tstruct async_poll *apoll = req->apoll;\n\n\t\t/* non-poll requests have submit ref still */\n\t\tdo_complete = __io_poll_remove_one(req, &apoll->poll);\n\t\tif (do_complete) {\n\t\t\tio_put_req(req);\n\t\t\tkfree(apoll->double_poll);\n\t\t\tkfree(apoll);\n\t\t}\n\t}\n\n\tif (do_complete) {\n\t\tio_cqring_fill_event(req, -ECANCELED);\n\t\tio_commit_cqring(req->ctx);\n\t\treq_set_fail_links(req);\n\t\tio_put_req_deferred(req, 1);\n\t}\n\n\treturn do_complete;\n}\n\n/*\n * Returns true if we found and killed one or more poll requests\n */\nstatic bool io_poll_remove_all(struct io_ring_ctx *ctx, struct task_struct *tsk,\n\t\t\t       struct files_struct *files)\n{\n\tstruct hlist_node *tmp;\n\tstruct io_kiocb *req;\n\tint posted = 0, i;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tfor (i = 0; i < (1U << ctx->cancel_hash_bits); i++) {\n\t\tstruct hlist_head *list;\n\n\t\tlist = &ctx->cancel_hash[i];\n\t\thlist_for_each_entry_safe(req, tmp, list, hash_node) {\n\t\t\tif (io_match_task(req, tsk, files))\n\t\t\t\tposted += io_poll_remove_one(req);\n\t\t}\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tif (posted)\n\t\tio_cqring_ev_posted(ctx);\n\n\treturn posted != 0;\n}\n\nstatic int io_poll_cancel(struct io_ring_ctx *ctx, __u64 sqe_addr)\n{\n\tstruct hlist_head *list;\n\tstruct io_kiocb *req;\n\n\tlist = &ctx->cancel_hash[hash_long(sqe_addr, ctx->cancel_hash_bits)];\n\thlist_for_each_entry(req, list, hash_node) {\n\t\tif (sqe_addr != req->user_data)\n\t\t\tcontinue;\n\t\tif (io_poll_remove_one(req))\n\t\t\treturn 0;\n\t\treturn -EALREADY;\n\t}\n\n\treturn -ENOENT;\n}\n\nstatic int io_poll_remove_prep(struct io_kiocb *req,\n\t\t\t       const struct io_uring_sqe *sqe)\n{\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->off || sqe->len || sqe->buf_index ||\n\t    sqe->poll_events)\n\t\treturn -EINVAL;\n\n\treq->poll_remove.addr = READ_ONCE(sqe->addr);\n\treturn 0;\n}\n\n/*\n * Find a running poll command that matches one specified in sqe->addr,\n * and remove it if found.\n */\nstatic int io_poll_remove(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tret = io_poll_cancel(ctx, req->poll_remove.addr);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_poll_wake(struct wait_queue_entry *wait, unsigned mode, int sync,\n\t\t\tvoid *key)\n{\n\tstruct io_kiocb *req = wait->private;\n\tstruct io_poll_iocb *poll = &req->poll;\n\n\treturn __io_async_wake(req, poll, key_to_poll(key), io_poll_task_func);\n}\n\nstatic void io_poll_queue_proc(struct file *file, struct wait_queue_head *head,\n\t\t\t       struct poll_table_struct *p)\n{\n\tstruct io_poll_table *pt = container_of(p, struct io_poll_table, pt);\n\n\t__io_queue_proc(&pt->req->poll, pt, head, (struct io_poll_iocb **) &pt->req->async_data);\n}\n\nstatic int io_poll_add_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_poll_iocb *poll = &req->poll;\n\tu32 events;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->addr || sqe->ioprio || sqe->off || sqe->len || sqe->buf_index)\n\t\treturn -EINVAL;\n\n\tevents = READ_ONCE(sqe->poll32_events);\n#ifdef __BIG_ENDIAN\n\tevents = swahw32(events);\n#endif\n\tpoll->events = demangle_poll(events) | EPOLLERR | EPOLLHUP |\n\t\t       (events & EPOLLEXCLUSIVE);\n\treturn 0;\n}\n\nstatic int io_poll_add(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_poll_iocb *poll = &req->poll;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_poll_table ipt;\n\t__poll_t mask;\n\n\tipt.pt._qproc = io_poll_queue_proc;\n\n\tmask = __io_arm_poll_handler(req, &req->poll, &ipt, poll->events,\n\t\t\t\t\tio_poll_wake);\n\n\tif (mask) { /* no async, we'd stolen it */\n\t\tipt.error = 0;\n\t\tio_poll_complete(req, mask, 0);\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tif (mask) {\n\t\tio_cqring_ev_posted(ctx);\n\t\tio_put_req(req);\n\t}\n\treturn ipt.error;\n}\n\nstatic enum hrtimer_restart io_timeout_fn(struct hrtimer *timer)\n{\n\tstruct io_timeout_data *data = container_of(timer,\n\t\t\t\t\t\tstruct io_timeout_data, timer);\n\tstruct io_kiocb *req = data->req;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tlist_del_init(&req->timeout.list);\n\tatomic_set(&req->ctx->cq_timeouts,\n\t\tatomic_read(&req->ctx->cq_timeouts) + 1);\n\n\tio_cqring_fill_event(req, -ETIME);\n\tio_commit_cqring(ctx);\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\tio_cqring_ev_posted(ctx);\n\treq_set_fail_links(req);\n\tio_put_req(req);\n\treturn HRTIMER_NORESTART;\n}\n\nstatic struct io_kiocb *io_timeout_extract(struct io_ring_ctx *ctx,\n\t\t\t\t\t   __u64 user_data)\n{\n\tstruct io_timeout_data *io;\n\tstruct io_kiocb *req;\n\tint ret = -ENOENT;\n\n\tlist_for_each_entry(req, &ctx->timeout_list, timeout.list) {\n\t\tif (user_data == req->user_data) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (ret == -ENOENT)\n\t\treturn ERR_PTR(ret);\n\n\tio = req->async_data;\n\tret = hrtimer_try_to_cancel(&io->timer);\n\tif (ret == -1)\n\t\treturn ERR_PTR(-EALREADY);\n\tlist_del_init(&req->timeout.list);\n\treturn req;\n}\n\nstatic int io_timeout_cancel(struct io_ring_ctx *ctx, __u64 user_data)\n{\n\tstruct io_kiocb *req = io_timeout_extract(ctx, user_data);\n\n\tif (IS_ERR(req))\n\t\treturn PTR_ERR(req);\n\n\treq_set_fail_links(req);\n\tio_cqring_fill_event(req, -ECANCELED);\n\tio_put_req_deferred(req, 1);\n\treturn 0;\n}\n\nstatic int io_timeout_update(struct io_ring_ctx *ctx, __u64 user_data,\n\t\t\t     struct timespec64 *ts, enum hrtimer_mode mode)\n{\n\tstruct io_kiocb *req = io_timeout_extract(ctx, user_data);\n\tstruct io_timeout_data *data;\n\n\tif (IS_ERR(req))\n\t\treturn PTR_ERR(req);\n\n\treq->timeout.off = 0; /* noseq */\n\tdata = req->async_data;\n\tlist_add_tail(&req->timeout.list, &ctx->timeout_list);\n\thrtimer_init(&data->timer, CLOCK_MONOTONIC, mode);\n\tdata->timer.function = io_timeout_fn;\n\thrtimer_start(&data->timer, timespec64_to_ktime(*ts), mode);\n\treturn 0;\n}\n\nstatic int io_timeout_remove_prep(struct io_kiocb *req,\n\t\t\t\t  const struct io_uring_sqe *sqe)\n{\n\tstruct io_timeout_rem *tr = &req->timeout_rem;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (unlikely(req->flags & (REQ_F_FIXED_FILE | REQ_F_BUFFER_SELECT)))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->buf_index || sqe->len)\n\t\treturn -EINVAL;\n\n\ttr->addr = READ_ONCE(sqe->addr);\n\ttr->flags = READ_ONCE(sqe->timeout_flags);\n\tif (tr->flags & IORING_TIMEOUT_UPDATE) {\n\t\tif (tr->flags & ~(IORING_TIMEOUT_UPDATE|IORING_TIMEOUT_ABS))\n\t\t\treturn -EINVAL;\n\t\tif (get_timespec64(&tr->ts, u64_to_user_ptr(sqe->addr2)))\n\t\t\treturn -EFAULT;\n\t} else if (tr->flags) {\n\t\t/* timeout removal doesn't support flags */\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic inline enum hrtimer_mode io_translate_timeout_mode(unsigned int flags)\n{\n\treturn (flags & IORING_TIMEOUT_ABS) ? HRTIMER_MODE_ABS\n\t\t\t\t\t    : HRTIMER_MODE_REL;\n}\n\n/*\n * Remove or update an existing timeout command\n */\nstatic int io_timeout_remove(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_timeout_rem *tr = &req->timeout_rem;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tif (!(req->timeout_rem.flags & IORING_TIMEOUT_UPDATE))\n\t\tret = io_timeout_cancel(ctx, tr->addr);\n\telse\n\t\tret = io_timeout_update(ctx, tr->addr, &tr->ts,\n\t\t\t\t\tio_translate_timeout_mode(tr->flags));\n\n\tio_cqring_fill_event(req, ret);\n\tio_commit_cqring(ctx);\n\tspin_unlock_irq(&ctx->completion_lock);\n\tio_cqring_ev_posted(ctx);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_put_req(req);\n\treturn 0;\n}\n\nstatic int io_timeout_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,\n\t\t\t   bool is_timeout_link)\n{\n\tstruct io_timeout_data *data;\n\tunsigned flags;\n\tu32 off = READ_ONCE(sqe->off);\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->buf_index || sqe->len != 1)\n\t\treturn -EINVAL;\n\tif (off && is_timeout_link)\n\t\treturn -EINVAL;\n\tflags = READ_ONCE(sqe->timeout_flags);\n\tif (flags & ~IORING_TIMEOUT_ABS)\n\t\treturn -EINVAL;\n\n\treq->timeout.off = off;\n\n\tif (!req->async_data && io_alloc_async_data(req))\n\t\treturn -ENOMEM;\n\n\tdata = req->async_data;\n\tdata->req = req;\n\n\tif (get_timespec64(&data->ts, u64_to_user_ptr(sqe->addr)))\n\t\treturn -EFAULT;\n\n\tdata->mode = io_translate_timeout_mode(flags);\n\thrtimer_init(&data->timer, CLOCK_MONOTONIC, data->mode);\n\treturn 0;\n}\n\nstatic int io_timeout(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_timeout_data *data = req->async_data;\n\tstruct list_head *entry;\n\tu32 tail, off = req->timeout.off;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\n\t/*\n\t * sqe->off holds how many events that need to occur for this\n\t * timeout event to be satisfied. If it isn't set, then this is\n\t * a pure timeout request, sequence isn't used.\n\t */\n\tif (io_is_timeout_noseq(req)) {\n\t\tentry = ctx->timeout_list.prev;\n\t\tgoto add;\n\t}\n\n\ttail = ctx->cached_cq_tail - atomic_read(&ctx->cq_timeouts);\n\treq->timeout.target_seq = tail + off;\n\n\t/* Update the last seq here in case io_flush_timeouts() hasn't.\n\t * This is safe because ->completion_lock is held, and submissions\n\t * and completions are never mixed in the same ->completion_lock section.\n\t */\n\tctx->cq_last_tm_flush = tail;\n\n\t/*\n\t * Insertion sort, ensuring the first entry in the list is always\n\t * the one we need first.\n\t */\n\tlist_for_each_prev(entry, &ctx->timeout_list) {\n\t\tstruct io_kiocb *nxt = list_entry(entry, struct io_kiocb,\n\t\t\t\t\t\t  timeout.list);\n\n\t\tif (io_is_timeout_noseq(nxt))\n\t\t\tcontinue;\n\t\t/* nxt.seq is behind @tail, otherwise would've been completed */\n\t\tif (off >= nxt->timeout.target_seq - tail)\n\t\t\tbreak;\n\t}\nadd:\n\tlist_add(&req->timeout.list, entry);\n\tdata->timer.function = io_timeout_fn;\n\thrtimer_start(&data->timer, timespec64_to_ktime(data->ts), data->mode);\n\tspin_unlock_irq(&ctx->completion_lock);\n\treturn 0;\n}\n\nstatic bool io_cancel_cb(struct io_wq_work *work, void *data)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\n\treturn req->user_data == (unsigned long) data;\n}\n\nstatic int io_async_cancel_one(struct io_uring_task *tctx, void *sqe_addr)\n{\n\tenum io_wq_cancel cancel_ret;\n\tint ret = 0;\n\n\tif (!tctx->io_wq)\n\t\treturn -ENOENT;\n\n\tcancel_ret = io_wq_cancel_cb(tctx->io_wq, io_cancel_cb, sqe_addr, false);\n\tswitch (cancel_ret) {\n\tcase IO_WQ_CANCEL_OK:\n\t\tret = 0;\n\t\tbreak;\n\tcase IO_WQ_CANCEL_RUNNING:\n\t\tret = -EALREADY;\n\t\tbreak;\n\tcase IO_WQ_CANCEL_NOTFOUND:\n\t\tret = -ENOENT;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic void io_async_find_and_cancel(struct io_ring_ctx *ctx,\n\t\t\t\t     struct io_kiocb *req, __u64 sqe_addr,\n\t\t\t\t     int success_ret)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tret = io_async_cancel_one(req->task->io_uring,\n\t\t\t\t\t(void *) (unsigned long) sqe_addr);\n\tif (ret != -ENOENT) {\n\t\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\t\tgoto done;\n\t}\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tret = io_timeout_cancel(ctx, sqe_addr);\n\tif (ret != -ENOENT)\n\t\tgoto done;\n\tret = io_poll_cancel(ctx, sqe_addr);\ndone:\n\tif (!ret)\n\t\tret = success_ret;\n\tio_cqring_fill_event(req, ret);\n\tio_commit_cqring(ctx);\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\tio_cqring_ev_posted(ctx);\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_put_req(req);\n}\n\nstatic int io_async_cancel_prep(struct io_kiocb *req,\n\t\t\t\tconst struct io_uring_sqe *sqe)\n{\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (unlikely(req->flags & (REQ_F_FIXED_FILE | REQ_F_BUFFER_SELECT)))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->off || sqe->len || sqe->cancel_flags)\n\t\treturn -EINVAL;\n\n\treq->cancel.addr = READ_ONCE(sqe->addr);\n\treturn 0;\n}\n\nstatic int io_async_cancel(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_async_find_and_cancel(ctx, req, req->cancel.addr, 0);\n\treturn 0;\n}\n\nstatic int io_rsrc_update_prep(struct io_kiocb *req,\n\t\t\t\tconst struct io_uring_sqe *sqe)\n{\n\tif (unlikely(req->ctx->flags & IORING_SETUP_SQPOLL))\n\t\treturn -EINVAL;\n\tif (unlikely(req->flags & (REQ_F_FIXED_FILE | REQ_F_BUFFER_SELECT)))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->rw_flags)\n\t\treturn -EINVAL;\n\n\treq->rsrc_update.offset = READ_ONCE(sqe->off);\n\treq->rsrc_update.nr_args = READ_ONCE(sqe->len);\n\tif (!req->rsrc_update.nr_args)\n\t\treturn -EINVAL;\n\treq->rsrc_update.arg = READ_ONCE(sqe->addr);\n\treturn 0;\n}\n\nstatic int io_files_update(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_uring_rsrc_update up;\n\tint ret;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\n\tup.offset = req->rsrc_update.offset;\n\tup.data = req->rsrc_update.arg;\n\n\tmutex_lock(&ctx->uring_lock);\n\tret = __io_sqe_files_update(ctx, &up, req->rsrc_update.nr_args);\n\tmutex_unlock(&ctx->uring_lock);\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, issue_flags, ret, 0);\n\treturn 0;\n}\n\nstatic int io_req_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tswitch (req->opcode) {\n\tcase IORING_OP_NOP:\n\t\treturn 0;\n\tcase IORING_OP_READV:\n\tcase IORING_OP_READ_FIXED:\n\tcase IORING_OP_READ:\n\t\treturn io_read_prep(req, sqe);\n\tcase IORING_OP_WRITEV:\n\tcase IORING_OP_WRITE_FIXED:\n\tcase IORING_OP_WRITE:\n\t\treturn io_write_prep(req, sqe);\n\tcase IORING_OP_POLL_ADD:\n\t\treturn io_poll_add_prep(req, sqe);\n\tcase IORING_OP_POLL_REMOVE:\n\t\treturn io_poll_remove_prep(req, sqe);\n\tcase IORING_OP_FSYNC:\n\t\treturn io_fsync_prep(req, sqe);\n\tcase IORING_OP_SYNC_FILE_RANGE:\n\t\treturn io_sfr_prep(req, sqe);\n\tcase IORING_OP_SENDMSG:\n\tcase IORING_OP_SEND:\n\t\treturn io_sendmsg_prep(req, sqe);\n\tcase IORING_OP_RECVMSG:\n\tcase IORING_OP_RECV:\n\t\treturn io_recvmsg_prep(req, sqe);\n\tcase IORING_OP_CONNECT:\n\t\treturn io_connect_prep(req, sqe);\n\tcase IORING_OP_TIMEOUT:\n\t\treturn io_timeout_prep(req, sqe, false);\n\tcase IORING_OP_TIMEOUT_REMOVE:\n\t\treturn io_timeout_remove_prep(req, sqe);\n\tcase IORING_OP_ASYNC_CANCEL:\n\t\treturn io_async_cancel_prep(req, sqe);\n\tcase IORING_OP_LINK_TIMEOUT:\n\t\treturn io_timeout_prep(req, sqe, true);\n\tcase IORING_OP_ACCEPT:\n\t\treturn io_accept_prep(req, sqe);\n\tcase IORING_OP_FALLOCATE:\n\t\treturn io_fallocate_prep(req, sqe);\n\tcase IORING_OP_OPENAT:\n\t\treturn io_openat_prep(req, sqe);\n\tcase IORING_OP_CLOSE:\n\t\treturn io_close_prep(req, sqe);\n\tcase IORING_OP_FILES_UPDATE:\n\t\treturn io_rsrc_update_prep(req, sqe);\n\tcase IORING_OP_STATX:\n\t\treturn io_statx_prep(req, sqe);\n\tcase IORING_OP_FADVISE:\n\t\treturn io_fadvise_prep(req, sqe);\n\tcase IORING_OP_MADVISE:\n\t\treturn io_madvise_prep(req, sqe);\n\tcase IORING_OP_OPENAT2:\n\t\treturn io_openat2_prep(req, sqe);\n\tcase IORING_OP_EPOLL_CTL:\n\t\treturn io_epoll_ctl_prep(req, sqe);\n\tcase IORING_OP_SPLICE:\n\t\treturn io_splice_prep(req, sqe);\n\tcase IORING_OP_PROVIDE_BUFFERS:\n\t\treturn io_provide_buffers_prep(req, sqe);\n\tcase IORING_OP_REMOVE_BUFFERS:\n\t\treturn io_remove_buffers_prep(req, sqe);\n\tcase IORING_OP_TEE:\n\t\treturn io_tee_prep(req, sqe);\n\tcase IORING_OP_SHUTDOWN:\n\t\treturn io_shutdown_prep(req, sqe);\n\tcase IORING_OP_RENAMEAT:\n\t\treturn io_renameat_prep(req, sqe);\n\tcase IORING_OP_UNLINKAT:\n\t\treturn io_unlinkat_prep(req, sqe);\n\t}\n\n\tprintk_once(KERN_WARNING \"io_uring: unhandled opcode %d\\n\",\n\t\t\treq->opcode);\n\treturn-EINVAL;\n}\n\nstatic int io_req_prep_async(struct io_kiocb *req)\n{\n\tswitch (req->opcode) {\n\tcase IORING_OP_READV:\n\tcase IORING_OP_READ_FIXED:\n\tcase IORING_OP_READ:\n\t\treturn io_rw_prep_async(req, READ);\n\tcase IORING_OP_WRITEV:\n\tcase IORING_OP_WRITE_FIXED:\n\tcase IORING_OP_WRITE:\n\t\treturn io_rw_prep_async(req, WRITE);\n\tcase IORING_OP_SENDMSG:\n\tcase IORING_OP_SEND:\n\t\treturn io_sendmsg_prep_async(req);\n\tcase IORING_OP_RECVMSG:\n\tcase IORING_OP_RECV:\n\t\treturn io_recvmsg_prep_async(req);\n\tcase IORING_OP_CONNECT:\n\t\treturn io_connect_prep_async(req);\n\t}\n\treturn 0;\n}\n\nstatic int io_req_defer_prep(struct io_kiocb *req)\n{\n\tif (!io_op_defs[req->opcode].needs_async_data)\n\t\treturn 0;\n\t/* some opcodes init it during the inital prep */\n\tif (req->async_data)\n\t\treturn 0;\n\tif (__io_alloc_async_data(req))\n\t\treturn -EAGAIN;\n\treturn io_req_prep_async(req);\n}\n\nstatic u32 io_get_sequence(struct io_kiocb *req)\n{\n\tstruct io_kiocb *pos;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tu32 total_submitted, nr_reqs = 0;\n\n\tio_for_each_link(pos, req)\n\t\tnr_reqs++;\n\n\ttotal_submitted = ctx->cached_sq_head - ctx->cached_sq_dropped;\n\treturn total_submitted - nr_reqs;\n}\n\nstatic int io_req_defer(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_defer_entry *de;\n\tint ret;\n\tu32 seq;\n\n\t/* Still need defer if there is pending req in defer list. */\n\tif (likely(list_empty_careful(&ctx->defer_list) &&\n\t\t!(req->flags & REQ_F_IO_DRAIN)))\n\t\treturn 0;\n\n\tseq = io_get_sequence(req);\n\t/* Still a chance to pass the sequence check */\n\tif (!req_need_defer(req, seq) && list_empty_careful(&ctx->defer_list))\n\t\treturn 0;\n\n\tret = io_req_defer_prep(req);\n\tif (ret)\n\t\treturn ret;\n\tio_prep_async_link(req);\n\tde = kmalloc(sizeof(*de), GFP_KERNEL);\n\tif (!de)\n\t\treturn -ENOMEM;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tif (!req_need_defer(req, seq) && list_empty(&ctx->defer_list)) {\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t\tkfree(de);\n\t\tio_queue_async_work(req);\n\t\treturn -EIOCBQUEUED;\n\t}\n\n\ttrace_io_uring_defer(ctx, req, req->user_data);\n\tde->req = req;\n\tde->seq = seq;\n\tlist_add_tail(&de->list, &ctx->defer_list);\n\tspin_unlock_irq(&ctx->completion_lock);\n\treturn -EIOCBQUEUED;\n}\n\nstatic void __io_clean_op(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_BUFFER_SELECTED) {\n\t\tswitch (req->opcode) {\n\t\tcase IORING_OP_READV:\n\t\tcase IORING_OP_READ_FIXED:\n\t\tcase IORING_OP_READ:\n\t\t\tkfree((void *)(unsigned long)req->rw.addr);\n\t\t\tbreak;\n\t\tcase IORING_OP_RECVMSG:\n\t\tcase IORING_OP_RECV:\n\t\t\tkfree(req->sr_msg.kbuf);\n\t\t\tbreak;\n\t\t}\n\t\treq->flags &= ~REQ_F_BUFFER_SELECTED;\n\t}\n\n\tif (req->flags & REQ_F_NEED_CLEANUP) {\n\t\tswitch (req->opcode) {\n\t\tcase IORING_OP_READV:\n\t\tcase IORING_OP_READ_FIXED:\n\t\tcase IORING_OP_READ:\n\t\tcase IORING_OP_WRITEV:\n\t\tcase IORING_OP_WRITE_FIXED:\n\t\tcase IORING_OP_WRITE: {\n\t\t\tstruct io_async_rw *io = req->async_data;\n\t\t\tif (io->free_iovec)\n\t\t\t\tkfree(io->free_iovec);\n\t\t\tbreak;\n\t\t\t}\n\t\tcase IORING_OP_RECVMSG:\n\t\tcase IORING_OP_SENDMSG: {\n\t\t\tstruct io_async_msghdr *io = req->async_data;\n\n\t\t\tkfree(io->free_iov);\n\t\t\tbreak;\n\t\t\t}\n\t\tcase IORING_OP_SPLICE:\n\t\tcase IORING_OP_TEE:\n\t\t\tio_put_file(req, req->splice.file_in,\n\t\t\t\t    (req->splice.flags & SPLICE_F_FD_IN_FIXED));\n\t\t\tbreak;\n\t\tcase IORING_OP_OPENAT:\n\t\tcase IORING_OP_OPENAT2:\n\t\t\tif (req->open.filename)\n\t\t\t\tputname(req->open.filename);\n\t\t\tbreak;\n\t\tcase IORING_OP_RENAMEAT:\n\t\t\tputname(req->rename.oldpath);\n\t\t\tputname(req->rename.newpath);\n\t\t\tbreak;\n\t\tcase IORING_OP_UNLINKAT:\n\t\t\tputname(req->unlink.filename);\n\t\t\tbreak;\n\t\t}\n\t\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\t}\n}\n\nstatic int io_issue_sqe(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tconst struct cred *creds = NULL;\n\tint ret;\n\n\tif (req->work.personality) {\n\t\tconst struct cred *new_creds;\n\n\t\tif (!(issue_flags & IO_URING_F_NONBLOCK))\n\t\t\tmutex_lock(&ctx->uring_lock);\n\t\tnew_creds = idr_find(&ctx->personality_idr, req->work.personality);\n\t\tif (!(issue_flags & IO_URING_F_NONBLOCK))\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t\tif (!new_creds)\n\t\t\treturn -EINVAL;\n\t\tcreds = override_creds(new_creds);\n\t}\n\n\tswitch (req->opcode) {\n\tcase IORING_OP_NOP:\n\t\tret = io_nop(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_READV:\n\tcase IORING_OP_READ_FIXED:\n\tcase IORING_OP_READ:\n\t\tret = io_read(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_WRITEV:\n\tcase IORING_OP_WRITE_FIXED:\n\tcase IORING_OP_WRITE:\n\t\tret = io_write(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_FSYNC:\n\t\tret = io_fsync(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_POLL_ADD:\n\t\tret = io_poll_add(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_POLL_REMOVE:\n\t\tret = io_poll_remove(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_SYNC_FILE_RANGE:\n\t\tret = io_sync_file_range(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_SENDMSG:\n\t\tret = io_sendmsg(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_SEND:\n\t\tret = io_send(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_RECVMSG:\n\t\tret = io_recvmsg(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_RECV:\n\t\tret = io_recv(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_TIMEOUT:\n\t\tret = io_timeout(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_TIMEOUT_REMOVE:\n\t\tret = io_timeout_remove(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_ACCEPT:\n\t\tret = io_accept(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_CONNECT:\n\t\tret = io_connect(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_ASYNC_CANCEL:\n\t\tret = io_async_cancel(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_FALLOCATE:\n\t\tret = io_fallocate(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_OPENAT:\n\t\tret = io_openat(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_CLOSE:\n\t\tret = io_close(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_FILES_UPDATE:\n\t\tret = io_files_update(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_STATX:\n\t\tret = io_statx(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_FADVISE:\n\t\tret = io_fadvise(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_MADVISE:\n\t\tret = io_madvise(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_OPENAT2:\n\t\tret = io_openat2(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_EPOLL_CTL:\n\t\tret = io_epoll_ctl(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_SPLICE:\n\t\tret = io_splice(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_PROVIDE_BUFFERS:\n\t\tret = io_provide_buffers(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_REMOVE_BUFFERS:\n\t\tret = io_remove_buffers(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_TEE:\n\t\tret = io_tee(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_SHUTDOWN:\n\t\tret = io_shutdown(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_RENAMEAT:\n\t\tret = io_renameat(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_UNLINKAT:\n\t\tret = io_unlinkat(req, issue_flags);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\tif (creds)\n\t\trevert_creds(creds);\n\n\tif (ret)\n\t\treturn ret;\n\n\t/* If the op doesn't have a file, we're not polling for it */\n\tif ((ctx->flags & IORING_SETUP_IOPOLL) && req->file) {\n\t\tconst bool in_async = io_wq_current_is_worker();\n\n\t\t/* workqueue context doesn't hold uring_lock, grab it now */\n\t\tif (in_async)\n\t\t\tmutex_lock(&ctx->uring_lock);\n\n\t\tio_iopoll_req_issued(req, in_async);\n\n\t\tif (in_async)\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t}\n\n\treturn 0;\n}\n\nstatic void io_wq_submit_work(struct io_wq_work *work)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\tstruct io_kiocb *timeout;\n\tint ret = 0;\n\n\ttimeout = io_prep_linked_timeout(req);\n\tif (timeout)\n\t\tio_queue_linked_timeout(timeout);\n\n\tif (work->flags & IO_WQ_WORK_CANCEL)\n\t\tret = -ECANCELED;\n\n\tif (!ret) {\n\t\tdo {\n\t\t\tret = io_issue_sqe(req, 0);\n\t\t\t/*\n\t\t\t * We can get EAGAIN for polled IO even though we're\n\t\t\t * forcing a sync submission from here, since we can't\n\t\t\t * wait for request slots on the block side.\n\t\t\t */\n\t\t\tif (ret != -EAGAIN)\n\t\t\t\tbreak;\n\t\t\tcond_resched();\n\t\t} while (1);\n\t}\n\n\t/* avoid locking problems by failing it from a clean context */\n\tif (ret) {\n\t\t/* io-wq is going to take one down */\n\t\trefcount_inc(&req->refs);\n\t\tio_req_task_queue_fail(req, ret);\n\t}\n}\n\nstatic inline struct file *io_file_from_index(struct io_ring_ctx *ctx,\n\t\t\t\t\t      int index)\n{\n\tstruct fixed_rsrc_table *table;\n\n\ttable = &ctx->file_data->table[index >> IORING_FILE_TABLE_SHIFT];\n\treturn table->files[index & IORING_FILE_TABLE_MASK];\n}\n\nstatic struct file *io_file_get(struct io_submit_state *state,\n\t\t\t\tstruct io_kiocb *req, int fd, bool fixed)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct file *file;\n\n\tif (fixed) {\n\t\tif (unlikely((unsigned int)fd >= ctx->nr_user_files))\n\t\t\treturn NULL;\n\t\tfd = array_index_nospec(fd, ctx->nr_user_files);\n\t\tfile = io_file_from_index(ctx, fd);\n\t\tio_set_resource_node(req);\n\t} else {\n\t\ttrace_io_uring_file_get(ctx, fd);\n\t\tfile = __io_file_get(state, fd);\n\t}\n\n\tif (file && unlikely(file->f_op == &io_uring_fops))\n\t\tio_req_track_inflight(req);\n\treturn file;\n}\n\nstatic enum hrtimer_restart io_link_timeout_fn(struct hrtimer *timer)\n{\n\tstruct io_timeout_data *data = container_of(timer,\n\t\t\t\t\t\tstruct io_timeout_data, timer);\n\tstruct io_kiocb *prev, *req = data->req;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tprev = req->timeout.head;\n\treq->timeout.head = NULL;\n\n\t/*\n\t * We don't expect the list to be empty, that will only happen if we\n\t * race with the completion of the linked work.\n\t */\n\tif (prev && refcount_inc_not_zero(&prev->refs))\n\t\tio_remove_next_linked(prev);\n\telse\n\t\tprev = NULL;\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\tif (prev) {\n\t\treq_set_fail_links(prev);\n\t\tio_async_find_and_cancel(ctx, req, prev->user_data, -ETIME);\n\t\tio_put_req_deferred(prev, 1);\n\t} else {\n\t\tio_req_complete_post(req, -ETIME, 0);\n\t\tio_put_req_deferred(req, 1);\n\t}\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void __io_queue_linked_timeout(struct io_kiocb *req)\n{\n\t/*\n\t * If the back reference is NULL, then our linked request finished\n\t * before we got a chance to setup the timer\n\t */\n\tif (req->timeout.head) {\n\t\tstruct io_timeout_data *data = req->async_data;\n\n\t\tdata->timer.function = io_link_timeout_fn;\n\t\thrtimer_start(&data->timer, timespec64_to_ktime(data->ts),\n\t\t\t\tdata->mode);\n\t}\n}\n\nstatic void io_queue_linked_timeout(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\t__io_queue_linked_timeout(req);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\t/* drop submission reference */\n\tio_put_req(req);\n}\n\nstatic struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt = req->link;\n\n\tif (!nxt || (req->flags & REQ_F_LINK_TIMEOUT) ||\n\t    nxt->opcode != IORING_OP_LINK_TIMEOUT)\n\t\treturn NULL;\n\n\tnxt->timeout.head = req;\n\tnxt->flags |= REQ_F_LTIMEOUT_ACTIVE;\n\treq->flags |= REQ_F_LINK_TIMEOUT;\n\treturn nxt;\n}\n\nstatic void __io_queue_sqe(struct io_kiocb *req)\n{\n\tstruct io_kiocb *linked_timeout = io_prep_linked_timeout(req);\n\tint ret;\n\n\tret = io_issue_sqe(req, IO_URING_F_NONBLOCK|IO_URING_F_COMPLETE_DEFER);\n\n\t/*\n\t * We async punt it if the file wasn't marked NOWAIT, or if the file\n\t * doesn't support non-blocking read/write attempts\n\t */\n\tif (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {\n\t\tif (!io_arm_poll_handler(req)) {\n\t\t\t/*\n\t\t\t * Queued up for async execution, worker will release\n\t\t\t * submit reference when the iocb is actually submitted.\n\t\t\t */\n\t\t\tio_queue_async_work(req);\n\t\t}\n\t} else if (likely(!ret)) {\n\t\t/* drop submission reference */\n\t\tif (req->flags & REQ_F_COMPLETE_INLINE) {\n\t\t\tstruct io_ring_ctx *ctx = req->ctx;\n\t\t\tstruct io_comp_state *cs = &ctx->submit_state.comp;\n\n\t\t\tcs->reqs[cs->nr++] = req;\n\t\t\tif (cs->nr == ARRAY_SIZE(cs->reqs))\n\t\t\t\tio_submit_flush_completions(cs, ctx);\n\t\t} else {\n\t\t\tio_put_req(req);\n\t\t}\n\t} else {\n\t\treq_set_fail_links(req);\n\t\tio_put_req(req);\n\t\tio_req_complete(req, ret);\n\t}\n\tif (linked_timeout)\n\t\tio_queue_linked_timeout(linked_timeout);\n}\n\nstatic void io_queue_sqe(struct io_kiocb *req)\n{\n\tint ret;\n\n\tret = io_req_defer(req);\n\tif (ret) {\n\t\tif (ret != -EIOCBQUEUED) {\nfail_req:\n\t\t\treq_set_fail_links(req);\n\t\t\tio_put_req(req);\n\t\t\tio_req_complete(req, ret);\n\t\t}\n\t} else if (req->flags & REQ_F_FORCE_ASYNC) {\n\t\tret = io_req_defer_prep(req);\n\t\tif (unlikely(ret))\n\t\t\tgoto fail_req;\n\t\tio_queue_async_work(req);\n\t} else {\n\t\t__io_queue_sqe(req);\n\t}\n}\n\n/*\n * Check SQE restrictions (opcode and flags).\n *\n * Returns 'true' if SQE is allowed, 'false' otherwise.\n */\nstatic inline bool io_check_restriction(struct io_ring_ctx *ctx,\n\t\t\t\t\tstruct io_kiocb *req,\n\t\t\t\t\tunsigned int sqe_flags)\n{\n\tif (!ctx->restricted)\n\t\treturn true;\n\n\tif (!test_bit(req->opcode, ctx->restrictions.sqe_op))\n\t\treturn false;\n\n\tif ((sqe_flags & ctx->restrictions.sqe_flags_required) !=\n\t    ctx->restrictions.sqe_flags_required)\n\t\treturn false;\n\n\tif (sqe_flags & ~(ctx->restrictions.sqe_flags_allowed |\n\t\t\t  ctx->restrictions.sqe_flags_required))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,\n\t\t       const struct io_uring_sqe *sqe)\n{\n\tstruct io_submit_state *state;\n\tunsigned int sqe_flags;\n\tint ret = 0;\n\n\treq->opcode = READ_ONCE(sqe->opcode);\n\t/* same numerical values with corresponding REQ_F_*, safe to copy */\n\treq->flags = sqe_flags = READ_ONCE(sqe->flags);\n\treq->user_data = READ_ONCE(sqe->user_data);\n\treq->async_data = NULL;\n\treq->file = NULL;\n\treq->ctx = ctx;\n\treq->link = NULL;\n\treq->fixed_rsrc_refs = NULL;\n\t/* one is dropped after submission, the other at completion */\n\trefcount_set(&req->refs, 2);\n\treq->task = current;\n\treq->result = 0;\n\n\t/* enforce forwards compatibility on users */\n\tif (unlikely(sqe_flags & ~SQE_VALID_FLAGS)) {\n\t\treq->flags = 0;\n\t\treturn -EINVAL;\n\t}\n\n\tif (unlikely(req->opcode >= IORING_OP_LAST))\n\t\treturn -EINVAL;\n\n\tif (unlikely(!io_check_restriction(ctx, req, sqe_flags)))\n\t\treturn -EACCES;\n\n\tif ((sqe_flags & IOSQE_BUFFER_SELECT) &&\n\t    !io_op_defs[req->opcode].buffer_select)\n\t\treturn -EOPNOTSUPP;\n\n\treq->work.list.next = NULL;\n\treq->work.flags = 0;\n\treq->work.personality = READ_ONCE(sqe->personality);\n\tstate = &ctx->submit_state;\n\n\t/*\n\t * Plug now if we have more than 1 IO left after this, and the target\n\t * is potentially a read/write to block based storage.\n\t */\n\tif (!state->plug_started && state->ios_left > 1 &&\n\t    io_op_defs[req->opcode].plug) {\n\t\tblk_start_plug(&state->plug);\n\t\tstate->plug_started = true;\n\t}\n\n\tif (io_op_defs[req->opcode].needs_file) {\n\t\tbool fixed = req->flags & REQ_F_FIXED_FILE;\n\n\t\treq->file = io_file_get(state, req, READ_ONCE(sqe->fd), fixed);\n\t\tif (unlikely(!req->file))\n\t\t\tret = -EBADF;\n\t}\n\n\tstate->ios_left--;\n\treturn ret;\n}\n\nstatic int io_submit_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,\n\t\t\t const struct io_uring_sqe *sqe)\n{\n\tstruct io_submit_link *link = &ctx->submit_state.link;\n\tint ret;\n\n\tret = io_init_req(ctx, req, sqe);\n\tif (unlikely(ret)) {\nfail_req:\n\t\tio_put_req(req);\n\t\tio_req_complete(req, ret);\n\t\tif (link->head) {\n\t\t\t/* fail even hard links since we don't submit */\n\t\t\tlink->head->flags |= REQ_F_FAIL_LINK;\n\t\t\tio_put_req(link->head);\n\t\t\tio_req_complete(link->head, -ECANCELED);\n\t\t\tlink->head = NULL;\n\t\t}\n\t\treturn ret;\n\t}\n\tret = io_req_prep(req, sqe);\n\tif (unlikely(ret))\n\t\tgoto fail_req;\n\n\t/* don't need @sqe from now on */\n\ttrace_io_uring_submit_sqe(ctx, req->opcode, req->user_data,\n\t\t\t\ttrue, ctx->flags & IORING_SETUP_SQPOLL);\n\n\t/*\n\t * If we already have a head request, queue this one for async\n\t * submittal once the head completes. If we don't have a head but\n\t * IOSQE_IO_LINK is set in the sqe, start a new head. This one will be\n\t * submitted sync once the chain is complete. If none of those\n\t * conditions are true (normal request), then just queue it.\n\t */\n\tif (link->head) {\n\t\tstruct io_kiocb *head = link->head;\n\n\t\t/*\n\t\t * Taking sequential execution of a link, draining both sides\n\t\t * of the link also fullfils IOSQE_IO_DRAIN semantics for all\n\t\t * requests in the link. So, it drains the head and the\n\t\t * next after the link request. The last one is done via\n\t\t * drain_next flag to persist the effect across calls.\n\t\t */\n\t\tif (req->flags & REQ_F_IO_DRAIN) {\n\t\t\thead->flags |= REQ_F_IO_DRAIN;\n\t\t\tctx->drain_next = 1;\n\t\t}\n\t\tret = io_req_defer_prep(req);\n\t\tif (unlikely(ret))\n\t\t\tgoto fail_req;\n\t\ttrace_io_uring_link(ctx, req, head);\n\t\tlink->last->link = req;\n\t\tlink->last = req;\n\n\t\t/* last request of a link, enqueue the link */\n\t\tif (!(req->flags & (REQ_F_LINK | REQ_F_HARDLINK))) {\n\t\t\tio_queue_sqe(head);\n\t\t\tlink->head = NULL;\n\t\t}\n\t} else {\n\t\tif (unlikely(ctx->drain_next)) {\n\t\t\treq->flags |= REQ_F_IO_DRAIN;\n\t\t\tctx->drain_next = 0;\n\t\t}\n\t\tif (req->flags & (REQ_F_LINK | REQ_F_HARDLINK)) {\n\t\t\tlink->head = req;\n\t\t\tlink->last = req;\n\t\t} else {\n\t\t\tio_queue_sqe(req);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n * Batched submission is done, ensure local IO is flushed out.\n */\nstatic void io_submit_state_end(struct io_submit_state *state,\n\t\t\t\tstruct io_ring_ctx *ctx)\n{\n\tif (state->link.head)\n\t\tio_queue_sqe(state->link.head);\n\tif (state->comp.nr)\n\t\tio_submit_flush_completions(&state->comp, ctx);\n\tif (state->plug_started)\n\t\tblk_finish_plug(&state->plug);\n\tio_state_file_put(state);\n}\n\n/*\n * Start submission side cache.\n */\nstatic void io_submit_state_start(struct io_submit_state *state,\n\t\t\t\t  unsigned int max_ios)\n{\n\tstate->plug_started = false;\n\tstate->ios_left = max_ios;\n\t/* set only head, no need to init link_last in advance */\n\tstate->link.head = NULL;\n}\n\nstatic void io_commit_sqring(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *rings = ctx->rings;\n\n\t/*\n\t * Ensure any loads from the SQEs are done at this point,\n\t * since once we write the new head, the application could\n\t * write new data to them.\n\t */\n\tsmp_store_release(&rings->sq.head, ctx->cached_sq_head);\n}\n\n/*\n * Fetch an sqe, if one is available. Note that sqe_ptr will point to memory\n * that is mapped by userspace. This means that care needs to be taken to\n * ensure that reads are stable, as we cannot rely on userspace always\n * being a good citizen. If members of the sqe are validated and then later\n * used, it's important that those reads are done through READ_ONCE() to\n * prevent a re-load down the line.\n */\nstatic const struct io_uring_sqe *io_get_sqe(struct io_ring_ctx *ctx)\n{\n\tu32 *sq_array = ctx->sq_array;\n\tunsigned head;\n\n\t/*\n\t * The cached sq head (or cq tail) serves two purposes:\n\t *\n\t * 1) allows us to batch the cost of updating the user visible\n\t *    head updates.\n\t * 2) allows the kernel side to track the head on its own, even\n\t *    though the application is the one updating it.\n\t */\n\thead = READ_ONCE(sq_array[ctx->cached_sq_head++ & ctx->sq_mask]);\n\tif (likely(head < ctx->sq_entries))\n\t\treturn &ctx->sq_sqes[head];\n\n\t/* drop invalid entries */\n\tctx->cached_sq_dropped++;\n\tWRITE_ONCE(ctx->rings->sq_dropped, ctx->cached_sq_dropped);\n\treturn NULL;\n}\n\nstatic int io_submit_sqes(struct io_ring_ctx *ctx, unsigned int nr)\n{\n\tint submitted = 0;\n\n\t/* if we have a backlog and couldn't flush it all, return BUSY */\n\tif (test_bit(0, &ctx->sq_check_overflow)) {\n\t\tif (!__io_cqring_overflow_flush(ctx, false, NULL, NULL))\n\t\t\treturn -EBUSY;\n\t}\n\n\t/* make sure SQ entry isn't read before tail */\n\tnr = min3(nr, ctx->sq_entries, io_sqring_entries(ctx));\n\n\tif (!percpu_ref_tryget_many(&ctx->refs, nr))\n\t\treturn -EAGAIN;\n\n\tpercpu_counter_add(&current->io_uring->inflight, nr);\n\trefcount_add(nr, &current->usage);\n\tio_submit_state_start(&ctx->submit_state, nr);\n\n\twhile (submitted < nr) {\n\t\tconst struct io_uring_sqe *sqe;\n\t\tstruct io_kiocb *req;\n\n\t\treq = io_alloc_req(ctx);\n\t\tif (unlikely(!req)) {\n\t\t\tif (!submitted)\n\t\t\t\tsubmitted = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tsqe = io_get_sqe(ctx);\n\t\tif (unlikely(!sqe)) {\n\t\t\tkmem_cache_free(req_cachep, req);\n\t\t\tbreak;\n\t\t}\n\t\t/* will complete beyond this point, count as submitted */\n\t\tsubmitted++;\n\t\tif (io_submit_sqe(ctx, req, sqe))\n\t\t\tbreak;\n\t}\n\n\tif (unlikely(submitted != nr)) {\n\t\tint ref_used = (submitted == -EAGAIN) ? 0 : submitted;\n\t\tstruct io_uring_task *tctx = current->io_uring;\n\t\tint unused = nr - ref_used;\n\n\t\tpercpu_ref_put_many(&ctx->refs, unused);\n\t\tpercpu_counter_sub(&tctx->inflight, unused);\n\t\tput_task_struct_many(current, unused);\n\t}\n\n\tio_submit_state_end(&ctx->submit_state, ctx);\n\t /* Commit SQ ring head once we've consumed and submitted all SQEs */\n\tio_commit_sqring(ctx);\n\n\treturn submitted;\n}\n\nstatic inline void io_ring_set_wakeup_flag(struct io_ring_ctx *ctx)\n{\n\t/* Tell userspace we may need a wakeup call */\n\tspin_lock_irq(&ctx->completion_lock);\n\tctx->rings->sq_flags |= IORING_SQ_NEED_WAKEUP;\n\tspin_unlock_irq(&ctx->completion_lock);\n}\n\nstatic inline void io_ring_clear_wakeup_flag(struct io_ring_ctx *ctx)\n{\n\tspin_lock_irq(&ctx->completion_lock);\n\tctx->rings->sq_flags &= ~IORING_SQ_NEED_WAKEUP;\n\tspin_unlock_irq(&ctx->completion_lock);\n}\n\nstatic int __io_sq_thread(struct io_ring_ctx *ctx, bool cap_entries)\n{\n\tunsigned int to_submit;\n\tint ret = 0;\n\n\tto_submit = io_sqring_entries(ctx);\n\t/* if we're handling multiple rings, cap submit size for fairness */\n\tif (cap_entries && to_submit > 8)\n\t\tto_submit = 8;\n\n\tif (!list_empty(&ctx->iopoll_list) || to_submit) {\n\t\tunsigned nr_events = 0;\n\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tif (!list_empty(&ctx->iopoll_list))\n\t\t\tio_do_iopoll(ctx, &nr_events, 0);\n\n\t\tif (to_submit && likely(!percpu_ref_is_dying(&ctx->refs)))\n\t\t\tret = io_submit_sqes(ctx, to_submit);\n\t\tmutex_unlock(&ctx->uring_lock);\n\t}\n\n\tif (!io_sqring_full(ctx) && wq_has_sleeper(&ctx->sqo_sq_wait))\n\t\twake_up(&ctx->sqo_sq_wait);\n\n\treturn ret;\n}\n\nstatic void io_sqd_update_thread_idle(struct io_sq_data *sqd)\n{\n\tstruct io_ring_ctx *ctx;\n\tunsigned sq_thread_idle = 0;\n\n\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list) {\n\t\tif (sq_thread_idle < ctx->sq_thread_idle)\n\t\t\tsq_thread_idle = ctx->sq_thread_idle;\n\t}\n\n\tsqd->sq_thread_idle = sq_thread_idle;\n}\n\nstatic void io_sqd_init_new(struct io_sq_data *sqd)\n{\n\tstruct io_ring_ctx *ctx;\n\n\twhile (!list_empty(&sqd->ctx_new_list)) {\n\t\tctx = list_first_entry(&sqd->ctx_new_list, struct io_ring_ctx, sqd_list);\n\t\tlist_move_tail(&ctx->sqd_list, &sqd->ctx_list);\n\t\tcomplete(&ctx->sq_thread_comp);\n\t}\n\n\tio_sqd_update_thread_idle(sqd);\n}\n\nstatic bool io_sq_thread_should_stop(struct io_sq_data *sqd)\n{\n\treturn test_bit(IO_SQ_THREAD_SHOULD_STOP, &sqd->state);\n}\n\nstatic bool io_sq_thread_should_park(struct io_sq_data *sqd)\n{\n\treturn test_bit(IO_SQ_THREAD_SHOULD_PARK, &sqd->state);\n}\n\nstatic void io_sq_thread_parkme(struct io_sq_data *sqd)\n{\n\tfor (;;) {\n\t\t/*\n\t\t * TASK_PARKED is a special state; we must serialize against\n\t\t * possible pending wakeups to avoid store-store collisions on\n\t\t * task->state.\n\t\t *\n\t\t * Such a collision might possibly result in the task state\n\t\t * changin from TASK_PARKED and us failing the\n\t\t * wait_task_inactive() in kthread_park().\n\t\t */\n\t\tset_special_state(TASK_PARKED);\n\t\tif (!test_bit(IO_SQ_THREAD_SHOULD_PARK, &sqd->state))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Thread is going to call schedule(), do not preempt it,\n\t\t * or the caller of kthread_park() may spend more time in\n\t\t * wait_task_inactive().\n\t\t */\n\t\tpreempt_disable();\n\t\tcomplete(&sqd->completion);\n\t\tschedule_preempt_disabled();\n\t\tpreempt_enable();\n\t}\n\t__set_current_state(TASK_RUNNING);\n}\n\nstatic int io_sq_thread(void *data)\n{\n\tstruct io_sq_data *sqd = data;\n\tstruct io_ring_ctx *ctx;\n\tunsigned long timeout = 0;\n\tchar buf[TASK_COMM_LEN];\n\tDEFINE_WAIT(wait);\n\n\tsprintf(buf, \"iou-sqp-%d\", sqd->task_pid);\n\tset_task_comm(current, buf);\n\tsqd->thread = current;\n\tcurrent->pf_io_worker = NULL;\n\n\tif (sqd->sq_cpu != -1)\n\t\tset_cpus_allowed_ptr(current, cpumask_of(sqd->sq_cpu));\n\telse\n\t\tset_cpus_allowed_ptr(current, cpu_online_mask);\n\tcurrent->flags |= PF_NO_SETAFFINITY;\n\n\tcomplete(&sqd->completion);\n\n\twait_for_completion(&sqd->startup);\n\n\twhile (!io_sq_thread_should_stop(sqd)) {\n\t\tint ret;\n\t\tbool cap_entries, sqt_spin, needs_sched;\n\n\t\t/*\n\t\t * Any changes to the sqd lists are synchronized through the\n\t\t * thread parking. This synchronizes the thread vs users,\n\t\t * the users are synchronized on the sqd->ctx_lock.\n\t\t */\n\t\tif (io_sq_thread_should_park(sqd)) {\n\t\t\tio_sq_thread_parkme(sqd);\n\t\t\tcontinue;\n\t\t}\n\t\tif (unlikely(!list_empty(&sqd->ctx_new_list))) {\n\t\t\tio_sqd_init_new(sqd);\n\t\t\ttimeout = jiffies + sqd->sq_thread_idle;\n\t\t}\n\t\tif (fatal_signal_pending(current))\n\t\t\tbreak;\n\t\tsqt_spin = false;\n\t\tcap_entries = !list_is_singular(&sqd->ctx_list);\n\t\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list) {\n\t\t\tret = __io_sq_thread(ctx, cap_entries);\n\t\t\tif (!sqt_spin && (ret > 0 || !list_empty(&ctx->iopoll_list)))\n\t\t\t\tsqt_spin = true;\n\t\t}\n\n\t\tif (sqt_spin || !time_after(jiffies, timeout)) {\n\t\t\tio_run_task_work();\n\t\t\tcond_resched();\n\t\t\tif (sqt_spin)\n\t\t\t\ttimeout = jiffies + sqd->sq_thread_idle;\n\t\t\tcontinue;\n\t\t}\n\n\t\tneeds_sched = true;\n\t\tprepare_to_wait(&sqd->wait, &wait, TASK_INTERRUPTIBLE);\n\t\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list) {\n\t\t\tif ((ctx->flags & IORING_SETUP_IOPOLL) &&\n\t\t\t    !list_empty_careful(&ctx->iopoll_list)) {\n\t\t\t\tneeds_sched = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (io_sqring_entries(ctx)) {\n\t\t\t\tneeds_sched = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (needs_sched && !io_sq_thread_should_park(sqd)) {\n\t\t\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list)\n\t\t\t\tio_ring_set_wakeup_flag(ctx);\n\n\t\t\tschedule();\n\t\t\ttry_to_freeze();\n\t\t\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list)\n\t\t\t\tio_ring_clear_wakeup_flag(ctx);\n\t\t}\n\n\t\tfinish_wait(&sqd->wait, &wait);\n\t\ttimeout = jiffies + sqd->sq_thread_idle;\n\t}\n\n\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list)\n\t\tio_uring_cancel_sqpoll(ctx);\n\n\tio_run_task_work();\n\n\tif (io_sq_thread_should_park(sqd))\n\t\tio_sq_thread_parkme(sqd);\n\n\t/*\n\t * Clear thread under lock so that concurrent parks work correctly\n\t */\n\tcomplete(&sqd->completion);\n\tmutex_lock(&sqd->lock);\n\tsqd->thread = NULL;\n\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list) {\n\t\tctx->sqo_exec = 1;\n\t\tio_ring_set_wakeup_flag(ctx);\n\t}\n\n\tcomplete(&sqd->exited);\n\tmutex_unlock(&sqd->lock);\n\tdo_exit(0);\n}\n\nstruct io_wait_queue {\n\tstruct wait_queue_entry wq;\n\tstruct io_ring_ctx *ctx;\n\tunsigned to_wait;\n\tunsigned nr_timeouts;\n};\n\nstatic inline bool io_should_wake(struct io_wait_queue *iowq)\n{\n\tstruct io_ring_ctx *ctx = iowq->ctx;\n\n\t/*\n\t * Wake up if we have enough events, or if a timeout occurred since we\n\t * started waiting. For timeouts, we always want to return to userspace,\n\t * regardless of event count.\n\t */\n\treturn io_cqring_events(ctx) >= iowq->to_wait ||\n\t\t\tatomic_read(&ctx->cq_timeouts) != iowq->nr_timeouts;\n}\n\nstatic int io_wake_function(struct wait_queue_entry *curr, unsigned int mode,\n\t\t\t    int wake_flags, void *key)\n{\n\tstruct io_wait_queue *iowq = container_of(curr, struct io_wait_queue,\n\t\t\t\t\t\t\twq);\n\n\t/*\n\t * Cannot safely flush overflowed CQEs from here, ensure we wake up\n\t * the task, and the next invocation will do it.\n\t */\n\tif (io_should_wake(iowq) || test_bit(0, &iowq->ctx->cq_check_overflow))\n\t\treturn autoremove_wake_function(curr, mode, wake_flags, key);\n\treturn -1;\n}\n\nstatic int io_run_task_work_sig(void)\n{\n\tif (io_run_task_work())\n\t\treturn 1;\n\tif (!signal_pending(current))\n\t\treturn 0;\n\tif (test_tsk_thread_flag(current, TIF_NOTIFY_SIGNAL))\n\t\treturn -ERESTARTSYS;\n\treturn -EINTR;\n}\n\n/* when returns >0, the caller should retry */\nstatic inline int io_cqring_wait_schedule(struct io_ring_ctx *ctx,\n\t\t\t\t\t  struct io_wait_queue *iowq,\n\t\t\t\t\t  signed long *timeout)\n{\n\tint ret;\n\n\t/* make sure we run task_work before checking for signals */\n\tret = io_run_task_work_sig();\n\tif (ret || io_should_wake(iowq))\n\t\treturn ret;\n\t/* let the caller flush overflows, retry */\n\tif (test_bit(0, &ctx->cq_check_overflow))\n\t\treturn 1;\n\n\t*timeout = schedule_timeout(*timeout);\n\treturn !*timeout ? -ETIME : 1;\n}\n\n/*\n * Wait until events become available, if we don't already have some. The\n * application must reap them itself, as they reside on the shared cq ring.\n */\nstatic int io_cqring_wait(struct io_ring_ctx *ctx, int min_events,\n\t\t\t  const sigset_t __user *sig, size_t sigsz,\n\t\t\t  struct __kernel_timespec __user *uts)\n{\n\tstruct io_wait_queue iowq = {\n\t\t.wq = {\n\t\t\t.private\t= current,\n\t\t\t.func\t\t= io_wake_function,\n\t\t\t.entry\t\t= LIST_HEAD_INIT(iowq.wq.entry),\n\t\t},\n\t\t.ctx\t\t= ctx,\n\t\t.to_wait\t= min_events,\n\t};\n\tstruct io_rings *rings = ctx->rings;\n\tsigned long timeout = MAX_SCHEDULE_TIMEOUT;\n\tint ret;\n\n\tdo {\n\t\tio_cqring_overflow_flush(ctx, false, NULL, NULL);\n\t\tif (io_cqring_events(ctx) >= min_events)\n\t\t\treturn 0;\n\t\tif (!io_run_task_work())\n\t\t\tbreak;\n\t} while (1);\n\n\tif (sig) {\n#ifdef CONFIG_COMPAT\n\t\tif (in_compat_syscall())\n\t\t\tret = set_compat_user_sigmask((const compat_sigset_t __user *)sig,\n\t\t\t\t\t\t      sigsz);\n\t\telse\n#endif\n\t\t\tret = set_user_sigmask(sig, sigsz);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (uts) {\n\t\tstruct timespec64 ts;\n\n\t\tif (get_timespec64(&ts, uts))\n\t\t\treturn -EFAULT;\n\t\ttimeout = timespec64_to_jiffies(&ts);\n\t}\n\n\tiowq.nr_timeouts = atomic_read(&ctx->cq_timeouts);\n\ttrace_io_uring_cqring_wait(ctx, min_events);\n\tdo {\n\t\tio_cqring_overflow_flush(ctx, false, NULL, NULL);\n\t\tprepare_to_wait_exclusive(&ctx->wait, &iowq.wq,\n\t\t\t\t\t\tTASK_INTERRUPTIBLE);\n\t\tret = io_cqring_wait_schedule(ctx, &iowq, &timeout);\n\t\tfinish_wait(&ctx->wait, &iowq.wq);\n\t} while (ret > 0);\n\n\trestore_saved_sigmask_unless(ret == -EINTR);\n\n\treturn READ_ONCE(rings->cq.head) == READ_ONCE(rings->cq.tail) ? ret : 0;\n}\n\nstatic void __io_sqe_files_unregister(struct io_ring_ctx *ctx)\n{\n#if defined(CONFIG_UNIX)\n\tif (ctx->ring_sock) {\n\t\tstruct sock *sock = ctx->ring_sock->sk;\n\t\tstruct sk_buff *skb;\n\n\t\twhile ((skb = skb_dequeue(&sock->sk_receive_queue)) != NULL)\n\t\t\tkfree_skb(skb);\n\t}\n#else\n\tint i;\n\n\tfor (i = 0; i < ctx->nr_user_files; i++) {\n\t\tstruct file *file;\n\n\t\tfile = io_file_from_index(ctx, i);\n\t\tif (file)\n\t\t\tfput(file);\n\t}\n#endif\n}\n\nstatic void io_rsrc_data_ref_zero(struct percpu_ref *ref)\n{\n\tstruct fixed_rsrc_data *data;\n\n\tdata = container_of(ref, struct fixed_rsrc_data, refs);\n\tcomplete(&data->done);\n}\n\nstatic inline void io_rsrc_ref_lock(struct io_ring_ctx *ctx)\n{\n\tspin_lock_bh(&ctx->rsrc_ref_lock);\n}\n\nstatic inline void io_rsrc_ref_unlock(struct io_ring_ctx *ctx)\n{\n\tspin_unlock_bh(&ctx->rsrc_ref_lock);\n}\n\nstatic void io_sqe_rsrc_set_node(struct io_ring_ctx *ctx,\n\t\t\t\t struct fixed_rsrc_data *rsrc_data,\n\t\t\t\t struct fixed_rsrc_ref_node *ref_node)\n{\n\tio_rsrc_ref_lock(ctx);\n\trsrc_data->node = ref_node;\n\tlist_add_tail(&ref_node->node, &ctx->rsrc_ref_list);\n\tio_rsrc_ref_unlock(ctx);\n\tpercpu_ref_get(&rsrc_data->refs);\n}\n\nstatic void io_sqe_rsrc_kill_node(struct io_ring_ctx *ctx, struct fixed_rsrc_data *data)\n{\n\tstruct fixed_rsrc_ref_node *ref_node = NULL;\n\n\tio_rsrc_ref_lock(ctx);\n\tref_node = data->node;\n\tdata->node = NULL;\n\tio_rsrc_ref_unlock(ctx);\n\tif (ref_node)\n\t\tpercpu_ref_kill(&ref_node->refs);\n}\n\nstatic int io_rsrc_ref_quiesce(struct fixed_rsrc_data *data,\n\t\t\t       struct io_ring_ctx *ctx,\n\t\t\t       void (*rsrc_put)(struct io_ring_ctx *ctx,\n\t\t\t                        struct io_rsrc_put *prsrc))\n{\n\tstruct fixed_rsrc_ref_node *backup_node;\n\tint ret;\n\n\tif (data->quiesce)\n\t\treturn -ENXIO;\n\n\tdata->quiesce = true;\n\tdo {\n\t\tret = -ENOMEM;\n\t\tbackup_node = alloc_fixed_rsrc_ref_node(ctx);\n\t\tif (!backup_node)\n\t\t\tbreak;\n\t\tbackup_node->rsrc_data = data;\n\t\tbackup_node->rsrc_put = rsrc_put;\n\n\t\tio_sqe_rsrc_kill_node(ctx, data);\n\t\tpercpu_ref_kill(&data->refs);\n\t\tflush_delayed_work(&ctx->rsrc_put_work);\n\n\t\tret = wait_for_completion_interruptible(&data->done);\n\t\tif (!ret)\n\t\t\tbreak;\n\n\t\tpercpu_ref_resurrect(&data->refs);\n\t\tio_sqe_rsrc_set_node(ctx, data, backup_node);\n\t\tbackup_node = NULL;\n\t\treinit_completion(&data->done);\n\t\tmutex_unlock(&ctx->uring_lock);\n\t\tret = io_run_task_work_sig();\n\t\tmutex_lock(&ctx->uring_lock);\n\t} while (ret >= 0);\n\tdata->quiesce = false;\n\n\tif (backup_node)\n\t\tdestroy_fixed_rsrc_ref_node(backup_node);\n\treturn ret;\n}\n\nstatic struct fixed_rsrc_data *alloc_fixed_rsrc_data(struct io_ring_ctx *ctx)\n{\n\tstruct fixed_rsrc_data *data;\n\n\tdata = kzalloc(sizeof(*data), GFP_KERNEL);\n\tif (!data)\n\t\treturn NULL;\n\n\tif (percpu_ref_init(&data->refs, io_rsrc_data_ref_zero,\n\t\t\t    PERCPU_REF_ALLOW_REINIT, GFP_KERNEL)) {\n\t\tkfree(data);\n\t\treturn NULL;\n\t}\n\tdata->ctx = ctx;\n\tinit_completion(&data->done);\n\treturn data;\n}\n\nstatic void free_fixed_rsrc_data(struct fixed_rsrc_data *data)\n{\n\tpercpu_ref_exit(&data->refs);\n\tkfree(data->table);\n\tkfree(data);\n}\n\nstatic int io_sqe_files_unregister(struct io_ring_ctx *ctx)\n{\n\tstruct fixed_rsrc_data *data = ctx->file_data;\n\tunsigned nr_tables, i;\n\tint ret;\n\n\t/*\n\t * percpu_ref_is_dying() is to stop parallel files unregister\n\t * Since we possibly drop uring lock later in this function to\n\t * run task work.\n\t */\n\tif (!data || percpu_ref_is_dying(&data->refs))\n\t\treturn -ENXIO;\n\tret = io_rsrc_ref_quiesce(data, ctx, io_ring_file_put);\n\tif (ret)\n\t\treturn ret;\n\n\t__io_sqe_files_unregister(ctx);\n\tnr_tables = DIV_ROUND_UP(ctx->nr_user_files, IORING_MAX_FILES_TABLE);\n\tfor (i = 0; i < nr_tables; i++)\n\t\tkfree(data->table[i].files);\n\tfree_fixed_rsrc_data(data);\n\tctx->file_data = NULL;\n\tctx->nr_user_files = 0;\n\treturn 0;\n}\n\nstatic void io_sq_thread_unpark(struct io_sq_data *sqd)\n\t__releases(&sqd->lock)\n{\n\tif (!sqd->thread)\n\t\treturn;\n\tif (sqd->thread == current)\n\t\treturn;\n\tclear_bit(IO_SQ_THREAD_SHOULD_PARK, &sqd->state);\n\twake_up_state(sqd->thread, TASK_PARKED);\n\tmutex_unlock(&sqd->lock);\n}\n\nstatic bool io_sq_thread_park(struct io_sq_data *sqd)\n\t__acquires(&sqd->lock)\n{\n\tif (sqd->thread == current)\n\t\treturn true;\n\tmutex_lock(&sqd->lock);\n\tif (!sqd->thread) {\n\t\tmutex_unlock(&sqd->lock);\n\t\treturn false;\n\t}\n\tset_bit(IO_SQ_THREAD_SHOULD_PARK, &sqd->state);\n\twake_up_process(sqd->thread);\n\twait_for_completion(&sqd->completion);\n\treturn true;\n}\n\nstatic void io_sq_thread_stop(struct io_sq_data *sqd)\n{\n\tif (test_bit(IO_SQ_THREAD_SHOULD_STOP, &sqd->state))\n\t\treturn;\n\tmutex_lock(&sqd->lock);\n\tif (sqd->thread) {\n\t\tset_bit(IO_SQ_THREAD_SHOULD_STOP, &sqd->state);\n\t\tWARN_ON_ONCE(test_bit(IO_SQ_THREAD_SHOULD_PARK, &sqd->state));\n\t\twake_up_process(sqd->thread);\n\t\tmutex_unlock(&sqd->lock);\n\t\twait_for_completion(&sqd->exited);\n\t\tWARN_ON_ONCE(sqd->thread);\n\t} else {\n\t\tmutex_unlock(&sqd->lock);\n\t}\n}\n\nstatic void io_put_sq_data(struct io_sq_data *sqd)\n{\n\tif (refcount_dec_and_test(&sqd->refs)) {\n\t\tio_sq_thread_stop(sqd);\n\t\tkfree(sqd);\n\t}\n}\n\nstatic void io_sq_thread_finish(struct io_ring_ctx *ctx)\n{\n\tstruct io_sq_data *sqd = ctx->sq_data;\n\n\tif (sqd) {\n\t\tcomplete(&sqd->startup);\n\t\tif (sqd->thread) {\n\t\t\twait_for_completion(&ctx->sq_thread_comp);\n\t\t\tio_sq_thread_park(sqd);\n\t\t}\n\n\t\tmutex_lock(&sqd->ctx_lock);\n\t\tlist_del(&ctx->sqd_list);\n\t\tio_sqd_update_thread_idle(sqd);\n\t\tmutex_unlock(&sqd->ctx_lock);\n\n\t\tif (sqd->thread)\n\t\t\tio_sq_thread_unpark(sqd);\n\n\t\tio_put_sq_data(sqd);\n\t\tctx->sq_data = NULL;\n\t}\n}\n\nstatic struct io_sq_data *io_attach_sq_data(struct io_uring_params *p)\n{\n\tstruct io_ring_ctx *ctx_attach;\n\tstruct io_sq_data *sqd;\n\tstruct fd f;\n\n\tf = fdget(p->wq_fd);\n\tif (!f.file)\n\t\treturn ERR_PTR(-ENXIO);\n\tif (f.file->f_op != &io_uring_fops) {\n\t\tfdput(f);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tctx_attach = f.file->private_data;\n\tsqd = ctx_attach->sq_data;\n\tif (!sqd) {\n\t\tfdput(f);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\trefcount_inc(&sqd->refs);\n\tfdput(f);\n\treturn sqd;\n}\n\nstatic struct io_sq_data *io_get_sq_data(struct io_uring_params *p)\n{\n\tstruct io_sq_data *sqd;\n\n\tif (p->flags & IORING_SETUP_ATTACH_WQ)\n\t\treturn io_attach_sq_data(p);\n\n\tsqd = kzalloc(sizeof(*sqd), GFP_KERNEL);\n\tif (!sqd)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\trefcount_set(&sqd->refs, 1);\n\tINIT_LIST_HEAD(&sqd->ctx_list);\n\tINIT_LIST_HEAD(&sqd->ctx_new_list);\n\tmutex_init(&sqd->ctx_lock);\n\tmutex_init(&sqd->lock);\n\tinit_waitqueue_head(&sqd->wait);\n\tinit_completion(&sqd->startup);\n\tinit_completion(&sqd->completion);\n\tinit_completion(&sqd->exited);\n\treturn sqd;\n}\n\n#if defined(CONFIG_UNIX)\n/*\n * Ensure the UNIX gc is aware of our file set, so we are certain that\n * the io_uring can be safely unregistered on process exit, even if we have\n * loops in the file referencing.\n */\nstatic int __io_sqe_files_scm(struct io_ring_ctx *ctx, int nr, int offset)\n{\n\tstruct sock *sk = ctx->ring_sock->sk;\n\tstruct scm_fp_list *fpl;\n\tstruct sk_buff *skb;\n\tint i, nr_files;\n\n\tfpl = kzalloc(sizeof(*fpl), GFP_KERNEL);\n\tif (!fpl)\n\t\treturn -ENOMEM;\n\n\tskb = alloc_skb(0, GFP_KERNEL);\n\tif (!skb) {\n\t\tkfree(fpl);\n\t\treturn -ENOMEM;\n\t}\n\n\tskb->sk = sk;\n\n\tnr_files = 0;\n\tfpl->user = get_uid(current_user());\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct file *file = io_file_from_index(ctx, i + offset);\n\n\t\tif (!file)\n\t\t\tcontinue;\n\t\tfpl->fp[nr_files] = get_file(file);\n\t\tunix_inflight(fpl->user, fpl->fp[nr_files]);\n\t\tnr_files++;\n\t}\n\n\tif (nr_files) {\n\t\tfpl->max = SCM_MAX_FD;\n\t\tfpl->count = nr_files;\n\t\tUNIXCB(skb).fp = fpl;\n\t\tskb->destructor = unix_destruct_scm;\n\t\trefcount_add(skb->truesize, &sk->sk_wmem_alloc);\n\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\n\t\tfor (i = 0; i < nr_files; i++)\n\t\t\tfput(fpl->fp[i]);\n\t} else {\n\t\tkfree_skb(skb);\n\t\tkfree(fpl);\n\t}\n\n\treturn 0;\n}\n\n/*\n * If UNIX sockets are enabled, fd passing can cause a reference cycle which\n * causes regular reference counting to break down. We rely on the UNIX\n * garbage collection to take care of this problem for us.\n */\nstatic int io_sqe_files_scm(struct io_ring_ctx *ctx)\n{\n\tunsigned left, total;\n\tint ret = 0;\n\n\ttotal = 0;\n\tleft = ctx->nr_user_files;\n\twhile (left) {\n\t\tunsigned this_files = min_t(unsigned, left, SCM_MAX_FD);\n\n\t\tret = __io_sqe_files_scm(ctx, this_files, total);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tleft -= this_files;\n\t\ttotal += this_files;\n\t}\n\n\tif (!ret)\n\t\treturn 0;\n\n\twhile (total < ctx->nr_user_files) {\n\t\tstruct file *file = io_file_from_index(ctx, total);\n\n\t\tif (file)\n\t\t\tfput(file);\n\t\ttotal++;\n\t}\n\n\treturn ret;\n}\n#else\nstatic int io_sqe_files_scm(struct io_ring_ctx *ctx)\n{\n\treturn 0;\n}\n#endif\n\nstatic int io_sqe_alloc_file_tables(struct fixed_rsrc_data *file_data,\n\t\t\t\t    unsigned nr_tables, unsigned nr_files)\n{\n\tint i;\n\n\tfor (i = 0; i < nr_tables; i++) {\n\t\tstruct fixed_rsrc_table *table = &file_data->table[i];\n\t\tunsigned this_files;\n\n\t\tthis_files = min(nr_files, IORING_MAX_FILES_TABLE);\n\t\ttable->files = kcalloc(this_files, sizeof(struct file *),\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (!table->files)\n\t\t\tbreak;\n\t\tnr_files -= this_files;\n\t}\n\n\tif (i == nr_tables)\n\t\treturn 0;\n\n\tfor (i = 0; i < nr_tables; i++) {\n\t\tstruct fixed_rsrc_table *table = &file_data->table[i];\n\t\tkfree(table->files);\n\t}\n\treturn 1;\n}\n\nstatic void io_ring_file_put(struct io_ring_ctx *ctx, struct io_rsrc_put *prsrc)\n{\n\tstruct file *file = prsrc->file;\n#if defined(CONFIG_UNIX)\n\tstruct sock *sock = ctx->ring_sock->sk;\n\tstruct sk_buff_head list, *head = &sock->sk_receive_queue;\n\tstruct sk_buff *skb;\n\tint i;\n\n\t__skb_queue_head_init(&list);\n\n\t/*\n\t * Find the skb that holds this file in its SCM_RIGHTS. When found,\n\t * remove this entry and rearrange the file array.\n\t */\n\tskb = skb_dequeue(head);\n\twhile (skb) {\n\t\tstruct scm_fp_list *fp;\n\n\t\tfp = UNIXCB(skb).fp;\n\t\tfor (i = 0; i < fp->count; i++) {\n\t\t\tint left;\n\n\t\t\tif (fp->fp[i] != file)\n\t\t\t\tcontinue;\n\n\t\t\tunix_notinflight(fp->user, fp->fp[i]);\n\t\t\tleft = fp->count - 1 - i;\n\t\t\tif (left) {\n\t\t\t\tmemmove(&fp->fp[i], &fp->fp[i + 1],\n\t\t\t\t\t\tleft * sizeof(struct file *));\n\t\t\t}\n\t\t\tfp->count--;\n\t\t\tif (!fp->count) {\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tskb = NULL;\n\t\t\t} else {\n\t\t\t\t__skb_queue_tail(&list, skb);\n\t\t\t}\n\t\t\tfput(file);\n\t\t\tfile = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!file)\n\t\t\tbreak;\n\n\t\t__skb_queue_tail(&list, skb);\n\n\t\tskb = skb_dequeue(head);\n\t}\n\n\tif (skb_peek(&list)) {\n\t\tspin_lock_irq(&head->lock);\n\t\twhile ((skb = __skb_dequeue(&list)) != NULL)\n\t\t\t__skb_queue_tail(head, skb);\n\t\tspin_unlock_irq(&head->lock);\n\t}\n#else\n\tfput(file);\n#endif\n}\n\nstatic void __io_rsrc_put_work(struct fixed_rsrc_ref_node *ref_node)\n{\n\tstruct fixed_rsrc_data *rsrc_data = ref_node->rsrc_data;\n\tstruct io_ring_ctx *ctx = rsrc_data->ctx;\n\tstruct io_rsrc_put *prsrc, *tmp;\n\n\tlist_for_each_entry_safe(prsrc, tmp, &ref_node->rsrc_list, list) {\n\t\tlist_del(&prsrc->list);\n\t\tref_node->rsrc_put(ctx, prsrc);\n\t\tkfree(prsrc);\n\t}\n\n\tpercpu_ref_exit(&ref_node->refs);\n\tkfree(ref_node);\n\tpercpu_ref_put(&rsrc_data->refs);\n}\n\nstatic void io_rsrc_put_work(struct work_struct *work)\n{\n\tstruct io_ring_ctx *ctx;\n\tstruct llist_node *node;\n\n\tctx = container_of(work, struct io_ring_ctx, rsrc_put_work.work);\n\tnode = llist_del_all(&ctx->rsrc_put_llist);\n\n\twhile (node) {\n\t\tstruct fixed_rsrc_ref_node *ref_node;\n\t\tstruct llist_node *next = node->next;\n\n\t\tref_node = llist_entry(node, struct fixed_rsrc_ref_node, llist);\n\t\t__io_rsrc_put_work(ref_node);\n\t\tnode = next;\n\t}\n}\n\nstatic struct file **io_fixed_file_slot(struct fixed_rsrc_data *file_data,\n\t\t\t\t\tunsigned i)\n{\n\tstruct fixed_rsrc_table *table;\n\n\ttable = &file_data->table[i >> IORING_FILE_TABLE_SHIFT];\n\treturn &table->files[i & IORING_FILE_TABLE_MASK];\n}\n\nstatic void io_rsrc_node_ref_zero(struct percpu_ref *ref)\n{\n\tstruct fixed_rsrc_ref_node *ref_node;\n\tstruct fixed_rsrc_data *data;\n\tstruct io_ring_ctx *ctx;\n\tbool first_add = false;\n\tint delay = HZ;\n\n\tref_node = container_of(ref, struct fixed_rsrc_ref_node, refs);\n\tdata = ref_node->rsrc_data;\n\tctx = data->ctx;\n\n\tio_rsrc_ref_lock(ctx);\n\tref_node->done = true;\n\n\twhile (!list_empty(&ctx->rsrc_ref_list)) {\n\t\tref_node = list_first_entry(&ctx->rsrc_ref_list,\n\t\t\t\t\tstruct fixed_rsrc_ref_node, node);\n\t\t/* recycle ref nodes in order */\n\t\tif (!ref_node->done)\n\t\t\tbreak;\n\t\tlist_del(&ref_node->node);\n\t\tfirst_add |= llist_add(&ref_node->llist, &ctx->rsrc_put_llist);\n\t}\n\tio_rsrc_ref_unlock(ctx);\n\n\tif (percpu_ref_is_dying(&data->refs))\n\t\tdelay = 0;\n\n\tif (!delay)\n\t\tmod_delayed_work(system_wq, &ctx->rsrc_put_work, 0);\n\telse if (first_add)\n\t\tqueue_delayed_work(system_wq, &ctx->rsrc_put_work, delay);\n}\n\nstatic struct fixed_rsrc_ref_node *alloc_fixed_rsrc_ref_node(\n\t\t\tstruct io_ring_ctx *ctx)\n{\n\tstruct fixed_rsrc_ref_node *ref_node;\n\n\tref_node = kzalloc(sizeof(*ref_node), GFP_KERNEL);\n\tif (!ref_node)\n\t\treturn NULL;\n\n\tif (percpu_ref_init(&ref_node->refs, io_rsrc_node_ref_zero,\n\t\t\t    0, GFP_KERNEL)) {\n\t\tkfree(ref_node);\n\t\treturn NULL;\n\t}\n\tINIT_LIST_HEAD(&ref_node->node);\n\tINIT_LIST_HEAD(&ref_node->rsrc_list);\n\tref_node->done = false;\n\treturn ref_node;\n}\n\nstatic void init_fixed_file_ref_node(struct io_ring_ctx *ctx,\n\t\t\t\t     struct fixed_rsrc_ref_node *ref_node)\n{\n\tref_node->rsrc_data = ctx->file_data;\n\tref_node->rsrc_put = io_ring_file_put;\n}\n\nstatic void destroy_fixed_rsrc_ref_node(struct fixed_rsrc_ref_node *ref_node)\n{\n\tpercpu_ref_exit(&ref_node->refs);\n\tkfree(ref_node);\n}\n\n\nstatic int io_sqe_files_register(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t\t unsigned nr_args)\n{\n\t__s32 __user *fds = (__s32 __user *) arg;\n\tunsigned nr_tables, i;\n\tstruct file *file;\n\tint fd, ret = -ENOMEM;\n\tstruct fixed_rsrc_ref_node *ref_node;\n\tstruct fixed_rsrc_data *file_data;\n\n\tif (ctx->file_data)\n\t\treturn -EBUSY;\n\tif (!nr_args)\n\t\treturn -EINVAL;\n\tif (nr_args > IORING_MAX_FIXED_FILES)\n\t\treturn -EMFILE;\n\n\tfile_data = alloc_fixed_rsrc_data(ctx);\n\tif (!file_data)\n\t\treturn -ENOMEM;\n\tctx->file_data = file_data;\n\n\tnr_tables = DIV_ROUND_UP(nr_args, IORING_MAX_FILES_TABLE);\n\tfile_data->table = kcalloc(nr_tables, sizeof(*file_data->table),\n\t\t\t\t   GFP_KERNEL);\n\tif (!file_data->table)\n\t\tgoto out_free;\n\n\tif (io_sqe_alloc_file_tables(file_data, nr_tables, nr_args))\n\t\tgoto out_free;\n\n\tfor (i = 0; i < nr_args; i++, ctx->nr_user_files++) {\n\t\tif (copy_from_user(&fd, &fds[i], sizeof(fd))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out_fput;\n\t\t}\n\t\t/* allow sparse sets */\n\t\tif (fd == -1)\n\t\t\tcontinue;\n\n\t\tfile = fget(fd);\n\t\tret = -EBADF;\n\t\tif (!file)\n\t\t\tgoto out_fput;\n\n\t\t/*\n\t\t * Don't allow io_uring instances to be registered. If UNIX\n\t\t * isn't enabled, then this causes a reference cycle and this\n\t\t * instance can never get freed. If UNIX is enabled we'll\n\t\t * handle it just fine, but there's still no point in allowing\n\t\t * a ring fd as it doesn't support regular read/write anyway.\n\t\t */\n\t\tif (file->f_op == &io_uring_fops) {\n\t\t\tfput(file);\n\t\t\tgoto out_fput;\n\t\t}\n\t\t*io_fixed_file_slot(file_data, i) = file;\n\t}\n\n\tret = io_sqe_files_scm(ctx);\n\tif (ret) {\n\t\tio_sqe_files_unregister(ctx);\n\t\treturn ret;\n\t}\n\n\tref_node = alloc_fixed_rsrc_ref_node(ctx);\n\tif (!ref_node) {\n\t\tio_sqe_files_unregister(ctx);\n\t\treturn -ENOMEM;\n\t}\n\tinit_fixed_file_ref_node(ctx, ref_node);\n\n\tio_sqe_rsrc_set_node(ctx, file_data, ref_node);\n\treturn ret;\nout_fput:\n\tfor (i = 0; i < ctx->nr_user_files; i++) {\n\t\tfile = io_file_from_index(ctx, i);\n\t\tif (file)\n\t\t\tfput(file);\n\t}\n\tfor (i = 0; i < nr_tables; i++)\n\t\tkfree(file_data->table[i].files);\n\tctx->nr_user_files = 0;\nout_free:\n\tfree_fixed_rsrc_data(ctx->file_data);\n\tctx->file_data = NULL;\n\treturn ret;\n}\n\nstatic int io_sqe_file_register(struct io_ring_ctx *ctx, struct file *file,\n\t\t\t\tint index)\n{\n#if defined(CONFIG_UNIX)\n\tstruct sock *sock = ctx->ring_sock->sk;\n\tstruct sk_buff_head *head = &sock->sk_receive_queue;\n\tstruct sk_buff *skb;\n\n\t/*\n\t * See if we can merge this file into an existing skb SCM_RIGHTS\n\t * file set. If there's no room, fall back to allocating a new skb\n\t * and filling it in.\n\t */\n\tspin_lock_irq(&head->lock);\n\tskb = skb_peek(head);\n\tif (skb) {\n\t\tstruct scm_fp_list *fpl = UNIXCB(skb).fp;\n\n\t\tif (fpl->count < SCM_MAX_FD) {\n\t\t\t__skb_unlink(skb, head);\n\t\t\tspin_unlock_irq(&head->lock);\n\t\t\tfpl->fp[fpl->count] = get_file(file);\n\t\t\tunix_inflight(fpl->user, fpl->fp[fpl->count]);\n\t\t\tfpl->count++;\n\t\t\tspin_lock_irq(&head->lock);\n\t\t\t__skb_queue_head(head, skb);\n\t\t} else {\n\t\t\tskb = NULL;\n\t\t}\n\t}\n\tspin_unlock_irq(&head->lock);\n\n\tif (skb) {\n\t\tfput(file);\n\t\treturn 0;\n\t}\n\n\treturn __io_sqe_files_scm(ctx, 1, index);\n#else\n\treturn 0;\n#endif\n}\n\nstatic int io_queue_rsrc_removal(struct fixed_rsrc_data *data, void *rsrc)\n{\n\tstruct io_rsrc_put *prsrc;\n\tstruct fixed_rsrc_ref_node *ref_node = data->node;\n\n\tprsrc = kzalloc(sizeof(*prsrc), GFP_KERNEL);\n\tif (!prsrc)\n\t\treturn -ENOMEM;\n\n\tprsrc->rsrc = rsrc;\n\tlist_add(&prsrc->list, &ref_node->rsrc_list);\n\n\treturn 0;\n}\n\nstatic inline int io_queue_file_removal(struct fixed_rsrc_data *data,\n\t\t\t\t\tstruct file *file)\n{\n\treturn io_queue_rsrc_removal(data, (void *)file);\n}\n\nstatic int __io_sqe_files_update(struct io_ring_ctx *ctx,\n\t\t\t\t struct io_uring_rsrc_update *up,\n\t\t\t\t unsigned nr_args)\n{\n\tstruct fixed_rsrc_data *data = ctx->file_data;\n\tstruct fixed_rsrc_ref_node *ref_node;\n\tstruct file *file, **file_slot;\n\t__s32 __user *fds;\n\tint fd, i, err;\n\t__u32 done;\n\tbool needs_switch = false;\n\n\tif (check_add_overflow(up->offset, nr_args, &done))\n\t\treturn -EOVERFLOW;\n\tif (done > ctx->nr_user_files)\n\t\treturn -EINVAL;\n\n\tref_node = alloc_fixed_rsrc_ref_node(ctx);\n\tif (!ref_node)\n\t\treturn -ENOMEM;\n\tinit_fixed_file_ref_node(ctx, ref_node);\n\n\tfds = u64_to_user_ptr(up->data);\n\tfor (done = 0; done < nr_args; done++) {\n\t\terr = 0;\n\t\tif (copy_from_user(&fd, &fds[done], sizeof(fd))) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (fd == IORING_REGISTER_FILES_SKIP)\n\t\t\tcontinue;\n\n\t\ti = array_index_nospec(up->offset + done, ctx->nr_user_files);\n\t\tfile_slot = io_fixed_file_slot(ctx->file_data, i);\n\n\t\tif (*file_slot) {\n\t\t\terr = io_queue_file_removal(data, *file_slot);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\t*file_slot = NULL;\n\t\t\tneeds_switch = true;\n\t\t}\n\t\tif (fd != -1) {\n\t\t\tfile = fget(fd);\n\t\t\tif (!file) {\n\t\t\t\terr = -EBADF;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Don't allow io_uring instances to be registered. If\n\t\t\t * UNIX isn't enabled, then this causes a reference\n\t\t\t * cycle and this instance can never get freed. If UNIX\n\t\t\t * is enabled we'll handle it just fine, but there's\n\t\t\t * still no point in allowing a ring fd as it doesn't\n\t\t\t * support regular read/write anyway.\n\t\t\t */\n\t\t\tif (file->f_op == &io_uring_fops) {\n\t\t\t\tfput(file);\n\t\t\t\terr = -EBADF;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t*file_slot = file;\n\t\t\terr = io_sqe_file_register(ctx, file, i);\n\t\t\tif (err) {\n\t\t\t\t*file_slot = NULL;\n\t\t\t\tfput(file);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (needs_switch) {\n\t\tpercpu_ref_kill(&data->node->refs);\n\t\tio_sqe_rsrc_set_node(ctx, data, ref_node);\n\t} else\n\t\tdestroy_fixed_rsrc_ref_node(ref_node);\n\n\treturn done ? done : err;\n}\n\nstatic int io_sqe_files_update(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t       unsigned nr_args)\n{\n\tstruct io_uring_rsrc_update up;\n\n\tif (!ctx->file_data)\n\t\treturn -ENXIO;\n\tif (!nr_args)\n\t\treturn -EINVAL;\n\tif (copy_from_user(&up, arg, sizeof(up)))\n\t\treturn -EFAULT;\n\tif (up.resv)\n\t\treturn -EINVAL;\n\n\treturn __io_sqe_files_update(ctx, &up, nr_args);\n}\n\nstatic struct io_wq_work *io_free_work(struct io_wq_work *work)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\n\treq = io_put_req_find_next(req);\n\treturn req ? &req->work : NULL;\n}\n\nstatic struct io_wq *io_init_wq_offload(struct io_ring_ctx *ctx)\n{\n\tstruct io_wq_hash *hash;\n\tstruct io_wq_data data;\n\tunsigned int concurrency;\n\n\thash = ctx->hash_map;\n\tif (!hash) {\n\t\thash = kzalloc(sizeof(*hash), GFP_KERNEL);\n\t\tif (!hash)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\trefcount_set(&hash->refs, 1);\n\t\tinit_waitqueue_head(&hash->wait);\n\t\tctx->hash_map = hash;\n\t}\n\n\tdata.hash = hash;\n\tdata.free_work = io_free_work;\n\tdata.do_work = io_wq_submit_work;\n\n\t/* Do QD, or 4 * CPUS, whatever is smallest */\n\tconcurrency = min(ctx->sq_entries, 4 * num_online_cpus());\n\n\treturn io_wq_create(concurrency, &data);\n}\n\nstatic int io_uring_alloc_task_context(struct task_struct *task,\n\t\t\t\t       struct io_ring_ctx *ctx)\n{\n\tstruct io_uring_task *tctx;\n\tint ret;\n\n\ttctx = kmalloc(sizeof(*tctx), GFP_KERNEL);\n\tif (unlikely(!tctx))\n\t\treturn -ENOMEM;\n\n\tret = percpu_counter_init(&tctx->inflight, 0, GFP_KERNEL);\n\tif (unlikely(ret)) {\n\t\tkfree(tctx);\n\t\treturn ret;\n\t}\n\n\ttctx->io_wq = io_init_wq_offload(ctx);\n\tif (IS_ERR(tctx->io_wq)) {\n\t\tret = PTR_ERR(tctx->io_wq);\n\t\tpercpu_counter_destroy(&tctx->inflight);\n\t\tkfree(tctx);\n\t\treturn ret;\n\t}\n\n\txa_init(&tctx->xa);\n\tinit_waitqueue_head(&tctx->wait);\n\ttctx->last = NULL;\n\tatomic_set(&tctx->in_idle, 0);\n\ttctx->sqpoll = false;\n\ttask->io_uring = tctx;\n\tspin_lock_init(&tctx->task_lock);\n\tINIT_WQ_LIST(&tctx->task_list);\n\ttctx->task_state = 0;\n\tinit_task_work(&tctx->task_work, tctx_task_work);\n\treturn 0;\n}\n\nvoid __io_uring_free(struct task_struct *tsk)\n{\n\tstruct io_uring_task *tctx = tsk->io_uring;\n\n\tWARN_ON_ONCE(!xa_empty(&tctx->xa));\n\tWARN_ON_ONCE(tctx->io_wq);\n\n\tpercpu_counter_destroy(&tctx->inflight);\n\tkfree(tctx);\n\ttsk->io_uring = NULL;\n}\n\nstatic int io_sq_thread_fork(struct io_sq_data *sqd, struct io_ring_ctx *ctx)\n{\n\tint ret;\n\n\tclear_bit(IO_SQ_THREAD_SHOULD_STOP, &sqd->state);\n\treinit_completion(&sqd->completion);\n\tctx->sqo_exec = 0;\n\tsqd->task_pid = current->pid;\n\tcurrent->flags |= PF_IO_WORKER;\n\tret = io_wq_fork_thread(io_sq_thread, sqd);\n\tcurrent->flags &= ~PF_IO_WORKER;\n\tif (ret < 0) {\n\t\tsqd->thread = NULL;\n\t\treturn ret;\n\t}\n\twait_for_completion(&sqd->completion);\n\treturn io_uring_alloc_task_context(sqd->thread, ctx);\n}\n\nstatic int io_sq_offload_create(struct io_ring_ctx *ctx,\n\t\t\t\tstruct io_uring_params *p)\n{\n\tint ret;\n\n\t/* Retain compatibility with failing for an invalid attach attempt */\n\tif ((ctx->flags & (IORING_SETUP_ATTACH_WQ | IORING_SETUP_SQPOLL)) ==\n\t\t\t\tIORING_SETUP_ATTACH_WQ) {\n\t\tstruct fd f;\n\n\t\tf = fdget(p->wq_fd);\n\t\tif (!f.file)\n\t\t\treturn -ENXIO;\n\t\tif (f.file->f_op != &io_uring_fops) {\n\t\t\tfdput(f);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tfdput(f);\n\t}\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\tstruct io_sq_data *sqd;\n\n\t\tret = -EPERM;\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_NICE))\n\t\t\tgoto err;\n\n\t\tsqd = io_get_sq_data(p);\n\t\tif (IS_ERR(sqd)) {\n\t\t\tret = PTR_ERR(sqd);\n\t\t\tgoto err;\n\t\t}\n\n\t\tctx->sq_data = sqd;\n\t\tio_sq_thread_park(sqd);\n\t\tmutex_lock(&sqd->ctx_lock);\n\t\tlist_add(&ctx->sqd_list, &sqd->ctx_new_list);\n\t\tmutex_unlock(&sqd->ctx_lock);\n\t\tio_sq_thread_unpark(sqd);\n\n\t\tctx->sq_thread_idle = msecs_to_jiffies(p->sq_thread_idle);\n\t\tif (!ctx->sq_thread_idle)\n\t\t\tctx->sq_thread_idle = HZ;\n\n\t\tif (sqd->thread)\n\t\t\treturn 0;\n\n\t\tif (p->flags & IORING_SETUP_SQ_AFF) {\n\t\t\tint cpu = p->sq_thread_cpu;\n\n\t\t\tret = -EINVAL;\n\t\t\tif (cpu >= nr_cpu_ids)\n\t\t\t\tgoto err;\n\t\t\tif (!cpu_online(cpu))\n\t\t\t\tgoto err;\n\n\t\t\tsqd->sq_cpu = cpu;\n\t\t} else {\n\t\t\tsqd->sq_cpu = -1;\n\t\t}\n\n\t\tsqd->task_pid = current->pid;\n\t\tcurrent->flags |= PF_IO_WORKER;\n\t\tret = io_wq_fork_thread(io_sq_thread, sqd);\n\t\tcurrent->flags &= ~PF_IO_WORKER;\n\t\tif (ret < 0) {\n\t\t\tsqd->thread = NULL;\n\t\t\tgoto err;\n\t\t}\n\t\twait_for_completion(&sqd->completion);\n\t\tret = io_uring_alloc_task_context(sqd->thread, ctx);\n\t\tif (ret)\n\t\t\tgoto err;\n\t} else if (p->flags & IORING_SETUP_SQ_AFF) {\n\t\t/* Can't have SQ_AFF without SQPOLL */\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\n\treturn 0;\nerr:\n\tio_sq_thread_finish(ctx);\n\treturn ret;\n}\n\nstatic void io_sq_offload_start(struct io_ring_ctx *ctx)\n{\n\tstruct io_sq_data *sqd = ctx->sq_data;\n\n\tctx->flags &= ~IORING_SETUP_R_DISABLED;\n\tif (ctx->flags & IORING_SETUP_SQPOLL)\n\t\tcomplete(&sqd->startup);\n}\n\nstatic inline void __io_unaccount_mem(struct user_struct *user,\n\t\t\t\t      unsigned long nr_pages)\n{\n\tatomic_long_sub(nr_pages, &user->locked_vm);\n}\n\nstatic inline int __io_account_mem(struct user_struct *user,\n\t\t\t\t   unsigned long nr_pages)\n{\n\tunsigned long page_limit, cur_pages, new_pages;\n\n\t/* Don't allow more pages than we can safely lock */\n\tpage_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\n\n\tdo {\n\t\tcur_pages = atomic_long_read(&user->locked_vm);\n\t\tnew_pages = cur_pages + nr_pages;\n\t\tif (new_pages > page_limit)\n\t\t\treturn -ENOMEM;\n\t} while (atomic_long_cmpxchg(&user->locked_vm, cur_pages,\n\t\t\t\t\tnew_pages) != cur_pages);\n\n\treturn 0;\n}\n\nstatic void io_unaccount_mem(struct io_ring_ctx *ctx, unsigned long nr_pages)\n{\n\tif (ctx->user)\n\t\t__io_unaccount_mem(ctx->user, nr_pages);\n\n\tif (ctx->mm_account)\n\t\tatomic64_sub(nr_pages, &ctx->mm_account->pinned_vm);\n}\n\nstatic int io_account_mem(struct io_ring_ctx *ctx, unsigned long nr_pages)\n{\n\tint ret;\n\n\tif (ctx->user) {\n\t\tret = __io_account_mem(ctx->user, nr_pages);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (ctx->mm_account)\n\t\tatomic64_add(nr_pages, &ctx->mm_account->pinned_vm);\n\n\treturn 0;\n}\n\nstatic void io_mem_free(void *ptr)\n{\n\tstruct page *page;\n\n\tif (!ptr)\n\t\treturn;\n\n\tpage = virt_to_head_page(ptr);\n\tif (put_page_testzero(page))\n\t\tfree_compound_page(page);\n}\n\nstatic void *io_mem_alloc(size_t size)\n{\n\tgfp_t gfp_flags = GFP_KERNEL | __GFP_ZERO | __GFP_NOWARN | __GFP_COMP |\n\t\t\t\t__GFP_NORETRY | __GFP_ACCOUNT;\n\n\treturn (void *) __get_free_pages(gfp_flags, get_order(size));\n}\n\nstatic unsigned long rings_size(unsigned sq_entries, unsigned cq_entries,\n\t\t\t\tsize_t *sq_offset)\n{\n\tstruct io_rings *rings;\n\tsize_t off, sq_array_size;\n\n\toff = struct_size(rings, cqes, cq_entries);\n\tif (off == SIZE_MAX)\n\t\treturn SIZE_MAX;\n\n#ifdef CONFIG_SMP\n\toff = ALIGN(off, SMP_CACHE_BYTES);\n\tif (off == 0)\n\t\treturn SIZE_MAX;\n#endif\n\n\tif (sq_offset)\n\t\t*sq_offset = off;\n\n\tsq_array_size = array_size(sizeof(u32), sq_entries);\n\tif (sq_array_size == SIZE_MAX)\n\t\treturn SIZE_MAX;\n\n\tif (check_add_overflow(off, sq_array_size, &off))\n\t\treturn SIZE_MAX;\n\n\treturn off;\n}\n\nstatic int io_sqe_buffers_unregister(struct io_ring_ctx *ctx)\n{\n\tint i, j;\n\n\tif (!ctx->user_bufs)\n\t\treturn -ENXIO;\n\n\tfor (i = 0; i < ctx->nr_user_bufs; i++) {\n\t\tstruct io_mapped_ubuf *imu = &ctx->user_bufs[i];\n\n\t\tfor (j = 0; j < imu->nr_bvecs; j++)\n\t\t\tunpin_user_page(imu->bvec[j].bv_page);\n\n\t\tif (imu->acct_pages)\n\t\t\tio_unaccount_mem(ctx, imu->acct_pages);\n\t\tkvfree(imu->bvec);\n\t\timu->nr_bvecs = 0;\n\t}\n\n\tkfree(ctx->user_bufs);\n\tctx->user_bufs = NULL;\n\tctx->nr_user_bufs = 0;\n\treturn 0;\n}\n\nstatic int io_copy_iov(struct io_ring_ctx *ctx, struct iovec *dst,\n\t\t       void __user *arg, unsigned index)\n{\n\tstruct iovec __user *src;\n\n#ifdef CONFIG_COMPAT\n\tif (ctx->compat) {\n\t\tstruct compat_iovec __user *ciovs;\n\t\tstruct compat_iovec ciov;\n\n\t\tciovs = (struct compat_iovec __user *) arg;\n\t\tif (copy_from_user(&ciov, &ciovs[index], sizeof(ciov)))\n\t\t\treturn -EFAULT;\n\n\t\tdst->iov_base = u64_to_user_ptr((u64)ciov.iov_base);\n\t\tdst->iov_len = ciov.iov_len;\n\t\treturn 0;\n\t}\n#endif\n\tsrc = (struct iovec __user *) arg;\n\tif (copy_from_user(dst, &src[index], sizeof(*dst)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n/*\n * Not super efficient, but this is just a registration time. And we do cache\n * the last compound head, so generally we'll only do a full search if we don't\n * match that one.\n *\n * We check if the given compound head page has already been accounted, to\n * avoid double accounting it. This allows us to account the full size of the\n * page, not just the constituent pages of a huge page.\n */\nstatic bool headpage_already_acct(struct io_ring_ctx *ctx, struct page **pages,\n\t\t\t\t  int nr_pages, struct page *hpage)\n{\n\tint i, j;\n\n\t/* check current page array */\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tif (!PageCompound(pages[i]))\n\t\t\tcontinue;\n\t\tif (compound_head(pages[i]) == hpage)\n\t\t\treturn true;\n\t}\n\n\t/* check previously registered pages */\n\tfor (i = 0; i < ctx->nr_user_bufs; i++) {\n\t\tstruct io_mapped_ubuf *imu = &ctx->user_bufs[i];\n\n\t\tfor (j = 0; j < imu->nr_bvecs; j++) {\n\t\t\tif (!PageCompound(imu->bvec[j].bv_page))\n\t\t\t\tcontinue;\n\t\t\tif (compound_head(imu->bvec[j].bv_page) == hpage)\n\t\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic int io_buffer_account_pin(struct io_ring_ctx *ctx, struct page **pages,\n\t\t\t\t int nr_pages, struct io_mapped_ubuf *imu,\n\t\t\t\t struct page **last_hpage)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tif (!PageCompound(pages[i])) {\n\t\t\timu->acct_pages++;\n\t\t} else {\n\t\t\tstruct page *hpage;\n\n\t\t\thpage = compound_head(pages[i]);\n\t\t\tif (hpage == *last_hpage)\n\t\t\t\tcontinue;\n\t\t\t*last_hpage = hpage;\n\t\t\tif (headpage_already_acct(ctx, pages, i, hpage))\n\t\t\t\tcontinue;\n\t\t\timu->acct_pages += page_size(hpage) >> PAGE_SHIFT;\n\t\t}\n\t}\n\n\tif (!imu->acct_pages)\n\t\treturn 0;\n\n\tret = io_account_mem(ctx, imu->acct_pages);\n\tif (ret)\n\t\timu->acct_pages = 0;\n\treturn ret;\n}\n\nstatic int io_sqe_buffer_register(struct io_ring_ctx *ctx, struct iovec *iov,\n\t\t\t\t  struct io_mapped_ubuf *imu,\n\t\t\t\t  struct page **last_hpage)\n{\n\tstruct vm_area_struct **vmas = NULL;\n\tstruct page **pages = NULL;\n\tunsigned long off, start, end, ubuf;\n\tsize_t size;\n\tint ret, pret, nr_pages, i;\n\n\tubuf = (unsigned long) iov->iov_base;\n\tend = (ubuf + iov->iov_len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tstart = ubuf >> PAGE_SHIFT;\n\tnr_pages = end - start;\n\n\tret = -ENOMEM;\n\n\tpages = kvmalloc_array(nr_pages, sizeof(struct page *), GFP_KERNEL);\n\tif (!pages)\n\t\tgoto done;\n\n\tvmas = kvmalloc_array(nr_pages, sizeof(struct vm_area_struct *),\n\t\t\t      GFP_KERNEL);\n\tif (!vmas)\n\t\tgoto done;\n\n\timu->bvec = kvmalloc_array(nr_pages, sizeof(struct bio_vec),\n\t\t\t\t   GFP_KERNEL);\n\tif (!imu->bvec)\n\t\tgoto done;\n\n\tret = 0;\n\tmmap_read_lock(current->mm);\n\tpret = pin_user_pages(ubuf, nr_pages, FOLL_WRITE | FOLL_LONGTERM,\n\t\t\t      pages, vmas);\n\tif (pret == nr_pages) {\n\t\t/* don't support file backed memory */\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct vm_area_struct *vma = vmas[i];\n\n\t\t\tif (vma->vm_file &&\n\t\t\t    !is_file_hugepages(vma->vm_file)) {\n\t\t\t\tret = -EOPNOTSUPP;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tret = pret < 0 ? pret : -EFAULT;\n\t}\n\tmmap_read_unlock(current->mm);\n\tif (ret) {\n\t\t/*\n\t\t * if we did partial map, or found file backed vmas,\n\t\t * release any pages we did get\n\t\t */\n\t\tif (pret > 0)\n\t\t\tunpin_user_pages(pages, pret);\n\t\tkvfree(imu->bvec);\n\t\tgoto done;\n\t}\n\n\tret = io_buffer_account_pin(ctx, pages, pret, imu, last_hpage);\n\tif (ret) {\n\t\tunpin_user_pages(pages, pret);\n\t\tkvfree(imu->bvec);\n\t\tgoto done;\n\t}\n\n\toff = ubuf & ~PAGE_MASK;\n\tsize = iov->iov_len;\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tsize_t vec_len;\n\n\t\tvec_len = min_t(size_t, size, PAGE_SIZE - off);\n\t\timu->bvec[i].bv_page = pages[i];\n\t\timu->bvec[i].bv_len = vec_len;\n\t\timu->bvec[i].bv_offset = off;\n\t\toff = 0;\n\t\tsize -= vec_len;\n\t}\n\t/* store original address for later verification */\n\timu->ubuf = ubuf;\n\timu->len = iov->iov_len;\n\timu->nr_bvecs = nr_pages;\n\tret = 0;\ndone:\n\tkvfree(pages);\n\tkvfree(vmas);\n\treturn ret;\n}\n\nstatic int io_buffers_map_alloc(struct io_ring_ctx *ctx, unsigned int nr_args)\n{\n\tif (ctx->user_bufs)\n\t\treturn -EBUSY;\n\tif (!nr_args || nr_args > UIO_MAXIOV)\n\t\treturn -EINVAL;\n\n\tctx->user_bufs = kcalloc(nr_args, sizeof(struct io_mapped_ubuf),\n\t\t\t\t\tGFP_KERNEL);\n\tif (!ctx->user_bufs)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic int io_buffer_validate(struct iovec *iov)\n{\n\t/*\n\t * Don't impose further limits on the size and buffer\n\t * constraints here, we'll -EINVAL later when IO is\n\t * submitted if they are wrong.\n\t */\n\tif (!iov->iov_base || !iov->iov_len)\n\t\treturn -EFAULT;\n\n\t/* arbitrary limit, but we need something */\n\tif (iov->iov_len > SZ_1G)\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int io_sqe_buffers_register(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t\t   unsigned int nr_args)\n{\n\tint i, ret;\n\tstruct iovec iov;\n\tstruct page *last_hpage = NULL;\n\n\tret = io_buffers_map_alloc(ctx, nr_args);\n\tif (ret)\n\t\treturn ret;\n\n\tfor (i = 0; i < nr_args; i++) {\n\t\tstruct io_mapped_ubuf *imu = &ctx->user_bufs[i];\n\n\t\tret = io_copy_iov(ctx, &iov, arg, i);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tret = io_buffer_validate(&iov);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tret = io_sqe_buffer_register(ctx, &iov, imu, &last_hpage);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tctx->nr_user_bufs++;\n\t}\n\n\tif (ret)\n\t\tio_sqe_buffers_unregister(ctx);\n\n\treturn ret;\n}\n\nstatic int io_eventfd_register(struct io_ring_ctx *ctx, void __user *arg)\n{\n\t__s32 __user *fds = arg;\n\tint fd;\n\n\tif (ctx->cq_ev_fd)\n\t\treturn -EBUSY;\n\n\tif (copy_from_user(&fd, fds, sizeof(*fds)))\n\t\treturn -EFAULT;\n\n\tctx->cq_ev_fd = eventfd_ctx_fdget(fd);\n\tif (IS_ERR(ctx->cq_ev_fd)) {\n\t\tint ret = PTR_ERR(ctx->cq_ev_fd);\n\t\tctx->cq_ev_fd = NULL;\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int io_eventfd_unregister(struct io_ring_ctx *ctx)\n{\n\tif (ctx->cq_ev_fd) {\n\t\teventfd_ctx_put(ctx->cq_ev_fd);\n\t\tctx->cq_ev_fd = NULL;\n\t\treturn 0;\n\t}\n\n\treturn -ENXIO;\n}\n\nstatic int __io_destroy_buffers(int id, void *p, void *data)\n{\n\tstruct io_ring_ctx *ctx = data;\n\tstruct io_buffer *buf = p;\n\n\t__io_remove_buffers(ctx, buf, id, -1U);\n\treturn 0;\n}\n\nstatic void io_destroy_buffers(struct io_ring_ctx *ctx)\n{\n\tidr_for_each(&ctx->io_buffer_idr, __io_destroy_buffers, ctx);\n\tidr_destroy(&ctx->io_buffer_idr);\n}\n\nstatic void io_req_cache_free(struct list_head *list, struct task_struct *tsk)\n{\n\tstruct io_kiocb *req, *nxt;\n\n\tlist_for_each_entry_safe(req, nxt, list, compl.list) {\n\t\tif (tsk && req->task != tsk)\n\t\t\tcontinue;\n\t\tlist_del(&req->compl.list);\n\t\tkmem_cache_free(req_cachep, req);\n\t}\n}\n\nstatic void io_req_caches_free(struct io_ring_ctx *ctx)\n{\n\tstruct io_submit_state *submit_state = &ctx->submit_state;\n\tstruct io_comp_state *cs = &ctx->submit_state.comp;\n\n\tmutex_lock(&ctx->uring_lock);\n\n\tif (submit_state->free_reqs) {\n\t\tkmem_cache_free_bulk(req_cachep, submit_state->free_reqs,\n\t\t\t\t     submit_state->reqs);\n\t\tsubmit_state->free_reqs = 0;\n\t}\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tlist_splice_init(&cs->locked_free_list, &cs->free_list);\n\tcs->locked_free_nr = 0;\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tio_req_cache_free(&cs->free_list, NULL);\n\n\tmutex_unlock(&ctx->uring_lock);\n}\n\nstatic void io_ring_ctx_free(struct io_ring_ctx *ctx)\n{\n\t/*\n\t * Some may use context even when all refs and requests have been put,\n\t * and they are free to do so while still holding uring_lock, see\n\t * __io_req_task_submit(). Wait for them to finish.\n\t */\n\tmutex_lock(&ctx->uring_lock);\n\tmutex_unlock(&ctx->uring_lock);\n\n\tio_sq_thread_finish(ctx);\n\tio_sqe_buffers_unregister(ctx);\n\n\tif (ctx->mm_account) {\n\t\tmmdrop(ctx->mm_account);\n\t\tctx->mm_account = NULL;\n\t}\n\n\tmutex_lock(&ctx->uring_lock);\n\tio_sqe_files_unregister(ctx);\n\tmutex_unlock(&ctx->uring_lock);\n\tio_eventfd_unregister(ctx);\n\tio_destroy_buffers(ctx);\n\tidr_destroy(&ctx->personality_idr);\n\n#if defined(CONFIG_UNIX)\n\tif (ctx->ring_sock) {\n\t\tctx->ring_sock->file = NULL; /* so that iput() is called */\n\t\tsock_release(ctx->ring_sock);\n\t}\n#endif\n\n\tio_mem_free(ctx->rings);\n\tio_mem_free(ctx->sq_sqes);\n\n\tpercpu_ref_exit(&ctx->refs);\n\tfree_uid(ctx->user);\n\tio_req_caches_free(ctx);\n\tif (ctx->hash_map)\n\t\tio_wq_put_hash(ctx->hash_map);\n\tkfree(ctx->cancel_hash);\n\tkfree(ctx);\n}\n\nstatic __poll_t io_uring_poll(struct file *file, poll_table *wait)\n{\n\tstruct io_ring_ctx *ctx = file->private_data;\n\t__poll_t mask = 0;\n\n\tpoll_wait(file, &ctx->cq_wait, wait);\n\t/*\n\t * synchronizes with barrier from wq_has_sleeper call in\n\t * io_commit_cqring\n\t */\n\tsmp_rmb();\n\tif (!io_sqring_full(ctx))\n\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\n\t/*\n\t * Don't flush cqring overflow list here, just do a simple check.\n\t * Otherwise there could possible be ABBA deadlock:\n\t *      CPU0                    CPU1\n\t *      ----                    ----\n\t * lock(&ctx->uring_lock);\n\t *                              lock(&ep->mtx);\n\t *                              lock(&ctx->uring_lock);\n\t * lock(&ep->mtx);\n\t *\n\t * Users may get EPOLLIN meanwhile seeing nothing in cqring, this\n\t * pushs them to do the flush.\n\t */\n\tif (io_cqring_events(ctx) || test_bit(0, &ctx->cq_check_overflow))\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\treturn mask;\n}\n\nstatic int io_uring_fasync(int fd, struct file *file, int on)\n{\n\tstruct io_ring_ctx *ctx = file->private_data;\n\n\treturn fasync_helper(fd, file, on, &ctx->cq_fasync);\n}\n\nstatic int io_unregister_personality(struct io_ring_ctx *ctx, unsigned id)\n{\n\tconst struct cred *creds;\n\n\tcreds = idr_remove(&ctx->personality_idr, id);\n\tif (creds) {\n\t\tput_cred(creds);\n\t\treturn 0;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int io_remove_personalities(int id, void *p, void *data)\n{\n\tstruct io_ring_ctx *ctx = data;\n\n\tio_unregister_personality(ctx, id);\n\treturn 0;\n}\n\nstatic bool io_run_ctx_fallback(struct io_ring_ctx *ctx)\n{\n\tstruct callback_head *work, *next;\n\tbool executed = false;\n\n\tdo {\n\t\twork = xchg(&ctx->exit_task_work, NULL);\n\t\tif (!work)\n\t\t\tbreak;\n\n\t\tdo {\n\t\t\tnext = work->next;\n\t\t\twork->func(work);\n\t\t\twork = next;\n\t\t\tcond_resched();\n\t\t} while (work);\n\t\texecuted = true;\n\t} while (1);\n\n\treturn executed;\n}\n\nstatic void io_ring_exit_work(struct work_struct *work)\n{\n\tstruct io_ring_ctx *ctx = container_of(work, struct io_ring_ctx,\n\t\t\t\t\t       exit_work);\n\n\t/*\n\t * If we're doing polled IO and end up having requests being\n\t * submitted async (out-of-line), then completions can come in while\n\t * we're waiting for refs to drop. We need to reap these manually,\n\t * as nobody else will be looking for them.\n\t */\n\tdo {\n\t\tio_uring_try_cancel_requests(ctx, NULL, NULL);\n\t} while (!wait_for_completion_timeout(&ctx->ref_comp, HZ/20));\n\tio_ring_ctx_free(ctx);\n}\n\nstatic void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)\n{\n\tmutex_lock(&ctx->uring_lock);\n\tpercpu_ref_kill(&ctx->refs);\n\t/* if force is set, the ring is going away. always drop after that */\n\tctx->cq_overflow_flushed = 1;\n\tif (ctx->rings)\n\t\t__io_cqring_overflow_flush(ctx, true, NULL, NULL);\n\tidr_for_each(&ctx->personality_idr, io_remove_personalities, ctx);\n\tmutex_unlock(&ctx->uring_lock);\n\n\tio_kill_timeouts(ctx, NULL, NULL);\n\tio_poll_remove_all(ctx, NULL, NULL);\n\n\t/* if we failed setting up the ctx, we might not have any rings */\n\tio_iopoll_try_reap_events(ctx);\n\n\tINIT_WORK(&ctx->exit_work, io_ring_exit_work);\n\t/*\n\t * Use system_unbound_wq to avoid spawning tons of event kworkers\n\t * if we're exiting a ton of rings at the same time. It just adds\n\t * noise and overhead, there's no discernable change in runtime\n\t * over using system_wq.\n\t */\n\tqueue_work(system_unbound_wq, &ctx->exit_work);\n}\n\nstatic int io_uring_release(struct inode *inode, struct file *file)\n{\n\tstruct io_ring_ctx *ctx = file->private_data;\n\n\tfile->private_data = NULL;\n\tio_ring_ctx_wait_and_kill(ctx);\n\treturn 0;\n}\n\nstruct io_task_cancel {\n\tstruct task_struct *task;\n\tstruct files_struct *files;\n};\n\nstatic bool io_cancel_task_cb(struct io_wq_work *work, void *data)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\tstruct io_task_cancel *cancel = data;\n\tbool ret;\n\n\tif (cancel->files && (req->flags & REQ_F_LINK_TIMEOUT)) {\n\t\tunsigned long flags;\n\t\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t\t/* protect against races with linked timeouts */\n\t\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\t\tret = io_match_task(req, cancel->task, cancel->files);\n\t\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\t} else {\n\t\tret = io_match_task(req, cancel->task, cancel->files);\n\t}\n\treturn ret;\n}\n\nstatic void io_cancel_defer_files(struct io_ring_ctx *ctx,\n\t\t\t\t  struct task_struct *task,\n\t\t\t\t  struct files_struct *files)\n{\n\tstruct io_defer_entry *de = NULL;\n\tLIST_HEAD(list);\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tlist_for_each_entry_reverse(de, &ctx->defer_list, list) {\n\t\tif (io_match_task(de->req, task, files)) {\n\t\t\tlist_cut_position(&list, &ctx->defer_list, &de->list);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\twhile (!list_empty(&list)) {\n\t\tde = list_first_entry(&list, struct io_defer_entry, list);\n\t\tlist_del_init(&de->list);\n\t\treq_set_fail_links(de->req);\n\t\tio_put_req(de->req);\n\t\tio_req_complete(de->req, -ECANCELED);\n\t\tkfree(de);\n\t}\n}\n\nstatic void io_uring_try_cancel_requests(struct io_ring_ctx *ctx,\n\t\t\t\t\t struct task_struct *task,\n\t\t\t\t\t struct files_struct *files)\n{\n\tstruct io_task_cancel cancel = { .task = task, .files = files, };\n\tstruct task_struct *tctx_task = task ?: current;\n\tstruct io_uring_task *tctx = tctx_task->io_uring;\n\n\twhile (1) {\n\t\tenum io_wq_cancel cret;\n\t\tbool ret = false;\n\n\t\tif (tctx && tctx->io_wq) {\n\t\t\tcret = io_wq_cancel_cb(tctx->io_wq, io_cancel_task_cb,\n\t\t\t\t\t       &cancel, true);\n\t\t\tret |= (cret != IO_WQ_CANCEL_NOTFOUND);\n\t\t}\n\n\t\t/* SQPOLL thread does its own polling */\n\t\tif (!(ctx->flags & IORING_SETUP_SQPOLL) && !files) {\n\t\t\twhile (!list_empty_careful(&ctx->iopoll_list)) {\n\t\t\t\tio_iopoll_try_reap_events(ctx);\n\t\t\t\tret = true;\n\t\t\t}\n\t\t}\n\n\t\tret |= io_poll_remove_all(ctx, task, files);\n\t\tret |= io_kill_timeouts(ctx, task, files);\n\t\tret |= io_run_task_work();\n\t\tret |= io_run_ctx_fallback(ctx);\n\t\tio_cqring_overflow_flush(ctx, true, task, files);\n\t\tif (!ret)\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n}\n\nstatic int io_uring_count_inflight(struct io_ring_ctx *ctx,\n\t\t\t\t   struct task_struct *task,\n\t\t\t\t   struct files_struct *files)\n{\n\tstruct io_kiocb *req;\n\tint cnt = 0;\n\n\tspin_lock_irq(&ctx->inflight_lock);\n\tlist_for_each_entry(req, &ctx->inflight_list, inflight_entry)\n\t\tcnt += io_match_task(req, task, files);\n\tspin_unlock_irq(&ctx->inflight_lock);\n\treturn cnt;\n}\n\nstatic void io_uring_cancel_files(struct io_ring_ctx *ctx,\n\t\t\t\t  struct task_struct *task,\n\t\t\t\t  struct files_struct *files)\n{\n\twhile (!list_empty_careful(&ctx->inflight_list)) {\n\t\tDEFINE_WAIT(wait);\n\t\tint inflight;\n\n\t\tinflight = io_uring_count_inflight(ctx, task, files);\n\t\tif (!inflight)\n\t\t\tbreak;\n\n\t\tio_uring_try_cancel_requests(ctx, task, files);\n\n\t\tif (ctx->sq_data)\n\t\t\tio_sq_thread_unpark(ctx->sq_data);\n\t\tprepare_to_wait(&task->io_uring->wait, &wait,\n\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tif (inflight == io_uring_count_inflight(ctx, task, files))\n\t\t\tschedule();\n\t\tfinish_wait(&task->io_uring->wait, &wait);\n\t\tif (ctx->sq_data)\n\t\t\tio_sq_thread_park(ctx->sq_data);\n\t}\n}\n\n/*\n * We need to iteratively cancel requests, in case a request has dependent\n * hard links. These persist even for failure of cancelations, hence keep\n * looping until none are found.\n */\nstatic void io_uring_cancel_task_requests(struct io_ring_ctx *ctx,\n\t\t\t\t\t  struct files_struct *files)\n{\n\tstruct task_struct *task = current;\n\tbool did_park = false;\n\n\tif ((ctx->flags & IORING_SETUP_SQPOLL) && ctx->sq_data) {\n\t\t/* never started, nothing to cancel */\n\t\tif (ctx->flags & IORING_SETUP_R_DISABLED) {\n\t\t\tio_sq_offload_start(ctx);\n\t\t\treturn;\n\t\t}\n\t\tdid_park = io_sq_thread_park(ctx->sq_data);\n\t\tif (did_park) {\n\t\t\ttask = ctx->sq_data->thread;\n\t\t\tatomic_inc(&task->io_uring->in_idle);\n\t\t}\n\t}\n\n\tio_cancel_defer_files(ctx, task, files);\n\n\tio_uring_cancel_files(ctx, task, files);\n\tif (!files)\n\t\tio_uring_try_cancel_requests(ctx, task, NULL);\n\n\tif (did_park) {\n\t\tatomic_dec(&task->io_uring->in_idle);\n\t\tio_sq_thread_unpark(ctx->sq_data);\n\t}\n}\n\n/*\n * Note that this task has used io_uring. We use it for cancelation purposes.\n */\nstatic int io_uring_add_task_file(struct io_ring_ctx *ctx, struct file *file)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tint ret;\n\n\tif (unlikely(!tctx)) {\n\t\tret = io_uring_alloc_task_context(current, ctx);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t\ttctx = current->io_uring;\n\t}\n\tif (tctx->last != file) {\n\t\tvoid *old = xa_load(&tctx->xa, (unsigned long)file);\n\n\t\tif (!old) {\n\t\t\tget_file(file);\n\t\t\tret = xa_err(xa_store(&tctx->xa, (unsigned long)file,\n\t\t\t\t\t\tfile, GFP_KERNEL));\n\t\t\tif (ret) {\n\t\t\t\tfput(file);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t\ttctx->last = file;\n\t}\n\n\t/*\n\t * This is race safe in that the task itself is doing this, hence it\n\t * cannot be going through the exit/cancel paths at the same time.\n\t * This cannot be modified while exit/cancel is running.\n\t */\n\tif (!tctx->sqpoll && (ctx->flags & IORING_SETUP_SQPOLL))\n\t\ttctx->sqpoll = true;\n\n\treturn 0;\n}\n\n/*\n * Remove this io_uring_file -> task mapping.\n */\nstatic void io_uring_del_task_file(struct file *file)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\n\tif (tctx->last == file)\n\t\ttctx->last = NULL;\n\tfile = xa_erase(&tctx->xa, (unsigned long)file);\n\tif (file)\n\t\tfput(file);\n}\n\nstatic void io_uring_clean_tctx(struct io_uring_task *tctx)\n{\n\tstruct file *file;\n\tunsigned long index;\n\n\txa_for_each(&tctx->xa, index, file)\n\t\tio_uring_del_task_file(file);\n\tif (tctx->io_wq) {\n\t\tio_wq_put_and_exit(tctx->io_wq);\n\t\ttctx->io_wq = NULL;\n\t}\n}\n\nvoid __io_uring_files_cancel(struct files_struct *files)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tstruct file *file;\n\tunsigned long index;\n\n\t/* make sure overflow events are dropped */\n\tatomic_inc(&tctx->in_idle);\n\txa_for_each(&tctx->xa, index, file)\n\t\tio_uring_cancel_task_requests(file->private_data, files);\n\tatomic_dec(&tctx->in_idle);\n\n\tif (files)\n\t\tio_uring_clean_tctx(tctx);\n}\n\nstatic s64 tctx_inflight(struct io_uring_task *tctx)\n{\n\treturn percpu_counter_sum(&tctx->inflight);\n}\n\nstatic void io_uring_cancel_sqpoll(struct io_ring_ctx *ctx)\n{\n\tstruct io_sq_data *sqd = ctx->sq_data;\n\tstruct io_uring_task *tctx;\n\ts64 inflight;\n\tDEFINE_WAIT(wait);\n\n\tif (!sqd)\n\t\treturn;\n\tif (!io_sq_thread_park(sqd))\n\t\treturn;\n\ttctx = ctx->sq_data->thread->io_uring;\n\t/* can happen on fork/alloc failure, just ignore that state */\n\tif (!tctx) {\n\t\tio_sq_thread_unpark(sqd);\n\t\treturn;\n\t}\n\n\tatomic_inc(&tctx->in_idle);\n\tdo {\n\t\t/* read completions before cancelations */\n\t\tinflight = tctx_inflight(tctx);\n\t\tif (!inflight)\n\t\t\tbreak;\n\t\tio_uring_cancel_task_requests(ctx, NULL);\n\n\t\tprepare_to_wait(&tctx->wait, &wait, TASK_UNINTERRUPTIBLE);\n\t\t/*\n\t\t * If we've seen completions, retry without waiting. This\n\t\t * avoids a race where a completion comes in before we did\n\t\t * prepare_to_wait().\n\t\t */\n\t\tif (inflight == tctx_inflight(tctx))\n\t\t\tschedule();\n\t\tfinish_wait(&tctx->wait, &wait);\n\t} while (1);\n\tatomic_dec(&tctx->in_idle);\n\tio_sq_thread_unpark(sqd);\n}\n\n/*\n * Find any io_uring fd that this task has registered or done IO on, and cancel\n * requests.\n */\nvoid __io_uring_task_cancel(void)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tDEFINE_WAIT(wait);\n\ts64 inflight;\n\n\t/* make sure overflow events are dropped */\n\tatomic_inc(&tctx->in_idle);\n\n\tif (tctx->sqpoll) {\n\t\tstruct file *file;\n\t\tunsigned long index;\n\n\t\txa_for_each(&tctx->xa, index, file)\n\t\t\tio_uring_cancel_sqpoll(file->private_data);\n\t}\n\n\tdo {\n\t\t/* read completions before cancelations */\n\t\tinflight = tctx_inflight(tctx);\n\t\tif (!inflight)\n\t\t\tbreak;\n\t\t__io_uring_files_cancel(NULL);\n\n\t\tprepare_to_wait(&tctx->wait, &wait, TASK_UNINTERRUPTIBLE);\n\n\t\t/*\n\t\t * If we've seen completions, retry without waiting. This\n\t\t * avoids a race where a completion comes in before we did\n\t\t * prepare_to_wait().\n\t\t */\n\t\tif (inflight == tctx_inflight(tctx))\n\t\t\tschedule();\n\t\tfinish_wait(&tctx->wait, &wait);\n\t} while (1);\n\n\tatomic_dec(&tctx->in_idle);\n\n\tio_uring_clean_tctx(tctx);\n\t/* all current's requests should be gone, we can kill tctx */\n\t__io_uring_free(current);\n}\n\nvoid __io_uring_unshare(void)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tstruct file *file;\n\tunsigned long index;\n\n\tio_wq_unshare(tctx->io_wq);\n\tif (!tctx->sqpoll)\n\t\treturn;\n\n\txa_for_each(&tctx->xa, index, file) {\n\t\tstruct io_ring_ctx *ctx = file->private_data;\n\n\t\tif (ctx->sq_data)\n\t\t\tio_sq_thread_stop(ctx->sq_data);\n\t}\n}\n\nstatic void *io_uring_validate_mmap_request(struct file *file,\n\t\t\t\t\t    loff_t pgoff, size_t sz)\n{\n\tstruct io_ring_ctx *ctx = file->private_data;\n\tloff_t offset = pgoff << PAGE_SHIFT;\n\tstruct page *page;\n\tvoid *ptr;\n\n\tswitch (offset) {\n\tcase IORING_OFF_SQ_RING:\n\tcase IORING_OFF_CQ_RING:\n\t\tptr = ctx->rings;\n\t\tbreak;\n\tcase IORING_OFF_SQES:\n\t\tptr = ctx->sq_sqes;\n\t\tbreak;\n\tdefault:\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tpage = virt_to_head_page(ptr);\n\tif (sz > page_size(page))\n\t\treturn ERR_PTR(-EINVAL);\n\n\treturn ptr;\n}\n\n#ifdef CONFIG_MMU\n\nstatic int io_uring_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tsize_t sz = vma->vm_end - vma->vm_start;\n\tunsigned long pfn;\n\tvoid *ptr;\n\n\tptr = io_uring_validate_mmap_request(file, vma->vm_pgoff, sz);\n\tif (IS_ERR(ptr))\n\t\treturn PTR_ERR(ptr);\n\n\tpfn = virt_to_phys(ptr) >> PAGE_SHIFT;\n\treturn remap_pfn_range(vma, vma->vm_start, pfn, sz, vma->vm_page_prot);\n}\n\n#else /* !CONFIG_MMU */\n\nstatic int io_uring_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\treturn vma->vm_flags & (VM_SHARED | VM_MAYSHARE) ? 0 : -EINVAL;\n}\n\nstatic unsigned int io_uring_nommu_mmap_capabilities(struct file *file)\n{\n\treturn NOMMU_MAP_DIRECT | NOMMU_MAP_READ | NOMMU_MAP_WRITE;\n}\n\nstatic unsigned long io_uring_nommu_get_unmapped_area(struct file *file,\n\tunsigned long addr, unsigned long len,\n\tunsigned long pgoff, unsigned long flags)\n{\n\tvoid *ptr;\n\n\tptr = io_uring_validate_mmap_request(file, pgoff, len);\n\tif (IS_ERR(ptr))\n\t\treturn PTR_ERR(ptr);\n\n\treturn (unsigned long) ptr;\n}\n\n#endif /* !CONFIG_MMU */\n\nstatic int io_sqpoll_wait_sq(struct io_ring_ctx *ctx)\n{\n\tint ret = 0;\n\tDEFINE_WAIT(wait);\n\n\tdo {\n\t\tif (!io_sqring_full(ctx))\n\t\t\tbreak;\n\t\tprepare_to_wait(&ctx->sqo_sq_wait, &wait, TASK_INTERRUPTIBLE);\n\n\t\tif (!io_sqring_full(ctx))\n\t\t\tbreak;\n\t\tschedule();\n\t} while (!signal_pending(current));\n\n\tfinish_wait(&ctx->sqo_sq_wait, &wait);\n\treturn ret;\n}\n\nstatic int io_get_ext_arg(unsigned flags, const void __user *argp, size_t *argsz,\n\t\t\t  struct __kernel_timespec __user **ts,\n\t\t\t  const sigset_t __user **sig)\n{\n\tstruct io_uring_getevents_arg arg;\n\n\t/*\n\t * If EXT_ARG isn't set, then we have no timespec and the argp pointer\n\t * is just a pointer to the sigset_t.\n\t */\n\tif (!(flags & IORING_ENTER_EXT_ARG)) {\n\t\t*sig = (const sigset_t __user *) argp;\n\t\t*ts = NULL;\n\t\treturn 0;\n\t}\n\n\t/*\n\t * EXT_ARG is set - ensure we agree on the size of it and copy in our\n\t * timespec and sigset_t pointers if good.\n\t */\n\tif (*argsz != sizeof(arg))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&arg, argp, sizeof(arg)))\n\t\treturn -EFAULT;\n\t*sig = u64_to_user_ptr(arg.sigmask);\n\t*argsz = arg.sigmask_sz;\n\t*ts = u64_to_user_ptr(arg.ts);\n\treturn 0;\n}\n\nSYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,\n\t\tu32, min_complete, u32, flags, const void __user *, argp,\n\t\tsize_t, argsz)\n{\n\tstruct io_ring_ctx *ctx;\n\tlong ret = -EBADF;\n\tint submitted = 0;\n\tstruct fd f;\n\n\tio_run_task_work();\n\n\tif (flags & ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP |\n\t\t\tIORING_ENTER_SQ_WAIT | IORING_ENTER_EXT_ARG))\n\t\treturn -EINVAL;\n\n\tf = fdget(fd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tret = -EOPNOTSUPP;\n\tif (f.file->f_op != &io_uring_fops)\n\t\tgoto out_fput;\n\n\tret = -ENXIO;\n\tctx = f.file->private_data;\n\tif (!percpu_ref_tryget(&ctx->refs))\n\t\tgoto out_fput;\n\n\tret = -EBADFD;\n\tif (ctx->flags & IORING_SETUP_R_DISABLED)\n\t\tgoto out;\n\n\t/*\n\t * For SQ polling, the thread will do all submissions and completions.\n\t * Just return the requested submit count, and wake the thread if\n\t * we were asked to.\n\t */\n\tret = 0;\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\tio_cqring_overflow_flush(ctx, false, NULL, NULL);\n\n\t\tif (unlikely(ctx->sqo_exec)) {\n\t\t\tstruct io_sq_data *sqd = ctx->sq_data;\n\n\t\t\tret = io_sq_thread_fork(sqd, ctx);\n\t\t\tif (ret)\n\t\t\t\tset_bit(IO_SQ_THREAD_SHOULD_STOP, &sqd->state);\n\t\t\tcomplete(&sqd->startup);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t\tret = -EOWNERDEAD;\n\t\tif (flags & IORING_ENTER_SQ_WAKEUP)\n\t\t\twake_up(&ctx->sq_data->wait);\n\t\tif (flags & IORING_ENTER_SQ_WAIT) {\n\t\t\tret = io_sqpoll_wait_sq(ctx);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t\tsubmitted = to_submit;\n\t} else if (to_submit) {\n\t\tret = io_uring_add_task_file(ctx, f.file);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tsubmitted = io_submit_sqes(ctx, to_submit);\n\t\tmutex_unlock(&ctx->uring_lock);\n\n\t\tif (submitted != to_submit)\n\t\t\tgoto out;\n\t}\n\tif (flags & IORING_ENTER_GETEVENTS) {\n\t\tconst sigset_t __user *sig;\n\t\tstruct __kernel_timespec __user *ts;\n\n\t\tret = io_get_ext_arg(flags, argp, &argsz, &ts, &sig);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\n\t\tmin_complete = min(min_complete, ctx->cq_entries);\n\n\t\t/*\n\t\t * When SETUP_IOPOLL and SETUP_SQPOLL are both enabled, user\n\t\t * space applications don't need to do io completion events\n\t\t * polling again, they can rely on io_sq_thread to do polling\n\t\t * work, which can reduce cpu usage and uring_lock contention.\n\t\t */\n\t\tif (ctx->flags & IORING_SETUP_IOPOLL &&\n\t\t    !(ctx->flags & IORING_SETUP_SQPOLL)) {\n\t\t\tret = io_iopoll_check(ctx, min_complete);\n\t\t} else {\n\t\t\tret = io_cqring_wait(ctx, min_complete, sig, argsz, ts);\n\t\t}\n\t}\n\nout:\n\tpercpu_ref_put(&ctx->refs);\nout_fput:\n\tfdput(f);\n\treturn submitted ? submitted : ret;\n}\n\n#ifdef CONFIG_PROC_FS\nstatic int io_uring_show_cred(int id, void *p, void *data)\n{\n\tconst struct cred *cred = p;\n\tstruct seq_file *m = data;\n\tstruct user_namespace *uns = seq_user_ns(m);\n\tstruct group_info *gi;\n\tkernel_cap_t cap;\n\tunsigned __capi;\n\tint g;\n\n\tseq_printf(m, \"%5d\\n\", id);\n\tseq_put_decimal_ull(m, \"\\tUid:\\t\", from_kuid_munged(uns, cred->uid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->euid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->suid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->fsuid));\n\tseq_put_decimal_ull(m, \"\\n\\tGid:\\t\", from_kgid_munged(uns, cred->gid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->egid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->sgid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->fsgid));\n\tseq_puts(m, \"\\n\\tGroups:\\t\");\n\tgi = cred->group_info;\n\tfor (g = 0; g < gi->ngroups; g++) {\n\t\tseq_put_decimal_ull(m, g ? \" \" : \"\",\n\t\t\t\t\tfrom_kgid_munged(uns, gi->gid[g]));\n\t}\n\tseq_puts(m, \"\\n\\tCapEff:\\t\");\n\tcap = cred->cap_effective;\n\tCAP_FOR_EACH_U32(__capi)\n\t\tseq_put_hex_ll(m, NULL, cap.cap[CAP_LAST_U32 - __capi], 8);\n\tseq_putc(m, '\\n');\n\treturn 0;\n}\n\nstatic void __io_uring_show_fdinfo(struct io_ring_ctx *ctx, struct seq_file *m)\n{\n\tstruct io_sq_data *sq = NULL;\n\tbool has_lock;\n\tint i;\n\n\t/*\n\t * Avoid ABBA deadlock between the seq lock and the io_uring mutex,\n\t * since fdinfo case grabs it in the opposite direction of normal use\n\t * cases. If we fail to get the lock, we just don't iterate any\n\t * structures that could be going away outside the io_uring mutex.\n\t */\n\thas_lock = mutex_trylock(&ctx->uring_lock);\n\n\tif (has_lock && (ctx->flags & IORING_SETUP_SQPOLL)) {\n\t\tsq = ctx->sq_data;\n\t\tif (!sq->thread)\n\t\t\tsq = NULL;\n\t}\n\n\tseq_printf(m, \"SqThread:\\t%d\\n\", sq ? task_pid_nr(sq->thread) : -1);\n\tseq_printf(m, \"SqThreadCpu:\\t%d\\n\", sq ? task_cpu(sq->thread) : -1);\n\tseq_printf(m, \"UserFiles:\\t%u\\n\", ctx->nr_user_files);\n\tfor (i = 0; has_lock && i < ctx->nr_user_files; i++) {\n\t\tstruct file *f = *io_fixed_file_slot(ctx->file_data, i);\n\n\t\tif (f)\n\t\t\tseq_printf(m, \"%5u: %s\\n\", i, file_dentry(f)->d_iname);\n\t\telse\n\t\t\tseq_printf(m, \"%5u: <none>\\n\", i);\n\t}\n\tseq_printf(m, \"UserBufs:\\t%u\\n\", ctx->nr_user_bufs);\n\tfor (i = 0; has_lock && i < ctx->nr_user_bufs; i++) {\n\t\tstruct io_mapped_ubuf *buf = &ctx->user_bufs[i];\n\n\t\tseq_printf(m, \"%5u: 0x%llx/%u\\n\", i, buf->ubuf,\n\t\t\t\t\t\t(unsigned int) buf->len);\n\t}\n\tif (has_lock && !idr_is_empty(&ctx->personality_idr)) {\n\t\tseq_printf(m, \"Personalities:\\n\");\n\t\tidr_for_each(&ctx->personality_idr, io_uring_show_cred, m);\n\t}\n\tseq_printf(m, \"PollList:\\n\");\n\tspin_lock_irq(&ctx->completion_lock);\n\tfor (i = 0; i < (1U << ctx->cancel_hash_bits); i++) {\n\t\tstruct hlist_head *list = &ctx->cancel_hash[i];\n\t\tstruct io_kiocb *req;\n\n\t\thlist_for_each_entry(req, list, hash_node)\n\t\t\tseq_printf(m, \"  op=%d, task_works=%d\\n\", req->opcode,\n\t\t\t\t\treq->task->task_works != NULL);\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\tif (has_lock)\n\t\tmutex_unlock(&ctx->uring_lock);\n}\n\nstatic void io_uring_show_fdinfo(struct seq_file *m, struct file *f)\n{\n\tstruct io_ring_ctx *ctx = f->private_data;\n\n\tif (percpu_ref_tryget(&ctx->refs)) {\n\t\t__io_uring_show_fdinfo(ctx, m);\n\t\tpercpu_ref_put(&ctx->refs);\n\t}\n}\n#endif\n\nstatic const struct file_operations io_uring_fops = {\n\t.release\t= io_uring_release,\n\t.mmap\t\t= io_uring_mmap,\n#ifndef CONFIG_MMU\n\t.get_unmapped_area = io_uring_nommu_get_unmapped_area,\n\t.mmap_capabilities = io_uring_nommu_mmap_capabilities,\n#endif\n\t.poll\t\t= io_uring_poll,\n\t.fasync\t\t= io_uring_fasync,\n#ifdef CONFIG_PROC_FS\n\t.show_fdinfo\t= io_uring_show_fdinfo,\n#endif\n};\n\nstatic int io_allocate_scq_urings(struct io_ring_ctx *ctx,\n\t\t\t\t  struct io_uring_params *p)\n{\n\tstruct io_rings *rings;\n\tsize_t size, sq_array_offset;\n\n\t/* make sure these are sane, as we already accounted them */\n\tctx->sq_entries = p->sq_entries;\n\tctx->cq_entries = p->cq_entries;\n\n\tsize = rings_size(p->sq_entries, p->cq_entries, &sq_array_offset);\n\tif (size == SIZE_MAX)\n\t\treturn -EOVERFLOW;\n\n\trings = io_mem_alloc(size);\n\tif (!rings)\n\t\treturn -ENOMEM;\n\n\tctx->rings = rings;\n\tctx->sq_array = (u32 *)((char *)rings + sq_array_offset);\n\trings->sq_ring_mask = p->sq_entries - 1;\n\trings->cq_ring_mask = p->cq_entries - 1;\n\trings->sq_ring_entries = p->sq_entries;\n\trings->cq_ring_entries = p->cq_entries;\n\tctx->sq_mask = rings->sq_ring_mask;\n\tctx->cq_mask = rings->cq_ring_mask;\n\n\tsize = array_size(sizeof(struct io_uring_sqe), p->sq_entries);\n\tif (size == SIZE_MAX) {\n\t\tio_mem_free(ctx->rings);\n\t\tctx->rings = NULL;\n\t\treturn -EOVERFLOW;\n\t}\n\n\tctx->sq_sqes = io_mem_alloc(size);\n\tif (!ctx->sq_sqes) {\n\t\tio_mem_free(ctx->rings);\n\t\tctx->rings = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic int io_uring_install_fd(struct io_ring_ctx *ctx, struct file *file)\n{\n\tint ret, fd;\n\n\tfd = get_unused_fd_flags(O_RDWR | O_CLOEXEC);\n\tif (fd < 0)\n\t\treturn fd;\n\n\tret = io_uring_add_task_file(ctx, file);\n\tif (ret) {\n\t\tput_unused_fd(fd);\n\t\treturn ret;\n\t}\n\tfd_install(fd, file);\n\treturn fd;\n}\n\n/*\n * Allocate an anonymous fd, this is what constitutes the application\n * visible backing of an io_uring instance. The application mmaps this\n * fd to gain access to the SQ/CQ ring details. If UNIX sockets are enabled,\n * we have to tie this fd to a socket for file garbage collection purposes.\n */\nstatic struct file *io_uring_get_file(struct io_ring_ctx *ctx)\n{\n\tstruct file *file;\n#if defined(CONFIG_UNIX)\n\tint ret;\n\n\tret = sock_create_kern(&init_net, PF_UNIX, SOCK_RAW, IPPROTO_IP,\n\t\t\t\t&ctx->ring_sock);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n#endif\n\n\tfile = anon_inode_getfile(\"[io_uring]\", &io_uring_fops, ctx,\n\t\t\t\t\tO_RDWR | O_CLOEXEC);\n#if defined(CONFIG_UNIX)\n\tif (IS_ERR(file)) {\n\t\tsock_release(ctx->ring_sock);\n\t\tctx->ring_sock = NULL;\n\t} else {\n\t\tctx->ring_sock->file = file;\n\t}\n#endif\n\treturn file;\n}\n\nstatic int io_uring_create(unsigned entries, struct io_uring_params *p,\n\t\t\t   struct io_uring_params __user *params)\n{\n\tstruct io_ring_ctx *ctx;\n\tstruct file *file;\n\tint ret;\n\n\tif (!entries)\n\t\treturn -EINVAL;\n\tif (entries > IORING_MAX_ENTRIES) {\n\t\tif (!(p->flags & IORING_SETUP_CLAMP))\n\t\t\treturn -EINVAL;\n\t\tentries = IORING_MAX_ENTRIES;\n\t}\n\n\t/*\n\t * Use twice as many entries for the CQ ring. It's possible for the\n\t * application to drive a higher depth than the size of the SQ ring,\n\t * since the sqes are only used at submission time. This allows for\n\t * some flexibility in overcommitting a bit. If the application has\n\t * set IORING_SETUP_CQSIZE, it will have passed in the desired number\n\t * of CQ ring entries manually.\n\t */\n\tp->sq_entries = roundup_pow_of_two(entries);\n\tif (p->flags & IORING_SETUP_CQSIZE) {\n\t\t/*\n\t\t * If IORING_SETUP_CQSIZE is set, we do the same roundup\n\t\t * to a power-of-two, if it isn't already. We do NOT impose\n\t\t * any cq vs sq ring sizing.\n\t\t */\n\t\tif (!p->cq_entries)\n\t\t\treturn -EINVAL;\n\t\tif (p->cq_entries > IORING_MAX_CQ_ENTRIES) {\n\t\t\tif (!(p->flags & IORING_SETUP_CLAMP))\n\t\t\t\treturn -EINVAL;\n\t\t\tp->cq_entries = IORING_MAX_CQ_ENTRIES;\n\t\t}\n\t\tp->cq_entries = roundup_pow_of_two(p->cq_entries);\n\t\tif (p->cq_entries < p->sq_entries)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tp->cq_entries = 2 * p->sq_entries;\n\t}\n\n\tctx = io_ring_ctx_alloc(p);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\tctx->compat = in_compat_syscall();\n\tif (!capable(CAP_IPC_LOCK))\n\t\tctx->user = get_uid(current_user());\n\n\t/*\n\t * This is just grabbed for accounting purposes. When a process exits,\n\t * the mm is exited and dropped before the files, hence we need to hang\n\t * on to this mm purely for the purposes of being able to unaccount\n\t * memory (locked/pinned vm). It's not used for anything else.\n\t */\n\tmmgrab(current->mm);\n\tctx->mm_account = current->mm;\n\n\tret = io_allocate_scq_urings(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\tret = io_sq_offload_create(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\tif (!(p->flags & IORING_SETUP_R_DISABLED))\n\t\tio_sq_offload_start(ctx);\n\n\tmemset(&p->sq_off, 0, sizeof(p->sq_off));\n\tp->sq_off.head = offsetof(struct io_rings, sq.head);\n\tp->sq_off.tail = offsetof(struct io_rings, sq.tail);\n\tp->sq_off.ring_mask = offsetof(struct io_rings, sq_ring_mask);\n\tp->sq_off.ring_entries = offsetof(struct io_rings, sq_ring_entries);\n\tp->sq_off.flags = offsetof(struct io_rings, sq_flags);\n\tp->sq_off.dropped = offsetof(struct io_rings, sq_dropped);\n\tp->sq_off.array = (char *)ctx->sq_array - (char *)ctx->rings;\n\n\tmemset(&p->cq_off, 0, sizeof(p->cq_off));\n\tp->cq_off.head = offsetof(struct io_rings, cq.head);\n\tp->cq_off.tail = offsetof(struct io_rings, cq.tail);\n\tp->cq_off.ring_mask = offsetof(struct io_rings, cq_ring_mask);\n\tp->cq_off.ring_entries = offsetof(struct io_rings, cq_ring_entries);\n\tp->cq_off.overflow = offsetof(struct io_rings, cq_overflow);\n\tp->cq_off.cqes = offsetof(struct io_rings, cqes);\n\tp->cq_off.flags = offsetof(struct io_rings, cq_flags);\n\n\tp->features = IORING_FEAT_SINGLE_MMAP | IORING_FEAT_NODROP |\n\t\t\tIORING_FEAT_SUBMIT_STABLE | IORING_FEAT_RW_CUR_POS |\n\t\t\tIORING_FEAT_CUR_PERSONALITY | IORING_FEAT_FAST_POLL |\n\t\t\tIORING_FEAT_POLL_32BITS | IORING_FEAT_SQPOLL_NONFIXED |\n\t\t\tIORING_FEAT_EXT_ARG | IORING_FEAT_NATIVE_WORKERS;\n\n\tif (copy_to_user(params, p, sizeof(*p))) {\n\t\tret = -EFAULT;\n\t\tgoto err;\n\t}\n\n\tfile = io_uring_get_file(ctx);\n\tif (IS_ERR(file)) {\n\t\tret = PTR_ERR(file);\n\t\tgoto err;\n\t}\n\n\t/*\n\t * Install ring fd as the very last thing, so we don't risk someone\n\t * having closed it before we finish setup\n\t */\n\tret = io_uring_install_fd(ctx, file);\n\tif (ret < 0) {\n\t\t/* fput will clean it up */\n\t\tfput(file);\n\t\treturn ret;\n\t}\n\n\ttrace_io_uring_create(ret, ctx, p->sq_entries, p->cq_entries, p->flags);\n\treturn ret;\nerr:\n\tio_ring_ctx_wait_and_kill(ctx);\n\treturn ret;\n}\n\n/*\n * Sets up an aio uring context, and returns the fd. Applications asks for a\n * ring size, we return the actual sq/cq ring sizes (among other things) in the\n * params structure passed in.\n */\nstatic long io_uring_setup(u32 entries, struct io_uring_params __user *params)\n{\n\tstruct io_uring_params p;\n\tint i;\n\n\tif (copy_from_user(&p, params, sizeof(p)))\n\t\treturn -EFAULT;\n\tfor (i = 0; i < ARRAY_SIZE(p.resv); i++) {\n\t\tif (p.resv[i])\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (p.flags & ~(IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL |\n\t\t\tIORING_SETUP_SQ_AFF | IORING_SETUP_CQSIZE |\n\t\t\tIORING_SETUP_CLAMP | IORING_SETUP_ATTACH_WQ |\n\t\t\tIORING_SETUP_R_DISABLED))\n\t\treturn -EINVAL;\n\n\treturn  io_uring_create(entries, &p, params);\n}\n\nSYSCALL_DEFINE2(io_uring_setup, u32, entries,\n\t\tstruct io_uring_params __user *, params)\n{\n\treturn io_uring_setup(entries, params);\n}\n\nstatic int io_probe(struct io_ring_ctx *ctx, void __user *arg, unsigned nr_args)\n{\n\tstruct io_uring_probe *p;\n\tsize_t size;\n\tint i, ret;\n\n\tsize = struct_size(p, ops, nr_args);\n\tif (size == SIZE_MAX)\n\t\treturn -EOVERFLOW;\n\tp = kzalloc(size, GFP_KERNEL);\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tret = -EFAULT;\n\tif (copy_from_user(p, arg, size))\n\t\tgoto out;\n\tret = -EINVAL;\n\tif (memchr_inv(p, 0, size))\n\t\tgoto out;\n\n\tp->last_op = IORING_OP_LAST - 1;\n\tif (nr_args > IORING_OP_LAST)\n\t\tnr_args = IORING_OP_LAST;\n\n\tfor (i = 0; i < nr_args; i++) {\n\t\tp->ops[i].op = i;\n\t\tif (!io_op_defs[i].not_supported)\n\t\t\tp->ops[i].flags = IO_URING_OP_SUPPORTED;\n\t}\n\tp->ops_len = i;\n\n\tret = 0;\n\tif (copy_to_user(arg, p, size))\n\t\tret = -EFAULT;\nout:\n\tkfree(p);\n\treturn ret;\n}\n\nstatic int io_register_personality(struct io_ring_ctx *ctx)\n{\n\tconst struct cred *creds;\n\tint ret;\n\n\tcreds = get_current_cred();\n\n\tret = idr_alloc_cyclic(&ctx->personality_idr, (void *) creds, 1,\n\t\t\t\tUSHRT_MAX, GFP_KERNEL);\n\tif (ret < 0)\n\t\tput_cred(creds);\n\treturn ret;\n}\n\nstatic int io_register_restrictions(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t\t    unsigned int nr_args)\n{\n\tstruct io_uring_restriction *res;\n\tsize_t size;\n\tint i, ret;\n\n\t/* Restrictions allowed only if rings started disabled */\n\tif (!(ctx->flags & IORING_SETUP_R_DISABLED))\n\t\treturn -EBADFD;\n\n\t/* We allow only a single restrictions registration */\n\tif (ctx->restrictions.registered)\n\t\treturn -EBUSY;\n\n\tif (!arg || nr_args > IORING_MAX_RESTRICTIONS)\n\t\treturn -EINVAL;\n\n\tsize = array_size(nr_args, sizeof(*res));\n\tif (size == SIZE_MAX)\n\t\treturn -EOVERFLOW;\n\n\tres = memdup_user(arg, size);\n\tif (IS_ERR(res))\n\t\treturn PTR_ERR(res);\n\n\tret = 0;\n\n\tfor (i = 0; i < nr_args; i++) {\n\t\tswitch (res[i].opcode) {\n\t\tcase IORING_RESTRICTION_REGISTER_OP:\n\t\t\tif (res[i].register_op >= IORING_REGISTER_LAST) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t__set_bit(res[i].register_op,\n\t\t\t\t  ctx->restrictions.register_op);\n\t\t\tbreak;\n\t\tcase IORING_RESTRICTION_SQE_OP:\n\t\t\tif (res[i].sqe_op >= IORING_OP_LAST) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t__set_bit(res[i].sqe_op, ctx->restrictions.sqe_op);\n\t\t\tbreak;\n\t\tcase IORING_RESTRICTION_SQE_FLAGS_ALLOWED:\n\t\t\tctx->restrictions.sqe_flags_allowed = res[i].sqe_flags;\n\t\t\tbreak;\n\t\tcase IORING_RESTRICTION_SQE_FLAGS_REQUIRED:\n\t\t\tctx->restrictions.sqe_flags_required = res[i].sqe_flags;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\t/* Reset all restrictions if an error happened */\n\tif (ret != 0)\n\t\tmemset(&ctx->restrictions, 0, sizeof(ctx->restrictions));\n\telse\n\t\tctx->restrictions.registered = true;\n\n\tkfree(res);\n\treturn ret;\n}\n\nstatic int io_register_enable_rings(struct io_ring_ctx *ctx)\n{\n\tif (!(ctx->flags & IORING_SETUP_R_DISABLED))\n\t\treturn -EBADFD;\n\n\tif (ctx->restrictions.registered)\n\t\tctx->restricted = 1;\n\n\tio_sq_offload_start(ctx);\n\treturn 0;\n}\n\nstatic bool io_register_op_must_quiesce(int op)\n{\n\tswitch (op) {\n\tcase IORING_UNREGISTER_FILES:\n\tcase IORING_REGISTER_FILES_UPDATE:\n\tcase IORING_REGISTER_PROBE:\n\tcase IORING_REGISTER_PERSONALITY:\n\tcase IORING_UNREGISTER_PERSONALITY:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}\n\nstatic int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,\n\t\t\t       void __user *arg, unsigned nr_args)\n\t__releases(ctx->uring_lock)\n\t__acquires(ctx->uring_lock)\n{\n\tint ret;\n\n\t/*\n\t * We're inside the ring mutex, if the ref is already dying, then\n\t * someone else killed the ctx or is already going through\n\t * io_uring_register().\n\t */\n\tif (percpu_ref_is_dying(&ctx->refs))\n\t\treturn -ENXIO;\n\n\tif (io_register_op_must_quiesce(opcode)) {\n\t\tpercpu_ref_kill(&ctx->refs);\n\n\t\t/*\n\t\t * Drop uring mutex before waiting for references to exit. If\n\t\t * another thread is currently inside io_uring_enter() it might\n\t\t * need to grab the uring_lock to make progress. If we hold it\n\t\t * here across the drain wait, then we can deadlock. It's safe\n\t\t * to drop the mutex here, since no new references will come in\n\t\t * after we've killed the percpu ref.\n\t\t */\n\t\tmutex_unlock(&ctx->uring_lock);\n\t\tdo {\n\t\t\tret = wait_for_completion_interruptible(&ctx->ref_comp);\n\t\t\tif (!ret)\n\t\t\t\tbreak;\n\t\t\tret = io_run_task_work_sig();\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t} while (1);\n\n\t\tmutex_lock(&ctx->uring_lock);\n\n\t\tif (ret) {\n\t\t\tpercpu_ref_resurrect(&ctx->refs);\n\t\t\tgoto out_quiesce;\n\t\t}\n\t}\n\n\tif (ctx->restricted) {\n\t\tif (opcode >= IORING_REGISTER_LAST) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (!test_bit(opcode, ctx->restrictions.register_op)) {\n\t\t\tret = -EACCES;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tswitch (opcode) {\n\tcase IORING_REGISTER_BUFFERS:\n\t\tret = io_sqe_buffers_register(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_UNREGISTER_BUFFERS:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_sqe_buffers_unregister(ctx);\n\t\tbreak;\n\tcase IORING_REGISTER_FILES:\n\t\tret = io_sqe_files_register(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_UNREGISTER_FILES:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_sqe_files_unregister(ctx);\n\t\tbreak;\n\tcase IORING_REGISTER_FILES_UPDATE:\n\t\tret = io_sqe_files_update(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_REGISTER_EVENTFD:\n\tcase IORING_REGISTER_EVENTFD_ASYNC:\n\t\tret = -EINVAL;\n\t\tif (nr_args != 1)\n\t\t\tbreak;\n\t\tret = io_eventfd_register(ctx, arg);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (opcode == IORING_REGISTER_EVENTFD_ASYNC)\n\t\t\tctx->eventfd_async = 1;\n\t\telse\n\t\t\tctx->eventfd_async = 0;\n\t\tbreak;\n\tcase IORING_UNREGISTER_EVENTFD:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_eventfd_unregister(ctx);\n\t\tbreak;\n\tcase IORING_REGISTER_PROBE:\n\t\tret = -EINVAL;\n\t\tif (!arg || nr_args > 256)\n\t\t\tbreak;\n\t\tret = io_probe(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_REGISTER_PERSONALITY:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_register_personality(ctx);\n\t\tbreak;\n\tcase IORING_UNREGISTER_PERSONALITY:\n\t\tret = -EINVAL;\n\t\tif (arg)\n\t\t\tbreak;\n\t\tret = io_unregister_personality(ctx, nr_args);\n\t\tbreak;\n\tcase IORING_REGISTER_ENABLE_RINGS:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_register_enable_rings(ctx);\n\t\tbreak;\n\tcase IORING_REGISTER_RESTRICTIONS:\n\t\tret = io_register_restrictions(ctx, arg, nr_args);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\nout:\n\tif (io_register_op_must_quiesce(opcode)) {\n\t\t/* bring the ctx back to life */\n\t\tpercpu_ref_reinit(&ctx->refs);\nout_quiesce:\n\t\treinit_completion(&ctx->ref_comp);\n\t}\n\treturn ret;\n}\n\nSYSCALL_DEFINE4(io_uring_register, unsigned int, fd, unsigned int, opcode,\n\t\tvoid __user *, arg, unsigned int, nr_args)\n{\n\tstruct io_ring_ctx *ctx;\n\tlong ret = -EBADF;\n\tstruct fd f;\n\n\tf = fdget(fd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tret = -EOPNOTSUPP;\n\tif (f.file->f_op != &io_uring_fops)\n\t\tgoto out_fput;\n\n\tctx = f.file->private_data;\n\n\tio_run_task_work();\n\n\tmutex_lock(&ctx->uring_lock);\n\tret = __io_uring_register(ctx, opcode, arg, nr_args);\n\tmutex_unlock(&ctx->uring_lock);\n\ttrace_io_uring_register(ctx, opcode, ctx->nr_user_files, ctx->nr_user_bufs,\n\t\t\t\t\t\t\tctx->cq_ev_fd != NULL, ret);\nout_fput:\n\tfdput(f);\n\treturn ret;\n}\n\nstatic int __init io_uring_init(void)\n{\n#define __BUILD_BUG_VERIFY_ELEMENT(stype, eoffset, etype, ename) do { \\\n\tBUILD_BUG_ON(offsetof(stype, ename) != eoffset); \\\n\tBUILD_BUG_ON(sizeof(etype) != sizeof_field(stype, ename)); \\\n} while (0)\n\n#define BUILD_BUG_SQE_ELEM(eoffset, etype, ename) \\\n\t__BUILD_BUG_VERIFY_ELEMENT(struct io_uring_sqe, eoffset, etype, ename)\n\tBUILD_BUG_ON(sizeof(struct io_uring_sqe) != 64);\n\tBUILD_BUG_SQE_ELEM(0,  __u8,   opcode);\n\tBUILD_BUG_SQE_ELEM(1,  __u8,   flags);\n\tBUILD_BUG_SQE_ELEM(2,  __u16,  ioprio);\n\tBUILD_BUG_SQE_ELEM(4,  __s32,  fd);\n\tBUILD_BUG_SQE_ELEM(8,  __u64,  off);\n\tBUILD_BUG_SQE_ELEM(8,  __u64,  addr2);\n\tBUILD_BUG_SQE_ELEM(16, __u64,  addr);\n\tBUILD_BUG_SQE_ELEM(16, __u64,  splice_off_in);\n\tBUILD_BUG_SQE_ELEM(24, __u32,  len);\n\tBUILD_BUG_SQE_ELEM(28,     __kernel_rwf_t, rw_flags);\n\tBUILD_BUG_SQE_ELEM(28, /* compat */   int, rw_flags);\n\tBUILD_BUG_SQE_ELEM(28, /* compat */ __u32, rw_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  fsync_flags);\n\tBUILD_BUG_SQE_ELEM(28, /* compat */ __u16,  poll_events);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  poll32_events);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  sync_range_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  msg_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  timeout_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  accept_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  cancel_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  open_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  statx_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  fadvise_advice);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  splice_flags);\n\tBUILD_BUG_SQE_ELEM(32, __u64,  user_data);\n\tBUILD_BUG_SQE_ELEM(40, __u16,  buf_index);\n\tBUILD_BUG_SQE_ELEM(42, __u16,  personality);\n\tBUILD_BUG_SQE_ELEM(44, __s32,  splice_fd_in);\n\n\tBUILD_BUG_ON(ARRAY_SIZE(io_op_defs) != IORING_OP_LAST);\n\tBUILD_BUG_ON(__REQ_F_LAST_BIT >= 8 * sizeof(int));\n\treq_cachep = KMEM_CACHE(io_kiocb, SLAB_HWCACHE_ALIGN | SLAB_PANIC |\n\t\t\t\tSLAB_ACCOUNT);\n\treturn 0;\n};\n__initcall(io_uring_init);\n"}, "0": {"id": 0, "path": "/src/include/linux/skbuff.h", "content": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/*\n *\tDefinitions for the 'struct sk_buff' memory handlers.\n *\n *\tAuthors:\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\t\tFlorian La Roche, <rzsfl@rz.uni-sb.de>\n */\n\n#ifndef _LINUX_SKBUFF_H\n#define _LINUX_SKBUFF_H\n\n#include <linux/kernel.h>\n#include <linux/compiler.h>\n#include <linux/time.h>\n#include <linux/bug.h>\n#include <linux/bvec.h>\n#include <linux/cache.h>\n#include <linux/rbtree.h>\n#include <linux/socket.h>\n#include <linux/refcount.h>\n\n#include <linux/atomic.h>\n#include <asm/types.h>\n#include <linux/spinlock.h>\n#include <linux/net.h>\n#include <linux/textsearch.h>\n#include <net/checksum.h>\n#include <linux/rcupdate.h>\n#include <linux/hrtimer.h>\n#include <linux/dma-mapping.h>\n#include <linux/netdev_features.h>\n#include <linux/sched.h>\n#include <linux/sched/clock.h>\n#include <net/flow_dissector.h>\n#include <linux/splice.h>\n#include <linux/in6.h>\n#include <linux/if_packet.h>\n#include <net/flow.h>\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n#include <linux/netfilter/nf_conntrack_common.h>\n#endif\n\n/* The interface for checksum offload between the stack and networking drivers\n * is as follows...\n *\n * A. IP checksum related features\n *\n * Drivers advertise checksum offload capabilities in the features of a device.\n * From the stack's point of view these are capabilities offered by the driver.\n * A driver typically only advertises features that it is capable of offloading\n * to its device.\n *\n * The checksum related features are:\n *\n *\tNETIF_F_HW_CSUM\t- The driver (or its device) is able to compute one\n *\t\t\t  IP (one's complement) checksum for any combination\n *\t\t\t  of protocols or protocol layering. The checksum is\n *\t\t\t  computed and set in a packet per the CHECKSUM_PARTIAL\n *\t\t\t  interface (see below).\n *\n *\tNETIF_F_IP_CSUM - Driver (device) is only able to checksum plain\n *\t\t\t  TCP or UDP packets over IPv4. These are specifically\n *\t\t\t  unencapsulated packets of the form IPv4|TCP or\n *\t\t\t  IPv4|UDP where the Protocol field in the IPv4 header\n *\t\t\t  is TCP or UDP. The IPv4 header may contain IP options.\n *\t\t\t  This feature cannot be set in features for a device\n *\t\t\t  with NETIF_F_HW_CSUM also set. This feature is being\n *\t\t\t  DEPRECATED (see below).\n *\n *\tNETIF_F_IPV6_CSUM - Driver (device) is only able to checksum plain\n *\t\t\t  TCP or UDP packets over IPv6. These are specifically\n *\t\t\t  unencapsulated packets of the form IPv6|TCP or\n *\t\t\t  IPv6|UDP where the Next Header field in the IPv6\n *\t\t\t  header is either TCP or UDP. IPv6 extension headers\n *\t\t\t  are not supported with this feature. This feature\n *\t\t\t  cannot be set in features for a device with\n *\t\t\t  NETIF_F_HW_CSUM also set. This feature is being\n *\t\t\t  DEPRECATED (see below).\n *\n *\tNETIF_F_RXCSUM - Driver (device) performs receive checksum offload.\n *\t\t\t This flag is only used to disable the RX checksum\n *\t\t\t feature for a device. The stack will accept receive\n *\t\t\t checksum indication in packets received on a device\n *\t\t\t regardless of whether NETIF_F_RXCSUM is set.\n *\n * B. Checksumming of received packets by device. Indication of checksum\n *    verification is set in skb->ip_summed. Possible values are:\n *\n * CHECKSUM_NONE:\n *\n *   Device did not checksum this packet e.g. due to lack of capabilities.\n *   The packet contains full (though not verified) checksum in packet but\n *   not in skb->csum. Thus, skb->csum is undefined in this case.\n *\n * CHECKSUM_UNNECESSARY:\n *\n *   The hardware you're dealing with doesn't calculate the full checksum\n *   (as in CHECKSUM_COMPLETE), but it does parse headers and verify checksums\n *   for specific protocols. For such packets it will set CHECKSUM_UNNECESSARY\n *   if their checksums are okay. skb->csum is still undefined in this case\n *   though. A driver or device must never modify the checksum field in the\n *   packet even if checksum is verified.\n *\n *   CHECKSUM_UNNECESSARY is applicable to following protocols:\n *     TCP: IPv6 and IPv4.\n *     UDP: IPv4 and IPv6. A device may apply CHECKSUM_UNNECESSARY to a\n *       zero UDP checksum for either IPv4 or IPv6, the networking stack\n *       may perform further validation in this case.\n *     GRE: only if the checksum is present in the header.\n *     SCTP: indicates the CRC in SCTP header has been validated.\n *     FCOE: indicates the CRC in FC frame has been validated.\n *\n *   skb->csum_level indicates the number of consecutive checksums found in\n *   the packet minus one that have been verified as CHECKSUM_UNNECESSARY.\n *   For instance if a device receives an IPv6->UDP->GRE->IPv4->TCP packet\n *   and a device is able to verify the checksums for UDP (possibly zero),\n *   GRE (checksum flag is set) and TCP, skb->csum_level would be set to\n *   two. If the device were only able to verify the UDP checksum and not\n *   GRE, either because it doesn't support GRE checksum or because GRE\n *   checksum is bad, skb->csum_level would be set to zero (TCP checksum is\n *   not considered in this case).\n *\n * CHECKSUM_COMPLETE:\n *\n *   This is the most generic way. The device supplied checksum of the _whole_\n *   packet as seen by netif_rx() and fills in skb->csum. This means the\n *   hardware doesn't need to parse L3/L4 headers to implement this.\n *\n *   Notes:\n *   - Even if device supports only some protocols, but is able to produce\n *     skb->csum, it MUST use CHECKSUM_COMPLETE, not CHECKSUM_UNNECESSARY.\n *   - CHECKSUM_COMPLETE is not applicable to SCTP and FCoE protocols.\n *\n * CHECKSUM_PARTIAL:\n *\n *   A checksum is set up to be offloaded to a device as described in the\n *   output description for CHECKSUM_PARTIAL. This may occur on a packet\n *   received directly from another Linux OS, e.g., a virtualized Linux kernel\n *   on the same host, or it may be set in the input path in GRO or remote\n *   checksum offload. For the purposes of checksum verification, the checksum\n *   referred to by skb->csum_start + skb->csum_offset and any preceding\n *   checksums in the packet are considered verified. Any checksums in the\n *   packet that are after the checksum being offloaded are not considered to\n *   be verified.\n *\n * C. Checksumming on transmit for non-GSO. The stack requests checksum offload\n *    in the skb->ip_summed for a packet. Values are:\n *\n * CHECKSUM_PARTIAL:\n *\n *   The driver is required to checksum the packet as seen by hard_start_xmit()\n *   from skb->csum_start up to the end, and to record/write the checksum at\n *   offset skb->csum_start + skb->csum_offset. A driver may verify that the\n *   csum_start and csum_offset values are valid values given the length and\n *   offset of the packet, but it should not attempt to validate that the\n *   checksum refers to a legitimate transport layer checksum -- it is the\n *   purview of the stack to validate that csum_start and csum_offset are set\n *   correctly.\n *\n *   When the stack requests checksum offload for a packet, the driver MUST\n *   ensure that the checksum is set correctly. A driver can either offload the\n *   checksum calculation to the device, or call skb_checksum_help (in the case\n *   that the device does not support offload for a particular checksum).\n *\n *   NETIF_F_IP_CSUM and NETIF_F_IPV6_CSUM are being deprecated in favor of\n *   NETIF_F_HW_CSUM. New devices should use NETIF_F_HW_CSUM to indicate\n *   checksum offload capability.\n *   skb_csum_hwoffload_help() can be called to resolve CHECKSUM_PARTIAL based\n *   on network device checksumming capabilities: if a packet does not match\n *   them, skb_checksum_help or skb_crc32c_help (depending on the value of\n *   csum_not_inet, see item D.) is called to resolve the checksum.\n *\n * CHECKSUM_NONE:\n *\n *   The skb was already checksummed by the protocol, or a checksum is not\n *   required.\n *\n * CHECKSUM_UNNECESSARY:\n *\n *   This has the same meaning as CHECKSUM_NONE for checksum offload on\n *   output.\n *\n * CHECKSUM_COMPLETE:\n *   Not used in checksum output. If a driver observes a packet with this value\n *   set in skbuff, it should treat the packet as if CHECKSUM_NONE were set.\n *\n * D. Non-IP checksum (CRC) offloads\n *\n *   NETIF_F_SCTP_CRC - This feature indicates that a device is capable of\n *     offloading the SCTP CRC in a packet. To perform this offload the stack\n *     will set csum_start and csum_offset accordingly, set ip_summed to\n *     CHECKSUM_PARTIAL and set csum_not_inet to 1, to provide an indication in\n *     the skbuff that the CHECKSUM_PARTIAL refers to CRC32c.\n *     A driver that supports both IP checksum offload and SCTP CRC32c offload\n *     must verify which offload is configured for a packet by testing the\n *     value of skb->csum_not_inet; skb_crc32c_csum_help is provided to resolve\n *     CHECKSUM_PARTIAL on skbs where csum_not_inet is set to 1.\n *\n *   NETIF_F_FCOE_CRC - This feature indicates that a device is capable of\n *     offloading the FCOE CRC in a packet. To perform this offload the stack\n *     will set ip_summed to CHECKSUM_PARTIAL and set csum_start and csum_offset\n *     accordingly. Note that there is no indication in the skbuff that the\n *     CHECKSUM_PARTIAL refers to an FCOE checksum, so a driver that supports\n *     both IP checksum offload and FCOE CRC offload must verify which offload\n *     is configured for a packet, presumably by inspecting packet headers.\n *\n * E. Checksumming on output with GSO.\n *\n * In the case of a GSO packet (skb_is_gso(skb) is true), checksum offload\n * is implied by the SKB_GSO_* flags in gso_type. Most obviously, if the\n * gso_type is SKB_GSO_TCPV4 or SKB_GSO_TCPV6, TCP checksum offload as\n * part of the GSO operation is implied. If a checksum is being offloaded\n * with GSO then ip_summed is CHECKSUM_PARTIAL, and both csum_start and\n * csum_offset are set to refer to the outermost checksum being offloaded\n * (two offloaded checksums are possible with UDP encapsulation).\n */\n\n/* Don't change this without changing skb_csum_unnecessary! */\n#define CHECKSUM_NONE\t\t0\n#define CHECKSUM_UNNECESSARY\t1\n#define CHECKSUM_COMPLETE\t2\n#define CHECKSUM_PARTIAL\t3\n\n/* Maximum value in skb->csum_level */\n#define SKB_MAX_CSUM_LEVEL\t3\n\n#define SKB_DATA_ALIGN(X)\tALIGN(X, SMP_CACHE_BYTES)\n#define SKB_WITH_OVERHEAD(X)\t\\\n\t((X) - SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))\n#define SKB_MAX_ORDER(X, ORDER) \\\n\tSKB_WITH_OVERHEAD((PAGE_SIZE << (ORDER)) - (X))\n#define SKB_MAX_HEAD(X)\t\t(SKB_MAX_ORDER((X), 0))\n#define SKB_MAX_ALLOC\t\t(SKB_MAX_ORDER(0, 2))\n\n/* return minimum truesize of one skb containing X bytes of data */\n#define SKB_TRUESIZE(X) ((X) +\t\t\t\t\t\t\\\n\t\t\t SKB_DATA_ALIGN(sizeof(struct sk_buff)) +\t\\\n\t\t\t SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))\n\nstruct ahash_request;\nstruct net_device;\nstruct scatterlist;\nstruct pipe_inode_info;\nstruct iov_iter;\nstruct napi_struct;\nstruct bpf_prog;\nunion bpf_attr;\nstruct skb_ext;\n\n#if IS_ENABLED(CONFIG_BRIDGE_NETFILTER)\nstruct nf_bridge_info {\n\tenum {\n\t\tBRNF_PROTO_UNCHANGED,\n\t\tBRNF_PROTO_8021Q,\n\t\tBRNF_PROTO_PPPOE\n\t} orig_proto:8;\n\tu8\t\t\tpkt_otherhost:1;\n\tu8\t\t\tin_prerouting:1;\n\tu8\t\t\tbridged_dnat:1;\n\t__u16\t\t\tfrag_max_size;\n\tstruct net_device\t*physindev;\n\n\t/* always valid & non-NULL from FORWARD on, for physdev match */\n\tstruct net_device\t*physoutdev;\n\tunion {\n\t\t/* prerouting: detect dnat in orig/reply direction */\n\t\t__be32          ipv4_daddr;\n\t\tstruct in6_addr ipv6_daddr;\n\n\t\t/* after prerouting + nat detected: store original source\n\t\t * mac since neigh resolution overwrites it, only used while\n\t\t * skb is out in neigh layer.\n\t\t */\n\t\tchar neigh_header[8];\n\t};\n};\n#endif\n\n#if IS_ENABLED(CONFIG_NET_TC_SKB_EXT)\n/* Chain in tc_skb_ext will be used to share the tc chain with\n * ovs recirc_id. It will be set to the current chain by tc\n * and read by ovs to recirc_id.\n */\nstruct tc_skb_ext {\n\t__u32 chain;\n\t__u16 mru;\n};\n#endif\n\nstruct sk_buff_head {\n\t/* These two members must be first. */\n\tstruct sk_buff\t*next;\n\tstruct sk_buff\t*prev;\n\n\t__u32\t\tqlen;\n\tspinlock_t\tlock;\n};\n\nstruct sk_buff;\n\n/* To allow 64K frame to be packed as single skb without frag_list we\n * require 64K/PAGE_SIZE pages plus 1 additional page to allow for\n * buffers which do not start on a page boundary.\n *\n * Since GRO uses frags we allocate at least 16 regardless of page\n * size.\n */\n#if (65536/PAGE_SIZE + 1) < 16\n#define MAX_SKB_FRAGS 16UL\n#else\n#define MAX_SKB_FRAGS (65536/PAGE_SIZE + 1)\n#endif\nextern int sysctl_max_skb_frags;\n\n/* Set skb_shinfo(skb)->gso_size to this in case you want skb_segment to\n * segment using its current segmentation instead.\n */\n#define GSO_BY_FRAGS\t0xFFFF\n\ntypedef struct bio_vec skb_frag_t;\n\n/**\n * skb_frag_size() - Returns the size of a skb fragment\n * @frag: skb fragment\n */\nstatic inline unsigned int skb_frag_size(const skb_frag_t *frag)\n{\n\treturn frag->bv_len;\n}\n\n/**\n * skb_frag_size_set() - Sets the size of a skb fragment\n * @frag: skb fragment\n * @size: size of fragment\n */\nstatic inline void skb_frag_size_set(skb_frag_t *frag, unsigned int size)\n{\n\tfrag->bv_len = size;\n}\n\n/**\n * skb_frag_size_add() - Increments the size of a skb fragment by @delta\n * @frag: skb fragment\n * @delta: value to add\n */\nstatic inline void skb_frag_size_add(skb_frag_t *frag, int delta)\n{\n\tfrag->bv_len += delta;\n}\n\n/**\n * skb_frag_size_sub() - Decrements the size of a skb fragment by @delta\n * @frag: skb fragment\n * @delta: value to subtract\n */\nstatic inline void skb_frag_size_sub(skb_frag_t *frag, int delta)\n{\n\tfrag->bv_len -= delta;\n}\n\n/**\n * skb_frag_must_loop - Test if %p is a high memory page\n * @p: fragment's page\n */\nstatic inline bool skb_frag_must_loop(struct page *p)\n{\n#if defined(CONFIG_HIGHMEM)\n\tif (IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL_FORCE_MAP) || PageHighMem(p))\n\t\treturn true;\n#endif\n\treturn false;\n}\n\n/**\n *\tskb_frag_foreach_page - loop over pages in a fragment\n *\n *\t@f:\t\tskb frag to operate on\n *\t@f_off:\t\toffset from start of f->bv_page\n *\t@f_len:\t\tlength from f_off to loop over\n *\t@p:\t\t(temp var) current page\n *\t@p_off:\t\t(temp var) offset from start of current page,\n *\t                           non-zero only on first page.\n *\t@p_len:\t\t(temp var) length in current page,\n *\t\t\t\t   < PAGE_SIZE only on first and last page.\n *\t@copied:\t(temp var) length so far, excluding current p_len.\n *\n *\tA fragment can hold a compound page, in which case per-page\n *\toperations, notably kmap_atomic, must be called for each\n *\tregular page.\n */\n#define skb_frag_foreach_page(f, f_off, f_len, p, p_off, p_len, copied)\t\\\n\tfor (p = skb_frag_page(f) + ((f_off) >> PAGE_SHIFT),\t\t\\\n\t     p_off = (f_off) & (PAGE_SIZE - 1),\t\t\t\t\\\n\t     p_len = skb_frag_must_loop(p) ?\t\t\t\t\\\n\t     min_t(u32, f_len, PAGE_SIZE - p_off) : f_len,\t\t\\\n\t     copied = 0;\t\t\t\t\t\t\\\n\t     copied < f_len;\t\t\t\t\t\t\\\n\t     copied += p_len, p++, p_off = 0,\t\t\t\t\\\n\t     p_len = min_t(u32, f_len - copied, PAGE_SIZE))\t\t\\\n\n#define HAVE_HW_TIME_STAMP\n\n/**\n * struct skb_shared_hwtstamps - hardware time stamps\n * @hwtstamp:\thardware time stamp transformed into duration\n *\t\tsince arbitrary point in time\n *\n * Software time stamps generated by ktime_get_real() are stored in\n * skb->tstamp.\n *\n * hwtstamps can only be compared against other hwtstamps from\n * the same device.\n *\n * This structure is attached to packets as part of the\n * &skb_shared_info. Use skb_hwtstamps() to get a pointer.\n */\nstruct skb_shared_hwtstamps {\n\tktime_t\thwtstamp;\n};\n\n/* Definitions for tx_flags in struct skb_shared_info */\nenum {\n\t/* generate hardware time stamp */\n\tSKBTX_HW_TSTAMP = 1 << 0,\n\n\t/* generate software time stamp when queueing packet to NIC */\n\tSKBTX_SW_TSTAMP = 1 << 1,\n\n\t/* device driver is going to provide hardware time stamp */\n\tSKBTX_IN_PROGRESS = 1 << 2,\n\n\t/* generate wifi status information (where possible) */\n\tSKBTX_WIFI_STATUS = 1 << 4,\n\n\t/* generate software time stamp when entering packet scheduling */\n\tSKBTX_SCHED_TSTAMP = 1 << 6,\n};\n\n#define SKBTX_ANY_SW_TSTAMP\t(SKBTX_SW_TSTAMP    | \\\n\t\t\t\t SKBTX_SCHED_TSTAMP)\n#define SKBTX_ANY_TSTAMP\t(SKBTX_HW_TSTAMP | SKBTX_ANY_SW_TSTAMP)\n\n/* Definitions for flags in struct skb_shared_info */\nenum {\n\t/* use zcopy routines */\n\tSKBFL_ZEROCOPY_ENABLE = BIT(0),\n\n\t/* This indicates at least one fragment might be overwritten\n\t * (as in vmsplice(), sendfile() ...)\n\t * If we need to compute a TX checksum, we'll need to copy\n\t * all frags to avoid possible bad checksum\n\t */\n\tSKBFL_SHARED_FRAG = BIT(1),\n};\n\n#define SKBFL_ZEROCOPY_FRAG\t(SKBFL_ZEROCOPY_ENABLE | SKBFL_SHARED_FRAG)\n\n/*\n * The callback notifies userspace to release buffers when skb DMA is done in\n * lower device, the skb last reference should be 0 when calling this.\n * The zerocopy_success argument is true if zero copy transmit occurred,\n * false on data copy or out of memory error caused by data copy attempt.\n * The ctx field is used to track device context.\n * The desc field is used to track userspace buffer index.\n */\nstruct ubuf_info {\n\tvoid (*callback)(struct sk_buff *, struct ubuf_info *,\n\t\t\t bool zerocopy_success);\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long desc;\n\t\t\tvoid *ctx;\n\t\t};\n\t\tstruct {\n\t\t\tu32 id;\n\t\t\tu16 len;\n\t\t\tu16 zerocopy:1;\n\t\t\tu32 bytelen;\n\t\t};\n\t};\n\trefcount_t refcnt;\n\tu8 flags;\n\n\tstruct mmpin {\n\t\tstruct user_struct *user;\n\t\tunsigned int num_pg;\n\t} mmp;\n};\n\n#define skb_uarg(SKB)\t((struct ubuf_info *)(skb_shinfo(SKB)->destructor_arg))\n\nint mm_account_pinned_pages(struct mmpin *mmp, size_t size);\nvoid mm_unaccount_pinned_pages(struct mmpin *mmp);\n\nstruct ubuf_info *msg_zerocopy_alloc(struct sock *sk, size_t size);\nstruct ubuf_info *msg_zerocopy_realloc(struct sock *sk, size_t size,\n\t\t\t\t       struct ubuf_info *uarg);\n\nvoid msg_zerocopy_put_abort(struct ubuf_info *uarg, bool have_uref);\n\nvoid msg_zerocopy_callback(struct sk_buff *skb, struct ubuf_info *uarg,\n\t\t\t   bool success);\n\nint skb_zerocopy_iter_dgram(struct sk_buff *skb, struct msghdr *msg, int len);\nint skb_zerocopy_iter_stream(struct sock *sk, struct sk_buff *skb,\n\t\t\t     struct msghdr *msg, int len,\n\t\t\t     struct ubuf_info *uarg);\n\n/* This data is invariant across clones and lives at\n * the end of the header data, ie. at skb->end.\n */\nstruct skb_shared_info {\n\t__u8\t\tflags;\n\t__u8\t\tmeta_len;\n\t__u8\t\tnr_frags;\n\t__u8\t\ttx_flags;\n\tunsigned short\tgso_size;\n\t/* Warning: this field is not always filled in (UFO)! */\n\tunsigned short\tgso_segs;\n\tstruct sk_buff\t*frag_list;\n\tstruct skb_shared_hwtstamps hwtstamps;\n\tunsigned int\tgso_type;\n\tu32\t\ttskey;\n\n\t/*\n\t * Warning : all fields before dataref are cleared in __alloc_skb()\n\t */\n\tatomic_t\tdataref;\n\n\t/* Intermediate layers must ensure that destructor_arg\n\t * remains valid until skb destructor */\n\tvoid *\t\tdestructor_arg;\n\n\t/* must be last field, see pskb_expand_head() */\n\tskb_frag_t\tfrags[MAX_SKB_FRAGS];\n};\n\n/* We divide dataref into two halves.  The higher 16 bits hold references\n * to the payload part of skb->data.  The lower 16 bits hold references to\n * the entire skb->data.  A clone of a headerless skb holds the length of\n * the header in skb->hdr_len.\n *\n * All users must obey the rule that the skb->data reference count must be\n * greater than or equal to the payload reference count.\n *\n * Holding a reference to the payload part means that the user does not\n * care about modifications to the header part of skb->data.\n */\n#define SKB_DATAREF_SHIFT 16\n#define SKB_DATAREF_MASK ((1 << SKB_DATAREF_SHIFT) - 1)\n\n\nenum {\n\tSKB_FCLONE_UNAVAILABLE,\t/* skb has no fclone (from head_cache) */\n\tSKB_FCLONE_ORIG,\t/* orig skb (from fclone_cache) */\n\tSKB_FCLONE_CLONE,\t/* companion fclone skb (from fclone_cache) */\n};\n\nenum {\n\tSKB_GSO_TCPV4 = 1 << 0,\n\n\t/* This indicates the skb is from an untrusted source. */\n\tSKB_GSO_DODGY = 1 << 1,\n\n\t/* This indicates the tcp segment has CWR set. */\n\tSKB_GSO_TCP_ECN = 1 << 2,\n\n\tSKB_GSO_TCP_FIXEDID = 1 << 3,\n\n\tSKB_GSO_TCPV6 = 1 << 4,\n\n\tSKB_GSO_FCOE = 1 << 5,\n\n\tSKB_GSO_GRE = 1 << 6,\n\n\tSKB_GSO_GRE_CSUM = 1 << 7,\n\n\tSKB_GSO_IPXIP4 = 1 << 8,\n\n\tSKB_GSO_IPXIP6 = 1 << 9,\n\n\tSKB_GSO_UDP_TUNNEL = 1 << 10,\n\n\tSKB_GSO_UDP_TUNNEL_CSUM = 1 << 11,\n\n\tSKB_GSO_PARTIAL = 1 << 12,\n\n\tSKB_GSO_TUNNEL_REMCSUM = 1 << 13,\n\n\tSKB_GSO_SCTP = 1 << 14,\n\n\tSKB_GSO_ESP = 1 << 15,\n\n\tSKB_GSO_UDP = 1 << 16,\n\n\tSKB_GSO_UDP_L4 = 1 << 17,\n\n\tSKB_GSO_FRAGLIST = 1 << 18,\n};\n\n#if BITS_PER_LONG > 32\n#define NET_SKBUFF_DATA_USES_OFFSET 1\n#endif\n\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\ntypedef unsigned int sk_buff_data_t;\n#else\ntypedef unsigned char *sk_buff_data_t;\n#endif\n\n/**\n *\tstruct sk_buff - socket buffer\n *\t@next: Next buffer in list\n *\t@prev: Previous buffer in list\n *\t@tstamp: Time we arrived/left\n *\t@skb_mstamp_ns: (aka @tstamp) earliest departure time; start point\n *\t\tfor retransmit timer\n *\t@rbnode: RB tree node, alternative to next/prev for netem/tcp\n *\t@list: queue head\n *\t@sk: Socket we are owned by\n *\t@ip_defrag_offset: (aka @sk) alternate use of @sk, used in\n *\t\tfragmentation management\n *\t@dev: Device we arrived on/are leaving by\n *\t@dev_scratch: (aka @dev) alternate use of @dev when @dev would be %NULL\n *\t@cb: Control buffer. Free for use by every layer. Put private vars here\n *\t@_skb_refdst: destination entry (with norefcount bit)\n *\t@sp: the security path, used for xfrm\n *\t@len: Length of actual data\n *\t@data_len: Data length\n *\t@mac_len: Length of link layer header\n *\t@hdr_len: writable header length of cloned skb\n *\t@csum: Checksum (must include start/offset pair)\n *\t@csum_start: Offset from skb->head where checksumming should start\n *\t@csum_offset: Offset from csum_start where checksum should be stored\n *\t@priority: Packet queueing priority\n *\t@ignore_df: allow local fragmentation\n *\t@cloned: Head may be cloned (check refcnt to be sure)\n *\t@ip_summed: Driver fed us an IP checksum\n *\t@nohdr: Payload reference only, must not modify header\n *\t@pkt_type: Packet class\n *\t@fclone: skbuff clone status\n *\t@ipvs_property: skbuff is owned by ipvs\n *\t@inner_protocol_type: whether the inner protocol is\n *\t\tENCAP_TYPE_ETHER or ENCAP_TYPE_IPPROTO\n *\t@remcsum_offload: remote checksum offload is enabled\n *\t@offload_fwd_mark: Packet was L2-forwarded in hardware\n *\t@offload_l3_fwd_mark: Packet was L3-forwarded in hardware\n *\t@tc_skip_classify: do not classify packet. set by IFB device\n *\t@tc_at_ingress: used within tc_classify to distinguish in/egress\n *\t@redirected: packet was redirected by packet classifier\n *\t@from_ingress: packet was redirected from the ingress path\n *\t@peeked: this packet has been seen already, so stats have been\n *\t\tdone for it, don't do them again\n *\t@nf_trace: netfilter packet trace flag\n *\t@protocol: Packet protocol from driver\n *\t@destructor: Destruct function\n *\t@tcp_tsorted_anchor: list structure for TCP (tp->tsorted_sent_queue)\n *\t@_nfct: Associated connection, if any (with nfctinfo bits)\n *\t@nf_bridge: Saved data about a bridged frame - see br_netfilter.c\n *\t@skb_iif: ifindex of device we arrived on\n *\t@tc_index: Traffic control index\n *\t@hash: the packet hash\n *\t@queue_mapping: Queue mapping for multiqueue devices\n *\t@head_frag: skb was allocated from page fragments,\n *\t\tnot allocated by kmalloc() or vmalloc().\n *\t@pfmemalloc: skbuff was allocated from PFMEMALLOC reserves\n *\t@active_extensions: active extensions (skb_ext_id types)\n *\t@ndisc_nodetype: router type (from link layer)\n *\t@ooo_okay: allow the mapping of a socket to a queue to be changed\n *\t@l4_hash: indicate hash is a canonical 4-tuple hash over transport\n *\t\tports.\n *\t@sw_hash: indicates hash was computed in software stack\n *\t@wifi_acked_valid: wifi_acked was set\n *\t@wifi_acked: whether frame was acked on wifi or not\n *\t@no_fcs:  Request NIC to treat last 4 bytes as Ethernet FCS\n *\t@encapsulation: indicates the inner headers in the skbuff are valid\n *\t@encap_hdr_csum: software checksum is needed\n *\t@csum_valid: checksum is already valid\n *\t@csum_not_inet: use CRC32c to resolve CHECKSUM_PARTIAL\n *\t@csum_complete_sw: checksum was completed by software\n *\t@csum_level: indicates the number of consecutive checksums found in\n *\t\tthe packet minus one that have been verified as\n *\t\tCHECKSUM_UNNECESSARY (max 3)\n *\t@dst_pending_confirm: need to confirm neighbour\n *\t@decrypted: Decrypted SKB\n *\t@napi_id: id of the NAPI struct this skb came from\n *\t@sender_cpu: (aka @napi_id) source CPU in XPS\n *\t@secmark: security marking\n *\t@mark: Generic packet mark\n *\t@reserved_tailroom: (aka @mark) number of bytes of free space available\n *\t\tat the tail of an sk_buff\n *\t@vlan_present: VLAN tag is present\n *\t@vlan_proto: vlan encapsulation protocol\n *\t@vlan_tci: vlan tag control information\n *\t@inner_protocol: Protocol (encapsulation)\n *\t@inner_ipproto: (aka @inner_protocol) stores ipproto when\n *\t\tskb->inner_protocol_type == ENCAP_TYPE_IPPROTO;\n *\t@inner_transport_header: Inner transport layer header (encapsulation)\n *\t@inner_network_header: Network layer header (encapsulation)\n *\t@inner_mac_header: Link layer header (encapsulation)\n *\t@transport_header: Transport layer header\n *\t@network_header: Network layer header\n *\t@mac_header: Link layer header\n *\t@kcov_handle: KCOV remote handle for remote coverage collection\n *\t@tail: Tail pointer\n *\t@end: End pointer\n *\t@head: Head of buffer\n *\t@data: Data head pointer\n *\t@truesize: Buffer size\n *\t@users: User count - see {datagram,tcp}.c\n *\t@extensions: allocated extensions, valid if active_extensions is nonzero\n */\n\nstruct sk_buff {\n\tunion {\n\t\tstruct {\n\t\t\t/* These two members must be first. */\n\t\t\tstruct sk_buff\t\t*next;\n\t\t\tstruct sk_buff\t\t*prev;\n\n\t\t\tunion {\n\t\t\t\tstruct net_device\t*dev;\n\t\t\t\t/* Some protocols might use this space to store information,\n\t\t\t\t * while device pointer would be NULL.\n\t\t\t\t * UDP receive path is one user.\n\t\t\t\t */\n\t\t\t\tunsigned long\t\tdev_scratch;\n\t\t\t};\n\t\t};\n\t\tstruct rb_node\t\trbnode; /* used in netem, ip4 defrag, and tcp stack */\n\t\tstruct list_head\tlist;\n\t};\n\n\tunion {\n\t\tstruct sock\t\t*sk;\n\t\tint\t\t\tip_defrag_offset;\n\t};\n\n\tunion {\n\t\tktime_t\t\ttstamp;\n\t\tu64\t\tskb_mstamp_ns; /* earliest departure time */\n\t};\n\t/*\n\t * This is the control buffer. It is free to use for every\n\t * layer. Please put your private variables there. If you\n\t * want to keep them across layers you have to do a skb_clone()\n\t * first. This is owned by whoever has the skb queued ATM.\n\t */\n\tchar\t\t\tcb[48] __aligned(8);\n\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long\t_skb_refdst;\n\t\t\tvoid\t\t(*destructor)(struct sk_buff *skb);\n\t\t};\n\t\tstruct list_head\ttcp_tsorted_anchor;\n\t};\n\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tunsigned long\t\t _nfct;\n#endif\n\tunsigned int\t\tlen,\n\t\t\t\tdata_len;\n\t__u16\t\t\tmac_len,\n\t\t\t\thdr_len;\n\n\t/* Following fields are _not_ copied in __copy_skb_header()\n\t * Note that queue_mapping is here mostly to fill a hole.\n\t */\n\t__u16\t\t\tqueue_mapping;\n\n/* if you move cloned around you also must adapt those constants */\n#ifdef __BIG_ENDIAN_BITFIELD\n#define CLONED_MASK\t(1 << 7)\n#else\n#define CLONED_MASK\t1\n#endif\n#define CLONED_OFFSET()\t\toffsetof(struct sk_buff, __cloned_offset)\n\n\t/* private: */\n\t__u8\t\t\t__cloned_offset[0];\n\t/* public: */\n\t__u8\t\t\tcloned:1,\n\t\t\t\tnohdr:1,\n\t\t\t\tfclone:2,\n\t\t\t\tpeeked:1,\n\t\t\t\thead_frag:1,\n\t\t\t\tpfmemalloc:1;\n#ifdef CONFIG_SKB_EXTENSIONS\n\t__u8\t\t\tactive_extensions;\n#endif\n\t/* fields enclosed in headers_start/headers_end are copied\n\t * using a single memcpy() in __copy_skb_header()\n\t */\n\t/* private: */\n\t__u32\t\t\theaders_start[0];\n\t/* public: */\n\n/* if you move pkt_type around you also must adapt those constants */\n#ifdef __BIG_ENDIAN_BITFIELD\n#define PKT_TYPE_MAX\t(7 << 5)\n#else\n#define PKT_TYPE_MAX\t7\n#endif\n#define PKT_TYPE_OFFSET()\toffsetof(struct sk_buff, __pkt_type_offset)\n\n\t/* private: */\n\t__u8\t\t\t__pkt_type_offset[0];\n\t/* public: */\n\t__u8\t\t\tpkt_type:3;\n\t__u8\t\t\tignore_df:1;\n\t__u8\t\t\tnf_trace:1;\n\t__u8\t\t\tip_summed:2;\n\t__u8\t\t\tooo_okay:1;\n\n\t__u8\t\t\tl4_hash:1;\n\t__u8\t\t\tsw_hash:1;\n\t__u8\t\t\twifi_acked_valid:1;\n\t__u8\t\t\twifi_acked:1;\n\t__u8\t\t\tno_fcs:1;\n\t/* Indicates the inner headers are valid in the skbuff. */\n\t__u8\t\t\tencapsulation:1;\n\t__u8\t\t\tencap_hdr_csum:1;\n\t__u8\t\t\tcsum_valid:1;\n\n#ifdef __BIG_ENDIAN_BITFIELD\n#define PKT_VLAN_PRESENT_BIT\t7\n#else\n#define PKT_VLAN_PRESENT_BIT\t0\n#endif\n#define PKT_VLAN_PRESENT_OFFSET()\toffsetof(struct sk_buff, __pkt_vlan_present_offset)\n\t/* private: */\n\t__u8\t\t\t__pkt_vlan_present_offset[0];\n\t/* public: */\n\t__u8\t\t\tvlan_present:1;\n\t__u8\t\t\tcsum_complete_sw:1;\n\t__u8\t\t\tcsum_level:2;\n\t__u8\t\t\tcsum_not_inet:1;\n\t__u8\t\t\tdst_pending_confirm:1;\n#ifdef CONFIG_IPV6_NDISC_NODETYPE\n\t__u8\t\t\tndisc_nodetype:2;\n#endif\n\n\t__u8\t\t\tipvs_property:1;\n\t__u8\t\t\tinner_protocol_type:1;\n\t__u8\t\t\tremcsum_offload:1;\n#ifdef CONFIG_NET_SWITCHDEV\n\t__u8\t\t\toffload_fwd_mark:1;\n\t__u8\t\t\toffload_l3_fwd_mark:1;\n#endif\n#ifdef CONFIG_NET_CLS_ACT\n\t__u8\t\t\ttc_skip_classify:1;\n\t__u8\t\t\ttc_at_ingress:1;\n#endif\n#ifdef CONFIG_NET_REDIRECT\n\t__u8\t\t\tredirected:1;\n\t__u8\t\t\tfrom_ingress:1;\n#endif\n#ifdef CONFIG_TLS_DEVICE\n\t__u8\t\t\tdecrypted:1;\n#endif\n\n#ifdef CONFIG_NET_SCHED\n\t__u16\t\t\ttc_index;\t/* traffic control index */\n#endif\n\n\tunion {\n\t\t__wsum\t\tcsum;\n\t\tstruct {\n\t\t\t__u16\tcsum_start;\n\t\t\t__u16\tcsum_offset;\n\t\t};\n\t};\n\t__u32\t\t\tpriority;\n\tint\t\t\tskb_iif;\n\t__u32\t\t\thash;\n\t__be16\t\t\tvlan_proto;\n\t__u16\t\t\tvlan_tci;\n#if defined(CONFIG_NET_RX_BUSY_POLL) || defined(CONFIG_XPS)\n\tunion {\n\t\tunsigned int\tnapi_id;\n\t\tunsigned int\tsender_cpu;\n\t};\n#endif\n#ifdef CONFIG_NETWORK_SECMARK\n\t__u32\t\tsecmark;\n#endif\n\n\tunion {\n\t\t__u32\t\tmark;\n\t\t__u32\t\treserved_tailroom;\n\t};\n\n\tunion {\n\t\t__be16\t\tinner_protocol;\n\t\t__u8\t\tinner_ipproto;\n\t};\n\n\t__u16\t\t\tinner_transport_header;\n\t__u16\t\t\tinner_network_header;\n\t__u16\t\t\tinner_mac_header;\n\n\t__be16\t\t\tprotocol;\n\t__u16\t\t\ttransport_header;\n\t__u16\t\t\tnetwork_header;\n\t__u16\t\t\tmac_header;\n\n#ifdef CONFIG_KCOV\n\tu64\t\t\tkcov_handle;\n#endif\n\n\t/* private: */\n\t__u32\t\t\theaders_end[0];\n\t/* public: */\n\n\t/* These elements must be at the end, see alloc_skb() for details.  */\n\tsk_buff_data_t\t\ttail;\n\tsk_buff_data_t\t\tend;\n\tunsigned char\t\t*head,\n\t\t\t\t*data;\n\tunsigned int\t\ttruesize;\n\trefcount_t\t\tusers;\n\n#ifdef CONFIG_SKB_EXTENSIONS\n\t/* only useable after checking ->active_extensions != 0 */\n\tstruct skb_ext\t\t*extensions;\n#endif\n};\n\n#ifdef __KERNEL__\n/*\n *\tHandling routines are only of interest to the kernel\n */\n\n#define SKB_ALLOC_FCLONE\t0x01\n#define SKB_ALLOC_RX\t\t0x02\n#define SKB_ALLOC_NAPI\t\t0x04\n\n/**\n * skb_pfmemalloc - Test if the skb was allocated from PFMEMALLOC reserves\n * @skb: buffer\n */\nstatic inline bool skb_pfmemalloc(const struct sk_buff *skb)\n{\n\treturn unlikely(skb->pfmemalloc);\n}\n\n/*\n * skb might have a dst pointer attached, refcounted or not.\n * _skb_refdst low order bit is set if refcount was _not_ taken\n */\n#define SKB_DST_NOREF\t1UL\n#define SKB_DST_PTRMASK\t~(SKB_DST_NOREF)\n\n/**\n * skb_dst - returns skb dst_entry\n * @skb: buffer\n *\n * Returns skb dst_entry, regardless of reference taken or not.\n */\nstatic inline struct dst_entry *skb_dst(const struct sk_buff *skb)\n{\n\t/* If refdst was not refcounted, check we still are in a\n\t * rcu_read_lock section\n\t */\n\tWARN_ON((skb->_skb_refdst & SKB_DST_NOREF) &&\n\t\t!rcu_read_lock_held() &&\n\t\t!rcu_read_lock_bh_held());\n\treturn (struct dst_entry *)(skb->_skb_refdst & SKB_DST_PTRMASK);\n}\n\n/**\n * skb_dst_set - sets skb dst\n * @skb: buffer\n * @dst: dst entry\n *\n * Sets skb dst, assuming a reference was taken on dst and should\n * be released by skb_dst_drop()\n */\nstatic inline void skb_dst_set(struct sk_buff *skb, struct dst_entry *dst)\n{\n\tskb->_skb_refdst = (unsigned long)dst;\n}\n\n/**\n * skb_dst_set_noref - sets skb dst, hopefully, without taking reference\n * @skb: buffer\n * @dst: dst entry\n *\n * Sets skb dst, assuming a reference was not taken on dst.\n * If dst entry is cached, we do not take reference and dst_release\n * will be avoided by refdst_drop. If dst entry is not cached, we take\n * reference, so that last dst_release can destroy the dst immediately.\n */\nstatic inline void skb_dst_set_noref(struct sk_buff *skb, struct dst_entry *dst)\n{\n\tWARN_ON(!rcu_read_lock_held() && !rcu_read_lock_bh_held());\n\tskb->_skb_refdst = (unsigned long)dst | SKB_DST_NOREF;\n}\n\n/**\n * skb_dst_is_noref - Test if skb dst isn't refcounted\n * @skb: buffer\n */\nstatic inline bool skb_dst_is_noref(const struct sk_buff *skb)\n{\n\treturn (skb->_skb_refdst & SKB_DST_NOREF) && skb_dst(skb);\n}\n\n/**\n * skb_rtable - Returns the skb &rtable\n * @skb: buffer\n */\nstatic inline struct rtable *skb_rtable(const struct sk_buff *skb)\n{\n\treturn (struct rtable *)skb_dst(skb);\n}\n\n/* For mangling skb->pkt_type from user space side from applications\n * such as nft, tc, etc, we only allow a conservative subset of\n * possible pkt_types to be set.\n*/\nstatic inline bool skb_pkt_type_ok(u32 ptype)\n{\n\treturn ptype <= PACKET_OTHERHOST;\n}\n\n/**\n * skb_napi_id - Returns the skb's NAPI id\n * @skb: buffer\n */\nstatic inline unsigned int skb_napi_id(const struct sk_buff *skb)\n{\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\treturn skb->napi_id;\n#else\n\treturn 0;\n#endif\n}\n\n/**\n * skb_unref - decrement the skb's reference count\n * @skb: buffer\n *\n * Returns true if we can free the skb.\n */\nstatic inline bool skb_unref(struct sk_buff *skb)\n{\n\tif (unlikely(!skb))\n\t\treturn false;\n\tif (likely(refcount_read(&skb->users) == 1))\n\t\tsmp_rmb();\n\telse if (likely(!refcount_dec_and_test(&skb->users)))\n\t\treturn false;\n\n\treturn true;\n}\n\nvoid skb_release_head_state(struct sk_buff *skb);\nvoid kfree_skb(struct sk_buff *skb);\nvoid kfree_skb_list(struct sk_buff *segs);\nvoid skb_dump(const char *level, const struct sk_buff *skb, bool full_pkt);\nvoid skb_tx_error(struct sk_buff *skb);\n\n#ifdef CONFIG_TRACEPOINTS\nvoid consume_skb(struct sk_buff *skb);\n#else\nstatic inline void consume_skb(struct sk_buff *skb)\n{\n\treturn kfree_skb(skb);\n}\n#endif\n\nvoid __consume_stateless_skb(struct sk_buff *skb);\nvoid  __kfree_skb(struct sk_buff *skb);\nextern struct kmem_cache *skbuff_head_cache;\n\nvoid kfree_skb_partial(struct sk_buff *skb, bool head_stolen);\nbool skb_try_coalesce(struct sk_buff *to, struct sk_buff *from,\n\t\t      bool *fragstolen, int *delta_truesize);\n\nstruct sk_buff *__alloc_skb(unsigned int size, gfp_t priority, int flags,\n\t\t\t    int node);\nstruct sk_buff *__build_skb(void *data, unsigned int frag_size);\nstruct sk_buff *build_skb(void *data, unsigned int frag_size);\nstruct sk_buff *build_skb_around(struct sk_buff *skb,\n\t\t\t\t void *data, unsigned int frag_size);\n\nstruct sk_buff *napi_build_skb(void *data, unsigned int frag_size);\n\n/**\n * alloc_skb - allocate a network buffer\n * @size: size to allocate\n * @priority: allocation mask\n *\n * This function is a convenient wrapper around __alloc_skb().\n */\nstatic inline struct sk_buff *alloc_skb(unsigned int size,\n\t\t\t\t\tgfp_t priority)\n{\n\treturn __alloc_skb(size, priority, 0, NUMA_NO_NODE);\n}\n\nstruct sk_buff *alloc_skb_with_frags(unsigned long header_len,\n\t\t\t\t     unsigned long data_len,\n\t\t\t\t     int max_page_order,\n\t\t\t\t     int *errcode,\n\t\t\t\t     gfp_t gfp_mask);\nstruct sk_buff *alloc_skb_for_msg(struct sk_buff *first);\n\n/* Layout of fast clones : [skb1][skb2][fclone_ref] */\nstruct sk_buff_fclones {\n\tstruct sk_buff\tskb1;\n\n\tstruct sk_buff\tskb2;\n\n\trefcount_t\tfclone_ref;\n};\n\n/**\n *\tskb_fclone_busy - check if fclone is busy\n *\t@sk: socket\n *\t@skb: buffer\n *\n * Returns true if skb is a fast clone, and its clone is not freed.\n * Some drivers call skb_orphan() in their ndo_start_xmit(),\n * so we also check that this didnt happen.\n */\nstatic inline bool skb_fclone_busy(const struct sock *sk,\n\t\t\t\t   const struct sk_buff *skb)\n{\n\tconst struct sk_buff_fclones *fclones;\n\n\tfclones = container_of(skb, struct sk_buff_fclones, skb1);\n\n\treturn skb->fclone == SKB_FCLONE_ORIG &&\n\t       refcount_read(&fclones->fclone_ref) > 1 &&\n\t       fclones->skb2.sk == sk;\n}\n\n/**\n * alloc_skb_fclone - allocate a network buffer from fclone cache\n * @size: size to allocate\n * @priority: allocation mask\n *\n * This function is a convenient wrapper around __alloc_skb().\n */\nstatic inline struct sk_buff *alloc_skb_fclone(unsigned int size,\n\t\t\t\t\t       gfp_t priority)\n{\n\treturn __alloc_skb(size, priority, SKB_ALLOC_FCLONE, NUMA_NO_NODE);\n}\n\nstruct sk_buff *skb_morph(struct sk_buff *dst, struct sk_buff *src);\nvoid skb_headers_offset_update(struct sk_buff *skb, int off);\nint skb_copy_ubufs(struct sk_buff *skb, gfp_t gfp_mask);\nstruct sk_buff *skb_clone(struct sk_buff *skb, gfp_t priority);\nvoid skb_copy_header(struct sk_buff *new, const struct sk_buff *old);\nstruct sk_buff *skb_copy(const struct sk_buff *skb, gfp_t priority);\nstruct sk_buff *__pskb_copy_fclone(struct sk_buff *skb, int headroom,\n\t\t\t\t   gfp_t gfp_mask, bool fclone);\nstatic inline struct sk_buff *__pskb_copy(struct sk_buff *skb, int headroom,\n\t\t\t\t\t  gfp_t gfp_mask)\n{\n\treturn __pskb_copy_fclone(skb, headroom, gfp_mask, false);\n}\n\nint pskb_expand_head(struct sk_buff *skb, int nhead, int ntail, gfp_t gfp_mask);\nstruct sk_buff *skb_realloc_headroom(struct sk_buff *skb,\n\t\t\t\t     unsigned int headroom);\nstruct sk_buff *skb_copy_expand(const struct sk_buff *skb, int newheadroom,\n\t\t\t\tint newtailroom, gfp_t priority);\nint __must_check skb_to_sgvec_nomark(struct sk_buff *skb, struct scatterlist *sg,\n\t\t\t\t     int offset, int len);\nint __must_check skb_to_sgvec(struct sk_buff *skb, struct scatterlist *sg,\n\t\t\t      int offset, int len);\nint skb_cow_data(struct sk_buff *skb, int tailbits, struct sk_buff **trailer);\nint __skb_pad(struct sk_buff *skb, int pad, bool free_on_error);\n\n/**\n *\tskb_pad\t\t\t-\tzero pad the tail of an skb\n *\t@skb: buffer to pad\n *\t@pad: space to pad\n *\n *\tEnsure that a buffer is followed by a padding area that is zero\n *\tfilled. Used by network drivers which may DMA or transfer data\n *\tbeyond the buffer end onto the wire.\n *\n *\tMay return error in out of memory cases. The skb is freed on error.\n */\nstatic inline int skb_pad(struct sk_buff *skb, int pad)\n{\n\treturn __skb_pad(skb, pad, true);\n}\n#define dev_kfree_skb(a)\tconsume_skb(a)\n\nint skb_append_pagefrags(struct sk_buff *skb, struct page *page,\n\t\t\t int offset, size_t size);\n\nstruct skb_seq_state {\n\t__u32\t\tlower_offset;\n\t__u32\t\tupper_offset;\n\t__u32\t\tfrag_idx;\n\t__u32\t\tstepped_offset;\n\tstruct sk_buff\t*root_skb;\n\tstruct sk_buff\t*cur_skb;\n\t__u8\t\t*frag_data;\n\t__u32\t\tfrag_off;\n};\n\nvoid skb_prepare_seq_read(struct sk_buff *skb, unsigned int from,\n\t\t\t  unsigned int to, struct skb_seq_state *st);\nunsigned int skb_seq_read(unsigned int consumed, const u8 **data,\n\t\t\t  struct skb_seq_state *st);\nvoid skb_abort_seq_read(struct skb_seq_state *st);\n\nunsigned int skb_find_text(struct sk_buff *skb, unsigned int from,\n\t\t\t   unsigned int to, struct ts_config *config);\n\n/*\n * Packet hash types specify the type of hash in skb_set_hash.\n *\n * Hash types refer to the protocol layer addresses which are used to\n * construct a packet's hash. The hashes are used to differentiate or identify\n * flows of the protocol layer for the hash type. Hash types are either\n * layer-2 (L2), layer-3 (L3), or layer-4 (L4).\n *\n * Properties of hashes:\n *\n * 1) Two packets in different flows have different hash values\n * 2) Two packets in the same flow should have the same hash value\n *\n * A hash at a higher layer is considered to be more specific. A driver should\n * set the most specific hash possible.\n *\n * A driver cannot indicate a more specific hash than the layer at which a hash\n * was computed. For instance an L3 hash cannot be set as an L4 hash.\n *\n * A driver may indicate a hash level which is less specific than the\n * actual layer the hash was computed on. For instance, a hash computed\n * at L4 may be considered an L3 hash. This should only be done if the\n * driver can't unambiguously determine that the HW computed the hash at\n * the higher layer. Note that the \"should\" in the second property above\n * permits this.\n */\nenum pkt_hash_types {\n\tPKT_HASH_TYPE_NONE,\t/* Undefined type */\n\tPKT_HASH_TYPE_L2,\t/* Input: src_MAC, dest_MAC */\n\tPKT_HASH_TYPE_L3,\t/* Input: src_IP, dst_IP */\n\tPKT_HASH_TYPE_L4,\t/* Input: src_IP, dst_IP, src_port, dst_port */\n};\n\nstatic inline void skb_clear_hash(struct sk_buff *skb)\n{\n\tskb->hash = 0;\n\tskb->sw_hash = 0;\n\tskb->l4_hash = 0;\n}\n\nstatic inline void skb_clear_hash_if_not_l4(struct sk_buff *skb)\n{\n\tif (!skb->l4_hash)\n\t\tskb_clear_hash(skb);\n}\n\nstatic inline void\n__skb_set_hash(struct sk_buff *skb, __u32 hash, bool is_sw, bool is_l4)\n{\n\tskb->l4_hash = is_l4;\n\tskb->sw_hash = is_sw;\n\tskb->hash = hash;\n}\n\nstatic inline void\nskb_set_hash(struct sk_buff *skb, __u32 hash, enum pkt_hash_types type)\n{\n\t/* Used by drivers to set hash from HW */\n\t__skb_set_hash(skb, hash, false, type == PKT_HASH_TYPE_L4);\n}\n\nstatic inline void\n__skb_set_sw_hash(struct sk_buff *skb, __u32 hash, bool is_l4)\n{\n\t__skb_set_hash(skb, hash, true, is_l4);\n}\n\nvoid __skb_get_hash(struct sk_buff *skb);\nu32 __skb_get_hash_symmetric(const struct sk_buff *skb);\nu32 skb_get_poff(const struct sk_buff *skb);\nu32 __skb_get_poff(const struct sk_buff *skb, void *data,\n\t\t   const struct flow_keys_basic *keys, int hlen);\n__be32 __skb_flow_get_ports(const struct sk_buff *skb, int thoff, u8 ip_proto,\n\t\t\t    void *data, int hlen_proto);\n\nstatic inline __be32 skb_flow_get_ports(const struct sk_buff *skb,\n\t\t\t\t\tint thoff, u8 ip_proto)\n{\n\treturn __skb_flow_get_ports(skb, thoff, ip_proto, NULL, 0);\n}\n\nvoid skb_flow_dissector_init(struct flow_dissector *flow_dissector,\n\t\t\t     const struct flow_dissector_key *key,\n\t\t\t     unsigned int key_count);\n\nstruct bpf_flow_dissector;\nbool bpf_flow_dissect(struct bpf_prog *prog, struct bpf_flow_dissector *ctx,\n\t\t      __be16 proto, int nhoff, int hlen, unsigned int flags);\n\nbool __skb_flow_dissect(const struct net *net,\n\t\t\tconst struct sk_buff *skb,\n\t\t\tstruct flow_dissector *flow_dissector,\n\t\t\tvoid *target_container,\n\t\t\tvoid *data, __be16 proto, int nhoff, int hlen,\n\t\t\tunsigned int flags);\n\nstatic inline bool skb_flow_dissect(const struct sk_buff *skb,\n\t\t\t\t    struct flow_dissector *flow_dissector,\n\t\t\t\t    void *target_container, unsigned int flags)\n{\n\treturn __skb_flow_dissect(NULL, skb, flow_dissector,\n\t\t\t\t  target_container, NULL, 0, 0, 0, flags);\n}\n\nstatic inline bool skb_flow_dissect_flow_keys(const struct sk_buff *skb,\n\t\t\t\t\t      struct flow_keys *flow,\n\t\t\t\t\t      unsigned int flags)\n{\n\tmemset(flow, 0, sizeof(*flow));\n\treturn __skb_flow_dissect(NULL, skb, &flow_keys_dissector,\n\t\t\t\t  flow, NULL, 0, 0, 0, flags);\n}\n\nstatic inline bool\nskb_flow_dissect_flow_keys_basic(const struct net *net,\n\t\t\t\t const struct sk_buff *skb,\n\t\t\t\t struct flow_keys_basic *flow, void *data,\n\t\t\t\t __be16 proto, int nhoff, int hlen,\n\t\t\t\t unsigned int flags)\n{\n\tmemset(flow, 0, sizeof(*flow));\n\treturn __skb_flow_dissect(net, skb, &flow_keys_basic_dissector, flow,\n\t\t\t\t  data, proto, nhoff, hlen, flags);\n}\n\nvoid skb_flow_dissect_meta(const struct sk_buff *skb,\n\t\t\t   struct flow_dissector *flow_dissector,\n\t\t\t   void *target_container);\n\n/* Gets a skb connection tracking info, ctinfo map should be a\n * map of mapsize to translate enum ip_conntrack_info states\n * to user states.\n */\nvoid\nskb_flow_dissect_ct(const struct sk_buff *skb,\n\t\t    struct flow_dissector *flow_dissector,\n\t\t    void *target_container,\n\t\t    u16 *ctinfo_map, size_t mapsize,\n\t\t    bool post_ct);\nvoid\nskb_flow_dissect_tunnel_info(const struct sk_buff *skb,\n\t\t\t     struct flow_dissector *flow_dissector,\n\t\t\t     void *target_container);\n\nvoid skb_flow_dissect_hash(const struct sk_buff *skb,\n\t\t\t   struct flow_dissector *flow_dissector,\n\t\t\t   void *target_container);\n\nstatic inline __u32 skb_get_hash(struct sk_buff *skb)\n{\n\tif (!skb->l4_hash && !skb->sw_hash)\n\t\t__skb_get_hash(skb);\n\n\treturn skb->hash;\n}\n\nstatic inline __u32 skb_get_hash_flowi6(struct sk_buff *skb, const struct flowi6 *fl6)\n{\n\tif (!skb->l4_hash && !skb->sw_hash) {\n\t\tstruct flow_keys keys;\n\t\t__u32 hash = __get_hash_from_flowi6(fl6, &keys);\n\n\t\t__skb_set_sw_hash(skb, hash, flow_keys_have_l4(&keys));\n\t}\n\n\treturn skb->hash;\n}\n\n__u32 skb_get_hash_perturb(const struct sk_buff *skb,\n\t\t\t   const siphash_key_t *perturb);\n\nstatic inline __u32 skb_get_hash_raw(const struct sk_buff *skb)\n{\n\treturn skb->hash;\n}\n\nstatic inline void skb_copy_hash(struct sk_buff *to, const struct sk_buff *from)\n{\n\tto->hash = from->hash;\n\tto->sw_hash = from->sw_hash;\n\tto->l4_hash = from->l4_hash;\n};\n\nstatic inline void skb_copy_decrypted(struct sk_buff *to,\n\t\t\t\t      const struct sk_buff *from)\n{\n#ifdef CONFIG_TLS_DEVICE\n\tto->decrypted = from->decrypted;\n#endif\n}\n\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\nstatic inline unsigned char *skb_end_pointer(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->end;\n}\n\nstatic inline unsigned int skb_end_offset(const struct sk_buff *skb)\n{\n\treturn skb->end;\n}\n#else\nstatic inline unsigned char *skb_end_pointer(const struct sk_buff *skb)\n{\n\treturn skb->end;\n}\n\nstatic inline unsigned int skb_end_offset(const struct sk_buff *skb)\n{\n\treturn skb->end - skb->head;\n}\n#endif\n\n/* Internal */\n#define skb_shinfo(SKB)\t((struct skb_shared_info *)(skb_end_pointer(SKB)))\n\nstatic inline struct skb_shared_hwtstamps *skb_hwtstamps(struct sk_buff *skb)\n{\n\treturn &skb_shinfo(skb)->hwtstamps;\n}\n\nstatic inline struct ubuf_info *skb_zcopy(struct sk_buff *skb)\n{\n\tbool is_zcopy = skb && skb_shinfo(skb)->flags & SKBFL_ZEROCOPY_ENABLE;\n\n\treturn is_zcopy ? skb_uarg(skb) : NULL;\n}\n\nstatic inline void net_zcopy_get(struct ubuf_info *uarg)\n{\n\trefcount_inc(&uarg->refcnt);\n}\n\nstatic inline void skb_zcopy_init(struct sk_buff *skb, struct ubuf_info *uarg)\n{\n\tskb_shinfo(skb)->destructor_arg = uarg;\n\tskb_shinfo(skb)->flags |= uarg->flags;\n}\n\nstatic inline void skb_zcopy_set(struct sk_buff *skb, struct ubuf_info *uarg,\n\t\t\t\t bool *have_ref)\n{\n\tif (skb && uarg && !skb_zcopy(skb)) {\n\t\tif (unlikely(have_ref && *have_ref))\n\t\t\t*have_ref = false;\n\t\telse\n\t\t\tnet_zcopy_get(uarg);\n\t\tskb_zcopy_init(skb, uarg);\n\t}\n}\n\nstatic inline void skb_zcopy_set_nouarg(struct sk_buff *skb, void *val)\n{\n\tskb_shinfo(skb)->destructor_arg = (void *)((uintptr_t) val | 0x1UL);\n\tskb_shinfo(skb)->flags |= SKBFL_ZEROCOPY_FRAG;\n}\n\nstatic inline bool skb_zcopy_is_nouarg(struct sk_buff *skb)\n{\n\treturn (uintptr_t) skb_shinfo(skb)->destructor_arg & 0x1UL;\n}\n\nstatic inline void *skb_zcopy_get_nouarg(struct sk_buff *skb)\n{\n\treturn (void *)((uintptr_t) skb_shinfo(skb)->destructor_arg & ~0x1UL);\n}\n\nstatic inline void net_zcopy_put(struct ubuf_info *uarg)\n{\n\tif (uarg)\n\t\tuarg->callback(NULL, uarg, true);\n}\n\nstatic inline void net_zcopy_put_abort(struct ubuf_info *uarg, bool have_uref)\n{\n\tif (uarg) {\n\t\tif (uarg->callback == msg_zerocopy_callback)\n\t\t\tmsg_zerocopy_put_abort(uarg, have_uref);\n\t\telse if (have_uref)\n\t\t\tnet_zcopy_put(uarg);\n\t}\n}\n\n/* Release a reference on a zerocopy structure */\nstatic inline void skb_zcopy_clear(struct sk_buff *skb, bool zerocopy_success)\n{\n\tstruct ubuf_info *uarg = skb_zcopy(skb);\n\n\tif (uarg) {\n\t\tif (!skb_zcopy_is_nouarg(skb))\n\t\t\tuarg->callback(skb, uarg, zerocopy_success);\n\n\t\tskb_shinfo(skb)->flags &= ~SKBFL_ZEROCOPY_FRAG;\n\t}\n}\n\nstatic inline void skb_mark_not_on_list(struct sk_buff *skb)\n{\n\tskb->next = NULL;\n}\n\n/* Iterate through singly-linked GSO fragments of an skb. */\n#define skb_list_walk_safe(first, skb, next_skb)                               \\\n\tfor ((skb) = (first), (next_skb) = (skb) ? (skb)->next : NULL; (skb);  \\\n\t     (skb) = (next_skb), (next_skb) = (skb) ? (skb)->next : NULL)\n\nstatic inline void skb_list_del_init(struct sk_buff *skb)\n{\n\t__list_del_entry(&skb->list);\n\tskb_mark_not_on_list(skb);\n}\n\n/**\n *\tskb_queue_empty - check if a queue is empty\n *\t@list: queue head\n *\n *\tReturns true if the queue is empty, false otherwise.\n */\nstatic inline int skb_queue_empty(const struct sk_buff_head *list)\n{\n\treturn list->next == (const struct sk_buff *) list;\n}\n\n/**\n *\tskb_queue_empty_lockless - check if a queue is empty\n *\t@list: queue head\n *\n *\tReturns true if the queue is empty, false otherwise.\n *\tThis variant can be used in lockless contexts.\n */\nstatic inline bool skb_queue_empty_lockless(const struct sk_buff_head *list)\n{\n\treturn READ_ONCE(list->next) == (const struct sk_buff *) list;\n}\n\n\n/**\n *\tskb_queue_is_last - check if skb is the last entry in the queue\n *\t@list: queue head\n *\t@skb: buffer\n *\n *\tReturns true if @skb is the last buffer on the list.\n */\nstatic inline bool skb_queue_is_last(const struct sk_buff_head *list,\n\t\t\t\t     const struct sk_buff *skb)\n{\n\treturn skb->next == (const struct sk_buff *) list;\n}\n\n/**\n *\tskb_queue_is_first - check if skb is the first entry in the queue\n *\t@list: queue head\n *\t@skb: buffer\n *\n *\tReturns true if @skb is the first buffer on the list.\n */\nstatic inline bool skb_queue_is_first(const struct sk_buff_head *list,\n\t\t\t\t      const struct sk_buff *skb)\n{\n\treturn skb->prev == (const struct sk_buff *) list;\n}\n\n/**\n *\tskb_queue_next - return the next packet in the queue\n *\t@list: queue head\n *\t@skb: current buffer\n *\n *\tReturn the next packet in @list after @skb.  It is only valid to\n *\tcall this if skb_queue_is_last() evaluates to false.\n */\nstatic inline struct sk_buff *skb_queue_next(const struct sk_buff_head *list,\n\t\t\t\t\t     const struct sk_buff *skb)\n{\n\t/* This BUG_ON may seem severe, but if we just return then we\n\t * are going to dereference garbage.\n\t */\n\tBUG_ON(skb_queue_is_last(list, skb));\n\treturn skb->next;\n}\n\n/**\n *\tskb_queue_prev - return the prev packet in the queue\n *\t@list: queue head\n *\t@skb: current buffer\n *\n *\tReturn the prev packet in @list before @skb.  It is only valid to\n *\tcall this if skb_queue_is_first() evaluates to false.\n */\nstatic inline struct sk_buff *skb_queue_prev(const struct sk_buff_head *list,\n\t\t\t\t\t     const struct sk_buff *skb)\n{\n\t/* This BUG_ON may seem severe, but if we just return then we\n\t * are going to dereference garbage.\n\t */\n\tBUG_ON(skb_queue_is_first(list, skb));\n\treturn skb->prev;\n}\n\n/**\n *\tskb_get - reference buffer\n *\t@skb: buffer to reference\n *\n *\tMakes another reference to a socket buffer and returns a pointer\n *\tto the buffer.\n */\nstatic inline struct sk_buff *skb_get(struct sk_buff *skb)\n{\n\trefcount_inc(&skb->users);\n\treturn skb;\n}\n\n/*\n * If users == 1, we are the only owner and can avoid redundant atomic changes.\n */\n\n/**\n *\tskb_cloned - is the buffer a clone\n *\t@skb: buffer to check\n *\n *\tReturns true if the buffer was generated with skb_clone() and is\n *\tone of multiple shared copies of the buffer. Cloned buffers are\n *\tshared data so must not be written to under normal circumstances.\n */\nstatic inline int skb_cloned(const struct sk_buff *skb)\n{\n\treturn skb->cloned &&\n\t       (atomic_read(&skb_shinfo(skb)->dataref) & SKB_DATAREF_MASK) != 1;\n}\n\nstatic inline int skb_unclone(struct sk_buff *skb, gfp_t pri)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(pri));\n\n\tif (skb_cloned(skb))\n\t\treturn pskb_expand_head(skb, 0, 0, pri);\n\n\treturn 0;\n}\n\n/**\n *\tskb_header_cloned - is the header a clone\n *\t@skb: buffer to check\n *\n *\tReturns true if modifying the header part of the buffer requires\n *\tthe data to be copied.\n */\nstatic inline int skb_header_cloned(const struct sk_buff *skb)\n{\n\tint dataref;\n\n\tif (!skb->cloned)\n\t\treturn 0;\n\n\tdataref = atomic_read(&skb_shinfo(skb)->dataref);\n\tdataref = (dataref & SKB_DATAREF_MASK) - (dataref >> SKB_DATAREF_SHIFT);\n\treturn dataref != 1;\n}\n\nstatic inline int skb_header_unclone(struct sk_buff *skb, gfp_t pri)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(pri));\n\n\tif (skb_header_cloned(skb))\n\t\treturn pskb_expand_head(skb, 0, 0, pri);\n\n\treturn 0;\n}\n\n/**\n *\t__skb_header_release - release reference to header\n *\t@skb: buffer to operate on\n */\nstatic inline void __skb_header_release(struct sk_buff *skb)\n{\n\tskb->nohdr = 1;\n\tatomic_set(&skb_shinfo(skb)->dataref, 1 + (1 << SKB_DATAREF_SHIFT));\n}\n\n\n/**\n *\tskb_shared - is the buffer shared\n *\t@skb: buffer to check\n *\n *\tReturns true if more than one person has a reference to this\n *\tbuffer.\n */\nstatic inline int skb_shared(const struct sk_buff *skb)\n{\n\treturn refcount_read(&skb->users) != 1;\n}\n\n/**\n *\tskb_share_check - check if buffer is shared and if so clone it\n *\t@skb: buffer to check\n *\t@pri: priority for memory allocation\n *\n *\tIf the buffer is shared the buffer is cloned and the old copy\n *\tdrops a reference. A new clone with a single reference is returned.\n *\tIf the buffer is not shared the original buffer is returned. When\n *\tbeing called from interrupt status or with spinlocks held pri must\n *\tbe GFP_ATOMIC.\n *\n *\tNULL is returned on a memory allocation failure.\n */\nstatic inline struct sk_buff *skb_share_check(struct sk_buff *skb, gfp_t pri)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(pri));\n\tif (skb_shared(skb)) {\n\t\tstruct sk_buff *nskb = skb_clone(skb, pri);\n\n\t\tif (likely(nskb))\n\t\t\tconsume_skb(skb);\n\t\telse\n\t\t\tkfree_skb(skb);\n\t\tskb = nskb;\n\t}\n\treturn skb;\n}\n\n/*\n *\tCopy shared buffers into a new sk_buff. We effectively do COW on\n *\tpackets to handle cases where we have a local reader and forward\n *\tand a couple of other messy ones. The normal one is tcpdumping\n *\ta packet thats being forwarded.\n */\n\n/**\n *\tskb_unshare - make a copy of a shared buffer\n *\t@skb: buffer to check\n *\t@pri: priority for memory allocation\n *\n *\tIf the socket buffer is a clone then this function creates a new\n *\tcopy of the data, drops a reference count on the old copy and returns\n *\tthe new copy with the reference count at 1. If the buffer is not a clone\n *\tthe original buffer is returned. When called with a spinlock held or\n *\tfrom interrupt state @pri must be %GFP_ATOMIC\n *\n *\t%NULL is returned on a memory allocation failure.\n */\nstatic inline struct sk_buff *skb_unshare(struct sk_buff *skb,\n\t\t\t\t\t  gfp_t pri)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(pri));\n\tif (skb_cloned(skb)) {\n\t\tstruct sk_buff *nskb = skb_copy(skb, pri);\n\n\t\t/* Free our shared copy */\n\t\tif (likely(nskb))\n\t\t\tconsume_skb(skb);\n\t\telse\n\t\t\tkfree_skb(skb);\n\t\tskb = nskb;\n\t}\n\treturn skb;\n}\n\n/**\n *\tskb_peek - peek at the head of an &sk_buff_head\n *\t@list_: list to peek at\n *\n *\tPeek an &sk_buff. Unlike most other operations you _MUST_\n *\tbe careful with this one. A peek leaves the buffer on the\n *\tlist and someone else may run off with it. You must hold\n *\tthe appropriate locks or have a private queue to do this.\n *\n *\tReturns %NULL for an empty list or a pointer to the head element.\n *\tThe reference count is not incremented and the reference is therefore\n *\tvolatile. Use with caution.\n */\nstatic inline struct sk_buff *skb_peek(const struct sk_buff_head *list_)\n{\n\tstruct sk_buff *skb = list_->next;\n\n\tif (skb == (struct sk_buff *)list_)\n\t\tskb = NULL;\n\treturn skb;\n}\n\n/**\n *\t__skb_peek - peek at the head of a non-empty &sk_buff_head\n *\t@list_: list to peek at\n *\n *\tLike skb_peek(), but the caller knows that the list is not empty.\n */\nstatic inline struct sk_buff *__skb_peek(const struct sk_buff_head *list_)\n{\n\treturn list_->next;\n}\n\n/**\n *\tskb_peek_next - peek skb following the given one from a queue\n *\t@skb: skb to start from\n *\t@list_: list to peek at\n *\n *\tReturns %NULL when the end of the list is met or a pointer to the\n *\tnext element. The reference count is not incremented and the\n *\treference is therefore volatile. Use with caution.\n */\nstatic inline struct sk_buff *skb_peek_next(struct sk_buff *skb,\n\t\tconst struct sk_buff_head *list_)\n{\n\tstruct sk_buff *next = skb->next;\n\n\tif (next == (struct sk_buff *)list_)\n\t\tnext = NULL;\n\treturn next;\n}\n\n/**\n *\tskb_peek_tail - peek at the tail of an &sk_buff_head\n *\t@list_: list to peek at\n *\n *\tPeek an &sk_buff. Unlike most other operations you _MUST_\n *\tbe careful with this one. A peek leaves the buffer on the\n *\tlist and someone else may run off with it. You must hold\n *\tthe appropriate locks or have a private queue to do this.\n *\n *\tReturns %NULL for an empty list or a pointer to the tail element.\n *\tThe reference count is not incremented and the reference is therefore\n *\tvolatile. Use with caution.\n */\nstatic inline struct sk_buff *skb_peek_tail(const struct sk_buff_head *list_)\n{\n\tstruct sk_buff *skb = READ_ONCE(list_->prev);\n\n\tif (skb == (struct sk_buff *)list_)\n\t\tskb = NULL;\n\treturn skb;\n\n}\n\n/**\n *\tskb_queue_len\t- get queue length\n *\t@list_: list to measure\n *\n *\tReturn the length of an &sk_buff queue.\n */\nstatic inline __u32 skb_queue_len(const struct sk_buff_head *list_)\n{\n\treturn list_->qlen;\n}\n\n/**\n *\tskb_queue_len_lockless\t- get queue length\n *\t@list_: list to measure\n *\n *\tReturn the length of an &sk_buff queue.\n *\tThis variant can be used in lockless contexts.\n */\nstatic inline __u32 skb_queue_len_lockless(const struct sk_buff_head *list_)\n{\n\treturn READ_ONCE(list_->qlen);\n}\n\n/**\n *\t__skb_queue_head_init - initialize non-spinlock portions of sk_buff_head\n *\t@list: queue to initialize\n *\n *\tThis initializes only the list and queue length aspects of\n *\tan sk_buff_head object.  This allows to initialize the list\n *\taspects of an sk_buff_head without reinitializing things like\n *\tthe spinlock.  It can also be used for on-stack sk_buff_head\n *\tobjects where the spinlock is known to not be used.\n */\nstatic inline void __skb_queue_head_init(struct sk_buff_head *list)\n{\n\tlist->prev = list->next = (struct sk_buff *)list;\n\tlist->qlen = 0;\n}\n\n/*\n * This function creates a split out lock class for each invocation;\n * this is needed for now since a whole lot of users of the skb-queue\n * infrastructure in drivers have different locking usage (in hardirq)\n * than the networking core (in softirq only). In the long run either the\n * network layer or drivers should need annotation to consolidate the\n * main types of usage into 3 classes.\n */\nstatic inline void skb_queue_head_init(struct sk_buff_head *list)\n{\n\tspin_lock_init(&list->lock);\n\t__skb_queue_head_init(list);\n}\n\nstatic inline void skb_queue_head_init_class(struct sk_buff_head *list,\n\t\tstruct lock_class_key *class)\n{\n\tskb_queue_head_init(list);\n\tlockdep_set_class(&list->lock, class);\n}\n\n/*\n *\tInsert an sk_buff on a list.\n *\n *\tThe \"__skb_xxxx()\" functions are the non-atomic ones that\n *\tcan only be called with interrupts disabled.\n */\nstatic inline void __skb_insert(struct sk_buff *newsk,\n\t\t\t\tstruct sk_buff *prev, struct sk_buff *next,\n\t\t\t\tstruct sk_buff_head *list)\n{\n\t/* See skb_queue_empty_lockless() and skb_peek_tail()\n\t * for the opposite READ_ONCE()\n\t */\n\tWRITE_ONCE(newsk->next, next);\n\tWRITE_ONCE(newsk->prev, prev);\n\tWRITE_ONCE(next->prev, newsk);\n\tWRITE_ONCE(prev->next, newsk);\n\tlist->qlen++;\n}\n\nstatic inline void __skb_queue_splice(const struct sk_buff_head *list,\n\t\t\t\t      struct sk_buff *prev,\n\t\t\t\t      struct sk_buff *next)\n{\n\tstruct sk_buff *first = list->next;\n\tstruct sk_buff *last = list->prev;\n\n\tWRITE_ONCE(first->prev, prev);\n\tWRITE_ONCE(prev->next, first);\n\n\tWRITE_ONCE(last->next, next);\n\tWRITE_ONCE(next->prev, last);\n}\n\n/**\n *\tskb_queue_splice - join two skb lists, this is designed for stacks\n *\t@list: the new list to add\n *\t@head: the place to add it in the first list\n */\nstatic inline void skb_queue_splice(const struct sk_buff_head *list,\n\t\t\t\t    struct sk_buff_head *head)\n{\n\tif (!skb_queue_empty(list)) {\n\t\t__skb_queue_splice(list, (struct sk_buff *) head, head->next);\n\t\thead->qlen += list->qlen;\n\t}\n}\n\n/**\n *\tskb_queue_splice_init - join two skb lists and reinitialise the emptied list\n *\t@list: the new list to add\n *\t@head: the place to add it in the first list\n *\n *\tThe list at @list is reinitialised\n */\nstatic inline void skb_queue_splice_init(struct sk_buff_head *list,\n\t\t\t\t\t struct sk_buff_head *head)\n{\n\tif (!skb_queue_empty(list)) {\n\t\t__skb_queue_splice(list, (struct sk_buff *) head, head->next);\n\t\thead->qlen += list->qlen;\n\t\t__skb_queue_head_init(list);\n\t}\n}\n\n/**\n *\tskb_queue_splice_tail - join two skb lists, each list being a queue\n *\t@list: the new list to add\n *\t@head: the place to add it in the first list\n */\nstatic inline void skb_queue_splice_tail(const struct sk_buff_head *list,\n\t\t\t\t\t struct sk_buff_head *head)\n{\n\tif (!skb_queue_empty(list)) {\n\t\t__skb_queue_splice(list, head->prev, (struct sk_buff *) head);\n\t\thead->qlen += list->qlen;\n\t}\n}\n\n/**\n *\tskb_queue_splice_tail_init - join two skb lists and reinitialise the emptied list\n *\t@list: the new list to add\n *\t@head: the place to add it in the first list\n *\n *\tEach of the lists is a queue.\n *\tThe list at @list is reinitialised\n */\nstatic inline void skb_queue_splice_tail_init(struct sk_buff_head *list,\n\t\t\t\t\t      struct sk_buff_head *head)\n{\n\tif (!skb_queue_empty(list)) {\n\t\t__skb_queue_splice(list, head->prev, (struct sk_buff *) head);\n\t\thead->qlen += list->qlen;\n\t\t__skb_queue_head_init(list);\n\t}\n}\n\n/**\n *\t__skb_queue_after - queue a buffer at the list head\n *\t@list: list to use\n *\t@prev: place after this buffer\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer int the middle of a list. This function takes no locks\n *\tand you must therefore hold required locks before calling it.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nstatic inline void __skb_queue_after(struct sk_buff_head *list,\n\t\t\t\t     struct sk_buff *prev,\n\t\t\t\t     struct sk_buff *newsk)\n{\n\t__skb_insert(newsk, prev, prev->next, list);\n}\n\nvoid skb_append(struct sk_buff *old, struct sk_buff *newsk,\n\t\tstruct sk_buff_head *list);\n\nstatic inline void __skb_queue_before(struct sk_buff_head *list,\n\t\t\t\t      struct sk_buff *next,\n\t\t\t\t      struct sk_buff *newsk)\n{\n\t__skb_insert(newsk, next->prev, next, list);\n}\n\n/**\n *\t__skb_queue_head - queue a buffer at the list head\n *\t@list: list to use\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer at the start of a list. This function takes no locks\n *\tand you must therefore hold required locks before calling it.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nstatic inline void __skb_queue_head(struct sk_buff_head *list,\n\t\t\t\t    struct sk_buff *newsk)\n{\n\t__skb_queue_after(list, (struct sk_buff *)list, newsk);\n}\nvoid skb_queue_head(struct sk_buff_head *list, struct sk_buff *newsk);\n\n/**\n *\t__skb_queue_tail - queue a buffer at the list tail\n *\t@list: list to use\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer at the end of a list. This function takes no locks\n *\tand you must therefore hold required locks before calling it.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nstatic inline void __skb_queue_tail(struct sk_buff_head *list,\n\t\t\t\t   struct sk_buff *newsk)\n{\n\t__skb_queue_before(list, (struct sk_buff *)list, newsk);\n}\nvoid skb_queue_tail(struct sk_buff_head *list, struct sk_buff *newsk);\n\n/*\n * remove sk_buff from list. _Must_ be called atomically, and with\n * the list known..\n */\nvoid skb_unlink(struct sk_buff *skb, struct sk_buff_head *list);\nstatic inline void __skb_unlink(struct sk_buff *skb, struct sk_buff_head *list)\n{\n\tstruct sk_buff *next, *prev;\n\n\tWRITE_ONCE(list->qlen, list->qlen - 1);\n\tnext\t   = skb->next;\n\tprev\t   = skb->prev;\n\tskb->next  = skb->prev = NULL;\n\tWRITE_ONCE(next->prev, prev);\n\tWRITE_ONCE(prev->next, next);\n}\n\n/**\n *\t__skb_dequeue - remove from the head of the queue\n *\t@list: list to dequeue from\n *\n *\tRemove the head of the list. This function does not take any locks\n *\tso must be used with appropriate locks held only. The head item is\n *\treturned or %NULL if the list is empty.\n */\nstatic inline struct sk_buff *__skb_dequeue(struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb = skb_peek(list);\n\tif (skb)\n\t\t__skb_unlink(skb, list);\n\treturn skb;\n}\nstruct sk_buff *skb_dequeue(struct sk_buff_head *list);\n\n/**\n *\t__skb_dequeue_tail - remove from the tail of the queue\n *\t@list: list to dequeue from\n *\n *\tRemove the tail of the list. This function does not take any locks\n *\tso must be used with appropriate locks held only. The tail item is\n *\treturned or %NULL if the list is empty.\n */\nstatic inline struct sk_buff *__skb_dequeue_tail(struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb = skb_peek_tail(list);\n\tif (skb)\n\t\t__skb_unlink(skb, list);\n\treturn skb;\n}\nstruct sk_buff *skb_dequeue_tail(struct sk_buff_head *list);\n\n\nstatic inline bool skb_is_nonlinear(const struct sk_buff *skb)\n{\n\treturn skb->data_len;\n}\n\nstatic inline unsigned int skb_headlen(const struct sk_buff *skb)\n{\n\treturn skb->len - skb->data_len;\n}\n\nstatic inline unsigned int __skb_pagelen(const struct sk_buff *skb)\n{\n\tunsigned int i, len = 0;\n\n\tfor (i = skb_shinfo(skb)->nr_frags - 1; (int)i >= 0; i--)\n\t\tlen += skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\treturn len;\n}\n\nstatic inline unsigned int skb_pagelen(const struct sk_buff *skb)\n{\n\treturn skb_headlen(skb) + __skb_pagelen(skb);\n}\n\n/**\n * __skb_fill_page_desc - initialise a paged fragment in an skb\n * @skb: buffer containing fragment to be initialised\n * @i: paged fragment index to initialise\n * @page: the page to use for this fragment\n * @off: the offset to the data with @page\n * @size: the length of the data\n *\n * Initialises the @i'th fragment of @skb to point to &size bytes at\n * offset @off within @page.\n *\n * Does not take any additional reference on the fragment.\n */\nstatic inline void __skb_fill_page_desc(struct sk_buff *skb, int i,\n\t\t\t\t\tstruct page *page, int off, int size)\n{\n\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t/*\n\t * Propagate page pfmemalloc to the skb if we can. The problem is\n\t * that not all callers have unique ownership of the page but rely\n\t * on page_is_pfmemalloc doing the right thing(tm).\n\t */\n\tfrag->bv_page\t\t  = page;\n\tfrag->bv_offset\t\t  = off;\n\tskb_frag_size_set(frag, size);\n\n\tpage = compound_head(page);\n\tif (page_is_pfmemalloc(page))\n\t\tskb->pfmemalloc\t= true;\n}\n\n/**\n * skb_fill_page_desc - initialise a paged fragment in an skb\n * @skb: buffer containing fragment to be initialised\n * @i: paged fragment index to initialise\n * @page: the page to use for this fragment\n * @off: the offset to the data with @page\n * @size: the length of the data\n *\n * As per __skb_fill_page_desc() -- initialises the @i'th fragment of\n * @skb to point to @size bytes at offset @off within @page. In\n * addition updates @skb such that @i is the last fragment.\n *\n * Does not take any additional reference on the fragment.\n */\nstatic inline void skb_fill_page_desc(struct sk_buff *skb, int i,\n\t\t\t\t      struct page *page, int off, int size)\n{\n\t__skb_fill_page_desc(skb, i, page, off, size);\n\tskb_shinfo(skb)->nr_frags = i + 1;\n}\n\nvoid skb_add_rx_frag(struct sk_buff *skb, int i, struct page *page, int off,\n\t\t     int size, unsigned int truesize);\n\nvoid skb_coalesce_rx_frag(struct sk_buff *skb, int i, int size,\n\t\t\t  unsigned int truesize);\n\n#define SKB_LINEAR_ASSERT(skb)  BUG_ON(skb_is_nonlinear(skb))\n\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\nstatic inline unsigned char *skb_tail_pointer(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->tail;\n}\n\nstatic inline void skb_reset_tail_pointer(struct sk_buff *skb)\n{\n\tskb->tail = skb->data - skb->head;\n}\n\nstatic inline void skb_set_tail_pointer(struct sk_buff *skb, const int offset)\n{\n\tskb_reset_tail_pointer(skb);\n\tskb->tail += offset;\n}\n\n#else /* NET_SKBUFF_DATA_USES_OFFSET */\nstatic inline unsigned char *skb_tail_pointer(const struct sk_buff *skb)\n{\n\treturn skb->tail;\n}\n\nstatic inline void skb_reset_tail_pointer(struct sk_buff *skb)\n{\n\tskb->tail = skb->data;\n}\n\nstatic inline void skb_set_tail_pointer(struct sk_buff *skb, const int offset)\n{\n\tskb->tail = skb->data + offset;\n}\n\n#endif /* NET_SKBUFF_DATA_USES_OFFSET */\n\n/*\n *\tAdd data to an sk_buff\n */\nvoid *pskb_put(struct sk_buff *skb, struct sk_buff *tail, int len);\nvoid *skb_put(struct sk_buff *skb, unsigned int len);\nstatic inline void *__skb_put(struct sk_buff *skb, unsigned int len)\n{\n\tvoid *tmp = skb_tail_pointer(skb);\n\tSKB_LINEAR_ASSERT(skb);\n\tskb->tail += len;\n\tskb->len  += len;\n\treturn tmp;\n}\n\nstatic inline void *__skb_put_zero(struct sk_buff *skb, unsigned int len)\n{\n\tvoid *tmp = __skb_put(skb, len);\n\n\tmemset(tmp, 0, len);\n\treturn tmp;\n}\n\nstatic inline void *__skb_put_data(struct sk_buff *skb, const void *data,\n\t\t\t\t   unsigned int len)\n{\n\tvoid *tmp = __skb_put(skb, len);\n\n\tmemcpy(tmp, data, len);\n\treturn tmp;\n}\n\nstatic inline void __skb_put_u8(struct sk_buff *skb, u8 val)\n{\n\t*(u8 *)__skb_put(skb, 1) = val;\n}\n\nstatic inline void *skb_put_zero(struct sk_buff *skb, unsigned int len)\n{\n\tvoid *tmp = skb_put(skb, len);\n\n\tmemset(tmp, 0, len);\n\n\treturn tmp;\n}\n\nstatic inline void *skb_put_data(struct sk_buff *skb, const void *data,\n\t\t\t\t unsigned int len)\n{\n\tvoid *tmp = skb_put(skb, len);\n\n\tmemcpy(tmp, data, len);\n\n\treturn tmp;\n}\n\nstatic inline void skb_put_u8(struct sk_buff *skb, u8 val)\n{\n\t*(u8 *)skb_put(skb, 1) = val;\n}\n\nvoid *skb_push(struct sk_buff *skb, unsigned int len);\nstatic inline void *__skb_push(struct sk_buff *skb, unsigned int len)\n{\n\tskb->data -= len;\n\tskb->len  += len;\n\treturn skb->data;\n}\n\nvoid *skb_pull(struct sk_buff *skb, unsigned int len);\nstatic inline void *__skb_pull(struct sk_buff *skb, unsigned int len)\n{\n\tskb->len -= len;\n\tBUG_ON(skb->len < skb->data_len);\n\treturn skb->data += len;\n}\n\nstatic inline void *skb_pull_inline(struct sk_buff *skb, unsigned int len)\n{\n\treturn unlikely(len > skb->len) ? NULL : __skb_pull(skb, len);\n}\n\nvoid *__pskb_pull_tail(struct sk_buff *skb, int delta);\n\nstatic inline void *__pskb_pull(struct sk_buff *skb, unsigned int len)\n{\n\tif (len > skb_headlen(skb) &&\n\t    !__pskb_pull_tail(skb, len - skb_headlen(skb)))\n\t\treturn NULL;\n\tskb->len -= len;\n\treturn skb->data += len;\n}\n\nstatic inline void *pskb_pull(struct sk_buff *skb, unsigned int len)\n{\n\treturn unlikely(len > skb->len) ? NULL : __pskb_pull(skb, len);\n}\n\nstatic inline bool pskb_may_pull(struct sk_buff *skb, unsigned int len)\n{\n\tif (likely(len <= skb_headlen(skb)))\n\t\treturn true;\n\tif (unlikely(len > skb->len))\n\t\treturn false;\n\treturn __pskb_pull_tail(skb, len - skb_headlen(skb)) != NULL;\n}\n\nvoid skb_condense(struct sk_buff *skb);\n\n/**\n *\tskb_headroom - bytes at buffer head\n *\t@skb: buffer to check\n *\n *\tReturn the number of bytes of free space at the head of an &sk_buff.\n */\nstatic inline unsigned int skb_headroom(const struct sk_buff *skb)\n{\n\treturn skb->data - skb->head;\n}\n\n/**\n *\tskb_tailroom - bytes at buffer end\n *\t@skb: buffer to check\n *\n *\tReturn the number of bytes of free space at the tail of an sk_buff\n */\nstatic inline int skb_tailroom(const struct sk_buff *skb)\n{\n\treturn skb_is_nonlinear(skb) ? 0 : skb->end - skb->tail;\n}\n\n/**\n *\tskb_availroom - bytes at buffer end\n *\t@skb: buffer to check\n *\n *\tReturn the number of bytes of free space at the tail of an sk_buff\n *\tallocated by sk_stream_alloc()\n */\nstatic inline int skb_availroom(const struct sk_buff *skb)\n{\n\tif (skb_is_nonlinear(skb))\n\t\treturn 0;\n\n\treturn skb->end - skb->tail - skb->reserved_tailroom;\n}\n\n/**\n *\tskb_reserve - adjust headroom\n *\t@skb: buffer to alter\n *\t@len: bytes to move\n *\n *\tIncrease the headroom of an empty &sk_buff by reducing the tail\n *\troom. This is only allowed for an empty buffer.\n */\nstatic inline void skb_reserve(struct sk_buff *skb, int len)\n{\n\tskb->data += len;\n\tskb->tail += len;\n}\n\n/**\n *\tskb_tailroom_reserve - adjust reserved_tailroom\n *\t@skb: buffer to alter\n *\t@mtu: maximum amount of headlen permitted\n *\t@needed_tailroom: minimum amount of reserved_tailroom\n *\n *\tSet reserved_tailroom so that headlen can be as large as possible but\n *\tnot larger than mtu and tailroom cannot be smaller than\n *\tneeded_tailroom.\n *\tThe required headroom should already have been reserved before using\n *\tthis function.\n */\nstatic inline void skb_tailroom_reserve(struct sk_buff *skb, unsigned int mtu,\n\t\t\t\t\tunsigned int needed_tailroom)\n{\n\tSKB_LINEAR_ASSERT(skb);\n\tif (mtu < skb_tailroom(skb) - needed_tailroom)\n\t\t/* use at most mtu */\n\t\tskb->reserved_tailroom = skb_tailroom(skb) - mtu;\n\telse\n\t\t/* use up to all available space */\n\t\tskb->reserved_tailroom = needed_tailroom;\n}\n\n#define ENCAP_TYPE_ETHER\t0\n#define ENCAP_TYPE_IPPROTO\t1\n\nstatic inline void skb_set_inner_protocol(struct sk_buff *skb,\n\t\t\t\t\t  __be16 protocol)\n{\n\tskb->inner_protocol = protocol;\n\tskb->inner_protocol_type = ENCAP_TYPE_ETHER;\n}\n\nstatic inline void skb_set_inner_ipproto(struct sk_buff *skb,\n\t\t\t\t\t __u8 ipproto)\n{\n\tskb->inner_ipproto = ipproto;\n\tskb->inner_protocol_type = ENCAP_TYPE_IPPROTO;\n}\n\nstatic inline void skb_reset_inner_headers(struct sk_buff *skb)\n{\n\tskb->inner_mac_header = skb->mac_header;\n\tskb->inner_network_header = skb->network_header;\n\tskb->inner_transport_header = skb->transport_header;\n}\n\nstatic inline void skb_reset_mac_len(struct sk_buff *skb)\n{\n\tskb->mac_len = skb->network_header - skb->mac_header;\n}\n\nstatic inline unsigned char *skb_inner_transport_header(const struct sk_buff\n\t\t\t\t\t\t\t*skb)\n{\n\treturn skb->head + skb->inner_transport_header;\n}\n\nstatic inline int skb_inner_transport_offset(const struct sk_buff *skb)\n{\n\treturn skb_inner_transport_header(skb) - skb->data;\n}\n\nstatic inline void skb_reset_inner_transport_header(struct sk_buff *skb)\n{\n\tskb->inner_transport_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_inner_transport_header(struct sk_buff *skb,\n\t\t\t\t\t\t   const int offset)\n{\n\tskb_reset_inner_transport_header(skb);\n\tskb->inner_transport_header += offset;\n}\n\nstatic inline unsigned char *skb_inner_network_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->inner_network_header;\n}\n\nstatic inline void skb_reset_inner_network_header(struct sk_buff *skb)\n{\n\tskb->inner_network_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_inner_network_header(struct sk_buff *skb,\n\t\t\t\t\t\tconst int offset)\n{\n\tskb_reset_inner_network_header(skb);\n\tskb->inner_network_header += offset;\n}\n\nstatic inline unsigned char *skb_inner_mac_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->inner_mac_header;\n}\n\nstatic inline void skb_reset_inner_mac_header(struct sk_buff *skb)\n{\n\tskb->inner_mac_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_inner_mac_header(struct sk_buff *skb,\n\t\t\t\t\t    const int offset)\n{\n\tskb_reset_inner_mac_header(skb);\n\tskb->inner_mac_header += offset;\n}\nstatic inline bool skb_transport_header_was_set(const struct sk_buff *skb)\n{\n\treturn skb->transport_header != (typeof(skb->transport_header))~0U;\n}\n\nstatic inline unsigned char *skb_transport_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->transport_header;\n}\n\nstatic inline void skb_reset_transport_header(struct sk_buff *skb)\n{\n\tskb->transport_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_transport_header(struct sk_buff *skb,\n\t\t\t\t\t    const int offset)\n{\n\tskb_reset_transport_header(skb);\n\tskb->transport_header += offset;\n}\n\nstatic inline unsigned char *skb_network_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->network_header;\n}\n\nstatic inline void skb_reset_network_header(struct sk_buff *skb)\n{\n\tskb->network_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_network_header(struct sk_buff *skb, const int offset)\n{\n\tskb_reset_network_header(skb);\n\tskb->network_header += offset;\n}\n\nstatic inline unsigned char *skb_mac_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->mac_header;\n}\n\nstatic inline int skb_mac_offset(const struct sk_buff *skb)\n{\n\treturn skb_mac_header(skb) - skb->data;\n}\n\nstatic inline u32 skb_mac_header_len(const struct sk_buff *skb)\n{\n\treturn skb->network_header - skb->mac_header;\n}\n\nstatic inline int skb_mac_header_was_set(const struct sk_buff *skb)\n{\n\treturn skb->mac_header != (typeof(skb->mac_header))~0U;\n}\n\nstatic inline void skb_unset_mac_header(struct sk_buff *skb)\n{\n\tskb->mac_header = (typeof(skb->mac_header))~0U;\n}\n\nstatic inline void skb_reset_mac_header(struct sk_buff *skb)\n{\n\tskb->mac_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_mac_header(struct sk_buff *skb, const int offset)\n{\n\tskb_reset_mac_header(skb);\n\tskb->mac_header += offset;\n}\n\nstatic inline void skb_pop_mac_header(struct sk_buff *skb)\n{\n\tskb->mac_header = skb->network_header;\n}\n\nstatic inline void skb_probe_transport_header(struct sk_buff *skb)\n{\n\tstruct flow_keys_basic keys;\n\n\tif (skb_transport_header_was_set(skb))\n\t\treturn;\n\n\tif (skb_flow_dissect_flow_keys_basic(NULL, skb, &keys,\n\t\t\t\t\t     NULL, 0, 0, 0, 0))\n\t\tskb_set_transport_header(skb, keys.control.thoff);\n}\n\nstatic inline void skb_mac_header_rebuild(struct sk_buff *skb)\n{\n\tif (skb_mac_header_was_set(skb)) {\n\t\tconst unsigned char *old_mac = skb_mac_header(skb);\n\n\t\tskb_set_mac_header(skb, -skb->mac_len);\n\t\tmemmove(skb_mac_header(skb), old_mac, skb->mac_len);\n\t}\n}\n\nstatic inline int skb_checksum_start_offset(const struct sk_buff *skb)\n{\n\treturn skb->csum_start - skb_headroom(skb);\n}\n\nstatic inline unsigned char *skb_checksum_start(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->csum_start;\n}\n\nstatic inline int skb_transport_offset(const struct sk_buff *skb)\n{\n\treturn skb_transport_header(skb) - skb->data;\n}\n\nstatic inline u32 skb_network_header_len(const struct sk_buff *skb)\n{\n\treturn skb->transport_header - skb->network_header;\n}\n\nstatic inline u32 skb_inner_network_header_len(const struct sk_buff *skb)\n{\n\treturn skb->inner_transport_header - skb->inner_network_header;\n}\n\nstatic inline int skb_network_offset(const struct sk_buff *skb)\n{\n\treturn skb_network_header(skb) - skb->data;\n}\n\nstatic inline int skb_inner_network_offset(const struct sk_buff *skb)\n{\n\treturn skb_inner_network_header(skb) - skb->data;\n}\n\nstatic inline int pskb_network_may_pull(struct sk_buff *skb, unsigned int len)\n{\n\treturn pskb_may_pull(skb, skb_network_offset(skb) + len);\n}\n\n/*\n * CPUs often take a performance hit when accessing unaligned memory\n * locations. The actual performance hit varies, it can be small if the\n * hardware handles it or large if we have to take an exception and fix it\n * in software.\n *\n * Since an ethernet header is 14 bytes network drivers often end up with\n * the IP header at an unaligned offset. The IP header can be aligned by\n * shifting the start of the packet by 2 bytes. Drivers should do this\n * with:\n *\n * skb_reserve(skb, NET_IP_ALIGN);\n *\n * The downside to this alignment of the IP header is that the DMA is now\n * unaligned. On some architectures the cost of an unaligned DMA is high\n * and this cost outweighs the gains made by aligning the IP header.\n *\n * Since this trade off varies between architectures, we allow NET_IP_ALIGN\n * to be overridden.\n */\n#ifndef NET_IP_ALIGN\n#define NET_IP_ALIGN\t2\n#endif\n\n/*\n * The networking layer reserves some headroom in skb data (via\n * dev_alloc_skb). This is used to avoid having to reallocate skb data when\n * the header has to grow. In the default case, if the header has to grow\n * 32 bytes or less we avoid the reallocation.\n *\n * Unfortunately this headroom changes the DMA alignment of the resulting\n * network packet. As for NET_IP_ALIGN, this unaligned DMA is expensive\n * on some architectures. An architecture can override this value,\n * perhaps setting it to a cacheline in size (since that will maintain\n * cacheline alignment of the DMA). It must be a power of 2.\n *\n * Various parts of the networking layer expect at least 32 bytes of\n * headroom, you should not reduce this.\n *\n * Using max(32, L1_CACHE_BYTES) makes sense (especially with RPS)\n * to reduce average number of cache lines per packet.\n * get_rps_cpu() for example only access one 64 bytes aligned block :\n * NET_IP_ALIGN(2) + ethernet_header(14) + IP_header(20/40) + ports(8)\n */\n#ifndef NET_SKB_PAD\n#define NET_SKB_PAD\tmax(32, L1_CACHE_BYTES)\n#endif\n\nint ___pskb_trim(struct sk_buff *skb, unsigned int len);\n\nstatic inline void __skb_set_length(struct sk_buff *skb, unsigned int len)\n{\n\tif (WARN_ON(skb_is_nonlinear(skb)))\n\t\treturn;\n\tskb->len = len;\n\tskb_set_tail_pointer(skb, len);\n}\n\nstatic inline void __skb_trim(struct sk_buff *skb, unsigned int len)\n{\n\t__skb_set_length(skb, len);\n}\n\nvoid skb_trim(struct sk_buff *skb, unsigned int len);\n\nstatic inline int __pskb_trim(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->data_len)\n\t\treturn ___pskb_trim(skb, len);\n\t__skb_trim(skb, len);\n\treturn 0;\n}\n\nstatic inline int pskb_trim(struct sk_buff *skb, unsigned int len)\n{\n\treturn (len < skb->len) ? __pskb_trim(skb, len) : 0;\n}\n\n/**\n *\tpskb_trim_unique - remove end from a paged unique (not cloned) buffer\n *\t@skb: buffer to alter\n *\t@len: new length\n *\n *\tThis is identical to pskb_trim except that the caller knows that\n *\tthe skb is not cloned so we should never get an error due to out-\n *\tof-memory.\n */\nstatic inline void pskb_trim_unique(struct sk_buff *skb, unsigned int len)\n{\n\tint err = pskb_trim(skb, len);\n\tBUG_ON(err);\n}\n\nstatic inline int __skb_grow(struct sk_buff *skb, unsigned int len)\n{\n\tunsigned int diff = len - skb->len;\n\n\tif (skb_tailroom(skb) < diff) {\n\t\tint ret = pskb_expand_head(skb, 0, diff - skb_tailroom(skb),\n\t\t\t\t\t   GFP_ATOMIC);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\t__skb_set_length(skb, len);\n\treturn 0;\n}\n\n/**\n *\tskb_orphan - orphan a buffer\n *\t@skb: buffer to orphan\n *\n *\tIf a buffer currently has an owner then we call the owner's\n *\tdestructor function and make the @skb unowned. The buffer continues\n *\tto exist but is no longer charged to its former owner.\n */\nstatic inline void skb_orphan(struct sk_buff *skb)\n{\n\tif (skb->destructor) {\n\t\tskb->destructor(skb);\n\t\tskb->destructor = NULL;\n\t\tskb->sk\t\t= NULL;\n\t} else {\n\t\tBUG_ON(skb->sk);\n\t}\n}\n\n/**\n *\tskb_orphan_frags - orphan the frags contained in a buffer\n *\t@skb: buffer to orphan frags from\n *\t@gfp_mask: allocation mask for replacement pages\n *\n *\tFor each frag in the SKB which needs a destructor (i.e. has an\n *\towner) create a copy of that frag and release the original\n *\tpage by calling the destructor.\n */\nstatic inline int skb_orphan_frags(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tif (likely(!skb_zcopy(skb)))\n\t\treturn 0;\n\tif (!skb_zcopy_is_nouarg(skb) &&\n\t    skb_uarg(skb)->callback == msg_zerocopy_callback)\n\t\treturn 0;\n\treturn skb_copy_ubufs(skb, gfp_mask);\n}\n\n/* Frags must be orphaned, even if refcounted, if skb might loop to rx path */\nstatic inline int skb_orphan_frags_rx(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tif (likely(!skb_zcopy(skb)))\n\t\treturn 0;\n\treturn skb_copy_ubufs(skb, gfp_mask);\n}\n\n/**\n *\t__skb_queue_purge - empty a list\n *\t@list: list to empty\n *\n *\tDelete all buffers on an &sk_buff list. Each buffer is removed from\n *\tthe list and one reference dropped. This function does not take the\n *\tlist lock and the caller must hold the relevant locks to use it.\n */\nstatic inline void __skb_queue_purge(struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb;\n\twhile ((skb = __skb_dequeue(list)) != NULL)\n\t\tkfree_skb(skb);\n}\nvoid skb_queue_purge(struct sk_buff_head *list);\n\nunsigned int skb_rbtree_purge(struct rb_root *root);\n\nvoid *__netdev_alloc_frag_align(unsigned int fragsz, unsigned int align_mask);\n\n/**\n * netdev_alloc_frag - allocate a page fragment\n * @fragsz: fragment size\n *\n * Allocates a frag from a page for receive buffer.\n * Uses GFP_ATOMIC allocations.\n */\nstatic inline void *netdev_alloc_frag(unsigned int fragsz)\n{\n\treturn __netdev_alloc_frag_align(fragsz, ~0u);\n}\n\nstatic inline void *netdev_alloc_frag_align(unsigned int fragsz,\n\t\t\t\t\t    unsigned int align)\n{\n\tWARN_ON_ONCE(!is_power_of_2(align));\n\treturn __netdev_alloc_frag_align(fragsz, -align);\n}\n\nstruct sk_buff *__netdev_alloc_skb(struct net_device *dev, unsigned int length,\n\t\t\t\t   gfp_t gfp_mask);\n\n/**\n *\tnetdev_alloc_skb - allocate an skbuff for rx on a specific device\n *\t@dev: network device to receive on\n *\t@length: length to allocate\n *\n *\tAllocate a new &sk_buff and assign it a usage count of one. The\n *\tbuffer has unspecified headroom built in. Users should allocate\n *\tthe headroom they think they need without accounting for the\n *\tbuilt in space. The built in space is used for optimisations.\n *\n *\t%NULL is returned if there is no free memory. Although this function\n *\tallocates memory it can be called from an interrupt.\n */\nstatic inline struct sk_buff *netdev_alloc_skb(struct net_device *dev,\n\t\t\t\t\t       unsigned int length)\n{\n\treturn __netdev_alloc_skb(dev, length, GFP_ATOMIC);\n}\n\n/* legacy helper around __netdev_alloc_skb() */\nstatic inline struct sk_buff *__dev_alloc_skb(unsigned int length,\n\t\t\t\t\t      gfp_t gfp_mask)\n{\n\treturn __netdev_alloc_skb(NULL, length, gfp_mask);\n}\n\n/* legacy helper around netdev_alloc_skb() */\nstatic inline struct sk_buff *dev_alloc_skb(unsigned int length)\n{\n\treturn netdev_alloc_skb(NULL, length);\n}\n\n\nstatic inline struct sk_buff *__netdev_alloc_skb_ip_align(struct net_device *dev,\n\t\tunsigned int length, gfp_t gfp)\n{\n\tstruct sk_buff *skb = __netdev_alloc_skb(dev, length + NET_IP_ALIGN, gfp);\n\n\tif (NET_IP_ALIGN && skb)\n\t\tskb_reserve(skb, NET_IP_ALIGN);\n\treturn skb;\n}\n\nstatic inline struct sk_buff *netdev_alloc_skb_ip_align(struct net_device *dev,\n\t\tunsigned int length)\n{\n\treturn __netdev_alloc_skb_ip_align(dev, length, GFP_ATOMIC);\n}\n\nstatic inline void skb_free_frag(void *addr)\n{\n\tpage_frag_free(addr);\n}\n\nvoid *__napi_alloc_frag_align(unsigned int fragsz, unsigned int align_mask);\n\nstatic inline void *napi_alloc_frag(unsigned int fragsz)\n{\n\treturn __napi_alloc_frag_align(fragsz, ~0u);\n}\n\nstatic inline void *napi_alloc_frag_align(unsigned int fragsz,\n\t\t\t\t\t  unsigned int align)\n{\n\tWARN_ON_ONCE(!is_power_of_2(align));\n\treturn __napi_alloc_frag_align(fragsz, -align);\n}\n\nstruct sk_buff *__napi_alloc_skb(struct napi_struct *napi,\n\t\t\t\t unsigned int length, gfp_t gfp_mask);\nstatic inline struct sk_buff *napi_alloc_skb(struct napi_struct *napi,\n\t\t\t\t\t     unsigned int length)\n{\n\treturn __napi_alloc_skb(napi, length, GFP_ATOMIC);\n}\nvoid napi_consume_skb(struct sk_buff *skb, int budget);\n\nvoid napi_skb_free_stolen_head(struct sk_buff *skb);\nvoid __kfree_skb_defer(struct sk_buff *skb);\n\n/**\n * __dev_alloc_pages - allocate page for network Rx\n * @gfp_mask: allocation priority. Set __GFP_NOMEMALLOC if not for network Rx\n * @order: size of the allocation\n *\n * Allocate a new page.\n *\n * %NULL is returned if there is no free memory.\n*/\nstatic inline struct page *__dev_alloc_pages(gfp_t gfp_mask,\n\t\t\t\t\t     unsigned int order)\n{\n\t/* This piece of code contains several assumptions.\n\t * 1.  This is for device Rx, therefor a cold page is preferred.\n\t * 2.  The expectation is the user wants a compound page.\n\t * 3.  If requesting a order 0 page it will not be compound\n\t *     due to the check to see if order has a value in prep_new_page\n\t * 4.  __GFP_MEMALLOC is ignored if __GFP_NOMEMALLOC is set due to\n\t *     code in gfp_to_alloc_flags that should be enforcing this.\n\t */\n\tgfp_mask |= __GFP_COMP | __GFP_MEMALLOC;\n\n\treturn alloc_pages_node(NUMA_NO_NODE, gfp_mask, order);\n}\n\nstatic inline struct page *dev_alloc_pages(unsigned int order)\n{\n\treturn __dev_alloc_pages(GFP_ATOMIC | __GFP_NOWARN, order);\n}\n\n/**\n * __dev_alloc_page - allocate a page for network Rx\n * @gfp_mask: allocation priority. Set __GFP_NOMEMALLOC if not for network Rx\n *\n * Allocate a new page.\n *\n * %NULL is returned if there is no free memory.\n */\nstatic inline struct page *__dev_alloc_page(gfp_t gfp_mask)\n{\n\treturn __dev_alloc_pages(gfp_mask, 0);\n}\n\nstatic inline struct page *dev_alloc_page(void)\n{\n\treturn dev_alloc_pages(0);\n}\n\n/**\n * dev_page_is_reusable - check whether a page can be reused for network Rx\n * @page: the page to test\n *\n * A page shouldn't be considered for reusing/recycling if it was allocated\n * under memory pressure or at a distant memory node.\n *\n * Returns false if this page should be returned to page allocator, true\n * otherwise.\n */\nstatic inline bool dev_page_is_reusable(const struct page *page)\n{\n\treturn likely(page_to_nid(page) == numa_mem_id() &&\n\t\t      !page_is_pfmemalloc(page));\n}\n\n/**\n *\tskb_propagate_pfmemalloc - Propagate pfmemalloc if skb is allocated after RX page\n *\t@page: The page that was allocated from skb_alloc_page\n *\t@skb: The skb that may need pfmemalloc set\n */\nstatic inline void skb_propagate_pfmemalloc(const struct page *page,\n\t\t\t\t\t    struct sk_buff *skb)\n{\n\tif (page_is_pfmemalloc(page))\n\t\tskb->pfmemalloc = true;\n}\n\n/**\n * skb_frag_off() - Returns the offset of a skb fragment\n * @frag: the paged fragment\n */\nstatic inline unsigned int skb_frag_off(const skb_frag_t *frag)\n{\n\treturn frag->bv_offset;\n}\n\n/**\n * skb_frag_off_add() - Increments the offset of a skb fragment by @delta\n * @frag: skb fragment\n * @delta: value to add\n */\nstatic inline void skb_frag_off_add(skb_frag_t *frag, int delta)\n{\n\tfrag->bv_offset += delta;\n}\n\n/**\n * skb_frag_off_set() - Sets the offset of a skb fragment\n * @frag: skb fragment\n * @offset: offset of fragment\n */\nstatic inline void skb_frag_off_set(skb_frag_t *frag, unsigned int offset)\n{\n\tfrag->bv_offset = offset;\n}\n\n/**\n * skb_frag_off_copy() - Sets the offset of a skb fragment from another fragment\n * @fragto: skb fragment where offset is set\n * @fragfrom: skb fragment offset is copied from\n */\nstatic inline void skb_frag_off_copy(skb_frag_t *fragto,\n\t\t\t\t     const skb_frag_t *fragfrom)\n{\n\tfragto->bv_offset = fragfrom->bv_offset;\n}\n\n/**\n * skb_frag_page - retrieve the page referred to by a paged fragment\n * @frag: the paged fragment\n *\n * Returns the &struct page associated with @frag.\n */\nstatic inline struct page *skb_frag_page(const skb_frag_t *frag)\n{\n\treturn frag->bv_page;\n}\n\n/**\n * __skb_frag_ref - take an addition reference on a paged fragment.\n * @frag: the paged fragment\n *\n * Takes an additional reference on the paged fragment @frag.\n */\nstatic inline void __skb_frag_ref(skb_frag_t *frag)\n{\n\tget_page(skb_frag_page(frag));\n}\n\n/**\n * skb_frag_ref - take an addition reference on a paged fragment of an skb.\n * @skb: the buffer\n * @f: the fragment offset.\n *\n * Takes an additional reference on the @f'th paged fragment of @skb.\n */\nstatic inline void skb_frag_ref(struct sk_buff *skb, int f)\n{\n\t__skb_frag_ref(&skb_shinfo(skb)->frags[f]);\n}\n\n/**\n * __skb_frag_unref - release a reference on a paged fragment.\n * @frag: the paged fragment\n *\n * Releases a reference on the paged fragment @frag.\n */\nstatic inline void __skb_frag_unref(skb_frag_t *frag)\n{\n\tput_page(skb_frag_page(frag));\n}\n\n/**\n * skb_frag_unref - release a reference on a paged fragment of an skb.\n * @skb: the buffer\n * @f: the fragment offset\n *\n * Releases a reference on the @f'th paged fragment of @skb.\n */\nstatic inline void skb_frag_unref(struct sk_buff *skb, int f)\n{\n\t__skb_frag_unref(&skb_shinfo(skb)->frags[f]);\n}\n\n/**\n * skb_frag_address - gets the address of the data contained in a paged fragment\n * @frag: the paged fragment buffer\n *\n * Returns the address of the data within @frag. The page must already\n * be mapped.\n */\nstatic inline void *skb_frag_address(const skb_frag_t *frag)\n{\n\treturn page_address(skb_frag_page(frag)) + skb_frag_off(frag);\n}\n\n/**\n * skb_frag_address_safe - gets the address of the data contained in a paged fragment\n * @frag: the paged fragment buffer\n *\n * Returns the address of the data within @frag. Checks that the page\n * is mapped and returns %NULL otherwise.\n */\nstatic inline void *skb_frag_address_safe(const skb_frag_t *frag)\n{\n\tvoid *ptr = page_address(skb_frag_page(frag));\n\tif (unlikely(!ptr))\n\t\treturn NULL;\n\n\treturn ptr + skb_frag_off(frag);\n}\n\n/**\n * skb_frag_page_copy() - sets the page in a fragment from another fragment\n * @fragto: skb fragment where page is set\n * @fragfrom: skb fragment page is copied from\n */\nstatic inline void skb_frag_page_copy(skb_frag_t *fragto,\n\t\t\t\t      const skb_frag_t *fragfrom)\n{\n\tfragto->bv_page = fragfrom->bv_page;\n}\n\n/**\n * __skb_frag_set_page - sets the page contained in a paged fragment\n * @frag: the paged fragment\n * @page: the page to set\n *\n * Sets the fragment @frag to contain @page.\n */\nstatic inline void __skb_frag_set_page(skb_frag_t *frag, struct page *page)\n{\n\tfrag->bv_page = page;\n}\n\n/**\n * skb_frag_set_page - sets the page contained in a paged fragment of an skb\n * @skb: the buffer\n * @f: the fragment offset\n * @page: the page to set\n *\n * Sets the @f'th fragment of @skb to contain @page.\n */\nstatic inline void skb_frag_set_page(struct sk_buff *skb, int f,\n\t\t\t\t     struct page *page)\n{\n\t__skb_frag_set_page(&skb_shinfo(skb)->frags[f], page);\n}\n\nbool skb_page_frag_refill(unsigned int sz, struct page_frag *pfrag, gfp_t prio);\n\n/**\n * skb_frag_dma_map - maps a paged fragment via the DMA API\n * @dev: the device to map the fragment to\n * @frag: the paged fragment to map\n * @offset: the offset within the fragment (starting at the\n *          fragment's own offset)\n * @size: the number of bytes to map\n * @dir: the direction of the mapping (``PCI_DMA_*``)\n *\n * Maps the page associated with @frag to @device.\n */\nstatic inline dma_addr_t skb_frag_dma_map(struct device *dev,\n\t\t\t\t\t  const skb_frag_t *frag,\n\t\t\t\t\t  size_t offset, size_t size,\n\t\t\t\t\t  enum dma_data_direction dir)\n{\n\treturn dma_map_page(dev, skb_frag_page(frag),\n\t\t\t    skb_frag_off(frag) + offset, size, dir);\n}\n\nstatic inline struct sk_buff *pskb_copy(struct sk_buff *skb,\n\t\t\t\t\tgfp_t gfp_mask)\n{\n\treturn __pskb_copy(skb, skb_headroom(skb), gfp_mask);\n}\n\n\nstatic inline struct sk_buff *pskb_copy_for_clone(struct sk_buff *skb,\n\t\t\t\t\t\t  gfp_t gfp_mask)\n{\n\treturn __pskb_copy_fclone(skb, skb_headroom(skb), gfp_mask, true);\n}\n\n\n/**\n *\tskb_clone_writable - is the header of a clone writable\n *\t@skb: buffer to check\n *\t@len: length up to which to write\n *\n *\tReturns true if modifying the header part of the cloned buffer\n *\tdoes not requires the data to be copied.\n */\nstatic inline int skb_clone_writable(const struct sk_buff *skb, unsigned int len)\n{\n\treturn !skb_header_cloned(skb) &&\n\t       skb_headroom(skb) + len <= skb->hdr_len;\n}\n\nstatic inline int skb_try_make_writable(struct sk_buff *skb,\n\t\t\t\t\tunsigned int write_len)\n{\n\treturn skb_cloned(skb) && !skb_clone_writable(skb, write_len) &&\n\t       pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n}\n\nstatic inline int __skb_cow(struct sk_buff *skb, unsigned int headroom,\n\t\t\t    int cloned)\n{\n\tint delta = 0;\n\n\tif (headroom > skb_headroom(skb))\n\t\tdelta = headroom - skb_headroom(skb);\n\n\tif (delta || cloned)\n\t\treturn pskb_expand_head(skb, ALIGN(delta, NET_SKB_PAD), 0,\n\t\t\t\t\tGFP_ATOMIC);\n\treturn 0;\n}\n\n/**\n *\tskb_cow - copy header of skb when it is required\n *\t@skb: buffer to cow\n *\t@headroom: needed headroom\n *\n *\tIf the skb passed lacks sufficient headroom or its data part\n *\tis shared, data is reallocated. If reallocation fails, an error\n *\tis returned and original skb is not changed.\n *\n *\tThe result is skb with writable area skb->head...skb->tail\n *\tand at least @headroom of space at head.\n */\nstatic inline int skb_cow(struct sk_buff *skb, unsigned int headroom)\n{\n\treturn __skb_cow(skb, headroom, skb_cloned(skb));\n}\n\n/**\n *\tskb_cow_head - skb_cow but only making the head writable\n *\t@skb: buffer to cow\n *\t@headroom: needed headroom\n *\n *\tThis function is identical to skb_cow except that we replace the\n *\tskb_cloned check by skb_header_cloned.  It should be used when\n *\tyou only need to push on some header and do not need to modify\n *\tthe data.\n */\nstatic inline int skb_cow_head(struct sk_buff *skb, unsigned int headroom)\n{\n\treturn __skb_cow(skb, headroom, skb_header_cloned(skb));\n}\n\n/**\n *\tskb_padto\t- pad an skbuff up to a minimal size\n *\t@skb: buffer to pad\n *\t@len: minimal length\n *\n *\tPads up a buffer to ensure the trailing bytes exist and are\n *\tblanked. If the buffer already contains sufficient data it\n *\tis untouched. Otherwise it is extended. Returns zero on\n *\tsuccess. The skb is freed on error.\n */\nstatic inline int skb_padto(struct sk_buff *skb, unsigned int len)\n{\n\tunsigned int size = skb->len;\n\tif (likely(size >= len))\n\t\treturn 0;\n\treturn skb_pad(skb, len - size);\n}\n\n/**\n *\t__skb_put_padto - increase size and pad an skbuff up to a minimal size\n *\t@skb: buffer to pad\n *\t@len: minimal length\n *\t@free_on_error: free buffer on error\n *\n *\tPads up a buffer to ensure the trailing bytes exist and are\n *\tblanked. If the buffer already contains sufficient data it\n *\tis untouched. Otherwise it is extended. Returns zero on\n *\tsuccess. The skb is freed on error if @free_on_error is true.\n */\nstatic inline int __must_check __skb_put_padto(struct sk_buff *skb,\n\t\t\t\t\t       unsigned int len,\n\t\t\t\t\t       bool free_on_error)\n{\n\tunsigned int size = skb->len;\n\n\tif (unlikely(size < len)) {\n\t\tlen -= size;\n\t\tif (__skb_pad(skb, len, free_on_error))\n\t\t\treturn -ENOMEM;\n\t\t__skb_put(skb, len);\n\t}\n\treturn 0;\n}\n\n/**\n *\tskb_put_padto - increase size and pad an skbuff up to a minimal size\n *\t@skb: buffer to pad\n *\t@len: minimal length\n *\n *\tPads up a buffer to ensure the trailing bytes exist and are\n *\tblanked. If the buffer already contains sufficient data it\n *\tis untouched. Otherwise it is extended. Returns zero on\n *\tsuccess. The skb is freed on error.\n */\nstatic inline int __must_check skb_put_padto(struct sk_buff *skb, unsigned int len)\n{\n\treturn __skb_put_padto(skb, len, true);\n}\n\nstatic inline int skb_add_data(struct sk_buff *skb,\n\t\t\t       struct iov_iter *from, int copy)\n{\n\tconst int off = skb->len;\n\n\tif (skb->ip_summed == CHECKSUM_NONE) {\n\t\t__wsum csum = 0;\n\t\tif (csum_and_copy_from_iter_full(skb_put(skb, copy), copy,\n\t\t\t\t\t         &csum, from)) {\n\t\t\tskb->csum = csum_block_add(skb->csum, csum, off);\n\t\t\treturn 0;\n\t\t}\n\t} else if (copy_from_iter_full(skb_put(skb, copy), copy, from))\n\t\treturn 0;\n\n\t__skb_trim(skb, off);\n\treturn -EFAULT;\n}\n\nstatic inline bool skb_can_coalesce(struct sk_buff *skb, int i,\n\t\t\t\t    const struct page *page, int off)\n{\n\tif (skb_zcopy(skb))\n\t\treturn false;\n\tif (i) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i - 1];\n\n\t\treturn page == skb_frag_page(frag) &&\n\t\t       off == skb_frag_off(frag) + skb_frag_size(frag);\n\t}\n\treturn false;\n}\n\nstatic inline int __skb_linearize(struct sk_buff *skb)\n{\n\treturn __pskb_pull_tail(skb, skb->data_len) ? 0 : -ENOMEM;\n}\n\n/**\n *\tskb_linearize - convert paged skb to linear one\n *\t@skb: buffer to linarize\n *\n *\tIf there is no free memory -ENOMEM is returned, otherwise zero\n *\tis returned and the old skb data released.\n */\nstatic inline int skb_linearize(struct sk_buff *skb)\n{\n\treturn skb_is_nonlinear(skb) ? __skb_linearize(skb) : 0;\n}\n\n/**\n * skb_has_shared_frag - can any frag be overwritten\n * @skb: buffer to test\n *\n * Return true if the skb has at least one frag that might be modified\n * by an external entity (as in vmsplice()/sendfile())\n */\nstatic inline bool skb_has_shared_frag(const struct sk_buff *skb)\n{\n\treturn skb_is_nonlinear(skb) &&\n\t       skb_shinfo(skb)->flags & SKBFL_SHARED_FRAG;\n}\n\n/**\n *\tskb_linearize_cow - make sure skb is linear and writable\n *\t@skb: buffer to process\n *\n *\tIf there is no free memory -ENOMEM is returned, otherwise zero\n *\tis returned and the old skb data released.\n */\nstatic inline int skb_linearize_cow(struct sk_buff *skb)\n{\n\treturn skb_is_nonlinear(skb) || skb_cloned(skb) ?\n\t       __skb_linearize(skb) : 0;\n}\n\nstatic __always_inline void\n__skb_postpull_rcsum(struct sk_buff *skb, const void *start, unsigned int len,\n\t\t     unsigned int off)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->csum = csum_block_sub(skb->csum,\n\t\t\t\t\t   csum_partial(start, len, 0), off);\n\telse if (skb->ip_summed == CHECKSUM_PARTIAL &&\n\t\t skb_checksum_start_offset(skb) < 0)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n}\n\n/**\n *\tskb_postpull_rcsum - update checksum for received skb after pull\n *\t@skb: buffer to update\n *\t@start: start of data before pull\n *\t@len: length of data pulled\n *\n *\tAfter doing a pull on a received packet, you need to call this to\n *\tupdate the CHECKSUM_COMPLETE checksum, or set ip_summed to\n *\tCHECKSUM_NONE so that it can be recomputed from scratch.\n */\nstatic inline void skb_postpull_rcsum(struct sk_buff *skb,\n\t\t\t\t      const void *start, unsigned int len)\n{\n\t__skb_postpull_rcsum(skb, start, len, 0);\n}\n\nstatic __always_inline void\n__skb_postpush_rcsum(struct sk_buff *skb, const void *start, unsigned int len,\n\t\t     unsigned int off)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->csum = csum_block_add(skb->csum,\n\t\t\t\t\t   csum_partial(start, len, 0), off);\n}\n\n/**\n *\tskb_postpush_rcsum - update checksum for received skb after push\n *\t@skb: buffer to update\n *\t@start: start of data after push\n *\t@len: length of data pushed\n *\n *\tAfter doing a push on a received packet, you need to call this to\n *\tupdate the CHECKSUM_COMPLETE checksum.\n */\nstatic inline void skb_postpush_rcsum(struct sk_buff *skb,\n\t\t\t\t      const void *start, unsigned int len)\n{\n\t__skb_postpush_rcsum(skb, start, len, 0);\n}\n\nvoid *skb_pull_rcsum(struct sk_buff *skb, unsigned int len);\n\n/**\n *\tskb_push_rcsum - push skb and update receive checksum\n *\t@skb: buffer to update\n *\t@len: length of data pulled\n *\n *\tThis function performs an skb_push on the packet and updates\n *\tthe CHECKSUM_COMPLETE checksum.  It should be used on\n *\treceive path processing instead of skb_push unless you know\n *\tthat the checksum difference is zero (e.g., a valid IP header)\n *\tor you are setting ip_summed to CHECKSUM_NONE.\n */\nstatic inline void *skb_push_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tskb_push(skb, len);\n\tskb_postpush_rcsum(skb, skb->data, len);\n\treturn skb->data;\n}\n\nint pskb_trim_rcsum_slow(struct sk_buff *skb, unsigned int len);\n/**\n *\tpskb_trim_rcsum - trim received skb and update checksum\n *\t@skb: buffer to trim\n *\t@len: new length\n *\n *\tThis is exactly the same as pskb_trim except that it ensures the\n *\tchecksum of received packets are still valid after the operation.\n *\tIt can change skb pointers.\n */\n\nstatic inline int pskb_trim_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tif (likely(len >= skb->len))\n\t\treturn 0;\n\treturn pskb_trim_rcsum_slow(skb, len);\n}\n\nstatic inline int __skb_trim_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t__skb_trim(skb, len);\n\treturn 0;\n}\n\nstatic inline int __skb_grow_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\treturn __skb_grow(skb, len);\n}\n\n#define rb_to_skb(rb) rb_entry_safe(rb, struct sk_buff, rbnode)\n#define skb_rb_first(root) rb_to_skb(rb_first(root))\n#define skb_rb_last(root)  rb_to_skb(rb_last(root))\n#define skb_rb_next(skb)   rb_to_skb(rb_next(&(skb)->rbnode))\n#define skb_rb_prev(skb)   rb_to_skb(rb_prev(&(skb)->rbnode))\n\n#define skb_queue_walk(queue, skb) \\\n\t\tfor (skb = (queue)->next;\t\t\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = skb->next)\n\n#define skb_queue_walk_safe(queue, skb, tmp)\t\t\t\t\t\\\n\t\tfor (skb = (queue)->next, tmp = skb->next;\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = tmp, tmp = skb->next)\n\n#define skb_queue_walk_from(queue, skb)\t\t\t\t\t\t\\\n\t\tfor (; skb != (struct sk_buff *)(queue);\t\t\t\\\n\t\t     skb = skb->next)\n\n#define skb_rbtree_walk(skb, root)\t\t\t\t\t\t\\\n\t\tfor (skb = skb_rb_first(root); skb != NULL;\t\t\t\\\n\t\t     skb = skb_rb_next(skb))\n\n#define skb_rbtree_walk_from(skb)\t\t\t\t\t\t\\\n\t\tfor (; skb != NULL;\t\t\t\t\t\t\\\n\t\t     skb = skb_rb_next(skb))\n\n#define skb_rbtree_walk_from_safe(skb, tmp)\t\t\t\t\t\\\n\t\tfor (; tmp = skb ? skb_rb_next(skb) : NULL, (skb != NULL);\t\\\n\t\t     skb = tmp)\n\n#define skb_queue_walk_from_safe(queue, skb, tmp)\t\t\t\t\\\n\t\tfor (tmp = skb->next;\t\t\t\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = tmp, tmp = skb->next)\n\n#define skb_queue_reverse_walk(queue, skb) \\\n\t\tfor (skb = (queue)->prev;\t\t\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = skb->prev)\n\n#define skb_queue_reverse_walk_safe(queue, skb, tmp)\t\t\t\t\\\n\t\tfor (skb = (queue)->prev, tmp = skb->prev;\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = tmp, tmp = skb->prev)\n\n#define skb_queue_reverse_walk_from_safe(queue, skb, tmp)\t\t\t\\\n\t\tfor (tmp = skb->prev;\t\t\t\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = tmp, tmp = skb->prev)\n\nstatic inline bool skb_has_frag_list(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->frag_list != NULL;\n}\n\nstatic inline void skb_frag_list_init(struct sk_buff *skb)\n{\n\tskb_shinfo(skb)->frag_list = NULL;\n}\n\n#define skb_walk_frags(skb, iter)\t\\\n\tfor (iter = skb_shinfo(skb)->frag_list; iter; iter = iter->next)\n\n\nint __skb_wait_for_more_packets(struct sock *sk, struct sk_buff_head *queue,\n\t\t\t\tint *err, long *timeo_p,\n\t\t\t\tconst struct sk_buff *skb);\nstruct sk_buff *__skb_try_recv_from_queue(struct sock *sk,\n\t\t\t\t\t  struct sk_buff_head *queue,\n\t\t\t\t\t  unsigned int flags,\n\t\t\t\t\t  int *off, int *err,\n\t\t\t\t\t  struct sk_buff **last);\nstruct sk_buff *__skb_try_recv_datagram(struct sock *sk,\n\t\t\t\t\tstruct sk_buff_head *queue,\n\t\t\t\t\tunsigned int flags, int *off, int *err,\n\t\t\t\t\tstruct sk_buff **last);\nstruct sk_buff *__skb_recv_datagram(struct sock *sk,\n\t\t\t\t    struct sk_buff_head *sk_queue,\n\t\t\t\t    unsigned int flags, int *off, int *err);\nstruct sk_buff *skb_recv_datagram(struct sock *sk, unsigned flags, int noblock,\n\t\t\t\t  int *err);\n__poll_t datagram_poll(struct file *file, struct socket *sock,\n\t\t\t   struct poll_table_struct *wait);\nint skb_copy_datagram_iter(const struct sk_buff *from, int offset,\n\t\t\t   struct iov_iter *to, int size);\nstatic inline int skb_copy_datagram_msg(const struct sk_buff *from, int offset,\n\t\t\t\t\tstruct msghdr *msg, int size)\n{\n\treturn skb_copy_datagram_iter(from, offset, &msg->msg_iter, size);\n}\nint skb_copy_and_csum_datagram_msg(struct sk_buff *skb, int hlen,\n\t\t\t\t   struct msghdr *msg);\nint skb_copy_and_hash_datagram_iter(const struct sk_buff *skb, int offset,\n\t\t\t   struct iov_iter *to, int len,\n\t\t\t   struct ahash_request *hash);\nint skb_copy_datagram_from_iter(struct sk_buff *skb, int offset,\n\t\t\t\t struct iov_iter *from, int len);\nint zerocopy_sg_from_iter(struct sk_buff *skb, struct iov_iter *frm);\nvoid skb_free_datagram(struct sock *sk, struct sk_buff *skb);\nvoid __skb_free_datagram_locked(struct sock *sk, struct sk_buff *skb, int len);\nstatic inline void skb_free_datagram_locked(struct sock *sk,\n\t\t\t\t\t    struct sk_buff *skb)\n{\n\t__skb_free_datagram_locked(sk, skb, 0);\n}\nint skb_kill_datagram(struct sock *sk, struct sk_buff *skb, unsigned int flags);\nint skb_copy_bits(const struct sk_buff *skb, int offset, void *to, int len);\nint skb_store_bits(struct sk_buff *skb, int offset, const void *from, int len);\n__wsum skb_copy_and_csum_bits(const struct sk_buff *skb, int offset, u8 *to,\n\t\t\t      int len);\nint skb_splice_bits(struct sk_buff *skb, struct sock *sk, unsigned int offset,\n\t\t    struct pipe_inode_info *pipe, unsigned int len,\n\t\t    unsigned int flags);\nint skb_send_sock_locked(struct sock *sk, struct sk_buff *skb, int offset,\n\t\t\t int len);\nvoid skb_copy_and_csum_dev(const struct sk_buff *skb, u8 *to);\nunsigned int skb_zerocopy_headlen(const struct sk_buff *from);\nint skb_zerocopy(struct sk_buff *to, struct sk_buff *from,\n\t\t int len, int hlen);\nvoid skb_split(struct sk_buff *skb, struct sk_buff *skb1, const u32 len);\nint skb_shift(struct sk_buff *tgt, struct sk_buff *skb, int shiftlen);\nvoid skb_scrub_packet(struct sk_buff *skb, bool xnet);\nbool skb_gso_validate_network_len(const struct sk_buff *skb, unsigned int mtu);\nbool skb_gso_validate_mac_len(const struct sk_buff *skb, unsigned int len);\nstruct sk_buff *skb_segment(struct sk_buff *skb, netdev_features_t features);\nstruct sk_buff *skb_segment_list(struct sk_buff *skb, netdev_features_t features,\n\t\t\t\t unsigned int offset);\nstruct sk_buff *skb_vlan_untag(struct sk_buff *skb);\nint skb_ensure_writable(struct sk_buff *skb, int write_len);\nint __skb_vlan_pop(struct sk_buff *skb, u16 *vlan_tci);\nint skb_vlan_pop(struct sk_buff *skb);\nint skb_vlan_push(struct sk_buff *skb, __be16 vlan_proto, u16 vlan_tci);\nint skb_eth_pop(struct sk_buff *skb);\nint skb_eth_push(struct sk_buff *skb, const unsigned char *dst,\n\t\t const unsigned char *src);\nint skb_mpls_push(struct sk_buff *skb, __be32 mpls_lse, __be16 mpls_proto,\n\t\t  int mac_len, bool ethernet);\nint skb_mpls_pop(struct sk_buff *skb, __be16 next_proto, int mac_len,\n\t\t bool ethernet);\nint skb_mpls_update_lse(struct sk_buff *skb, __be32 mpls_lse);\nint skb_mpls_dec_ttl(struct sk_buff *skb);\nstruct sk_buff *pskb_extract(struct sk_buff *skb, int off, int to_copy,\n\t\t\t     gfp_t gfp);\n\nstatic inline int memcpy_from_msg(void *data, struct msghdr *msg, int len)\n{\n\treturn copy_from_iter_full(data, len, &msg->msg_iter) ? 0 : -EFAULT;\n}\n\nstatic inline int memcpy_to_msg(struct msghdr *msg, void *data, int len)\n{\n\treturn copy_to_iter(data, len, &msg->msg_iter) == len ? 0 : -EFAULT;\n}\n\nstruct skb_checksum_ops {\n\t__wsum (*update)(const void *mem, int len, __wsum wsum);\n\t__wsum (*combine)(__wsum csum, __wsum csum2, int offset, int len);\n};\n\nextern const struct skb_checksum_ops *crc32c_csum_stub __read_mostly;\n\n__wsum __skb_checksum(const struct sk_buff *skb, int offset, int len,\n\t\t      __wsum csum, const struct skb_checksum_ops *ops);\n__wsum skb_checksum(const struct sk_buff *skb, int offset, int len,\n\t\t    __wsum csum);\n\nstatic inline void * __must_check\n__skb_header_pointer(const struct sk_buff *skb, int offset,\n\t\t     int len, void *data, int hlen, void *buffer)\n{\n\tif (hlen - offset >= len)\n\t\treturn data + offset;\n\n\tif (!skb ||\n\t    skb_copy_bits(skb, offset, buffer, len) < 0)\n\t\treturn NULL;\n\n\treturn buffer;\n}\n\nstatic inline void * __must_check\nskb_header_pointer(const struct sk_buff *skb, int offset, int len, void *buffer)\n{\n\treturn __skb_header_pointer(skb, offset, len, skb->data,\n\t\t\t\t    skb_headlen(skb), buffer);\n}\n\n/**\n *\tskb_needs_linearize - check if we need to linearize a given skb\n *\t\t\t      depending on the given device features.\n *\t@skb: socket buffer to check\n *\t@features: net device features\n *\n *\tReturns true if either:\n *\t1. skb has frag_list and the device doesn't support FRAGLIST, or\n *\t2. skb is fragmented and the device does not support SG.\n */\nstatic inline bool skb_needs_linearize(struct sk_buff *skb,\n\t\t\t\t       netdev_features_t features)\n{\n\treturn skb_is_nonlinear(skb) &&\n\t       ((skb_has_frag_list(skb) && !(features & NETIF_F_FRAGLIST)) ||\n\t\t(skb_shinfo(skb)->nr_frags && !(features & NETIF_F_SG)));\n}\n\nstatic inline void skb_copy_from_linear_data(const struct sk_buff *skb,\n\t\t\t\t\t     void *to,\n\t\t\t\t\t     const unsigned int len)\n{\n\tmemcpy(to, skb->data, len);\n}\n\nstatic inline void skb_copy_from_linear_data_offset(const struct sk_buff *skb,\n\t\t\t\t\t\t    const int offset, void *to,\n\t\t\t\t\t\t    const unsigned int len)\n{\n\tmemcpy(to, skb->data + offset, len);\n}\n\nstatic inline void skb_copy_to_linear_data(struct sk_buff *skb,\n\t\t\t\t\t   const void *from,\n\t\t\t\t\t   const unsigned int len)\n{\n\tmemcpy(skb->data, from, len);\n}\n\nstatic inline void skb_copy_to_linear_data_offset(struct sk_buff *skb,\n\t\t\t\t\t\t  const int offset,\n\t\t\t\t\t\t  const void *from,\n\t\t\t\t\t\t  const unsigned int len)\n{\n\tmemcpy(skb->data + offset, from, len);\n}\n\nvoid skb_init(void);\n\nstatic inline ktime_t skb_get_ktime(const struct sk_buff *skb)\n{\n\treturn skb->tstamp;\n}\n\n/**\n *\tskb_get_timestamp - get timestamp from a skb\n *\t@skb: skb to get stamp from\n *\t@stamp: pointer to struct __kernel_old_timeval to store stamp in\n *\n *\tTimestamps are stored in the skb as offsets to a base timestamp.\n *\tThis function converts the offset back to a struct timeval and stores\n *\tit in stamp.\n */\nstatic inline void skb_get_timestamp(const struct sk_buff *skb,\n\t\t\t\t     struct __kernel_old_timeval *stamp)\n{\n\t*stamp = ns_to_kernel_old_timeval(skb->tstamp);\n}\n\nstatic inline void skb_get_new_timestamp(const struct sk_buff *skb,\n\t\t\t\t\t struct __kernel_sock_timeval *stamp)\n{\n\tstruct timespec64 ts = ktime_to_timespec64(skb->tstamp);\n\n\tstamp->tv_sec = ts.tv_sec;\n\tstamp->tv_usec = ts.tv_nsec / 1000;\n}\n\nstatic inline void skb_get_timestampns(const struct sk_buff *skb,\n\t\t\t\t       struct __kernel_old_timespec *stamp)\n{\n\tstruct timespec64 ts = ktime_to_timespec64(skb->tstamp);\n\n\tstamp->tv_sec = ts.tv_sec;\n\tstamp->tv_nsec = ts.tv_nsec;\n}\n\nstatic inline void skb_get_new_timestampns(const struct sk_buff *skb,\n\t\t\t\t\t   struct __kernel_timespec *stamp)\n{\n\tstruct timespec64 ts = ktime_to_timespec64(skb->tstamp);\n\n\tstamp->tv_sec = ts.tv_sec;\n\tstamp->tv_nsec = ts.tv_nsec;\n}\n\nstatic inline void __net_timestamp(struct sk_buff *skb)\n{\n\tskb->tstamp = ktime_get_real();\n}\n\nstatic inline ktime_t net_timedelta(ktime_t t)\n{\n\treturn ktime_sub(ktime_get_real(), t);\n}\n\nstatic inline ktime_t net_invalid_timestamp(void)\n{\n\treturn 0;\n}\n\nstatic inline u8 skb_metadata_len(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->meta_len;\n}\n\nstatic inline void *skb_metadata_end(const struct sk_buff *skb)\n{\n\treturn skb_mac_header(skb);\n}\n\nstatic inline bool __skb_metadata_differs(const struct sk_buff *skb_a,\n\t\t\t\t\t  const struct sk_buff *skb_b,\n\t\t\t\t\t  u8 meta_len)\n{\n\tconst void *a = skb_metadata_end(skb_a);\n\tconst void *b = skb_metadata_end(skb_b);\n\t/* Using more efficient varaiant than plain call to memcmp(). */\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n\tu64 diffs = 0;\n\n\tswitch (meta_len) {\n#define __it(x, op) (x -= sizeof(u##op))\n#define __it_diff(a, b, op) (*(u##op *)__it(a, op)) ^ (*(u##op *)__it(b, op))\n\tcase 32: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase 24: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase 16: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase  8: diffs |= __it_diff(a, b, 64);\n\t\tbreak;\n\tcase 28: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase 20: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase 12: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase  4: diffs |= __it_diff(a, b, 32);\n\t\tbreak;\n\t}\n\treturn diffs;\n#else\n\treturn memcmp(a - meta_len, b - meta_len, meta_len);\n#endif\n}\n\nstatic inline bool skb_metadata_differs(const struct sk_buff *skb_a,\n\t\t\t\t\tconst struct sk_buff *skb_b)\n{\n\tu8 len_a = skb_metadata_len(skb_a);\n\tu8 len_b = skb_metadata_len(skb_b);\n\n\tif (!(len_a | len_b))\n\t\treturn false;\n\n\treturn len_a != len_b ?\n\t       true : __skb_metadata_differs(skb_a, skb_b, len_a);\n}\n\nstatic inline void skb_metadata_set(struct sk_buff *skb, u8 meta_len)\n{\n\tskb_shinfo(skb)->meta_len = meta_len;\n}\n\nstatic inline void skb_metadata_clear(struct sk_buff *skb)\n{\n\tskb_metadata_set(skb, 0);\n}\n\nstruct sk_buff *skb_clone_sk(struct sk_buff *skb);\n\n#ifdef CONFIG_NETWORK_PHY_TIMESTAMPING\n\nvoid skb_clone_tx_timestamp(struct sk_buff *skb);\nbool skb_defer_rx_timestamp(struct sk_buff *skb);\n\n#else /* CONFIG_NETWORK_PHY_TIMESTAMPING */\n\nstatic inline void skb_clone_tx_timestamp(struct sk_buff *skb)\n{\n}\n\nstatic inline bool skb_defer_rx_timestamp(struct sk_buff *skb)\n{\n\treturn false;\n}\n\n#endif /* !CONFIG_NETWORK_PHY_TIMESTAMPING */\n\n/**\n * skb_complete_tx_timestamp() - deliver cloned skb with tx timestamps\n *\n * PHY drivers may accept clones of transmitted packets for\n * timestamping via their phy_driver.txtstamp method. These drivers\n * must call this function to return the skb back to the stack with a\n * timestamp.\n *\n * @skb: clone of the original outgoing packet\n * @hwtstamps: hardware time stamps\n *\n */\nvoid skb_complete_tx_timestamp(struct sk_buff *skb,\n\t\t\t       struct skb_shared_hwtstamps *hwtstamps);\n\nvoid __skb_tstamp_tx(struct sk_buff *orig_skb, const struct sk_buff *ack_skb,\n\t\t     struct skb_shared_hwtstamps *hwtstamps,\n\t\t     struct sock *sk, int tstype);\n\n/**\n * skb_tstamp_tx - queue clone of skb with send time stamps\n * @orig_skb:\tthe original outgoing packet\n * @hwtstamps:\thardware time stamps, may be NULL if not available\n *\n * If the skb has a socket associated, then this function clones the\n * skb (thus sharing the actual data and optional structures), stores\n * the optional hardware time stamping information (if non NULL) or\n * generates a software time stamp (otherwise), then queues the clone\n * to the error queue of the socket.  Errors are silently ignored.\n */\nvoid skb_tstamp_tx(struct sk_buff *orig_skb,\n\t\t   struct skb_shared_hwtstamps *hwtstamps);\n\n/**\n * skb_tx_timestamp() - Driver hook for transmit timestamping\n *\n * Ethernet MAC Drivers should call this function in their hard_xmit()\n * function immediately before giving the sk_buff to the MAC hardware.\n *\n * Specifically, one should make absolutely sure that this function is\n * called before TX completion of this packet can trigger.  Otherwise\n * the packet could potentially already be freed.\n *\n * @skb: A socket buffer.\n */\nstatic inline void skb_tx_timestamp(struct sk_buff *skb)\n{\n\tskb_clone_tx_timestamp(skb);\n\tif (skb_shinfo(skb)->tx_flags & SKBTX_SW_TSTAMP)\n\t\tskb_tstamp_tx(skb, NULL);\n}\n\n/**\n * skb_complete_wifi_ack - deliver skb with wifi status\n *\n * @skb: the original outgoing packet\n * @acked: ack status\n *\n */\nvoid skb_complete_wifi_ack(struct sk_buff *skb, bool acked);\n\n__sum16 __skb_checksum_complete_head(struct sk_buff *skb, int len);\n__sum16 __skb_checksum_complete(struct sk_buff *skb);\n\nstatic inline int skb_csum_unnecessary(const struct sk_buff *skb)\n{\n\treturn ((skb->ip_summed == CHECKSUM_UNNECESSARY) ||\n\t\tskb->csum_valid ||\n\t\t(skb->ip_summed == CHECKSUM_PARTIAL &&\n\t\t skb_checksum_start_offset(skb) >= 0));\n}\n\n/**\n *\tskb_checksum_complete - Calculate checksum of an entire packet\n *\t@skb: packet to process\n *\n *\tThis function calculates the checksum over the entire packet plus\n *\tthe value of skb->csum.  The latter can be used to supply the\n *\tchecksum of a pseudo header as used by TCP/UDP.  It returns the\n *\tchecksum.\n *\n *\tFor protocols that contain complete checksums such as ICMP/TCP/UDP,\n *\tthis function can be used to verify that checksum on received\n *\tpackets.  In that case the function should return zero if the\n *\tchecksum is correct.  In particular, this function will return zero\n *\tif skb->ip_summed is CHECKSUM_UNNECESSARY which indicates that the\n *\thardware has already verified the correctness of the checksum.\n */\nstatic inline __sum16 skb_checksum_complete(struct sk_buff *skb)\n{\n\treturn skb_csum_unnecessary(skb) ?\n\t       0 : __skb_checksum_complete(skb);\n}\n\nstatic inline void __skb_decr_checksum_unnecessary(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\tif (skb->csum_level == 0)\n\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\telse\n\t\t\tskb->csum_level--;\n\t}\n}\n\nstatic inline void __skb_incr_checksum_unnecessary(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\tif (skb->csum_level < SKB_MAX_CSUM_LEVEL)\n\t\t\tskb->csum_level++;\n\t} else if (skb->ip_summed == CHECKSUM_NONE) {\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\tskb->csum_level = 0;\n\t}\n}\n\nstatic inline void __skb_reset_checksum_unnecessary(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tskb->csum_level = 0;\n\t}\n}\n\n/* Check if we need to perform checksum complete validation.\n *\n * Returns true if checksum complete is needed, false otherwise\n * (either checksum is unnecessary or zero checksum is allowed).\n */\nstatic inline bool __skb_checksum_validate_needed(struct sk_buff *skb,\n\t\t\t\t\t\t  bool zero_okay,\n\t\t\t\t\t\t  __sum16 check)\n{\n\tif (skb_csum_unnecessary(skb) || (zero_okay && !check)) {\n\t\tskb->csum_valid = 1;\n\t\t__skb_decr_checksum_unnecessary(skb);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n/* For small packets <= CHECKSUM_BREAK perform checksum complete directly\n * in checksum_init.\n */\n#define CHECKSUM_BREAK 76\n\n/* Unset checksum-complete\n *\n * Unset checksum complete can be done when packet is being modified\n * (uncompressed for instance) and checksum-complete value is\n * invalidated.\n */\nstatic inline void skb_checksum_complete_unset(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n}\n\n/* Validate (init) checksum based on checksum complete.\n *\n * Return values:\n *   0: checksum is validated or try to in skb_checksum_complete. In the latter\n *\tcase the ip_summed will not be CHECKSUM_UNNECESSARY and the pseudo\n *\tchecksum is stored in skb->csum for use in __skb_checksum_complete\n *   non-zero: value of invalid checksum\n *\n */\nstatic inline __sum16 __skb_checksum_validate_complete(struct sk_buff *skb,\n\t\t\t\t\t\t       bool complete,\n\t\t\t\t\t\t       __wsum psum)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\tif (!csum_fold(csum_add(psum, skb->csum))) {\n\t\t\tskb->csum_valid = 1;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tskb->csum = psum;\n\n\tif (complete || skb->len <= CHECKSUM_BREAK) {\n\t\t__sum16 csum;\n\n\t\tcsum = __skb_checksum_complete(skb);\n\t\tskb->csum_valid = !csum;\n\t\treturn csum;\n\t}\n\n\treturn 0;\n}\n\nstatic inline __wsum null_compute_pseudo(struct sk_buff *skb, int proto)\n{\n\treturn 0;\n}\n\n/* Perform checksum validate (init). Note that this is a macro since we only\n * want to calculate the pseudo header which is an input function if necessary.\n * First we try to validate without any computation (checksum unnecessary) and\n * then calculate based on checksum complete calling the function to compute\n * pseudo header.\n *\n * Return values:\n *   0: checksum is validated or try to in skb_checksum_complete\n *   non-zero: value of invalid checksum\n */\n#define __skb_checksum_validate(skb, proto, complete,\t\t\t\\\n\t\t\t\tzero_okay, check, compute_pseudo)\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__sum16 __ret = 0;\t\t\t\t\t\t\\\n\tskb->csum_valid = 0;\t\t\t\t\t\t\\\n\tif (__skb_checksum_validate_needed(skb, zero_okay, check))\t\\\n\t\t__ret = __skb_checksum_validate_complete(skb,\t\t\\\n\t\t\t\tcomplete, compute_pseudo(skb, proto));\t\\\n\t__ret;\t\t\t\t\t\t\t\t\\\n})\n\n#define skb_checksum_init(skb, proto, compute_pseudo)\t\t\t\\\n\t__skb_checksum_validate(skb, proto, false, false, 0, compute_pseudo)\n\n#define skb_checksum_init_zero_check(skb, proto, check, compute_pseudo)\t\\\n\t__skb_checksum_validate(skb, proto, false, true, check, compute_pseudo)\n\n#define skb_checksum_validate(skb, proto, compute_pseudo)\t\t\\\n\t__skb_checksum_validate(skb, proto, true, false, 0, compute_pseudo)\n\n#define skb_checksum_validate_zero_check(skb, proto, check,\t\t\\\n\t\t\t\t\t compute_pseudo)\t\t\\\n\t__skb_checksum_validate(skb, proto, true, true, check, compute_pseudo)\n\n#define skb_checksum_simple_validate(skb)\t\t\t\t\\\n\t__skb_checksum_validate(skb, 0, true, false, 0, null_compute_pseudo)\n\nstatic inline bool __skb_checksum_convert_check(struct sk_buff *skb)\n{\n\treturn (skb->ip_summed == CHECKSUM_NONE && skb->csum_valid);\n}\n\nstatic inline void __skb_checksum_convert(struct sk_buff *skb, __wsum pseudo)\n{\n\tskb->csum = ~pseudo;\n\tskb->ip_summed = CHECKSUM_COMPLETE;\n}\n\n#define skb_checksum_try_convert(skb, proto, compute_pseudo)\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (__skb_checksum_convert_check(skb))\t\t\t\t\\\n\t\t__skb_checksum_convert(skb, compute_pseudo(skb, proto)); \\\n} while (0)\n\nstatic inline void skb_remcsum_adjust_partial(struct sk_buff *skb, void *ptr,\n\t\t\t\t\t      u16 start, u16 offset)\n{\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\tskb->csum_start = ((unsigned char *)ptr + start) - skb->head;\n\tskb->csum_offset = offset - start;\n}\n\n/* Update skbuf and packet to reflect the remote checksum offload operation.\n * When called, ptr indicates the starting point for skb->csum when\n * ip_summed is CHECKSUM_COMPLETE. If we need create checksum complete\n * here, skb_postpull_rcsum is done so skb->csum start is ptr.\n */\nstatic inline void skb_remcsum_process(struct sk_buff *skb, void *ptr,\n\t\t\t\t       int start, int offset, bool nopartial)\n{\n\t__wsum delta;\n\n\tif (!nopartial) {\n\t\tskb_remcsum_adjust_partial(skb, ptr, start, offset);\n\t\treturn;\n\t}\n\n\t if (unlikely(skb->ip_summed != CHECKSUM_COMPLETE)) {\n\t\t__skb_checksum_complete(skb);\n\t\tskb_postpull_rcsum(skb, skb->data, ptr - (void *)skb->data);\n\t}\n\n\tdelta = remcsum_adjust(ptr, skb->csum, start, offset);\n\n\t/* Adjust skb->csum since we changed the packet */\n\tskb->csum = csum_add(skb->csum, delta);\n}\n\nstatic inline struct nf_conntrack *skb_nfct(const struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\treturn (void *)(skb->_nfct & NFCT_PTRMASK);\n#else\n\treturn NULL;\n#endif\n}\n\nstatic inline unsigned long skb_get_nfct(const struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\treturn skb->_nfct;\n#else\n\treturn 0UL;\n#endif\n}\n\nstatic inline void skb_set_nfct(struct sk_buff *skb, unsigned long nfct)\n{\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\tskb->_nfct = nfct;\n#endif\n}\n\n#ifdef CONFIG_SKB_EXTENSIONS\nenum skb_ext_id {\n#if IS_ENABLED(CONFIG_BRIDGE_NETFILTER)\n\tSKB_EXT_BRIDGE_NF,\n#endif\n#ifdef CONFIG_XFRM\n\tSKB_EXT_SEC_PATH,\n#endif\n#if IS_ENABLED(CONFIG_NET_TC_SKB_EXT)\n\tTC_SKB_EXT,\n#endif\n#if IS_ENABLED(CONFIG_MPTCP)\n\tSKB_EXT_MPTCP,\n#endif\n\tSKB_EXT_NUM, /* must be last */\n};\n\n/**\n *\tstruct skb_ext - sk_buff extensions\n *\t@refcnt: 1 on allocation, deallocated on 0\n *\t@offset: offset to add to @data to obtain extension address\n *\t@chunks: size currently allocated, stored in SKB_EXT_ALIGN_SHIFT units\n *\t@data: start of extension data, variable sized\n *\n *\tNote: offsets/lengths are stored in chunks of 8 bytes, this allows\n *\tto use 'u8' types while allowing up to 2kb worth of extension data.\n */\nstruct skb_ext {\n\trefcount_t refcnt;\n\tu8 offset[SKB_EXT_NUM]; /* in chunks of 8 bytes */\n\tu8 chunks;\t\t/* same */\n\tchar data[] __aligned(8);\n};\n\nstruct skb_ext *__skb_ext_alloc(gfp_t flags);\nvoid *__skb_ext_set(struct sk_buff *skb, enum skb_ext_id id,\n\t\t    struct skb_ext *ext);\nvoid *skb_ext_add(struct sk_buff *skb, enum skb_ext_id id);\nvoid __skb_ext_del(struct sk_buff *skb, enum skb_ext_id id);\nvoid __skb_ext_put(struct skb_ext *ext);\n\nstatic inline void skb_ext_put(struct sk_buff *skb)\n{\n\tif (skb->active_extensions)\n\t\t__skb_ext_put(skb->extensions);\n}\n\nstatic inline void __skb_ext_copy(struct sk_buff *dst,\n\t\t\t\t  const struct sk_buff *src)\n{\n\tdst->active_extensions = src->active_extensions;\n\n\tif (src->active_extensions) {\n\t\tstruct skb_ext *ext = src->extensions;\n\n\t\trefcount_inc(&ext->refcnt);\n\t\tdst->extensions = ext;\n\t}\n}\n\nstatic inline void skb_ext_copy(struct sk_buff *dst, const struct sk_buff *src)\n{\n\tskb_ext_put(dst);\n\t__skb_ext_copy(dst, src);\n}\n\nstatic inline bool __skb_ext_exist(const struct skb_ext *ext, enum skb_ext_id i)\n{\n\treturn !!ext->offset[i];\n}\n\nstatic inline bool skb_ext_exist(const struct sk_buff *skb, enum skb_ext_id id)\n{\n\treturn skb->active_extensions & (1 << id);\n}\n\nstatic inline void skb_ext_del(struct sk_buff *skb, enum skb_ext_id id)\n{\n\tif (skb_ext_exist(skb, id))\n\t\t__skb_ext_del(skb, id);\n}\n\nstatic inline void *skb_ext_find(const struct sk_buff *skb, enum skb_ext_id id)\n{\n\tif (skb_ext_exist(skb, id)) {\n\t\tstruct skb_ext *ext = skb->extensions;\n\n\t\treturn (void *)ext + (ext->offset[id] << 3);\n\t}\n\n\treturn NULL;\n}\n\nstatic inline void skb_ext_reset(struct sk_buff *skb)\n{\n\tif (unlikely(skb->active_extensions)) {\n\t\t__skb_ext_put(skb->extensions);\n\t\tskb->active_extensions = 0;\n\t}\n}\n\nstatic inline bool skb_has_extensions(struct sk_buff *skb)\n{\n\treturn unlikely(skb->active_extensions);\n}\n#else\nstatic inline void skb_ext_put(struct sk_buff *skb) {}\nstatic inline void skb_ext_reset(struct sk_buff *skb) {}\nstatic inline void skb_ext_del(struct sk_buff *skb, int unused) {}\nstatic inline void __skb_ext_copy(struct sk_buff *d, const struct sk_buff *s) {}\nstatic inline void skb_ext_copy(struct sk_buff *dst, const struct sk_buff *s) {}\nstatic inline bool skb_has_extensions(struct sk_buff *skb) { return false; }\n#endif /* CONFIG_SKB_EXTENSIONS */\n\nstatic inline void nf_reset_ct(struct sk_buff *skb)\n{\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tnf_conntrack_put(skb_nfct(skb));\n\tskb->_nfct = 0;\n#endif\n}\n\nstatic inline void nf_reset_trace(struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE) || defined(CONFIG_NF_TABLES)\n\tskb->nf_trace = 0;\n#endif\n}\n\nstatic inline void ipvs_reset(struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_IP_VS)\n\tskb->ipvs_property = 0;\n#endif\n}\n\n/* Note: This doesn't put any conntrack info in dst. */\nstatic inline void __nf_copy(struct sk_buff *dst, const struct sk_buff *src,\n\t\t\t     bool copy)\n{\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tdst->_nfct = src->_nfct;\n\tnf_conntrack_get(skb_nfct(src));\n#endif\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE) || defined(CONFIG_NF_TABLES)\n\tif (copy)\n\t\tdst->nf_trace = src->nf_trace;\n#endif\n}\n\nstatic inline void nf_copy(struct sk_buff *dst, const struct sk_buff *src)\n{\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tnf_conntrack_put(skb_nfct(dst));\n#endif\n\t__nf_copy(dst, src, true);\n}\n\n#ifdef CONFIG_NETWORK_SECMARK\nstatic inline void skb_copy_secmark(struct sk_buff *to, const struct sk_buff *from)\n{\n\tto->secmark = from->secmark;\n}\n\nstatic inline void skb_init_secmark(struct sk_buff *skb)\n{\n\tskb->secmark = 0;\n}\n#else\nstatic inline void skb_copy_secmark(struct sk_buff *to, const struct sk_buff *from)\n{ }\n\nstatic inline void skb_init_secmark(struct sk_buff *skb)\n{ }\n#endif\n\nstatic inline int secpath_exists(const struct sk_buff *skb)\n{\n#ifdef CONFIG_XFRM\n\treturn skb_ext_exist(skb, SKB_EXT_SEC_PATH);\n#else\n\treturn 0;\n#endif\n}\n\nstatic inline bool skb_irq_freeable(const struct sk_buff *skb)\n{\n\treturn !skb->destructor &&\n\t\t!secpath_exists(skb) &&\n\t\t!skb_nfct(skb) &&\n\t\t!skb->_skb_refdst &&\n\t\t!skb_has_frag_list(skb);\n}\n\nstatic inline void skb_set_queue_mapping(struct sk_buff *skb, u16 queue_mapping)\n{\n\tskb->queue_mapping = queue_mapping;\n}\n\nstatic inline u16 skb_get_queue_mapping(const struct sk_buff *skb)\n{\n\treturn skb->queue_mapping;\n}\n\nstatic inline void skb_copy_queue_mapping(struct sk_buff *to, const struct sk_buff *from)\n{\n\tto->queue_mapping = from->queue_mapping;\n}\n\nstatic inline void skb_record_rx_queue(struct sk_buff *skb, u16 rx_queue)\n{\n\tskb->queue_mapping = rx_queue + 1;\n}\n\nstatic inline u16 skb_get_rx_queue(const struct sk_buff *skb)\n{\n\treturn skb->queue_mapping - 1;\n}\n\nstatic inline bool skb_rx_queue_recorded(const struct sk_buff *skb)\n{\n\treturn skb->queue_mapping != 0;\n}\n\nstatic inline void skb_set_dst_pending_confirm(struct sk_buff *skb, u32 val)\n{\n\tskb->dst_pending_confirm = val;\n}\n\nstatic inline bool skb_get_dst_pending_confirm(const struct sk_buff *skb)\n{\n\treturn skb->dst_pending_confirm != 0;\n}\n\nstatic inline struct sec_path *skb_sec_path(const struct sk_buff *skb)\n{\n#ifdef CONFIG_XFRM\n\treturn skb_ext_find(skb, SKB_EXT_SEC_PATH);\n#else\n\treturn NULL;\n#endif\n}\n\n/* Keeps track of mac header offset relative to skb->head.\n * It is useful for TSO of Tunneling protocol. e.g. GRE.\n * For non-tunnel skb it points to skb_mac_header() and for\n * tunnel skb it points to outer mac header.\n * Keeps track of level of encapsulation of network headers.\n */\nstruct skb_gso_cb {\n\tunion {\n\t\tint\tmac_offset;\n\t\tint\tdata_offset;\n\t};\n\tint\tencap_level;\n\t__wsum\tcsum;\n\t__u16\tcsum_start;\n};\n#define SKB_GSO_CB_OFFSET\t32\n#define SKB_GSO_CB(skb) ((struct skb_gso_cb *)((skb)->cb + SKB_GSO_CB_OFFSET))\n\nstatic inline int skb_tnl_header_len(const struct sk_buff *inner_skb)\n{\n\treturn (skb_mac_header(inner_skb) - inner_skb->head) -\n\t\tSKB_GSO_CB(inner_skb)->mac_offset;\n}\n\nstatic inline int gso_pskb_expand_head(struct sk_buff *skb, int extra)\n{\n\tint new_headroom, headroom;\n\tint ret;\n\n\theadroom = skb_headroom(skb);\n\tret = pskb_expand_head(skb, extra, 0, GFP_ATOMIC);\n\tif (ret)\n\t\treturn ret;\n\n\tnew_headroom = skb_headroom(skb);\n\tSKB_GSO_CB(skb)->mac_offset += (new_headroom - headroom);\n\treturn 0;\n}\n\nstatic inline void gso_reset_checksum(struct sk_buff *skb, __wsum res)\n{\n\t/* Do not update partial checksums if remote checksum is enabled. */\n\tif (skb->remcsum_offload)\n\t\treturn;\n\n\tSKB_GSO_CB(skb)->csum = res;\n\tSKB_GSO_CB(skb)->csum_start = skb_checksum_start(skb) - skb->head;\n}\n\n/* Compute the checksum for a gso segment. First compute the checksum value\n * from the start of transport header to SKB_GSO_CB(skb)->csum_start, and\n * then add in skb->csum (checksum from csum_start to end of packet).\n * skb->csum and csum_start are then updated to reflect the checksum of the\n * resultant packet starting from the transport header-- the resultant checksum\n * is in the res argument (i.e. normally zero or ~ of checksum of a pseudo\n * header.\n */\nstatic inline __sum16 gso_make_checksum(struct sk_buff *skb, __wsum res)\n{\n\tunsigned char *csum_start = skb_transport_header(skb);\n\tint plen = (skb->head + SKB_GSO_CB(skb)->csum_start) - csum_start;\n\t__wsum partial = SKB_GSO_CB(skb)->csum;\n\n\tSKB_GSO_CB(skb)->csum = res;\n\tSKB_GSO_CB(skb)->csum_start = csum_start - skb->head;\n\n\treturn csum_fold(csum_partial(csum_start, plen, partial));\n}\n\nstatic inline bool skb_is_gso(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->gso_size;\n}\n\n/* Note: Should be called only if skb_is_gso(skb) is true */\nstatic inline bool skb_is_gso_v6(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->gso_type & SKB_GSO_TCPV6;\n}\n\n/* Note: Should be called only if skb_is_gso(skb) is true */\nstatic inline bool skb_is_gso_sctp(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->gso_type & SKB_GSO_SCTP;\n}\n\n/* Note: Should be called only if skb_is_gso(skb) is true */\nstatic inline bool skb_is_gso_tcp(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6);\n}\n\nstatic inline void skb_gso_reset(struct sk_buff *skb)\n{\n\tskb_shinfo(skb)->gso_size = 0;\n\tskb_shinfo(skb)->gso_segs = 0;\n\tskb_shinfo(skb)->gso_type = 0;\n}\n\nstatic inline void skb_increase_gso_size(struct skb_shared_info *shinfo,\n\t\t\t\t\t u16 increment)\n{\n\tif (WARN_ON_ONCE(shinfo->gso_size == GSO_BY_FRAGS))\n\t\treturn;\n\tshinfo->gso_size += increment;\n}\n\nstatic inline void skb_decrease_gso_size(struct skb_shared_info *shinfo,\n\t\t\t\t\t u16 decrement)\n{\n\tif (WARN_ON_ONCE(shinfo->gso_size == GSO_BY_FRAGS))\n\t\treturn;\n\tshinfo->gso_size -= decrement;\n}\n\nvoid __skb_warn_lro_forwarding(const struct sk_buff *skb);\n\nstatic inline bool skb_warn_if_lro(const struct sk_buff *skb)\n{\n\t/* LRO sets gso_size but not gso_type, whereas if GSO is really\n\t * wanted then gso_type will be set. */\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\tif (skb_is_nonlinear(skb) && shinfo->gso_size != 0 &&\n\t    unlikely(shinfo->gso_type == 0)) {\n\t\t__skb_warn_lro_forwarding(skb);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline void skb_forward_csum(struct sk_buff *skb)\n{\n\t/* Unfortunately we don't support this one.  Any brave souls? */\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n}\n\n/**\n * skb_checksum_none_assert - make sure skb ip_summed is CHECKSUM_NONE\n * @skb: skb to check\n *\n * fresh skbs have their ip_summed set to CHECKSUM_NONE.\n * Instead of forcing ip_summed to CHECKSUM_NONE, we can\n * use this helper, to document places where we make this assertion.\n */\nstatic inline void skb_checksum_none_assert(const struct sk_buff *skb)\n{\n#ifdef DEBUG\n\tBUG_ON(skb->ip_summed != CHECKSUM_NONE);\n#endif\n}\n\nbool skb_partial_csum_set(struct sk_buff *skb, u16 start, u16 off);\n\nint skb_checksum_setup(struct sk_buff *skb, bool recalculate);\nstruct sk_buff *skb_checksum_trimmed(struct sk_buff *skb,\n\t\t\t\t     unsigned int transport_len,\n\t\t\t\t     __sum16(*skb_chkf)(struct sk_buff *skb));\n\n/**\n * skb_head_is_locked - Determine if the skb->head is locked down\n * @skb: skb to check\n *\n * The head on skbs build around a head frag can be removed if they are\n * not cloned.  This function returns true if the skb head is locked down\n * due to either being allocated via kmalloc, or by being a clone with\n * multiple references to the head.\n */\nstatic inline bool skb_head_is_locked(const struct sk_buff *skb)\n{\n\treturn !skb->head_frag || skb_cloned(skb);\n}\n\n/* Local Checksum Offload.\n * Compute outer checksum based on the assumption that the\n * inner checksum will be offloaded later.\n * See Documentation/networking/checksum-offloads.rst for\n * explanation of how this works.\n * Fill in outer checksum adjustment (e.g. with sum of outer\n * pseudo-header) before calling.\n * Also ensure that inner checksum is in linear data area.\n */\nstatic inline __wsum lco_csum(struct sk_buff *skb)\n{\n\tunsigned char *csum_start = skb_checksum_start(skb);\n\tunsigned char *l4_hdr = skb_transport_header(skb);\n\t__wsum partial;\n\n\t/* Start with complement of inner checksum adjustment */\n\tpartial = ~csum_unfold(*(__force __sum16 *)(csum_start +\n\t\t\t\t\t\t    skb->csum_offset));\n\n\t/* Add in checksum of our headers (incl. outer checksum\n\t * adjustment filled in by caller) and return result.\n\t */\n\treturn csum_partial(l4_hdr, csum_start - l4_hdr, partial);\n}\n\nstatic inline bool skb_is_redirected(const struct sk_buff *skb)\n{\n#ifdef CONFIG_NET_REDIRECT\n\treturn skb->redirected;\n#else\n\treturn false;\n#endif\n}\n\nstatic inline void skb_set_redirected(struct sk_buff *skb, bool from_ingress)\n{\n#ifdef CONFIG_NET_REDIRECT\n\tskb->redirected = 1;\n\tskb->from_ingress = from_ingress;\n\tif (skb->from_ingress)\n\t\tskb->tstamp = 0;\n#endif\n}\n\nstatic inline void skb_reset_redirect(struct sk_buff *skb)\n{\n#ifdef CONFIG_NET_REDIRECT\n\tskb->redirected = 0;\n#endif\n}\n\nstatic inline bool skb_csum_is_sctp(struct sk_buff *skb)\n{\n\treturn skb->csum_not_inet;\n}\n\nstatic inline void skb_set_kcov_handle(struct sk_buff *skb,\n\t\t\t\t       const u64 kcov_handle)\n{\n#ifdef CONFIG_KCOV\n\tskb->kcov_handle = kcov_handle;\n#endif\n}\n\nstatic inline u64 skb_get_kcov_handle(struct sk_buff *skb)\n{\n#ifdef CONFIG_KCOV\n\treturn skb->kcov_handle;\n#else\n\treturn 0;\n#endif\n}\n\n#endif\t/* __KERNEL__ */\n#endif\t/* _LINUX_SKBUFF_H */\n"}, "2": {"id": 2, "path": "/src/net/core/flow_dissector.c", "content": "// SPDX-License-Identifier: GPL-2.0-only\n#include <linux/kernel.h>\n#include <linux/skbuff.h>\n#include <linux/export.h>\n#include <linux/ip.h>\n#include <linux/ipv6.h>\n#include <linux/if_vlan.h>\n#include <net/dsa.h>\n#include <net/dst_metadata.h>\n#include <net/ip.h>\n#include <net/ipv6.h>\n#include <net/gre.h>\n#include <net/pptp.h>\n#include <net/tipc.h>\n#include <linux/igmp.h>\n#include <linux/icmp.h>\n#include <linux/sctp.h>\n#include <linux/dccp.h>\n#include <linux/if_tunnel.h>\n#include <linux/if_pppox.h>\n#include <linux/ppp_defs.h>\n#include <linux/stddef.h>\n#include <linux/if_ether.h>\n#include <linux/mpls.h>\n#include <linux/tcp.h>\n#include <linux/ptp_classify.h>\n#include <net/flow_dissector.h>\n#include <scsi/fc/fc_fcoe.h>\n#include <uapi/linux/batadv_packet.h>\n#include <linux/bpf.h>\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n#include <net/netfilter/nf_conntrack_core.h>\n#include <net/netfilter/nf_conntrack_labels.h>\n#endif\n#include <linux/bpf-netns.h>\n\nstatic void dissector_set_key(struct flow_dissector *flow_dissector,\n\t\t\t      enum flow_dissector_key_id key_id)\n{\n\tflow_dissector->used_keys |= (1 << key_id);\n}\n\nvoid skb_flow_dissector_init(struct flow_dissector *flow_dissector,\n\t\t\t     const struct flow_dissector_key *key,\n\t\t\t     unsigned int key_count)\n{\n\tunsigned int i;\n\n\tmemset(flow_dissector, 0, sizeof(*flow_dissector));\n\n\tfor (i = 0; i < key_count; i++, key++) {\n\t\t/* User should make sure that every key target offset is within\n\t\t * boundaries of unsigned short.\n\t\t */\n\t\tBUG_ON(key->offset > USHRT_MAX);\n\t\tBUG_ON(dissector_uses_key(flow_dissector,\n\t\t\t\t\t  key->key_id));\n\n\t\tdissector_set_key(flow_dissector, key->key_id);\n\t\tflow_dissector->offset[key->key_id] = key->offset;\n\t}\n\n\t/* Ensure that the dissector always includes control and basic key.\n\t * That way we are able to avoid handling lack of these in fast path.\n\t */\n\tBUG_ON(!dissector_uses_key(flow_dissector,\n\t\t\t\t   FLOW_DISSECTOR_KEY_CONTROL));\n\tBUG_ON(!dissector_uses_key(flow_dissector,\n\t\t\t\t   FLOW_DISSECTOR_KEY_BASIC));\n}\nEXPORT_SYMBOL(skb_flow_dissector_init);\n\n#ifdef CONFIG_BPF_SYSCALL\nint flow_dissector_bpf_prog_attach_check(struct net *net,\n\t\t\t\t\t struct bpf_prog *prog)\n{\n\tenum netns_bpf_attach_type type = NETNS_BPF_FLOW_DISSECTOR;\n\n\tif (net == &init_net) {\n\t\t/* BPF flow dissector in the root namespace overrides\n\t\t * any per-net-namespace one. When attaching to root,\n\t\t * make sure we don't have any BPF program attached\n\t\t * to the non-root namespaces.\n\t\t */\n\t\tstruct net *ns;\n\n\t\tfor_each_net(ns) {\n\t\t\tif (ns == &init_net)\n\t\t\t\tcontinue;\n\t\t\tif (rcu_access_pointer(ns->bpf.run_array[type]))\n\t\t\t\treturn -EEXIST;\n\t\t}\n\t} else {\n\t\t/* Make sure root flow dissector is not attached\n\t\t * when attaching to the non-root namespace.\n\t\t */\n\t\tif (rcu_access_pointer(init_net.bpf.run_array[type]))\n\t\t\treturn -EEXIST;\n\t}\n\n\treturn 0;\n}\n#endif /* CONFIG_BPF_SYSCALL */\n\n/**\n * __skb_flow_get_ports - extract the upper layer ports and return them\n * @skb: sk_buff to extract the ports from\n * @thoff: transport header offset\n * @ip_proto: protocol for which to get port offset\n * @data: raw buffer pointer to the packet, if NULL use skb->data\n * @hlen: packet header length, if @data is NULL use skb_headlen(skb)\n *\n * The function will try to retrieve the ports at offset thoff + poff where poff\n * is the protocol port offset returned from proto_ports_offset\n */\n__be32 __skb_flow_get_ports(const struct sk_buff *skb, int thoff, u8 ip_proto,\n\t\t\t    void *data, int hlen)\n{\n\tint poff = proto_ports_offset(ip_proto);\n\n\tif (!data) {\n\t\tdata = skb->data;\n\t\thlen = skb_headlen(skb);\n\t}\n\n\tif (poff >= 0) {\n\t\t__be32 *ports, _ports;\n\n\t\tports = __skb_header_pointer(skb, thoff + poff,\n\t\t\t\t\t     sizeof(_ports), data, hlen, &_ports);\n\t\tif (ports)\n\t\t\treturn *ports;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(__skb_flow_get_ports);\n\nstatic bool icmp_has_id(u8 type)\n{\n\tswitch (type) {\n\tcase ICMP_ECHO:\n\tcase ICMP_ECHOREPLY:\n\tcase ICMP_TIMESTAMP:\n\tcase ICMP_TIMESTAMPREPLY:\n\tcase ICMPV6_ECHO_REQUEST:\n\tcase ICMPV6_ECHO_REPLY:\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/**\n * skb_flow_get_icmp_tci - extract ICMP(6) Type, Code and Identifier fields\n * @skb: sk_buff to extract from\n * @key_icmp: struct flow_dissector_key_icmp to fill\n * @data: raw buffer pointer to the packet\n * @thoff: offset to extract at\n * @hlen: packet header length\n */\nvoid skb_flow_get_icmp_tci(const struct sk_buff *skb,\n\t\t\t   struct flow_dissector_key_icmp *key_icmp,\n\t\t\t   void *data, int thoff, int hlen)\n{\n\tstruct icmphdr *ih, _ih;\n\n\tih = __skb_header_pointer(skb, thoff, sizeof(_ih), data, hlen, &_ih);\n\tif (!ih)\n\t\treturn;\n\n\tkey_icmp->type = ih->type;\n\tkey_icmp->code = ih->code;\n\n\t/* As we use 0 to signal that the Id field is not present,\n\t * avoid confusion with packets without such field\n\t */\n\tif (icmp_has_id(ih->type))\n\t\tkey_icmp->id = ih->un.echo.id ? : 1;\n\telse\n\t\tkey_icmp->id = 0;\n}\nEXPORT_SYMBOL(skb_flow_get_icmp_tci);\n\n/* If FLOW_DISSECTOR_KEY_ICMP is set, dissect an ICMP packet\n * using skb_flow_get_icmp_tci().\n */\nstatic void __skb_flow_dissect_icmp(const struct sk_buff *skb,\n\t\t\t\t    struct flow_dissector *flow_dissector,\n\t\t\t\t    void *target_container,\n\t\t\t\t    void *data, int thoff, int hlen)\n{\n\tstruct flow_dissector_key_icmp *key_icmp;\n\n\tif (!dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_ICMP))\n\t\treturn;\n\n\tkey_icmp = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t     FLOW_DISSECTOR_KEY_ICMP,\n\t\t\t\t\t     target_container);\n\n\tskb_flow_get_icmp_tci(skb, key_icmp, data, thoff, hlen);\n}\n\nvoid skb_flow_dissect_meta(const struct sk_buff *skb,\n\t\t\t   struct flow_dissector *flow_dissector,\n\t\t\t   void *target_container)\n{\n\tstruct flow_dissector_key_meta *meta;\n\n\tif (!dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_META))\n\t\treturn;\n\n\tmeta = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t FLOW_DISSECTOR_KEY_META,\n\t\t\t\t\t target_container);\n\tmeta->ingress_ifindex = skb->skb_iif;\n}\nEXPORT_SYMBOL(skb_flow_dissect_meta);\n\nstatic void\nskb_flow_dissect_set_enc_addr_type(enum flow_dissector_key_id type,\n\t\t\t\t   struct flow_dissector *flow_dissector,\n\t\t\t\t   void *target_container)\n{\n\tstruct flow_dissector_key_control *ctrl;\n\n\tif (!dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_ENC_CONTROL))\n\t\treturn;\n\n\tctrl = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t FLOW_DISSECTOR_KEY_ENC_CONTROL,\n\t\t\t\t\t target_container);\n\tctrl->addr_type = type;\n}\n\nvoid\nskb_flow_dissect_ct(const struct sk_buff *skb,\n\t\t    struct flow_dissector *flow_dissector,\n\t\t    void *target_container, u16 *ctinfo_map,\n\t\t    size_t mapsize, bool post_ct)\n{\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\tstruct flow_dissector_key_ct *key;\n\tenum ip_conntrack_info ctinfo;\n\tstruct nf_conn_labels *cl;\n\tstruct nf_conn *ct;\n\n\tif (!dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_CT))\n\t\treturn;\n\n\tct = nf_ct_get(skb, &ctinfo);\n\tif (!ct && !post_ct)\n\t\treturn;\n\n\tkey = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\tFLOW_DISSECTOR_KEY_CT,\n\t\t\t\t\ttarget_container);\n\n\tif (!ct) {\n\t\tkey->ct_state = TCA_FLOWER_KEY_CT_FLAGS_TRACKED |\n\t\t\t\tTCA_FLOWER_KEY_CT_FLAGS_INVALID;\n\t\treturn;\n\t}\n\n\tif (ctinfo < mapsize)\n\t\tkey->ct_state = ctinfo_map[ctinfo];\n#if IS_ENABLED(CONFIG_NF_CONNTRACK_ZONES)\n\tkey->ct_zone = ct->zone.id;\n#endif\n#if IS_ENABLED(CONFIG_NF_CONNTRACK_MARK)\n\tkey->ct_mark = ct->mark;\n#endif\n\n\tcl = nf_ct_labels_find(ct);\n\tif (cl)\n\t\tmemcpy(key->ct_labels, cl->bits, sizeof(key->ct_labels));\n#endif /* CONFIG_NF_CONNTRACK */\n}\nEXPORT_SYMBOL(skb_flow_dissect_ct);\n\nvoid\nskb_flow_dissect_tunnel_info(const struct sk_buff *skb,\n\t\t\t     struct flow_dissector *flow_dissector,\n\t\t\t     void *target_container)\n{\n\tstruct ip_tunnel_info *info;\n\tstruct ip_tunnel_key *key;\n\n\t/* A quick check to see if there might be something to do. */\n\tif (!dissector_uses_key(flow_dissector,\n\t\t\t\tFLOW_DISSECTOR_KEY_ENC_KEYID) &&\n\t    !dissector_uses_key(flow_dissector,\n\t\t\t\tFLOW_DISSECTOR_KEY_ENC_IPV4_ADDRS) &&\n\t    !dissector_uses_key(flow_dissector,\n\t\t\t\tFLOW_DISSECTOR_KEY_ENC_IPV6_ADDRS) &&\n\t    !dissector_uses_key(flow_dissector,\n\t\t\t\tFLOW_DISSECTOR_KEY_ENC_CONTROL) &&\n\t    !dissector_uses_key(flow_dissector,\n\t\t\t\tFLOW_DISSECTOR_KEY_ENC_PORTS) &&\n\t    !dissector_uses_key(flow_dissector,\n\t\t\t\tFLOW_DISSECTOR_KEY_ENC_IP) &&\n\t    !dissector_uses_key(flow_dissector,\n\t\t\t\tFLOW_DISSECTOR_KEY_ENC_OPTS))\n\t\treturn;\n\n\tinfo = skb_tunnel_info(skb);\n\tif (!info)\n\t\treturn;\n\n\tkey = &info->key;\n\n\tswitch (ip_tunnel_info_af(info)) {\n\tcase AF_INET:\n\t\tskb_flow_dissect_set_enc_addr_type(FLOW_DISSECTOR_KEY_IPV4_ADDRS,\n\t\t\t\t\t\t   flow_dissector,\n\t\t\t\t\t\t   target_container);\n\t\tif (dissector_uses_key(flow_dissector,\n\t\t\t\t       FLOW_DISSECTOR_KEY_ENC_IPV4_ADDRS)) {\n\t\t\tstruct flow_dissector_key_ipv4_addrs *ipv4;\n\n\t\t\tipv4 = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t FLOW_DISSECTOR_KEY_ENC_IPV4_ADDRS,\n\t\t\t\t\t\t\t target_container);\n\t\t\tipv4->src = key->u.ipv4.src;\n\t\t\tipv4->dst = key->u.ipv4.dst;\n\t\t}\n\t\tbreak;\n\tcase AF_INET6:\n\t\tskb_flow_dissect_set_enc_addr_type(FLOW_DISSECTOR_KEY_IPV6_ADDRS,\n\t\t\t\t\t\t   flow_dissector,\n\t\t\t\t\t\t   target_container);\n\t\tif (dissector_uses_key(flow_dissector,\n\t\t\t\t       FLOW_DISSECTOR_KEY_ENC_IPV6_ADDRS)) {\n\t\t\tstruct flow_dissector_key_ipv6_addrs *ipv6;\n\n\t\t\tipv6 = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t FLOW_DISSECTOR_KEY_ENC_IPV6_ADDRS,\n\t\t\t\t\t\t\t target_container);\n\t\t\tipv6->src = key->u.ipv6.src;\n\t\t\tipv6->dst = key->u.ipv6.dst;\n\t\t}\n\t\tbreak;\n\t}\n\n\tif (dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_ENC_KEYID)) {\n\t\tstruct flow_dissector_key_keyid *keyid;\n\n\t\tkeyid = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t  FLOW_DISSECTOR_KEY_ENC_KEYID,\n\t\t\t\t\t\t  target_container);\n\t\tkeyid->keyid = tunnel_id_to_key32(key->tun_id);\n\t}\n\n\tif (dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_ENC_PORTS)) {\n\t\tstruct flow_dissector_key_ports *tp;\n\n\t\ttp = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t       FLOW_DISSECTOR_KEY_ENC_PORTS,\n\t\t\t\t\t       target_container);\n\t\ttp->src = key->tp_src;\n\t\ttp->dst = key->tp_dst;\n\t}\n\n\tif (dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_ENC_IP)) {\n\t\tstruct flow_dissector_key_ip *ip;\n\n\t\tip = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t       FLOW_DISSECTOR_KEY_ENC_IP,\n\t\t\t\t\t       target_container);\n\t\tip->tos = key->tos;\n\t\tip->ttl = key->ttl;\n\t}\n\n\tif (dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_ENC_OPTS)) {\n\t\tstruct flow_dissector_key_enc_opts *enc_opt;\n\n\t\tenc_opt = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t    FLOW_DISSECTOR_KEY_ENC_OPTS,\n\t\t\t\t\t\t    target_container);\n\n\t\tif (info->options_len) {\n\t\t\tenc_opt->len = info->options_len;\n\t\t\tip_tunnel_info_opts_get(enc_opt->data, info);\n\t\t\tenc_opt->dst_opt_type = info->key.tun_flags &\n\t\t\t\t\t\tTUNNEL_OPTIONS_PRESENT;\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(skb_flow_dissect_tunnel_info);\n\nvoid skb_flow_dissect_hash(const struct sk_buff *skb,\n\t\t\t   struct flow_dissector *flow_dissector,\n\t\t\t   void *target_container)\n{\n\tstruct flow_dissector_key_hash *key;\n\n\tif (!dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_HASH))\n\t\treturn;\n\n\tkey = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\tFLOW_DISSECTOR_KEY_HASH,\n\t\t\t\t\ttarget_container);\n\n\tkey->hash = skb_get_hash_raw(skb);\n}\nEXPORT_SYMBOL(skb_flow_dissect_hash);\n\nstatic enum flow_dissect_ret\n__skb_flow_dissect_mpls(const struct sk_buff *skb,\n\t\t\tstruct flow_dissector *flow_dissector,\n\t\t\tvoid *target_container, void *data, int nhoff, int hlen,\n\t\t\tint lse_index, bool *entropy_label)\n{\n\tstruct mpls_label *hdr, _hdr;\n\tu32 entry, label, bos;\n\n\tif (!dissector_uses_key(flow_dissector,\n\t\t\t\tFLOW_DISSECTOR_KEY_MPLS_ENTROPY) &&\n\t    !dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_MPLS))\n\t\treturn FLOW_DISSECT_RET_OUT_GOOD;\n\n\tif (lse_index >= FLOW_DIS_MPLS_MAX)\n\t\treturn FLOW_DISSECT_RET_OUT_GOOD;\n\n\thdr = __skb_header_pointer(skb, nhoff, sizeof(_hdr), data,\n\t\t\t\t   hlen, &_hdr);\n\tif (!hdr)\n\t\treturn FLOW_DISSECT_RET_OUT_BAD;\n\n\tentry = ntohl(hdr->entry);\n\tlabel = (entry & MPLS_LS_LABEL_MASK) >> MPLS_LS_LABEL_SHIFT;\n\tbos = (entry & MPLS_LS_S_MASK) >> MPLS_LS_S_SHIFT;\n\n\tif (dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_MPLS)) {\n\t\tstruct flow_dissector_key_mpls *key_mpls;\n\t\tstruct flow_dissector_mpls_lse *lse;\n\n\t\tkey_mpls = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t     FLOW_DISSECTOR_KEY_MPLS,\n\t\t\t\t\t\t     target_container);\n\t\tlse = &key_mpls->ls[lse_index];\n\n\t\tlse->mpls_ttl = (entry & MPLS_LS_TTL_MASK) >> MPLS_LS_TTL_SHIFT;\n\t\tlse->mpls_bos = bos;\n\t\tlse->mpls_tc = (entry & MPLS_LS_TC_MASK) >> MPLS_LS_TC_SHIFT;\n\t\tlse->mpls_label = label;\n\t\tdissector_set_mpls_lse(key_mpls, lse_index);\n\t}\n\n\tif (*entropy_label &&\n\t    dissector_uses_key(flow_dissector,\n\t\t\t       FLOW_DISSECTOR_KEY_MPLS_ENTROPY)) {\n\t\tstruct flow_dissector_key_keyid *key_keyid;\n\n\t\tkey_keyid = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t      FLOW_DISSECTOR_KEY_MPLS_ENTROPY,\n\t\t\t\t\t\t      target_container);\n\t\tkey_keyid->keyid = cpu_to_be32(label);\n\t}\n\n\t*entropy_label = label == MPLS_LABEL_ENTROPY;\n\n\treturn bos ? FLOW_DISSECT_RET_OUT_GOOD : FLOW_DISSECT_RET_PROTO_AGAIN;\n}\n\nstatic enum flow_dissect_ret\n__skb_flow_dissect_arp(const struct sk_buff *skb,\n\t\t       struct flow_dissector *flow_dissector,\n\t\t       void *target_container, void *data, int nhoff, int hlen)\n{\n\tstruct flow_dissector_key_arp *key_arp;\n\tstruct {\n\t\tunsigned char ar_sha[ETH_ALEN];\n\t\tunsigned char ar_sip[4];\n\t\tunsigned char ar_tha[ETH_ALEN];\n\t\tunsigned char ar_tip[4];\n\t} *arp_eth, _arp_eth;\n\tconst struct arphdr *arp;\n\tstruct arphdr _arp;\n\n\tif (!dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_ARP))\n\t\treturn FLOW_DISSECT_RET_OUT_GOOD;\n\n\tarp = __skb_header_pointer(skb, nhoff, sizeof(_arp), data,\n\t\t\t\t   hlen, &_arp);\n\tif (!arp)\n\t\treturn FLOW_DISSECT_RET_OUT_BAD;\n\n\tif (arp->ar_hrd != htons(ARPHRD_ETHER) ||\n\t    arp->ar_pro != htons(ETH_P_IP) ||\n\t    arp->ar_hln != ETH_ALEN ||\n\t    arp->ar_pln != 4 ||\n\t    (arp->ar_op != htons(ARPOP_REPLY) &&\n\t     arp->ar_op != htons(ARPOP_REQUEST)))\n\t\treturn FLOW_DISSECT_RET_OUT_BAD;\n\n\tarp_eth = __skb_header_pointer(skb, nhoff + sizeof(_arp),\n\t\t\t\t       sizeof(_arp_eth), data,\n\t\t\t\t       hlen, &_arp_eth);\n\tif (!arp_eth)\n\t\treturn FLOW_DISSECT_RET_OUT_BAD;\n\n\tkey_arp = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t    FLOW_DISSECTOR_KEY_ARP,\n\t\t\t\t\t    target_container);\n\n\tmemcpy(&key_arp->sip, arp_eth->ar_sip, sizeof(key_arp->sip));\n\tmemcpy(&key_arp->tip, arp_eth->ar_tip, sizeof(key_arp->tip));\n\n\t/* Only store the lower byte of the opcode;\n\t * this covers ARPOP_REPLY and ARPOP_REQUEST.\n\t */\n\tkey_arp->op = ntohs(arp->ar_op) & 0xff;\n\n\tether_addr_copy(key_arp->sha, arp_eth->ar_sha);\n\tether_addr_copy(key_arp->tha, arp_eth->ar_tha);\n\n\treturn FLOW_DISSECT_RET_OUT_GOOD;\n}\n\nstatic enum flow_dissect_ret\n__skb_flow_dissect_gre(const struct sk_buff *skb,\n\t\t       struct flow_dissector_key_control *key_control,\n\t\t       struct flow_dissector *flow_dissector,\n\t\t       void *target_container, void *data,\n\t\t       __be16 *p_proto, int *p_nhoff, int *p_hlen,\n\t\t       unsigned int flags)\n{\n\tstruct flow_dissector_key_keyid *key_keyid;\n\tstruct gre_base_hdr *hdr, _hdr;\n\tint offset = 0;\n\tu16 gre_ver;\n\n\thdr = __skb_header_pointer(skb, *p_nhoff, sizeof(_hdr),\n\t\t\t\t   data, *p_hlen, &_hdr);\n\tif (!hdr)\n\t\treturn FLOW_DISSECT_RET_OUT_BAD;\n\n\t/* Only look inside GRE without routing */\n\tif (hdr->flags & GRE_ROUTING)\n\t\treturn FLOW_DISSECT_RET_OUT_GOOD;\n\n\t/* Only look inside GRE for version 0 and 1 */\n\tgre_ver = ntohs(hdr->flags & GRE_VERSION);\n\tif (gre_ver > 1)\n\t\treturn FLOW_DISSECT_RET_OUT_GOOD;\n\n\t*p_proto = hdr->protocol;\n\tif (gre_ver) {\n\t\t/* Version1 must be PPTP, and check the flags */\n\t\tif (!(*p_proto == GRE_PROTO_PPP && (hdr->flags & GRE_KEY)))\n\t\t\treturn FLOW_DISSECT_RET_OUT_GOOD;\n\t}\n\n\toffset += sizeof(struct gre_base_hdr);\n\n\tif (hdr->flags & GRE_CSUM)\n\t\toffset += sizeof_field(struct gre_full_hdr, csum) +\n\t\t\t  sizeof_field(struct gre_full_hdr, reserved1);\n\n\tif (hdr->flags & GRE_KEY) {\n\t\tconst __be32 *keyid;\n\t\t__be32 _keyid;\n\n\t\tkeyid = __skb_header_pointer(skb, *p_nhoff + offset,\n\t\t\t\t\t     sizeof(_keyid),\n\t\t\t\t\t     data, *p_hlen, &_keyid);\n\t\tif (!keyid)\n\t\t\treturn FLOW_DISSECT_RET_OUT_BAD;\n\n\t\tif (dissector_uses_key(flow_dissector,\n\t\t\t\t       FLOW_DISSECTOR_KEY_GRE_KEYID)) {\n\t\t\tkey_keyid = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t      FLOW_DISSECTOR_KEY_GRE_KEYID,\n\t\t\t\t\t\t\t      target_container);\n\t\t\tif (gre_ver == 0)\n\t\t\t\tkey_keyid->keyid = *keyid;\n\t\t\telse\n\t\t\t\tkey_keyid->keyid = *keyid & GRE_PPTP_KEY_MASK;\n\t\t}\n\t\toffset += sizeof_field(struct gre_full_hdr, key);\n\t}\n\n\tif (hdr->flags & GRE_SEQ)\n\t\toffset += sizeof_field(struct pptp_gre_header, seq);\n\n\tif (gre_ver == 0) {\n\t\tif (*p_proto == htons(ETH_P_TEB)) {\n\t\t\tconst struct ethhdr *eth;\n\t\t\tstruct ethhdr _eth;\n\n\t\t\teth = __skb_header_pointer(skb, *p_nhoff + offset,\n\t\t\t\t\t\t   sizeof(_eth),\n\t\t\t\t\t\t   data, *p_hlen, &_eth);\n\t\t\tif (!eth)\n\t\t\t\treturn FLOW_DISSECT_RET_OUT_BAD;\n\t\t\t*p_proto = eth->h_proto;\n\t\t\toffset += sizeof(*eth);\n\n\t\t\t/* Cap headers that we access via pointers at the\n\t\t\t * end of the Ethernet header as our maximum alignment\n\t\t\t * at that point is only 2 bytes.\n\t\t\t */\n\t\t\tif (NET_IP_ALIGN)\n\t\t\t\t*p_hlen = *p_nhoff + offset;\n\t\t}\n\t} else { /* version 1, must be PPTP */\n\t\tu8 _ppp_hdr[PPP_HDRLEN];\n\t\tu8 *ppp_hdr;\n\n\t\tif (hdr->flags & GRE_ACK)\n\t\t\toffset += sizeof_field(struct pptp_gre_header, ack);\n\n\t\tppp_hdr = __skb_header_pointer(skb, *p_nhoff + offset,\n\t\t\t\t\t       sizeof(_ppp_hdr),\n\t\t\t\t\t       data, *p_hlen, _ppp_hdr);\n\t\tif (!ppp_hdr)\n\t\t\treturn FLOW_DISSECT_RET_OUT_BAD;\n\n\t\tswitch (PPP_PROTOCOL(ppp_hdr)) {\n\t\tcase PPP_IP:\n\t\t\t*p_proto = htons(ETH_P_IP);\n\t\t\tbreak;\n\t\tcase PPP_IPV6:\n\t\t\t*p_proto = htons(ETH_P_IPV6);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t/* Could probably catch some more like MPLS */\n\t\t\tbreak;\n\t\t}\n\n\t\toffset += PPP_HDRLEN;\n\t}\n\n\t*p_nhoff += offset;\n\tkey_control->flags |= FLOW_DIS_ENCAPSULATION;\n\tif (flags & FLOW_DISSECTOR_F_STOP_AT_ENCAP)\n\t\treturn FLOW_DISSECT_RET_OUT_GOOD;\n\n\treturn FLOW_DISSECT_RET_PROTO_AGAIN;\n}\n\n/**\n * __skb_flow_dissect_batadv() - dissect batman-adv header\n * @skb: sk_buff to with the batman-adv header\n * @key_control: flow dissectors control key\n * @data: raw buffer pointer to the packet, if NULL use skb->data\n * @p_proto: pointer used to update the protocol to process next\n * @p_nhoff: pointer used to update inner network header offset\n * @hlen: packet header length\n * @flags: any combination of FLOW_DISSECTOR_F_*\n *\n * ETH_P_BATMAN packets are tried to be dissected. Only\n * &struct batadv_unicast packets are actually processed because they contain an\n * inner ethernet header and are usually followed by actual network header. This\n * allows the flow dissector to continue processing the packet.\n *\n * Return: FLOW_DISSECT_RET_PROTO_AGAIN when &struct batadv_unicast was found,\n *  FLOW_DISSECT_RET_OUT_GOOD when dissector should stop after encapsulation,\n *  otherwise FLOW_DISSECT_RET_OUT_BAD\n */\nstatic enum flow_dissect_ret\n__skb_flow_dissect_batadv(const struct sk_buff *skb,\n\t\t\t  struct flow_dissector_key_control *key_control,\n\t\t\t  void *data, __be16 *p_proto, int *p_nhoff, int hlen,\n\t\t\t  unsigned int flags)\n{\n\tstruct {\n\t\tstruct batadv_unicast_packet batadv_unicast;\n\t\tstruct ethhdr eth;\n\t} *hdr, _hdr;\n\n\thdr = __skb_header_pointer(skb, *p_nhoff, sizeof(_hdr), data, hlen,\n\t\t\t\t   &_hdr);\n\tif (!hdr)\n\t\treturn FLOW_DISSECT_RET_OUT_BAD;\n\n\tif (hdr->batadv_unicast.version != BATADV_COMPAT_VERSION)\n\t\treturn FLOW_DISSECT_RET_OUT_BAD;\n\n\tif (hdr->batadv_unicast.packet_type != BATADV_UNICAST)\n\t\treturn FLOW_DISSECT_RET_OUT_BAD;\n\n\t*p_proto = hdr->eth.h_proto;\n\t*p_nhoff += sizeof(*hdr);\n\n\tkey_control->flags |= FLOW_DIS_ENCAPSULATION;\n\tif (flags & FLOW_DISSECTOR_F_STOP_AT_ENCAP)\n\t\treturn FLOW_DISSECT_RET_OUT_GOOD;\n\n\treturn FLOW_DISSECT_RET_PROTO_AGAIN;\n}\n\nstatic void\n__skb_flow_dissect_tcp(const struct sk_buff *skb,\n\t\t       struct flow_dissector *flow_dissector,\n\t\t       void *target_container, void *data, int thoff, int hlen)\n{\n\tstruct flow_dissector_key_tcp *key_tcp;\n\tstruct tcphdr *th, _th;\n\n\tif (!dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_TCP))\n\t\treturn;\n\n\tth = __skb_header_pointer(skb, thoff, sizeof(_th), data, hlen, &_th);\n\tif (!th)\n\t\treturn;\n\n\tif (unlikely(__tcp_hdrlen(th) < sizeof(_th)))\n\t\treturn;\n\n\tkey_tcp = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t    FLOW_DISSECTOR_KEY_TCP,\n\t\t\t\t\t    target_container);\n\tkey_tcp->flags = (*(__be16 *) &tcp_flag_word(th) & htons(0x0FFF));\n}\n\nstatic void\n__skb_flow_dissect_ports(const struct sk_buff *skb,\n\t\t\t struct flow_dissector *flow_dissector,\n\t\t\t void *target_container, void *data, int nhoff,\n\t\t\t u8 ip_proto, int hlen)\n{\n\tenum flow_dissector_key_id dissector_ports = FLOW_DISSECTOR_KEY_MAX;\n\tstruct flow_dissector_key_ports *key_ports;\n\n\tif (dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_PORTS))\n\t\tdissector_ports = FLOW_DISSECTOR_KEY_PORTS;\n\telse if (dissector_uses_key(flow_dissector,\n\t\t\t\t    FLOW_DISSECTOR_KEY_PORTS_RANGE))\n\t\tdissector_ports = FLOW_DISSECTOR_KEY_PORTS_RANGE;\n\n\tif (dissector_ports == FLOW_DISSECTOR_KEY_MAX)\n\t\treturn;\n\n\tkey_ports = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t      dissector_ports,\n\t\t\t\t\t      target_container);\n\tkey_ports->ports = __skb_flow_get_ports(skb, nhoff, ip_proto,\n\t\t\t\t\t\tdata, hlen);\n}\n\nstatic void\n__skb_flow_dissect_ipv4(const struct sk_buff *skb,\n\t\t\tstruct flow_dissector *flow_dissector,\n\t\t\tvoid *target_container, void *data, const struct iphdr *iph)\n{\n\tstruct flow_dissector_key_ip *key_ip;\n\n\tif (!dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_IP))\n\t\treturn;\n\n\tkey_ip = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t   FLOW_DISSECTOR_KEY_IP,\n\t\t\t\t\t   target_container);\n\tkey_ip->tos = iph->tos;\n\tkey_ip->ttl = iph->ttl;\n}\n\nstatic void\n__skb_flow_dissect_ipv6(const struct sk_buff *skb,\n\t\t\tstruct flow_dissector *flow_dissector,\n\t\t\tvoid *target_container, void *data, const struct ipv6hdr *iph)\n{\n\tstruct flow_dissector_key_ip *key_ip;\n\n\tif (!dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_IP))\n\t\treturn;\n\n\tkey_ip = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t   FLOW_DISSECTOR_KEY_IP,\n\t\t\t\t\t   target_container);\n\tkey_ip->tos = ipv6_get_dsfield(iph);\n\tkey_ip->ttl = iph->hop_limit;\n}\n\n/* Maximum number of protocol headers that can be parsed in\n * __skb_flow_dissect\n */\n#define MAX_FLOW_DISSECT_HDRS\t15\n\nstatic bool skb_flow_dissect_allowed(int *num_hdrs)\n{\n\t++*num_hdrs;\n\n\treturn (*num_hdrs <= MAX_FLOW_DISSECT_HDRS);\n}\n\nstatic void __skb_flow_bpf_to_target(const struct bpf_flow_keys *flow_keys,\n\t\t\t\t     struct flow_dissector *flow_dissector,\n\t\t\t\t     void *target_container)\n{\n\tstruct flow_dissector_key_ports *key_ports = NULL;\n\tstruct flow_dissector_key_control *key_control;\n\tstruct flow_dissector_key_basic *key_basic;\n\tstruct flow_dissector_key_addrs *key_addrs;\n\tstruct flow_dissector_key_tags *key_tags;\n\n\tkey_control = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\tFLOW_DISSECTOR_KEY_CONTROL,\n\t\t\t\t\t\ttarget_container);\n\tkey_control->thoff = flow_keys->thoff;\n\tif (flow_keys->is_frag)\n\t\tkey_control->flags |= FLOW_DIS_IS_FRAGMENT;\n\tif (flow_keys->is_first_frag)\n\t\tkey_control->flags |= FLOW_DIS_FIRST_FRAG;\n\tif (flow_keys->is_encap)\n\t\tkey_control->flags |= FLOW_DIS_ENCAPSULATION;\n\n\tkey_basic = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t      FLOW_DISSECTOR_KEY_BASIC,\n\t\t\t\t\t      target_container);\n\tkey_basic->n_proto = flow_keys->n_proto;\n\tkey_basic->ip_proto = flow_keys->ip_proto;\n\n\tif (flow_keys->addr_proto == ETH_P_IP &&\n\t    dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_IPV4_ADDRS)) {\n\t\tkey_addrs = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t      FLOW_DISSECTOR_KEY_IPV4_ADDRS,\n\t\t\t\t\t\t      target_container);\n\t\tkey_addrs->v4addrs.src = flow_keys->ipv4_src;\n\t\tkey_addrs->v4addrs.dst = flow_keys->ipv4_dst;\n\t\tkey_control->addr_type = FLOW_DISSECTOR_KEY_IPV4_ADDRS;\n\t} else if (flow_keys->addr_proto == ETH_P_IPV6 &&\n\t\t   dissector_uses_key(flow_dissector,\n\t\t\t\t      FLOW_DISSECTOR_KEY_IPV6_ADDRS)) {\n\t\tkey_addrs = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t      FLOW_DISSECTOR_KEY_IPV6_ADDRS,\n\t\t\t\t\t\t      target_container);\n\t\tmemcpy(&key_addrs->v6addrs, &flow_keys->ipv6_src,\n\t\t       sizeof(key_addrs->v6addrs));\n\t\tkey_control->addr_type = FLOW_DISSECTOR_KEY_IPV6_ADDRS;\n\t}\n\n\tif (dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_PORTS))\n\t\tkey_ports = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t      FLOW_DISSECTOR_KEY_PORTS,\n\t\t\t\t\t\t      target_container);\n\telse if (dissector_uses_key(flow_dissector,\n\t\t\t\t    FLOW_DISSECTOR_KEY_PORTS_RANGE))\n\t\tkey_ports = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t      FLOW_DISSECTOR_KEY_PORTS_RANGE,\n\t\t\t\t\t\t      target_container);\n\n\tif (key_ports) {\n\t\tkey_ports->src = flow_keys->sport;\n\t\tkey_ports->dst = flow_keys->dport;\n\t}\n\n\tif (dissector_uses_key(flow_dissector,\n\t\t\t       FLOW_DISSECTOR_KEY_FLOW_LABEL)) {\n\t\tkey_tags = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t     FLOW_DISSECTOR_KEY_FLOW_LABEL,\n\t\t\t\t\t\t     target_container);\n\t\tkey_tags->flow_label = ntohl(flow_keys->flow_label);\n\t}\n}\n\nbool bpf_flow_dissect(struct bpf_prog *prog, struct bpf_flow_dissector *ctx,\n\t\t      __be16 proto, int nhoff, int hlen, unsigned int flags)\n{\n\tstruct bpf_flow_keys *flow_keys = ctx->flow_keys;\n\tu32 result;\n\n\t/* Pass parameters to the BPF program */\n\tmemset(flow_keys, 0, sizeof(*flow_keys));\n\tflow_keys->n_proto = proto;\n\tflow_keys->nhoff = nhoff;\n\tflow_keys->thoff = flow_keys->nhoff;\n\n\tBUILD_BUG_ON((int)BPF_FLOW_DISSECTOR_F_PARSE_1ST_FRAG !=\n\t\t     (int)FLOW_DISSECTOR_F_PARSE_1ST_FRAG);\n\tBUILD_BUG_ON((int)BPF_FLOW_DISSECTOR_F_STOP_AT_FLOW_LABEL !=\n\t\t     (int)FLOW_DISSECTOR_F_STOP_AT_FLOW_LABEL);\n\tBUILD_BUG_ON((int)BPF_FLOW_DISSECTOR_F_STOP_AT_ENCAP !=\n\t\t     (int)FLOW_DISSECTOR_F_STOP_AT_ENCAP);\n\tflow_keys->flags = flags;\n\n\tresult = bpf_prog_run_pin_on_cpu(prog, ctx);\n\n\tflow_keys->nhoff = clamp_t(u16, flow_keys->nhoff, nhoff, hlen);\n\tflow_keys->thoff = clamp_t(u16, flow_keys->thoff,\n\t\t\t\t   flow_keys->nhoff, hlen);\n\n\treturn result == BPF_OK;\n}\n\n/**\n * __skb_flow_dissect - extract the flow_keys struct and return it\n * @net: associated network namespace, derived from @skb if NULL\n * @skb: sk_buff to extract the flow from, can be NULL if the rest are specified\n * @flow_dissector: list of keys to dissect\n * @target_container: target structure to put dissected values into\n * @data: raw buffer pointer to the packet, if NULL use skb->data\n * @proto: protocol for which to get the flow, if @data is NULL use skb->protocol\n * @nhoff: network header offset, if @data is NULL use skb_network_offset(skb)\n * @hlen: packet header length, if @data is NULL use skb_headlen(skb)\n * @flags: flags that control the dissection process, e.g.\n *         FLOW_DISSECTOR_F_STOP_AT_ENCAP.\n *\n * The function will try to retrieve individual keys into target specified\n * by flow_dissector from either the skbuff or a raw buffer specified by the\n * rest parameters.\n *\n * Caller must take care of zeroing target container memory.\n */\nbool __skb_flow_dissect(const struct net *net,\n\t\t\tconst struct sk_buff *skb,\n\t\t\tstruct flow_dissector *flow_dissector,\n\t\t\tvoid *target_container,\n\t\t\tvoid *data, __be16 proto, int nhoff, int hlen,\n\t\t\tunsigned int flags)\n{\n\tstruct flow_dissector_key_control *key_control;\n\tstruct flow_dissector_key_basic *key_basic;\n\tstruct flow_dissector_key_addrs *key_addrs;\n\tstruct flow_dissector_key_tags *key_tags;\n\tstruct flow_dissector_key_vlan *key_vlan;\n\tenum flow_dissect_ret fdret;\n\tenum flow_dissector_key_id dissector_vlan = FLOW_DISSECTOR_KEY_MAX;\n\tbool mpls_el = false;\n\tint mpls_lse = 0;\n\tint num_hdrs = 0;\n\tu8 ip_proto = 0;\n\tbool ret;\n\n\tif (!data) {\n\t\tdata = skb->data;\n\t\tproto = skb_vlan_tag_present(skb) ?\n\t\t\t skb->vlan_proto : skb->protocol;\n\t\tnhoff = skb_network_offset(skb);\n\t\thlen = skb_headlen(skb);\n#if IS_ENABLED(CONFIG_NET_DSA)\n\t\tif (unlikely(skb->dev && netdev_uses_dsa(skb->dev) &&\n\t\t\t     proto == htons(ETH_P_XDSA))) {\n\t\t\tconst struct dsa_device_ops *ops;\n\t\t\tint offset = 0;\n\n\t\t\tops = skb->dev->dsa_ptr->tag_ops;\n\t\t\t/* Tail taggers don't break flow dissection */\n\t\t\tif (!ops->tail_tag) {\n\t\t\t\tif (ops->flow_dissect)\n\t\t\t\t\tops->flow_dissect(skb, &proto, &offset);\n\t\t\t\telse\n\t\t\t\t\tdsa_tag_generic_flow_dissect(skb,\n\t\t\t\t\t\t\t\t     &proto,\n\t\t\t\t\t\t\t\t     &offset);\n\t\t\t\thlen -= offset;\n\t\t\t\tnhoff += offset;\n\t\t\t}\n\t\t}\n#endif\n\t}\n\n\t/* It is ensured by skb_flow_dissector_init() that control key will\n\t * be always present.\n\t */\n\tkey_control = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\tFLOW_DISSECTOR_KEY_CONTROL,\n\t\t\t\t\t\ttarget_container);\n\n\t/* It is ensured by skb_flow_dissector_init() that basic key will\n\t * be always present.\n\t */\n\tkey_basic = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t      FLOW_DISSECTOR_KEY_BASIC,\n\t\t\t\t\t      target_container);\n\n\tif (skb) {\n\t\tif (!net) {\n\t\t\tif (skb->dev)\n\t\t\t\tnet = dev_net(skb->dev);\n\t\t\telse if (skb->sk)\n\t\t\t\tnet = sock_net(skb->sk);\n\t\t}\n\t}\n\n\tWARN_ON_ONCE(!net);\n\tif (net) {\n\t\tenum netns_bpf_attach_type type = NETNS_BPF_FLOW_DISSECTOR;\n\t\tstruct bpf_prog_array *run_array;\n\n\t\trcu_read_lock();\n\t\trun_array = rcu_dereference(init_net.bpf.run_array[type]);\n\t\tif (!run_array)\n\t\t\trun_array = rcu_dereference(net->bpf.run_array[type]);\n\n\t\tif (run_array) {\n\t\t\tstruct bpf_flow_keys flow_keys;\n\t\t\tstruct bpf_flow_dissector ctx = {\n\t\t\t\t.flow_keys = &flow_keys,\n\t\t\t\t.data = data,\n\t\t\t\t.data_end = data + hlen,\n\t\t\t};\n\t\t\t__be16 n_proto = proto;\n\t\t\tstruct bpf_prog *prog;\n\n\t\t\tif (skb) {\n\t\t\t\tctx.skb = skb;\n\t\t\t\t/* we can't use 'proto' in the skb case\n\t\t\t\t * because it might be set to skb->vlan_proto\n\t\t\t\t * which has been pulled from the data\n\t\t\t\t */\n\t\t\t\tn_proto = skb->protocol;\n\t\t\t}\n\n\t\t\tprog = READ_ONCE(run_array->items[0].prog);\n\t\t\tret = bpf_flow_dissect(prog, &ctx, n_proto, nhoff,\n\t\t\t\t\t       hlen, flags);\n\t\t\t__skb_flow_bpf_to_target(&flow_keys, flow_dissector,\n\t\t\t\t\t\t target_container);\n\t\t\trcu_read_unlock();\n\t\t\treturn ret;\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tif (dissector_uses_key(flow_dissector,\n\t\t\t       FLOW_DISSECTOR_KEY_ETH_ADDRS)) {\n\t\tstruct ethhdr *eth = eth_hdr(skb);\n\t\tstruct flow_dissector_key_eth_addrs *key_eth_addrs;\n\n\t\tkey_eth_addrs = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t  FLOW_DISSECTOR_KEY_ETH_ADDRS,\n\t\t\t\t\t\t\t  target_container);\n\t\tmemcpy(key_eth_addrs, &eth->h_dest, sizeof(*key_eth_addrs));\n\t}\n\nproto_again:\n\tfdret = FLOW_DISSECT_RET_CONTINUE;\n\n\tswitch (proto) {\n\tcase htons(ETH_P_IP): {\n\t\tconst struct iphdr *iph;\n\t\tstruct iphdr _iph;\n\n\t\tiph = __skb_header_pointer(skb, nhoff, sizeof(_iph), data, hlen, &_iph);\n\t\tif (!iph || iph->ihl < 5) {\n\t\t\tfdret = FLOW_DISSECT_RET_OUT_BAD;\n\t\t\tbreak;\n\t\t}\n\n\t\tnhoff += iph->ihl * 4;\n\n\t\tip_proto = iph->protocol;\n\n\t\tif (dissector_uses_key(flow_dissector,\n\t\t\t\t       FLOW_DISSECTOR_KEY_IPV4_ADDRS)) {\n\t\t\tkey_addrs = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t      FLOW_DISSECTOR_KEY_IPV4_ADDRS,\n\t\t\t\t\t\t\t      target_container);\n\n\t\t\tmemcpy(&key_addrs->v4addrs, &iph->saddr,\n\t\t\t       sizeof(key_addrs->v4addrs));\n\t\t\tkey_control->addr_type = FLOW_DISSECTOR_KEY_IPV4_ADDRS;\n\t\t}\n\n\t\t__skb_flow_dissect_ipv4(skb, flow_dissector,\n\t\t\t\t\ttarget_container, data, iph);\n\n\t\tif (ip_is_fragment(iph)) {\n\t\t\tkey_control->flags |= FLOW_DIS_IS_FRAGMENT;\n\n\t\t\tif (iph->frag_off & htons(IP_OFFSET)) {\n\t\t\t\tfdret = FLOW_DISSECT_RET_OUT_GOOD;\n\t\t\t\tbreak;\n\t\t\t} else {\n\t\t\t\tkey_control->flags |= FLOW_DIS_FIRST_FRAG;\n\t\t\t\tif (!(flags &\n\t\t\t\t      FLOW_DISSECTOR_F_PARSE_1ST_FRAG)) {\n\t\t\t\t\tfdret = FLOW_DISSECT_RET_OUT_GOOD;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tbreak;\n\t}\n\tcase htons(ETH_P_IPV6): {\n\t\tconst struct ipv6hdr *iph;\n\t\tstruct ipv6hdr _iph;\n\n\t\tiph = __skb_header_pointer(skb, nhoff, sizeof(_iph), data, hlen, &_iph);\n\t\tif (!iph) {\n\t\t\tfdret = FLOW_DISSECT_RET_OUT_BAD;\n\t\t\tbreak;\n\t\t}\n\n\t\tip_proto = iph->nexthdr;\n\t\tnhoff += sizeof(struct ipv6hdr);\n\n\t\tif (dissector_uses_key(flow_dissector,\n\t\t\t\t       FLOW_DISSECTOR_KEY_IPV6_ADDRS)) {\n\t\t\tkey_addrs = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t      FLOW_DISSECTOR_KEY_IPV6_ADDRS,\n\t\t\t\t\t\t\t      target_container);\n\n\t\t\tmemcpy(&key_addrs->v6addrs, &iph->saddr,\n\t\t\t       sizeof(key_addrs->v6addrs));\n\t\t\tkey_control->addr_type = FLOW_DISSECTOR_KEY_IPV6_ADDRS;\n\t\t}\n\n\t\tif ((dissector_uses_key(flow_dissector,\n\t\t\t\t\tFLOW_DISSECTOR_KEY_FLOW_LABEL) ||\n\t\t     (flags & FLOW_DISSECTOR_F_STOP_AT_FLOW_LABEL)) &&\n\t\t    ip6_flowlabel(iph)) {\n\t\t\t__be32 flow_label = ip6_flowlabel(iph);\n\n\t\t\tif (dissector_uses_key(flow_dissector,\n\t\t\t\t\t       FLOW_DISSECTOR_KEY_FLOW_LABEL)) {\n\t\t\t\tkey_tags = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t\t     FLOW_DISSECTOR_KEY_FLOW_LABEL,\n\t\t\t\t\t\t\t\t     target_container);\n\t\t\t\tkey_tags->flow_label = ntohl(flow_label);\n\t\t\t}\n\t\t\tif (flags & FLOW_DISSECTOR_F_STOP_AT_FLOW_LABEL) {\n\t\t\t\tfdret = FLOW_DISSECT_RET_OUT_GOOD;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t__skb_flow_dissect_ipv6(skb, flow_dissector,\n\t\t\t\t\ttarget_container, data, iph);\n\n\t\tbreak;\n\t}\n\tcase htons(ETH_P_8021AD):\n\tcase htons(ETH_P_8021Q): {\n\t\tconst struct vlan_hdr *vlan = NULL;\n\t\tstruct vlan_hdr _vlan;\n\t\t__be16 saved_vlan_tpid = proto;\n\n\t\tif (dissector_vlan == FLOW_DISSECTOR_KEY_MAX &&\n\t\t    skb && skb_vlan_tag_present(skb)) {\n\t\t\tproto = skb->protocol;\n\t\t} else {\n\t\t\tvlan = __skb_header_pointer(skb, nhoff, sizeof(_vlan),\n\t\t\t\t\t\t    data, hlen, &_vlan);\n\t\t\tif (!vlan) {\n\t\t\t\tfdret = FLOW_DISSECT_RET_OUT_BAD;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tproto = vlan->h_vlan_encapsulated_proto;\n\t\t\tnhoff += sizeof(*vlan);\n\t\t}\n\n\t\tif (dissector_vlan == FLOW_DISSECTOR_KEY_MAX) {\n\t\t\tdissector_vlan = FLOW_DISSECTOR_KEY_VLAN;\n\t\t} else if (dissector_vlan == FLOW_DISSECTOR_KEY_VLAN) {\n\t\t\tdissector_vlan = FLOW_DISSECTOR_KEY_CVLAN;\n\t\t} else {\n\t\t\tfdret = FLOW_DISSECT_RET_PROTO_AGAIN;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (dissector_uses_key(flow_dissector, dissector_vlan)) {\n\t\t\tkey_vlan = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t     dissector_vlan,\n\t\t\t\t\t\t\t     target_container);\n\n\t\t\tif (!vlan) {\n\t\t\t\tkey_vlan->vlan_id = skb_vlan_tag_get_id(skb);\n\t\t\t\tkey_vlan->vlan_priority = skb_vlan_tag_get_prio(skb);\n\t\t\t} else {\n\t\t\t\tkey_vlan->vlan_id = ntohs(vlan->h_vlan_TCI) &\n\t\t\t\t\tVLAN_VID_MASK;\n\t\t\t\tkey_vlan->vlan_priority =\n\t\t\t\t\t(ntohs(vlan->h_vlan_TCI) &\n\t\t\t\t\t VLAN_PRIO_MASK) >> VLAN_PRIO_SHIFT;\n\t\t\t}\n\t\t\tkey_vlan->vlan_tpid = saved_vlan_tpid;\n\t\t}\n\n\t\tfdret = FLOW_DISSECT_RET_PROTO_AGAIN;\n\t\tbreak;\n\t}\n\tcase htons(ETH_P_PPP_SES): {\n\t\tstruct {\n\t\t\tstruct pppoe_hdr hdr;\n\t\t\t__be16 proto;\n\t\t} *hdr, _hdr;\n\t\thdr = __skb_header_pointer(skb, nhoff, sizeof(_hdr), data, hlen, &_hdr);\n\t\tif (!hdr) {\n\t\t\tfdret = FLOW_DISSECT_RET_OUT_BAD;\n\t\t\tbreak;\n\t\t}\n\n\t\tproto = hdr->proto;\n\t\tnhoff += PPPOE_SES_HLEN;\n\t\tswitch (proto) {\n\t\tcase htons(PPP_IP):\n\t\t\tproto = htons(ETH_P_IP);\n\t\t\tfdret = FLOW_DISSECT_RET_PROTO_AGAIN;\n\t\t\tbreak;\n\t\tcase htons(PPP_IPV6):\n\t\t\tproto = htons(ETH_P_IPV6);\n\t\t\tfdret = FLOW_DISSECT_RET_PROTO_AGAIN;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tfdret = FLOW_DISSECT_RET_OUT_BAD;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\tcase htons(ETH_P_TIPC): {\n\t\tstruct tipc_basic_hdr *hdr, _hdr;\n\n\t\thdr = __skb_header_pointer(skb, nhoff, sizeof(_hdr),\n\t\t\t\t\t   data, hlen, &_hdr);\n\t\tif (!hdr) {\n\t\t\tfdret = FLOW_DISSECT_RET_OUT_BAD;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (dissector_uses_key(flow_dissector,\n\t\t\t\t       FLOW_DISSECTOR_KEY_TIPC)) {\n\t\t\tkey_addrs = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t      FLOW_DISSECTOR_KEY_TIPC,\n\t\t\t\t\t\t\t      target_container);\n\t\t\tkey_addrs->tipckey.key = tipc_hdr_rps_key(hdr);\n\t\t\tkey_control->addr_type = FLOW_DISSECTOR_KEY_TIPC;\n\t\t}\n\t\tfdret = FLOW_DISSECT_RET_OUT_GOOD;\n\t\tbreak;\n\t}\n\n\tcase htons(ETH_P_MPLS_UC):\n\tcase htons(ETH_P_MPLS_MC):\n\t\tfdret = __skb_flow_dissect_mpls(skb, flow_dissector,\n\t\t\t\t\t\ttarget_container, data,\n\t\t\t\t\t\tnhoff, hlen, mpls_lse,\n\t\t\t\t\t\t&mpls_el);\n\t\tnhoff += sizeof(struct mpls_label);\n\t\tmpls_lse++;\n\t\tbreak;\n\tcase htons(ETH_P_FCOE):\n\t\tif ((hlen - nhoff) < FCOE_HEADER_LEN) {\n\t\t\tfdret = FLOW_DISSECT_RET_OUT_BAD;\n\t\t\tbreak;\n\t\t}\n\n\t\tnhoff += FCOE_HEADER_LEN;\n\t\tfdret = FLOW_DISSECT_RET_OUT_GOOD;\n\t\tbreak;\n\n\tcase htons(ETH_P_ARP):\n\tcase htons(ETH_P_RARP):\n\t\tfdret = __skb_flow_dissect_arp(skb, flow_dissector,\n\t\t\t\t\t       target_container, data,\n\t\t\t\t\t       nhoff, hlen);\n\t\tbreak;\n\n\tcase htons(ETH_P_BATMAN):\n\t\tfdret = __skb_flow_dissect_batadv(skb, key_control, data,\n\t\t\t\t\t\t  &proto, &nhoff, hlen, flags);\n\t\tbreak;\n\n\tcase htons(ETH_P_1588): {\n\t\tstruct ptp_header *hdr, _hdr;\n\n\t\thdr = __skb_header_pointer(skb, nhoff, sizeof(_hdr), data,\n\t\t\t\t\t   hlen, &_hdr);\n\t\tif (!hdr) {\n\t\t\tfdret = FLOW_DISSECT_RET_OUT_BAD;\n\t\t\tbreak;\n\t\t}\n\n\t\tnhoff += ntohs(hdr->message_length);\n\t\tfdret = FLOW_DISSECT_RET_OUT_GOOD;\n\t\tbreak;\n\t}\n\n\tdefault:\n\t\tfdret = FLOW_DISSECT_RET_OUT_BAD;\n\t\tbreak;\n\t}\n\n\t/* Process result of proto processing */\n\tswitch (fdret) {\n\tcase FLOW_DISSECT_RET_OUT_GOOD:\n\t\tgoto out_good;\n\tcase FLOW_DISSECT_RET_PROTO_AGAIN:\n\t\tif (skb_flow_dissect_allowed(&num_hdrs))\n\t\t\tgoto proto_again;\n\t\tgoto out_good;\n\tcase FLOW_DISSECT_RET_CONTINUE:\n\tcase FLOW_DISSECT_RET_IPPROTO_AGAIN:\n\t\tbreak;\n\tcase FLOW_DISSECT_RET_OUT_BAD:\n\tdefault:\n\t\tgoto out_bad;\n\t}\n\nip_proto_again:\n\tfdret = FLOW_DISSECT_RET_CONTINUE;\n\n\tswitch (ip_proto) {\n\tcase IPPROTO_GRE:\n\t\tfdret = __skb_flow_dissect_gre(skb, key_control, flow_dissector,\n\t\t\t\t\t       target_container, data,\n\t\t\t\t\t       &proto, &nhoff, &hlen, flags);\n\t\tbreak;\n\n\tcase NEXTHDR_HOP:\n\tcase NEXTHDR_ROUTING:\n\tcase NEXTHDR_DEST: {\n\t\tu8 _opthdr[2], *opthdr;\n\n\t\tif (proto != htons(ETH_P_IPV6))\n\t\t\tbreak;\n\n\t\topthdr = __skb_header_pointer(skb, nhoff, sizeof(_opthdr),\n\t\t\t\t\t      data, hlen, &_opthdr);\n\t\tif (!opthdr) {\n\t\t\tfdret = FLOW_DISSECT_RET_OUT_BAD;\n\t\t\tbreak;\n\t\t}\n\n\t\tip_proto = opthdr[0];\n\t\tnhoff += (opthdr[1] + 1) << 3;\n\n\t\tfdret = FLOW_DISSECT_RET_IPPROTO_AGAIN;\n\t\tbreak;\n\t}\n\tcase NEXTHDR_FRAGMENT: {\n\t\tstruct frag_hdr _fh, *fh;\n\n\t\tif (proto != htons(ETH_P_IPV6))\n\t\t\tbreak;\n\n\t\tfh = __skb_header_pointer(skb, nhoff, sizeof(_fh),\n\t\t\t\t\t  data, hlen, &_fh);\n\n\t\tif (!fh) {\n\t\t\tfdret = FLOW_DISSECT_RET_OUT_BAD;\n\t\t\tbreak;\n\t\t}\n\n\t\tkey_control->flags |= FLOW_DIS_IS_FRAGMENT;\n\n\t\tnhoff += sizeof(_fh);\n\t\tip_proto = fh->nexthdr;\n\n\t\tif (!(fh->frag_off & htons(IP6_OFFSET))) {\n\t\t\tkey_control->flags |= FLOW_DIS_FIRST_FRAG;\n\t\t\tif (flags & FLOW_DISSECTOR_F_PARSE_1ST_FRAG) {\n\t\t\t\tfdret = FLOW_DISSECT_RET_IPPROTO_AGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tfdret = FLOW_DISSECT_RET_OUT_GOOD;\n\t\tbreak;\n\t}\n\tcase IPPROTO_IPIP:\n\t\tproto = htons(ETH_P_IP);\n\n\t\tkey_control->flags |= FLOW_DIS_ENCAPSULATION;\n\t\tif (flags & FLOW_DISSECTOR_F_STOP_AT_ENCAP) {\n\t\t\tfdret = FLOW_DISSECT_RET_OUT_GOOD;\n\t\t\tbreak;\n\t\t}\n\n\t\tfdret = FLOW_DISSECT_RET_PROTO_AGAIN;\n\t\tbreak;\n\n\tcase IPPROTO_IPV6:\n\t\tproto = htons(ETH_P_IPV6);\n\n\t\tkey_control->flags |= FLOW_DIS_ENCAPSULATION;\n\t\tif (flags & FLOW_DISSECTOR_F_STOP_AT_ENCAP) {\n\t\t\tfdret = FLOW_DISSECT_RET_OUT_GOOD;\n\t\t\tbreak;\n\t\t}\n\n\t\tfdret = FLOW_DISSECT_RET_PROTO_AGAIN;\n\t\tbreak;\n\n\n\tcase IPPROTO_MPLS:\n\t\tproto = htons(ETH_P_MPLS_UC);\n\t\tfdret = FLOW_DISSECT_RET_PROTO_AGAIN;\n\t\tbreak;\n\n\tcase IPPROTO_TCP:\n\t\t__skb_flow_dissect_tcp(skb, flow_dissector, target_container,\n\t\t\t\t       data, nhoff, hlen);\n\t\tbreak;\n\n\tcase IPPROTO_ICMP:\n\tcase IPPROTO_ICMPV6:\n\t\t__skb_flow_dissect_icmp(skb, flow_dissector, target_container,\n\t\t\t\t\tdata, nhoff, hlen);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (!(key_control->flags & FLOW_DIS_IS_FRAGMENT))\n\t\t__skb_flow_dissect_ports(skb, flow_dissector, target_container,\n\t\t\t\t\t data, nhoff, ip_proto, hlen);\n\n\t/* Process result of IP proto processing */\n\tswitch (fdret) {\n\tcase FLOW_DISSECT_RET_PROTO_AGAIN:\n\t\tif (skb_flow_dissect_allowed(&num_hdrs))\n\t\t\tgoto proto_again;\n\t\tbreak;\n\tcase FLOW_DISSECT_RET_IPPROTO_AGAIN:\n\t\tif (skb_flow_dissect_allowed(&num_hdrs))\n\t\t\tgoto ip_proto_again;\n\t\tbreak;\n\tcase FLOW_DISSECT_RET_OUT_GOOD:\n\tcase FLOW_DISSECT_RET_CONTINUE:\n\t\tbreak;\n\tcase FLOW_DISSECT_RET_OUT_BAD:\n\tdefault:\n\t\tgoto out_bad;\n\t}\n\nout_good:\n\tret = true;\n\nout:\n\tkey_control->thoff = min_t(u16, nhoff, skb ? skb->len : hlen);\n\tkey_basic->n_proto = proto;\n\tkey_basic->ip_proto = ip_proto;\n\n\treturn ret;\n\nout_bad:\n\tret = false;\n\tgoto out;\n}\nEXPORT_SYMBOL(__skb_flow_dissect);\n\nstatic siphash_key_t hashrnd __read_mostly;\nstatic __always_inline void __flow_hash_secret_init(void)\n{\n\tnet_get_random_once(&hashrnd, sizeof(hashrnd));\n}\n\nstatic const void *flow_keys_hash_start(const struct flow_keys *flow)\n{\n\tBUILD_BUG_ON(FLOW_KEYS_HASH_OFFSET % SIPHASH_ALIGNMENT);\n\treturn &flow->FLOW_KEYS_HASH_START_FIELD;\n}\n\nstatic inline size_t flow_keys_hash_length(const struct flow_keys *flow)\n{\n\tsize_t diff = FLOW_KEYS_HASH_OFFSET + sizeof(flow->addrs);\n\n\tBUILD_BUG_ON((sizeof(*flow) - FLOW_KEYS_HASH_OFFSET) % sizeof(u32));\n\n\tswitch (flow->control.addr_type) {\n\tcase FLOW_DISSECTOR_KEY_IPV4_ADDRS:\n\t\tdiff -= sizeof(flow->addrs.v4addrs);\n\t\tbreak;\n\tcase FLOW_DISSECTOR_KEY_IPV6_ADDRS:\n\t\tdiff -= sizeof(flow->addrs.v6addrs);\n\t\tbreak;\n\tcase FLOW_DISSECTOR_KEY_TIPC:\n\t\tdiff -= sizeof(flow->addrs.tipckey);\n\t\tbreak;\n\t}\n\treturn sizeof(*flow) - diff;\n}\n\n__be32 flow_get_u32_src(const struct flow_keys *flow)\n{\n\tswitch (flow->control.addr_type) {\n\tcase FLOW_DISSECTOR_KEY_IPV4_ADDRS:\n\t\treturn flow->addrs.v4addrs.src;\n\tcase FLOW_DISSECTOR_KEY_IPV6_ADDRS:\n\t\treturn (__force __be32)ipv6_addr_hash(\n\t\t\t&flow->addrs.v6addrs.src);\n\tcase FLOW_DISSECTOR_KEY_TIPC:\n\t\treturn flow->addrs.tipckey.key;\n\tdefault:\n\t\treturn 0;\n\t}\n}\nEXPORT_SYMBOL(flow_get_u32_src);\n\n__be32 flow_get_u32_dst(const struct flow_keys *flow)\n{\n\tswitch (flow->control.addr_type) {\n\tcase FLOW_DISSECTOR_KEY_IPV4_ADDRS:\n\t\treturn flow->addrs.v4addrs.dst;\n\tcase FLOW_DISSECTOR_KEY_IPV6_ADDRS:\n\t\treturn (__force __be32)ipv6_addr_hash(\n\t\t\t&flow->addrs.v6addrs.dst);\n\tdefault:\n\t\treturn 0;\n\t}\n}\nEXPORT_SYMBOL(flow_get_u32_dst);\n\n/* Sort the source and destination IP (and the ports if the IP are the same),\n * to have consistent hash within the two directions\n */\nstatic inline void __flow_hash_consistentify(struct flow_keys *keys)\n{\n\tint addr_diff, i;\n\n\tswitch (keys->control.addr_type) {\n\tcase FLOW_DISSECTOR_KEY_IPV4_ADDRS:\n\t\taddr_diff = (__force u32)keys->addrs.v4addrs.dst -\n\t\t\t    (__force u32)keys->addrs.v4addrs.src;\n\t\tif ((addr_diff < 0) ||\n\t\t    (addr_diff == 0 &&\n\t\t     ((__force u16)keys->ports.dst <\n\t\t      (__force u16)keys->ports.src))) {\n\t\t\tswap(keys->addrs.v4addrs.src, keys->addrs.v4addrs.dst);\n\t\t\tswap(keys->ports.src, keys->ports.dst);\n\t\t}\n\t\tbreak;\n\tcase FLOW_DISSECTOR_KEY_IPV6_ADDRS:\n\t\taddr_diff = memcmp(&keys->addrs.v6addrs.dst,\n\t\t\t\t   &keys->addrs.v6addrs.src,\n\t\t\t\t   sizeof(keys->addrs.v6addrs.dst));\n\t\tif ((addr_diff < 0) ||\n\t\t    (addr_diff == 0 &&\n\t\t     ((__force u16)keys->ports.dst <\n\t\t      (__force u16)keys->ports.src))) {\n\t\t\tfor (i = 0; i < 4; i++)\n\t\t\t\tswap(keys->addrs.v6addrs.src.s6_addr32[i],\n\t\t\t\t     keys->addrs.v6addrs.dst.s6_addr32[i]);\n\t\t\tswap(keys->ports.src, keys->ports.dst);\n\t\t}\n\t\tbreak;\n\t}\n}\n\nstatic inline u32 __flow_hash_from_keys(struct flow_keys *keys,\n\t\t\t\t\tconst siphash_key_t *keyval)\n{\n\tu32 hash;\n\n\t__flow_hash_consistentify(keys);\n\n\thash = siphash(flow_keys_hash_start(keys),\n\t\t       flow_keys_hash_length(keys), keyval);\n\tif (!hash)\n\t\thash = 1;\n\n\treturn hash;\n}\n\nu32 flow_hash_from_keys(struct flow_keys *keys)\n{\n\t__flow_hash_secret_init();\n\treturn __flow_hash_from_keys(keys, &hashrnd);\n}\nEXPORT_SYMBOL(flow_hash_from_keys);\n\nstatic inline u32 ___skb_get_hash(const struct sk_buff *skb,\n\t\t\t\t  struct flow_keys *keys,\n\t\t\t\t  const siphash_key_t *keyval)\n{\n\tskb_flow_dissect_flow_keys(skb, keys,\n\t\t\t\t   FLOW_DISSECTOR_F_STOP_AT_FLOW_LABEL);\n\n\treturn __flow_hash_from_keys(keys, keyval);\n}\n\nstruct _flow_keys_digest_data {\n\t__be16\tn_proto;\n\tu8\tip_proto;\n\tu8\tpadding;\n\t__be32\tports;\n\t__be32\tsrc;\n\t__be32\tdst;\n};\n\nvoid make_flow_keys_digest(struct flow_keys_digest *digest,\n\t\t\t   const struct flow_keys *flow)\n{\n\tstruct _flow_keys_digest_data *data =\n\t    (struct _flow_keys_digest_data *)digest;\n\n\tBUILD_BUG_ON(sizeof(*data) > sizeof(*digest));\n\n\tmemset(digest, 0, sizeof(*digest));\n\n\tdata->n_proto = flow->basic.n_proto;\n\tdata->ip_proto = flow->basic.ip_proto;\n\tdata->ports = flow->ports.ports;\n\tdata->src = flow->addrs.v4addrs.src;\n\tdata->dst = flow->addrs.v4addrs.dst;\n}\nEXPORT_SYMBOL(make_flow_keys_digest);\n\nstatic struct flow_dissector flow_keys_dissector_symmetric __read_mostly;\n\nu32 __skb_get_hash_symmetric(const struct sk_buff *skb)\n{\n\tstruct flow_keys keys;\n\n\t__flow_hash_secret_init();\n\n\tmemset(&keys, 0, sizeof(keys));\n\t__skb_flow_dissect(NULL, skb, &flow_keys_dissector_symmetric,\n\t\t\t   &keys, NULL, 0, 0, 0,\n\t\t\t   FLOW_DISSECTOR_F_STOP_AT_FLOW_LABEL);\n\n\treturn __flow_hash_from_keys(&keys, &hashrnd);\n}\nEXPORT_SYMBOL_GPL(__skb_get_hash_symmetric);\n\n/**\n * __skb_get_hash: calculate a flow hash\n * @skb: sk_buff to calculate flow hash from\n *\n * This function calculates a flow hash based on src/dst addresses\n * and src/dst port numbers.  Sets hash in skb to non-zero hash value\n * on success, zero indicates no valid hash.  Also, sets l4_hash in skb\n * if hash is a canonical 4-tuple hash over transport ports.\n */\nvoid __skb_get_hash(struct sk_buff *skb)\n{\n\tstruct flow_keys keys;\n\tu32 hash;\n\n\t__flow_hash_secret_init();\n\n\thash = ___skb_get_hash(skb, &keys, &hashrnd);\n\n\t__skb_set_sw_hash(skb, hash, flow_keys_have_l4(&keys));\n}\nEXPORT_SYMBOL(__skb_get_hash);\n\n__u32 skb_get_hash_perturb(const struct sk_buff *skb,\n\t\t\t   const siphash_key_t *perturb)\n{\n\tstruct flow_keys keys;\n\n\treturn ___skb_get_hash(skb, &keys, perturb);\n}\nEXPORT_SYMBOL(skb_get_hash_perturb);\n\nu32 __skb_get_poff(const struct sk_buff *skb, void *data,\n\t\t   const struct flow_keys_basic *keys, int hlen)\n{\n\tu32 poff = keys->control.thoff;\n\n\t/* skip L4 headers for fragments after the first */\n\tif ((keys->control.flags & FLOW_DIS_IS_FRAGMENT) &&\n\t    !(keys->control.flags & FLOW_DIS_FIRST_FRAG))\n\t\treturn poff;\n\n\tswitch (keys->basic.ip_proto) {\n\tcase IPPROTO_TCP: {\n\t\t/* access doff as u8 to avoid unaligned access */\n\t\tconst u8 *doff;\n\t\tu8 _doff;\n\n\t\tdoff = __skb_header_pointer(skb, poff + 12, sizeof(_doff),\n\t\t\t\t\t    data, hlen, &_doff);\n\t\tif (!doff)\n\t\t\treturn poff;\n\n\t\tpoff += max_t(u32, sizeof(struct tcphdr), (*doff & 0xF0) >> 2);\n\t\tbreak;\n\t}\n\tcase IPPROTO_UDP:\n\tcase IPPROTO_UDPLITE:\n\t\tpoff += sizeof(struct udphdr);\n\t\tbreak;\n\t/* For the rest, we do not really care about header\n\t * extensions at this point for now.\n\t */\n\tcase IPPROTO_ICMP:\n\t\tpoff += sizeof(struct icmphdr);\n\t\tbreak;\n\tcase IPPROTO_ICMPV6:\n\t\tpoff += sizeof(struct icmp6hdr);\n\t\tbreak;\n\tcase IPPROTO_IGMP:\n\t\tpoff += sizeof(struct igmphdr);\n\t\tbreak;\n\tcase IPPROTO_DCCP:\n\t\tpoff += sizeof(struct dccp_hdr);\n\t\tbreak;\n\tcase IPPROTO_SCTP:\n\t\tpoff += sizeof(struct sctphdr);\n\t\tbreak;\n\t}\n\n\treturn poff;\n}\n\n/**\n * skb_get_poff - get the offset to the payload\n * @skb: sk_buff to get the payload offset from\n *\n * The function will get the offset to the payload as far as it could\n * be dissected.  The main user is currently BPF, so that we can dynamically\n * truncate packets without needing to push actual payload to the user\n * space and can analyze headers only, instead.\n */\nu32 skb_get_poff(const struct sk_buff *skb)\n{\n\tstruct flow_keys_basic keys;\n\n\tif (!skb_flow_dissect_flow_keys_basic(NULL, skb, &keys,\n\t\t\t\t\t      NULL, 0, 0, 0, 0))\n\t\treturn 0;\n\n\treturn __skb_get_poff(skb, skb->data, &keys, skb_headlen(skb));\n}\n\n__u32 __get_hash_from_flowi6(const struct flowi6 *fl6, struct flow_keys *keys)\n{\n\tmemset(keys, 0, sizeof(*keys));\n\n\tmemcpy(&keys->addrs.v6addrs.src, &fl6->saddr,\n\t    sizeof(keys->addrs.v6addrs.src));\n\tmemcpy(&keys->addrs.v6addrs.dst, &fl6->daddr,\n\t    sizeof(keys->addrs.v6addrs.dst));\n\tkeys->control.addr_type = FLOW_DISSECTOR_KEY_IPV6_ADDRS;\n\tkeys->ports.src = fl6->fl6_sport;\n\tkeys->ports.dst = fl6->fl6_dport;\n\tkeys->keyid.keyid = fl6->fl6_gre_key;\n\tkeys->tags.flow_label = (__force u32)flowi6_get_flowlabel(fl6);\n\tkeys->basic.ip_proto = fl6->flowi6_proto;\n\n\treturn flow_hash_from_keys(keys);\n}\nEXPORT_SYMBOL(__get_hash_from_flowi6);\n\nstatic const struct flow_dissector_key flow_keys_dissector_keys[] = {\n\t{\n\t\t.key_id = FLOW_DISSECTOR_KEY_CONTROL,\n\t\t.offset = offsetof(struct flow_keys, control),\n\t},\n\t{\n\t\t.key_id = FLOW_DISSECTOR_KEY_BASIC,\n\t\t.offset = offsetof(struct flow_keys, basic),\n\t},\n\t{\n\t\t.key_id = FLOW_DISSECTOR_KEY_IPV4_ADDRS,\n\t\t.offset = offsetof(struct flow_keys, addrs.v4addrs),\n\t},\n\t{\n\t\t.key_id = FLOW_DISSECTOR_KEY_IPV6_ADDRS,\n\t\t.offset = offsetof(struct flow_keys, addrs.v6addrs),\n\t},\n\t{\n\t\t.key_id = FLOW_DISSECTOR_KEY_TIPC,\n\t\t.offset = offsetof(struct flow_keys, addrs.tipckey),\n\t},\n\t{\n\t\t.key_id = FLOW_DISSECTOR_KEY_PORTS,\n\t\t.offset = offsetof(struct flow_keys, ports),\n\t},\n\t{\n\t\t.key_id = FLOW_DISSECTOR_KEY_VLAN,\n\t\t.offset = offsetof(struct flow_keys, vlan),\n\t},\n\t{\n\t\t.key_id = FLOW_DISSECTOR_KEY_FLOW_LABEL,\n\t\t.offset = offsetof(struct flow_keys, tags),\n\t},\n\t{\n\t\t.key_id = FLOW_DISSECTOR_KEY_GRE_KEYID,\n\t\t.offset = offsetof(struct flow_keys, keyid),\n\t},\n};\n\nstatic const struct flow_dissector_key flow_keys_dissector_symmetric_keys[] = {\n\t{\n\t\t.key_id = FLOW_DISSECTOR_KEY_CONTROL,\n\t\t.offset = offsetof(struct flow_keys, control),\n\t},\n\t{\n\t\t.key_id = FLOW_DISSECTOR_KEY_BASIC,\n\t\t.offset = offsetof(struct flow_keys, basic),\n\t},\n\t{\n\t\t.key_id = FLOW_DISSECTOR_KEY_IPV4_ADDRS,\n\t\t.offset = offsetof(struct flow_keys, addrs.v4addrs),\n\t},\n\t{\n\t\t.key_id = FLOW_DISSECTOR_KEY_IPV6_ADDRS,\n\t\t.offset = offsetof(struct flow_keys, addrs.v6addrs),\n\t},\n\t{\n\t\t.key_id = FLOW_DISSECTOR_KEY_PORTS,\n\t\t.offset = offsetof(struct flow_keys, ports),\n\t},\n};\n\nstatic const struct flow_dissector_key flow_keys_basic_dissector_keys[] = {\n\t{\n\t\t.key_id = FLOW_DISSECTOR_KEY_CONTROL,\n\t\t.offset = offsetof(struct flow_keys, control),\n\t},\n\t{\n\t\t.key_id = FLOW_DISSECTOR_KEY_BASIC,\n\t\t.offset = offsetof(struct flow_keys, basic),\n\t},\n};\n\nstruct flow_dissector flow_keys_dissector __read_mostly;\nEXPORT_SYMBOL(flow_keys_dissector);\n\nstruct flow_dissector flow_keys_basic_dissector __read_mostly;\nEXPORT_SYMBOL(flow_keys_basic_dissector);\n\nstatic int __init init_default_flow_dissectors(void)\n{\n\tskb_flow_dissector_init(&flow_keys_dissector,\n\t\t\t\tflow_keys_dissector_keys,\n\t\t\t\tARRAY_SIZE(flow_keys_dissector_keys));\n\tskb_flow_dissector_init(&flow_keys_dissector_symmetric,\n\t\t\t\tflow_keys_dissector_symmetric_keys,\n\t\t\t\tARRAY_SIZE(flow_keys_dissector_symmetric_keys));\n\tskb_flow_dissector_init(&flow_keys_basic_dissector,\n\t\t\t\tflow_keys_basic_dissector_keys,\n\t\t\t\tARRAY_SIZE(flow_keys_basic_dissector_keys));\n\treturn 0;\n}\ncore_initcall(init_default_flow_dissectors);\n"}, "3": {"id": 3, "path": "/src/include/asm-generic/bug.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_GENERIC_BUG_H\n#define _ASM_GENERIC_BUG_H\n\n#include <linux/compiler.h>\n#include <linux/instrumentation.h>\n\n#define CUT_HERE\t\t\"------------[ cut here ]------------\\n\"\n\n#ifdef CONFIG_GENERIC_BUG\n#define BUGFLAG_WARNING\t\t(1 << 0)\n#define BUGFLAG_ONCE\t\t(1 << 1)\n#define BUGFLAG_DONE\t\t(1 << 2)\n#define BUGFLAG_NO_CUT_HERE\t(1 << 3)\t/* CUT_HERE already sent */\n#define BUGFLAG_TAINT(taint)\t((taint) << 8)\n#define BUG_GET_TAINT(bug)\t((bug)->flags >> 8)\n#endif\n\n#ifndef __ASSEMBLY__\n#include <linux/kernel.h>\n\n#ifdef CONFIG_BUG\n\n#ifdef CONFIG_GENERIC_BUG\nstruct bug_entry {\n#ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS\n\tunsigned long\tbug_addr;\n#else\n\tsigned int\tbug_addr_disp;\n#endif\n#ifdef CONFIG_DEBUG_BUGVERBOSE\n#ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS\n\tconst char\t*file;\n#else\n\tsigned int\tfile_disp;\n#endif\n\tunsigned short\tline;\n#endif\n\tunsigned short\tflags;\n};\n#endif\t/* CONFIG_GENERIC_BUG */\n\n/*\n * Don't use BUG() or BUG_ON() unless there's really no way out; one\n * example might be detecting data structure corruption in the middle\n * of an operation that can't be backed out of.  If the (sub)system\n * can somehow continue operating, perhaps with reduced functionality,\n * it's probably not BUG-worthy.\n *\n * If you're tempted to BUG(), think again:  is completely giving up\n * really the *only* solution?  There are usually better options, where\n * users don't need to reboot ASAP and can mostly shut down cleanly.\n */\n#ifndef HAVE_ARCH_BUG\n#define BUG() do { \\\n\tprintk(\"BUG: failure at %s:%d/%s()!\\n\", __FILE__, __LINE__, __func__); \\\n\tbarrier_before_unreachable(); \\\n\tpanic(\"BUG!\"); \\\n} while (0)\n#endif\n\n#ifndef HAVE_ARCH_BUG_ON\n#define BUG_ON(condition) do { if (unlikely(condition)) BUG(); } while (0)\n#endif\n\n/*\n * WARN(), WARN_ON(), WARN_ON_ONCE, and so on can be used to report\n * significant kernel issues that need prompt attention if they should ever\n * appear at runtime.\n *\n * Do not use these macros when checking for invalid external inputs\n * (e.g. invalid system call arguments, or invalid data coming from\n * network/devices), and on transient conditions like ENOMEM or EAGAIN.\n * These macros should be used for recoverable kernel issues only.\n * For invalid external inputs, transient conditions, etc use\n * pr_err[_once/_ratelimited]() followed by dump_stack(), if necessary.\n * Do not include \"BUG\"/\"WARNING\" in format strings manually to make these\n * conditions distinguishable from kernel issues.\n *\n * Use the versions with printk format strings to provide better diagnostics.\n */\n#ifndef __WARN_FLAGS\nextern __printf(4, 5)\nvoid warn_slowpath_fmt(const char *file, const int line, unsigned taint,\n\t\t       const char *fmt, ...);\n#define __WARN()\t\t__WARN_printf(TAINT_WARN, NULL)\n#define __WARN_printf(taint, arg...) do {\t\t\t\t\\\n\t\tinstrumentation_begin();\t\t\t\t\\\n\t\twarn_slowpath_fmt(__FILE__, __LINE__, taint, arg);\t\\\n\t\tinstrumentation_end();\t\t\t\t\t\\\n\t} while (0)\n#else\nextern __printf(1, 2) void __warn_printk(const char *fmt, ...);\n#define __WARN()\t\t__WARN_FLAGS(BUGFLAG_TAINT(TAINT_WARN))\n#define __WARN_printf(taint, arg...) do {\t\t\t\t\\\n\t\tinstrumentation_begin();\t\t\t\t\\\n\t\t__warn_printk(arg);\t\t\t\t\t\\\n\t\t__WARN_FLAGS(BUGFLAG_NO_CUT_HERE | BUGFLAG_TAINT(taint));\\\n\t\tinstrumentation_end();\t\t\t\t\t\\\n\t} while (0)\n#define WARN_ON_ONCE(condition) ({\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\\\n\t\t__WARN_FLAGS(BUGFLAG_ONCE |\t\t\t\\\n\t\t\t     BUGFLAG_TAINT(TAINT_WARN));\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\\\n})\n#endif\n\n/* used internally by panic.c */\nstruct warn_args;\nstruct pt_regs;\n\nvoid __warn(const char *file, int line, void *caller, unsigned taint,\n\t    struct pt_regs *regs, struct warn_args *args);\n\n#ifndef WARN_ON\n#define WARN_ON(condition) ({\t\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\t\\\n\t\t__WARN();\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#ifndef WARN\n#define WARN(condition, format...) ({\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\t\\\n\t\t__WARN_printf(TAINT_WARN, format);\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#define WARN_TAINT(condition, taint, format...) ({\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\t\\\n\t\t__WARN_printf(taint, format);\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n\n#ifndef WARN_ON_ONCE\n#define WARN_ON_ONCE(condition)\t({\t\t\t\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\\\n\tint __ret_warn_once = !!(condition);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (unlikely(__ret_warn_once && !__warned)) {\t\t\\\n\t\t__warned = true;\t\t\t\t\\\n\t\tWARN_ON(1);\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\\\n})\n#endif\n\n#define WARN_ONCE(condition, format...)\t({\t\t\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\\\n\tint __ret_warn_once = !!(condition);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (unlikely(__ret_warn_once && !__warned)) {\t\t\\\n\t\t__warned = true;\t\t\t\t\\\n\t\tWARN(1, format);\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\\\n})\n\n#define WARN_TAINT_ONCE(condition, taint, format...)\t({\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\\\n\tint __ret_warn_once = !!(condition);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (unlikely(__ret_warn_once && !__warned)) {\t\t\\\n\t\t__warned = true;\t\t\t\t\\\n\t\tWARN_TAINT(1, taint, format);\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\\\n})\n\n#else /* !CONFIG_BUG */\n#ifndef HAVE_ARCH_BUG\n#define BUG() do {} while (1)\n#endif\n\n#ifndef HAVE_ARCH_BUG_ON\n#define BUG_ON(condition) do { if (unlikely(condition)) BUG(); } while (0)\n#endif\n\n#ifndef HAVE_ARCH_WARN_ON\n#define WARN_ON(condition) ({\t\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#ifndef WARN\n#define WARN(condition, format...) ({\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tno_printk(format);\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#define WARN_ON_ONCE(condition) WARN_ON(condition)\n#define WARN_ONCE(condition, format...) WARN(condition, format)\n#define WARN_TAINT(condition, taint, format...) WARN(condition, format)\n#define WARN_TAINT_ONCE(condition, taint, format...) WARN(condition, format)\n\n#endif\n\n/*\n * WARN_ON_SMP() is for cases that the warning is either\n * meaningless for !SMP or may even cause failures.\n * It can also be used with values that are only defined\n * on SMP:\n *\n * struct foo {\n *  [...]\n * #ifdef CONFIG_SMP\n *\tint bar;\n * #endif\n * };\n *\n * void func(struct foo *zoot)\n * {\n *\tWARN_ON_SMP(!zoot->bar);\n *\n * For CONFIG_SMP, WARN_ON_SMP() should act the same as WARN_ON(),\n * and should be a nop and return false for uniprocessor.\n *\n * if (WARN_ON_SMP(x)) returns true only when CONFIG_SMP is set\n * and x is true.\n */\n#ifdef CONFIG_SMP\n# define WARN_ON_SMP(x)\t\t\tWARN_ON(x)\n#else\n/*\n * Use of ({0;}) because WARN_ON_SMP(x) may be used either as\n * a stand alone line statement or as a condition in an if ()\n * statement.\n * A simple \"0\" would cause gcc to give a \"statement has no effect\"\n * warning.\n */\n# define WARN_ON_SMP(x)\t\t\t({0;})\n#endif\n\n#endif /* __ASSEMBLY__ */\n\n#endif\n"}, "4": {"id": 4, "path": "/src/arch/x86/include/asm/bug.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_X86_BUG_H\n#define _ASM_X86_BUG_H\n\n#include <linux/stringify.h>\n#include <linux/instrumentation.h>\n\n/*\n * Despite that some emulators terminate on UD2, we use it for WARN().\n *\n * Since various instruction decoders/specs disagree on the encoding of\n * UD0/UD1.\n */\n\n#define ASM_UD0\t\t\".byte 0x0f, 0xff\" /* + ModRM (for Intel) */\n#define ASM_UD1\t\t\".byte 0x0f, 0xb9\" /* + ModRM */\n#define ASM_UD2\t\t\".byte 0x0f, 0x0b\"\n\n#define INSN_UD0\t0xff0f\n#define INSN_UD2\t0x0b0f\n\n#define LEN_UD2\t\t2\n\n#ifdef CONFIG_GENERIC_BUG\n\n#ifdef CONFIG_X86_32\n# define __BUG_REL(val)\t\".long \" __stringify(val)\n#else\n# define __BUG_REL(val)\t\".long \" __stringify(val) \" - 2b\"\n#endif\n\n#ifdef CONFIG_DEBUG_BUGVERBOSE\n\n#define _BUG_FLAGS(ins, flags)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tasm_inline volatile(\"1:\\t\" ins \"\\n\"\t\t\t\t\\\n\t\t     \".pushsection __bug_table,\\\"aw\\\"\\n\"\t\t\\\n\t\t     \"2:\\t\" __BUG_REL(1b) \"\\t# bug_entry::bug_addr\\n\"\t\\\n\t\t     \"\\t\"  __BUG_REL(%c0) \"\\t# bug_entry::file\\n\"\t\\\n\t\t     \"\\t.word %c1\"        \"\\t# bug_entry::line\\n\"\t\\\n\t\t     \"\\t.word %c2\"        \"\\t# bug_entry::flags\\n\"\t\\\n\t\t     \"\\t.org 2b+%c3\\n\"\t\t\t\t\t\\\n\t\t     \".popsection\"\t\t\t\t\t\\\n\t\t     : : \"i\" (__FILE__), \"i\" (__LINE__),\t\t\\\n\t\t\t \"i\" (flags),\t\t\t\t\t\\\n\t\t\t \"i\" (sizeof(struct bug_entry)));\t\t\\\n} while (0)\n\n#else /* !CONFIG_DEBUG_BUGVERBOSE */\n\n#define _BUG_FLAGS(ins, flags)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tasm_inline volatile(\"1:\\t\" ins \"\\n\"\t\t\t\t\\\n\t\t     \".pushsection __bug_table,\\\"aw\\\"\\n\"\t\t\\\n\t\t     \"2:\\t\" __BUG_REL(1b) \"\\t# bug_entry::bug_addr\\n\"\t\\\n\t\t     \"\\t.word %c0\"        \"\\t# bug_entry::flags\\n\"\t\\\n\t\t     \"\\t.org 2b+%c1\\n\"\t\t\t\t\t\\\n\t\t     \".popsection\"\t\t\t\t\t\\\n\t\t     : : \"i\" (flags),\t\t\t\t\t\\\n\t\t\t \"i\" (sizeof(struct bug_entry)));\t\t\\\n} while (0)\n\n#endif /* CONFIG_DEBUG_BUGVERBOSE */\n\n#else\n\n#define _BUG_FLAGS(ins, flags)  asm volatile(ins)\n\n#endif /* CONFIG_GENERIC_BUG */\n\n#define HAVE_ARCH_BUG\n#define BUG()\t\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tinstrumentation_begin();\t\t\t\t\\\n\t_BUG_FLAGS(ASM_UD2, 0);\t\t\t\t\t\\\n\tunreachable();\t\t\t\t\t\t\\\n} while (0)\n\n/*\n * This instrumentation_begin() is strictly speaking incorrect; but it\n * suppresses the complaints from WARN()s in noinstr code. If such a WARN()\n * were to trigger, we'd rather wreck the machine in an attempt to get the\n * message out than not know about it.\n */\n#define __WARN_FLAGS(flags)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tinstrumentation_begin();\t\t\t\t\\\n\t_BUG_FLAGS(ASM_UD2, BUGFLAG_WARNING|(flags));\t\t\\\n\tannotate_reachable();\t\t\t\t\t\\\n\tinstrumentation_end();\t\t\t\t\t\\\n} while (0)\n\n#include <asm-generic/bug.h>\n\n#endif /* _ASM_X86_BUG_H */\n"}, "5": {"id": 5, "path": "/src/include/linux/instrumentation.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_INSTRUMENTATION_H\n#define __LINUX_INSTRUMENTATION_H\n\n#if defined(CONFIG_DEBUG_ENTRY) && defined(CONFIG_STACK_VALIDATION)\n\n/* Begin/end of an instrumentation safe region */\n#define instrumentation_begin() ({\t\t\t\t\t\\\n\tasm volatile(\"%c0: nop\\n\\t\"\t\t\t\t\t\t\\\n\t\t     \".pushsection .discard.instr_begin\\n\\t\"\t\t\\\n\t\t     \".long %c0b - .\\n\\t\"\t\t\t\t\\\n\t\t     \".popsection\\n\\t\" : : \"i\" (__COUNTER__));\t\t\\\n})\n\n/*\n * Because instrumentation_{begin,end}() can nest, objtool validation considers\n * _begin() a +1 and _end() a -1 and computes a sum over the instructions.\n * When the value is greater than 0, we consider instrumentation allowed.\n *\n * There is a problem with code like:\n *\n * noinstr void foo()\n * {\n *\tinstrumentation_begin();\n *\t...\n *\tif (cond) {\n *\t\tinstrumentation_begin();\n *\t\t...\n *\t\tinstrumentation_end();\n *\t}\n *\tbar();\n *\tinstrumentation_end();\n * }\n *\n * If instrumentation_end() would be an empty label, like all the other\n * annotations, the inner _end(), which is at the end of a conditional block,\n * would land on the instruction after the block.\n *\n * If we then consider the sum of the !cond path, we'll see that the call to\n * bar() is with a 0-value, even though, we meant it to happen with a positive\n * value.\n *\n * To avoid this, have _end() be a NOP instruction, this ensures it will be\n * part of the condition block and does not escape.\n */\n#define instrumentation_end() ({\t\t\t\t\t\\\n\tasm volatile(\"%c0: nop\\n\\t\"\t\t\t\t\t\\\n\t\t     \".pushsection .discard.instr_end\\n\\t\"\t\t\\\n\t\t     \".long %c0b - .\\n\\t\"\t\t\t\t\\\n\t\t     \".popsection\\n\\t\" : : \"i\" (__COUNTER__));\t\t\\\n})\n#else\n# define instrumentation_begin()\tdo { } while(0)\n# define instrumentation_end()\t\tdo { } while(0)\n#endif\n\n#endif /* __LINUX_INSTRUMENTATION_H */\n"}, "6": {"id": 6, "path": "/src/include/linux/if_ether.h", "content": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tGlobal definitions for the Ethernet IEEE 802.3 interface.\n *\n * Version:\t@(#)if_ether.h\t1.0.1a\t02/08/94\n *\n * Author:\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tDonald Becker, <becker@super.org>\n *\t\tAlan Cox, <alan@lxorguk.ukuu.org.uk>\n *\t\tSteve Whitehouse, <gw7rrm@eeshack3.swan.ac.uk>\n */\n#ifndef _LINUX_IF_ETHER_H\n#define _LINUX_IF_ETHER_H\n\n#include <linux/skbuff.h>\n#include <uapi/linux/if_ether.h>\n\nstatic inline struct ethhdr *eth_hdr(const struct sk_buff *skb)\n{\n\treturn (struct ethhdr *)skb_mac_header(skb);\n}\n\n/* Prefer this version in TX path, instead of\n * skb_reset_mac_header() + eth_hdr()\n */\nstatic inline struct ethhdr *skb_eth_hdr(const struct sk_buff *skb)\n{\n\treturn (struct ethhdr *)skb->data;\n}\n\nstatic inline struct ethhdr *inner_eth_hdr(const struct sk_buff *skb)\n{\n\treturn (struct ethhdr *)skb_inner_mac_header(skb);\n}\n\nint eth_header_parse(const struct sk_buff *skb, unsigned char *haddr);\n\nextern ssize_t sysfs_format_mac(char *buf, const unsigned char *addr, int len);\n\n#endif\t/* _LINUX_IF_ETHER_H */\n"}, "7": {"id": 7, "path": "/src/include/asm-generic/rwonce.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n/*\n * Prevent the compiler from merging or refetching reads or writes. The\n * compiler is also forbidden from reordering successive instances of\n * READ_ONCE and WRITE_ONCE, but only when the compiler is aware of some\n * particular ordering. One way to make the compiler aware of ordering is to\n * put the two invocations of READ_ONCE or WRITE_ONCE in different C\n * statements.\n *\n * These two macros will also work on aggregate data types like structs or\n * unions.\n *\n * Their two major use cases are: (1) Mediating communication between\n * process-level code and irq/NMI handlers, all running on the same CPU,\n * and (2) Ensuring that the compiler does not fold, spindle, or otherwise\n * mutilate accesses that either do not require ordering or that interact\n * with an explicit memory barrier or atomic instruction that provides the\n * required ordering.\n */\n#ifndef __ASM_GENERIC_RWONCE_H\n#define __ASM_GENERIC_RWONCE_H\n\n#ifndef __ASSEMBLY__\n\n#include <linux/compiler_types.h>\n#include <linux/kasan-checks.h>\n#include <linux/kcsan-checks.h>\n\n/*\n * Yes, this permits 64-bit accesses on 32-bit architectures. These will\n * actually be atomic in some cases (namely Armv7 + LPAE), but for others we\n * rely on the access being split into 2x32-bit accesses for a 32-bit quantity\n * (e.g. a virtual address) and a strong prevailing wind.\n */\n#define compiletime_assert_rwonce_type(t)\t\t\t\t\t\\\n\tcompiletime_assert(__native_word(t) || sizeof(t) == sizeof(long long),\t\\\n\t\t\"Unsupported access size for {READ,WRITE}_ONCE().\")\n\n/*\n * Use __READ_ONCE() instead of READ_ONCE() if you do not require any\n * atomicity. Note that this may result in tears!\n */\n#ifndef __READ_ONCE\n#define __READ_ONCE(x)\t(*(const volatile __unqual_scalar_typeof(x) *)&(x))\n#endif\n\n#define READ_ONCE(x)\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tcompiletime_assert_rwonce_type(x);\t\t\t\t\\\n\t__READ_ONCE(x);\t\t\t\t\t\t\t\\\n})\n\n#define __WRITE_ONCE(x, val)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t*(volatile typeof(x) *)&(x) = (val);\t\t\t\t\\\n} while (0)\n\n#define WRITE_ONCE(x, val)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tcompiletime_assert_rwonce_type(x);\t\t\t\t\\\n\t__WRITE_ONCE(x, val);\t\t\t\t\t\t\\\n} while (0)\n\nstatic __no_sanitize_or_inline\nunsigned long __read_once_word_nocheck(const void *addr)\n{\n\treturn __READ_ONCE(*(unsigned long *)addr);\n}\n\n/*\n * Use READ_ONCE_NOCHECK() instead of READ_ONCE() if you need to load a\n * word from memory atomically but without telling KASAN/KCSAN. This is\n * usually used by unwinding code when walking the stack of a running process.\n */\n#define READ_ONCE_NOCHECK(x)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tcompiletime_assert(sizeof(x) == sizeof(unsigned long),\t\t\\\n\t\t\"Unsupported access size for READ_ONCE_NOCHECK().\");\t\\\n\t(typeof(x))__read_once_word_nocheck(&(x));\t\t\t\\\n})\n\nstatic __no_kasan_or_inline\nunsigned long read_word_at_a_time(const void *addr)\n{\n\tkasan_check_read(addr, 1);\n\treturn *(unsigned long *)addr;\n}\n\n#endif /* __ASSEMBLY__ */\n#endif\t/* __ASM_GENERIC_RWONCE_H */\n"}, "8": {"id": 8, "path": "/src/net/ipv4/ip_output.c", "content": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tThe Internet Protocol (IP) output module.\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tDonald Becker, <becker@super.org>\n *\t\tAlan Cox, <Alan.Cox@linux.org>\n *\t\tRichard Underwood\n *\t\tStefan Becker, <stefanb@yello.ping.de>\n *\t\tJorge Cwik, <jorge@laser.satlink.net>\n *\t\tArnt Gulbrandsen, <agulbra@nvg.unit.no>\n *\t\tHirokazu Takahashi, <taka@valinux.co.jp>\n *\n *\tSee ip_input.c for original log\n *\n *\tFixes:\n *\t\tAlan Cox\t:\tMissing nonblock feature in ip_build_xmit.\n *\t\tMike Kilburn\t:\thtons() missing in ip_build_xmit.\n *\t\tBradford Johnson:\tFix faulty handling of some frames when\n *\t\t\t\t\tno route is found.\n *\t\tAlexander Demenshin:\tMissing sk/skb free in ip_queue_xmit\n *\t\t\t\t\t(in case if packet not accepted by\n *\t\t\t\t\toutput firewall rules)\n *\t\tMike McLagan\t:\tRouting by source\n *\t\tAlexey Kuznetsov:\tuse new route cache\n *\t\tAndi Kleen:\t\tFix broken PMTU recovery and remove\n *\t\t\t\t\tsome redundant tests.\n *\tVitaly E. Lavrov\t:\tTransparent proxy revived after year coma.\n *\t\tAndi Kleen\t: \tReplace ip_reply with ip_send_reply.\n *\t\tAndi Kleen\t:\tSplit fast and slow ip_build_xmit path\n *\t\t\t\t\tfor decreased register pressure on x86\n *\t\t\t\t\tand more readibility.\n *\t\tMarc Boucher\t:\tWhen call_out_firewall returns FW_QUEUE,\n *\t\t\t\t\tsilently drop skb instead of failing with -EPERM.\n *\t\tDetlev Wengorz\t:\tCopy protocol for fragments.\n *\t\tHirokazu Takahashi:\tHW checksumming for outgoing UDP\n *\t\t\t\t\tdatagrams.\n *\t\tHirokazu Takahashi:\tsendfile() on UDP works now.\n */\n\n#include <linux/uaccess.h>\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/string.h>\n#include <linux/errno.h>\n#include <linux/highmem.h>\n#include <linux/slab.h>\n\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/proc_fs.h>\n#include <linux/stat.h>\n#include <linux/init.h>\n\n#include <net/snmp.h>\n#include <net/ip.h>\n#include <net/protocol.h>\n#include <net/route.h>\n#include <net/xfrm.h>\n#include <linux/skbuff.h>\n#include <net/sock.h>\n#include <net/arp.h>\n#include <net/icmp.h>\n#include <net/checksum.h>\n#include <net/inetpeer.h>\n#include <net/inet_ecn.h>\n#include <net/lwtunnel.h>\n#include <linux/bpf-cgroup.h>\n#include <linux/igmp.h>\n#include <linux/netfilter_ipv4.h>\n#include <linux/netfilter_bridge.h>\n#include <linux/netlink.h>\n#include <linux/tcp.h>\n\nstatic int\nip_fragment(struct net *net, struct sock *sk, struct sk_buff *skb,\n\t    unsigned int mtu,\n\t    int (*output)(struct net *, struct sock *, struct sk_buff *));\n\n/* Generate a checksum for an outgoing IP datagram. */\nvoid ip_send_check(struct iphdr *iph)\n{\n\tiph->check = 0;\n\tiph->check = ip_fast_csum((unsigned char *)iph, iph->ihl);\n}\nEXPORT_SYMBOL(ip_send_check);\n\nint __ip_local_out(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tstruct iphdr *iph = ip_hdr(skb);\n\n\tiph->tot_len = htons(skb->len);\n\tip_send_check(iph);\n\n\t/* if egress device is enslaved to an L3 master device pass the\n\t * skb to its handler for processing\n\t */\n\tskb = l3mdev_ip_out(sk, skb);\n\tif (unlikely(!skb))\n\t\treturn 0;\n\n\tskb->protocol = htons(ETH_P_IP);\n\n\treturn nf_hook(NFPROTO_IPV4, NF_INET_LOCAL_OUT,\n\t\t       net, sk, skb, NULL, skb_dst(skb)->dev,\n\t\t       dst_output);\n}\n\nint ip_local_out(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tint err;\n\n\terr = __ip_local_out(net, sk, skb);\n\tif (likely(err == 1))\n\t\terr = dst_output(net, sk, skb);\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(ip_local_out);\n\nstatic inline int ip_select_ttl(struct inet_sock *inet, struct dst_entry *dst)\n{\n\tint ttl = inet->uc_ttl;\n\n\tif (ttl < 0)\n\t\tttl = ip4_dst_hoplimit(dst);\n\treturn ttl;\n}\n\n/*\n *\t\tAdd an ip header to a skbuff and send it out.\n *\n */\nint ip_build_and_send_pkt(struct sk_buff *skb, const struct sock *sk,\n\t\t\t  __be32 saddr, __be32 daddr, struct ip_options_rcu *opt,\n\t\t\t  u8 tos)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct net *net = sock_net(sk);\n\tstruct iphdr *iph;\n\n\t/* Build the IP header. */\n\tskb_push(skb, sizeof(struct iphdr) + (opt ? opt->opt.optlen : 0));\n\tskb_reset_network_header(skb);\n\tiph = ip_hdr(skb);\n\tiph->version  = 4;\n\tiph->ihl      = 5;\n\tiph->tos      = tos;\n\tiph->ttl      = ip_select_ttl(inet, &rt->dst);\n\tiph->daddr    = (opt && opt->opt.srr ? opt->opt.faddr : daddr);\n\tiph->saddr    = saddr;\n\tiph->protocol = sk->sk_protocol;\n\tif (ip_dont_fragment(sk, &rt->dst)) {\n\t\tiph->frag_off = htons(IP_DF);\n\t\tiph->id = 0;\n\t} else {\n\t\tiph->frag_off = 0;\n\t\t__ip_select_ident(net, iph, 1);\n\t}\n\n\tif (opt && opt->opt.optlen) {\n\t\tiph->ihl += opt->opt.optlen>>2;\n\t\tip_options_build(skb, &opt->opt, daddr, rt, 0);\n\t}\n\n\tskb->priority = sk->sk_priority;\n\tif (!skb->mark)\n\t\tskb->mark = sk->sk_mark;\n\n\t/* Send it out. */\n\treturn ip_local_out(net, skb->sk, skb);\n}\nEXPORT_SYMBOL_GPL(ip_build_and_send_pkt);\n\nstatic int ip_finish_output2(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tstruct dst_entry *dst = skb_dst(skb);\n\tstruct rtable *rt = (struct rtable *)dst;\n\tstruct net_device *dev = dst->dev;\n\tunsigned int hh_len = LL_RESERVED_SPACE(dev);\n\tstruct neighbour *neigh;\n\tbool is_v6gw = false;\n\n\tif (rt->rt_type == RTN_MULTICAST) {\n\t\tIP_UPD_PO_STATS(net, IPSTATS_MIB_OUTMCAST, skb->len);\n\t} else if (rt->rt_type == RTN_BROADCAST)\n\t\tIP_UPD_PO_STATS(net, IPSTATS_MIB_OUTBCAST, skb->len);\n\n\t/* Be paranoid, rather than too clever. */\n\tif (unlikely(skb_headroom(skb) < hh_len && dev->header_ops)) {\n\t\tstruct sk_buff *skb2;\n\n\t\tskb2 = skb_realloc_headroom(skb, LL_RESERVED_SPACE(dev));\n\t\tif (!skb2) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tif (skb->sk)\n\t\t\tskb_set_owner_w(skb2, skb->sk);\n\t\tconsume_skb(skb);\n\t\tskb = skb2;\n\t}\n\n\tif (lwtunnel_xmit_redirect(dst->lwtstate)) {\n\t\tint res = lwtunnel_xmit(skb);\n\n\t\tif (res < 0 || res == LWTUNNEL_XMIT_DONE)\n\t\t\treturn res;\n\t}\n\n\trcu_read_lock_bh();\n\tneigh = ip_neigh_for_gw(rt, skb, &is_v6gw);\n\tif (!IS_ERR(neigh)) {\n\t\tint res;\n\n\t\tsock_confirm_neigh(skb, neigh);\n\t\t/* if crossing protocols, can not use the cached header */\n\t\tres = neigh_output(neigh, skb, is_v6gw);\n\t\trcu_read_unlock_bh();\n\t\treturn res;\n\t}\n\trcu_read_unlock_bh();\n\n\tnet_dbg_ratelimited(\"%s: No header cache and no neighbour!\\n\",\n\t\t\t    __func__);\n\tkfree_skb(skb);\n\treturn -EINVAL;\n}\n\nstatic int ip_finish_output_gso(struct net *net, struct sock *sk,\n\t\t\t\tstruct sk_buff *skb, unsigned int mtu)\n{\n\tstruct sk_buff *segs, *nskb;\n\tnetdev_features_t features;\n\tint ret = 0;\n\n\t/* common case: seglen is <= mtu\n\t */\n\tif (skb_gso_validate_network_len(skb, mtu))\n\t\treturn ip_finish_output2(net, sk, skb);\n\n\t/* Slowpath -  GSO segment length exceeds the egress MTU.\n\t *\n\t * This can happen in several cases:\n\t *  - Forwarding of a TCP GRO skb, when DF flag is not set.\n\t *  - Forwarding of an skb that arrived on a virtualization interface\n\t *    (virtio-net/vhost/tap) with TSO/GSO size set by other network\n\t *    stack.\n\t *  - Local GSO skb transmitted on an NETIF_F_TSO tunnel stacked over an\n\t *    interface with a smaller MTU.\n\t *  - Arriving GRO skb (or GSO skb in a virtualized environment) that is\n\t *    bridged to a NETIF_F_TSO tunnel stacked over an interface with an\n\t *    insufficent MTU.\n\t */\n\tfeatures = netif_skb_features(skb);\n\tBUILD_BUG_ON(sizeof(*IPCB(skb)) > SKB_GSO_CB_OFFSET);\n\tsegs = skb_gso_segment(skb, features & ~NETIF_F_GSO_MASK);\n\tif (IS_ERR_OR_NULL(segs)) {\n\t\tkfree_skb(skb);\n\t\treturn -ENOMEM;\n\t}\n\n\tconsume_skb(skb);\n\n\tskb_list_walk_safe(segs, segs, nskb) {\n\t\tint err;\n\n\t\tskb_mark_not_on_list(segs);\n\t\terr = ip_fragment(net, sk, segs, mtu, ip_finish_output2);\n\n\t\tif (err && ret == 0)\n\t\t\tret = err;\n\t}\n\n\treturn ret;\n}\n\nstatic int __ip_finish_output(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tunsigned int mtu;\n\n#if defined(CONFIG_NETFILTER) && defined(CONFIG_XFRM)\n\t/* Policy lookup after SNAT yielded a new policy */\n\tif (skb_dst(skb)->xfrm) {\n\t\tIPCB(skb)->flags |= IPSKB_REROUTED;\n\t\treturn dst_output(net, sk, skb);\n\t}\n#endif\n\tmtu = ip_skb_dst_mtu(sk, skb);\n\tif (skb_is_gso(skb))\n\t\treturn ip_finish_output_gso(net, sk, skb, mtu);\n\n\tif (skb->len > mtu || IPCB(skb)->frag_max_size)\n\t\treturn ip_fragment(net, sk, skb, mtu, ip_finish_output2);\n\n\treturn ip_finish_output2(net, sk, skb);\n}\n\nstatic int ip_finish_output(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tint ret;\n\n\tret = BPF_CGROUP_RUN_PROG_INET_EGRESS(sk, skb);\n\tswitch (ret) {\n\tcase NET_XMIT_SUCCESS:\n\t\treturn __ip_finish_output(net, sk, skb);\n\tcase NET_XMIT_CN:\n\t\treturn __ip_finish_output(net, sk, skb) ? : ret;\n\tdefault:\n\t\tkfree_skb(skb);\n\t\treturn ret;\n\t}\n}\n\nstatic int ip_mc_finish_output(struct net *net, struct sock *sk,\n\t\t\t       struct sk_buff *skb)\n{\n\tstruct rtable *new_rt;\n\tbool do_cn = false;\n\tint ret, err;\n\n\tret = BPF_CGROUP_RUN_PROG_INET_EGRESS(sk, skb);\n\tswitch (ret) {\n\tcase NET_XMIT_CN:\n\t\tdo_cn = true;\n\t\tfallthrough;\n\tcase NET_XMIT_SUCCESS:\n\t\tbreak;\n\tdefault:\n\t\tkfree_skb(skb);\n\t\treturn ret;\n\t}\n\n\t/* Reset rt_iif so that inet_iif() will return skb->skb_iif. Setting\n\t * this to non-zero causes ipi_ifindex in in_pktinfo to be overwritten,\n\t * see ipv4_pktinfo_prepare().\n\t */\n\tnew_rt = rt_dst_clone(net->loopback_dev, skb_rtable(skb));\n\tif (new_rt) {\n\t\tnew_rt->rt_iif = 0;\n\t\tskb_dst_drop(skb);\n\t\tskb_dst_set(skb, &new_rt->dst);\n\t}\n\n\terr = dev_loopback_xmit(net, sk, skb);\n\treturn (do_cn && err) ? ret : err;\n}\n\nint ip_mc_output(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct net_device *dev = rt->dst.dev;\n\n\t/*\n\t *\tIf the indicated interface is up and running, send the packet.\n\t */\n\tIP_UPD_PO_STATS(net, IPSTATS_MIB_OUT, skb->len);\n\n\tskb->dev = dev;\n\tskb->protocol = htons(ETH_P_IP);\n\n\t/*\n\t *\tMulticasts are looped back for other local users\n\t */\n\n\tif (rt->rt_flags&RTCF_MULTICAST) {\n\t\tif (sk_mc_loop(sk)\n#ifdef CONFIG_IP_MROUTE\n\t\t/* Small optimization: do not loopback not local frames,\n\t\t   which returned after forwarding; they will be  dropped\n\t\t   by ip_mr_input in any case.\n\t\t   Note, that local frames are looped back to be delivered\n\t\t   to local recipients.\n\n\t\t   This check is duplicated in ip_mr_input at the moment.\n\t\t */\n\t\t    &&\n\t\t    ((rt->rt_flags & RTCF_LOCAL) ||\n\t\t     !(IPCB(skb)->flags & IPSKB_FORWARDED))\n#endif\n\t\t   ) {\n\t\t\tstruct sk_buff *newskb = skb_clone(skb, GFP_ATOMIC);\n\t\t\tif (newskb)\n\t\t\t\tNF_HOOK(NFPROTO_IPV4, NF_INET_POST_ROUTING,\n\t\t\t\t\tnet, sk, newskb, NULL, newskb->dev,\n\t\t\t\t\tip_mc_finish_output);\n\t\t}\n\n\t\t/* Multicasts with ttl 0 must not go beyond the host */\n\n\t\tif (ip_hdr(skb)->ttl == 0) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif (rt->rt_flags&RTCF_BROADCAST) {\n\t\tstruct sk_buff *newskb = skb_clone(skb, GFP_ATOMIC);\n\t\tif (newskb)\n\t\t\tNF_HOOK(NFPROTO_IPV4, NF_INET_POST_ROUTING,\n\t\t\t\tnet, sk, newskb, NULL, newskb->dev,\n\t\t\t\tip_mc_finish_output);\n\t}\n\n\treturn NF_HOOK_COND(NFPROTO_IPV4, NF_INET_POST_ROUTING,\n\t\t\t    net, sk, skb, NULL, skb->dev,\n\t\t\t    ip_finish_output,\n\t\t\t    !(IPCB(skb)->flags & IPSKB_REROUTED));\n}\n\nint ip_output(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb_dst(skb)->dev, *indev = skb->dev;\n\n\tIP_UPD_PO_STATS(net, IPSTATS_MIB_OUT, skb->len);\n\n\tskb->dev = dev;\n\tskb->protocol = htons(ETH_P_IP);\n\n\treturn NF_HOOK_COND(NFPROTO_IPV4, NF_INET_POST_ROUTING,\n\t\t\t    net, sk, skb, indev, dev,\n\t\t\t    ip_finish_output,\n\t\t\t    !(IPCB(skb)->flags & IPSKB_REROUTED));\n}\nEXPORT_SYMBOL(ip_output);\n\n/*\n * copy saddr and daddr, possibly using 64bit load/stores\n * Equivalent to :\n *   iph->saddr = fl4->saddr;\n *   iph->daddr = fl4->daddr;\n */\nstatic void ip_copy_addrs(struct iphdr *iph, const struct flowi4 *fl4)\n{\n\tBUILD_BUG_ON(offsetof(typeof(*fl4), daddr) !=\n\t\t     offsetof(typeof(*fl4), saddr) + sizeof(fl4->saddr));\n\tmemcpy(&iph->saddr, &fl4->saddr,\n\t       sizeof(fl4->saddr) + sizeof(fl4->daddr));\n}\n\n/* Note: skb->sk can be different from sk, in case of tunnels */\nint __ip_queue_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl,\n\t\t    __u8 tos)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct ip_options_rcu *inet_opt;\n\tstruct flowi4 *fl4;\n\tstruct rtable *rt;\n\tstruct iphdr *iph;\n\tint res;\n\n\t/* Skip all of this if the packet is already routed,\n\t * f.e. by something like SCTP.\n\t */\n\trcu_read_lock();\n\tinet_opt = rcu_dereference(inet->inet_opt);\n\tfl4 = &fl->u.ip4;\n\trt = skb_rtable(skb);\n\tif (rt)\n\t\tgoto packet_routed;\n\n\t/* Make sure we can route this packet. */\n\trt = (struct rtable *)__sk_dst_check(sk, 0);\n\tif (!rt) {\n\t\t__be32 daddr;\n\n\t\t/* Use correct destination address if we have options. */\n\t\tdaddr = inet->inet_daddr;\n\t\tif (inet_opt && inet_opt->opt.srr)\n\t\t\tdaddr = inet_opt->opt.faddr;\n\n\t\t/* If this fails, retransmit mechanism of transport layer will\n\t\t * keep trying until route appears or the connection times\n\t\t * itself out.\n\t\t */\n\t\trt = ip_route_output_ports(net, fl4, sk,\n\t\t\t\t\t   daddr, inet->inet_saddr,\n\t\t\t\t\t   inet->inet_dport,\n\t\t\t\t\t   inet->inet_sport,\n\t\t\t\t\t   sk->sk_protocol,\n\t\t\t\t\t   RT_CONN_FLAGS_TOS(sk, tos),\n\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\tif (IS_ERR(rt))\n\t\t\tgoto no_route;\n\t\tsk_setup_caps(sk, &rt->dst);\n\t}\n\tskb_dst_set_noref(skb, &rt->dst);\n\npacket_routed:\n\tif (inet_opt && inet_opt->opt.is_strictroute && rt->rt_uses_gateway)\n\t\tgoto no_route;\n\n\t/* OK, we know where to send it, allocate and build IP header. */\n\tskb_push(skb, sizeof(struct iphdr) + (inet_opt ? inet_opt->opt.optlen : 0));\n\tskb_reset_network_header(skb);\n\tiph = ip_hdr(skb);\n\t*((__be16 *)iph) = htons((4 << 12) | (5 << 8) | (tos & 0xff));\n\tif (ip_dont_fragment(sk, &rt->dst) && !skb->ignore_df)\n\t\tiph->frag_off = htons(IP_DF);\n\telse\n\t\tiph->frag_off = 0;\n\tiph->ttl      = ip_select_ttl(inet, &rt->dst);\n\tiph->protocol = sk->sk_protocol;\n\tip_copy_addrs(iph, fl4);\n\n\t/* Transport layer set skb->h.foo itself. */\n\n\tif (inet_opt && inet_opt->opt.optlen) {\n\t\tiph->ihl += inet_opt->opt.optlen >> 2;\n\t\tip_options_build(skb, &inet_opt->opt, inet->inet_daddr, rt, 0);\n\t}\n\n\tip_select_ident_segs(net, skb, sk,\n\t\t\t     skb_shinfo(skb)->gso_segs ?: 1);\n\n\t/* TODO : should we use skb->sk here instead of sk ? */\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\n\tres = ip_local_out(net, sk, skb);\n\trcu_read_unlock();\n\treturn res;\n\nno_route:\n\trcu_read_unlock();\n\tIP_INC_STATS(net, IPSTATS_MIB_OUTNOROUTES);\n\tkfree_skb(skb);\n\treturn -EHOSTUNREACH;\n}\nEXPORT_SYMBOL(__ip_queue_xmit);\n\nint ip_queue_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl)\n{\n\treturn __ip_queue_xmit(sk, skb, fl, inet_sk(sk)->tos);\n}\nEXPORT_SYMBOL(ip_queue_xmit);\n\nstatic void ip_copy_metadata(struct sk_buff *to, struct sk_buff *from)\n{\n\tto->pkt_type = from->pkt_type;\n\tto->priority = from->priority;\n\tto->protocol = from->protocol;\n\tto->skb_iif = from->skb_iif;\n\tskb_dst_drop(to);\n\tskb_dst_copy(to, from);\n\tto->dev = from->dev;\n\tto->mark = from->mark;\n\n\tskb_copy_hash(to, from);\n\n#ifdef CONFIG_NET_SCHED\n\tto->tc_index = from->tc_index;\n#endif\n\tnf_copy(to, from);\n\tskb_ext_copy(to, from);\n#if IS_ENABLED(CONFIG_IP_VS)\n\tto->ipvs_property = from->ipvs_property;\n#endif\n\tskb_copy_secmark(to, from);\n}\n\nstatic int ip_fragment(struct net *net, struct sock *sk, struct sk_buff *skb,\n\t\t       unsigned int mtu,\n\t\t       int (*output)(struct net *, struct sock *, struct sk_buff *))\n{\n\tstruct iphdr *iph = ip_hdr(skb);\n\n\tif ((iph->frag_off & htons(IP_DF)) == 0)\n\t\treturn ip_do_fragment(net, sk, skb, output);\n\n\tif (unlikely(!skb->ignore_df ||\n\t\t     (IPCB(skb)->frag_max_size &&\n\t\t      IPCB(skb)->frag_max_size > mtu))) {\n\t\tIP_INC_STATS(net, IPSTATS_MIB_FRAGFAILS);\n\t\ticmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED,\n\t\t\t  htonl(mtu));\n\t\tkfree_skb(skb);\n\t\treturn -EMSGSIZE;\n\t}\n\n\treturn ip_do_fragment(net, sk, skb, output);\n}\n\nvoid ip_fraglist_init(struct sk_buff *skb, struct iphdr *iph,\n\t\t      unsigned int hlen, struct ip_fraglist_iter *iter)\n{\n\tunsigned int first_len = skb_pagelen(skb);\n\n\titer->frag = skb_shinfo(skb)->frag_list;\n\tskb_frag_list_init(skb);\n\n\titer->offset = 0;\n\titer->iph = iph;\n\titer->hlen = hlen;\n\n\tskb->data_len = first_len - skb_headlen(skb);\n\tskb->len = first_len;\n\tiph->tot_len = htons(first_len);\n\tiph->frag_off = htons(IP_MF);\n\tip_send_check(iph);\n}\nEXPORT_SYMBOL(ip_fraglist_init);\n\nstatic void ip_fraglist_ipcb_prepare(struct sk_buff *skb,\n\t\t\t\t     struct ip_fraglist_iter *iter)\n{\n\tstruct sk_buff *to = iter->frag;\n\n\t/* Copy the flags to each fragment. */\n\tIPCB(to)->flags = IPCB(skb)->flags;\n\n\tif (iter->offset == 0)\n\t\tip_options_fragment(to);\n}\n\nvoid ip_fraglist_prepare(struct sk_buff *skb, struct ip_fraglist_iter *iter)\n{\n\tunsigned int hlen = iter->hlen;\n\tstruct iphdr *iph = iter->iph;\n\tstruct sk_buff *frag;\n\n\tfrag = iter->frag;\n\tfrag->ip_summed = CHECKSUM_NONE;\n\tskb_reset_transport_header(frag);\n\t__skb_push(frag, hlen);\n\tskb_reset_network_header(frag);\n\tmemcpy(skb_network_header(frag), iph, hlen);\n\titer->iph = ip_hdr(frag);\n\tiph = iter->iph;\n\tiph->tot_len = htons(frag->len);\n\tip_copy_metadata(frag, skb);\n\titer->offset += skb->len - hlen;\n\tiph->frag_off = htons(iter->offset >> 3);\n\tif (frag->next)\n\t\tiph->frag_off |= htons(IP_MF);\n\t/* Ready, complete checksum */\n\tip_send_check(iph);\n}\nEXPORT_SYMBOL(ip_fraglist_prepare);\n\nvoid ip_frag_init(struct sk_buff *skb, unsigned int hlen,\n\t\t  unsigned int ll_rs, unsigned int mtu, bool DF,\n\t\t  struct ip_frag_state *state)\n{\n\tstruct iphdr *iph = ip_hdr(skb);\n\n\tstate->DF = DF;\n\tstate->hlen = hlen;\n\tstate->ll_rs = ll_rs;\n\tstate->mtu = mtu;\n\n\tstate->left = skb->len - hlen;\t/* Space per frame */\n\tstate->ptr = hlen;\t\t/* Where to start from */\n\n\tstate->offset = (ntohs(iph->frag_off) & IP_OFFSET) << 3;\n\tstate->not_last_frag = iph->frag_off & htons(IP_MF);\n}\nEXPORT_SYMBOL(ip_frag_init);\n\nstatic void ip_frag_ipcb(struct sk_buff *from, struct sk_buff *to,\n\t\t\t bool first_frag, struct ip_frag_state *state)\n{\n\t/* Copy the flags to each fragment. */\n\tIPCB(to)->flags = IPCB(from)->flags;\n\n\t/* ANK: dirty, but effective trick. Upgrade options only if\n\t * the segment to be fragmented was THE FIRST (otherwise,\n\t * options are already fixed) and make it ONCE\n\t * on the initial skb, so that all the following fragments\n\t * will inherit fixed options.\n\t */\n\tif (first_frag)\n\t\tip_options_fragment(from);\n}\n\nstruct sk_buff *ip_frag_next(struct sk_buff *skb, struct ip_frag_state *state)\n{\n\tunsigned int len = state->left;\n\tstruct sk_buff *skb2;\n\tstruct iphdr *iph;\n\n\tlen = state->left;\n\t/* IF: it doesn't fit, use 'mtu' - the data space left */\n\tif (len > state->mtu)\n\t\tlen = state->mtu;\n\t/* IF: we are not sending up to and including the packet end\n\t   then align the next start on an eight byte boundary */\n\tif (len < state->left)\t{\n\t\tlen &= ~7;\n\t}\n\n\t/* Allocate buffer */\n\tskb2 = alloc_skb(len + state->hlen + state->ll_rs, GFP_ATOMIC);\n\tif (!skb2)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t/*\n\t *\tSet up data on packet\n\t */\n\n\tip_copy_metadata(skb2, skb);\n\tskb_reserve(skb2, state->ll_rs);\n\tskb_put(skb2, len + state->hlen);\n\tskb_reset_network_header(skb2);\n\tskb2->transport_header = skb2->network_header + state->hlen;\n\n\t/*\n\t *\tCharge the memory for the fragment to any owner\n\t *\tit might possess\n\t */\n\n\tif (skb->sk)\n\t\tskb_set_owner_w(skb2, skb->sk);\n\n\t/*\n\t *\tCopy the packet header into the new buffer.\n\t */\n\n\tskb_copy_from_linear_data(skb, skb_network_header(skb2), state->hlen);\n\n\t/*\n\t *\tCopy a block of the IP datagram.\n\t */\n\tif (skb_copy_bits(skb, state->ptr, skb_transport_header(skb2), len))\n\t\tBUG();\n\tstate->left -= len;\n\n\t/*\n\t *\tFill in the new header fields.\n\t */\n\tiph = ip_hdr(skb2);\n\tiph->frag_off = htons((state->offset >> 3));\n\tif (state->DF)\n\t\tiph->frag_off |= htons(IP_DF);\n\n\t/*\n\t *\tAdded AC : If we are fragmenting a fragment that's not the\n\t *\t\t   last fragment then keep MF on each bit\n\t */\n\tif (state->left > 0 || state->not_last_frag)\n\t\tiph->frag_off |= htons(IP_MF);\n\tstate->ptr += len;\n\tstate->offset += len;\n\n\tiph->tot_len = htons(len + state->hlen);\n\n\tip_send_check(iph);\n\n\treturn skb2;\n}\nEXPORT_SYMBOL(ip_frag_next);\n\n/*\n *\tThis IP datagram is too large to be sent in one piece.  Break it up into\n *\tsmaller pieces (each of size equal to IP header plus\n *\ta block of the data of the original IP data part) that will yet fit in a\n *\tsingle device frame, and queue such a frame for sending.\n */\n\nint ip_do_fragment(struct net *net, struct sock *sk, struct sk_buff *skb,\n\t\t   int (*output)(struct net *, struct sock *, struct sk_buff *))\n{\n\tstruct iphdr *iph;\n\tstruct sk_buff *skb2;\n\tstruct rtable *rt = skb_rtable(skb);\n\tunsigned int mtu, hlen, ll_rs;\n\tstruct ip_fraglist_iter iter;\n\tktime_t tstamp = skb->tstamp;\n\tstruct ip_frag_state state;\n\tint err = 0;\n\n\t/* for offloaded checksums cleanup checksum before fragmentation */\n\tif (skb->ip_summed == CHECKSUM_PARTIAL &&\n\t    (err = skb_checksum_help(skb)))\n\t\tgoto fail;\n\n\t/*\n\t *\tPoint into the IP datagram header.\n\t */\n\n\tiph = ip_hdr(skb);\n\n\tmtu = ip_skb_dst_mtu(sk, skb);\n\tif (IPCB(skb)->frag_max_size && IPCB(skb)->frag_max_size < mtu)\n\t\tmtu = IPCB(skb)->frag_max_size;\n\n\t/*\n\t *\tSetup starting values.\n\t */\n\n\thlen = iph->ihl * 4;\n\tmtu = mtu - hlen;\t/* Size of data space */\n\tIPCB(skb)->flags |= IPSKB_FRAG_COMPLETE;\n\tll_rs = LL_RESERVED_SPACE(rt->dst.dev);\n\n\t/* When frag_list is given, use it. First, check its validity:\n\t * some transformers could create wrong frag_list or break existing\n\t * one, it is not prohibited. In this case fall back to copying.\n\t *\n\t * LATER: this step can be merged to real generation of fragments,\n\t * we can switch to copy when see the first bad fragment.\n\t */\n\tif (skb_has_frag_list(skb)) {\n\t\tstruct sk_buff *frag, *frag2;\n\t\tunsigned int first_len = skb_pagelen(skb);\n\n\t\tif (first_len - hlen > mtu ||\n\t\t    ((first_len - hlen) & 7) ||\n\t\t    ip_is_fragment(iph) ||\n\t\t    skb_cloned(skb) ||\n\t\t    skb_headroom(skb) < ll_rs)\n\t\t\tgoto slow_path;\n\n\t\tskb_walk_frags(skb, frag) {\n\t\t\t/* Correct geometry. */\n\t\t\tif (frag->len > mtu ||\n\t\t\t    ((frag->len & 7) && frag->next) ||\n\t\t\t    skb_headroom(frag) < hlen + ll_rs)\n\t\t\t\tgoto slow_path_clean;\n\n\t\t\t/* Partially cloned skb? */\n\t\t\tif (skb_shared(frag))\n\t\t\t\tgoto slow_path_clean;\n\n\t\t\tBUG_ON(frag->sk);\n\t\t\tif (skb->sk) {\n\t\t\t\tfrag->sk = skb->sk;\n\t\t\t\tfrag->destructor = sock_wfree;\n\t\t\t}\n\t\t\tskb->truesize -= frag->truesize;\n\t\t}\n\n\t\t/* Everything is OK. Generate! */\n\t\tip_fraglist_init(skb, iph, hlen, &iter);\n\n\t\tfor (;;) {\n\t\t\t/* Prepare header of the next frame,\n\t\t\t * before previous one went down. */\n\t\t\tif (iter.frag) {\n\t\t\t\tip_fraglist_ipcb_prepare(skb, &iter);\n\t\t\t\tip_fraglist_prepare(skb, &iter);\n\t\t\t}\n\n\t\t\tskb->tstamp = tstamp;\n\t\t\terr = output(net, sk, skb);\n\n\t\t\tif (!err)\n\t\t\t\tIP_INC_STATS(net, IPSTATS_MIB_FRAGCREATES);\n\t\t\tif (err || !iter.frag)\n\t\t\t\tbreak;\n\n\t\t\tskb = ip_fraglist_next(&iter);\n\t\t}\n\n\t\tif (err == 0) {\n\t\t\tIP_INC_STATS(net, IPSTATS_MIB_FRAGOKS);\n\t\t\treturn 0;\n\t\t}\n\n\t\tkfree_skb_list(iter.frag);\n\n\t\tIP_INC_STATS(net, IPSTATS_MIB_FRAGFAILS);\n\t\treturn err;\n\nslow_path_clean:\n\t\tskb_walk_frags(skb, frag2) {\n\t\t\tif (frag2 == frag)\n\t\t\t\tbreak;\n\t\t\tfrag2->sk = NULL;\n\t\t\tfrag2->destructor = NULL;\n\t\t\tskb->truesize += frag2->truesize;\n\t\t}\n\t}\n\nslow_path:\n\t/*\n\t *\tFragment the datagram.\n\t */\n\n\tip_frag_init(skb, hlen, ll_rs, mtu, IPCB(skb)->flags & IPSKB_FRAG_PMTU,\n\t\t     &state);\n\n\t/*\n\t *\tKeep copying data until we run out.\n\t */\n\n\twhile (state.left > 0) {\n\t\tbool first_frag = (state.offset == 0);\n\n\t\tskb2 = ip_frag_next(skb, &state);\n\t\tif (IS_ERR(skb2)) {\n\t\t\terr = PTR_ERR(skb2);\n\t\t\tgoto fail;\n\t\t}\n\t\tip_frag_ipcb(skb, skb2, first_frag, &state);\n\n\t\t/*\n\t\t *\tPut this fragment into the sending queue.\n\t\t */\n\t\tskb2->tstamp = tstamp;\n\t\terr = output(net, sk, skb2);\n\t\tif (err)\n\t\t\tgoto fail;\n\n\t\tIP_INC_STATS(net, IPSTATS_MIB_FRAGCREATES);\n\t}\n\tconsume_skb(skb);\n\tIP_INC_STATS(net, IPSTATS_MIB_FRAGOKS);\n\treturn err;\n\nfail:\n\tkfree_skb(skb);\n\tIP_INC_STATS(net, IPSTATS_MIB_FRAGFAILS);\n\treturn err;\n}\nEXPORT_SYMBOL(ip_do_fragment);\n\nint\nip_generic_getfrag(void *from, char *to, int offset, int len, int odd, struct sk_buff *skb)\n{\n\tstruct msghdr *msg = from;\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tif (!copy_from_iter_full(to, len, &msg->msg_iter))\n\t\t\treturn -EFAULT;\n\t} else {\n\t\t__wsum csum = 0;\n\t\tif (!csum_and_copy_from_iter_full(to, len, &csum, &msg->msg_iter))\n\t\t\treturn -EFAULT;\n\t\tskb->csum = csum_block_add(skb->csum, csum, odd);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(ip_generic_getfrag);\n\nstatic inline __wsum\ncsum_page(struct page *page, int offset, int copy)\n{\n\tchar *kaddr;\n\t__wsum csum;\n\tkaddr = kmap(page);\n\tcsum = csum_partial(kaddr + offset, copy, 0);\n\tkunmap(page);\n\treturn csum;\n}\n\nstatic int __ip_append_data(struct sock *sk,\n\t\t\t    struct flowi4 *fl4,\n\t\t\t    struct sk_buff_head *queue,\n\t\t\t    struct inet_cork *cork,\n\t\t\t    struct page_frag *pfrag,\n\t\t\t    int getfrag(void *from, char *to, int offset,\n\t\t\t\t\tint len, int odd, struct sk_buff *skb),\n\t\t\t    void *from, int length, int transhdrlen,\n\t\t\t    unsigned int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ubuf_info *uarg = NULL;\n\tstruct sk_buff *skb;\n\n\tstruct ip_options *opt = cork->opt;\n\tint hh_len;\n\tint exthdrlen;\n\tint mtu;\n\tint copy;\n\tint err;\n\tint offset = 0;\n\tunsigned int maxfraglen, fragheaderlen, maxnonfragsize;\n\tint csummode = CHECKSUM_NONE;\n\tstruct rtable *rt = (struct rtable *)cork->dst;\n\tunsigned int wmem_alloc_delta = 0;\n\tbool paged, extra_uref = false;\n\tu32 tskey = 0;\n\n\tskb = skb_peek_tail(queue);\n\n\texthdrlen = !skb ? rt->dst.header_len : 0;\n\tmtu = cork->gso_size ? IP_MAX_MTU : cork->fragsize;\n\tpaged = !!cork->gso_size;\n\n\tif (cork->tx_flags & SKBTX_ANY_SW_TSTAMP &&\n\t    sk->sk_tsflags & SOF_TIMESTAMPING_OPT_ID)\n\t\ttskey = sk->sk_tskey++;\n\n\thh_len = LL_RESERVED_SPACE(rt->dst.dev);\n\n\tfragheaderlen = sizeof(struct iphdr) + (opt ? opt->optlen : 0);\n\tmaxfraglen = ((mtu - fragheaderlen) & ~7) + fragheaderlen;\n\tmaxnonfragsize = ip_sk_ignore_df(sk) ? IP_MAX_MTU : mtu;\n\n\tif (cork->length + length > maxnonfragsize - fragheaderlen) {\n\t\tip_local_error(sk, EMSGSIZE, fl4->daddr, inet->inet_dport,\n\t\t\t       mtu - (opt ? opt->optlen : 0));\n\t\treturn -EMSGSIZE;\n\t}\n\n\t/*\n\t * transhdrlen > 0 means that this is the first fragment and we wish\n\t * it won't be fragmented in the future.\n\t */\n\tif (transhdrlen &&\n\t    length + fragheaderlen <= mtu &&\n\t    rt->dst.dev->features & (NETIF_F_HW_CSUM | NETIF_F_IP_CSUM) &&\n\t    (!(flags & MSG_MORE) || cork->gso_size) &&\n\t    (!exthdrlen || (rt->dst.dev->features & NETIF_F_HW_ESP_TX_CSUM)))\n\t\tcsummode = CHECKSUM_PARTIAL;\n\n\tif (flags & MSG_ZEROCOPY && length && sock_flag(sk, SOCK_ZEROCOPY)) {\n\t\tuarg = msg_zerocopy_realloc(sk, length, skb_zcopy(skb));\n\t\tif (!uarg)\n\t\t\treturn -ENOBUFS;\n\t\textra_uref = !skb_zcopy(skb);\t/* only ref on new uarg */\n\t\tif (rt->dst.dev->features & NETIF_F_SG &&\n\t\t    csummode == CHECKSUM_PARTIAL) {\n\t\t\tpaged = true;\n\t\t} else {\n\t\t\tuarg->zerocopy = 0;\n\t\t\tskb_zcopy_set(skb, uarg, &extra_uref);\n\t\t}\n\t}\n\n\tcork->length += length;\n\n\t/* So, what's going on in the loop below?\n\t *\n\t * We use calculated fragment length to generate chained skb,\n\t * each of segments is IP fragment ready for sending to network after\n\t * adding appropriate IP header.\n\t */\n\n\tif (!skb)\n\t\tgoto alloc_new_skb;\n\n\twhile (length > 0) {\n\t\t/* Check if the remaining data fits into current packet. */\n\t\tcopy = mtu - skb->len;\n\t\tif (copy < length)\n\t\t\tcopy = maxfraglen - skb->len;\n\t\tif (copy <= 0) {\n\t\t\tchar *data;\n\t\t\tunsigned int datalen;\n\t\t\tunsigned int fraglen;\n\t\t\tunsigned int fraggap;\n\t\t\tunsigned int alloclen;\n\t\t\tunsigned int pagedlen;\n\t\t\tstruct sk_buff *skb_prev;\nalloc_new_skb:\n\t\t\tskb_prev = skb;\n\t\t\tif (skb_prev)\n\t\t\t\tfraggap = skb_prev->len - maxfraglen;\n\t\t\telse\n\t\t\t\tfraggap = 0;\n\n\t\t\t/*\n\t\t\t * If remaining data exceeds the mtu,\n\t\t\t * we know we need more fragment(s).\n\t\t\t */\n\t\t\tdatalen = length + fraggap;\n\t\t\tif (datalen > mtu - fragheaderlen)\n\t\t\t\tdatalen = maxfraglen - fragheaderlen;\n\t\t\tfraglen = datalen + fragheaderlen;\n\t\t\tpagedlen = 0;\n\n\t\t\tif ((flags & MSG_MORE) &&\n\t\t\t    !(rt->dst.dev->features&NETIF_F_SG))\n\t\t\t\talloclen = mtu;\n\t\t\telse if (!paged)\n\t\t\t\talloclen = fraglen;\n\t\t\telse {\n\t\t\t\talloclen = min_t(int, fraglen, MAX_HEADER);\n\t\t\t\tpagedlen = fraglen - alloclen;\n\t\t\t}\n\n\t\t\talloclen += exthdrlen;\n\n\t\t\t/* The last fragment gets additional space at tail.\n\t\t\t * Note, with MSG_MORE we overallocate on fragments,\n\t\t\t * because we have no idea what fragment will be\n\t\t\t * the last.\n\t\t\t */\n\t\t\tif (datalen == length + fraggap)\n\t\t\t\talloclen += rt->dst.trailer_len;\n\n\t\t\tif (transhdrlen) {\n\t\t\t\tskb = sock_alloc_send_skb(sk,\n\t\t\t\t\t\talloclen + hh_len + 15,\n\t\t\t\t\t\t(flags & MSG_DONTWAIT), &err);\n\t\t\t} else {\n\t\t\t\tskb = NULL;\n\t\t\t\tif (refcount_read(&sk->sk_wmem_alloc) + wmem_alloc_delta <=\n\t\t\t\t    2 * sk->sk_sndbuf)\n\t\t\t\t\tskb = alloc_skb(alloclen + hh_len + 15,\n\t\t\t\t\t\t\tsk->sk_allocation);\n\t\t\t\tif (unlikely(!skb))\n\t\t\t\t\terr = -ENOBUFS;\n\t\t\t}\n\t\t\tif (!skb)\n\t\t\t\tgoto error;\n\n\t\t\t/*\n\t\t\t *\tFill in the control structures\n\t\t\t */\n\t\t\tskb->ip_summed = csummode;\n\t\t\tskb->csum = 0;\n\t\t\tskb_reserve(skb, hh_len);\n\n\t\t\t/*\n\t\t\t *\tFind where to start putting bytes.\n\t\t\t */\n\t\t\tdata = skb_put(skb, fraglen + exthdrlen - pagedlen);\n\t\t\tskb_set_network_header(skb, exthdrlen);\n\t\t\tskb->transport_header = (skb->network_header +\n\t\t\t\t\t\t fragheaderlen);\n\t\t\tdata += fragheaderlen + exthdrlen;\n\n\t\t\tif (fraggap) {\n\t\t\t\tskb->csum = skb_copy_and_csum_bits(\n\t\t\t\t\tskb_prev, maxfraglen,\n\t\t\t\t\tdata + transhdrlen, fraggap);\n\t\t\t\tskb_prev->csum = csum_sub(skb_prev->csum,\n\t\t\t\t\t\t\t  skb->csum);\n\t\t\t\tdata += fraggap;\n\t\t\t\tpskb_trim_unique(skb_prev, maxfraglen);\n\t\t\t}\n\n\t\t\tcopy = datalen - transhdrlen - fraggap - pagedlen;\n\t\t\tif (copy > 0 && getfrag(from, data + transhdrlen, offset, copy, fraggap, skb) < 0) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tgoto error;\n\t\t\t}\n\n\t\t\toffset += copy;\n\t\t\tlength -= copy + transhdrlen;\n\t\t\ttranshdrlen = 0;\n\t\t\texthdrlen = 0;\n\t\t\tcsummode = CHECKSUM_NONE;\n\n\t\t\t/* only the initial fragment is time stamped */\n\t\t\tskb_shinfo(skb)->tx_flags = cork->tx_flags;\n\t\t\tcork->tx_flags = 0;\n\t\t\tskb_shinfo(skb)->tskey = tskey;\n\t\t\ttskey = 0;\n\t\t\tskb_zcopy_set(skb, uarg, &extra_uref);\n\n\t\t\tif ((flags & MSG_CONFIRM) && !skb_prev)\n\t\t\t\tskb_set_dst_pending_confirm(skb, 1);\n\n\t\t\t/*\n\t\t\t * Put the packet on the pending queue.\n\t\t\t */\n\t\t\tif (!skb->destructor) {\n\t\t\t\tskb->destructor = sock_wfree;\n\t\t\t\tskb->sk = sk;\n\t\t\t\twmem_alloc_delta += skb->truesize;\n\t\t\t}\n\t\t\t__skb_queue_tail(queue, skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (copy > length)\n\t\t\tcopy = length;\n\n\t\tif (!(rt->dst.dev->features&NETIF_F_SG) &&\n\t\t    skb_tailroom(skb) >= copy) {\n\t\t\tunsigned int off;\n\n\t\t\toff = skb->len;\n\t\t\tif (getfrag(from, skb_put(skb, copy),\n\t\t\t\t\toffset, copy, off, skb) < 0) {\n\t\t\t\t__skb_trim(skb, off);\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t} else if (!uarg || !uarg->zerocopy) {\n\t\t\tint i = skb_shinfo(skb)->nr_frags;\n\n\t\t\terr = -ENOMEM;\n\t\t\tif (!sk_page_frag_refill(sk, pfrag))\n\t\t\t\tgoto error;\n\n\t\t\tif (!skb_can_coalesce(skb, i, pfrag->page,\n\t\t\t\t\t      pfrag->offset)) {\n\t\t\t\terr = -EMSGSIZE;\n\t\t\t\tif (i == MAX_SKB_FRAGS)\n\t\t\t\t\tgoto error;\n\n\t\t\t\t__skb_fill_page_desc(skb, i, pfrag->page,\n\t\t\t\t\t\t     pfrag->offset, 0);\n\t\t\t\tskb_shinfo(skb)->nr_frags = ++i;\n\t\t\t\tget_page(pfrag->page);\n\t\t\t}\n\t\t\tcopy = min_t(int, copy, pfrag->size - pfrag->offset);\n\t\t\tif (getfrag(from,\n\t\t\t\t    page_address(pfrag->page) + pfrag->offset,\n\t\t\t\t    offset, copy, skb->len, skb) < 0)\n\t\t\t\tgoto error_efault;\n\n\t\t\tpfrag->offset += copy;\n\t\t\tskb_frag_size_add(&skb_shinfo(skb)->frags[i - 1], copy);\n\t\t\tskb->len += copy;\n\t\t\tskb->data_len += copy;\n\t\t\tskb->truesize += copy;\n\t\t\twmem_alloc_delta += copy;\n\t\t} else {\n\t\t\terr = skb_zerocopy_iter_dgram(skb, from, copy);\n\t\t\tif (err < 0)\n\t\t\t\tgoto error;\n\t\t}\n\t\toffset += copy;\n\t\tlength -= copy;\n\t}\n\n\tif (wmem_alloc_delta)\n\t\trefcount_add(wmem_alloc_delta, &sk->sk_wmem_alloc);\n\treturn 0;\n\nerror_efault:\n\terr = -EFAULT;\nerror:\n\tnet_zcopy_put_abort(uarg, extra_uref);\n\tcork->length -= length;\n\tIP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTDISCARDS);\n\trefcount_add(wmem_alloc_delta, &sk->sk_wmem_alloc);\n\treturn err;\n}\n\nstatic int ip_setup_cork(struct sock *sk, struct inet_cork *cork,\n\t\t\t struct ipcm_cookie *ipc, struct rtable **rtp)\n{\n\tstruct ip_options_rcu *opt;\n\tstruct rtable *rt;\n\n\trt = *rtp;\n\tif (unlikely(!rt))\n\t\treturn -EFAULT;\n\n\t/*\n\t * setup for corking.\n\t */\n\topt = ipc->opt;\n\tif (opt) {\n\t\tif (!cork->opt) {\n\t\t\tcork->opt = kmalloc(sizeof(struct ip_options) + 40,\n\t\t\t\t\t    sk->sk_allocation);\n\t\t\tif (unlikely(!cork->opt))\n\t\t\t\treturn -ENOBUFS;\n\t\t}\n\t\tmemcpy(cork->opt, &opt->opt, sizeof(struct ip_options) + opt->opt.optlen);\n\t\tcork->flags |= IPCORK_OPT;\n\t\tcork->addr = ipc->addr;\n\t}\n\n\tcork->fragsize = ip_sk_use_pmtu(sk) ?\n\t\t\t dst_mtu(&rt->dst) : READ_ONCE(rt->dst.dev->mtu);\n\n\tif (!inetdev_valid_mtu(cork->fragsize))\n\t\treturn -ENETUNREACH;\n\n\tcork->gso_size = ipc->gso_size;\n\n\tcork->dst = &rt->dst;\n\t/* We stole this route, caller should not release it. */\n\t*rtp = NULL;\n\n\tcork->length = 0;\n\tcork->ttl = ipc->ttl;\n\tcork->tos = ipc->tos;\n\tcork->mark = ipc->sockc.mark;\n\tcork->priority = ipc->priority;\n\tcork->transmit_time = ipc->sockc.transmit_time;\n\tcork->tx_flags = 0;\n\tsock_tx_timestamp(sk, ipc->sockc.tsflags, &cork->tx_flags);\n\n\treturn 0;\n}\n\n/*\n *\tip_append_data() and ip_append_page() can make one large IP datagram\n *\tfrom many pieces of data. Each pieces will be holded on the socket\n *\tuntil ip_push_pending_frames() is called. Each piece can be a page\n *\tor non-page data.\n *\n *\tNot only UDP, other transport protocols - e.g. raw sockets - can use\n *\tthis interface potentially.\n *\n *\tLATER: length must be adjusted by pad at tail, when it is required.\n */\nint ip_append_data(struct sock *sk, struct flowi4 *fl4,\n\t\t   int getfrag(void *from, char *to, int offset, int len,\n\t\t\t       int odd, struct sk_buff *skb),\n\t\t   void *from, int length, int transhdrlen,\n\t\t   struct ipcm_cookie *ipc, struct rtable **rtp,\n\t\t   unsigned int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tint err;\n\n\tif (flags&MSG_PROBE)\n\t\treturn 0;\n\n\tif (skb_queue_empty(&sk->sk_write_queue)) {\n\t\terr = ip_setup_cork(sk, &inet->cork.base, ipc, rtp);\n\t\tif (err)\n\t\t\treturn err;\n\t} else {\n\t\ttranshdrlen = 0;\n\t}\n\n\treturn __ip_append_data(sk, fl4, &sk->sk_write_queue, &inet->cork.base,\n\t\t\t\tsk_page_frag(sk), getfrag,\n\t\t\t\tfrom, length, transhdrlen, flags);\n}\n\nssize_t\tip_append_page(struct sock *sk, struct flowi4 *fl4, struct page *page,\n\t\t       int offset, size_t size, int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct rtable *rt;\n\tstruct ip_options *opt = NULL;\n\tstruct inet_cork *cork;\n\tint hh_len;\n\tint mtu;\n\tint len;\n\tint err;\n\tunsigned int maxfraglen, fragheaderlen, fraggap, maxnonfragsize;\n\n\tif (inet->hdrincl)\n\t\treturn -EPERM;\n\n\tif (flags&MSG_PROBE)\n\t\treturn 0;\n\n\tif (skb_queue_empty(&sk->sk_write_queue))\n\t\treturn -EINVAL;\n\n\tcork = &inet->cork.base;\n\trt = (struct rtable *)cork->dst;\n\tif (cork->flags & IPCORK_OPT)\n\t\topt = cork->opt;\n\n\tif (!(rt->dst.dev->features & NETIF_F_SG))\n\t\treturn -EOPNOTSUPP;\n\n\thh_len = LL_RESERVED_SPACE(rt->dst.dev);\n\tmtu = cork->gso_size ? IP_MAX_MTU : cork->fragsize;\n\n\tfragheaderlen = sizeof(struct iphdr) + (opt ? opt->optlen : 0);\n\tmaxfraglen = ((mtu - fragheaderlen) & ~7) + fragheaderlen;\n\tmaxnonfragsize = ip_sk_ignore_df(sk) ? 0xFFFF : mtu;\n\n\tif (cork->length + size > maxnonfragsize - fragheaderlen) {\n\t\tip_local_error(sk, EMSGSIZE, fl4->daddr, inet->inet_dport,\n\t\t\t       mtu - (opt ? opt->optlen : 0));\n\t\treturn -EMSGSIZE;\n\t}\n\n\tskb = skb_peek_tail(&sk->sk_write_queue);\n\tif (!skb)\n\t\treturn -EINVAL;\n\n\tcork->length += size;\n\n\twhile (size > 0) {\n\t\t/* Check if the remaining data fits into current packet. */\n\t\tlen = mtu - skb->len;\n\t\tif (len < size)\n\t\t\tlen = maxfraglen - skb->len;\n\n\t\tif (len <= 0) {\n\t\t\tstruct sk_buff *skb_prev;\n\t\t\tint alloclen;\n\n\t\t\tskb_prev = skb;\n\t\t\tfraggap = skb_prev->len - maxfraglen;\n\n\t\t\talloclen = fragheaderlen + hh_len + fraggap + 15;\n\t\t\tskb = sock_wmalloc(sk, alloclen, 1, sk->sk_allocation);\n\t\t\tif (unlikely(!skb)) {\n\t\t\t\terr = -ENOBUFS;\n\t\t\t\tgoto error;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t *\tFill in the control structures\n\t\t\t */\n\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\t\tskb->csum = 0;\n\t\t\tskb_reserve(skb, hh_len);\n\n\t\t\t/*\n\t\t\t *\tFind where to start putting bytes.\n\t\t\t */\n\t\t\tskb_put(skb, fragheaderlen + fraggap);\n\t\t\tskb_reset_network_header(skb);\n\t\t\tskb->transport_header = (skb->network_header +\n\t\t\t\t\t\t fragheaderlen);\n\t\t\tif (fraggap) {\n\t\t\t\tskb->csum = skb_copy_and_csum_bits(skb_prev,\n\t\t\t\t\t\t\t\t   maxfraglen,\n\t\t\t\t\t\t    skb_transport_header(skb),\n\t\t\t\t\t\t\t\t   fraggap);\n\t\t\t\tskb_prev->csum = csum_sub(skb_prev->csum,\n\t\t\t\t\t\t\t  skb->csum);\n\t\t\t\tpskb_trim_unique(skb_prev, maxfraglen);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Put the packet on the pending queue.\n\t\t\t */\n\t\t\t__skb_queue_tail(&sk->sk_write_queue, skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (len > size)\n\t\t\tlen = size;\n\n\t\tif (skb_append_pagefrags(skb, page, offset, len)) {\n\t\t\terr = -EMSGSIZE;\n\t\t\tgoto error;\n\t\t}\n\n\t\tif (skb->ip_summed == CHECKSUM_NONE) {\n\t\t\t__wsum csum;\n\t\t\tcsum = csum_page(page, offset, len);\n\t\t\tskb->csum = csum_block_add(skb->csum, csum, skb->len);\n\t\t}\n\n\t\tskb->len += len;\n\t\tskb->data_len += len;\n\t\tskb->truesize += len;\n\t\trefcount_add(len, &sk->sk_wmem_alloc);\n\t\toffset += len;\n\t\tsize -= len;\n\t}\n\treturn 0;\n\nerror:\n\tcork->length -= size;\n\tIP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTDISCARDS);\n\treturn err;\n}\n\nstatic void ip_cork_release(struct inet_cork *cork)\n{\n\tcork->flags &= ~IPCORK_OPT;\n\tkfree(cork->opt);\n\tcork->opt = NULL;\n\tdst_release(cork->dst);\n\tcork->dst = NULL;\n}\n\n/*\n *\tCombined all pending IP fragments on the socket as one IP datagram\n *\tand push them out.\n */\nstruct sk_buff *__ip_make_skb(struct sock *sk,\n\t\t\t      struct flowi4 *fl4,\n\t\t\t      struct sk_buff_head *queue,\n\t\t\t      struct inet_cork *cork)\n{\n\tstruct sk_buff *skb, *tmp_skb;\n\tstruct sk_buff **tail_skb;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct ip_options *opt = NULL;\n\tstruct rtable *rt = (struct rtable *)cork->dst;\n\tstruct iphdr *iph;\n\t__be16 df = 0;\n\t__u8 ttl;\n\n\tskb = __skb_dequeue(queue);\n\tif (!skb)\n\t\tgoto out;\n\ttail_skb = &(skb_shinfo(skb)->frag_list);\n\n\t/* move skb->data to ip header from ext header */\n\tif (skb->data < skb_network_header(skb))\n\t\t__skb_pull(skb, skb_network_offset(skb));\n\twhile ((tmp_skb = __skb_dequeue(queue)) != NULL) {\n\t\t__skb_pull(tmp_skb, skb_network_header_len(skb));\n\t\t*tail_skb = tmp_skb;\n\t\ttail_skb = &(tmp_skb->next);\n\t\tskb->len += tmp_skb->len;\n\t\tskb->data_len += tmp_skb->len;\n\t\tskb->truesize += tmp_skb->truesize;\n\t\ttmp_skb->destructor = NULL;\n\t\ttmp_skb->sk = NULL;\n\t}\n\n\t/* Unless user demanded real pmtu discovery (IP_PMTUDISC_DO), we allow\n\t * to fragment the frame generated here. No matter, what transforms\n\t * how transforms change size of the packet, it will come out.\n\t */\n\tskb->ignore_df = ip_sk_ignore_df(sk);\n\n\t/* DF bit is set when we want to see DF on outgoing frames.\n\t * If ignore_df is set too, we still allow to fragment this frame\n\t * locally. */\n\tif (inet->pmtudisc == IP_PMTUDISC_DO ||\n\t    inet->pmtudisc == IP_PMTUDISC_PROBE ||\n\t    (skb->len <= dst_mtu(&rt->dst) &&\n\t     ip_dont_fragment(sk, &rt->dst)))\n\t\tdf = htons(IP_DF);\n\n\tif (cork->flags & IPCORK_OPT)\n\t\topt = cork->opt;\n\n\tif (cork->ttl != 0)\n\t\tttl = cork->ttl;\n\telse if (rt->rt_type == RTN_MULTICAST)\n\t\tttl = inet->mc_ttl;\n\telse\n\t\tttl = ip_select_ttl(inet, &rt->dst);\n\n\tiph = ip_hdr(skb);\n\tiph->version = 4;\n\tiph->ihl = 5;\n\tiph->tos = (cork->tos != -1) ? cork->tos : inet->tos;\n\tiph->frag_off = df;\n\tiph->ttl = ttl;\n\tiph->protocol = sk->sk_protocol;\n\tip_copy_addrs(iph, fl4);\n\tip_select_ident(net, skb, sk);\n\n\tif (opt) {\n\t\tiph->ihl += opt->optlen >> 2;\n\t\tip_options_build(skb, opt, cork->addr, rt, 0);\n\t}\n\n\tskb->priority = (cork->tos != -1) ? cork->priority: sk->sk_priority;\n\tskb->mark = cork->mark;\n\tskb->tstamp = cork->transmit_time;\n\t/*\n\t * Steal rt from cork.dst to avoid a pair of atomic_inc/atomic_dec\n\t * on dst refcount\n\t */\n\tcork->dst = NULL;\n\tskb_dst_set(skb, &rt->dst);\n\n\tif (iph->protocol == IPPROTO_ICMP)\n\t\ticmp_out_count(net, ((struct icmphdr *)\n\t\t\tskb_transport_header(skb))->type);\n\n\tip_cork_release(cork);\nout:\n\treturn skb;\n}\n\nint ip_send_skb(struct net *net, struct sk_buff *skb)\n{\n\tint err;\n\n\terr = ip_local_out(net, skb->sk, skb);\n\tif (err) {\n\t\tif (err > 0)\n\t\t\terr = net_xmit_errno(err);\n\t\tif (err)\n\t\t\tIP_INC_STATS(net, IPSTATS_MIB_OUTDISCARDS);\n\t}\n\n\treturn err;\n}\n\nint ip_push_pending_frames(struct sock *sk, struct flowi4 *fl4)\n{\n\tstruct sk_buff *skb;\n\n\tskb = ip_finish_skb(sk, fl4);\n\tif (!skb)\n\t\treturn 0;\n\n\t/* Netfilter gets whole the not fragmented skb. */\n\treturn ip_send_skb(sock_net(sk), skb);\n}\n\n/*\n *\tThrow away all pending data on the socket.\n */\nstatic void __ip_flush_pending_frames(struct sock *sk,\n\t\t\t\t      struct sk_buff_head *queue,\n\t\t\t\t      struct inet_cork *cork)\n{\n\tstruct sk_buff *skb;\n\n\twhile ((skb = __skb_dequeue_tail(queue)) != NULL)\n\t\tkfree_skb(skb);\n\n\tip_cork_release(cork);\n}\n\nvoid ip_flush_pending_frames(struct sock *sk)\n{\n\t__ip_flush_pending_frames(sk, &sk->sk_write_queue, &inet_sk(sk)->cork.base);\n}\n\nstruct sk_buff *ip_make_skb(struct sock *sk,\n\t\t\t    struct flowi4 *fl4,\n\t\t\t    int getfrag(void *from, char *to, int offset,\n\t\t\t\t\tint len, int odd, struct sk_buff *skb),\n\t\t\t    void *from, int length, int transhdrlen,\n\t\t\t    struct ipcm_cookie *ipc, struct rtable **rtp,\n\t\t\t    struct inet_cork *cork, unsigned int flags)\n{\n\tstruct sk_buff_head queue;\n\tint err;\n\n\tif (flags & MSG_PROBE)\n\t\treturn NULL;\n\n\t__skb_queue_head_init(&queue);\n\n\tcork->flags = 0;\n\tcork->addr = 0;\n\tcork->opt = NULL;\n\terr = ip_setup_cork(sk, cork, ipc, rtp);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\terr = __ip_append_data(sk, fl4, &queue, cork,\n\t\t\t       &current->task_frag, getfrag,\n\t\t\t       from, length, transhdrlen, flags);\n\tif (err) {\n\t\t__ip_flush_pending_frames(sk, &queue, cork);\n\t\treturn ERR_PTR(err);\n\t}\n\n\treturn __ip_make_skb(sk, fl4, &queue, cork);\n}\n\n/*\n *\tFetch data from kernel space and fill in checksum if needed.\n */\nstatic int ip_reply_glue_bits(void *dptr, char *to, int offset,\n\t\t\t      int len, int odd, struct sk_buff *skb)\n{\n\t__wsum csum;\n\n\tcsum = csum_partial_copy_nocheck(dptr+offset, to, len);\n\tskb->csum = csum_block_add(skb->csum, csum, odd);\n\treturn 0;\n}\n\n/*\n *\tGeneric function to send a packet as reply to another packet.\n *\tUsed to send some TCP resets/acks so far.\n */\nvoid ip_send_unicast_reply(struct sock *sk, struct sk_buff *skb,\n\t\t\t   const struct ip_options *sopt,\n\t\t\t   __be32 daddr, __be32 saddr,\n\t\t\t   const struct ip_reply_arg *arg,\n\t\t\t   unsigned int len, u64 transmit_time)\n{\n\tstruct ip_options_data replyopts;\n\tstruct ipcm_cookie ipc;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct net *net = sock_net(sk);\n\tstruct sk_buff *nskb;\n\tint err;\n\tint oif;\n\n\tif (__ip_options_echo(net, &replyopts.opt.opt, skb, sopt))\n\t\treturn;\n\n\tipcm_init(&ipc);\n\tipc.addr = daddr;\n\tipc.sockc.transmit_time = transmit_time;\n\n\tif (replyopts.opt.opt.optlen) {\n\t\tipc.opt = &replyopts.opt;\n\n\t\tif (replyopts.opt.opt.srr)\n\t\t\tdaddr = replyopts.opt.opt.faddr;\n\t}\n\n\toif = arg->bound_dev_if;\n\tif (!oif && netif_index_is_l3_master(net, skb->skb_iif))\n\t\toif = skb->skb_iif;\n\n\tflowi4_init_output(&fl4, oif,\n\t\t\t   IP4_REPLY_MARK(net, skb->mark) ?: sk->sk_mark,\n\t\t\t   RT_TOS(arg->tos),\n\t\t\t   RT_SCOPE_UNIVERSE, ip_hdr(skb)->protocol,\n\t\t\t   ip_reply_arg_flowi_flags(arg),\n\t\t\t   daddr, saddr,\n\t\t\t   tcp_hdr(skb)->source, tcp_hdr(skb)->dest,\n\t\t\t   arg->uid);\n\tsecurity_skb_classify_flow(skb, flowi4_to_flowi_common(&fl4));\n\trt = ip_route_output_key(net, &fl4);\n\tif (IS_ERR(rt))\n\t\treturn;\n\n\tinet_sk(sk)->tos = arg->tos & ~INET_ECN_MASK;\n\n\tsk->sk_protocol = ip_hdr(skb)->protocol;\n\tsk->sk_bound_dev_if = arg->bound_dev_if;\n\tsk->sk_sndbuf = sysctl_wmem_default;\n\tipc.sockc.mark = fl4.flowi4_mark;\n\terr = ip_append_data(sk, &fl4, ip_reply_glue_bits, arg->iov->iov_base,\n\t\t\t     len, 0, &ipc, &rt, MSG_DONTWAIT);\n\tif (unlikely(err)) {\n\t\tip_flush_pending_frames(sk);\n\t\tgoto out;\n\t}\n\n\tnskb = skb_peek(&sk->sk_write_queue);\n\tif (nskb) {\n\t\tif (arg->csumoffset >= 0)\n\t\t\t*((__sum16 *)skb_transport_header(nskb) +\n\t\t\t  arg->csumoffset) = csum_fold(csum_add(nskb->csum,\n\t\t\t\t\t\t\t\targ->csum));\n\t\tnskb->ip_summed = CHECKSUM_NONE;\n\t\tip_push_pending_frames(sk, &fl4);\n\t}\nout:\n\tip_rt_put(rt);\n}\n\nvoid __init ip_init(void)\n{\n\tip_rt_init();\n\tinet_initpeers();\n\n#if defined(CONFIG_IP_MULTICAST)\n\tigmp_mc_init();\n#endif\n}\n"}, "9": {"id": 9, "path": "/src/include/net/ip.h", "content": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tDefinitions for the IP module.\n *\n * Version:\t@(#)ip.h\t1.0.2\t05/07/93\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\n * Changes:\n *\t\tMike McLagan    :       Routing by source\n */\n#ifndef _IP_H\n#define _IP_H\n\n#include <linux/types.h>\n#include <linux/ip.h>\n#include <linux/in.h>\n#include <linux/skbuff.h>\n#include <linux/jhash.h>\n#include <linux/sockptr.h>\n\n#include <net/inet_sock.h>\n#include <net/route.h>\n#include <net/snmp.h>\n#include <net/flow.h>\n#include <net/flow_dissector.h>\n#include <net/netns/hash.h>\n\n#define IPV4_MAX_PMTU\t\t65535U\t\t/* RFC 2675, Section 5.1 */\n#define IPV4_MIN_MTU\t\t68\t\t\t/* RFC 791 */\n\nextern unsigned int sysctl_fib_sync_mem;\nextern unsigned int sysctl_fib_sync_mem_min;\nextern unsigned int sysctl_fib_sync_mem_max;\n\nstruct sock;\n\nstruct inet_skb_parm {\n\tint\t\t\tiif;\n\tstruct ip_options\topt;\t\t/* Compiled IP options\t\t*/\n\tu16\t\t\tflags;\n\n#define IPSKB_FORWARDED\t\tBIT(0)\n#define IPSKB_XFRM_TUNNEL_SIZE\tBIT(1)\n#define IPSKB_XFRM_TRANSFORMED\tBIT(2)\n#define IPSKB_FRAG_COMPLETE\tBIT(3)\n#define IPSKB_REROUTED\t\tBIT(4)\n#define IPSKB_DOREDIRECT\tBIT(5)\n#define IPSKB_FRAG_PMTU\t\tBIT(6)\n#define IPSKB_L3SLAVE\t\tBIT(7)\n\n\tu16\t\t\tfrag_max_size;\n};\n\nstatic inline bool ipv4_l3mdev_skb(u16 flags)\n{\n\treturn !!(flags & IPSKB_L3SLAVE);\n}\n\nstatic inline unsigned int ip_hdrlen(const struct sk_buff *skb)\n{\n\treturn ip_hdr(skb)->ihl * 4;\n}\n\nstruct ipcm_cookie {\n\tstruct sockcm_cookie\tsockc;\n\t__be32\t\t\taddr;\n\tint\t\t\toif;\n\tstruct ip_options_rcu\t*opt;\n\t__u8\t\t\tttl;\n\t__s16\t\t\ttos;\n\tchar\t\t\tpriority;\n\t__u16\t\t\tgso_size;\n};\n\nstatic inline void ipcm_init(struct ipcm_cookie *ipcm)\n{\n\t*ipcm = (struct ipcm_cookie) { .tos = -1 };\n}\n\nstatic inline void ipcm_init_sk(struct ipcm_cookie *ipcm,\n\t\t\t\tconst struct inet_sock *inet)\n{\n\tipcm_init(ipcm);\n\n\tipcm->sockc.mark = inet->sk.sk_mark;\n\tipcm->sockc.tsflags = inet->sk.sk_tsflags;\n\tipcm->oif = inet->sk.sk_bound_dev_if;\n\tipcm->addr = inet->inet_saddr;\n}\n\n#define IPCB(skb) ((struct inet_skb_parm*)((skb)->cb))\n#define PKTINFO_SKB_CB(skb) ((struct in_pktinfo *)((skb)->cb))\n\n/* return enslaved device index if relevant */\nstatic inline int inet_sdif(const struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NET_L3_MASTER_DEV)\n\tif (skb && ipv4_l3mdev_skb(IPCB(skb)->flags))\n\t\treturn IPCB(skb)->iif;\n#endif\n\treturn 0;\n}\n\n/* Special input handler for packets caught by router alert option.\n   They are selected only by protocol field, and then processed likely\n   local ones; but only if someone wants them! Otherwise, router\n   not running rsvpd will kill RSVP.\n\n   It is user level problem, what it will make with them.\n   I have no idea, how it will masquearde or NAT them (it is joke, joke :-)),\n   but receiver should be enough clever f.e. to forward mtrace requests,\n   sent to multicast group to reach destination designated router.\n */\n\nstruct ip_ra_chain {\n\tstruct ip_ra_chain __rcu *next;\n\tstruct sock\t\t*sk;\n\tunion {\n\t\tvoid\t\t\t(*destructor)(struct sock *);\n\t\tstruct sock\t\t*saved_sk;\n\t};\n\tstruct rcu_head\t\trcu;\n};\n\n/* IP flags. */\n#define IP_CE\t\t0x8000\t\t/* Flag: \"Congestion\"\t\t*/\n#define IP_DF\t\t0x4000\t\t/* Flag: \"Don't Fragment\"\t*/\n#define IP_MF\t\t0x2000\t\t/* Flag: \"More Fragments\"\t*/\n#define IP_OFFSET\t0x1FFF\t\t/* \"Fragment Offset\" part\t*/\n\n#define IP_FRAG_TIME\t(30 * HZ)\t\t/* fragment lifetime\t*/\n\nstruct msghdr;\nstruct net_device;\nstruct packet_type;\nstruct rtable;\nstruct sockaddr;\n\nint igmp_mc_init(void);\n\n/*\n *\tFunctions provided by ip.c\n */\n\nint ip_build_and_send_pkt(struct sk_buff *skb, const struct sock *sk,\n\t\t\t  __be32 saddr, __be32 daddr,\n\t\t\t  struct ip_options_rcu *opt, u8 tos);\nint ip_rcv(struct sk_buff *skb, struct net_device *dev, struct packet_type *pt,\n\t   struct net_device *orig_dev);\nvoid ip_list_rcv(struct list_head *head, struct packet_type *pt,\n\t\t struct net_device *orig_dev);\nint ip_local_deliver(struct sk_buff *skb);\nvoid ip_protocol_deliver_rcu(struct net *net, struct sk_buff *skb, int proto);\nint ip_mr_input(struct sk_buff *skb);\nint ip_output(struct net *net, struct sock *sk, struct sk_buff *skb);\nint ip_mc_output(struct net *net, struct sock *sk, struct sk_buff *skb);\nint ip_do_fragment(struct net *net, struct sock *sk, struct sk_buff *skb,\n\t\t   int (*output)(struct net *, struct sock *, struct sk_buff *));\n\nstruct ip_fraglist_iter {\n\tstruct sk_buff\t*frag;\n\tstruct iphdr\t*iph;\n\tint\t\toffset;\n\tunsigned int\thlen;\n};\n\nvoid ip_fraglist_init(struct sk_buff *skb, struct iphdr *iph,\n\t\t      unsigned int hlen, struct ip_fraglist_iter *iter);\nvoid ip_fraglist_prepare(struct sk_buff *skb, struct ip_fraglist_iter *iter);\n\nstatic inline struct sk_buff *ip_fraglist_next(struct ip_fraglist_iter *iter)\n{\n\tstruct sk_buff *skb = iter->frag;\n\n\titer->frag = skb->next;\n\tskb_mark_not_on_list(skb);\n\n\treturn skb;\n}\n\nstruct ip_frag_state {\n\tbool\t\tDF;\n\tunsigned int\thlen;\n\tunsigned int\tll_rs;\n\tunsigned int\tmtu;\n\tunsigned int\tleft;\n\tint\t\toffset;\n\tint\t\tptr;\n\t__be16\t\tnot_last_frag;\n};\n\nvoid ip_frag_init(struct sk_buff *skb, unsigned int hlen, unsigned int ll_rs,\n\t\t  unsigned int mtu, bool DF, struct ip_frag_state *state);\nstruct sk_buff *ip_frag_next(struct sk_buff *skb,\n\t\t\t     struct ip_frag_state *state);\n\nvoid ip_send_check(struct iphdr *ip);\nint __ip_local_out(struct net *net, struct sock *sk, struct sk_buff *skb);\nint ip_local_out(struct net *net, struct sock *sk, struct sk_buff *skb);\n\nint __ip_queue_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl,\n\t\t    __u8 tos);\nvoid ip_init(void);\nint ip_append_data(struct sock *sk, struct flowi4 *fl4,\n\t\t   int getfrag(void *from, char *to, int offset, int len,\n\t\t\t       int odd, struct sk_buff *skb),\n\t\t   void *from, int len, int protolen,\n\t\t   struct ipcm_cookie *ipc,\n\t\t   struct rtable **rt,\n\t\t   unsigned int flags);\nint ip_generic_getfrag(void *from, char *to, int offset, int len, int odd,\n\t\t       struct sk_buff *skb);\nssize_t ip_append_page(struct sock *sk, struct flowi4 *fl4, struct page *page,\n\t\t       int offset, size_t size, int flags);\nstruct sk_buff *__ip_make_skb(struct sock *sk, struct flowi4 *fl4,\n\t\t\t      struct sk_buff_head *queue,\n\t\t\t      struct inet_cork *cork);\nint ip_send_skb(struct net *net, struct sk_buff *skb);\nint ip_push_pending_frames(struct sock *sk, struct flowi4 *fl4);\nvoid ip_flush_pending_frames(struct sock *sk);\nstruct sk_buff *ip_make_skb(struct sock *sk, struct flowi4 *fl4,\n\t\t\t    int getfrag(void *from, char *to, int offset,\n\t\t\t\t\tint len, int odd, struct sk_buff *skb),\n\t\t\t    void *from, int length, int transhdrlen,\n\t\t\t    struct ipcm_cookie *ipc, struct rtable **rtp,\n\t\t\t    struct inet_cork *cork, unsigned int flags);\n\nint ip_queue_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl);\n\nstatic inline struct sk_buff *ip_finish_skb(struct sock *sk, struct flowi4 *fl4)\n{\n\treturn __ip_make_skb(sk, fl4, &sk->sk_write_queue, &inet_sk(sk)->cork.base);\n}\n\nstatic inline __u8 get_rttos(struct ipcm_cookie* ipc, struct inet_sock *inet)\n{\n\treturn (ipc->tos != -1) ? RT_TOS(ipc->tos) : RT_TOS(inet->tos);\n}\n\nstatic inline __u8 get_rtconn_flags(struct ipcm_cookie* ipc, struct sock* sk)\n{\n\treturn (ipc->tos != -1) ? RT_CONN_FLAGS_TOS(sk, ipc->tos) : RT_CONN_FLAGS(sk);\n}\n\n/* datagram.c */\nint __ip4_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len);\nint ip4_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len);\n\nvoid ip4_datagram_release_cb(struct sock *sk);\n\nstruct ip_reply_arg {\n\tstruct kvec iov[1];\n\tint\t    flags;\n\t__wsum \t    csum;\n\tint\t    csumoffset; /* u16 offset of csum in iov[0].iov_base */\n\t\t\t\t/* -1 if not needed */\n\tint\t    bound_dev_if;\n\tu8  \t    tos;\n\tkuid_t\t    uid;\n};\n\n#define IP_REPLY_ARG_NOSRCCHECK 1\n\nstatic inline __u8 ip_reply_arg_flowi_flags(const struct ip_reply_arg *arg)\n{\n\treturn (arg->flags & IP_REPLY_ARG_NOSRCCHECK) ? FLOWI_FLAG_ANYSRC : 0;\n}\n\nvoid ip_send_unicast_reply(struct sock *sk, struct sk_buff *skb,\n\t\t\t   const struct ip_options *sopt,\n\t\t\t   __be32 daddr, __be32 saddr,\n\t\t\t   const struct ip_reply_arg *arg,\n\t\t\t   unsigned int len, u64 transmit_time);\n\n#define IP_INC_STATS(net, field)\tSNMP_INC_STATS64((net)->mib.ip_statistics, field)\n#define __IP_INC_STATS(net, field)\t__SNMP_INC_STATS64((net)->mib.ip_statistics, field)\n#define IP_ADD_STATS(net, field, val)\tSNMP_ADD_STATS64((net)->mib.ip_statistics, field, val)\n#define __IP_ADD_STATS(net, field, val) __SNMP_ADD_STATS64((net)->mib.ip_statistics, field, val)\n#define IP_UPD_PO_STATS(net, field, val) SNMP_UPD_PO_STATS64((net)->mib.ip_statistics, field, val)\n#define __IP_UPD_PO_STATS(net, field, val) __SNMP_UPD_PO_STATS64((net)->mib.ip_statistics, field, val)\n#define NET_INC_STATS(net, field)\tSNMP_INC_STATS((net)->mib.net_statistics, field)\n#define __NET_INC_STATS(net, field)\t__SNMP_INC_STATS((net)->mib.net_statistics, field)\n#define NET_ADD_STATS(net, field, adnd)\tSNMP_ADD_STATS((net)->mib.net_statistics, field, adnd)\n#define __NET_ADD_STATS(net, field, adnd) __SNMP_ADD_STATS((net)->mib.net_statistics, field, adnd)\n\nu64 snmp_get_cpu_field(void __percpu *mib, int cpu, int offct);\nunsigned long snmp_fold_field(void __percpu *mib, int offt);\n#if BITS_PER_LONG==32\nu64 snmp_get_cpu_field64(void __percpu *mib, int cpu, int offct,\n\t\t\t size_t syncp_offset);\nu64 snmp_fold_field64(void __percpu *mib, int offt, size_t sync_off);\n#else\nstatic inline u64  snmp_get_cpu_field64(void __percpu *mib, int cpu, int offct,\n\t\t\t\t\tsize_t syncp_offset)\n{\n\treturn snmp_get_cpu_field(mib, cpu, offct);\n\n}\n\nstatic inline u64 snmp_fold_field64(void __percpu *mib, int offt, size_t syncp_off)\n{\n\treturn snmp_fold_field(mib, offt);\n}\n#endif\n\n#define snmp_get_cpu_field64_batch(buff64, stats_list, mib_statistic, offset) \\\n{ \\\n\tint i, c; \\\n\tfor_each_possible_cpu(c) { \\\n\t\tfor (i = 0; stats_list[i].name; i++) \\\n\t\t\tbuff64[i] += snmp_get_cpu_field64( \\\n\t\t\t\t\tmib_statistic, \\\n\t\t\t\t\tc, stats_list[i].entry, \\\n\t\t\t\t\toffset); \\\n\t} \\\n}\n\n#define snmp_get_cpu_field_batch(buff, stats_list, mib_statistic) \\\n{ \\\n\tint i, c; \\\n\tfor_each_possible_cpu(c) { \\\n\t\tfor (i = 0; stats_list[i].name; i++) \\\n\t\t\tbuff[i] += snmp_get_cpu_field( \\\n\t\t\t\t\t\tmib_statistic, \\\n\t\t\t\t\t\tc, stats_list[i].entry); \\\n\t} \\\n}\n\nvoid inet_get_local_port_range(struct net *net, int *low, int *high);\n\n#ifdef CONFIG_SYSCTL\nstatic inline bool inet_is_local_reserved_port(struct net *net, unsigned short port)\n{\n\tif (!net->ipv4.sysctl_local_reserved_ports)\n\t\treturn false;\n\treturn test_bit(port, net->ipv4.sysctl_local_reserved_ports);\n}\n\nstatic inline bool sysctl_dev_name_is_allowed(const char *name)\n{\n\treturn strcmp(name, \"default\") != 0  && strcmp(name, \"all\") != 0;\n}\n\nstatic inline bool inet_port_requires_bind_service(struct net *net, unsigned short port)\n{\n\treturn port < net->ipv4.sysctl_ip_prot_sock;\n}\n\n#else\nstatic inline bool inet_is_local_reserved_port(struct net *net, unsigned short port)\n{\n\treturn false;\n}\n\nstatic inline bool inet_port_requires_bind_service(struct net *net, unsigned short port)\n{\n\treturn port < PROT_SOCK;\n}\n#endif\n\n__be32 inet_current_timestamp(void);\n\n/* From inetpeer.c */\nextern int inet_peer_threshold;\nextern int inet_peer_minttl;\nextern int inet_peer_maxttl;\n\nvoid ipfrag_init(void);\n\nvoid ip_static_sysctl_init(void);\n\n#define IP4_REPLY_MARK(net, mark) \\\n\t((net)->ipv4.sysctl_fwmark_reflect ? (mark) : 0)\n\nstatic inline bool ip_is_fragment(const struct iphdr *iph)\n{\n\treturn (iph->frag_off & htons(IP_MF | IP_OFFSET)) != 0;\n}\n\n#ifdef CONFIG_INET\n#include <net/dst.h>\n\n/* The function in 2.2 was invalid, producing wrong result for\n * check=0xFEFF. It was noticed by Arthur Skawina _year_ ago. --ANK(000625) */\nstatic inline\nint ip_decrease_ttl(struct iphdr *iph)\n{\n\tu32 check = (__force u32)iph->check;\n\tcheck += (__force u32)htons(0x0100);\n\tiph->check = (__force __sum16)(check + (check>=0xFFFF));\n\treturn --iph->ttl;\n}\n\nstatic inline int ip_mtu_locked(const struct dst_entry *dst)\n{\n\tconst struct rtable *rt = (const struct rtable *)dst;\n\n\treturn rt->rt_mtu_locked || dst_metric_locked(dst, RTAX_MTU);\n}\n\nstatic inline\nint ip_dont_fragment(const struct sock *sk, const struct dst_entry *dst)\n{\n\tu8 pmtudisc = READ_ONCE(inet_sk(sk)->pmtudisc);\n\n\treturn  pmtudisc == IP_PMTUDISC_DO ||\n\t\t(pmtudisc == IP_PMTUDISC_WANT &&\n\t\t !ip_mtu_locked(dst));\n}\n\nstatic inline bool ip_sk_accept_pmtu(const struct sock *sk)\n{\n\treturn inet_sk(sk)->pmtudisc != IP_PMTUDISC_INTERFACE &&\n\t       inet_sk(sk)->pmtudisc != IP_PMTUDISC_OMIT;\n}\n\nstatic inline bool ip_sk_use_pmtu(const struct sock *sk)\n{\n\treturn inet_sk(sk)->pmtudisc < IP_PMTUDISC_PROBE;\n}\n\nstatic inline bool ip_sk_ignore_df(const struct sock *sk)\n{\n\treturn inet_sk(sk)->pmtudisc < IP_PMTUDISC_DO ||\n\t       inet_sk(sk)->pmtudisc == IP_PMTUDISC_OMIT;\n}\n\nstatic inline unsigned int ip_dst_mtu_maybe_forward(const struct dst_entry *dst,\n\t\t\t\t\t\t    bool forwarding)\n{\n\tstruct net *net = dev_net(dst->dev);\n\tunsigned int mtu;\n\n\tif (net->ipv4.sysctl_ip_fwd_use_pmtu ||\n\t    ip_mtu_locked(dst) ||\n\t    !forwarding)\n\t\treturn dst_mtu(dst);\n\n\t/* 'forwarding = true' case should always honour route mtu */\n\tmtu = dst_metric_raw(dst, RTAX_MTU);\n\tif (mtu)\n\t\treturn mtu;\n\n\treturn min(READ_ONCE(dst->dev->mtu), IP_MAX_MTU);\n}\n\nstatic inline unsigned int ip_skb_dst_mtu(struct sock *sk,\n\t\t\t\t\t  const struct sk_buff *skb)\n{\n\tif (!sk || !sk_fullsock(sk) || ip_sk_use_pmtu(sk)) {\n\t\tbool forwarding = IPCB(skb)->flags & IPSKB_FORWARDED;\n\n\t\treturn ip_dst_mtu_maybe_forward(skb_dst(skb), forwarding);\n\t}\n\n\treturn min(READ_ONCE(skb_dst(skb)->dev->mtu), IP_MAX_MTU);\n}\n\nstruct dst_metrics *ip_fib_metrics_init(struct net *net, struct nlattr *fc_mx,\n\t\t\t\t\tint fc_mx_len,\n\t\t\t\t\tstruct netlink_ext_ack *extack);\nstatic inline void ip_fib_metrics_put(struct dst_metrics *fib_metrics)\n{\n\tif (fib_metrics != &dst_default_metrics &&\n\t    refcount_dec_and_test(&fib_metrics->refcnt))\n\t\tkfree(fib_metrics);\n}\n\n/* ipv4 and ipv6 both use refcounted metrics if it is not the default */\nstatic inline\nvoid ip_dst_init_metrics(struct dst_entry *dst, struct dst_metrics *fib_metrics)\n{\n\tdst_init_metrics(dst, fib_metrics->metrics, true);\n\n\tif (fib_metrics != &dst_default_metrics) {\n\t\tdst->_metrics |= DST_METRICS_REFCOUNTED;\n\t\trefcount_inc(&fib_metrics->refcnt);\n\t}\n}\n\nstatic inline\nvoid ip_dst_metrics_put(struct dst_entry *dst)\n{\n\tstruct dst_metrics *p = (struct dst_metrics *)DST_METRICS_PTR(dst);\n\n\tif (p != &dst_default_metrics && refcount_dec_and_test(&p->refcnt))\n\t\tkfree(p);\n}\n\nu32 ip_idents_reserve(u32 hash, int segs);\nvoid __ip_select_ident(struct net *net, struct iphdr *iph, int segs);\n\nstatic inline void ip_select_ident_segs(struct net *net, struct sk_buff *skb,\n\t\t\t\t\tstruct sock *sk, int segs)\n{\n\tstruct iphdr *iph = ip_hdr(skb);\n\n\tif ((iph->frag_off & htons(IP_DF)) && !skb->ignore_df) {\n\t\t/* This is only to work around buggy Windows95/2000\n\t\t * VJ compression implementations.  If the ID field\n\t\t * does not change, they drop every other packet in\n\t\t * a TCP stream using header compression.\n\t\t */\n\t\tif (sk && inet_sk(sk)->inet_daddr) {\n\t\t\tiph->id = htons(inet_sk(sk)->inet_id);\n\t\t\tinet_sk(sk)->inet_id += segs;\n\t\t} else {\n\t\t\tiph->id = 0;\n\t\t}\n\t} else {\n\t\t__ip_select_ident(net, iph, segs);\n\t}\n}\n\nstatic inline void ip_select_ident(struct net *net, struct sk_buff *skb,\n\t\t\t\t   struct sock *sk)\n{\n\tip_select_ident_segs(net, skb, sk, 1);\n}\n\nstatic inline __wsum inet_compute_pseudo(struct sk_buff *skb, int proto)\n{\n\treturn csum_tcpudp_nofold(ip_hdr(skb)->saddr, ip_hdr(skb)->daddr,\n\t\t\t\t  skb->len, proto, 0);\n}\n\n/* copy IPv4 saddr & daddr to flow_keys, possibly using 64bit load/store\n * Equivalent to :\tflow->v4addrs.src = iph->saddr;\n *\t\t\tflow->v4addrs.dst = iph->daddr;\n */\nstatic inline void iph_to_flow_copy_v4addrs(struct flow_keys *flow,\n\t\t\t\t\t    const struct iphdr *iph)\n{\n\tBUILD_BUG_ON(offsetof(typeof(flow->addrs), v4addrs.dst) !=\n\t\t     offsetof(typeof(flow->addrs), v4addrs.src) +\n\t\t\t      sizeof(flow->addrs.v4addrs.src));\n\tmemcpy(&flow->addrs.v4addrs, &iph->saddr, sizeof(flow->addrs.v4addrs));\n\tflow->control.addr_type = FLOW_DISSECTOR_KEY_IPV4_ADDRS;\n}\n\nstatic inline __wsum inet_gro_compute_pseudo(struct sk_buff *skb, int proto)\n{\n\tconst struct iphdr *iph = skb_gro_network_header(skb);\n\n\treturn csum_tcpudp_nofold(iph->saddr, iph->daddr,\n\t\t\t\t  skb_gro_len(skb), proto, 0);\n}\n\n/*\n *\tMap a multicast IP onto multicast MAC for type ethernet.\n */\n\nstatic inline void ip_eth_mc_map(__be32 naddr, char *buf)\n{\n\t__u32 addr=ntohl(naddr);\n\tbuf[0]=0x01;\n\tbuf[1]=0x00;\n\tbuf[2]=0x5e;\n\tbuf[5]=addr&0xFF;\n\taddr>>=8;\n\tbuf[4]=addr&0xFF;\n\taddr>>=8;\n\tbuf[3]=addr&0x7F;\n}\n\n/*\n *\tMap a multicast IP onto multicast MAC for type IP-over-InfiniBand.\n *\tLeave P_Key as 0 to be filled in by driver.\n */\n\nstatic inline void ip_ib_mc_map(__be32 naddr, const unsigned char *broadcast, char *buf)\n{\n\t__u32 addr;\n\tunsigned char scope = broadcast[5] & 0xF;\n\n\tbuf[0]  = 0;\t\t/* Reserved */\n\tbuf[1]  = 0xff;\t\t/* Multicast QPN */\n\tbuf[2]  = 0xff;\n\tbuf[3]  = 0xff;\n\taddr    = ntohl(naddr);\n\tbuf[4]  = 0xff;\n\tbuf[5]  = 0x10 | scope;\t/* scope from broadcast address */\n\tbuf[6]  = 0x40;\t\t/* IPv4 signature */\n\tbuf[7]  = 0x1b;\n\tbuf[8]  = broadcast[8];\t\t/* P_Key */\n\tbuf[9]  = broadcast[9];\n\tbuf[10] = 0;\n\tbuf[11] = 0;\n\tbuf[12] = 0;\n\tbuf[13] = 0;\n\tbuf[14] = 0;\n\tbuf[15] = 0;\n\tbuf[19] = addr & 0xff;\n\taddr  >>= 8;\n\tbuf[18] = addr & 0xff;\n\taddr  >>= 8;\n\tbuf[17] = addr & 0xff;\n\taddr  >>= 8;\n\tbuf[16] = addr & 0x0f;\n}\n\nstatic inline void ip_ipgre_mc_map(__be32 naddr, const unsigned char *broadcast, char *buf)\n{\n\tif ((broadcast[0] | broadcast[1] | broadcast[2] | broadcast[3]) != 0)\n\t\tmemcpy(buf, broadcast, 4);\n\telse\n\t\tmemcpy(buf, &naddr, sizeof(naddr));\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\n#include <linux/ipv6.h>\n#endif\n\nstatic __inline__ void inet_reset_saddr(struct sock *sk)\n{\n\tinet_sk(sk)->inet_rcv_saddr = inet_sk(sk)->inet_saddr = 0;\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (sk->sk_family == PF_INET6) {\n\t\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\n\t\tmemset(&np->saddr, 0, sizeof(np->saddr));\n\t\tmemset(&sk->sk_v6_rcv_saddr, 0, sizeof(sk->sk_v6_rcv_saddr));\n\t}\n#endif\n}\n\n#endif\n\nstatic inline unsigned int ipv4_addr_hash(__be32 ip)\n{\n\treturn (__force unsigned int) ip;\n}\n\nstatic inline u32 ipv4_portaddr_hash(const struct net *net,\n\t\t\t\t     __be32 saddr,\n\t\t\t\t     unsigned int port)\n{\n\treturn jhash_1word((__force u32)saddr, net_hash_mix(net)) ^ port;\n}\n\nbool ip_call_ra_chain(struct sk_buff *skb);\n\n/*\n *\tFunctions provided by ip_fragment.c\n */\n\nenum ip_defrag_users {\n\tIP_DEFRAG_LOCAL_DELIVER,\n\tIP_DEFRAG_CALL_RA_CHAIN,\n\tIP_DEFRAG_CONNTRACK_IN,\n\t__IP_DEFRAG_CONNTRACK_IN_END\t= IP_DEFRAG_CONNTRACK_IN + USHRT_MAX,\n\tIP_DEFRAG_CONNTRACK_OUT,\n\t__IP_DEFRAG_CONNTRACK_OUT_END\t= IP_DEFRAG_CONNTRACK_OUT + USHRT_MAX,\n\tIP_DEFRAG_CONNTRACK_BRIDGE_IN,\n\t__IP_DEFRAG_CONNTRACK_BRIDGE_IN = IP_DEFRAG_CONNTRACK_BRIDGE_IN + USHRT_MAX,\n\tIP_DEFRAG_VS_IN,\n\tIP_DEFRAG_VS_OUT,\n\tIP_DEFRAG_VS_FWD,\n\tIP_DEFRAG_AF_PACKET,\n\tIP_DEFRAG_MACVLAN,\n};\n\n/* Return true if the value of 'user' is between 'lower_bond'\n * and 'upper_bond' inclusively.\n */\nstatic inline bool ip_defrag_user_in_between(u32 user,\n\t\t\t\t\t     enum ip_defrag_users lower_bond,\n\t\t\t\t\t     enum ip_defrag_users upper_bond)\n{\n\treturn user >= lower_bond && user <= upper_bond;\n}\n\nint ip_defrag(struct net *net, struct sk_buff *skb, u32 user);\n#ifdef CONFIG_INET\nstruct sk_buff *ip_check_defrag(struct net *net, struct sk_buff *skb, u32 user);\n#else\nstatic inline struct sk_buff *ip_check_defrag(struct net *net, struct sk_buff *skb, u32 user)\n{\n\treturn skb;\n}\n#endif\n\n/*\n *\tFunctions provided by ip_forward.c\n */\n\nint ip_forward(struct sk_buff *skb);\n\n/*\n *\tFunctions provided by ip_options.c\n */\n\nvoid ip_options_build(struct sk_buff *skb, struct ip_options *opt,\n\t\t      __be32 daddr, struct rtable *rt, int is_frag);\n\nint __ip_options_echo(struct net *net, struct ip_options *dopt,\n\t\t      struct sk_buff *skb, const struct ip_options *sopt);\nstatic inline int ip_options_echo(struct net *net, struct ip_options *dopt,\n\t\t\t\t  struct sk_buff *skb)\n{\n\treturn __ip_options_echo(net, dopt, skb, &IPCB(skb)->opt);\n}\n\nvoid ip_options_fragment(struct sk_buff *skb);\nint __ip_options_compile(struct net *net, struct ip_options *opt,\n\t\t\t struct sk_buff *skb, __be32 *info);\nint ip_options_compile(struct net *net, struct ip_options *opt,\n\t\t       struct sk_buff *skb);\nint ip_options_get(struct net *net, struct ip_options_rcu **optp,\n\t\t   sockptr_t data, int optlen);\nvoid ip_options_undo(struct ip_options *opt);\nvoid ip_forward_options(struct sk_buff *skb);\nint ip_options_rcv_srr(struct sk_buff *skb, struct net_device *dev);\n\n/*\n *\tFunctions provided by ip_sockglue.c\n */\n\nvoid ipv4_pktinfo_prepare(const struct sock *sk, struct sk_buff *skb);\nvoid ip_cmsg_recv_offset(struct msghdr *msg, struct sock *sk,\n\t\t\t struct sk_buff *skb, int tlen, int offset);\nint ip_cmsg_send(struct sock *sk, struct msghdr *msg,\n\t\t struct ipcm_cookie *ipc, bool allow_ipv6);\nint ip_setsockopt(struct sock *sk, int level, int optname, sockptr_t optval,\n\t\t  unsigned int optlen);\nint ip_getsockopt(struct sock *sk, int level, int optname, char __user *optval,\n\t\t  int __user *optlen);\nint ip_ra_control(struct sock *sk, unsigned char on,\n\t\t  void (*destructor)(struct sock *));\n\nint ip_recv_error(struct sock *sk, struct msghdr *msg, int len, int *addr_len);\nvoid ip_icmp_error(struct sock *sk, struct sk_buff *skb, int err, __be16 port,\n\t\t   u32 info, u8 *payload);\nvoid ip_local_error(struct sock *sk, int err, __be32 daddr, __be16 dport,\n\t\t    u32 info);\n\nstatic inline void ip_cmsg_recv(struct msghdr *msg, struct sk_buff *skb)\n{\n\tip_cmsg_recv_offset(msg, skb->sk, skb, 0, 0);\n}\n\nbool icmp_global_allow(void);\nextern int sysctl_icmp_msgs_per_sec;\nextern int sysctl_icmp_msgs_burst;\n\n#ifdef CONFIG_PROC_FS\nint ip_misc_proc_init(void);\n#endif\n\nint rtm_getroute_parse_ip_proto(struct nlattr *attr, u8 *ip_proto, u8 family,\n\t\t\t\tstruct netlink_ext_ack *extack);\n\nstatic inline bool inetdev_valid_mtu(unsigned int mtu)\n{\n\treturn likely(mtu >= IPV4_MIN_MTU);\n}\n\nvoid ip_sock_set_freebind(struct sock *sk);\nint ip_sock_set_mtu_discover(struct sock *sk, int val);\nvoid ip_sock_set_pktinfo(struct sock *sk);\nvoid ip_sock_set_recverr(struct sock *sk);\nvoid ip_sock_set_tos(struct sock *sk, int val);\n\n#endif\t/* _IP_H */\n"}, "10": {"id": 10, "path": "/src/include/linux/compiler.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_COMPILER_H\n#define __LINUX_COMPILER_H\n\n#include <linux/compiler_types.h>\n\n#ifndef __ASSEMBLY__\n\n#ifdef __KERNEL__\n\n/*\n * Note: DISABLE_BRANCH_PROFILING can be used by special lowlevel code\n * to disable branch tracing on a per file basis.\n */\n#if defined(CONFIG_TRACE_BRANCH_PROFILING) \\\n    && !defined(DISABLE_BRANCH_PROFILING) && !defined(__CHECKER__)\nvoid ftrace_likely_update(struct ftrace_likely_data *f, int val,\n\t\t\t  int expect, int is_constant);\n\n#define likely_notrace(x)\t__builtin_expect(!!(x), 1)\n#define unlikely_notrace(x)\t__builtin_expect(!!(x), 0)\n\n#define __branch_check__(x, expect, is_constant) ({\t\t\t\\\n\t\t\tlong ______r;\t\t\t\t\t\\\n\t\t\tstatic struct ftrace_likely_data\t\t\\\n\t\t\t\t__aligned(4)\t\t\t\t\\\n\t\t\t\t__section(\"_ftrace_annotated_branch\")\t\\\n\t\t\t\t______f = {\t\t\t\t\\\n\t\t\t\t.data.func = __func__,\t\t\t\\\n\t\t\t\t.data.file = __FILE__,\t\t\t\\\n\t\t\t\t.data.line = __LINE__,\t\t\t\\\n\t\t\t};\t\t\t\t\t\t\\\n\t\t\t______r = __builtin_expect(!!(x), expect);\t\\\n\t\t\tftrace_likely_update(&______f, ______r,\t\t\\\n\t\t\t\t\t     expect, is_constant);\t\\\n\t\t\t______r;\t\t\t\t\t\\\n\t\t})\n\n/*\n * Using __builtin_constant_p(x) to ignore cases where the return\n * value is always the same.  This idea is taken from a similar patch\n * written by Daniel Walker.\n */\n# ifndef likely\n#  define likely(x)\t(__branch_check__(x, 1, __builtin_constant_p(x)))\n# endif\n# ifndef unlikely\n#  define unlikely(x)\t(__branch_check__(x, 0, __builtin_constant_p(x)))\n# endif\n\n#ifdef CONFIG_PROFILE_ALL_BRANCHES\n/*\n * \"Define 'is'\", Bill Clinton\n * \"Define 'if'\", Steven Rostedt\n */\n#define if(cond, ...) if ( __trace_if_var( !!(cond , ## __VA_ARGS__) ) )\n\n#define __trace_if_var(cond) (__builtin_constant_p(cond) ? (cond) : __trace_if_value(cond))\n\n#define __trace_if_value(cond) ({\t\t\t\\\n\tstatic struct ftrace_branch_data\t\t\\\n\t\t__aligned(4)\t\t\t\t\\\n\t\t__section(\"_ftrace_branch\")\t\t\\\n\t\t__if_trace = {\t\t\t\t\\\n\t\t\t.func = __func__,\t\t\\\n\t\t\t.file = __FILE__,\t\t\\\n\t\t\t.line = __LINE__,\t\t\\\n\t\t};\t\t\t\t\t\\\n\t(cond) ?\t\t\t\t\t\\\n\t\t(__if_trace.miss_hit[1]++,1) :\t\t\\\n\t\t(__if_trace.miss_hit[0]++,0);\t\t\\\n})\n\n#endif /* CONFIG_PROFILE_ALL_BRANCHES */\n\n#else\n# define likely(x)\t__builtin_expect(!!(x), 1)\n# define unlikely(x)\t__builtin_expect(!!(x), 0)\n# define likely_notrace(x)\tlikely(x)\n# define unlikely_notrace(x)\tunlikely(x)\n#endif\n\n/* Optimization barrier */\n#ifndef barrier\n/* The \"volatile\" is due to gcc bugs */\n# define barrier() __asm__ __volatile__(\"\": : :\"memory\")\n#endif\n\n#ifndef barrier_data\n/*\n * This version is i.e. to prevent dead stores elimination on @ptr\n * where gcc and llvm may behave differently when otherwise using\n * normal barrier(): while gcc behavior gets along with a normal\n * barrier(), llvm needs an explicit input variable to be assumed\n * clobbered. The issue is as follows: while the inline asm might\n * access any memory it wants, the compiler could have fit all of\n * @ptr into memory registers instead, and since @ptr never escaped\n * from that, it proved that the inline asm wasn't touching any of\n * it. This version works well with both compilers, i.e. we're telling\n * the compiler that the inline asm absolutely may see the contents\n * of @ptr. See also: https://llvm.org/bugs/show_bug.cgi?id=15495\n */\n# define barrier_data(ptr) __asm__ __volatile__(\"\": :\"r\"(ptr) :\"memory\")\n#endif\n\n/* workaround for GCC PR82365 if needed */\n#ifndef barrier_before_unreachable\n# define barrier_before_unreachable() do { } while (0)\n#endif\n\n/* Unreachable code */\n#ifdef CONFIG_STACK_VALIDATION\n/*\n * These macros help objtool understand GCC code flow for unreachable code.\n * The __COUNTER__ based labels are a hack to make each instance of the macros\n * unique, to convince GCC not to merge duplicate inline asm statements.\n */\n#define annotate_reachable() ({\t\t\t\t\t\t\\\n\tasm volatile(\"%c0:\\n\\t\"\t\t\t\t\t\t\\\n\t\t     \".pushsection .discard.reachable\\n\\t\"\t\t\\\n\t\t     \".long %c0b - .\\n\\t\"\t\t\t\t\\\n\t\t     \".popsection\\n\\t\" : : \"i\" (__COUNTER__));\t\t\\\n})\n#define annotate_unreachable() ({\t\t\t\t\t\\\n\tasm volatile(\"%c0:\\n\\t\"\t\t\t\t\t\t\\\n\t\t     \".pushsection .discard.unreachable\\n\\t\"\t\t\\\n\t\t     \".long %c0b - .\\n\\t\"\t\t\t\t\\\n\t\t     \".popsection\\n\\t\" : : \"i\" (__COUNTER__));\t\t\\\n})\n#define ASM_UNREACHABLE\t\t\t\t\t\t\t\\\n\t\"999:\\n\\t\"\t\t\t\t\t\t\t\\\n\t\".pushsection .discard.unreachable\\n\\t\"\t\t\t\t\\\n\t\".long 999b - .\\n\\t\"\t\t\t\t\t\t\\\n\t\".popsection\\n\\t\"\n\n/* Annotate a C jump table to allow objtool to follow the code flow */\n#define __annotate_jump_table __section(\".rodata..c_jump_table\")\n\n#else\n#define annotate_reachable()\n#define annotate_unreachable()\n#define __annotate_jump_table\n#endif\n\n#ifndef ASM_UNREACHABLE\n# define ASM_UNREACHABLE\n#endif\n#ifndef unreachable\n# define unreachable() do {\t\t\\\n\tannotate_unreachable();\t\t\\\n\t__builtin_unreachable();\t\\\n} while (0)\n#endif\n\n/*\n * KENTRY - kernel entry point\n * This can be used to annotate symbols (functions or data) that are used\n * without their linker symbol being referenced explicitly. For example,\n * interrupt vector handlers, or functions in the kernel image that are found\n * programatically.\n *\n * Not required for symbols exported with EXPORT_SYMBOL, or initcalls. Those\n * are handled in their own way (with KEEP() in linker scripts).\n *\n * KENTRY can be avoided if the symbols in question are marked as KEEP() in the\n * linker script. For example an architecture could KEEP() its entire\n * boot/exception vector code rather than annotate each function and data.\n */\n#ifndef KENTRY\n# define KENTRY(sym)\t\t\t\t\t\t\\\n\textern typeof(sym) sym;\t\t\t\t\t\\\n\tstatic const unsigned long __kentry_##sym\t\t\\\n\t__used\t\t\t\t\t\t\t\\\n\t__attribute__((__section__(\"___kentry+\" #sym)))\t\t\\\n\t= (unsigned long)&sym;\n#endif\n\n#ifndef RELOC_HIDE\n# define RELOC_HIDE(ptr, off)\t\t\t\t\t\\\n  ({ unsigned long __ptr;\t\t\t\t\t\\\n     __ptr = (unsigned long) (ptr);\t\t\t\t\\\n    (typeof(ptr)) (__ptr + (off)); })\n#endif\n\n#ifndef OPTIMIZER_HIDE_VAR\n/* Make the optimizer believe the variable can be manipulated arbitrarily. */\n#define OPTIMIZER_HIDE_VAR(var)\t\t\t\t\t\t\\\n\t__asm__ (\"\" : \"=r\" (var) : \"0\" (var))\n#endif\n\n/* Not-quite-unique ID. */\n#ifndef __UNIQUE_ID\n# define __UNIQUE_ID(prefix) __PASTE(__PASTE(__UNIQUE_ID_, prefix), __LINE__)\n#endif\n\n/**\n * data_race - mark an expression as containing intentional data races\n *\n * This data_race() macro is useful for situations in which data races\n * should be forgiven.  One example is diagnostic code that accesses\n * shared variables but is not a part of the core synchronization design.\n *\n * This macro *does not* affect normal code generation, but is a hint\n * to tooling that data races here are to be ignored.\n */\n#define data_race(expr)\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__unqual_scalar_typeof(({ expr; })) __v = ({\t\t\t\\\n\t\t__kcsan_disable_current();\t\t\t\t\\\n\t\texpr;\t\t\t\t\t\t\t\\\n\t});\t\t\t\t\t\t\t\t\\\n\t__kcsan_enable_current();\t\t\t\t\t\\\n\t__v;\t\t\t\t\t\t\t\t\\\n})\n\n#endif /* __KERNEL__ */\n\n/*\n * Force the compiler to emit 'sym' as a symbol, so that we can reference\n * it from inline assembler. Necessary in case 'sym' could be inlined\n * otherwise, or eliminated entirely due to lack of references that are\n * visible to the compiler.\n */\n#define __ADDRESSABLE(sym) \\\n\tstatic void * __section(\".discard.addressable\") __used \\\n\t\t__UNIQUE_ID(__PASTE(__addressable_,sym)) = (void *)&sym;\n\n/**\n * offset_to_ptr - convert a relative memory offset to an absolute pointer\n * @off:\tthe address of the 32-bit offset value\n */\nstatic inline void *offset_to_ptr(const int *off)\n{\n\treturn (void *)((unsigned long)off + *off);\n}\n\n#endif /* __ASSEMBLY__ */\n\n/* &a[0] degrades to a pointer: a different type from an array */\n#define __must_be_array(a)\tBUILD_BUG_ON_ZERO(__same_type((a), &(a)[0]))\n\n/*\n * This is needed in functions which generate the stack canary, see\n * arch/x86/kernel/smpboot.c::start_secondary() for an example.\n */\n#define prevent_tail_call_optimization()\tmb()\n\n#include <asm/rwonce.h>\n\n#endif /* __LINUX_COMPILER_H */\n"}, "11": {"id": 11, "path": "/src/include/linux/compiler_types.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_COMPILER_TYPES_H\n#define __LINUX_COMPILER_TYPES_H\n\n#ifndef __ASSEMBLY__\n\n#ifdef __CHECKER__\n/* address spaces */\n# define __kernel\t__attribute__((address_space(0)))\n# define __user\t\t__attribute__((noderef, address_space(__user)))\n# define __iomem\t__attribute__((noderef, address_space(__iomem)))\n# define __percpu\t__attribute__((noderef, address_space(__percpu)))\n# define __rcu\t\t__attribute__((noderef, address_space(__rcu)))\nstatic inline void __chk_user_ptr(const volatile void __user *ptr) { }\nstatic inline void __chk_io_ptr(const volatile void __iomem *ptr) { }\n/* context/locking */\n# define __must_hold(x)\t__attribute__((context(x,1,1)))\n# define __acquires(x)\t__attribute__((context(x,0,1)))\n# define __releases(x)\t__attribute__((context(x,1,0)))\n# define __acquire(x)\t__context__(x,1)\n# define __release(x)\t__context__(x,-1)\n# define __cond_lock(x,c)\t((c) ? ({ __acquire(x); 1; }) : 0)\n/* other */\n# define __force\t__attribute__((force))\n# define __nocast\t__attribute__((nocast))\n# define __safe\t\t__attribute__((safe))\n# define __private\t__attribute__((noderef))\n# define ACCESS_PRIVATE(p, member) (*((typeof((p)->member) __force *) &(p)->member))\n#else /* __CHECKER__ */\n/* address spaces */\n# define __kernel\n# ifdef STRUCTLEAK_PLUGIN\n#  define __user\t__attribute__((user))\n# else\n#  define __user\n# endif\n# define __iomem\n# define __percpu\n# define __rcu\n# define __chk_user_ptr(x)\t(void)0\n# define __chk_io_ptr(x)\t(void)0\n/* context/locking */\n# define __must_hold(x)\n# define __acquires(x)\n# define __releases(x)\n# define __acquire(x)\t(void)0\n# define __release(x)\t(void)0\n# define __cond_lock(x,c) (c)\n/* other */\n# define __force\n# define __nocast\n# define __safe\n# define __private\n# define ACCESS_PRIVATE(p, member) ((p)->member)\n# define __builtin_warning(x, y...) (1)\n#endif /* __CHECKER__ */\n\n/* Indirect macros required for expanded argument pasting, eg. __LINE__. */\n#define ___PASTE(a,b) a##b\n#define __PASTE(a,b) ___PASTE(a,b)\n\n#ifdef __KERNEL__\n\n/* Attributes */\n#include <linux/compiler_attributes.h>\n\n/* Builtins */\n\n/*\n * __has_builtin is supported on gcc >= 10, clang >= 3 and icc >= 21.\n * In the meantime, to support gcc < 10, we implement __has_builtin\n * by hand.\n */\n#ifndef __has_builtin\n#define __has_builtin(x) (0)\n#endif\n\n/* Compiler specific macros. */\n#ifdef __clang__\n#include <linux/compiler-clang.h>\n#elif defined(__INTEL_COMPILER)\n#include <linux/compiler-intel.h>\n#elif defined(__GNUC__)\n/* The above compilers also define __GNUC__, so order is important here. */\n#include <linux/compiler-gcc.h>\n#else\n#error \"Unknown compiler\"\n#endif\n\n/*\n * Some architectures need to provide custom definitions of macros provided\n * by linux/compiler-*.h, and can do so using asm/compiler.h. We include that\n * conditionally rather than using an asm-generic wrapper in order to avoid\n * build failures if any C compilation, which will include this file via an\n * -include argument in c_flags, occurs prior to the asm-generic wrappers being\n * generated.\n */\n#ifdef CONFIG_HAVE_ARCH_COMPILER_H\n#include <asm/compiler.h>\n#endif\n\nstruct ftrace_branch_data {\n\tconst char *func;\n\tconst char *file;\n\tunsigned line;\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long correct;\n\t\t\tunsigned long incorrect;\n\t\t};\n\t\tstruct {\n\t\t\tunsigned long miss;\n\t\t\tunsigned long hit;\n\t\t};\n\t\tunsigned long miss_hit[2];\n\t};\n};\n\nstruct ftrace_likely_data {\n\tstruct ftrace_branch_data\tdata;\n\tunsigned long\t\t\tconstant;\n};\n\n#if defined(CC_USING_HOTPATCH)\n#define notrace\t\t\t__attribute__((hotpatch(0, 0)))\n#elif defined(CC_USING_PATCHABLE_FUNCTION_ENTRY)\n#define notrace\t\t\t__attribute__((patchable_function_entry(0, 0)))\n#else\n#define notrace\t\t\t__attribute__((__no_instrument_function__))\n#endif\n\n/*\n * it doesn't make sense on ARM (currently the only user of __naked)\n * to trace naked functions because then mcount is called without\n * stack and frame pointer being set up and there is no chance to\n * restore the lr register to the value before mcount was called.\n */\n#define __naked\t\t\t__attribute__((__naked__)) notrace\n\n#define __compiler_offsetof(a, b)\t__builtin_offsetof(a, b)\n\n/*\n * Prefer gnu_inline, so that extern inline functions do not emit an\n * externally visible function. This makes extern inline behave as per gnu89\n * semantics rather than c99. This prevents multiple symbol definition errors\n * of extern inline functions at link time.\n * A lot of inline functions can cause havoc with function tracing.\n */\n#define inline inline __gnu_inline __inline_maybe_unused notrace\n\n/*\n * gcc provides both __inline__ and __inline as alternate spellings of\n * the inline keyword, though the latter is undocumented. New kernel\n * code should only use the inline spelling, but some existing code\n * uses __inline__. Since we #define inline above, to ensure\n * __inline__ has the same semantics, we need this #define.\n *\n * However, the spelling __inline is strictly reserved for referring\n * to the bare keyword.\n */\n#define __inline__ inline\n\n/*\n * GCC does not warn about unused static inline functions for -Wunused-function.\n * Suppress the warning in clang as well by using __maybe_unused, but enable it\n * for W=1 build. This will allow clang to find unused functions. Remove the\n * __inline_maybe_unused entirely after fixing most of -Wunused-function warnings.\n */\n#ifdef KBUILD_EXTRA_WARN1\n#define __inline_maybe_unused\n#else\n#define __inline_maybe_unused __maybe_unused\n#endif\n\n/*\n * Rather then using noinline to prevent stack consumption, use\n * noinline_for_stack instead.  For documentation reasons.\n */\n#define noinline_for_stack noinline\n\n/*\n * Sanitizer helper attributes: Because using __always_inline and\n * __no_sanitize_* conflict, provide helper attributes that will either expand\n * to __no_sanitize_* in compilation units where instrumentation is enabled\n * (__SANITIZE_*__), or __always_inline in compilation units without\n * instrumentation (__SANITIZE_*__ undefined).\n */\n#ifdef __SANITIZE_ADDRESS__\n/*\n * We can't declare function 'inline' because __no_sanitize_address conflicts\n * with inlining. Attempt to inline it may cause a build failure.\n *     https://gcc.gnu.org/bugzilla/show_bug.cgi?id=67368\n * '__maybe_unused' allows us to avoid defined-but-not-used warnings.\n */\n# define __no_kasan_or_inline __no_sanitize_address notrace __maybe_unused\n# define __no_sanitize_or_inline __no_kasan_or_inline\n#else\n# define __no_kasan_or_inline __always_inline\n#endif\n\n#define __no_kcsan __no_sanitize_thread\n#ifdef __SANITIZE_THREAD__\n# define __no_sanitize_or_inline __no_kcsan notrace __maybe_unused\n#endif\n\n#ifndef __no_sanitize_or_inline\n#define __no_sanitize_or_inline __always_inline\n#endif\n\n/* Section for code which can't be instrumented at all */\n#define noinstr\t\t\t\t\t\t\t\t\\\n\tnoinline notrace __attribute((__section__(\".noinstr.text\")))\t\\\n\t__no_kcsan __no_sanitize_address\n\n#endif /* __KERNEL__ */\n\n#endif /* __ASSEMBLY__ */\n\n/*\n * The below symbols may be defined for one or more, but not ALL, of the above\n * compilers. We don't consider that to be an error, so set them to nothing.\n * For example, some of them are for compiler specific plugins.\n */\n#ifndef __latent_entropy\n# define __latent_entropy\n#endif\n\n#ifndef __randomize_layout\n# define __randomize_layout __designated_init\n#endif\n\n#ifndef __no_randomize_layout\n# define __no_randomize_layout\n#endif\n\n#ifndef randomized_struct_fields_start\n# define randomized_struct_fields_start\n# define randomized_struct_fields_end\n#endif\n\n#ifndef __noscs\n# define __noscs\n#endif\n\n#ifndef asm_volatile_goto\n#define asm_volatile_goto(x...) asm goto(x)\n#endif\n\n#ifdef CONFIG_CC_HAS_ASM_INLINE\n#define asm_inline asm __inline\n#else\n#define asm_inline asm\n#endif\n\n/* Are two types/vars the same type (ignoring qualifiers)? */\n#define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b))\n\n/*\n * __unqual_scalar_typeof(x) - Declare an unqualified scalar type, leaving\n *\t\t\t       non-scalar types unchanged.\n */\n/*\n * Prefer C11 _Generic for better compile-times and simpler code. Note: 'char'\n * is not type-compatible with 'signed char', and we define a separate case.\n */\n#define __scalar_type_to_expr_cases(type)\t\t\t\t\\\n\t\tunsigned type:\t(unsigned type)0,\t\t\t\\\n\t\tsigned type:\t(signed type)0\n\n#define __unqual_scalar_typeof(x) typeof(\t\t\t\t\\\n\t\t_Generic((x),\t\t\t\t\t\t\\\n\t\t\t char:\t(char)0,\t\t\t\t\\\n\t\t\t __scalar_type_to_expr_cases(char),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(short),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(int),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(long),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(long long),\t\\\n\t\t\t default: (x)))\n\n/* Is this type a native word size -- useful for atomic operations */\n#define __native_word(t) \\\n\t(sizeof(t) == sizeof(char) || sizeof(t) == sizeof(short) || \\\n\t sizeof(t) == sizeof(int) || sizeof(t) == sizeof(long))\n\n/* Compile time object size, -1 for unknown */\n#ifndef __compiletime_object_size\n# define __compiletime_object_size(obj) -1\n#endif\n#ifndef __compiletime_warning\n# define __compiletime_warning(message)\n#endif\n#ifndef __compiletime_error\n# define __compiletime_error(message)\n#endif\n\n#ifdef __OPTIMIZE__\n# define __compiletime_assert(condition, msg, prefix, suffix)\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\textern void prefix ## suffix(void) __compiletime_error(msg); \\\n\t\tif (!(condition))\t\t\t\t\t\\\n\t\t\tprefix ## suffix();\t\t\t\t\\\n\t} while (0)\n#else\n# define __compiletime_assert(condition, msg, prefix, suffix) do { } while (0)\n#endif\n\n#define _compiletime_assert(condition, msg, prefix, suffix) \\\n\t__compiletime_assert(condition, msg, prefix, suffix)\n\n/**\n * compiletime_assert - break build and emit msg if condition is false\n * @condition: a compile-time constant condition to check\n * @msg:       a message to emit if condition is false\n *\n * In tradition of POSIX assert, this macro will break the build if the\n * supplied condition is *false*, emitting the supplied error message if the\n * compiler has support to do so.\n */\n#define compiletime_assert(condition, msg) \\\n\t_compiletime_assert(condition, msg, __compiletime_assert_, __COUNTER__)\n\n#define compiletime_assert_atomic_type(t)\t\t\t\t\\\n\tcompiletime_assert(__native_word(t),\t\t\t\t\\\n\t\t\"Need native word sized stores/loads for atomicity.\")\n\n/* Helpers for emitting diagnostics in pragmas. */\n#ifndef __diag\n#define __diag(string)\n#endif\n\n#ifndef __diag_GCC\n#define __diag_GCC(version, severity, string)\n#endif\n\n#define __diag_push()\t__diag(push)\n#define __diag_pop()\t__diag(pop)\n\n#define __diag_ignore(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, ignore, option)\n#define __diag_warn(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, warn, option)\n#define __diag_error(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, error, option)\n\n#endif /* __LINUX_COMPILER_TYPES_H */\n"}, "12": {"id": 12, "path": "/src/net/unix/garbage.c", "content": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * NET3:\tGarbage Collector For AF_UNIX sockets\n *\n * Garbage Collector:\n *\tCopyright (C) Barak A. Pearlmutter.\n *\n * Chopped about by Alan Cox 22/3/96 to make it fit the AF_UNIX socket problem.\n * If it doesn't work blame me, it worked when Barak sent it.\n *\n * Assumptions:\n *\n *  - object w/ a bit\n *  - free list\n *\n * Current optimizations:\n *\n *  - explicit stack instead of recursion\n *  - tail recurse on first born instead of immediate push/pop\n *  - we gather the stuff that should not be killed into tree\n *    and stack is just a path from root to the current pointer.\n *\n *  Future optimizations:\n *\n *  - don't just push entire root set; process in place\n *\n *  Fixes:\n *\tAlan Cox\t07 Sept\t1997\tVmalloc internal stack as needed.\n *\t\t\t\t\tCope with changing max_files.\n *\tAl Viro\t\t11 Oct 1998\n *\t\tGraph may have cycles. That is, we can send the descriptor\n *\t\tof foo to bar and vice versa. Current code chokes on that.\n *\t\tFix: move SCM_RIGHTS ones into the separate list and then\n *\t\tskb_free() them all instead of doing explicit fput's.\n *\t\tAnother problem: since fput() may block somebody may\n *\t\tcreate a new unix_socket when we are in the middle of sweep\n *\t\tphase. Fix: revert the logic wrt MARKED. Mark everything\n *\t\tupon the beginning and unmark non-junk ones.\n *\n *\t\t[12 Oct 1998] AAARGH! New code purges all SCM_RIGHTS\n *\t\tsent to connect()'ed but still not accept()'ed sockets.\n *\t\tFixed. Old code had slightly different problem here:\n *\t\textra fput() in situation when we passed the descriptor via\n *\t\tsuch socket and closed it (descriptor). That would happen on\n *\t\teach unix_gc() until the accept(). Since the struct file in\n *\t\tquestion would go to the free list and might be reused...\n *\t\tThat might be the reason of random oopses on filp_close()\n *\t\tin unrelated processes.\n *\n *\tAV\t\t28 Feb 1999\n *\t\tKill the explicit allocation of stack. Now we keep the tree\n *\t\twith root in dummy + pointer (gc_current) to one of the nodes.\n *\t\tStack is represented as path from gc_current to dummy. Unmark\n *\t\tnow means \"add to tree\". Push == \"make it a son of gc_current\".\n *\t\tPop == \"move gc_current to parent\". We keep only pointers to\n *\t\tparents (->gc_tree).\n *\tAV\t\t1 Mar 1999\n *\t\tDamn. Added missing check for ->dead in listen queues scanning.\n *\n *\tMiklos Szeredi 25 Jun 2007\n *\t\tReimplement with a cycle collecting algorithm. This should\n *\t\tsolve several problems with the previous code, like being racy\n *\t\twrt receive and holding up unrelated socket operations.\n */\n\n#include <linux/kernel.h>\n#include <linux/string.h>\n#include <linux/socket.h>\n#include <linux/un.h>\n#include <linux/net.h>\n#include <linux/fs.h>\n#include <linux/skbuff.h>\n#include <linux/netdevice.h>\n#include <linux/file.h>\n#include <linux/proc_fs.h>\n#include <linux/mutex.h>\n#include <linux/wait.h>\n\n#include <net/sock.h>\n#include <net/af_unix.h>\n#include <net/scm.h>\n#include <net/tcp_states.h>\n\n#include \"scm.h\"\n\n/* Internal data structures and random procedures: */\n\nstatic LIST_HEAD(gc_candidates);\nstatic DECLARE_WAIT_QUEUE_HEAD(unix_gc_wait);\n\nstatic void scan_inflight(struct sock *x, void (*func)(struct unix_sock *),\n\t\t\t  struct sk_buff_head *hitlist)\n{\n\tstruct sk_buff *skb;\n\tstruct sk_buff *next;\n\n\tspin_lock(&x->sk_receive_queue.lock);\n\tskb_queue_walk_safe(&x->sk_receive_queue, skb, next) {\n\t\t/* Do we have file descriptors ? */\n\t\tif (UNIXCB(skb).fp) {\n\t\t\tbool hit = false;\n\t\t\t/* Process the descriptors of this socket */\n\t\t\tint nfd = UNIXCB(skb).fp->count;\n\t\t\tstruct file **fp = UNIXCB(skb).fp->fp;\n\n\t\t\twhile (nfd--) {\n\t\t\t\t/* Get the socket the fd matches if it indeed does so */\n\t\t\t\tstruct sock *sk = unix_get_socket(*fp++);\n\n\t\t\t\tif (sk) {\n\t\t\t\t\tstruct unix_sock *u = unix_sk(sk);\n\n\t\t\t\t\t/* Ignore non-candidates, they could\n\t\t\t\t\t * have been added to the queues after\n\t\t\t\t\t * starting the garbage collection\n\t\t\t\t\t */\n\t\t\t\t\tif (test_bit(UNIX_GC_CANDIDATE, &u->gc_flags)) {\n\t\t\t\t\t\thit = true;\n\n\t\t\t\t\t\tfunc(u);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (hit && hitlist != NULL) {\n\t\t\t\t__skb_unlink(skb, &x->sk_receive_queue);\n\t\t\t\t__skb_queue_tail(hitlist, skb);\n\t\t\t}\n\t\t}\n\t}\n\tspin_unlock(&x->sk_receive_queue.lock);\n}\n\nstatic void scan_children(struct sock *x, void (*func)(struct unix_sock *),\n\t\t\t  struct sk_buff_head *hitlist)\n{\n\tif (x->sk_state != TCP_LISTEN) {\n\t\tscan_inflight(x, func, hitlist);\n\t} else {\n\t\tstruct sk_buff *skb;\n\t\tstruct sk_buff *next;\n\t\tstruct unix_sock *u;\n\t\tLIST_HEAD(embryos);\n\n\t\t/* For a listening socket collect the queued embryos\n\t\t * and perform a scan on them as well.\n\t\t */\n\t\tspin_lock(&x->sk_receive_queue.lock);\n\t\tskb_queue_walk_safe(&x->sk_receive_queue, skb, next) {\n\t\t\tu = unix_sk(skb->sk);\n\n\t\t\t/* An embryo cannot be in-flight, so it's safe\n\t\t\t * to use the list link.\n\t\t\t */\n\t\t\tBUG_ON(!list_empty(&u->link));\n\t\t\tlist_add_tail(&u->link, &embryos);\n\t\t}\n\t\tspin_unlock(&x->sk_receive_queue.lock);\n\n\t\twhile (!list_empty(&embryos)) {\n\t\t\tu = list_entry(embryos.next, struct unix_sock, link);\n\t\t\tscan_inflight(&u->sk, func, hitlist);\n\t\t\tlist_del_init(&u->link);\n\t\t}\n\t}\n}\n\nstatic void dec_inflight(struct unix_sock *usk)\n{\n\tatomic_long_dec(&usk->inflight);\n}\n\nstatic void inc_inflight(struct unix_sock *usk)\n{\n\tatomic_long_inc(&usk->inflight);\n}\n\nstatic void inc_inflight_move_tail(struct unix_sock *u)\n{\n\tatomic_long_inc(&u->inflight);\n\t/* If this still might be part of a cycle, move it to the end\n\t * of the list, so that it's checked even if it was already\n\t * passed over\n\t */\n\tif (test_bit(UNIX_GC_MAYBE_CYCLE, &u->gc_flags))\n\t\tlist_move_tail(&u->link, &gc_candidates);\n}\n\nstatic bool gc_in_progress;\n#define UNIX_INFLIGHT_TRIGGER_GC 16000\n\nvoid wait_for_unix_gc(void)\n{\n\t/* If number of inflight sockets is insane,\n\t * force a garbage collect right now.\n\t */\n\tif (unix_tot_inflight > UNIX_INFLIGHT_TRIGGER_GC && !gc_in_progress)\n\t\tunix_gc();\n\twait_event(unix_gc_wait, gc_in_progress == false);\n}\n\n/* The external entry point: unix_gc() */\nvoid unix_gc(void)\n{\n\tstruct unix_sock *u;\n\tstruct unix_sock *next;\n\tstruct sk_buff_head hitlist;\n\tstruct list_head cursor;\n\tLIST_HEAD(not_cycle_list);\n\n\tspin_lock(&unix_gc_lock);\n\n\t/* Avoid a recursive GC. */\n\tif (gc_in_progress)\n\t\tgoto out;\n\n\tgc_in_progress = true;\n\t/* First, select candidates for garbage collection.  Only\n\t * in-flight sockets are considered, and from those only ones\n\t * which don't have any external reference.\n\t *\n\t * Holding unix_gc_lock will protect these candidates from\n\t * being detached, and hence from gaining an external\n\t * reference.  Since there are no possible receivers, all\n\t * buffers currently on the candidates' queues stay there\n\t * during the garbage collection.\n\t *\n\t * We also know that no new candidate can be added onto the\n\t * receive queues.  Other, non candidate sockets _can_ be\n\t * added to queue, so we must make sure only to touch\n\t * candidates.\n\t */\n\tlist_for_each_entry_safe(u, next, &gc_inflight_list, link) {\n\t\tlong total_refs;\n\t\tlong inflight_refs;\n\n\t\ttotal_refs = file_count(u->sk.sk_socket->file);\n\t\tinflight_refs = atomic_long_read(&u->inflight);\n\n\t\tBUG_ON(inflight_refs < 1);\n\t\tBUG_ON(total_refs < inflight_refs);\n\t\tif (total_refs == inflight_refs) {\n\t\t\tlist_move_tail(&u->link, &gc_candidates);\n\t\t\t__set_bit(UNIX_GC_CANDIDATE, &u->gc_flags);\n\t\t\t__set_bit(UNIX_GC_MAYBE_CYCLE, &u->gc_flags);\n\t\t}\n\t}\n\n\t/* Now remove all internal in-flight reference to children of\n\t * the candidates.\n\t */\n\tlist_for_each_entry(u, &gc_candidates, link)\n\t\tscan_children(&u->sk, dec_inflight, NULL);\n\n\t/* Restore the references for children of all candidates,\n\t * which have remaining references.  Do this recursively, so\n\t * only those remain, which form cyclic references.\n\t *\n\t * Use a \"cursor\" link, to make the list traversal safe, even\n\t * though elements might be moved about.\n\t */\n\tlist_add(&cursor, &gc_candidates);\n\twhile (cursor.next != &gc_candidates) {\n\t\tu = list_entry(cursor.next, struct unix_sock, link);\n\n\t\t/* Move cursor to after the current position. */\n\t\tlist_move(&cursor, &u->link);\n\n\t\tif (atomic_long_read(&u->inflight) > 0) {\n\t\t\tlist_move_tail(&u->link, &not_cycle_list);\n\t\t\t__clear_bit(UNIX_GC_MAYBE_CYCLE, &u->gc_flags);\n\t\t\tscan_children(&u->sk, inc_inflight_move_tail, NULL);\n\t\t}\n\t}\n\tlist_del(&cursor);\n\n\t/* Now gc_candidates contains only garbage.  Restore original\n\t * inflight counters for these as well, and remove the skbuffs\n\t * which are creating the cycle(s).\n\t */\n\tskb_queue_head_init(&hitlist);\n\tlist_for_each_entry(u, &gc_candidates, link)\n\t\tscan_children(&u->sk, inc_inflight, &hitlist);\n\n\t/* not_cycle_list contains those sockets which do not make up a\n\t * cycle.  Restore these to the inflight list.\n\t */\n\twhile (!list_empty(&not_cycle_list)) {\n\t\tu = list_entry(not_cycle_list.next, struct unix_sock, link);\n\t\t__clear_bit(UNIX_GC_CANDIDATE, &u->gc_flags);\n\t\tlist_move_tail(&u->link, &gc_inflight_list);\n\t}\n\n\tspin_unlock(&unix_gc_lock);\n\n\t/* Here we are. Hitlist is filled. Die. */\n\t__skb_queue_purge(&hitlist);\n\n\tspin_lock(&unix_gc_lock);\n\n\t/* All candidates should have been detached by now. */\n\tBUG_ON(!list_empty(&gc_candidates));\n\tgc_in_progress = false;\n\twake_up(&unix_gc_wait);\n\n out:\n\tspin_unlock(&unix_gc_lock);\n}\n"}, "13": {"id": 13, "path": "/src/include/linux/list.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_LIST_H\n#define _LINUX_LIST_H\n\n#include <linux/types.h>\n#include <linux/stddef.h>\n#include <linux/poison.h>\n#include <linux/const.h>\n#include <linux/kernel.h>\n\n/*\n * Circular doubly linked list implementation.\n *\n * Some of the internal functions (\"__xxx\") are useful when\n * manipulating whole lists rather than single entries, as\n * sometimes we already know the next/prev entries and we can\n * generate better code by using them directly rather than\n * using the generic single-entry routines.\n */\n\n#define LIST_HEAD_INIT(name) { &(name), &(name) }\n\n#define LIST_HEAD(name) \\\n\tstruct list_head name = LIST_HEAD_INIT(name)\n\n/**\n * INIT_LIST_HEAD - Initialize a list_head structure\n * @list: list_head structure to be initialized.\n *\n * Initializes the list_head to point to itself.  If it is a list header,\n * the result is an empty list.\n */\nstatic inline void INIT_LIST_HEAD(struct list_head *list)\n{\n\tWRITE_ONCE(list->next, list);\n\tlist->prev = list;\n}\n\n#ifdef CONFIG_DEBUG_LIST\nextern bool __list_add_valid(struct list_head *new,\n\t\t\t      struct list_head *prev,\n\t\t\t      struct list_head *next);\nextern bool __list_del_entry_valid(struct list_head *entry);\n#else\nstatic inline bool __list_add_valid(struct list_head *new,\n\t\t\t\tstruct list_head *prev,\n\t\t\t\tstruct list_head *next)\n{\n\treturn true;\n}\nstatic inline bool __list_del_entry_valid(struct list_head *entry)\n{\n\treturn true;\n}\n#endif\n\n/*\n * Insert a new entry between two known consecutive entries.\n *\n * This is only for internal list manipulation where we know\n * the prev/next entries already!\n */\nstatic inline void __list_add(struct list_head *new,\n\t\t\t      struct list_head *prev,\n\t\t\t      struct list_head *next)\n{\n\tif (!__list_add_valid(new, prev, next))\n\t\treturn;\n\n\tnext->prev = new;\n\tnew->next = next;\n\tnew->prev = prev;\n\tWRITE_ONCE(prev->next, new);\n}\n\n/**\n * list_add - add a new entry\n * @new: new entry to be added\n * @head: list head to add it after\n *\n * Insert a new entry after the specified head.\n * This is good for implementing stacks.\n */\nstatic inline void list_add(struct list_head *new, struct list_head *head)\n{\n\t__list_add(new, head, head->next);\n}\n\n\n/**\n * list_add_tail - add a new entry\n * @new: new entry to be added\n * @head: list head to add it before\n *\n * Insert a new entry before the specified head.\n * This is useful for implementing queues.\n */\nstatic inline void list_add_tail(struct list_head *new, struct list_head *head)\n{\n\t__list_add(new, head->prev, head);\n}\n\n/*\n * Delete a list entry by making the prev/next entries\n * point to each other.\n *\n * This is only for internal list manipulation where we know\n * the prev/next entries already!\n */\nstatic inline void __list_del(struct list_head * prev, struct list_head * next)\n{\n\tnext->prev = prev;\n\tWRITE_ONCE(prev->next, next);\n}\n\n/*\n * Delete a list entry and clear the 'prev' pointer.\n *\n * This is a special-purpose list clearing method used in the networking code\n * for lists allocated as per-cpu, where we don't want to incur the extra\n * WRITE_ONCE() overhead of a regular list_del_init(). The code that uses this\n * needs to check the node 'prev' pointer instead of calling list_empty().\n */\nstatic inline void __list_del_clearprev(struct list_head *entry)\n{\n\t__list_del(entry->prev, entry->next);\n\tentry->prev = NULL;\n}\n\nstatic inline void __list_del_entry(struct list_head *entry)\n{\n\tif (!__list_del_entry_valid(entry))\n\t\treturn;\n\n\t__list_del(entry->prev, entry->next);\n}\n\n/**\n * list_del - deletes entry from list.\n * @entry: the element to delete from the list.\n * Note: list_empty() on entry does not return true after this, the entry is\n * in an undefined state.\n */\nstatic inline void list_del(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tentry->next = LIST_POISON1;\n\tentry->prev = LIST_POISON2;\n}\n\n/**\n * list_replace - replace old entry by new one\n * @old : the element to be replaced\n * @new : the new element to insert\n *\n * If @old was empty, it will be overwritten.\n */\nstatic inline void list_replace(struct list_head *old,\n\t\t\t\tstruct list_head *new)\n{\n\tnew->next = old->next;\n\tnew->next->prev = new;\n\tnew->prev = old->prev;\n\tnew->prev->next = new;\n}\n\n/**\n * list_replace_init - replace old entry by new one and initialize the old one\n * @old : the element to be replaced\n * @new : the new element to insert\n *\n * If @old was empty, it will be overwritten.\n */\nstatic inline void list_replace_init(struct list_head *old,\n\t\t\t\t     struct list_head *new)\n{\n\tlist_replace(old, new);\n\tINIT_LIST_HEAD(old);\n}\n\n/**\n * list_swap - replace entry1 with entry2 and re-add entry1 at entry2's position\n * @entry1: the location to place entry2\n * @entry2: the location to place entry1\n */\nstatic inline void list_swap(struct list_head *entry1,\n\t\t\t     struct list_head *entry2)\n{\n\tstruct list_head *pos = entry2->prev;\n\n\tlist_del(entry2);\n\tlist_replace(entry1, entry2);\n\tif (pos == entry1)\n\t\tpos = entry2;\n\tlist_add(entry1, pos);\n}\n\n/**\n * list_del_init - deletes entry from list and reinitialize it.\n * @entry: the element to delete from the list.\n */\nstatic inline void list_del_init(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tINIT_LIST_HEAD(entry);\n}\n\n/**\n * list_move - delete from one list and add as another's head\n * @list: the entry to move\n * @head: the head that will precede our entry\n */\nstatic inline void list_move(struct list_head *list, struct list_head *head)\n{\n\t__list_del_entry(list);\n\tlist_add(list, head);\n}\n\n/**\n * list_move_tail - delete from one list and add as another's tail\n * @list: the entry to move\n * @head: the head that will follow our entry\n */\nstatic inline void list_move_tail(struct list_head *list,\n\t\t\t\t  struct list_head *head)\n{\n\t__list_del_entry(list);\n\tlist_add_tail(list, head);\n}\n\n/**\n * list_bulk_move_tail - move a subsection of a list to its tail\n * @head: the head that will follow our entry\n * @first: first entry to move\n * @last: last entry to move, can be the same as first\n *\n * Move all entries between @first and including @last before @head.\n * All three entries must belong to the same linked list.\n */\nstatic inline void list_bulk_move_tail(struct list_head *head,\n\t\t\t\t       struct list_head *first,\n\t\t\t\t       struct list_head *last)\n{\n\tfirst->prev->next = last->next;\n\tlast->next->prev = first->prev;\n\n\thead->prev->next = first;\n\tfirst->prev = head->prev;\n\n\tlast->next = head;\n\thead->prev = last;\n}\n\n/**\n * list_is_first -- tests whether @list is the first entry in list @head\n * @list: the entry to test\n * @head: the head of the list\n */\nstatic inline int list_is_first(const struct list_head *list,\n\t\t\t\t\tconst struct list_head *head)\n{\n\treturn list->prev == head;\n}\n\n/**\n * list_is_last - tests whether @list is the last entry in list @head\n * @list: the entry to test\n * @head: the head of the list\n */\nstatic inline int list_is_last(const struct list_head *list,\n\t\t\t\tconst struct list_head *head)\n{\n\treturn list->next == head;\n}\n\n/**\n * list_empty - tests whether a list is empty\n * @head: the list to test.\n */\nstatic inline int list_empty(const struct list_head *head)\n{\n\treturn READ_ONCE(head->next) == head;\n}\n\n/**\n * list_del_init_careful - deletes entry from list and reinitialize it.\n * @entry: the element to delete from the list.\n *\n * This is the same as list_del_init(), except designed to be used\n * together with list_empty_careful() in a way to guarantee ordering\n * of other memory operations.\n *\n * Any memory operations done before a list_del_init_careful() are\n * guaranteed to be visible after a list_empty_careful() test.\n */\nstatic inline void list_del_init_careful(struct list_head *entry)\n{\n\t__list_del_entry(entry);\n\tentry->prev = entry;\n\tsmp_store_release(&entry->next, entry);\n}\n\n/**\n * list_empty_careful - tests whether a list is empty and not being modified\n * @head: the list to test\n *\n * Description:\n * tests whether a list is empty _and_ checks that no other CPU might be\n * in the process of modifying either member (next or prev)\n *\n * NOTE: using list_empty_careful() without synchronization\n * can only be safe if the only activity that can happen\n * to the list entry is list_del_init(). Eg. it cannot be used\n * if another CPU could re-list_add() it.\n */\nstatic inline int list_empty_careful(const struct list_head *head)\n{\n\tstruct list_head *next = smp_load_acquire(&head->next);\n\treturn (next == head) && (next == head->prev);\n}\n\n/**\n * list_rotate_left - rotate the list to the left\n * @head: the head of the list\n */\nstatic inline void list_rotate_left(struct list_head *head)\n{\n\tstruct list_head *first;\n\n\tif (!list_empty(head)) {\n\t\tfirst = head->next;\n\t\tlist_move_tail(first, head);\n\t}\n}\n\n/**\n * list_rotate_to_front() - Rotate list to specific item.\n * @list: The desired new front of the list.\n * @head: The head of the list.\n *\n * Rotates list so that @list becomes the new front of the list.\n */\nstatic inline void list_rotate_to_front(struct list_head *list,\n\t\t\t\t\tstruct list_head *head)\n{\n\t/*\n\t * Deletes the list head from the list denoted by @head and\n\t * places it as the tail of @list, this effectively rotates the\n\t * list so that @list is at the front.\n\t */\n\tlist_move_tail(head, list);\n}\n\n/**\n * list_is_singular - tests whether a list has just one entry.\n * @head: the list to test.\n */\nstatic inline int list_is_singular(const struct list_head *head)\n{\n\treturn !list_empty(head) && (head->next == head->prev);\n}\n\nstatic inline void __list_cut_position(struct list_head *list,\n\t\tstruct list_head *head, struct list_head *entry)\n{\n\tstruct list_head *new_first = entry->next;\n\tlist->next = head->next;\n\tlist->next->prev = list;\n\tlist->prev = entry;\n\tentry->next = list;\n\thead->next = new_first;\n\tnew_first->prev = head;\n}\n\n/**\n * list_cut_position - cut a list into two\n * @list: a new list to add all removed entries\n * @head: a list with entries\n * @entry: an entry within head, could be the head itself\n *\tand if so we won't cut the list\n *\n * This helper moves the initial part of @head, up to and\n * including @entry, from @head to @list. You should\n * pass on @entry an element you know is on @head. @list\n * should be an empty list or a list you do not care about\n * losing its data.\n *\n */\nstatic inline void list_cut_position(struct list_head *list,\n\t\tstruct list_head *head, struct list_head *entry)\n{\n\tif (list_empty(head))\n\t\treturn;\n\tif (list_is_singular(head) &&\n\t\t(head->next != entry && head != entry))\n\t\treturn;\n\tif (entry == head)\n\t\tINIT_LIST_HEAD(list);\n\telse\n\t\t__list_cut_position(list, head, entry);\n}\n\n/**\n * list_cut_before - cut a list into two, before given entry\n * @list: a new list to add all removed entries\n * @head: a list with entries\n * @entry: an entry within head, could be the head itself\n *\n * This helper moves the initial part of @head, up to but\n * excluding @entry, from @head to @list.  You should pass\n * in @entry an element you know is on @head.  @list should\n * be an empty list or a list you do not care about losing\n * its data.\n * If @entry == @head, all entries on @head are moved to\n * @list.\n */\nstatic inline void list_cut_before(struct list_head *list,\n\t\t\t\t   struct list_head *head,\n\t\t\t\t   struct list_head *entry)\n{\n\tif (head->next == entry) {\n\t\tINIT_LIST_HEAD(list);\n\t\treturn;\n\t}\n\tlist->next = head->next;\n\tlist->next->prev = list;\n\tlist->prev = entry->prev;\n\tlist->prev->next = list;\n\thead->next = entry;\n\tentry->prev = head;\n}\n\nstatic inline void __list_splice(const struct list_head *list,\n\t\t\t\t struct list_head *prev,\n\t\t\t\t struct list_head *next)\n{\n\tstruct list_head *first = list->next;\n\tstruct list_head *last = list->prev;\n\n\tfirst->prev = prev;\n\tprev->next = first;\n\n\tlast->next = next;\n\tnext->prev = last;\n}\n\n/**\n * list_splice - join two lists, this is designed for stacks\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n */\nstatic inline void list_splice(const struct list_head *list,\n\t\t\t\tstruct list_head *head)\n{\n\tif (!list_empty(list))\n\t\t__list_splice(list, head, head->next);\n}\n\n/**\n * list_splice_tail - join two lists, each list being a queue\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n */\nstatic inline void list_splice_tail(struct list_head *list,\n\t\t\t\tstruct list_head *head)\n{\n\tif (!list_empty(list))\n\t\t__list_splice(list, head->prev, head);\n}\n\n/**\n * list_splice_init - join two lists and reinitialise the emptied list.\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n *\n * The list at @list is reinitialised\n */\nstatic inline void list_splice_init(struct list_head *list,\n\t\t\t\t    struct list_head *head)\n{\n\tif (!list_empty(list)) {\n\t\t__list_splice(list, head, head->next);\n\t\tINIT_LIST_HEAD(list);\n\t}\n}\n\n/**\n * list_splice_tail_init - join two lists and reinitialise the emptied list\n * @list: the new list to add.\n * @head: the place to add it in the first list.\n *\n * Each of the lists is a queue.\n * The list at @list is reinitialised\n */\nstatic inline void list_splice_tail_init(struct list_head *list,\n\t\t\t\t\t struct list_head *head)\n{\n\tif (!list_empty(list)) {\n\t\t__list_splice(list, head->prev, head);\n\t\tINIT_LIST_HEAD(list);\n\t}\n}\n\n/**\n * list_entry - get the struct for this entry\n * @ptr:\tthe &struct list_head pointer.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_entry(ptr, type, member) \\\n\tcontainer_of(ptr, type, member)\n\n/**\n * list_first_entry - get the first element from a list\n * @ptr:\tthe list head to take the element from.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n *\n * Note, that list is expected to be not empty.\n */\n#define list_first_entry(ptr, type, member) \\\n\tlist_entry((ptr)->next, type, member)\n\n/**\n * list_last_entry - get the last element from a list\n * @ptr:\tthe list head to take the element from.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n *\n * Note, that list is expected to be not empty.\n */\n#define list_last_entry(ptr, type, member) \\\n\tlist_entry((ptr)->prev, type, member)\n\n/**\n * list_first_entry_or_null - get the first element from a list\n * @ptr:\tthe list head to take the element from.\n * @type:\tthe type of the struct this is embedded in.\n * @member:\tthe name of the list_head within the struct.\n *\n * Note that if the list is empty, it returns NULL.\n */\n#define list_first_entry_or_null(ptr, type, member) ({ \\\n\tstruct list_head *head__ = (ptr); \\\n\tstruct list_head *pos__ = READ_ONCE(head__->next); \\\n\tpos__ != head__ ? list_entry(pos__, type, member) : NULL; \\\n})\n\n/**\n * list_next_entry - get the next element in list\n * @pos:\tthe type * to cursor\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_next_entry(pos, member) \\\n\tlist_entry((pos)->member.next, typeof(*(pos)), member)\n\n/**\n * list_prev_entry - get the prev element in list\n * @pos:\tthe type * to cursor\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_prev_entry(pos, member) \\\n\tlist_entry((pos)->member.prev, typeof(*(pos)), member)\n\n/**\n * list_for_each\t-\titerate over a list\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @head:\tthe head for your list.\n */\n#define list_for_each(pos, head) \\\n\tfor (pos = (head)->next; pos != (head); pos = pos->next)\n\n/**\n * list_for_each_continue - continue iteration over a list\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @head:\tthe head for your list.\n *\n * Continue to iterate over a list, continuing after the current position.\n */\n#define list_for_each_continue(pos, head) \\\n\tfor (pos = pos->next; pos != (head); pos = pos->next)\n\n/**\n * list_for_each_prev\t-\titerate over a list backwards\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @head:\tthe head for your list.\n */\n#define list_for_each_prev(pos, head) \\\n\tfor (pos = (head)->prev; pos != (head); pos = pos->prev)\n\n/**\n * list_for_each_safe - iterate over a list safe against removal of list entry\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @n:\t\tanother &struct list_head to use as temporary storage\n * @head:\tthe head for your list.\n */\n#define list_for_each_safe(pos, n, head) \\\n\tfor (pos = (head)->next, n = pos->next; pos != (head); \\\n\t\tpos = n, n = pos->next)\n\n/**\n * list_for_each_prev_safe - iterate over a list backwards safe against removal of list entry\n * @pos:\tthe &struct list_head to use as a loop cursor.\n * @n:\t\tanother &struct list_head to use as temporary storage\n * @head:\tthe head for your list.\n */\n#define list_for_each_prev_safe(pos, n, head) \\\n\tfor (pos = (head)->prev, n = pos->prev; \\\n\t     pos != (head); \\\n\t     pos = n, n = pos->prev)\n\n/**\n * list_entry_is_head - test if the entry points to the head of the list\n * @pos:\tthe type * to cursor\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_entry_is_head(pos, head, member)\t\t\t\t\\\n\t(&pos->member == (head))\n\n/**\n * list_for_each_entry\t-\titerate over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_for_each_entry(pos, head, member)\t\t\t\t\\\n\tfor (pos = list_first_entry(head, typeof(*pos), member);\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_next_entry(pos, member))\n\n/**\n * list_for_each_entry_reverse - iterate backwards over list of given type.\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_for_each_entry_reverse(pos, head, member)\t\t\t\\\n\tfor (pos = list_last_entry(head, typeof(*pos), member);\t\t\\\n\t     !list_entry_is_head(pos, head, member); \t\t\t\\\n\t     pos = list_prev_entry(pos, member))\n\n/**\n * list_prepare_entry - prepare a pos entry for use in list_for_each_entry_continue()\n * @pos:\tthe type * to use as a start point\n * @head:\tthe head of the list\n * @member:\tthe name of the list_head within the struct.\n *\n * Prepares a pos entry for use as a start point in list_for_each_entry_continue().\n */\n#define list_prepare_entry(pos, head, member) \\\n\t((pos) ? : list_entry(head, typeof(*pos), member))\n\n/**\n * list_for_each_entry_continue - continue iteration over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Continue to iterate over list of given type, continuing after\n * the current position.\n */\n#define list_for_each_entry_continue(pos, head, member) \t\t\\\n\tfor (pos = list_next_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_next_entry(pos, member))\n\n/**\n * list_for_each_entry_continue_reverse - iterate backwards from the given point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Start to iterate over list of given type backwards, continuing after\n * the current position.\n */\n#define list_for_each_entry_continue_reverse(pos, head, member)\t\t\\\n\tfor (pos = list_prev_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_prev_entry(pos, member))\n\n/**\n * list_for_each_entry_from - iterate over list of given type from the current point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate over list of given type, continuing from current position.\n */\n#define list_for_each_entry_from(pos, head, member) \t\t\t\\\n\tfor (; !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_next_entry(pos, member))\n\n/**\n * list_for_each_entry_from_reverse - iterate backwards over list of given type\n *                                    from the current point\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate backwards over list of given type, continuing from current position.\n */\n#define list_for_each_entry_from_reverse(pos, head, member)\t\t\\\n\tfor (; !list_entry_is_head(pos, head, member);\t\t\t\\\n\t     pos = list_prev_entry(pos, member))\n\n/**\n * list_for_each_entry_safe - iterate over list of given type safe against removal of list entry\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n */\n#define list_for_each_entry_safe(pos, n, head, member)\t\t\t\\\n\tfor (pos = list_first_entry(head, typeof(*pos), member),\t\\\n\t\tn = list_next_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member); \t\t\t\\\n\t     pos = n, n = list_next_entry(n, member))\n\n/**\n * list_for_each_entry_safe_continue - continue list iteration safe against removal\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate over list of given type, continuing after current point,\n * safe against removal of list entry.\n */\n#define list_for_each_entry_safe_continue(pos, n, head, member) \t\t\\\n\tfor (pos = list_next_entry(pos, member), \t\t\t\t\\\n\t\tn = list_next_entry(pos, member);\t\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\t\\\n\t     pos = n, n = list_next_entry(n, member))\n\n/**\n * list_for_each_entry_safe_from - iterate over list from current point safe against removal\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate over list of given type from current point, safe against\n * removal of list entry.\n */\n#define list_for_each_entry_safe_from(pos, n, head, member) \t\t\t\\\n\tfor (n = list_next_entry(pos, member);\t\t\t\t\t\\\n\t     !list_entry_is_head(pos, head, member);\t\t\t\t\\\n\t     pos = n, n = list_next_entry(n, member))\n\n/**\n * list_for_each_entry_safe_reverse - iterate backwards over list safe against removal\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\tanother type * to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the list_head within the struct.\n *\n * Iterate backwards over list of given type, safe against removal\n * of list entry.\n */\n#define list_for_each_entry_safe_reverse(pos, n, head, member)\t\t\\\n\tfor (pos = list_last_entry(head, typeof(*pos), member),\t\t\\\n\t\tn = list_prev_entry(pos, member);\t\t\t\\\n\t     !list_entry_is_head(pos, head, member); \t\t\t\\\n\t     pos = n, n = list_prev_entry(n, member))\n\n/**\n * list_safe_reset_next - reset a stale list_for_each_entry_safe loop\n * @pos:\tthe loop cursor used in the list_for_each_entry_safe loop\n * @n:\t\ttemporary storage used in list_for_each_entry_safe\n * @member:\tthe name of the list_head within the struct.\n *\n * list_safe_reset_next is not safe to use in general if the list may be\n * modified concurrently (eg. the lock is dropped in the loop body). An\n * exception to this is if the cursor element (pos) is pinned in the list,\n * and list_safe_reset_next is called after re-taking the lock and before\n * completing the current iteration of the loop body.\n */\n#define list_safe_reset_next(pos, n, member)\t\t\t\t\\\n\tn = list_next_entry(pos, member)\n\n/*\n * Double linked lists with a single pointer list head.\n * Mostly useful for hash tables where the two pointer list head is\n * too wasteful.\n * You lose the ability to access the tail in O(1).\n */\n\n#define HLIST_HEAD_INIT { .first = NULL }\n#define HLIST_HEAD(name) struct hlist_head name = {  .first = NULL }\n#define INIT_HLIST_HEAD(ptr) ((ptr)->first = NULL)\nstatic inline void INIT_HLIST_NODE(struct hlist_node *h)\n{\n\th->next = NULL;\n\th->pprev = NULL;\n}\n\n/**\n * hlist_unhashed - Has node been removed from list and reinitialized?\n * @h: Node to be checked\n *\n * Not that not all removal functions will leave a node in unhashed\n * state.  For example, hlist_nulls_del_init_rcu() does leave the\n * node in unhashed state, but hlist_nulls_del() does not.\n */\nstatic inline int hlist_unhashed(const struct hlist_node *h)\n{\n\treturn !h->pprev;\n}\n\n/**\n * hlist_unhashed_lockless - Version of hlist_unhashed for lockless use\n * @h: Node to be checked\n *\n * This variant of hlist_unhashed() must be used in lockless contexts\n * to avoid potential load-tearing.  The READ_ONCE() is paired with the\n * various WRITE_ONCE() in hlist helpers that are defined below.\n */\nstatic inline int hlist_unhashed_lockless(const struct hlist_node *h)\n{\n\treturn !READ_ONCE(h->pprev);\n}\n\n/**\n * hlist_empty - Is the specified hlist_head structure an empty hlist?\n * @h: Structure to check.\n */\nstatic inline int hlist_empty(const struct hlist_head *h)\n{\n\treturn !READ_ONCE(h->first);\n}\n\nstatic inline void __hlist_del(struct hlist_node *n)\n{\n\tstruct hlist_node *next = n->next;\n\tstruct hlist_node **pprev = n->pprev;\n\n\tWRITE_ONCE(*pprev, next);\n\tif (next)\n\t\tWRITE_ONCE(next->pprev, pprev);\n}\n\n/**\n * hlist_del - Delete the specified hlist_node from its list\n * @n: Node to delete.\n *\n * Note that this function leaves the node in hashed state.  Use\n * hlist_del_init() or similar instead to unhash @n.\n */\nstatic inline void hlist_del(struct hlist_node *n)\n{\n\t__hlist_del(n);\n\tn->next = LIST_POISON1;\n\tn->pprev = LIST_POISON2;\n}\n\n/**\n * hlist_del_init - Delete the specified hlist_node from its list and initialize\n * @n: Node to delete.\n *\n * Note that this function leaves the node in unhashed state.\n */\nstatic inline void hlist_del_init(struct hlist_node *n)\n{\n\tif (!hlist_unhashed(n)) {\n\t\t__hlist_del(n);\n\t\tINIT_HLIST_NODE(n);\n\t}\n}\n\n/**\n * hlist_add_head - add a new entry at the beginning of the hlist\n * @n: new entry to be added\n * @h: hlist head to add it after\n *\n * Insert a new entry after the specified head.\n * This is good for implementing stacks.\n */\nstatic inline void hlist_add_head(struct hlist_node *n, struct hlist_head *h)\n{\n\tstruct hlist_node *first = h->first;\n\tWRITE_ONCE(n->next, first);\n\tif (first)\n\t\tWRITE_ONCE(first->pprev, &n->next);\n\tWRITE_ONCE(h->first, n);\n\tWRITE_ONCE(n->pprev, &h->first);\n}\n\n/**\n * hlist_add_before - add a new entry before the one specified\n * @n: new entry to be added\n * @next: hlist node to add it before, which must be non-NULL\n */\nstatic inline void hlist_add_before(struct hlist_node *n,\n\t\t\t\t    struct hlist_node *next)\n{\n\tWRITE_ONCE(n->pprev, next->pprev);\n\tWRITE_ONCE(n->next, next);\n\tWRITE_ONCE(next->pprev, &n->next);\n\tWRITE_ONCE(*(n->pprev), n);\n}\n\n/**\n * hlist_add_behind - add a new entry after the one specified\n * @n: new entry to be added\n * @prev: hlist node to add it after, which must be non-NULL\n */\nstatic inline void hlist_add_behind(struct hlist_node *n,\n\t\t\t\t    struct hlist_node *prev)\n{\n\tWRITE_ONCE(n->next, prev->next);\n\tWRITE_ONCE(prev->next, n);\n\tWRITE_ONCE(n->pprev, &prev->next);\n\n\tif (n->next)\n\t\tWRITE_ONCE(n->next->pprev, &n->next);\n}\n\n/**\n * hlist_add_fake - create a fake hlist consisting of a single headless node\n * @n: Node to make a fake list out of\n *\n * This makes @n appear to be its own predecessor on a headless hlist.\n * The point of this is to allow things like hlist_del() to work correctly\n * in cases where there is no list.\n */\nstatic inline void hlist_add_fake(struct hlist_node *n)\n{\n\tn->pprev = &n->next;\n}\n\n/**\n * hlist_fake: Is this node a fake hlist?\n * @h: Node to check for being a self-referential fake hlist.\n */\nstatic inline bool hlist_fake(struct hlist_node *h)\n{\n\treturn h->pprev == &h->next;\n}\n\n/**\n * hlist_is_singular_node - is node the only element of the specified hlist?\n * @n: Node to check for singularity.\n * @h: Header for potentially singular list.\n *\n * Check whether the node is the only node of the head without\n * accessing head, thus avoiding unnecessary cache misses.\n */\nstatic inline bool\nhlist_is_singular_node(struct hlist_node *n, struct hlist_head *h)\n{\n\treturn !n->next && n->pprev == &h->first;\n}\n\n/**\n * hlist_move_list - Move an hlist\n * @old: hlist_head for old list.\n * @new: hlist_head for new list.\n *\n * Move a list from one list head to another. Fixup the pprev\n * reference of the first entry if it exists.\n */\nstatic inline void hlist_move_list(struct hlist_head *old,\n\t\t\t\t   struct hlist_head *new)\n{\n\tnew->first = old->first;\n\tif (new->first)\n\t\tnew->first->pprev = &new->first;\n\told->first = NULL;\n}\n\n#define hlist_entry(ptr, type, member) container_of(ptr,type,member)\n\n#define hlist_for_each(pos, head) \\\n\tfor (pos = (head)->first; pos ; pos = pos->next)\n\n#define hlist_for_each_safe(pos, n, head) \\\n\tfor (pos = (head)->first; pos && ({ n = pos->next; 1; }); \\\n\t     pos = n)\n\n#define hlist_entry_safe(ptr, type, member) \\\n\t({ typeof(ptr) ____ptr = (ptr); \\\n\t   ____ptr ? hlist_entry(____ptr, type, member) : NULL; \\\n\t})\n\n/**\n * hlist_for_each_entry\t- iterate over list of given type\n * @pos:\tthe type * to use as a loop cursor.\n * @head:\tthe head for your list.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry(pos, head, member)\t\t\t\t\\\n\tfor (pos = hlist_entry_safe((head)->first, typeof(*(pos)), member);\\\n\t     pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_continue - iterate over a hlist continuing after current point\n * @pos:\tthe type * to use as a loop cursor.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_continue(pos, member)\t\t\t\\\n\tfor (pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member);\\\n\t     pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_from - iterate over a hlist continuing from current point\n * @pos:\tthe type * to use as a loop cursor.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_from(pos, member)\t\t\t\t\\\n\tfor (; pos;\t\t\t\t\t\t\t\\\n\t     pos = hlist_entry_safe((pos)->member.next, typeof(*(pos)), member))\n\n/**\n * hlist_for_each_entry_safe - iterate over list of given type safe against removal of list entry\n * @pos:\tthe type * to use as a loop cursor.\n * @n:\t\ta &struct hlist_node to use as temporary storage\n * @head:\tthe head for your list.\n * @member:\tthe name of the hlist_node within the struct.\n */\n#define hlist_for_each_entry_safe(pos, n, head, member) \t\t\\\n\tfor (pos = hlist_entry_safe((head)->first, typeof(*pos), member);\\\n\t     pos && ({ n = pos->member.next; 1; });\t\t\t\\\n\t     pos = hlist_entry_safe(n, typeof(*pos), member))\n\n#endif\n"}, "14": {"id": 14, "path": "/src/include/linux/kernel.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_KERNEL_H\n#define _LINUX_KERNEL_H\n\n#include <stdarg.h>\n#include <linux/limits.h>\n#include <linux/linkage.h>\n#include <linux/stddef.h>\n#include <linux/types.h>\n#include <linux/compiler.h>\n#include <linux/bitops.h>\n#include <linux/log2.h>\n#include <linux/math.h>\n#include <linux/minmax.h>\n#include <linux/typecheck.h>\n#include <linux/printk.h>\n#include <linux/build_bug.h>\n#include <linux/static_call_types.h>\n#include <asm/byteorder.h>\n\n#include <uapi/linux/kernel.h>\n\n#define STACK_MAGIC\t0xdeadbeef\n\n/**\n * REPEAT_BYTE - repeat the value @x multiple times as an unsigned long value\n * @x: value to repeat\n *\n * NOTE: @x is not checked for > 0xff; larger values produce odd results.\n */\n#define REPEAT_BYTE(x)\t((~0ul / 0xff) * (x))\n\n/* @a is a power of 2 value */\n#define ALIGN(x, a)\t\t__ALIGN_KERNEL((x), (a))\n#define ALIGN_DOWN(x, a)\t__ALIGN_KERNEL((x) - ((a) - 1), (a))\n#define __ALIGN_MASK(x, mask)\t__ALIGN_KERNEL_MASK((x), (mask))\n#define PTR_ALIGN(p, a)\t\t((typeof(p))ALIGN((unsigned long)(p), (a)))\n#define PTR_ALIGN_DOWN(p, a)\t((typeof(p))ALIGN_DOWN((unsigned long)(p), (a)))\n#define IS_ALIGNED(x, a)\t\t(((x) & ((typeof(x))(a) - 1)) == 0)\n\n/* generic data direction definitions */\n#define READ\t\t\t0\n#define WRITE\t\t\t1\n\n/**\n * ARRAY_SIZE - get the number of elements in array @arr\n * @arr: array to be sized\n */\n#define ARRAY_SIZE(arr) (sizeof(arr) / sizeof((arr)[0]) + __must_be_array(arr))\n\n#define u64_to_user_ptr(x) (\t\t\\\n{\t\t\t\t\t\\\n\ttypecheck(u64, (x));\t\t\\\n\t(void __user *)(uintptr_t)(x);\t\\\n}\t\t\t\t\t\\\n)\n\n#define typeof_member(T, m)\ttypeof(((T*)0)->m)\n\n#define _RET_IP_\t\t(unsigned long)__builtin_return_address(0)\n#define _THIS_IP_  ({ __label__ __here; __here: (unsigned long)&&__here; })\n\n/**\n * upper_32_bits - return bits 32-63 of a number\n * @n: the number we're accessing\n *\n * A basic shift-right of a 64- or 32-bit quantity.  Use this to suppress\n * the \"right shift count >= width of type\" warning when that quantity is\n * 32-bits.\n */\n#define upper_32_bits(n) ((u32)(((n) >> 16) >> 16))\n\n/**\n * lower_32_bits - return bits 0-31 of a number\n * @n: the number we're accessing\n */\n#define lower_32_bits(n) ((u32)((n) & 0xffffffff))\n\nstruct completion;\nstruct pt_regs;\nstruct user;\n\n#ifdef CONFIG_PREEMPT_VOLUNTARY\n\nextern int __cond_resched(void);\n# define might_resched() __cond_resched()\n\n#elif defined(CONFIG_PREEMPT_DYNAMIC)\n\nextern int __cond_resched(void);\n\nDECLARE_STATIC_CALL(might_resched, __cond_resched);\n\nstatic __always_inline void might_resched(void)\n{\n\tstatic_call_mod(might_resched)();\n}\n\n#else\n\n# define might_resched() do { } while (0)\n\n#endif /* CONFIG_PREEMPT_* */\n\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\nextern void ___might_sleep(const char *file, int line, int preempt_offset);\nextern void __might_sleep(const char *file, int line, int preempt_offset);\nextern void __cant_sleep(const char *file, int line, int preempt_offset);\nextern void __cant_migrate(const char *file, int line);\n\n/**\n * might_sleep - annotation for functions that can sleep\n *\n * this macro will print a stack trace if it is executed in an atomic\n * context (spinlock, irq-handler, ...). Additional sections where blocking is\n * not allowed can be annotated with non_block_start() and non_block_end()\n * pairs.\n *\n * This is a useful debugging help to be able to catch problems early and not\n * be bitten later when the calling function happens to sleep when it is not\n * supposed to.\n */\n# define might_sleep() \\\n\tdo { __might_sleep(__FILE__, __LINE__, 0); might_resched(); } while (0)\n/**\n * cant_sleep - annotation for functions that cannot sleep\n *\n * this macro will print a stack trace if it is executed with preemption enabled\n */\n# define cant_sleep() \\\n\tdo { __cant_sleep(__FILE__, __LINE__, 0); } while (0)\n# define sched_annotate_sleep()\t(current->task_state_change = 0)\n\n/**\n * cant_migrate - annotation for functions that cannot migrate\n *\n * Will print a stack trace if executed in code which is migratable\n */\n# define cant_migrate()\t\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (IS_ENABLED(CONFIG_SMP))\t\t\t\t\\\n\t\t\t__cant_migrate(__FILE__, __LINE__);\t\t\\\n\t} while (0)\n\n/**\n * non_block_start - annotate the start of section where sleeping is prohibited\n *\n * This is on behalf of the oom reaper, specifically when it is calling the mmu\n * notifiers. The problem is that if the notifier were to block on, for example,\n * mutex_lock() and if the process which holds that mutex were to perform a\n * sleeping memory allocation, the oom reaper is now blocked on completion of\n * that memory allocation. Other blocking calls like wait_event() pose similar\n * issues.\n */\n# define non_block_start() (current->non_block_count++)\n/**\n * non_block_end - annotate the end of section where sleeping is prohibited\n *\n * Closes a section opened by non_block_start().\n */\n# define non_block_end() WARN_ON(current->non_block_count-- == 0)\n#else\n  static inline void ___might_sleep(const char *file, int line,\n\t\t\t\t   int preempt_offset) { }\n  static inline void __might_sleep(const char *file, int line,\n\t\t\t\t   int preempt_offset) { }\n# define might_sleep() do { might_resched(); } while (0)\n# define cant_sleep() do { } while (0)\n# define cant_migrate()\t\tdo { } while (0)\n# define sched_annotate_sleep() do { } while (0)\n# define non_block_start() do { } while (0)\n# define non_block_end() do { } while (0)\n#endif\n\n#define might_sleep_if(cond) do { if (cond) might_sleep(); } while (0)\n\n#if defined(CONFIG_MMU) && \\\n\t(defined(CONFIG_PROVE_LOCKING) || defined(CONFIG_DEBUG_ATOMIC_SLEEP))\n#define might_fault() __might_fault(__FILE__, __LINE__)\nvoid __might_fault(const char *file, int line);\n#else\nstatic inline void might_fault(void) { }\n#endif\n\nextern struct atomic_notifier_head panic_notifier_list;\nextern long (*panic_blink)(int state);\n__printf(1, 2)\nvoid panic(const char *fmt, ...) __noreturn __cold;\nvoid nmi_panic(struct pt_regs *regs, const char *msg);\nextern void oops_enter(void);\nextern void oops_exit(void);\nextern bool oops_may_print(void);\nvoid do_exit(long error_code) __noreturn;\nvoid complete_and_exit(struct completion *, long) __noreturn;\n\n/* Internal, do not use. */\nint __must_check _kstrtoul(const char *s, unsigned int base, unsigned long *res);\nint __must_check _kstrtol(const char *s, unsigned int base, long *res);\n\nint __must_check kstrtoull(const char *s, unsigned int base, unsigned long long *res);\nint __must_check kstrtoll(const char *s, unsigned int base, long long *res);\n\n/**\n * kstrtoul - convert a string to an unsigned long\n * @s: The start of the string. The string must be null-terminated, and may also\n *  include a single newline before its terminating null. The first character\n *  may also be a plus sign, but not a minus sign.\n * @base: The number base to use. The maximum supported base is 16. If base is\n *  given as 0, then the base of the string is automatically detected with the\n *  conventional semantics - If it begins with 0x the number will be parsed as a\n *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be\n *  parsed as an octal number. Otherwise it will be parsed as a decimal.\n * @res: Where to write the result of the conversion on success.\n *\n * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.\n * Preferred over simple_strtoul(). Return code must be checked.\n*/\nstatic inline int __must_check kstrtoul(const char *s, unsigned int base, unsigned long *res)\n{\n\t/*\n\t * We want to shortcut function call, but\n\t * __builtin_types_compatible_p(unsigned long, unsigned long long) = 0.\n\t */\n\tif (sizeof(unsigned long) == sizeof(unsigned long long) &&\n\t    __alignof__(unsigned long) == __alignof__(unsigned long long))\n\t\treturn kstrtoull(s, base, (unsigned long long *)res);\n\telse\n\t\treturn _kstrtoul(s, base, res);\n}\n\n/**\n * kstrtol - convert a string to a long\n * @s: The start of the string. The string must be null-terminated, and may also\n *  include a single newline before its terminating null. The first character\n *  may also be a plus sign or a minus sign.\n * @base: The number base to use. The maximum supported base is 16. If base is\n *  given as 0, then the base of the string is automatically detected with the\n *  conventional semantics - If it begins with 0x the number will be parsed as a\n *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be\n *  parsed as an octal number. Otherwise it will be parsed as a decimal.\n * @res: Where to write the result of the conversion on success.\n *\n * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.\n * Preferred over simple_strtol(). Return code must be checked.\n */\nstatic inline int __must_check kstrtol(const char *s, unsigned int base, long *res)\n{\n\t/*\n\t * We want to shortcut function call, but\n\t * __builtin_types_compatible_p(long, long long) = 0.\n\t */\n\tif (sizeof(long) == sizeof(long long) &&\n\t    __alignof__(long) == __alignof__(long long))\n\t\treturn kstrtoll(s, base, (long long *)res);\n\telse\n\t\treturn _kstrtol(s, base, res);\n}\n\nint __must_check kstrtouint(const char *s, unsigned int base, unsigned int *res);\nint __must_check kstrtoint(const char *s, unsigned int base, int *res);\n\nstatic inline int __must_check kstrtou64(const char *s, unsigned int base, u64 *res)\n{\n\treturn kstrtoull(s, base, res);\n}\n\nstatic inline int __must_check kstrtos64(const char *s, unsigned int base, s64 *res)\n{\n\treturn kstrtoll(s, base, res);\n}\n\nstatic inline int __must_check kstrtou32(const char *s, unsigned int base, u32 *res)\n{\n\treturn kstrtouint(s, base, res);\n}\n\nstatic inline int __must_check kstrtos32(const char *s, unsigned int base, s32 *res)\n{\n\treturn kstrtoint(s, base, res);\n}\n\nint __must_check kstrtou16(const char *s, unsigned int base, u16 *res);\nint __must_check kstrtos16(const char *s, unsigned int base, s16 *res);\nint __must_check kstrtou8(const char *s, unsigned int base, u8 *res);\nint __must_check kstrtos8(const char *s, unsigned int base, s8 *res);\nint __must_check kstrtobool(const char *s, bool *res);\n\nint __must_check kstrtoull_from_user(const char __user *s, size_t count, unsigned int base, unsigned long long *res);\nint __must_check kstrtoll_from_user(const char __user *s, size_t count, unsigned int base, long long *res);\nint __must_check kstrtoul_from_user(const char __user *s, size_t count, unsigned int base, unsigned long *res);\nint __must_check kstrtol_from_user(const char __user *s, size_t count, unsigned int base, long *res);\nint __must_check kstrtouint_from_user(const char __user *s, size_t count, unsigned int base, unsigned int *res);\nint __must_check kstrtoint_from_user(const char __user *s, size_t count, unsigned int base, int *res);\nint __must_check kstrtou16_from_user(const char __user *s, size_t count, unsigned int base, u16 *res);\nint __must_check kstrtos16_from_user(const char __user *s, size_t count, unsigned int base, s16 *res);\nint __must_check kstrtou8_from_user(const char __user *s, size_t count, unsigned int base, u8 *res);\nint __must_check kstrtos8_from_user(const char __user *s, size_t count, unsigned int base, s8 *res);\nint __must_check kstrtobool_from_user(const char __user *s, size_t count, bool *res);\n\nstatic inline int __must_check kstrtou64_from_user(const char __user *s, size_t count, unsigned int base, u64 *res)\n{\n\treturn kstrtoull_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtos64_from_user(const char __user *s, size_t count, unsigned int base, s64 *res)\n{\n\treturn kstrtoll_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtou32_from_user(const char __user *s, size_t count, unsigned int base, u32 *res)\n{\n\treturn kstrtouint_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtos32_from_user(const char __user *s, size_t count, unsigned int base, s32 *res)\n{\n\treturn kstrtoint_from_user(s, count, base, res);\n}\n\n/*\n * Use kstrto<foo> instead.\n *\n * NOTE: simple_strto<foo> does not check for the range overflow and,\n *\t depending on the input, may give interesting results.\n *\n * Use these functions if and only if you cannot use kstrto<foo>, because\n * the conversion ends on the first non-digit character, which may be far\n * beyond the supported range. It might be useful to parse the strings like\n * 10x50 or 12:21 without altering original string or temporary buffer in use.\n * Keep in mind above caveat.\n */\n\nextern unsigned long simple_strtoul(const char *,char **,unsigned int);\nextern long simple_strtol(const char *,char **,unsigned int);\nextern unsigned long long simple_strtoull(const char *,char **,unsigned int);\nextern long long simple_strtoll(const char *,char **,unsigned int);\n\nextern int num_to_str(char *buf, int size,\n\t\t      unsigned long long num, unsigned int width);\n\n/* lib/printf utilities */\n\nextern __printf(2, 3) int sprintf(char *buf, const char * fmt, ...);\nextern __printf(2, 0) int vsprintf(char *buf, const char *, va_list);\nextern __printf(3, 4)\nint snprintf(char *buf, size_t size, const char *fmt, ...);\nextern __printf(3, 0)\nint vsnprintf(char *buf, size_t size, const char *fmt, va_list args);\nextern __printf(3, 4)\nint scnprintf(char *buf, size_t size, const char *fmt, ...);\nextern __printf(3, 0)\nint vscnprintf(char *buf, size_t size, const char *fmt, va_list args);\nextern __printf(2, 3) __malloc\nchar *kasprintf(gfp_t gfp, const char *fmt, ...);\nextern __printf(2, 0) __malloc\nchar *kvasprintf(gfp_t gfp, const char *fmt, va_list args);\nextern __printf(2, 0)\nconst char *kvasprintf_const(gfp_t gfp, const char *fmt, va_list args);\n\nextern __scanf(2, 3)\nint sscanf(const char *, const char *, ...);\nextern __scanf(2, 0)\nint vsscanf(const char *, const char *, va_list);\n\nextern int get_option(char **str, int *pint);\nextern char *get_options(const char *str, int nints, int *ints);\nextern unsigned long long memparse(const char *ptr, char **retptr);\nextern bool parse_option_str(const char *str, const char *option);\nextern char *next_arg(char *args, char **param, char **val);\n\nextern int core_kernel_text(unsigned long addr);\nextern int init_kernel_text(unsigned long addr);\nextern int core_kernel_data(unsigned long addr);\nextern int __kernel_text_address(unsigned long addr);\nextern int kernel_text_address(unsigned long addr);\nextern int func_ptr_is_kernel_text(void *ptr);\n\n#ifdef CONFIG_SMP\nextern unsigned int sysctl_oops_all_cpu_backtrace;\n#else\n#define sysctl_oops_all_cpu_backtrace 0\n#endif /* CONFIG_SMP */\n\nextern void bust_spinlocks(int yes);\nextern int panic_timeout;\nextern unsigned long panic_print;\nextern int panic_on_oops;\nextern int panic_on_unrecovered_nmi;\nextern int panic_on_io_nmi;\nextern int panic_on_warn;\nextern unsigned long panic_on_taint;\nextern bool panic_on_taint_nousertaint;\nextern int sysctl_panic_on_rcu_stall;\nextern int sysctl_max_rcu_stall_to_panic;\nextern int sysctl_panic_on_stackoverflow;\n\nextern bool crash_kexec_post_notifiers;\n\n/*\n * panic_cpu is used for synchronizing panic() and crash_kexec() execution. It\n * holds a CPU number which is executing panic() currently. A value of\n * PANIC_CPU_INVALID means no CPU has entered panic() or crash_kexec().\n */\nextern atomic_t panic_cpu;\n#define PANIC_CPU_INVALID\t-1\n\n/*\n * Only to be used by arch init code. If the user over-wrote the default\n * CONFIG_PANIC_TIMEOUT, honor it.\n */\nstatic inline void set_arch_panic_timeout(int timeout, int arch_default_timeout)\n{\n\tif (panic_timeout == arch_default_timeout)\n\t\tpanic_timeout = timeout;\n}\nextern const char *print_tainted(void);\nenum lockdep_ok {\n\tLOCKDEP_STILL_OK,\n\tLOCKDEP_NOW_UNRELIABLE\n};\nextern void add_taint(unsigned flag, enum lockdep_ok);\nextern int test_taint(unsigned flag);\nextern unsigned long get_taint(void);\nextern int root_mountflags;\n\nextern bool early_boot_irqs_disabled;\n\n/*\n * Values used for system_state. Ordering of the states must not be changed\n * as code checks for <, <=, >, >= STATE.\n */\nextern enum system_states {\n\tSYSTEM_BOOTING,\n\tSYSTEM_SCHEDULING,\n\tSYSTEM_RUNNING,\n\tSYSTEM_HALT,\n\tSYSTEM_POWER_OFF,\n\tSYSTEM_RESTART,\n\tSYSTEM_SUSPEND,\n} system_state;\n\n/* This cannot be an enum because some may be used in assembly source. */\n#define TAINT_PROPRIETARY_MODULE\t0\n#define TAINT_FORCED_MODULE\t\t1\n#define TAINT_CPU_OUT_OF_SPEC\t\t2\n#define TAINT_FORCED_RMMOD\t\t3\n#define TAINT_MACHINE_CHECK\t\t4\n#define TAINT_BAD_PAGE\t\t\t5\n#define TAINT_USER\t\t\t6\n#define TAINT_DIE\t\t\t7\n#define TAINT_OVERRIDDEN_ACPI_TABLE\t8\n#define TAINT_WARN\t\t\t9\n#define TAINT_CRAP\t\t\t10\n#define TAINT_FIRMWARE_WORKAROUND\t11\n#define TAINT_OOT_MODULE\t\t12\n#define TAINT_UNSIGNED_MODULE\t\t13\n#define TAINT_SOFTLOCKUP\t\t14\n#define TAINT_LIVEPATCH\t\t\t15\n#define TAINT_AUX\t\t\t16\n#define TAINT_RANDSTRUCT\t\t17\n#define TAINT_FLAGS_COUNT\t\t18\n#define TAINT_FLAGS_MAX\t\t\t((1UL << TAINT_FLAGS_COUNT) - 1)\n\nstruct taint_flag {\n\tchar c_true;\t/* character printed when tainted */\n\tchar c_false;\t/* character printed when not tainted */\n\tbool module;\t/* also show as a per-module taint flag */\n};\n\nextern const struct taint_flag taint_flags[TAINT_FLAGS_COUNT];\n\nextern const char hex_asc[];\n#define hex_asc_lo(x)\thex_asc[((x) & 0x0f)]\n#define hex_asc_hi(x)\thex_asc[((x) & 0xf0) >> 4]\n\nstatic inline char *hex_byte_pack(char *buf, u8 byte)\n{\n\t*buf++ = hex_asc_hi(byte);\n\t*buf++ = hex_asc_lo(byte);\n\treturn buf;\n}\n\nextern const char hex_asc_upper[];\n#define hex_asc_upper_lo(x)\thex_asc_upper[((x) & 0x0f)]\n#define hex_asc_upper_hi(x)\thex_asc_upper[((x) & 0xf0) >> 4]\n\nstatic inline char *hex_byte_pack_upper(char *buf, u8 byte)\n{\n\t*buf++ = hex_asc_upper_hi(byte);\n\t*buf++ = hex_asc_upper_lo(byte);\n\treturn buf;\n}\n\nextern int hex_to_bin(char ch);\nextern int __must_check hex2bin(u8 *dst, const char *src, size_t count);\nextern char *bin2hex(char *dst, const void *src, size_t count);\n\nbool mac_pton(const char *s, u8 *mac);\n\n/*\n * General tracing related utility functions - trace_printk(),\n * tracing_on/tracing_off and tracing_start()/tracing_stop\n *\n * Use tracing_on/tracing_off when you want to quickly turn on or off\n * tracing. It simply enables or disables the recording of the trace events.\n * This also corresponds to the user space /sys/kernel/debug/tracing/tracing_on\n * file, which gives a means for the kernel and userspace to interact.\n * Place a tracing_off() in the kernel where you want tracing to end.\n * From user space, examine the trace, and then echo 1 > tracing_on\n * to continue tracing.\n *\n * tracing_stop/tracing_start has slightly more overhead. It is used\n * by things like suspend to ram where disabling the recording of the\n * trace is not enough, but tracing must actually stop because things\n * like calling smp_processor_id() may crash the system.\n *\n * Most likely, you want to use tracing_on/tracing_off.\n */\n\nenum ftrace_dump_mode {\n\tDUMP_NONE,\n\tDUMP_ALL,\n\tDUMP_ORIG,\n};\n\n#ifdef CONFIG_TRACING\nvoid tracing_on(void);\nvoid tracing_off(void);\nint tracing_is_on(void);\nvoid tracing_snapshot(void);\nvoid tracing_snapshot_alloc(void);\n\nextern void tracing_start(void);\nextern void tracing_stop(void);\n\nstatic inline __printf(1, 2)\nvoid ____trace_printk_check_format(const char *fmt, ...)\n{\n}\n#define __trace_printk_check_format(fmt, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\t\\\n\t\t____trace_printk_check_format(fmt, ##args);\t\t\\\n} while (0)\n\n/**\n * trace_printk - printf formatting in the ftrace buffer\n * @fmt: the printf format for printing\n *\n * Note: __trace_printk is an internal function for trace_printk() and\n *       the @ip is passed in via the trace_printk() macro.\n *\n * This function allows a kernel developer to debug fast path sections\n * that printk is not appropriate for. By scattering in various\n * printk like tracing in the code, a developer can quickly see\n * where problems are occurring.\n *\n * This is intended as a debugging tool for the developer only.\n * Please refrain from leaving trace_printks scattered around in\n * your code. (Extra memory is used for special buffers that are\n * allocated when trace_printk() is used.)\n *\n * A little optimization trick is done here. If there's only one\n * argument, there's no need to scan the string for printf formats.\n * The trace_puts() will suffice. But how can we take advantage of\n * using trace_puts() when trace_printk() has only one argument?\n * By stringifying the args and checking the size we can tell\n * whether or not there are args. __stringify((__VA_ARGS__)) will\n * turn into \"()\\0\" with a size of 3 when there are no args, anything\n * else will be bigger. All we need to do is define a string to this,\n * and then take its size and compare to 3. If it's bigger, use\n * do_trace_printk() otherwise, optimize it to trace_puts(). Then just\n * let gcc optimize the rest.\n */\n\n#define trace_printk(fmt, ...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\\\n\tchar _______STR[] = __stringify((__VA_ARGS__));\t\\\n\tif (sizeof(_______STR) > 3)\t\t\t\\\n\t\tdo_trace_printk(fmt, ##__VA_ARGS__);\t\\\n\telse\t\t\t\t\t\t\\\n\t\ttrace_puts(fmt);\t\t\t\\\n} while (0)\n\n#define do_trace_printk(fmt, args...)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tstatic const char *trace_printk_fmt __used\t\t\t\\\n\t\t__section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t__builtin_constant_p(fmt) ? fmt : NULL;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t__trace_printk_check_format(fmt, ##args);\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(fmt))\t\t\t\t\t\\\n\t\t__trace_bprintk(_THIS_IP_, trace_printk_fmt, ##args);\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t__trace_printk(_THIS_IP_, fmt, ##args);\t\t\t\\\n} while (0)\n\nextern __printf(2, 3)\nint __trace_bprintk(unsigned long ip, const char *fmt, ...);\n\nextern __printf(2, 3)\nint __trace_printk(unsigned long ip, const char *fmt, ...);\n\n/**\n * trace_puts - write a string into the ftrace buffer\n * @str: the string to record\n *\n * Note: __trace_bputs is an internal function for trace_puts and\n *       the @ip is passed in via the trace_puts macro.\n *\n * This is similar to trace_printk() but is made for those really fast\n * paths that a developer wants the least amount of \"Heisenbug\" effects,\n * where the processing of the print format is still too much.\n *\n * This function allows a kernel developer to debug fast path sections\n * that printk is not appropriate for. By scattering in various\n * printk like tracing in the code, a developer can quickly see\n * where problems are occurring.\n *\n * This is intended as a debugging tool for the developer only.\n * Please refrain from leaving trace_puts scattered around in\n * your code. (Extra memory is used for special buffers that are\n * allocated when trace_puts() is used.)\n *\n * Returns: 0 if nothing was written, positive # if string was.\n *  (1 when __trace_bputs is used, strlen(str) when __trace_puts is used)\n */\n\n#define trace_puts(str) ({\t\t\t\t\t\t\\\n\tstatic const char *trace_printk_fmt __used\t\t\t\\\n\t\t__section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t__builtin_constant_p(str) ? str : NULL;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(str))\t\t\t\t\t\\\n\t\t__trace_bputs(_THIS_IP_, trace_printk_fmt);\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t__trace_puts(_THIS_IP_, str, strlen(str));\t\t\\\n})\nextern int __trace_bputs(unsigned long ip, const char *str);\nextern int __trace_puts(unsigned long ip, const char *str, int size);\n\nextern void trace_dump_stack(int skip);\n\n/*\n * The double __builtin_constant_p is because gcc will give us an error\n * if we try to allocate the static variable to fmt if it is not a\n * constant. Even with the outer if statement.\n */\n#define ftrace_vprintk(fmt, vargs)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(fmt)) {\t\t\t\t\\\n\t\tstatic const char *trace_printk_fmt __used\t\t\\\n\t\t  __section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t\t__builtin_constant_p(fmt) ? fmt : NULL;\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t__ftrace_vbprintk(_THIS_IP_, trace_printk_fmt, vargs);\t\\\n\t} else\t\t\t\t\t\t\t\t\\\n\t\t__ftrace_vprintk(_THIS_IP_, fmt, vargs);\t\t\\\n} while (0)\n\nextern __printf(2, 0) int\n__ftrace_vbprintk(unsigned long ip, const char *fmt, va_list ap);\n\nextern __printf(2, 0) int\n__ftrace_vprintk(unsigned long ip, const char *fmt, va_list ap);\n\nextern void ftrace_dump(enum ftrace_dump_mode oops_dump_mode);\n#else\nstatic inline void tracing_start(void) { }\nstatic inline void tracing_stop(void) { }\nstatic inline void trace_dump_stack(int skip) { }\n\nstatic inline void tracing_on(void) { }\nstatic inline void tracing_off(void) { }\nstatic inline int tracing_is_on(void) { return 0; }\nstatic inline void tracing_snapshot(void) { }\nstatic inline void tracing_snapshot_alloc(void) { }\n\nstatic inline __printf(1, 2)\nint trace_printk(const char *fmt, ...)\n{\n\treturn 0;\n}\nstatic __printf(1, 0) inline int\nftrace_vprintk(const char *fmt, va_list ap)\n{\n\treturn 0;\n}\nstatic inline void ftrace_dump(enum ftrace_dump_mode oops_dump_mode) { }\n#endif /* CONFIG_TRACING */\n\n/* This counts to 12. Any more, it will return 13th argument. */\n#define __COUNT_ARGS(_0, _1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _n, X...) _n\n#define COUNT_ARGS(X...) __COUNT_ARGS(, ##X, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0)\n\n#define __CONCAT(a, b) a ## b\n#define CONCATENATE(a, b) __CONCAT(a, b)\n\n/**\n * container_of - cast a member of a structure out to the containing structure\n * @ptr:\tthe pointer to the member.\n * @type:\tthe type of the container struct this is embedded in.\n * @member:\tthe name of the member within the struct.\n *\n */\n#define container_of(ptr, type, member) ({\t\t\t\t\\\n\tvoid *__mptr = (void *)(ptr);\t\t\t\t\t\\\n\tBUILD_BUG_ON_MSG(!__same_type(*(ptr), ((type *)0)->member) &&\t\\\n\t\t\t !__same_type(*(ptr), void),\t\t\t\\\n\t\t\t \"pointer type mismatch in container_of()\");\t\\\n\t((type *)(__mptr - offsetof(type, member))); })\n\n/**\n * container_of_safe - cast a member of a structure out to the containing structure\n * @ptr:\tthe pointer to the member.\n * @type:\tthe type of the container struct this is embedded in.\n * @member:\tthe name of the member within the struct.\n *\n * If IS_ERR_OR_NULL(ptr), ptr is returned unchanged.\n */\n#define container_of_safe(ptr, type, member) ({\t\t\t\t\\\n\tvoid *__mptr = (void *)(ptr);\t\t\t\t\t\\\n\tBUILD_BUG_ON_MSG(!__same_type(*(ptr), ((type *)0)->member) &&\t\\\n\t\t\t !__same_type(*(ptr), void),\t\t\t\\\n\t\t\t \"pointer type mismatch in container_of()\");\t\\\n\tIS_ERR_OR_NULL(__mptr) ? ERR_CAST(__mptr) :\t\t\t\\\n\t\t((type *)(__mptr - offsetof(type, member))); })\n\n/* Rebuild everything on CONFIG_FTRACE_MCOUNT_RECORD */\n#ifdef CONFIG_FTRACE_MCOUNT_RECORD\n# define REBUILD_DUE_TO_FTRACE_MCOUNT_RECORD\n#endif\n\n/* Permissions on a sysfs file: you didn't miss the 0 prefix did you? */\n#define VERIFY_OCTAL_PERMISSIONS(perms)\t\t\t\t\t\t\\\n\t(BUILD_BUG_ON_ZERO((perms) < 0) +\t\t\t\t\t\\\n\t BUILD_BUG_ON_ZERO((perms) > 0777) +\t\t\t\t\t\\\n\t /* USER_READABLE >= GROUP_READABLE >= OTHER_READABLE */\t\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 6) & 4) < (((perms) >> 3) & 4)) +\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 3) & 4) < ((perms) & 4)) +\t\t\\\n\t /* USER_WRITABLE >= GROUP_WRITABLE */\t\t\t\t\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 6) & 2) < (((perms) >> 3) & 2)) +\t\\\n\t /* OTHER_WRITABLE?  Generally considered a bad idea. */\t\t\\\n\t BUILD_BUG_ON_ZERO((perms) & 2) +\t\t\t\t\t\\\n\t (perms))\n#endif\n"}, "15": {"id": 15, "path": "/src/net/ipv6/ip6_output.c", "content": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n *\tIPv6 output functions\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tBased on linux/net/ipv4/ip_output.c\n *\n *\tChanges:\n *\tA.N.Kuznetsov\t:\tairthmetics in fragmentation.\n *\t\t\t\textension headers are implemented.\n *\t\t\t\troute changes now work.\n *\t\t\t\tip6_forward does not confuse sniffers.\n *\t\t\t\tetc.\n *\n *      H. von Brand    :       Added missing #include <linux/string.h>\n *\tImran Patel\t:\tfrag id should be in NBO\n *      Kazunori MIYAZAWA @USAGI\n *\t\t\t:       add ip6_append_data and related functions\n *\t\t\t\tfor datagram xmit\n */\n\n#include <linux/errno.h>\n#include <linux/kernel.h>\n#include <linux/string.h>\n#include <linux/socket.h>\n#include <linux/net.h>\n#include <linux/netdevice.h>\n#include <linux/if_arp.h>\n#include <linux/in6.h>\n#include <linux/tcp.h>\n#include <linux/route.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n\n#include <linux/bpf-cgroup.h>\n#include <linux/netfilter.h>\n#include <linux/netfilter_ipv6.h>\n\n#include <net/sock.h>\n#include <net/snmp.h>\n\n#include <net/ipv6.h>\n#include <net/ndisc.h>\n#include <net/protocol.h>\n#include <net/ip6_route.h>\n#include <net/addrconf.h>\n#include <net/rawv6.h>\n#include <net/icmp.h>\n#include <net/xfrm.h>\n#include <net/checksum.h>\n#include <linux/mroute6.h>\n#include <net/l3mdev.h>\n#include <net/lwtunnel.h>\n#include <net/ip_tunnels.h>\n\nstatic int ip6_finish_output2(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tstruct dst_entry *dst = skb_dst(skb);\n\tstruct net_device *dev = dst->dev;\n\tconst struct in6_addr *nexthop;\n\tstruct neighbour *neigh;\n\tint ret;\n\n\tif (ipv6_addr_is_multicast(&ipv6_hdr(skb)->daddr)) {\n\t\tstruct inet6_dev *idev = ip6_dst_idev(skb_dst(skb));\n\n\t\tif (!(dev->flags & IFF_LOOPBACK) && sk_mc_loop(sk) &&\n\t\t    ((mroute6_is_socket(net, skb) &&\n\t\t     !(IP6CB(skb)->flags & IP6SKB_FORWARDED)) ||\n\t\t     ipv6_chk_mcast_addr(dev, &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t &ipv6_hdr(skb)->saddr))) {\n\t\t\tstruct sk_buff *newskb = skb_clone(skb, GFP_ATOMIC);\n\n\t\t\t/* Do not check for IFF_ALLMULTI; multicast routing\n\t\t\t   is not supported in any case.\n\t\t\t */\n\t\t\tif (newskb)\n\t\t\t\tNF_HOOK(NFPROTO_IPV6, NF_INET_POST_ROUTING,\n\t\t\t\t\tnet, sk, newskb, NULL, newskb->dev,\n\t\t\t\t\tdev_loopback_xmit);\n\n\t\t\tif (ipv6_hdr(skb)->hop_limit == 0) {\n\t\t\t\tIP6_INC_STATS(net, idev,\n\t\t\t\t\t      IPSTATS_MIB_OUTDISCARDS);\n\t\t\t\tkfree_skb(skb);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\n\t\tIP6_UPD_PO_STATS(net, idev, IPSTATS_MIB_OUTMCAST, skb->len);\n\n\t\tif (IPV6_ADDR_MC_SCOPE(&ipv6_hdr(skb)->daddr) <=\n\t\t    IPV6_ADDR_SCOPE_NODELOCAL &&\n\t\t    !(dev->flags & IFF_LOOPBACK)) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif (lwtunnel_xmit_redirect(dst->lwtstate)) {\n\t\tint res = lwtunnel_xmit(skb);\n\n\t\tif (res < 0 || res == LWTUNNEL_XMIT_DONE)\n\t\t\treturn res;\n\t}\n\n\trcu_read_lock_bh();\n\tnexthop = rt6_nexthop((struct rt6_info *)dst, &ipv6_hdr(skb)->daddr);\n\tneigh = __ipv6_neigh_lookup_noref(dst->dev, nexthop);\n\tif (unlikely(!neigh))\n\t\tneigh = __neigh_create(&nd_tbl, nexthop, dst->dev, false);\n\tif (!IS_ERR(neigh)) {\n\t\tsock_confirm_neigh(skb, neigh);\n\t\tret = neigh_output(neigh, skb, false);\n\t\trcu_read_unlock_bh();\n\t\treturn ret;\n\t}\n\trcu_read_unlock_bh();\n\n\tIP6_INC_STATS(net, ip6_dst_idev(dst), IPSTATS_MIB_OUTNOROUTES);\n\tkfree_skb(skb);\n\treturn -EINVAL;\n}\n\nstatic int\nip6_finish_output_gso_slowpath_drop(struct net *net, struct sock *sk,\n\t\t\t\t    struct sk_buff *skb, unsigned int mtu)\n{\n\tstruct sk_buff *segs, *nskb;\n\tnetdev_features_t features;\n\tint ret = 0;\n\n\t/* Please see corresponding comment in ip_finish_output_gso\n\t * describing the cases where GSO segment length exceeds the\n\t * egress MTU.\n\t */\n\tfeatures = netif_skb_features(skb);\n\tsegs = skb_gso_segment(skb, features & ~NETIF_F_GSO_MASK);\n\tif (IS_ERR_OR_NULL(segs)) {\n\t\tkfree_skb(skb);\n\t\treturn -ENOMEM;\n\t}\n\n\tconsume_skb(skb);\n\n\tskb_list_walk_safe(segs, segs, nskb) {\n\t\tint err;\n\n\t\tskb_mark_not_on_list(segs);\n\t\terr = ip6_fragment(net, sk, segs, ip6_finish_output2);\n\t\tif (err && ret == 0)\n\t\t\tret = err;\n\t}\n\n\treturn ret;\n}\n\nstatic int __ip6_finish_output(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tunsigned int mtu;\n\n#if defined(CONFIG_NETFILTER) && defined(CONFIG_XFRM)\n\t/* Policy lookup after SNAT yielded a new policy */\n\tif (skb_dst(skb)->xfrm) {\n\t\tIPCB(skb)->flags |= IPSKB_REROUTED;\n\t\treturn dst_output(net, sk, skb);\n\t}\n#endif\n\n\tmtu = ip6_skb_dst_mtu(skb);\n\tif (skb_is_gso(skb) && !skb_gso_validate_network_len(skb, mtu))\n\t\treturn ip6_finish_output_gso_slowpath_drop(net, sk, skb, mtu);\n\n\tif ((skb->len > mtu && !skb_is_gso(skb)) ||\n\t    dst_allfrag(skb_dst(skb)) ||\n\t    (IP6CB(skb)->frag_max_size && skb->len > IP6CB(skb)->frag_max_size))\n\t\treturn ip6_fragment(net, sk, skb, ip6_finish_output2);\n\telse\n\t\treturn ip6_finish_output2(net, sk, skb);\n}\n\nstatic int ip6_finish_output(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tint ret;\n\n\tret = BPF_CGROUP_RUN_PROG_INET_EGRESS(sk, skb);\n\tswitch (ret) {\n\tcase NET_XMIT_SUCCESS:\n\t\treturn __ip6_finish_output(net, sk, skb);\n\tcase NET_XMIT_CN:\n\t\treturn __ip6_finish_output(net, sk, skb) ? : ret;\n\tdefault:\n\t\tkfree_skb(skb);\n\t\treturn ret;\n\t}\n}\n\nint ip6_output(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb_dst(skb)->dev, *indev = skb->dev;\n\tstruct inet6_dev *idev = ip6_dst_idev(skb_dst(skb));\n\n\tskb->protocol = htons(ETH_P_IPV6);\n\tskb->dev = dev;\n\n\tif (unlikely(idev->cnf.disable_ipv6)) {\n\t\tIP6_INC_STATS(net, idev, IPSTATS_MIB_OUTDISCARDS);\n\t\tkfree_skb(skb);\n\t\treturn 0;\n\t}\n\n\treturn NF_HOOK_COND(NFPROTO_IPV6, NF_INET_POST_ROUTING,\n\t\t\t    net, sk, skb, indev, dev,\n\t\t\t    ip6_finish_output,\n\t\t\t    !(IP6CB(skb)->flags & IP6SKB_REROUTED));\n}\nEXPORT_SYMBOL(ip6_output);\n\nbool ip6_autoflowlabel(struct net *net, const struct ipv6_pinfo *np)\n{\n\tif (!np->autoflowlabel_set)\n\t\treturn ip6_default_np_autolabel(net);\n\telse\n\t\treturn np->autoflowlabel;\n}\n\n/*\n * xmit an sk_buff (used by TCP, SCTP and DCCP)\n * Note : socket lock is not held for SYNACK packets, but might be modified\n * by calls to skb_set_owner_w() and ipv6_local_error(),\n * which are using proper atomic operations or spinlocks.\n */\nint ip6_xmit(const struct sock *sk, struct sk_buff *skb, struct flowi6 *fl6,\n\t     __u32 mark, struct ipv6_txoptions *opt, int tclass, u32 priority)\n{\n\tstruct net *net = sock_net(sk);\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *first_hop = &fl6->daddr;\n\tstruct dst_entry *dst = skb_dst(skb);\n\tunsigned int head_room;\n\tstruct ipv6hdr *hdr;\n\tu8  proto = fl6->flowi6_proto;\n\tint seg_len = skb->len;\n\tint hlimit = -1;\n\tu32 mtu;\n\n\thead_room = sizeof(struct ipv6hdr) + LL_RESERVED_SPACE(dst->dev);\n\tif (opt)\n\t\thead_room += opt->opt_nflen + opt->opt_flen;\n\n\tif (unlikely(skb_headroom(skb) < head_room)) {\n\t\tstruct sk_buff *skb2 = skb_realloc_headroom(skb, head_room);\n\t\tif (!skb2) {\n\t\t\tIP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t      IPSTATS_MIB_OUTDISCARDS);\n\t\t\tkfree_skb(skb);\n\t\t\treturn -ENOBUFS;\n\t\t}\n\t\tif (skb->sk)\n\t\t\tskb_set_owner_w(skb2, skb->sk);\n\t\tconsume_skb(skb);\n\t\tskb = skb2;\n\t}\n\n\tif (opt) {\n\t\tseg_len += opt->opt_nflen + opt->opt_flen;\n\n\t\tif (opt->opt_flen)\n\t\t\tipv6_push_frag_opts(skb, opt, &proto);\n\n\t\tif (opt->opt_nflen)\n\t\t\tipv6_push_nfrag_opts(skb, opt, &proto, &first_hop,\n\t\t\t\t\t     &fl6->saddr);\n\t}\n\n\tskb_push(skb, sizeof(struct ipv6hdr));\n\tskb_reset_network_header(skb);\n\thdr = ipv6_hdr(skb);\n\n\t/*\n\t *\tFill in the IPv6 header\n\t */\n\tif (np)\n\t\thlimit = np->hop_limit;\n\tif (hlimit < 0)\n\t\thlimit = ip6_dst_hoplimit(dst);\n\n\tip6_flow_hdr(hdr, tclass, ip6_make_flowlabel(net, skb, fl6->flowlabel,\n\t\t\t\tip6_autoflowlabel(net, np), fl6));\n\n\thdr->payload_len = htons(seg_len);\n\thdr->nexthdr = proto;\n\thdr->hop_limit = hlimit;\n\n\thdr->saddr = fl6->saddr;\n\thdr->daddr = *first_hop;\n\n\tskb->protocol = htons(ETH_P_IPV6);\n\tskb->priority = priority;\n\tskb->mark = mark;\n\n\tmtu = dst_mtu(dst);\n\tif ((skb->len <= mtu) || skb->ignore_df || skb_is_gso(skb)) {\n\t\tIP6_UPD_PO_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t      IPSTATS_MIB_OUT, skb->len);\n\n\t\t/* if egress device is enslaved to an L3 master device pass the\n\t\t * skb to its handler for processing\n\t\t */\n\t\tskb = l3mdev_ip6_out((struct sock *)sk, skb);\n\t\tif (unlikely(!skb))\n\t\t\treturn 0;\n\n\t\t/* hooks should never assume socket lock is held.\n\t\t * we promote our socket to non const\n\t\t */\n\t\treturn NF_HOOK(NFPROTO_IPV6, NF_INET_LOCAL_OUT,\n\t\t\t       net, (struct sock *)sk, skb, NULL, dst->dev,\n\t\t\t       dst_output);\n\t}\n\n\tskb->dev = dst->dev;\n\t/* ipv6_local_error() does not require socket lock,\n\t * we promote our socket to non const\n\t */\n\tipv6_local_error((struct sock *)sk, EMSGSIZE, fl6, mtu);\n\n\tIP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)), IPSTATS_MIB_FRAGFAILS);\n\tkfree_skb(skb);\n\treturn -EMSGSIZE;\n}\nEXPORT_SYMBOL(ip6_xmit);\n\nstatic int ip6_call_ra_chain(struct sk_buff *skb, int sel)\n{\n\tstruct ip6_ra_chain *ra;\n\tstruct sock *last = NULL;\n\n\tread_lock(&ip6_ra_lock);\n\tfor (ra = ip6_ra_chain; ra; ra = ra->next) {\n\t\tstruct sock *sk = ra->sk;\n\t\tif (sk && ra->sel == sel &&\n\t\t    (!sk->sk_bound_dev_if ||\n\t\t     sk->sk_bound_dev_if == skb->dev->ifindex)) {\n\t\t\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\n\t\t\tif (np && np->rtalert_isolate &&\n\t\t\t    !net_eq(sock_net(sk), dev_net(skb->dev))) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (last) {\n\t\t\t\tstruct sk_buff *skb2 = skb_clone(skb, GFP_ATOMIC);\n\t\t\t\tif (skb2)\n\t\t\t\t\trawv6_rcv(last, skb2);\n\t\t\t}\n\t\t\tlast = sk;\n\t\t}\n\t}\n\n\tif (last) {\n\t\trawv6_rcv(last, skb);\n\t\tread_unlock(&ip6_ra_lock);\n\t\treturn 1;\n\t}\n\tread_unlock(&ip6_ra_lock);\n\treturn 0;\n}\n\nstatic int ip6_forward_proxy_check(struct sk_buff *skb)\n{\n\tstruct ipv6hdr *hdr = ipv6_hdr(skb);\n\tu8 nexthdr = hdr->nexthdr;\n\t__be16 frag_off;\n\tint offset;\n\n\tif (ipv6_ext_hdr(nexthdr)) {\n\t\toffset = ipv6_skip_exthdr(skb, sizeof(*hdr), &nexthdr, &frag_off);\n\t\tif (offset < 0)\n\t\t\treturn 0;\n\t} else\n\t\toffset = sizeof(struct ipv6hdr);\n\n\tif (nexthdr == IPPROTO_ICMPV6) {\n\t\tstruct icmp6hdr *icmp6;\n\n\t\tif (!pskb_may_pull(skb, (skb_network_header(skb) +\n\t\t\t\t\t offset + 1 - skb->data)))\n\t\t\treturn 0;\n\n\t\ticmp6 = (struct icmp6hdr *)(skb_network_header(skb) + offset);\n\n\t\tswitch (icmp6->icmp6_type) {\n\t\tcase NDISC_ROUTER_SOLICITATION:\n\t\tcase NDISC_ROUTER_ADVERTISEMENT:\n\t\tcase NDISC_NEIGHBOUR_SOLICITATION:\n\t\tcase NDISC_NEIGHBOUR_ADVERTISEMENT:\n\t\tcase NDISC_REDIRECT:\n\t\t\t/* For reaction involving unicast neighbor discovery\n\t\t\t * message destined to the proxied address, pass it to\n\t\t\t * input function.\n\t\t\t */\n\t\t\treturn 1;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/*\n\t * The proxying router can't forward traffic sent to a link-local\n\t * address, so signal the sender and discard the packet. This\n\t * behavior is clarified by the MIPv6 specification.\n\t */\n\tif (ipv6_addr_type(&hdr->daddr) & IPV6_ADDR_LINKLOCAL) {\n\t\tdst_link_failure(skb);\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\nstatic inline int ip6_forward_finish(struct net *net, struct sock *sk,\n\t\t\t\t     struct sk_buff *skb)\n{\n\tstruct dst_entry *dst = skb_dst(skb);\n\n\t__IP6_INC_STATS(net, ip6_dst_idev(dst), IPSTATS_MIB_OUTFORWDATAGRAMS);\n\t__IP6_ADD_STATS(net, ip6_dst_idev(dst), IPSTATS_MIB_OUTOCTETS, skb->len);\n\n#ifdef CONFIG_NET_SWITCHDEV\n\tif (skb->offload_l3_fwd_mark) {\n\t\tconsume_skb(skb);\n\t\treturn 0;\n\t}\n#endif\n\n\tskb->tstamp = 0;\n\treturn dst_output(net, sk, skb);\n}\n\nstatic bool ip6_pkt_too_big(const struct sk_buff *skb, unsigned int mtu)\n{\n\tif (skb->len <= mtu)\n\t\treturn false;\n\n\t/* ipv6 conntrack defrag sets max_frag_size + ignore_df */\n\tif (IP6CB(skb)->frag_max_size && IP6CB(skb)->frag_max_size > mtu)\n\t\treturn true;\n\n\tif (skb->ignore_df)\n\t\treturn false;\n\n\tif (skb_is_gso(skb) && skb_gso_validate_network_len(skb, mtu))\n\t\treturn false;\n\n\treturn true;\n}\n\nint ip6_forward(struct sk_buff *skb)\n{\n\tstruct inet6_dev *idev = __in6_dev_get_safely(skb->dev);\n\tstruct dst_entry *dst = skb_dst(skb);\n\tstruct ipv6hdr *hdr = ipv6_hdr(skb);\n\tstruct inet6_skb_parm *opt = IP6CB(skb);\n\tstruct net *net = dev_net(dst->dev);\n\tu32 mtu;\n\n\tif (net->ipv6.devconf_all->forwarding == 0)\n\t\tgoto error;\n\n\tif (skb->pkt_type != PACKET_HOST)\n\t\tgoto drop;\n\n\tif (unlikely(skb->sk))\n\t\tgoto drop;\n\n\tif (skb_warn_if_lro(skb))\n\t\tgoto drop;\n\n\tif (!xfrm6_policy_check(NULL, XFRM_POLICY_FWD, skb)) {\n\t\t__IP6_INC_STATS(net, idev, IPSTATS_MIB_INDISCARDS);\n\t\tgoto drop;\n\t}\n\n\tskb_forward_csum(skb);\n\n\t/*\n\t *\tWe DO NOT make any processing on\n\t *\tRA packets, pushing them to user level AS IS\n\t *\twithout ane WARRANTY that application will be able\n\t *\tto interpret them. The reason is that we\n\t *\tcannot make anything clever here.\n\t *\n\t *\tWe are not end-node, so that if packet contains\n\t *\tAH/ESP, we cannot make anything.\n\t *\tDefragmentation also would be mistake, RA packets\n\t *\tcannot be fragmented, because there is no warranty\n\t *\tthat different fragments will go along one path. --ANK\n\t */\n\tif (unlikely(opt->flags & IP6SKB_ROUTERALERT)) {\n\t\tif (ip6_call_ra_chain(skb, ntohs(opt->ra)))\n\t\t\treturn 0;\n\t}\n\n\t/*\n\t *\tcheck and decrement ttl\n\t */\n\tif (hdr->hop_limit <= 1) {\n\t\ticmpv6_send(skb, ICMPV6_TIME_EXCEED, ICMPV6_EXC_HOPLIMIT, 0);\n\t\t__IP6_INC_STATS(net, idev, IPSTATS_MIB_INHDRERRORS);\n\n\t\tkfree_skb(skb);\n\t\treturn -ETIMEDOUT;\n\t}\n\n\t/* XXX: idev->cnf.proxy_ndp? */\n\tif (net->ipv6.devconf_all->proxy_ndp &&\n\t    pneigh_lookup(&nd_tbl, net, &hdr->daddr, skb->dev, 0)) {\n\t\tint proxied = ip6_forward_proxy_check(skb);\n\t\tif (proxied > 0)\n\t\t\treturn ip6_input(skb);\n\t\telse if (proxied < 0) {\n\t\t\t__IP6_INC_STATS(net, idev, IPSTATS_MIB_INDISCARDS);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (!xfrm6_route_forward(skb)) {\n\t\t__IP6_INC_STATS(net, idev, IPSTATS_MIB_INDISCARDS);\n\t\tgoto drop;\n\t}\n\tdst = skb_dst(skb);\n\n\t/* IPv6 specs say nothing about it, but it is clear that we cannot\n\t   send redirects to source routed frames.\n\t   We don't send redirects to frames decapsulated from IPsec.\n\t */\n\tif (IP6CB(skb)->iif == dst->dev->ifindex &&\n\t    opt->srcrt == 0 && !skb_sec_path(skb)) {\n\t\tstruct in6_addr *target = NULL;\n\t\tstruct inet_peer *peer;\n\t\tstruct rt6_info *rt;\n\n\t\t/*\n\t\t *\tincoming and outgoing devices are the same\n\t\t *\tsend a redirect.\n\t\t */\n\n\t\trt = (struct rt6_info *) dst;\n\t\tif (rt->rt6i_flags & RTF_GATEWAY)\n\t\t\ttarget = &rt->rt6i_gateway;\n\t\telse\n\t\t\ttarget = &hdr->daddr;\n\n\t\tpeer = inet_getpeer_v6(net->ipv6.peers, &hdr->daddr, 1);\n\n\t\t/* Limit redirects both by destination (here)\n\t\t   and by source (inside ndisc_send_redirect)\n\t\t */\n\t\tif (inet_peer_xrlim_allow(peer, 1*HZ))\n\t\t\tndisc_send_redirect(skb, target);\n\t\tif (peer)\n\t\t\tinet_putpeer(peer);\n\t} else {\n\t\tint addrtype = ipv6_addr_type(&hdr->saddr);\n\n\t\t/* This check is security critical. */\n\t\tif (addrtype == IPV6_ADDR_ANY ||\n\t\t    addrtype & (IPV6_ADDR_MULTICAST | IPV6_ADDR_LOOPBACK))\n\t\t\tgoto error;\n\t\tif (addrtype & IPV6_ADDR_LINKLOCAL) {\n\t\t\ticmpv6_send(skb, ICMPV6_DEST_UNREACH,\n\t\t\t\t    ICMPV6_NOT_NEIGHBOUR, 0);\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\tmtu = ip6_dst_mtu_forward(dst);\n\tif (mtu < IPV6_MIN_MTU)\n\t\tmtu = IPV6_MIN_MTU;\n\n\tif (ip6_pkt_too_big(skb, mtu)) {\n\t\t/* Again, force OUTPUT device used as source address */\n\t\tskb->dev = dst->dev;\n\t\ticmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);\n\t\t__IP6_INC_STATS(net, idev, IPSTATS_MIB_INTOOBIGERRORS);\n\t\t__IP6_INC_STATS(net, ip6_dst_idev(dst),\n\t\t\t\tIPSTATS_MIB_FRAGFAILS);\n\t\tkfree_skb(skb);\n\t\treturn -EMSGSIZE;\n\t}\n\n\tif (skb_cow(skb, dst->dev->hard_header_len)) {\n\t\t__IP6_INC_STATS(net, ip6_dst_idev(dst),\n\t\t\t\tIPSTATS_MIB_OUTDISCARDS);\n\t\tgoto drop;\n\t}\n\n\thdr = ipv6_hdr(skb);\n\n\t/* Mangling hops number delayed to point after skb COW */\n\n\thdr->hop_limit--;\n\n\treturn NF_HOOK(NFPROTO_IPV6, NF_INET_FORWARD,\n\t\t       net, NULL, skb, skb->dev, dst->dev,\n\t\t       ip6_forward_finish);\n\nerror:\n\t__IP6_INC_STATS(net, idev, IPSTATS_MIB_INADDRERRORS);\ndrop:\n\tkfree_skb(skb);\n\treturn -EINVAL;\n}\n\nstatic void ip6_copy_metadata(struct sk_buff *to, struct sk_buff *from)\n{\n\tto->pkt_type = from->pkt_type;\n\tto->priority = from->priority;\n\tto->protocol = from->protocol;\n\tskb_dst_drop(to);\n\tskb_dst_set(to, dst_clone(skb_dst(from)));\n\tto->dev = from->dev;\n\tto->mark = from->mark;\n\n\tskb_copy_hash(to, from);\n\n#ifdef CONFIG_NET_SCHED\n\tto->tc_index = from->tc_index;\n#endif\n\tnf_copy(to, from);\n\tskb_ext_copy(to, from);\n\tskb_copy_secmark(to, from);\n}\n\nint ip6_fraglist_init(struct sk_buff *skb, unsigned int hlen, u8 *prevhdr,\n\t\t      u8 nexthdr, __be32 frag_id,\n\t\t      struct ip6_fraglist_iter *iter)\n{\n\tunsigned int first_len;\n\tstruct frag_hdr *fh;\n\n\t/* BUILD HEADER */\n\t*prevhdr = NEXTHDR_FRAGMENT;\n\titer->tmp_hdr = kmemdup(skb_network_header(skb), hlen, GFP_ATOMIC);\n\tif (!iter->tmp_hdr)\n\t\treturn -ENOMEM;\n\n\titer->frag = skb_shinfo(skb)->frag_list;\n\tskb_frag_list_init(skb);\n\n\titer->offset = 0;\n\titer->hlen = hlen;\n\titer->frag_id = frag_id;\n\titer->nexthdr = nexthdr;\n\n\t__skb_pull(skb, hlen);\n\tfh = __skb_push(skb, sizeof(struct frag_hdr));\n\t__skb_push(skb, hlen);\n\tskb_reset_network_header(skb);\n\tmemcpy(skb_network_header(skb), iter->tmp_hdr, hlen);\n\n\tfh->nexthdr = nexthdr;\n\tfh->reserved = 0;\n\tfh->frag_off = htons(IP6_MF);\n\tfh->identification = frag_id;\n\n\tfirst_len = skb_pagelen(skb);\n\tskb->data_len = first_len - skb_headlen(skb);\n\tskb->len = first_len;\n\tipv6_hdr(skb)->payload_len = htons(first_len - sizeof(struct ipv6hdr));\n\n\treturn 0;\n}\nEXPORT_SYMBOL(ip6_fraglist_init);\n\nvoid ip6_fraglist_prepare(struct sk_buff *skb,\n\t\t\t  struct ip6_fraglist_iter *iter)\n{\n\tstruct sk_buff *frag = iter->frag;\n\tunsigned int hlen = iter->hlen;\n\tstruct frag_hdr *fh;\n\n\tfrag->ip_summed = CHECKSUM_NONE;\n\tskb_reset_transport_header(frag);\n\tfh = __skb_push(frag, sizeof(struct frag_hdr));\n\t__skb_push(frag, hlen);\n\tskb_reset_network_header(frag);\n\tmemcpy(skb_network_header(frag), iter->tmp_hdr, hlen);\n\titer->offset += skb->len - hlen - sizeof(struct frag_hdr);\n\tfh->nexthdr = iter->nexthdr;\n\tfh->reserved = 0;\n\tfh->frag_off = htons(iter->offset);\n\tif (frag->next)\n\t\tfh->frag_off |= htons(IP6_MF);\n\tfh->identification = iter->frag_id;\n\tipv6_hdr(frag)->payload_len = htons(frag->len - sizeof(struct ipv6hdr));\n\tip6_copy_metadata(frag, skb);\n}\nEXPORT_SYMBOL(ip6_fraglist_prepare);\n\nvoid ip6_frag_init(struct sk_buff *skb, unsigned int hlen, unsigned int mtu,\n\t\t   unsigned short needed_tailroom, int hdr_room, u8 *prevhdr,\n\t\t   u8 nexthdr, __be32 frag_id, struct ip6_frag_state *state)\n{\n\tstate->prevhdr = prevhdr;\n\tstate->nexthdr = nexthdr;\n\tstate->frag_id = frag_id;\n\n\tstate->hlen = hlen;\n\tstate->mtu = mtu;\n\n\tstate->left = skb->len - hlen;\t/* Space per frame */\n\tstate->ptr = hlen;\t\t/* Where to start from */\n\n\tstate->hroom = hdr_room;\n\tstate->troom = needed_tailroom;\n\n\tstate->offset = 0;\n}\nEXPORT_SYMBOL(ip6_frag_init);\n\nstruct sk_buff *ip6_frag_next(struct sk_buff *skb, struct ip6_frag_state *state)\n{\n\tu8 *prevhdr = state->prevhdr, *fragnexthdr_offset;\n\tstruct sk_buff *frag;\n\tstruct frag_hdr *fh;\n\tunsigned int len;\n\n\tlen = state->left;\n\t/* IF: it doesn't fit, use 'mtu' - the data space left */\n\tif (len > state->mtu)\n\t\tlen = state->mtu;\n\t/* IF: we are not sending up to and including the packet end\n\t   then align the next start on an eight byte boundary */\n\tif (len < state->left)\n\t\tlen &= ~7;\n\n\t/* Allocate buffer */\n\tfrag = alloc_skb(len + state->hlen + sizeof(struct frag_hdr) +\n\t\t\t state->hroom + state->troom, GFP_ATOMIC);\n\tif (!frag)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t/*\n\t *\tSet up data on packet\n\t */\n\n\tip6_copy_metadata(frag, skb);\n\tskb_reserve(frag, state->hroom);\n\tskb_put(frag, len + state->hlen + sizeof(struct frag_hdr));\n\tskb_reset_network_header(frag);\n\tfh = (struct frag_hdr *)(skb_network_header(frag) + state->hlen);\n\tfrag->transport_header = (frag->network_header + state->hlen +\n\t\t\t\t  sizeof(struct frag_hdr));\n\n\t/*\n\t *\tCharge the memory for the fragment to any owner\n\t *\tit might possess\n\t */\n\tif (skb->sk)\n\t\tskb_set_owner_w(frag, skb->sk);\n\n\t/*\n\t *\tCopy the packet header into the new buffer.\n\t */\n\tskb_copy_from_linear_data(skb, skb_network_header(frag), state->hlen);\n\n\tfragnexthdr_offset = skb_network_header(frag);\n\tfragnexthdr_offset += prevhdr - skb_network_header(skb);\n\t*fragnexthdr_offset = NEXTHDR_FRAGMENT;\n\n\t/*\n\t *\tBuild fragment header.\n\t */\n\tfh->nexthdr = state->nexthdr;\n\tfh->reserved = 0;\n\tfh->identification = state->frag_id;\n\n\t/*\n\t *\tCopy a block of the IP datagram.\n\t */\n\tBUG_ON(skb_copy_bits(skb, state->ptr, skb_transport_header(frag),\n\t\t\t     len));\n\tstate->left -= len;\n\n\tfh->frag_off = htons(state->offset);\n\tif (state->left > 0)\n\t\tfh->frag_off |= htons(IP6_MF);\n\tipv6_hdr(frag)->payload_len = htons(frag->len - sizeof(struct ipv6hdr));\n\n\tstate->ptr += len;\n\tstate->offset += len;\n\n\treturn frag;\n}\nEXPORT_SYMBOL(ip6_frag_next);\n\nint ip6_fragment(struct net *net, struct sock *sk, struct sk_buff *skb,\n\t\t int (*output)(struct net *, struct sock *, struct sk_buff *))\n{\n\tstruct sk_buff *frag;\n\tstruct rt6_info *rt = (struct rt6_info *)skb_dst(skb);\n\tstruct ipv6_pinfo *np = skb->sk && !dev_recursion_level() ?\n\t\t\t\tinet6_sk(skb->sk) : NULL;\n\tstruct ip6_frag_state state;\n\tunsigned int mtu, hlen, nexthdr_offset;\n\tktime_t tstamp = skb->tstamp;\n\tint hroom, err = 0;\n\t__be32 frag_id;\n\tu8 *prevhdr, nexthdr = 0;\n\n\terr = ip6_find_1stfragopt(skb, &prevhdr);\n\tif (err < 0)\n\t\tgoto fail;\n\thlen = err;\n\tnexthdr = *prevhdr;\n\tnexthdr_offset = prevhdr - skb_network_header(skb);\n\n\tmtu = ip6_skb_dst_mtu(skb);\n\n\t/* We must not fragment if the socket is set to force MTU discovery\n\t * or if the skb it not generated by a local socket.\n\t */\n\tif (unlikely(!skb->ignore_df && skb->len > mtu))\n\t\tgoto fail_toobig;\n\n\tif (IP6CB(skb)->frag_max_size) {\n\t\tif (IP6CB(skb)->frag_max_size > mtu)\n\t\t\tgoto fail_toobig;\n\n\t\t/* don't send fragments larger than what we received */\n\t\tmtu = IP6CB(skb)->frag_max_size;\n\t\tif (mtu < IPV6_MIN_MTU)\n\t\t\tmtu = IPV6_MIN_MTU;\n\t}\n\n\tif (np && np->frag_size < mtu) {\n\t\tif (np->frag_size)\n\t\t\tmtu = np->frag_size;\n\t}\n\tif (mtu < hlen + sizeof(struct frag_hdr) + 8)\n\t\tgoto fail_toobig;\n\tmtu -= hlen + sizeof(struct frag_hdr);\n\n\tfrag_id = ipv6_select_ident(net, &ipv6_hdr(skb)->daddr,\n\t\t\t\t    &ipv6_hdr(skb)->saddr);\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL &&\n\t    (err = skb_checksum_help(skb)))\n\t\tgoto fail;\n\n\tprevhdr = skb_network_header(skb) + nexthdr_offset;\n\throom = LL_RESERVED_SPACE(rt->dst.dev);\n\tif (skb_has_frag_list(skb)) {\n\t\tunsigned int first_len = skb_pagelen(skb);\n\t\tstruct ip6_fraglist_iter iter;\n\t\tstruct sk_buff *frag2;\n\n\t\tif (first_len - hlen > mtu ||\n\t\t    ((first_len - hlen) & 7) ||\n\t\t    skb_cloned(skb) ||\n\t\t    skb_headroom(skb) < (hroom + sizeof(struct frag_hdr)))\n\t\t\tgoto slow_path;\n\n\t\tskb_walk_frags(skb, frag) {\n\t\t\t/* Correct geometry. */\n\t\t\tif (frag->len > mtu ||\n\t\t\t    ((frag->len & 7) && frag->next) ||\n\t\t\t    skb_headroom(frag) < (hlen + hroom + sizeof(struct frag_hdr)))\n\t\t\t\tgoto slow_path_clean;\n\n\t\t\t/* Partially cloned skb? */\n\t\t\tif (skb_shared(frag))\n\t\t\t\tgoto slow_path_clean;\n\n\t\t\tBUG_ON(frag->sk);\n\t\t\tif (skb->sk) {\n\t\t\t\tfrag->sk = skb->sk;\n\t\t\t\tfrag->destructor = sock_wfree;\n\t\t\t}\n\t\t\tskb->truesize -= frag->truesize;\n\t\t}\n\n\t\terr = ip6_fraglist_init(skb, hlen, prevhdr, nexthdr, frag_id,\n\t\t\t\t\t&iter);\n\t\tif (err < 0)\n\t\t\tgoto fail;\n\n\t\tfor (;;) {\n\t\t\t/* Prepare header of the next frame,\n\t\t\t * before previous one went down. */\n\t\t\tif (iter.frag)\n\t\t\t\tip6_fraglist_prepare(skb, &iter);\n\n\t\t\tskb->tstamp = tstamp;\n\t\t\terr = output(net, sk, skb);\n\t\t\tif (!err)\n\t\t\t\tIP6_INC_STATS(net, ip6_dst_idev(&rt->dst),\n\t\t\t\t\t      IPSTATS_MIB_FRAGCREATES);\n\n\t\t\tif (err || !iter.frag)\n\t\t\t\tbreak;\n\n\t\t\tskb = ip6_fraglist_next(&iter);\n\t\t}\n\n\t\tkfree(iter.tmp_hdr);\n\n\t\tif (err == 0) {\n\t\t\tIP6_INC_STATS(net, ip6_dst_idev(&rt->dst),\n\t\t\t\t      IPSTATS_MIB_FRAGOKS);\n\t\t\treturn 0;\n\t\t}\n\n\t\tkfree_skb_list(iter.frag);\n\n\t\tIP6_INC_STATS(net, ip6_dst_idev(&rt->dst),\n\t\t\t      IPSTATS_MIB_FRAGFAILS);\n\t\treturn err;\n\nslow_path_clean:\n\t\tskb_walk_frags(skb, frag2) {\n\t\t\tif (frag2 == frag)\n\t\t\t\tbreak;\n\t\t\tfrag2->sk = NULL;\n\t\t\tfrag2->destructor = NULL;\n\t\t\tskb->truesize += frag2->truesize;\n\t\t}\n\t}\n\nslow_path:\n\t/*\n\t *\tFragment the datagram.\n\t */\n\n\tip6_frag_init(skb, hlen, mtu, rt->dst.dev->needed_tailroom,\n\t\t      LL_RESERVED_SPACE(rt->dst.dev), prevhdr, nexthdr, frag_id,\n\t\t      &state);\n\n\t/*\n\t *\tKeep copying data until we run out.\n\t */\n\n\twhile (state.left > 0) {\n\t\tfrag = ip6_frag_next(skb, &state);\n\t\tif (IS_ERR(frag)) {\n\t\t\terr = PTR_ERR(frag);\n\t\t\tgoto fail;\n\t\t}\n\n\t\t/*\n\t\t *\tPut this fragment into the sending queue.\n\t\t */\n\t\tfrag->tstamp = tstamp;\n\t\terr = output(net, sk, frag);\n\t\tif (err)\n\t\t\tgoto fail;\n\n\t\tIP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t      IPSTATS_MIB_FRAGCREATES);\n\t}\n\tIP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t      IPSTATS_MIB_FRAGOKS);\n\tconsume_skb(skb);\n\treturn err;\n\nfail_toobig:\n\tif (skb->sk && dst_allfrag(skb_dst(skb)))\n\t\tsk_nocaps_add(skb->sk, NETIF_F_GSO_MASK);\n\n\ticmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);\n\terr = -EMSGSIZE;\n\nfail:\n\tIP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t      IPSTATS_MIB_FRAGFAILS);\n\tkfree_skb(skb);\n\treturn err;\n}\n\nstatic inline int ip6_rt_check(const struct rt6key *rt_key,\n\t\t\t       const struct in6_addr *fl_addr,\n\t\t\t       const struct in6_addr *addr_cache)\n{\n\treturn (rt_key->plen != 128 || !ipv6_addr_equal(fl_addr, &rt_key->addr)) &&\n\t\t(!addr_cache || !ipv6_addr_equal(fl_addr, addr_cache));\n}\n\nstatic struct dst_entry *ip6_sk_dst_check(struct sock *sk,\n\t\t\t\t\t  struct dst_entry *dst,\n\t\t\t\t\t  const struct flowi6 *fl6)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct rt6_info *rt;\n\n\tif (!dst)\n\t\tgoto out;\n\n\tif (dst->ops->family != AF_INET6) {\n\t\tdst_release(dst);\n\t\treturn NULL;\n\t}\n\n\trt = (struct rt6_info *)dst;\n\t/* Yes, checking route validity in not connected\n\t * case is not very simple. Take into account,\n\t * that we do not support routing by source, TOS,\n\t * and MSG_DONTROUTE\t\t--ANK (980726)\n\t *\n\t * 1. ip6_rt_check(): If route was host route,\n\t *    check that cached destination is current.\n\t *    If it is network route, we still may\n\t *    check its validity using saved pointer\n\t *    to the last used address: daddr_cache.\n\t *    We do not want to save whole address now,\n\t *    (because main consumer of this service\n\t *    is tcp, which has not this problem),\n\t *    so that the last trick works only on connected\n\t *    sockets.\n\t * 2. oif also should be the same.\n\t */\n\tif (ip6_rt_check(&rt->rt6i_dst, &fl6->daddr, np->daddr_cache) ||\n#ifdef CONFIG_IPV6_SUBTREES\n\t    ip6_rt_check(&rt->rt6i_src, &fl6->saddr, np->saddr_cache) ||\n#endif\n\t   (!(fl6->flowi6_flags & FLOWI_FLAG_SKIP_NH_OIF) &&\n\t      (fl6->flowi6_oif && fl6->flowi6_oif != dst->dev->ifindex))) {\n\t\tdst_release(dst);\n\t\tdst = NULL;\n\t}\n\nout:\n\treturn dst;\n}\n\nstatic int ip6_dst_lookup_tail(struct net *net, const struct sock *sk,\n\t\t\t       struct dst_entry **dst, struct flowi6 *fl6)\n{\n#ifdef CONFIG_IPV6_OPTIMISTIC_DAD\n\tstruct neighbour *n;\n\tstruct rt6_info *rt;\n#endif\n\tint err;\n\tint flags = 0;\n\n\t/* The correct way to handle this would be to do\n\t * ip6_route_get_saddr, and then ip6_route_output; however,\n\t * the route-specific preferred source forces the\n\t * ip6_route_output call _before_ ip6_route_get_saddr.\n\t *\n\t * In source specific routing (no src=any default route),\n\t * ip6_route_output will fail given src=any saddr, though, so\n\t * that's why we try it again later.\n\t */\n\tif (ipv6_addr_any(&fl6->saddr) && (!*dst || !(*dst)->error)) {\n\t\tstruct fib6_info *from;\n\t\tstruct rt6_info *rt;\n\t\tbool had_dst = *dst != NULL;\n\n\t\tif (!had_dst)\n\t\t\t*dst = ip6_route_output(net, sk, fl6);\n\t\trt = (*dst)->error ? NULL : (struct rt6_info *)*dst;\n\n\t\trcu_read_lock();\n\t\tfrom = rt ? rcu_dereference(rt->from) : NULL;\n\t\terr = ip6_route_get_saddr(net, from, &fl6->daddr,\n\t\t\t\t\t  sk ? inet6_sk(sk)->srcprefs : 0,\n\t\t\t\t\t  &fl6->saddr);\n\t\trcu_read_unlock();\n\n\t\tif (err)\n\t\t\tgoto out_err_release;\n\n\t\t/* If we had an erroneous initial result, pretend it\n\t\t * never existed and let the SA-enabled version take\n\t\t * over.\n\t\t */\n\t\tif (!had_dst && (*dst)->error) {\n\t\t\tdst_release(*dst);\n\t\t\t*dst = NULL;\n\t\t}\n\n\t\tif (fl6->flowi6_oif)\n\t\t\tflags |= RT6_LOOKUP_F_IFACE;\n\t}\n\n\tif (!*dst)\n\t\t*dst = ip6_route_output_flags(net, sk, fl6, flags);\n\n\terr = (*dst)->error;\n\tif (err)\n\t\tgoto out_err_release;\n\n#ifdef CONFIG_IPV6_OPTIMISTIC_DAD\n\t/*\n\t * Here if the dst entry we've looked up\n\t * has a neighbour entry that is in the INCOMPLETE\n\t * state and the src address from the flow is\n\t * marked as OPTIMISTIC, we release the found\n\t * dst entry and replace it instead with the\n\t * dst entry of the nexthop router\n\t */\n\trt = (struct rt6_info *) *dst;\n\trcu_read_lock_bh();\n\tn = __ipv6_neigh_lookup_noref(rt->dst.dev,\n\t\t\t\t      rt6_nexthop(rt, &fl6->daddr));\n\terr = n && !(n->nud_state & NUD_VALID) ? -EINVAL : 0;\n\trcu_read_unlock_bh();\n\n\tif (err) {\n\t\tstruct inet6_ifaddr *ifp;\n\t\tstruct flowi6 fl_gw6;\n\t\tint redirect;\n\n\t\tifp = ipv6_get_ifaddr(net, &fl6->saddr,\n\t\t\t\t      (*dst)->dev, 1);\n\n\t\tredirect = (ifp && ifp->flags & IFA_F_OPTIMISTIC);\n\t\tif (ifp)\n\t\t\tin6_ifa_put(ifp);\n\n\t\tif (redirect) {\n\t\t\t/*\n\t\t\t * We need to get the dst entry for the\n\t\t\t * default router instead\n\t\t\t */\n\t\t\tdst_release(*dst);\n\t\t\tmemcpy(&fl_gw6, fl6, sizeof(struct flowi6));\n\t\t\tmemset(&fl_gw6.daddr, 0, sizeof(struct in6_addr));\n\t\t\t*dst = ip6_route_output(net, sk, &fl_gw6);\n\t\t\terr = (*dst)->error;\n\t\t\tif (err)\n\t\t\t\tgoto out_err_release;\n\t\t}\n\t}\n#endif\n\tif (ipv6_addr_v4mapped(&fl6->saddr) &&\n\t    !(ipv6_addr_v4mapped(&fl6->daddr) || ipv6_addr_any(&fl6->daddr))) {\n\t\terr = -EAFNOSUPPORT;\n\t\tgoto out_err_release;\n\t}\n\n\treturn 0;\n\nout_err_release:\n\tdst_release(*dst);\n\t*dst = NULL;\n\n\tif (err == -ENETUNREACH)\n\t\tIP6_INC_STATS(net, NULL, IPSTATS_MIB_OUTNOROUTES);\n\treturn err;\n}\n\n/**\n *\tip6_dst_lookup - perform route lookup on flow\n *\t@net: Network namespace to perform lookup in\n *\t@sk: socket which provides route info\n *\t@dst: pointer to dst_entry * for result\n *\t@fl6: flow to lookup\n *\n *\tThis function performs a route lookup on the given flow.\n *\n *\tIt returns zero on success, or a standard errno code on error.\n */\nint ip6_dst_lookup(struct net *net, struct sock *sk, struct dst_entry **dst,\n\t\t   struct flowi6 *fl6)\n{\n\t*dst = NULL;\n\treturn ip6_dst_lookup_tail(net, sk, dst, fl6);\n}\nEXPORT_SYMBOL_GPL(ip6_dst_lookup);\n\n/**\n *\tip6_dst_lookup_flow - perform route lookup on flow with ipsec\n *\t@net: Network namespace to perform lookup in\n *\t@sk: socket which provides route info\n *\t@fl6: flow to lookup\n *\t@final_dst: final destination address for ipsec lookup\n *\n *\tThis function performs a route lookup on the given flow.\n *\n *\tIt returns a valid dst pointer on success, or a pointer encoded\n *\terror code.\n */\nstruct dst_entry *ip6_dst_lookup_flow(struct net *net, const struct sock *sk, struct flowi6 *fl6,\n\t\t\t\t      const struct in6_addr *final_dst)\n{\n\tstruct dst_entry *dst = NULL;\n\tint err;\n\n\terr = ip6_dst_lookup_tail(net, sk, &dst, fl6);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\tif (final_dst)\n\t\tfl6->daddr = *final_dst;\n\n\treturn xfrm_lookup_route(net, dst, flowi6_to_flowi(fl6), sk, 0);\n}\nEXPORT_SYMBOL_GPL(ip6_dst_lookup_flow);\n\n/**\n *\tip6_sk_dst_lookup_flow - perform socket cached route lookup on flow\n *\t@sk: socket which provides the dst cache and route info\n *\t@fl6: flow to lookup\n *\t@final_dst: final destination address for ipsec lookup\n *\t@connected: whether @sk is connected or not\n *\n *\tThis function performs a route lookup on the given flow with the\n *\tpossibility of using the cached route in the socket if it is valid.\n *\tIt will take the socket dst lock when operating on the dst cache.\n *\tAs a result, this function can only be used in process context.\n *\n *\tIn addition, for a connected socket, cache the dst in the socket\n *\tif the current cache is not valid.\n *\n *\tIt returns a valid dst pointer on success, or a pointer encoded\n *\terror code.\n */\nstruct dst_entry *ip6_sk_dst_lookup_flow(struct sock *sk, struct flowi6 *fl6,\n\t\t\t\t\t const struct in6_addr *final_dst,\n\t\t\t\t\t bool connected)\n{\n\tstruct dst_entry *dst = sk_dst_check(sk, inet6_sk(sk)->dst_cookie);\n\n\tdst = ip6_sk_dst_check(sk, dst, fl6);\n\tif (dst)\n\t\treturn dst;\n\n\tdst = ip6_dst_lookup_flow(sock_net(sk), sk, fl6, final_dst);\n\tif (connected && !IS_ERR(dst))\n\t\tip6_sk_dst_store_flow(sk, dst_clone(dst), fl6);\n\n\treturn dst;\n}\nEXPORT_SYMBOL_GPL(ip6_sk_dst_lookup_flow);\n\n/**\n *      ip6_dst_lookup_tunnel - perform route lookup on tunnel\n *      @skb: Packet for which lookup is done\n *      @dev: Tunnel device\n *      @net: Network namespace of tunnel device\n *      @sock: Socket which provides route info\n *      @saddr: Memory to store the src ip address\n *      @info: Tunnel information\n *      @protocol: IP protocol\n *      @use_cache: Flag to enable cache usage\n *      This function performs a route lookup on a tunnel\n *\n *      It returns a valid dst pointer and stores src address to be used in\n *      tunnel in param saddr on success, else a pointer encoded error code.\n */\n\nstruct dst_entry *ip6_dst_lookup_tunnel(struct sk_buff *skb,\n\t\t\t\t\tstruct net_device *dev,\n\t\t\t\t\tstruct net *net,\n\t\t\t\t\tstruct socket *sock,\n\t\t\t\t\tstruct in6_addr *saddr,\n\t\t\t\t\tconst struct ip_tunnel_info *info,\n\t\t\t\t\tu8 protocol,\n\t\t\t\t\tbool use_cache)\n{\n\tstruct dst_entry *dst = NULL;\n#ifdef CONFIG_DST_CACHE\n\tstruct dst_cache *dst_cache;\n#endif\n\tstruct flowi6 fl6;\n\t__u8 prio;\n\n#ifdef CONFIG_DST_CACHE\n\tdst_cache = (struct dst_cache *)&info->dst_cache;\n\tif (use_cache) {\n\t\tdst = dst_cache_get_ip6(dst_cache, saddr);\n\t\tif (dst)\n\t\t\treturn dst;\n\t}\n#endif\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_mark = skb->mark;\n\tfl6.flowi6_proto = protocol;\n\tfl6.daddr = info->key.u.ipv6.dst;\n\tfl6.saddr = info->key.u.ipv6.src;\n\tprio = info->key.tos;\n\tfl6.flowlabel = ip6_make_flowinfo(RT_TOS(prio),\n\t\t\t\t\t  info->key.label);\n\n\tdst = ipv6_stub->ipv6_dst_lookup_flow(net, sock->sk, &fl6,\n\t\t\t\t\t      NULL);\n\tif (IS_ERR(dst)) {\n\t\tnetdev_dbg(dev, \"no route to %pI6\\n\", &fl6.daddr);\n\t\treturn ERR_PTR(-ENETUNREACH);\n\t}\n\tif (dst->dev == dev) { /* is this necessary? */\n\t\tnetdev_dbg(dev, \"circular route to %pI6\\n\", &fl6.daddr);\n\t\tdst_release(dst);\n\t\treturn ERR_PTR(-ELOOP);\n\t}\n#ifdef CONFIG_DST_CACHE\n\tif (use_cache)\n\t\tdst_cache_set_ip6(dst_cache, dst, &fl6.saddr);\n#endif\n\t*saddr = fl6.saddr;\n\treturn dst;\n}\nEXPORT_SYMBOL_GPL(ip6_dst_lookup_tunnel);\n\nstatic inline struct ipv6_opt_hdr *ip6_opt_dup(struct ipv6_opt_hdr *src,\n\t\t\t\t\t       gfp_t gfp)\n{\n\treturn src ? kmemdup(src, (src->hdrlen + 1) * 8, gfp) : NULL;\n}\n\nstatic inline struct ipv6_rt_hdr *ip6_rthdr_dup(struct ipv6_rt_hdr *src,\n\t\t\t\t\t\tgfp_t gfp)\n{\n\treturn src ? kmemdup(src, (src->hdrlen + 1) * 8, gfp) : NULL;\n}\n\nstatic void ip6_append_data_mtu(unsigned int *mtu,\n\t\t\t\tint *maxfraglen,\n\t\t\t\tunsigned int fragheaderlen,\n\t\t\t\tstruct sk_buff *skb,\n\t\t\t\tstruct rt6_info *rt,\n\t\t\t\tunsigned int orig_mtu)\n{\n\tif (!(rt->dst.flags & DST_XFRM_TUNNEL)) {\n\t\tif (!skb) {\n\t\t\t/* first fragment, reserve header_len */\n\t\t\t*mtu = orig_mtu - rt->dst.header_len;\n\n\t\t} else {\n\t\t\t/*\n\t\t\t * this fragment is not first, the headers\n\t\t\t * space is regarded as data space.\n\t\t\t */\n\t\t\t*mtu = orig_mtu;\n\t\t}\n\t\t*maxfraglen = ((*mtu - fragheaderlen) & ~7)\n\t\t\t      + fragheaderlen - sizeof(struct frag_hdr);\n\t}\n}\n\nstatic int ip6_setup_cork(struct sock *sk, struct inet_cork_full *cork,\n\t\t\t  struct inet6_cork *v6_cork, struct ipcm6_cookie *ipc6,\n\t\t\t  struct rt6_info *rt, struct flowi6 *fl6)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tunsigned int mtu;\n\tstruct ipv6_txoptions *opt = ipc6->opt;\n\n\t/*\n\t * setup for corking\n\t */\n\tif (opt) {\n\t\tif (WARN_ON(v6_cork->opt))\n\t\t\treturn -EINVAL;\n\n\t\tv6_cork->opt = kzalloc(sizeof(*opt), sk->sk_allocation);\n\t\tif (unlikely(!v6_cork->opt))\n\t\t\treturn -ENOBUFS;\n\n\t\tv6_cork->opt->tot_len = sizeof(*opt);\n\t\tv6_cork->opt->opt_flen = opt->opt_flen;\n\t\tv6_cork->opt->opt_nflen = opt->opt_nflen;\n\n\t\tv6_cork->opt->dst0opt = ip6_opt_dup(opt->dst0opt,\n\t\t\t\t\t\t    sk->sk_allocation);\n\t\tif (opt->dst0opt && !v6_cork->opt->dst0opt)\n\t\t\treturn -ENOBUFS;\n\n\t\tv6_cork->opt->dst1opt = ip6_opt_dup(opt->dst1opt,\n\t\t\t\t\t\t    sk->sk_allocation);\n\t\tif (opt->dst1opt && !v6_cork->opt->dst1opt)\n\t\t\treturn -ENOBUFS;\n\n\t\tv6_cork->opt->hopopt = ip6_opt_dup(opt->hopopt,\n\t\t\t\t\t\t   sk->sk_allocation);\n\t\tif (opt->hopopt && !v6_cork->opt->hopopt)\n\t\t\treturn -ENOBUFS;\n\n\t\tv6_cork->opt->srcrt = ip6_rthdr_dup(opt->srcrt,\n\t\t\t\t\t\t    sk->sk_allocation);\n\t\tif (opt->srcrt && !v6_cork->opt->srcrt)\n\t\t\treturn -ENOBUFS;\n\n\t\t/* need source address above miyazawa*/\n\t}\n\tdst_hold(&rt->dst);\n\tcork->base.dst = &rt->dst;\n\tcork->fl.u.ip6 = *fl6;\n\tv6_cork->hop_limit = ipc6->hlimit;\n\tv6_cork->tclass = ipc6->tclass;\n\tif (rt->dst.flags & DST_XFRM_TUNNEL)\n\t\tmtu = np->pmtudisc >= IPV6_PMTUDISC_PROBE ?\n\t\t      READ_ONCE(rt->dst.dev->mtu) : dst_mtu(&rt->dst);\n\telse\n\t\tmtu = np->pmtudisc >= IPV6_PMTUDISC_PROBE ?\n\t\t\tREAD_ONCE(rt->dst.dev->mtu) : dst_mtu(xfrm_dst_path(&rt->dst));\n\tif (np->frag_size < mtu) {\n\t\tif (np->frag_size)\n\t\t\tmtu = np->frag_size;\n\t}\n\tif (mtu < IPV6_MIN_MTU)\n\t\treturn -EINVAL;\n\tcork->base.fragsize = mtu;\n\tcork->base.gso_size = ipc6->gso_size;\n\tcork->base.tx_flags = 0;\n\tcork->base.mark = ipc6->sockc.mark;\n\tsock_tx_timestamp(sk, ipc6->sockc.tsflags, &cork->base.tx_flags);\n\n\tif (dst_allfrag(xfrm_dst_path(&rt->dst)))\n\t\tcork->base.flags |= IPCORK_ALLFRAG;\n\tcork->base.length = 0;\n\n\tcork->base.transmit_time = ipc6->sockc.transmit_time;\n\n\treturn 0;\n}\n\nstatic int __ip6_append_data(struct sock *sk,\n\t\t\t     struct flowi6 *fl6,\n\t\t\t     struct sk_buff_head *queue,\n\t\t\t     struct inet_cork *cork,\n\t\t\t     struct inet6_cork *v6_cork,\n\t\t\t     struct page_frag *pfrag,\n\t\t\t     int getfrag(void *from, char *to, int offset,\n\t\t\t\t\t int len, int odd, struct sk_buff *skb),\n\t\t\t     void *from, int length, int transhdrlen,\n\t\t\t     unsigned int flags, struct ipcm6_cookie *ipc6)\n{\n\tstruct sk_buff *skb, *skb_prev = NULL;\n\tunsigned int maxfraglen, fragheaderlen, mtu, orig_mtu, pmtu;\n\tstruct ubuf_info *uarg = NULL;\n\tint exthdrlen = 0;\n\tint dst_exthdrlen = 0;\n\tint hh_len;\n\tint copy;\n\tint err;\n\tint offset = 0;\n\tu32 tskey = 0;\n\tstruct rt6_info *rt = (struct rt6_info *)cork->dst;\n\tstruct ipv6_txoptions *opt = v6_cork->opt;\n\tint csummode = CHECKSUM_NONE;\n\tunsigned int maxnonfragsize, headersize;\n\tunsigned int wmem_alloc_delta = 0;\n\tbool paged, extra_uref = false;\n\n\tskb = skb_peek_tail(queue);\n\tif (!skb) {\n\t\texthdrlen = opt ? opt->opt_flen : 0;\n\t\tdst_exthdrlen = rt->dst.header_len - rt->rt6i_nfheader_len;\n\t}\n\n\tpaged = !!cork->gso_size;\n\tmtu = cork->gso_size ? IP6_MAX_MTU : cork->fragsize;\n\torig_mtu = mtu;\n\n\tif (cork->tx_flags & SKBTX_ANY_SW_TSTAMP &&\n\t    sk->sk_tsflags & SOF_TIMESTAMPING_OPT_ID)\n\t\ttskey = sk->sk_tskey++;\n\n\thh_len = LL_RESERVED_SPACE(rt->dst.dev);\n\n\tfragheaderlen = sizeof(struct ipv6hdr) + rt->rt6i_nfheader_len +\n\t\t\t(opt ? opt->opt_nflen : 0);\n\tmaxfraglen = ((mtu - fragheaderlen) & ~7) + fragheaderlen -\n\t\t     sizeof(struct frag_hdr);\n\n\theadersize = sizeof(struct ipv6hdr) +\n\t\t     (opt ? opt->opt_flen + opt->opt_nflen : 0) +\n\t\t     (dst_allfrag(&rt->dst) ?\n\t\t      sizeof(struct frag_hdr) : 0) +\n\t\t     rt->rt6i_nfheader_len;\n\n\t/* as per RFC 7112 section 5, the entire IPv6 Header Chain must fit\n\t * the first fragment\n\t */\n\tif (headersize + transhdrlen > mtu)\n\t\tgoto emsgsize;\n\n\tif (cork->length + length > mtu - headersize && ipc6->dontfrag &&\n\t    (sk->sk_protocol == IPPROTO_UDP ||\n\t     sk->sk_protocol == IPPROTO_RAW)) {\n\t\tipv6_local_rxpmtu(sk, fl6, mtu - headersize +\n\t\t\t\tsizeof(struct ipv6hdr));\n\t\tgoto emsgsize;\n\t}\n\n\tif (ip6_sk_ignore_df(sk))\n\t\tmaxnonfragsize = sizeof(struct ipv6hdr) + IPV6_MAXPLEN;\n\telse\n\t\tmaxnonfragsize = mtu;\n\n\tif (cork->length + length > maxnonfragsize - headersize) {\nemsgsize:\n\t\tpmtu = max_t(int, mtu - headersize + sizeof(struct ipv6hdr), 0);\n\t\tipv6_local_error(sk, EMSGSIZE, fl6, pmtu);\n\t\treturn -EMSGSIZE;\n\t}\n\n\t/* CHECKSUM_PARTIAL only with no extension headers and when\n\t * we are not going to fragment\n\t */\n\tif (transhdrlen && sk->sk_protocol == IPPROTO_UDP &&\n\t    headersize == sizeof(struct ipv6hdr) &&\n\t    length <= mtu - headersize &&\n\t    (!(flags & MSG_MORE) || cork->gso_size) &&\n\t    rt->dst.dev->features & (NETIF_F_IPV6_CSUM | NETIF_F_HW_CSUM))\n\t\tcsummode = CHECKSUM_PARTIAL;\n\n\tif (flags & MSG_ZEROCOPY && length && sock_flag(sk, SOCK_ZEROCOPY)) {\n\t\tuarg = msg_zerocopy_realloc(sk, length, skb_zcopy(skb));\n\t\tif (!uarg)\n\t\t\treturn -ENOBUFS;\n\t\textra_uref = !skb_zcopy(skb);\t/* only ref on new uarg */\n\t\tif (rt->dst.dev->features & NETIF_F_SG &&\n\t\t    csummode == CHECKSUM_PARTIAL) {\n\t\t\tpaged = true;\n\t\t} else {\n\t\t\tuarg->zerocopy = 0;\n\t\t\tskb_zcopy_set(skb, uarg, &extra_uref);\n\t\t}\n\t}\n\n\t/*\n\t * Let's try using as much space as possible.\n\t * Use MTU if total length of the message fits into the MTU.\n\t * Otherwise, we need to reserve fragment header and\n\t * fragment alignment (= 8-15 octects, in total).\n\t *\n\t * Note that we may need to \"move\" the data from the tail\n\t * of the buffer to the new fragment when we split\n\t * the message.\n\t *\n\t * FIXME: It may be fragmented into multiple chunks\n\t *        at once if non-fragmentable extension headers\n\t *        are too large.\n\t * --yoshfuji\n\t */\n\n\tcork->length += length;\n\tif (!skb)\n\t\tgoto alloc_new_skb;\n\n\twhile (length > 0) {\n\t\t/* Check if the remaining data fits into current packet. */\n\t\tcopy = (cork->length <= mtu && !(cork->flags & IPCORK_ALLFRAG) ? mtu : maxfraglen) - skb->len;\n\t\tif (copy < length)\n\t\t\tcopy = maxfraglen - skb->len;\n\n\t\tif (copy <= 0) {\n\t\t\tchar *data;\n\t\t\tunsigned int datalen;\n\t\t\tunsigned int fraglen;\n\t\t\tunsigned int fraggap;\n\t\t\tunsigned int alloclen;\n\t\t\tunsigned int pagedlen;\nalloc_new_skb:\n\t\t\t/* There's no room in the current skb */\n\t\t\tif (skb)\n\t\t\t\tfraggap = skb->len - maxfraglen;\n\t\t\telse\n\t\t\t\tfraggap = 0;\n\t\t\t/* update mtu and maxfraglen if necessary */\n\t\t\tif (!skb || !skb_prev)\n\t\t\t\tip6_append_data_mtu(&mtu, &maxfraglen,\n\t\t\t\t\t\t    fragheaderlen, skb, rt,\n\t\t\t\t\t\t    orig_mtu);\n\n\t\t\tskb_prev = skb;\n\n\t\t\t/*\n\t\t\t * If remaining data exceeds the mtu,\n\t\t\t * we know we need more fragment(s).\n\t\t\t */\n\t\t\tdatalen = length + fraggap;\n\n\t\t\tif (datalen > (cork->length <= mtu && !(cork->flags & IPCORK_ALLFRAG) ? mtu : maxfraglen) - fragheaderlen)\n\t\t\t\tdatalen = maxfraglen - fragheaderlen - rt->dst.trailer_len;\n\t\t\tfraglen = datalen + fragheaderlen;\n\t\t\tpagedlen = 0;\n\n\t\t\tif ((flags & MSG_MORE) &&\n\t\t\t    !(rt->dst.dev->features&NETIF_F_SG))\n\t\t\t\talloclen = mtu;\n\t\t\telse if (!paged)\n\t\t\t\talloclen = fraglen;\n\t\t\telse {\n\t\t\t\talloclen = min_t(int, fraglen, MAX_HEADER);\n\t\t\t\tpagedlen = fraglen - alloclen;\n\t\t\t}\n\n\t\t\talloclen += dst_exthdrlen;\n\n\t\t\tif (datalen != length + fraggap) {\n\t\t\t\t/*\n\t\t\t\t * this is not the last fragment, the trailer\n\t\t\t\t * space is regarded as data space.\n\t\t\t\t */\n\t\t\t\tdatalen += rt->dst.trailer_len;\n\t\t\t}\n\n\t\t\talloclen += rt->dst.trailer_len;\n\t\t\tfraglen = datalen + fragheaderlen;\n\n\t\t\t/*\n\t\t\t * We just reserve space for fragment header.\n\t\t\t * Note: this may be overallocation if the message\n\t\t\t * (without MSG_MORE) fits into the MTU.\n\t\t\t */\n\t\t\talloclen += sizeof(struct frag_hdr);\n\n\t\t\tcopy = datalen - transhdrlen - fraggap - pagedlen;\n\t\t\tif (copy < 0) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (transhdrlen) {\n\t\t\t\tskb = sock_alloc_send_skb(sk,\n\t\t\t\t\t\talloclen + hh_len,\n\t\t\t\t\t\t(flags & MSG_DONTWAIT), &err);\n\t\t\t} else {\n\t\t\t\tskb = NULL;\n\t\t\t\tif (refcount_read(&sk->sk_wmem_alloc) + wmem_alloc_delta <=\n\t\t\t\t    2 * sk->sk_sndbuf)\n\t\t\t\t\tskb = alloc_skb(alloclen + hh_len,\n\t\t\t\t\t\t\tsk->sk_allocation);\n\t\t\t\tif (unlikely(!skb))\n\t\t\t\t\terr = -ENOBUFS;\n\t\t\t}\n\t\t\tif (!skb)\n\t\t\t\tgoto error;\n\t\t\t/*\n\t\t\t *\tFill in the control structures\n\t\t\t */\n\t\t\tskb->protocol = htons(ETH_P_IPV6);\n\t\t\tskb->ip_summed = csummode;\n\t\t\tskb->csum = 0;\n\t\t\t/* reserve for fragmentation and ipsec header */\n\t\t\tskb_reserve(skb, hh_len + sizeof(struct frag_hdr) +\n\t\t\t\t    dst_exthdrlen);\n\n\t\t\t/*\n\t\t\t *\tFind where to start putting bytes\n\t\t\t */\n\t\t\tdata = skb_put(skb, fraglen - pagedlen);\n\t\t\tskb_set_network_header(skb, exthdrlen);\n\t\t\tdata += fragheaderlen;\n\t\t\tskb->transport_header = (skb->network_header +\n\t\t\t\t\t\t fragheaderlen);\n\t\t\tif (fraggap) {\n\t\t\t\tskb->csum = skb_copy_and_csum_bits(\n\t\t\t\t\tskb_prev, maxfraglen,\n\t\t\t\t\tdata + transhdrlen, fraggap);\n\t\t\t\tskb_prev->csum = csum_sub(skb_prev->csum,\n\t\t\t\t\t\t\t  skb->csum);\n\t\t\t\tdata += fraggap;\n\t\t\t\tpskb_trim_unique(skb_prev, maxfraglen);\n\t\t\t}\n\t\t\tif (copy > 0 &&\n\t\t\t    getfrag(from, data + transhdrlen, offset,\n\t\t\t\t    copy, fraggap, skb) < 0) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tgoto error;\n\t\t\t}\n\n\t\t\toffset += copy;\n\t\t\tlength -= copy + transhdrlen;\n\t\t\ttranshdrlen = 0;\n\t\t\texthdrlen = 0;\n\t\t\tdst_exthdrlen = 0;\n\n\t\t\t/* Only the initial fragment is time stamped */\n\t\t\tskb_shinfo(skb)->tx_flags = cork->tx_flags;\n\t\t\tcork->tx_flags = 0;\n\t\t\tskb_shinfo(skb)->tskey = tskey;\n\t\t\ttskey = 0;\n\t\t\tskb_zcopy_set(skb, uarg, &extra_uref);\n\n\t\t\tif ((flags & MSG_CONFIRM) && !skb_prev)\n\t\t\t\tskb_set_dst_pending_confirm(skb, 1);\n\n\t\t\t/*\n\t\t\t * Put the packet on the pending queue\n\t\t\t */\n\t\t\tif (!skb->destructor) {\n\t\t\t\tskb->destructor = sock_wfree;\n\t\t\t\tskb->sk = sk;\n\t\t\t\twmem_alloc_delta += skb->truesize;\n\t\t\t}\n\t\t\t__skb_queue_tail(queue, skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (copy > length)\n\t\t\tcopy = length;\n\n\t\tif (!(rt->dst.dev->features&NETIF_F_SG) &&\n\t\t    skb_tailroom(skb) >= copy) {\n\t\t\tunsigned int off;\n\n\t\t\toff = skb->len;\n\t\t\tif (getfrag(from, skb_put(skb, copy),\n\t\t\t\t\t\toffset, copy, off, skb) < 0) {\n\t\t\t\t__skb_trim(skb, off);\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t} else if (!uarg || !uarg->zerocopy) {\n\t\t\tint i = skb_shinfo(skb)->nr_frags;\n\n\t\t\terr = -ENOMEM;\n\t\t\tif (!sk_page_frag_refill(sk, pfrag))\n\t\t\t\tgoto error;\n\n\t\t\tif (!skb_can_coalesce(skb, i, pfrag->page,\n\t\t\t\t\t      pfrag->offset)) {\n\t\t\t\terr = -EMSGSIZE;\n\t\t\t\tif (i == MAX_SKB_FRAGS)\n\t\t\t\t\tgoto error;\n\n\t\t\t\t__skb_fill_page_desc(skb, i, pfrag->page,\n\t\t\t\t\t\t     pfrag->offset, 0);\n\t\t\t\tskb_shinfo(skb)->nr_frags = ++i;\n\t\t\t\tget_page(pfrag->page);\n\t\t\t}\n\t\t\tcopy = min_t(int, copy, pfrag->size - pfrag->offset);\n\t\t\tif (getfrag(from,\n\t\t\t\t    page_address(pfrag->page) + pfrag->offset,\n\t\t\t\t    offset, copy, skb->len, skb) < 0)\n\t\t\t\tgoto error_efault;\n\n\t\t\tpfrag->offset += copy;\n\t\t\tskb_frag_size_add(&skb_shinfo(skb)->frags[i - 1], copy);\n\t\t\tskb->len += copy;\n\t\t\tskb->data_len += copy;\n\t\t\tskb->truesize += copy;\n\t\t\twmem_alloc_delta += copy;\n\t\t} else {\n\t\t\terr = skb_zerocopy_iter_dgram(skb, from, copy);\n\t\t\tif (err < 0)\n\t\t\t\tgoto error;\n\t\t}\n\t\toffset += copy;\n\t\tlength -= copy;\n\t}\n\n\tif (wmem_alloc_delta)\n\t\trefcount_add(wmem_alloc_delta, &sk->sk_wmem_alloc);\n\treturn 0;\n\nerror_efault:\n\terr = -EFAULT;\nerror:\n\tnet_zcopy_put_abort(uarg, extra_uref);\n\tcork->length -= length;\n\tIP6_INC_STATS(sock_net(sk), rt->rt6i_idev, IPSTATS_MIB_OUTDISCARDS);\n\trefcount_add(wmem_alloc_delta, &sk->sk_wmem_alloc);\n\treturn err;\n}\n\nint ip6_append_data(struct sock *sk,\n\t\t    int getfrag(void *from, char *to, int offset, int len,\n\t\t\t\tint odd, struct sk_buff *skb),\n\t\t    void *from, int length, int transhdrlen,\n\t\t    struct ipcm6_cookie *ipc6, struct flowi6 *fl6,\n\t\t    struct rt6_info *rt, unsigned int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tint exthdrlen;\n\tint err;\n\n\tif (flags&MSG_PROBE)\n\t\treturn 0;\n\tif (skb_queue_empty(&sk->sk_write_queue)) {\n\t\t/*\n\t\t * setup for corking\n\t\t */\n\t\terr = ip6_setup_cork(sk, &inet->cork, &np->cork,\n\t\t\t\t     ipc6, rt, fl6);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\texthdrlen = (ipc6->opt ? ipc6->opt->opt_flen : 0);\n\t\tlength += exthdrlen;\n\t\ttranshdrlen += exthdrlen;\n\t} else {\n\t\tfl6 = &inet->cork.fl.u.ip6;\n\t\ttranshdrlen = 0;\n\t}\n\n\treturn __ip6_append_data(sk, fl6, &sk->sk_write_queue, &inet->cork.base,\n\t\t\t\t &np->cork, sk_page_frag(sk), getfrag,\n\t\t\t\t from, length, transhdrlen, flags, ipc6);\n}\nEXPORT_SYMBOL_GPL(ip6_append_data);\n\nstatic void ip6_cork_release(struct inet_cork_full *cork,\n\t\t\t     struct inet6_cork *v6_cork)\n{\n\tif (v6_cork->opt) {\n\t\tkfree(v6_cork->opt->dst0opt);\n\t\tkfree(v6_cork->opt->dst1opt);\n\t\tkfree(v6_cork->opt->hopopt);\n\t\tkfree(v6_cork->opt->srcrt);\n\t\tkfree(v6_cork->opt);\n\t\tv6_cork->opt = NULL;\n\t}\n\n\tif (cork->base.dst) {\n\t\tdst_release(cork->base.dst);\n\t\tcork->base.dst = NULL;\n\t\tcork->base.flags &= ~IPCORK_ALLFRAG;\n\t}\n\tmemset(&cork->fl, 0, sizeof(cork->fl));\n}\n\nstruct sk_buff *__ip6_make_skb(struct sock *sk,\n\t\t\t       struct sk_buff_head *queue,\n\t\t\t       struct inet_cork_full *cork,\n\t\t\t       struct inet6_cork *v6_cork)\n{\n\tstruct sk_buff *skb, *tmp_skb;\n\tstruct sk_buff **tail_skb;\n\tstruct in6_addr final_dst_buf, *final_dst = &final_dst_buf;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct ipv6hdr *hdr;\n\tstruct ipv6_txoptions *opt = v6_cork->opt;\n\tstruct rt6_info *rt = (struct rt6_info *)cork->base.dst;\n\tstruct flowi6 *fl6 = &cork->fl.u.ip6;\n\tunsigned char proto = fl6->flowi6_proto;\n\n\tskb = __skb_dequeue(queue);\n\tif (!skb)\n\t\tgoto out;\n\ttail_skb = &(skb_shinfo(skb)->frag_list);\n\n\t/* move skb->data to ip header from ext header */\n\tif (skb->data < skb_network_header(skb))\n\t\t__skb_pull(skb, skb_network_offset(skb));\n\twhile ((tmp_skb = __skb_dequeue(queue)) != NULL) {\n\t\t__skb_pull(tmp_skb, skb_network_header_len(skb));\n\t\t*tail_skb = tmp_skb;\n\t\ttail_skb = &(tmp_skb->next);\n\t\tskb->len += tmp_skb->len;\n\t\tskb->data_len += tmp_skb->len;\n\t\tskb->truesize += tmp_skb->truesize;\n\t\ttmp_skb->destructor = NULL;\n\t\ttmp_skb->sk = NULL;\n\t}\n\n\t/* Allow local fragmentation. */\n\tskb->ignore_df = ip6_sk_ignore_df(sk);\n\n\t*final_dst = fl6->daddr;\n\t__skb_pull(skb, skb_network_header_len(skb));\n\tif (opt && opt->opt_flen)\n\t\tipv6_push_frag_opts(skb, opt, &proto);\n\tif (opt && opt->opt_nflen)\n\t\tipv6_push_nfrag_opts(skb, opt, &proto, &final_dst, &fl6->saddr);\n\n\tskb_push(skb, sizeof(struct ipv6hdr));\n\tskb_reset_network_header(skb);\n\thdr = ipv6_hdr(skb);\n\n\tip6_flow_hdr(hdr, v6_cork->tclass,\n\t\t     ip6_make_flowlabel(net, skb, fl6->flowlabel,\n\t\t\t\t\tip6_autoflowlabel(net, np), fl6));\n\thdr->hop_limit = v6_cork->hop_limit;\n\thdr->nexthdr = proto;\n\thdr->saddr = fl6->saddr;\n\thdr->daddr = *final_dst;\n\n\tskb->priority = sk->sk_priority;\n\tskb->mark = cork->base.mark;\n\n\tskb->tstamp = cork->base.transmit_time;\n\n\tskb_dst_set(skb, dst_clone(&rt->dst));\n\tIP6_UPD_PO_STATS(net, rt->rt6i_idev, IPSTATS_MIB_OUT, skb->len);\n\tif (proto == IPPROTO_ICMPV6) {\n\t\tstruct inet6_dev *idev = ip6_dst_idev(skb_dst(skb));\n\n\t\tICMP6MSGOUT_INC_STATS(net, idev, icmp6_hdr(skb)->icmp6_type);\n\t\tICMP6_INC_STATS(net, idev, ICMP6_MIB_OUTMSGS);\n\t}\n\n\tip6_cork_release(cork, v6_cork);\nout:\n\treturn skb;\n}\n\nint ip6_send_skb(struct sk_buff *skb)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct rt6_info *rt = (struct rt6_info *)skb_dst(skb);\n\tint err;\n\n\terr = ip6_local_out(net, skb->sk, skb);\n\tif (err) {\n\t\tif (err > 0)\n\t\t\terr = net_xmit_errno(err);\n\t\tif (err)\n\t\t\tIP6_INC_STATS(net, rt->rt6i_idev,\n\t\t\t\t      IPSTATS_MIB_OUTDISCARDS);\n\t}\n\n\treturn err;\n}\n\nint ip6_push_pending_frames(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\n\tskb = ip6_finish_skb(sk);\n\tif (!skb)\n\t\treturn 0;\n\n\treturn ip6_send_skb(skb);\n}\nEXPORT_SYMBOL_GPL(ip6_push_pending_frames);\n\nstatic void __ip6_flush_pending_frames(struct sock *sk,\n\t\t\t\t       struct sk_buff_head *queue,\n\t\t\t\t       struct inet_cork_full *cork,\n\t\t\t\t       struct inet6_cork *v6_cork)\n{\n\tstruct sk_buff *skb;\n\n\twhile ((skb = __skb_dequeue_tail(queue)) != NULL) {\n\t\tif (skb_dst(skb))\n\t\t\tIP6_INC_STATS(sock_net(sk), ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t      IPSTATS_MIB_OUTDISCARDS);\n\t\tkfree_skb(skb);\n\t}\n\n\tip6_cork_release(cork, v6_cork);\n}\n\nvoid ip6_flush_pending_frames(struct sock *sk)\n{\n\t__ip6_flush_pending_frames(sk, &sk->sk_write_queue,\n\t\t\t\t   &inet_sk(sk)->cork, &inet6_sk(sk)->cork);\n}\nEXPORT_SYMBOL_GPL(ip6_flush_pending_frames);\n\nstruct sk_buff *ip6_make_skb(struct sock *sk,\n\t\t\t     int getfrag(void *from, char *to, int offset,\n\t\t\t\t\t int len, int odd, struct sk_buff *skb),\n\t\t\t     void *from, int length, int transhdrlen,\n\t\t\t     struct ipcm6_cookie *ipc6, struct flowi6 *fl6,\n\t\t\t     struct rt6_info *rt, unsigned int flags,\n\t\t\t     struct inet_cork_full *cork)\n{\n\tstruct inet6_cork v6_cork;\n\tstruct sk_buff_head queue;\n\tint exthdrlen = (ipc6->opt ? ipc6->opt->opt_flen : 0);\n\tint err;\n\n\tif (flags & MSG_PROBE)\n\t\treturn NULL;\n\n\t__skb_queue_head_init(&queue);\n\n\tcork->base.flags = 0;\n\tcork->base.addr = 0;\n\tcork->base.opt = NULL;\n\tcork->base.dst = NULL;\n\tv6_cork.opt = NULL;\n\terr = ip6_setup_cork(sk, cork, &v6_cork, ipc6, rt, fl6);\n\tif (err) {\n\t\tip6_cork_release(cork, &v6_cork);\n\t\treturn ERR_PTR(err);\n\t}\n\tif (ipc6->dontfrag < 0)\n\t\tipc6->dontfrag = inet6_sk(sk)->dontfrag;\n\n\terr = __ip6_append_data(sk, fl6, &queue, &cork->base, &v6_cork,\n\t\t\t\t&current->task_frag, getfrag, from,\n\t\t\t\tlength + exthdrlen, transhdrlen + exthdrlen,\n\t\t\t\tflags, ipc6);\n\tif (err) {\n\t\t__ip6_flush_pending_frames(sk, &queue, cork, &v6_cork);\n\t\treturn ERR_PTR(err);\n\t}\n\n\treturn __ip6_make_skb(sk, &queue, cork, &v6_cork);\n}\n"}, "16": {"id": 16, "path": "/src/net/ipv6/raw.c", "content": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n *\tRAW sockets for IPv6\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tAdapted from linux/net/ipv4/raw.c\n *\n *\tFixes:\n *\tHideaki YOSHIFUJI\t:\tsin6_scope_id support\n *\tYOSHIFUJI,H.@USAGI\t:\traw checksum (RFC2292(bis) compliance)\n *\tKazunori MIYAZAWA @USAGI:\tchange process style to use ip6_append_data\n */\n\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/slab.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/in6.h>\n#include <linux/netdevice.h>\n#include <linux/if_arp.h>\n#include <linux/icmpv6.h>\n#include <linux/netfilter.h>\n#include <linux/netfilter_ipv6.h>\n#include <linux/skbuff.h>\n#include <linux/compat.h>\n#include <linux/uaccess.h>\n#include <asm/ioctls.h>\n\n#include <net/net_namespace.h>\n#include <net/ip.h>\n#include <net/sock.h>\n#include <net/snmp.h>\n\n#include <net/ipv6.h>\n#include <net/ndisc.h>\n#include <net/protocol.h>\n#include <net/ip6_route.h>\n#include <net/ip6_checksum.h>\n#include <net/addrconf.h>\n#include <net/transp_v6.h>\n#include <net/udp.h>\n#include <net/inet_common.h>\n#include <net/tcp_states.h>\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n#include <net/mip6.h>\n#endif\n#include <linux/mroute6.h>\n\n#include <net/raw.h>\n#include <net/rawv6.h>\n#include <net/xfrm.h>\n\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/export.h>\n\n#define\tICMPV6_HDRLEN\t4\t/* ICMPv6 header, RFC 4443 Section 2.1 */\n\nstruct raw_hashinfo raw_v6_hashinfo = {\n\t.lock = __RW_LOCK_UNLOCKED(raw_v6_hashinfo.lock),\n};\nEXPORT_SYMBOL_GPL(raw_v6_hashinfo);\n\nstruct sock *__raw_v6_lookup(struct net *net, struct sock *sk,\n\t\tunsigned short num, const struct in6_addr *loc_addr,\n\t\tconst struct in6_addr *rmt_addr, int dif, int sdif)\n{\n\tbool is_multicast = ipv6_addr_is_multicast(loc_addr);\n\n\tsk_for_each_from(sk)\n\t\tif (inet_sk(sk)->inet_num == num) {\n\n\t\t\tif (!net_eq(sock_net(sk), net))\n\t\t\t\tcontinue;\n\n\t\t\tif (!ipv6_addr_any(&sk->sk_v6_daddr) &&\n\t\t\t    !ipv6_addr_equal(&sk->sk_v6_daddr, rmt_addr))\n\t\t\t\tcontinue;\n\n\t\t\tif (!raw_sk_bound_dev_eq(net, sk->sk_bound_dev_if,\n\t\t\t\t\t\t dif, sdif))\n\t\t\t\tcontinue;\n\n\t\t\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\t\tif (ipv6_addr_equal(&sk->sk_v6_rcv_saddr, loc_addr))\n\t\t\t\t\tgoto found;\n\t\t\t\tif (is_multicast &&\n\t\t\t\t    inet6_mc_check(sk, loc_addr, rmt_addr))\n\t\t\t\t\tgoto found;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tgoto found;\n\t\t}\n\tsk = NULL;\nfound:\n\treturn sk;\n}\nEXPORT_SYMBOL_GPL(__raw_v6_lookup);\n\n/*\n *\t0 - deliver\n *\t1 - block\n */\nstatic int icmpv6_filter(const struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct icmp6hdr _hdr;\n\tconst struct icmp6hdr *hdr;\n\n\t/* We require only the four bytes of the ICMPv6 header, not any\n\t * additional bytes of message body in \"struct icmp6hdr\".\n\t */\n\thdr = skb_header_pointer(skb, skb_transport_offset(skb),\n\t\t\t\t ICMPV6_HDRLEN, &_hdr);\n\tif (hdr) {\n\t\tconst __u32 *data = &raw6_sk(sk)->filter.data[0];\n\t\tunsigned int type = hdr->icmp6_type;\n\n\t\treturn (data[type >> 5] & (1U << (type & 31))) != 0;\n\t}\n\treturn 1;\n}\n\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\ntypedef int mh_filter_t(struct sock *sock, struct sk_buff *skb);\n\nstatic mh_filter_t __rcu *mh_filter __read_mostly;\n\nint rawv6_mh_filter_register(mh_filter_t filter)\n{\n\trcu_assign_pointer(mh_filter, filter);\n\treturn 0;\n}\nEXPORT_SYMBOL(rawv6_mh_filter_register);\n\nint rawv6_mh_filter_unregister(mh_filter_t filter)\n{\n\tRCU_INIT_POINTER(mh_filter, NULL);\n\tsynchronize_rcu();\n\treturn 0;\n}\nEXPORT_SYMBOL(rawv6_mh_filter_unregister);\n\n#endif\n\n/*\n *\tdemultiplex raw sockets.\n *\t(should consider queueing the skb in the sock receive_queue\n *\twithout calling rawv6.c)\n *\n *\tCaller owns SKB so we must make clones.\n */\nstatic bool ipv6_raw_deliver(struct sk_buff *skb, int nexthdr)\n{\n\tconst struct in6_addr *saddr;\n\tconst struct in6_addr *daddr;\n\tstruct sock *sk;\n\tbool delivered = false;\n\t__u8 hash;\n\tstruct net *net;\n\n\tsaddr = &ipv6_hdr(skb)->saddr;\n\tdaddr = saddr + 1;\n\n\thash = nexthdr & (RAW_HTABLE_SIZE - 1);\n\n\tread_lock(&raw_v6_hashinfo.lock);\n\tsk = sk_head(&raw_v6_hashinfo.ht[hash]);\n\n\tif (!sk)\n\t\tgoto out;\n\n\tnet = dev_net(skb->dev);\n\tsk = __raw_v6_lookup(net, sk, nexthdr, daddr, saddr,\n\t\t\t     inet6_iif(skb), inet6_sdif(skb));\n\n\twhile (sk) {\n\t\tint filtered;\n\n\t\tdelivered = true;\n\t\tswitch (nexthdr) {\n\t\tcase IPPROTO_ICMPV6:\n\t\t\tfiltered = icmpv6_filter(sk, skb);\n\t\t\tbreak;\n\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\tcase IPPROTO_MH:\n\t\t{\n\t\t\t/* XXX: To validate MH only once for each packet,\n\t\t\t * this is placed here. It should be after checking\n\t\t\t * xfrm policy, however it doesn't. The checking xfrm\n\t\t\t * policy is placed in rawv6_rcv() because it is\n\t\t\t * required for each socket.\n\t\t\t */\n\t\t\tmh_filter_t *filter;\n\n\t\t\tfilter = rcu_dereference(mh_filter);\n\t\t\tfiltered = filter ? (*filter)(sk, skb) : 0;\n\t\t\tbreak;\n\t\t}\n#endif\n\t\tdefault:\n\t\t\tfiltered = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (filtered < 0)\n\t\t\tbreak;\n\t\tif (filtered == 0) {\n\t\t\tstruct sk_buff *clone = skb_clone(skb, GFP_ATOMIC);\n\n\t\t\t/* Not releasing hash table! */\n\t\t\tif (clone) {\n\t\t\t\tnf_reset_ct(clone);\n\t\t\t\trawv6_rcv(sk, clone);\n\t\t\t}\n\t\t}\n\t\tsk = __raw_v6_lookup(net, sk_next(sk), nexthdr, daddr, saddr,\n\t\t\t\t     inet6_iif(skb), inet6_sdif(skb));\n\t}\nout:\n\tread_unlock(&raw_v6_hashinfo.lock);\n\treturn delivered;\n}\n\nbool raw6_local_deliver(struct sk_buff *skb, int nexthdr)\n{\n\tstruct sock *raw_sk;\n\n\traw_sk = sk_head(&raw_v6_hashinfo.ht[nexthdr & (RAW_HTABLE_SIZE - 1)]);\n\tif (raw_sk && !ipv6_raw_deliver(skb, nexthdr))\n\t\traw_sk = NULL;\n\n\treturn raw_sk != NULL;\n}\n\n/* This cleans up af_inet6 a bit. -DaveM */\nstatic int rawv6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_in6 *addr = (struct sockaddr_in6 *) uaddr;\n\t__be32 v4addr = 0;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (addr->sin6_family != AF_INET6)\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&addr->sin6_addr);\n\n\t/* Raw sockets are IPv6 only */\n\tif (addr_type == IPV6_ADDR_MAPPED)\n\t\treturn -EADDRNOTAVAIL;\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_CLOSE)\n\t\tgoto out;\n\n\trcu_read_lock();\n\t/* Check if the address belongs to the host. */\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t    addr->sin6_scope_id) {\n\t\t\t\t/* Override any existing binding, if another\n\t\t\t\t * one is supplied by user.\n\t\t\t\t */\n\t\t\t\tsk->sk_bound_dev_if = addr->sin6_scope_id;\n\t\t\t}\n\n\t\t\t/* Binding to link-local address requires an interface */\n\t\t\tif (!sk->sk_bound_dev_if)\n\t\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (sk->sk_bound_dev_if) {\n\t\t\terr = -ENODEV;\n\t\t\tdev = dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\t\tif (!dev)\n\t\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t */\n\t\tv4addr = LOOPBACK4_IPV6;\n\t\tif (!(addr_type & IPV6_ADDR_MULTICAST) &&\n\t\t    !sock_net(sk)->ipv6.sysctl.ip_nonlocal_bind) {\n\t\t\terr = -EADDRNOTAVAIL;\n\t\t\tif (!ipv6_chk_addr(sock_net(sk), &addr->sin6_addr,\n\t\t\t\t\t   dev, 0)) {\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t}\n\t}\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = v4addr;\n\tsk->sk_v6_rcv_saddr = addr->sin6_addr;\n\tif (!(addr_type & IPV6_ADDR_MULTICAST))\n\t\tnp->saddr = addr->sin6_addr;\n\terr = 0;\nout_unlock:\n\trcu_read_unlock();\nout:\n\trelease_sock(sk);\n\treturn err;\n}\n\nstatic void rawv6_err(struct sock *sk, struct sk_buff *skb,\n\t       struct inet6_skb_parm *opt,\n\t       u8 type, u8 code, int offset, __be32 info)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tint err;\n\tint harderr;\n\n\t/* Report error on raw socket, if:\n\t   1. User requested recverr.\n\t   2. Socket is connected (otherwise the error indication\n\t      is useless without recverr and error is hard.\n\t */\n\tif (!np->recverr && sk->sk_state != TCP_ESTABLISHED)\n\t\treturn;\n\n\tharderr = icmpv6_err_convert(type, code, &err);\n\tif (type == ICMPV6_PKT_TOOBIG) {\n\t\tip6_sk_update_pmtu(skb, sk, info);\n\t\tharderr = (np->pmtudisc == IPV6_PMTUDISC_DO);\n\t}\n\tif (type == NDISC_REDIRECT) {\n\t\tip6_sk_redirect(skb, sk);\n\t\treturn;\n\t}\n\tif (np->recverr) {\n\t\tu8 *payload = skb->data;\n\t\tif (!inet->hdrincl)\n\t\t\tpayload += offset;\n\t\tipv6_icmp_error(sk, skb, err, 0, ntohl(info), payload);\n\t}\n\n\tif (np->recverr || harderr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t}\n}\n\nvoid raw6_icmp_error(struct sk_buff *skb, int nexthdr,\n\t\tu8 type, u8 code, int inner_offset, __be32 info)\n{\n\tstruct sock *sk;\n\tint hash;\n\tconst struct in6_addr *saddr, *daddr;\n\tstruct net *net;\n\n\thash = nexthdr & (RAW_HTABLE_SIZE - 1);\n\n\tread_lock(&raw_v6_hashinfo.lock);\n\tsk = sk_head(&raw_v6_hashinfo.ht[hash]);\n\tif (sk) {\n\t\t/* Note: ipv6_hdr(skb) != skb->data */\n\t\tconst struct ipv6hdr *ip6h = (const struct ipv6hdr *)skb->data;\n\t\tsaddr = &ip6h->saddr;\n\t\tdaddr = &ip6h->daddr;\n\t\tnet = dev_net(skb->dev);\n\n\t\twhile ((sk = __raw_v6_lookup(net, sk, nexthdr, saddr, daddr,\n\t\t\t\t\t     inet6_iif(skb), inet6_iif(skb)))) {\n\t\t\trawv6_err(sk, skb, NULL, type, code,\n\t\t\t\t\tinner_offset, info);\n\t\t\tsk = sk_next(sk);\n\t\t}\n\t}\n\tread_unlock(&raw_v6_hashinfo.lock);\n}\n\nstatic inline int rawv6_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tif ((raw6_sk(sk)->checksum || rcu_access_pointer(sk->sk_filter)) &&\n\t    skb_checksum_complete(skb)) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\n\t/* Charge it to the socket. */\n\tskb_dst_drop(skb);\n\tif (sock_queue_rcv_skb(sk, skb) < 0) {\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\n\treturn 0;\n}\n\n/*\n *\tThis is next to useless...\n *\tif we demultiplex in network layer we don't need the extra call\n *\tjust to queue the skb...\n *\tmaybe we could have the network decide upon a hint if it\n *\tshould call raw_rcv for demultiplexing\n */\nint rawv6_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb)) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\n\tif (!rp->checksum)\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\tskb_postpull_rcsum(skb, skb_network_header(skb),\n\t\t\t\t   skb_network_header_len(skb));\n\t\tif (!csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t     &ipv6_hdr(skb)->daddr,\n\t\t\t\t     skb->len, inet->inet_num, skb->csum))\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t}\n\tif (!skb_csum_unnecessary(skb))\n\t\tskb->csum = ~csum_unfold(csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t\t\t\t &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t\t\t skb->len,\n\t\t\t\t\t\t\t inet->inet_num, 0));\n\n\tif (inet->hdrincl) {\n\t\tif (skb_checksum_complete(skb)) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tkfree_skb(skb);\n\t\t\treturn NET_RX_DROP;\n\t\t}\n\t}\n\n\trawv6_rcv_skb(sk, skb);\n\treturn 0;\n}\n\n\n/*\n *\tThis should be easy, if there is something there\n *\twe return it, otherwise we block.\n */\n\nstatic int rawv6_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,\n\t\t\t int noblock, int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct sk_buff *skb;\n\tsize_t copied;\n\tint err;\n\n\tif (flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len, addr_len);\n\n\tif (np->rxpmtu && np->rxopt.bits.rxpmtu)\n\t\treturn ipv6_recv_rxpmtu(sk, msg, len, addr_len);\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tcopied = len;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\tif (skb_csum_unnecessary(skb)) {\n\t\terr = skb_copy_datagram_msg(skb, 0, msg, copied);\n\t} else if (msg->msg_flags&MSG_TRUNC) {\n\t\tif (__skb_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t\terr = skb_copy_datagram_msg(skb, 0, msg, copied);\n\t} else {\n\t\terr = skb_copy_and_csum_datagram_msg(skb, 0, msg);\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (err)\n\t\tgoto out_free;\n\n\t/* Copy the address. */\n\tif (sin6) {\n\t\tsin6->sin6_family = AF_INET6;\n\t\tsin6->sin6_port = 0;\n\t\tsin6->sin6_addr = ipv6_hdr(skb)->saddr;\n\t\tsin6->sin6_flowinfo = 0;\n\t\tsin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,\n\t\t\t\t\t\t\t  inet6_iif(skb));\n\t\t*addr_len = sizeof(*sin6);\n\t}\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tif (np->rxopt.all)\n\t\tip6_datagram_recv_ctl(sk, msg, skb);\n\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = skb->len;\n\nout_free:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err;\n\ncsum_copy_err:\n\tskb_kill_datagram(sk, skb, flags);\n\n\t/* Error for blocking case is chosen to masquerade\n\t   as some normal condition.\n\t */\n\terr = (flags&MSG_DONTWAIT) ? -EAGAIN : -EHOSTUNREACH;\n\tgoto out;\n}\n\nstatic int rawv6_push_pending_frames(struct sock *sk, struct flowi6 *fl6,\n\t\t\t\t     struct raw6_sock *rp)\n{\n\tstruct sk_buff *skb;\n\tint err = 0;\n\tint offset;\n\tint len;\n\tint total_len;\n\t__wsum tmp_csum;\n\t__sum16 csum;\n\n\tif (!rp->checksum)\n\t\tgoto send;\n\n\tskb = skb_peek(&sk->sk_write_queue);\n\tif (!skb)\n\t\tgoto out;\n\n\toffset = rp->offset;\n\ttotal_len = inet_sk(sk)->cork.base.length;\n\tif (offset >= total_len - 1) {\n\t\terr = -EINVAL;\n\t\tip6_flush_pending_frames(sk);\n\t\tgoto out;\n\t}\n\n\t/* should be check HW csum miyazawa */\n\tif (skb_queue_len(&sk->sk_write_queue) == 1) {\n\t\t/*\n\t\t * Only one fragment on the socket.\n\t\t */\n\t\ttmp_csum = skb->csum;\n\t} else {\n\t\tstruct sk_buff *csum_skb = NULL;\n\t\ttmp_csum = 0;\n\n\t\tskb_queue_walk(&sk->sk_write_queue, skb) {\n\t\t\ttmp_csum = csum_add(tmp_csum, skb->csum);\n\n\t\t\tif (csum_skb)\n\t\t\t\tcontinue;\n\n\t\t\tlen = skb->len - skb_transport_offset(skb);\n\t\t\tif (offset >= len) {\n\t\t\t\toffset -= len;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tcsum_skb = skb;\n\t\t}\n\n\t\tskb = csum_skb;\n\t}\n\n\toffset += skb_transport_offset(skb);\n\terr = skb_copy_bits(skb, offset, &csum, 2);\n\tif (err < 0) {\n\t\tip6_flush_pending_frames(sk);\n\t\tgoto out;\n\t}\n\n\t/* in case cksum was not initialized */\n\tif (unlikely(csum))\n\t\ttmp_csum = csum_sub(tmp_csum, csum_unfold(csum));\n\n\tcsum = csum_ipv6_magic(&fl6->saddr, &fl6->daddr,\n\t\t\t       total_len, fl6->flowi6_proto, tmp_csum);\n\n\tif (csum == 0 && fl6->flowi6_proto == IPPROTO_UDP)\n\t\tcsum = CSUM_MANGLED_0;\n\n\tBUG_ON(skb_store_bits(skb, offset, &csum, 2));\n\nsend:\n\terr = ip6_push_pending_frames(sk);\nout:\n\treturn err;\n}\n\nstatic int rawv6_send_hdrinc(struct sock *sk, struct msghdr *msg, int length,\n\t\t\tstruct flowi6 *fl6, struct dst_entry **dstp,\n\t\t\tunsigned int flags, const struct sockcm_cookie *sockc)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct ipv6hdr *iph;\n\tstruct sk_buff *skb;\n\tint err;\n\tstruct rt6_info *rt = (struct rt6_info *)*dstp;\n\tint hlen = LL_RESERVED_SPACE(rt->dst.dev);\n\tint tlen = rt->dst.dev->needed_tailroom;\n\n\tif (length > rt->dst.dev->mtu) {\n\t\tipv6_local_error(sk, EMSGSIZE, fl6, rt->dst.dev->mtu);\n\t\treturn -EMSGSIZE;\n\t}\n\tif (length < sizeof(struct ipv6hdr))\n\t\treturn -EINVAL;\n\tif (flags&MSG_PROBE)\n\t\tgoto out;\n\n\tskb = sock_alloc_send_skb(sk,\n\t\t\t\t  length + hlen + tlen + 15,\n\t\t\t\t  flags & MSG_DONTWAIT, &err);\n\tif (!skb)\n\t\tgoto error;\n\tskb_reserve(skb, hlen);\n\n\tskb->protocol = htons(ETH_P_IPV6);\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sockc->mark;\n\tskb->tstamp = sockc->transmit_time;\n\n\tskb_put(skb, length);\n\tskb_reset_network_header(skb);\n\tiph = ipv6_hdr(skb);\n\n\tskb->ip_summed = CHECKSUM_NONE;\n\n\tskb_setup_tx_timestamp(skb, sockc->tsflags);\n\n\tif (flags & MSG_CONFIRM)\n\t\tskb_set_dst_pending_confirm(skb, 1);\n\n\tskb->transport_header = skb->network_header;\n\terr = memcpy_from_msg(iph, msg, length);\n\tif (err) {\n\t\terr = -EFAULT;\n\t\tkfree_skb(skb);\n\t\tgoto error;\n\t}\n\n\tskb_dst_set(skb, &rt->dst);\n\t*dstp = NULL;\n\n\t/* if egress device is enslaved to an L3 master device pass the\n\t * skb to its handler for processing\n\t */\n\tskb = l3mdev_ip6_out(sk, skb);\n\tif (unlikely(!skb))\n\t\treturn 0;\n\n\t/* Acquire rcu_read_lock() in case we need to use rt->rt6i_idev\n\t * in the error path. Since skb has been freed, the dst could\n\t * have been queued for deletion.\n\t */\n\trcu_read_lock();\n\tIP6_UPD_PO_STATS(net, rt->rt6i_idev, IPSTATS_MIB_OUT, skb->len);\n\terr = NF_HOOK(NFPROTO_IPV6, NF_INET_LOCAL_OUT, net, sk, skb,\n\t\t      NULL, rt->dst.dev, dst_output);\n\tif (err > 0)\n\t\terr = net_xmit_errno(err);\n\tif (err) {\n\t\tIP6_INC_STATS(net, rt->rt6i_idev, IPSTATS_MIB_OUTDISCARDS);\n\t\trcu_read_unlock();\n\t\tgoto error_check;\n\t}\n\trcu_read_unlock();\nout:\n\treturn 0;\n\nerror:\n\tIP6_INC_STATS(net, rt->rt6i_idev, IPSTATS_MIB_OUTDISCARDS);\nerror_check:\n\tif (err == -ENOBUFS && !np->recverr)\n\t\terr = 0;\n\treturn err;\n}\n\nstruct raw6_frag_vec {\n\tstruct msghdr *msg;\n\tint hlen;\n\tchar c[4];\n};\n\nstatic int rawv6_probe_proto_opt(struct raw6_frag_vec *rfv, struct flowi6 *fl6)\n{\n\tint err = 0;\n\tswitch (fl6->flowi6_proto) {\n\tcase IPPROTO_ICMPV6:\n\t\trfv->hlen = 2;\n\t\terr = memcpy_from_msg(rfv->c, rfv->msg, rfv->hlen);\n\t\tif (!err) {\n\t\t\tfl6->fl6_icmp_type = rfv->c[0];\n\t\t\tfl6->fl6_icmp_code = rfv->c[1];\n\t\t}\n\t\tbreak;\n\tcase IPPROTO_MH:\n\t\trfv->hlen = 4;\n\t\terr = memcpy_from_msg(rfv->c, rfv->msg, rfv->hlen);\n\t\tif (!err)\n\t\t\tfl6->fl6_mh_type = rfv->c[2];\n\t}\n\treturn err;\n}\n\nstatic int raw6_getfrag(void *from, char *to, int offset, int len, int odd,\n\t\t       struct sk_buff *skb)\n{\n\tstruct raw6_frag_vec *rfv = from;\n\n\tif (offset < rfv->hlen) {\n\t\tint copy = min(rfv->hlen - offset, len);\n\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\t\tmemcpy(to, rfv->c + offset, copy);\n\t\telse\n\t\t\tskb->csum = csum_block_add(\n\t\t\t\tskb->csum,\n\t\t\t\tcsum_partial_copy_nocheck(rfv->c + offset,\n\t\t\t\t\t\t\t  to, copy),\n\t\t\t\todd);\n\n\t\todd = 0;\n\t\toffset += copy;\n\t\tto += copy;\n\t\tlen -= copy;\n\n\t\tif (!len)\n\t\t\treturn 0;\n\t}\n\n\toffset -= rfv->hlen;\n\n\treturn ip_generic_getfrag(rfv->msg, to, offset, len, odd, skb);\n}\n\nstatic int rawv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct raw6_frag_vec rfv;\n\tstruct flowi6 fl6;\n\tstruct ipcm6_cookie ipc6;\n\tint addr_len = msg->msg_namelen;\n\tint hdrincl;\n\tu16 proto;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/* hdrincl should be READ_ONCE(inet->hdrincl)\n\t * but READ_ONCE() doesn't work with bit fields.\n\t * Doing this indirectly yields the same result.\n\t */\n\thdrincl = inet->hdrincl;\n\thdrincl = READ_ONCE(hdrincl);\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.flowi6_uid = sk->sk_uid;\n\n\tipcm6_init(&ipc6);\n\tipc6.sockc.tsflags = sk->sk_tsflags;\n\tipc6.sockc.mark = sk->sk_mark;\n\n\tif (sin6) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (sin6->sin6_family && sin6->sin6_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\t/* port is the proto value [0..255] carried in nexthdr */\n\t\tproto = ntohs(sin6->sin6_port);\n\n\t\tif (!proto)\n\t\t\tproto = inet->inet_num;\n\t\telse if (proto != inet->inet_num)\n\t\t\treturn -EINVAL;\n\n\t\tif (proto > 255)\n\t\t\treturn -EINVAL;\n\n\t\tdaddr = &sin6->sin6_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (IS_ERR(flowlabel))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tproto = inet->inet_num;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\t\tipc6.opt = opt;\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, &ipc6);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (IS_ERR(flowlabel))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = proto;\n\tfl6.flowi6_mark = ipc6.sockc.mark;\n\n\tif (!hdrincl) {\n\t\trfv.msg = msg;\n\t\trfv.hlen = 0;\n\t\terr = rawv6_probe_proto_opt(&rfv, &fl6);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi_common(&fl6));\n\n\tif (hdrincl)\n\t\tfl6.flowi6_flags |= FLOWI_FLAG_KNOWN_NH;\n\n\tif (ipc6.tclass < 0)\n\t\tipc6.tclass = np->tclass;\n\n\tfl6.flowlabel = ip6_make_flowinfo(ipc6.tclass, fl6.flowlabel);\n\n\tdst = ip6_dst_lookup_flow(sock_net(sk), sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\tif (ipc6.hlimit < 0)\n\t\tipc6.hlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (ipc6.dontfrag < 0)\n\t\tipc6.dontfrag = np->dontfrag;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tif (hdrincl)\n\t\terr = rawv6_send_hdrinc(sk, msg, len, &fl6, &dst,\n\t\t\t\t\tmsg->msg_flags, &ipc6.sockc);\n\telse {\n\t\tipc6.opt = opt;\n\t\tlock_sock(sk);\n\t\terr = ip6_append_data(sk, raw6_getfrag, &rfv,\n\t\t\tlen, 0, &ipc6, &fl6, (struct rt6_info *)dst,\n\t\t\tmsg->msg_flags);\n\n\t\tif (err)\n\t\t\tip6_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE))\n\t\t\terr = rawv6_push_pending_frames(sk, &fl6, rp);\n\t\trelease_sock(sk);\n\t}\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\treturn err < 0 ? err : len;\ndo_confirm:\n\tif (msg->msg_flags & MSG_PROBE)\n\t\tdst_confirm_neigh(dst, &fl6.daddr);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}\n\nstatic int rawv6_seticmpfilter(struct sock *sk, int level, int optname,\n\t\t\t       sockptr_t optval, int optlen)\n{\n\tswitch (optname) {\n\tcase ICMPV6_FILTER:\n\t\tif (optlen > sizeof(struct icmp6_filter))\n\t\t\toptlen = sizeof(struct icmp6_filter);\n\t\tif (copy_from_sockptr(&raw6_sk(sk)->filter, optval, optlen))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\treturn 0;\n}\n\nstatic int rawv6_geticmpfilter(struct sock *sk, int level, int optname,\n\t\t\t       char __user *optval, int __user *optlen)\n{\n\tint len;\n\n\tswitch (optname) {\n\tcase ICMPV6_FILTER:\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (len < 0)\n\t\t\treturn -EINVAL;\n\t\tif (len > sizeof(struct icmp6_filter))\n\t\t\tlen = sizeof(struct icmp6_filter);\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &raw6_sk(sk)->filter, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\treturn 0;\n}\n\n\nstatic int do_rawv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t       sockptr_t optval, unsigned int optlen)\n{\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\tint val;\n\n\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\treturn -EFAULT;\n\n\tswitch (optname) {\n\tcase IPV6_HDRINCL:\n\t\tif (sk->sk_type != SOCK_RAW)\n\t\t\treturn -EINVAL;\n\t\tinet_sk(sk)->hdrincl = !!val;\n\t\treturn 0;\n\tcase IPV6_CHECKSUM:\n\t\tif (inet_sk(sk)->inet_num == IPPROTO_ICMPV6 &&\n\t\t    level == IPPROTO_IPV6) {\n\t\t\t/*\n\t\t\t * RFC3542 tells that IPV6_CHECKSUM socket\n\t\t\t * option in the IPPROTO_IPV6 level is not\n\t\t\t * allowed on ICMPv6 sockets.\n\t\t\t * If you want to set it, use IPPROTO_RAW\n\t\t\t * level IPV6_CHECKSUM socket option\n\t\t\t * (Linux extension).\n\t\t\t */\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* You may get strange result with a positive odd offset;\n\t\t   RFC2292bis agrees with me. */\n\t\tif (val > 0 && (val&1))\n\t\t\treturn -EINVAL;\n\t\tif (val < 0) {\n\t\t\trp->checksum = 0;\n\t\t} else {\n\t\t\trp->checksum = 1;\n\t\t\trp->offset = val;\n\t\t}\n\n\t\treturn 0;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}\n\nstatic int rawv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t    sockptr_t optval, unsigned int optlen)\n{\n\tswitch (level) {\n\tcase SOL_RAW:\n\t\tbreak;\n\n\tcase SOL_ICMPV6:\n\t\tif (inet_sk(sk)->inet_num != IPPROTO_ICMPV6)\n\t\t\treturn -EOPNOTSUPP;\n\t\treturn rawv6_seticmpfilter(sk, level, optname, optval, optlen);\n\tcase SOL_IPV6:\n\t\tif (optname == IPV6_CHECKSUM ||\n\t\t    optname == IPV6_HDRINCL)\n\t\t\tbreak;\n\t\tfallthrough;\n\tdefault:\n\t\treturn ipv6_setsockopt(sk, level, optname, optval, optlen);\n\t}\n\n\treturn do_rawv6_setsockopt(sk, level, optname, optval, optlen);\n}\n\nstatic int do_rawv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t    char __user *optval, int __user *optlen)\n{\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\tint val, len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tswitch (optname) {\n\tcase IPV6_HDRINCL:\n\t\tval = inet_sk(sk)->hdrincl;\n\t\tbreak;\n\tcase IPV6_CHECKSUM:\n\t\t/*\n\t\t * We allow getsockopt() for IPPROTO_IPV6-level\n\t\t * IPV6_CHECKSUM socket option on ICMPv6 sockets\n\t\t * since RFC3542 is silent about it.\n\t\t */\n\t\tif (rp->checksum == 0)\n\t\t\tval = -1;\n\t\telse\n\t\t\tval = rp->offset;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tlen = min_t(unsigned int, sizeof(int), len);\n\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int rawv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, int __user *optlen)\n{\n\tswitch (level) {\n\tcase SOL_RAW:\n\t\tbreak;\n\n\tcase SOL_ICMPV6:\n\t\tif (inet_sk(sk)->inet_num != IPPROTO_ICMPV6)\n\t\t\treturn -EOPNOTSUPP;\n\t\treturn rawv6_geticmpfilter(sk, level, optname, optval, optlen);\n\tcase SOL_IPV6:\n\t\tif (optname == IPV6_CHECKSUM ||\n\t\t    optname == IPV6_HDRINCL)\n\t\t\tbreak;\n\t\tfallthrough;\n\tdefault:\n\t\treturn ipv6_getsockopt(sk, level, optname, optval, optlen);\n\t}\n\n\treturn do_rawv6_getsockopt(sk, level, optname, optval, optlen);\n}\n\nstatic int rawv6_ioctl(struct sock *sk, int cmd, unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase SIOCOUTQ: {\n\t\tint amount = sk_wmem_alloc_get(sk);\n\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n\tcase SIOCINQ: {\n\t\tstruct sk_buff *skb;\n\t\tint amount = 0;\n\n\t\tspin_lock_bh(&sk->sk_receive_queue.lock);\n\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\tif (skb)\n\t\t\tamount = skb->len;\n\t\tspin_unlock_bh(&sk->sk_receive_queue.lock);\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n\n\tdefault:\n#ifdef CONFIG_IPV6_MROUTE\n\t\treturn ip6mr_ioctl(sk, cmd, (void __user *)arg);\n#else\n\t\treturn -ENOIOCTLCMD;\n#endif\n\t}\n}\n\n#ifdef CONFIG_COMPAT\nstatic int compat_rawv6_ioctl(struct sock *sk, unsigned int cmd, unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase SIOCOUTQ:\n\tcase SIOCINQ:\n\t\treturn -ENOIOCTLCMD;\n\tdefault:\n#ifdef CONFIG_IPV6_MROUTE\n\t\treturn ip6mr_compat_ioctl(sk, cmd, compat_ptr(arg));\n#else\n\t\treturn -ENOIOCTLCMD;\n#endif\n\t}\n}\n#endif\n\nstatic void rawv6_close(struct sock *sk, long timeout)\n{\n\tif (inet_sk(sk)->inet_num == IPPROTO_RAW)\n\t\tip6_ra_control(sk, -1);\n\tip6mr_sk_done(sk);\n\tsk_common_release(sk);\n}\n\nstatic void raw6_destroy(struct sock *sk)\n{\n\tlock_sock(sk);\n\tip6_flush_pending_frames(sk);\n\trelease_sock(sk);\n\n\tinet6_destroy_sock(sk);\n}\n\nstatic int rawv6_init_sk(struct sock *sk)\n{\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\n\tswitch (inet_sk(sk)->inet_num) {\n\tcase IPPROTO_ICMPV6:\n\t\trp->checksum = 1;\n\t\trp->offset   = 2;\n\t\tbreak;\n\tcase IPPROTO_MH:\n\t\trp->checksum = 1;\n\t\trp->offset   = 4;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstruct proto rawv6_prot = {\n\t.name\t\t   = \"RAWv6\",\n\t.owner\t\t   = THIS_MODULE,\n\t.close\t\t   = rawv6_close,\n\t.destroy\t   = raw6_destroy,\n\t.connect\t   = ip6_datagram_connect_v6_only,\n\t.disconnect\t   = __udp_disconnect,\n\t.ioctl\t\t   = rawv6_ioctl,\n\t.init\t\t   = rawv6_init_sk,\n\t.setsockopt\t   = rawv6_setsockopt,\n\t.getsockopt\t   = rawv6_getsockopt,\n\t.sendmsg\t   = rawv6_sendmsg,\n\t.recvmsg\t   = rawv6_recvmsg,\n\t.bind\t\t   = rawv6_bind,\n\t.backlog_rcv\t   = rawv6_rcv_skb,\n\t.hash\t\t   = raw_hash_sk,\n\t.unhash\t\t   = raw_unhash_sk,\n\t.obj_size\t   = sizeof(struct raw6_sock),\n\t.useroffset\t   = offsetof(struct raw6_sock, filter),\n\t.usersize\t   = sizeof_field(struct raw6_sock, filter),\n\t.h.raw_hash\t   = &raw_v6_hashinfo,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl\t   = compat_rawv6_ioctl,\n#endif\n\t.diag_destroy\t   = raw_abort,\n};\n\n#ifdef CONFIG_PROC_FS\nstatic int raw6_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN) {\n\t\tseq_puts(seq, IPV6_SEQ_DGRAM_HEADER);\n\t} else {\n\t\tstruct sock *sp = v;\n\t\t__u16 srcp  = inet_sk(sp)->inet_num;\n\t\tip6_dgram_sock_seq_show(seq, v, srcp, 0,\n\t\t\t\t\traw_seq_private(seq)->bucket);\n\t}\n\treturn 0;\n}\n\nstatic const struct seq_operations raw6_seq_ops = {\n\t.start =\traw_seq_start,\n\t.next =\t\traw_seq_next,\n\t.stop =\t\traw_seq_stop,\n\t.show =\t\traw6_seq_show,\n};\n\nstatic int __net_init raw6_init_net(struct net *net)\n{\n\tif (!proc_create_net_data(\"raw6\", 0444, net->proc_net, &raw6_seq_ops,\n\t\t\tsizeof(struct raw_iter_state), &raw_v6_hashinfo))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void __net_exit raw6_exit_net(struct net *net)\n{\n\tremove_proc_entry(\"raw6\", net->proc_net);\n}\n\nstatic struct pernet_operations raw6_net_ops = {\n\t.init = raw6_init_net,\n\t.exit = raw6_exit_net,\n};\n\nint __init raw6_proc_init(void)\n{\n\treturn register_pernet_subsys(&raw6_net_ops);\n}\n\nvoid raw6_proc_exit(void)\n{\n\tunregister_pernet_subsys(&raw6_net_ops);\n}\n#endif\t/* CONFIG_PROC_FS */\n\n/* Same as inet6_dgram_ops, sans udp_poll.  */\nconst struct proto_ops inet6_sockraw_ops = {\n\t.family\t\t   = PF_INET6,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet6_release,\n\t.bind\t\t   = inet6_bind,\n\t.connect\t   = inet_dgram_connect,\t/* ok\t\t*/\n\t.socketpair\t   = sock_no_socketpair,\t/* a do nothing\t*/\n\t.accept\t\t   = sock_no_accept,\t\t/* a do nothing\t*/\n\t.getname\t   = inet6_getname,\n\t.poll\t\t   = datagram_poll,\t\t/* ok\t\t*/\n\t.ioctl\t\t   = inet6_ioctl,\t\t/* must change  */\n\t.gettstamp\t   = sock_gettstamp,\n\t.listen\t\t   = sock_no_listen,\t\t/* ok\t\t*/\n\t.shutdown\t   = inet_shutdown,\t\t/* ok\t\t*/\n\t.setsockopt\t   = sock_common_setsockopt,\t/* ok\t\t*/\n\t.getsockopt\t   = sock_common_getsockopt,\t/* ok\t\t*/\n\t.sendmsg\t   = inet_sendmsg,\t\t/* ok\t\t*/\n\t.recvmsg\t   = sock_common_recvmsg,\t/* ok\t\t*/\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = sock_no_sendpage,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl\t   = inet6_compat_ioctl,\n#endif\n};\n\nstatic struct inet_protosw rawv6_protosw = {\n\t.type\t\t= SOCK_RAW,\n\t.protocol\t= IPPROTO_IP,\t/* wild card */\n\t.prot\t\t= &rawv6_prot,\n\t.ops\t\t= &inet6_sockraw_ops,\n\t.flags\t\t= INET_PROTOSW_REUSE,\n};\n\nint __init rawv6_init(void)\n{\n\treturn inet6_register_protosw(&rawv6_protosw);\n}\n\nvoid rawv6_exit(void)\n{\n\tinet6_unregister_protosw(&rawv6_protosw);\n}\n"}, "17": {"id": 17, "path": "/src/include/linux/net.h", "content": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/*\n * NET\t\tAn implementation of the SOCKET network access protocol.\n *\t\tThis is the master header file for the Linux NET layer,\n *\t\tor, in plain English: the networking handling part of the\n *\t\tkernel.\n *\n * Version:\t@(#)net.h\t1.0.3\t05/25/93\n *\n * Authors:\tOrest Zborowski, <obz@Kodak.COM>\n *\t\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n */\n#ifndef _LINUX_NET_H\n#define _LINUX_NET_H\n\n#include <linux/stringify.h>\n#include <linux/random.h>\n#include <linux/wait.h>\n#include <linux/fcntl.h>\t/* For O_CLOEXEC and O_NONBLOCK */\n#include <linux/rcupdate.h>\n#include <linux/once.h>\n#include <linux/fs.h>\n#include <linux/mm.h>\n#include <linux/sockptr.h>\n\n#include <uapi/linux/net.h>\n\nstruct poll_table_struct;\nstruct pipe_inode_info;\nstruct inode;\nstruct file;\nstruct net;\n\n/* Historically, SOCKWQ_ASYNC_NOSPACE & SOCKWQ_ASYNC_WAITDATA were located\n * in sock->flags, but moved into sk->sk_wq->flags to be RCU protected.\n * Eventually all flags will be in sk->sk_wq->flags.\n */\n#define SOCKWQ_ASYNC_NOSPACE\t0\n#define SOCKWQ_ASYNC_WAITDATA\t1\n#define SOCK_NOSPACE\t\t2\n#define SOCK_PASSCRED\t\t3\n#define SOCK_PASSSEC\t\t4\n\n#ifndef ARCH_HAS_SOCKET_TYPES\n/**\n * enum sock_type - Socket types\n * @SOCK_STREAM: stream (connection) socket\n * @SOCK_DGRAM: datagram (conn.less) socket\n * @SOCK_RAW: raw socket\n * @SOCK_RDM: reliably-delivered message\n * @SOCK_SEQPACKET: sequential packet socket\n * @SOCK_DCCP: Datagram Congestion Control Protocol socket\n * @SOCK_PACKET: linux specific way of getting packets at the dev level.\n *\t\t  For writing rarp and other similar things on the user level.\n *\n * When adding some new socket type please\n * grep ARCH_HAS_SOCKET_TYPE include/asm-* /socket.h, at least MIPS\n * overrides this enum for binary compat reasons.\n */\nenum sock_type {\n\tSOCK_STREAM\t= 1,\n\tSOCK_DGRAM\t= 2,\n\tSOCK_RAW\t= 3,\n\tSOCK_RDM\t= 4,\n\tSOCK_SEQPACKET\t= 5,\n\tSOCK_DCCP\t= 6,\n\tSOCK_PACKET\t= 10,\n};\n\n#define SOCK_MAX (SOCK_PACKET + 1)\n/* Mask which covers at least up to SOCK_MASK-1.  The\n * remaining bits are used as flags. */\n#define SOCK_TYPE_MASK 0xf\n\n/* Flags for socket, socketpair, accept4 */\n#define SOCK_CLOEXEC\tO_CLOEXEC\n#ifndef SOCK_NONBLOCK\n#define SOCK_NONBLOCK\tO_NONBLOCK\n#endif\n\n#endif /* ARCH_HAS_SOCKET_TYPES */\n\n/**\n * enum sock_shutdown_cmd - Shutdown types\n * @SHUT_RD: shutdown receptions\n * @SHUT_WR: shutdown transmissions\n * @SHUT_RDWR: shutdown receptions/transmissions\n */\nenum sock_shutdown_cmd {\n\tSHUT_RD,\n\tSHUT_WR,\n\tSHUT_RDWR,\n};\n\nstruct socket_wq {\n\t/* Note: wait MUST be first field of socket_wq */\n\twait_queue_head_t\twait;\n\tstruct fasync_struct\t*fasync_list;\n\tunsigned long\t\tflags; /* %SOCKWQ_ASYNC_NOSPACE, etc */\n\tstruct rcu_head\t\trcu;\n} ____cacheline_aligned_in_smp;\n\n/**\n *  struct socket - general BSD socket\n *  @state: socket state (%SS_CONNECTED, etc)\n *  @type: socket type (%SOCK_STREAM, etc)\n *  @flags: socket flags (%SOCK_NOSPACE, etc)\n *  @ops: protocol specific socket operations\n *  @file: File back pointer for gc\n *  @sk: internal networking protocol agnostic socket representation\n *  @wq: wait queue for several uses\n */\nstruct socket {\n\tsocket_state\t\tstate;\n\n\tshort\t\t\ttype;\n\n\tunsigned long\t\tflags;\n\n\tstruct file\t\t*file;\n\tstruct sock\t\t*sk;\n\tconst struct proto_ops\t*ops;\n\n\tstruct socket_wq\twq;\n};\n\nstruct vm_area_struct;\nstruct page;\nstruct sockaddr;\nstruct msghdr;\nstruct module;\nstruct sk_buff;\ntypedef int (*sk_read_actor_t)(read_descriptor_t *, struct sk_buff *,\n\t\t\t       unsigned int, size_t);\n\nstruct proto_ops {\n\tint\t\tfamily;\n\tstruct module\t*owner;\n\tint\t\t(*release)   (struct socket *sock);\n\tint\t\t(*bind)\t     (struct socket *sock,\n\t\t\t\t      struct sockaddr *myaddr,\n\t\t\t\t      int sockaddr_len);\n\tint\t\t(*connect)   (struct socket *sock,\n\t\t\t\t      struct sockaddr *vaddr,\n\t\t\t\t      int sockaddr_len, int flags);\n\tint\t\t(*socketpair)(struct socket *sock1,\n\t\t\t\t      struct socket *sock2);\n\tint\t\t(*accept)    (struct socket *sock,\n\t\t\t\t      struct socket *newsock, int flags, bool kern);\n\tint\t\t(*getname)   (struct socket *sock,\n\t\t\t\t      struct sockaddr *addr,\n\t\t\t\t      int peer);\n\t__poll_t\t(*poll)\t     (struct file *file, struct socket *sock,\n\t\t\t\t      struct poll_table_struct *wait);\n\tint\t\t(*ioctl)     (struct socket *sock, unsigned int cmd,\n\t\t\t\t      unsigned long arg);\n#ifdef CONFIG_COMPAT\n\tint\t \t(*compat_ioctl) (struct socket *sock, unsigned int cmd,\n\t\t\t\t      unsigned long arg);\n#endif\n\tint\t\t(*gettstamp) (struct socket *sock, void __user *userstamp,\n\t\t\t\t      bool timeval, bool time32);\n\tint\t\t(*listen)    (struct socket *sock, int len);\n\tint\t\t(*shutdown)  (struct socket *sock, int flags);\n\tint\t\t(*setsockopt)(struct socket *sock, int level,\n\t\t\t\t      int optname, sockptr_t optval,\n\t\t\t\t      unsigned int optlen);\n\tint\t\t(*getsockopt)(struct socket *sock, int level,\n\t\t\t\t      int optname, char __user *optval, int __user *optlen);\n\tvoid\t\t(*show_fdinfo)(struct seq_file *m, struct socket *sock);\n\tint\t\t(*sendmsg)   (struct socket *sock, struct msghdr *m,\n\t\t\t\t      size_t total_len);\n\t/* Notes for implementing recvmsg:\n\t * ===============================\n\t * msg->msg_namelen should get updated by the recvmsg handlers\n\t * iff msg_name != NULL. It is by default 0 to prevent\n\t * returning uninitialized memory to user space.  The recvfrom\n\t * handlers can assume that msg.msg_name is either NULL or has\n\t * a minimum size of sizeof(struct sockaddr_storage).\n\t */\n\tint\t\t(*recvmsg)   (struct socket *sock, struct msghdr *m,\n\t\t\t\t      size_t total_len, int flags);\n\tint\t\t(*mmap)\t     (struct file *file, struct socket *sock,\n\t\t\t\t      struct vm_area_struct * vma);\n\tssize_t\t\t(*sendpage)  (struct socket *sock, struct page *page,\n\t\t\t\t      int offset, size_t size, int flags);\n\tssize_t \t(*splice_read)(struct socket *sock,  loff_t *ppos,\n\t\t\t\t       struct pipe_inode_info *pipe, size_t len, unsigned int flags);\n\tint\t\t(*set_peek_off)(struct sock *sk, int val);\n\tint\t\t(*peek_len)(struct socket *sock);\n\n\t/* The following functions are called internally by kernel with\n\t * sock lock already held.\n\t */\n\tint\t\t(*read_sock)(struct sock *sk, read_descriptor_t *desc,\n\t\t\t\t     sk_read_actor_t recv_actor);\n\tint\t\t(*sendpage_locked)(struct sock *sk, struct page *page,\n\t\t\t\t\t   int offset, size_t size, int flags);\n\tint\t\t(*sendmsg_locked)(struct sock *sk, struct msghdr *msg,\n\t\t\t\t\t  size_t size);\n\tint\t\t(*set_rcvlowat)(struct sock *sk, int val);\n};\n\n#define DECLARE_SOCKADDR(type, dst, src)\t\\\n\ttype dst = ({ __sockaddr_check_size(sizeof(*dst)); (type) src; })\n\nstruct net_proto_family {\n\tint\t\tfamily;\n\tint\t\t(*create)(struct net *net, struct socket *sock,\n\t\t\t\t  int protocol, int kern);\n\tstruct module\t*owner;\n};\n\nstruct iovec;\nstruct kvec;\n\nenum {\n\tSOCK_WAKE_IO,\n\tSOCK_WAKE_WAITD,\n\tSOCK_WAKE_SPACE,\n\tSOCK_WAKE_URG,\n};\n\nint sock_wake_async(struct socket_wq *sk_wq, int how, int band);\nint sock_register(const struct net_proto_family *fam);\nvoid sock_unregister(int family);\nbool sock_is_registered(int family);\nint __sock_create(struct net *net, int family, int type, int proto,\n\t\t  struct socket **res, int kern);\nint sock_create(int family, int type, int proto, struct socket **res);\nint sock_create_kern(struct net *net, int family, int type, int proto, struct socket **res);\nint sock_create_lite(int family, int type, int proto, struct socket **res);\nstruct socket *sock_alloc(void);\nvoid sock_release(struct socket *sock);\nint sock_sendmsg(struct socket *sock, struct msghdr *msg);\nint sock_recvmsg(struct socket *sock, struct msghdr *msg, int flags);\nstruct file *sock_alloc_file(struct socket *sock, int flags, const char *dname);\nstruct socket *sockfd_lookup(int fd, int *err);\nstruct socket *sock_from_file(struct file *file);\n#define\t\t     sockfd_put(sock) fput(sock->file)\nint net_ratelimit(void);\n\n#define net_ratelimited_function(function, ...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (net_ratelimit())\t\t\t\t\t\\\n\t\tfunction(__VA_ARGS__);\t\t\t\t\\\n} while (0)\n\n#define net_emerg_ratelimited(fmt, ...)\t\t\t\t\\\n\tnet_ratelimited_function(pr_emerg, fmt, ##__VA_ARGS__)\n#define net_alert_ratelimited(fmt, ...)\t\t\t\t\\\n\tnet_ratelimited_function(pr_alert, fmt, ##__VA_ARGS__)\n#define net_crit_ratelimited(fmt, ...)\t\t\t\t\\\n\tnet_ratelimited_function(pr_crit, fmt, ##__VA_ARGS__)\n#define net_err_ratelimited(fmt, ...)\t\t\t\t\\\n\tnet_ratelimited_function(pr_err, fmt, ##__VA_ARGS__)\n#define net_notice_ratelimited(fmt, ...)\t\t\t\\\n\tnet_ratelimited_function(pr_notice, fmt, ##__VA_ARGS__)\n#define net_warn_ratelimited(fmt, ...)\t\t\t\t\\\n\tnet_ratelimited_function(pr_warn, fmt, ##__VA_ARGS__)\n#define net_info_ratelimited(fmt, ...)\t\t\t\t\\\n\tnet_ratelimited_function(pr_info, fmt, ##__VA_ARGS__)\n#if defined(CONFIG_DYNAMIC_DEBUG) || \\\n\t(defined(CONFIG_DYNAMIC_DEBUG_CORE) && defined(DYNAMIC_DEBUG_MODULE))\n#define net_dbg_ratelimited(fmt, ...)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tDEFINE_DYNAMIC_DEBUG_METADATA(descriptor, fmt);\t\t\t\\\n\tif (DYNAMIC_DEBUG_BRANCH(descriptor) &&\t\t\t\t\\\n\t    net_ratelimit())\t\t\t\t\t\t\\\n\t\t__dynamic_pr_debug(&descriptor, pr_fmt(fmt),\t\t\\\n\t\t                   ##__VA_ARGS__);\t\t\t\\\n} while (0)\n#elif defined(DEBUG)\n#define net_dbg_ratelimited(fmt, ...)\t\t\t\t\\\n\tnet_ratelimited_function(pr_debug, fmt, ##__VA_ARGS__)\n#else\n#define net_dbg_ratelimited(fmt, ...)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tif (0)\t\t\t\t\t\t\\\n\t\t\tno_printk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__); \\\n\t} while (0)\n#endif\n\n#define net_get_random_once(buf, nbytes)\t\t\t\\\n\tget_random_once((buf), (nbytes))\n#define net_get_random_once_wait(buf, nbytes)\t\t\t\\\n\tget_random_once_wait((buf), (nbytes))\n\n/*\n * E.g. XFS meta- & log-data is in slab pages, or bcache meta\n * data pages, or other high order pages allocated by\n * __get_free_pages() without __GFP_COMP, which have a page_count\n * of 0 and/or have PageSlab() set. We cannot use send_page for\n * those, as that does get_page(); put_page(); and would cause\n * either a VM_BUG directly, or __page_cache_release a page that\n * would actually still be referenced by someone, leading to some\n * obscure delayed Oops somewhere else.\n */\nstatic inline bool sendpage_ok(struct page *page)\n{\n\treturn !PageSlab(page) && page_count(page) >= 1;\n}\n\nint kernel_sendmsg(struct socket *sock, struct msghdr *msg, struct kvec *vec,\n\t\t   size_t num, size_t len);\nint kernel_sendmsg_locked(struct sock *sk, struct msghdr *msg,\n\t\t\t  struct kvec *vec, size_t num, size_t len);\nint kernel_recvmsg(struct socket *sock, struct msghdr *msg, struct kvec *vec,\n\t\t   size_t num, size_t len, int flags);\n\nint kernel_bind(struct socket *sock, struct sockaddr *addr, int addrlen);\nint kernel_listen(struct socket *sock, int backlog);\nint kernel_accept(struct socket *sock, struct socket **newsock, int flags);\nint kernel_connect(struct socket *sock, struct sockaddr *addr, int addrlen,\n\t\t   int flags);\nint kernel_getsockname(struct socket *sock, struct sockaddr *addr);\nint kernel_getpeername(struct socket *sock, struct sockaddr *addr);\nint kernel_sendpage(struct socket *sock, struct page *page, int offset,\n\t\t    size_t size, int flags);\nint kernel_sendpage_locked(struct sock *sk, struct page *page, int offset,\n\t\t\t   size_t size, int flags);\nint kernel_sock_shutdown(struct socket *sock, enum sock_shutdown_cmd how);\n\n/* Routine returns the IP overhead imposed by a (caller-protected) socket. */\nu32 kernel_sock_ip_overhead(struct sock *sk);\n\n#define MODULE_ALIAS_NETPROTO(proto) \\\n\tMODULE_ALIAS(\"net-pf-\" __stringify(proto))\n\n#define MODULE_ALIAS_NET_PF_PROTO(pf, proto) \\\n\tMODULE_ALIAS(\"net-pf-\" __stringify(pf) \"-proto-\" __stringify(proto))\n\n#define MODULE_ALIAS_NET_PF_PROTO_TYPE(pf, proto, type) \\\n\tMODULE_ALIAS(\"net-pf-\" __stringify(pf) \"-proto-\" __stringify(proto) \\\n\t\t     \"-type-\" __stringify(type))\n\n#define MODULE_ALIAS_NET_PF_PROTO_NAME(pf, proto, name) \\\n\tMODULE_ALIAS(\"net-pf-\" __stringify(pf) \"-proto-\" __stringify(proto) \\\n\t\t     name)\n#endif\t/* _LINUX_NET_H */\n"}, "18": {"id": 18, "path": "/src/include/linux/socket.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_SOCKET_H\n#define _LINUX_SOCKET_H\n\n\n#include <asm/socket.h>\t\t\t/* arch-dependent defines\t*/\n#include <linux/sockios.h>\t\t/* the SIOCxxx I/O controls\t*/\n#include <linux/uio.h>\t\t\t/* iovec support\t\t*/\n#include <linux/types.h>\t\t/* pid_t\t\t\t*/\n#include <linux/compiler.h>\t\t/* __user\t\t\t*/\n#include <uapi/linux/socket.h>\n\nstruct file;\nstruct pid;\nstruct cred;\nstruct socket;\n\n#define __sockaddr_check_size(size)\t\\\n\tBUILD_BUG_ON(((size) > sizeof(struct __kernel_sockaddr_storage)))\n\n#ifdef CONFIG_PROC_FS\nstruct seq_file;\nextern void socket_seq_show(struct seq_file *seq);\n#endif\n\ntypedef __kernel_sa_family_t\tsa_family_t;\n\n/*\n *\t1003.1g requires sa_family_t and that sa_data is char.\n */\n\nstruct sockaddr {\n\tsa_family_t\tsa_family;\t/* address family, AF_xxx\t*/\n\tchar\t\tsa_data[14];\t/* 14 bytes of protocol address\t*/\n};\n\nstruct linger {\n\tint\t\tl_onoff;\t/* Linger active\t\t*/\n\tint\t\tl_linger;\t/* How long to linger for\t*/\n};\n\n#define sockaddr_storage __kernel_sockaddr_storage\n\n/*\n *\tAs we do 4.4BSD message passing we use a 4.4BSD message passing\n *\tsystem, not 4.3. Thus msg_accrights(len) are now missing. They\n *\tbelong in an obscure libc emulation or the bin.\n */\n\nstruct msghdr {\n\tvoid\t\t*msg_name;\t/* ptr to socket address structure */\n\tint\t\tmsg_namelen;\t/* size of socket address structure */\n\tstruct iov_iter\tmsg_iter;\t/* data */\n\n\t/*\n\t * Ancillary data. msg_control_user is the user buffer used for the\n\t * recv* side when msg_control_is_user is set, msg_control is the kernel\n\t * buffer used for all other cases.\n\t */\n\tunion {\n\t\tvoid\t\t*msg_control;\n\t\tvoid __user\t*msg_control_user;\n\t};\n\tbool\t\tmsg_control_is_user : 1;\n\t__kernel_size_t\tmsg_controllen;\t/* ancillary data buffer length */\n\tunsigned int\tmsg_flags;\t/* flags on received message */\n\tstruct kiocb\t*msg_iocb;\t/* ptr to iocb for async requests */\n};\n\nstruct user_msghdr {\n\tvoid\t\t__user *msg_name;\t/* ptr to socket address structure */\n\tint\t\tmsg_namelen;\t\t/* size of socket address structure */\n\tstruct iovec\t__user *msg_iov;\t/* scatter/gather array */\n\t__kernel_size_t\tmsg_iovlen;\t\t/* # elements in msg_iov */\n\tvoid\t\t__user *msg_control;\t/* ancillary data */\n\t__kernel_size_t\tmsg_controllen;\t\t/* ancillary data buffer length */\n\tunsigned int\tmsg_flags;\t\t/* flags on received message */\n};\n\n/* For recvmmsg/sendmmsg */\nstruct mmsghdr {\n\tstruct user_msghdr  msg_hdr;\n\tunsigned int        msg_len;\n};\n\n/*\n *\tPOSIX 1003.1g - ancillary data object information\n *\tAncillary data consits of a sequence of pairs of\n *\t(cmsghdr, cmsg_data[])\n */\n\nstruct cmsghdr {\n\t__kernel_size_t\tcmsg_len;\t/* data byte count, including hdr */\n        int\t\tcmsg_level;\t/* originating protocol */\n        int\t\tcmsg_type;\t/* protocol-specific type */\n};\n\n/*\n *\tAncillary data object information MACROS\n *\tTable 5-14 of POSIX 1003.1g\n */\n\n#define __CMSG_NXTHDR(ctl, len, cmsg) __cmsg_nxthdr((ctl),(len),(cmsg))\n#define CMSG_NXTHDR(mhdr, cmsg) cmsg_nxthdr((mhdr), (cmsg))\n\n#define CMSG_ALIGN(len) ( ((len)+sizeof(long)-1) & ~(sizeof(long)-1) )\n\n#define CMSG_DATA(cmsg) \\\n\t((void *)(cmsg) + sizeof(struct cmsghdr))\n#define CMSG_USER_DATA(cmsg) \\\n\t((void __user *)(cmsg) + sizeof(struct cmsghdr))\n#define CMSG_SPACE(len) (sizeof(struct cmsghdr) + CMSG_ALIGN(len))\n#define CMSG_LEN(len) (sizeof(struct cmsghdr) + (len))\n\n#define __CMSG_FIRSTHDR(ctl,len) ((len) >= sizeof(struct cmsghdr) ? \\\n\t\t\t\t  (struct cmsghdr *)(ctl) : \\\n\t\t\t\t  (struct cmsghdr *)NULL)\n#define CMSG_FIRSTHDR(msg)\t__CMSG_FIRSTHDR((msg)->msg_control, (msg)->msg_controllen)\n#define CMSG_OK(mhdr, cmsg) ((cmsg)->cmsg_len >= sizeof(struct cmsghdr) && \\\n\t\t\t     (cmsg)->cmsg_len <= (unsigned long) \\\n\t\t\t     ((mhdr)->msg_controllen - \\\n\t\t\t      ((char *)(cmsg) - (char *)(mhdr)->msg_control)))\n#define for_each_cmsghdr(cmsg, msg) \\\n\tfor (cmsg = CMSG_FIRSTHDR(msg); \\\n\t     cmsg; \\\n\t     cmsg = CMSG_NXTHDR(msg, cmsg))\n\n/*\n *\tGet the next cmsg header\n *\n *\tPLEASE, do not touch this function. If you think, that it is\n *\tincorrect, grep kernel sources and think about consequences\n *\tbefore trying to improve it.\n *\n *\tNow it always returns valid, not truncated ancillary object\n *\tHEADER. But caller still MUST check, that cmsg->cmsg_len is\n *\tinside range, given by msg->msg_controllen before using\n *\tancillary object DATA.\t\t\t\t--ANK (980731)\n */\n\nstatic inline struct cmsghdr * __cmsg_nxthdr(void *__ctl, __kernel_size_t __size,\n\t\t\t\t\t       struct cmsghdr *__cmsg)\n{\n\tstruct cmsghdr * __ptr;\n\n\t__ptr = (struct cmsghdr*)(((unsigned char *) __cmsg) +  CMSG_ALIGN(__cmsg->cmsg_len));\n\tif ((unsigned long)((char*)(__ptr+1) - (char *) __ctl) > __size)\n\t\treturn (struct cmsghdr *)0;\n\n\treturn __ptr;\n}\n\nstatic inline struct cmsghdr * cmsg_nxthdr (struct msghdr *__msg, struct cmsghdr *__cmsg)\n{\n\treturn __cmsg_nxthdr(__msg->msg_control, __msg->msg_controllen, __cmsg);\n}\n\nstatic inline size_t msg_data_left(struct msghdr *msg)\n{\n\treturn iov_iter_count(&msg->msg_iter);\n}\n\n/* \"Socket\"-level control message types: */\n\n#define\tSCM_RIGHTS\t0x01\t\t/* rw: access rights (array of int) */\n#define SCM_CREDENTIALS 0x02\t\t/* rw: struct ucred\t\t*/\n#define SCM_SECURITY\t0x03\t\t/* rw: security label\t\t*/\n\nstruct ucred {\n\t__u32\tpid;\n\t__u32\tuid;\n\t__u32\tgid;\n};\n\n/* Supported address families. */\n#define AF_UNSPEC\t0\n#define AF_UNIX\t\t1\t/* Unix domain sockets \t\t*/\n#define AF_LOCAL\t1\t/* POSIX name for AF_UNIX\t*/\n#define AF_INET\t\t2\t/* Internet IP Protocol \t*/\n#define AF_AX25\t\t3\t/* Amateur Radio AX.25 \t\t*/\n#define AF_IPX\t\t4\t/* Novell IPX \t\t\t*/\n#define AF_APPLETALK\t5\t/* AppleTalk DDP \t\t*/\n#define AF_NETROM\t6\t/* Amateur Radio NET/ROM \t*/\n#define AF_BRIDGE\t7\t/* Multiprotocol bridge \t*/\n#define AF_ATMPVC\t8\t/* ATM PVCs\t\t\t*/\n#define AF_X25\t\t9\t/* Reserved for X.25 project \t*/\n#define AF_INET6\t10\t/* IP version 6\t\t\t*/\n#define AF_ROSE\t\t11\t/* Amateur Radio X.25 PLP\t*/\n#define AF_DECnet\t12\t/* Reserved for DECnet project\t*/\n#define AF_NETBEUI\t13\t/* Reserved for 802.2LLC project*/\n#define AF_SECURITY\t14\t/* Security callback pseudo AF */\n#define AF_KEY\t\t15      /* PF_KEY key management API */\n#define AF_NETLINK\t16\n#define AF_ROUTE\tAF_NETLINK /* Alias to emulate 4.4BSD */\n#define AF_PACKET\t17\t/* Packet family\t\t*/\n#define AF_ASH\t\t18\t/* Ash\t\t\t\t*/\n#define AF_ECONET\t19\t/* Acorn Econet\t\t\t*/\n#define AF_ATMSVC\t20\t/* ATM SVCs\t\t\t*/\n#define AF_RDS\t\t21\t/* RDS sockets \t\t\t*/\n#define AF_SNA\t\t22\t/* Linux SNA Project (nutters!) */\n#define AF_IRDA\t\t23\t/* IRDA sockets\t\t\t*/\n#define AF_PPPOX\t24\t/* PPPoX sockets\t\t*/\n#define AF_WANPIPE\t25\t/* Wanpipe API Sockets */\n#define AF_LLC\t\t26\t/* Linux LLC\t\t\t*/\n#define AF_IB\t\t27\t/* Native InfiniBand address\t*/\n#define AF_MPLS\t\t28\t/* MPLS */\n#define AF_CAN\t\t29\t/* Controller Area Network      */\n#define AF_TIPC\t\t30\t/* TIPC sockets\t\t\t*/\n#define AF_BLUETOOTH\t31\t/* Bluetooth sockets \t\t*/\n#define AF_IUCV\t\t32\t/* IUCV sockets\t\t\t*/\n#define AF_RXRPC\t33\t/* RxRPC sockets \t\t*/\n#define AF_ISDN\t\t34\t/* mISDN sockets \t\t*/\n#define AF_PHONET\t35\t/* Phonet sockets\t\t*/\n#define AF_IEEE802154\t36\t/* IEEE802154 sockets\t\t*/\n#define AF_CAIF\t\t37\t/* CAIF sockets\t\t\t*/\n#define AF_ALG\t\t38\t/* Algorithm sockets\t\t*/\n#define AF_NFC\t\t39\t/* NFC sockets\t\t\t*/\n#define AF_VSOCK\t40\t/* vSockets\t\t\t*/\n#define AF_KCM\t\t41\t/* Kernel Connection Multiplexor*/\n#define AF_QIPCRTR\t42\t/* Qualcomm IPC Router          */\n#define AF_SMC\t\t43\t/* smc sockets: reserve number for\n\t\t\t\t * PF_SMC protocol family that\n\t\t\t\t * reuses AF_INET address family\n\t\t\t\t */\n#define AF_XDP\t\t44\t/* XDP sockets\t\t\t*/\n\n#define AF_MAX\t\t45\t/* For now.. */\n\n/* Protocol families, same as address families. */\n#define PF_UNSPEC\tAF_UNSPEC\n#define PF_UNIX\t\tAF_UNIX\n#define PF_LOCAL\tAF_LOCAL\n#define PF_INET\t\tAF_INET\n#define PF_AX25\t\tAF_AX25\n#define PF_IPX\t\tAF_IPX\n#define PF_APPLETALK\tAF_APPLETALK\n#define\tPF_NETROM\tAF_NETROM\n#define PF_BRIDGE\tAF_BRIDGE\n#define PF_ATMPVC\tAF_ATMPVC\n#define PF_X25\t\tAF_X25\n#define PF_INET6\tAF_INET6\n#define PF_ROSE\t\tAF_ROSE\n#define PF_DECnet\tAF_DECnet\n#define PF_NETBEUI\tAF_NETBEUI\n#define PF_SECURITY\tAF_SECURITY\n#define PF_KEY\t\tAF_KEY\n#define PF_NETLINK\tAF_NETLINK\n#define PF_ROUTE\tAF_ROUTE\n#define PF_PACKET\tAF_PACKET\n#define PF_ASH\t\tAF_ASH\n#define PF_ECONET\tAF_ECONET\n#define PF_ATMSVC\tAF_ATMSVC\n#define PF_RDS\t\tAF_RDS\n#define PF_SNA\t\tAF_SNA\n#define PF_IRDA\t\tAF_IRDA\n#define PF_PPPOX\tAF_PPPOX\n#define PF_WANPIPE\tAF_WANPIPE\n#define PF_LLC\t\tAF_LLC\n#define PF_IB\t\tAF_IB\n#define PF_MPLS\t\tAF_MPLS\n#define PF_CAN\t\tAF_CAN\n#define PF_TIPC\t\tAF_TIPC\n#define PF_BLUETOOTH\tAF_BLUETOOTH\n#define PF_IUCV\t\tAF_IUCV\n#define PF_RXRPC\tAF_RXRPC\n#define PF_ISDN\t\tAF_ISDN\n#define PF_PHONET\tAF_PHONET\n#define PF_IEEE802154\tAF_IEEE802154\n#define PF_CAIF\t\tAF_CAIF\n#define PF_ALG\t\tAF_ALG\n#define PF_NFC\t\tAF_NFC\n#define PF_VSOCK\tAF_VSOCK\n#define PF_KCM\t\tAF_KCM\n#define PF_QIPCRTR\tAF_QIPCRTR\n#define PF_SMC\t\tAF_SMC\n#define PF_XDP\t\tAF_XDP\n#define PF_MAX\t\tAF_MAX\n\n/* Maximum queue length specifiable by listen.  */\n#define SOMAXCONN\t4096\n\n/* Flags we can use with send/ and recv.\n   Added those for 1003.1g not all are supported yet\n */\n\n#define MSG_OOB\t\t1\n#define MSG_PEEK\t2\n#define MSG_DONTROUTE\t4\n#define MSG_TRYHARD     4       /* Synonym for MSG_DONTROUTE for DECnet */\n#define MSG_CTRUNC\t8\n#define MSG_PROBE\t0x10\t/* Do not send. Only probe path f.e. for MTU */\n#define MSG_TRUNC\t0x20\n#define MSG_DONTWAIT\t0x40\t/* Nonblocking io\t\t */\n#define MSG_EOR         0x80\t/* End of record */\n#define MSG_WAITALL\t0x100\t/* Wait for a full request */\n#define MSG_FIN         0x200\n#define MSG_SYN\t\t0x400\n#define MSG_CONFIRM\t0x800\t/* Confirm path validity */\n#define MSG_RST\t\t0x1000\n#define MSG_ERRQUEUE\t0x2000\t/* Fetch message from error queue */\n#define MSG_NOSIGNAL\t0x4000\t/* Do not generate SIGPIPE */\n#define MSG_MORE\t0x8000\t/* Sender will send more */\n#define MSG_WAITFORONE\t0x10000\t/* recvmmsg(): block until 1+ packets avail */\n#define MSG_SENDPAGE_NOPOLICY 0x10000 /* sendpage() internal : do no apply policy */\n#define MSG_SENDPAGE_NOTLAST 0x20000 /* sendpage() internal : not the last page */\n#define MSG_BATCH\t0x40000 /* sendmmsg(): more messages coming */\n#define MSG_EOF         MSG_FIN\n#define MSG_NO_SHARED_FRAGS 0x80000 /* sendpage() internal : page frags are not shared */\n#define MSG_SENDPAGE_DECRYPTED\t0x100000 /* sendpage() internal : page may carry\n\t\t\t\t\t  * plain text and require encryption\n\t\t\t\t\t  */\n\n#define MSG_ZEROCOPY\t0x4000000\t/* Use user data in kernel path */\n#define MSG_FASTOPEN\t0x20000000\t/* Send data in TCP SYN */\n#define MSG_CMSG_CLOEXEC 0x40000000\t/* Set close_on_exec for file\n\t\t\t\t\t   descriptor received through\n\t\t\t\t\t   SCM_RIGHTS */\n#if defined(CONFIG_COMPAT)\n#define MSG_CMSG_COMPAT\t0x80000000\t/* This message needs 32 bit fixups */\n#else\n#define MSG_CMSG_COMPAT\t0\t\t/* We never have 32 bit fixups */\n#endif\n\n\n/* Setsockoptions(2) level. Thanks to BSD these must match IPPROTO_xxx */\n#define SOL_IP\t\t0\n/* #define SOL_ICMP\t1\tNo-no-no! Due to Linux :-) we cannot use SOL_ICMP=1 */\n#define SOL_TCP\t\t6\n#define SOL_UDP\t\t17\n#define SOL_IPV6\t41\n#define SOL_ICMPV6\t58\n#define SOL_SCTP\t132\n#define SOL_UDPLITE\t136     /* UDP-Lite (RFC 3828) */\n#define SOL_RAW\t\t255\n#define SOL_IPX\t\t256\n#define SOL_AX25\t257\n#define SOL_ATALK\t258\n#define SOL_NETROM\t259\n#define SOL_ROSE\t260\n#define SOL_DECNET\t261\n#define\tSOL_X25\t\t262\n#define SOL_PACKET\t263\n#define SOL_ATM\t\t264\t/* ATM layer (cell level) */\n#define SOL_AAL\t\t265\t/* ATM Adaption Layer (packet level) */\n#define SOL_IRDA        266\n#define SOL_NETBEUI\t267\n#define SOL_LLC\t\t268\n#define SOL_DCCP\t269\n#define SOL_NETLINK\t270\n#define SOL_TIPC\t271\n#define SOL_RXRPC\t272\n#define SOL_PPPOL2TP\t273\n#define SOL_BLUETOOTH\t274\n#define SOL_PNPIPE\t275\n#define SOL_RDS\t\t276\n#define SOL_IUCV\t277\n#define SOL_CAIF\t278\n#define SOL_ALG\t\t279\n#define SOL_NFC\t\t280\n#define SOL_KCM\t\t281\n#define SOL_TLS\t\t282\n#define SOL_XDP\t\t283\n\n/* IPX options */\n#define IPX_TYPE\t1\n\nextern int move_addr_to_kernel(void __user *uaddr, int ulen, struct sockaddr_storage *kaddr);\nextern int put_cmsg(struct msghdr*, int level, int type, int len, void *data);\n\nstruct timespec64;\nstruct __kernel_timespec;\nstruct old_timespec32;\n\nstruct scm_timestamping_internal {\n\tstruct timespec64 ts[3];\n};\n\nextern void put_cmsg_scm_timestamping64(struct msghdr *msg, struct scm_timestamping_internal *tss);\nextern void put_cmsg_scm_timestamping(struct msghdr *msg, struct scm_timestamping_internal *tss);\n\n/* The __sys_...msg variants allow MSG_CMSG_COMPAT iff\n * forbid_cmsg_compat==false\n */\nextern long __sys_recvmsg(int fd, struct user_msghdr __user *msg,\n\t\t\t  unsigned int flags, bool forbid_cmsg_compat);\nextern long __sys_sendmsg(int fd, struct user_msghdr __user *msg,\n\t\t\t  unsigned int flags, bool forbid_cmsg_compat);\nextern int __sys_recvmmsg(int fd, struct mmsghdr __user *mmsg,\n\t\t\t  unsigned int vlen, unsigned int flags,\n\t\t\t  struct __kernel_timespec __user *timeout,\n\t\t\t  struct old_timespec32 __user *timeout32);\nextern int __sys_sendmmsg(int fd, struct mmsghdr __user *mmsg,\n\t\t\t  unsigned int vlen, unsigned int flags,\n\t\t\t  bool forbid_cmsg_compat);\nextern long __sys_sendmsg_sock(struct socket *sock, struct msghdr *msg,\n\t\t\t       unsigned int flags);\nextern long __sys_recvmsg_sock(struct socket *sock, struct msghdr *msg,\n\t\t\t       struct user_msghdr __user *umsg,\n\t\t\t       struct sockaddr __user *uaddr,\n\t\t\t       unsigned int flags);\nextern int sendmsg_copy_msghdr(struct msghdr *msg,\n\t\t\t       struct user_msghdr __user *umsg, unsigned flags,\n\t\t\t       struct iovec **iov);\nextern int recvmsg_copy_msghdr(struct msghdr *msg,\n\t\t\t       struct user_msghdr __user *umsg, unsigned flags,\n\t\t\t       struct sockaddr __user **uaddr,\n\t\t\t       struct iovec **iov);\nextern int __copy_msghdr_from_user(struct msghdr *kmsg,\n\t\t\t\t   struct user_msghdr __user *umsg,\n\t\t\t\t   struct sockaddr __user **save_addr,\n\t\t\t\t   struct iovec __user **uiov, size_t *nsegs);\n\n/* helpers which do the actual work for syscalls */\nextern int __sys_recvfrom(int fd, void __user *ubuf, size_t size,\n\t\t\t  unsigned int flags, struct sockaddr __user *addr,\n\t\t\t  int __user *addr_len);\nextern int __sys_sendto(int fd, void __user *buff, size_t len,\n\t\t\tunsigned int flags, struct sockaddr __user *addr,\n\t\t\tint addr_len);\nextern int __sys_accept4_file(struct file *file, unsigned file_flags,\n\t\t\tstruct sockaddr __user *upeer_sockaddr,\n\t\t\t int __user *upeer_addrlen, int flags,\n\t\t\t unsigned long nofile);\nextern int __sys_accept4(int fd, struct sockaddr __user *upeer_sockaddr,\n\t\t\t int __user *upeer_addrlen, int flags);\nextern int __sys_socket(int family, int type, int protocol);\nextern int __sys_bind(int fd, struct sockaddr __user *umyaddr, int addrlen);\nextern int __sys_connect_file(struct file *file, struct sockaddr_storage *addr,\n\t\t\t      int addrlen, int file_flags);\nextern int __sys_connect(int fd, struct sockaddr __user *uservaddr,\n\t\t\t int addrlen);\nextern int __sys_listen(int fd, int backlog);\nextern int __sys_getsockname(int fd, struct sockaddr __user *usockaddr,\n\t\t\t     int __user *usockaddr_len);\nextern int __sys_getpeername(int fd, struct sockaddr __user *usockaddr,\n\t\t\t     int __user *usockaddr_len);\nextern int __sys_socketpair(int family, int type, int protocol,\n\t\t\t    int __user *usockvec);\nextern int __sys_shutdown_sock(struct socket *sock, int how);\nextern int __sys_shutdown(int fd, int how);\n\nextern struct ns_common *get_net_ns(struct ns_common *ns);\n#endif /* _LINUX_SOCKET_H */\n"}, "19": {"id": 19, "path": "/src/include/linux/build_bug.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_BUILD_BUG_H\n#define _LINUX_BUILD_BUG_H\n\n#include <linux/compiler.h>\n\n#ifdef __CHECKER__\n#define BUILD_BUG_ON_ZERO(e) (0)\n#else /* __CHECKER__ */\n/*\n * Force a compilation error if condition is true, but also produce a\n * result (of value 0 and type int), so the expression can be used\n * e.g. in a structure initializer (or where-ever else comma expressions\n * aren't permitted).\n */\n#define BUILD_BUG_ON_ZERO(e) ((int)(sizeof(struct { int:(-!!(e)); })))\n#endif /* __CHECKER__ */\n\n/* Force a compilation error if a constant expression is not a power of 2 */\n#define __BUILD_BUG_ON_NOT_POWER_OF_2(n)\t\\\n\tBUILD_BUG_ON(((n) & ((n) - 1)) != 0)\n#define BUILD_BUG_ON_NOT_POWER_OF_2(n)\t\t\t\\\n\tBUILD_BUG_ON((n) == 0 || (((n) & ((n) - 1)) != 0))\n\n/*\n * BUILD_BUG_ON_INVALID() permits the compiler to check the validity of the\n * expression but avoids the generation of any code, even if that expression\n * has side-effects.\n */\n#define BUILD_BUG_ON_INVALID(e) ((void)(sizeof((__force long)(e))))\n\n/**\n * BUILD_BUG_ON_MSG - break compile if a condition is true & emit supplied\n *\t\t      error message.\n * @condition: the condition which the compiler should know is false.\n *\n * See BUILD_BUG_ON for description.\n */\n#define BUILD_BUG_ON_MSG(cond, msg) compiletime_assert(!(cond), msg)\n\n/**\n * BUILD_BUG_ON - break compile if a condition is true.\n * @condition: the condition which the compiler should know is false.\n *\n * If you have some code which relies on certain constants being equal, or\n * some other compile-time-evaluated condition, you should use BUILD_BUG_ON to\n * detect if someone changes it.\n */\n#define BUILD_BUG_ON(condition) \\\n\tBUILD_BUG_ON_MSG(condition, \"BUILD_BUG_ON failed: \" #condition)\n\n/**\n * BUILD_BUG - break compile if used.\n *\n * If you have some code that you expect the compiler to eliminate at\n * build time, you should use BUILD_BUG to detect if it is\n * unexpectedly used.\n */\n#define BUILD_BUG() BUILD_BUG_ON_MSG(1, \"BUILD_BUG failed\")\n\n/**\n * static_assert - check integer constant expression at build time\n *\n * static_assert() is a wrapper for the C11 _Static_assert, with a\n * little macro magic to make the message optional (defaulting to the\n * stringification of the tested expression).\n *\n * Contrary to BUILD_BUG_ON(), static_assert() can be used at global\n * scope, but requires the expression to be an integer constant\n * expression (i.e., it is not enough that __builtin_constant_p() is\n * true for expr).\n *\n * Also note that BUILD_BUG_ON() fails the build if the condition is\n * true, while static_assert() fails the build if the expression is\n * false.\n */\n#define static_assert(expr, ...) __static_assert(expr, ##__VA_ARGS__, #expr)\n#define __static_assert(expr, msg, ...) _Static_assert(expr, msg)\n\n#endif\t/* _LINUX_BUILD_BUG_H */\n"}, "20": {"id": 20, "path": "/src/net/wireless/nl80211.c", "content": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n * This is the new netlink-based wireless configuration interface.\n *\n * Copyright 2006-2010\tJohannes Berg <johannes@sipsolutions.net>\n * Copyright 2013-2014  Intel Mobile Communications GmbH\n * Copyright 2015-2017\tIntel Deutschland GmbH\n * Copyright (C) 2018-2020 Intel Corporation\n */\n\n#include <linux/if.h>\n#include <linux/module.h>\n#include <linux/err.h>\n#include <linux/slab.h>\n#include <linux/list.h>\n#include <linux/if_ether.h>\n#include <linux/ieee80211.h>\n#include <linux/nl80211.h>\n#include <linux/rtnetlink.h>\n#include <linux/netlink.h>\n#include <linux/nospec.h>\n#include <linux/etherdevice.h>\n#include <linux/if_vlan.h>\n#include <net/net_namespace.h>\n#include <net/genetlink.h>\n#include <net/cfg80211.h>\n#include <net/sock.h>\n#include <net/inet_connection_sock.h>\n#include \"core.h\"\n#include \"nl80211.h\"\n#include \"reg.h\"\n#include \"rdev-ops.h\"\n\nstatic int nl80211_crypto_settings(struct cfg80211_registered_device *rdev,\n\t\t\t\t   struct genl_info *info,\n\t\t\t\t   struct cfg80211_crypto_settings *settings,\n\t\t\t\t   int cipher_limit);\n\n/* the netlink family */\nstatic struct genl_family nl80211_fam;\n\n/* multicast groups */\nenum nl80211_multicast_groups {\n\tNL80211_MCGRP_CONFIG,\n\tNL80211_MCGRP_SCAN,\n\tNL80211_MCGRP_REGULATORY,\n\tNL80211_MCGRP_MLME,\n\tNL80211_MCGRP_VENDOR,\n\tNL80211_MCGRP_NAN,\n\tNL80211_MCGRP_TESTMODE /* keep last - ifdef! */\n};\n\nstatic const struct genl_multicast_group nl80211_mcgrps[] = {\n\t[NL80211_MCGRP_CONFIG] = { .name = NL80211_MULTICAST_GROUP_CONFIG },\n\t[NL80211_MCGRP_SCAN] = { .name = NL80211_MULTICAST_GROUP_SCAN },\n\t[NL80211_MCGRP_REGULATORY] = { .name = NL80211_MULTICAST_GROUP_REG },\n\t[NL80211_MCGRP_MLME] = { .name = NL80211_MULTICAST_GROUP_MLME },\n\t[NL80211_MCGRP_VENDOR] = { .name = NL80211_MULTICAST_GROUP_VENDOR },\n\t[NL80211_MCGRP_NAN] = { .name = NL80211_MULTICAST_GROUP_NAN },\n#ifdef CONFIG_NL80211_TESTMODE\n\t[NL80211_MCGRP_TESTMODE] = { .name = NL80211_MULTICAST_GROUP_TESTMODE }\n#endif\n};\n\n/* returns ERR_PTR values */\nstatic struct wireless_dev *\n__cfg80211_wdev_from_attrs(struct cfg80211_registered_device *rdev,\n\t\t\t   struct net *netns, struct nlattr **attrs)\n{\n\tstruct wireless_dev *result = NULL;\n\tbool have_ifidx = attrs[NL80211_ATTR_IFINDEX];\n\tbool have_wdev_id = attrs[NL80211_ATTR_WDEV];\n\tu64 wdev_id;\n\tint wiphy_idx = -1;\n\tint ifidx = -1;\n\n\tif (!have_ifidx && !have_wdev_id)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (have_ifidx)\n\t\tifidx = nla_get_u32(attrs[NL80211_ATTR_IFINDEX]);\n\tif (have_wdev_id) {\n\t\twdev_id = nla_get_u64(attrs[NL80211_ATTR_WDEV]);\n\t\twiphy_idx = wdev_id >> 32;\n\t}\n\n\tif (rdev) {\n\t\tstruct wireless_dev *wdev;\n\n\t\tlockdep_assert_held(&rdev->wiphy.mtx);\n\n\t\tlist_for_each_entry(wdev, &rdev->wiphy.wdev_list, list) {\n\t\t\tif (have_ifidx && wdev->netdev &&\n\t\t\t    wdev->netdev->ifindex == ifidx) {\n\t\t\t\tresult = wdev;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (have_wdev_id && wdev->identifier == (u32)wdev_id) {\n\t\t\t\tresult = wdev;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\treturn result ?: ERR_PTR(-ENODEV);\n\t}\n\n\tASSERT_RTNL();\n\n\tlist_for_each_entry(rdev, &cfg80211_rdev_list, list) {\n\t\tstruct wireless_dev *wdev;\n\n\t\tif (wiphy_net(&rdev->wiphy) != netns)\n\t\t\tcontinue;\n\n\t\tif (have_wdev_id && rdev->wiphy_idx != wiphy_idx)\n\t\t\tcontinue;\n\n\t\tlist_for_each_entry(wdev, &rdev->wiphy.wdev_list, list) {\n\t\t\tif (have_ifidx && wdev->netdev &&\n\t\t\t    wdev->netdev->ifindex == ifidx) {\n\t\t\t\tresult = wdev;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (have_wdev_id && wdev->identifier == (u32)wdev_id) {\n\t\t\t\tresult = wdev;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (result)\n\t\t\tbreak;\n\t}\n\n\tif (result)\n\t\treturn result;\n\treturn ERR_PTR(-ENODEV);\n}\n\nstatic struct cfg80211_registered_device *\n__cfg80211_rdev_from_attrs(struct net *netns, struct nlattr **attrs)\n{\n\tstruct cfg80211_registered_device *rdev = NULL, *tmp;\n\tstruct net_device *netdev;\n\n\tASSERT_RTNL();\n\n\tif (!attrs[NL80211_ATTR_WIPHY] &&\n\t    !attrs[NL80211_ATTR_IFINDEX] &&\n\t    !attrs[NL80211_ATTR_WDEV])\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (attrs[NL80211_ATTR_WIPHY])\n\t\trdev = cfg80211_rdev_by_wiphy_idx(\n\t\t\t\tnla_get_u32(attrs[NL80211_ATTR_WIPHY]));\n\n\tif (attrs[NL80211_ATTR_WDEV]) {\n\t\tu64 wdev_id = nla_get_u64(attrs[NL80211_ATTR_WDEV]);\n\t\tstruct wireless_dev *wdev;\n\t\tbool found = false;\n\n\t\ttmp = cfg80211_rdev_by_wiphy_idx(wdev_id >> 32);\n\t\tif (tmp) {\n\t\t\t/* make sure wdev exists */\n\t\t\tlist_for_each_entry(wdev, &tmp->wiphy.wdev_list, list) {\n\t\t\t\tif (wdev->identifier != (u32)wdev_id)\n\t\t\t\t\tcontinue;\n\t\t\t\tfound = true;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (!found)\n\t\t\t\ttmp = NULL;\n\n\t\t\tif (rdev && tmp != rdev)\n\t\t\t\treturn ERR_PTR(-EINVAL);\n\t\t\trdev = tmp;\n\t\t}\n\t}\n\n\tif (attrs[NL80211_ATTR_IFINDEX]) {\n\t\tint ifindex = nla_get_u32(attrs[NL80211_ATTR_IFINDEX]);\n\n\t\tnetdev = __dev_get_by_index(netns, ifindex);\n\t\tif (netdev) {\n\t\t\tif (netdev->ieee80211_ptr)\n\t\t\t\ttmp = wiphy_to_rdev(\n\t\t\t\t\tnetdev->ieee80211_ptr->wiphy);\n\t\t\telse\n\t\t\t\ttmp = NULL;\n\n\t\t\t/* not wireless device -- return error */\n\t\t\tif (!tmp)\n\t\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\t\t/* mismatch -- return error */\n\t\t\tif (rdev && tmp != rdev)\n\t\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\t\trdev = tmp;\n\t\t}\n\t}\n\n\tif (!rdev)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tif (netns != wiphy_net(&rdev->wiphy))\n\t\treturn ERR_PTR(-ENODEV);\n\n\treturn rdev;\n}\n\n/*\n * This function returns a pointer to the driver\n * that the genl_info item that is passed refers to.\n *\n * The result of this can be a PTR_ERR and hence must\n * be checked with IS_ERR() for errors.\n */\nstatic struct cfg80211_registered_device *\ncfg80211_get_dev_from_info(struct net *netns, struct genl_info *info)\n{\n\treturn __cfg80211_rdev_from_attrs(netns, info->attrs);\n}\n\nstatic int validate_beacon_head(const struct nlattr *attr,\n\t\t\t\tstruct netlink_ext_ack *extack)\n{\n\tconst u8 *data = nla_data(attr);\n\tunsigned int len = nla_len(attr);\n\tconst struct element *elem;\n\tconst struct ieee80211_mgmt *mgmt = (void *)data;\n\tbool s1g_bcn = ieee80211_is_s1g_beacon(mgmt->frame_control);\n\tunsigned int fixedlen, hdrlen;\n\n\tif (s1g_bcn) {\n\t\tfixedlen = offsetof(struct ieee80211_ext,\n\t\t\t\t    u.s1g_beacon.variable);\n\t\thdrlen = offsetof(struct ieee80211_ext, u.s1g_beacon);\n\t} else {\n\t\tfixedlen = offsetof(struct ieee80211_mgmt,\n\t\t\t\t    u.beacon.variable);\n\t\thdrlen = offsetof(struct ieee80211_mgmt, u.beacon);\n\t}\n\n\tif (len < fixedlen)\n\t\tgoto err;\n\n\tif (ieee80211_hdrlen(mgmt->frame_control) != hdrlen)\n\t\tgoto err;\n\n\tdata += fixedlen;\n\tlen -= fixedlen;\n\n\tfor_each_element(elem, data, len) {\n\t\t/* nothing */\n\t}\n\n\tif (for_each_element_completed(elem, data, len))\n\t\treturn 0;\n\nerr:\n\tNL_SET_ERR_MSG_ATTR(extack, attr, \"malformed beacon head\");\n\treturn -EINVAL;\n}\n\nstatic int validate_ie_attr(const struct nlattr *attr,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tconst u8 *data = nla_data(attr);\n\tunsigned int len = nla_len(attr);\n\tconst struct element *elem;\n\n\tfor_each_element(elem, data, len) {\n\t\t/* nothing */\n\t}\n\n\tif (for_each_element_completed(elem, data, len))\n\t\treturn 0;\n\n\tNL_SET_ERR_MSG_ATTR(extack, attr, \"malformed information elements\");\n\treturn -EINVAL;\n}\n\n/* policy for the attributes */\nstatic const struct nla_policy nl80211_policy[NUM_NL80211_ATTR];\n\nstatic const struct nla_policy\nnl80211_ftm_responder_policy[NL80211_FTM_RESP_ATTR_MAX + 1] = {\n\t[NL80211_FTM_RESP_ATTR_ENABLED] = { .type = NLA_FLAG, },\n\t[NL80211_FTM_RESP_ATTR_LCI] = { .type = NLA_BINARY,\n\t\t\t\t\t.len = U8_MAX },\n\t[NL80211_FTM_RESP_ATTR_CIVICLOC] = { .type = NLA_BINARY,\n\t\t\t\t\t     .len = U8_MAX },\n};\n\nstatic const struct nla_policy\nnl80211_pmsr_ftm_req_attr_policy[NL80211_PMSR_FTM_REQ_ATTR_MAX + 1] = {\n\t[NL80211_PMSR_FTM_REQ_ATTR_ASAP] = { .type = NLA_FLAG },\n\t[NL80211_PMSR_FTM_REQ_ATTR_PREAMBLE] = { .type = NLA_U32 },\n\t[NL80211_PMSR_FTM_REQ_ATTR_NUM_BURSTS_EXP] =\n\t\tNLA_POLICY_MAX(NLA_U8, 15),\n\t[NL80211_PMSR_FTM_REQ_ATTR_BURST_PERIOD] = { .type = NLA_U16 },\n\t[NL80211_PMSR_FTM_REQ_ATTR_BURST_DURATION] =\n\t\tNLA_POLICY_MAX(NLA_U8, 15),\n\t[NL80211_PMSR_FTM_REQ_ATTR_FTMS_PER_BURST] =\n\t\tNLA_POLICY_MAX(NLA_U8, 31),\n\t[NL80211_PMSR_FTM_REQ_ATTR_NUM_FTMR_RETRIES] = { .type = NLA_U8 },\n\t[NL80211_PMSR_FTM_REQ_ATTR_REQUEST_LCI] = { .type = NLA_FLAG },\n\t[NL80211_PMSR_FTM_REQ_ATTR_REQUEST_CIVICLOC] = { .type = NLA_FLAG },\n\t[NL80211_PMSR_FTM_REQ_ATTR_TRIGGER_BASED] = { .type = NLA_FLAG },\n\t[NL80211_PMSR_FTM_REQ_ATTR_NON_TRIGGER_BASED] = { .type = NLA_FLAG },\n};\n\nstatic const struct nla_policy\nnl80211_pmsr_req_data_policy[NL80211_PMSR_TYPE_MAX + 1] = {\n\t[NL80211_PMSR_TYPE_FTM] =\n\t\tNLA_POLICY_NESTED(nl80211_pmsr_ftm_req_attr_policy),\n};\n\nstatic const struct nla_policy\nnl80211_pmsr_req_attr_policy[NL80211_PMSR_REQ_ATTR_MAX + 1] = {\n\t[NL80211_PMSR_REQ_ATTR_DATA] =\n\t\tNLA_POLICY_NESTED(nl80211_pmsr_req_data_policy),\n\t[NL80211_PMSR_REQ_ATTR_GET_AP_TSF] = { .type = NLA_FLAG },\n};\n\nstatic const struct nla_policy\nnl80211_psmr_peer_attr_policy[NL80211_PMSR_PEER_ATTR_MAX + 1] = {\n\t[NL80211_PMSR_PEER_ATTR_ADDR] = NLA_POLICY_ETH_ADDR,\n\t[NL80211_PMSR_PEER_ATTR_CHAN] = NLA_POLICY_NESTED(nl80211_policy),\n\t[NL80211_PMSR_PEER_ATTR_REQ] =\n\t\tNLA_POLICY_NESTED(nl80211_pmsr_req_attr_policy),\n\t[NL80211_PMSR_PEER_ATTR_RESP] = { .type = NLA_REJECT },\n};\n\nstatic const struct nla_policy\nnl80211_pmsr_attr_policy[NL80211_PMSR_ATTR_MAX + 1] = {\n\t[NL80211_PMSR_ATTR_MAX_PEERS] = { .type = NLA_REJECT },\n\t[NL80211_PMSR_ATTR_REPORT_AP_TSF] = { .type = NLA_REJECT },\n\t[NL80211_PMSR_ATTR_RANDOMIZE_MAC_ADDR] = { .type = NLA_REJECT },\n\t[NL80211_PMSR_ATTR_TYPE_CAPA] = { .type = NLA_REJECT },\n\t[NL80211_PMSR_ATTR_PEERS] =\n\t\tNLA_POLICY_NESTED_ARRAY(nl80211_psmr_peer_attr_policy),\n};\n\nstatic const struct nla_policy\nhe_obss_pd_policy[NL80211_HE_OBSS_PD_ATTR_MAX + 1] = {\n\t[NL80211_HE_OBSS_PD_ATTR_MIN_OFFSET] =\n\t\tNLA_POLICY_RANGE(NLA_U8, 1, 20),\n\t[NL80211_HE_OBSS_PD_ATTR_MAX_OFFSET] =\n\t\tNLA_POLICY_RANGE(NLA_U8, 1, 20),\n\t[NL80211_HE_OBSS_PD_ATTR_NON_SRG_MAX_OFFSET] =\n\t\tNLA_POLICY_RANGE(NLA_U8, 1, 20),\n\t[NL80211_HE_OBSS_PD_ATTR_BSS_COLOR_BITMAP] =\n\t\tNLA_POLICY_EXACT_LEN(8),\n\t[NL80211_HE_OBSS_PD_ATTR_PARTIAL_BSSID_BITMAP] =\n\t\tNLA_POLICY_EXACT_LEN(8),\n\t[NL80211_HE_OBSS_PD_ATTR_SR_CTRL] = { .type = NLA_U8 },\n};\n\nstatic const struct nla_policy\nhe_bss_color_policy[NL80211_HE_BSS_COLOR_ATTR_MAX + 1] = {\n\t[NL80211_HE_BSS_COLOR_ATTR_COLOR] = NLA_POLICY_RANGE(NLA_U8, 1, 63),\n\t[NL80211_HE_BSS_COLOR_ATTR_DISABLED] = { .type = NLA_FLAG },\n\t[NL80211_HE_BSS_COLOR_ATTR_PARTIAL] = { .type = NLA_FLAG },\n};\n\nstatic const struct nla_policy nl80211_txattr_policy[NL80211_TXRATE_MAX + 1] = {\n\t[NL80211_TXRATE_LEGACY] = { .type = NLA_BINARY,\n\t\t\t\t    .len = NL80211_MAX_SUPP_RATES },\n\t[NL80211_TXRATE_HT] = { .type = NLA_BINARY,\n\t\t\t\t.len = NL80211_MAX_SUPP_HT_RATES },\n\t[NL80211_TXRATE_VHT] = NLA_POLICY_EXACT_LEN_WARN(sizeof(struct nl80211_txrate_vht)),\n\t[NL80211_TXRATE_GI] = { .type = NLA_U8 },\n\t[NL80211_TXRATE_HE] = NLA_POLICY_EXACT_LEN(sizeof(struct nl80211_txrate_he)),\n\t[NL80211_TXRATE_HE_GI] =  NLA_POLICY_RANGE(NLA_U8,\n\t\t\t\t\t\t   NL80211_RATE_INFO_HE_GI_0_8,\n\t\t\t\t\t\t   NL80211_RATE_INFO_HE_GI_3_2),\n\t[NL80211_TXRATE_HE_LTF] = NLA_POLICY_RANGE(NLA_U8,\n\t\t\t\t\t\t   NL80211_RATE_INFO_HE_1XLTF,\n\t\t\t\t\t\t   NL80211_RATE_INFO_HE_4XLTF),\n};\n\nstatic const struct nla_policy\nnl80211_tid_config_attr_policy[NL80211_TID_CONFIG_ATTR_MAX + 1] = {\n\t[NL80211_TID_CONFIG_ATTR_VIF_SUPP] = { .type = NLA_U64 },\n\t[NL80211_TID_CONFIG_ATTR_PEER_SUPP] = { .type = NLA_U64 },\n\t[NL80211_TID_CONFIG_ATTR_OVERRIDE] = { .type = NLA_FLAG },\n\t[NL80211_TID_CONFIG_ATTR_TIDS] = NLA_POLICY_RANGE(NLA_U16, 1, 0xff),\n\t[NL80211_TID_CONFIG_ATTR_NOACK] =\n\t\t\tNLA_POLICY_MAX(NLA_U8, NL80211_TID_CONFIG_DISABLE),\n\t[NL80211_TID_CONFIG_ATTR_RETRY_SHORT] = NLA_POLICY_MIN(NLA_U8, 1),\n\t[NL80211_TID_CONFIG_ATTR_RETRY_LONG] = NLA_POLICY_MIN(NLA_U8, 1),\n\t[NL80211_TID_CONFIG_ATTR_AMPDU_CTRL] =\n\t\t\tNLA_POLICY_MAX(NLA_U8, NL80211_TID_CONFIG_DISABLE),\n\t[NL80211_TID_CONFIG_ATTR_RTSCTS_CTRL] =\n\t\t\tNLA_POLICY_MAX(NLA_U8, NL80211_TID_CONFIG_DISABLE),\n\t[NL80211_TID_CONFIG_ATTR_AMSDU_CTRL] =\n\t\t\tNLA_POLICY_MAX(NLA_U8, NL80211_TID_CONFIG_DISABLE),\n\t[NL80211_TID_CONFIG_ATTR_TX_RATE_TYPE] =\n\t\t\tNLA_POLICY_MAX(NLA_U8, NL80211_TX_RATE_FIXED),\n\t[NL80211_TID_CONFIG_ATTR_TX_RATE] =\n\t\t\tNLA_POLICY_NESTED(nl80211_txattr_policy),\n};\n\nstatic const struct nla_policy\nnl80211_fils_discovery_policy[NL80211_FILS_DISCOVERY_ATTR_MAX + 1] = {\n\t[NL80211_FILS_DISCOVERY_ATTR_INT_MIN] = NLA_POLICY_MAX(NLA_U32, 10000),\n\t[NL80211_FILS_DISCOVERY_ATTR_INT_MAX] = NLA_POLICY_MAX(NLA_U32, 10000),\n\tNLA_POLICY_RANGE(NLA_BINARY,\n\t\t\t NL80211_FILS_DISCOVERY_TMPL_MIN_LEN,\n\t\t\t IEEE80211_MAX_DATA_LEN),\n};\n\nstatic const struct nla_policy\nnl80211_unsol_bcast_probe_resp_policy[NL80211_UNSOL_BCAST_PROBE_RESP_ATTR_MAX + 1] = {\n\t[NL80211_UNSOL_BCAST_PROBE_RESP_ATTR_INT] = NLA_POLICY_MAX(NLA_U32, 20),\n\t[NL80211_UNSOL_BCAST_PROBE_RESP_ATTR_TMPL] = { .type = NLA_BINARY,\n\t\t\t\t\t\t       .len = IEEE80211_MAX_DATA_LEN }\n};\n\nstatic const struct nla_policy\nsar_specs_policy[NL80211_SAR_ATTR_SPECS_MAX + 1] = {\n\t[NL80211_SAR_ATTR_SPECS_POWER] = { .type = NLA_S32 },\n\t[NL80211_SAR_ATTR_SPECS_RANGE_INDEX] = {.type = NLA_U32 },\n};\n\nstatic const struct nla_policy\nsar_policy[NL80211_SAR_ATTR_MAX + 1] = {\n\t[NL80211_SAR_ATTR_TYPE] = NLA_POLICY_MAX(NLA_U32, NUM_NL80211_SAR_TYPE),\n\t[NL80211_SAR_ATTR_SPECS] = NLA_POLICY_NESTED_ARRAY(sar_specs_policy),\n};\n\nstatic const struct nla_policy nl80211_policy[NUM_NL80211_ATTR] = {\n\t[0] = { .strict_start_type = NL80211_ATTR_HE_OBSS_PD },\n\t[NL80211_ATTR_WIPHY] = { .type = NLA_U32 },\n\t[NL80211_ATTR_WIPHY_NAME] = { .type = NLA_NUL_STRING,\n\t\t\t\t      .len = 20-1 },\n\t[NL80211_ATTR_WIPHY_TXQ_PARAMS] = { .type = NLA_NESTED },\n\n\t[NL80211_ATTR_WIPHY_FREQ] = { .type = NLA_U32 },\n\t[NL80211_ATTR_WIPHY_CHANNEL_TYPE] = { .type = NLA_U32 },\n\t[NL80211_ATTR_WIPHY_EDMG_CHANNELS] = NLA_POLICY_RANGE(NLA_U8,\n\t\t\t\t\t\tNL80211_EDMG_CHANNELS_MIN,\n\t\t\t\t\t\tNL80211_EDMG_CHANNELS_MAX),\n\t[NL80211_ATTR_WIPHY_EDMG_BW_CONFIG] = NLA_POLICY_RANGE(NLA_U8,\n\t\t\t\t\t\tNL80211_EDMG_BW_CONFIG_MIN,\n\t\t\t\t\t\tNL80211_EDMG_BW_CONFIG_MAX),\n\n\t[NL80211_ATTR_CHANNEL_WIDTH] = { .type = NLA_U32 },\n\t[NL80211_ATTR_CENTER_FREQ1] = { .type = NLA_U32 },\n\t[NL80211_ATTR_CENTER_FREQ1_OFFSET] = NLA_POLICY_RANGE(NLA_U32, 0, 999),\n\t[NL80211_ATTR_CENTER_FREQ2] = { .type = NLA_U32 },\n\n\t[NL80211_ATTR_WIPHY_RETRY_SHORT] = NLA_POLICY_MIN(NLA_U8, 1),\n\t[NL80211_ATTR_WIPHY_RETRY_LONG] = NLA_POLICY_MIN(NLA_U8, 1),\n\t[NL80211_ATTR_WIPHY_FRAG_THRESHOLD] = { .type = NLA_U32 },\n\t[NL80211_ATTR_WIPHY_RTS_THRESHOLD] = { .type = NLA_U32 },\n\t[NL80211_ATTR_WIPHY_COVERAGE_CLASS] = { .type = NLA_U8 },\n\t[NL80211_ATTR_WIPHY_DYN_ACK] = { .type = NLA_FLAG },\n\n\t[NL80211_ATTR_IFTYPE] = NLA_POLICY_MAX(NLA_U32, NL80211_IFTYPE_MAX),\n\t[NL80211_ATTR_IFINDEX] = { .type = NLA_U32 },\n\t[NL80211_ATTR_IFNAME] = { .type = NLA_NUL_STRING, .len = IFNAMSIZ-1 },\n\n\t[NL80211_ATTR_MAC] = NLA_POLICY_EXACT_LEN_WARN(ETH_ALEN),\n\t[NL80211_ATTR_PREV_BSSID] = NLA_POLICY_EXACT_LEN_WARN(ETH_ALEN),\n\n\t[NL80211_ATTR_KEY] = { .type = NLA_NESTED, },\n\t[NL80211_ATTR_KEY_DATA] = { .type = NLA_BINARY,\n\t\t\t\t    .len = WLAN_MAX_KEY_LEN },\n\t[NL80211_ATTR_KEY_IDX] = NLA_POLICY_MAX(NLA_U8, 7),\n\t[NL80211_ATTR_KEY_CIPHER] = { .type = NLA_U32 },\n\t[NL80211_ATTR_KEY_DEFAULT] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_KEY_SEQ] = { .type = NLA_BINARY, .len = 16 },\n\t[NL80211_ATTR_KEY_TYPE] =\n\t\tNLA_POLICY_MAX(NLA_U32, NUM_NL80211_KEYTYPES),\n\n\t[NL80211_ATTR_BEACON_INTERVAL] = { .type = NLA_U32 },\n\t[NL80211_ATTR_DTIM_PERIOD] = { .type = NLA_U32 },\n\t[NL80211_ATTR_BEACON_HEAD] =\n\t\tNLA_POLICY_VALIDATE_FN(NLA_BINARY, validate_beacon_head,\n\t\t\t\t       IEEE80211_MAX_DATA_LEN),\n\t[NL80211_ATTR_BEACON_TAIL] =\n\t\tNLA_POLICY_VALIDATE_FN(NLA_BINARY, validate_ie_attr,\n\t\t\t\t       IEEE80211_MAX_DATA_LEN),\n\t[NL80211_ATTR_STA_AID] =\n\t\tNLA_POLICY_RANGE(NLA_U16, 1, IEEE80211_MAX_AID),\n\t[NL80211_ATTR_STA_FLAGS] = { .type = NLA_NESTED },\n\t[NL80211_ATTR_STA_LISTEN_INTERVAL] = { .type = NLA_U16 },\n\t[NL80211_ATTR_STA_SUPPORTED_RATES] = { .type = NLA_BINARY,\n\t\t\t\t\t       .len = NL80211_MAX_SUPP_RATES },\n\t[NL80211_ATTR_STA_PLINK_ACTION] =\n\t\tNLA_POLICY_MAX(NLA_U8, NUM_NL80211_PLINK_ACTIONS - 1),\n\t[NL80211_ATTR_STA_TX_POWER_SETTING] =\n\t\tNLA_POLICY_RANGE(NLA_U8,\n\t\t\t\t NL80211_TX_POWER_AUTOMATIC,\n\t\t\t\t NL80211_TX_POWER_FIXED),\n\t[NL80211_ATTR_STA_TX_POWER] = { .type = NLA_S16 },\n\t[NL80211_ATTR_STA_VLAN] = { .type = NLA_U32 },\n\t[NL80211_ATTR_MNTR_FLAGS] = { /* NLA_NESTED can't be empty */ },\n\t[NL80211_ATTR_MESH_ID] = { .type = NLA_BINARY,\n\t\t\t\t   .len = IEEE80211_MAX_MESH_ID_LEN },\n\t[NL80211_ATTR_MPATH_NEXT_HOP] = NLA_POLICY_ETH_ADDR_COMPAT,\n\n\t[NL80211_ATTR_REG_ALPHA2] = { .type = NLA_STRING, .len = 2 },\n\t[NL80211_ATTR_REG_RULES] = { .type = NLA_NESTED },\n\n\t[NL80211_ATTR_BSS_CTS_PROT] = { .type = NLA_U8 },\n\t[NL80211_ATTR_BSS_SHORT_PREAMBLE] = { .type = NLA_U8 },\n\t[NL80211_ATTR_BSS_SHORT_SLOT_TIME] = { .type = NLA_U8 },\n\t[NL80211_ATTR_BSS_BASIC_RATES] = { .type = NLA_BINARY,\n\t\t\t\t\t   .len = NL80211_MAX_SUPP_RATES },\n\t[NL80211_ATTR_BSS_HT_OPMODE] = { .type = NLA_U16 },\n\n\t[NL80211_ATTR_MESH_CONFIG] = { .type = NLA_NESTED },\n\t[NL80211_ATTR_SUPPORT_MESH_AUTH] = { .type = NLA_FLAG },\n\n\t[NL80211_ATTR_HT_CAPABILITY] = NLA_POLICY_EXACT_LEN_WARN(NL80211_HT_CAPABILITY_LEN),\n\n\t[NL80211_ATTR_MGMT_SUBTYPE] = { .type = NLA_U8 },\n\t[NL80211_ATTR_IE] = NLA_POLICY_VALIDATE_FN(NLA_BINARY,\n\t\t\t\t\t\t   validate_ie_attr,\n\t\t\t\t\t\t   IEEE80211_MAX_DATA_LEN),\n\t[NL80211_ATTR_SCAN_FREQUENCIES] = { .type = NLA_NESTED },\n\t[NL80211_ATTR_SCAN_SSIDS] = { .type = NLA_NESTED },\n\n\t[NL80211_ATTR_SSID] = { .type = NLA_BINARY,\n\t\t\t\t.len = IEEE80211_MAX_SSID_LEN },\n\t[NL80211_ATTR_AUTH_TYPE] = { .type = NLA_U32 },\n\t[NL80211_ATTR_REASON_CODE] = { .type = NLA_U16 },\n\t[NL80211_ATTR_FREQ_FIXED] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_TIMED_OUT] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_USE_MFP] = NLA_POLICY_RANGE(NLA_U32,\n\t\t\t\t\t\t  NL80211_MFP_NO,\n\t\t\t\t\t\t  NL80211_MFP_OPTIONAL),\n\t[NL80211_ATTR_STA_FLAGS2] = {\n\t\t.len = sizeof(struct nl80211_sta_flag_update),\n\t},\n\t[NL80211_ATTR_CONTROL_PORT] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_CONTROL_PORT_ETHERTYPE] = { .type = NLA_U16 },\n\t[NL80211_ATTR_CONTROL_PORT_NO_ENCRYPT] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_CONTROL_PORT_OVER_NL80211] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_PRIVACY] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_STATUS_CODE] = { .type = NLA_U16 },\n\t[NL80211_ATTR_CIPHER_SUITE_GROUP] = { .type = NLA_U32 },\n\t[NL80211_ATTR_WPA_VERSIONS] = { .type = NLA_U32 },\n\t[NL80211_ATTR_PID] = { .type = NLA_U32 },\n\t[NL80211_ATTR_4ADDR] = { .type = NLA_U8 },\n\t[NL80211_ATTR_PMKID] = NLA_POLICY_EXACT_LEN_WARN(WLAN_PMKID_LEN),\n\t[NL80211_ATTR_DURATION] = { .type = NLA_U32 },\n\t[NL80211_ATTR_COOKIE] = { .type = NLA_U64 },\n\t[NL80211_ATTR_TX_RATES] = { .type = NLA_NESTED },\n\t[NL80211_ATTR_FRAME] = { .type = NLA_BINARY,\n\t\t\t\t .len = IEEE80211_MAX_DATA_LEN },\n\t[NL80211_ATTR_FRAME_MATCH] = { .type = NLA_BINARY, },\n\t[NL80211_ATTR_PS_STATE] = NLA_POLICY_RANGE(NLA_U32,\n\t\t\t\t\t\t   NL80211_PS_DISABLED,\n\t\t\t\t\t\t   NL80211_PS_ENABLED),\n\t[NL80211_ATTR_CQM] = { .type = NLA_NESTED, },\n\t[NL80211_ATTR_LOCAL_STATE_CHANGE] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_AP_ISOLATE] = { .type = NLA_U8 },\n\t[NL80211_ATTR_WIPHY_TX_POWER_SETTING] = { .type = NLA_U32 },\n\t[NL80211_ATTR_WIPHY_TX_POWER_LEVEL] = { .type = NLA_U32 },\n\t[NL80211_ATTR_FRAME_TYPE] = { .type = NLA_U16 },\n\t[NL80211_ATTR_WIPHY_ANTENNA_TX] = { .type = NLA_U32 },\n\t[NL80211_ATTR_WIPHY_ANTENNA_RX] = { .type = NLA_U32 },\n\t[NL80211_ATTR_MCAST_RATE] = { .type = NLA_U32 },\n\t[NL80211_ATTR_OFFCHANNEL_TX_OK] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_KEY_DEFAULT_TYPES] = { .type = NLA_NESTED },\n\t[NL80211_ATTR_WOWLAN_TRIGGERS] = { .type = NLA_NESTED },\n\t[NL80211_ATTR_STA_PLINK_STATE] =\n\t\tNLA_POLICY_MAX(NLA_U8, NUM_NL80211_PLINK_STATES - 1),\n\t[NL80211_ATTR_MEASUREMENT_DURATION] = { .type = NLA_U16 },\n\t[NL80211_ATTR_MEASUREMENT_DURATION_MANDATORY] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_MESH_PEER_AID] =\n\t\tNLA_POLICY_RANGE(NLA_U16, 1, IEEE80211_MAX_AID),\n\t[NL80211_ATTR_SCHED_SCAN_INTERVAL] = { .type = NLA_U32 },\n\t[NL80211_ATTR_REKEY_DATA] = { .type = NLA_NESTED },\n\t[NL80211_ATTR_SCAN_SUPP_RATES] = { .type = NLA_NESTED },\n\t[NL80211_ATTR_HIDDEN_SSID] =\n\t\tNLA_POLICY_RANGE(NLA_U32,\n\t\t\t\t NL80211_HIDDEN_SSID_NOT_IN_USE,\n\t\t\t\t NL80211_HIDDEN_SSID_ZERO_CONTENTS),\n\t[NL80211_ATTR_IE_PROBE_RESP] =\n\t\tNLA_POLICY_VALIDATE_FN(NLA_BINARY, validate_ie_attr,\n\t\t\t\t       IEEE80211_MAX_DATA_LEN),\n\t[NL80211_ATTR_IE_ASSOC_RESP] =\n\t\tNLA_POLICY_VALIDATE_FN(NLA_BINARY, validate_ie_attr,\n\t\t\t\t       IEEE80211_MAX_DATA_LEN),\n\t[NL80211_ATTR_ROAM_SUPPORT] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_SCHED_SCAN_MATCH] = { .type = NLA_NESTED },\n\t[NL80211_ATTR_TX_NO_CCK_RATE] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_TDLS_ACTION] = { .type = NLA_U8 },\n\t[NL80211_ATTR_TDLS_DIALOG_TOKEN] = { .type = NLA_U8 },\n\t[NL80211_ATTR_TDLS_OPERATION] = { .type = NLA_U8 },\n\t[NL80211_ATTR_TDLS_SUPPORT] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_TDLS_EXTERNAL_SETUP] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_TDLS_INITIATOR] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_DONT_WAIT_FOR_ACK] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_PROBE_RESP] = { .type = NLA_BINARY,\n\t\t\t\t      .len = IEEE80211_MAX_DATA_LEN },\n\t[NL80211_ATTR_DFS_REGION] = { .type = NLA_U8 },\n\t[NL80211_ATTR_DISABLE_HT] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_HT_CAPABILITY_MASK] = {\n\t\t.len = NL80211_HT_CAPABILITY_LEN\n\t},\n\t[NL80211_ATTR_NOACK_MAP] = { .type = NLA_U16 },\n\t[NL80211_ATTR_INACTIVITY_TIMEOUT] = { .type = NLA_U16 },\n\t[NL80211_ATTR_BG_SCAN_PERIOD] = { .type = NLA_U16 },\n\t[NL80211_ATTR_WDEV] = { .type = NLA_U64 },\n\t[NL80211_ATTR_USER_REG_HINT_TYPE] = { .type = NLA_U32 },\n\n\t/* need to include at least Auth Transaction and Status Code */\n\t[NL80211_ATTR_AUTH_DATA] = NLA_POLICY_MIN_LEN(4),\n\n\t[NL80211_ATTR_VHT_CAPABILITY] = NLA_POLICY_EXACT_LEN_WARN(NL80211_VHT_CAPABILITY_LEN),\n\t[NL80211_ATTR_SCAN_FLAGS] = { .type = NLA_U32 },\n\t[NL80211_ATTR_P2P_CTWINDOW] = NLA_POLICY_MAX(NLA_U8, 127),\n\t[NL80211_ATTR_P2P_OPPPS] = NLA_POLICY_MAX(NLA_U8, 1),\n\t[NL80211_ATTR_LOCAL_MESH_POWER_MODE] =\n\t\tNLA_POLICY_RANGE(NLA_U32,\n\t\t\t\t NL80211_MESH_POWER_UNKNOWN + 1,\n\t\t\t\t NL80211_MESH_POWER_MAX),\n\t[NL80211_ATTR_ACL_POLICY] = {. type = NLA_U32 },\n\t[NL80211_ATTR_MAC_ADDRS] = { .type = NLA_NESTED },\n\t[NL80211_ATTR_STA_CAPABILITY] = { .type = NLA_U16 },\n\t[NL80211_ATTR_STA_EXT_CAPABILITY] = { .type = NLA_BINARY, },\n\t[NL80211_ATTR_SPLIT_WIPHY_DUMP] = { .type = NLA_FLAG, },\n\t[NL80211_ATTR_DISABLE_VHT] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_VHT_CAPABILITY_MASK] = {\n\t\t.len = NL80211_VHT_CAPABILITY_LEN,\n\t},\n\t[NL80211_ATTR_MDID] = { .type = NLA_U16 },\n\t[NL80211_ATTR_IE_RIC] = { .type = NLA_BINARY,\n\t\t\t\t  .len = IEEE80211_MAX_DATA_LEN },\n\t[NL80211_ATTR_CRIT_PROT_ID] = { .type = NLA_U16 },\n\t[NL80211_ATTR_MAX_CRIT_PROT_DURATION] =\n\t\tNLA_POLICY_MAX(NLA_U16, NL80211_CRIT_PROTO_MAX_DURATION),\n\t[NL80211_ATTR_PEER_AID] =\n\t\tNLA_POLICY_RANGE(NLA_U16, 1, IEEE80211_MAX_AID),\n\t[NL80211_ATTR_CH_SWITCH_COUNT] = { .type = NLA_U32 },\n\t[NL80211_ATTR_CH_SWITCH_BLOCK_TX] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_CSA_IES] = { .type = NLA_NESTED },\n\t[NL80211_ATTR_CNTDWN_OFFS_BEACON] = { .type = NLA_BINARY },\n\t[NL80211_ATTR_CNTDWN_OFFS_PRESP] = { .type = NLA_BINARY },\n\t[NL80211_ATTR_STA_SUPPORTED_CHANNELS] = NLA_POLICY_MIN_LEN(2),\n\t/*\n\t * The value of the Length field of the Supported Operating\n\t * Classes element is between 2 and 253.\n\t */\n\t[NL80211_ATTR_STA_SUPPORTED_OPER_CLASSES] =\n\t\tNLA_POLICY_RANGE(NLA_BINARY, 2, 253),\n\t[NL80211_ATTR_HANDLE_DFS] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_OPMODE_NOTIF] = { .type = NLA_U8 },\n\t[NL80211_ATTR_VENDOR_ID] = { .type = NLA_U32 },\n\t[NL80211_ATTR_VENDOR_SUBCMD] = { .type = NLA_U32 },\n\t[NL80211_ATTR_VENDOR_DATA] = { .type = NLA_BINARY },\n\t[NL80211_ATTR_QOS_MAP] = NLA_POLICY_RANGE(NLA_BINARY,\n\t\t\t\t\t\t  IEEE80211_QOS_MAP_LEN_MIN,\n\t\t\t\t\t\t  IEEE80211_QOS_MAP_LEN_MAX),\n\t[NL80211_ATTR_MAC_HINT] = NLA_POLICY_EXACT_LEN_WARN(ETH_ALEN),\n\t[NL80211_ATTR_WIPHY_FREQ_HINT] = { .type = NLA_U32 },\n\t[NL80211_ATTR_TDLS_PEER_CAPABILITY] = { .type = NLA_U32 },\n\t[NL80211_ATTR_SOCKET_OWNER] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_CSA_C_OFFSETS_TX] = { .type = NLA_BINARY },\n\t[NL80211_ATTR_USE_RRM] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_TSID] = NLA_POLICY_MAX(NLA_U8, IEEE80211_NUM_TIDS - 1),\n\t[NL80211_ATTR_USER_PRIO] =\n\t\tNLA_POLICY_MAX(NLA_U8, IEEE80211_NUM_UPS - 1),\n\t[NL80211_ATTR_ADMITTED_TIME] = { .type = NLA_U16 },\n\t[NL80211_ATTR_SMPS_MODE] = { .type = NLA_U8 },\n\t[NL80211_ATTR_OPER_CLASS] = { .type = NLA_U8 },\n\t[NL80211_ATTR_MAC_MASK] = NLA_POLICY_EXACT_LEN_WARN(ETH_ALEN),\n\t[NL80211_ATTR_WIPHY_SELF_MANAGED_REG] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_NETNS_FD] = { .type = NLA_U32 },\n\t[NL80211_ATTR_SCHED_SCAN_DELAY] = { .type = NLA_U32 },\n\t[NL80211_ATTR_REG_INDOOR] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_PBSS] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_BSS_SELECT] = { .type = NLA_NESTED },\n\t[NL80211_ATTR_STA_SUPPORT_P2P_PS] =\n\t\tNLA_POLICY_MAX(NLA_U8, NUM_NL80211_P2P_PS_STATUS - 1),\n\t[NL80211_ATTR_MU_MIMO_GROUP_DATA] = {\n\t\t.len = VHT_MUMIMO_GROUPS_DATA_LEN\n\t},\n\t[NL80211_ATTR_MU_MIMO_FOLLOW_MAC_ADDR] = NLA_POLICY_EXACT_LEN_WARN(ETH_ALEN),\n\t[NL80211_ATTR_NAN_MASTER_PREF] = NLA_POLICY_MIN(NLA_U8, 1),\n\t[NL80211_ATTR_BANDS] = { .type = NLA_U32 },\n\t[NL80211_ATTR_NAN_FUNC] = { .type = NLA_NESTED },\n\t[NL80211_ATTR_FILS_KEK] = { .type = NLA_BINARY,\n\t\t\t\t    .len = FILS_MAX_KEK_LEN },\n\t[NL80211_ATTR_FILS_NONCES] = NLA_POLICY_EXACT_LEN_WARN(2 * FILS_NONCE_LEN),\n\t[NL80211_ATTR_MULTICAST_TO_UNICAST_ENABLED] = { .type = NLA_FLAG, },\n\t[NL80211_ATTR_BSSID] = NLA_POLICY_EXACT_LEN_WARN(ETH_ALEN),\n\t[NL80211_ATTR_SCHED_SCAN_RELATIVE_RSSI] = { .type = NLA_S8 },\n\t[NL80211_ATTR_SCHED_SCAN_RSSI_ADJUST] = {\n\t\t.len = sizeof(struct nl80211_bss_select_rssi_adjust)\n\t},\n\t[NL80211_ATTR_TIMEOUT_REASON] = { .type = NLA_U32 },\n\t[NL80211_ATTR_FILS_ERP_USERNAME] = { .type = NLA_BINARY,\n\t\t\t\t\t     .len = FILS_ERP_MAX_USERNAME_LEN },\n\t[NL80211_ATTR_FILS_ERP_REALM] = { .type = NLA_BINARY,\n\t\t\t\t\t  .len = FILS_ERP_MAX_REALM_LEN },\n\t[NL80211_ATTR_FILS_ERP_NEXT_SEQ_NUM] = { .type = NLA_U16 },\n\t[NL80211_ATTR_FILS_ERP_RRK] = { .type = NLA_BINARY,\n\t\t\t\t\t.len = FILS_ERP_MAX_RRK_LEN },\n\t[NL80211_ATTR_FILS_CACHE_ID] = NLA_POLICY_EXACT_LEN_WARN(2),\n\t[NL80211_ATTR_PMK] = { .type = NLA_BINARY, .len = PMK_MAX_LEN },\n\t[NL80211_ATTR_PMKR0_NAME] = NLA_POLICY_EXACT_LEN(WLAN_PMK_NAME_LEN),\n\t[NL80211_ATTR_SCHED_SCAN_MULTI] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_EXTERNAL_AUTH_SUPPORT] = { .type = NLA_FLAG },\n\n\t[NL80211_ATTR_TXQ_LIMIT] = { .type = NLA_U32 },\n\t[NL80211_ATTR_TXQ_MEMORY_LIMIT] = { .type = NLA_U32 },\n\t[NL80211_ATTR_TXQ_QUANTUM] = { .type = NLA_U32 },\n\t[NL80211_ATTR_HE_CAPABILITY] =\n\t\tNLA_POLICY_RANGE(NLA_BINARY,\n\t\t\t\t NL80211_HE_MIN_CAPABILITY_LEN,\n\t\t\t\t NL80211_HE_MAX_CAPABILITY_LEN),\n\t[NL80211_ATTR_FTM_RESPONDER] =\n\t\tNLA_POLICY_NESTED(nl80211_ftm_responder_policy),\n\t[NL80211_ATTR_TIMEOUT] = NLA_POLICY_MIN(NLA_U32, 1),\n\t[NL80211_ATTR_PEER_MEASUREMENTS] =\n\t\tNLA_POLICY_NESTED(nl80211_pmsr_attr_policy),\n\t[NL80211_ATTR_AIRTIME_WEIGHT] = NLA_POLICY_MIN(NLA_U16, 1),\n\t[NL80211_ATTR_SAE_PASSWORD] = { .type = NLA_BINARY,\n\t\t\t\t\t.len = SAE_PASSWORD_MAX_LEN },\n\t[NL80211_ATTR_TWT_RESPONDER] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_HE_OBSS_PD] = NLA_POLICY_NESTED(he_obss_pd_policy),\n\t[NL80211_ATTR_VLAN_ID] = NLA_POLICY_RANGE(NLA_U16, 1, VLAN_N_VID - 2),\n\t[NL80211_ATTR_HE_BSS_COLOR] = NLA_POLICY_NESTED(he_bss_color_policy),\n\t[NL80211_ATTR_TID_CONFIG] =\n\t\tNLA_POLICY_NESTED_ARRAY(nl80211_tid_config_attr_policy),\n\t[NL80211_ATTR_CONTROL_PORT_NO_PREAUTH] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_PMK_LIFETIME] = NLA_POLICY_MIN(NLA_U32, 1),\n\t[NL80211_ATTR_PMK_REAUTH_THRESHOLD] = NLA_POLICY_RANGE(NLA_U8, 1, 100),\n\t[NL80211_ATTR_RECEIVE_MULTICAST] = { .type = NLA_FLAG },\n\t[NL80211_ATTR_WIPHY_FREQ_OFFSET] = NLA_POLICY_RANGE(NLA_U32, 0, 999),\n\t[NL80211_ATTR_SCAN_FREQ_KHZ] = { .type = NLA_NESTED },\n\t[NL80211_ATTR_HE_6GHZ_CAPABILITY] =\n\t\tNLA_POLICY_EXACT_LEN(sizeof(struct ieee80211_he_6ghz_capa)),\n\t[NL80211_ATTR_FILS_DISCOVERY] =\n\t\tNLA_POLICY_NESTED(nl80211_fils_discovery_policy),\n\t[NL80211_ATTR_UNSOL_BCAST_PROBE_RESP] =\n\t\tNLA_POLICY_NESTED(nl80211_unsol_bcast_probe_resp_policy),\n\t[NL80211_ATTR_S1G_CAPABILITY] =\n\t\tNLA_POLICY_EXACT_LEN(IEEE80211_S1G_CAPABILITY_LEN),\n\t[NL80211_ATTR_S1G_CAPABILITY_MASK] =\n\t\tNLA_POLICY_EXACT_LEN(IEEE80211_S1G_CAPABILITY_LEN),\n\t[NL80211_ATTR_SAE_PWE] =\n\t\tNLA_POLICY_RANGE(NLA_U8, NL80211_SAE_PWE_HUNT_AND_PECK,\n\t\t\t\t NL80211_SAE_PWE_BOTH),\n\t[NL80211_ATTR_RECONNECT_REQUESTED] = { .type = NLA_REJECT },\n\t[NL80211_ATTR_SAR_SPEC] = NLA_POLICY_NESTED(sar_policy),\n\t[NL80211_ATTR_DISABLE_HE] = { .type = NLA_FLAG },\n};\n\n/* policy for the key attributes */\nstatic const struct nla_policy nl80211_key_policy[NL80211_KEY_MAX + 1] = {\n\t[NL80211_KEY_DATA] = { .type = NLA_BINARY, .len = WLAN_MAX_KEY_LEN },\n\t[NL80211_KEY_IDX] = { .type = NLA_U8 },\n\t[NL80211_KEY_CIPHER] = { .type = NLA_U32 },\n\t[NL80211_KEY_SEQ] = { .type = NLA_BINARY, .len = 16 },\n\t[NL80211_KEY_DEFAULT] = { .type = NLA_FLAG },\n\t[NL80211_KEY_DEFAULT_MGMT] = { .type = NLA_FLAG },\n\t[NL80211_KEY_TYPE] = NLA_POLICY_MAX(NLA_U32, NUM_NL80211_KEYTYPES - 1),\n\t[NL80211_KEY_DEFAULT_TYPES] = { .type = NLA_NESTED },\n\t[NL80211_KEY_MODE] = NLA_POLICY_RANGE(NLA_U8, 0, NL80211_KEY_SET_TX),\n};\n\n/* policy for the key default flags */\nstatic const struct nla_policy\nnl80211_key_default_policy[NUM_NL80211_KEY_DEFAULT_TYPES] = {\n\t[NL80211_KEY_DEFAULT_TYPE_UNICAST] = { .type = NLA_FLAG },\n\t[NL80211_KEY_DEFAULT_TYPE_MULTICAST] = { .type = NLA_FLAG },\n};\n\n#ifdef CONFIG_PM\n/* policy for WoWLAN attributes */\nstatic const struct nla_policy\nnl80211_wowlan_policy[NUM_NL80211_WOWLAN_TRIG] = {\n\t[NL80211_WOWLAN_TRIG_ANY] = { .type = NLA_FLAG },\n\t[NL80211_WOWLAN_TRIG_DISCONNECT] = { .type = NLA_FLAG },\n\t[NL80211_WOWLAN_TRIG_MAGIC_PKT] = { .type = NLA_FLAG },\n\t[NL80211_WOWLAN_TRIG_PKT_PATTERN] = { .type = NLA_NESTED },\n\t[NL80211_WOWLAN_TRIG_GTK_REKEY_FAILURE] = { .type = NLA_FLAG },\n\t[NL80211_WOWLAN_TRIG_EAP_IDENT_REQUEST] = { .type = NLA_FLAG },\n\t[NL80211_WOWLAN_TRIG_4WAY_HANDSHAKE] = { .type = NLA_FLAG },\n\t[NL80211_WOWLAN_TRIG_RFKILL_RELEASE] = { .type = NLA_FLAG },\n\t[NL80211_WOWLAN_TRIG_TCP_CONNECTION] = { .type = NLA_NESTED },\n\t[NL80211_WOWLAN_TRIG_NET_DETECT] = { .type = NLA_NESTED },\n};\n\nstatic const struct nla_policy\nnl80211_wowlan_tcp_policy[NUM_NL80211_WOWLAN_TCP] = {\n\t[NL80211_WOWLAN_TCP_SRC_IPV4] = { .type = NLA_U32 },\n\t[NL80211_WOWLAN_TCP_DST_IPV4] = { .type = NLA_U32 },\n\t[NL80211_WOWLAN_TCP_DST_MAC] = NLA_POLICY_EXACT_LEN_WARN(ETH_ALEN),\n\t[NL80211_WOWLAN_TCP_SRC_PORT] = { .type = NLA_U16 },\n\t[NL80211_WOWLAN_TCP_DST_PORT] = { .type = NLA_U16 },\n\t[NL80211_WOWLAN_TCP_DATA_PAYLOAD] = NLA_POLICY_MIN_LEN(1),\n\t[NL80211_WOWLAN_TCP_DATA_PAYLOAD_SEQ] = {\n\t\t.len = sizeof(struct nl80211_wowlan_tcp_data_seq)\n\t},\n\t[NL80211_WOWLAN_TCP_DATA_PAYLOAD_TOKEN] = {\n\t\t.len = sizeof(struct nl80211_wowlan_tcp_data_token)\n\t},\n\t[NL80211_WOWLAN_TCP_DATA_INTERVAL] = { .type = NLA_U32 },\n\t[NL80211_WOWLAN_TCP_WAKE_PAYLOAD] = NLA_POLICY_MIN_LEN(1),\n\t[NL80211_WOWLAN_TCP_WAKE_MASK] = NLA_POLICY_MIN_LEN(1),\n};\n#endif /* CONFIG_PM */\n\n/* policy for coalesce rule attributes */\nstatic const struct nla_policy\nnl80211_coalesce_policy[NUM_NL80211_ATTR_COALESCE_RULE] = {\n\t[NL80211_ATTR_COALESCE_RULE_DELAY] = { .type = NLA_U32 },\n\t[NL80211_ATTR_COALESCE_RULE_CONDITION] =\n\t\tNLA_POLICY_RANGE(NLA_U32,\n\t\t\t\t NL80211_COALESCE_CONDITION_MATCH,\n\t\t\t\t NL80211_COALESCE_CONDITION_NO_MATCH),\n\t[NL80211_ATTR_COALESCE_RULE_PKT_PATTERN] = { .type = NLA_NESTED },\n};\n\n/* policy for GTK rekey offload attributes */\nstatic const struct nla_policy\nnl80211_rekey_policy[NUM_NL80211_REKEY_DATA] = {\n\t[NL80211_REKEY_DATA_KEK] = {\n\t\t.type = NLA_BINARY,\n\t\t.len = NL80211_KEK_EXT_LEN\n\t},\n\t[NL80211_REKEY_DATA_KCK] = {\n\t\t.type = NLA_BINARY,\n\t\t.len = NL80211_KCK_EXT_LEN\n\t},\n\t[NL80211_REKEY_DATA_REPLAY_CTR] = NLA_POLICY_EXACT_LEN(NL80211_REPLAY_CTR_LEN),\n\t[NL80211_REKEY_DATA_AKM] = { .type = NLA_U32 },\n};\n\nstatic const struct nla_policy\nnl80211_match_band_rssi_policy[NUM_NL80211_BANDS] = {\n\t[NL80211_BAND_2GHZ] = { .type = NLA_S32 },\n\t[NL80211_BAND_5GHZ] = { .type = NLA_S32 },\n\t[NL80211_BAND_6GHZ] = { .type = NLA_S32 },\n\t[NL80211_BAND_60GHZ] = { .type = NLA_S32 },\n};\n\nstatic const struct nla_policy\nnl80211_match_policy[NL80211_SCHED_SCAN_MATCH_ATTR_MAX + 1] = {\n\t[NL80211_SCHED_SCAN_MATCH_ATTR_SSID] = { .type = NLA_BINARY,\n\t\t\t\t\t\t .len = IEEE80211_MAX_SSID_LEN },\n\t[NL80211_SCHED_SCAN_MATCH_ATTR_BSSID] = NLA_POLICY_EXACT_LEN_WARN(ETH_ALEN),\n\t[NL80211_SCHED_SCAN_MATCH_ATTR_RSSI] = { .type = NLA_U32 },\n\t[NL80211_SCHED_SCAN_MATCH_PER_BAND_RSSI] =\n\t\tNLA_POLICY_NESTED(nl80211_match_band_rssi_policy),\n};\n\nstatic const struct nla_policy\nnl80211_plan_policy[NL80211_SCHED_SCAN_PLAN_MAX + 1] = {\n\t[NL80211_SCHED_SCAN_PLAN_INTERVAL] = { .type = NLA_U32 },\n\t[NL80211_SCHED_SCAN_PLAN_ITERATIONS] = { .type = NLA_U32 },\n};\n\nstatic const struct nla_policy\nnl80211_bss_select_policy[NL80211_BSS_SELECT_ATTR_MAX + 1] = {\n\t[NL80211_BSS_SELECT_ATTR_RSSI] = { .type = NLA_FLAG },\n\t[NL80211_BSS_SELECT_ATTR_BAND_PREF] = { .type = NLA_U32 },\n\t[NL80211_BSS_SELECT_ATTR_RSSI_ADJUST] = {\n\t\t.len = sizeof(struct nl80211_bss_select_rssi_adjust)\n\t},\n};\n\n/* policy for NAN function attributes */\nstatic const struct nla_policy\nnl80211_nan_func_policy[NL80211_NAN_FUNC_ATTR_MAX + 1] = {\n\t[NL80211_NAN_FUNC_TYPE] =\n\t\tNLA_POLICY_MAX(NLA_U8, NL80211_NAN_FUNC_MAX_TYPE),\n\t[NL80211_NAN_FUNC_SERVICE_ID] = {\n\t\t\t\t    .len = NL80211_NAN_FUNC_SERVICE_ID_LEN },\n\t[NL80211_NAN_FUNC_PUBLISH_TYPE] = { .type = NLA_U8 },\n\t[NL80211_NAN_FUNC_PUBLISH_BCAST] = { .type = NLA_FLAG },\n\t[NL80211_NAN_FUNC_SUBSCRIBE_ACTIVE] = { .type = NLA_FLAG },\n\t[NL80211_NAN_FUNC_FOLLOW_UP_ID] = { .type = NLA_U8 },\n\t[NL80211_NAN_FUNC_FOLLOW_UP_REQ_ID] = { .type = NLA_U8 },\n\t[NL80211_NAN_FUNC_FOLLOW_UP_DEST] = NLA_POLICY_EXACT_LEN_WARN(ETH_ALEN),\n\t[NL80211_NAN_FUNC_CLOSE_RANGE] = { .type = NLA_FLAG },\n\t[NL80211_NAN_FUNC_TTL] = { .type = NLA_U32 },\n\t[NL80211_NAN_FUNC_SERVICE_INFO] = { .type = NLA_BINARY,\n\t\t\t.len = NL80211_NAN_FUNC_SERVICE_SPEC_INFO_MAX_LEN },\n\t[NL80211_NAN_FUNC_SRF] = { .type = NLA_NESTED },\n\t[NL80211_NAN_FUNC_RX_MATCH_FILTER] = { .type = NLA_NESTED },\n\t[NL80211_NAN_FUNC_TX_MATCH_FILTER] = { .type = NLA_NESTED },\n\t[NL80211_NAN_FUNC_INSTANCE_ID] = { .type = NLA_U8 },\n\t[NL80211_NAN_FUNC_TERM_REASON] = { .type = NLA_U8 },\n};\n\n/* policy for Service Response Filter attributes */\nstatic const struct nla_policy\nnl80211_nan_srf_policy[NL80211_NAN_SRF_ATTR_MAX + 1] = {\n\t[NL80211_NAN_SRF_INCLUDE] = { .type = NLA_FLAG },\n\t[NL80211_NAN_SRF_BF] = { .type = NLA_BINARY,\n\t\t\t\t .len =  NL80211_NAN_FUNC_SRF_MAX_LEN },\n\t[NL80211_NAN_SRF_BF_IDX] = { .type = NLA_U8 },\n\t[NL80211_NAN_SRF_MAC_ADDRS] = { .type = NLA_NESTED },\n};\n\n/* policy for packet pattern attributes */\nstatic const struct nla_policy\nnl80211_packet_pattern_policy[MAX_NL80211_PKTPAT + 1] = {\n\t[NL80211_PKTPAT_MASK] = { .type = NLA_BINARY, },\n\t[NL80211_PKTPAT_PATTERN] = { .type = NLA_BINARY, },\n\t[NL80211_PKTPAT_OFFSET] = { .type = NLA_U32 },\n};\n\nint nl80211_prepare_wdev_dump(struct netlink_callback *cb,\n\t\t\t      struct cfg80211_registered_device **rdev,\n\t\t\t      struct wireless_dev **wdev)\n{\n\tint err;\n\n\tif (!cb->args[0]) {\n\t\tstruct nlattr **attrbuf;\n\n\t\tattrbuf = kcalloc(NUM_NL80211_ATTR, sizeof(*attrbuf),\n\t\t\t\t  GFP_KERNEL);\n\t\tif (!attrbuf)\n\t\t\treturn -ENOMEM;\n\n\t\terr = nlmsg_parse_deprecated(cb->nlh,\n\t\t\t\t\t     GENL_HDRLEN + nl80211_fam.hdrsize,\n\t\t\t\t\t     attrbuf, nl80211_fam.maxattr,\n\t\t\t\t\t     nl80211_policy, NULL);\n\t\tif (err) {\n\t\t\tkfree(attrbuf);\n\t\t\treturn err;\n\t\t}\n\n\t\trtnl_lock();\n\t\t*wdev = __cfg80211_wdev_from_attrs(NULL, sock_net(cb->skb->sk),\n\t\t\t\t\t\t   attrbuf);\n\t\tkfree(attrbuf);\n\t\tif (IS_ERR(*wdev)) {\n\t\t\trtnl_unlock();\n\t\t\treturn PTR_ERR(*wdev);\n\t\t}\n\t\t*rdev = wiphy_to_rdev((*wdev)->wiphy);\n\t\tmutex_lock(&(*rdev)->wiphy.mtx);\n\t\trtnl_unlock();\n\t\t/* 0 is the first index - add 1 to parse only once */\n\t\tcb->args[0] = (*rdev)->wiphy_idx + 1;\n\t\tcb->args[1] = (*wdev)->identifier;\n\t} else {\n\t\t/* subtract the 1 again here */\n\t\tstruct wiphy *wiphy;\n\t\tstruct wireless_dev *tmp;\n\n\t\trtnl_lock();\n\t\twiphy = wiphy_idx_to_wiphy(cb->args[0] - 1);\n\t\tif (!wiphy) {\n\t\t\trtnl_unlock();\n\t\t\treturn -ENODEV;\n\t\t}\n\t\t*rdev = wiphy_to_rdev(wiphy);\n\t\t*wdev = NULL;\n\n\t\tlist_for_each_entry(tmp, &(*rdev)->wiphy.wdev_list, list) {\n\t\t\tif (tmp->identifier == cb->args[1]) {\n\t\t\t\t*wdev = tmp;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!*wdev) {\n\t\t\trtnl_unlock();\n\t\t\treturn -ENODEV;\n\t\t}\n\t\tmutex_lock(&(*rdev)->wiphy.mtx);\n\t\trtnl_unlock();\n\t}\n\n\treturn 0;\n}\n\n/* message building helper */\nvoid *nl80211hdr_put(struct sk_buff *skb, u32 portid, u32 seq,\n\t\t     int flags, u8 cmd)\n{\n\t/* since there is no private header just add the generic one */\n\treturn genlmsg_put(skb, portid, seq, &nl80211_fam, flags, cmd);\n}\n\nstatic int nl80211_msg_put_wmm_rules(struct sk_buff *msg,\n\t\t\t\t     const struct ieee80211_reg_rule *rule)\n{\n\tint j;\n\tstruct nlattr *nl_wmm_rules =\n\t\tnla_nest_start_noflag(msg, NL80211_FREQUENCY_ATTR_WMM);\n\n\tif (!nl_wmm_rules)\n\t\tgoto nla_put_failure;\n\n\tfor (j = 0; j < IEEE80211_NUM_ACS; j++) {\n\t\tstruct nlattr *nl_wmm_rule = nla_nest_start_noflag(msg, j);\n\n\t\tif (!nl_wmm_rule)\n\t\t\tgoto nla_put_failure;\n\n\t\tif (nla_put_u16(msg, NL80211_WMMR_CW_MIN,\n\t\t\t\trule->wmm_rule.client[j].cw_min) ||\n\t\t    nla_put_u16(msg, NL80211_WMMR_CW_MAX,\n\t\t\t\trule->wmm_rule.client[j].cw_max) ||\n\t\t    nla_put_u8(msg, NL80211_WMMR_AIFSN,\n\t\t\t       rule->wmm_rule.client[j].aifsn) ||\n\t\t    nla_put_u16(msg, NL80211_WMMR_TXOP,\n\t\t\t        rule->wmm_rule.client[j].cot))\n\t\t\tgoto nla_put_failure;\n\n\t\tnla_nest_end(msg, nl_wmm_rule);\n\t}\n\tnla_nest_end(msg, nl_wmm_rules);\n\n\treturn 0;\n\nnla_put_failure:\n\treturn -ENOBUFS;\n}\n\nstatic int nl80211_msg_put_channel(struct sk_buff *msg, struct wiphy *wiphy,\n\t\t\t\t   struct ieee80211_channel *chan,\n\t\t\t\t   bool large)\n{\n\t/* Some channels must be completely excluded from the\n\t * list to protect old user-space tools from breaking\n\t */\n\tif (!large && chan->flags &\n\t    (IEEE80211_CHAN_NO_10MHZ | IEEE80211_CHAN_NO_20MHZ))\n\t\treturn 0;\n\tif (!large && chan->freq_offset)\n\t\treturn 0;\n\n\tif (nla_put_u32(msg, NL80211_FREQUENCY_ATTR_FREQ,\n\t\t\tchan->center_freq))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(msg, NL80211_FREQUENCY_ATTR_OFFSET, chan->freq_offset))\n\t\tgoto nla_put_failure;\n\n\tif ((chan->flags & IEEE80211_CHAN_DISABLED) &&\n\t    nla_put_flag(msg, NL80211_FREQUENCY_ATTR_DISABLED))\n\t\tgoto nla_put_failure;\n\tif (chan->flags & IEEE80211_CHAN_NO_IR) {\n\t\tif (nla_put_flag(msg, NL80211_FREQUENCY_ATTR_NO_IR))\n\t\t\tgoto nla_put_failure;\n\t\tif (nla_put_flag(msg, __NL80211_FREQUENCY_ATTR_NO_IBSS))\n\t\t\tgoto nla_put_failure;\n\t}\n\tif (chan->flags & IEEE80211_CHAN_RADAR) {\n\t\tif (nla_put_flag(msg, NL80211_FREQUENCY_ATTR_RADAR))\n\t\t\tgoto nla_put_failure;\n\t\tif (large) {\n\t\t\tu32 time;\n\n\t\t\ttime = elapsed_jiffies_msecs(chan->dfs_state_entered);\n\n\t\t\tif (nla_put_u32(msg, NL80211_FREQUENCY_ATTR_DFS_STATE,\n\t\t\t\t\tchan->dfs_state))\n\t\t\t\tgoto nla_put_failure;\n\t\t\tif (nla_put_u32(msg, NL80211_FREQUENCY_ATTR_DFS_TIME,\n\t\t\t\t\ttime))\n\t\t\t\tgoto nla_put_failure;\n\t\t\tif (nla_put_u32(msg,\n\t\t\t\t\tNL80211_FREQUENCY_ATTR_DFS_CAC_TIME,\n\t\t\t\t\tchan->dfs_cac_ms))\n\t\t\t\tgoto nla_put_failure;\n\t\t}\n\t}\n\n\tif (large) {\n\t\tif ((chan->flags & IEEE80211_CHAN_NO_HT40MINUS) &&\n\t\t    nla_put_flag(msg, NL80211_FREQUENCY_ATTR_NO_HT40_MINUS))\n\t\t\tgoto nla_put_failure;\n\t\tif ((chan->flags & IEEE80211_CHAN_NO_HT40PLUS) &&\n\t\t    nla_put_flag(msg, NL80211_FREQUENCY_ATTR_NO_HT40_PLUS))\n\t\t\tgoto nla_put_failure;\n\t\tif ((chan->flags & IEEE80211_CHAN_NO_80MHZ) &&\n\t\t    nla_put_flag(msg, NL80211_FREQUENCY_ATTR_NO_80MHZ))\n\t\t\tgoto nla_put_failure;\n\t\tif ((chan->flags & IEEE80211_CHAN_NO_160MHZ) &&\n\t\t    nla_put_flag(msg, NL80211_FREQUENCY_ATTR_NO_160MHZ))\n\t\t\tgoto nla_put_failure;\n\t\tif ((chan->flags & IEEE80211_CHAN_INDOOR_ONLY) &&\n\t\t    nla_put_flag(msg, NL80211_FREQUENCY_ATTR_INDOOR_ONLY))\n\t\t\tgoto nla_put_failure;\n\t\tif ((chan->flags & IEEE80211_CHAN_IR_CONCURRENT) &&\n\t\t    nla_put_flag(msg, NL80211_FREQUENCY_ATTR_IR_CONCURRENT))\n\t\t\tgoto nla_put_failure;\n\t\tif ((chan->flags & IEEE80211_CHAN_NO_20MHZ) &&\n\t\t    nla_put_flag(msg, NL80211_FREQUENCY_ATTR_NO_20MHZ))\n\t\t\tgoto nla_put_failure;\n\t\tif ((chan->flags & IEEE80211_CHAN_NO_10MHZ) &&\n\t\t    nla_put_flag(msg, NL80211_FREQUENCY_ATTR_NO_10MHZ))\n\t\t\tgoto nla_put_failure;\n\t\tif ((chan->flags & IEEE80211_CHAN_NO_HE) &&\n\t\t    nla_put_flag(msg, NL80211_FREQUENCY_ATTR_NO_HE))\n\t\t\tgoto nla_put_failure;\n\t\tif ((chan->flags & IEEE80211_CHAN_1MHZ) &&\n\t\t    nla_put_flag(msg, NL80211_FREQUENCY_ATTR_1MHZ))\n\t\t\tgoto nla_put_failure;\n\t\tif ((chan->flags & IEEE80211_CHAN_2MHZ) &&\n\t\t    nla_put_flag(msg, NL80211_FREQUENCY_ATTR_2MHZ))\n\t\t\tgoto nla_put_failure;\n\t\tif ((chan->flags & IEEE80211_CHAN_4MHZ) &&\n\t\t    nla_put_flag(msg, NL80211_FREQUENCY_ATTR_4MHZ))\n\t\t\tgoto nla_put_failure;\n\t\tif ((chan->flags & IEEE80211_CHAN_8MHZ) &&\n\t\t    nla_put_flag(msg, NL80211_FREQUENCY_ATTR_8MHZ))\n\t\t\tgoto nla_put_failure;\n\t\tif ((chan->flags & IEEE80211_CHAN_16MHZ) &&\n\t\t    nla_put_flag(msg, NL80211_FREQUENCY_ATTR_16MHZ))\n\t\t\tgoto nla_put_failure;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_FREQUENCY_ATTR_MAX_TX_POWER,\n\t\t\tDBM_TO_MBM(chan->max_power)))\n\t\tgoto nla_put_failure;\n\n\tif (large) {\n\t\tconst struct ieee80211_reg_rule *rule =\n\t\t\tfreq_reg_info(wiphy, MHZ_TO_KHZ(chan->center_freq));\n\n\t\tif (!IS_ERR_OR_NULL(rule) && rule->has_wmm) {\n\t\t\tif (nl80211_msg_put_wmm_rules(msg, rule))\n\t\t\t\tgoto nla_put_failure;\n\t\t}\n\t}\n\n\treturn 0;\n\n nla_put_failure:\n\treturn -ENOBUFS;\n}\n\nstatic bool nl80211_put_txq_stats(struct sk_buff *msg,\n\t\t\t\t  struct cfg80211_txq_stats *txqstats,\n\t\t\t\t  int attrtype)\n{\n\tstruct nlattr *txqattr;\n\n#define PUT_TXQVAL_U32(attr, memb) do {\t\t\t\t\t  \\\n\tif (txqstats->filled & BIT(NL80211_TXQ_STATS_ ## attr) &&\t  \\\n\t    nla_put_u32(msg, NL80211_TXQ_STATS_ ## attr, txqstats->memb)) \\\n\t\treturn false;\t\t\t\t\t\t  \\\n\t} while (0)\n\n\ttxqattr = nla_nest_start_noflag(msg, attrtype);\n\tif (!txqattr)\n\t\treturn false;\n\n\tPUT_TXQVAL_U32(BACKLOG_BYTES, backlog_bytes);\n\tPUT_TXQVAL_U32(BACKLOG_PACKETS, backlog_packets);\n\tPUT_TXQVAL_U32(FLOWS, flows);\n\tPUT_TXQVAL_U32(DROPS, drops);\n\tPUT_TXQVAL_U32(ECN_MARKS, ecn_marks);\n\tPUT_TXQVAL_U32(OVERLIMIT, overlimit);\n\tPUT_TXQVAL_U32(OVERMEMORY, overmemory);\n\tPUT_TXQVAL_U32(COLLISIONS, collisions);\n\tPUT_TXQVAL_U32(TX_BYTES, tx_bytes);\n\tPUT_TXQVAL_U32(TX_PACKETS, tx_packets);\n\tPUT_TXQVAL_U32(MAX_FLOWS, max_flows);\n\tnla_nest_end(msg, txqattr);\n\n#undef PUT_TXQVAL_U32\n\treturn true;\n}\n\n/* netlink command implementations */\n\nstruct key_parse {\n\tstruct key_params p;\n\tint idx;\n\tint type;\n\tbool def, defmgmt, defbeacon;\n\tbool def_uni, def_multi;\n};\n\nstatic int nl80211_parse_key_new(struct genl_info *info, struct nlattr *key,\n\t\t\t\t struct key_parse *k)\n{\n\tstruct nlattr *tb[NL80211_KEY_MAX + 1];\n\tint err = nla_parse_nested_deprecated(tb, NL80211_KEY_MAX, key,\n\t\t\t\t\t      nl80211_key_policy,\n\t\t\t\t\t      info->extack);\n\tif (err)\n\t\treturn err;\n\n\tk->def = !!tb[NL80211_KEY_DEFAULT];\n\tk->defmgmt = !!tb[NL80211_KEY_DEFAULT_MGMT];\n\tk->defbeacon = !!tb[NL80211_KEY_DEFAULT_BEACON];\n\n\tif (k->def) {\n\t\tk->def_uni = true;\n\t\tk->def_multi = true;\n\t}\n\tif (k->defmgmt || k->defbeacon)\n\t\tk->def_multi = true;\n\n\tif (tb[NL80211_KEY_IDX])\n\t\tk->idx = nla_get_u8(tb[NL80211_KEY_IDX]);\n\n\tif (tb[NL80211_KEY_DATA]) {\n\t\tk->p.key = nla_data(tb[NL80211_KEY_DATA]);\n\t\tk->p.key_len = nla_len(tb[NL80211_KEY_DATA]);\n\t}\n\n\tif (tb[NL80211_KEY_SEQ]) {\n\t\tk->p.seq = nla_data(tb[NL80211_KEY_SEQ]);\n\t\tk->p.seq_len = nla_len(tb[NL80211_KEY_SEQ]);\n\t}\n\n\tif (tb[NL80211_KEY_CIPHER])\n\t\tk->p.cipher = nla_get_u32(tb[NL80211_KEY_CIPHER]);\n\n\tif (tb[NL80211_KEY_TYPE])\n\t\tk->type = nla_get_u32(tb[NL80211_KEY_TYPE]);\n\n\tif (tb[NL80211_KEY_DEFAULT_TYPES]) {\n\t\tstruct nlattr *kdt[NUM_NL80211_KEY_DEFAULT_TYPES];\n\n\t\terr = nla_parse_nested_deprecated(kdt,\n\t\t\t\t\t\t  NUM_NL80211_KEY_DEFAULT_TYPES - 1,\n\t\t\t\t\t\t  tb[NL80211_KEY_DEFAULT_TYPES],\n\t\t\t\t\t\t  nl80211_key_default_policy,\n\t\t\t\t\t\t  info->extack);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tk->def_uni = kdt[NL80211_KEY_DEFAULT_TYPE_UNICAST];\n\t\tk->def_multi = kdt[NL80211_KEY_DEFAULT_TYPE_MULTICAST];\n\t}\n\n\tif (tb[NL80211_KEY_MODE])\n\t\tk->p.mode = nla_get_u8(tb[NL80211_KEY_MODE]);\n\n\treturn 0;\n}\n\nstatic int nl80211_parse_key_old(struct genl_info *info, struct key_parse *k)\n{\n\tif (info->attrs[NL80211_ATTR_KEY_DATA]) {\n\t\tk->p.key = nla_data(info->attrs[NL80211_ATTR_KEY_DATA]);\n\t\tk->p.key_len = nla_len(info->attrs[NL80211_ATTR_KEY_DATA]);\n\t}\n\n\tif (info->attrs[NL80211_ATTR_KEY_SEQ]) {\n\t\tk->p.seq = nla_data(info->attrs[NL80211_ATTR_KEY_SEQ]);\n\t\tk->p.seq_len = nla_len(info->attrs[NL80211_ATTR_KEY_SEQ]);\n\t}\n\n\tif (info->attrs[NL80211_ATTR_KEY_IDX])\n\t\tk->idx = nla_get_u8(info->attrs[NL80211_ATTR_KEY_IDX]);\n\n\tif (info->attrs[NL80211_ATTR_KEY_CIPHER])\n\t\tk->p.cipher = nla_get_u32(info->attrs[NL80211_ATTR_KEY_CIPHER]);\n\n\tk->def = !!info->attrs[NL80211_ATTR_KEY_DEFAULT];\n\tk->defmgmt = !!info->attrs[NL80211_ATTR_KEY_DEFAULT_MGMT];\n\n\tif (k->def) {\n\t\tk->def_uni = true;\n\t\tk->def_multi = true;\n\t}\n\tif (k->defmgmt)\n\t\tk->def_multi = true;\n\n\tif (info->attrs[NL80211_ATTR_KEY_TYPE])\n\t\tk->type = nla_get_u32(info->attrs[NL80211_ATTR_KEY_TYPE]);\n\n\tif (info->attrs[NL80211_ATTR_KEY_DEFAULT_TYPES]) {\n\t\tstruct nlattr *kdt[NUM_NL80211_KEY_DEFAULT_TYPES];\n\t\tint err = nla_parse_nested_deprecated(kdt,\n\t\t\t\t\t\t      NUM_NL80211_KEY_DEFAULT_TYPES - 1,\n\t\t\t\t\t\t      info->attrs[NL80211_ATTR_KEY_DEFAULT_TYPES],\n\t\t\t\t\t\t      nl80211_key_default_policy,\n\t\t\t\t\t\t      info->extack);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tk->def_uni = kdt[NL80211_KEY_DEFAULT_TYPE_UNICAST];\n\t\tk->def_multi = kdt[NL80211_KEY_DEFAULT_TYPE_MULTICAST];\n\t}\n\n\treturn 0;\n}\n\nstatic int nl80211_parse_key(struct genl_info *info, struct key_parse *k)\n{\n\tint err;\n\n\tmemset(k, 0, sizeof(*k));\n\tk->idx = -1;\n\tk->type = -1;\n\n\tif (info->attrs[NL80211_ATTR_KEY])\n\t\terr = nl80211_parse_key_new(info, info->attrs[NL80211_ATTR_KEY], k);\n\telse\n\t\terr = nl80211_parse_key_old(info, k);\n\n\tif (err)\n\t\treturn err;\n\n\tif ((k->def ? 1 : 0) + (k->defmgmt ? 1 : 0) +\n\t    (k->defbeacon ? 1 : 0) > 1) {\n\t\tGENL_SET_ERR_MSG(info,\n\t\t\t\t \"key with multiple default flags is invalid\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (k->defmgmt || k->defbeacon) {\n\t\tif (k->def_uni || !k->def_multi) {\n\t\t\tGENL_SET_ERR_MSG(info,\n\t\t\t\t\t \"defmgmt/defbeacon key must be mcast\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (k->idx != -1) {\n\t\tif (k->defmgmt) {\n\t\t\tif (k->idx < 4 || k->idx > 5) {\n\t\t\t\tGENL_SET_ERR_MSG(info,\n\t\t\t\t\t\t \"defmgmt key idx not 4 or 5\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else if (k->defbeacon) {\n\t\t\tif (k->idx < 6 || k->idx > 7) {\n\t\t\t\tGENL_SET_ERR_MSG(info,\n\t\t\t\t\t\t \"defbeacon key idx not 6 or 7\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else if (k->def) {\n\t\t\tif (k->idx < 0 || k->idx > 3) {\n\t\t\t\tGENL_SET_ERR_MSG(info, \"def key idx not 0-3\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (k->idx < 0 || k->idx > 7) {\n\t\t\t\tGENL_SET_ERR_MSG(info, \"key idx not 0-7\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic struct cfg80211_cached_keys *\nnl80211_parse_connkeys(struct cfg80211_registered_device *rdev,\n\t\t       struct genl_info *info, bool *no_ht)\n{\n\tstruct nlattr *keys = info->attrs[NL80211_ATTR_KEYS];\n\tstruct key_parse parse;\n\tstruct nlattr *key;\n\tstruct cfg80211_cached_keys *result;\n\tint rem, err, def = 0;\n\tbool have_key = false;\n\n\tnla_for_each_nested(key, keys, rem) {\n\t\thave_key = true;\n\t\tbreak;\n\t}\n\n\tif (!have_key)\n\t\treturn NULL;\n\n\tresult = kzalloc(sizeof(*result), GFP_KERNEL);\n\tif (!result)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tresult->def = -1;\n\n\tnla_for_each_nested(key, keys, rem) {\n\t\tmemset(&parse, 0, sizeof(parse));\n\t\tparse.idx = -1;\n\n\t\terr = nl80211_parse_key_new(info, key, &parse);\n\t\tif (err)\n\t\t\tgoto error;\n\t\terr = -EINVAL;\n\t\tif (!parse.p.key)\n\t\t\tgoto error;\n\t\tif (parse.idx < 0 || parse.idx > 3) {\n\t\t\tGENL_SET_ERR_MSG(info, \"key index out of range [0-3]\");\n\t\t\tgoto error;\n\t\t}\n\t\tif (parse.def) {\n\t\t\tif (def) {\n\t\t\t\tGENL_SET_ERR_MSG(info,\n\t\t\t\t\t\t \"only one key can be default\");\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tdef = 1;\n\t\t\tresult->def = parse.idx;\n\t\t\tif (!parse.def_uni || !parse.def_multi)\n\t\t\t\tgoto error;\n\t\t} else if (parse.defmgmt)\n\t\t\tgoto error;\n\t\terr = cfg80211_validate_key_settings(rdev, &parse.p,\n\t\t\t\t\t\t     parse.idx, false, NULL);\n\t\tif (err)\n\t\t\tgoto error;\n\t\tif (parse.p.cipher != WLAN_CIPHER_SUITE_WEP40 &&\n\t\t    parse.p.cipher != WLAN_CIPHER_SUITE_WEP104) {\n\t\t\tGENL_SET_ERR_MSG(info, \"connect key must be WEP\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto error;\n\t\t}\n\t\tresult->params[parse.idx].cipher = parse.p.cipher;\n\t\tresult->params[parse.idx].key_len = parse.p.key_len;\n\t\tresult->params[parse.idx].key = result->data[parse.idx];\n\t\tmemcpy(result->data[parse.idx], parse.p.key, parse.p.key_len);\n\n\t\t/* must be WEP key if we got here */\n\t\tif (no_ht)\n\t\t\t*no_ht = true;\n\t}\n\n\tif (result->def < 0) {\n\t\terr = -EINVAL;\n\t\tGENL_SET_ERR_MSG(info, \"need a default/TX key\");\n\t\tgoto error;\n\t}\n\n\treturn result;\n error:\n\tkfree(result);\n\treturn ERR_PTR(err);\n}\n\nstatic int nl80211_key_allowed(struct wireless_dev *wdev)\n{\n\tASSERT_WDEV_LOCK(wdev);\n\n\tswitch (wdev->iftype) {\n\tcase NL80211_IFTYPE_AP:\n\tcase NL80211_IFTYPE_AP_VLAN:\n\tcase NL80211_IFTYPE_P2P_GO:\n\tcase NL80211_IFTYPE_MESH_POINT:\n\t\tbreak;\n\tcase NL80211_IFTYPE_ADHOC:\n\tcase NL80211_IFTYPE_STATION:\n\tcase NL80211_IFTYPE_P2P_CLIENT:\n\t\tif (!wdev->current_bss)\n\t\t\treturn -ENOLINK;\n\t\tbreak;\n\tcase NL80211_IFTYPE_UNSPECIFIED:\n\tcase NL80211_IFTYPE_OCB:\n\tcase NL80211_IFTYPE_MONITOR:\n\tcase NL80211_IFTYPE_NAN:\n\tcase NL80211_IFTYPE_P2P_DEVICE:\n\tcase NL80211_IFTYPE_WDS:\n\tcase NUM_NL80211_IFTYPES:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic struct ieee80211_channel *nl80211_get_valid_chan(struct wiphy *wiphy,\n\t\t\t\t\t\t\tu32 freq)\n{\n\tstruct ieee80211_channel *chan;\n\n\tchan = ieee80211_get_channel_khz(wiphy, freq);\n\tif (!chan || chan->flags & IEEE80211_CHAN_DISABLED)\n\t\treturn NULL;\n\treturn chan;\n}\n\nstatic int nl80211_put_iftypes(struct sk_buff *msg, u32 attr, u16 ifmodes)\n{\n\tstruct nlattr *nl_modes = nla_nest_start_noflag(msg, attr);\n\tint i;\n\n\tif (!nl_modes)\n\t\tgoto nla_put_failure;\n\n\ti = 0;\n\twhile (ifmodes) {\n\t\tif ((ifmodes & 1) && nla_put_flag(msg, i))\n\t\t\tgoto nla_put_failure;\n\t\tifmodes >>= 1;\n\t\ti++;\n\t}\n\n\tnla_nest_end(msg, nl_modes);\n\treturn 0;\n\nnla_put_failure:\n\treturn -ENOBUFS;\n}\n\nstatic int nl80211_put_iface_combinations(struct wiphy *wiphy,\n\t\t\t\t\t  struct sk_buff *msg,\n\t\t\t\t\t  bool large)\n{\n\tstruct nlattr *nl_combis;\n\tint i, j;\n\n\tnl_combis = nla_nest_start_noflag(msg,\n\t\t\t\t\t  NL80211_ATTR_INTERFACE_COMBINATIONS);\n\tif (!nl_combis)\n\t\tgoto nla_put_failure;\n\n\tfor (i = 0; i < wiphy->n_iface_combinations; i++) {\n\t\tconst struct ieee80211_iface_combination *c;\n\t\tstruct nlattr *nl_combi, *nl_limits;\n\n\t\tc = &wiphy->iface_combinations[i];\n\n\t\tnl_combi = nla_nest_start_noflag(msg, i + 1);\n\t\tif (!nl_combi)\n\t\t\tgoto nla_put_failure;\n\n\t\tnl_limits = nla_nest_start_noflag(msg,\n\t\t\t\t\t\t  NL80211_IFACE_COMB_LIMITS);\n\t\tif (!nl_limits)\n\t\t\tgoto nla_put_failure;\n\n\t\tfor (j = 0; j < c->n_limits; j++) {\n\t\t\tstruct nlattr *nl_limit;\n\n\t\t\tnl_limit = nla_nest_start_noflag(msg, j + 1);\n\t\t\tif (!nl_limit)\n\t\t\t\tgoto nla_put_failure;\n\t\t\tif (nla_put_u32(msg, NL80211_IFACE_LIMIT_MAX,\n\t\t\t\t\tc->limits[j].max))\n\t\t\t\tgoto nla_put_failure;\n\t\t\tif (nl80211_put_iftypes(msg, NL80211_IFACE_LIMIT_TYPES,\n\t\t\t\t\t\tc->limits[j].types))\n\t\t\t\tgoto nla_put_failure;\n\t\t\tnla_nest_end(msg, nl_limit);\n\t\t}\n\n\t\tnla_nest_end(msg, nl_limits);\n\n\t\tif (c->beacon_int_infra_match &&\n\t\t    nla_put_flag(msg, NL80211_IFACE_COMB_STA_AP_BI_MATCH))\n\t\t\tgoto nla_put_failure;\n\t\tif (nla_put_u32(msg, NL80211_IFACE_COMB_NUM_CHANNELS,\n\t\t\t\tc->num_different_channels) ||\n\t\t    nla_put_u32(msg, NL80211_IFACE_COMB_MAXNUM,\n\t\t\t\tc->max_interfaces))\n\t\t\tgoto nla_put_failure;\n\t\tif (large &&\n\t\t    (nla_put_u32(msg, NL80211_IFACE_COMB_RADAR_DETECT_WIDTHS,\n\t\t\t\tc->radar_detect_widths) ||\n\t\t     nla_put_u32(msg, NL80211_IFACE_COMB_RADAR_DETECT_REGIONS,\n\t\t\t\tc->radar_detect_regions)))\n\t\t\tgoto nla_put_failure;\n\t\tif (c->beacon_int_min_gcd &&\n\t\t    nla_put_u32(msg, NL80211_IFACE_COMB_BI_MIN_GCD,\n\t\t\t\tc->beacon_int_min_gcd))\n\t\t\tgoto nla_put_failure;\n\n\t\tnla_nest_end(msg, nl_combi);\n\t}\n\n\tnla_nest_end(msg, nl_combis);\n\n\treturn 0;\nnla_put_failure:\n\treturn -ENOBUFS;\n}\n\n#ifdef CONFIG_PM\nstatic int nl80211_send_wowlan_tcp_caps(struct cfg80211_registered_device *rdev,\n\t\t\t\t\tstruct sk_buff *msg)\n{\n\tconst struct wiphy_wowlan_tcp_support *tcp = rdev->wiphy.wowlan->tcp;\n\tstruct nlattr *nl_tcp;\n\n\tif (!tcp)\n\t\treturn 0;\n\n\tnl_tcp = nla_nest_start_noflag(msg,\n\t\t\t\t       NL80211_WOWLAN_TRIG_TCP_CONNECTION);\n\tif (!nl_tcp)\n\t\treturn -ENOBUFS;\n\n\tif (nla_put_u32(msg, NL80211_WOWLAN_TCP_DATA_PAYLOAD,\n\t\t\ttcp->data_payload_max))\n\t\treturn -ENOBUFS;\n\n\tif (nla_put_u32(msg, NL80211_WOWLAN_TCP_DATA_PAYLOAD,\n\t\t\ttcp->data_payload_max))\n\t\treturn -ENOBUFS;\n\n\tif (tcp->seq && nla_put_flag(msg, NL80211_WOWLAN_TCP_DATA_PAYLOAD_SEQ))\n\t\treturn -ENOBUFS;\n\n\tif (tcp->tok && nla_put(msg, NL80211_WOWLAN_TCP_DATA_PAYLOAD_TOKEN,\n\t\t\t\tsizeof(*tcp->tok), tcp->tok))\n\t\treturn -ENOBUFS;\n\n\tif (nla_put_u32(msg, NL80211_WOWLAN_TCP_DATA_INTERVAL,\n\t\t\ttcp->data_interval_max))\n\t\treturn -ENOBUFS;\n\n\tif (nla_put_u32(msg, NL80211_WOWLAN_TCP_WAKE_PAYLOAD,\n\t\t\ttcp->wake_payload_max))\n\t\treturn -ENOBUFS;\n\n\tnla_nest_end(msg, nl_tcp);\n\treturn 0;\n}\n\nstatic int nl80211_send_wowlan(struct sk_buff *msg,\n\t\t\t       struct cfg80211_registered_device *rdev,\n\t\t\t       bool large)\n{\n\tstruct nlattr *nl_wowlan;\n\n\tif (!rdev->wiphy.wowlan)\n\t\treturn 0;\n\n\tnl_wowlan = nla_nest_start_noflag(msg,\n\t\t\t\t\t  NL80211_ATTR_WOWLAN_TRIGGERS_SUPPORTED);\n\tif (!nl_wowlan)\n\t\treturn -ENOBUFS;\n\n\tif (((rdev->wiphy.wowlan->flags & WIPHY_WOWLAN_ANY) &&\n\t     nla_put_flag(msg, NL80211_WOWLAN_TRIG_ANY)) ||\n\t    ((rdev->wiphy.wowlan->flags & WIPHY_WOWLAN_DISCONNECT) &&\n\t     nla_put_flag(msg, NL80211_WOWLAN_TRIG_DISCONNECT)) ||\n\t    ((rdev->wiphy.wowlan->flags & WIPHY_WOWLAN_MAGIC_PKT) &&\n\t     nla_put_flag(msg, NL80211_WOWLAN_TRIG_MAGIC_PKT)) ||\n\t    ((rdev->wiphy.wowlan->flags & WIPHY_WOWLAN_SUPPORTS_GTK_REKEY) &&\n\t     nla_put_flag(msg, NL80211_WOWLAN_TRIG_GTK_REKEY_SUPPORTED)) ||\n\t    ((rdev->wiphy.wowlan->flags & WIPHY_WOWLAN_GTK_REKEY_FAILURE) &&\n\t     nla_put_flag(msg, NL80211_WOWLAN_TRIG_GTK_REKEY_FAILURE)) ||\n\t    ((rdev->wiphy.wowlan->flags & WIPHY_WOWLAN_EAP_IDENTITY_REQ) &&\n\t     nla_put_flag(msg, NL80211_WOWLAN_TRIG_EAP_IDENT_REQUEST)) ||\n\t    ((rdev->wiphy.wowlan->flags & WIPHY_WOWLAN_4WAY_HANDSHAKE) &&\n\t     nla_put_flag(msg, NL80211_WOWLAN_TRIG_4WAY_HANDSHAKE)) ||\n\t    ((rdev->wiphy.wowlan->flags & WIPHY_WOWLAN_RFKILL_RELEASE) &&\n\t     nla_put_flag(msg, NL80211_WOWLAN_TRIG_RFKILL_RELEASE)))\n\t\treturn -ENOBUFS;\n\n\tif (rdev->wiphy.wowlan->n_patterns) {\n\t\tstruct nl80211_pattern_support pat = {\n\t\t\t.max_patterns = rdev->wiphy.wowlan->n_patterns,\n\t\t\t.min_pattern_len = rdev->wiphy.wowlan->pattern_min_len,\n\t\t\t.max_pattern_len = rdev->wiphy.wowlan->pattern_max_len,\n\t\t\t.max_pkt_offset = rdev->wiphy.wowlan->max_pkt_offset,\n\t\t};\n\n\t\tif (nla_put(msg, NL80211_WOWLAN_TRIG_PKT_PATTERN,\n\t\t\t    sizeof(pat), &pat))\n\t\t\treturn -ENOBUFS;\n\t}\n\n\tif ((rdev->wiphy.wowlan->flags & WIPHY_WOWLAN_NET_DETECT) &&\n\t    nla_put_u32(msg, NL80211_WOWLAN_TRIG_NET_DETECT,\n\t\t\trdev->wiphy.wowlan->max_nd_match_sets))\n\t\treturn -ENOBUFS;\n\n\tif (large && nl80211_send_wowlan_tcp_caps(rdev, msg))\n\t\treturn -ENOBUFS;\n\n\tnla_nest_end(msg, nl_wowlan);\n\n\treturn 0;\n}\n#endif\n\nstatic int nl80211_send_coalesce(struct sk_buff *msg,\n\t\t\t\t struct cfg80211_registered_device *rdev)\n{\n\tstruct nl80211_coalesce_rule_support rule;\n\n\tif (!rdev->wiphy.coalesce)\n\t\treturn 0;\n\n\trule.max_rules = rdev->wiphy.coalesce->n_rules;\n\trule.max_delay = rdev->wiphy.coalesce->max_delay;\n\trule.pat.max_patterns = rdev->wiphy.coalesce->n_patterns;\n\trule.pat.min_pattern_len = rdev->wiphy.coalesce->pattern_min_len;\n\trule.pat.max_pattern_len = rdev->wiphy.coalesce->pattern_max_len;\n\trule.pat.max_pkt_offset = rdev->wiphy.coalesce->max_pkt_offset;\n\n\tif (nla_put(msg, NL80211_ATTR_COALESCE_RULE, sizeof(rule), &rule))\n\t\treturn -ENOBUFS;\n\n\treturn 0;\n}\n\nstatic int\nnl80211_send_iftype_data(struct sk_buff *msg,\n\t\t\t const struct ieee80211_supported_band *sband,\n\t\t\t const struct ieee80211_sband_iftype_data *iftdata)\n{\n\tconst struct ieee80211_sta_he_cap *he_cap = &iftdata->he_cap;\n\n\tif (nl80211_put_iftypes(msg, NL80211_BAND_IFTYPE_ATTR_IFTYPES,\n\t\t\t\tiftdata->types_mask))\n\t\treturn -ENOBUFS;\n\n\tif (he_cap->has_he) {\n\t\tif (nla_put(msg, NL80211_BAND_IFTYPE_ATTR_HE_CAP_MAC,\n\t\t\t    sizeof(he_cap->he_cap_elem.mac_cap_info),\n\t\t\t    he_cap->he_cap_elem.mac_cap_info) ||\n\t\t    nla_put(msg, NL80211_BAND_IFTYPE_ATTR_HE_CAP_PHY,\n\t\t\t    sizeof(he_cap->he_cap_elem.phy_cap_info),\n\t\t\t    he_cap->he_cap_elem.phy_cap_info) ||\n\t\t    nla_put(msg, NL80211_BAND_IFTYPE_ATTR_HE_CAP_MCS_SET,\n\t\t\t    sizeof(he_cap->he_mcs_nss_supp),\n\t\t\t    &he_cap->he_mcs_nss_supp) ||\n\t\t    nla_put(msg, NL80211_BAND_IFTYPE_ATTR_HE_CAP_PPE,\n\t\t\t    sizeof(he_cap->ppe_thres), he_cap->ppe_thres))\n\t\t\treturn -ENOBUFS;\n\t}\n\n\tif (sband->band == NL80211_BAND_6GHZ &&\n\t    nla_put(msg, NL80211_BAND_IFTYPE_ATTR_HE_6GHZ_CAPA,\n\t\t    sizeof(iftdata->he_6ghz_capa),\n\t\t    &iftdata->he_6ghz_capa))\n\t\treturn -ENOBUFS;\n\n\treturn 0;\n}\n\nstatic int nl80211_send_band_rateinfo(struct sk_buff *msg,\n\t\t\t\t      struct ieee80211_supported_band *sband,\n\t\t\t\t      bool large)\n{\n\tstruct nlattr *nl_rates, *nl_rate;\n\tstruct ieee80211_rate *rate;\n\tint i;\n\n\t/* add HT info */\n\tif (sband->ht_cap.ht_supported &&\n\t    (nla_put(msg, NL80211_BAND_ATTR_HT_MCS_SET,\n\t\t     sizeof(sband->ht_cap.mcs),\n\t\t     &sband->ht_cap.mcs) ||\n\t     nla_put_u16(msg, NL80211_BAND_ATTR_HT_CAPA,\n\t\t\t sband->ht_cap.cap) ||\n\t     nla_put_u8(msg, NL80211_BAND_ATTR_HT_AMPDU_FACTOR,\n\t\t\tsband->ht_cap.ampdu_factor) ||\n\t     nla_put_u8(msg, NL80211_BAND_ATTR_HT_AMPDU_DENSITY,\n\t\t\tsband->ht_cap.ampdu_density)))\n\t\treturn -ENOBUFS;\n\n\t/* add VHT info */\n\tif (sband->vht_cap.vht_supported &&\n\t    (nla_put(msg, NL80211_BAND_ATTR_VHT_MCS_SET,\n\t\t     sizeof(sband->vht_cap.vht_mcs),\n\t\t     &sband->vht_cap.vht_mcs) ||\n\t     nla_put_u32(msg, NL80211_BAND_ATTR_VHT_CAPA,\n\t\t\t sband->vht_cap.cap)))\n\t\treturn -ENOBUFS;\n\n\tif (large && sband->n_iftype_data) {\n\t\tstruct nlattr *nl_iftype_data =\n\t\t\tnla_nest_start_noflag(msg,\n\t\t\t\t\t      NL80211_BAND_ATTR_IFTYPE_DATA);\n\t\tint err;\n\n\t\tif (!nl_iftype_data)\n\t\t\treturn -ENOBUFS;\n\n\t\tfor (i = 0; i < sband->n_iftype_data; i++) {\n\t\t\tstruct nlattr *iftdata;\n\n\t\t\tiftdata = nla_nest_start_noflag(msg, i + 1);\n\t\t\tif (!iftdata)\n\t\t\t\treturn -ENOBUFS;\n\n\t\t\terr = nl80211_send_iftype_data(msg, sband,\n\t\t\t\t\t\t       &sband->iftype_data[i]);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tnla_nest_end(msg, iftdata);\n\t\t}\n\n\t\tnla_nest_end(msg, nl_iftype_data);\n\t}\n\n\t/* add EDMG info */\n\tif (large && sband->edmg_cap.channels &&\n\t    (nla_put_u8(msg, NL80211_BAND_ATTR_EDMG_CHANNELS,\n\t\t       sband->edmg_cap.channels) ||\n\t    nla_put_u8(msg, NL80211_BAND_ATTR_EDMG_BW_CONFIG,\n\t\t       sband->edmg_cap.bw_config)))\n\n\t\treturn -ENOBUFS;\n\n\t/* add bitrates */\n\tnl_rates = nla_nest_start_noflag(msg, NL80211_BAND_ATTR_RATES);\n\tif (!nl_rates)\n\t\treturn -ENOBUFS;\n\n\tfor (i = 0; i < sband->n_bitrates; i++) {\n\t\tnl_rate = nla_nest_start_noflag(msg, i);\n\t\tif (!nl_rate)\n\t\t\treturn -ENOBUFS;\n\n\t\trate = &sband->bitrates[i];\n\t\tif (nla_put_u32(msg, NL80211_BITRATE_ATTR_RATE,\n\t\t\t\trate->bitrate))\n\t\t\treturn -ENOBUFS;\n\t\tif ((rate->flags & IEEE80211_RATE_SHORT_PREAMBLE) &&\n\t\t    nla_put_flag(msg,\n\t\t\t\t NL80211_BITRATE_ATTR_2GHZ_SHORTPREAMBLE))\n\t\t\treturn -ENOBUFS;\n\n\t\tnla_nest_end(msg, nl_rate);\n\t}\n\n\tnla_nest_end(msg, nl_rates);\n\n\treturn 0;\n}\n\nstatic int\nnl80211_send_mgmt_stypes(struct sk_buff *msg,\n\t\t\t const struct ieee80211_txrx_stypes *mgmt_stypes)\n{\n\tu16 stypes;\n\tstruct nlattr *nl_ftypes, *nl_ifs;\n\tenum nl80211_iftype ift;\n\tint i;\n\n\tif (!mgmt_stypes)\n\t\treturn 0;\n\n\tnl_ifs = nla_nest_start_noflag(msg, NL80211_ATTR_TX_FRAME_TYPES);\n\tif (!nl_ifs)\n\t\treturn -ENOBUFS;\n\n\tfor (ift = 0; ift < NUM_NL80211_IFTYPES; ift++) {\n\t\tnl_ftypes = nla_nest_start_noflag(msg, ift);\n\t\tif (!nl_ftypes)\n\t\t\treturn -ENOBUFS;\n\t\ti = 0;\n\t\tstypes = mgmt_stypes[ift].tx;\n\t\twhile (stypes) {\n\t\t\tif ((stypes & 1) &&\n\t\t\t    nla_put_u16(msg, NL80211_ATTR_FRAME_TYPE,\n\t\t\t\t\t(i << 4) | IEEE80211_FTYPE_MGMT))\n\t\t\t\treturn -ENOBUFS;\n\t\t\tstypes >>= 1;\n\t\t\ti++;\n\t\t}\n\t\tnla_nest_end(msg, nl_ftypes);\n\t}\n\n\tnla_nest_end(msg, nl_ifs);\n\n\tnl_ifs = nla_nest_start_noflag(msg, NL80211_ATTR_RX_FRAME_TYPES);\n\tif (!nl_ifs)\n\t\treturn -ENOBUFS;\n\n\tfor (ift = 0; ift < NUM_NL80211_IFTYPES; ift++) {\n\t\tnl_ftypes = nla_nest_start_noflag(msg, ift);\n\t\tif (!nl_ftypes)\n\t\t\treturn -ENOBUFS;\n\t\ti = 0;\n\t\tstypes = mgmt_stypes[ift].rx;\n\t\twhile (stypes) {\n\t\t\tif ((stypes & 1) &&\n\t\t\t    nla_put_u16(msg, NL80211_ATTR_FRAME_TYPE,\n\t\t\t\t\t(i << 4) | IEEE80211_FTYPE_MGMT))\n\t\t\t\treturn -ENOBUFS;\n\t\t\tstypes >>= 1;\n\t\t\ti++;\n\t\t}\n\t\tnla_nest_end(msg, nl_ftypes);\n\t}\n\tnla_nest_end(msg, nl_ifs);\n\n\treturn 0;\n}\n\n#define CMD(op, n)\t\t\t\t\t\t\t\\\n\t do {\t\t\t\t\t\t\t\t\\\n\t\tif (rdev->ops->op) {\t\t\t\t\t\\\n\t\t\ti++;\t\t\t\t\t\t\\\n\t\t\tif (nla_put_u32(msg, i, NL80211_CMD_ ## n)) \t\\\n\t\t\t\tgoto nla_put_failure;\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\nstatic int nl80211_add_commands_unsplit(struct cfg80211_registered_device *rdev,\n\t\t\t\t\tstruct sk_buff *msg)\n{\n\tint i = 0;\n\n\t/*\n\t * do *NOT* add anything into this function, new things need to be\n\t * advertised only to new versions of userspace that can deal with\n\t * the split (and they can't possibly care about new features...\n\t */\n\tCMD(add_virtual_intf, NEW_INTERFACE);\n\tCMD(change_virtual_intf, SET_INTERFACE);\n\tCMD(add_key, NEW_KEY);\n\tCMD(start_ap, START_AP);\n\tCMD(add_station, NEW_STATION);\n\tCMD(add_mpath, NEW_MPATH);\n\tCMD(update_mesh_config, SET_MESH_CONFIG);\n\tCMD(change_bss, SET_BSS);\n\tCMD(auth, AUTHENTICATE);\n\tCMD(assoc, ASSOCIATE);\n\tCMD(deauth, DEAUTHENTICATE);\n\tCMD(disassoc, DISASSOCIATE);\n\tCMD(join_ibss, JOIN_IBSS);\n\tCMD(join_mesh, JOIN_MESH);\n\tCMD(set_pmksa, SET_PMKSA);\n\tCMD(del_pmksa, DEL_PMKSA);\n\tCMD(flush_pmksa, FLUSH_PMKSA);\n\tif (rdev->wiphy.flags & WIPHY_FLAG_HAS_REMAIN_ON_CHANNEL)\n\t\tCMD(remain_on_channel, REMAIN_ON_CHANNEL);\n\tCMD(set_bitrate_mask, SET_TX_BITRATE_MASK);\n\tCMD(mgmt_tx, FRAME);\n\tCMD(mgmt_tx_cancel_wait, FRAME_WAIT_CANCEL);\n\tif (rdev->wiphy.flags & WIPHY_FLAG_NETNS_OK) {\n\t\ti++;\n\t\tif (nla_put_u32(msg, i, NL80211_CMD_SET_WIPHY_NETNS))\n\t\t\tgoto nla_put_failure;\n\t}\n\tif (rdev->ops->set_monitor_channel || rdev->ops->start_ap ||\n\t    rdev->ops->join_mesh) {\n\t\ti++;\n\t\tif (nla_put_u32(msg, i, NL80211_CMD_SET_CHANNEL))\n\t\t\tgoto nla_put_failure;\n\t}\n\tif (rdev->wiphy.flags & WIPHY_FLAG_SUPPORTS_TDLS) {\n\t\tCMD(tdls_mgmt, TDLS_MGMT);\n\t\tCMD(tdls_oper, TDLS_OPER);\n\t}\n\tif (rdev->wiphy.max_sched_scan_reqs)\n\t\tCMD(sched_scan_start, START_SCHED_SCAN);\n\tCMD(probe_client, PROBE_CLIENT);\n\tCMD(set_noack_map, SET_NOACK_MAP);\n\tif (rdev->wiphy.flags & WIPHY_FLAG_REPORTS_OBSS) {\n\t\ti++;\n\t\tif (nla_put_u32(msg, i, NL80211_CMD_REGISTER_BEACONS))\n\t\t\tgoto nla_put_failure;\n\t}\n\tCMD(start_p2p_device, START_P2P_DEVICE);\n\tCMD(set_mcast_rate, SET_MCAST_RATE);\n#ifdef CONFIG_NL80211_TESTMODE\n\tCMD(testmode_cmd, TESTMODE);\n#endif\n\n\tif (rdev->ops->connect || rdev->ops->auth) {\n\t\ti++;\n\t\tif (nla_put_u32(msg, i, NL80211_CMD_CONNECT))\n\t\t\tgoto nla_put_failure;\n\t}\n\n\tif (rdev->ops->disconnect || rdev->ops->deauth) {\n\t\ti++;\n\t\tif (nla_put_u32(msg, i, NL80211_CMD_DISCONNECT))\n\t\t\tgoto nla_put_failure;\n\t}\n\n\treturn i;\n nla_put_failure:\n\treturn -ENOBUFS;\n}\n\nstatic int\nnl80211_send_pmsr_ftm_capa(const struct cfg80211_pmsr_capabilities *cap,\n\t\t\t   struct sk_buff *msg)\n{\n\tstruct nlattr *ftm;\n\n\tif (!cap->ftm.supported)\n\t\treturn 0;\n\n\tftm = nla_nest_start_noflag(msg, NL80211_PMSR_TYPE_FTM);\n\tif (!ftm)\n\t\treturn -ENOBUFS;\n\n\tif (cap->ftm.asap && nla_put_flag(msg, NL80211_PMSR_FTM_CAPA_ATTR_ASAP))\n\t\treturn -ENOBUFS;\n\tif (cap->ftm.non_asap &&\n\t    nla_put_flag(msg, NL80211_PMSR_FTM_CAPA_ATTR_NON_ASAP))\n\t\treturn -ENOBUFS;\n\tif (cap->ftm.request_lci &&\n\t    nla_put_flag(msg, NL80211_PMSR_FTM_CAPA_ATTR_REQ_LCI))\n\t\treturn -ENOBUFS;\n\tif (cap->ftm.request_civicloc &&\n\t    nla_put_flag(msg, NL80211_PMSR_FTM_CAPA_ATTR_REQ_CIVICLOC))\n\t\treturn -ENOBUFS;\n\tif (nla_put_u32(msg, NL80211_PMSR_FTM_CAPA_ATTR_PREAMBLES,\n\t\t\tcap->ftm.preambles))\n\t\treturn -ENOBUFS;\n\tif (nla_put_u32(msg, NL80211_PMSR_FTM_CAPA_ATTR_BANDWIDTHS,\n\t\t\tcap->ftm.bandwidths))\n\t\treturn -ENOBUFS;\n\tif (cap->ftm.max_bursts_exponent >= 0 &&\n\t    nla_put_u32(msg, NL80211_PMSR_FTM_CAPA_ATTR_MAX_BURSTS_EXPONENT,\n\t\t\tcap->ftm.max_bursts_exponent))\n\t\treturn -ENOBUFS;\n\tif (cap->ftm.max_ftms_per_burst &&\n\t    nla_put_u32(msg, NL80211_PMSR_FTM_CAPA_ATTR_MAX_FTMS_PER_BURST,\n\t\t\tcap->ftm.max_ftms_per_burst))\n\t\treturn -ENOBUFS;\n\tif (cap->ftm.trigger_based &&\n\t    nla_put_flag(msg, NL80211_PMSR_FTM_CAPA_ATTR_TRIGGER_BASED))\n\t\treturn -ENOBUFS;\n\tif (cap->ftm.non_trigger_based &&\n\t    nla_put_flag(msg, NL80211_PMSR_FTM_CAPA_ATTR_NON_TRIGGER_BASED))\n\t\treturn -ENOBUFS;\n\n\tnla_nest_end(msg, ftm);\n\treturn 0;\n}\n\nstatic int nl80211_send_pmsr_capa(struct cfg80211_registered_device *rdev,\n\t\t\t\t  struct sk_buff *msg)\n{\n\tconst struct cfg80211_pmsr_capabilities *cap = rdev->wiphy.pmsr_capa;\n\tstruct nlattr *pmsr, *caps;\n\n\tif (!cap)\n\t\treturn 0;\n\n\t/*\n\t * we don't need to clean up anything here since the caller\n\t * will genlmsg_cancel() if we fail\n\t */\n\n\tpmsr = nla_nest_start_noflag(msg, NL80211_ATTR_PEER_MEASUREMENTS);\n\tif (!pmsr)\n\t\treturn -ENOBUFS;\n\n\tif (nla_put_u32(msg, NL80211_PMSR_ATTR_MAX_PEERS, cap->max_peers))\n\t\treturn -ENOBUFS;\n\n\tif (cap->report_ap_tsf &&\n\t    nla_put_flag(msg, NL80211_PMSR_ATTR_REPORT_AP_TSF))\n\t\treturn -ENOBUFS;\n\n\tif (cap->randomize_mac_addr &&\n\t    nla_put_flag(msg, NL80211_PMSR_ATTR_RANDOMIZE_MAC_ADDR))\n\t\treturn -ENOBUFS;\n\n\tcaps = nla_nest_start_noflag(msg, NL80211_PMSR_ATTR_TYPE_CAPA);\n\tif (!caps)\n\t\treturn -ENOBUFS;\n\n\tif (nl80211_send_pmsr_ftm_capa(cap, msg))\n\t\treturn -ENOBUFS;\n\n\tnla_nest_end(msg, caps);\n\tnla_nest_end(msg, pmsr);\n\n\treturn 0;\n}\n\nstatic int\nnl80211_put_iftype_akm_suites(struct cfg80211_registered_device *rdev,\n\t\t\t      struct sk_buff *msg)\n{\n\tint i;\n\tstruct nlattr *nested, *nested_akms;\n\tconst struct wiphy_iftype_akm_suites *iftype_akms;\n\n\tif (!rdev->wiphy.num_iftype_akm_suites ||\n\t    !rdev->wiphy.iftype_akm_suites)\n\t\treturn 0;\n\n\tnested = nla_nest_start(msg, NL80211_ATTR_IFTYPE_AKM_SUITES);\n\tif (!nested)\n\t\treturn -ENOBUFS;\n\n\tfor (i = 0; i < rdev->wiphy.num_iftype_akm_suites; i++) {\n\t\tnested_akms = nla_nest_start(msg, i + 1);\n\t\tif (!nested_akms)\n\t\t\treturn -ENOBUFS;\n\n\t\tiftype_akms = &rdev->wiphy.iftype_akm_suites[i];\n\n\t\tif (nl80211_put_iftypes(msg, NL80211_IFTYPE_AKM_ATTR_IFTYPES,\n\t\t\t\t\tiftype_akms->iftypes_mask))\n\t\t\treturn -ENOBUFS;\n\n\t\tif (nla_put(msg, NL80211_IFTYPE_AKM_ATTR_SUITES,\n\t\t\t    sizeof(u32) * iftype_akms->n_akm_suites,\n\t\t\t    iftype_akms->akm_suites)) {\n\t\t\treturn -ENOBUFS;\n\t\t}\n\t\tnla_nest_end(msg, nested_akms);\n\t}\n\n\tnla_nest_end(msg, nested);\n\n\treturn 0;\n}\n\nstatic int\nnl80211_put_tid_config_support(struct cfg80211_registered_device *rdev,\n\t\t\t       struct sk_buff *msg)\n{\n\tstruct nlattr *supp;\n\n\tif (!rdev->wiphy.tid_config_support.vif &&\n\t    !rdev->wiphy.tid_config_support.peer)\n\t\treturn 0;\n\n\tsupp = nla_nest_start(msg, NL80211_ATTR_TID_CONFIG);\n\tif (!supp)\n\t\treturn -ENOSPC;\n\n\tif (rdev->wiphy.tid_config_support.vif &&\n\t    nla_put_u64_64bit(msg, NL80211_TID_CONFIG_ATTR_VIF_SUPP,\n\t\t\t      rdev->wiphy.tid_config_support.vif,\n\t\t\t      NL80211_TID_CONFIG_ATTR_PAD))\n\t\tgoto fail;\n\n\tif (rdev->wiphy.tid_config_support.peer &&\n\t    nla_put_u64_64bit(msg, NL80211_TID_CONFIG_ATTR_PEER_SUPP,\n\t\t\t      rdev->wiphy.tid_config_support.peer,\n\t\t\t      NL80211_TID_CONFIG_ATTR_PAD))\n\t\tgoto fail;\n\n\t/* for now we just use the same value ... makes more sense */\n\tif (nla_put_u8(msg, NL80211_TID_CONFIG_ATTR_RETRY_SHORT,\n\t\t       rdev->wiphy.tid_config_support.max_retry))\n\t\tgoto fail;\n\tif (nla_put_u8(msg, NL80211_TID_CONFIG_ATTR_RETRY_LONG,\n\t\t       rdev->wiphy.tid_config_support.max_retry))\n\t\tgoto fail;\n\n\tnla_nest_end(msg, supp);\n\n\treturn 0;\nfail:\n\tnla_nest_cancel(msg, supp);\n\treturn -ENOBUFS;\n}\n\nstatic int\nnl80211_put_sar_specs(struct cfg80211_registered_device *rdev,\n\t\t      struct sk_buff *msg)\n{\n\tstruct nlattr *sar_capa, *specs, *sub_freq_range;\n\tu8 num_freq_ranges;\n\tint i;\n\n\tif (!rdev->wiphy.sar_capa)\n\t\treturn 0;\n\n\tnum_freq_ranges = rdev->wiphy.sar_capa->num_freq_ranges;\n\n\tsar_capa = nla_nest_start(msg, NL80211_ATTR_SAR_SPEC);\n\tif (!sar_capa)\n\t\treturn -ENOSPC;\n\n\tif (nla_put_u32(msg, NL80211_SAR_ATTR_TYPE, rdev->wiphy.sar_capa->type))\n\t\tgoto fail;\n\n\tspecs = nla_nest_start(msg, NL80211_SAR_ATTR_SPECS);\n\tif (!specs)\n\t\tgoto fail;\n\n\t/* report supported freq_ranges */\n\tfor (i = 0; i < num_freq_ranges; i++) {\n\t\tsub_freq_range = nla_nest_start(msg, i + 1);\n\t\tif (!sub_freq_range)\n\t\t\tgoto fail;\n\n\t\tif (nla_put_u32(msg, NL80211_SAR_ATTR_SPECS_START_FREQ,\n\t\t\t\trdev->wiphy.sar_capa->freq_ranges[i].start_freq))\n\t\t\tgoto fail;\n\n\t\tif (nla_put_u32(msg, NL80211_SAR_ATTR_SPECS_END_FREQ,\n\t\t\t\trdev->wiphy.sar_capa->freq_ranges[i].end_freq))\n\t\t\tgoto fail;\n\n\t\tnla_nest_end(msg, sub_freq_range);\n\t}\n\n\tnla_nest_end(msg, specs);\n\tnla_nest_end(msg, sar_capa);\n\n\treturn 0;\nfail:\n\tnla_nest_cancel(msg, sar_capa);\n\treturn -ENOBUFS;\n}\n\nstruct nl80211_dump_wiphy_state {\n\ts64 filter_wiphy;\n\tlong start;\n\tlong split_start, band_start, chan_start, capa_start;\n\tbool split;\n};\n\nstatic int nl80211_send_wiphy(struct cfg80211_registered_device *rdev,\n\t\t\t      enum nl80211_commands cmd,\n\t\t\t      struct sk_buff *msg, u32 portid, u32 seq,\n\t\t\t      int flags, struct nl80211_dump_wiphy_state *state)\n{\n\tvoid *hdr;\n\tstruct nlattr *nl_bands, *nl_band;\n\tstruct nlattr *nl_freqs, *nl_freq;\n\tstruct nlattr *nl_cmds;\n\tenum nl80211_band band;\n\tstruct ieee80211_channel *chan;\n\tint i;\n\tconst struct ieee80211_txrx_stypes *mgmt_stypes =\n\t\t\t\trdev->wiphy.mgmt_stypes;\n\tu32 features;\n\n\thdr = nl80211hdr_put(msg, portid, seq, flags, cmd);\n\tif (!hdr)\n\t\treturn -ENOBUFS;\n\n\tif (WARN_ON(!state))\n\t\treturn -EINVAL;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_string(msg, NL80211_ATTR_WIPHY_NAME,\n\t\t\t   wiphy_name(&rdev->wiphy)) ||\n\t    nla_put_u32(msg, NL80211_ATTR_GENERATION,\n\t\t\tcfg80211_rdev_list_generation))\n\t\tgoto nla_put_failure;\n\n\tif (cmd != NL80211_CMD_NEW_WIPHY)\n\t\tgoto finish;\n\n\tswitch (state->split_start) {\n\tcase 0:\n\t\tif (nla_put_u8(msg, NL80211_ATTR_WIPHY_RETRY_SHORT,\n\t\t\t       rdev->wiphy.retry_short) ||\n\t\t    nla_put_u8(msg, NL80211_ATTR_WIPHY_RETRY_LONG,\n\t\t\t       rdev->wiphy.retry_long) ||\n\t\t    nla_put_u32(msg, NL80211_ATTR_WIPHY_FRAG_THRESHOLD,\n\t\t\t\trdev->wiphy.frag_threshold) ||\n\t\t    nla_put_u32(msg, NL80211_ATTR_WIPHY_RTS_THRESHOLD,\n\t\t\t\trdev->wiphy.rts_threshold) ||\n\t\t    nla_put_u8(msg, NL80211_ATTR_WIPHY_COVERAGE_CLASS,\n\t\t\t       rdev->wiphy.coverage_class) ||\n\t\t    nla_put_u8(msg, NL80211_ATTR_MAX_NUM_SCAN_SSIDS,\n\t\t\t       rdev->wiphy.max_scan_ssids) ||\n\t\t    nla_put_u8(msg, NL80211_ATTR_MAX_NUM_SCHED_SCAN_SSIDS,\n\t\t\t       rdev->wiphy.max_sched_scan_ssids) ||\n\t\t    nla_put_u16(msg, NL80211_ATTR_MAX_SCAN_IE_LEN,\n\t\t\t\trdev->wiphy.max_scan_ie_len) ||\n\t\t    nla_put_u16(msg, NL80211_ATTR_MAX_SCHED_SCAN_IE_LEN,\n\t\t\t\trdev->wiphy.max_sched_scan_ie_len) ||\n\t\t    nla_put_u8(msg, NL80211_ATTR_MAX_MATCH_SETS,\n\t\t\t       rdev->wiphy.max_match_sets))\n\t\t\tgoto nla_put_failure;\n\n\t\tif ((rdev->wiphy.flags & WIPHY_FLAG_IBSS_RSN) &&\n\t\t    nla_put_flag(msg, NL80211_ATTR_SUPPORT_IBSS_RSN))\n\t\t\tgoto nla_put_failure;\n\t\tif ((rdev->wiphy.flags & WIPHY_FLAG_MESH_AUTH) &&\n\t\t    nla_put_flag(msg, NL80211_ATTR_SUPPORT_MESH_AUTH))\n\t\t\tgoto nla_put_failure;\n\t\tif ((rdev->wiphy.flags & WIPHY_FLAG_AP_UAPSD) &&\n\t\t    nla_put_flag(msg, NL80211_ATTR_SUPPORT_AP_UAPSD))\n\t\t\tgoto nla_put_failure;\n\t\tif ((rdev->wiphy.flags & WIPHY_FLAG_SUPPORTS_FW_ROAM) &&\n\t\t    nla_put_flag(msg, NL80211_ATTR_ROAM_SUPPORT))\n\t\t\tgoto nla_put_failure;\n\t\tif ((rdev->wiphy.flags & WIPHY_FLAG_SUPPORTS_TDLS) &&\n\t\t    nla_put_flag(msg, NL80211_ATTR_TDLS_SUPPORT))\n\t\t\tgoto nla_put_failure;\n\t\tif ((rdev->wiphy.flags & WIPHY_FLAG_TDLS_EXTERNAL_SETUP) &&\n\t\t    nla_put_flag(msg, NL80211_ATTR_TDLS_EXTERNAL_SETUP))\n\t\t\tgoto nla_put_failure;\n\t\tstate->split_start++;\n\t\tif (state->split)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase 1:\n\t\tif (nla_put(msg, NL80211_ATTR_CIPHER_SUITES,\n\t\t\t    sizeof(u32) * rdev->wiphy.n_cipher_suites,\n\t\t\t    rdev->wiphy.cipher_suites))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (nla_put_u8(msg, NL80211_ATTR_MAX_NUM_PMKIDS,\n\t\t\t       rdev->wiphy.max_num_pmkids))\n\t\t\tgoto nla_put_failure;\n\n\t\tif ((rdev->wiphy.flags & WIPHY_FLAG_CONTROL_PORT_PROTOCOL) &&\n\t\t    nla_put_flag(msg, NL80211_ATTR_CONTROL_PORT_ETHERTYPE))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY_ANTENNA_AVAIL_TX,\n\t\t\t\trdev->wiphy.available_antennas_tx) ||\n\t\t    nla_put_u32(msg, NL80211_ATTR_WIPHY_ANTENNA_AVAIL_RX,\n\t\t\t\trdev->wiphy.available_antennas_rx))\n\t\t\tgoto nla_put_failure;\n\n\t\tif ((rdev->wiphy.flags & WIPHY_FLAG_AP_PROBE_RESP_OFFLOAD) &&\n\t\t    nla_put_u32(msg, NL80211_ATTR_PROBE_RESP_OFFLOAD,\n\t\t\t\trdev->wiphy.probe_resp_offload))\n\t\t\tgoto nla_put_failure;\n\n\t\tif ((rdev->wiphy.available_antennas_tx ||\n\t\t     rdev->wiphy.available_antennas_rx) &&\n\t\t    rdev->ops->get_antenna) {\n\t\t\tu32 tx_ant = 0, rx_ant = 0;\n\t\t\tint res;\n\n\t\t\tres = rdev_get_antenna(rdev, &tx_ant, &rx_ant);\n\t\t\tif (!res) {\n\t\t\t\tif (nla_put_u32(msg,\n\t\t\t\t\t\tNL80211_ATTR_WIPHY_ANTENNA_TX,\n\t\t\t\t\t\ttx_ant) ||\n\t\t\t\t    nla_put_u32(msg,\n\t\t\t\t\t\tNL80211_ATTR_WIPHY_ANTENNA_RX,\n\t\t\t\t\t\trx_ant))\n\t\t\t\t\tgoto nla_put_failure;\n\t\t\t}\n\t\t}\n\n\t\tstate->split_start++;\n\t\tif (state->split)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase 2:\n\t\tif (nl80211_put_iftypes(msg, NL80211_ATTR_SUPPORTED_IFTYPES,\n\t\t\t\t\trdev->wiphy.interface_modes))\n\t\t\t\tgoto nla_put_failure;\n\t\tstate->split_start++;\n\t\tif (state->split)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase 3:\n\t\tnl_bands = nla_nest_start_noflag(msg,\n\t\t\t\t\t\t NL80211_ATTR_WIPHY_BANDS);\n\t\tif (!nl_bands)\n\t\t\tgoto nla_put_failure;\n\n\t\tfor (band = state->band_start;\n\t\t     band < NUM_NL80211_BANDS; band++) {\n\t\t\tstruct ieee80211_supported_band *sband;\n\n\t\t\t/* omit higher bands for ancient software */\n\t\t\tif (band > NL80211_BAND_5GHZ && !state->split)\n\t\t\t\tbreak;\n\n\t\t\tsband = rdev->wiphy.bands[band];\n\n\t\t\tif (!sband)\n\t\t\t\tcontinue;\n\n\t\t\tnl_band = nla_nest_start_noflag(msg, band);\n\t\t\tif (!nl_band)\n\t\t\t\tgoto nla_put_failure;\n\n\t\t\tswitch (state->chan_start) {\n\t\t\tcase 0:\n\t\t\t\tif (nl80211_send_band_rateinfo(msg, sband,\n\t\t\t\t\t\t\t       state->split))\n\t\t\t\t\tgoto nla_put_failure;\n\t\t\t\tstate->chan_start++;\n\t\t\t\tif (state->split)\n\t\t\t\t\tbreak;\n\t\t\t\tfallthrough;\n\t\t\tdefault:\n\t\t\t\t/* add frequencies */\n\t\t\t\tnl_freqs = nla_nest_start_noflag(msg,\n\t\t\t\t\t\t\t\t NL80211_BAND_ATTR_FREQS);\n\t\t\t\tif (!nl_freqs)\n\t\t\t\t\tgoto nla_put_failure;\n\n\t\t\t\tfor (i = state->chan_start - 1;\n\t\t\t\t     i < sband->n_channels;\n\t\t\t\t     i++) {\n\t\t\t\t\tnl_freq = nla_nest_start_noflag(msg,\n\t\t\t\t\t\t\t\t\ti);\n\t\t\t\t\tif (!nl_freq)\n\t\t\t\t\t\tgoto nla_put_failure;\n\n\t\t\t\t\tchan = &sband->channels[i];\n\n\t\t\t\t\tif (nl80211_msg_put_channel(\n\t\t\t\t\t\t\tmsg, &rdev->wiphy, chan,\n\t\t\t\t\t\t\tstate->split))\n\t\t\t\t\t\tgoto nla_put_failure;\n\n\t\t\t\t\tnla_nest_end(msg, nl_freq);\n\t\t\t\t\tif (state->split)\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (i < sband->n_channels)\n\t\t\t\t\tstate->chan_start = i + 2;\n\t\t\t\telse\n\t\t\t\t\tstate->chan_start = 0;\n\t\t\t\tnla_nest_end(msg, nl_freqs);\n\t\t\t}\n\n\t\t\tnla_nest_end(msg, nl_band);\n\n\t\t\tif (state->split) {\n\t\t\t\t/* start again here */\n\t\t\t\tif (state->chan_start)\n\t\t\t\t\tband--;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tnla_nest_end(msg, nl_bands);\n\n\t\tif (band < NUM_NL80211_BANDS)\n\t\t\tstate->band_start = band + 1;\n\t\telse\n\t\t\tstate->band_start = 0;\n\n\t\t/* if bands & channels are done, continue outside */\n\t\tif (state->band_start == 0 && state->chan_start == 0)\n\t\t\tstate->split_start++;\n\t\tif (state->split)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase 4:\n\t\tnl_cmds = nla_nest_start_noflag(msg,\n\t\t\t\t\t\tNL80211_ATTR_SUPPORTED_COMMANDS);\n\t\tif (!nl_cmds)\n\t\t\tgoto nla_put_failure;\n\n\t\ti = nl80211_add_commands_unsplit(rdev, msg);\n\t\tif (i < 0)\n\t\t\tgoto nla_put_failure;\n\t\tif (state->split) {\n\t\t\tCMD(crit_proto_start, CRIT_PROTOCOL_START);\n\t\t\tCMD(crit_proto_stop, CRIT_PROTOCOL_STOP);\n\t\t\tif (rdev->wiphy.flags & WIPHY_FLAG_HAS_CHANNEL_SWITCH)\n\t\t\t\tCMD(channel_switch, CHANNEL_SWITCH);\n\t\t\tCMD(set_qos_map, SET_QOS_MAP);\n\t\t\tif (rdev->wiphy.features &\n\t\t\t\t\tNL80211_FEATURE_SUPPORTS_WMM_ADMISSION)\n\t\t\t\tCMD(add_tx_ts, ADD_TX_TS);\n\t\t\tCMD(set_multicast_to_unicast, SET_MULTICAST_TO_UNICAST);\n\t\t\tCMD(update_connect_params, UPDATE_CONNECT_PARAMS);\n\t\t\tCMD(update_ft_ies, UPDATE_FT_IES);\n\t\t\tif (rdev->wiphy.sar_capa)\n\t\t\t\tCMD(set_sar_specs, SET_SAR_SPECS);\n\t\t}\n#undef CMD\n\n\t\tnla_nest_end(msg, nl_cmds);\n\t\tstate->split_start++;\n\t\tif (state->split)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase 5:\n\t\tif (rdev->ops->remain_on_channel &&\n\t\t    (rdev->wiphy.flags & WIPHY_FLAG_HAS_REMAIN_ON_CHANNEL) &&\n\t\t    nla_put_u32(msg,\n\t\t\t\tNL80211_ATTR_MAX_REMAIN_ON_CHANNEL_DURATION,\n\t\t\t\trdev->wiphy.max_remain_on_channel_duration))\n\t\t\tgoto nla_put_failure;\n\n\t\tif ((rdev->wiphy.flags & WIPHY_FLAG_OFFCHAN_TX) &&\n\t\t    nla_put_flag(msg, NL80211_ATTR_OFFCHANNEL_TX_OK))\n\t\t\tgoto nla_put_failure;\n\n\t\tstate->split_start++;\n\t\tif (state->split)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase 6:\n#ifdef CONFIG_PM\n\t\tif (nl80211_send_wowlan(msg, rdev, state->split))\n\t\t\tgoto nla_put_failure;\n\t\tstate->split_start++;\n\t\tif (state->split)\n\t\t\tbreak;\n#else\n\t\tstate->split_start++;\n#endif\n\t\tfallthrough;\n\tcase 7:\n\t\tif (nl80211_put_iftypes(msg, NL80211_ATTR_SOFTWARE_IFTYPES,\n\t\t\t\t\trdev->wiphy.software_iftypes))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (nl80211_put_iface_combinations(&rdev->wiphy, msg,\n\t\t\t\t\t\t   state->split))\n\t\t\tgoto nla_put_failure;\n\n\t\tstate->split_start++;\n\t\tif (state->split)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase 8:\n\t\tif ((rdev->wiphy.flags & WIPHY_FLAG_HAVE_AP_SME) &&\n\t\t    nla_put_u32(msg, NL80211_ATTR_DEVICE_AP_SME,\n\t\t\t\trdev->wiphy.ap_sme_capa))\n\t\t\tgoto nla_put_failure;\n\n\t\tfeatures = rdev->wiphy.features;\n\t\t/*\n\t\t * We can only add the per-channel limit information if the\n\t\t * dump is split, otherwise it makes it too big. Therefore\n\t\t * only advertise it in that case.\n\t\t */\n\t\tif (state->split)\n\t\t\tfeatures |= NL80211_FEATURE_ADVERTISE_CHAN_LIMITS;\n\t\tif (nla_put_u32(msg, NL80211_ATTR_FEATURE_FLAGS, features))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (rdev->wiphy.ht_capa_mod_mask &&\n\t\t    nla_put(msg, NL80211_ATTR_HT_CAPABILITY_MASK,\n\t\t\t    sizeof(*rdev->wiphy.ht_capa_mod_mask),\n\t\t\t    rdev->wiphy.ht_capa_mod_mask))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (rdev->wiphy.flags & WIPHY_FLAG_HAVE_AP_SME &&\n\t\t    rdev->wiphy.max_acl_mac_addrs &&\n\t\t    nla_put_u32(msg, NL80211_ATTR_MAC_ACL_MAX,\n\t\t\t\trdev->wiphy.max_acl_mac_addrs))\n\t\t\tgoto nla_put_failure;\n\n\t\t/*\n\t\t * Any information below this point is only available to\n\t\t * applications that can deal with it being split. This\n\t\t * helps ensure that newly added capabilities don't break\n\t\t * older tools by overrunning their buffers.\n\t\t *\n\t\t * We still increment split_start so that in the split\n\t\t * case we'll continue with more data in the next round,\n\t\t * but break unconditionally so unsplit data stops here.\n\t\t */\n\t\tif (state->split)\n\t\t\tstate->split_start++;\n\t\telse\n\t\t\tstate->split_start = 0;\n\t\tbreak;\n\tcase 9:\n\t\tif (nl80211_send_mgmt_stypes(msg, mgmt_stypes))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (nla_put_u32(msg, NL80211_ATTR_MAX_NUM_SCHED_SCAN_PLANS,\n\t\t\t\trdev->wiphy.max_sched_scan_plans) ||\n\t\t    nla_put_u32(msg, NL80211_ATTR_MAX_SCAN_PLAN_INTERVAL,\n\t\t\t\trdev->wiphy.max_sched_scan_plan_interval) ||\n\t\t    nla_put_u32(msg, NL80211_ATTR_MAX_SCAN_PLAN_ITERATIONS,\n\t\t\t\trdev->wiphy.max_sched_scan_plan_iterations))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (rdev->wiphy.extended_capabilities &&\n\t\t    (nla_put(msg, NL80211_ATTR_EXT_CAPA,\n\t\t\t     rdev->wiphy.extended_capabilities_len,\n\t\t\t     rdev->wiphy.extended_capabilities) ||\n\t\t     nla_put(msg, NL80211_ATTR_EXT_CAPA_MASK,\n\t\t\t     rdev->wiphy.extended_capabilities_len,\n\t\t\t     rdev->wiphy.extended_capabilities_mask)))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (rdev->wiphy.vht_capa_mod_mask &&\n\t\t    nla_put(msg, NL80211_ATTR_VHT_CAPABILITY_MASK,\n\t\t\t    sizeof(*rdev->wiphy.vht_capa_mod_mask),\n\t\t\t    rdev->wiphy.vht_capa_mod_mask))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN,\n\t\t\t    rdev->wiphy.perm_addr))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (!is_zero_ether_addr(rdev->wiphy.addr_mask) &&\n\t\t    nla_put(msg, NL80211_ATTR_MAC_MASK, ETH_ALEN,\n\t\t\t    rdev->wiphy.addr_mask))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (rdev->wiphy.n_addresses > 1) {\n\t\t\tvoid *attr;\n\n\t\t\tattr = nla_nest_start(msg, NL80211_ATTR_MAC_ADDRS);\n\t\t\tif (!attr)\n\t\t\t\tgoto nla_put_failure;\n\n\t\t\tfor (i = 0; i < rdev->wiphy.n_addresses; i++)\n\t\t\t\tif (nla_put(msg, i + 1, ETH_ALEN,\n\t\t\t\t\t    rdev->wiphy.addresses[i].addr))\n\t\t\t\t\tgoto nla_put_failure;\n\n\t\t\tnla_nest_end(msg, attr);\n\t\t}\n\n\t\tstate->split_start++;\n\t\tbreak;\n\tcase 10:\n\t\tif (nl80211_send_coalesce(msg, rdev))\n\t\t\tgoto nla_put_failure;\n\n\t\tif ((rdev->wiphy.flags & WIPHY_FLAG_SUPPORTS_5_10_MHZ) &&\n\t\t    (nla_put_flag(msg, NL80211_ATTR_SUPPORT_5_MHZ) ||\n\t\t     nla_put_flag(msg, NL80211_ATTR_SUPPORT_10_MHZ)))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (rdev->wiphy.max_ap_assoc_sta &&\n\t\t    nla_put_u32(msg, NL80211_ATTR_MAX_AP_ASSOC_STA,\n\t\t\t\trdev->wiphy.max_ap_assoc_sta))\n\t\t\tgoto nla_put_failure;\n\n\t\tstate->split_start++;\n\t\tbreak;\n\tcase 11:\n\t\tif (rdev->wiphy.n_vendor_commands) {\n\t\t\tconst struct nl80211_vendor_cmd_info *info;\n\t\t\tstruct nlattr *nested;\n\n\t\t\tnested = nla_nest_start_noflag(msg,\n\t\t\t\t\t\t       NL80211_ATTR_VENDOR_DATA);\n\t\t\tif (!nested)\n\t\t\t\tgoto nla_put_failure;\n\n\t\t\tfor (i = 0; i < rdev->wiphy.n_vendor_commands; i++) {\n\t\t\t\tinfo = &rdev->wiphy.vendor_commands[i].info;\n\t\t\t\tif (nla_put(msg, i + 1, sizeof(*info), info))\n\t\t\t\t\tgoto nla_put_failure;\n\t\t\t}\n\t\t\tnla_nest_end(msg, nested);\n\t\t}\n\n\t\tif (rdev->wiphy.n_vendor_events) {\n\t\t\tconst struct nl80211_vendor_cmd_info *info;\n\t\t\tstruct nlattr *nested;\n\n\t\t\tnested = nla_nest_start_noflag(msg,\n\t\t\t\t\t\t       NL80211_ATTR_VENDOR_EVENTS);\n\t\t\tif (!nested)\n\t\t\t\tgoto nla_put_failure;\n\n\t\t\tfor (i = 0; i < rdev->wiphy.n_vendor_events; i++) {\n\t\t\t\tinfo = &rdev->wiphy.vendor_events[i];\n\t\t\t\tif (nla_put(msg, i + 1, sizeof(*info), info))\n\t\t\t\t\tgoto nla_put_failure;\n\t\t\t}\n\t\t\tnla_nest_end(msg, nested);\n\t\t}\n\t\tstate->split_start++;\n\t\tbreak;\n\tcase 12:\n\t\tif (rdev->wiphy.flags & WIPHY_FLAG_HAS_CHANNEL_SWITCH &&\n\t\t    nla_put_u8(msg, NL80211_ATTR_MAX_CSA_COUNTERS,\n\t\t\t       rdev->wiphy.max_num_csa_counters))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (rdev->wiphy.regulatory_flags & REGULATORY_WIPHY_SELF_MANAGED &&\n\t\t    nla_put_flag(msg, NL80211_ATTR_WIPHY_SELF_MANAGED_REG))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (rdev->wiphy.max_sched_scan_reqs &&\n\t\t    nla_put_u32(msg, NL80211_ATTR_SCHED_SCAN_MAX_REQS,\n\t\t\t\trdev->wiphy.max_sched_scan_reqs))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (nla_put(msg, NL80211_ATTR_EXT_FEATURES,\n\t\t\t    sizeof(rdev->wiphy.ext_features),\n\t\t\t    rdev->wiphy.ext_features))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (rdev->wiphy.bss_select_support) {\n\t\t\tstruct nlattr *nested;\n\t\t\tu32 bss_select_support = rdev->wiphy.bss_select_support;\n\n\t\t\tnested = nla_nest_start_noflag(msg,\n\t\t\t\t\t\t       NL80211_ATTR_BSS_SELECT);\n\t\t\tif (!nested)\n\t\t\t\tgoto nla_put_failure;\n\n\t\t\ti = 0;\n\t\t\twhile (bss_select_support) {\n\t\t\t\tif ((bss_select_support & 1) &&\n\t\t\t\t    nla_put_flag(msg, i))\n\t\t\t\t\tgoto nla_put_failure;\n\t\t\t\ti++;\n\t\t\t\tbss_select_support >>= 1;\n\t\t\t}\n\t\t\tnla_nest_end(msg, nested);\n\t\t}\n\n\t\tstate->split_start++;\n\t\tbreak;\n\tcase 13:\n\t\tif (rdev->wiphy.num_iftype_ext_capab &&\n\t\t    rdev->wiphy.iftype_ext_capab) {\n\t\t\tstruct nlattr *nested_ext_capab, *nested;\n\n\t\t\tnested = nla_nest_start_noflag(msg,\n\t\t\t\t\t\t       NL80211_ATTR_IFTYPE_EXT_CAPA);\n\t\t\tif (!nested)\n\t\t\t\tgoto nla_put_failure;\n\n\t\t\tfor (i = state->capa_start;\n\t\t\t     i < rdev->wiphy.num_iftype_ext_capab; i++) {\n\t\t\t\tconst struct wiphy_iftype_ext_capab *capab;\n\n\t\t\t\tcapab = &rdev->wiphy.iftype_ext_capab[i];\n\n\t\t\t\tnested_ext_capab = nla_nest_start_noflag(msg,\n\t\t\t\t\t\t\t\t\t i);\n\t\t\t\tif (!nested_ext_capab ||\n\t\t\t\t    nla_put_u32(msg, NL80211_ATTR_IFTYPE,\n\t\t\t\t\t\tcapab->iftype) ||\n\t\t\t\t    nla_put(msg, NL80211_ATTR_EXT_CAPA,\n\t\t\t\t\t    capab->extended_capabilities_len,\n\t\t\t\t\t    capab->extended_capabilities) ||\n\t\t\t\t    nla_put(msg, NL80211_ATTR_EXT_CAPA_MASK,\n\t\t\t\t\t    capab->extended_capabilities_len,\n\t\t\t\t\t    capab->extended_capabilities_mask))\n\t\t\t\t\tgoto nla_put_failure;\n\n\t\t\t\tnla_nest_end(msg, nested_ext_capab);\n\t\t\t\tif (state->split)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tnla_nest_end(msg, nested);\n\t\t\tif (i < rdev->wiphy.num_iftype_ext_capab) {\n\t\t\t\tstate->capa_start = i + 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (nla_put_u32(msg, NL80211_ATTR_BANDS,\n\t\t\t\trdev->wiphy.nan_supported_bands))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t\t    NL80211_EXT_FEATURE_TXQS)) {\n\t\t\tstruct cfg80211_txq_stats txqstats = {};\n\t\t\tint res;\n\n\t\t\tres = rdev_get_txq_stats(rdev, NULL, &txqstats);\n\t\t\tif (!res &&\n\t\t\t    !nl80211_put_txq_stats(msg, &txqstats,\n\t\t\t\t\t\t   NL80211_ATTR_TXQ_STATS))\n\t\t\t\tgoto nla_put_failure;\n\n\t\t\tif (nla_put_u32(msg, NL80211_ATTR_TXQ_LIMIT,\n\t\t\t\t\trdev->wiphy.txq_limit))\n\t\t\t\tgoto nla_put_failure;\n\t\t\tif (nla_put_u32(msg, NL80211_ATTR_TXQ_MEMORY_LIMIT,\n\t\t\t\t\trdev->wiphy.txq_memory_limit))\n\t\t\t\tgoto nla_put_failure;\n\t\t\tif (nla_put_u32(msg, NL80211_ATTR_TXQ_QUANTUM,\n\t\t\t\t\trdev->wiphy.txq_quantum))\n\t\t\t\tgoto nla_put_failure;\n\t\t}\n\n\t\tstate->split_start++;\n\t\tbreak;\n\tcase 14:\n\t\tif (nl80211_send_pmsr_capa(rdev, msg))\n\t\t\tgoto nla_put_failure;\n\n\t\tstate->split_start++;\n\t\tbreak;\n\tcase 15:\n\t\tif (rdev->wiphy.akm_suites &&\n\t\t    nla_put(msg, NL80211_ATTR_AKM_SUITES,\n\t\t\t    sizeof(u32) * rdev->wiphy.n_akm_suites,\n\t\t\t    rdev->wiphy.akm_suites))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (nl80211_put_iftype_akm_suites(rdev, msg))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (nl80211_put_tid_config_support(rdev, msg))\n\t\t\tgoto nla_put_failure;\n\t\tstate->split_start++;\n\t\tbreak;\n\tcase 16:\n\t\tif (nl80211_put_sar_specs(rdev, msg))\n\t\t\tgoto nla_put_failure;\n\n\t\t/* done */\n\t\tstate->split_start = 0;\n\t\tbreak;\n\t}\n finish:\n\tgenlmsg_end(msg, hdr);\n\treturn 0;\n\n nla_put_failure:\n\tgenlmsg_cancel(msg, hdr);\n\treturn -EMSGSIZE;\n}\n\nstatic int nl80211_dump_wiphy_parse(struct sk_buff *skb,\n\t\t\t\t    struct netlink_callback *cb,\n\t\t\t\t    struct nl80211_dump_wiphy_state *state)\n{\n\tstruct nlattr **tb = kcalloc(NUM_NL80211_ATTR, sizeof(*tb), GFP_KERNEL);\n\tint ret;\n\n\tif (!tb)\n\t\treturn -ENOMEM;\n\n\tret = nlmsg_parse_deprecated(cb->nlh,\n\t\t\t\t     GENL_HDRLEN + nl80211_fam.hdrsize,\n\t\t\t\t     tb, nl80211_fam.maxattr,\n\t\t\t\t     nl80211_policy, NULL);\n\t/* ignore parse errors for backward compatibility */\n\tif (ret) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tstate->split = tb[NL80211_ATTR_SPLIT_WIPHY_DUMP];\n\tif (tb[NL80211_ATTR_WIPHY])\n\t\tstate->filter_wiphy = nla_get_u32(tb[NL80211_ATTR_WIPHY]);\n\tif (tb[NL80211_ATTR_WDEV])\n\t\tstate->filter_wiphy = nla_get_u64(tb[NL80211_ATTR_WDEV]) >> 32;\n\tif (tb[NL80211_ATTR_IFINDEX]) {\n\t\tstruct net_device *netdev;\n\t\tstruct cfg80211_registered_device *rdev;\n\t\tint ifidx = nla_get_u32(tb[NL80211_ATTR_IFINDEX]);\n\n\t\tnetdev = __dev_get_by_index(sock_net(skb->sk), ifidx);\n\t\tif (!netdev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out;\n\t\t}\n\t\tif (netdev->ieee80211_ptr) {\n\t\t\trdev = wiphy_to_rdev(\n\t\t\t\tnetdev->ieee80211_ptr->wiphy);\n\t\t\tstate->filter_wiphy = rdev->wiphy_idx;\n\t\t}\n\t}\n\n\tret = 0;\nout:\n\tkfree(tb);\n\treturn ret;\n}\n\nstatic int nl80211_dump_wiphy(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tint idx = 0, ret;\n\tstruct nl80211_dump_wiphy_state *state = (void *)cb->args[0];\n\tstruct cfg80211_registered_device *rdev;\n\n\trtnl_lock();\n\tif (!state) {\n\t\tstate = kzalloc(sizeof(*state), GFP_KERNEL);\n\t\tif (!state) {\n\t\t\trtnl_unlock();\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tstate->filter_wiphy = -1;\n\t\tret = nl80211_dump_wiphy_parse(skb, cb, state);\n\t\tif (ret) {\n\t\t\tkfree(state);\n\t\t\trtnl_unlock();\n\t\t\treturn ret;\n\t\t}\n\t\tcb->args[0] = (long)state;\n\t}\n\n\tlist_for_each_entry(rdev, &cfg80211_rdev_list, list) {\n\t\tif (!net_eq(wiphy_net(&rdev->wiphy), sock_net(skb->sk)))\n\t\t\tcontinue;\n\t\tif (++idx <= state->start)\n\t\t\tcontinue;\n\t\tif (state->filter_wiphy != -1 &&\n\t\t    state->filter_wiphy != rdev->wiphy_idx)\n\t\t\tcontinue;\n\t\t/* attempt to fit multiple wiphy data chunks into the skb */\n\t\tdo {\n\t\t\tret = nl80211_send_wiphy(rdev, NL80211_CMD_NEW_WIPHY,\n\t\t\t\t\t\t skb,\n\t\t\t\t\t\t NETLINK_CB(cb->skb).portid,\n\t\t\t\t\t\t cb->nlh->nlmsg_seq,\n\t\t\t\t\t\t NLM_F_MULTI, state);\n\t\t\tif (ret < 0) {\n\t\t\t\t/*\n\t\t\t\t * If sending the wiphy data didn't fit (ENOBUFS\n\t\t\t\t * or EMSGSIZE returned), this SKB is still\n\t\t\t\t * empty (so it's not too big because another\n\t\t\t\t * wiphy dataset is already in the skb) and\n\t\t\t\t * we've not tried to adjust the dump allocation\n\t\t\t\t * yet ... then adjust the alloc size to be\n\t\t\t\t * bigger, and return 1 but with the empty skb.\n\t\t\t\t * This results in an empty message being RX'ed\n\t\t\t\t * in userspace, but that is ignored.\n\t\t\t\t *\n\t\t\t\t * We can then retry with the larger buffer.\n\t\t\t\t */\n\t\t\t\tif ((ret == -ENOBUFS || ret == -EMSGSIZE) &&\n\t\t\t\t    !skb->len && !state->split &&\n\t\t\t\t    cb->min_dump_alloc < 4096) {\n\t\t\t\t\tcb->min_dump_alloc = 4096;\n\t\t\t\t\tstate->split_start = 0;\n\t\t\t\t\trtnl_unlock();\n\t\t\t\t\treturn 1;\n\t\t\t\t}\n\t\t\t\tidx--;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} while (state->split_start > 0);\n\t\tbreak;\n\t}\n\trtnl_unlock();\n\n\tstate->start = idx;\n\n\treturn skb->len;\n}\n\nstatic int nl80211_dump_wiphy_done(struct netlink_callback *cb)\n{\n\tkfree((void *)cb->args[0]);\n\treturn 0;\n}\n\nstatic int nl80211_get_wiphy(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct sk_buff *msg;\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct nl80211_dump_wiphy_state state = {};\n\n\tmsg = nlmsg_new(4096, GFP_KERNEL);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\tif (nl80211_send_wiphy(rdev, NL80211_CMD_NEW_WIPHY, msg,\n\t\t\t       info->snd_portid, info->snd_seq, 0,\n\t\t\t       &state) < 0) {\n\t\tnlmsg_free(msg);\n\t\treturn -ENOBUFS;\n\t}\n\n\treturn genlmsg_reply(msg, info);\n}\n\nstatic const struct nla_policy txq_params_policy[NL80211_TXQ_ATTR_MAX + 1] = {\n\t[NL80211_TXQ_ATTR_QUEUE]\t\t= { .type = NLA_U8 },\n\t[NL80211_TXQ_ATTR_TXOP]\t\t\t= { .type = NLA_U16 },\n\t[NL80211_TXQ_ATTR_CWMIN]\t\t= { .type = NLA_U16 },\n\t[NL80211_TXQ_ATTR_CWMAX]\t\t= { .type = NLA_U16 },\n\t[NL80211_TXQ_ATTR_AIFS]\t\t\t= { .type = NLA_U8 },\n};\n\nstatic int parse_txq_params(struct nlattr *tb[],\n\t\t\t    struct ieee80211_txq_params *txq_params)\n{\n\tu8 ac;\n\n\tif (!tb[NL80211_TXQ_ATTR_AC] || !tb[NL80211_TXQ_ATTR_TXOP] ||\n\t    !tb[NL80211_TXQ_ATTR_CWMIN] || !tb[NL80211_TXQ_ATTR_CWMAX] ||\n\t    !tb[NL80211_TXQ_ATTR_AIFS])\n\t\treturn -EINVAL;\n\n\tac = nla_get_u8(tb[NL80211_TXQ_ATTR_AC]);\n\ttxq_params->txop = nla_get_u16(tb[NL80211_TXQ_ATTR_TXOP]);\n\ttxq_params->cwmin = nla_get_u16(tb[NL80211_TXQ_ATTR_CWMIN]);\n\ttxq_params->cwmax = nla_get_u16(tb[NL80211_TXQ_ATTR_CWMAX]);\n\ttxq_params->aifs = nla_get_u8(tb[NL80211_TXQ_ATTR_AIFS]);\n\n\tif (ac >= NL80211_NUM_ACS)\n\t\treturn -EINVAL;\n\ttxq_params->ac = array_index_nospec(ac, NL80211_NUM_ACS);\n\treturn 0;\n}\n\nstatic bool nl80211_can_set_dev_channel(struct wireless_dev *wdev)\n{\n\t/*\n\t * You can only set the channel explicitly for some interfaces,\n\t * most have their channel managed via their respective\n\t * \"establish a connection\" command (connect, join, ...)\n\t *\n\t * For AP/GO and mesh mode, the channel can be set with the\n\t * channel userspace API, but is only stored and passed to the\n\t * low-level driver when the AP starts or the mesh is joined.\n\t * This is for backward compatibility, userspace can also give\n\t * the channel in the start-ap or join-mesh commands instead.\n\t *\n\t * Monitors are special as they are normally slaved to\n\t * whatever else is going on, so they have their own special\n\t * operation to set the monitor channel if possible.\n\t */\n\treturn !wdev ||\n\t\twdev->iftype == NL80211_IFTYPE_AP ||\n\t\twdev->iftype == NL80211_IFTYPE_MESH_POINT ||\n\t\twdev->iftype == NL80211_IFTYPE_MONITOR ||\n\t\twdev->iftype == NL80211_IFTYPE_P2P_GO;\n}\n\nint nl80211_parse_chandef(struct cfg80211_registered_device *rdev,\n\t\t\t  struct genl_info *info,\n\t\t\t  struct cfg80211_chan_def *chandef)\n{\n\tstruct netlink_ext_ack *extack = info->extack;\n\tstruct nlattr **attrs = info->attrs;\n\tu32 control_freq;\n\n\tif (!attrs[NL80211_ATTR_WIPHY_FREQ])\n\t\treturn -EINVAL;\n\n\tcontrol_freq = MHZ_TO_KHZ(\n\t\t\tnla_get_u32(info->attrs[NL80211_ATTR_WIPHY_FREQ]));\n\tif (info->attrs[NL80211_ATTR_WIPHY_FREQ_OFFSET])\n\t\tcontrol_freq +=\n\t\t    nla_get_u32(info->attrs[NL80211_ATTR_WIPHY_FREQ_OFFSET]);\n\n\tmemset(chandef, 0, sizeof(*chandef));\n\tchandef->chan = ieee80211_get_channel_khz(&rdev->wiphy, control_freq);\n\tchandef->width = NL80211_CHAN_WIDTH_20_NOHT;\n\tchandef->center_freq1 = KHZ_TO_MHZ(control_freq);\n\tchandef->freq1_offset = control_freq % 1000;\n\tchandef->center_freq2 = 0;\n\n\t/* Primary channel not allowed */\n\tif (!chandef->chan || chandef->chan->flags & IEEE80211_CHAN_DISABLED) {\n\t\tNL_SET_ERR_MSG_ATTR(extack, attrs[NL80211_ATTR_WIPHY_FREQ],\n\t\t\t\t    \"Channel is disabled\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (attrs[NL80211_ATTR_WIPHY_CHANNEL_TYPE]) {\n\t\tenum nl80211_channel_type chantype;\n\n\t\tchantype = nla_get_u32(attrs[NL80211_ATTR_WIPHY_CHANNEL_TYPE]);\n\n\t\tswitch (chantype) {\n\t\tcase NL80211_CHAN_NO_HT:\n\t\tcase NL80211_CHAN_HT20:\n\t\tcase NL80211_CHAN_HT40PLUS:\n\t\tcase NL80211_CHAN_HT40MINUS:\n\t\t\tcfg80211_chandef_create(chandef, chandef->chan,\n\t\t\t\t\t\tchantype);\n\t\t\t/* user input for center_freq is incorrect */\n\t\t\tif (attrs[NL80211_ATTR_CENTER_FREQ1] &&\n\t\t\t    chandef->center_freq1 != nla_get_u32(attrs[NL80211_ATTR_CENTER_FREQ1])) {\n\t\t\t\tNL_SET_ERR_MSG_ATTR(extack,\n\t\t\t\t\t\t    attrs[NL80211_ATTR_CENTER_FREQ1],\n\t\t\t\t\t\t    \"bad center frequency 1\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* center_freq2 must be zero */\n\t\t\tif (attrs[NL80211_ATTR_CENTER_FREQ2] &&\n\t\t\t    nla_get_u32(attrs[NL80211_ATTR_CENTER_FREQ2])) {\n\t\t\t\tNL_SET_ERR_MSG_ATTR(extack,\n\t\t\t\t\t\t    attrs[NL80211_ATTR_CENTER_FREQ2],\n\t\t\t\t\t\t    \"center frequency 2 can't be used\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tNL_SET_ERR_MSG_ATTR(extack,\n\t\t\t\t\t    attrs[NL80211_ATTR_WIPHY_CHANNEL_TYPE],\n\t\t\t\t\t    \"invalid channel type\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (attrs[NL80211_ATTR_CHANNEL_WIDTH]) {\n\t\tchandef->width =\n\t\t\tnla_get_u32(attrs[NL80211_ATTR_CHANNEL_WIDTH]);\n\t\tif (attrs[NL80211_ATTR_CENTER_FREQ1]) {\n\t\t\tchandef->center_freq1 =\n\t\t\t\tnla_get_u32(attrs[NL80211_ATTR_CENTER_FREQ1]);\n\t\t\tif (attrs[NL80211_ATTR_CENTER_FREQ1_OFFSET])\n\t\t\t\tchandef->freq1_offset = nla_get_u32(\n\t\t\t\t      attrs[NL80211_ATTR_CENTER_FREQ1_OFFSET]);\n\t\t\telse\n\t\t\t\tchandef->freq1_offset = 0;\n\t\t}\n\t\tif (attrs[NL80211_ATTR_CENTER_FREQ2])\n\t\t\tchandef->center_freq2 =\n\t\t\t\tnla_get_u32(attrs[NL80211_ATTR_CENTER_FREQ2]);\n\t}\n\n\tif (info->attrs[NL80211_ATTR_WIPHY_EDMG_CHANNELS]) {\n\t\tchandef->edmg.channels =\n\t\t      nla_get_u8(info->attrs[NL80211_ATTR_WIPHY_EDMG_CHANNELS]);\n\n\t\tif (info->attrs[NL80211_ATTR_WIPHY_EDMG_BW_CONFIG])\n\t\t\tchandef->edmg.bw_config =\n\t\t     nla_get_u8(info->attrs[NL80211_ATTR_WIPHY_EDMG_BW_CONFIG]);\n\t} else {\n\t\tchandef->edmg.bw_config = 0;\n\t\tchandef->edmg.channels = 0;\n\t}\n\n\tif (!cfg80211_chandef_valid(chandef)) {\n\t\tNL_SET_ERR_MSG(extack, \"invalid channel definition\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!cfg80211_chandef_usable(&rdev->wiphy, chandef,\n\t\t\t\t     IEEE80211_CHAN_DISABLED)) {\n\t\tNL_SET_ERR_MSG(extack, \"(extension) channel is disabled\");\n\t\treturn -EINVAL;\n\t}\n\n\tif ((chandef->width == NL80211_CHAN_WIDTH_5 ||\n\t     chandef->width == NL80211_CHAN_WIDTH_10) &&\n\t    !(rdev->wiphy.flags & WIPHY_FLAG_SUPPORTS_5_10_MHZ)) {\n\t\tNL_SET_ERR_MSG(extack, \"5/10 MHz not supported\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int __nl80211_set_channel(struct cfg80211_registered_device *rdev,\n\t\t\t\t struct net_device *dev,\n\t\t\t\t struct genl_info *info)\n{\n\tstruct cfg80211_chan_def chandef;\n\tint result;\n\tenum nl80211_iftype iftype = NL80211_IFTYPE_MONITOR;\n\tstruct wireless_dev *wdev = NULL;\n\n\tif (dev)\n\t\twdev = dev->ieee80211_ptr;\n\tif (!nl80211_can_set_dev_channel(wdev))\n\t\treturn -EOPNOTSUPP;\n\tif (wdev)\n\t\tiftype = wdev->iftype;\n\n\tresult = nl80211_parse_chandef(rdev, info, &chandef);\n\tif (result)\n\t\treturn result;\n\n\tswitch (iftype) {\n\tcase NL80211_IFTYPE_AP:\n\tcase NL80211_IFTYPE_P2P_GO:\n\t\tif (!cfg80211_reg_can_beacon_relax(&rdev->wiphy, &chandef,\n\t\t\t\t\t\t   iftype)) {\n\t\t\tresult = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tif (wdev->beacon_interval) {\n\t\t\tif (!dev || !rdev->ops->set_ap_chanwidth ||\n\t\t\t    !(rdev->wiphy.features &\n\t\t\t      NL80211_FEATURE_AP_MODE_CHAN_WIDTH_CHANGE)) {\n\t\t\t\tresult = -EBUSY;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Only allow dynamic channel width changes */\n\t\t\tif (chandef.chan != wdev->preset_chandef.chan) {\n\t\t\t\tresult = -EBUSY;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tresult = rdev_set_ap_chanwidth(rdev, dev, &chandef);\n\t\t\tif (result)\n\t\t\t\tbreak;\n\t\t}\n\t\twdev->preset_chandef = chandef;\n\t\tresult = 0;\n\t\tbreak;\n\tcase NL80211_IFTYPE_MESH_POINT:\n\t\tresult = cfg80211_set_mesh_channel(rdev, wdev, &chandef);\n\t\tbreak;\n\tcase NL80211_IFTYPE_MONITOR:\n\t\tresult = cfg80211_set_monitor_channel(rdev, &chandef);\n\t\tbreak;\n\tdefault:\n\t\tresult = -EINVAL;\n\t}\n\n\treturn result;\n}\n\nstatic int nl80211_set_channel(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *netdev = info->user_ptr[1];\n\n\treturn __nl80211_set_channel(rdev, netdev, info);\n}\n\nstatic int nl80211_set_wiphy(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = NULL;\n\tstruct net_device *netdev = NULL;\n\tstruct wireless_dev *wdev;\n\tint result = 0, rem_txq_params = 0;\n\tstruct nlattr *nl_txq_params;\n\tu32 changed;\n\tu8 retry_short = 0, retry_long = 0;\n\tu32 frag_threshold = 0, rts_threshold = 0;\n\tu8 coverage_class = 0;\n\tu32 txq_limit = 0, txq_memory_limit = 0, txq_quantum = 0;\n\n\trtnl_lock();\n\t/*\n\t * Try to find the wiphy and netdev. Normally this\n\t * function shouldn't need the netdev, but this is\n\t * done for backward compatibility -- previously\n\t * setting the channel was done per wiphy, but now\n\t * it is per netdev. Previous userland like hostapd\n\t * also passed a netdev to set_wiphy, so that it is\n\t * possible to let that go to the right netdev!\n\t */\n\n\tif (info->attrs[NL80211_ATTR_IFINDEX]) {\n\t\tint ifindex = nla_get_u32(info->attrs[NL80211_ATTR_IFINDEX]);\n\n\t\tnetdev = __dev_get_by_index(genl_info_net(info), ifindex);\n\t\tif (netdev && netdev->ieee80211_ptr)\n\t\t\trdev = wiphy_to_rdev(netdev->ieee80211_ptr->wiphy);\n\t\telse\n\t\t\tnetdev = NULL;\n\t}\n\n\tif (!netdev) {\n\t\trdev = __cfg80211_rdev_from_attrs(genl_info_net(info),\n\t\t\t\t\t\t  info->attrs);\n\t\tif (IS_ERR(rdev)) {\n\t\t\trtnl_unlock();\n\t\t\treturn PTR_ERR(rdev);\n\t\t}\n\t\twdev = NULL;\n\t\tnetdev = NULL;\n\t\tresult = 0;\n\t} else\n\t\twdev = netdev->ieee80211_ptr;\n\n\twiphy_lock(&rdev->wiphy);\n\n\t/*\n\t * end workaround code, by now the rdev is available\n\t * and locked, and wdev may or may not be NULL.\n\t */\n\n\tif (info->attrs[NL80211_ATTR_WIPHY_NAME])\n\t\tresult = cfg80211_dev_rename(\n\t\t\trdev, nla_data(info->attrs[NL80211_ATTR_WIPHY_NAME]));\n\trtnl_unlock();\n\n\tif (result)\n\t\tgoto out;\n\n\tif (info->attrs[NL80211_ATTR_WIPHY_TXQ_PARAMS]) {\n\t\tstruct ieee80211_txq_params txq_params;\n\t\tstruct nlattr *tb[NL80211_TXQ_ATTR_MAX + 1];\n\n\t\tif (!rdev->ops->set_txq_params) {\n\t\t\tresult = -EOPNOTSUPP;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (!netdev) {\n\t\t\tresult = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (netdev->ieee80211_ptr->iftype != NL80211_IFTYPE_AP &&\n\t\t    netdev->ieee80211_ptr->iftype != NL80211_IFTYPE_P2P_GO) {\n\t\t\tresult = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (!netif_running(netdev)) {\n\t\t\tresult = -ENETDOWN;\n\t\t\tgoto out;\n\t\t}\n\n\t\tnla_for_each_nested(nl_txq_params,\n\t\t\t\t    info->attrs[NL80211_ATTR_WIPHY_TXQ_PARAMS],\n\t\t\t\t    rem_txq_params) {\n\t\t\tresult = nla_parse_nested_deprecated(tb,\n\t\t\t\t\t\t\t     NL80211_TXQ_ATTR_MAX,\n\t\t\t\t\t\t\t     nl_txq_params,\n\t\t\t\t\t\t\t     txq_params_policy,\n\t\t\t\t\t\t\t     info->extack);\n\t\t\tif (result)\n\t\t\t\tgoto out;\n\t\t\tresult = parse_txq_params(tb, &txq_params);\n\t\t\tif (result)\n\t\t\t\tgoto out;\n\n\t\t\tresult = rdev_set_txq_params(rdev, netdev,\n\t\t\t\t\t\t     &txq_params);\n\t\t\tif (result)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (info->attrs[NL80211_ATTR_WIPHY_FREQ]) {\n\t\tresult = __nl80211_set_channel(\n\t\t\trdev,\n\t\t\tnl80211_can_set_dev_channel(wdev) ? netdev : NULL,\n\t\t\tinfo);\n\t\tif (result)\n\t\t\tgoto out;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_WIPHY_TX_POWER_SETTING]) {\n\t\tstruct wireless_dev *txp_wdev = wdev;\n\t\tenum nl80211_tx_power_setting type;\n\t\tint idx, mbm = 0;\n\n\t\tif (!(rdev->wiphy.features & NL80211_FEATURE_VIF_TXPOWER))\n\t\t\ttxp_wdev = NULL;\n\n\t\tif (!rdev->ops->set_tx_power) {\n\t\t\tresult = -EOPNOTSUPP;\n\t\t\tgoto out;\n\t\t}\n\n\t\tidx = NL80211_ATTR_WIPHY_TX_POWER_SETTING;\n\t\ttype = nla_get_u32(info->attrs[idx]);\n\n\t\tif (!info->attrs[NL80211_ATTR_WIPHY_TX_POWER_LEVEL] &&\n\t\t    (type != NL80211_TX_POWER_AUTOMATIC)) {\n\t\t\tresult = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (type != NL80211_TX_POWER_AUTOMATIC) {\n\t\t\tidx = NL80211_ATTR_WIPHY_TX_POWER_LEVEL;\n\t\t\tmbm = nla_get_u32(info->attrs[idx]);\n\t\t}\n\n\t\tresult = rdev_set_tx_power(rdev, txp_wdev, type, mbm);\n\t\tif (result)\n\t\t\tgoto out;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_WIPHY_ANTENNA_TX] &&\n\t    info->attrs[NL80211_ATTR_WIPHY_ANTENNA_RX]) {\n\t\tu32 tx_ant, rx_ant;\n\n\t\tif ((!rdev->wiphy.available_antennas_tx &&\n\t\t     !rdev->wiphy.available_antennas_rx) ||\n\t\t    !rdev->ops->set_antenna) {\n\t\t\tresult = -EOPNOTSUPP;\n\t\t\tgoto out;\n\t\t}\n\n\t\ttx_ant = nla_get_u32(info->attrs[NL80211_ATTR_WIPHY_ANTENNA_TX]);\n\t\trx_ant = nla_get_u32(info->attrs[NL80211_ATTR_WIPHY_ANTENNA_RX]);\n\n\t\t/* reject antenna configurations which don't match the\n\t\t * available antenna masks, except for the \"all\" mask */\n\t\tif ((~tx_ant && (tx_ant & ~rdev->wiphy.available_antennas_tx)) ||\n\t\t    (~rx_ant && (rx_ant & ~rdev->wiphy.available_antennas_rx))) {\n\t\t\tresult = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\ttx_ant = tx_ant & rdev->wiphy.available_antennas_tx;\n\t\trx_ant = rx_ant & rdev->wiphy.available_antennas_rx;\n\n\t\tresult = rdev_set_antenna(rdev, tx_ant, rx_ant);\n\t\tif (result)\n\t\t\tgoto out;\n\t}\n\n\tchanged = 0;\n\n\tif (info->attrs[NL80211_ATTR_WIPHY_RETRY_SHORT]) {\n\t\tretry_short = nla_get_u8(\n\t\t\tinfo->attrs[NL80211_ATTR_WIPHY_RETRY_SHORT]);\n\n\t\tchanged |= WIPHY_PARAM_RETRY_SHORT;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_WIPHY_RETRY_LONG]) {\n\t\tretry_long = nla_get_u8(\n\t\t\tinfo->attrs[NL80211_ATTR_WIPHY_RETRY_LONG]);\n\n\t\tchanged |= WIPHY_PARAM_RETRY_LONG;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_WIPHY_FRAG_THRESHOLD]) {\n\t\tfrag_threshold = nla_get_u32(\n\t\t\tinfo->attrs[NL80211_ATTR_WIPHY_FRAG_THRESHOLD]);\n\t\tif (frag_threshold < 256) {\n\t\t\tresult = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (frag_threshold != (u32) -1) {\n\t\t\t/*\n\t\t\t * Fragments (apart from the last one) are required to\n\t\t\t * have even length. Make the fragmentation code\n\t\t\t * simpler by stripping LSB should someone try to use\n\t\t\t * odd threshold value.\n\t\t\t */\n\t\t\tfrag_threshold &= ~0x1;\n\t\t}\n\t\tchanged |= WIPHY_PARAM_FRAG_THRESHOLD;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_WIPHY_RTS_THRESHOLD]) {\n\t\trts_threshold = nla_get_u32(\n\t\t\tinfo->attrs[NL80211_ATTR_WIPHY_RTS_THRESHOLD]);\n\t\tchanged |= WIPHY_PARAM_RTS_THRESHOLD;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_WIPHY_COVERAGE_CLASS]) {\n\t\tif (info->attrs[NL80211_ATTR_WIPHY_DYN_ACK]) {\n\t\t\tresult = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tcoverage_class = nla_get_u8(\n\t\t\tinfo->attrs[NL80211_ATTR_WIPHY_COVERAGE_CLASS]);\n\t\tchanged |= WIPHY_PARAM_COVERAGE_CLASS;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_WIPHY_DYN_ACK]) {\n\t\tif (!(rdev->wiphy.features & NL80211_FEATURE_ACKTO_ESTIMATION)) {\n\t\t\tresult = -EOPNOTSUPP;\n\t\t\tgoto out;\n\t\t}\n\n\t\tchanged |= WIPHY_PARAM_DYN_ACK;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_TXQ_LIMIT]) {\n\t\tif (!wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t\t     NL80211_EXT_FEATURE_TXQS)) {\n\t\t\tresult = -EOPNOTSUPP;\n\t\t\tgoto out;\n\t\t}\n\t\ttxq_limit = nla_get_u32(\n\t\t\tinfo->attrs[NL80211_ATTR_TXQ_LIMIT]);\n\t\tchanged |= WIPHY_PARAM_TXQ_LIMIT;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_TXQ_MEMORY_LIMIT]) {\n\t\tif (!wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t\t     NL80211_EXT_FEATURE_TXQS)) {\n\t\t\tresult = -EOPNOTSUPP;\n\t\t\tgoto out;\n\t\t}\n\t\ttxq_memory_limit = nla_get_u32(\n\t\t\tinfo->attrs[NL80211_ATTR_TXQ_MEMORY_LIMIT]);\n\t\tchanged |= WIPHY_PARAM_TXQ_MEMORY_LIMIT;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_TXQ_QUANTUM]) {\n\t\tif (!wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t\t     NL80211_EXT_FEATURE_TXQS)) {\n\t\t\tresult = -EOPNOTSUPP;\n\t\t\tgoto out;\n\t\t}\n\t\ttxq_quantum = nla_get_u32(\n\t\t\tinfo->attrs[NL80211_ATTR_TXQ_QUANTUM]);\n\t\tchanged |= WIPHY_PARAM_TXQ_QUANTUM;\n\t}\n\n\tif (changed) {\n\t\tu8 old_retry_short, old_retry_long;\n\t\tu32 old_frag_threshold, old_rts_threshold;\n\t\tu8 old_coverage_class;\n\t\tu32 old_txq_limit, old_txq_memory_limit, old_txq_quantum;\n\n\t\tif (!rdev->ops->set_wiphy_params) {\n\t\t\tresult = -EOPNOTSUPP;\n\t\t\tgoto out;\n\t\t}\n\n\t\told_retry_short = rdev->wiphy.retry_short;\n\t\told_retry_long = rdev->wiphy.retry_long;\n\t\told_frag_threshold = rdev->wiphy.frag_threshold;\n\t\told_rts_threshold = rdev->wiphy.rts_threshold;\n\t\told_coverage_class = rdev->wiphy.coverage_class;\n\t\told_txq_limit = rdev->wiphy.txq_limit;\n\t\told_txq_memory_limit = rdev->wiphy.txq_memory_limit;\n\t\told_txq_quantum = rdev->wiphy.txq_quantum;\n\n\t\tif (changed & WIPHY_PARAM_RETRY_SHORT)\n\t\t\trdev->wiphy.retry_short = retry_short;\n\t\tif (changed & WIPHY_PARAM_RETRY_LONG)\n\t\t\trdev->wiphy.retry_long = retry_long;\n\t\tif (changed & WIPHY_PARAM_FRAG_THRESHOLD)\n\t\t\trdev->wiphy.frag_threshold = frag_threshold;\n\t\tif (changed & WIPHY_PARAM_RTS_THRESHOLD)\n\t\t\trdev->wiphy.rts_threshold = rts_threshold;\n\t\tif (changed & WIPHY_PARAM_COVERAGE_CLASS)\n\t\t\trdev->wiphy.coverage_class = coverage_class;\n\t\tif (changed & WIPHY_PARAM_TXQ_LIMIT)\n\t\t\trdev->wiphy.txq_limit = txq_limit;\n\t\tif (changed & WIPHY_PARAM_TXQ_MEMORY_LIMIT)\n\t\t\trdev->wiphy.txq_memory_limit = txq_memory_limit;\n\t\tif (changed & WIPHY_PARAM_TXQ_QUANTUM)\n\t\t\trdev->wiphy.txq_quantum = txq_quantum;\n\n\t\tresult = rdev_set_wiphy_params(rdev, changed);\n\t\tif (result) {\n\t\t\trdev->wiphy.retry_short = old_retry_short;\n\t\t\trdev->wiphy.retry_long = old_retry_long;\n\t\t\trdev->wiphy.frag_threshold = old_frag_threshold;\n\t\t\trdev->wiphy.rts_threshold = old_rts_threshold;\n\t\t\trdev->wiphy.coverage_class = old_coverage_class;\n\t\t\trdev->wiphy.txq_limit = old_txq_limit;\n\t\t\trdev->wiphy.txq_memory_limit = old_txq_memory_limit;\n\t\t\trdev->wiphy.txq_quantum = old_txq_quantum;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tresult = 0;\n\nout:\n\twiphy_unlock(&rdev->wiphy);\n\treturn result;\n}\n\nstatic int nl80211_send_chandef(struct sk_buff *msg,\n\t\t\t\tconst struct cfg80211_chan_def *chandef)\n{\n\tif (WARN_ON(!cfg80211_chandef_valid(chandef)))\n\t\treturn -EINVAL;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY_FREQ,\n\t\t\tchandef->chan->center_freq))\n\t\treturn -ENOBUFS;\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY_FREQ_OFFSET,\n\t\t\tchandef->chan->freq_offset))\n\t\treturn -ENOBUFS;\n\tswitch (chandef->width) {\n\tcase NL80211_CHAN_WIDTH_20_NOHT:\n\tcase NL80211_CHAN_WIDTH_20:\n\tcase NL80211_CHAN_WIDTH_40:\n\t\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY_CHANNEL_TYPE,\n\t\t\t\tcfg80211_get_chandef_type(chandef)))\n\t\t\treturn -ENOBUFS;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tif (nla_put_u32(msg, NL80211_ATTR_CHANNEL_WIDTH, chandef->width))\n\t\treturn -ENOBUFS;\n\tif (nla_put_u32(msg, NL80211_ATTR_CENTER_FREQ1, chandef->center_freq1))\n\t\treturn -ENOBUFS;\n\tif (chandef->center_freq2 &&\n\t    nla_put_u32(msg, NL80211_ATTR_CENTER_FREQ2, chandef->center_freq2))\n\t\treturn -ENOBUFS;\n\treturn 0;\n}\n\nstatic int nl80211_send_iface(struct sk_buff *msg, u32 portid, u32 seq, int flags,\n\t\t\t      struct cfg80211_registered_device *rdev,\n\t\t\t      struct wireless_dev *wdev,\n\t\t\t      enum nl80211_commands cmd)\n{\n\tstruct net_device *dev = wdev->netdev;\n\tvoid *hdr;\n\n\tWARN_ON(cmd != NL80211_CMD_NEW_INTERFACE &&\n\t\tcmd != NL80211_CMD_DEL_INTERFACE &&\n\t\tcmd != NL80211_CMD_SET_INTERFACE);\n\n\thdr = nl80211hdr_put(msg, portid, seq, flags, cmd);\n\tif (!hdr)\n\t\treturn -1;\n\n\tif (dev &&\n\t    (nla_put_u32(msg, NL80211_ATTR_IFINDEX, dev->ifindex) ||\n\t     nla_put_string(msg, NL80211_ATTR_IFNAME, dev->name)))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFTYPE, wdev->iftype) ||\n\t    nla_put_u64_64bit(msg, NL80211_ATTR_WDEV, wdev_id(wdev),\n\t\t\t      NL80211_ATTR_PAD) ||\n\t    nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, wdev_address(wdev)) ||\n\t    nla_put_u32(msg, NL80211_ATTR_GENERATION,\n\t\t\trdev->devlist_generation ^\n\t\t\t(cfg80211_rdev_list_generation << 2)) ||\n\t    nla_put_u8(msg, NL80211_ATTR_4ADDR, wdev->use_4addr))\n\t\tgoto nla_put_failure;\n\n\tif (rdev->ops->get_channel) {\n\t\tint ret;\n\t\tstruct cfg80211_chan_def chandef = {};\n\n\t\tret = rdev_get_channel(rdev, wdev, &chandef);\n\t\tif (ret == 0) {\n\t\t\tif (nl80211_send_chandef(msg, &chandef))\n\t\t\t\tgoto nla_put_failure;\n\t\t}\n\t}\n\n\tif (rdev->ops->get_tx_power) {\n\t\tint dbm, ret;\n\n\t\tret = rdev_get_tx_power(rdev, wdev, &dbm);\n\t\tif (ret == 0 &&\n\t\t    nla_put_u32(msg, NL80211_ATTR_WIPHY_TX_POWER_LEVEL,\n\t\t\t\tDBM_TO_MBM(dbm)))\n\t\t\tgoto nla_put_failure;\n\t}\n\n\twdev_lock(wdev);\n\tswitch (wdev->iftype) {\n\tcase NL80211_IFTYPE_AP:\n\t\tif (wdev->ssid_len &&\n\t\t    nla_put(msg, NL80211_ATTR_SSID, wdev->ssid_len, wdev->ssid))\n\t\t\tgoto nla_put_failure_locked;\n\t\tbreak;\n\tcase NL80211_IFTYPE_STATION:\n\tcase NL80211_IFTYPE_P2P_CLIENT:\n\tcase NL80211_IFTYPE_ADHOC: {\n\t\tconst u8 *ssid_ie;\n\t\tif (!wdev->current_bss)\n\t\t\tbreak;\n\t\trcu_read_lock();\n\t\tssid_ie = ieee80211_bss_get_ie(&wdev->current_bss->pub,\n\t\t\t\t\t       WLAN_EID_SSID);\n\t\tif (ssid_ie &&\n\t\t    nla_put(msg, NL80211_ATTR_SSID, ssid_ie[1], ssid_ie + 2))\n\t\t\tgoto nla_put_failure_rcu_locked;\n\t\trcu_read_unlock();\n\t\tbreak;\n\t\t}\n\tdefault:\n\t\t/* nothing */\n\t\tbreak;\n\t}\n\twdev_unlock(wdev);\n\n\tif (rdev->ops->get_txq_stats) {\n\t\tstruct cfg80211_txq_stats txqstats = {};\n\t\tint ret = rdev_get_txq_stats(rdev, wdev, &txqstats);\n\n\t\tif (ret == 0 &&\n\t\t    !nl80211_put_txq_stats(msg, &txqstats,\n\t\t\t\t\t   NL80211_ATTR_TXQ_STATS))\n\t\t\tgoto nla_put_failure;\n\t}\n\n\tgenlmsg_end(msg, hdr);\n\treturn 0;\n\n nla_put_failure_rcu_locked:\n\trcu_read_unlock();\n nla_put_failure_locked:\n\twdev_unlock(wdev);\n nla_put_failure:\n\tgenlmsg_cancel(msg, hdr);\n\treturn -EMSGSIZE;\n}\n\nstatic int nl80211_dump_interface(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tint wp_idx = 0;\n\tint if_idx = 0;\n\tint wp_start = cb->args[0];\n\tint if_start = cb->args[1];\n\tint filter_wiphy = -1;\n\tstruct cfg80211_registered_device *rdev;\n\tstruct wireless_dev *wdev;\n\tint ret;\n\n\trtnl_lock();\n\tif (!cb->args[2]) {\n\t\tstruct nl80211_dump_wiphy_state state = {\n\t\t\t.filter_wiphy = -1,\n\t\t};\n\n\t\tret = nl80211_dump_wiphy_parse(skb, cb, &state);\n\t\tif (ret)\n\t\t\tgoto out_unlock;\n\n\t\tfilter_wiphy = state.filter_wiphy;\n\n\t\t/*\n\t\t * if filtering, set cb->args[2] to +1 since 0 is the default\n\t\t * value needed to determine that parsing is necessary.\n\t\t */\n\t\tif (filter_wiphy >= 0)\n\t\t\tcb->args[2] = filter_wiphy + 1;\n\t\telse\n\t\t\tcb->args[2] = -1;\n\t} else if (cb->args[2] > 0) {\n\t\tfilter_wiphy = cb->args[2] - 1;\n\t}\n\n\tlist_for_each_entry(rdev, &cfg80211_rdev_list, list) {\n\t\tif (!net_eq(wiphy_net(&rdev->wiphy), sock_net(skb->sk)))\n\t\t\tcontinue;\n\t\tif (wp_idx < wp_start) {\n\t\t\twp_idx++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (filter_wiphy >= 0 && filter_wiphy != rdev->wiphy_idx)\n\t\t\tcontinue;\n\n\t\tif_idx = 0;\n\n\t\tlist_for_each_entry(wdev, &rdev->wiphy.wdev_list, list) {\n\t\t\tif (if_idx < if_start) {\n\t\t\t\tif_idx++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (nl80211_send_iface(skb, NETLINK_CB(cb->skb).portid,\n\t\t\t\t\t       cb->nlh->nlmsg_seq, NLM_F_MULTI,\n\t\t\t\t\t       rdev, wdev,\n\t\t\t\t\t       NL80211_CMD_NEW_INTERFACE) < 0) {\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif_idx++;\n\t\t}\n\n\t\twp_idx++;\n\t}\n out:\n\tcb->args[0] = wp_idx;\n\tcb->args[1] = if_idx;\n\n\tret = skb->len;\n out_unlock:\n\trtnl_unlock();\n\n\treturn ret;\n}\n\nstatic int nl80211_get_interface(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct sk_buff *msg;\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev = info->user_ptr[1];\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\tif (nl80211_send_iface(msg, info->snd_portid, info->snd_seq, 0,\n\t\t\t       rdev, wdev, NL80211_CMD_NEW_INTERFACE) < 0) {\n\t\tnlmsg_free(msg);\n\t\treturn -ENOBUFS;\n\t}\n\n\treturn genlmsg_reply(msg, info);\n}\n\nstatic const struct nla_policy mntr_flags_policy[NL80211_MNTR_FLAG_MAX + 1] = {\n\t[NL80211_MNTR_FLAG_FCSFAIL] = { .type = NLA_FLAG },\n\t[NL80211_MNTR_FLAG_PLCPFAIL] = { .type = NLA_FLAG },\n\t[NL80211_MNTR_FLAG_CONTROL] = { .type = NLA_FLAG },\n\t[NL80211_MNTR_FLAG_OTHER_BSS] = { .type = NLA_FLAG },\n\t[NL80211_MNTR_FLAG_COOK_FRAMES] = { .type = NLA_FLAG },\n\t[NL80211_MNTR_FLAG_ACTIVE] = { .type = NLA_FLAG },\n};\n\nstatic int parse_monitor_flags(struct nlattr *nla, u32 *mntrflags)\n{\n\tstruct nlattr *flags[NL80211_MNTR_FLAG_MAX + 1];\n\tint flag;\n\n\t*mntrflags = 0;\n\n\tif (!nla)\n\t\treturn -EINVAL;\n\n\tif (nla_parse_nested_deprecated(flags, NL80211_MNTR_FLAG_MAX, nla, mntr_flags_policy, NULL))\n\t\treturn -EINVAL;\n\n\tfor (flag = 1; flag <= NL80211_MNTR_FLAG_MAX; flag++)\n\t\tif (flags[flag])\n\t\t\t*mntrflags |= (1<<flag);\n\n\t*mntrflags |= MONITOR_FLAG_CHANGED;\n\n\treturn 0;\n}\n\nstatic int nl80211_parse_mon_options(struct cfg80211_registered_device *rdev,\n\t\t\t\t     enum nl80211_iftype type,\n\t\t\t\t     struct genl_info *info,\n\t\t\t\t     struct vif_params *params)\n{\n\tbool change = false;\n\tint err;\n\n\tif (info->attrs[NL80211_ATTR_MNTR_FLAGS]) {\n\t\tif (type != NL80211_IFTYPE_MONITOR)\n\t\t\treturn -EINVAL;\n\n\t\terr = parse_monitor_flags(info->attrs[NL80211_ATTR_MNTR_FLAGS],\n\t\t\t\t\t  &params->flags);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tchange = true;\n\t}\n\n\tif (params->flags & MONITOR_FLAG_ACTIVE &&\n\t    !(rdev->wiphy.features & NL80211_FEATURE_ACTIVE_MONITOR))\n\t\treturn -EOPNOTSUPP;\n\n\tif (info->attrs[NL80211_ATTR_MU_MIMO_GROUP_DATA]) {\n\t\tconst u8 *mumimo_groups;\n\t\tu32 cap_flag = NL80211_EXT_FEATURE_MU_MIMO_AIR_SNIFFER;\n\n\t\tif (type != NL80211_IFTYPE_MONITOR)\n\t\t\treturn -EINVAL;\n\n\t\tif (!wiphy_ext_feature_isset(&rdev->wiphy, cap_flag))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tmumimo_groups =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_MU_MIMO_GROUP_DATA]);\n\n\t\t/* bits 0 and 63 are reserved and must be zero */\n\t\tif ((mumimo_groups[0] & BIT(0)) ||\n\t\t    (mumimo_groups[VHT_MUMIMO_GROUPS_DATA_LEN - 1] & BIT(7)))\n\t\t\treturn -EINVAL;\n\n\t\tparams->vht_mumimo_groups = mumimo_groups;\n\t\tchange = true;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_MU_MIMO_FOLLOW_MAC_ADDR]) {\n\t\tu32 cap_flag = NL80211_EXT_FEATURE_MU_MIMO_AIR_SNIFFER;\n\n\t\tif (type != NL80211_IFTYPE_MONITOR)\n\t\t\treturn -EINVAL;\n\n\t\tif (!wiphy_ext_feature_isset(&rdev->wiphy, cap_flag))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tparams->vht_mumimo_follow_addr =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_MU_MIMO_FOLLOW_MAC_ADDR]);\n\t\tchange = true;\n\t}\n\n\treturn change ? 1 : 0;\n}\n\nstatic int nl80211_valid_4addr(struct cfg80211_registered_device *rdev,\n\t\t\t       struct net_device *netdev, u8 use_4addr,\n\t\t\t       enum nl80211_iftype iftype)\n{\n\tif (!use_4addr) {\n\t\tif (netdev && netif_is_bridge_port(netdev))\n\t\t\treturn -EBUSY;\n\t\treturn 0;\n\t}\n\n\tswitch (iftype) {\n\tcase NL80211_IFTYPE_AP_VLAN:\n\t\tif (rdev->wiphy.flags & WIPHY_FLAG_4ADDR_AP)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase NL80211_IFTYPE_STATION:\n\t\tif (rdev->wiphy.flags & WIPHY_FLAG_4ADDR_STATION)\n\t\t\treturn 0;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn -EOPNOTSUPP;\n}\n\nstatic int nl80211_set_interface(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct vif_params params;\n\tint err;\n\tenum nl80211_iftype otype, ntype;\n\tstruct net_device *dev = info->user_ptr[1];\n\tbool change = false;\n\n\tmemset(&params, 0, sizeof(params));\n\n\totype = ntype = dev->ieee80211_ptr->iftype;\n\n\tif (info->attrs[NL80211_ATTR_IFTYPE]) {\n\t\tntype = nla_get_u32(info->attrs[NL80211_ATTR_IFTYPE]);\n\t\tif (otype != ntype)\n\t\t\tchange = true;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_MESH_ID]) {\n\t\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\n\t\tif (ntype != NL80211_IFTYPE_MESH_POINT)\n\t\t\treturn -EINVAL;\n\t\tif (netif_running(dev))\n\t\t\treturn -EBUSY;\n\n\t\twdev_lock(wdev);\n\t\tBUILD_BUG_ON(IEEE80211_MAX_SSID_LEN !=\n\t\t\t     IEEE80211_MAX_MESH_ID_LEN);\n\t\twdev->mesh_id_up_len =\n\t\t\tnla_len(info->attrs[NL80211_ATTR_MESH_ID]);\n\t\tmemcpy(wdev->ssid, nla_data(info->attrs[NL80211_ATTR_MESH_ID]),\n\t\t       wdev->mesh_id_up_len);\n\t\twdev_unlock(wdev);\n\t}\n\n\tif (info->attrs[NL80211_ATTR_4ADDR]) {\n\t\tparams.use_4addr = !!nla_get_u8(info->attrs[NL80211_ATTR_4ADDR]);\n\t\tchange = true;\n\t\terr = nl80211_valid_4addr(rdev, dev, params.use_4addr, ntype);\n\t\tif (err)\n\t\t\treturn err;\n\t} else {\n\t\tparams.use_4addr = -1;\n\t}\n\n\terr = nl80211_parse_mon_options(rdev, ntype, info, &params);\n\tif (err < 0)\n\t\treturn err;\n\tif (err > 0)\n\t\tchange = true;\n\n\tif (change)\n\t\terr = cfg80211_change_iface(rdev, dev, ntype, &params);\n\telse\n\t\terr = 0;\n\n\tif (!err && params.use_4addr != -1)\n\t\tdev->ieee80211_ptr->use_4addr = params.use_4addr;\n\n\tif (change && !err) {\n\t\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\n\t\tnl80211_notify_iface(rdev, wdev, NL80211_CMD_SET_INTERFACE);\n\t}\n\n\treturn err;\n}\n\nstatic int nl80211_new_interface(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct vif_params params;\n\tstruct wireless_dev *wdev;\n\tstruct sk_buff *msg;\n\tint err;\n\tenum nl80211_iftype type = NL80211_IFTYPE_UNSPECIFIED;\n\n\t/* to avoid failing a new interface creation due to pending removal */\n\tcfg80211_destroy_ifaces(rdev);\n\n\tmemset(&params, 0, sizeof(params));\n\n\tif (!info->attrs[NL80211_ATTR_IFNAME])\n\t\treturn -EINVAL;\n\n\tif (info->attrs[NL80211_ATTR_IFTYPE])\n\t\ttype = nla_get_u32(info->attrs[NL80211_ATTR_IFTYPE]);\n\n\tif (!rdev->ops->add_virtual_intf)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((type == NL80211_IFTYPE_P2P_DEVICE || type == NL80211_IFTYPE_NAN ||\n\t     rdev->wiphy.features & NL80211_FEATURE_MAC_ON_CREATE) &&\n\t    info->attrs[NL80211_ATTR_MAC]) {\n\t\tnla_memcpy(params.macaddr, info->attrs[NL80211_ATTR_MAC],\n\t\t\t   ETH_ALEN);\n\t\tif (!is_valid_ether_addr(params.macaddr))\n\t\t\treturn -EADDRNOTAVAIL;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_4ADDR]) {\n\t\tparams.use_4addr = !!nla_get_u8(info->attrs[NL80211_ATTR_4ADDR]);\n\t\terr = nl80211_valid_4addr(rdev, NULL, params.use_4addr, type);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (!cfg80211_iftype_allowed(&rdev->wiphy, type, params.use_4addr, 0))\n\t\treturn -EOPNOTSUPP;\n\n\terr = nl80211_parse_mon_options(rdev, type, info, &params);\n\tif (err < 0)\n\t\treturn err;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\twdev = rdev_add_virtual_intf(rdev,\n\t\t\t\tnla_data(info->attrs[NL80211_ATTR_IFNAME]),\n\t\t\t\tNET_NAME_USER, type, &params);\n\tif (WARN_ON(!wdev)) {\n\t\tnlmsg_free(msg);\n\t\treturn -EPROTO;\n\t} else if (IS_ERR(wdev)) {\n\t\tnlmsg_free(msg);\n\t\treturn PTR_ERR(wdev);\n\t}\n\n\tif (info->attrs[NL80211_ATTR_SOCKET_OWNER])\n\t\twdev->owner_nlportid = info->snd_portid;\n\n\tswitch (type) {\n\tcase NL80211_IFTYPE_MESH_POINT:\n\t\tif (!info->attrs[NL80211_ATTR_MESH_ID])\n\t\t\tbreak;\n\t\twdev_lock(wdev);\n\t\tBUILD_BUG_ON(IEEE80211_MAX_SSID_LEN !=\n\t\t\t     IEEE80211_MAX_MESH_ID_LEN);\n\t\twdev->mesh_id_up_len =\n\t\t\tnla_len(info->attrs[NL80211_ATTR_MESH_ID]);\n\t\tmemcpy(wdev->ssid, nla_data(info->attrs[NL80211_ATTR_MESH_ID]),\n\t\t       wdev->mesh_id_up_len);\n\t\twdev_unlock(wdev);\n\t\tbreak;\n\tcase NL80211_IFTYPE_NAN:\n\tcase NL80211_IFTYPE_P2P_DEVICE:\n\t\t/*\n\t\t * P2P Device and NAN do not have a netdev, so don't go\n\t\t * through the netdev notifier and must be added here\n\t\t */\n\t\tcfg80211_init_wdev(wdev);\n\t\tcfg80211_register_wdev(rdev, wdev);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (nl80211_send_iface(msg, info->snd_portid, info->snd_seq, 0,\n\t\t\t       rdev, wdev, NL80211_CMD_NEW_INTERFACE) < 0) {\n\t\tnlmsg_free(msg);\n\t\treturn -ENOBUFS;\n\t}\n\n\treturn genlmsg_reply(msg, info);\n}\n\nstatic int nl80211_del_interface(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev = info->user_ptr[1];\n\n\tif (!rdev->ops->del_virtual_intf)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t * We hold RTNL, so this is safe, without RTNL opencount cannot\n\t * reach 0, and thus the rdev cannot be deleted.\n\t *\n\t * We need to do it for the dev_close(), since that will call\n\t * the netdev notifiers, and we need to acquire the mutex there\n\t * but don't know if we get there from here or from some other\n\t * place (e.g. \"ip link set ... down\").\n\t */\n\tmutex_unlock(&rdev->wiphy.mtx);\n\n\t/*\n\t * If we remove a wireless device without a netdev then clear\n\t * user_ptr[1] so that nl80211_post_doit won't dereference it\n\t * to check if it needs to do dev_put(). Otherwise it crashes\n\t * since the wdev has been freed, unlike with a netdev where\n\t * we need the dev_put() for the netdev to really be freed.\n\t */\n\tif (!wdev->netdev)\n\t\tinfo->user_ptr[1] = NULL;\n\telse\n\t\tdev_close(wdev->netdev);\n\n\tmutex_lock(&rdev->wiphy.mtx);\n\n\treturn rdev_del_virtual_intf(rdev, wdev);\n}\n\nstatic int nl80211_set_noack_map(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tu16 noack_map;\n\n\tif (!info->attrs[NL80211_ATTR_NOACK_MAP])\n\t\treturn -EINVAL;\n\n\tif (!rdev->ops->set_noack_map)\n\t\treturn -EOPNOTSUPP;\n\n\tnoack_map = nla_get_u16(info->attrs[NL80211_ATTR_NOACK_MAP]);\n\n\treturn rdev_set_noack_map(rdev, dev, noack_map);\n}\n\nstruct get_key_cookie {\n\tstruct sk_buff *msg;\n\tint error;\n\tint idx;\n};\n\nstatic void get_key_callback(void *c, struct key_params *params)\n{\n\tstruct nlattr *key;\n\tstruct get_key_cookie *cookie = c;\n\n\tif ((params->key &&\n\t     nla_put(cookie->msg, NL80211_ATTR_KEY_DATA,\n\t\t     params->key_len, params->key)) ||\n\t    (params->seq &&\n\t     nla_put(cookie->msg, NL80211_ATTR_KEY_SEQ,\n\t\t     params->seq_len, params->seq)) ||\n\t    (params->cipher &&\n\t     nla_put_u32(cookie->msg, NL80211_ATTR_KEY_CIPHER,\n\t\t\t params->cipher)))\n\t\tgoto nla_put_failure;\n\n\tkey = nla_nest_start_noflag(cookie->msg, NL80211_ATTR_KEY);\n\tif (!key)\n\t\tgoto nla_put_failure;\n\n\tif ((params->key &&\n\t     nla_put(cookie->msg, NL80211_KEY_DATA,\n\t\t     params->key_len, params->key)) ||\n\t    (params->seq &&\n\t     nla_put(cookie->msg, NL80211_KEY_SEQ,\n\t\t     params->seq_len, params->seq)) ||\n\t    (params->cipher &&\n\t     nla_put_u32(cookie->msg, NL80211_KEY_CIPHER,\n\t\t\t params->cipher)))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u8(cookie->msg, NL80211_KEY_IDX, cookie->idx))\n\t\tgoto nla_put_failure;\n\n\tnla_nest_end(cookie->msg, key);\n\n\treturn;\n nla_put_failure:\n\tcookie->error = 1;\n}\n\nstatic int nl80211_get_key(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tint err;\n\tstruct net_device *dev = info->user_ptr[1];\n\tu8 key_idx = 0;\n\tconst u8 *mac_addr = NULL;\n\tbool pairwise;\n\tstruct get_key_cookie cookie = {\n\t\t.error = 0,\n\t};\n\tvoid *hdr;\n\tstruct sk_buff *msg;\n\tbool bigtk_support = false;\n\n\tif (wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t    NL80211_EXT_FEATURE_BEACON_PROTECTION))\n\t\tbigtk_support = true;\n\n\tif ((dev->ieee80211_ptr->iftype == NL80211_IFTYPE_STATION ||\n\t     dev->ieee80211_ptr->iftype == NL80211_IFTYPE_P2P_CLIENT) &&\n\t    wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t    NL80211_EXT_FEATURE_BEACON_PROTECTION_CLIENT))\n\t\tbigtk_support = true;\n\n\tif (info->attrs[NL80211_ATTR_KEY_IDX]) {\n\t\tkey_idx = nla_get_u8(info->attrs[NL80211_ATTR_KEY_IDX]);\n\n\t\tif (key_idx >= 6 && key_idx <= 7 && !bigtk_support) {\n\t\t\tGENL_SET_ERR_MSG(info, \"BIGTK not supported\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (info->attrs[NL80211_ATTR_MAC])\n\t\tmac_addr = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\n\tpairwise = !!mac_addr;\n\tif (info->attrs[NL80211_ATTR_KEY_TYPE]) {\n\t\tu32 kt = nla_get_u32(info->attrs[NL80211_ATTR_KEY_TYPE]);\n\n\t\tif (kt != NL80211_KEYTYPE_GROUP &&\n\t\t    kt != NL80211_KEYTYPE_PAIRWISE)\n\t\t\treturn -EINVAL;\n\t\tpairwise = kt == NL80211_KEYTYPE_PAIRWISE;\n\t}\n\n\tif (!rdev->ops->get_key)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!pairwise && mac_addr && !(rdev->wiphy.flags & WIPHY_FLAG_IBSS_RSN))\n\t\treturn -ENOENT;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\thdr = nl80211hdr_put(msg, info->snd_portid, info->snd_seq, 0,\n\t\t\t     NL80211_CMD_NEW_KEY);\n\tif (!hdr)\n\t\tgoto nla_put_failure;\n\n\tcookie.msg = msg;\n\tcookie.idx = key_idx;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_IFINDEX, dev->ifindex) ||\n\t    nla_put_u8(msg, NL80211_ATTR_KEY_IDX, key_idx))\n\t\tgoto nla_put_failure;\n\tif (mac_addr &&\n\t    nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, mac_addr))\n\t\tgoto nla_put_failure;\n\n\terr = rdev_get_key(rdev, dev, key_idx, pairwise, mac_addr, &cookie,\n\t\t\t   get_key_callback);\n\n\tif (err)\n\t\tgoto free_msg;\n\n\tif (cookie.error)\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\treturn genlmsg_reply(msg, info);\n\n nla_put_failure:\n\terr = -ENOBUFS;\n free_msg:\n\tnlmsg_free(msg);\n\treturn err;\n}\n\nstatic int nl80211_set_key(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct key_parse key;\n\tint err;\n\tstruct net_device *dev = info->user_ptr[1];\n\n\terr = nl80211_parse_key(info, &key);\n\tif (err)\n\t\treturn err;\n\n\tif (key.idx < 0)\n\t\treturn -EINVAL;\n\n\t/* Only support setting default key and\n\t * Extended Key ID action NL80211_KEY_SET_TX.\n\t */\n\tif (!key.def && !key.defmgmt && !key.defbeacon &&\n\t    !(key.p.mode == NL80211_KEY_SET_TX))\n\t\treturn -EINVAL;\n\n\twdev_lock(dev->ieee80211_ptr);\n\n\tif (key.def) {\n\t\tif (!rdev->ops->set_default_key) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = nl80211_key_allowed(dev->ieee80211_ptr);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = rdev_set_default_key(rdev, dev, key.idx,\n\t\t\t\t\t\t key.def_uni, key.def_multi);\n\n\t\tif (err)\n\t\t\tgoto out;\n\n#ifdef CONFIG_CFG80211_WEXT\n\t\tdev->ieee80211_ptr->wext.default_key = key.idx;\n#endif\n\t} else if (key.defmgmt) {\n\t\tif (key.def_uni || !key.def_multi) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (!rdev->ops->set_default_mgmt_key) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = nl80211_key_allowed(dev->ieee80211_ptr);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = rdev_set_default_mgmt_key(rdev, dev, key.idx);\n\t\tif (err)\n\t\t\tgoto out;\n\n#ifdef CONFIG_CFG80211_WEXT\n\t\tdev->ieee80211_ptr->wext.default_mgmt_key = key.idx;\n#endif\n\t} else if (key.defbeacon) {\n\t\tif (key.def_uni || !key.def_multi) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (!rdev->ops->set_default_beacon_key) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = nl80211_key_allowed(dev->ieee80211_ptr);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = rdev_set_default_beacon_key(rdev, dev, key.idx);\n\t\tif (err)\n\t\t\tgoto out;\n\t} else if (key.p.mode == NL80211_KEY_SET_TX &&\n\t\t   wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t\t   NL80211_EXT_FEATURE_EXT_KEY_ID)) {\n\t\tu8 *mac_addr = NULL;\n\n\t\tif (info->attrs[NL80211_ATTR_MAC])\n\t\t\tmac_addr = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\n\t\tif (!mac_addr || key.idx < 0 || key.idx > 1) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = rdev_add_key(rdev, dev, key.idx,\n\t\t\t\t   NL80211_KEYTYPE_PAIRWISE,\n\t\t\t\t   mac_addr, &key.p);\n\t} else {\n\t\terr = -EINVAL;\n\t}\n out:\n\twdev_unlock(dev->ieee80211_ptr);\n\n\treturn err;\n}\n\nstatic int nl80211_new_key(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tint err;\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct key_parse key;\n\tconst u8 *mac_addr = NULL;\n\n\terr = nl80211_parse_key(info, &key);\n\tif (err)\n\t\treturn err;\n\n\tif (!key.p.key) {\n\t\tGENL_SET_ERR_MSG(info, \"no key\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_MAC])\n\t\tmac_addr = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\n\tif (key.type == -1) {\n\t\tif (mac_addr)\n\t\t\tkey.type = NL80211_KEYTYPE_PAIRWISE;\n\t\telse\n\t\t\tkey.type = NL80211_KEYTYPE_GROUP;\n\t}\n\n\t/* for now */\n\tif (key.type != NL80211_KEYTYPE_PAIRWISE &&\n\t    key.type != NL80211_KEYTYPE_GROUP) {\n\t\tGENL_SET_ERR_MSG(info, \"key type not pairwise or group\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (key.type == NL80211_KEYTYPE_GROUP &&\n\t    info->attrs[NL80211_ATTR_VLAN_ID])\n\t\tkey.p.vlan_id = nla_get_u16(info->attrs[NL80211_ATTR_VLAN_ID]);\n\n\tif (!rdev->ops->add_key)\n\t\treturn -EOPNOTSUPP;\n\n\tif (cfg80211_validate_key_settings(rdev, &key.p, key.idx,\n\t\t\t\t\t   key.type == NL80211_KEYTYPE_PAIRWISE,\n\t\t\t\t\t   mac_addr)) {\n\t\tGENL_SET_ERR_MSG(info, \"key setting validation failed\");\n\t\treturn -EINVAL;\n\t}\n\n\twdev_lock(dev->ieee80211_ptr);\n\terr = nl80211_key_allowed(dev->ieee80211_ptr);\n\tif (err)\n\t\tGENL_SET_ERR_MSG(info, \"key not allowed\");\n\tif (!err) {\n\t\terr = rdev_add_key(rdev, dev, key.idx,\n\t\t\t\t   key.type == NL80211_KEYTYPE_PAIRWISE,\n\t\t\t\t    mac_addr, &key.p);\n\t\tif (err)\n\t\t\tGENL_SET_ERR_MSG(info, \"key addition failed\");\n\t}\n\twdev_unlock(dev->ieee80211_ptr);\n\n\treturn err;\n}\n\nstatic int nl80211_del_key(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tint err;\n\tstruct net_device *dev = info->user_ptr[1];\n\tu8 *mac_addr = NULL;\n\tstruct key_parse key;\n\n\terr = nl80211_parse_key(info, &key);\n\tif (err)\n\t\treturn err;\n\n\tif (info->attrs[NL80211_ATTR_MAC])\n\t\tmac_addr = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\n\tif (key.type == -1) {\n\t\tif (mac_addr)\n\t\t\tkey.type = NL80211_KEYTYPE_PAIRWISE;\n\t\telse\n\t\t\tkey.type = NL80211_KEYTYPE_GROUP;\n\t}\n\n\t/* for now */\n\tif (key.type != NL80211_KEYTYPE_PAIRWISE &&\n\t    key.type != NL80211_KEYTYPE_GROUP)\n\t\treturn -EINVAL;\n\n\tif (!cfg80211_valid_key_idx(rdev, key.idx,\n\t\t\t\t    key.type == NL80211_KEYTYPE_PAIRWISE))\n\t\treturn -EINVAL;\n\n\tif (!rdev->ops->del_key)\n\t\treturn -EOPNOTSUPP;\n\n\twdev_lock(dev->ieee80211_ptr);\n\terr = nl80211_key_allowed(dev->ieee80211_ptr);\n\n\tif (key.type == NL80211_KEYTYPE_GROUP && mac_addr &&\n\t    !(rdev->wiphy.flags & WIPHY_FLAG_IBSS_RSN))\n\t\terr = -ENOENT;\n\n\tif (!err)\n\t\terr = rdev_del_key(rdev, dev, key.idx,\n\t\t\t\t   key.type == NL80211_KEYTYPE_PAIRWISE,\n\t\t\t\t   mac_addr);\n\n#ifdef CONFIG_CFG80211_WEXT\n\tif (!err) {\n\t\tif (key.idx == dev->ieee80211_ptr->wext.default_key)\n\t\t\tdev->ieee80211_ptr->wext.default_key = -1;\n\t\telse if (key.idx == dev->ieee80211_ptr->wext.default_mgmt_key)\n\t\t\tdev->ieee80211_ptr->wext.default_mgmt_key = -1;\n\t}\n#endif\n\twdev_unlock(dev->ieee80211_ptr);\n\n\treturn err;\n}\n\n/* This function returns an error or the number of nested attributes */\nstatic int validate_acl_mac_addrs(struct nlattr *nl_attr)\n{\n\tstruct nlattr *attr;\n\tint n_entries = 0, tmp;\n\n\tnla_for_each_nested(attr, nl_attr, tmp) {\n\t\tif (nla_len(attr) != ETH_ALEN)\n\t\t\treturn -EINVAL;\n\n\t\tn_entries++;\n\t}\n\n\treturn n_entries;\n}\n\n/*\n * This function parses ACL information and allocates memory for ACL data.\n * On successful return, the calling function is responsible to free the\n * ACL buffer returned by this function.\n */\nstatic struct cfg80211_acl_data *parse_acl_data(struct wiphy *wiphy,\n\t\t\t\t\t\tstruct genl_info *info)\n{\n\tenum nl80211_acl_policy acl_policy;\n\tstruct nlattr *attr;\n\tstruct cfg80211_acl_data *acl;\n\tint i = 0, n_entries, tmp;\n\n\tif (!wiphy->max_acl_mac_addrs)\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\tif (!info->attrs[NL80211_ATTR_ACL_POLICY])\n\t\treturn ERR_PTR(-EINVAL);\n\n\tacl_policy = nla_get_u32(info->attrs[NL80211_ATTR_ACL_POLICY]);\n\tif (acl_policy != NL80211_ACL_POLICY_ACCEPT_UNLESS_LISTED &&\n\t    acl_policy != NL80211_ACL_POLICY_DENY_UNLESS_LISTED)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!info->attrs[NL80211_ATTR_MAC_ADDRS])\n\t\treturn ERR_PTR(-EINVAL);\n\n\tn_entries = validate_acl_mac_addrs(info->attrs[NL80211_ATTR_MAC_ADDRS]);\n\tif (n_entries < 0)\n\t\treturn ERR_PTR(n_entries);\n\n\tif (n_entries > wiphy->max_acl_mac_addrs)\n\t\treturn ERR_PTR(-ENOTSUPP);\n\n\tacl = kzalloc(struct_size(acl, mac_addrs, n_entries), GFP_KERNEL);\n\tif (!acl)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tnla_for_each_nested(attr, info->attrs[NL80211_ATTR_MAC_ADDRS], tmp) {\n\t\tmemcpy(acl->mac_addrs[i].addr, nla_data(attr), ETH_ALEN);\n\t\ti++;\n\t}\n\n\tacl->n_acl_entries = n_entries;\n\tacl->acl_policy = acl_policy;\n\n\treturn acl;\n}\n\nstatic int nl80211_set_mac_acl(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct cfg80211_acl_data *acl;\n\tint err;\n\n\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_AP &&\n\t    dev->ieee80211_ptr->iftype != NL80211_IFTYPE_P2P_GO)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!dev->ieee80211_ptr->beacon_interval)\n\t\treturn -EINVAL;\n\n\tacl = parse_acl_data(&rdev->wiphy, info);\n\tif (IS_ERR(acl))\n\t\treturn PTR_ERR(acl);\n\n\terr = rdev_set_mac_acl(rdev, dev, acl);\n\n\tkfree(acl);\n\n\treturn err;\n}\n\nstatic u32 rateset_to_mask(struct ieee80211_supported_band *sband,\n\t\t\t   u8 *rates, u8 rates_len)\n{\n\tu8 i;\n\tu32 mask = 0;\n\n\tfor (i = 0; i < rates_len; i++) {\n\t\tint rate = (rates[i] & 0x7f) * 5;\n\t\tint ridx;\n\n\t\tfor (ridx = 0; ridx < sband->n_bitrates; ridx++) {\n\t\t\tstruct ieee80211_rate *srate =\n\t\t\t\t&sband->bitrates[ridx];\n\t\t\tif (rate == srate->bitrate) {\n\t\t\t\tmask |= 1 << ridx;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (ridx == sband->n_bitrates)\n\t\t\treturn 0; /* rate not found */\n\t}\n\n\treturn mask;\n}\n\nstatic bool ht_rateset_to_mask(struct ieee80211_supported_band *sband,\n\t\t\t       u8 *rates, u8 rates_len,\n\t\t\t       u8 mcs[IEEE80211_HT_MCS_MASK_LEN])\n{\n\tu8 i;\n\n\tmemset(mcs, 0, IEEE80211_HT_MCS_MASK_LEN);\n\n\tfor (i = 0; i < rates_len; i++) {\n\t\tint ridx, rbit;\n\n\t\tridx = rates[i] / 8;\n\t\trbit = BIT(rates[i] % 8);\n\n\t\t/* check validity */\n\t\tif ((ridx < 0) || (ridx >= IEEE80211_HT_MCS_MASK_LEN))\n\t\t\treturn false;\n\n\t\t/* check availability */\n\t\tridx = array_index_nospec(ridx, IEEE80211_HT_MCS_MASK_LEN);\n\t\tif (sband->ht_cap.mcs.rx_mask[ridx] & rbit)\n\t\t\tmcs[ridx] |= rbit;\n\t\telse\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic u16 vht_mcs_map_to_mcs_mask(u8 vht_mcs_map)\n{\n\tu16 mcs_mask = 0;\n\n\tswitch (vht_mcs_map) {\n\tcase IEEE80211_VHT_MCS_NOT_SUPPORTED:\n\t\tbreak;\n\tcase IEEE80211_VHT_MCS_SUPPORT_0_7:\n\t\tmcs_mask = 0x00FF;\n\t\tbreak;\n\tcase IEEE80211_VHT_MCS_SUPPORT_0_8:\n\t\tmcs_mask = 0x01FF;\n\t\tbreak;\n\tcase IEEE80211_VHT_MCS_SUPPORT_0_9:\n\t\tmcs_mask = 0x03FF;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn mcs_mask;\n}\n\nstatic void vht_build_mcs_mask(u16 vht_mcs_map,\n\t\t\t       u16 vht_mcs_mask[NL80211_VHT_NSS_MAX])\n{\n\tu8 nss;\n\n\tfor (nss = 0; nss < NL80211_VHT_NSS_MAX; nss++) {\n\t\tvht_mcs_mask[nss] = vht_mcs_map_to_mcs_mask(vht_mcs_map & 0x03);\n\t\tvht_mcs_map >>= 2;\n\t}\n}\n\nstatic bool vht_set_mcs_mask(struct ieee80211_supported_band *sband,\n\t\t\t     struct nl80211_txrate_vht *txrate,\n\t\t\t     u16 mcs[NL80211_VHT_NSS_MAX])\n{\n\tu16 tx_mcs_map = le16_to_cpu(sband->vht_cap.vht_mcs.tx_mcs_map);\n\tu16 tx_mcs_mask[NL80211_VHT_NSS_MAX] = {};\n\tu8 i;\n\n\tif (!sband->vht_cap.vht_supported)\n\t\treturn false;\n\n\tmemset(mcs, 0, sizeof(u16) * NL80211_VHT_NSS_MAX);\n\n\t/* Build vht_mcs_mask from VHT capabilities */\n\tvht_build_mcs_mask(tx_mcs_map, tx_mcs_mask);\n\n\tfor (i = 0; i < NL80211_VHT_NSS_MAX; i++) {\n\t\tif ((tx_mcs_mask[i] & txrate->mcs[i]) == txrate->mcs[i])\n\t\t\tmcs[i] = txrate->mcs[i];\n\t\telse\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic u16 he_mcs_map_to_mcs_mask(u8 he_mcs_map)\n{\n\tswitch (he_mcs_map) {\n\tcase IEEE80211_HE_MCS_NOT_SUPPORTED:\n\t\treturn 0;\n\tcase IEEE80211_HE_MCS_SUPPORT_0_7:\n\t\treturn 0x00FF;\n\tcase IEEE80211_HE_MCS_SUPPORT_0_9:\n\t\treturn 0x03FF;\n\tcase IEEE80211_HE_MCS_SUPPORT_0_11:\n\t\treturn 0xFFF;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic void he_build_mcs_mask(u16 he_mcs_map,\n\t\t\t      u16 he_mcs_mask[NL80211_HE_NSS_MAX])\n{\n\tu8 nss;\n\n\tfor (nss = 0; nss < NL80211_HE_NSS_MAX; nss++) {\n\t\the_mcs_mask[nss] = he_mcs_map_to_mcs_mask(he_mcs_map & 0x03);\n\t\the_mcs_map >>= 2;\n\t}\n}\n\nstatic u16 he_get_txmcsmap(struct genl_info *info,\n\t\t\t   const struct ieee80211_sta_he_cap *he_cap)\n{\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\t__le16\ttx_mcs;\n\n\tswitch (wdev->chandef.width) {\n\tcase NL80211_CHAN_WIDTH_80P80:\n\t\ttx_mcs = he_cap->he_mcs_nss_supp.tx_mcs_80p80;\n\t\tbreak;\n\tcase NL80211_CHAN_WIDTH_160:\n\t\ttx_mcs = he_cap->he_mcs_nss_supp.tx_mcs_160;\n\t\tbreak;\n\tdefault:\n\t\ttx_mcs = he_cap->he_mcs_nss_supp.tx_mcs_80;\n\t\tbreak;\n\t}\n\treturn le16_to_cpu(tx_mcs);\n}\n\nstatic bool he_set_mcs_mask(struct genl_info *info,\n\t\t\t    struct wireless_dev *wdev,\n\t\t\t    struct ieee80211_supported_band *sband,\n\t\t\t    struct nl80211_txrate_he *txrate,\n\t\t\t    u16 mcs[NL80211_HE_NSS_MAX])\n{\n\tconst struct ieee80211_sta_he_cap *he_cap;\n\tu16 tx_mcs_mask[NL80211_HE_NSS_MAX] = {};\n\tu16 tx_mcs_map = 0;\n\tu8 i;\n\n\the_cap = ieee80211_get_he_iftype_cap(sband, wdev->iftype);\n\tif (!he_cap)\n\t\treturn false;\n\n\tmemset(mcs, 0, sizeof(u16) * NL80211_HE_NSS_MAX);\n\n\ttx_mcs_map = he_get_txmcsmap(info, he_cap);\n\n\t/* Build he_mcs_mask from HE capabilities */\n\the_build_mcs_mask(tx_mcs_map, tx_mcs_mask);\n\n\tfor (i = 0; i < NL80211_HE_NSS_MAX; i++) {\n\t\tif ((tx_mcs_mask[i] & txrate->mcs[i]) == txrate->mcs[i])\n\t\t\tmcs[i] = txrate->mcs[i];\n\t\telse\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int nl80211_parse_tx_bitrate_mask(struct genl_info *info,\n\t\t\t\t\t struct nlattr *attrs[],\n\t\t\t\t\t enum nl80211_attrs attr,\n\t\t\t\t\t struct cfg80211_bitrate_mask *mask,\n\t\t\t\t\t struct net_device *dev,\n\t\t\t\t\t bool default_all_enabled)\n{\n\tstruct nlattr *tb[NL80211_TXRATE_MAX + 1];\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tint rem, i;\n\tstruct nlattr *tx_rates;\n\tstruct ieee80211_supported_band *sband;\n\tu16 vht_tx_mcs_map, he_tx_mcs_map;\n\n\tmemset(mask, 0, sizeof(*mask));\n\t/* Default to all rates enabled */\n\tfor (i = 0; i < NUM_NL80211_BANDS; i++) {\n\t\tconst struct ieee80211_sta_he_cap *he_cap;\n\n\t\tif (!default_all_enabled)\n\t\t\tbreak;\n\n\t\tsband = rdev->wiphy.bands[i];\n\n\t\tif (!sband)\n\t\t\tcontinue;\n\n\t\tmask->control[i].legacy = (1 << sband->n_bitrates) - 1;\n\t\tmemcpy(mask->control[i].ht_mcs,\n\t\t       sband->ht_cap.mcs.rx_mask,\n\t\t       sizeof(mask->control[i].ht_mcs));\n\n\t\tif (!sband->vht_cap.vht_supported)\n\t\t\tcontinue;\n\n\t\tvht_tx_mcs_map = le16_to_cpu(sband->vht_cap.vht_mcs.tx_mcs_map);\n\t\tvht_build_mcs_mask(vht_tx_mcs_map, mask->control[i].vht_mcs);\n\n\t\the_cap = ieee80211_get_he_iftype_cap(sband, wdev->iftype);\n\t\tif (!he_cap)\n\t\t\tcontinue;\n\n\t\the_tx_mcs_map = he_get_txmcsmap(info, he_cap);\n\t\the_build_mcs_mask(he_tx_mcs_map, mask->control[i].he_mcs);\n\n\t\tmask->control[i].he_gi = 0xFF;\n\t\tmask->control[i].he_ltf = 0xFF;\n\t}\n\n\t/* if no rates are given set it back to the defaults */\n\tif (!attrs[attr])\n\t\tgoto out;\n\n\t/* The nested attribute uses enum nl80211_band as the index. This maps\n\t * directly to the enum nl80211_band values used in cfg80211.\n\t */\n\tBUILD_BUG_ON(NL80211_MAX_SUPP_HT_RATES > IEEE80211_HT_MCS_MASK_LEN * 8);\n\tnla_for_each_nested(tx_rates, attrs[attr], rem) {\n\t\tenum nl80211_band band = nla_type(tx_rates);\n\t\tint err;\n\n\t\tif (band < 0 || band >= NUM_NL80211_BANDS)\n\t\t\treturn -EINVAL;\n\t\tsband = rdev->wiphy.bands[band];\n\t\tif (sband == NULL)\n\t\t\treturn -EINVAL;\n\t\terr = nla_parse_nested_deprecated(tb, NL80211_TXRATE_MAX,\n\t\t\t\t\t\t  tx_rates,\n\t\t\t\t\t\t  nl80211_txattr_policy,\n\t\t\t\t\t\t  info->extack);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (tb[NL80211_TXRATE_LEGACY]) {\n\t\t\tmask->control[band].legacy = rateset_to_mask(\n\t\t\t\tsband,\n\t\t\t\tnla_data(tb[NL80211_TXRATE_LEGACY]),\n\t\t\t\tnla_len(tb[NL80211_TXRATE_LEGACY]));\n\t\t\tif ((mask->control[band].legacy == 0) &&\n\t\t\t    nla_len(tb[NL80211_TXRATE_LEGACY]))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (tb[NL80211_TXRATE_HT]) {\n\t\t\tif (!ht_rateset_to_mask(\n\t\t\t\t\tsband,\n\t\t\t\t\tnla_data(tb[NL80211_TXRATE_HT]),\n\t\t\t\t\tnla_len(tb[NL80211_TXRATE_HT]),\n\t\t\t\t\tmask->control[band].ht_mcs))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (tb[NL80211_TXRATE_VHT]) {\n\t\t\tif (!vht_set_mcs_mask(\n\t\t\t\t\tsband,\n\t\t\t\t\tnla_data(tb[NL80211_TXRATE_VHT]),\n\t\t\t\t\tmask->control[band].vht_mcs))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (tb[NL80211_TXRATE_GI]) {\n\t\t\tmask->control[band].gi =\n\t\t\t\tnla_get_u8(tb[NL80211_TXRATE_GI]);\n\t\t\tif (mask->control[band].gi > NL80211_TXRATE_FORCE_LGI)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (tb[NL80211_TXRATE_HE] &&\n\t\t    !he_set_mcs_mask(info, wdev, sband,\n\t\t\t\t     nla_data(tb[NL80211_TXRATE_HE]),\n\t\t\t\t     mask->control[band].he_mcs))\n\t\t\treturn -EINVAL;\n\n\t\tif (tb[NL80211_TXRATE_HE_GI])\n\t\t\tmask->control[band].he_gi =\n\t\t\t\tnla_get_u8(tb[NL80211_TXRATE_HE_GI]);\n\t\tif (tb[NL80211_TXRATE_HE_LTF])\n\t\t\tmask->control[band].he_ltf =\n\t\t\t\tnla_get_u8(tb[NL80211_TXRATE_HE_LTF]);\n\n\t\tif (mask->control[band].legacy == 0) {\n\t\t\t/* don't allow empty legacy rates if HT, VHT or HE\n\t\t\t * are not even supported.\n\t\t\t */\n\t\t\tif (!(rdev->wiphy.bands[band]->ht_cap.ht_supported ||\n\t\t\t      rdev->wiphy.bands[band]->vht_cap.vht_supported ||\n\t\t\t      ieee80211_get_he_iftype_cap(sband, wdev->iftype)))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tfor (i = 0; i < IEEE80211_HT_MCS_MASK_LEN; i++)\n\t\t\t\tif (mask->control[band].ht_mcs[i])\n\t\t\t\t\tgoto out;\n\n\t\t\tfor (i = 0; i < NL80211_VHT_NSS_MAX; i++)\n\t\t\t\tif (mask->control[band].vht_mcs[i])\n\t\t\t\t\tgoto out;\n\n\t\t\tfor (i = 0; i < NL80211_HE_NSS_MAX; i++)\n\t\t\t\tif (mask->control[band].he_mcs[i])\n\t\t\t\t\tgoto out;\n\n\t\t\t/* legacy and mcs rates may not be both empty */\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\nout:\n\treturn 0;\n}\n\nstatic int validate_beacon_tx_rate(struct cfg80211_registered_device *rdev,\n\t\t\t\t   enum nl80211_band band,\n\t\t\t\t   struct cfg80211_bitrate_mask *beacon_rate)\n{\n\tu32 count_ht, count_vht, count_he, i;\n\tu32 rate = beacon_rate->control[band].legacy;\n\n\t/* Allow only one rate */\n\tif (hweight32(rate) > 1)\n\t\treturn -EINVAL;\n\n\tcount_ht = 0;\n\tfor (i = 0; i < IEEE80211_HT_MCS_MASK_LEN; i++) {\n\t\tif (hweight8(beacon_rate->control[band].ht_mcs[i]) > 1) {\n\t\t\treturn -EINVAL;\n\t\t} else if (beacon_rate->control[band].ht_mcs[i]) {\n\t\t\tcount_ht++;\n\t\t\tif (count_ht > 1)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (count_ht && rate)\n\t\t\treturn -EINVAL;\n\t}\n\n\tcount_vht = 0;\n\tfor (i = 0; i < NL80211_VHT_NSS_MAX; i++) {\n\t\tif (hweight16(beacon_rate->control[band].vht_mcs[i]) > 1) {\n\t\t\treturn -EINVAL;\n\t\t} else if (beacon_rate->control[band].vht_mcs[i]) {\n\t\t\tcount_vht++;\n\t\t\tif (count_vht > 1)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (count_vht && rate)\n\t\t\treturn -EINVAL;\n\t}\n\n\tcount_he = 0;\n\tfor (i = 0; i < NL80211_HE_NSS_MAX; i++) {\n\t\tif (hweight16(beacon_rate->control[band].he_mcs[i]) > 1) {\n\t\t\treturn -EINVAL;\n\t\t} else if (beacon_rate->control[band].he_mcs[i]) {\n\t\t\tcount_he++;\n\t\t\tif (count_he > 1)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (count_he && rate)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif ((count_ht && count_vht && count_he) ||\n\t    (!rate && !count_ht && !count_vht && !count_he))\n\t\treturn -EINVAL;\n\n\tif (rate &&\n\t    !wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t     NL80211_EXT_FEATURE_BEACON_RATE_LEGACY))\n\t\treturn -EINVAL;\n\tif (count_ht &&\n\t    !wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t     NL80211_EXT_FEATURE_BEACON_RATE_HT))\n\t\treturn -EINVAL;\n\tif (count_vht &&\n\t    !wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t     NL80211_EXT_FEATURE_BEACON_RATE_VHT))\n\t\treturn -EINVAL;\n\tif (count_he &&\n\t    !wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t     NL80211_EXT_FEATURE_BEACON_RATE_HE))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int nl80211_parse_beacon(struct cfg80211_registered_device *rdev,\n\t\t\t\tstruct nlattr *attrs[],\n\t\t\t\tstruct cfg80211_beacon_data *bcn)\n{\n\tbool haveinfo = false;\n\tint err;\n\n\tmemset(bcn, 0, sizeof(*bcn));\n\n\tif (attrs[NL80211_ATTR_BEACON_HEAD]) {\n\t\tbcn->head = nla_data(attrs[NL80211_ATTR_BEACON_HEAD]);\n\t\tbcn->head_len = nla_len(attrs[NL80211_ATTR_BEACON_HEAD]);\n\t\tif (!bcn->head_len)\n\t\t\treturn -EINVAL;\n\t\thaveinfo = true;\n\t}\n\n\tif (attrs[NL80211_ATTR_BEACON_TAIL]) {\n\t\tbcn->tail = nla_data(attrs[NL80211_ATTR_BEACON_TAIL]);\n\t\tbcn->tail_len = nla_len(attrs[NL80211_ATTR_BEACON_TAIL]);\n\t\thaveinfo = true;\n\t}\n\n\tif (!haveinfo)\n\t\treturn -EINVAL;\n\n\tif (attrs[NL80211_ATTR_IE]) {\n\t\tbcn->beacon_ies = nla_data(attrs[NL80211_ATTR_IE]);\n\t\tbcn->beacon_ies_len = nla_len(attrs[NL80211_ATTR_IE]);\n\t}\n\n\tif (attrs[NL80211_ATTR_IE_PROBE_RESP]) {\n\t\tbcn->proberesp_ies =\n\t\t\tnla_data(attrs[NL80211_ATTR_IE_PROBE_RESP]);\n\t\tbcn->proberesp_ies_len =\n\t\t\tnla_len(attrs[NL80211_ATTR_IE_PROBE_RESP]);\n\t}\n\n\tif (attrs[NL80211_ATTR_IE_ASSOC_RESP]) {\n\t\tbcn->assocresp_ies =\n\t\t\tnla_data(attrs[NL80211_ATTR_IE_ASSOC_RESP]);\n\t\tbcn->assocresp_ies_len =\n\t\t\tnla_len(attrs[NL80211_ATTR_IE_ASSOC_RESP]);\n\t}\n\n\tif (attrs[NL80211_ATTR_PROBE_RESP]) {\n\t\tbcn->probe_resp = nla_data(attrs[NL80211_ATTR_PROBE_RESP]);\n\t\tbcn->probe_resp_len = nla_len(attrs[NL80211_ATTR_PROBE_RESP]);\n\t}\n\n\tif (attrs[NL80211_ATTR_FTM_RESPONDER]) {\n\t\tstruct nlattr *tb[NL80211_FTM_RESP_ATTR_MAX + 1];\n\n\t\terr = nla_parse_nested_deprecated(tb,\n\t\t\t\t\t\t  NL80211_FTM_RESP_ATTR_MAX,\n\t\t\t\t\t\t  attrs[NL80211_ATTR_FTM_RESPONDER],\n\t\t\t\t\t\t  NULL, NULL);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (tb[NL80211_FTM_RESP_ATTR_ENABLED] &&\n\t\t    wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t\t    NL80211_EXT_FEATURE_ENABLE_FTM_RESPONDER))\n\t\t\tbcn->ftm_responder = 1;\n\t\telse\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (tb[NL80211_FTM_RESP_ATTR_LCI]) {\n\t\t\tbcn->lci = nla_data(tb[NL80211_FTM_RESP_ATTR_LCI]);\n\t\t\tbcn->lci_len = nla_len(tb[NL80211_FTM_RESP_ATTR_LCI]);\n\t\t}\n\n\t\tif (tb[NL80211_FTM_RESP_ATTR_CIVICLOC]) {\n\t\t\tbcn->civicloc = nla_data(tb[NL80211_FTM_RESP_ATTR_CIVICLOC]);\n\t\t\tbcn->civicloc_len = nla_len(tb[NL80211_FTM_RESP_ATTR_CIVICLOC]);\n\t\t}\n\t} else {\n\t\tbcn->ftm_responder = -1;\n\t}\n\n\treturn 0;\n}\n\nstatic int nl80211_parse_he_obss_pd(struct nlattr *attrs,\n\t\t\t\t    struct ieee80211_he_obss_pd *he_obss_pd)\n{\n\tstruct nlattr *tb[NL80211_HE_OBSS_PD_ATTR_MAX + 1];\n\tint err;\n\n\terr = nla_parse_nested(tb, NL80211_HE_OBSS_PD_ATTR_MAX, attrs,\n\t\t\t       he_obss_pd_policy, NULL);\n\tif (err)\n\t\treturn err;\n\n\tif (!tb[NL80211_HE_OBSS_PD_ATTR_SR_CTRL])\n\t\treturn -EINVAL;\n\n\the_obss_pd->sr_ctrl = nla_get_u8(tb[NL80211_HE_OBSS_PD_ATTR_SR_CTRL]);\n\n\tif (tb[NL80211_HE_OBSS_PD_ATTR_MIN_OFFSET])\n\t\the_obss_pd->min_offset =\n\t\t\tnla_get_u8(tb[NL80211_HE_OBSS_PD_ATTR_MIN_OFFSET]);\n\tif (tb[NL80211_HE_OBSS_PD_ATTR_MAX_OFFSET])\n\t\the_obss_pd->max_offset =\n\t\t\tnla_get_u8(tb[NL80211_HE_OBSS_PD_ATTR_MAX_OFFSET]);\n\tif (tb[NL80211_HE_OBSS_PD_ATTR_NON_SRG_MAX_OFFSET])\n\t\the_obss_pd->non_srg_max_offset =\n\t\t\tnla_get_u8(tb[NL80211_HE_OBSS_PD_ATTR_NON_SRG_MAX_OFFSET]);\n\n\tif (he_obss_pd->min_offset > he_obss_pd->max_offset)\n\t\treturn -EINVAL;\n\n\tif (tb[NL80211_HE_OBSS_PD_ATTR_BSS_COLOR_BITMAP])\n\t\tmemcpy(he_obss_pd->bss_color_bitmap,\n\t\t       nla_data(tb[NL80211_HE_OBSS_PD_ATTR_BSS_COLOR_BITMAP]),\n\t\t       sizeof(he_obss_pd->bss_color_bitmap));\n\n\tif (tb[NL80211_HE_OBSS_PD_ATTR_PARTIAL_BSSID_BITMAP])\n\t\tmemcpy(he_obss_pd->partial_bssid_bitmap,\n\t\t       nla_data(tb[NL80211_HE_OBSS_PD_ATTR_PARTIAL_BSSID_BITMAP]),\n\t\t       sizeof(he_obss_pd->partial_bssid_bitmap));\n\n\the_obss_pd->enable = true;\n\n\treturn 0;\n}\n\nstatic int nl80211_parse_he_bss_color(struct nlattr *attrs,\n\t\t\t\t      struct cfg80211_he_bss_color *he_bss_color)\n{\n\tstruct nlattr *tb[NL80211_HE_BSS_COLOR_ATTR_MAX + 1];\n\tint err;\n\n\terr = nla_parse_nested(tb, NL80211_HE_BSS_COLOR_ATTR_MAX, attrs,\n\t\t\t       he_bss_color_policy, NULL);\n\tif (err)\n\t\treturn err;\n\n\tif (!tb[NL80211_HE_BSS_COLOR_ATTR_COLOR])\n\t\treturn -EINVAL;\n\n\the_bss_color->color =\n\t\tnla_get_u8(tb[NL80211_HE_BSS_COLOR_ATTR_COLOR]);\n\the_bss_color->enabled =\n\t\t!nla_get_flag(tb[NL80211_HE_BSS_COLOR_ATTR_DISABLED]);\n\the_bss_color->partial =\n\t\tnla_get_flag(tb[NL80211_HE_BSS_COLOR_ATTR_PARTIAL]);\n\n\treturn 0;\n}\n\nstatic int nl80211_parse_fils_discovery(struct cfg80211_registered_device *rdev,\n\t\t\t\t\tstruct nlattr *attrs,\n\t\t\t\t\tstruct cfg80211_ap_settings *params)\n{\n\tstruct nlattr *tb[NL80211_FILS_DISCOVERY_ATTR_MAX + 1];\n\tint ret;\n\tstruct cfg80211_fils_discovery *fd = &params->fils_discovery;\n\n\tif (!wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t     NL80211_EXT_FEATURE_FILS_DISCOVERY))\n\t\treturn -EINVAL;\n\n\tret = nla_parse_nested(tb, NL80211_FILS_DISCOVERY_ATTR_MAX, attrs,\n\t\t\t       NULL, NULL);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!tb[NL80211_FILS_DISCOVERY_ATTR_INT_MIN] ||\n\t    !tb[NL80211_FILS_DISCOVERY_ATTR_INT_MAX] ||\n\t    !tb[NL80211_FILS_DISCOVERY_ATTR_TMPL])\n\t\treturn -EINVAL;\n\n\tfd->tmpl_len = nla_len(tb[NL80211_FILS_DISCOVERY_ATTR_TMPL]);\n\tfd->tmpl = nla_data(tb[NL80211_FILS_DISCOVERY_ATTR_TMPL]);\n\tfd->min_interval = nla_get_u32(tb[NL80211_FILS_DISCOVERY_ATTR_INT_MIN]);\n\tfd->max_interval = nla_get_u32(tb[NL80211_FILS_DISCOVERY_ATTR_INT_MAX]);\n\n\treturn 0;\n}\n\nstatic int\nnl80211_parse_unsol_bcast_probe_resp(struct cfg80211_registered_device *rdev,\n\t\t\t\t     struct nlattr *attrs,\n\t\t\t\t     struct cfg80211_ap_settings *params)\n{\n\tstruct nlattr *tb[NL80211_UNSOL_BCAST_PROBE_RESP_ATTR_MAX + 1];\n\tint ret;\n\tstruct cfg80211_unsol_bcast_probe_resp *presp =\n\t\t\t\t\t&params->unsol_bcast_probe_resp;\n\n\tif (!wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t     NL80211_EXT_FEATURE_UNSOL_BCAST_PROBE_RESP))\n\t\treturn -EINVAL;\n\n\tret = nla_parse_nested(tb, NL80211_UNSOL_BCAST_PROBE_RESP_ATTR_MAX,\n\t\t\t       attrs, NULL, NULL);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!tb[NL80211_UNSOL_BCAST_PROBE_RESP_ATTR_INT] ||\n\t    !tb[NL80211_UNSOL_BCAST_PROBE_RESP_ATTR_TMPL])\n\t\treturn -EINVAL;\n\n\tpresp->tmpl = nla_data(tb[NL80211_UNSOL_BCAST_PROBE_RESP_ATTR_TMPL]);\n\tpresp->tmpl_len = nla_len(tb[NL80211_UNSOL_BCAST_PROBE_RESP_ATTR_TMPL]);\n\tpresp->interval = nla_get_u32(tb[NL80211_UNSOL_BCAST_PROBE_RESP_ATTR_INT]);\n\treturn 0;\n}\n\nstatic void nl80211_check_ap_rate_selectors(struct cfg80211_ap_settings *params,\n\t\t\t\t\t    const u8 *rates)\n{\n\tint i;\n\n\tif (!rates)\n\t\treturn;\n\n\tfor (i = 0; i < rates[1]; i++) {\n\t\tif (rates[2 + i] == BSS_MEMBERSHIP_SELECTOR_HT_PHY)\n\t\t\tparams->ht_required = true;\n\t\tif (rates[2 + i] == BSS_MEMBERSHIP_SELECTOR_VHT_PHY)\n\t\t\tparams->vht_required = true;\n\t\tif (rates[2 + i] == BSS_MEMBERSHIP_SELECTOR_HE_PHY)\n\t\t\tparams->he_required = true;\n\t\tif (rates[2 + i] == BSS_MEMBERSHIP_SELECTOR_SAE_H2E)\n\t\t\tparams->sae_h2e_required = true;\n\t}\n}\n\n/*\n * Since the nl80211 API didn't include, from the beginning, attributes about\n * HT/VHT requirements/capabilities, we parse them out of the IEs for the\n * benefit of drivers that rebuild IEs in the firmware.\n */\nstatic void nl80211_calculate_ap_params(struct cfg80211_ap_settings *params)\n{\n\tconst struct cfg80211_beacon_data *bcn = &params->beacon;\n\tsize_t ies_len = bcn->tail_len;\n\tconst u8 *ies = bcn->tail;\n\tconst u8 *rates;\n\tconst u8 *cap;\n\n\trates = cfg80211_find_ie(WLAN_EID_SUPP_RATES, ies, ies_len);\n\tnl80211_check_ap_rate_selectors(params, rates);\n\n\trates = cfg80211_find_ie(WLAN_EID_EXT_SUPP_RATES, ies, ies_len);\n\tnl80211_check_ap_rate_selectors(params, rates);\n\n\tcap = cfg80211_find_ie(WLAN_EID_HT_CAPABILITY, ies, ies_len);\n\tif (cap && cap[1] >= sizeof(*params->ht_cap))\n\t\tparams->ht_cap = (void *)(cap + 2);\n\tcap = cfg80211_find_ie(WLAN_EID_VHT_CAPABILITY, ies, ies_len);\n\tif (cap && cap[1] >= sizeof(*params->vht_cap))\n\t\tparams->vht_cap = (void *)(cap + 2);\n\tcap = cfg80211_find_ext_ie(WLAN_EID_EXT_HE_CAPABILITY, ies, ies_len);\n\tif (cap && cap[1] >= sizeof(*params->he_cap) + 1)\n\t\tparams->he_cap = (void *)(cap + 3);\n\tcap = cfg80211_find_ext_ie(WLAN_EID_EXT_HE_OPERATION, ies, ies_len);\n\tif (cap && cap[1] >= sizeof(*params->he_oper) + 1)\n\t\tparams->he_oper = (void *)(cap + 3);\n}\n\nstatic bool nl80211_get_ap_channel(struct cfg80211_registered_device *rdev,\n\t\t\t\t   struct cfg80211_ap_settings *params)\n{\n\tstruct wireless_dev *wdev;\n\tbool ret = false;\n\n\tlist_for_each_entry(wdev, &rdev->wiphy.wdev_list, list) {\n\t\tif (wdev->iftype != NL80211_IFTYPE_AP &&\n\t\t    wdev->iftype != NL80211_IFTYPE_P2P_GO)\n\t\t\tcontinue;\n\n\t\tif (!wdev->preset_chandef.chan)\n\t\t\tcontinue;\n\n\t\tparams->chandef = wdev->preset_chandef;\n\t\tret = true;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic bool nl80211_valid_auth_type(struct cfg80211_registered_device *rdev,\n\t\t\t\t    enum nl80211_auth_type auth_type,\n\t\t\t\t    enum nl80211_commands cmd)\n{\n\tif (auth_type > NL80211_AUTHTYPE_MAX)\n\t\treturn false;\n\n\tswitch (cmd) {\n\tcase NL80211_CMD_AUTHENTICATE:\n\t\tif (!(rdev->wiphy.features & NL80211_FEATURE_SAE) &&\n\t\t    auth_type == NL80211_AUTHTYPE_SAE)\n\t\t\treturn false;\n\t\tif (!wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t\t     NL80211_EXT_FEATURE_FILS_STA) &&\n\t\t    (auth_type == NL80211_AUTHTYPE_FILS_SK ||\n\t\t     auth_type == NL80211_AUTHTYPE_FILS_SK_PFS ||\n\t\t     auth_type == NL80211_AUTHTYPE_FILS_PK))\n\t\t\treturn false;\n\t\treturn true;\n\tcase NL80211_CMD_CONNECT:\n\t\tif (!(rdev->wiphy.features & NL80211_FEATURE_SAE) &&\n\t\t    !wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t\t     NL80211_EXT_FEATURE_SAE_OFFLOAD) &&\n\t\t    auth_type == NL80211_AUTHTYPE_SAE)\n\t\t\treturn false;\n\n\t\t/* FILS with SK PFS or PK not supported yet */\n\t\tif (auth_type == NL80211_AUTHTYPE_FILS_SK_PFS ||\n\t\t    auth_type == NL80211_AUTHTYPE_FILS_PK)\n\t\t\treturn false;\n\t\tif (!wiphy_ext_feature_isset(\n\t\t\t    &rdev->wiphy,\n\t\t\t    NL80211_EXT_FEATURE_FILS_SK_OFFLOAD) &&\n\t\t    auth_type == NL80211_AUTHTYPE_FILS_SK)\n\t\t\treturn false;\n\t\treturn true;\n\tcase NL80211_CMD_START_AP:\n\t\tif (!wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t\t     NL80211_EXT_FEATURE_SAE_OFFLOAD_AP) &&\n\t\t    auth_type == NL80211_AUTHTYPE_SAE)\n\t\t\treturn false;\n\t\t/* FILS not supported yet */\n\t\tif (auth_type == NL80211_AUTHTYPE_FILS_SK ||\n\t\t    auth_type == NL80211_AUTHTYPE_FILS_SK_PFS ||\n\t\t    auth_type == NL80211_AUTHTYPE_FILS_PK)\n\t\t\treturn false;\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic int nl80211_start_ap(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct cfg80211_ap_settings params;\n\tint err;\n\n\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_AP &&\n\t    dev->ieee80211_ptr->iftype != NL80211_IFTYPE_P2P_GO)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!rdev->ops->start_ap)\n\t\treturn -EOPNOTSUPP;\n\n\tif (wdev->beacon_interval)\n\t\treturn -EALREADY;\n\n\tmemset(&params, 0, sizeof(params));\n\n\t/* these are required for START_AP */\n\tif (!info->attrs[NL80211_ATTR_BEACON_INTERVAL] ||\n\t    !info->attrs[NL80211_ATTR_DTIM_PERIOD] ||\n\t    !info->attrs[NL80211_ATTR_BEACON_HEAD])\n\t\treturn -EINVAL;\n\n\terr = nl80211_parse_beacon(rdev, info->attrs, &params.beacon);\n\tif (err)\n\t\treturn err;\n\n\tparams.beacon_interval =\n\t\tnla_get_u32(info->attrs[NL80211_ATTR_BEACON_INTERVAL]);\n\tparams.dtim_period =\n\t\tnla_get_u32(info->attrs[NL80211_ATTR_DTIM_PERIOD]);\n\n\terr = cfg80211_validate_beacon_int(rdev, dev->ieee80211_ptr->iftype,\n\t\t\t\t\t   params.beacon_interval);\n\tif (err)\n\t\treturn err;\n\n\t/*\n\t * In theory, some of these attributes should be required here\n\t * but since they were not used when the command was originally\n\t * added, keep them optional for old user space programs to let\n\t * them continue to work with drivers that do not need the\n\t * additional information -- drivers must check!\n\t */\n\tif (info->attrs[NL80211_ATTR_SSID]) {\n\t\tparams.ssid = nla_data(info->attrs[NL80211_ATTR_SSID]);\n\t\tparams.ssid_len =\n\t\t\tnla_len(info->attrs[NL80211_ATTR_SSID]);\n\t\tif (params.ssid_len == 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_HIDDEN_SSID])\n\t\tparams.hidden_ssid = nla_get_u32(\n\t\t\tinfo->attrs[NL80211_ATTR_HIDDEN_SSID]);\n\n\tparams.privacy = !!info->attrs[NL80211_ATTR_PRIVACY];\n\n\tif (info->attrs[NL80211_ATTR_AUTH_TYPE]) {\n\t\tparams.auth_type = nla_get_u32(\n\t\t\tinfo->attrs[NL80211_ATTR_AUTH_TYPE]);\n\t\tif (!nl80211_valid_auth_type(rdev, params.auth_type,\n\t\t\t\t\t     NL80211_CMD_START_AP))\n\t\t\treturn -EINVAL;\n\t} else\n\t\tparams.auth_type = NL80211_AUTHTYPE_AUTOMATIC;\n\n\terr = nl80211_crypto_settings(rdev, info, &params.crypto,\n\t\t\t\t      NL80211_MAX_NR_CIPHER_SUITES);\n\tif (err)\n\t\treturn err;\n\n\tif (info->attrs[NL80211_ATTR_INACTIVITY_TIMEOUT]) {\n\t\tif (!(rdev->wiphy.features & NL80211_FEATURE_INACTIVITY_TIMER))\n\t\t\treturn -EOPNOTSUPP;\n\t\tparams.inactivity_timeout = nla_get_u16(\n\t\t\tinfo->attrs[NL80211_ATTR_INACTIVITY_TIMEOUT]);\n\t}\n\n\tif (info->attrs[NL80211_ATTR_P2P_CTWINDOW]) {\n\t\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_P2P_GO)\n\t\t\treturn -EINVAL;\n\t\tparams.p2p_ctwindow =\n\t\t\tnla_get_u8(info->attrs[NL80211_ATTR_P2P_CTWINDOW]);\n\t\tif (params.p2p_ctwindow != 0 &&\n\t\t    !(rdev->wiphy.features & NL80211_FEATURE_P2P_GO_CTWIN))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_P2P_OPPPS]) {\n\t\tu8 tmp;\n\n\t\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_P2P_GO)\n\t\t\treturn -EINVAL;\n\t\ttmp = nla_get_u8(info->attrs[NL80211_ATTR_P2P_OPPPS]);\n\t\tparams.p2p_opp_ps = tmp;\n\t\tif (params.p2p_opp_ps != 0 &&\n\t\t    !(rdev->wiphy.features & NL80211_FEATURE_P2P_GO_OPPPS))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_WIPHY_FREQ]) {\n\t\terr = nl80211_parse_chandef(rdev, info, &params.chandef);\n\t\tif (err)\n\t\t\treturn err;\n\t} else if (wdev->preset_chandef.chan) {\n\t\tparams.chandef = wdev->preset_chandef;\n\t} else if (!nl80211_get_ap_channel(rdev, &params))\n\t\treturn -EINVAL;\n\n\tif (!cfg80211_reg_can_beacon_relax(&rdev->wiphy, &params.chandef,\n\t\t\t\t\t   wdev->iftype))\n\t\treturn -EINVAL;\n\n\tif (info->attrs[NL80211_ATTR_TX_RATES]) {\n\t\terr = nl80211_parse_tx_bitrate_mask(info, info->attrs,\n\t\t\t\t\t\t    NL80211_ATTR_TX_RATES,\n\t\t\t\t\t\t    &params.beacon_rate,\n\t\t\t\t\t\t    dev, false);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\terr = validate_beacon_tx_rate(rdev, params.chandef.chan->band,\n\t\t\t\t\t      &params.beacon_rate);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_SMPS_MODE]) {\n\t\tparams.smps_mode =\n\t\t\tnla_get_u8(info->attrs[NL80211_ATTR_SMPS_MODE]);\n\t\tswitch (params.smps_mode) {\n\t\tcase NL80211_SMPS_OFF:\n\t\t\tbreak;\n\t\tcase NL80211_SMPS_STATIC:\n\t\t\tif (!(rdev->wiphy.features &\n\t\t\t      NL80211_FEATURE_STATIC_SMPS))\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase NL80211_SMPS_DYNAMIC:\n\t\t\tif (!(rdev->wiphy.features &\n\t\t\t      NL80211_FEATURE_DYNAMIC_SMPS))\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\tparams.smps_mode = NL80211_SMPS_OFF;\n\t}\n\n\tparams.pbss = nla_get_flag(info->attrs[NL80211_ATTR_PBSS]);\n\tif (params.pbss && !rdev->wiphy.bands[NL80211_BAND_60GHZ])\n\t\treturn -EOPNOTSUPP;\n\n\tif (info->attrs[NL80211_ATTR_ACL_POLICY]) {\n\t\tparams.acl = parse_acl_data(&rdev->wiphy, info);\n\t\tif (IS_ERR(params.acl))\n\t\t\treturn PTR_ERR(params.acl);\n\t}\n\n\tparams.twt_responder =\n\t\t    nla_get_flag(info->attrs[NL80211_ATTR_TWT_RESPONDER]);\n\n\tif (info->attrs[NL80211_ATTR_HE_OBSS_PD]) {\n\t\terr = nl80211_parse_he_obss_pd(\n\t\t\t\t\tinfo->attrs[NL80211_ATTR_HE_OBSS_PD],\n\t\t\t\t\t&params.he_obss_pd);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_HE_BSS_COLOR]) {\n\t\terr = nl80211_parse_he_bss_color(\n\t\t\t\t\tinfo->attrs[NL80211_ATTR_HE_BSS_COLOR],\n\t\t\t\t\t&params.he_bss_color);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_FILS_DISCOVERY]) {\n\t\terr = nl80211_parse_fils_discovery(rdev,\n\t\t\t\t\t\t   info->attrs[NL80211_ATTR_FILS_DISCOVERY],\n\t\t\t\t\t\t   &params);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_UNSOL_BCAST_PROBE_RESP]) {\n\t\terr = nl80211_parse_unsol_bcast_probe_resp(\n\t\t\trdev, info->attrs[NL80211_ATTR_UNSOL_BCAST_PROBE_RESP],\n\t\t\t&params);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tnl80211_calculate_ap_params(&params);\n\n\tif (info->attrs[NL80211_ATTR_EXTERNAL_AUTH_SUPPORT])\n\t\tparams.flags |= AP_SETTINGS_EXTERNAL_AUTH_SUPPORT;\n\n\twdev_lock(wdev);\n\terr = rdev_start_ap(rdev, dev, &params);\n\tif (!err) {\n\t\twdev->preset_chandef = params.chandef;\n\t\twdev->beacon_interval = params.beacon_interval;\n\t\twdev->chandef = params.chandef;\n\t\twdev->ssid_len = params.ssid_len;\n\t\tmemcpy(wdev->ssid, params.ssid, wdev->ssid_len);\n\n\t\tif (info->attrs[NL80211_ATTR_SOCKET_OWNER])\n\t\t\twdev->conn_owner_nlportid = info->snd_portid;\n\t}\n\twdev_unlock(wdev);\n\nout:\n\tkfree(params.acl);\n\n\treturn err;\n}\n\nstatic int nl80211_set_beacon(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct cfg80211_beacon_data params;\n\tint err;\n\n\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_AP &&\n\t    dev->ieee80211_ptr->iftype != NL80211_IFTYPE_P2P_GO)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!rdev->ops->change_beacon)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!wdev->beacon_interval)\n\t\treturn -EINVAL;\n\n\terr = nl80211_parse_beacon(rdev, info->attrs, &params);\n\tif (err)\n\t\treturn err;\n\n\twdev_lock(wdev);\n\terr = rdev_change_beacon(rdev, dev, &params);\n\twdev_unlock(wdev);\n\n\treturn err;\n}\n\nstatic int nl80211_stop_ap(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\n\treturn cfg80211_stop_ap(rdev, dev, false);\n}\n\nstatic const struct nla_policy sta_flags_policy[NL80211_STA_FLAG_MAX + 1] = {\n\t[NL80211_STA_FLAG_AUTHORIZED] = { .type = NLA_FLAG },\n\t[NL80211_STA_FLAG_SHORT_PREAMBLE] = { .type = NLA_FLAG },\n\t[NL80211_STA_FLAG_WME] = { .type = NLA_FLAG },\n\t[NL80211_STA_FLAG_MFP] = { .type = NLA_FLAG },\n\t[NL80211_STA_FLAG_AUTHENTICATED] = { .type = NLA_FLAG },\n\t[NL80211_STA_FLAG_TDLS_PEER] = { .type = NLA_FLAG },\n};\n\nstatic int parse_station_flags(struct genl_info *info,\n\t\t\t       enum nl80211_iftype iftype,\n\t\t\t       struct station_parameters *params)\n{\n\tstruct nlattr *flags[NL80211_STA_FLAG_MAX + 1];\n\tstruct nlattr *nla;\n\tint flag;\n\n\t/*\n\t * Try parsing the new attribute first so userspace\n\t * can specify both for older kernels.\n\t */\n\tnla = info->attrs[NL80211_ATTR_STA_FLAGS2];\n\tif (nla) {\n\t\tstruct nl80211_sta_flag_update *sta_flags;\n\n\t\tsta_flags = nla_data(nla);\n\t\tparams->sta_flags_mask = sta_flags->mask;\n\t\tparams->sta_flags_set = sta_flags->set;\n\t\tparams->sta_flags_set &= params->sta_flags_mask;\n\t\tif ((params->sta_flags_mask |\n\t\t     params->sta_flags_set) & BIT(__NL80211_STA_FLAG_INVALID))\n\t\t\treturn -EINVAL;\n\t\treturn 0;\n\t}\n\n\t/* if present, parse the old attribute */\n\n\tnla = info->attrs[NL80211_ATTR_STA_FLAGS];\n\tif (!nla)\n\t\treturn 0;\n\n\tif (nla_parse_nested_deprecated(flags, NL80211_STA_FLAG_MAX, nla, sta_flags_policy, info->extack))\n\t\treturn -EINVAL;\n\n\t/*\n\t * Only allow certain flags for interface types so that\n\t * other attributes are silently ignored. Remember that\n\t * this is backward compatibility code with old userspace\n\t * and shouldn't be hit in other cases anyway.\n\t */\n\tswitch (iftype) {\n\tcase NL80211_IFTYPE_AP:\n\tcase NL80211_IFTYPE_AP_VLAN:\n\tcase NL80211_IFTYPE_P2P_GO:\n\t\tparams->sta_flags_mask = BIT(NL80211_STA_FLAG_AUTHORIZED) |\n\t\t\t\t\t BIT(NL80211_STA_FLAG_SHORT_PREAMBLE) |\n\t\t\t\t\t BIT(NL80211_STA_FLAG_WME) |\n\t\t\t\t\t BIT(NL80211_STA_FLAG_MFP);\n\t\tbreak;\n\tcase NL80211_IFTYPE_P2P_CLIENT:\n\tcase NL80211_IFTYPE_STATION:\n\t\tparams->sta_flags_mask = BIT(NL80211_STA_FLAG_AUTHORIZED) |\n\t\t\t\t\t BIT(NL80211_STA_FLAG_TDLS_PEER);\n\t\tbreak;\n\tcase NL80211_IFTYPE_MESH_POINT:\n\t\tparams->sta_flags_mask = BIT(NL80211_STA_FLAG_AUTHENTICATED) |\n\t\t\t\t\t BIT(NL80211_STA_FLAG_MFP) |\n\t\t\t\t\t BIT(NL80211_STA_FLAG_AUTHORIZED);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tfor (flag = 1; flag <= NL80211_STA_FLAG_MAX; flag++) {\n\t\tif (flags[flag]) {\n\t\t\tparams->sta_flags_set |= (1<<flag);\n\n\t\t\t/* no longer support new API additions in old API */\n\t\t\tif (flag > NL80211_STA_FLAG_MAX_OLD_API)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nbool nl80211_put_sta_rate(struct sk_buff *msg, struct rate_info *info, int attr)\n{\n\tstruct nlattr *rate;\n\tu32 bitrate;\n\tu16 bitrate_compat;\n\tenum nl80211_rate_info rate_flg;\n\n\trate = nla_nest_start_noflag(msg, attr);\n\tif (!rate)\n\t\treturn false;\n\n\t/* cfg80211_calculate_bitrate will return 0 for mcs >= 32 */\n\tbitrate = cfg80211_calculate_bitrate(info);\n\t/* report 16-bit bitrate only if we can */\n\tbitrate_compat = bitrate < (1UL << 16) ? bitrate : 0;\n\tif (bitrate > 0 &&\n\t    nla_put_u32(msg, NL80211_RATE_INFO_BITRATE32, bitrate))\n\t\treturn false;\n\tif (bitrate_compat > 0 &&\n\t    nla_put_u16(msg, NL80211_RATE_INFO_BITRATE, bitrate_compat))\n\t\treturn false;\n\n\tswitch (info->bw) {\n\tcase RATE_INFO_BW_5:\n\t\trate_flg = NL80211_RATE_INFO_5_MHZ_WIDTH;\n\t\tbreak;\n\tcase RATE_INFO_BW_10:\n\t\trate_flg = NL80211_RATE_INFO_10_MHZ_WIDTH;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t\tfallthrough;\n\tcase RATE_INFO_BW_20:\n\t\trate_flg = 0;\n\t\tbreak;\n\tcase RATE_INFO_BW_40:\n\t\trate_flg = NL80211_RATE_INFO_40_MHZ_WIDTH;\n\t\tbreak;\n\tcase RATE_INFO_BW_80:\n\t\trate_flg = NL80211_RATE_INFO_80_MHZ_WIDTH;\n\t\tbreak;\n\tcase RATE_INFO_BW_160:\n\t\trate_flg = NL80211_RATE_INFO_160_MHZ_WIDTH;\n\t\tbreak;\n\tcase RATE_INFO_BW_HE_RU:\n\t\trate_flg = 0;\n\t\tWARN_ON(!(info->flags & RATE_INFO_FLAGS_HE_MCS));\n\t}\n\n\tif (rate_flg && nla_put_flag(msg, rate_flg))\n\t\treturn false;\n\n\tif (info->flags & RATE_INFO_FLAGS_MCS) {\n\t\tif (nla_put_u8(msg, NL80211_RATE_INFO_MCS, info->mcs))\n\t\t\treturn false;\n\t\tif (info->flags & RATE_INFO_FLAGS_SHORT_GI &&\n\t\t    nla_put_flag(msg, NL80211_RATE_INFO_SHORT_GI))\n\t\t\treturn false;\n\t} else if (info->flags & RATE_INFO_FLAGS_VHT_MCS) {\n\t\tif (nla_put_u8(msg, NL80211_RATE_INFO_VHT_MCS, info->mcs))\n\t\t\treturn false;\n\t\tif (nla_put_u8(msg, NL80211_RATE_INFO_VHT_NSS, info->nss))\n\t\t\treturn false;\n\t\tif (info->flags & RATE_INFO_FLAGS_SHORT_GI &&\n\t\t    nla_put_flag(msg, NL80211_RATE_INFO_SHORT_GI))\n\t\t\treturn false;\n\t} else if (info->flags & RATE_INFO_FLAGS_HE_MCS) {\n\t\tif (nla_put_u8(msg, NL80211_RATE_INFO_HE_MCS, info->mcs))\n\t\t\treturn false;\n\t\tif (nla_put_u8(msg, NL80211_RATE_INFO_HE_NSS, info->nss))\n\t\t\treturn false;\n\t\tif (nla_put_u8(msg, NL80211_RATE_INFO_HE_GI, info->he_gi))\n\t\t\treturn false;\n\t\tif (nla_put_u8(msg, NL80211_RATE_INFO_HE_DCM, info->he_dcm))\n\t\t\treturn false;\n\t\tif (info->bw == RATE_INFO_BW_HE_RU &&\n\t\t    nla_put_u8(msg, NL80211_RATE_INFO_HE_RU_ALLOC,\n\t\t\t       info->he_ru_alloc))\n\t\t\treturn false;\n\t}\n\n\tnla_nest_end(msg, rate);\n\treturn true;\n}\n\nstatic bool nl80211_put_signal(struct sk_buff *msg, u8 mask, s8 *signal,\n\t\t\t       int id)\n{\n\tvoid *attr;\n\tint i = 0;\n\n\tif (!mask)\n\t\treturn true;\n\n\tattr = nla_nest_start_noflag(msg, id);\n\tif (!attr)\n\t\treturn false;\n\n\tfor (i = 0; i < IEEE80211_MAX_CHAINS; i++) {\n\t\tif (!(mask & BIT(i)))\n\t\t\tcontinue;\n\n\t\tif (nla_put_u8(msg, i, signal[i]))\n\t\t\treturn false;\n\t}\n\n\tnla_nest_end(msg, attr);\n\n\treturn true;\n}\n\nstatic int nl80211_send_station(struct sk_buff *msg, u32 cmd, u32 portid,\n\t\t\t\tu32 seq, int flags,\n\t\t\t\tstruct cfg80211_registered_device *rdev,\n\t\t\t\tstruct net_device *dev,\n\t\t\t\tconst u8 *mac_addr, struct station_info *sinfo)\n{\n\tvoid *hdr;\n\tstruct nlattr *sinfoattr, *bss_param;\n\n\thdr = nl80211hdr_put(msg, portid, seq, flags, cmd);\n\tif (!hdr) {\n\t\tcfg80211_sinfo_release_content(sinfo);\n\t\treturn -1;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_IFINDEX, dev->ifindex) ||\n\t    nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, mac_addr) ||\n\t    nla_put_u32(msg, NL80211_ATTR_GENERATION, sinfo->generation))\n\t\tgoto nla_put_failure;\n\n\tsinfoattr = nla_nest_start_noflag(msg, NL80211_ATTR_STA_INFO);\n\tif (!sinfoattr)\n\t\tgoto nla_put_failure;\n\n#define PUT_SINFO(attr, memb, type) do {\t\t\t\t\\\n\tBUILD_BUG_ON(sizeof(type) == sizeof(u64));\t\t\t\\\n\tif (sinfo->filled & BIT_ULL(NL80211_STA_INFO_ ## attr) &&\t\\\n\t    nla_put_ ## type(msg, NL80211_STA_INFO_ ## attr,\t\t\\\n\t\t\t     sinfo->memb))\t\t\t\t\\\n\t\tgoto nla_put_failure;\t\t\t\t\t\\\n\t} while (0)\n#define PUT_SINFO_U64(attr, memb) do {\t\t\t\t\t\\\n\tif (sinfo->filled & BIT_ULL(NL80211_STA_INFO_ ## attr) &&\t\\\n\t    nla_put_u64_64bit(msg, NL80211_STA_INFO_ ## attr,\t\t\\\n\t\t\t      sinfo->memb, NL80211_STA_INFO_PAD))\t\\\n\t\tgoto nla_put_failure;\t\t\t\t\t\\\n\t} while (0)\n\n\tPUT_SINFO(CONNECTED_TIME, connected_time, u32);\n\tPUT_SINFO(INACTIVE_TIME, inactive_time, u32);\n\tPUT_SINFO_U64(ASSOC_AT_BOOTTIME, assoc_at);\n\n\tif (sinfo->filled & (BIT_ULL(NL80211_STA_INFO_RX_BYTES) |\n\t\t\t     BIT_ULL(NL80211_STA_INFO_RX_BYTES64)) &&\n\t    nla_put_u32(msg, NL80211_STA_INFO_RX_BYTES,\n\t\t\t(u32)sinfo->rx_bytes))\n\t\tgoto nla_put_failure;\n\n\tif (sinfo->filled & (BIT_ULL(NL80211_STA_INFO_TX_BYTES) |\n\t\t\t     BIT_ULL(NL80211_STA_INFO_TX_BYTES64)) &&\n\t    nla_put_u32(msg, NL80211_STA_INFO_TX_BYTES,\n\t\t\t(u32)sinfo->tx_bytes))\n\t\tgoto nla_put_failure;\n\n\tPUT_SINFO_U64(RX_BYTES64, rx_bytes);\n\tPUT_SINFO_U64(TX_BYTES64, tx_bytes);\n\tPUT_SINFO(LLID, llid, u16);\n\tPUT_SINFO(PLID, plid, u16);\n\tPUT_SINFO(PLINK_STATE, plink_state, u8);\n\tPUT_SINFO_U64(RX_DURATION, rx_duration);\n\tPUT_SINFO_U64(TX_DURATION, tx_duration);\n\n\tif (wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t    NL80211_EXT_FEATURE_AIRTIME_FAIRNESS))\n\t\tPUT_SINFO(AIRTIME_WEIGHT, airtime_weight, u16);\n\n\tswitch (rdev->wiphy.signal_type) {\n\tcase CFG80211_SIGNAL_TYPE_MBM:\n\t\tPUT_SINFO(SIGNAL, signal, u8);\n\t\tPUT_SINFO(SIGNAL_AVG, signal_avg, u8);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tif (sinfo->filled & BIT_ULL(NL80211_STA_INFO_CHAIN_SIGNAL)) {\n\t\tif (!nl80211_put_signal(msg, sinfo->chains,\n\t\t\t\t\tsinfo->chain_signal,\n\t\t\t\t\tNL80211_STA_INFO_CHAIN_SIGNAL))\n\t\t\tgoto nla_put_failure;\n\t}\n\tif (sinfo->filled & BIT_ULL(NL80211_STA_INFO_CHAIN_SIGNAL_AVG)) {\n\t\tif (!nl80211_put_signal(msg, sinfo->chains,\n\t\t\t\t\tsinfo->chain_signal_avg,\n\t\t\t\t\tNL80211_STA_INFO_CHAIN_SIGNAL_AVG))\n\t\t\tgoto nla_put_failure;\n\t}\n\tif (sinfo->filled & BIT_ULL(NL80211_STA_INFO_TX_BITRATE)) {\n\t\tif (!nl80211_put_sta_rate(msg, &sinfo->txrate,\n\t\t\t\t\t  NL80211_STA_INFO_TX_BITRATE))\n\t\t\tgoto nla_put_failure;\n\t}\n\tif (sinfo->filled & BIT_ULL(NL80211_STA_INFO_RX_BITRATE)) {\n\t\tif (!nl80211_put_sta_rate(msg, &sinfo->rxrate,\n\t\t\t\t\t  NL80211_STA_INFO_RX_BITRATE))\n\t\t\tgoto nla_put_failure;\n\t}\n\n\tPUT_SINFO(RX_PACKETS, rx_packets, u32);\n\tPUT_SINFO(TX_PACKETS, tx_packets, u32);\n\tPUT_SINFO(TX_RETRIES, tx_retries, u32);\n\tPUT_SINFO(TX_FAILED, tx_failed, u32);\n\tPUT_SINFO(EXPECTED_THROUGHPUT, expected_throughput, u32);\n\tPUT_SINFO(AIRTIME_LINK_METRIC, airtime_link_metric, u32);\n\tPUT_SINFO(BEACON_LOSS, beacon_loss_count, u32);\n\tPUT_SINFO(LOCAL_PM, local_pm, u32);\n\tPUT_SINFO(PEER_PM, peer_pm, u32);\n\tPUT_SINFO(NONPEER_PM, nonpeer_pm, u32);\n\tPUT_SINFO(CONNECTED_TO_GATE, connected_to_gate, u8);\n\tPUT_SINFO(CONNECTED_TO_AS, connected_to_as, u8);\n\n\tif (sinfo->filled & BIT_ULL(NL80211_STA_INFO_BSS_PARAM)) {\n\t\tbss_param = nla_nest_start_noflag(msg,\n\t\t\t\t\t\t  NL80211_STA_INFO_BSS_PARAM);\n\t\tif (!bss_param)\n\t\t\tgoto nla_put_failure;\n\n\t\tif (((sinfo->bss_param.flags & BSS_PARAM_FLAGS_CTS_PROT) &&\n\t\t     nla_put_flag(msg, NL80211_STA_BSS_PARAM_CTS_PROT)) ||\n\t\t    ((sinfo->bss_param.flags & BSS_PARAM_FLAGS_SHORT_PREAMBLE) &&\n\t\t     nla_put_flag(msg, NL80211_STA_BSS_PARAM_SHORT_PREAMBLE)) ||\n\t\t    ((sinfo->bss_param.flags & BSS_PARAM_FLAGS_SHORT_SLOT_TIME) &&\n\t\t     nla_put_flag(msg, NL80211_STA_BSS_PARAM_SHORT_SLOT_TIME)) ||\n\t\t    nla_put_u8(msg, NL80211_STA_BSS_PARAM_DTIM_PERIOD,\n\t\t\t       sinfo->bss_param.dtim_period) ||\n\t\t    nla_put_u16(msg, NL80211_STA_BSS_PARAM_BEACON_INTERVAL,\n\t\t\t\tsinfo->bss_param.beacon_interval))\n\t\t\tgoto nla_put_failure;\n\n\t\tnla_nest_end(msg, bss_param);\n\t}\n\tif ((sinfo->filled & BIT_ULL(NL80211_STA_INFO_STA_FLAGS)) &&\n\t    nla_put(msg, NL80211_STA_INFO_STA_FLAGS,\n\t\t    sizeof(struct nl80211_sta_flag_update),\n\t\t    &sinfo->sta_flags))\n\t\tgoto nla_put_failure;\n\n\tPUT_SINFO_U64(T_OFFSET, t_offset);\n\tPUT_SINFO_U64(RX_DROP_MISC, rx_dropped_misc);\n\tPUT_SINFO_U64(BEACON_RX, rx_beacon);\n\tPUT_SINFO(BEACON_SIGNAL_AVG, rx_beacon_signal_avg, u8);\n\tPUT_SINFO(RX_MPDUS, rx_mpdu_count, u32);\n\tPUT_SINFO(FCS_ERROR_COUNT, fcs_err_count, u32);\n\tif (wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t    NL80211_EXT_FEATURE_ACK_SIGNAL_SUPPORT)) {\n\t\tPUT_SINFO(ACK_SIGNAL, ack_signal, u8);\n\t\tPUT_SINFO(ACK_SIGNAL_AVG, avg_ack_signal, s8);\n\t}\n\n#undef PUT_SINFO\n#undef PUT_SINFO_U64\n\n\tif (sinfo->pertid) {\n\t\tstruct nlattr *tidsattr;\n\t\tint tid;\n\n\t\ttidsattr = nla_nest_start_noflag(msg,\n\t\t\t\t\t\t NL80211_STA_INFO_TID_STATS);\n\t\tif (!tidsattr)\n\t\t\tgoto nla_put_failure;\n\n\t\tfor (tid = 0; tid < IEEE80211_NUM_TIDS + 1; tid++) {\n\t\t\tstruct cfg80211_tid_stats *tidstats;\n\t\t\tstruct nlattr *tidattr;\n\n\t\t\ttidstats = &sinfo->pertid[tid];\n\n\t\t\tif (!tidstats->filled)\n\t\t\t\tcontinue;\n\n\t\t\ttidattr = nla_nest_start_noflag(msg, tid + 1);\n\t\t\tif (!tidattr)\n\t\t\t\tgoto nla_put_failure;\n\n#define PUT_TIDVAL_U64(attr, memb) do {\t\t\t\t\t\\\n\tif (tidstats->filled & BIT(NL80211_TID_STATS_ ## attr) &&\t\\\n\t    nla_put_u64_64bit(msg, NL80211_TID_STATS_ ## attr,\t\t\\\n\t\t\t      tidstats->memb, NL80211_TID_STATS_PAD))\t\\\n\t\tgoto nla_put_failure;\t\t\t\t\t\\\n\t} while (0)\n\n\t\t\tPUT_TIDVAL_U64(RX_MSDU, rx_msdu);\n\t\t\tPUT_TIDVAL_U64(TX_MSDU, tx_msdu);\n\t\t\tPUT_TIDVAL_U64(TX_MSDU_RETRIES, tx_msdu_retries);\n\t\t\tPUT_TIDVAL_U64(TX_MSDU_FAILED, tx_msdu_failed);\n\n#undef PUT_TIDVAL_U64\n\t\t\tif ((tidstats->filled &\n\t\t\t     BIT(NL80211_TID_STATS_TXQ_STATS)) &&\n\t\t\t    !nl80211_put_txq_stats(msg, &tidstats->txq_stats,\n\t\t\t\t\t\t   NL80211_TID_STATS_TXQ_STATS))\n\t\t\t\tgoto nla_put_failure;\n\n\t\t\tnla_nest_end(msg, tidattr);\n\t\t}\n\n\t\tnla_nest_end(msg, tidsattr);\n\t}\n\n\tnla_nest_end(msg, sinfoattr);\n\n\tif (sinfo->assoc_req_ies_len &&\n\t    nla_put(msg, NL80211_ATTR_IE, sinfo->assoc_req_ies_len,\n\t\t    sinfo->assoc_req_ies))\n\t\tgoto nla_put_failure;\n\n\tcfg80211_sinfo_release_content(sinfo);\n\tgenlmsg_end(msg, hdr);\n\treturn 0;\n\n nla_put_failure:\n\tcfg80211_sinfo_release_content(sinfo);\n\tgenlmsg_cancel(msg, hdr);\n\treturn -EMSGSIZE;\n}\n\nstatic int nl80211_dump_station(struct sk_buff *skb,\n\t\t\t\tstruct netlink_callback *cb)\n{\n\tstruct station_info sinfo;\n\tstruct cfg80211_registered_device *rdev;\n\tstruct wireless_dev *wdev;\n\tu8 mac_addr[ETH_ALEN];\n\tint sta_idx = cb->args[2];\n\tint err;\n\n\terr = nl80211_prepare_wdev_dump(cb, &rdev, &wdev);\n\tif (err)\n\t\treturn err;\n\t/* nl80211_prepare_wdev_dump acquired it in the successful case */\n\t__acquire(&rdev->wiphy.mtx);\n\n\tif (!wdev->netdev) {\n\t\terr = -EINVAL;\n\t\tgoto out_err;\n\t}\n\n\tif (!rdev->ops->dump_station) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out_err;\n\t}\n\n\twhile (1) {\n\t\tmemset(&sinfo, 0, sizeof(sinfo));\n\t\terr = rdev_dump_station(rdev, wdev->netdev, sta_idx,\n\t\t\t\t\tmac_addr, &sinfo);\n\t\tif (err == -ENOENT)\n\t\t\tbreak;\n\t\tif (err)\n\t\t\tgoto out_err;\n\n\t\tif (nl80211_send_station(skb, NL80211_CMD_NEW_STATION,\n\t\t\t\tNETLINK_CB(cb->skb).portid,\n\t\t\t\tcb->nlh->nlmsg_seq, NLM_F_MULTI,\n\t\t\t\trdev, wdev->netdev, mac_addr,\n\t\t\t\t&sinfo) < 0)\n\t\t\tgoto out;\n\n\t\tsta_idx++;\n\t}\n\n out:\n\tcb->args[2] = sta_idx;\n\terr = skb->len;\n out_err:\n\twiphy_unlock(&rdev->wiphy);\n\n\treturn err;\n}\n\nstatic int nl80211_get_station(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct station_info sinfo;\n\tstruct sk_buff *msg;\n\tu8 *mac_addr = NULL;\n\tint err;\n\n\tmemset(&sinfo, 0, sizeof(sinfo));\n\n\tif (!info->attrs[NL80211_ATTR_MAC])\n\t\treturn -EINVAL;\n\n\tmac_addr = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\n\tif (!rdev->ops->get_station)\n\t\treturn -EOPNOTSUPP;\n\n\terr = rdev_get_station(rdev, dev, mac_addr, &sinfo);\n\tif (err)\n\t\treturn err;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg) {\n\t\tcfg80211_sinfo_release_content(&sinfo);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (nl80211_send_station(msg, NL80211_CMD_NEW_STATION,\n\t\t\t\t info->snd_portid, info->snd_seq, 0,\n\t\t\t\t rdev, dev, mac_addr, &sinfo) < 0) {\n\t\tnlmsg_free(msg);\n\t\treturn -ENOBUFS;\n\t}\n\n\treturn genlmsg_reply(msg, info);\n}\n\nint cfg80211_check_station_change(struct wiphy *wiphy,\n\t\t\t\t  struct station_parameters *params,\n\t\t\t\t  enum cfg80211_station_type statype)\n{\n\tif (params->listen_interval != -1 &&\n\t    statype != CFG80211_STA_AP_CLIENT_UNASSOC)\n\t\treturn -EINVAL;\n\n\tif (params->support_p2p_ps != -1 &&\n\t    statype != CFG80211_STA_AP_CLIENT_UNASSOC)\n\t\treturn -EINVAL;\n\n\tif (params->aid &&\n\t    !(params->sta_flags_set & BIT(NL80211_STA_FLAG_TDLS_PEER)) &&\n\t    statype != CFG80211_STA_AP_CLIENT_UNASSOC)\n\t\treturn -EINVAL;\n\n\t/* When you run into this, adjust the code below for the new flag */\n\tBUILD_BUG_ON(NL80211_STA_FLAG_MAX != 7);\n\n\tswitch (statype) {\n\tcase CFG80211_STA_MESH_PEER_KERNEL:\n\tcase CFG80211_STA_MESH_PEER_USER:\n\t\t/*\n\t\t * No ignoring the TDLS flag here -- the userspace mesh\n\t\t * code doesn't have the bug of including TDLS in the\n\t\t * mask everywhere.\n\t\t */\n\t\tif (params->sta_flags_mask &\n\t\t\t\t~(BIT(NL80211_STA_FLAG_AUTHENTICATED) |\n\t\t\t\t  BIT(NL80211_STA_FLAG_MFP) |\n\t\t\t\t  BIT(NL80211_STA_FLAG_AUTHORIZED)))\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase CFG80211_STA_TDLS_PEER_SETUP:\n\tcase CFG80211_STA_TDLS_PEER_ACTIVE:\n\t\tif (!(params->sta_flags_set & BIT(NL80211_STA_FLAG_TDLS_PEER)))\n\t\t\treturn -EINVAL;\n\t\t/* ignore since it can't change */\n\t\tparams->sta_flags_mask &= ~BIT(NL80211_STA_FLAG_TDLS_PEER);\n\t\tbreak;\n\tdefault:\n\t\t/* disallow mesh-specific things */\n\t\tif (params->plink_action != NL80211_PLINK_ACTION_NO_ACTION)\n\t\t\treturn -EINVAL;\n\t\tif (params->local_pm)\n\t\t\treturn -EINVAL;\n\t\tif (params->sta_modify_mask & STATION_PARAM_APPLY_PLINK_STATE)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (statype != CFG80211_STA_TDLS_PEER_SETUP &&\n\t    statype != CFG80211_STA_TDLS_PEER_ACTIVE) {\n\t\t/* TDLS can't be set, ... */\n\t\tif (params->sta_flags_set & BIT(NL80211_STA_FLAG_TDLS_PEER))\n\t\t\treturn -EINVAL;\n\t\t/*\n\t\t * ... but don't bother the driver with it. This works around\n\t\t * a hostapd/wpa_supplicant issue -- it always includes the\n\t\t * TLDS_PEER flag in the mask even for AP mode.\n\t\t */\n\t\tparams->sta_flags_mask &= ~BIT(NL80211_STA_FLAG_TDLS_PEER);\n\t}\n\n\tif (statype != CFG80211_STA_TDLS_PEER_SETUP &&\n\t    statype != CFG80211_STA_AP_CLIENT_UNASSOC) {\n\t\t/* reject other things that can't change */\n\t\tif (params->sta_modify_mask & STATION_PARAM_APPLY_UAPSD)\n\t\t\treturn -EINVAL;\n\t\tif (params->sta_modify_mask & STATION_PARAM_APPLY_CAPABILITY)\n\t\t\treturn -EINVAL;\n\t\tif (params->supported_rates)\n\t\t\treturn -EINVAL;\n\t\tif (params->ext_capab || params->ht_capa || params->vht_capa ||\n\t\t    params->he_capa)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (statype != CFG80211_STA_AP_CLIENT &&\n\t    statype != CFG80211_STA_AP_CLIENT_UNASSOC) {\n\t\tif (params->vlan)\n\t\t\treturn -EINVAL;\n\t}\n\n\tswitch (statype) {\n\tcase CFG80211_STA_AP_MLME_CLIENT:\n\t\t/* Use this only for authorizing/unauthorizing a station */\n\t\tif (!(params->sta_flags_mask & BIT(NL80211_STA_FLAG_AUTHORIZED)))\n\t\t\treturn -EOPNOTSUPP;\n\t\tbreak;\n\tcase CFG80211_STA_AP_CLIENT:\n\tcase CFG80211_STA_AP_CLIENT_UNASSOC:\n\t\t/* accept only the listed bits */\n\t\tif (params->sta_flags_mask &\n\t\t\t\t~(BIT(NL80211_STA_FLAG_AUTHORIZED) |\n\t\t\t\t  BIT(NL80211_STA_FLAG_AUTHENTICATED) |\n\t\t\t\t  BIT(NL80211_STA_FLAG_ASSOCIATED) |\n\t\t\t\t  BIT(NL80211_STA_FLAG_SHORT_PREAMBLE) |\n\t\t\t\t  BIT(NL80211_STA_FLAG_WME) |\n\t\t\t\t  BIT(NL80211_STA_FLAG_MFP)))\n\t\t\treturn -EINVAL;\n\n\t\t/* but authenticated/associated only if driver handles it */\n\t\tif (!(wiphy->features & NL80211_FEATURE_FULL_AP_CLIENT_STATE) &&\n\t\t    params->sta_flags_mask &\n\t\t\t\t(BIT(NL80211_STA_FLAG_AUTHENTICATED) |\n\t\t\t\t BIT(NL80211_STA_FLAG_ASSOCIATED)))\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase CFG80211_STA_IBSS:\n\tcase CFG80211_STA_AP_STA:\n\t\t/* reject any changes other than AUTHORIZED */\n\t\tif (params->sta_flags_mask & ~BIT(NL80211_STA_FLAG_AUTHORIZED))\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase CFG80211_STA_TDLS_PEER_SETUP:\n\t\t/* reject any changes other than AUTHORIZED or WME */\n\t\tif (params->sta_flags_mask & ~(BIT(NL80211_STA_FLAG_AUTHORIZED) |\n\t\t\t\t\t       BIT(NL80211_STA_FLAG_WME)))\n\t\t\treturn -EINVAL;\n\t\t/* force (at least) rates when authorizing */\n\t\tif (params->sta_flags_set & BIT(NL80211_STA_FLAG_AUTHORIZED) &&\n\t\t    !params->supported_rates)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase CFG80211_STA_TDLS_PEER_ACTIVE:\n\t\t/* reject any changes */\n\t\treturn -EINVAL;\n\tcase CFG80211_STA_MESH_PEER_KERNEL:\n\t\tif (params->sta_modify_mask & STATION_PARAM_APPLY_PLINK_STATE)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase CFG80211_STA_MESH_PEER_USER:\n\t\tif (params->plink_action != NL80211_PLINK_ACTION_NO_ACTION &&\n\t\t    params->plink_action != NL80211_PLINK_ACTION_BLOCK)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\t}\n\n\t/*\n\t * Older kernel versions ignored this attribute entirely, so don't\n\t * reject attempts to update it but mark it as unused instead so the\n\t * driver won't look at the data.\n\t */\n\tif (statype != CFG80211_STA_AP_CLIENT_UNASSOC &&\n\t    statype != CFG80211_STA_TDLS_PEER_SETUP)\n\t\tparams->opmode_notif_used = false;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(cfg80211_check_station_change);\n\n/*\n * Get vlan interface making sure it is running and on the right wiphy.\n */\nstatic struct net_device *get_vlan(struct genl_info *info,\n\t\t\t\t   struct cfg80211_registered_device *rdev)\n{\n\tstruct nlattr *vlanattr = info->attrs[NL80211_ATTR_STA_VLAN];\n\tstruct net_device *v;\n\tint ret;\n\n\tif (!vlanattr)\n\t\treturn NULL;\n\n\tv = dev_get_by_index(genl_info_net(info), nla_get_u32(vlanattr));\n\tif (!v)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tif (!v->ieee80211_ptr || v->ieee80211_ptr->wiphy != &rdev->wiphy) {\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\n\tif (v->ieee80211_ptr->iftype != NL80211_IFTYPE_AP_VLAN &&\n\t    v->ieee80211_ptr->iftype != NL80211_IFTYPE_AP &&\n\t    v->ieee80211_ptr->iftype != NL80211_IFTYPE_P2P_GO) {\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\n\tif (!netif_running(v)) {\n\t\tret = -ENETDOWN;\n\t\tgoto error;\n\t}\n\n\treturn v;\n error:\n\tdev_put(v);\n\treturn ERR_PTR(ret);\n}\n\nstatic const struct nla_policy\nnl80211_sta_wme_policy[NL80211_STA_WME_MAX + 1] = {\n\t[NL80211_STA_WME_UAPSD_QUEUES] = { .type = NLA_U8 },\n\t[NL80211_STA_WME_MAX_SP] = { .type = NLA_U8 },\n};\n\nstatic int nl80211_parse_sta_wme(struct genl_info *info,\n\t\t\t\t struct station_parameters *params)\n{\n\tstruct nlattr *tb[NL80211_STA_WME_MAX + 1];\n\tstruct nlattr *nla;\n\tint err;\n\n\t/* parse WME attributes if present */\n\tif (!info->attrs[NL80211_ATTR_STA_WME])\n\t\treturn 0;\n\n\tnla = info->attrs[NL80211_ATTR_STA_WME];\n\terr = nla_parse_nested_deprecated(tb, NL80211_STA_WME_MAX, nla,\n\t\t\t\t\t  nl80211_sta_wme_policy,\n\t\t\t\t\t  info->extack);\n\tif (err)\n\t\treturn err;\n\n\tif (tb[NL80211_STA_WME_UAPSD_QUEUES])\n\t\tparams->uapsd_queues = nla_get_u8(\n\t\t\ttb[NL80211_STA_WME_UAPSD_QUEUES]);\n\tif (params->uapsd_queues & ~IEEE80211_WMM_IE_STA_QOSINFO_AC_MASK)\n\t\treturn -EINVAL;\n\n\tif (tb[NL80211_STA_WME_MAX_SP])\n\t\tparams->max_sp = nla_get_u8(tb[NL80211_STA_WME_MAX_SP]);\n\n\tif (params->max_sp & ~IEEE80211_WMM_IE_STA_QOSINFO_SP_MASK)\n\t\treturn -EINVAL;\n\n\tparams->sta_modify_mask |= STATION_PARAM_APPLY_UAPSD;\n\n\treturn 0;\n}\n\nstatic int nl80211_parse_sta_channel_info(struct genl_info *info,\n\t\t\t\t      struct station_parameters *params)\n{\n\tif (info->attrs[NL80211_ATTR_STA_SUPPORTED_CHANNELS]) {\n\t\tparams->supported_channels =\n\t\t     nla_data(info->attrs[NL80211_ATTR_STA_SUPPORTED_CHANNELS]);\n\t\tparams->supported_channels_len =\n\t\t     nla_len(info->attrs[NL80211_ATTR_STA_SUPPORTED_CHANNELS]);\n\t\t/*\n\t\t * Need to include at least one (first channel, number of\n\t\t * channels) tuple for each subband (checked in policy),\n\t\t * and must have proper tuples for the rest of the data as well.\n\t\t */\n\t\tif (params->supported_channels_len % 2)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_STA_SUPPORTED_OPER_CLASSES]) {\n\t\tparams->supported_oper_classes =\n\t\t nla_data(info->attrs[NL80211_ATTR_STA_SUPPORTED_OPER_CLASSES]);\n\t\tparams->supported_oper_classes_len =\n\t\t  nla_len(info->attrs[NL80211_ATTR_STA_SUPPORTED_OPER_CLASSES]);\n\t}\n\treturn 0;\n}\n\nstatic int nl80211_set_station_tdls(struct genl_info *info,\n\t\t\t\t    struct station_parameters *params)\n{\n\tint err;\n\t/* Dummy STA entry gets updated once the peer capabilities are known */\n\tif (info->attrs[NL80211_ATTR_PEER_AID])\n\t\tparams->aid = nla_get_u16(info->attrs[NL80211_ATTR_PEER_AID]);\n\tif (info->attrs[NL80211_ATTR_HT_CAPABILITY])\n\t\tparams->ht_capa =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_HT_CAPABILITY]);\n\tif (info->attrs[NL80211_ATTR_VHT_CAPABILITY])\n\t\tparams->vht_capa =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_VHT_CAPABILITY]);\n\tif (info->attrs[NL80211_ATTR_HE_CAPABILITY]) {\n\t\tparams->he_capa =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_HE_CAPABILITY]);\n\t\tparams->he_capa_len =\n\t\t\tnla_len(info->attrs[NL80211_ATTR_HE_CAPABILITY]);\n\t}\n\n\terr = nl80211_parse_sta_channel_info(info, params);\n\tif (err)\n\t\treturn err;\n\n\treturn nl80211_parse_sta_wme(info, params);\n}\n\nstatic int nl80211_parse_sta_txpower_setting(struct genl_info *info,\n\t\t\t\t\t     struct station_parameters *params)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tint idx;\n\n\tif (info->attrs[NL80211_ATTR_STA_TX_POWER_SETTING]) {\n\t\tif (!rdev->ops->set_tx_power ||\n\t\t    !wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t\t NL80211_EXT_FEATURE_STA_TX_PWR))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tidx = NL80211_ATTR_STA_TX_POWER_SETTING;\n\t\tparams->txpwr.type = nla_get_u8(info->attrs[idx]);\n\n\t\tif (params->txpwr.type == NL80211_TX_POWER_LIMITED) {\n\t\t\tidx = NL80211_ATTR_STA_TX_POWER;\n\n\t\t\tif (info->attrs[idx])\n\t\t\t\tparams->txpwr.power =\n\t\t\t\t\tnla_get_s16(info->attrs[idx]);\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tparams->sta_modify_mask |= STATION_PARAM_APPLY_STA_TXPOWER;\n\t}\n\n\treturn 0;\n}\n\nstatic int nl80211_set_station(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct station_parameters params;\n\tu8 *mac_addr;\n\tint err;\n\n\tmemset(&params, 0, sizeof(params));\n\n\tif (!rdev->ops->change_station)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t * AID and listen_interval properties can be set only for unassociated\n\t * station. Include these parameters here and will check them in\n\t * cfg80211_check_station_change().\n\t */\n\tif (info->attrs[NL80211_ATTR_STA_AID])\n\t\tparams.aid = nla_get_u16(info->attrs[NL80211_ATTR_STA_AID]);\n\n\tif (info->attrs[NL80211_ATTR_VLAN_ID])\n\t\tparams.vlan_id = nla_get_u16(info->attrs[NL80211_ATTR_VLAN_ID]);\n\n\tif (info->attrs[NL80211_ATTR_STA_LISTEN_INTERVAL])\n\t\tparams.listen_interval =\n\t\t     nla_get_u16(info->attrs[NL80211_ATTR_STA_LISTEN_INTERVAL]);\n\telse\n\t\tparams.listen_interval = -1;\n\n\tif (info->attrs[NL80211_ATTR_STA_SUPPORT_P2P_PS])\n\t\tparams.support_p2p_ps =\n\t\t\tnla_get_u8(info->attrs[NL80211_ATTR_STA_SUPPORT_P2P_PS]);\n\telse\n\t\tparams.support_p2p_ps = -1;\n\n\tif (!info->attrs[NL80211_ATTR_MAC])\n\t\treturn -EINVAL;\n\n\tmac_addr = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\n\tif (info->attrs[NL80211_ATTR_STA_SUPPORTED_RATES]) {\n\t\tparams.supported_rates =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_STA_SUPPORTED_RATES]);\n\t\tparams.supported_rates_len =\n\t\t\tnla_len(info->attrs[NL80211_ATTR_STA_SUPPORTED_RATES]);\n\t}\n\n\tif (info->attrs[NL80211_ATTR_STA_CAPABILITY]) {\n\t\tparams.capability =\n\t\t\tnla_get_u16(info->attrs[NL80211_ATTR_STA_CAPABILITY]);\n\t\tparams.sta_modify_mask |= STATION_PARAM_APPLY_CAPABILITY;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_STA_EXT_CAPABILITY]) {\n\t\tparams.ext_capab =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_STA_EXT_CAPABILITY]);\n\t\tparams.ext_capab_len =\n\t\t\tnla_len(info->attrs[NL80211_ATTR_STA_EXT_CAPABILITY]);\n\t}\n\n\tif (parse_station_flags(info, dev->ieee80211_ptr->iftype, &params))\n\t\treturn -EINVAL;\n\n\tif (info->attrs[NL80211_ATTR_STA_PLINK_ACTION])\n\t\tparams.plink_action =\n\t\t\tnla_get_u8(info->attrs[NL80211_ATTR_STA_PLINK_ACTION]);\n\n\tif (info->attrs[NL80211_ATTR_STA_PLINK_STATE]) {\n\t\tparams.plink_state =\n\t\t\tnla_get_u8(info->attrs[NL80211_ATTR_STA_PLINK_STATE]);\n\t\tif (info->attrs[NL80211_ATTR_MESH_PEER_AID])\n\t\t\tparams.peer_aid = nla_get_u16(\n\t\t\t\tinfo->attrs[NL80211_ATTR_MESH_PEER_AID]);\n\t\tparams.sta_modify_mask |= STATION_PARAM_APPLY_PLINK_STATE;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_LOCAL_MESH_POWER_MODE])\n\t\tparams.local_pm = nla_get_u32(\n\t\t\tinfo->attrs[NL80211_ATTR_LOCAL_MESH_POWER_MODE]);\n\n\tif (info->attrs[NL80211_ATTR_OPMODE_NOTIF]) {\n\t\tparams.opmode_notif_used = true;\n\t\tparams.opmode_notif =\n\t\t\tnla_get_u8(info->attrs[NL80211_ATTR_OPMODE_NOTIF]);\n\t}\n\n\tif (info->attrs[NL80211_ATTR_HE_6GHZ_CAPABILITY])\n\t\tparams.he_6ghz_capa =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_HE_6GHZ_CAPABILITY]);\n\n\tif (info->attrs[NL80211_ATTR_AIRTIME_WEIGHT])\n\t\tparams.airtime_weight =\n\t\t\tnla_get_u16(info->attrs[NL80211_ATTR_AIRTIME_WEIGHT]);\n\n\tif (params.airtime_weight &&\n\t    !wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t     NL80211_EXT_FEATURE_AIRTIME_FAIRNESS))\n\t\treturn -EOPNOTSUPP;\n\n\terr = nl80211_parse_sta_txpower_setting(info, &params);\n\tif (err)\n\t\treturn err;\n\n\t/* Include parameters for TDLS peer (will check later) */\n\terr = nl80211_set_station_tdls(info, &params);\n\tif (err)\n\t\treturn err;\n\n\tparams.vlan = get_vlan(info, rdev);\n\tif (IS_ERR(params.vlan))\n\t\treturn PTR_ERR(params.vlan);\n\n\tswitch (dev->ieee80211_ptr->iftype) {\n\tcase NL80211_IFTYPE_AP:\n\tcase NL80211_IFTYPE_AP_VLAN:\n\tcase NL80211_IFTYPE_P2P_GO:\n\tcase NL80211_IFTYPE_P2P_CLIENT:\n\tcase NL80211_IFTYPE_STATION:\n\tcase NL80211_IFTYPE_ADHOC:\n\tcase NL80211_IFTYPE_MESH_POINT:\n\t\tbreak;\n\tdefault:\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out_put_vlan;\n\t}\n\n\t/* driver will call cfg80211_check_station_change() */\n\terr = rdev_change_station(rdev, dev, mac_addr, &params);\n\n out_put_vlan:\n\tif (params.vlan)\n\t\tdev_put(params.vlan);\n\n\treturn err;\n}\n\nstatic int nl80211_new_station(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tint err;\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct station_parameters params;\n\tu8 *mac_addr = NULL;\n\tu32 auth_assoc = BIT(NL80211_STA_FLAG_AUTHENTICATED) |\n\t\t\t BIT(NL80211_STA_FLAG_ASSOCIATED);\n\n\tmemset(&params, 0, sizeof(params));\n\n\tif (!rdev->ops->add_station)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!info->attrs[NL80211_ATTR_MAC])\n\t\treturn -EINVAL;\n\n\tif (!info->attrs[NL80211_ATTR_STA_LISTEN_INTERVAL])\n\t\treturn -EINVAL;\n\n\tif (!info->attrs[NL80211_ATTR_STA_SUPPORTED_RATES])\n\t\treturn -EINVAL;\n\n\tif (!info->attrs[NL80211_ATTR_STA_AID] &&\n\t    !info->attrs[NL80211_ATTR_PEER_AID])\n\t\treturn -EINVAL;\n\n\tmac_addr = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\tparams.supported_rates =\n\t\tnla_data(info->attrs[NL80211_ATTR_STA_SUPPORTED_RATES]);\n\tparams.supported_rates_len =\n\t\tnla_len(info->attrs[NL80211_ATTR_STA_SUPPORTED_RATES]);\n\tparams.listen_interval =\n\t\tnla_get_u16(info->attrs[NL80211_ATTR_STA_LISTEN_INTERVAL]);\n\n\tif (info->attrs[NL80211_ATTR_VLAN_ID])\n\t\tparams.vlan_id = nla_get_u16(info->attrs[NL80211_ATTR_VLAN_ID]);\n\n\tif (info->attrs[NL80211_ATTR_STA_SUPPORT_P2P_PS]) {\n\t\tparams.support_p2p_ps =\n\t\t\tnla_get_u8(info->attrs[NL80211_ATTR_STA_SUPPORT_P2P_PS]);\n\t} else {\n\t\t/*\n\t\t * if not specified, assume it's supported for P2P GO interface,\n\t\t * and is NOT supported for AP interface\n\t\t */\n\t\tparams.support_p2p_ps =\n\t\t\tdev->ieee80211_ptr->iftype == NL80211_IFTYPE_P2P_GO;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_PEER_AID])\n\t\tparams.aid = nla_get_u16(info->attrs[NL80211_ATTR_PEER_AID]);\n\telse\n\t\tparams.aid = nla_get_u16(info->attrs[NL80211_ATTR_STA_AID]);\n\n\tif (info->attrs[NL80211_ATTR_STA_CAPABILITY]) {\n\t\tparams.capability =\n\t\t\tnla_get_u16(info->attrs[NL80211_ATTR_STA_CAPABILITY]);\n\t\tparams.sta_modify_mask |= STATION_PARAM_APPLY_CAPABILITY;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_STA_EXT_CAPABILITY]) {\n\t\tparams.ext_capab =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_STA_EXT_CAPABILITY]);\n\t\tparams.ext_capab_len =\n\t\t\tnla_len(info->attrs[NL80211_ATTR_STA_EXT_CAPABILITY]);\n\t}\n\n\tif (info->attrs[NL80211_ATTR_HT_CAPABILITY])\n\t\tparams.ht_capa =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_HT_CAPABILITY]);\n\n\tif (info->attrs[NL80211_ATTR_VHT_CAPABILITY])\n\t\tparams.vht_capa =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_VHT_CAPABILITY]);\n\n\tif (info->attrs[NL80211_ATTR_HE_CAPABILITY]) {\n\t\tparams.he_capa =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_HE_CAPABILITY]);\n\t\tparams.he_capa_len =\n\t\t\tnla_len(info->attrs[NL80211_ATTR_HE_CAPABILITY]);\n\t}\n\n\tif (info->attrs[NL80211_ATTR_HE_6GHZ_CAPABILITY])\n\t\tparams.he_6ghz_capa =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_HE_6GHZ_CAPABILITY]);\n\n\tif (info->attrs[NL80211_ATTR_OPMODE_NOTIF]) {\n\t\tparams.opmode_notif_used = true;\n\t\tparams.opmode_notif =\n\t\t\tnla_get_u8(info->attrs[NL80211_ATTR_OPMODE_NOTIF]);\n\t}\n\n\tif (info->attrs[NL80211_ATTR_STA_PLINK_ACTION])\n\t\tparams.plink_action =\n\t\t\tnla_get_u8(info->attrs[NL80211_ATTR_STA_PLINK_ACTION]);\n\n\tif (info->attrs[NL80211_ATTR_AIRTIME_WEIGHT])\n\t\tparams.airtime_weight =\n\t\t\tnla_get_u16(info->attrs[NL80211_ATTR_AIRTIME_WEIGHT]);\n\n\tif (params.airtime_weight &&\n\t    !wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t     NL80211_EXT_FEATURE_AIRTIME_FAIRNESS))\n\t\treturn -EOPNOTSUPP;\n\n\terr = nl80211_parse_sta_txpower_setting(info, &params);\n\tif (err)\n\t\treturn err;\n\n\terr = nl80211_parse_sta_channel_info(info, &params);\n\tif (err)\n\t\treturn err;\n\n\terr = nl80211_parse_sta_wme(info, &params);\n\tif (err)\n\t\treturn err;\n\n\tif (parse_station_flags(info, dev->ieee80211_ptr->iftype, &params))\n\t\treturn -EINVAL;\n\n\t/* HT/VHT requires QoS, but if we don't have that just ignore HT/VHT\n\t * as userspace might just pass through the capabilities from the IEs\n\t * directly, rather than enforcing this restriction and returning an\n\t * error in this case.\n\t */\n\tif (!(params.sta_flags_set & BIT(NL80211_STA_FLAG_WME))) {\n\t\tparams.ht_capa = NULL;\n\t\tparams.vht_capa = NULL;\n\n\t\t/* HE requires WME */\n\t\tif (params.he_capa_len || params.he_6ghz_capa)\n\t\t\treturn -EINVAL;\n\t}\n\n\t/* Ensure that HT/VHT capabilities are not set for 6 GHz HE STA */\n\tif (params.he_6ghz_capa && (params.ht_capa || params.vht_capa))\n\t\treturn -EINVAL;\n\n\t/* When you run into this, adjust the code below for the new flag */\n\tBUILD_BUG_ON(NL80211_STA_FLAG_MAX != 7);\n\n\tswitch (dev->ieee80211_ptr->iftype) {\n\tcase NL80211_IFTYPE_AP:\n\tcase NL80211_IFTYPE_AP_VLAN:\n\tcase NL80211_IFTYPE_P2P_GO:\n\t\t/* ignore WME attributes if iface/sta is not capable */\n\t\tif (!(rdev->wiphy.flags & WIPHY_FLAG_AP_UAPSD) ||\n\t\t    !(params.sta_flags_set & BIT(NL80211_STA_FLAG_WME)))\n\t\t\tparams.sta_modify_mask &= ~STATION_PARAM_APPLY_UAPSD;\n\n\t\t/* TDLS peers cannot be added */\n\t\tif ((params.sta_flags_set & BIT(NL80211_STA_FLAG_TDLS_PEER)) ||\n\t\t    info->attrs[NL80211_ATTR_PEER_AID])\n\t\t\treturn -EINVAL;\n\t\t/* but don't bother the driver with it */\n\t\tparams.sta_flags_mask &= ~BIT(NL80211_STA_FLAG_TDLS_PEER);\n\n\t\t/* allow authenticated/associated only if driver handles it */\n\t\tif (!(rdev->wiphy.features &\n\t\t\t\tNL80211_FEATURE_FULL_AP_CLIENT_STATE) &&\n\t\t    params.sta_flags_mask & auth_assoc)\n\t\t\treturn -EINVAL;\n\n\t\t/* Older userspace, or userspace wanting to be compatible with\n\t\t * !NL80211_FEATURE_FULL_AP_CLIENT_STATE, will not set the auth\n\t\t * and assoc flags in the mask, but assumes the station will be\n\t\t * added as associated anyway since this was the required driver\n\t\t * behaviour before NL80211_FEATURE_FULL_AP_CLIENT_STATE was\n\t\t * introduced.\n\t\t * In order to not bother drivers with this quirk in the API\n\t\t * set the flags in both the mask and set for new stations in\n\t\t * this case.\n\t\t */\n\t\tif (!(params.sta_flags_mask & auth_assoc)) {\n\t\t\tparams.sta_flags_mask |= auth_assoc;\n\t\t\tparams.sta_flags_set |= auth_assoc;\n\t\t}\n\n\t\t/* must be last in here for error handling */\n\t\tparams.vlan = get_vlan(info, rdev);\n\t\tif (IS_ERR(params.vlan))\n\t\t\treturn PTR_ERR(params.vlan);\n\t\tbreak;\n\tcase NL80211_IFTYPE_MESH_POINT:\n\t\t/* ignore uAPSD data */\n\t\tparams.sta_modify_mask &= ~STATION_PARAM_APPLY_UAPSD;\n\n\t\t/* associated is disallowed */\n\t\tif (params.sta_flags_mask & BIT(NL80211_STA_FLAG_ASSOCIATED))\n\t\t\treturn -EINVAL;\n\t\t/* TDLS peers cannot be added */\n\t\tif ((params.sta_flags_set & BIT(NL80211_STA_FLAG_TDLS_PEER)) ||\n\t\t    info->attrs[NL80211_ATTR_PEER_AID])\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase NL80211_IFTYPE_STATION:\n\tcase NL80211_IFTYPE_P2P_CLIENT:\n\t\t/* ignore uAPSD data */\n\t\tparams.sta_modify_mask &= ~STATION_PARAM_APPLY_UAPSD;\n\n\t\t/* these are disallowed */\n\t\tif (params.sta_flags_mask &\n\t\t\t\t(BIT(NL80211_STA_FLAG_ASSOCIATED) |\n\t\t\t\t BIT(NL80211_STA_FLAG_AUTHENTICATED)))\n\t\t\treturn -EINVAL;\n\t\t/* Only TDLS peers can be added */\n\t\tif (!(params.sta_flags_set & BIT(NL80211_STA_FLAG_TDLS_PEER)))\n\t\t\treturn -EINVAL;\n\t\t/* Can only add if TDLS ... */\n\t\tif (!(rdev->wiphy.flags & WIPHY_FLAG_SUPPORTS_TDLS))\n\t\t\treturn -EOPNOTSUPP;\n\t\t/* ... with external setup is supported */\n\t\tif (!(rdev->wiphy.flags & WIPHY_FLAG_TDLS_EXTERNAL_SETUP))\n\t\t\treturn -EOPNOTSUPP;\n\t\t/*\n\t\t * Older wpa_supplicant versions always mark the TDLS peer\n\t\t * as authorized, but it shouldn't yet be.\n\t\t */\n\t\tparams.sta_flags_mask &= ~BIT(NL80211_STA_FLAG_AUTHORIZED);\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t/* be aware of params.vlan when changing code here */\n\n\terr = rdev_add_station(rdev, dev, mac_addr, &params);\n\n\tif (params.vlan)\n\t\tdev_put(params.vlan);\n\treturn err;\n}\n\nstatic int nl80211_del_station(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct station_del_parameters params;\n\n\tmemset(&params, 0, sizeof(params));\n\n\tif (info->attrs[NL80211_ATTR_MAC])\n\t\tparams.mac = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\n\tswitch (dev->ieee80211_ptr->iftype) {\n\tcase NL80211_IFTYPE_AP:\n\tcase NL80211_IFTYPE_AP_VLAN:\n\tcase NL80211_IFTYPE_MESH_POINT:\n\tcase NL80211_IFTYPE_P2P_GO:\n\t\t/* always accept these */\n\t\tbreak;\n\tcase NL80211_IFTYPE_ADHOC:\n\t\t/* conditionally accept */\n\t\tif (wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t\t    NL80211_EXT_FEATURE_DEL_IBSS_STA))\n\t\t\tbreak;\n\t\treturn -EINVAL;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (!rdev->ops->del_station)\n\t\treturn -EOPNOTSUPP;\n\n\tif (info->attrs[NL80211_ATTR_MGMT_SUBTYPE]) {\n\t\tparams.subtype =\n\t\t\tnla_get_u8(info->attrs[NL80211_ATTR_MGMT_SUBTYPE]);\n\t\tif (params.subtype != IEEE80211_STYPE_DISASSOC >> 4 &&\n\t\t    params.subtype != IEEE80211_STYPE_DEAUTH >> 4)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\t/* Default to Deauthentication frame */\n\t\tparams.subtype = IEEE80211_STYPE_DEAUTH >> 4;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_REASON_CODE]) {\n\t\tparams.reason_code =\n\t\t\tnla_get_u16(info->attrs[NL80211_ATTR_REASON_CODE]);\n\t\tif (params.reason_code == 0)\n\t\t\treturn -EINVAL; /* 0 is reserved */\n\t} else {\n\t\t/* Default to reason code 2 */\n\t\tparams.reason_code = WLAN_REASON_PREV_AUTH_NOT_VALID;\n\t}\n\n\treturn rdev_del_station(rdev, dev, &params);\n}\n\nstatic int nl80211_send_mpath(struct sk_buff *msg, u32 portid, u32 seq,\n\t\t\t\tint flags, struct net_device *dev,\n\t\t\t\tu8 *dst, u8 *next_hop,\n\t\t\t\tstruct mpath_info *pinfo)\n{\n\tvoid *hdr;\n\tstruct nlattr *pinfoattr;\n\n\thdr = nl80211hdr_put(msg, portid, seq, flags, NL80211_CMD_NEW_MPATH);\n\tif (!hdr)\n\t\treturn -1;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_IFINDEX, dev->ifindex) ||\n\t    nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, dst) ||\n\t    nla_put(msg, NL80211_ATTR_MPATH_NEXT_HOP, ETH_ALEN, next_hop) ||\n\t    nla_put_u32(msg, NL80211_ATTR_GENERATION, pinfo->generation))\n\t\tgoto nla_put_failure;\n\n\tpinfoattr = nla_nest_start_noflag(msg, NL80211_ATTR_MPATH_INFO);\n\tif (!pinfoattr)\n\t\tgoto nla_put_failure;\n\tif ((pinfo->filled & MPATH_INFO_FRAME_QLEN) &&\n\t    nla_put_u32(msg, NL80211_MPATH_INFO_FRAME_QLEN,\n\t\t\tpinfo->frame_qlen))\n\t\tgoto nla_put_failure;\n\tif (((pinfo->filled & MPATH_INFO_SN) &&\n\t     nla_put_u32(msg, NL80211_MPATH_INFO_SN, pinfo->sn)) ||\n\t    ((pinfo->filled & MPATH_INFO_METRIC) &&\n\t     nla_put_u32(msg, NL80211_MPATH_INFO_METRIC,\n\t\t\t pinfo->metric)) ||\n\t    ((pinfo->filled & MPATH_INFO_EXPTIME) &&\n\t     nla_put_u32(msg, NL80211_MPATH_INFO_EXPTIME,\n\t\t\t pinfo->exptime)) ||\n\t    ((pinfo->filled & MPATH_INFO_FLAGS) &&\n\t     nla_put_u8(msg, NL80211_MPATH_INFO_FLAGS,\n\t\t\tpinfo->flags)) ||\n\t    ((pinfo->filled & MPATH_INFO_DISCOVERY_TIMEOUT) &&\n\t     nla_put_u32(msg, NL80211_MPATH_INFO_DISCOVERY_TIMEOUT,\n\t\t\t pinfo->discovery_timeout)) ||\n\t    ((pinfo->filled & MPATH_INFO_DISCOVERY_RETRIES) &&\n\t     nla_put_u8(msg, NL80211_MPATH_INFO_DISCOVERY_RETRIES,\n\t\t\tpinfo->discovery_retries)) ||\n\t    ((pinfo->filled & MPATH_INFO_HOP_COUNT) &&\n\t     nla_put_u8(msg, NL80211_MPATH_INFO_HOP_COUNT,\n\t\t\tpinfo->hop_count)) ||\n\t    ((pinfo->filled & MPATH_INFO_PATH_CHANGE) &&\n\t     nla_put_u32(msg, NL80211_MPATH_INFO_PATH_CHANGE,\n\t\t\t pinfo->path_change_count)))\n\t\tgoto nla_put_failure;\n\n\tnla_nest_end(msg, pinfoattr);\n\n\tgenlmsg_end(msg, hdr);\n\treturn 0;\n\n nla_put_failure:\n\tgenlmsg_cancel(msg, hdr);\n\treturn -EMSGSIZE;\n}\n\nstatic int nl80211_dump_mpath(struct sk_buff *skb,\n\t\t\t      struct netlink_callback *cb)\n{\n\tstruct mpath_info pinfo;\n\tstruct cfg80211_registered_device *rdev;\n\tstruct wireless_dev *wdev;\n\tu8 dst[ETH_ALEN];\n\tu8 next_hop[ETH_ALEN];\n\tint path_idx = cb->args[2];\n\tint err;\n\n\terr = nl80211_prepare_wdev_dump(cb, &rdev, &wdev);\n\tif (err)\n\t\treturn err;\n\t/* nl80211_prepare_wdev_dump acquired it in the successful case */\n\t__acquire(&rdev->wiphy.mtx);\n\n\tif (!rdev->ops->dump_mpath) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out_err;\n\t}\n\n\tif (wdev->iftype != NL80211_IFTYPE_MESH_POINT) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out_err;\n\t}\n\n\twhile (1) {\n\t\terr = rdev_dump_mpath(rdev, wdev->netdev, path_idx, dst,\n\t\t\t\t      next_hop, &pinfo);\n\t\tif (err == -ENOENT)\n\t\t\tbreak;\n\t\tif (err)\n\t\t\tgoto out_err;\n\n\t\tif (nl80211_send_mpath(skb, NETLINK_CB(cb->skb).portid,\n\t\t\t\t       cb->nlh->nlmsg_seq, NLM_F_MULTI,\n\t\t\t\t       wdev->netdev, dst, next_hop,\n\t\t\t\t       &pinfo) < 0)\n\t\t\tgoto out;\n\n\t\tpath_idx++;\n\t}\n\n out:\n\tcb->args[2] = path_idx;\n\terr = skb->len;\n out_err:\n\twiphy_unlock(&rdev->wiphy);\n\treturn err;\n}\n\nstatic int nl80211_get_mpath(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tint err;\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct mpath_info pinfo;\n\tstruct sk_buff *msg;\n\tu8 *dst = NULL;\n\tu8 next_hop[ETH_ALEN];\n\n\tmemset(&pinfo, 0, sizeof(pinfo));\n\n\tif (!info->attrs[NL80211_ATTR_MAC])\n\t\treturn -EINVAL;\n\n\tdst = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\n\tif (!rdev->ops->get_mpath)\n\t\treturn -EOPNOTSUPP;\n\n\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_MESH_POINT)\n\t\treturn -EOPNOTSUPP;\n\n\terr = rdev_get_mpath(rdev, dev, dst, next_hop, &pinfo);\n\tif (err)\n\t\treturn err;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\tif (nl80211_send_mpath(msg, info->snd_portid, info->snd_seq, 0,\n\t\t\t\t dev, dst, next_hop, &pinfo) < 0) {\n\t\tnlmsg_free(msg);\n\t\treturn -ENOBUFS;\n\t}\n\n\treturn genlmsg_reply(msg, info);\n}\n\nstatic int nl80211_set_mpath(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tu8 *dst = NULL;\n\tu8 *next_hop = NULL;\n\n\tif (!info->attrs[NL80211_ATTR_MAC])\n\t\treturn -EINVAL;\n\n\tif (!info->attrs[NL80211_ATTR_MPATH_NEXT_HOP])\n\t\treturn -EINVAL;\n\n\tdst = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\tnext_hop = nla_data(info->attrs[NL80211_ATTR_MPATH_NEXT_HOP]);\n\n\tif (!rdev->ops->change_mpath)\n\t\treturn -EOPNOTSUPP;\n\n\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_MESH_POINT)\n\t\treturn -EOPNOTSUPP;\n\n\treturn rdev_change_mpath(rdev, dev, dst, next_hop);\n}\n\nstatic int nl80211_new_mpath(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tu8 *dst = NULL;\n\tu8 *next_hop = NULL;\n\n\tif (!info->attrs[NL80211_ATTR_MAC])\n\t\treturn -EINVAL;\n\n\tif (!info->attrs[NL80211_ATTR_MPATH_NEXT_HOP])\n\t\treturn -EINVAL;\n\n\tdst = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\tnext_hop = nla_data(info->attrs[NL80211_ATTR_MPATH_NEXT_HOP]);\n\n\tif (!rdev->ops->add_mpath)\n\t\treturn -EOPNOTSUPP;\n\n\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_MESH_POINT)\n\t\treturn -EOPNOTSUPP;\n\n\treturn rdev_add_mpath(rdev, dev, dst, next_hop);\n}\n\nstatic int nl80211_del_mpath(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tu8 *dst = NULL;\n\n\tif (info->attrs[NL80211_ATTR_MAC])\n\t\tdst = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\n\tif (!rdev->ops->del_mpath)\n\t\treturn -EOPNOTSUPP;\n\n\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_MESH_POINT)\n\t\treturn -EOPNOTSUPP;\n\n\treturn rdev_del_mpath(rdev, dev, dst);\n}\n\nstatic int nl80211_get_mpp(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tint err;\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct mpath_info pinfo;\n\tstruct sk_buff *msg;\n\tu8 *dst = NULL;\n\tu8 mpp[ETH_ALEN];\n\n\tmemset(&pinfo, 0, sizeof(pinfo));\n\n\tif (!info->attrs[NL80211_ATTR_MAC])\n\t\treturn -EINVAL;\n\n\tdst = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\n\tif (!rdev->ops->get_mpp)\n\t\treturn -EOPNOTSUPP;\n\n\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_MESH_POINT)\n\t\treturn -EOPNOTSUPP;\n\n\terr = rdev_get_mpp(rdev, dev, dst, mpp, &pinfo);\n\tif (err)\n\t\treturn err;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\tif (nl80211_send_mpath(msg, info->snd_portid, info->snd_seq, 0,\n\t\t\t       dev, dst, mpp, &pinfo) < 0) {\n\t\tnlmsg_free(msg);\n\t\treturn -ENOBUFS;\n\t}\n\n\treturn genlmsg_reply(msg, info);\n}\n\nstatic int nl80211_dump_mpp(struct sk_buff *skb,\n\t\t\t    struct netlink_callback *cb)\n{\n\tstruct mpath_info pinfo;\n\tstruct cfg80211_registered_device *rdev;\n\tstruct wireless_dev *wdev;\n\tu8 dst[ETH_ALEN];\n\tu8 mpp[ETH_ALEN];\n\tint path_idx = cb->args[2];\n\tint err;\n\n\terr = nl80211_prepare_wdev_dump(cb, &rdev, &wdev);\n\tif (err)\n\t\treturn err;\n\t/* nl80211_prepare_wdev_dump acquired it in the successful case */\n\t__acquire(&rdev->wiphy.mtx);\n\n\tif (!rdev->ops->dump_mpp) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out_err;\n\t}\n\n\tif (wdev->iftype != NL80211_IFTYPE_MESH_POINT) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out_err;\n\t}\n\n\twhile (1) {\n\t\terr = rdev_dump_mpp(rdev, wdev->netdev, path_idx, dst,\n\t\t\t\t    mpp, &pinfo);\n\t\tif (err == -ENOENT)\n\t\t\tbreak;\n\t\tif (err)\n\t\t\tgoto out_err;\n\n\t\tif (nl80211_send_mpath(skb, NETLINK_CB(cb->skb).portid,\n\t\t\t\t       cb->nlh->nlmsg_seq, NLM_F_MULTI,\n\t\t\t\t       wdev->netdev, dst, mpp,\n\t\t\t\t       &pinfo) < 0)\n\t\t\tgoto out;\n\n\t\tpath_idx++;\n\t}\n\n out:\n\tcb->args[2] = path_idx;\n\terr = skb->len;\n out_err:\n\twiphy_unlock(&rdev->wiphy);\n\treturn err;\n}\n\nstatic int nl80211_set_bss(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct bss_parameters params;\n\tint err;\n\n\tmemset(&params, 0, sizeof(params));\n\t/* default to not changing parameters */\n\tparams.use_cts_prot = -1;\n\tparams.use_short_preamble = -1;\n\tparams.use_short_slot_time = -1;\n\tparams.ap_isolate = -1;\n\tparams.ht_opmode = -1;\n\tparams.p2p_ctwindow = -1;\n\tparams.p2p_opp_ps = -1;\n\n\tif (info->attrs[NL80211_ATTR_BSS_CTS_PROT])\n\t\tparams.use_cts_prot =\n\t\t    nla_get_u8(info->attrs[NL80211_ATTR_BSS_CTS_PROT]);\n\tif (info->attrs[NL80211_ATTR_BSS_SHORT_PREAMBLE])\n\t\tparams.use_short_preamble =\n\t\t    nla_get_u8(info->attrs[NL80211_ATTR_BSS_SHORT_PREAMBLE]);\n\tif (info->attrs[NL80211_ATTR_BSS_SHORT_SLOT_TIME])\n\t\tparams.use_short_slot_time =\n\t\t    nla_get_u8(info->attrs[NL80211_ATTR_BSS_SHORT_SLOT_TIME]);\n\tif (info->attrs[NL80211_ATTR_BSS_BASIC_RATES]) {\n\t\tparams.basic_rates =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_BSS_BASIC_RATES]);\n\t\tparams.basic_rates_len =\n\t\t\tnla_len(info->attrs[NL80211_ATTR_BSS_BASIC_RATES]);\n\t}\n\tif (info->attrs[NL80211_ATTR_AP_ISOLATE])\n\t\tparams.ap_isolate = !!nla_get_u8(info->attrs[NL80211_ATTR_AP_ISOLATE]);\n\tif (info->attrs[NL80211_ATTR_BSS_HT_OPMODE])\n\t\tparams.ht_opmode =\n\t\t\tnla_get_u16(info->attrs[NL80211_ATTR_BSS_HT_OPMODE]);\n\n\tif (info->attrs[NL80211_ATTR_P2P_CTWINDOW]) {\n\t\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_P2P_GO)\n\t\t\treturn -EINVAL;\n\t\tparams.p2p_ctwindow =\n\t\t\tnla_get_u8(info->attrs[NL80211_ATTR_P2P_CTWINDOW]);\n\t\tif (params.p2p_ctwindow != 0 &&\n\t\t    !(rdev->wiphy.features & NL80211_FEATURE_P2P_GO_CTWIN))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_P2P_OPPPS]) {\n\t\tu8 tmp;\n\n\t\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_P2P_GO)\n\t\t\treturn -EINVAL;\n\t\ttmp = nla_get_u8(info->attrs[NL80211_ATTR_P2P_OPPPS]);\n\t\tparams.p2p_opp_ps = tmp;\n\t\tif (params.p2p_opp_ps &&\n\t\t    !(rdev->wiphy.features & NL80211_FEATURE_P2P_GO_OPPPS))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!rdev->ops->change_bss)\n\t\treturn -EOPNOTSUPP;\n\n\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_AP &&\n\t    dev->ieee80211_ptr->iftype != NL80211_IFTYPE_P2P_GO)\n\t\treturn -EOPNOTSUPP;\n\n\twdev_lock(wdev);\n\terr = rdev_change_bss(rdev, dev, &params);\n\twdev_unlock(wdev);\n\n\treturn err;\n}\n\nstatic int nl80211_req_set_reg(struct sk_buff *skb, struct genl_info *info)\n{\n\tchar *data = NULL;\n\tbool is_indoor;\n\tenum nl80211_user_reg_hint_type user_reg_hint_type;\n\tu32 owner_nlportid;\n\n\t/*\n\t * You should only get this when cfg80211 hasn't yet initialized\n\t * completely when built-in to the kernel right between the time\n\t * window between nl80211_init() and regulatory_init(), if that is\n\t * even possible.\n\t */\n\tif (unlikely(!rcu_access_pointer(cfg80211_regdomain)))\n\t\treturn -EINPROGRESS;\n\n\tif (info->attrs[NL80211_ATTR_USER_REG_HINT_TYPE])\n\t\tuser_reg_hint_type =\n\t\t  nla_get_u32(info->attrs[NL80211_ATTR_USER_REG_HINT_TYPE]);\n\telse\n\t\tuser_reg_hint_type = NL80211_USER_REG_HINT_USER;\n\n\tswitch (user_reg_hint_type) {\n\tcase NL80211_USER_REG_HINT_USER:\n\tcase NL80211_USER_REG_HINT_CELL_BASE:\n\t\tif (!info->attrs[NL80211_ATTR_REG_ALPHA2])\n\t\t\treturn -EINVAL;\n\n\t\tdata = nla_data(info->attrs[NL80211_ATTR_REG_ALPHA2]);\n\t\treturn regulatory_hint_user(data, user_reg_hint_type);\n\tcase NL80211_USER_REG_HINT_INDOOR:\n\t\tif (info->attrs[NL80211_ATTR_SOCKET_OWNER]) {\n\t\t\towner_nlportid = info->snd_portid;\n\t\t\tis_indoor = !!info->attrs[NL80211_ATTR_REG_INDOOR];\n\t\t} else {\n\t\t\towner_nlportid = 0;\n\t\t\tis_indoor = true;\n\t\t}\n\n\t\treturn regulatory_hint_indoor(is_indoor, owner_nlportid);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int nl80211_reload_regdb(struct sk_buff *skb, struct genl_info *info)\n{\n\treturn reg_reload_regdb();\n}\n\nstatic int nl80211_get_mesh_config(struct sk_buff *skb,\n\t\t\t\t   struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct mesh_config cur_params;\n\tint err = 0;\n\tvoid *hdr;\n\tstruct nlattr *pinfoattr;\n\tstruct sk_buff *msg;\n\n\tif (wdev->iftype != NL80211_IFTYPE_MESH_POINT)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!rdev->ops->get_mesh_config)\n\t\treturn -EOPNOTSUPP;\n\n\twdev_lock(wdev);\n\t/* If not connected, get default parameters */\n\tif (!wdev->mesh_id_len)\n\t\tmemcpy(&cur_params, &default_mesh_config, sizeof(cur_params));\n\telse\n\t\terr = rdev_get_mesh_config(rdev, dev, &cur_params);\n\twdev_unlock(wdev);\n\n\tif (err)\n\t\treturn err;\n\n\t/* Draw up a netlink message to send back */\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\thdr = nl80211hdr_put(msg, info->snd_portid, info->snd_seq, 0,\n\t\t\t     NL80211_CMD_GET_MESH_CONFIG);\n\tif (!hdr)\n\t\tgoto out;\n\tpinfoattr = nla_nest_start_noflag(msg, NL80211_ATTR_MESH_CONFIG);\n\tif (!pinfoattr)\n\t\tgoto nla_put_failure;\n\tif (nla_put_u32(msg, NL80211_ATTR_IFINDEX, dev->ifindex) ||\n\t    nla_put_u16(msg, NL80211_MESHCONF_RETRY_TIMEOUT,\n\t\t\tcur_params.dot11MeshRetryTimeout) ||\n\t    nla_put_u16(msg, NL80211_MESHCONF_CONFIRM_TIMEOUT,\n\t\t\tcur_params.dot11MeshConfirmTimeout) ||\n\t    nla_put_u16(msg, NL80211_MESHCONF_HOLDING_TIMEOUT,\n\t\t\tcur_params.dot11MeshHoldingTimeout) ||\n\t    nla_put_u16(msg, NL80211_MESHCONF_MAX_PEER_LINKS,\n\t\t\tcur_params.dot11MeshMaxPeerLinks) ||\n\t    nla_put_u8(msg, NL80211_MESHCONF_MAX_RETRIES,\n\t\t       cur_params.dot11MeshMaxRetries) ||\n\t    nla_put_u8(msg, NL80211_MESHCONF_TTL,\n\t\t       cur_params.dot11MeshTTL) ||\n\t    nla_put_u8(msg, NL80211_MESHCONF_ELEMENT_TTL,\n\t\t       cur_params.element_ttl) ||\n\t    nla_put_u8(msg, NL80211_MESHCONF_AUTO_OPEN_PLINKS,\n\t\t       cur_params.auto_open_plinks) ||\n\t    nla_put_u32(msg, NL80211_MESHCONF_SYNC_OFFSET_MAX_NEIGHBOR,\n\t\t\tcur_params.dot11MeshNbrOffsetMaxNeighbor) ||\n\t    nla_put_u8(msg, NL80211_MESHCONF_HWMP_MAX_PREQ_RETRIES,\n\t\t       cur_params.dot11MeshHWMPmaxPREQretries) ||\n\t    nla_put_u32(msg, NL80211_MESHCONF_PATH_REFRESH_TIME,\n\t\t\tcur_params.path_refresh_time) ||\n\t    nla_put_u16(msg, NL80211_MESHCONF_MIN_DISCOVERY_TIMEOUT,\n\t\t\tcur_params.min_discovery_timeout) ||\n\t    nla_put_u32(msg, NL80211_MESHCONF_HWMP_ACTIVE_PATH_TIMEOUT,\n\t\t\tcur_params.dot11MeshHWMPactivePathTimeout) ||\n\t    nla_put_u16(msg, NL80211_MESHCONF_HWMP_PREQ_MIN_INTERVAL,\n\t\t\tcur_params.dot11MeshHWMPpreqMinInterval) ||\n\t    nla_put_u16(msg, NL80211_MESHCONF_HWMP_PERR_MIN_INTERVAL,\n\t\t\tcur_params.dot11MeshHWMPperrMinInterval) ||\n\t    nla_put_u16(msg, NL80211_MESHCONF_HWMP_NET_DIAM_TRVS_TIME,\n\t\t\tcur_params.dot11MeshHWMPnetDiameterTraversalTime) ||\n\t    nla_put_u8(msg, NL80211_MESHCONF_HWMP_ROOTMODE,\n\t\t       cur_params.dot11MeshHWMPRootMode) ||\n\t    nla_put_u16(msg, NL80211_MESHCONF_HWMP_RANN_INTERVAL,\n\t\t\tcur_params.dot11MeshHWMPRannInterval) ||\n\t    nla_put_u8(msg, NL80211_MESHCONF_GATE_ANNOUNCEMENTS,\n\t\t       cur_params.dot11MeshGateAnnouncementProtocol) ||\n\t    nla_put_u8(msg, NL80211_MESHCONF_FORWARDING,\n\t\t       cur_params.dot11MeshForwarding) ||\n\t    nla_put_s32(msg, NL80211_MESHCONF_RSSI_THRESHOLD,\n\t\t\tcur_params.rssi_threshold) ||\n\t    nla_put_u32(msg, NL80211_MESHCONF_HT_OPMODE,\n\t\t\tcur_params.ht_opmode) ||\n\t    nla_put_u32(msg, NL80211_MESHCONF_HWMP_PATH_TO_ROOT_TIMEOUT,\n\t\t\tcur_params.dot11MeshHWMPactivePathToRootTimeout) ||\n\t    nla_put_u16(msg, NL80211_MESHCONF_HWMP_ROOT_INTERVAL,\n\t\t\tcur_params.dot11MeshHWMProotInterval) ||\n\t    nla_put_u16(msg, NL80211_MESHCONF_HWMP_CONFIRMATION_INTERVAL,\n\t\t\tcur_params.dot11MeshHWMPconfirmationInterval) ||\n\t    nla_put_u32(msg, NL80211_MESHCONF_POWER_MODE,\n\t\t\tcur_params.power_mode) ||\n\t    nla_put_u16(msg, NL80211_MESHCONF_AWAKE_WINDOW,\n\t\t\tcur_params.dot11MeshAwakeWindowDuration) ||\n\t    nla_put_u32(msg, NL80211_MESHCONF_PLINK_TIMEOUT,\n\t\t\tcur_params.plink_timeout) ||\n\t    nla_put_u8(msg, NL80211_MESHCONF_CONNECTED_TO_GATE,\n\t\t       cur_params.dot11MeshConnectedToMeshGate) ||\n\t    nla_put_u8(msg, NL80211_MESHCONF_NOLEARN,\n\t\t       cur_params.dot11MeshNolearn) ||\n\t    nla_put_u8(msg, NL80211_MESHCONF_CONNECTED_TO_AS,\n\t\t       cur_params.dot11MeshConnectedToAuthServer))\n\t\tgoto nla_put_failure;\n\tnla_nest_end(msg, pinfoattr);\n\tgenlmsg_end(msg, hdr);\n\treturn genlmsg_reply(msg, info);\n\n nla_put_failure:\n out:\n\tnlmsg_free(msg);\n\treturn -ENOBUFS;\n}\n\nstatic const struct nla_policy\nnl80211_meshconf_params_policy[NL80211_MESHCONF_ATTR_MAX+1] = {\n\t[NL80211_MESHCONF_RETRY_TIMEOUT] =\n\t\tNLA_POLICY_RANGE(NLA_U16, 1, 255),\n\t[NL80211_MESHCONF_CONFIRM_TIMEOUT] =\n\t\tNLA_POLICY_RANGE(NLA_U16, 1, 255),\n\t[NL80211_MESHCONF_HOLDING_TIMEOUT] =\n\t\tNLA_POLICY_RANGE(NLA_U16, 1, 255),\n\t[NL80211_MESHCONF_MAX_PEER_LINKS] =\n\t\tNLA_POLICY_RANGE(NLA_U16, 0, 255),\n\t[NL80211_MESHCONF_MAX_RETRIES] = NLA_POLICY_MAX(NLA_U8, 16),\n\t[NL80211_MESHCONF_TTL] = NLA_POLICY_MIN(NLA_U8, 1),\n\t[NL80211_MESHCONF_ELEMENT_TTL] = NLA_POLICY_MIN(NLA_U8, 1),\n\t[NL80211_MESHCONF_AUTO_OPEN_PLINKS] = NLA_POLICY_MAX(NLA_U8, 1),\n\t[NL80211_MESHCONF_SYNC_OFFSET_MAX_NEIGHBOR] =\n\t\tNLA_POLICY_RANGE(NLA_U32, 1, 255),\n\t[NL80211_MESHCONF_HWMP_MAX_PREQ_RETRIES] = { .type = NLA_U8 },\n\t[NL80211_MESHCONF_PATH_REFRESH_TIME] = { .type = NLA_U32 },\n\t[NL80211_MESHCONF_MIN_DISCOVERY_TIMEOUT] = NLA_POLICY_MIN(NLA_U16, 1),\n\t[NL80211_MESHCONF_HWMP_ACTIVE_PATH_TIMEOUT] = { .type = NLA_U32 },\n\t[NL80211_MESHCONF_HWMP_PREQ_MIN_INTERVAL] =\n\t\tNLA_POLICY_MIN(NLA_U16, 1),\n\t[NL80211_MESHCONF_HWMP_PERR_MIN_INTERVAL] =\n\t\tNLA_POLICY_MIN(NLA_U16, 1),\n\t[NL80211_MESHCONF_HWMP_NET_DIAM_TRVS_TIME] =\n\t\tNLA_POLICY_MIN(NLA_U16, 1),\n\t[NL80211_MESHCONF_HWMP_ROOTMODE] = NLA_POLICY_MAX(NLA_U8, 4),\n\t[NL80211_MESHCONF_HWMP_RANN_INTERVAL] =\n\t\tNLA_POLICY_MIN(NLA_U16, 1),\n\t[NL80211_MESHCONF_GATE_ANNOUNCEMENTS] = NLA_POLICY_MAX(NLA_U8, 1),\n\t[NL80211_MESHCONF_FORWARDING] = NLA_POLICY_MAX(NLA_U8, 1),\n\t[NL80211_MESHCONF_RSSI_THRESHOLD] =\n\t\tNLA_POLICY_RANGE(NLA_S32, -255, 0),\n\t[NL80211_MESHCONF_HT_OPMODE] = { .type = NLA_U16 },\n\t[NL80211_MESHCONF_HWMP_PATH_TO_ROOT_TIMEOUT] = { .type = NLA_U32 },\n\t[NL80211_MESHCONF_HWMP_ROOT_INTERVAL] =\n\t\tNLA_POLICY_MIN(NLA_U16, 1),\n\t[NL80211_MESHCONF_HWMP_CONFIRMATION_INTERVAL] =\n\t\tNLA_POLICY_MIN(NLA_U16, 1),\n\t[NL80211_MESHCONF_POWER_MODE] =\n\t\tNLA_POLICY_RANGE(NLA_U32,\n\t\t\t\t NL80211_MESH_POWER_ACTIVE,\n\t\t\t\t NL80211_MESH_POWER_MAX),\n\t[NL80211_MESHCONF_AWAKE_WINDOW] = { .type = NLA_U16 },\n\t[NL80211_MESHCONF_PLINK_TIMEOUT] = { .type = NLA_U32 },\n\t[NL80211_MESHCONF_CONNECTED_TO_GATE] = NLA_POLICY_RANGE(NLA_U8, 0, 1),\n\t[NL80211_MESHCONF_NOLEARN] = NLA_POLICY_RANGE(NLA_U8, 0, 1),\n\t[NL80211_MESHCONF_CONNECTED_TO_AS] = NLA_POLICY_RANGE(NLA_U8, 0, 1),\n};\n\nstatic const struct nla_policy\n\tnl80211_mesh_setup_params_policy[NL80211_MESH_SETUP_ATTR_MAX+1] = {\n\t[NL80211_MESH_SETUP_ENABLE_VENDOR_SYNC] = { .type = NLA_U8 },\n\t[NL80211_MESH_SETUP_ENABLE_VENDOR_PATH_SEL] = { .type = NLA_U8 },\n\t[NL80211_MESH_SETUP_ENABLE_VENDOR_METRIC] = { .type = NLA_U8 },\n\t[NL80211_MESH_SETUP_USERSPACE_AUTH] = { .type = NLA_FLAG },\n\t[NL80211_MESH_SETUP_AUTH_PROTOCOL] = { .type = NLA_U8 },\n\t[NL80211_MESH_SETUP_USERSPACE_MPM] = { .type = NLA_FLAG },\n\t[NL80211_MESH_SETUP_IE] =\n\t\tNLA_POLICY_VALIDATE_FN(NLA_BINARY, validate_ie_attr,\n\t\t\t\t       IEEE80211_MAX_DATA_LEN),\n\t[NL80211_MESH_SETUP_USERSPACE_AMPE] = { .type = NLA_FLAG },\n};\n\nstatic int nl80211_parse_mesh_config(struct genl_info *info,\n\t\t\t\t     struct mesh_config *cfg,\n\t\t\t\t     u32 *mask_out)\n{\n\tstruct nlattr *tb[NL80211_MESHCONF_ATTR_MAX + 1];\n\tu32 mask = 0;\n\tu16 ht_opmode;\n\n#define FILL_IN_MESH_PARAM_IF_SET(tb, cfg, param, mask, attr, fn)\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (tb[attr]) {\t\t\t\t\t\t\t\\\n\t\tcfg->param = fn(tb[attr]);\t\t\t\t\\\n\t\tmask |= BIT((attr) - 1);\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n\tif (!info->attrs[NL80211_ATTR_MESH_CONFIG])\n\t\treturn -EINVAL;\n\tif (nla_parse_nested_deprecated(tb, NL80211_MESHCONF_ATTR_MAX, info->attrs[NL80211_ATTR_MESH_CONFIG], nl80211_meshconf_params_policy, info->extack))\n\t\treturn -EINVAL;\n\n\t/* This makes sure that there aren't more than 32 mesh config\n\t * parameters (otherwise our bitfield scheme would not work.) */\n\tBUILD_BUG_ON(NL80211_MESHCONF_ATTR_MAX > 32);\n\n\t/* Fill in the params struct */\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, dot11MeshRetryTimeout, mask,\n\t\t\t\t  NL80211_MESHCONF_RETRY_TIMEOUT, nla_get_u16);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, dot11MeshConfirmTimeout, mask,\n\t\t\t\t  NL80211_MESHCONF_CONFIRM_TIMEOUT,\n\t\t\t\t  nla_get_u16);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, dot11MeshHoldingTimeout, mask,\n\t\t\t\t  NL80211_MESHCONF_HOLDING_TIMEOUT,\n\t\t\t\t  nla_get_u16);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, dot11MeshMaxPeerLinks, mask,\n\t\t\t\t  NL80211_MESHCONF_MAX_PEER_LINKS,\n\t\t\t\t  nla_get_u16);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, dot11MeshMaxRetries, mask,\n\t\t\t\t  NL80211_MESHCONF_MAX_RETRIES, nla_get_u8);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, dot11MeshTTL, mask,\n\t\t\t\t  NL80211_MESHCONF_TTL, nla_get_u8);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, element_ttl, mask,\n\t\t\t\t  NL80211_MESHCONF_ELEMENT_TTL, nla_get_u8);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, auto_open_plinks, mask,\n\t\t\t\t  NL80211_MESHCONF_AUTO_OPEN_PLINKS,\n\t\t\t\t  nla_get_u8);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, dot11MeshNbrOffsetMaxNeighbor,\n\t\t\t\t  mask,\n\t\t\t\t  NL80211_MESHCONF_SYNC_OFFSET_MAX_NEIGHBOR,\n\t\t\t\t  nla_get_u32);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, dot11MeshHWMPmaxPREQretries, mask,\n\t\t\t\t  NL80211_MESHCONF_HWMP_MAX_PREQ_RETRIES,\n\t\t\t\t  nla_get_u8);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, path_refresh_time, mask,\n\t\t\t\t  NL80211_MESHCONF_PATH_REFRESH_TIME,\n\t\t\t\t  nla_get_u32);\n\tif (mask & BIT(NL80211_MESHCONF_PATH_REFRESH_TIME) &&\n\t    (cfg->path_refresh_time < 1 || cfg->path_refresh_time > 65535))\n\t\treturn -EINVAL;\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, min_discovery_timeout, mask,\n\t\t\t\t  NL80211_MESHCONF_MIN_DISCOVERY_TIMEOUT,\n\t\t\t\t  nla_get_u16);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, dot11MeshHWMPactivePathTimeout,\n\t\t\t\t  mask,\n\t\t\t\t  NL80211_MESHCONF_HWMP_ACTIVE_PATH_TIMEOUT,\n\t\t\t\t  nla_get_u32);\n\tif (mask & BIT(NL80211_MESHCONF_HWMP_ACTIVE_PATH_TIMEOUT) &&\n\t    (cfg->dot11MeshHWMPactivePathTimeout < 1 ||\n\t     cfg->dot11MeshHWMPactivePathTimeout > 65535))\n\t\treturn -EINVAL;\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, dot11MeshHWMPpreqMinInterval, mask,\n\t\t\t\t  NL80211_MESHCONF_HWMP_PREQ_MIN_INTERVAL,\n\t\t\t\t  nla_get_u16);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, dot11MeshHWMPperrMinInterval, mask,\n\t\t\t\t  NL80211_MESHCONF_HWMP_PERR_MIN_INTERVAL,\n\t\t\t\t  nla_get_u16);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg,\n\t\t\t\t  dot11MeshHWMPnetDiameterTraversalTime, mask,\n\t\t\t\t  NL80211_MESHCONF_HWMP_NET_DIAM_TRVS_TIME,\n\t\t\t\t  nla_get_u16);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, dot11MeshHWMPRootMode, mask,\n\t\t\t\t  NL80211_MESHCONF_HWMP_ROOTMODE, nla_get_u8);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, dot11MeshHWMPRannInterval, mask,\n\t\t\t\t  NL80211_MESHCONF_HWMP_RANN_INTERVAL,\n\t\t\t\t  nla_get_u16);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, dot11MeshGateAnnouncementProtocol,\n\t\t\t\t  mask, NL80211_MESHCONF_GATE_ANNOUNCEMENTS,\n\t\t\t\t  nla_get_u8);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, dot11MeshForwarding, mask,\n\t\t\t\t  NL80211_MESHCONF_FORWARDING, nla_get_u8);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, rssi_threshold, mask,\n\t\t\t\t  NL80211_MESHCONF_RSSI_THRESHOLD,\n\t\t\t\t  nla_get_s32);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, dot11MeshConnectedToMeshGate, mask,\n\t\t\t\t  NL80211_MESHCONF_CONNECTED_TO_GATE,\n\t\t\t\t  nla_get_u8);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, dot11MeshConnectedToAuthServer, mask,\n\t\t\t\t  NL80211_MESHCONF_CONNECTED_TO_AS,\n\t\t\t\t  nla_get_u8);\n\t/*\n\t * Check HT operation mode based on\n\t * IEEE 802.11-2016 9.4.2.57 HT Operation element.\n\t */\n\tif (tb[NL80211_MESHCONF_HT_OPMODE]) {\n\t\tht_opmode = nla_get_u16(tb[NL80211_MESHCONF_HT_OPMODE]);\n\n\t\tif (ht_opmode & ~(IEEE80211_HT_OP_MODE_PROTECTION |\n\t\t\t\t  IEEE80211_HT_OP_MODE_NON_GF_STA_PRSNT |\n\t\t\t\t  IEEE80211_HT_OP_MODE_NON_HT_STA_PRSNT))\n\t\t\treturn -EINVAL;\n\n\t\t/* NON_HT_STA bit is reserved, but some programs set it */\n\t\tht_opmode &= ~IEEE80211_HT_OP_MODE_NON_HT_STA_PRSNT;\n\n\t\tcfg->ht_opmode = ht_opmode;\n\t\tmask |= (1 << (NL80211_MESHCONF_HT_OPMODE - 1));\n\t}\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg,\n\t\t\t\t  dot11MeshHWMPactivePathToRootTimeout, mask,\n\t\t\t\t  NL80211_MESHCONF_HWMP_PATH_TO_ROOT_TIMEOUT,\n\t\t\t\t  nla_get_u32);\n\tif (mask & BIT(NL80211_MESHCONF_HWMP_PATH_TO_ROOT_TIMEOUT) &&\n\t    (cfg->dot11MeshHWMPactivePathToRootTimeout < 1 ||\n\t     cfg->dot11MeshHWMPactivePathToRootTimeout > 65535))\n\t\treturn -EINVAL;\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, dot11MeshHWMProotInterval, mask,\n\t\t\t\t  NL80211_MESHCONF_HWMP_ROOT_INTERVAL,\n\t\t\t\t  nla_get_u16);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, dot11MeshHWMPconfirmationInterval,\n\t\t\t\t  mask,\n\t\t\t\t  NL80211_MESHCONF_HWMP_CONFIRMATION_INTERVAL,\n\t\t\t\t  nla_get_u16);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, power_mode, mask,\n\t\t\t\t  NL80211_MESHCONF_POWER_MODE, nla_get_u32);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, dot11MeshAwakeWindowDuration, mask,\n\t\t\t\t  NL80211_MESHCONF_AWAKE_WINDOW, nla_get_u16);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, plink_timeout, mask,\n\t\t\t\t  NL80211_MESHCONF_PLINK_TIMEOUT, nla_get_u32);\n\tFILL_IN_MESH_PARAM_IF_SET(tb, cfg, dot11MeshNolearn, mask,\n\t\t\t\t  NL80211_MESHCONF_NOLEARN, nla_get_u8);\n\tif (mask_out)\n\t\t*mask_out = mask;\n\n\treturn 0;\n\n#undef FILL_IN_MESH_PARAM_IF_SET\n}\n\nstatic int nl80211_parse_mesh_setup(struct genl_info *info,\n\t\t\t\t     struct mesh_setup *setup)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct nlattr *tb[NL80211_MESH_SETUP_ATTR_MAX + 1];\n\n\tif (!info->attrs[NL80211_ATTR_MESH_SETUP])\n\t\treturn -EINVAL;\n\tif (nla_parse_nested_deprecated(tb, NL80211_MESH_SETUP_ATTR_MAX, info->attrs[NL80211_ATTR_MESH_SETUP], nl80211_mesh_setup_params_policy, info->extack))\n\t\treturn -EINVAL;\n\n\tif (tb[NL80211_MESH_SETUP_ENABLE_VENDOR_SYNC])\n\t\tsetup->sync_method =\n\t\t(nla_get_u8(tb[NL80211_MESH_SETUP_ENABLE_VENDOR_SYNC])) ?\n\t\t IEEE80211_SYNC_METHOD_VENDOR :\n\t\t IEEE80211_SYNC_METHOD_NEIGHBOR_OFFSET;\n\n\tif (tb[NL80211_MESH_SETUP_ENABLE_VENDOR_PATH_SEL])\n\t\tsetup->path_sel_proto =\n\t\t(nla_get_u8(tb[NL80211_MESH_SETUP_ENABLE_VENDOR_PATH_SEL])) ?\n\t\t IEEE80211_PATH_PROTOCOL_VENDOR :\n\t\t IEEE80211_PATH_PROTOCOL_HWMP;\n\n\tif (tb[NL80211_MESH_SETUP_ENABLE_VENDOR_METRIC])\n\t\tsetup->path_metric =\n\t\t(nla_get_u8(tb[NL80211_MESH_SETUP_ENABLE_VENDOR_METRIC])) ?\n\t\t IEEE80211_PATH_METRIC_VENDOR :\n\t\t IEEE80211_PATH_METRIC_AIRTIME;\n\n\tif (tb[NL80211_MESH_SETUP_IE]) {\n\t\tstruct nlattr *ieattr =\n\t\t\ttb[NL80211_MESH_SETUP_IE];\n\t\tsetup->ie = nla_data(ieattr);\n\t\tsetup->ie_len = nla_len(ieattr);\n\t}\n\tif (tb[NL80211_MESH_SETUP_USERSPACE_MPM] &&\n\t    !(rdev->wiphy.features & NL80211_FEATURE_USERSPACE_MPM))\n\t\treturn -EINVAL;\n\tsetup->user_mpm = nla_get_flag(tb[NL80211_MESH_SETUP_USERSPACE_MPM]);\n\tsetup->is_authenticated = nla_get_flag(tb[NL80211_MESH_SETUP_USERSPACE_AUTH]);\n\tsetup->is_secure = nla_get_flag(tb[NL80211_MESH_SETUP_USERSPACE_AMPE]);\n\tif (setup->is_secure)\n\t\tsetup->user_mpm = true;\n\n\tif (tb[NL80211_MESH_SETUP_AUTH_PROTOCOL]) {\n\t\tif (!setup->user_mpm)\n\t\t\treturn -EINVAL;\n\t\tsetup->auth_id =\n\t\t\tnla_get_u8(tb[NL80211_MESH_SETUP_AUTH_PROTOCOL]);\n\t}\n\n\treturn 0;\n}\n\nstatic int nl80211_update_mesh_config(struct sk_buff *skb,\n\t\t\t\t      struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct mesh_config cfg;\n\tu32 mask;\n\tint err;\n\n\tif (wdev->iftype != NL80211_IFTYPE_MESH_POINT)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!rdev->ops->update_mesh_config)\n\t\treturn -EOPNOTSUPP;\n\n\terr = nl80211_parse_mesh_config(info, &cfg, &mask);\n\tif (err)\n\t\treturn err;\n\n\twdev_lock(wdev);\n\tif (!wdev->mesh_id_len)\n\t\terr = -ENOLINK;\n\n\tif (!err)\n\t\terr = rdev_update_mesh_config(rdev, dev, mask, &cfg);\n\n\twdev_unlock(wdev);\n\n\treturn err;\n}\n\nstatic int nl80211_put_regdom(const struct ieee80211_regdomain *regdom,\n\t\t\t      struct sk_buff *msg)\n{\n\tstruct nlattr *nl_reg_rules;\n\tunsigned int i;\n\n\tif (nla_put_string(msg, NL80211_ATTR_REG_ALPHA2, regdom->alpha2) ||\n\t    (regdom->dfs_region &&\n\t     nla_put_u8(msg, NL80211_ATTR_DFS_REGION, regdom->dfs_region)))\n\t\tgoto nla_put_failure;\n\n\tnl_reg_rules = nla_nest_start_noflag(msg, NL80211_ATTR_REG_RULES);\n\tif (!nl_reg_rules)\n\t\tgoto nla_put_failure;\n\n\tfor (i = 0; i < regdom->n_reg_rules; i++) {\n\t\tstruct nlattr *nl_reg_rule;\n\t\tconst struct ieee80211_reg_rule *reg_rule;\n\t\tconst struct ieee80211_freq_range *freq_range;\n\t\tconst struct ieee80211_power_rule *power_rule;\n\t\tunsigned int max_bandwidth_khz;\n\n\t\treg_rule = &regdom->reg_rules[i];\n\t\tfreq_range = &reg_rule->freq_range;\n\t\tpower_rule = &reg_rule->power_rule;\n\n\t\tnl_reg_rule = nla_nest_start_noflag(msg, i);\n\t\tif (!nl_reg_rule)\n\t\t\tgoto nla_put_failure;\n\n\t\tmax_bandwidth_khz = freq_range->max_bandwidth_khz;\n\t\tif (!max_bandwidth_khz)\n\t\t\tmax_bandwidth_khz = reg_get_max_bandwidth(regdom,\n\t\t\t\t\t\t\t\t  reg_rule);\n\n\t\tif (nla_put_u32(msg, NL80211_ATTR_REG_RULE_FLAGS,\n\t\t\t\treg_rule->flags) ||\n\t\t    nla_put_u32(msg, NL80211_ATTR_FREQ_RANGE_START,\n\t\t\t\tfreq_range->start_freq_khz) ||\n\t\t    nla_put_u32(msg, NL80211_ATTR_FREQ_RANGE_END,\n\t\t\t\tfreq_range->end_freq_khz) ||\n\t\t    nla_put_u32(msg, NL80211_ATTR_FREQ_RANGE_MAX_BW,\n\t\t\t\tmax_bandwidth_khz) ||\n\t\t    nla_put_u32(msg, NL80211_ATTR_POWER_RULE_MAX_ANT_GAIN,\n\t\t\t\tpower_rule->max_antenna_gain) ||\n\t\t    nla_put_u32(msg, NL80211_ATTR_POWER_RULE_MAX_EIRP,\n\t\t\t\tpower_rule->max_eirp) ||\n\t\t    nla_put_u32(msg, NL80211_ATTR_DFS_CAC_TIME,\n\t\t\t\treg_rule->dfs_cac_ms))\n\t\t\tgoto nla_put_failure;\n\n\t\tnla_nest_end(msg, nl_reg_rule);\n\t}\n\n\tnla_nest_end(msg, nl_reg_rules);\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}\n\nstatic int nl80211_get_reg_do(struct sk_buff *skb, struct genl_info *info)\n{\n\tconst struct ieee80211_regdomain *regdom = NULL;\n\tstruct cfg80211_registered_device *rdev;\n\tstruct wiphy *wiphy = NULL;\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn -ENOBUFS;\n\n\thdr = nl80211hdr_put(msg, info->snd_portid, info->snd_seq, 0,\n\t\t\t     NL80211_CMD_GET_REG);\n\tif (!hdr)\n\t\tgoto put_failure;\n\n\trtnl_lock();\n\n\tif (info->attrs[NL80211_ATTR_WIPHY]) {\n\t\tbool self_managed;\n\n\t\trdev = cfg80211_get_dev_from_info(genl_info_net(info), info);\n\t\tif (IS_ERR(rdev)) {\n\t\t\tnlmsg_free(msg);\n\t\t\trtnl_unlock();\n\t\t\treturn PTR_ERR(rdev);\n\t\t}\n\n\t\twiphy = &rdev->wiphy;\n\t\tself_managed = wiphy->regulatory_flags &\n\t\t\t       REGULATORY_WIPHY_SELF_MANAGED;\n\t\tregdom = get_wiphy_regdom(wiphy);\n\n\t\t/* a self-managed-reg device must have a private regdom */\n\t\tif (WARN_ON(!regdom && self_managed)) {\n\t\t\tnlmsg_free(msg);\n\t\t\trtnl_unlock();\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (regdom &&\n\t\t    nla_put_u32(msg, NL80211_ATTR_WIPHY, get_wiphy_idx(wiphy)))\n\t\t\tgoto nla_put_failure;\n\t}\n\n\tif (!wiphy && reg_last_request_cell_base() &&\n\t    nla_put_u32(msg, NL80211_ATTR_USER_REG_HINT_TYPE,\n\t\t\tNL80211_USER_REG_HINT_CELL_BASE))\n\t\tgoto nla_put_failure;\n\n\trcu_read_lock();\n\n\tif (!regdom)\n\t\tregdom = rcu_dereference(cfg80211_regdomain);\n\n\tif (nl80211_put_regdom(regdom, msg))\n\t\tgoto nla_put_failure_rcu;\n\n\trcu_read_unlock();\n\n\tgenlmsg_end(msg, hdr);\n\trtnl_unlock();\n\treturn genlmsg_reply(msg, info);\n\nnla_put_failure_rcu:\n\trcu_read_unlock();\nnla_put_failure:\n\trtnl_unlock();\nput_failure:\n\tnlmsg_free(msg);\n\treturn -EMSGSIZE;\n}\n\nstatic int nl80211_send_regdom(struct sk_buff *msg, struct netlink_callback *cb,\n\t\t\t       u32 seq, int flags, struct wiphy *wiphy,\n\t\t\t       const struct ieee80211_regdomain *regdom)\n{\n\tvoid *hdr = nl80211hdr_put(msg, NETLINK_CB(cb->skb).portid, seq, flags,\n\t\t\t\t   NL80211_CMD_GET_REG);\n\n\tif (!hdr)\n\t\treturn -1;\n\n\tgenl_dump_check_consistent(cb, hdr);\n\n\tif (nl80211_put_regdom(regdom, msg))\n\t\tgoto nla_put_failure;\n\n\tif (!wiphy && reg_last_request_cell_base() &&\n\t    nla_put_u32(msg, NL80211_ATTR_USER_REG_HINT_TYPE,\n\t\t\tNL80211_USER_REG_HINT_CELL_BASE))\n\t\tgoto nla_put_failure;\n\n\tif (wiphy &&\n\t    nla_put_u32(msg, NL80211_ATTR_WIPHY, get_wiphy_idx(wiphy)))\n\t\tgoto nla_put_failure;\n\n\tif (wiphy && wiphy->regulatory_flags & REGULATORY_WIPHY_SELF_MANAGED &&\n\t    nla_put_flag(msg, NL80211_ATTR_WIPHY_SELF_MANAGED_REG))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\treturn 0;\n\nnla_put_failure:\n\tgenlmsg_cancel(msg, hdr);\n\treturn -EMSGSIZE;\n}\n\nstatic int nl80211_get_reg_dump(struct sk_buff *skb,\n\t\t\t\tstruct netlink_callback *cb)\n{\n\tconst struct ieee80211_regdomain *regdom = NULL;\n\tstruct cfg80211_registered_device *rdev;\n\tint err, reg_idx, start = cb->args[2];\n\n\trtnl_lock();\n\n\tif (cfg80211_regdomain && start == 0) {\n\t\terr = nl80211_send_regdom(skb, cb, cb->nlh->nlmsg_seq,\n\t\t\t\t\t  NLM_F_MULTI, NULL,\n\t\t\t\t\t  rtnl_dereference(cfg80211_regdomain));\n\t\tif (err < 0)\n\t\t\tgoto out_err;\n\t}\n\n\t/* the global regdom is idx 0 */\n\treg_idx = 1;\n\tlist_for_each_entry(rdev, &cfg80211_rdev_list, list) {\n\t\tregdom = get_wiphy_regdom(&rdev->wiphy);\n\t\tif (!regdom)\n\t\t\tcontinue;\n\n\t\tif (++reg_idx <= start)\n\t\t\tcontinue;\n\n\t\terr = nl80211_send_regdom(skb, cb, cb->nlh->nlmsg_seq,\n\t\t\t\t\t  NLM_F_MULTI, &rdev->wiphy, regdom);\n\t\tif (err < 0) {\n\t\t\treg_idx--;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tcb->args[2] = reg_idx;\n\terr = skb->len;\nout_err:\n\trtnl_unlock();\n\treturn err;\n}\n\n#ifdef CONFIG_CFG80211_CRDA_SUPPORT\nstatic const struct nla_policy reg_rule_policy[NL80211_REG_RULE_ATTR_MAX + 1] = {\n\t[NL80211_ATTR_REG_RULE_FLAGS]\t\t= { .type = NLA_U32 },\n\t[NL80211_ATTR_FREQ_RANGE_START]\t\t= { .type = NLA_U32 },\n\t[NL80211_ATTR_FREQ_RANGE_END]\t\t= { .type = NLA_U32 },\n\t[NL80211_ATTR_FREQ_RANGE_MAX_BW]\t= { .type = NLA_U32 },\n\t[NL80211_ATTR_POWER_RULE_MAX_ANT_GAIN]\t= { .type = NLA_U32 },\n\t[NL80211_ATTR_POWER_RULE_MAX_EIRP]\t= { .type = NLA_U32 },\n\t[NL80211_ATTR_DFS_CAC_TIME]\t\t= { .type = NLA_U32 },\n};\n\nstatic int parse_reg_rule(struct nlattr *tb[],\n\tstruct ieee80211_reg_rule *reg_rule)\n{\n\tstruct ieee80211_freq_range *freq_range = &reg_rule->freq_range;\n\tstruct ieee80211_power_rule *power_rule = &reg_rule->power_rule;\n\n\tif (!tb[NL80211_ATTR_REG_RULE_FLAGS])\n\t\treturn -EINVAL;\n\tif (!tb[NL80211_ATTR_FREQ_RANGE_START])\n\t\treturn -EINVAL;\n\tif (!tb[NL80211_ATTR_FREQ_RANGE_END])\n\t\treturn -EINVAL;\n\tif (!tb[NL80211_ATTR_FREQ_RANGE_MAX_BW])\n\t\treturn -EINVAL;\n\tif (!tb[NL80211_ATTR_POWER_RULE_MAX_EIRP])\n\t\treturn -EINVAL;\n\n\treg_rule->flags = nla_get_u32(tb[NL80211_ATTR_REG_RULE_FLAGS]);\n\n\tfreq_range->start_freq_khz =\n\t\tnla_get_u32(tb[NL80211_ATTR_FREQ_RANGE_START]);\n\tfreq_range->end_freq_khz =\n\t\tnla_get_u32(tb[NL80211_ATTR_FREQ_RANGE_END]);\n\tfreq_range->max_bandwidth_khz =\n\t\tnla_get_u32(tb[NL80211_ATTR_FREQ_RANGE_MAX_BW]);\n\n\tpower_rule->max_eirp =\n\t\tnla_get_u32(tb[NL80211_ATTR_POWER_RULE_MAX_EIRP]);\n\n\tif (tb[NL80211_ATTR_POWER_RULE_MAX_ANT_GAIN])\n\t\tpower_rule->max_antenna_gain =\n\t\t\tnla_get_u32(tb[NL80211_ATTR_POWER_RULE_MAX_ANT_GAIN]);\n\n\tif (tb[NL80211_ATTR_DFS_CAC_TIME])\n\t\treg_rule->dfs_cac_ms =\n\t\t\tnla_get_u32(tb[NL80211_ATTR_DFS_CAC_TIME]);\n\n\treturn 0;\n}\n\nstatic int nl80211_set_reg(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct nlattr *tb[NL80211_REG_RULE_ATTR_MAX + 1];\n\tstruct nlattr *nl_reg_rule;\n\tchar *alpha2;\n\tint rem_reg_rules, r;\n\tu32 num_rules = 0, rule_idx = 0;\n\tenum nl80211_dfs_regions dfs_region = NL80211_DFS_UNSET;\n\tstruct ieee80211_regdomain *rd;\n\n\tif (!info->attrs[NL80211_ATTR_REG_ALPHA2])\n\t\treturn -EINVAL;\n\n\tif (!info->attrs[NL80211_ATTR_REG_RULES])\n\t\treturn -EINVAL;\n\n\talpha2 = nla_data(info->attrs[NL80211_ATTR_REG_ALPHA2]);\n\n\tif (info->attrs[NL80211_ATTR_DFS_REGION])\n\t\tdfs_region = nla_get_u8(info->attrs[NL80211_ATTR_DFS_REGION]);\n\n\tnla_for_each_nested(nl_reg_rule, info->attrs[NL80211_ATTR_REG_RULES],\n\t\t\t    rem_reg_rules) {\n\t\tnum_rules++;\n\t\tif (num_rules > NL80211_MAX_SUPP_REG_RULES)\n\t\t\treturn -EINVAL;\n\t}\n\n\trtnl_lock();\n\tif (!reg_is_valid_request(alpha2)) {\n\t\tr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\trd = kzalloc(struct_size(rd, reg_rules, num_rules), GFP_KERNEL);\n\tif (!rd) {\n\t\tr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trd->n_reg_rules = num_rules;\n\trd->alpha2[0] = alpha2[0];\n\trd->alpha2[1] = alpha2[1];\n\n\t/*\n\t * Disable DFS master mode if the DFS region was\n\t * not supported or known on this kernel.\n\t */\n\tif (reg_supported_dfs_region(dfs_region))\n\t\trd->dfs_region = dfs_region;\n\n\tnla_for_each_nested(nl_reg_rule, info->attrs[NL80211_ATTR_REG_RULES],\n\t\t\t    rem_reg_rules) {\n\t\tr = nla_parse_nested_deprecated(tb, NL80211_REG_RULE_ATTR_MAX,\n\t\t\t\t\t\tnl_reg_rule, reg_rule_policy,\n\t\t\t\t\t\tinfo->extack);\n\t\tif (r)\n\t\t\tgoto bad_reg;\n\t\tr = parse_reg_rule(tb, &rd->reg_rules[rule_idx]);\n\t\tif (r)\n\t\t\tgoto bad_reg;\n\n\t\trule_idx++;\n\n\t\tif (rule_idx > NL80211_MAX_SUPP_REG_RULES) {\n\t\t\tr = -EINVAL;\n\t\t\tgoto bad_reg;\n\t\t}\n\t}\n\n\tr = set_regdom(rd, REGD_SOURCE_CRDA);\n\t/* set_regdom takes ownership of rd */\n\trd = NULL;\n bad_reg:\n\tkfree(rd);\n out:\n\trtnl_unlock();\n\treturn r;\n}\n#endif /* CONFIG_CFG80211_CRDA_SUPPORT */\n\nstatic int validate_scan_freqs(struct nlattr *freqs)\n{\n\tstruct nlattr *attr1, *attr2;\n\tint n_channels = 0, tmp1, tmp2;\n\n\tnla_for_each_nested(attr1, freqs, tmp1)\n\t\tif (nla_len(attr1) != sizeof(u32))\n\t\t\treturn 0;\n\n\tnla_for_each_nested(attr1, freqs, tmp1) {\n\t\tn_channels++;\n\t\t/*\n\t\t * Some hardware has a limited channel list for\n\t\t * scanning, and it is pretty much nonsensical\n\t\t * to scan for a channel twice, so disallow that\n\t\t * and don't require drivers to check that the\n\t\t * channel list they get isn't longer than what\n\t\t * they can scan, as long as they can scan all\n\t\t * the channels they registered at once.\n\t\t */\n\t\tnla_for_each_nested(attr2, freqs, tmp2)\n\t\t\tif (attr1 != attr2 &&\n\t\t\t    nla_get_u32(attr1) == nla_get_u32(attr2))\n\t\t\t\treturn 0;\n\t}\n\n\treturn n_channels;\n}\n\nstatic bool is_band_valid(struct wiphy *wiphy, enum nl80211_band b)\n{\n\treturn b < NUM_NL80211_BANDS && wiphy->bands[b];\n}\n\nstatic int parse_bss_select(struct nlattr *nla, struct wiphy *wiphy,\n\t\t\t    struct cfg80211_bss_selection *bss_select)\n{\n\tstruct nlattr *attr[NL80211_BSS_SELECT_ATTR_MAX + 1];\n\tstruct nlattr *nest;\n\tint err;\n\tbool found = false;\n\tint i;\n\n\t/* only process one nested attribute */\n\tnest = nla_data(nla);\n\tif (!nla_ok(nest, nla_len(nest)))\n\t\treturn -EINVAL;\n\n\terr = nla_parse_nested_deprecated(attr, NL80211_BSS_SELECT_ATTR_MAX,\n\t\t\t\t\t  nest, nl80211_bss_select_policy,\n\t\t\t\t\t  NULL);\n\tif (err)\n\t\treturn err;\n\n\t/* only one attribute may be given */\n\tfor (i = 0; i <= NL80211_BSS_SELECT_ATTR_MAX; i++) {\n\t\tif (attr[i]) {\n\t\t\tif (found)\n\t\t\t\treturn -EINVAL;\n\t\t\tfound = true;\n\t\t}\n\t}\n\n\tbss_select->behaviour = __NL80211_BSS_SELECT_ATTR_INVALID;\n\n\tif (attr[NL80211_BSS_SELECT_ATTR_RSSI])\n\t\tbss_select->behaviour = NL80211_BSS_SELECT_ATTR_RSSI;\n\n\tif (attr[NL80211_BSS_SELECT_ATTR_BAND_PREF]) {\n\t\tbss_select->behaviour = NL80211_BSS_SELECT_ATTR_BAND_PREF;\n\t\tbss_select->param.band_pref =\n\t\t\tnla_get_u32(attr[NL80211_BSS_SELECT_ATTR_BAND_PREF]);\n\t\tif (!is_band_valid(wiphy, bss_select->param.band_pref))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (attr[NL80211_BSS_SELECT_ATTR_RSSI_ADJUST]) {\n\t\tstruct nl80211_bss_select_rssi_adjust *adj_param;\n\n\t\tadj_param = nla_data(attr[NL80211_BSS_SELECT_ATTR_RSSI_ADJUST]);\n\t\tbss_select->behaviour = NL80211_BSS_SELECT_ATTR_RSSI_ADJUST;\n\t\tbss_select->param.adjust.band = adj_param->band;\n\t\tbss_select->param.adjust.delta = adj_param->delta;\n\t\tif (!is_band_valid(wiphy, bss_select->param.adjust.band))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/* user-space did not provide behaviour attribute */\n\tif (bss_select->behaviour == __NL80211_BSS_SELECT_ATTR_INVALID)\n\t\treturn -EINVAL;\n\n\tif (!(wiphy->bss_select_support & BIT(bss_select->behaviour)))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nint nl80211_parse_random_mac(struct nlattr **attrs,\n\t\t\t     u8 *mac_addr, u8 *mac_addr_mask)\n{\n\tint i;\n\n\tif (!attrs[NL80211_ATTR_MAC] && !attrs[NL80211_ATTR_MAC_MASK]) {\n\t\teth_zero_addr(mac_addr);\n\t\teth_zero_addr(mac_addr_mask);\n\t\tmac_addr[0] = 0x2;\n\t\tmac_addr_mask[0] = 0x3;\n\n\t\treturn 0;\n\t}\n\n\t/* need both or none */\n\tif (!attrs[NL80211_ATTR_MAC] || !attrs[NL80211_ATTR_MAC_MASK])\n\t\treturn -EINVAL;\n\n\tmemcpy(mac_addr, nla_data(attrs[NL80211_ATTR_MAC]), ETH_ALEN);\n\tmemcpy(mac_addr_mask, nla_data(attrs[NL80211_ATTR_MAC_MASK]), ETH_ALEN);\n\n\t/* don't allow or configure an mcast address */\n\tif (!is_multicast_ether_addr(mac_addr_mask) ||\n\t    is_multicast_ether_addr(mac_addr))\n\t\treturn -EINVAL;\n\n\t/*\n\t * allow users to pass a MAC address that has bits set outside\n\t * of the mask, but don't bother drivers with having to deal\n\t * with such bits\n\t */\n\tfor (i = 0; i < ETH_ALEN; i++)\n\t\tmac_addr[i] &= mac_addr_mask[i];\n\n\treturn 0;\n}\n\nstatic bool cfg80211_off_channel_oper_allowed(struct wireless_dev *wdev)\n{\n\tASSERT_WDEV_LOCK(wdev);\n\n\tif (!cfg80211_beaconing_iface_active(wdev))\n\t\treturn true;\n\n\tif (!(wdev->chandef.chan->flags & IEEE80211_CHAN_RADAR))\n\t\treturn true;\n\n\treturn regulatory_pre_cac_allowed(wdev->wiphy);\n}\n\nstatic bool nl80211_check_scan_feat(struct wiphy *wiphy, u32 flags, u32 flag,\n\t\t\t\t    enum nl80211_ext_feature_index feat)\n{\n\tif (!(flags & flag))\n\t\treturn true;\n\tif (wiphy_ext_feature_isset(wiphy, feat))\n\t\treturn true;\n\treturn false;\n}\n\nstatic int\nnl80211_check_scan_flags(struct wiphy *wiphy, struct wireless_dev *wdev,\n\t\t\t void *request, struct nlattr **attrs,\n\t\t\t bool is_sched_scan)\n{\n\tu8 *mac_addr, *mac_addr_mask;\n\tu32 *flags;\n\tenum nl80211_feature_flags randomness_flag;\n\n\tif (!attrs[NL80211_ATTR_SCAN_FLAGS])\n\t\treturn 0;\n\n\tif (is_sched_scan) {\n\t\tstruct cfg80211_sched_scan_request *req = request;\n\n\t\trandomness_flag = wdev ?\n\t\t\t\t  NL80211_FEATURE_SCHED_SCAN_RANDOM_MAC_ADDR :\n\t\t\t\t  NL80211_FEATURE_ND_RANDOM_MAC_ADDR;\n\t\tflags = &req->flags;\n\t\tmac_addr = req->mac_addr;\n\t\tmac_addr_mask = req->mac_addr_mask;\n\t} else {\n\t\tstruct cfg80211_scan_request *req = request;\n\n\t\trandomness_flag = NL80211_FEATURE_SCAN_RANDOM_MAC_ADDR;\n\t\tflags = &req->flags;\n\t\tmac_addr = req->mac_addr;\n\t\tmac_addr_mask = req->mac_addr_mask;\n\t}\n\n\t*flags = nla_get_u32(attrs[NL80211_ATTR_SCAN_FLAGS]);\n\n\tif (((*flags & NL80211_SCAN_FLAG_LOW_PRIORITY) &&\n\t     !(wiphy->features & NL80211_FEATURE_LOW_PRIORITY_SCAN)) ||\n\t    !nl80211_check_scan_feat(wiphy, *flags,\n\t\t\t\t     NL80211_SCAN_FLAG_LOW_SPAN,\n\t\t\t\t     NL80211_EXT_FEATURE_LOW_SPAN_SCAN) ||\n\t    !nl80211_check_scan_feat(wiphy, *flags,\n\t\t\t\t     NL80211_SCAN_FLAG_LOW_POWER,\n\t\t\t\t     NL80211_EXT_FEATURE_LOW_POWER_SCAN) ||\n\t    !nl80211_check_scan_feat(wiphy, *flags,\n\t\t\t\t     NL80211_SCAN_FLAG_HIGH_ACCURACY,\n\t\t\t\t     NL80211_EXT_FEATURE_HIGH_ACCURACY_SCAN) ||\n\t    !nl80211_check_scan_feat(wiphy, *flags,\n\t\t\t\t     NL80211_SCAN_FLAG_FILS_MAX_CHANNEL_TIME,\n\t\t\t\t     NL80211_EXT_FEATURE_FILS_MAX_CHANNEL_TIME) ||\n\t    !nl80211_check_scan_feat(wiphy, *flags,\n\t\t\t\t     NL80211_SCAN_FLAG_ACCEPT_BCAST_PROBE_RESP,\n\t\t\t\t     NL80211_EXT_FEATURE_ACCEPT_BCAST_PROBE_RESP) ||\n\t    !nl80211_check_scan_feat(wiphy, *flags,\n\t\t\t\t     NL80211_SCAN_FLAG_OCE_PROBE_REQ_DEFERRAL_SUPPRESSION,\n\t\t\t\t     NL80211_EXT_FEATURE_OCE_PROBE_REQ_DEFERRAL_SUPPRESSION) ||\n\t    !nl80211_check_scan_feat(wiphy, *flags,\n\t\t\t\t     NL80211_SCAN_FLAG_OCE_PROBE_REQ_HIGH_TX_RATE,\n\t\t\t\t     NL80211_EXT_FEATURE_OCE_PROBE_REQ_HIGH_TX_RATE) ||\n\t    !nl80211_check_scan_feat(wiphy, *flags,\n\t\t\t\t     NL80211_SCAN_FLAG_RANDOM_SN,\n\t\t\t\t     NL80211_EXT_FEATURE_SCAN_RANDOM_SN) ||\n\t    !nl80211_check_scan_feat(wiphy, *flags,\n\t\t\t\t     NL80211_SCAN_FLAG_MIN_PREQ_CONTENT,\n\t\t\t\t     NL80211_EXT_FEATURE_SCAN_MIN_PREQ_CONTENT))\n\t\treturn -EOPNOTSUPP;\n\n\tif (*flags & NL80211_SCAN_FLAG_RANDOM_ADDR) {\n\t\tint err;\n\n\t\tif (!(wiphy->features & randomness_flag) ||\n\t\t    (wdev && wdev->current_bss))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\terr = nl80211_parse_random_mac(attrs, mac_addr, mac_addr_mask);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int nl80211_trigger_scan(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev = info->user_ptr[1];\n\tstruct cfg80211_scan_request *request;\n\tstruct nlattr *scan_freqs = NULL;\n\tbool scan_freqs_khz = false;\n\tstruct nlattr *attr;\n\tstruct wiphy *wiphy;\n\tint err, tmp, n_ssids = 0, n_channels, i;\n\tsize_t ie_len;\n\n\twiphy = &rdev->wiphy;\n\n\tif (wdev->iftype == NL80211_IFTYPE_NAN)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!rdev->ops->scan)\n\t\treturn -EOPNOTSUPP;\n\n\tif (rdev->scan_req || rdev->scan_msg)\n\t\treturn -EBUSY;\n\n\tif (info->attrs[NL80211_ATTR_SCAN_FREQ_KHZ]) {\n\t\tif (!wiphy_ext_feature_isset(wiphy,\n\t\t\t\t\t     NL80211_EXT_FEATURE_SCAN_FREQ_KHZ))\n\t\t\treturn -EOPNOTSUPP;\n\t\tscan_freqs = info->attrs[NL80211_ATTR_SCAN_FREQ_KHZ];\n\t\tscan_freqs_khz = true;\n\t} else if (info->attrs[NL80211_ATTR_SCAN_FREQUENCIES])\n\t\tscan_freqs = info->attrs[NL80211_ATTR_SCAN_FREQUENCIES];\n\n\tif (scan_freqs) {\n\t\tn_channels = validate_scan_freqs(scan_freqs);\n\t\tif (!n_channels)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tn_channels = ieee80211_get_num_supported_channels(wiphy);\n\t}\n\n\tif (info->attrs[NL80211_ATTR_SCAN_SSIDS])\n\t\tnla_for_each_nested(attr, info->attrs[NL80211_ATTR_SCAN_SSIDS], tmp)\n\t\t\tn_ssids++;\n\n\tif (n_ssids > wiphy->max_scan_ssids)\n\t\treturn -EINVAL;\n\n\tif (info->attrs[NL80211_ATTR_IE])\n\t\tie_len = nla_len(info->attrs[NL80211_ATTR_IE]);\n\telse\n\t\tie_len = 0;\n\n\tif (ie_len > wiphy->max_scan_ie_len)\n\t\treturn -EINVAL;\n\n\trequest = kzalloc(sizeof(*request)\n\t\t\t+ sizeof(*request->ssids) * n_ssids\n\t\t\t+ sizeof(*request->channels) * n_channels\n\t\t\t+ ie_len, GFP_KERNEL);\n\tif (!request)\n\t\treturn -ENOMEM;\n\n\tif (n_ssids)\n\t\trequest->ssids = (void *)&request->channels[n_channels];\n\trequest->n_ssids = n_ssids;\n\tif (ie_len) {\n\t\tif (n_ssids)\n\t\t\trequest->ie = (void *)(request->ssids + n_ssids);\n\t\telse\n\t\t\trequest->ie = (void *)(request->channels + n_channels);\n\t}\n\n\ti = 0;\n\tif (scan_freqs) {\n\t\t/* user specified, bail out if channel not found */\n\t\tnla_for_each_nested(attr, scan_freqs, tmp) {\n\t\t\tstruct ieee80211_channel *chan;\n\t\t\tint freq = nla_get_u32(attr);\n\n\t\t\tif (!scan_freqs_khz)\n\t\t\t\tfreq = MHZ_TO_KHZ(freq);\n\n\t\t\tchan = ieee80211_get_channel_khz(wiphy, freq);\n\t\t\tif (!chan) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\n\t\t\t/* ignore disabled channels */\n\t\t\tif (chan->flags & IEEE80211_CHAN_DISABLED)\n\t\t\t\tcontinue;\n\n\t\t\trequest->channels[i] = chan;\n\t\t\ti++;\n\t\t}\n\t} else {\n\t\tenum nl80211_band band;\n\n\t\t/* all channels */\n\t\tfor (band = 0; band < NUM_NL80211_BANDS; band++) {\n\t\t\tint j;\n\n\t\t\tif (!wiphy->bands[band])\n\t\t\t\tcontinue;\n\t\t\tfor (j = 0; j < wiphy->bands[band]->n_channels; j++) {\n\t\t\t\tstruct ieee80211_channel *chan;\n\n\t\t\t\tchan = &wiphy->bands[band]->channels[j];\n\n\t\t\t\tif (chan->flags & IEEE80211_CHAN_DISABLED)\n\t\t\t\t\tcontinue;\n\n\t\t\t\trequest->channels[i] = chan;\n\t\t\t\ti++;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!i) {\n\t\terr = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\trequest->n_channels = i;\n\n\twdev_lock(wdev);\n\tif (!cfg80211_off_channel_oper_allowed(wdev)) {\n\t\tstruct ieee80211_channel *chan;\n\n\t\tif (request->n_channels != 1) {\n\t\t\twdev_unlock(wdev);\n\t\t\terr = -EBUSY;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tchan = request->channels[0];\n\t\tif (chan->center_freq != wdev->chandef.chan->center_freq) {\n\t\t\twdev_unlock(wdev);\n\t\t\terr = -EBUSY;\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\twdev_unlock(wdev);\n\n\ti = 0;\n\tif (n_ssids) {\n\t\tnla_for_each_nested(attr, info->attrs[NL80211_ATTR_SCAN_SSIDS], tmp) {\n\t\t\tif (nla_len(attr) > IEEE80211_MAX_SSID_LEN) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t\trequest->ssids[i].ssid_len = nla_len(attr);\n\t\t\tmemcpy(request->ssids[i].ssid, nla_data(attr), nla_len(attr));\n\t\t\ti++;\n\t\t}\n\t}\n\n\tif (info->attrs[NL80211_ATTR_IE]) {\n\t\trequest->ie_len = nla_len(info->attrs[NL80211_ATTR_IE]);\n\t\tmemcpy((void *)request->ie,\n\t\t       nla_data(info->attrs[NL80211_ATTR_IE]),\n\t\t       request->ie_len);\n\t}\n\n\tfor (i = 0; i < NUM_NL80211_BANDS; i++)\n\t\tif (wiphy->bands[i])\n\t\t\trequest->rates[i] =\n\t\t\t\t(1 << wiphy->bands[i]->n_bitrates) - 1;\n\n\tif (info->attrs[NL80211_ATTR_SCAN_SUPP_RATES]) {\n\t\tnla_for_each_nested(attr,\n\t\t\t\t    info->attrs[NL80211_ATTR_SCAN_SUPP_RATES],\n\t\t\t\t    tmp) {\n\t\t\tenum nl80211_band band = nla_type(attr);\n\n\t\t\tif (band < 0 || band >= NUM_NL80211_BANDS) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\n\t\t\tif (!wiphy->bands[band])\n\t\t\t\tcontinue;\n\n\t\t\terr = ieee80211_get_ratemask(wiphy->bands[band],\n\t\t\t\t\t\t     nla_data(attr),\n\t\t\t\t\t\t     nla_len(attr),\n\t\t\t\t\t\t     &request->rates[band]);\n\t\t\tif (err)\n\t\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\tif (info->attrs[NL80211_ATTR_MEASUREMENT_DURATION]) {\n\t\trequest->duration =\n\t\t\tnla_get_u16(info->attrs[NL80211_ATTR_MEASUREMENT_DURATION]);\n\t\trequest->duration_mandatory =\n\t\t\tnla_get_flag(info->attrs[NL80211_ATTR_MEASUREMENT_DURATION_MANDATORY]);\n\t}\n\n\terr = nl80211_check_scan_flags(wiphy, wdev, request, info->attrs,\n\t\t\t\t       false);\n\tif (err)\n\t\tgoto out_free;\n\n\trequest->no_cck =\n\t\tnla_get_flag(info->attrs[NL80211_ATTR_TX_NO_CCK_RATE]);\n\n\t/* Initial implementation used NL80211_ATTR_MAC to set the specific\n\t * BSSID to scan for. This was problematic because that same attribute\n\t * was already used for another purpose (local random MAC address). The\n\t * NL80211_ATTR_BSSID attribute was added to fix this. For backwards\n\t * compatibility with older userspace components, also use the\n\t * NL80211_ATTR_MAC value here if it can be determined to be used for\n\t * the specific BSSID use case instead of the random MAC address\n\t * (NL80211_ATTR_SCAN_FLAGS is used to enable random MAC address use).\n\t */\n\tif (info->attrs[NL80211_ATTR_BSSID])\n\t\tmemcpy(request->bssid,\n\t\t       nla_data(info->attrs[NL80211_ATTR_BSSID]), ETH_ALEN);\n\telse if (!(request->flags & NL80211_SCAN_FLAG_RANDOM_ADDR) &&\n\t\t info->attrs[NL80211_ATTR_MAC])\n\t\tmemcpy(request->bssid, nla_data(info->attrs[NL80211_ATTR_MAC]),\n\t\t       ETH_ALEN);\n\telse\n\t\teth_broadcast_addr(request->bssid);\n\n\trequest->wdev = wdev;\n\trequest->wiphy = &rdev->wiphy;\n\trequest->scan_start = jiffies;\n\n\trdev->scan_req = request;\n\terr = cfg80211_scan(rdev);\n\n\tif (err)\n\t\tgoto out_free;\n\n\tnl80211_send_scan_start(rdev, wdev);\n\tif (wdev->netdev)\n\t\tdev_hold(wdev->netdev);\n\n\treturn 0;\n\n out_free:\n\trdev->scan_req = NULL;\n\tkfree(request);\n\n\treturn err;\n}\n\nstatic int nl80211_abort_scan(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev = info->user_ptr[1];\n\n\tif (!rdev->ops->abort_scan)\n\t\treturn -EOPNOTSUPP;\n\n\tif (rdev->scan_msg)\n\t\treturn 0;\n\n\tif (!rdev->scan_req)\n\t\treturn -ENOENT;\n\n\trdev_abort_scan(rdev, wdev);\n\treturn 0;\n}\n\nstatic int\nnl80211_parse_sched_scan_plans(struct wiphy *wiphy, int n_plans,\n\t\t\t       struct cfg80211_sched_scan_request *request,\n\t\t\t       struct nlattr **attrs)\n{\n\tint tmp, err, i = 0;\n\tstruct nlattr *attr;\n\n\tif (!attrs[NL80211_ATTR_SCHED_SCAN_PLANS]) {\n\t\tu32 interval;\n\n\t\t/*\n\t\t * If scan plans are not specified,\n\t\t * %NL80211_ATTR_SCHED_SCAN_INTERVAL will be specified. In this\n\t\t * case one scan plan will be set with the specified scan\n\t\t * interval and infinite number of iterations.\n\t\t */\n\t\tinterval = nla_get_u32(attrs[NL80211_ATTR_SCHED_SCAN_INTERVAL]);\n\t\tif (!interval)\n\t\t\treturn -EINVAL;\n\n\t\trequest->scan_plans[0].interval =\n\t\t\tDIV_ROUND_UP(interval, MSEC_PER_SEC);\n\t\tif (!request->scan_plans[0].interval)\n\t\t\treturn -EINVAL;\n\n\t\tif (request->scan_plans[0].interval >\n\t\t    wiphy->max_sched_scan_plan_interval)\n\t\t\trequest->scan_plans[0].interval =\n\t\t\t\twiphy->max_sched_scan_plan_interval;\n\n\t\treturn 0;\n\t}\n\n\tnla_for_each_nested(attr, attrs[NL80211_ATTR_SCHED_SCAN_PLANS], tmp) {\n\t\tstruct nlattr *plan[NL80211_SCHED_SCAN_PLAN_MAX + 1];\n\n\t\tif (WARN_ON(i >= n_plans))\n\t\t\treturn -EINVAL;\n\n\t\terr = nla_parse_nested_deprecated(plan,\n\t\t\t\t\t\t  NL80211_SCHED_SCAN_PLAN_MAX,\n\t\t\t\t\t\t  attr, nl80211_plan_policy,\n\t\t\t\t\t\t  NULL);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (!plan[NL80211_SCHED_SCAN_PLAN_INTERVAL])\n\t\t\treturn -EINVAL;\n\n\t\trequest->scan_plans[i].interval =\n\t\t\tnla_get_u32(plan[NL80211_SCHED_SCAN_PLAN_INTERVAL]);\n\t\tif (!request->scan_plans[i].interval ||\n\t\t    request->scan_plans[i].interval >\n\t\t    wiphy->max_sched_scan_plan_interval)\n\t\t\treturn -EINVAL;\n\n\t\tif (plan[NL80211_SCHED_SCAN_PLAN_ITERATIONS]) {\n\t\t\trequest->scan_plans[i].iterations =\n\t\t\t\tnla_get_u32(plan[NL80211_SCHED_SCAN_PLAN_ITERATIONS]);\n\t\t\tif (!request->scan_plans[i].iterations ||\n\t\t\t    (request->scan_plans[i].iterations >\n\t\t\t     wiphy->max_sched_scan_plan_iterations))\n\t\t\t\treturn -EINVAL;\n\t\t} else if (i < n_plans - 1) {\n\t\t\t/*\n\t\t\t * All scan plans but the last one must specify\n\t\t\t * a finite number of iterations\n\t\t\t */\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\ti++;\n\t}\n\n\t/*\n\t * The last scan plan must not specify the number of\n\t * iterations, it is supposed to run infinitely\n\t */\n\tif (request->scan_plans[n_plans - 1].iterations)\n\t\treturn  -EINVAL;\n\n\treturn 0;\n}\n\nstatic int\nnl80211_parse_sched_scan_per_band_rssi(struct wiphy *wiphy,\n\t\t\t\t       struct cfg80211_match_set *match_sets,\n\t\t\t\t       struct nlattr *tb_band_rssi,\n\t\t\t\t       s32 rssi_thold)\n{\n\tstruct nlattr *attr;\n\tint i, tmp, ret = 0;\n\n\tif (!wiphy_ext_feature_isset(wiphy,\n\t\t    NL80211_EXT_FEATURE_SCHED_SCAN_BAND_SPECIFIC_RSSI_THOLD)) {\n\t\tif (tb_band_rssi)\n\t\t\tret = -EOPNOTSUPP;\n\t\telse\n\t\t\tfor (i = 0; i < NUM_NL80211_BANDS; i++)\n\t\t\t\tmatch_sets->per_band_rssi_thold[i] =\n\t\t\t\t\tNL80211_SCAN_RSSI_THOLD_OFF;\n\t\treturn ret;\n\t}\n\n\tfor (i = 0; i < NUM_NL80211_BANDS; i++)\n\t\tmatch_sets->per_band_rssi_thold[i] = rssi_thold;\n\n\tnla_for_each_nested(attr, tb_band_rssi, tmp) {\n\t\tenum nl80211_band band = nla_type(attr);\n\n\t\tif (band < 0 || band >= NUM_NL80211_BANDS)\n\t\t\treturn -EINVAL;\n\n\t\tmatch_sets->per_band_rssi_thold[band] =\tnla_get_s32(attr);\n\t}\n\n\treturn 0;\n}\n\nstatic struct cfg80211_sched_scan_request *\nnl80211_parse_sched_scan(struct wiphy *wiphy, struct wireless_dev *wdev,\n\t\t\t struct nlattr **attrs, int max_match_sets)\n{\n\tstruct cfg80211_sched_scan_request *request;\n\tstruct nlattr *attr;\n\tint err, tmp, n_ssids = 0, n_match_sets = 0, n_channels, i, n_plans = 0;\n\tenum nl80211_band band;\n\tsize_t ie_len;\n\tstruct nlattr *tb[NL80211_SCHED_SCAN_MATCH_ATTR_MAX + 1];\n\ts32 default_match_rssi = NL80211_SCAN_RSSI_THOLD_OFF;\n\n\tif (attrs[NL80211_ATTR_SCAN_FREQUENCIES]) {\n\t\tn_channels = validate_scan_freqs(\n\t\t\t\tattrs[NL80211_ATTR_SCAN_FREQUENCIES]);\n\t\tif (!n_channels)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t} else {\n\t\tn_channels = ieee80211_get_num_supported_channels(wiphy);\n\t}\n\n\tif (attrs[NL80211_ATTR_SCAN_SSIDS])\n\t\tnla_for_each_nested(attr, attrs[NL80211_ATTR_SCAN_SSIDS],\n\t\t\t\t    tmp)\n\t\t\tn_ssids++;\n\n\tif (n_ssids > wiphy->max_sched_scan_ssids)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * First, count the number of 'real' matchsets. Due to an issue with\n\t * the old implementation, matchsets containing only the RSSI attribute\n\t * (NL80211_SCHED_SCAN_MATCH_ATTR_RSSI) are considered as the 'default'\n\t * RSSI for all matchsets, rather than their own matchset for reporting\n\t * all APs with a strong RSSI. This is needed to be compatible with\n\t * older userspace that treated a matchset with only the RSSI as the\n\t * global RSSI for all other matchsets - if there are other matchsets.\n\t */\n\tif (attrs[NL80211_ATTR_SCHED_SCAN_MATCH]) {\n\t\tnla_for_each_nested(attr,\n\t\t\t\t    attrs[NL80211_ATTR_SCHED_SCAN_MATCH],\n\t\t\t\t    tmp) {\n\t\t\tstruct nlattr *rssi;\n\n\t\t\terr = nla_parse_nested_deprecated(tb,\n\t\t\t\t\t\t\t  NL80211_SCHED_SCAN_MATCH_ATTR_MAX,\n\t\t\t\t\t\t\t  attr,\n\t\t\t\t\t\t\t  nl80211_match_policy,\n\t\t\t\t\t\t\t  NULL);\n\t\t\tif (err)\n\t\t\t\treturn ERR_PTR(err);\n\n\t\t\t/* SSID and BSSID are mutually exclusive */\n\t\t\tif (tb[NL80211_SCHED_SCAN_MATCH_ATTR_SSID] &&\n\t\t\t    tb[NL80211_SCHED_SCAN_MATCH_ATTR_BSSID])\n\t\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\t\t/* add other standalone attributes here */\n\t\t\tif (tb[NL80211_SCHED_SCAN_MATCH_ATTR_SSID] ||\n\t\t\t    tb[NL80211_SCHED_SCAN_MATCH_ATTR_BSSID]) {\n\t\t\t\tn_match_sets++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\trssi = tb[NL80211_SCHED_SCAN_MATCH_ATTR_RSSI];\n\t\t\tif (rssi)\n\t\t\t\tdefault_match_rssi = nla_get_s32(rssi);\n\t\t}\n\t}\n\n\t/* However, if there's no other matchset, add the RSSI one */\n\tif (!n_match_sets && default_match_rssi != NL80211_SCAN_RSSI_THOLD_OFF)\n\t\tn_match_sets = 1;\n\n\tif (n_match_sets > max_match_sets)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (attrs[NL80211_ATTR_IE])\n\t\tie_len = nla_len(attrs[NL80211_ATTR_IE]);\n\telse\n\t\tie_len = 0;\n\n\tif (ie_len > wiphy->max_sched_scan_ie_len)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (attrs[NL80211_ATTR_SCHED_SCAN_PLANS]) {\n\t\t/*\n\t\t * NL80211_ATTR_SCHED_SCAN_INTERVAL must not be specified since\n\t\t * each scan plan already specifies its own interval\n\t\t */\n\t\tif (attrs[NL80211_ATTR_SCHED_SCAN_INTERVAL])\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tnla_for_each_nested(attr,\n\t\t\t\t    attrs[NL80211_ATTR_SCHED_SCAN_PLANS], tmp)\n\t\t\tn_plans++;\n\t} else {\n\t\t/*\n\t\t * The scan interval attribute is kept for backward\n\t\t * compatibility. If no scan plans are specified and sched scan\n\t\t * interval is specified, one scan plan will be set with this\n\t\t * scan interval and infinite number of iterations.\n\t\t */\n\t\tif (!attrs[NL80211_ATTR_SCHED_SCAN_INTERVAL])\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tn_plans = 1;\n\t}\n\n\tif (!n_plans || n_plans > wiphy->max_sched_scan_plans)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!wiphy_ext_feature_isset(\n\t\t    wiphy, NL80211_EXT_FEATURE_SCHED_SCAN_RELATIVE_RSSI) &&\n\t    (attrs[NL80211_ATTR_SCHED_SCAN_RELATIVE_RSSI] ||\n\t     attrs[NL80211_ATTR_SCHED_SCAN_RSSI_ADJUST]))\n\t\treturn ERR_PTR(-EINVAL);\n\n\trequest = kzalloc(sizeof(*request)\n\t\t\t+ sizeof(*request->ssids) * n_ssids\n\t\t\t+ sizeof(*request->match_sets) * n_match_sets\n\t\t\t+ sizeof(*request->scan_plans) * n_plans\n\t\t\t+ sizeof(*request->channels) * n_channels\n\t\t\t+ ie_len, GFP_KERNEL);\n\tif (!request)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (n_ssids)\n\t\trequest->ssids = (void *)&request->channels[n_channels];\n\trequest->n_ssids = n_ssids;\n\tif (ie_len) {\n\t\tif (n_ssids)\n\t\t\trequest->ie = (void *)(request->ssids + n_ssids);\n\t\telse\n\t\t\trequest->ie = (void *)(request->channels + n_channels);\n\t}\n\n\tif (n_match_sets) {\n\t\tif (request->ie)\n\t\t\trequest->match_sets = (void *)(request->ie + ie_len);\n\t\telse if (n_ssids)\n\t\t\trequest->match_sets =\n\t\t\t\t(void *)(request->ssids + n_ssids);\n\t\telse\n\t\t\trequest->match_sets =\n\t\t\t\t(void *)(request->channels + n_channels);\n\t}\n\trequest->n_match_sets = n_match_sets;\n\n\tif (n_match_sets)\n\t\trequest->scan_plans = (void *)(request->match_sets +\n\t\t\t\t\t       n_match_sets);\n\telse if (request->ie)\n\t\trequest->scan_plans = (void *)(request->ie + ie_len);\n\telse if (n_ssids)\n\t\trequest->scan_plans = (void *)(request->ssids + n_ssids);\n\telse\n\t\trequest->scan_plans = (void *)(request->channels + n_channels);\n\n\trequest->n_scan_plans = n_plans;\n\n\ti = 0;\n\tif (attrs[NL80211_ATTR_SCAN_FREQUENCIES]) {\n\t\t/* user specified, bail out if channel not found */\n\t\tnla_for_each_nested(attr,\n\t\t\t\t    attrs[NL80211_ATTR_SCAN_FREQUENCIES],\n\t\t\t\t    tmp) {\n\t\t\tstruct ieee80211_channel *chan;\n\n\t\t\tchan = ieee80211_get_channel(wiphy, nla_get_u32(attr));\n\n\t\t\tif (!chan) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\n\t\t\t/* ignore disabled channels */\n\t\t\tif (chan->flags & IEEE80211_CHAN_DISABLED)\n\t\t\t\tcontinue;\n\n\t\t\trequest->channels[i] = chan;\n\t\t\ti++;\n\t\t}\n\t} else {\n\t\t/* all channels */\n\t\tfor (band = 0; band < NUM_NL80211_BANDS; band++) {\n\t\t\tint j;\n\n\t\t\tif (!wiphy->bands[band])\n\t\t\t\tcontinue;\n\t\t\tfor (j = 0; j < wiphy->bands[band]->n_channels; j++) {\n\t\t\t\tstruct ieee80211_channel *chan;\n\n\t\t\t\tchan = &wiphy->bands[band]->channels[j];\n\n\t\t\t\tif (chan->flags & IEEE80211_CHAN_DISABLED)\n\t\t\t\t\tcontinue;\n\n\t\t\t\trequest->channels[i] = chan;\n\t\t\t\ti++;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!i) {\n\t\terr = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\trequest->n_channels = i;\n\n\ti = 0;\n\tif (n_ssids) {\n\t\tnla_for_each_nested(attr, attrs[NL80211_ATTR_SCAN_SSIDS],\n\t\t\t\t    tmp) {\n\t\t\tif (nla_len(attr) > IEEE80211_MAX_SSID_LEN) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t\trequest->ssids[i].ssid_len = nla_len(attr);\n\t\t\tmemcpy(request->ssids[i].ssid, nla_data(attr),\n\t\t\t       nla_len(attr));\n\t\t\ti++;\n\t\t}\n\t}\n\n\ti = 0;\n\tif (attrs[NL80211_ATTR_SCHED_SCAN_MATCH]) {\n\t\tnla_for_each_nested(attr,\n\t\t\t\t    attrs[NL80211_ATTR_SCHED_SCAN_MATCH],\n\t\t\t\t    tmp) {\n\t\t\tstruct nlattr *ssid, *bssid, *rssi;\n\n\t\t\terr = nla_parse_nested_deprecated(tb,\n\t\t\t\t\t\t\t  NL80211_SCHED_SCAN_MATCH_ATTR_MAX,\n\t\t\t\t\t\t\t  attr,\n\t\t\t\t\t\t\t  nl80211_match_policy,\n\t\t\t\t\t\t\t  NULL);\n\t\t\tif (err)\n\t\t\t\tgoto out_free;\n\t\t\tssid = tb[NL80211_SCHED_SCAN_MATCH_ATTR_SSID];\n\t\t\tbssid = tb[NL80211_SCHED_SCAN_MATCH_ATTR_BSSID];\n\n\t\t\tif (!ssid && !bssid) {\n\t\t\t\ti++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (WARN_ON(i >= n_match_sets)) {\n\t\t\t\t/* this indicates a programming error,\n\t\t\t\t * the loop above should have verified\n\t\t\t\t * things properly\n\t\t\t\t */\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\n\t\t\tif (ssid) {\n\t\t\t\tmemcpy(request->match_sets[i].ssid.ssid,\n\t\t\t\t       nla_data(ssid), nla_len(ssid));\n\t\t\t\trequest->match_sets[i].ssid.ssid_len =\n\t\t\t\t\tnla_len(ssid);\n\t\t\t}\n\t\t\tif (bssid)\n\t\t\t\tmemcpy(request->match_sets[i].bssid,\n\t\t\t\t       nla_data(bssid), ETH_ALEN);\n\n\t\t\t/* special attribute - old implementation w/a */\n\t\t\trequest->match_sets[i].rssi_thold = default_match_rssi;\n\t\t\trssi = tb[NL80211_SCHED_SCAN_MATCH_ATTR_RSSI];\n\t\t\tif (rssi)\n\t\t\t\trequest->match_sets[i].rssi_thold =\n\t\t\t\t\tnla_get_s32(rssi);\n\n\t\t\t/* Parse per band RSSI attribute */\n\t\t\terr = nl80211_parse_sched_scan_per_band_rssi(wiphy,\n\t\t\t\t&request->match_sets[i],\n\t\t\t\ttb[NL80211_SCHED_SCAN_MATCH_PER_BAND_RSSI],\n\t\t\t\trequest->match_sets[i].rssi_thold);\n\t\t\tif (err)\n\t\t\t\tgoto out_free;\n\n\t\t\ti++;\n\t\t}\n\n\t\t/* there was no other matchset, so the RSSI one is alone */\n\t\tif (i == 0 && n_match_sets)\n\t\t\trequest->match_sets[0].rssi_thold = default_match_rssi;\n\n\t\trequest->min_rssi_thold = INT_MAX;\n\t\tfor (i = 0; i < n_match_sets; i++)\n\t\t\trequest->min_rssi_thold =\n\t\t\t\tmin(request->match_sets[i].rssi_thold,\n\t\t\t\t    request->min_rssi_thold);\n\t} else {\n\t\trequest->min_rssi_thold = NL80211_SCAN_RSSI_THOLD_OFF;\n\t}\n\n\tif (ie_len) {\n\t\trequest->ie_len = ie_len;\n\t\tmemcpy((void *)request->ie,\n\t\t       nla_data(attrs[NL80211_ATTR_IE]),\n\t\t       request->ie_len);\n\t}\n\n\terr = nl80211_check_scan_flags(wiphy, wdev, request, attrs, true);\n\tif (err)\n\t\tgoto out_free;\n\n\tif (attrs[NL80211_ATTR_SCHED_SCAN_DELAY])\n\t\trequest->delay =\n\t\t\tnla_get_u32(attrs[NL80211_ATTR_SCHED_SCAN_DELAY]);\n\n\tif (attrs[NL80211_ATTR_SCHED_SCAN_RELATIVE_RSSI]) {\n\t\trequest->relative_rssi = nla_get_s8(\n\t\t\tattrs[NL80211_ATTR_SCHED_SCAN_RELATIVE_RSSI]);\n\t\trequest->relative_rssi_set = true;\n\t}\n\n\tif (request->relative_rssi_set &&\n\t    attrs[NL80211_ATTR_SCHED_SCAN_RSSI_ADJUST]) {\n\t\tstruct nl80211_bss_select_rssi_adjust *rssi_adjust;\n\n\t\trssi_adjust = nla_data(\n\t\t\tattrs[NL80211_ATTR_SCHED_SCAN_RSSI_ADJUST]);\n\t\trequest->rssi_adjust.band = rssi_adjust->band;\n\t\trequest->rssi_adjust.delta = rssi_adjust->delta;\n\t\tif (!is_band_valid(wiphy, request->rssi_adjust.band)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\terr = nl80211_parse_sched_scan_plans(wiphy, n_plans, request, attrs);\n\tif (err)\n\t\tgoto out_free;\n\n\trequest->scan_start = jiffies;\n\n\treturn request;\n\nout_free:\n\tkfree(request);\n\treturn ERR_PTR(err);\n}\n\nstatic int nl80211_start_sched_scan(struct sk_buff *skb,\n\t\t\t\t    struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct cfg80211_sched_scan_request *sched_scan_req;\n\tbool want_multi;\n\tint err;\n\n\tif (!rdev->wiphy.max_sched_scan_reqs || !rdev->ops->sched_scan_start)\n\t\treturn -EOPNOTSUPP;\n\n\twant_multi = info->attrs[NL80211_ATTR_SCHED_SCAN_MULTI];\n\terr = cfg80211_sched_scan_req_possible(rdev, want_multi);\n\tif (err)\n\t\treturn err;\n\n\tsched_scan_req = nl80211_parse_sched_scan(&rdev->wiphy, wdev,\n\t\t\t\t\t\t  info->attrs,\n\t\t\t\t\t\t  rdev->wiphy.max_match_sets);\n\n\terr = PTR_ERR_OR_ZERO(sched_scan_req);\n\tif (err)\n\t\tgoto out_err;\n\n\t/* leave request id zero for legacy request\n\t * or if driver does not support multi-scheduled scan\n\t */\n\tif (want_multi && rdev->wiphy.max_sched_scan_reqs > 1)\n\t\tsched_scan_req->reqid = cfg80211_assign_cookie(rdev);\n\n\terr = rdev_sched_scan_start(rdev, dev, sched_scan_req);\n\tif (err)\n\t\tgoto out_free;\n\n\tsched_scan_req->dev = dev;\n\tsched_scan_req->wiphy = &rdev->wiphy;\n\n\tif (info->attrs[NL80211_ATTR_SOCKET_OWNER])\n\t\tsched_scan_req->owner_nlportid = info->snd_portid;\n\n\tcfg80211_add_sched_scan_req(rdev, sched_scan_req);\n\n\tnl80211_send_sched_scan(sched_scan_req, NL80211_CMD_START_SCHED_SCAN);\n\treturn 0;\n\nout_free:\n\tkfree(sched_scan_req);\nout_err:\n\treturn err;\n}\n\nstatic int nl80211_stop_sched_scan(struct sk_buff *skb,\n\t\t\t\t   struct genl_info *info)\n{\n\tstruct cfg80211_sched_scan_request *req;\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tu64 cookie;\n\n\tif (!rdev->wiphy.max_sched_scan_reqs || !rdev->ops->sched_scan_stop)\n\t\treturn -EOPNOTSUPP;\n\n\tif (info->attrs[NL80211_ATTR_COOKIE]) {\n\t\tcookie = nla_get_u64(info->attrs[NL80211_ATTR_COOKIE]);\n\t\treturn __cfg80211_stop_sched_scan(rdev, cookie, false);\n\t}\n\n\treq = list_first_or_null_rcu(&rdev->sched_scan_req_list,\n\t\t\t\t     struct cfg80211_sched_scan_request,\n\t\t\t\t     list);\n\tif (!req || req->reqid ||\n\t    (req->owner_nlportid &&\n\t     req->owner_nlportid != info->snd_portid))\n\t\treturn -ENOENT;\n\n\treturn cfg80211_stop_sched_scan_req(rdev, req, false);\n}\n\nstatic int nl80211_start_radar_detection(struct sk_buff *skb,\n\t\t\t\t\t struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct wiphy *wiphy = wdev->wiphy;\n\tstruct cfg80211_chan_def chandef;\n\tenum nl80211_dfs_regions dfs_region;\n\tunsigned int cac_time_ms;\n\tint err;\n\n\tdfs_region = reg_get_dfs_region(wiphy);\n\tif (dfs_region == NL80211_DFS_UNSET)\n\t\treturn -EINVAL;\n\n\terr = nl80211_parse_chandef(rdev, info, &chandef);\n\tif (err)\n\t\treturn err;\n\n\tif (netif_carrier_ok(dev))\n\t\treturn -EBUSY;\n\n\tif (wdev->cac_started)\n\t\treturn -EBUSY;\n\n\terr = cfg80211_chandef_dfs_required(wiphy, &chandef, wdev->iftype);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (err == 0)\n\t\treturn -EINVAL;\n\n\tif (!cfg80211_chandef_dfs_usable(wiphy, &chandef))\n\t\treturn -EINVAL;\n\n\t/* CAC start is offloaded to HW and can't be started manually */\n\tif (wiphy_ext_feature_isset(wiphy, NL80211_EXT_FEATURE_DFS_OFFLOAD))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!rdev->ops->start_radar_detection)\n\t\treturn -EOPNOTSUPP;\n\n\tcac_time_ms = cfg80211_chandef_dfs_cac_time(&rdev->wiphy, &chandef);\n\tif (WARN_ON(!cac_time_ms))\n\t\tcac_time_ms = IEEE80211_DFS_MIN_CAC_TIME_MS;\n\n\terr = rdev_start_radar_detection(rdev, dev, &chandef, cac_time_ms);\n\tif (!err) {\n\t\twdev->chandef = chandef;\n\t\twdev->cac_started = true;\n\t\twdev->cac_start_time = jiffies;\n\t\twdev->cac_time_ms = cac_time_ms;\n\t}\n\treturn err;\n}\n\nstatic int nl80211_notify_radar_detection(struct sk_buff *skb,\n\t\t\t\t\t  struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct wiphy *wiphy = wdev->wiphy;\n\tstruct cfg80211_chan_def chandef;\n\tenum nl80211_dfs_regions dfs_region;\n\tint err;\n\n\tdfs_region = reg_get_dfs_region(wiphy);\n\tif (dfs_region == NL80211_DFS_UNSET) {\n\t\tGENL_SET_ERR_MSG(info,\n\t\t\t\t \"DFS Region is not set. Unexpected Radar indication\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = nl80211_parse_chandef(rdev, info, &chandef);\n\tif (err) {\n\t\tGENL_SET_ERR_MSG(info, \"Unable to extract chandef info\");\n\t\treturn err;\n\t}\n\n\terr = cfg80211_chandef_dfs_required(wiphy, &chandef, wdev->iftype);\n\tif (err < 0) {\n\t\tGENL_SET_ERR_MSG(info, \"chandef is invalid\");\n\t\treturn err;\n\t}\n\n\tif (err == 0) {\n\t\tGENL_SET_ERR_MSG(info,\n\t\t\t\t \"Unexpected Radar indication for chandef/iftype\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* Do not process this notification if radar is already detected\n\t * by kernel on this channel, and return success.\n\t */\n\tif (chandef.chan->dfs_state == NL80211_DFS_UNAVAILABLE)\n\t\treturn 0;\n\n\tcfg80211_set_dfs_state(wiphy, &chandef, NL80211_DFS_UNAVAILABLE);\n\n\tcfg80211_sched_dfs_chan_update(rdev);\n\n\trdev->radar_chandef = chandef;\n\n\t/* Propagate this notification to other radios as well */\n\tqueue_work(cfg80211_wq, &rdev->propagate_radar_detect_wk);\n\n\treturn 0;\n}\n\nstatic int nl80211_channel_switch(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct cfg80211_csa_settings params;\n\tstruct nlattr **csa_attrs = NULL;\n\tint err;\n\tbool need_new_beacon = false;\n\tbool need_handle_dfs_flag = true;\n\tint len, i;\n\tu32 cs_count;\n\n\tif (!rdev->ops->channel_switch ||\n\t    !(rdev->wiphy.flags & WIPHY_FLAG_HAS_CHANNEL_SWITCH))\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (dev->ieee80211_ptr->iftype) {\n\tcase NL80211_IFTYPE_AP:\n\tcase NL80211_IFTYPE_P2P_GO:\n\t\tneed_new_beacon = true;\n\t\t/* For all modes except AP the handle_dfs flag needs to be\n\t\t * supplied to tell the kernel that userspace will handle radar\n\t\t * events when they happen. Otherwise a switch to a channel\n\t\t * requiring DFS will be rejected.\n\t\t */\n\t\tneed_handle_dfs_flag = false;\n\n\t\t/* useless if AP is not running */\n\t\tif (!wdev->beacon_interval)\n\t\t\treturn -ENOTCONN;\n\t\tbreak;\n\tcase NL80211_IFTYPE_ADHOC:\n\t\tif (!wdev->ssid_len)\n\t\t\treturn -ENOTCONN;\n\t\tbreak;\n\tcase NL80211_IFTYPE_MESH_POINT:\n\t\tif (!wdev->mesh_id_len)\n\t\t\treturn -ENOTCONN;\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmemset(&params, 0, sizeof(params));\n\tparams.beacon_csa.ftm_responder = -1;\n\n\tif (!info->attrs[NL80211_ATTR_WIPHY_FREQ] ||\n\t    !info->attrs[NL80211_ATTR_CH_SWITCH_COUNT])\n\t\treturn -EINVAL;\n\n\t/* only important for AP, IBSS and mesh create IEs internally */\n\tif (need_new_beacon && !info->attrs[NL80211_ATTR_CSA_IES])\n\t\treturn -EINVAL;\n\n\t/* Even though the attribute is u32, the specification says\n\t * u8, so let's make sure we don't overflow.\n\t */\n\tcs_count = nla_get_u32(info->attrs[NL80211_ATTR_CH_SWITCH_COUNT]);\n\tif (cs_count > 255)\n\t\treturn -EINVAL;\n\n\tparams.count = cs_count;\n\n\tif (!need_new_beacon)\n\t\tgoto skip_beacons;\n\n\terr = nl80211_parse_beacon(rdev, info->attrs, &params.beacon_after);\n\tif (err)\n\t\treturn err;\n\n\tcsa_attrs = kcalloc(NL80211_ATTR_MAX + 1, sizeof(*csa_attrs),\n\t\t\t    GFP_KERNEL);\n\tif (!csa_attrs)\n\t\treturn -ENOMEM;\n\n\terr = nla_parse_nested_deprecated(csa_attrs, NL80211_ATTR_MAX,\n\t\t\t\t\t  info->attrs[NL80211_ATTR_CSA_IES],\n\t\t\t\t\t  nl80211_policy, info->extack);\n\tif (err)\n\t\tgoto free;\n\n\terr = nl80211_parse_beacon(rdev, csa_attrs, &params.beacon_csa);\n\tif (err)\n\t\tgoto free;\n\n\tif (!csa_attrs[NL80211_ATTR_CNTDWN_OFFS_BEACON]) {\n\t\terr = -EINVAL;\n\t\tgoto free;\n\t}\n\n\tlen = nla_len(csa_attrs[NL80211_ATTR_CNTDWN_OFFS_BEACON]);\n\tif (!len || (len % sizeof(u16))) {\n\t\terr = -EINVAL;\n\t\tgoto free;\n\t}\n\n\tparams.n_counter_offsets_beacon = len / sizeof(u16);\n\tif (rdev->wiphy.max_num_csa_counters &&\n\t    (params.n_counter_offsets_beacon >\n\t     rdev->wiphy.max_num_csa_counters)) {\n\t\terr = -EINVAL;\n\t\tgoto free;\n\t}\n\n\tparams.counter_offsets_beacon =\n\t\tnla_data(csa_attrs[NL80211_ATTR_CNTDWN_OFFS_BEACON]);\n\n\t/* sanity checks - counters should fit and be the same */\n\tfor (i = 0; i < params.n_counter_offsets_beacon; i++) {\n\t\tu16 offset = params.counter_offsets_beacon[i];\n\n\t\tif (offset >= params.beacon_csa.tail_len) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto free;\n\t\t}\n\n\t\tif (params.beacon_csa.tail[offset] != params.count) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto free;\n\t\t}\n\t}\n\n\tif (csa_attrs[NL80211_ATTR_CNTDWN_OFFS_PRESP]) {\n\t\tlen = nla_len(csa_attrs[NL80211_ATTR_CNTDWN_OFFS_PRESP]);\n\t\tif (!len || (len % sizeof(u16))) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto free;\n\t\t}\n\n\t\tparams.n_counter_offsets_presp = len / sizeof(u16);\n\t\tif (rdev->wiphy.max_num_csa_counters &&\n\t\t    (params.n_counter_offsets_presp >\n\t\t     rdev->wiphy.max_num_csa_counters)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto free;\n\t\t}\n\n\t\tparams.counter_offsets_presp =\n\t\t\tnla_data(csa_attrs[NL80211_ATTR_CNTDWN_OFFS_PRESP]);\n\n\t\t/* sanity checks - counters should fit and be the same */\n\t\tfor (i = 0; i < params.n_counter_offsets_presp; i++) {\n\t\t\tu16 offset = params.counter_offsets_presp[i];\n\n\t\t\tif (offset >= params.beacon_csa.probe_resp_len) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto free;\n\t\t\t}\n\n\t\t\tif (params.beacon_csa.probe_resp[offset] !=\n\t\t\t    params.count) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto free;\n\t\t\t}\n\t\t}\n\t}\n\nskip_beacons:\n\terr = nl80211_parse_chandef(rdev, info, &params.chandef);\n\tif (err)\n\t\tgoto free;\n\n\tif (!cfg80211_reg_can_beacon_relax(&rdev->wiphy, &params.chandef,\n\t\t\t\t\t   wdev->iftype)) {\n\t\terr = -EINVAL;\n\t\tgoto free;\n\t}\n\n\terr = cfg80211_chandef_dfs_required(wdev->wiphy,\n\t\t\t\t\t    &params.chandef,\n\t\t\t\t\t    wdev->iftype);\n\tif (err < 0)\n\t\tgoto free;\n\n\tif (err > 0) {\n\t\tparams.radar_required = true;\n\t\tif (need_handle_dfs_flag &&\n\t\t    !nla_get_flag(info->attrs[NL80211_ATTR_HANDLE_DFS])) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto free;\n\t\t}\n\t}\n\n\tif (info->attrs[NL80211_ATTR_CH_SWITCH_BLOCK_TX])\n\t\tparams.block_tx = true;\n\n\twdev_lock(wdev);\n\terr = rdev_channel_switch(rdev, dev, &params);\n\twdev_unlock(wdev);\n\nfree:\n\tkfree(csa_attrs);\n\treturn err;\n}\n\nstatic int nl80211_send_bss(struct sk_buff *msg, struct netlink_callback *cb,\n\t\t\t    u32 seq, int flags,\n\t\t\t    struct cfg80211_registered_device *rdev,\n\t\t\t    struct wireless_dev *wdev,\n\t\t\t    struct cfg80211_internal_bss *intbss)\n{\n\tstruct cfg80211_bss *res = &intbss->pub;\n\tconst struct cfg80211_bss_ies *ies;\n\tvoid *hdr;\n\tstruct nlattr *bss;\n\n\tASSERT_WDEV_LOCK(wdev);\n\n\thdr = nl80211hdr_put(msg, NETLINK_CB(cb->skb).portid, seq, flags,\n\t\t\t     NL80211_CMD_NEW_SCAN_RESULTS);\n\tif (!hdr)\n\t\treturn -1;\n\n\tgenl_dump_check_consistent(cb, hdr);\n\n\tif (nla_put_u32(msg, NL80211_ATTR_GENERATION, rdev->bss_generation))\n\t\tgoto nla_put_failure;\n\tif (wdev->netdev &&\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, wdev->netdev->ifindex))\n\t\tgoto nla_put_failure;\n\tif (nla_put_u64_64bit(msg, NL80211_ATTR_WDEV, wdev_id(wdev),\n\t\t\t      NL80211_ATTR_PAD))\n\t\tgoto nla_put_failure;\n\n\tbss = nla_nest_start_noflag(msg, NL80211_ATTR_BSS);\n\tif (!bss)\n\t\tgoto nla_put_failure;\n\tif ((!is_zero_ether_addr(res->bssid) &&\n\t     nla_put(msg, NL80211_BSS_BSSID, ETH_ALEN, res->bssid)))\n\t\tgoto nla_put_failure;\n\n\trcu_read_lock();\n\t/* indicate whether we have probe response data or not */\n\tif (rcu_access_pointer(res->proberesp_ies) &&\n\t    nla_put_flag(msg, NL80211_BSS_PRESP_DATA))\n\t\tgoto fail_unlock_rcu;\n\n\t/* this pointer prefers to be pointed to probe response data\n\t * but is always valid\n\t */\n\ties = rcu_dereference(res->ies);\n\tif (ies) {\n\t\tif (nla_put_u64_64bit(msg, NL80211_BSS_TSF, ies->tsf,\n\t\t\t\t      NL80211_BSS_PAD))\n\t\t\tgoto fail_unlock_rcu;\n\t\tif (ies->len && nla_put(msg, NL80211_BSS_INFORMATION_ELEMENTS,\n\t\t\t\t\ties->len, ies->data))\n\t\t\tgoto fail_unlock_rcu;\n\t}\n\n\t/* and this pointer is always (unless driver didn't know) beacon data */\n\ties = rcu_dereference(res->beacon_ies);\n\tif (ies && ies->from_beacon) {\n\t\tif (nla_put_u64_64bit(msg, NL80211_BSS_BEACON_TSF, ies->tsf,\n\t\t\t\t      NL80211_BSS_PAD))\n\t\t\tgoto fail_unlock_rcu;\n\t\tif (ies->len && nla_put(msg, NL80211_BSS_BEACON_IES,\n\t\t\t\t\ties->len, ies->data))\n\t\t\tgoto fail_unlock_rcu;\n\t}\n\trcu_read_unlock();\n\n\tif (res->beacon_interval &&\n\t    nla_put_u16(msg, NL80211_BSS_BEACON_INTERVAL, res->beacon_interval))\n\t\tgoto nla_put_failure;\n\tif (nla_put_u16(msg, NL80211_BSS_CAPABILITY, res->capability) ||\n\t    nla_put_u32(msg, NL80211_BSS_FREQUENCY, res->channel->center_freq) ||\n\t    nla_put_u32(msg, NL80211_BSS_FREQUENCY_OFFSET,\n\t\t\tres->channel->freq_offset) ||\n\t    nla_put_u32(msg, NL80211_BSS_CHAN_WIDTH, res->scan_width) ||\n\t    nla_put_u32(msg, NL80211_BSS_SEEN_MS_AGO,\n\t\t\tjiffies_to_msecs(jiffies - intbss->ts)))\n\t\tgoto nla_put_failure;\n\n\tif (intbss->parent_tsf &&\n\t    (nla_put_u64_64bit(msg, NL80211_BSS_PARENT_TSF,\n\t\t\t       intbss->parent_tsf, NL80211_BSS_PAD) ||\n\t     nla_put(msg, NL80211_BSS_PARENT_BSSID, ETH_ALEN,\n\t\t     intbss->parent_bssid)))\n\t\tgoto nla_put_failure;\n\n\tif (intbss->ts_boottime &&\n\t    nla_put_u64_64bit(msg, NL80211_BSS_LAST_SEEN_BOOTTIME,\n\t\t\t      intbss->ts_boottime, NL80211_BSS_PAD))\n\t\tgoto nla_put_failure;\n\n\tif (!nl80211_put_signal(msg, intbss->pub.chains,\n\t\t\t\tintbss->pub.chain_signal,\n\t\t\t\tNL80211_BSS_CHAIN_SIGNAL))\n\t\tgoto nla_put_failure;\n\n\tswitch (rdev->wiphy.signal_type) {\n\tcase CFG80211_SIGNAL_TYPE_MBM:\n\t\tif (nla_put_u32(msg, NL80211_BSS_SIGNAL_MBM, res->signal))\n\t\t\tgoto nla_put_failure;\n\t\tbreak;\n\tcase CFG80211_SIGNAL_TYPE_UNSPEC:\n\t\tif (nla_put_u8(msg, NL80211_BSS_SIGNAL_UNSPEC, res->signal))\n\t\t\tgoto nla_put_failure;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tswitch (wdev->iftype) {\n\tcase NL80211_IFTYPE_P2P_CLIENT:\n\tcase NL80211_IFTYPE_STATION:\n\t\tif (intbss == wdev->current_bss &&\n\t\t    nla_put_u32(msg, NL80211_BSS_STATUS,\n\t\t\t\tNL80211_BSS_STATUS_ASSOCIATED))\n\t\t\tgoto nla_put_failure;\n\t\tbreak;\n\tcase NL80211_IFTYPE_ADHOC:\n\t\tif (intbss == wdev->current_bss &&\n\t\t    nla_put_u32(msg, NL80211_BSS_STATUS,\n\t\t\t\tNL80211_BSS_STATUS_IBSS_JOINED))\n\t\t\tgoto nla_put_failure;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tnla_nest_end(msg, bss);\n\n\tgenlmsg_end(msg, hdr);\n\treturn 0;\n\n fail_unlock_rcu:\n\trcu_read_unlock();\n nla_put_failure:\n\tgenlmsg_cancel(msg, hdr);\n\treturn -EMSGSIZE;\n}\n\nstatic int nl80211_dump_scan(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tstruct cfg80211_registered_device *rdev;\n\tstruct cfg80211_internal_bss *scan;\n\tstruct wireless_dev *wdev;\n\tint start = cb->args[2], idx = 0;\n\tint err;\n\n\terr = nl80211_prepare_wdev_dump(cb, &rdev, &wdev);\n\tif (err)\n\t\treturn err;\n\t/* nl80211_prepare_wdev_dump acquired it in the successful case */\n\t__acquire(&rdev->wiphy.mtx);\n\n\twdev_lock(wdev);\n\tspin_lock_bh(&rdev->bss_lock);\n\n\t/*\n\t * dump_scan will be called multiple times to break up the scan results\n\t * into multiple messages.  It is unlikely that any more bss-es will be\n\t * expired after the first call, so only call only call this on the\n\t * first dump_scan invocation.\n\t */\n\tif (start == 0)\n\t\tcfg80211_bss_expire(rdev);\n\n\tcb->seq = rdev->bss_generation;\n\n\tlist_for_each_entry(scan, &rdev->bss_list, list) {\n\t\tif (++idx <= start)\n\t\t\tcontinue;\n\t\tif (nl80211_send_bss(skb, cb,\n\t\t\t\tcb->nlh->nlmsg_seq, NLM_F_MULTI,\n\t\t\t\trdev, wdev, scan) < 0) {\n\t\t\tidx--;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tspin_unlock_bh(&rdev->bss_lock);\n\twdev_unlock(wdev);\n\n\tcb->args[2] = idx;\n\twiphy_unlock(&rdev->wiphy);\n\n\treturn skb->len;\n}\n\nstatic int nl80211_send_survey(struct sk_buff *msg, u32 portid, u32 seq,\n\t\t\t       int flags, struct net_device *dev,\n\t\t\t       bool allow_radio_stats,\n\t\t\t       struct survey_info *survey)\n{\n\tvoid *hdr;\n\tstruct nlattr *infoattr;\n\n\t/* skip radio stats if userspace didn't request them */\n\tif (!survey->channel && !allow_radio_stats)\n\t\treturn 0;\n\n\thdr = nl80211hdr_put(msg, portid, seq, flags,\n\t\t\t     NL80211_CMD_NEW_SURVEY_RESULTS);\n\tif (!hdr)\n\t\treturn -ENOMEM;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_IFINDEX, dev->ifindex))\n\t\tgoto nla_put_failure;\n\n\tinfoattr = nla_nest_start_noflag(msg, NL80211_ATTR_SURVEY_INFO);\n\tif (!infoattr)\n\t\tgoto nla_put_failure;\n\n\tif (survey->channel &&\n\t    nla_put_u32(msg, NL80211_SURVEY_INFO_FREQUENCY,\n\t\t\tsurvey->channel->center_freq))\n\t\tgoto nla_put_failure;\n\n\tif (survey->channel && survey->channel->freq_offset &&\n\t    nla_put_u32(msg, NL80211_SURVEY_INFO_FREQUENCY_OFFSET,\n\t\t\tsurvey->channel->freq_offset))\n\t\tgoto nla_put_failure;\n\n\tif ((survey->filled & SURVEY_INFO_NOISE_DBM) &&\n\t    nla_put_u8(msg, NL80211_SURVEY_INFO_NOISE, survey->noise))\n\t\tgoto nla_put_failure;\n\tif ((survey->filled & SURVEY_INFO_IN_USE) &&\n\t    nla_put_flag(msg, NL80211_SURVEY_INFO_IN_USE))\n\t\tgoto nla_put_failure;\n\tif ((survey->filled & SURVEY_INFO_TIME) &&\n\t    nla_put_u64_64bit(msg, NL80211_SURVEY_INFO_TIME,\n\t\t\tsurvey->time, NL80211_SURVEY_INFO_PAD))\n\t\tgoto nla_put_failure;\n\tif ((survey->filled & SURVEY_INFO_TIME_BUSY) &&\n\t    nla_put_u64_64bit(msg, NL80211_SURVEY_INFO_TIME_BUSY,\n\t\t\t      survey->time_busy, NL80211_SURVEY_INFO_PAD))\n\t\tgoto nla_put_failure;\n\tif ((survey->filled & SURVEY_INFO_TIME_EXT_BUSY) &&\n\t    nla_put_u64_64bit(msg, NL80211_SURVEY_INFO_TIME_EXT_BUSY,\n\t\t\t      survey->time_ext_busy, NL80211_SURVEY_INFO_PAD))\n\t\tgoto nla_put_failure;\n\tif ((survey->filled & SURVEY_INFO_TIME_RX) &&\n\t    nla_put_u64_64bit(msg, NL80211_SURVEY_INFO_TIME_RX,\n\t\t\t      survey->time_rx, NL80211_SURVEY_INFO_PAD))\n\t\tgoto nla_put_failure;\n\tif ((survey->filled & SURVEY_INFO_TIME_TX) &&\n\t    nla_put_u64_64bit(msg, NL80211_SURVEY_INFO_TIME_TX,\n\t\t\t      survey->time_tx, NL80211_SURVEY_INFO_PAD))\n\t\tgoto nla_put_failure;\n\tif ((survey->filled & SURVEY_INFO_TIME_SCAN) &&\n\t    nla_put_u64_64bit(msg, NL80211_SURVEY_INFO_TIME_SCAN,\n\t\t\t      survey->time_scan, NL80211_SURVEY_INFO_PAD))\n\t\tgoto nla_put_failure;\n\tif ((survey->filled & SURVEY_INFO_TIME_BSS_RX) &&\n\t    nla_put_u64_64bit(msg, NL80211_SURVEY_INFO_TIME_BSS_RX,\n\t\t\t      survey->time_bss_rx, NL80211_SURVEY_INFO_PAD))\n\t\tgoto nla_put_failure;\n\n\tnla_nest_end(msg, infoattr);\n\n\tgenlmsg_end(msg, hdr);\n\treturn 0;\n\n nla_put_failure:\n\tgenlmsg_cancel(msg, hdr);\n\treturn -EMSGSIZE;\n}\n\nstatic int nl80211_dump_survey(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tstruct nlattr **attrbuf;\n\tstruct survey_info survey;\n\tstruct cfg80211_registered_device *rdev;\n\tstruct wireless_dev *wdev;\n\tint survey_idx = cb->args[2];\n\tint res;\n\tbool radio_stats;\n\n\tattrbuf = kcalloc(NUM_NL80211_ATTR, sizeof(*attrbuf), GFP_KERNEL);\n\tif (!attrbuf)\n\t\treturn -ENOMEM;\n\n\tres = nl80211_prepare_wdev_dump(cb, &rdev, &wdev);\n\tif (res) {\n\t\tkfree(attrbuf);\n\t\treturn res;\n\t}\n\t/* nl80211_prepare_wdev_dump acquired it in the successful case */\n\t__acquire(&rdev->wiphy.mtx);\n\n\t/* prepare_wdev_dump parsed the attributes */\n\tradio_stats = attrbuf[NL80211_ATTR_SURVEY_RADIO_STATS];\n\n\tif (!wdev->netdev) {\n\t\tres = -EINVAL;\n\t\tgoto out_err;\n\t}\n\n\tif (!rdev->ops->dump_survey) {\n\t\tres = -EOPNOTSUPP;\n\t\tgoto out_err;\n\t}\n\n\twhile (1) {\n\t\tres = rdev_dump_survey(rdev, wdev->netdev, survey_idx, &survey);\n\t\tif (res == -ENOENT)\n\t\t\tbreak;\n\t\tif (res)\n\t\t\tgoto out_err;\n\n\t\t/* don't send disabled channels, but do send non-channel data */\n\t\tif (survey.channel &&\n\t\t    survey.channel->flags & IEEE80211_CHAN_DISABLED) {\n\t\t\tsurvey_idx++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (nl80211_send_survey(skb,\n\t\t\t\tNETLINK_CB(cb->skb).portid,\n\t\t\t\tcb->nlh->nlmsg_seq, NLM_F_MULTI,\n\t\t\t\twdev->netdev, radio_stats, &survey) < 0)\n\t\t\tgoto out;\n\t\tsurvey_idx++;\n\t}\n\n out:\n\tcb->args[2] = survey_idx;\n\tres = skb->len;\n out_err:\n\tkfree(attrbuf);\n\twiphy_unlock(&rdev->wiphy);\n\treturn res;\n}\n\nstatic bool nl80211_valid_wpa_versions(u32 wpa_versions)\n{\n\treturn !(wpa_versions & ~(NL80211_WPA_VERSION_1 |\n\t\t\t\t  NL80211_WPA_VERSION_2 |\n\t\t\t\t  NL80211_WPA_VERSION_3));\n}\n\nstatic int nl80211_authenticate(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct ieee80211_channel *chan;\n\tconst u8 *bssid, *ssid, *ie = NULL, *auth_data = NULL;\n\tint err, ssid_len, ie_len = 0, auth_data_len = 0;\n\tenum nl80211_auth_type auth_type;\n\tstruct key_parse key;\n\tbool local_state_change;\n\tu32 freq;\n\n\tif (!info->attrs[NL80211_ATTR_MAC])\n\t\treturn -EINVAL;\n\n\tif (!info->attrs[NL80211_ATTR_AUTH_TYPE])\n\t\treturn -EINVAL;\n\n\tif (!info->attrs[NL80211_ATTR_SSID])\n\t\treturn -EINVAL;\n\n\tif (!info->attrs[NL80211_ATTR_WIPHY_FREQ])\n\t\treturn -EINVAL;\n\n\terr = nl80211_parse_key(info, &key);\n\tif (err)\n\t\treturn err;\n\n\tif (key.idx >= 0) {\n\t\tif (key.type != -1 && key.type != NL80211_KEYTYPE_GROUP)\n\t\t\treturn -EINVAL;\n\t\tif (!key.p.key || !key.p.key_len)\n\t\t\treturn -EINVAL;\n\t\tif ((key.p.cipher != WLAN_CIPHER_SUITE_WEP40 ||\n\t\t     key.p.key_len != WLAN_KEY_LEN_WEP40) &&\n\t\t    (key.p.cipher != WLAN_CIPHER_SUITE_WEP104 ||\n\t\t     key.p.key_len != WLAN_KEY_LEN_WEP104))\n\t\t\treturn -EINVAL;\n\t\tif (key.idx > 3)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tkey.p.key_len = 0;\n\t\tkey.p.key = NULL;\n\t}\n\n\tif (key.idx >= 0) {\n\t\tint i;\n\t\tbool ok = false;\n\n\t\tfor (i = 0; i < rdev->wiphy.n_cipher_suites; i++) {\n\t\t\tif (key.p.cipher == rdev->wiphy.cipher_suites[i]) {\n\t\t\t\tok = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!ok)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!rdev->ops->auth)\n\t\treturn -EOPNOTSUPP;\n\n\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_STATION &&\n\t    dev->ieee80211_ptr->iftype != NL80211_IFTYPE_P2P_CLIENT)\n\t\treturn -EOPNOTSUPP;\n\n\tbssid = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\tfreq = MHZ_TO_KHZ(nla_get_u32(info->attrs[NL80211_ATTR_WIPHY_FREQ]));\n\tif (info->attrs[NL80211_ATTR_WIPHY_FREQ_OFFSET])\n\t\tfreq +=\n\t\t    nla_get_u32(info->attrs[NL80211_ATTR_WIPHY_FREQ_OFFSET]);\n\n\tchan = nl80211_get_valid_chan(&rdev->wiphy, freq);\n\tif (!chan)\n\t\treturn -EINVAL;\n\n\tssid = nla_data(info->attrs[NL80211_ATTR_SSID]);\n\tssid_len = nla_len(info->attrs[NL80211_ATTR_SSID]);\n\n\tif (info->attrs[NL80211_ATTR_IE]) {\n\t\tie = nla_data(info->attrs[NL80211_ATTR_IE]);\n\t\tie_len = nla_len(info->attrs[NL80211_ATTR_IE]);\n\t}\n\n\tauth_type = nla_get_u32(info->attrs[NL80211_ATTR_AUTH_TYPE]);\n\tif (!nl80211_valid_auth_type(rdev, auth_type, NL80211_CMD_AUTHENTICATE))\n\t\treturn -EINVAL;\n\n\tif ((auth_type == NL80211_AUTHTYPE_SAE ||\n\t     auth_type == NL80211_AUTHTYPE_FILS_SK ||\n\t     auth_type == NL80211_AUTHTYPE_FILS_SK_PFS ||\n\t     auth_type == NL80211_AUTHTYPE_FILS_PK) &&\n\t    !info->attrs[NL80211_ATTR_AUTH_DATA])\n\t\treturn -EINVAL;\n\n\tif (info->attrs[NL80211_ATTR_AUTH_DATA]) {\n\t\tif (auth_type != NL80211_AUTHTYPE_SAE &&\n\t\t    auth_type != NL80211_AUTHTYPE_FILS_SK &&\n\t\t    auth_type != NL80211_AUTHTYPE_FILS_SK_PFS &&\n\t\t    auth_type != NL80211_AUTHTYPE_FILS_PK)\n\t\t\treturn -EINVAL;\n\t\tauth_data = nla_data(info->attrs[NL80211_ATTR_AUTH_DATA]);\n\t\tauth_data_len = nla_len(info->attrs[NL80211_ATTR_AUTH_DATA]);\n\t}\n\n\tlocal_state_change = !!info->attrs[NL80211_ATTR_LOCAL_STATE_CHANGE];\n\n\t/*\n\t * Since we no longer track auth state, ignore\n\t * requests to only change local state.\n\t */\n\tif (local_state_change)\n\t\treturn 0;\n\n\twdev_lock(dev->ieee80211_ptr);\n\terr = cfg80211_mlme_auth(rdev, dev, chan, auth_type, bssid,\n\t\t\t\t ssid, ssid_len, ie, ie_len,\n\t\t\t\t key.p.key, key.p.key_len, key.idx,\n\t\t\t\t auth_data, auth_data_len);\n\twdev_unlock(dev->ieee80211_ptr);\n\treturn err;\n}\n\nstatic int validate_pae_over_nl80211(struct cfg80211_registered_device *rdev,\n\t\t\t\t     struct genl_info *info)\n{\n\tif (!info->attrs[NL80211_ATTR_SOCKET_OWNER]) {\n\t\tGENL_SET_ERR_MSG(info, \"SOCKET_OWNER not set\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!rdev->ops->tx_control_port ||\n\t    !wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t     NL80211_EXT_FEATURE_CONTROL_PORT_OVER_NL80211))\n\t\treturn -EOPNOTSUPP;\n\n\treturn 0;\n}\n\nstatic int nl80211_crypto_settings(struct cfg80211_registered_device *rdev,\n\t\t\t\t   struct genl_info *info,\n\t\t\t\t   struct cfg80211_crypto_settings *settings,\n\t\t\t\t   int cipher_limit)\n{\n\tmemset(settings, 0, sizeof(*settings));\n\n\tsettings->control_port = info->attrs[NL80211_ATTR_CONTROL_PORT];\n\n\tif (info->attrs[NL80211_ATTR_CONTROL_PORT_ETHERTYPE]) {\n\t\tu16 proto;\n\n\t\tproto = nla_get_u16(\n\t\t\tinfo->attrs[NL80211_ATTR_CONTROL_PORT_ETHERTYPE]);\n\t\tsettings->control_port_ethertype = cpu_to_be16(proto);\n\t\tif (!(rdev->wiphy.flags & WIPHY_FLAG_CONTROL_PORT_PROTOCOL) &&\n\t\t    proto != ETH_P_PAE)\n\t\t\treturn -EINVAL;\n\t\tif (info->attrs[NL80211_ATTR_CONTROL_PORT_NO_ENCRYPT])\n\t\t\tsettings->control_port_no_encrypt = true;\n\t} else\n\t\tsettings->control_port_ethertype = cpu_to_be16(ETH_P_PAE);\n\n\tif (info->attrs[NL80211_ATTR_CONTROL_PORT_OVER_NL80211]) {\n\t\tint r = validate_pae_over_nl80211(rdev, info);\n\n\t\tif (r < 0)\n\t\t\treturn r;\n\n\t\tsettings->control_port_over_nl80211 = true;\n\n\t\tif (info->attrs[NL80211_ATTR_CONTROL_PORT_NO_PREAUTH])\n\t\t\tsettings->control_port_no_preauth = true;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_CIPHER_SUITES_PAIRWISE]) {\n\t\tvoid *data;\n\t\tint len, i;\n\n\t\tdata = nla_data(info->attrs[NL80211_ATTR_CIPHER_SUITES_PAIRWISE]);\n\t\tlen = nla_len(info->attrs[NL80211_ATTR_CIPHER_SUITES_PAIRWISE]);\n\t\tsettings->n_ciphers_pairwise = len / sizeof(u32);\n\n\t\tif (len % sizeof(u32))\n\t\t\treturn -EINVAL;\n\n\t\tif (settings->n_ciphers_pairwise > cipher_limit)\n\t\t\treturn -EINVAL;\n\n\t\tmemcpy(settings->ciphers_pairwise, data, len);\n\n\t\tfor (i = 0; i < settings->n_ciphers_pairwise; i++)\n\t\t\tif (!cfg80211_supported_cipher_suite(\n\t\t\t\t\t&rdev->wiphy,\n\t\t\t\t\tsettings->ciphers_pairwise[i]))\n\t\t\t\treturn -EINVAL;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_CIPHER_SUITE_GROUP]) {\n\t\tsettings->cipher_group =\n\t\t\tnla_get_u32(info->attrs[NL80211_ATTR_CIPHER_SUITE_GROUP]);\n\t\tif (!cfg80211_supported_cipher_suite(&rdev->wiphy,\n\t\t\t\t\t\t     settings->cipher_group))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_WPA_VERSIONS]) {\n\t\tsettings->wpa_versions =\n\t\t\tnla_get_u32(info->attrs[NL80211_ATTR_WPA_VERSIONS]);\n\t\tif (!nl80211_valid_wpa_versions(settings->wpa_versions))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_AKM_SUITES]) {\n\t\tvoid *data;\n\t\tint len;\n\n\t\tdata = nla_data(info->attrs[NL80211_ATTR_AKM_SUITES]);\n\t\tlen = nla_len(info->attrs[NL80211_ATTR_AKM_SUITES]);\n\t\tsettings->n_akm_suites = len / sizeof(u32);\n\n\t\tif (len % sizeof(u32))\n\t\t\treturn -EINVAL;\n\n\t\tif (settings->n_akm_suites > NL80211_MAX_NR_AKM_SUITES)\n\t\t\treturn -EINVAL;\n\n\t\tmemcpy(settings->akm_suites, data, len);\n\t}\n\n\tif (info->attrs[NL80211_ATTR_PMK]) {\n\t\tif (nla_len(info->attrs[NL80211_ATTR_PMK]) != WLAN_PMK_LEN)\n\t\t\treturn -EINVAL;\n\t\tif (!wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t\t     NL80211_EXT_FEATURE_4WAY_HANDSHAKE_STA_PSK) &&\n\t\t    !wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t\t     NL80211_EXT_FEATURE_4WAY_HANDSHAKE_AP_PSK))\n\t\t\treturn -EINVAL;\n\t\tsettings->psk = nla_data(info->attrs[NL80211_ATTR_PMK]);\n\t}\n\n\tif (info->attrs[NL80211_ATTR_SAE_PASSWORD]) {\n\t\tif (!wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t\t     NL80211_EXT_FEATURE_SAE_OFFLOAD) &&\n\t\t    !wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t\t     NL80211_EXT_FEATURE_SAE_OFFLOAD_AP))\n\t\t\treturn -EINVAL;\n\t\tsettings->sae_pwd =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_SAE_PASSWORD]);\n\t\tsettings->sae_pwd_len =\n\t\t\tnla_len(info->attrs[NL80211_ATTR_SAE_PASSWORD]);\n\t}\n\n\tif (info->attrs[NL80211_ATTR_SAE_PWE])\n\t\tsettings->sae_pwe =\n\t\t\tnla_get_u8(info->attrs[NL80211_ATTR_SAE_PWE]);\n\telse\n\t\tsettings->sae_pwe = NL80211_SAE_PWE_UNSPECIFIED;\n\n\treturn 0;\n}\n\nstatic int nl80211_associate(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct ieee80211_channel *chan;\n\tstruct cfg80211_assoc_request req = {};\n\tconst u8 *bssid, *ssid;\n\tint err, ssid_len = 0;\n\tu32 freq;\n\n\tif (dev->ieee80211_ptr->conn_owner_nlportid &&\n\t    dev->ieee80211_ptr->conn_owner_nlportid != info->snd_portid)\n\t\treturn -EPERM;\n\n\tif (!info->attrs[NL80211_ATTR_MAC] ||\n\t    !info->attrs[NL80211_ATTR_SSID] ||\n\t    !info->attrs[NL80211_ATTR_WIPHY_FREQ])\n\t\treturn -EINVAL;\n\n\tif (!rdev->ops->assoc)\n\t\treturn -EOPNOTSUPP;\n\n\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_STATION &&\n\t    dev->ieee80211_ptr->iftype != NL80211_IFTYPE_P2P_CLIENT)\n\t\treturn -EOPNOTSUPP;\n\n\tbssid = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\n\tfreq = MHZ_TO_KHZ(nla_get_u32(info->attrs[NL80211_ATTR_WIPHY_FREQ]));\n\tif (info->attrs[NL80211_ATTR_WIPHY_FREQ_OFFSET])\n\t\tfreq +=\n\t\t    nla_get_u32(info->attrs[NL80211_ATTR_WIPHY_FREQ_OFFSET]);\n\tchan = nl80211_get_valid_chan(&rdev->wiphy, freq);\n\tif (!chan)\n\t\treturn -EINVAL;\n\n\tssid = nla_data(info->attrs[NL80211_ATTR_SSID]);\n\tssid_len = nla_len(info->attrs[NL80211_ATTR_SSID]);\n\n\tif (info->attrs[NL80211_ATTR_IE]) {\n\t\treq.ie = nla_data(info->attrs[NL80211_ATTR_IE]);\n\t\treq.ie_len = nla_len(info->attrs[NL80211_ATTR_IE]);\n\t}\n\n\tif (info->attrs[NL80211_ATTR_USE_MFP]) {\n\t\tenum nl80211_mfp mfp =\n\t\t\tnla_get_u32(info->attrs[NL80211_ATTR_USE_MFP]);\n\t\tif (mfp == NL80211_MFP_REQUIRED)\n\t\t\treq.use_mfp = true;\n\t\telse if (mfp != NL80211_MFP_NO)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_PREV_BSSID])\n\t\treq.prev_bssid = nla_data(info->attrs[NL80211_ATTR_PREV_BSSID]);\n\n\tif (nla_get_flag(info->attrs[NL80211_ATTR_DISABLE_HT]))\n\t\treq.flags |= ASSOC_REQ_DISABLE_HT;\n\n\tif (info->attrs[NL80211_ATTR_HT_CAPABILITY_MASK])\n\t\tmemcpy(&req.ht_capa_mask,\n\t\t       nla_data(info->attrs[NL80211_ATTR_HT_CAPABILITY_MASK]),\n\t\t       sizeof(req.ht_capa_mask));\n\n\tif (info->attrs[NL80211_ATTR_HT_CAPABILITY]) {\n\t\tif (!info->attrs[NL80211_ATTR_HT_CAPABILITY_MASK])\n\t\t\treturn -EINVAL;\n\t\tmemcpy(&req.ht_capa,\n\t\t       nla_data(info->attrs[NL80211_ATTR_HT_CAPABILITY]),\n\t\t       sizeof(req.ht_capa));\n\t}\n\n\tif (nla_get_flag(info->attrs[NL80211_ATTR_DISABLE_VHT]))\n\t\treq.flags |= ASSOC_REQ_DISABLE_VHT;\n\n\tif (nla_get_flag(info->attrs[NL80211_ATTR_DISABLE_HE]))\n\t\treq.flags |= ASSOC_REQ_DISABLE_HE;\n\n\tif (info->attrs[NL80211_ATTR_VHT_CAPABILITY_MASK])\n\t\tmemcpy(&req.vht_capa_mask,\n\t\t       nla_data(info->attrs[NL80211_ATTR_VHT_CAPABILITY_MASK]),\n\t\t       sizeof(req.vht_capa_mask));\n\n\tif (info->attrs[NL80211_ATTR_VHT_CAPABILITY]) {\n\t\tif (!info->attrs[NL80211_ATTR_VHT_CAPABILITY_MASK])\n\t\t\treturn -EINVAL;\n\t\tmemcpy(&req.vht_capa,\n\t\t       nla_data(info->attrs[NL80211_ATTR_VHT_CAPABILITY]),\n\t\t       sizeof(req.vht_capa));\n\t}\n\n\tif (nla_get_flag(info->attrs[NL80211_ATTR_USE_RRM])) {\n\t\tif (!((rdev->wiphy.features &\n\t\t\tNL80211_FEATURE_DS_PARAM_SET_IE_IN_PROBES) &&\n\t\t       (rdev->wiphy.features & NL80211_FEATURE_QUIET)) &&\n\t\t    !wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t\t     NL80211_EXT_FEATURE_RRM))\n\t\t\treturn -EINVAL;\n\t\treq.flags |= ASSOC_REQ_USE_RRM;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_FILS_KEK]) {\n\t\treq.fils_kek = nla_data(info->attrs[NL80211_ATTR_FILS_KEK]);\n\t\treq.fils_kek_len = nla_len(info->attrs[NL80211_ATTR_FILS_KEK]);\n\t\tif (!info->attrs[NL80211_ATTR_FILS_NONCES])\n\t\t\treturn -EINVAL;\n\t\treq.fils_nonces =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_FILS_NONCES]);\n\t}\n\n\tif (info->attrs[NL80211_ATTR_S1G_CAPABILITY_MASK]) {\n\t\tif (!info->attrs[NL80211_ATTR_S1G_CAPABILITY])\n\t\t\treturn -EINVAL;\n\t\tmemcpy(&req.s1g_capa_mask,\n\t\t       nla_data(info->attrs[NL80211_ATTR_S1G_CAPABILITY_MASK]),\n\t\t       sizeof(req.s1g_capa_mask));\n\t}\n\n\tif (info->attrs[NL80211_ATTR_S1G_CAPABILITY]) {\n\t\tif (!info->attrs[NL80211_ATTR_S1G_CAPABILITY_MASK])\n\t\t\treturn -EINVAL;\n\t\tmemcpy(&req.s1g_capa,\n\t\t       nla_data(info->attrs[NL80211_ATTR_S1G_CAPABILITY]),\n\t\t       sizeof(req.s1g_capa));\n\t}\n\n\terr = nl80211_crypto_settings(rdev, info, &req.crypto, 1);\n\tif (!err) {\n\t\twdev_lock(dev->ieee80211_ptr);\n\n\t\terr = cfg80211_mlme_assoc(rdev, dev, chan, bssid,\n\t\t\t\t\t  ssid, ssid_len, &req);\n\n\t\tif (!err && info->attrs[NL80211_ATTR_SOCKET_OWNER]) {\n\t\t\tdev->ieee80211_ptr->conn_owner_nlportid =\n\t\t\t\tinfo->snd_portid;\n\t\t\tmemcpy(dev->ieee80211_ptr->disconnect_bssid,\n\t\t\t       bssid, ETH_ALEN);\n\t\t}\n\n\t\twdev_unlock(dev->ieee80211_ptr);\n\t}\n\n\treturn err;\n}\n\nstatic int nl80211_deauthenticate(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tconst u8 *ie = NULL, *bssid;\n\tint ie_len = 0, err;\n\tu16 reason_code;\n\tbool local_state_change;\n\n\tif (dev->ieee80211_ptr->conn_owner_nlportid &&\n\t    dev->ieee80211_ptr->conn_owner_nlportid != info->snd_portid)\n\t\treturn -EPERM;\n\n\tif (!info->attrs[NL80211_ATTR_MAC])\n\t\treturn -EINVAL;\n\n\tif (!info->attrs[NL80211_ATTR_REASON_CODE])\n\t\treturn -EINVAL;\n\n\tif (!rdev->ops->deauth)\n\t\treturn -EOPNOTSUPP;\n\n\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_STATION &&\n\t    dev->ieee80211_ptr->iftype != NL80211_IFTYPE_P2P_CLIENT)\n\t\treturn -EOPNOTSUPP;\n\n\tbssid = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\n\treason_code = nla_get_u16(info->attrs[NL80211_ATTR_REASON_CODE]);\n\tif (reason_code == 0) {\n\t\t/* Reason Code 0 is reserved */\n\t\treturn -EINVAL;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_IE]) {\n\t\tie = nla_data(info->attrs[NL80211_ATTR_IE]);\n\t\tie_len = nla_len(info->attrs[NL80211_ATTR_IE]);\n\t}\n\n\tlocal_state_change = !!info->attrs[NL80211_ATTR_LOCAL_STATE_CHANGE];\n\n\twdev_lock(dev->ieee80211_ptr);\n\terr = cfg80211_mlme_deauth(rdev, dev, bssid, ie, ie_len, reason_code,\n\t\t\t\t   local_state_change);\n\twdev_unlock(dev->ieee80211_ptr);\n\treturn err;\n}\n\nstatic int nl80211_disassociate(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tconst u8 *ie = NULL, *bssid;\n\tint ie_len = 0, err;\n\tu16 reason_code;\n\tbool local_state_change;\n\n\tif (dev->ieee80211_ptr->conn_owner_nlportid &&\n\t    dev->ieee80211_ptr->conn_owner_nlportid != info->snd_portid)\n\t\treturn -EPERM;\n\n\tif (!info->attrs[NL80211_ATTR_MAC])\n\t\treturn -EINVAL;\n\n\tif (!info->attrs[NL80211_ATTR_REASON_CODE])\n\t\treturn -EINVAL;\n\n\tif (!rdev->ops->disassoc)\n\t\treturn -EOPNOTSUPP;\n\n\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_STATION &&\n\t    dev->ieee80211_ptr->iftype != NL80211_IFTYPE_P2P_CLIENT)\n\t\treturn -EOPNOTSUPP;\n\n\tbssid = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\n\treason_code = nla_get_u16(info->attrs[NL80211_ATTR_REASON_CODE]);\n\tif (reason_code == 0) {\n\t\t/* Reason Code 0 is reserved */\n\t\treturn -EINVAL;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_IE]) {\n\t\tie = nla_data(info->attrs[NL80211_ATTR_IE]);\n\t\tie_len = nla_len(info->attrs[NL80211_ATTR_IE]);\n\t}\n\n\tlocal_state_change = !!info->attrs[NL80211_ATTR_LOCAL_STATE_CHANGE];\n\n\twdev_lock(dev->ieee80211_ptr);\n\terr = cfg80211_mlme_disassoc(rdev, dev, bssid, ie, ie_len, reason_code,\n\t\t\t\t     local_state_change);\n\twdev_unlock(dev->ieee80211_ptr);\n\treturn err;\n}\n\nstatic bool\nnl80211_parse_mcast_rate(struct cfg80211_registered_device *rdev,\n\t\t\t int mcast_rate[NUM_NL80211_BANDS],\n\t\t\t int rateval)\n{\n\tstruct wiphy *wiphy = &rdev->wiphy;\n\tbool found = false;\n\tint band, i;\n\n\tfor (band = 0; band < NUM_NL80211_BANDS; band++) {\n\t\tstruct ieee80211_supported_band *sband;\n\n\t\tsband = wiphy->bands[band];\n\t\tif (!sband)\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < sband->n_bitrates; i++) {\n\t\t\tif (sband->bitrates[i].bitrate == rateval) {\n\t\t\t\tmcast_rate[band] = i + 1;\n\t\t\t\tfound = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn found;\n}\n\nstatic int nl80211_join_ibss(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct cfg80211_ibss_params ibss;\n\tstruct wiphy *wiphy;\n\tstruct cfg80211_cached_keys *connkeys = NULL;\n\tint err;\n\n\tmemset(&ibss, 0, sizeof(ibss));\n\n\tif (!info->attrs[NL80211_ATTR_SSID] ||\n\t    !nla_len(info->attrs[NL80211_ATTR_SSID]))\n\t\treturn -EINVAL;\n\n\tibss.beacon_interval = 100;\n\n\tif (info->attrs[NL80211_ATTR_BEACON_INTERVAL])\n\t\tibss.beacon_interval =\n\t\t\tnla_get_u32(info->attrs[NL80211_ATTR_BEACON_INTERVAL]);\n\n\terr = cfg80211_validate_beacon_int(rdev, NL80211_IFTYPE_ADHOC,\n\t\t\t\t\t   ibss.beacon_interval);\n\tif (err)\n\t\treturn err;\n\n\tif (!rdev->ops->join_ibss)\n\t\treturn -EOPNOTSUPP;\n\n\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_ADHOC)\n\t\treturn -EOPNOTSUPP;\n\n\twiphy = &rdev->wiphy;\n\n\tif (info->attrs[NL80211_ATTR_MAC]) {\n\t\tibss.bssid = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\n\t\tif (!is_valid_ether_addr(ibss.bssid))\n\t\t\treturn -EINVAL;\n\t}\n\tibss.ssid = nla_data(info->attrs[NL80211_ATTR_SSID]);\n\tibss.ssid_len = nla_len(info->attrs[NL80211_ATTR_SSID]);\n\n\tif (info->attrs[NL80211_ATTR_IE]) {\n\t\tibss.ie = nla_data(info->attrs[NL80211_ATTR_IE]);\n\t\tibss.ie_len = nla_len(info->attrs[NL80211_ATTR_IE]);\n\t}\n\n\terr = nl80211_parse_chandef(rdev, info, &ibss.chandef);\n\tif (err)\n\t\treturn err;\n\n\tif (!cfg80211_reg_can_beacon(&rdev->wiphy, &ibss.chandef,\n\t\t\t\t     NL80211_IFTYPE_ADHOC))\n\t\treturn -EINVAL;\n\n\tswitch (ibss.chandef.width) {\n\tcase NL80211_CHAN_WIDTH_5:\n\tcase NL80211_CHAN_WIDTH_10:\n\tcase NL80211_CHAN_WIDTH_20_NOHT:\n\t\tbreak;\n\tcase NL80211_CHAN_WIDTH_20:\n\tcase NL80211_CHAN_WIDTH_40:\n\t\tif (!(rdev->wiphy.features & NL80211_FEATURE_HT_IBSS))\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase NL80211_CHAN_WIDTH_80:\n\tcase NL80211_CHAN_WIDTH_80P80:\n\tcase NL80211_CHAN_WIDTH_160:\n\t\tif (!(rdev->wiphy.features & NL80211_FEATURE_HT_IBSS))\n\t\t\treturn -EINVAL;\n\t\tif (!wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t\t     NL80211_EXT_FEATURE_VHT_IBSS))\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tibss.channel_fixed = !!info->attrs[NL80211_ATTR_FREQ_FIXED];\n\tibss.privacy = !!info->attrs[NL80211_ATTR_PRIVACY];\n\n\tif (info->attrs[NL80211_ATTR_BSS_BASIC_RATES]) {\n\t\tu8 *rates =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_BSS_BASIC_RATES]);\n\t\tint n_rates =\n\t\t\tnla_len(info->attrs[NL80211_ATTR_BSS_BASIC_RATES]);\n\t\tstruct ieee80211_supported_band *sband =\n\t\t\twiphy->bands[ibss.chandef.chan->band];\n\n\t\terr = ieee80211_get_ratemask(sband, rates, n_rates,\n\t\t\t\t\t     &ibss.basic_rates);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_HT_CAPABILITY_MASK])\n\t\tmemcpy(&ibss.ht_capa_mask,\n\t\t       nla_data(info->attrs[NL80211_ATTR_HT_CAPABILITY_MASK]),\n\t\t       sizeof(ibss.ht_capa_mask));\n\n\tif (info->attrs[NL80211_ATTR_HT_CAPABILITY]) {\n\t\tif (!info->attrs[NL80211_ATTR_HT_CAPABILITY_MASK])\n\t\t\treturn -EINVAL;\n\t\tmemcpy(&ibss.ht_capa,\n\t\t       nla_data(info->attrs[NL80211_ATTR_HT_CAPABILITY]),\n\t\t       sizeof(ibss.ht_capa));\n\t}\n\n\tif (info->attrs[NL80211_ATTR_MCAST_RATE] &&\n\t    !nl80211_parse_mcast_rate(rdev, ibss.mcast_rate,\n\t\t\tnla_get_u32(info->attrs[NL80211_ATTR_MCAST_RATE])))\n\t\treturn -EINVAL;\n\n\tif (ibss.privacy && info->attrs[NL80211_ATTR_KEYS]) {\n\t\tbool no_ht = false;\n\n\t\tconnkeys = nl80211_parse_connkeys(rdev, info, &no_ht);\n\t\tif (IS_ERR(connkeys))\n\t\t\treturn PTR_ERR(connkeys);\n\n\t\tif ((ibss.chandef.width != NL80211_CHAN_WIDTH_20_NOHT) &&\n\t\t    no_ht) {\n\t\t\tkfree_sensitive(connkeys);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tibss.control_port =\n\t\tnla_get_flag(info->attrs[NL80211_ATTR_CONTROL_PORT]);\n\n\tif (info->attrs[NL80211_ATTR_CONTROL_PORT_OVER_NL80211]) {\n\t\tint r = validate_pae_over_nl80211(rdev, info);\n\n\t\tif (r < 0) {\n\t\t\tkfree_sensitive(connkeys);\n\t\t\treturn r;\n\t\t}\n\n\t\tibss.control_port_over_nl80211 = true;\n\t}\n\n\tibss.userspace_handles_dfs =\n\t\tnla_get_flag(info->attrs[NL80211_ATTR_HANDLE_DFS]);\n\n\twdev_lock(dev->ieee80211_ptr);\n\terr = __cfg80211_join_ibss(rdev, dev, &ibss, connkeys);\n\tif (err)\n\t\tkfree_sensitive(connkeys);\n\telse if (info->attrs[NL80211_ATTR_SOCKET_OWNER])\n\t\tdev->ieee80211_ptr->conn_owner_nlportid = info->snd_portid;\n\twdev_unlock(dev->ieee80211_ptr);\n\n\treturn err;\n}\n\nstatic int nl80211_leave_ibss(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\n\tif (!rdev->ops->leave_ibss)\n\t\treturn -EOPNOTSUPP;\n\n\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_ADHOC)\n\t\treturn -EOPNOTSUPP;\n\n\treturn cfg80211_leave_ibss(rdev, dev, false);\n}\n\nstatic int nl80211_set_mcast_rate(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tint mcast_rate[NUM_NL80211_BANDS];\n\tu32 nla_rate;\n\tint err;\n\n\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_ADHOC &&\n\t    dev->ieee80211_ptr->iftype != NL80211_IFTYPE_MESH_POINT &&\n\t    dev->ieee80211_ptr->iftype != NL80211_IFTYPE_OCB)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!rdev->ops->set_mcast_rate)\n\t\treturn -EOPNOTSUPP;\n\n\tmemset(mcast_rate, 0, sizeof(mcast_rate));\n\n\tif (!info->attrs[NL80211_ATTR_MCAST_RATE])\n\t\treturn -EINVAL;\n\n\tnla_rate = nla_get_u32(info->attrs[NL80211_ATTR_MCAST_RATE]);\n\tif (!nl80211_parse_mcast_rate(rdev, mcast_rate, nla_rate))\n\t\treturn -EINVAL;\n\n\terr = rdev_set_mcast_rate(rdev, dev, mcast_rate);\n\n\treturn err;\n}\n\nstatic struct sk_buff *\n__cfg80211_alloc_vendor_skb(struct cfg80211_registered_device *rdev,\n\t\t\t    struct wireless_dev *wdev, int approxlen,\n\t\t\t    u32 portid, u32 seq, enum nl80211_commands cmd,\n\t\t\t    enum nl80211_attrs attr,\n\t\t\t    const struct nl80211_vendor_cmd_info *info,\n\t\t\t    gfp_t gfp)\n{\n\tstruct sk_buff *skb;\n\tvoid *hdr;\n\tstruct nlattr *data;\n\n\tskb = nlmsg_new(approxlen + 100, gfp);\n\tif (!skb)\n\t\treturn NULL;\n\n\thdr = nl80211hdr_put(skb, portid, seq, 0, cmd);\n\tif (!hdr) {\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\t}\n\n\tif (nla_put_u32(skb, NL80211_ATTR_WIPHY, rdev->wiphy_idx))\n\t\tgoto nla_put_failure;\n\n\tif (info) {\n\t\tif (nla_put_u32(skb, NL80211_ATTR_VENDOR_ID,\n\t\t\t\tinfo->vendor_id))\n\t\t\tgoto nla_put_failure;\n\t\tif (nla_put_u32(skb, NL80211_ATTR_VENDOR_SUBCMD,\n\t\t\t\tinfo->subcmd))\n\t\t\tgoto nla_put_failure;\n\t}\n\n\tif (wdev) {\n\t\tif (nla_put_u64_64bit(skb, NL80211_ATTR_WDEV,\n\t\t\t\t      wdev_id(wdev), NL80211_ATTR_PAD))\n\t\t\tgoto nla_put_failure;\n\t\tif (wdev->netdev &&\n\t\t    nla_put_u32(skb, NL80211_ATTR_IFINDEX,\n\t\t\t\twdev->netdev->ifindex))\n\t\t\tgoto nla_put_failure;\n\t}\n\n\tdata = nla_nest_start_noflag(skb, attr);\n\tif (!data)\n\t\tgoto nla_put_failure;\n\n\t((void **)skb->cb)[0] = rdev;\n\t((void **)skb->cb)[1] = hdr;\n\t((void **)skb->cb)[2] = data;\n\n\treturn skb;\n\n nla_put_failure:\n\tkfree_skb(skb);\n\treturn NULL;\n}\n\nstruct sk_buff *__cfg80211_alloc_event_skb(struct wiphy *wiphy,\n\t\t\t\t\t   struct wireless_dev *wdev,\n\t\t\t\t\t   enum nl80211_commands cmd,\n\t\t\t\t\t   enum nl80211_attrs attr,\n\t\t\t\t\t   unsigned int portid,\n\t\t\t\t\t   int vendor_event_idx,\n\t\t\t\t\t   int approxlen, gfp_t gfp)\n{\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\tconst struct nl80211_vendor_cmd_info *info;\n\n\tswitch (cmd) {\n\tcase NL80211_CMD_TESTMODE:\n\t\tif (WARN_ON(vendor_event_idx != -1))\n\t\t\treturn NULL;\n\t\tinfo = NULL;\n\t\tbreak;\n\tcase NL80211_CMD_VENDOR:\n\t\tif (WARN_ON(vendor_event_idx < 0 ||\n\t\t\t    vendor_event_idx >= wiphy->n_vendor_events))\n\t\t\treturn NULL;\n\t\tinfo = &wiphy->vendor_events[vendor_event_idx];\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t\treturn NULL;\n\t}\n\n\treturn __cfg80211_alloc_vendor_skb(rdev, wdev, approxlen, portid, 0,\n\t\t\t\t\t   cmd, attr, info, gfp);\n}\nEXPORT_SYMBOL(__cfg80211_alloc_event_skb);\n\nvoid __cfg80211_send_event_skb(struct sk_buff *skb, gfp_t gfp)\n{\n\tstruct cfg80211_registered_device *rdev = ((void **)skb->cb)[0];\n\tvoid *hdr = ((void **)skb->cb)[1];\n\tstruct nlmsghdr *nlhdr = nlmsg_hdr(skb);\n\tstruct nlattr *data = ((void **)skb->cb)[2];\n\tenum nl80211_multicast_groups mcgrp = NL80211_MCGRP_TESTMODE;\n\n\t/* clear CB data for netlink core to own from now on */\n\tmemset(skb->cb, 0, sizeof(skb->cb));\n\n\tnla_nest_end(skb, data);\n\tgenlmsg_end(skb, hdr);\n\n\tif (nlhdr->nlmsg_pid) {\n\t\tgenlmsg_unicast(wiphy_net(&rdev->wiphy), skb,\n\t\t\t\tnlhdr->nlmsg_pid);\n\t} else {\n\t\tif (data->nla_type == NL80211_ATTR_VENDOR_DATA)\n\t\t\tmcgrp = NL80211_MCGRP_VENDOR;\n\n\t\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy),\n\t\t\t\t\tskb, 0, mcgrp, gfp);\n\t}\n}\nEXPORT_SYMBOL(__cfg80211_send_event_skb);\n\n#ifdef CONFIG_NL80211_TESTMODE\nstatic int nl80211_testmode_do(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev;\n\tint err;\n\n\tlockdep_assert_held(&rdev->wiphy.mtx);\n\n\twdev = __cfg80211_wdev_from_attrs(rdev, genl_info_net(info),\n\t\t\t\t\t  info->attrs);\n\n\tif (!rdev->ops->testmode_cmd)\n\t\treturn -EOPNOTSUPP;\n\n\tif (IS_ERR(wdev)) {\n\t\terr = PTR_ERR(wdev);\n\t\tif (err != -EINVAL)\n\t\t\treturn err;\n\t\twdev = NULL;\n\t} else if (wdev->wiphy != &rdev->wiphy) {\n\t\treturn -EINVAL;\n\t}\n\n\tif (!info->attrs[NL80211_ATTR_TESTDATA])\n\t\treturn -EINVAL;\n\n\trdev->cur_cmd_info = info;\n\terr = rdev_testmode_cmd(rdev, wdev,\n\t\t\t\tnla_data(info->attrs[NL80211_ATTR_TESTDATA]),\n\t\t\t\tnla_len(info->attrs[NL80211_ATTR_TESTDATA]));\n\trdev->cur_cmd_info = NULL;\n\n\treturn err;\n}\n\nstatic int nl80211_testmode_dump(struct sk_buff *skb,\n\t\t\t\t struct netlink_callback *cb)\n{\n\tstruct cfg80211_registered_device *rdev;\n\tstruct nlattr **attrbuf = NULL;\n\tint err;\n\tlong phy_idx;\n\tvoid *data = NULL;\n\tint data_len = 0;\n\n\trtnl_lock();\n\n\tif (cb->args[0]) {\n\t\t/*\n\t\t * 0 is a valid index, but not valid for args[0],\n\t\t * so we need to offset by 1.\n\t\t */\n\t\tphy_idx = cb->args[0] - 1;\n\n\t\trdev = cfg80211_rdev_by_wiphy_idx(phy_idx);\n\t\tif (!rdev) {\n\t\t\terr = -ENOENT;\n\t\t\tgoto out_err;\n\t\t}\n\t} else {\n\t\tattrbuf = kcalloc(NUM_NL80211_ATTR, sizeof(*attrbuf),\n\t\t\t\t  GFP_KERNEL);\n\t\tif (!attrbuf) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_err;\n\t\t}\n\n\t\terr = nlmsg_parse_deprecated(cb->nlh,\n\t\t\t\t\t     GENL_HDRLEN + nl80211_fam.hdrsize,\n\t\t\t\t\t     attrbuf, nl80211_fam.maxattr,\n\t\t\t\t\t     nl80211_policy, NULL);\n\t\tif (err)\n\t\t\tgoto out_err;\n\n\t\trdev = __cfg80211_rdev_from_attrs(sock_net(skb->sk), attrbuf);\n\t\tif (IS_ERR(rdev)) {\n\t\t\terr = PTR_ERR(rdev);\n\t\t\tgoto out_err;\n\t\t}\n\t\tphy_idx = rdev->wiphy_idx;\n\n\t\tif (attrbuf[NL80211_ATTR_TESTDATA])\n\t\t\tcb->args[1] = (long)attrbuf[NL80211_ATTR_TESTDATA];\n\t}\n\n\tif (cb->args[1]) {\n\t\tdata = nla_data((void *)cb->args[1]);\n\t\tdata_len = nla_len((void *)cb->args[1]);\n\t}\n\n\tif (!rdev->ops->testmode_dump) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out_err;\n\t}\n\n\twhile (1) {\n\t\tvoid *hdr = nl80211hdr_put(skb, NETLINK_CB(cb->skb).portid,\n\t\t\t\t\t   cb->nlh->nlmsg_seq, NLM_F_MULTI,\n\t\t\t\t\t   NL80211_CMD_TESTMODE);\n\t\tstruct nlattr *tmdata;\n\n\t\tif (!hdr)\n\t\t\tbreak;\n\n\t\tif (nla_put_u32(skb, NL80211_ATTR_WIPHY, phy_idx)) {\n\t\t\tgenlmsg_cancel(skb, hdr);\n\t\t\tbreak;\n\t\t}\n\n\t\ttmdata = nla_nest_start_noflag(skb, NL80211_ATTR_TESTDATA);\n\t\tif (!tmdata) {\n\t\t\tgenlmsg_cancel(skb, hdr);\n\t\t\tbreak;\n\t\t}\n\t\terr = rdev_testmode_dump(rdev, skb, cb, data, data_len);\n\t\tnla_nest_end(skb, tmdata);\n\n\t\tif (err == -ENOBUFS || err == -ENOENT) {\n\t\t\tgenlmsg_cancel(skb, hdr);\n\t\t\tbreak;\n\t\t} else if (err) {\n\t\t\tgenlmsg_cancel(skb, hdr);\n\t\t\tgoto out_err;\n\t\t}\n\n\t\tgenlmsg_end(skb, hdr);\n\t}\n\n\terr = skb->len;\n\t/* see above */\n\tcb->args[0] = phy_idx + 1;\n out_err:\n\tkfree(attrbuf);\n\trtnl_unlock();\n\treturn err;\n}\n#endif\n\nstatic int nl80211_connect(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct cfg80211_connect_params connect;\n\tstruct wiphy *wiphy;\n\tstruct cfg80211_cached_keys *connkeys = NULL;\n\tu32 freq = 0;\n\tint err;\n\n\tmemset(&connect, 0, sizeof(connect));\n\n\tif (!info->attrs[NL80211_ATTR_SSID] ||\n\t    !nla_len(info->attrs[NL80211_ATTR_SSID]))\n\t\treturn -EINVAL;\n\n\tif (info->attrs[NL80211_ATTR_AUTH_TYPE]) {\n\t\tconnect.auth_type =\n\t\t\tnla_get_u32(info->attrs[NL80211_ATTR_AUTH_TYPE]);\n\t\tif (!nl80211_valid_auth_type(rdev, connect.auth_type,\n\t\t\t\t\t     NL80211_CMD_CONNECT))\n\t\t\treturn -EINVAL;\n\t} else\n\t\tconnect.auth_type = NL80211_AUTHTYPE_AUTOMATIC;\n\n\tconnect.privacy = info->attrs[NL80211_ATTR_PRIVACY];\n\n\tif (info->attrs[NL80211_ATTR_WANT_1X_4WAY_HS] &&\n\t    !wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t     NL80211_EXT_FEATURE_4WAY_HANDSHAKE_STA_1X))\n\t\treturn -EINVAL;\n\tconnect.want_1x = info->attrs[NL80211_ATTR_WANT_1X_4WAY_HS];\n\n\terr = nl80211_crypto_settings(rdev, info, &connect.crypto,\n\t\t\t\t      NL80211_MAX_NR_CIPHER_SUITES);\n\tif (err)\n\t\treturn err;\n\n\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_STATION &&\n\t    dev->ieee80211_ptr->iftype != NL80211_IFTYPE_P2P_CLIENT)\n\t\treturn -EOPNOTSUPP;\n\n\twiphy = &rdev->wiphy;\n\n\tconnect.bg_scan_period = -1;\n\tif (info->attrs[NL80211_ATTR_BG_SCAN_PERIOD] &&\n\t\t(wiphy->flags & WIPHY_FLAG_SUPPORTS_FW_ROAM)) {\n\t\tconnect.bg_scan_period =\n\t\t\tnla_get_u16(info->attrs[NL80211_ATTR_BG_SCAN_PERIOD]);\n\t}\n\n\tif (info->attrs[NL80211_ATTR_MAC])\n\t\tconnect.bssid = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\telse if (info->attrs[NL80211_ATTR_MAC_HINT])\n\t\tconnect.bssid_hint =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_MAC_HINT]);\n\tconnect.ssid = nla_data(info->attrs[NL80211_ATTR_SSID]);\n\tconnect.ssid_len = nla_len(info->attrs[NL80211_ATTR_SSID]);\n\n\tif (info->attrs[NL80211_ATTR_IE]) {\n\t\tconnect.ie = nla_data(info->attrs[NL80211_ATTR_IE]);\n\t\tconnect.ie_len = nla_len(info->attrs[NL80211_ATTR_IE]);\n\t}\n\n\tif (info->attrs[NL80211_ATTR_USE_MFP]) {\n\t\tconnect.mfp = nla_get_u32(info->attrs[NL80211_ATTR_USE_MFP]);\n\t\tif (connect.mfp == NL80211_MFP_OPTIONAL &&\n\t\t    !wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t\t     NL80211_EXT_FEATURE_MFP_OPTIONAL))\n\t\t\treturn -EOPNOTSUPP;\n\t} else {\n\t\tconnect.mfp = NL80211_MFP_NO;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_PREV_BSSID])\n\t\tconnect.prev_bssid =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_PREV_BSSID]);\n\n\tif (info->attrs[NL80211_ATTR_WIPHY_FREQ])\n\t\tfreq = MHZ_TO_KHZ(nla_get_u32(\n\t\t\t\t\tinfo->attrs[NL80211_ATTR_WIPHY_FREQ]));\n\tif (info->attrs[NL80211_ATTR_WIPHY_FREQ_OFFSET])\n\t\tfreq +=\n\t\t    nla_get_u32(info->attrs[NL80211_ATTR_WIPHY_FREQ_OFFSET]);\n\n\tif (freq) {\n\t\tconnect.channel = nl80211_get_valid_chan(wiphy, freq);\n\t\tif (!connect.channel)\n\t\t\treturn -EINVAL;\n\t} else if (info->attrs[NL80211_ATTR_WIPHY_FREQ_HINT]) {\n\t\tfreq = nla_get_u32(info->attrs[NL80211_ATTR_WIPHY_FREQ_HINT]);\n\t\tfreq = MHZ_TO_KHZ(freq);\n\t\tconnect.channel_hint = nl80211_get_valid_chan(wiphy, freq);\n\t\tif (!connect.channel_hint)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_WIPHY_EDMG_CHANNELS]) {\n\t\tconnect.edmg.channels =\n\t\t      nla_get_u8(info->attrs[NL80211_ATTR_WIPHY_EDMG_CHANNELS]);\n\n\t\tif (info->attrs[NL80211_ATTR_WIPHY_EDMG_BW_CONFIG])\n\t\t\tconnect.edmg.bw_config =\n\t\t\t\tnla_get_u8(info->attrs[NL80211_ATTR_WIPHY_EDMG_BW_CONFIG]);\n\t}\n\n\tif (connect.privacy && info->attrs[NL80211_ATTR_KEYS]) {\n\t\tconnkeys = nl80211_parse_connkeys(rdev, info, NULL);\n\t\tif (IS_ERR(connkeys))\n\t\t\treturn PTR_ERR(connkeys);\n\t}\n\n\tif (nla_get_flag(info->attrs[NL80211_ATTR_DISABLE_HT]))\n\t\tconnect.flags |= ASSOC_REQ_DISABLE_HT;\n\n\tif (info->attrs[NL80211_ATTR_HT_CAPABILITY_MASK])\n\t\tmemcpy(&connect.ht_capa_mask,\n\t\t       nla_data(info->attrs[NL80211_ATTR_HT_CAPABILITY_MASK]),\n\t\t       sizeof(connect.ht_capa_mask));\n\n\tif (info->attrs[NL80211_ATTR_HT_CAPABILITY]) {\n\t\tif (!info->attrs[NL80211_ATTR_HT_CAPABILITY_MASK]) {\n\t\t\tkfree_sensitive(connkeys);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tmemcpy(&connect.ht_capa,\n\t\t       nla_data(info->attrs[NL80211_ATTR_HT_CAPABILITY]),\n\t\t       sizeof(connect.ht_capa));\n\t}\n\n\tif (nla_get_flag(info->attrs[NL80211_ATTR_DISABLE_VHT]))\n\t\tconnect.flags |= ASSOC_REQ_DISABLE_VHT;\n\n\tif (nla_get_flag(info->attrs[NL80211_ATTR_DISABLE_HE]))\n\t\tconnect.flags |= ASSOC_REQ_DISABLE_HE;\n\n\tif (info->attrs[NL80211_ATTR_VHT_CAPABILITY_MASK])\n\t\tmemcpy(&connect.vht_capa_mask,\n\t\t       nla_data(info->attrs[NL80211_ATTR_VHT_CAPABILITY_MASK]),\n\t\t       sizeof(connect.vht_capa_mask));\n\n\tif (info->attrs[NL80211_ATTR_VHT_CAPABILITY]) {\n\t\tif (!info->attrs[NL80211_ATTR_VHT_CAPABILITY_MASK]) {\n\t\t\tkfree_sensitive(connkeys);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tmemcpy(&connect.vht_capa,\n\t\t       nla_data(info->attrs[NL80211_ATTR_VHT_CAPABILITY]),\n\t\t       sizeof(connect.vht_capa));\n\t}\n\n\tif (nla_get_flag(info->attrs[NL80211_ATTR_USE_RRM])) {\n\t\tif (!((rdev->wiphy.features &\n\t\t\tNL80211_FEATURE_DS_PARAM_SET_IE_IN_PROBES) &&\n\t\t       (rdev->wiphy.features & NL80211_FEATURE_QUIET)) &&\n\t\t    !wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t\t     NL80211_EXT_FEATURE_RRM)) {\n\t\t\tkfree_sensitive(connkeys);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tconnect.flags |= ASSOC_REQ_USE_RRM;\n\t}\n\n\tconnect.pbss = nla_get_flag(info->attrs[NL80211_ATTR_PBSS]);\n\tif (connect.pbss && !rdev->wiphy.bands[NL80211_BAND_60GHZ]) {\n\t\tkfree_sensitive(connkeys);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_BSS_SELECT]) {\n\t\t/* bss selection makes no sense if bssid is set */\n\t\tif (connect.bssid) {\n\t\t\tkfree_sensitive(connkeys);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\terr = parse_bss_select(info->attrs[NL80211_ATTR_BSS_SELECT],\n\t\t\t\t       wiphy, &connect.bss_select);\n\t\tif (err) {\n\t\t\tkfree_sensitive(connkeys);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t    NL80211_EXT_FEATURE_FILS_SK_OFFLOAD) &&\n\t    info->attrs[NL80211_ATTR_FILS_ERP_USERNAME] &&\n\t    info->attrs[NL80211_ATTR_FILS_ERP_REALM] &&\n\t    info->attrs[NL80211_ATTR_FILS_ERP_NEXT_SEQ_NUM] &&\n\t    info->attrs[NL80211_ATTR_FILS_ERP_RRK]) {\n\t\tconnect.fils_erp_username =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_FILS_ERP_USERNAME]);\n\t\tconnect.fils_erp_username_len =\n\t\t\tnla_len(info->attrs[NL80211_ATTR_FILS_ERP_USERNAME]);\n\t\tconnect.fils_erp_realm =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_FILS_ERP_REALM]);\n\t\tconnect.fils_erp_realm_len =\n\t\t\tnla_len(info->attrs[NL80211_ATTR_FILS_ERP_REALM]);\n\t\tconnect.fils_erp_next_seq_num =\n\t\t\tnla_get_u16(\n\t\t\t   info->attrs[NL80211_ATTR_FILS_ERP_NEXT_SEQ_NUM]);\n\t\tconnect.fils_erp_rrk =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_FILS_ERP_RRK]);\n\t\tconnect.fils_erp_rrk_len =\n\t\t\tnla_len(info->attrs[NL80211_ATTR_FILS_ERP_RRK]);\n\t} else if (info->attrs[NL80211_ATTR_FILS_ERP_USERNAME] ||\n\t\t   info->attrs[NL80211_ATTR_FILS_ERP_REALM] ||\n\t\t   info->attrs[NL80211_ATTR_FILS_ERP_NEXT_SEQ_NUM] ||\n\t\t   info->attrs[NL80211_ATTR_FILS_ERP_RRK]) {\n\t\tkfree_sensitive(connkeys);\n\t\treturn -EINVAL;\n\t}\n\n\tif (nla_get_flag(info->attrs[NL80211_ATTR_EXTERNAL_AUTH_SUPPORT])) {\n\t\tif (!info->attrs[NL80211_ATTR_SOCKET_OWNER]) {\n\t\t\tkfree_sensitive(connkeys);\n\t\t\tGENL_SET_ERR_MSG(info,\n\t\t\t\t\t \"external auth requires connection ownership\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tconnect.flags |= CONNECT_REQ_EXTERNAL_AUTH_SUPPORT;\n\t}\n\n\twdev_lock(dev->ieee80211_ptr);\n\n\terr = cfg80211_connect(rdev, dev, &connect, connkeys,\n\t\t\t       connect.prev_bssid);\n\tif (err)\n\t\tkfree_sensitive(connkeys);\n\n\tif (!err && info->attrs[NL80211_ATTR_SOCKET_OWNER]) {\n\t\tdev->ieee80211_ptr->conn_owner_nlportid = info->snd_portid;\n\t\tif (connect.bssid)\n\t\t\tmemcpy(dev->ieee80211_ptr->disconnect_bssid,\n\t\t\t       connect.bssid, ETH_ALEN);\n\t\telse\n\t\t\teth_zero_addr(dev->ieee80211_ptr->disconnect_bssid);\n\t}\n\n\twdev_unlock(dev->ieee80211_ptr);\n\n\treturn err;\n}\n\nstatic int nl80211_update_connect_params(struct sk_buff *skb,\n\t\t\t\t\t struct genl_info *info)\n{\n\tstruct cfg80211_connect_params connect = {};\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tbool fils_sk_offload;\n\tu32 auth_type;\n\tu32 changed = 0;\n\tint ret;\n\n\tif (!rdev->ops->update_connect_params)\n\t\treturn -EOPNOTSUPP;\n\n\tif (info->attrs[NL80211_ATTR_IE]) {\n\t\tconnect.ie = nla_data(info->attrs[NL80211_ATTR_IE]);\n\t\tconnect.ie_len = nla_len(info->attrs[NL80211_ATTR_IE]);\n\t\tchanged |= UPDATE_ASSOC_IES;\n\t}\n\n\tfils_sk_offload = wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t\t\t  NL80211_EXT_FEATURE_FILS_SK_OFFLOAD);\n\n\t/*\n\t * when driver supports fils-sk offload all attributes must be\n\t * provided. So the else covers \"fils-sk-not-all\" and\n\t * \"no-fils-sk-any\".\n\t */\n\tif (fils_sk_offload &&\n\t    info->attrs[NL80211_ATTR_FILS_ERP_USERNAME] &&\n\t    info->attrs[NL80211_ATTR_FILS_ERP_REALM] &&\n\t    info->attrs[NL80211_ATTR_FILS_ERP_NEXT_SEQ_NUM] &&\n\t    info->attrs[NL80211_ATTR_FILS_ERP_RRK]) {\n\t\tconnect.fils_erp_username =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_FILS_ERP_USERNAME]);\n\t\tconnect.fils_erp_username_len =\n\t\t\tnla_len(info->attrs[NL80211_ATTR_FILS_ERP_USERNAME]);\n\t\tconnect.fils_erp_realm =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_FILS_ERP_REALM]);\n\t\tconnect.fils_erp_realm_len =\n\t\t\tnla_len(info->attrs[NL80211_ATTR_FILS_ERP_REALM]);\n\t\tconnect.fils_erp_next_seq_num =\n\t\t\tnla_get_u16(\n\t\t\t   info->attrs[NL80211_ATTR_FILS_ERP_NEXT_SEQ_NUM]);\n\t\tconnect.fils_erp_rrk =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_FILS_ERP_RRK]);\n\t\tconnect.fils_erp_rrk_len =\n\t\t\tnla_len(info->attrs[NL80211_ATTR_FILS_ERP_RRK]);\n\t\tchanged |= UPDATE_FILS_ERP_INFO;\n\t} else if (info->attrs[NL80211_ATTR_FILS_ERP_USERNAME] ||\n\t\t   info->attrs[NL80211_ATTR_FILS_ERP_REALM] ||\n\t\t   info->attrs[NL80211_ATTR_FILS_ERP_NEXT_SEQ_NUM] ||\n\t\t   info->attrs[NL80211_ATTR_FILS_ERP_RRK]) {\n\t\treturn -EINVAL;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_AUTH_TYPE]) {\n\t\tauth_type = nla_get_u32(info->attrs[NL80211_ATTR_AUTH_TYPE]);\n\t\tif (!nl80211_valid_auth_type(rdev, auth_type,\n\t\t\t\t\t     NL80211_CMD_CONNECT))\n\t\t\treturn -EINVAL;\n\n\t\tif (auth_type == NL80211_AUTHTYPE_FILS_SK &&\n\t\t    fils_sk_offload && !(changed & UPDATE_FILS_ERP_INFO))\n\t\t\treturn -EINVAL;\n\n\t\tconnect.auth_type = auth_type;\n\t\tchanged |= UPDATE_AUTH_TYPE;\n\t}\n\n\twdev_lock(dev->ieee80211_ptr);\n\tif (!wdev->current_bss)\n\t\tret = -ENOLINK;\n\telse\n\t\tret = rdev_update_connect_params(rdev, dev, &connect, changed);\n\twdev_unlock(dev->ieee80211_ptr);\n\n\treturn ret;\n}\n\nstatic int nl80211_disconnect(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tu16 reason;\n\tint ret;\n\n\tif (dev->ieee80211_ptr->conn_owner_nlportid &&\n\t    dev->ieee80211_ptr->conn_owner_nlportid != info->snd_portid)\n\t\treturn -EPERM;\n\n\tif (!info->attrs[NL80211_ATTR_REASON_CODE])\n\t\treason = WLAN_REASON_DEAUTH_LEAVING;\n\telse\n\t\treason = nla_get_u16(info->attrs[NL80211_ATTR_REASON_CODE]);\n\n\tif (reason == 0)\n\t\treturn -EINVAL;\n\n\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_STATION &&\n\t    dev->ieee80211_ptr->iftype != NL80211_IFTYPE_P2P_CLIENT)\n\t\treturn -EOPNOTSUPP;\n\n\twdev_lock(dev->ieee80211_ptr);\n\tret = cfg80211_disconnect(rdev, dev, reason, true);\n\twdev_unlock(dev->ieee80211_ptr);\n\treturn ret;\n}\n\nstatic int nl80211_wiphy_netns(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net *net;\n\tint err;\n\n\tif (info->attrs[NL80211_ATTR_PID]) {\n\t\tu32 pid = nla_get_u32(info->attrs[NL80211_ATTR_PID]);\n\n\t\tnet = get_net_ns_by_pid(pid);\n\t} else if (info->attrs[NL80211_ATTR_NETNS_FD]) {\n\t\tu32 fd = nla_get_u32(info->attrs[NL80211_ATTR_NETNS_FD]);\n\n\t\tnet = get_net_ns_by_fd(fd);\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\n\tif (IS_ERR(net))\n\t\treturn PTR_ERR(net);\n\n\terr = 0;\n\n\t/* check if anything to do */\n\tif (!net_eq(wiphy_net(&rdev->wiphy), net))\n\t\terr = cfg80211_switch_netns(rdev, net);\n\n\tput_net(net);\n\treturn err;\n}\n\nstatic int nl80211_setdel_pmksa(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tint (*rdev_ops)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\tstruct cfg80211_pmksa *pmksa) = NULL;\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct cfg80211_pmksa pmksa;\n\n\tmemset(&pmksa, 0, sizeof(struct cfg80211_pmksa));\n\n\tif (!info->attrs[NL80211_ATTR_PMKID])\n\t\treturn -EINVAL;\n\n\tpmksa.pmkid = nla_data(info->attrs[NL80211_ATTR_PMKID]);\n\n\tif (info->attrs[NL80211_ATTR_MAC]) {\n\t\tpmksa.bssid = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\t} else if (info->attrs[NL80211_ATTR_SSID] &&\n\t\t   info->attrs[NL80211_ATTR_FILS_CACHE_ID] &&\n\t\t   (info->genlhdr->cmd == NL80211_CMD_DEL_PMKSA ||\n\t\t    info->attrs[NL80211_ATTR_PMK])) {\n\t\tpmksa.ssid = nla_data(info->attrs[NL80211_ATTR_SSID]);\n\t\tpmksa.ssid_len = nla_len(info->attrs[NL80211_ATTR_SSID]);\n\t\tpmksa.cache_id =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_FILS_CACHE_ID]);\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\tif (info->attrs[NL80211_ATTR_PMK]) {\n\t\tpmksa.pmk = nla_data(info->attrs[NL80211_ATTR_PMK]);\n\t\tpmksa.pmk_len = nla_len(info->attrs[NL80211_ATTR_PMK]);\n\t}\n\n\tif (info->attrs[NL80211_ATTR_PMK_LIFETIME])\n\t\tpmksa.pmk_lifetime =\n\t\t\tnla_get_u32(info->attrs[NL80211_ATTR_PMK_LIFETIME]);\n\n\tif (info->attrs[NL80211_ATTR_PMK_REAUTH_THRESHOLD])\n\t\tpmksa.pmk_reauth_threshold =\n\t\t\tnla_get_u8(\n\t\t\t\tinfo->attrs[NL80211_ATTR_PMK_REAUTH_THRESHOLD]);\n\n\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_STATION &&\n\t    dev->ieee80211_ptr->iftype != NL80211_IFTYPE_P2P_CLIENT &&\n\t    !(dev->ieee80211_ptr->iftype == NL80211_IFTYPE_AP &&\n\t      wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t      NL80211_EXT_FEATURE_AP_PMKSA_CACHING)))\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (info->genlhdr->cmd) {\n\tcase NL80211_CMD_SET_PMKSA:\n\t\trdev_ops = rdev->ops->set_pmksa;\n\t\tbreak;\n\tcase NL80211_CMD_DEL_PMKSA:\n\t\trdev_ops = rdev->ops->del_pmksa;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t\tbreak;\n\t}\n\n\tif (!rdev_ops)\n\t\treturn -EOPNOTSUPP;\n\n\treturn rdev_ops(&rdev->wiphy, dev, &pmksa);\n}\n\nstatic int nl80211_flush_pmksa(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\n\tif (dev->ieee80211_ptr->iftype != NL80211_IFTYPE_STATION &&\n\t    dev->ieee80211_ptr->iftype != NL80211_IFTYPE_P2P_CLIENT)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!rdev->ops->flush_pmksa)\n\t\treturn -EOPNOTSUPP;\n\n\treturn rdev_flush_pmksa(rdev, dev);\n}\n\nstatic int nl80211_tdls_mgmt(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tu8 action_code, dialog_token;\n\tu32 peer_capability = 0;\n\tu16 status_code;\n\tu8 *peer;\n\tbool initiator;\n\n\tif (!(rdev->wiphy.flags & WIPHY_FLAG_SUPPORTS_TDLS) ||\n\t    !rdev->ops->tdls_mgmt)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!info->attrs[NL80211_ATTR_TDLS_ACTION] ||\n\t    !info->attrs[NL80211_ATTR_STATUS_CODE] ||\n\t    !info->attrs[NL80211_ATTR_TDLS_DIALOG_TOKEN] ||\n\t    !info->attrs[NL80211_ATTR_IE] ||\n\t    !info->attrs[NL80211_ATTR_MAC])\n\t\treturn -EINVAL;\n\n\tpeer = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\taction_code = nla_get_u8(info->attrs[NL80211_ATTR_TDLS_ACTION]);\n\tstatus_code = nla_get_u16(info->attrs[NL80211_ATTR_STATUS_CODE]);\n\tdialog_token = nla_get_u8(info->attrs[NL80211_ATTR_TDLS_DIALOG_TOKEN]);\n\tinitiator = nla_get_flag(info->attrs[NL80211_ATTR_TDLS_INITIATOR]);\n\tif (info->attrs[NL80211_ATTR_TDLS_PEER_CAPABILITY])\n\t\tpeer_capability =\n\t\t\tnla_get_u32(info->attrs[NL80211_ATTR_TDLS_PEER_CAPABILITY]);\n\n\treturn rdev_tdls_mgmt(rdev, dev, peer, action_code,\n\t\t\t      dialog_token, status_code, peer_capability,\n\t\t\t      initiator,\n\t\t\t      nla_data(info->attrs[NL80211_ATTR_IE]),\n\t\t\t      nla_len(info->attrs[NL80211_ATTR_IE]));\n}\n\nstatic int nl80211_tdls_oper(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tenum nl80211_tdls_operation operation;\n\tu8 *peer;\n\n\tif (!(rdev->wiphy.flags & WIPHY_FLAG_SUPPORTS_TDLS) ||\n\t    !rdev->ops->tdls_oper)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!info->attrs[NL80211_ATTR_TDLS_OPERATION] ||\n\t    !info->attrs[NL80211_ATTR_MAC])\n\t\treturn -EINVAL;\n\n\toperation = nla_get_u8(info->attrs[NL80211_ATTR_TDLS_OPERATION]);\n\tpeer = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\n\treturn rdev_tdls_oper(rdev, dev, peer, operation);\n}\n\nstatic int nl80211_remain_on_channel(struct sk_buff *skb,\n\t\t\t\t     struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev = info->user_ptr[1];\n\tstruct cfg80211_chan_def chandef;\n\tconst struct cfg80211_chan_def *compat_chandef;\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\tu64 cookie;\n\tu32 duration;\n\tint err;\n\n\tif (!info->attrs[NL80211_ATTR_WIPHY_FREQ] ||\n\t    !info->attrs[NL80211_ATTR_DURATION])\n\t\treturn -EINVAL;\n\n\tduration = nla_get_u32(info->attrs[NL80211_ATTR_DURATION]);\n\n\tif (!rdev->ops->remain_on_channel ||\n\t    !(rdev->wiphy.flags & WIPHY_FLAG_HAS_REMAIN_ON_CHANNEL))\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t * We should be on that channel for at least a minimum amount of\n\t * time (10ms) but no longer than the driver supports.\n\t */\n\tif (duration < NL80211_MIN_REMAIN_ON_CHANNEL_TIME ||\n\t    duration > rdev->wiphy.max_remain_on_channel_duration)\n\t\treturn -EINVAL;\n\n\terr = nl80211_parse_chandef(rdev, info, &chandef);\n\tif (err)\n\t\treturn err;\n\n\twdev_lock(wdev);\n\tif (!cfg80211_off_channel_oper_allowed(wdev) &&\n\t    !cfg80211_chandef_identical(&wdev->chandef, &chandef)) {\n\t\tcompat_chandef = cfg80211_chandef_compatible(&wdev->chandef,\n\t\t\t\t\t\t\t     &chandef);\n\t\tif (compat_chandef != &chandef) {\n\t\t\twdev_unlock(wdev);\n\t\t\treturn -EBUSY;\n\t\t}\n\t}\n\twdev_unlock(wdev);\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\thdr = nl80211hdr_put(msg, info->snd_portid, info->snd_seq, 0,\n\t\t\t     NL80211_CMD_REMAIN_ON_CHANNEL);\n\tif (!hdr) {\n\t\terr = -ENOBUFS;\n\t\tgoto free_msg;\n\t}\n\n\terr = rdev_remain_on_channel(rdev, wdev, chandef.chan,\n\t\t\t\t     duration, &cookie);\n\n\tif (err)\n\t\tgoto free_msg;\n\n\tif (nla_put_u64_64bit(msg, NL80211_ATTR_COOKIE, cookie,\n\t\t\t      NL80211_ATTR_PAD))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\n\treturn genlmsg_reply(msg, info);\n\n nla_put_failure:\n\terr = -ENOBUFS;\n free_msg:\n\tnlmsg_free(msg);\n\treturn err;\n}\n\nstatic int nl80211_cancel_remain_on_channel(struct sk_buff *skb,\n\t\t\t\t\t    struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev = info->user_ptr[1];\n\tu64 cookie;\n\n\tif (!info->attrs[NL80211_ATTR_COOKIE])\n\t\treturn -EINVAL;\n\n\tif (!rdev->ops->cancel_remain_on_channel)\n\t\treturn -EOPNOTSUPP;\n\n\tcookie = nla_get_u64(info->attrs[NL80211_ATTR_COOKIE]);\n\n\treturn rdev_cancel_remain_on_channel(rdev, wdev, cookie);\n}\n\nstatic int nl80211_set_tx_bitrate_mask(struct sk_buff *skb,\n\t\t\t\t       struct genl_info *info)\n{\n\tstruct cfg80211_bitrate_mask mask;\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tint err;\n\n\tif (!rdev->ops->set_bitrate_mask)\n\t\treturn -EOPNOTSUPP;\n\n\terr = nl80211_parse_tx_bitrate_mask(info, info->attrs,\n\t\t\t\t\t    NL80211_ATTR_TX_RATES, &mask,\n\t\t\t\t\t    dev, true);\n\tif (err)\n\t\treturn err;\n\n\treturn rdev_set_bitrate_mask(rdev, dev, NULL, &mask);\n}\n\nstatic int nl80211_register_mgmt(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev = info->user_ptr[1];\n\tu16 frame_type = IEEE80211_FTYPE_MGMT | IEEE80211_STYPE_ACTION;\n\n\tif (!info->attrs[NL80211_ATTR_FRAME_MATCH])\n\t\treturn -EINVAL;\n\n\tif (info->attrs[NL80211_ATTR_FRAME_TYPE])\n\t\tframe_type = nla_get_u16(info->attrs[NL80211_ATTR_FRAME_TYPE]);\n\n\tswitch (wdev->iftype) {\n\tcase NL80211_IFTYPE_STATION:\n\tcase NL80211_IFTYPE_ADHOC:\n\tcase NL80211_IFTYPE_P2P_CLIENT:\n\tcase NL80211_IFTYPE_AP:\n\tcase NL80211_IFTYPE_AP_VLAN:\n\tcase NL80211_IFTYPE_MESH_POINT:\n\tcase NL80211_IFTYPE_P2P_GO:\n\tcase NL80211_IFTYPE_P2P_DEVICE:\n\t\tbreak;\n\tcase NL80211_IFTYPE_NAN:\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t/* not much point in registering if we can't reply */\n\tif (!rdev->ops->mgmt_tx)\n\t\treturn -EOPNOTSUPP;\n\n\tif (info->attrs[NL80211_ATTR_RECEIVE_MULTICAST] &&\n\t    !wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t     NL80211_EXT_FEATURE_MULTICAST_REGISTRATIONS)) {\n\t\tGENL_SET_ERR_MSG(info,\n\t\t\t\t \"multicast RX registrations are not supported\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\treturn cfg80211_mlme_register_mgmt(wdev, info->snd_portid, frame_type,\n\t\t\t\t\t   nla_data(info->attrs[NL80211_ATTR_FRAME_MATCH]),\n\t\t\t\t\t   nla_len(info->attrs[NL80211_ATTR_FRAME_MATCH]),\n\t\t\t\t\t   info->attrs[NL80211_ATTR_RECEIVE_MULTICAST],\n\t\t\t\t\t   info->extack);\n}\n\nstatic int nl80211_tx_mgmt(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev = info->user_ptr[1];\n\tstruct cfg80211_chan_def chandef;\n\tint err;\n\tvoid *hdr = NULL;\n\tu64 cookie;\n\tstruct sk_buff *msg = NULL;\n\tstruct cfg80211_mgmt_tx_params params = {\n\t\t.dont_wait_for_ack =\n\t\t\tinfo->attrs[NL80211_ATTR_DONT_WAIT_FOR_ACK],\n\t};\n\n\tif (!info->attrs[NL80211_ATTR_FRAME])\n\t\treturn -EINVAL;\n\n\tif (!rdev->ops->mgmt_tx)\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (wdev->iftype) {\n\tcase NL80211_IFTYPE_P2P_DEVICE:\n\t\tif (!info->attrs[NL80211_ATTR_WIPHY_FREQ])\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase NL80211_IFTYPE_STATION:\n\tcase NL80211_IFTYPE_ADHOC:\n\tcase NL80211_IFTYPE_P2P_CLIENT:\n\tcase NL80211_IFTYPE_AP:\n\tcase NL80211_IFTYPE_AP_VLAN:\n\tcase NL80211_IFTYPE_MESH_POINT:\n\tcase NL80211_IFTYPE_P2P_GO:\n\t\tbreak;\n\tcase NL80211_IFTYPE_NAN:\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_DURATION]) {\n\t\tif (!(rdev->wiphy.flags & WIPHY_FLAG_OFFCHAN_TX))\n\t\t\treturn -EINVAL;\n\t\tparams.wait = nla_get_u32(info->attrs[NL80211_ATTR_DURATION]);\n\n\t\t/*\n\t\t * We should wait on the channel for at least a minimum amount\n\t\t * of time (10ms) but no longer than the driver supports.\n\t\t */\n\t\tif (params.wait < NL80211_MIN_REMAIN_ON_CHANNEL_TIME ||\n\t\t    params.wait > rdev->wiphy.max_remain_on_channel_duration)\n\t\t\treturn -EINVAL;\n\t}\n\n\tparams.offchan = info->attrs[NL80211_ATTR_OFFCHANNEL_TX_OK];\n\n\tif (params.offchan && !(rdev->wiphy.flags & WIPHY_FLAG_OFFCHAN_TX))\n\t\treturn -EINVAL;\n\n\tparams.no_cck = nla_get_flag(info->attrs[NL80211_ATTR_TX_NO_CCK_RATE]);\n\n\t/* get the channel if any has been specified, otherwise pass NULL to\n\t * the driver. The latter will use the current one\n\t */\n\tchandef.chan = NULL;\n\tif (info->attrs[NL80211_ATTR_WIPHY_FREQ]) {\n\t\terr = nl80211_parse_chandef(rdev, info, &chandef);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (!chandef.chan && params.offchan)\n\t\treturn -EINVAL;\n\n\twdev_lock(wdev);\n\tif (params.offchan && !cfg80211_off_channel_oper_allowed(wdev)) {\n\t\twdev_unlock(wdev);\n\t\treturn -EBUSY;\n\t}\n\twdev_unlock(wdev);\n\n\tparams.buf = nla_data(info->attrs[NL80211_ATTR_FRAME]);\n\tparams.len = nla_len(info->attrs[NL80211_ATTR_FRAME]);\n\n\tif (info->attrs[NL80211_ATTR_CSA_C_OFFSETS_TX]) {\n\t\tint len = nla_len(info->attrs[NL80211_ATTR_CSA_C_OFFSETS_TX]);\n\t\tint i;\n\n\t\tif (len % sizeof(u16))\n\t\t\treturn -EINVAL;\n\n\t\tparams.n_csa_offsets = len / sizeof(u16);\n\t\tparams.csa_offsets =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_CSA_C_OFFSETS_TX]);\n\n\t\t/* check that all the offsets fit the frame */\n\t\tfor (i = 0; i < params.n_csa_offsets; i++) {\n\t\t\tif (params.csa_offsets[i] >= params.len)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (!params.dont_wait_for_ack) {\n\t\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\t\tif (!msg)\n\t\t\treturn -ENOMEM;\n\n\t\thdr = nl80211hdr_put(msg, info->snd_portid, info->snd_seq, 0,\n\t\t\t\t     NL80211_CMD_FRAME);\n\t\tif (!hdr) {\n\t\t\terr = -ENOBUFS;\n\t\t\tgoto free_msg;\n\t\t}\n\t}\n\n\tparams.chan = chandef.chan;\n\terr = cfg80211_mlme_mgmt_tx(rdev, wdev, &params, &cookie);\n\tif (err)\n\t\tgoto free_msg;\n\n\tif (msg) {\n\t\tif (nla_put_u64_64bit(msg, NL80211_ATTR_COOKIE, cookie,\n\t\t\t\t      NL80211_ATTR_PAD))\n\t\t\tgoto nla_put_failure;\n\n\t\tgenlmsg_end(msg, hdr);\n\t\treturn genlmsg_reply(msg, info);\n\t}\n\n\treturn 0;\n\n nla_put_failure:\n\terr = -ENOBUFS;\n free_msg:\n\tnlmsg_free(msg);\n\treturn err;\n}\n\nstatic int nl80211_tx_mgmt_cancel_wait(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev = info->user_ptr[1];\n\tu64 cookie;\n\n\tif (!info->attrs[NL80211_ATTR_COOKIE])\n\t\treturn -EINVAL;\n\n\tif (!rdev->ops->mgmt_tx_cancel_wait)\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (wdev->iftype) {\n\tcase NL80211_IFTYPE_STATION:\n\tcase NL80211_IFTYPE_ADHOC:\n\tcase NL80211_IFTYPE_P2P_CLIENT:\n\tcase NL80211_IFTYPE_AP:\n\tcase NL80211_IFTYPE_AP_VLAN:\n\tcase NL80211_IFTYPE_P2P_GO:\n\tcase NL80211_IFTYPE_P2P_DEVICE:\n\t\tbreak;\n\tcase NL80211_IFTYPE_NAN:\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tcookie = nla_get_u64(info->attrs[NL80211_ATTR_COOKIE]);\n\n\treturn rdev_mgmt_tx_cancel_wait(rdev, wdev, cookie);\n}\n\nstatic int nl80211_set_power_save(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev;\n\tstruct net_device *dev = info->user_ptr[1];\n\tu8 ps_state;\n\tbool state;\n\tint err;\n\n\tif (!info->attrs[NL80211_ATTR_PS_STATE])\n\t\treturn -EINVAL;\n\n\tps_state = nla_get_u32(info->attrs[NL80211_ATTR_PS_STATE]);\n\n\twdev = dev->ieee80211_ptr;\n\n\tif (!rdev->ops->set_power_mgmt)\n\t\treturn -EOPNOTSUPP;\n\n\tstate = (ps_state == NL80211_PS_ENABLED) ? true : false;\n\n\tif (state == wdev->ps)\n\t\treturn 0;\n\n\terr = rdev_set_power_mgmt(rdev, dev, state, wdev->ps_timeout);\n\tif (!err)\n\t\twdev->ps = state;\n\treturn err;\n}\n\nstatic int nl80211_get_power_save(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tenum nl80211_ps_state ps_state;\n\tstruct wireless_dev *wdev;\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\tint err;\n\n\twdev = dev->ieee80211_ptr;\n\n\tif (!rdev->ops->set_power_mgmt)\n\t\treturn -EOPNOTSUPP;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\thdr = nl80211hdr_put(msg, info->snd_portid, info->snd_seq, 0,\n\t\t\t     NL80211_CMD_GET_POWER_SAVE);\n\tif (!hdr) {\n\t\terr = -ENOBUFS;\n\t\tgoto free_msg;\n\t}\n\n\tif (wdev->ps)\n\t\tps_state = NL80211_PS_ENABLED;\n\telse\n\t\tps_state = NL80211_PS_DISABLED;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_PS_STATE, ps_state))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\treturn genlmsg_reply(msg, info);\n\n nla_put_failure:\n\terr = -ENOBUFS;\n free_msg:\n\tnlmsg_free(msg);\n\treturn err;\n}\n\nstatic const struct nla_policy\nnl80211_attr_cqm_policy[NL80211_ATTR_CQM_MAX + 1] = {\n\t[NL80211_ATTR_CQM_RSSI_THOLD] = { .type = NLA_BINARY },\n\t[NL80211_ATTR_CQM_RSSI_HYST] = { .type = NLA_U32 },\n\t[NL80211_ATTR_CQM_RSSI_THRESHOLD_EVENT] = { .type = NLA_U32 },\n\t[NL80211_ATTR_CQM_TXE_RATE] = { .type = NLA_U32 },\n\t[NL80211_ATTR_CQM_TXE_PKTS] = { .type = NLA_U32 },\n\t[NL80211_ATTR_CQM_TXE_INTVL] = { .type = NLA_U32 },\n\t[NL80211_ATTR_CQM_RSSI_LEVEL] = { .type = NLA_S32 },\n};\n\nstatic int nl80211_set_cqm_txe(struct genl_info *info,\n\t\t\t       u32 rate, u32 pkts, u32 intvl)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\n\tif (rate > 100 || intvl > NL80211_CQM_TXE_MAX_INTVL)\n\t\treturn -EINVAL;\n\n\tif (!rdev->ops->set_cqm_txe_config)\n\t\treturn -EOPNOTSUPP;\n\n\tif (wdev->iftype != NL80211_IFTYPE_STATION &&\n\t    wdev->iftype != NL80211_IFTYPE_P2P_CLIENT)\n\t\treturn -EOPNOTSUPP;\n\n\treturn rdev_set_cqm_txe_config(rdev, dev, rate, pkts, intvl);\n}\n\nstatic int cfg80211_cqm_rssi_update(struct cfg80211_registered_device *rdev,\n\t\t\t\t    struct net_device *dev)\n{\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\ts32 last, low, high;\n\tu32 hyst;\n\tint i, n, low_index;\n\tint err;\n\n\t/* RSSI reporting disabled? */\n\tif (!wdev->cqm_config)\n\t\treturn rdev_set_cqm_rssi_range_config(rdev, dev, 0, 0);\n\n\t/*\n\t * Obtain current RSSI value if possible, if not and no RSSI threshold\n\t * event has been received yet, we should receive an event after a\n\t * connection is established and enough beacons received to calculate\n\t * the average.\n\t */\n\tif (!wdev->cqm_config->last_rssi_event_value && wdev->current_bss &&\n\t    rdev->ops->get_station) {\n\t\tstruct station_info sinfo = {};\n\t\tu8 *mac_addr;\n\n\t\tmac_addr = wdev->current_bss->pub.bssid;\n\n\t\terr = rdev_get_station(rdev, dev, mac_addr, &sinfo);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tcfg80211_sinfo_release_content(&sinfo);\n\t\tif (sinfo.filled & BIT_ULL(NL80211_STA_INFO_BEACON_SIGNAL_AVG))\n\t\t\twdev->cqm_config->last_rssi_event_value =\n\t\t\t\t(s8) sinfo.rx_beacon_signal_avg;\n\t}\n\n\tlast = wdev->cqm_config->last_rssi_event_value;\n\thyst = wdev->cqm_config->rssi_hyst;\n\tn = wdev->cqm_config->n_rssi_thresholds;\n\n\tfor (i = 0; i < n; i++) {\n\t\ti = array_index_nospec(i, n);\n\t\tif (last < wdev->cqm_config->rssi_thresholds[i])\n\t\t\tbreak;\n\t}\n\n\tlow_index = i - 1;\n\tif (low_index >= 0) {\n\t\tlow_index = array_index_nospec(low_index, n);\n\t\tlow = wdev->cqm_config->rssi_thresholds[low_index] - hyst;\n\t} else {\n\t\tlow = S32_MIN;\n\t}\n\tif (i < n) {\n\t\ti = array_index_nospec(i, n);\n\t\thigh = wdev->cqm_config->rssi_thresholds[i] + hyst - 1;\n\t} else {\n\t\thigh = S32_MAX;\n\t}\n\n\treturn rdev_set_cqm_rssi_range_config(rdev, dev, low, high);\n}\n\nstatic int nl80211_set_cqm_rssi(struct genl_info *info,\n\t\t\t\tconst s32 *thresholds, int n_thresholds,\n\t\t\t\tu32 hysteresis)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tint i, err;\n\ts32 prev = S32_MIN;\n\n\t/* Check all values negative and sorted */\n\tfor (i = 0; i < n_thresholds; i++) {\n\t\tif (thresholds[i] > 0 || thresholds[i] <= prev)\n\t\t\treturn -EINVAL;\n\n\t\tprev = thresholds[i];\n\t}\n\n\tif (wdev->iftype != NL80211_IFTYPE_STATION &&\n\t    wdev->iftype != NL80211_IFTYPE_P2P_CLIENT)\n\t\treturn -EOPNOTSUPP;\n\n\twdev_lock(wdev);\n\tcfg80211_cqm_config_free(wdev);\n\twdev_unlock(wdev);\n\n\tif (n_thresholds <= 1 && rdev->ops->set_cqm_rssi_config) {\n\t\tif (n_thresholds == 0 || thresholds[0] == 0) /* Disabling */\n\t\t\treturn rdev_set_cqm_rssi_config(rdev, dev, 0, 0);\n\n\t\treturn rdev_set_cqm_rssi_config(rdev, dev,\n\t\t\t\t\t\tthresholds[0], hysteresis);\n\t}\n\n\tif (!wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t     NL80211_EXT_FEATURE_CQM_RSSI_LIST))\n\t\treturn -EOPNOTSUPP;\n\n\tif (n_thresholds == 1 && thresholds[0] == 0) /* Disabling */\n\t\tn_thresholds = 0;\n\n\twdev_lock(wdev);\n\tif (n_thresholds) {\n\t\tstruct cfg80211_cqm_config *cqm_config;\n\n\t\tcqm_config = kzalloc(sizeof(struct cfg80211_cqm_config) +\n\t\t\t\t     n_thresholds * sizeof(s32), GFP_KERNEL);\n\t\tif (!cqm_config) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tcqm_config->rssi_hyst = hysteresis;\n\t\tcqm_config->n_rssi_thresholds = n_thresholds;\n\t\tmemcpy(cqm_config->rssi_thresholds, thresholds,\n\t\t       n_thresholds * sizeof(s32));\n\n\t\twdev->cqm_config = cqm_config;\n\t}\n\n\terr = cfg80211_cqm_rssi_update(rdev, dev);\n\nunlock:\n\twdev_unlock(wdev);\n\n\treturn err;\n}\n\nstatic int nl80211_set_cqm(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct nlattr *attrs[NL80211_ATTR_CQM_MAX + 1];\n\tstruct nlattr *cqm;\n\tint err;\n\n\tcqm = info->attrs[NL80211_ATTR_CQM];\n\tif (!cqm)\n\t\treturn -EINVAL;\n\n\terr = nla_parse_nested_deprecated(attrs, NL80211_ATTR_CQM_MAX, cqm,\n\t\t\t\t\t  nl80211_attr_cqm_policy,\n\t\t\t\t\t  info->extack);\n\tif (err)\n\t\treturn err;\n\n\tif (attrs[NL80211_ATTR_CQM_RSSI_THOLD] &&\n\t    attrs[NL80211_ATTR_CQM_RSSI_HYST]) {\n\t\tconst s32 *thresholds =\n\t\t\tnla_data(attrs[NL80211_ATTR_CQM_RSSI_THOLD]);\n\t\tint len = nla_len(attrs[NL80211_ATTR_CQM_RSSI_THOLD]);\n\t\tu32 hysteresis = nla_get_u32(attrs[NL80211_ATTR_CQM_RSSI_HYST]);\n\n\t\tif (len % 4)\n\t\t\treturn -EINVAL;\n\n\t\treturn nl80211_set_cqm_rssi(info, thresholds, len / 4,\n\t\t\t\t\t    hysteresis);\n\t}\n\n\tif (attrs[NL80211_ATTR_CQM_TXE_RATE] &&\n\t    attrs[NL80211_ATTR_CQM_TXE_PKTS] &&\n\t    attrs[NL80211_ATTR_CQM_TXE_INTVL]) {\n\t\tu32 rate = nla_get_u32(attrs[NL80211_ATTR_CQM_TXE_RATE]);\n\t\tu32 pkts = nla_get_u32(attrs[NL80211_ATTR_CQM_TXE_PKTS]);\n\t\tu32 intvl = nla_get_u32(attrs[NL80211_ATTR_CQM_TXE_INTVL]);\n\n\t\treturn nl80211_set_cqm_txe(info, rate, pkts, intvl);\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int nl80211_join_ocb(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct ocb_setup setup = {};\n\tint err;\n\n\terr = nl80211_parse_chandef(rdev, info, &setup.chandef);\n\tif (err)\n\t\treturn err;\n\n\treturn cfg80211_join_ocb(rdev, dev, &setup);\n}\n\nstatic int nl80211_leave_ocb(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\n\treturn cfg80211_leave_ocb(rdev, dev);\n}\n\nstatic int nl80211_join_mesh(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct mesh_config cfg;\n\tstruct mesh_setup setup;\n\tint err;\n\n\t/* start with default */\n\tmemcpy(&cfg, &default_mesh_config, sizeof(cfg));\n\tmemcpy(&setup, &default_mesh_setup, sizeof(setup));\n\n\tif (info->attrs[NL80211_ATTR_MESH_CONFIG]) {\n\t\t/* and parse parameters if given */\n\t\terr = nl80211_parse_mesh_config(info, &cfg, NULL);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (!info->attrs[NL80211_ATTR_MESH_ID] ||\n\t    !nla_len(info->attrs[NL80211_ATTR_MESH_ID]))\n\t\treturn -EINVAL;\n\n\tsetup.mesh_id = nla_data(info->attrs[NL80211_ATTR_MESH_ID]);\n\tsetup.mesh_id_len = nla_len(info->attrs[NL80211_ATTR_MESH_ID]);\n\n\tif (info->attrs[NL80211_ATTR_MCAST_RATE] &&\n\t    !nl80211_parse_mcast_rate(rdev, setup.mcast_rate,\n\t\t\t    nla_get_u32(info->attrs[NL80211_ATTR_MCAST_RATE])))\n\t\t\treturn -EINVAL;\n\n\tif (info->attrs[NL80211_ATTR_BEACON_INTERVAL]) {\n\t\tsetup.beacon_interval =\n\t\t\tnla_get_u32(info->attrs[NL80211_ATTR_BEACON_INTERVAL]);\n\n\t\terr = cfg80211_validate_beacon_int(rdev,\n\t\t\t\t\t\t   NL80211_IFTYPE_MESH_POINT,\n\t\t\t\t\t\t   setup.beacon_interval);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_DTIM_PERIOD]) {\n\t\tsetup.dtim_period =\n\t\t\tnla_get_u32(info->attrs[NL80211_ATTR_DTIM_PERIOD]);\n\t\tif (setup.dtim_period < 1 || setup.dtim_period > 100)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_MESH_SETUP]) {\n\t\t/* parse additional setup parameters if given */\n\t\terr = nl80211_parse_mesh_setup(info, &setup);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (setup.user_mpm)\n\t\tcfg.auto_open_plinks = false;\n\n\tif (info->attrs[NL80211_ATTR_WIPHY_FREQ]) {\n\t\terr = nl80211_parse_chandef(rdev, info, &setup.chandef);\n\t\tif (err)\n\t\t\treturn err;\n\t} else {\n\t\t/* __cfg80211_join_mesh() will sort it out */\n\t\tsetup.chandef.chan = NULL;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_BSS_BASIC_RATES]) {\n\t\tu8 *rates = nla_data(info->attrs[NL80211_ATTR_BSS_BASIC_RATES]);\n\t\tint n_rates =\n\t\t\tnla_len(info->attrs[NL80211_ATTR_BSS_BASIC_RATES]);\n\t\tstruct ieee80211_supported_band *sband;\n\n\t\tif (!setup.chandef.chan)\n\t\t\treturn -EINVAL;\n\n\t\tsband = rdev->wiphy.bands[setup.chandef.chan->band];\n\n\t\terr = ieee80211_get_ratemask(sband, rates, n_rates,\n\t\t\t\t\t     &setup.basic_rates);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_TX_RATES]) {\n\t\terr = nl80211_parse_tx_bitrate_mask(info, info->attrs,\n\t\t\t\t\t\t    NL80211_ATTR_TX_RATES,\n\t\t\t\t\t\t    &setup.beacon_rate,\n\t\t\t\t\t\t    dev, false);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (!setup.chandef.chan)\n\t\t\treturn -EINVAL;\n\n\t\terr = validate_beacon_tx_rate(rdev, setup.chandef.chan->band,\n\t\t\t\t\t      &setup.beacon_rate);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tsetup.userspace_handles_dfs =\n\t\tnla_get_flag(info->attrs[NL80211_ATTR_HANDLE_DFS]);\n\n\tif (info->attrs[NL80211_ATTR_CONTROL_PORT_OVER_NL80211]) {\n\t\tint r = validate_pae_over_nl80211(rdev, info);\n\n\t\tif (r < 0)\n\t\t\treturn r;\n\n\t\tsetup.control_port_over_nl80211 = true;\n\t}\n\n\twdev_lock(dev->ieee80211_ptr);\n\terr = __cfg80211_join_mesh(rdev, dev, &setup, &cfg);\n\tif (!err && info->attrs[NL80211_ATTR_SOCKET_OWNER])\n\t\tdev->ieee80211_ptr->conn_owner_nlportid = info->snd_portid;\n\twdev_unlock(dev->ieee80211_ptr);\n\n\treturn err;\n}\n\nstatic int nl80211_leave_mesh(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\n\treturn cfg80211_leave_mesh(rdev, dev);\n}\n\n#ifdef CONFIG_PM\nstatic int nl80211_send_wowlan_patterns(struct sk_buff *msg,\n\t\t\t\t\tstruct cfg80211_registered_device *rdev)\n{\n\tstruct cfg80211_wowlan *wowlan = rdev->wiphy.wowlan_config;\n\tstruct nlattr *nl_pats, *nl_pat;\n\tint i, pat_len;\n\n\tif (!wowlan->n_patterns)\n\t\treturn 0;\n\n\tnl_pats = nla_nest_start_noflag(msg, NL80211_WOWLAN_TRIG_PKT_PATTERN);\n\tif (!nl_pats)\n\t\treturn -ENOBUFS;\n\n\tfor (i = 0; i < wowlan->n_patterns; i++) {\n\t\tnl_pat = nla_nest_start_noflag(msg, i + 1);\n\t\tif (!nl_pat)\n\t\t\treturn -ENOBUFS;\n\t\tpat_len = wowlan->patterns[i].pattern_len;\n\t\tif (nla_put(msg, NL80211_PKTPAT_MASK, DIV_ROUND_UP(pat_len, 8),\n\t\t\t    wowlan->patterns[i].mask) ||\n\t\t    nla_put(msg, NL80211_PKTPAT_PATTERN, pat_len,\n\t\t\t    wowlan->patterns[i].pattern) ||\n\t\t    nla_put_u32(msg, NL80211_PKTPAT_OFFSET,\n\t\t\t\twowlan->patterns[i].pkt_offset))\n\t\t\treturn -ENOBUFS;\n\t\tnla_nest_end(msg, nl_pat);\n\t}\n\tnla_nest_end(msg, nl_pats);\n\n\treturn 0;\n}\n\nstatic int nl80211_send_wowlan_tcp(struct sk_buff *msg,\n\t\t\t\t   struct cfg80211_wowlan_tcp *tcp)\n{\n\tstruct nlattr *nl_tcp;\n\n\tif (!tcp)\n\t\treturn 0;\n\n\tnl_tcp = nla_nest_start_noflag(msg,\n\t\t\t\t       NL80211_WOWLAN_TRIG_TCP_CONNECTION);\n\tif (!nl_tcp)\n\t\treturn -ENOBUFS;\n\n\tif (nla_put_in_addr(msg, NL80211_WOWLAN_TCP_SRC_IPV4, tcp->src) ||\n\t    nla_put_in_addr(msg, NL80211_WOWLAN_TCP_DST_IPV4, tcp->dst) ||\n\t    nla_put(msg, NL80211_WOWLAN_TCP_DST_MAC, ETH_ALEN, tcp->dst_mac) ||\n\t    nla_put_u16(msg, NL80211_WOWLAN_TCP_SRC_PORT, tcp->src_port) ||\n\t    nla_put_u16(msg, NL80211_WOWLAN_TCP_DST_PORT, tcp->dst_port) ||\n\t    nla_put(msg, NL80211_WOWLAN_TCP_DATA_PAYLOAD,\n\t\t    tcp->payload_len, tcp->payload) ||\n\t    nla_put_u32(msg, NL80211_WOWLAN_TCP_DATA_INTERVAL,\n\t\t\ttcp->data_interval) ||\n\t    nla_put(msg, NL80211_WOWLAN_TCP_WAKE_PAYLOAD,\n\t\t    tcp->wake_len, tcp->wake_data) ||\n\t    nla_put(msg, NL80211_WOWLAN_TCP_WAKE_MASK,\n\t\t    DIV_ROUND_UP(tcp->wake_len, 8), tcp->wake_mask))\n\t\treturn -ENOBUFS;\n\n\tif (tcp->payload_seq.len &&\n\t    nla_put(msg, NL80211_WOWLAN_TCP_DATA_PAYLOAD_SEQ,\n\t\t    sizeof(tcp->payload_seq), &tcp->payload_seq))\n\t\treturn -ENOBUFS;\n\n\tif (tcp->payload_tok.len &&\n\t    nla_put(msg, NL80211_WOWLAN_TCP_DATA_PAYLOAD_TOKEN,\n\t\t    sizeof(tcp->payload_tok) + tcp->tokens_size,\n\t\t    &tcp->payload_tok))\n\t\treturn -ENOBUFS;\n\n\tnla_nest_end(msg, nl_tcp);\n\n\treturn 0;\n}\n\nstatic int nl80211_send_wowlan_nd(struct sk_buff *msg,\n\t\t\t\t  struct cfg80211_sched_scan_request *req)\n{\n\tstruct nlattr *nd, *freqs, *matches, *match, *scan_plans, *scan_plan;\n\tint i;\n\n\tif (!req)\n\t\treturn 0;\n\n\tnd = nla_nest_start_noflag(msg, NL80211_WOWLAN_TRIG_NET_DETECT);\n\tif (!nd)\n\t\treturn -ENOBUFS;\n\n\tif (req->n_scan_plans == 1 &&\n\t    nla_put_u32(msg, NL80211_ATTR_SCHED_SCAN_INTERVAL,\n\t\t\treq->scan_plans[0].interval * 1000))\n\t\treturn -ENOBUFS;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_SCHED_SCAN_DELAY, req->delay))\n\t\treturn -ENOBUFS;\n\n\tif (req->relative_rssi_set) {\n\t\tstruct nl80211_bss_select_rssi_adjust rssi_adjust;\n\n\t\tif (nla_put_s8(msg, NL80211_ATTR_SCHED_SCAN_RELATIVE_RSSI,\n\t\t\t       req->relative_rssi))\n\t\t\treturn -ENOBUFS;\n\n\t\trssi_adjust.band = req->rssi_adjust.band;\n\t\trssi_adjust.delta = req->rssi_adjust.delta;\n\t\tif (nla_put(msg, NL80211_ATTR_SCHED_SCAN_RSSI_ADJUST,\n\t\t\t    sizeof(rssi_adjust), &rssi_adjust))\n\t\t\treturn -ENOBUFS;\n\t}\n\n\tfreqs = nla_nest_start_noflag(msg, NL80211_ATTR_SCAN_FREQUENCIES);\n\tif (!freqs)\n\t\treturn -ENOBUFS;\n\n\tfor (i = 0; i < req->n_channels; i++) {\n\t\tif (nla_put_u32(msg, i, req->channels[i]->center_freq))\n\t\t\treturn -ENOBUFS;\n\t}\n\n\tnla_nest_end(msg, freqs);\n\n\tif (req->n_match_sets) {\n\t\tmatches = nla_nest_start_noflag(msg,\n\t\t\t\t\t\tNL80211_ATTR_SCHED_SCAN_MATCH);\n\t\tif (!matches)\n\t\t\treturn -ENOBUFS;\n\n\t\tfor (i = 0; i < req->n_match_sets; i++) {\n\t\t\tmatch = nla_nest_start_noflag(msg, i);\n\t\t\tif (!match)\n\t\t\t\treturn -ENOBUFS;\n\n\t\t\tif (nla_put(msg, NL80211_SCHED_SCAN_MATCH_ATTR_SSID,\n\t\t\t\t    req->match_sets[i].ssid.ssid_len,\n\t\t\t\t    req->match_sets[i].ssid.ssid))\n\t\t\t\treturn -ENOBUFS;\n\t\t\tnla_nest_end(msg, match);\n\t\t}\n\t\tnla_nest_end(msg, matches);\n\t}\n\n\tscan_plans = nla_nest_start_noflag(msg, NL80211_ATTR_SCHED_SCAN_PLANS);\n\tif (!scan_plans)\n\t\treturn -ENOBUFS;\n\n\tfor (i = 0; i < req->n_scan_plans; i++) {\n\t\tscan_plan = nla_nest_start_noflag(msg, i + 1);\n\t\tif (!scan_plan)\n\t\t\treturn -ENOBUFS;\n\n\t\tif (nla_put_u32(msg, NL80211_SCHED_SCAN_PLAN_INTERVAL,\n\t\t\t\treq->scan_plans[i].interval) ||\n\t\t    (req->scan_plans[i].iterations &&\n\t\t     nla_put_u32(msg, NL80211_SCHED_SCAN_PLAN_ITERATIONS,\n\t\t\t\t req->scan_plans[i].iterations)))\n\t\t\treturn -ENOBUFS;\n\t\tnla_nest_end(msg, scan_plan);\n\t}\n\tnla_nest_end(msg, scan_plans);\n\n\tnla_nest_end(msg, nd);\n\n\treturn 0;\n}\n\nstatic int nl80211_get_wowlan(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\tu32 size = NLMSG_DEFAULT_SIZE;\n\n\tif (!rdev->wiphy.wowlan)\n\t\treturn -EOPNOTSUPP;\n\n\tif (rdev->wiphy.wowlan_config && rdev->wiphy.wowlan_config->tcp) {\n\t\t/* adjust size to have room for all the data */\n\t\tsize += rdev->wiphy.wowlan_config->tcp->tokens_size +\n\t\t\trdev->wiphy.wowlan_config->tcp->payload_len +\n\t\t\trdev->wiphy.wowlan_config->tcp->wake_len +\n\t\t\trdev->wiphy.wowlan_config->tcp->wake_len / 8;\n\t}\n\n\tmsg = nlmsg_new(size, GFP_KERNEL);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\thdr = nl80211hdr_put(msg, info->snd_portid, info->snd_seq, 0,\n\t\t\t     NL80211_CMD_GET_WOWLAN);\n\tif (!hdr)\n\t\tgoto nla_put_failure;\n\n\tif (rdev->wiphy.wowlan_config) {\n\t\tstruct nlattr *nl_wowlan;\n\n\t\tnl_wowlan = nla_nest_start_noflag(msg,\n\t\t\t\t\t\t  NL80211_ATTR_WOWLAN_TRIGGERS);\n\t\tif (!nl_wowlan)\n\t\t\tgoto nla_put_failure;\n\n\t\tif ((rdev->wiphy.wowlan_config->any &&\n\t\t     nla_put_flag(msg, NL80211_WOWLAN_TRIG_ANY)) ||\n\t\t    (rdev->wiphy.wowlan_config->disconnect &&\n\t\t     nla_put_flag(msg, NL80211_WOWLAN_TRIG_DISCONNECT)) ||\n\t\t    (rdev->wiphy.wowlan_config->magic_pkt &&\n\t\t     nla_put_flag(msg, NL80211_WOWLAN_TRIG_MAGIC_PKT)) ||\n\t\t    (rdev->wiphy.wowlan_config->gtk_rekey_failure &&\n\t\t     nla_put_flag(msg, NL80211_WOWLAN_TRIG_GTK_REKEY_FAILURE)) ||\n\t\t    (rdev->wiphy.wowlan_config->eap_identity_req &&\n\t\t     nla_put_flag(msg, NL80211_WOWLAN_TRIG_EAP_IDENT_REQUEST)) ||\n\t\t    (rdev->wiphy.wowlan_config->four_way_handshake &&\n\t\t     nla_put_flag(msg, NL80211_WOWLAN_TRIG_4WAY_HANDSHAKE)) ||\n\t\t    (rdev->wiphy.wowlan_config->rfkill_release &&\n\t\t     nla_put_flag(msg, NL80211_WOWLAN_TRIG_RFKILL_RELEASE)))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (nl80211_send_wowlan_patterns(msg, rdev))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (nl80211_send_wowlan_tcp(msg,\n\t\t\t\t\t    rdev->wiphy.wowlan_config->tcp))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (nl80211_send_wowlan_nd(\n\t\t\t    msg,\n\t\t\t    rdev->wiphy.wowlan_config->nd_config))\n\t\t\tgoto nla_put_failure;\n\n\t\tnla_nest_end(msg, nl_wowlan);\n\t}\n\n\tgenlmsg_end(msg, hdr);\n\treturn genlmsg_reply(msg, info);\n\nnla_put_failure:\n\tnlmsg_free(msg);\n\treturn -ENOBUFS;\n}\n\nstatic int nl80211_parse_wowlan_tcp(struct cfg80211_registered_device *rdev,\n\t\t\t\t    struct nlattr *attr,\n\t\t\t\t    struct cfg80211_wowlan *trig)\n{\n\tstruct nlattr *tb[NUM_NL80211_WOWLAN_TCP];\n\tstruct cfg80211_wowlan_tcp *cfg;\n\tstruct nl80211_wowlan_tcp_data_token *tok = NULL;\n\tstruct nl80211_wowlan_tcp_data_seq *seq = NULL;\n\tu32 size;\n\tu32 data_size, wake_size, tokens_size = 0, wake_mask_size;\n\tint err, port;\n\n\tif (!rdev->wiphy.wowlan->tcp)\n\t\treturn -EINVAL;\n\n\terr = nla_parse_nested_deprecated(tb, MAX_NL80211_WOWLAN_TCP, attr,\n\t\t\t\t\t  nl80211_wowlan_tcp_policy, NULL);\n\tif (err)\n\t\treturn err;\n\n\tif (!tb[NL80211_WOWLAN_TCP_SRC_IPV4] ||\n\t    !tb[NL80211_WOWLAN_TCP_DST_IPV4] ||\n\t    !tb[NL80211_WOWLAN_TCP_DST_MAC] ||\n\t    !tb[NL80211_WOWLAN_TCP_DST_PORT] ||\n\t    !tb[NL80211_WOWLAN_TCP_DATA_PAYLOAD] ||\n\t    !tb[NL80211_WOWLAN_TCP_DATA_INTERVAL] ||\n\t    !tb[NL80211_WOWLAN_TCP_WAKE_PAYLOAD] ||\n\t    !tb[NL80211_WOWLAN_TCP_WAKE_MASK])\n\t\treturn -EINVAL;\n\n\tdata_size = nla_len(tb[NL80211_WOWLAN_TCP_DATA_PAYLOAD]);\n\tif (data_size > rdev->wiphy.wowlan->tcp->data_payload_max)\n\t\treturn -EINVAL;\n\n\tif (nla_get_u32(tb[NL80211_WOWLAN_TCP_DATA_INTERVAL]) >\n\t\t\trdev->wiphy.wowlan->tcp->data_interval_max ||\n\t    nla_get_u32(tb[NL80211_WOWLAN_TCP_DATA_INTERVAL]) == 0)\n\t\treturn -EINVAL;\n\n\twake_size = nla_len(tb[NL80211_WOWLAN_TCP_WAKE_PAYLOAD]);\n\tif (wake_size > rdev->wiphy.wowlan->tcp->wake_payload_max)\n\t\treturn -EINVAL;\n\n\twake_mask_size = nla_len(tb[NL80211_WOWLAN_TCP_WAKE_MASK]);\n\tif (wake_mask_size != DIV_ROUND_UP(wake_size, 8))\n\t\treturn -EINVAL;\n\n\tif (tb[NL80211_WOWLAN_TCP_DATA_PAYLOAD_TOKEN]) {\n\t\tu32 tokln = nla_len(tb[NL80211_WOWLAN_TCP_DATA_PAYLOAD_TOKEN]);\n\n\t\ttok = nla_data(tb[NL80211_WOWLAN_TCP_DATA_PAYLOAD_TOKEN]);\n\t\ttokens_size = tokln - sizeof(*tok);\n\n\t\tif (!tok->len || tokens_size % tok->len)\n\t\t\treturn -EINVAL;\n\t\tif (!rdev->wiphy.wowlan->tcp->tok)\n\t\t\treturn -EINVAL;\n\t\tif (tok->len > rdev->wiphy.wowlan->tcp->tok->max_len)\n\t\t\treturn -EINVAL;\n\t\tif (tok->len < rdev->wiphy.wowlan->tcp->tok->min_len)\n\t\t\treturn -EINVAL;\n\t\tif (tokens_size > rdev->wiphy.wowlan->tcp->tok->bufsize)\n\t\t\treturn -EINVAL;\n\t\tif (tok->offset + tok->len > data_size)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tb[NL80211_WOWLAN_TCP_DATA_PAYLOAD_SEQ]) {\n\t\tseq = nla_data(tb[NL80211_WOWLAN_TCP_DATA_PAYLOAD_SEQ]);\n\t\tif (!rdev->wiphy.wowlan->tcp->seq)\n\t\t\treturn -EINVAL;\n\t\tif (seq->len == 0 || seq->len > 4)\n\t\t\treturn -EINVAL;\n\t\tif (seq->len + seq->offset > data_size)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsize = sizeof(*cfg);\n\tsize += data_size;\n\tsize += wake_size + wake_mask_size;\n\tsize += tokens_size;\n\n\tcfg = kzalloc(size, GFP_KERNEL);\n\tif (!cfg)\n\t\treturn -ENOMEM;\n\tcfg->src = nla_get_in_addr(tb[NL80211_WOWLAN_TCP_SRC_IPV4]);\n\tcfg->dst = nla_get_in_addr(tb[NL80211_WOWLAN_TCP_DST_IPV4]);\n\tmemcpy(cfg->dst_mac, nla_data(tb[NL80211_WOWLAN_TCP_DST_MAC]),\n\t       ETH_ALEN);\n\tif (tb[NL80211_WOWLAN_TCP_SRC_PORT])\n\t\tport = nla_get_u16(tb[NL80211_WOWLAN_TCP_SRC_PORT]);\n\telse\n\t\tport = 0;\n#ifdef CONFIG_INET\n\t/* allocate a socket and port for it and use it */\n\terr = __sock_create(wiphy_net(&rdev->wiphy), PF_INET, SOCK_STREAM,\n\t\t\t    IPPROTO_TCP, &cfg->sock, 1);\n\tif (err) {\n\t\tkfree(cfg);\n\t\treturn err;\n\t}\n\tif (inet_csk_get_port(cfg->sock->sk, port)) {\n\t\tsock_release(cfg->sock);\n\t\tkfree(cfg);\n\t\treturn -EADDRINUSE;\n\t}\n\tcfg->src_port = inet_sk(cfg->sock->sk)->inet_num;\n#else\n\tif (!port) {\n\t\tkfree(cfg);\n\t\treturn -EINVAL;\n\t}\n\tcfg->src_port = port;\n#endif\n\n\tcfg->dst_port = nla_get_u16(tb[NL80211_WOWLAN_TCP_DST_PORT]);\n\tcfg->payload_len = data_size;\n\tcfg->payload = (u8 *)cfg + sizeof(*cfg) + tokens_size;\n\tmemcpy((void *)cfg->payload,\n\t       nla_data(tb[NL80211_WOWLAN_TCP_DATA_PAYLOAD]),\n\t       data_size);\n\tif (seq)\n\t\tcfg->payload_seq = *seq;\n\tcfg->data_interval = nla_get_u32(tb[NL80211_WOWLAN_TCP_DATA_INTERVAL]);\n\tcfg->wake_len = wake_size;\n\tcfg->wake_data = (u8 *)cfg + sizeof(*cfg) + tokens_size + data_size;\n\tmemcpy((void *)cfg->wake_data,\n\t       nla_data(tb[NL80211_WOWLAN_TCP_WAKE_PAYLOAD]),\n\t       wake_size);\n\tcfg->wake_mask = (u8 *)cfg + sizeof(*cfg) + tokens_size +\n\t\t\t data_size + wake_size;\n\tmemcpy((void *)cfg->wake_mask,\n\t       nla_data(tb[NL80211_WOWLAN_TCP_WAKE_MASK]),\n\t       wake_mask_size);\n\tif (tok) {\n\t\tcfg->tokens_size = tokens_size;\n\t\tmemcpy(&cfg->payload_tok, tok, sizeof(*tok) + tokens_size);\n\t}\n\n\ttrig->tcp = cfg;\n\n\treturn 0;\n}\n\nstatic int nl80211_parse_wowlan_nd(struct cfg80211_registered_device *rdev,\n\t\t\t\t   const struct wiphy_wowlan_support *wowlan,\n\t\t\t\t   struct nlattr *attr,\n\t\t\t\t   struct cfg80211_wowlan *trig)\n{\n\tstruct nlattr **tb;\n\tint err;\n\n\ttb = kcalloc(NUM_NL80211_ATTR, sizeof(*tb), GFP_KERNEL);\n\tif (!tb)\n\t\treturn -ENOMEM;\n\n\tif (!(wowlan->flags & WIPHY_WOWLAN_NET_DETECT)) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\terr = nla_parse_nested_deprecated(tb, NL80211_ATTR_MAX, attr,\n\t\t\t\t\t  nl80211_policy, NULL);\n\tif (err)\n\t\tgoto out;\n\n\ttrig->nd_config = nl80211_parse_sched_scan(&rdev->wiphy, NULL, tb,\n\t\t\t\t\t\t   wowlan->max_nd_match_sets);\n\terr = PTR_ERR_OR_ZERO(trig->nd_config);\n\tif (err)\n\t\ttrig->nd_config = NULL;\n\nout:\n\tkfree(tb);\n\treturn err;\n}\n\nstatic int nl80211_set_wowlan(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct nlattr *tb[NUM_NL80211_WOWLAN_TRIG];\n\tstruct cfg80211_wowlan new_triggers = {};\n\tstruct cfg80211_wowlan *ntrig;\n\tconst struct wiphy_wowlan_support *wowlan = rdev->wiphy.wowlan;\n\tint err, i;\n\tbool prev_enabled = rdev->wiphy.wowlan_config;\n\tbool regular = false;\n\n\tif (!wowlan)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!info->attrs[NL80211_ATTR_WOWLAN_TRIGGERS]) {\n\t\tcfg80211_rdev_free_wowlan(rdev);\n\t\trdev->wiphy.wowlan_config = NULL;\n\t\tgoto set_wakeup;\n\t}\n\n\terr = nla_parse_nested_deprecated(tb, MAX_NL80211_WOWLAN_TRIG,\n\t\t\t\t\t  info->attrs[NL80211_ATTR_WOWLAN_TRIGGERS],\n\t\t\t\t\t  nl80211_wowlan_policy, info->extack);\n\tif (err)\n\t\treturn err;\n\n\tif (tb[NL80211_WOWLAN_TRIG_ANY]) {\n\t\tif (!(wowlan->flags & WIPHY_WOWLAN_ANY))\n\t\t\treturn -EINVAL;\n\t\tnew_triggers.any = true;\n\t}\n\n\tif (tb[NL80211_WOWLAN_TRIG_DISCONNECT]) {\n\t\tif (!(wowlan->flags & WIPHY_WOWLAN_DISCONNECT))\n\t\t\treturn -EINVAL;\n\t\tnew_triggers.disconnect = true;\n\t\tregular = true;\n\t}\n\n\tif (tb[NL80211_WOWLAN_TRIG_MAGIC_PKT]) {\n\t\tif (!(wowlan->flags & WIPHY_WOWLAN_MAGIC_PKT))\n\t\t\treturn -EINVAL;\n\t\tnew_triggers.magic_pkt = true;\n\t\tregular = true;\n\t}\n\n\tif (tb[NL80211_WOWLAN_TRIG_GTK_REKEY_SUPPORTED])\n\t\treturn -EINVAL;\n\n\tif (tb[NL80211_WOWLAN_TRIG_GTK_REKEY_FAILURE]) {\n\t\tif (!(wowlan->flags & WIPHY_WOWLAN_GTK_REKEY_FAILURE))\n\t\t\treturn -EINVAL;\n\t\tnew_triggers.gtk_rekey_failure = true;\n\t\tregular = true;\n\t}\n\n\tif (tb[NL80211_WOWLAN_TRIG_EAP_IDENT_REQUEST]) {\n\t\tif (!(wowlan->flags & WIPHY_WOWLAN_EAP_IDENTITY_REQ))\n\t\t\treturn -EINVAL;\n\t\tnew_triggers.eap_identity_req = true;\n\t\tregular = true;\n\t}\n\n\tif (tb[NL80211_WOWLAN_TRIG_4WAY_HANDSHAKE]) {\n\t\tif (!(wowlan->flags & WIPHY_WOWLAN_4WAY_HANDSHAKE))\n\t\t\treturn -EINVAL;\n\t\tnew_triggers.four_way_handshake = true;\n\t\tregular = true;\n\t}\n\n\tif (tb[NL80211_WOWLAN_TRIG_RFKILL_RELEASE]) {\n\t\tif (!(wowlan->flags & WIPHY_WOWLAN_RFKILL_RELEASE))\n\t\t\treturn -EINVAL;\n\t\tnew_triggers.rfkill_release = true;\n\t\tregular = true;\n\t}\n\n\tif (tb[NL80211_WOWLAN_TRIG_PKT_PATTERN]) {\n\t\tstruct nlattr *pat;\n\t\tint n_patterns = 0;\n\t\tint rem, pat_len, mask_len, pkt_offset;\n\t\tstruct nlattr *pat_tb[NUM_NL80211_PKTPAT];\n\n\t\tregular = true;\n\n\t\tnla_for_each_nested(pat, tb[NL80211_WOWLAN_TRIG_PKT_PATTERN],\n\t\t\t\t    rem)\n\t\t\tn_patterns++;\n\t\tif (n_patterns > wowlan->n_patterns)\n\t\t\treturn -EINVAL;\n\n\t\tnew_triggers.patterns = kcalloc(n_patterns,\n\t\t\t\t\t\tsizeof(new_triggers.patterns[0]),\n\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!new_triggers.patterns)\n\t\t\treturn -ENOMEM;\n\n\t\tnew_triggers.n_patterns = n_patterns;\n\t\ti = 0;\n\n\t\tnla_for_each_nested(pat, tb[NL80211_WOWLAN_TRIG_PKT_PATTERN],\n\t\t\t\t    rem) {\n\t\t\tu8 *mask_pat;\n\n\t\t\terr = nla_parse_nested_deprecated(pat_tb,\n\t\t\t\t\t\t\t  MAX_NL80211_PKTPAT,\n\t\t\t\t\t\t\t  pat,\n\t\t\t\t\t\t\t  nl80211_packet_pattern_policy,\n\t\t\t\t\t\t\t  info->extack);\n\t\t\tif (err)\n\t\t\t\tgoto error;\n\n\t\t\terr = -EINVAL;\n\t\t\tif (!pat_tb[NL80211_PKTPAT_MASK] ||\n\t\t\t    !pat_tb[NL80211_PKTPAT_PATTERN])\n\t\t\t\tgoto error;\n\t\t\tpat_len = nla_len(pat_tb[NL80211_PKTPAT_PATTERN]);\n\t\t\tmask_len = DIV_ROUND_UP(pat_len, 8);\n\t\t\tif (nla_len(pat_tb[NL80211_PKTPAT_MASK]) != mask_len)\n\t\t\t\tgoto error;\n\t\t\tif (pat_len > wowlan->pattern_max_len ||\n\t\t\t    pat_len < wowlan->pattern_min_len)\n\t\t\t\tgoto error;\n\n\t\t\tif (!pat_tb[NL80211_PKTPAT_OFFSET])\n\t\t\t\tpkt_offset = 0;\n\t\t\telse\n\t\t\t\tpkt_offset = nla_get_u32(\n\t\t\t\t\tpat_tb[NL80211_PKTPAT_OFFSET]);\n\t\t\tif (pkt_offset > wowlan->max_pkt_offset)\n\t\t\t\tgoto error;\n\t\t\tnew_triggers.patterns[i].pkt_offset = pkt_offset;\n\n\t\t\tmask_pat = kmalloc(mask_len + pat_len, GFP_KERNEL);\n\t\t\tif (!mask_pat) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tnew_triggers.patterns[i].mask = mask_pat;\n\t\t\tmemcpy(mask_pat, nla_data(pat_tb[NL80211_PKTPAT_MASK]),\n\t\t\t       mask_len);\n\t\t\tmask_pat += mask_len;\n\t\t\tnew_triggers.patterns[i].pattern = mask_pat;\n\t\t\tnew_triggers.patterns[i].pattern_len = pat_len;\n\t\t\tmemcpy(mask_pat,\n\t\t\t       nla_data(pat_tb[NL80211_PKTPAT_PATTERN]),\n\t\t\t       pat_len);\n\t\t\ti++;\n\t\t}\n\t}\n\n\tif (tb[NL80211_WOWLAN_TRIG_TCP_CONNECTION]) {\n\t\tregular = true;\n\t\terr = nl80211_parse_wowlan_tcp(\n\t\t\trdev, tb[NL80211_WOWLAN_TRIG_TCP_CONNECTION],\n\t\t\t&new_triggers);\n\t\tif (err)\n\t\t\tgoto error;\n\t}\n\n\tif (tb[NL80211_WOWLAN_TRIG_NET_DETECT]) {\n\t\tregular = true;\n\t\terr = nl80211_parse_wowlan_nd(\n\t\t\trdev, wowlan, tb[NL80211_WOWLAN_TRIG_NET_DETECT],\n\t\t\t&new_triggers);\n\t\tif (err)\n\t\t\tgoto error;\n\t}\n\n\t/* The 'any' trigger means the device continues operating more or less\n\t * as in its normal operation mode and wakes up the host on most of the\n\t * normal interrupts (like packet RX, ...)\n\t * It therefore makes little sense to combine with the more constrained\n\t * wakeup trigger modes.\n\t */\n\tif (new_triggers.any && regular) {\n\t\terr = -EINVAL;\n\t\tgoto error;\n\t}\n\n\tntrig = kmemdup(&new_triggers, sizeof(new_triggers), GFP_KERNEL);\n\tif (!ntrig) {\n\t\terr = -ENOMEM;\n\t\tgoto error;\n\t}\n\tcfg80211_rdev_free_wowlan(rdev);\n\trdev->wiphy.wowlan_config = ntrig;\n\n set_wakeup:\n\tif (rdev->ops->set_wakeup &&\n\t    prev_enabled != !!rdev->wiphy.wowlan_config)\n\t\trdev_set_wakeup(rdev, rdev->wiphy.wowlan_config);\n\n\treturn 0;\n error:\n\tfor (i = 0; i < new_triggers.n_patterns; i++)\n\t\tkfree(new_triggers.patterns[i].mask);\n\tkfree(new_triggers.patterns);\n\tif (new_triggers.tcp && new_triggers.tcp->sock)\n\t\tsock_release(new_triggers.tcp->sock);\n\tkfree(new_triggers.tcp);\n\tkfree(new_triggers.nd_config);\n\treturn err;\n}\n#endif\n\nstatic int nl80211_send_coalesce_rules(struct sk_buff *msg,\n\t\t\t\t       struct cfg80211_registered_device *rdev)\n{\n\tstruct nlattr *nl_pats, *nl_pat, *nl_rule, *nl_rules;\n\tint i, j, pat_len;\n\tstruct cfg80211_coalesce_rules *rule;\n\n\tif (!rdev->coalesce->n_rules)\n\t\treturn 0;\n\n\tnl_rules = nla_nest_start_noflag(msg, NL80211_ATTR_COALESCE_RULE);\n\tif (!nl_rules)\n\t\treturn -ENOBUFS;\n\n\tfor (i = 0; i < rdev->coalesce->n_rules; i++) {\n\t\tnl_rule = nla_nest_start_noflag(msg, i + 1);\n\t\tif (!nl_rule)\n\t\t\treturn -ENOBUFS;\n\n\t\trule = &rdev->coalesce->rules[i];\n\t\tif (nla_put_u32(msg, NL80211_ATTR_COALESCE_RULE_DELAY,\n\t\t\t\trule->delay))\n\t\t\treturn -ENOBUFS;\n\n\t\tif (nla_put_u32(msg, NL80211_ATTR_COALESCE_RULE_CONDITION,\n\t\t\t\trule->condition))\n\t\t\treturn -ENOBUFS;\n\n\t\tnl_pats = nla_nest_start_noflag(msg,\n\t\t\t\t\t\tNL80211_ATTR_COALESCE_RULE_PKT_PATTERN);\n\t\tif (!nl_pats)\n\t\t\treturn -ENOBUFS;\n\n\t\tfor (j = 0; j < rule->n_patterns; j++) {\n\t\t\tnl_pat = nla_nest_start_noflag(msg, j + 1);\n\t\t\tif (!nl_pat)\n\t\t\t\treturn -ENOBUFS;\n\t\t\tpat_len = rule->patterns[j].pattern_len;\n\t\t\tif (nla_put(msg, NL80211_PKTPAT_MASK,\n\t\t\t\t    DIV_ROUND_UP(pat_len, 8),\n\t\t\t\t    rule->patterns[j].mask) ||\n\t\t\t    nla_put(msg, NL80211_PKTPAT_PATTERN, pat_len,\n\t\t\t\t    rule->patterns[j].pattern) ||\n\t\t\t    nla_put_u32(msg, NL80211_PKTPAT_OFFSET,\n\t\t\t\t\trule->patterns[j].pkt_offset))\n\t\t\t\treturn -ENOBUFS;\n\t\t\tnla_nest_end(msg, nl_pat);\n\t\t}\n\t\tnla_nest_end(msg, nl_pats);\n\t\tnla_nest_end(msg, nl_rule);\n\t}\n\tnla_nest_end(msg, nl_rules);\n\n\treturn 0;\n}\n\nstatic int nl80211_get_coalesce(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\tif (!rdev->wiphy.coalesce)\n\t\treturn -EOPNOTSUPP;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\thdr = nl80211hdr_put(msg, info->snd_portid, info->snd_seq, 0,\n\t\t\t     NL80211_CMD_GET_COALESCE);\n\tif (!hdr)\n\t\tgoto nla_put_failure;\n\n\tif (rdev->coalesce && nl80211_send_coalesce_rules(msg, rdev))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\treturn genlmsg_reply(msg, info);\n\nnla_put_failure:\n\tnlmsg_free(msg);\n\treturn -ENOBUFS;\n}\n\nvoid cfg80211_rdev_free_coalesce(struct cfg80211_registered_device *rdev)\n{\n\tstruct cfg80211_coalesce *coalesce = rdev->coalesce;\n\tint i, j;\n\tstruct cfg80211_coalesce_rules *rule;\n\n\tif (!coalesce)\n\t\treturn;\n\n\tfor (i = 0; i < coalesce->n_rules; i++) {\n\t\trule = &coalesce->rules[i];\n\t\tfor (j = 0; j < rule->n_patterns; j++)\n\t\t\tkfree(rule->patterns[j].mask);\n\t\tkfree(rule->patterns);\n\t}\n\tkfree(coalesce->rules);\n\tkfree(coalesce);\n\trdev->coalesce = NULL;\n}\n\nstatic int nl80211_parse_coalesce_rule(struct cfg80211_registered_device *rdev,\n\t\t\t\t       struct nlattr *rule,\n\t\t\t\t       struct cfg80211_coalesce_rules *new_rule)\n{\n\tint err, i;\n\tconst struct wiphy_coalesce_support *coalesce = rdev->wiphy.coalesce;\n\tstruct nlattr *tb[NUM_NL80211_ATTR_COALESCE_RULE], *pat;\n\tint rem, pat_len, mask_len, pkt_offset, n_patterns = 0;\n\tstruct nlattr *pat_tb[NUM_NL80211_PKTPAT];\n\n\terr = nla_parse_nested_deprecated(tb, NL80211_ATTR_COALESCE_RULE_MAX,\n\t\t\t\t\t  rule, nl80211_coalesce_policy, NULL);\n\tif (err)\n\t\treturn err;\n\n\tif (tb[NL80211_ATTR_COALESCE_RULE_DELAY])\n\t\tnew_rule->delay =\n\t\t\tnla_get_u32(tb[NL80211_ATTR_COALESCE_RULE_DELAY]);\n\tif (new_rule->delay > coalesce->max_delay)\n\t\treturn -EINVAL;\n\n\tif (tb[NL80211_ATTR_COALESCE_RULE_CONDITION])\n\t\tnew_rule->condition =\n\t\t\tnla_get_u32(tb[NL80211_ATTR_COALESCE_RULE_CONDITION]);\n\n\tif (!tb[NL80211_ATTR_COALESCE_RULE_PKT_PATTERN])\n\t\treturn -EINVAL;\n\n\tnla_for_each_nested(pat, tb[NL80211_ATTR_COALESCE_RULE_PKT_PATTERN],\n\t\t\t    rem)\n\t\tn_patterns++;\n\tif (n_patterns > coalesce->n_patterns)\n\t\treturn -EINVAL;\n\n\tnew_rule->patterns = kcalloc(n_patterns, sizeof(new_rule->patterns[0]),\n\t\t\t\t     GFP_KERNEL);\n\tif (!new_rule->patterns)\n\t\treturn -ENOMEM;\n\n\tnew_rule->n_patterns = n_patterns;\n\ti = 0;\n\n\tnla_for_each_nested(pat, tb[NL80211_ATTR_COALESCE_RULE_PKT_PATTERN],\n\t\t\t    rem) {\n\t\tu8 *mask_pat;\n\n\t\terr = nla_parse_nested_deprecated(pat_tb, MAX_NL80211_PKTPAT,\n\t\t\t\t\t\t  pat,\n\t\t\t\t\t\t  nl80211_packet_pattern_policy,\n\t\t\t\t\t\t  NULL);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (!pat_tb[NL80211_PKTPAT_MASK] ||\n\t\t    !pat_tb[NL80211_PKTPAT_PATTERN])\n\t\t\treturn -EINVAL;\n\t\tpat_len = nla_len(pat_tb[NL80211_PKTPAT_PATTERN]);\n\t\tmask_len = DIV_ROUND_UP(pat_len, 8);\n\t\tif (nla_len(pat_tb[NL80211_PKTPAT_MASK]) != mask_len)\n\t\t\treturn -EINVAL;\n\t\tif (pat_len > coalesce->pattern_max_len ||\n\t\t    pat_len < coalesce->pattern_min_len)\n\t\t\treturn -EINVAL;\n\n\t\tif (!pat_tb[NL80211_PKTPAT_OFFSET])\n\t\t\tpkt_offset = 0;\n\t\telse\n\t\t\tpkt_offset = nla_get_u32(pat_tb[NL80211_PKTPAT_OFFSET]);\n\t\tif (pkt_offset > coalesce->max_pkt_offset)\n\t\t\treturn -EINVAL;\n\t\tnew_rule->patterns[i].pkt_offset = pkt_offset;\n\n\t\tmask_pat = kmalloc(mask_len + pat_len, GFP_KERNEL);\n\t\tif (!mask_pat)\n\t\t\treturn -ENOMEM;\n\n\t\tnew_rule->patterns[i].mask = mask_pat;\n\t\tmemcpy(mask_pat, nla_data(pat_tb[NL80211_PKTPAT_MASK]),\n\t\t       mask_len);\n\n\t\tmask_pat += mask_len;\n\t\tnew_rule->patterns[i].pattern = mask_pat;\n\t\tnew_rule->patterns[i].pattern_len = pat_len;\n\t\tmemcpy(mask_pat, nla_data(pat_tb[NL80211_PKTPAT_PATTERN]),\n\t\t       pat_len);\n\t\ti++;\n\t}\n\n\treturn 0;\n}\n\nstatic int nl80211_set_coalesce(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tconst struct wiphy_coalesce_support *coalesce = rdev->wiphy.coalesce;\n\tstruct cfg80211_coalesce new_coalesce = {};\n\tstruct cfg80211_coalesce *n_coalesce;\n\tint err, rem_rule, n_rules = 0, i, j;\n\tstruct nlattr *rule;\n\tstruct cfg80211_coalesce_rules *tmp_rule;\n\n\tif (!rdev->wiphy.coalesce || !rdev->ops->set_coalesce)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!info->attrs[NL80211_ATTR_COALESCE_RULE]) {\n\t\tcfg80211_rdev_free_coalesce(rdev);\n\t\trdev_set_coalesce(rdev, NULL);\n\t\treturn 0;\n\t}\n\n\tnla_for_each_nested(rule, info->attrs[NL80211_ATTR_COALESCE_RULE],\n\t\t\t    rem_rule)\n\t\tn_rules++;\n\tif (n_rules > coalesce->n_rules)\n\t\treturn -EINVAL;\n\n\tnew_coalesce.rules = kcalloc(n_rules, sizeof(new_coalesce.rules[0]),\n\t\t\t\t     GFP_KERNEL);\n\tif (!new_coalesce.rules)\n\t\treturn -ENOMEM;\n\n\tnew_coalesce.n_rules = n_rules;\n\ti = 0;\n\n\tnla_for_each_nested(rule, info->attrs[NL80211_ATTR_COALESCE_RULE],\n\t\t\t    rem_rule) {\n\t\terr = nl80211_parse_coalesce_rule(rdev, rule,\n\t\t\t\t\t\t  &new_coalesce.rules[i]);\n\t\tif (err)\n\t\t\tgoto error;\n\n\t\ti++;\n\t}\n\n\terr = rdev_set_coalesce(rdev, &new_coalesce);\n\tif (err)\n\t\tgoto error;\n\n\tn_coalesce = kmemdup(&new_coalesce, sizeof(new_coalesce), GFP_KERNEL);\n\tif (!n_coalesce) {\n\t\terr = -ENOMEM;\n\t\tgoto error;\n\t}\n\tcfg80211_rdev_free_coalesce(rdev);\n\trdev->coalesce = n_coalesce;\n\n\treturn 0;\nerror:\n\tfor (i = 0; i < new_coalesce.n_rules; i++) {\n\t\ttmp_rule = &new_coalesce.rules[i];\n\t\tfor (j = 0; j < tmp_rule->n_patterns; j++)\n\t\t\tkfree(tmp_rule->patterns[j].mask);\n\t\tkfree(tmp_rule->patterns);\n\t}\n\tkfree(new_coalesce.rules);\n\n\treturn err;\n}\n\nstatic int nl80211_set_rekey_data(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct nlattr *tb[NUM_NL80211_REKEY_DATA];\n\tstruct cfg80211_gtk_rekey_data rekey_data = {};\n\tint err;\n\n\tif (!info->attrs[NL80211_ATTR_REKEY_DATA])\n\t\treturn -EINVAL;\n\n\terr = nla_parse_nested_deprecated(tb, MAX_NL80211_REKEY_DATA,\n\t\t\t\t\t  info->attrs[NL80211_ATTR_REKEY_DATA],\n\t\t\t\t\t  nl80211_rekey_policy, info->extack);\n\tif (err)\n\t\treturn err;\n\n\tif (!tb[NL80211_REKEY_DATA_REPLAY_CTR] || !tb[NL80211_REKEY_DATA_KEK] ||\n\t    !tb[NL80211_REKEY_DATA_KCK])\n\t\treturn -EINVAL;\n\tif (nla_len(tb[NL80211_REKEY_DATA_KEK]) != NL80211_KEK_LEN &&\n\t    !(rdev->wiphy.flags & WIPHY_FLAG_SUPPORTS_EXT_KEK_KCK &&\n\t      nla_len(tb[NL80211_REKEY_DATA_KEK]) == NL80211_KEK_EXT_LEN))\n\t\treturn -ERANGE;\n\tif (nla_len(tb[NL80211_REKEY_DATA_KCK]) != NL80211_KCK_LEN &&\n\t    !(rdev->wiphy.flags & WIPHY_FLAG_SUPPORTS_EXT_KEK_KCK &&\n\t      nla_len(tb[NL80211_REKEY_DATA_KEK]) == NL80211_KCK_EXT_LEN))\n\t\treturn -ERANGE;\n\n\trekey_data.kek = nla_data(tb[NL80211_REKEY_DATA_KEK]);\n\trekey_data.kck = nla_data(tb[NL80211_REKEY_DATA_KCK]);\n\trekey_data.replay_ctr = nla_data(tb[NL80211_REKEY_DATA_REPLAY_CTR]);\n\trekey_data.kek_len = nla_len(tb[NL80211_REKEY_DATA_KEK]);\n\trekey_data.kck_len = nla_len(tb[NL80211_REKEY_DATA_KCK]);\n\tif (tb[NL80211_REKEY_DATA_AKM])\n\t\trekey_data.akm = nla_get_u32(tb[NL80211_REKEY_DATA_AKM]);\n\n\twdev_lock(wdev);\n\tif (!wdev->current_bss) {\n\t\terr = -ENOTCONN;\n\t\tgoto out;\n\t}\n\n\tif (!rdev->ops->set_rekey_data) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\terr = rdev_set_rekey_data(rdev, dev, &rekey_data);\n out:\n\twdev_unlock(wdev);\n\treturn err;\n}\n\nstatic int nl80211_register_unexpected_frame(struct sk_buff *skb,\n\t\t\t\t\t     struct genl_info *info)\n{\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\n\tif (wdev->iftype != NL80211_IFTYPE_AP &&\n\t    wdev->iftype != NL80211_IFTYPE_P2P_GO)\n\t\treturn -EINVAL;\n\n\tif (wdev->ap_unexpected_nlportid)\n\t\treturn -EBUSY;\n\n\twdev->ap_unexpected_nlportid = info->snd_portid;\n\treturn 0;\n}\n\nstatic int nl80211_probe_client(struct sk_buff *skb,\n\t\t\t\tstruct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\tconst u8 *addr;\n\tu64 cookie;\n\tint err;\n\n\tif (wdev->iftype != NL80211_IFTYPE_AP &&\n\t    wdev->iftype != NL80211_IFTYPE_P2P_GO)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!info->attrs[NL80211_ATTR_MAC])\n\t\treturn -EINVAL;\n\n\tif (!rdev->ops->probe_client)\n\t\treturn -EOPNOTSUPP;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\thdr = nl80211hdr_put(msg, info->snd_portid, info->snd_seq, 0,\n\t\t\t     NL80211_CMD_PROBE_CLIENT);\n\tif (!hdr) {\n\t\terr = -ENOBUFS;\n\t\tgoto free_msg;\n\t}\n\n\taddr = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\n\terr = rdev_probe_client(rdev, dev, addr, &cookie);\n\tif (err)\n\t\tgoto free_msg;\n\n\tif (nla_put_u64_64bit(msg, NL80211_ATTR_COOKIE, cookie,\n\t\t\t      NL80211_ATTR_PAD))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\n\treturn genlmsg_reply(msg, info);\n\n nla_put_failure:\n\terr = -ENOBUFS;\n free_msg:\n\tnlmsg_free(msg);\n\treturn err;\n}\n\nstatic int nl80211_register_beacons(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct cfg80211_beacon_registration *reg, *nreg;\n\tint rv;\n\n\tif (!(rdev->wiphy.flags & WIPHY_FLAG_REPORTS_OBSS))\n\t\treturn -EOPNOTSUPP;\n\n\tnreg = kzalloc(sizeof(*nreg), GFP_KERNEL);\n\tif (!nreg)\n\t\treturn -ENOMEM;\n\n\t/* First, check if already registered. */\n\tspin_lock_bh(&rdev->beacon_registrations_lock);\n\tlist_for_each_entry(reg, &rdev->beacon_registrations, list) {\n\t\tif (reg->nlportid == info->snd_portid) {\n\t\t\trv = -EALREADY;\n\t\t\tgoto out_err;\n\t\t}\n\t}\n\t/* Add it to the list */\n\tnreg->nlportid = info->snd_portid;\n\tlist_add(&nreg->list, &rdev->beacon_registrations);\n\n\tspin_unlock_bh(&rdev->beacon_registrations_lock);\n\n\treturn 0;\nout_err:\n\tspin_unlock_bh(&rdev->beacon_registrations_lock);\n\tkfree(nreg);\n\treturn rv;\n}\n\nstatic int nl80211_start_p2p_device(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev = info->user_ptr[1];\n\tint err;\n\n\tif (!rdev->ops->start_p2p_device)\n\t\treturn -EOPNOTSUPP;\n\n\tif (wdev->iftype != NL80211_IFTYPE_P2P_DEVICE)\n\t\treturn -EOPNOTSUPP;\n\n\tif (wdev_running(wdev))\n\t\treturn 0;\n\n\tif (rfkill_blocked(rdev->rfkill))\n\t\treturn -ERFKILL;\n\n\terr = rdev_start_p2p_device(rdev, wdev);\n\tif (err)\n\t\treturn err;\n\n\twdev->is_running = true;\n\trdev->opencount++;\n\n\treturn 0;\n}\n\nstatic int nl80211_stop_p2p_device(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev = info->user_ptr[1];\n\n\tif (wdev->iftype != NL80211_IFTYPE_P2P_DEVICE)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!rdev->ops->stop_p2p_device)\n\t\treturn -EOPNOTSUPP;\n\n\tcfg80211_stop_p2p_device(rdev, wdev);\n\n\treturn 0;\n}\n\nstatic int nl80211_start_nan(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev = info->user_ptr[1];\n\tstruct cfg80211_nan_conf conf = {};\n\tint err;\n\n\tif (wdev->iftype != NL80211_IFTYPE_NAN)\n\t\treturn -EOPNOTSUPP;\n\n\tif (wdev_running(wdev))\n\t\treturn -EEXIST;\n\n\tif (rfkill_blocked(rdev->rfkill))\n\t\treturn -ERFKILL;\n\n\tif (!info->attrs[NL80211_ATTR_NAN_MASTER_PREF])\n\t\treturn -EINVAL;\n\n\tconf.master_pref =\n\t\tnla_get_u8(info->attrs[NL80211_ATTR_NAN_MASTER_PREF]);\n\n\tif (info->attrs[NL80211_ATTR_BANDS]) {\n\t\tu32 bands = nla_get_u32(info->attrs[NL80211_ATTR_BANDS]);\n\n\t\tif (bands & ~(u32)wdev->wiphy->nan_supported_bands)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (bands && !(bands & BIT(NL80211_BAND_2GHZ)))\n\t\t\treturn -EINVAL;\n\n\t\tconf.bands = bands;\n\t}\n\n\terr = rdev_start_nan(rdev, wdev, &conf);\n\tif (err)\n\t\treturn err;\n\n\twdev->is_running = true;\n\trdev->opencount++;\n\n\treturn 0;\n}\n\nstatic int nl80211_stop_nan(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev = info->user_ptr[1];\n\n\tif (wdev->iftype != NL80211_IFTYPE_NAN)\n\t\treturn -EOPNOTSUPP;\n\n\tcfg80211_stop_nan(rdev, wdev);\n\n\treturn 0;\n}\n\nstatic int validate_nan_filter(struct nlattr *filter_attr)\n{\n\tstruct nlattr *attr;\n\tint len = 0, n_entries = 0, rem;\n\n\tnla_for_each_nested(attr, filter_attr, rem) {\n\t\tlen += nla_len(attr);\n\t\tn_entries++;\n\t}\n\n\tif (len >= U8_MAX)\n\t\treturn -EINVAL;\n\n\treturn n_entries;\n}\n\nstatic int handle_nan_filter(struct nlattr *attr_filter,\n\t\t\t     struct cfg80211_nan_func *func,\n\t\t\t     bool tx)\n{\n\tstruct nlattr *attr;\n\tint n_entries, rem, i;\n\tstruct cfg80211_nan_func_filter *filter;\n\n\tn_entries = validate_nan_filter(attr_filter);\n\tif (n_entries < 0)\n\t\treturn n_entries;\n\n\tBUILD_BUG_ON(sizeof(*func->rx_filters) != sizeof(*func->tx_filters));\n\n\tfilter = kcalloc(n_entries, sizeof(*func->rx_filters), GFP_KERNEL);\n\tif (!filter)\n\t\treturn -ENOMEM;\n\n\ti = 0;\n\tnla_for_each_nested(attr, attr_filter, rem) {\n\t\tfilter[i].filter = nla_memdup(attr, GFP_KERNEL);\n\t\tfilter[i].len = nla_len(attr);\n\t\ti++;\n\t}\n\tif (tx) {\n\t\tfunc->num_tx_filters = n_entries;\n\t\tfunc->tx_filters = filter;\n\t} else {\n\t\tfunc->num_rx_filters = n_entries;\n\t\tfunc->rx_filters = filter;\n\t}\n\n\treturn 0;\n}\n\nstatic int nl80211_nan_add_func(struct sk_buff *skb,\n\t\t\t\tstruct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev = info->user_ptr[1];\n\tstruct nlattr *tb[NUM_NL80211_NAN_FUNC_ATTR], *func_attr;\n\tstruct cfg80211_nan_func *func;\n\tstruct sk_buff *msg = NULL;\n\tvoid *hdr = NULL;\n\tint err = 0;\n\n\tif (wdev->iftype != NL80211_IFTYPE_NAN)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!wdev_running(wdev))\n\t\treturn -ENOTCONN;\n\n\tif (!info->attrs[NL80211_ATTR_NAN_FUNC])\n\t\treturn -EINVAL;\n\n\terr = nla_parse_nested_deprecated(tb, NL80211_NAN_FUNC_ATTR_MAX,\n\t\t\t\t\t  info->attrs[NL80211_ATTR_NAN_FUNC],\n\t\t\t\t\t  nl80211_nan_func_policy,\n\t\t\t\t\t  info->extack);\n\tif (err)\n\t\treturn err;\n\n\tfunc = kzalloc(sizeof(*func), GFP_KERNEL);\n\tif (!func)\n\t\treturn -ENOMEM;\n\n\tfunc->cookie = cfg80211_assign_cookie(rdev);\n\n\tif (!tb[NL80211_NAN_FUNC_TYPE]) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\n\tfunc->type = nla_get_u8(tb[NL80211_NAN_FUNC_TYPE]);\n\n\tif (!tb[NL80211_NAN_FUNC_SERVICE_ID]) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmemcpy(func->service_id, nla_data(tb[NL80211_NAN_FUNC_SERVICE_ID]),\n\t       sizeof(func->service_id));\n\n\tfunc->close_range =\n\t\tnla_get_flag(tb[NL80211_NAN_FUNC_CLOSE_RANGE]);\n\n\tif (tb[NL80211_NAN_FUNC_SERVICE_INFO]) {\n\t\tfunc->serv_spec_info_len =\n\t\t\tnla_len(tb[NL80211_NAN_FUNC_SERVICE_INFO]);\n\t\tfunc->serv_spec_info =\n\t\t\tkmemdup(nla_data(tb[NL80211_NAN_FUNC_SERVICE_INFO]),\n\t\t\t\tfunc->serv_spec_info_len,\n\t\t\t\tGFP_KERNEL);\n\t\tif (!func->serv_spec_info) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (tb[NL80211_NAN_FUNC_TTL])\n\t\tfunc->ttl = nla_get_u32(tb[NL80211_NAN_FUNC_TTL]);\n\n\tswitch (func->type) {\n\tcase NL80211_NAN_FUNC_PUBLISH:\n\t\tif (!tb[NL80211_NAN_FUNC_PUBLISH_TYPE]) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tfunc->publish_type =\n\t\t\tnla_get_u8(tb[NL80211_NAN_FUNC_PUBLISH_TYPE]);\n\t\tfunc->publish_bcast =\n\t\t\tnla_get_flag(tb[NL80211_NAN_FUNC_PUBLISH_BCAST]);\n\n\t\tif ((!(func->publish_type & NL80211_NAN_SOLICITED_PUBLISH)) &&\n\t\t\tfunc->publish_bcast) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tbreak;\n\tcase NL80211_NAN_FUNC_SUBSCRIBE:\n\t\tfunc->subscribe_active =\n\t\t\tnla_get_flag(tb[NL80211_NAN_FUNC_SUBSCRIBE_ACTIVE]);\n\t\tbreak;\n\tcase NL80211_NAN_FUNC_FOLLOW_UP:\n\t\tif (!tb[NL80211_NAN_FUNC_FOLLOW_UP_ID] ||\n\t\t    !tb[NL80211_NAN_FUNC_FOLLOW_UP_REQ_ID] ||\n\t\t    !tb[NL80211_NAN_FUNC_FOLLOW_UP_DEST]) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tfunc->followup_id =\n\t\t\tnla_get_u8(tb[NL80211_NAN_FUNC_FOLLOW_UP_ID]);\n\t\tfunc->followup_reqid =\n\t\t\tnla_get_u8(tb[NL80211_NAN_FUNC_FOLLOW_UP_REQ_ID]);\n\t\tmemcpy(func->followup_dest.addr,\n\t\t       nla_data(tb[NL80211_NAN_FUNC_FOLLOW_UP_DEST]),\n\t\t       sizeof(func->followup_dest.addr));\n\t\tif (func->ttl) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (tb[NL80211_NAN_FUNC_SRF]) {\n\t\tstruct nlattr *srf_tb[NUM_NL80211_NAN_SRF_ATTR];\n\n\t\terr = nla_parse_nested_deprecated(srf_tb,\n\t\t\t\t\t\t  NL80211_NAN_SRF_ATTR_MAX,\n\t\t\t\t\t\t  tb[NL80211_NAN_FUNC_SRF],\n\t\t\t\t\t\t  nl80211_nan_srf_policy,\n\t\t\t\t\t\t  info->extack);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tfunc->srf_include =\n\t\t\tnla_get_flag(srf_tb[NL80211_NAN_SRF_INCLUDE]);\n\n\t\tif (srf_tb[NL80211_NAN_SRF_BF]) {\n\t\t\tif (srf_tb[NL80211_NAN_SRF_MAC_ADDRS] ||\n\t\t\t    !srf_tb[NL80211_NAN_SRF_BF_IDX]) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tfunc->srf_bf_len =\n\t\t\t\tnla_len(srf_tb[NL80211_NAN_SRF_BF]);\n\t\t\tfunc->srf_bf =\n\t\t\t\tkmemdup(nla_data(srf_tb[NL80211_NAN_SRF_BF]),\n\t\t\t\t\tfunc->srf_bf_len, GFP_KERNEL);\n\t\t\tif (!func->srf_bf) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tfunc->srf_bf_idx =\n\t\t\t\tnla_get_u8(srf_tb[NL80211_NAN_SRF_BF_IDX]);\n\t\t} else {\n\t\t\tstruct nlattr *attr, *mac_attr =\n\t\t\t\tsrf_tb[NL80211_NAN_SRF_MAC_ADDRS];\n\t\t\tint n_entries, rem, i = 0;\n\n\t\t\tif (!mac_attr) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tn_entries = validate_acl_mac_addrs(mac_attr);\n\t\t\tif (n_entries <= 0) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tfunc->srf_num_macs = n_entries;\n\t\t\tfunc->srf_macs =\n\t\t\t\tkcalloc(n_entries, sizeof(*func->srf_macs),\n\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!func->srf_macs) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tnla_for_each_nested(attr, mac_attr, rem)\n\t\t\t\tmemcpy(func->srf_macs[i++].addr, nla_data(attr),\n\t\t\t\t       sizeof(*func->srf_macs));\n\t\t}\n\t}\n\n\tif (tb[NL80211_NAN_FUNC_TX_MATCH_FILTER]) {\n\t\terr = handle_nan_filter(tb[NL80211_NAN_FUNC_TX_MATCH_FILTER],\n\t\t\t\t\tfunc, true);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tif (tb[NL80211_NAN_FUNC_RX_MATCH_FILTER]) {\n\t\terr = handle_nan_filter(tb[NL80211_NAN_FUNC_RX_MATCH_FILTER],\n\t\t\t\t\tfunc, false);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\thdr = nl80211hdr_put(msg, info->snd_portid, info->snd_seq, 0,\n\t\t\t     NL80211_CMD_ADD_NAN_FUNCTION);\n\t/* This can't really happen - we just allocated 4KB */\n\tif (WARN_ON(!hdr)) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\terr = rdev_add_nan_func(rdev, wdev, func);\nout:\n\tif (err < 0) {\n\t\tcfg80211_free_nan_func(func);\n\t\tnlmsg_free(msg);\n\t\treturn err;\n\t}\n\n\t/* propagate the instance id and cookie to userspace  */\n\tif (nla_put_u64_64bit(msg, NL80211_ATTR_COOKIE, func->cookie,\n\t\t\t      NL80211_ATTR_PAD))\n\t\tgoto nla_put_failure;\n\n\tfunc_attr = nla_nest_start_noflag(msg, NL80211_ATTR_NAN_FUNC);\n\tif (!func_attr)\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u8(msg, NL80211_NAN_FUNC_INSTANCE_ID,\n\t\t       func->instance_id))\n\t\tgoto nla_put_failure;\n\n\tnla_nest_end(msg, func_attr);\n\n\tgenlmsg_end(msg, hdr);\n\treturn genlmsg_reply(msg, info);\n\nnla_put_failure:\n\tnlmsg_free(msg);\n\treturn -ENOBUFS;\n}\n\nstatic int nl80211_nan_del_func(struct sk_buff *skb,\n\t\t\t       struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev = info->user_ptr[1];\n\tu64 cookie;\n\n\tif (wdev->iftype != NL80211_IFTYPE_NAN)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!wdev_running(wdev))\n\t\treturn -ENOTCONN;\n\n\tif (!info->attrs[NL80211_ATTR_COOKIE])\n\t\treturn -EINVAL;\n\n\tcookie = nla_get_u64(info->attrs[NL80211_ATTR_COOKIE]);\n\n\trdev_del_nan_func(rdev, wdev, cookie);\n\n\treturn 0;\n}\n\nstatic int nl80211_nan_change_config(struct sk_buff *skb,\n\t\t\t\t     struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev = info->user_ptr[1];\n\tstruct cfg80211_nan_conf conf = {};\n\tu32 changed = 0;\n\n\tif (wdev->iftype != NL80211_IFTYPE_NAN)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!wdev_running(wdev))\n\t\treturn -ENOTCONN;\n\n\tif (info->attrs[NL80211_ATTR_NAN_MASTER_PREF]) {\n\t\tconf.master_pref =\n\t\t\tnla_get_u8(info->attrs[NL80211_ATTR_NAN_MASTER_PREF]);\n\t\tif (conf.master_pref <= 1 || conf.master_pref == 255)\n\t\t\treturn -EINVAL;\n\n\t\tchanged |= CFG80211_NAN_CONF_CHANGED_PREF;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_BANDS]) {\n\t\tu32 bands = nla_get_u32(info->attrs[NL80211_ATTR_BANDS]);\n\n\t\tif (bands & ~(u32)wdev->wiphy->nan_supported_bands)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (bands && !(bands & BIT(NL80211_BAND_2GHZ)))\n\t\t\treturn -EINVAL;\n\n\t\tconf.bands = bands;\n\t\tchanged |= CFG80211_NAN_CONF_CHANGED_BANDS;\n\t}\n\n\tif (!changed)\n\t\treturn -EINVAL;\n\n\treturn rdev_nan_change_conf(rdev, wdev, &conf, changed);\n}\n\nvoid cfg80211_nan_match(struct wireless_dev *wdev,\n\t\t\tstruct cfg80211_nan_match_params *match, gfp_t gfp)\n{\n\tstruct wiphy *wiphy = wdev->wiphy;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\tstruct nlattr *match_attr, *local_func_attr, *peer_func_attr;\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\tif (WARN_ON(!match->inst_id || !match->peer_inst_id || !match->addr))\n\t\treturn;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, gfp);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_NAN_MATCH);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    (wdev->netdev && nla_put_u32(msg, NL80211_ATTR_IFINDEX,\n\t\t\t\t\t wdev->netdev->ifindex)) ||\n\t    nla_put_u64_64bit(msg, NL80211_ATTR_WDEV, wdev_id(wdev),\n\t\t\t      NL80211_ATTR_PAD))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u64_64bit(msg, NL80211_ATTR_COOKIE, match->cookie,\n\t\t\t      NL80211_ATTR_PAD) ||\n\t    nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, match->addr))\n\t\tgoto nla_put_failure;\n\n\tmatch_attr = nla_nest_start_noflag(msg, NL80211_ATTR_NAN_MATCH);\n\tif (!match_attr)\n\t\tgoto nla_put_failure;\n\n\tlocal_func_attr = nla_nest_start_noflag(msg,\n\t\t\t\t\t\tNL80211_NAN_MATCH_FUNC_LOCAL);\n\tif (!local_func_attr)\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u8(msg, NL80211_NAN_FUNC_INSTANCE_ID, match->inst_id))\n\t\tgoto nla_put_failure;\n\n\tnla_nest_end(msg, local_func_attr);\n\n\tpeer_func_attr = nla_nest_start_noflag(msg,\n\t\t\t\t\t       NL80211_NAN_MATCH_FUNC_PEER);\n\tif (!peer_func_attr)\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u8(msg, NL80211_NAN_FUNC_TYPE, match->type) ||\n\t    nla_put_u8(msg, NL80211_NAN_FUNC_INSTANCE_ID, match->peer_inst_id))\n\t\tgoto nla_put_failure;\n\n\tif (match->info && match->info_len &&\n\t    nla_put(msg, NL80211_NAN_FUNC_SERVICE_INFO, match->info_len,\n\t\t    match->info))\n\t\tgoto nla_put_failure;\n\n\tnla_nest_end(msg, peer_func_attr);\n\tnla_nest_end(msg, match_attr);\n\tgenlmsg_end(msg, hdr);\n\n\tif (!wdev->owner_nlportid)\n\t\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy),\n\t\t\t\t\tmsg, 0, NL80211_MCGRP_NAN, gfp);\n\telse\n\t\tgenlmsg_unicast(wiphy_net(&rdev->wiphy), msg,\n\t\t\t\twdev->owner_nlportid);\n\n\treturn;\n\nnla_put_failure:\n\tnlmsg_free(msg);\n}\nEXPORT_SYMBOL(cfg80211_nan_match);\n\nvoid cfg80211_nan_func_terminated(struct wireless_dev *wdev,\n\t\t\t\t  u8 inst_id,\n\t\t\t\t  enum nl80211_nan_func_term_reason reason,\n\t\t\t\t  u64 cookie, gfp_t gfp)\n{\n\tstruct wiphy *wiphy = wdev->wiphy;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\tstruct sk_buff *msg;\n\tstruct nlattr *func_attr;\n\tvoid *hdr;\n\n\tif (WARN_ON(!inst_id))\n\t\treturn;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, gfp);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_DEL_NAN_FUNCTION);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    (wdev->netdev && nla_put_u32(msg, NL80211_ATTR_IFINDEX,\n\t\t\t\t\t wdev->netdev->ifindex)) ||\n\t    nla_put_u64_64bit(msg, NL80211_ATTR_WDEV, wdev_id(wdev),\n\t\t\t      NL80211_ATTR_PAD))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u64_64bit(msg, NL80211_ATTR_COOKIE, cookie,\n\t\t\t      NL80211_ATTR_PAD))\n\t\tgoto nla_put_failure;\n\n\tfunc_attr = nla_nest_start_noflag(msg, NL80211_ATTR_NAN_FUNC);\n\tif (!func_attr)\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u8(msg, NL80211_NAN_FUNC_INSTANCE_ID, inst_id) ||\n\t    nla_put_u8(msg, NL80211_NAN_FUNC_TERM_REASON, reason))\n\t\tgoto nla_put_failure;\n\n\tnla_nest_end(msg, func_attr);\n\tgenlmsg_end(msg, hdr);\n\n\tif (!wdev->owner_nlportid)\n\t\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy),\n\t\t\t\t\tmsg, 0, NL80211_MCGRP_NAN, gfp);\n\telse\n\t\tgenlmsg_unicast(wiphy_net(&rdev->wiphy), msg,\n\t\t\t\twdev->owner_nlportid);\n\n\treturn;\n\nnla_put_failure:\n\tnlmsg_free(msg);\n}\nEXPORT_SYMBOL(cfg80211_nan_func_terminated);\n\nstatic int nl80211_get_protocol_features(struct sk_buff *skb,\n\t\t\t\t\t struct genl_info *info)\n{\n\tvoid *hdr;\n\tstruct sk_buff *msg;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\thdr = nl80211hdr_put(msg, info->snd_portid, info->snd_seq, 0,\n\t\t\t     NL80211_CMD_GET_PROTOCOL_FEATURES);\n\tif (!hdr)\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_PROTOCOL_FEATURES,\n\t\t\tNL80211_PROTOCOL_FEATURE_SPLIT_WIPHY_DUMP))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\treturn genlmsg_reply(msg, info);\n\n nla_put_failure:\n\tkfree_skb(msg);\n\treturn -ENOBUFS;\n}\n\nstatic int nl80211_update_ft_ies(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct cfg80211_update_ft_ies_params ft_params;\n\tstruct net_device *dev = info->user_ptr[1];\n\n\tif (!rdev->ops->update_ft_ies)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!info->attrs[NL80211_ATTR_MDID] ||\n\t    !info->attrs[NL80211_ATTR_IE])\n\t\treturn -EINVAL;\n\n\tmemset(&ft_params, 0, sizeof(ft_params));\n\tft_params.md = nla_get_u16(info->attrs[NL80211_ATTR_MDID]);\n\tft_params.ie = nla_data(info->attrs[NL80211_ATTR_IE]);\n\tft_params.ie_len = nla_len(info->attrs[NL80211_ATTR_IE]);\n\n\treturn rdev_update_ft_ies(rdev, dev, &ft_params);\n}\n\nstatic int nl80211_crit_protocol_start(struct sk_buff *skb,\n\t\t\t\t       struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev = info->user_ptr[1];\n\tenum nl80211_crit_proto_id proto = NL80211_CRIT_PROTO_UNSPEC;\n\tu16 duration;\n\tint ret;\n\n\tif (!rdev->ops->crit_proto_start)\n\t\treturn -EOPNOTSUPP;\n\n\tif (WARN_ON(!rdev->ops->crit_proto_stop))\n\t\treturn -EINVAL;\n\n\tif (rdev->crit_proto_nlportid)\n\t\treturn -EBUSY;\n\n\t/* determine protocol if provided */\n\tif (info->attrs[NL80211_ATTR_CRIT_PROT_ID])\n\t\tproto = nla_get_u16(info->attrs[NL80211_ATTR_CRIT_PROT_ID]);\n\n\tif (proto >= NUM_NL80211_CRIT_PROTO)\n\t\treturn -EINVAL;\n\n\t/* timeout must be provided */\n\tif (!info->attrs[NL80211_ATTR_MAX_CRIT_PROT_DURATION])\n\t\treturn -EINVAL;\n\n\tduration =\n\t\tnla_get_u16(info->attrs[NL80211_ATTR_MAX_CRIT_PROT_DURATION]);\n\n\tret = rdev_crit_proto_start(rdev, wdev, proto, duration);\n\tif (!ret)\n\t\trdev->crit_proto_nlportid = info->snd_portid;\n\n\treturn ret;\n}\n\nstatic int nl80211_crit_protocol_stop(struct sk_buff *skb,\n\t\t\t\t      struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev = info->user_ptr[1];\n\n\tif (!rdev->ops->crit_proto_stop)\n\t\treturn -EOPNOTSUPP;\n\n\tif (rdev->crit_proto_nlportid) {\n\t\trdev->crit_proto_nlportid = 0;\n\t\trdev_crit_proto_stop(rdev, wdev);\n\t}\n\treturn 0;\n}\n\nstatic int nl80211_vendor_check_policy(const struct wiphy_vendor_command *vcmd,\n\t\t\t\t       struct nlattr *attr,\n\t\t\t\t       struct netlink_ext_ack *extack)\n{\n\tif (vcmd->policy == VENDOR_CMD_RAW_DATA) {\n\t\tif (attr->nla_type & NLA_F_NESTED) {\n\t\t\tNL_SET_ERR_MSG_ATTR(extack, attr,\n\t\t\t\t\t    \"unexpected nested data\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\tif (!(attr->nla_type & NLA_F_NESTED)) {\n\t\tNL_SET_ERR_MSG_ATTR(extack, attr, \"expected nested data\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn nla_validate_nested(attr, vcmd->maxattr, vcmd->policy, extack);\n}\n\nstatic int nl80211_vendor_cmd(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct wireless_dev *wdev =\n\t\t__cfg80211_wdev_from_attrs(rdev, genl_info_net(info),\n\t\t\t\t\t   info->attrs);\n\tint i, err;\n\tu32 vid, subcmd;\n\n\tif (!rdev->wiphy.vendor_commands)\n\t\treturn -EOPNOTSUPP;\n\n\tif (IS_ERR(wdev)) {\n\t\terr = PTR_ERR(wdev);\n\t\tif (err != -EINVAL)\n\t\t\treturn err;\n\t\twdev = NULL;\n\t} else if (wdev->wiphy != &rdev->wiphy) {\n\t\treturn -EINVAL;\n\t}\n\n\tif (!info->attrs[NL80211_ATTR_VENDOR_ID] ||\n\t    !info->attrs[NL80211_ATTR_VENDOR_SUBCMD])\n\t\treturn -EINVAL;\n\n\tvid = nla_get_u32(info->attrs[NL80211_ATTR_VENDOR_ID]);\n\tsubcmd = nla_get_u32(info->attrs[NL80211_ATTR_VENDOR_SUBCMD]);\n\tfor (i = 0; i < rdev->wiphy.n_vendor_commands; i++) {\n\t\tconst struct wiphy_vendor_command *vcmd;\n\t\tvoid *data = NULL;\n\t\tint len = 0;\n\n\t\tvcmd = &rdev->wiphy.vendor_commands[i];\n\n\t\tif (vcmd->info.vendor_id != vid || vcmd->info.subcmd != subcmd)\n\t\t\tcontinue;\n\n\t\tif (vcmd->flags & (WIPHY_VENDOR_CMD_NEED_WDEV |\n\t\t\t\t   WIPHY_VENDOR_CMD_NEED_NETDEV)) {\n\t\t\tif (!wdev)\n\t\t\t\treturn -EINVAL;\n\t\t\tif (vcmd->flags & WIPHY_VENDOR_CMD_NEED_NETDEV &&\n\t\t\t    !wdev->netdev)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tif (vcmd->flags & WIPHY_VENDOR_CMD_NEED_RUNNING) {\n\t\t\t\tif (!wdev_running(wdev))\n\t\t\t\t\treturn -ENETDOWN;\n\t\t\t}\n\t\t} else {\n\t\t\twdev = NULL;\n\t\t}\n\n\t\tif (!vcmd->doit)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (info->attrs[NL80211_ATTR_VENDOR_DATA]) {\n\t\t\tdata = nla_data(info->attrs[NL80211_ATTR_VENDOR_DATA]);\n\t\t\tlen = nla_len(info->attrs[NL80211_ATTR_VENDOR_DATA]);\n\n\t\t\terr = nl80211_vendor_check_policy(vcmd,\n\t\t\t\t\tinfo->attrs[NL80211_ATTR_VENDOR_DATA],\n\t\t\t\t\tinfo->extack);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\trdev->cur_cmd_info = info;\n\t\terr = vcmd->doit(&rdev->wiphy, wdev, data, len);\n\t\trdev->cur_cmd_info = NULL;\n\t\treturn err;\n\t}\n\n\treturn -EOPNOTSUPP;\n}\n\nstatic int nl80211_prepare_vendor_dump(struct sk_buff *skb,\n\t\t\t\t       struct netlink_callback *cb,\n\t\t\t\t       struct cfg80211_registered_device **rdev,\n\t\t\t\t       struct wireless_dev **wdev)\n{\n\tstruct nlattr **attrbuf;\n\tu32 vid, subcmd;\n\tunsigned int i;\n\tint vcmd_idx = -1;\n\tint err;\n\tvoid *data = NULL;\n\tunsigned int data_len = 0;\n\n\tif (cb->args[0]) {\n\t\t/* subtract the 1 again here */\n\t\tstruct wiphy *wiphy = wiphy_idx_to_wiphy(cb->args[0] - 1);\n\t\tstruct wireless_dev *tmp;\n\n\t\tif (!wiphy)\n\t\t\treturn -ENODEV;\n\t\t*rdev = wiphy_to_rdev(wiphy);\n\t\t*wdev = NULL;\n\n\t\tif (cb->args[1]) {\n\t\t\tlist_for_each_entry(tmp, &wiphy->wdev_list, list) {\n\t\t\t\tif (tmp->identifier == cb->args[1] - 1) {\n\t\t\t\t\t*wdev = tmp;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t/* keep rtnl locked in successful case */\n\t\treturn 0;\n\t}\n\n\tattrbuf = kcalloc(NUM_NL80211_ATTR, sizeof(*attrbuf), GFP_KERNEL);\n\tif (!attrbuf)\n\t\treturn -ENOMEM;\n\n\terr = nlmsg_parse_deprecated(cb->nlh,\n\t\t\t\t     GENL_HDRLEN + nl80211_fam.hdrsize,\n\t\t\t\t     attrbuf, nl80211_fam.maxattr,\n\t\t\t\t     nl80211_policy, NULL);\n\tif (err)\n\t\tgoto out;\n\n\tif (!attrbuf[NL80211_ATTR_VENDOR_ID] ||\n\t    !attrbuf[NL80211_ATTR_VENDOR_SUBCMD]) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t*wdev = __cfg80211_wdev_from_attrs(NULL, sock_net(skb->sk), attrbuf);\n\tif (IS_ERR(*wdev))\n\t\t*wdev = NULL;\n\n\t*rdev = __cfg80211_rdev_from_attrs(sock_net(skb->sk), attrbuf);\n\tif (IS_ERR(*rdev)) {\n\t\terr = PTR_ERR(*rdev);\n\t\tgoto out;\n\t}\n\n\tvid = nla_get_u32(attrbuf[NL80211_ATTR_VENDOR_ID]);\n\tsubcmd = nla_get_u32(attrbuf[NL80211_ATTR_VENDOR_SUBCMD]);\n\n\tfor (i = 0; i < (*rdev)->wiphy.n_vendor_commands; i++) {\n\t\tconst struct wiphy_vendor_command *vcmd;\n\n\t\tvcmd = &(*rdev)->wiphy.vendor_commands[i];\n\n\t\tif (vcmd->info.vendor_id != vid || vcmd->info.subcmd != subcmd)\n\t\t\tcontinue;\n\n\t\tif (!vcmd->dumpit) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto out;\n\t\t}\n\n\t\tvcmd_idx = i;\n\t\tbreak;\n\t}\n\n\tif (vcmd_idx < 0) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (attrbuf[NL80211_ATTR_VENDOR_DATA]) {\n\t\tdata = nla_data(attrbuf[NL80211_ATTR_VENDOR_DATA]);\n\t\tdata_len = nla_len(attrbuf[NL80211_ATTR_VENDOR_DATA]);\n\n\t\terr = nl80211_vendor_check_policy(\n\t\t\t\t&(*rdev)->wiphy.vendor_commands[vcmd_idx],\n\t\t\t\tattrbuf[NL80211_ATTR_VENDOR_DATA],\n\t\t\t\tcb->extack);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\t/* 0 is the first index - add 1 to parse only once */\n\tcb->args[0] = (*rdev)->wiphy_idx + 1;\n\t/* add 1 to know if it was NULL */\n\tcb->args[1] = *wdev ? (*wdev)->identifier + 1 : 0;\n\tcb->args[2] = vcmd_idx;\n\tcb->args[3] = (unsigned long)data;\n\tcb->args[4] = data_len;\n\n\t/* keep rtnl locked in successful case */\n\terr = 0;\nout:\n\tkfree(attrbuf);\n\treturn err;\n}\n\nstatic int nl80211_vendor_cmd_dump(struct sk_buff *skb,\n\t\t\t\t   struct netlink_callback *cb)\n{\n\tstruct cfg80211_registered_device *rdev;\n\tstruct wireless_dev *wdev;\n\tunsigned int vcmd_idx;\n\tconst struct wiphy_vendor_command *vcmd;\n\tvoid *data;\n\tint data_len;\n\tint err;\n\tstruct nlattr *vendor_data;\n\n\trtnl_lock();\n\terr = nl80211_prepare_vendor_dump(skb, cb, &rdev, &wdev);\n\tif (err)\n\t\tgoto out;\n\n\tvcmd_idx = cb->args[2];\n\tdata = (void *)cb->args[3];\n\tdata_len = cb->args[4];\n\tvcmd = &rdev->wiphy.vendor_commands[vcmd_idx];\n\n\tif (vcmd->flags & (WIPHY_VENDOR_CMD_NEED_WDEV |\n\t\t\t   WIPHY_VENDOR_CMD_NEED_NETDEV)) {\n\t\tif (!wdev) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tif (vcmd->flags & WIPHY_VENDOR_CMD_NEED_NETDEV &&\n\t\t    !wdev->netdev) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (vcmd->flags & WIPHY_VENDOR_CMD_NEED_RUNNING) {\n\t\t\tif (!wdev_running(wdev)) {\n\t\t\t\terr = -ENETDOWN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\twhile (1) {\n\t\tvoid *hdr = nl80211hdr_put(skb, NETLINK_CB(cb->skb).portid,\n\t\t\t\t\t   cb->nlh->nlmsg_seq, NLM_F_MULTI,\n\t\t\t\t\t   NL80211_CMD_VENDOR);\n\t\tif (!hdr)\n\t\t\tbreak;\n\n\t\tif (nla_put_u32(skb, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t\t    (wdev && nla_put_u64_64bit(skb, NL80211_ATTR_WDEV,\n\t\t\t\t\t       wdev_id(wdev),\n\t\t\t\t\t       NL80211_ATTR_PAD))) {\n\t\t\tgenlmsg_cancel(skb, hdr);\n\t\t\tbreak;\n\t\t}\n\n\t\tvendor_data = nla_nest_start_noflag(skb,\n\t\t\t\t\t\t    NL80211_ATTR_VENDOR_DATA);\n\t\tif (!vendor_data) {\n\t\t\tgenlmsg_cancel(skb, hdr);\n\t\t\tbreak;\n\t\t}\n\n\t\terr = vcmd->dumpit(&rdev->wiphy, wdev, skb, data, data_len,\n\t\t\t\t   (unsigned long *)&cb->args[5]);\n\t\tnla_nest_end(skb, vendor_data);\n\n\t\tif (err == -ENOBUFS || err == -ENOENT) {\n\t\t\tgenlmsg_cancel(skb, hdr);\n\t\t\tbreak;\n\t\t} else if (err <= 0) {\n\t\t\tgenlmsg_cancel(skb, hdr);\n\t\t\tgoto out;\n\t\t}\n\n\t\tgenlmsg_end(skb, hdr);\n\t}\n\n\terr = skb->len;\n out:\n\trtnl_unlock();\n\treturn err;\n}\n\nstruct sk_buff *__cfg80211_alloc_reply_skb(struct wiphy *wiphy,\n\t\t\t\t\t   enum nl80211_commands cmd,\n\t\t\t\t\t   enum nl80211_attrs attr,\n\t\t\t\t\t   int approxlen)\n{\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\n\tif (WARN_ON(!rdev->cur_cmd_info))\n\t\treturn NULL;\n\n\treturn __cfg80211_alloc_vendor_skb(rdev, NULL, approxlen,\n\t\t\t\t\t   rdev->cur_cmd_info->snd_portid,\n\t\t\t\t\t   rdev->cur_cmd_info->snd_seq,\n\t\t\t\t\t   cmd, attr, NULL, GFP_KERNEL);\n}\nEXPORT_SYMBOL(__cfg80211_alloc_reply_skb);\n\nint cfg80211_vendor_cmd_reply(struct sk_buff *skb)\n{\n\tstruct cfg80211_registered_device *rdev = ((void **)skb->cb)[0];\n\tvoid *hdr = ((void **)skb->cb)[1];\n\tstruct nlattr *data = ((void **)skb->cb)[2];\n\n\t/* clear CB data for netlink core to own from now on */\n\tmemset(skb->cb, 0, sizeof(skb->cb));\n\n\tif (WARN_ON(!rdev->cur_cmd_info)) {\n\t\tkfree_skb(skb);\n\t\treturn -EINVAL;\n\t}\n\n\tnla_nest_end(skb, data);\n\tgenlmsg_end(skb, hdr);\n\treturn genlmsg_reply(skb, rdev->cur_cmd_info);\n}\nEXPORT_SYMBOL_GPL(cfg80211_vendor_cmd_reply);\n\nunsigned int cfg80211_vendor_cmd_get_sender(struct wiphy *wiphy)\n{\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\n\tif (WARN_ON(!rdev->cur_cmd_info))\n\t\treturn 0;\n\n\treturn rdev->cur_cmd_info->snd_portid;\n}\nEXPORT_SYMBOL_GPL(cfg80211_vendor_cmd_get_sender);\n\nstatic int nl80211_set_qos_map(struct sk_buff *skb,\n\t\t\t       struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct cfg80211_qos_map *qos_map = NULL;\n\tstruct net_device *dev = info->user_ptr[1];\n\tu8 *pos, len, num_des, des_len, des;\n\tint ret;\n\n\tif (!rdev->ops->set_qos_map)\n\t\treturn -EOPNOTSUPP;\n\n\tif (info->attrs[NL80211_ATTR_QOS_MAP]) {\n\t\tpos = nla_data(info->attrs[NL80211_ATTR_QOS_MAP]);\n\t\tlen = nla_len(info->attrs[NL80211_ATTR_QOS_MAP]);\n\n\t\tif (len % 2)\n\t\t\treturn -EINVAL;\n\n\t\tqos_map = kzalloc(sizeof(struct cfg80211_qos_map), GFP_KERNEL);\n\t\tif (!qos_map)\n\t\t\treturn -ENOMEM;\n\n\t\tnum_des = (len - IEEE80211_QOS_MAP_LEN_MIN) >> 1;\n\t\tif (num_des) {\n\t\t\tdes_len = num_des *\n\t\t\t\tsizeof(struct cfg80211_dscp_exception);\n\t\t\tmemcpy(qos_map->dscp_exception, pos, des_len);\n\t\t\tqos_map->num_des = num_des;\n\t\t\tfor (des = 0; des < num_des; des++) {\n\t\t\t\tif (qos_map->dscp_exception[des].up > 7) {\n\t\t\t\t\tkfree(qos_map);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t}\n\t\t\tpos += des_len;\n\t\t}\n\t\tmemcpy(qos_map->up, pos, IEEE80211_QOS_MAP_LEN_MIN);\n\t}\n\n\twdev_lock(dev->ieee80211_ptr);\n\tret = nl80211_key_allowed(dev->ieee80211_ptr);\n\tif (!ret)\n\t\tret = rdev_set_qos_map(rdev, dev, qos_map);\n\twdev_unlock(dev->ieee80211_ptr);\n\n\tkfree(qos_map);\n\treturn ret;\n}\n\nstatic int nl80211_add_tx_ts(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tconst u8 *peer;\n\tu8 tsid, up;\n\tu16 admitted_time = 0;\n\tint err;\n\n\tif (!(rdev->wiphy.features & NL80211_FEATURE_SUPPORTS_WMM_ADMISSION))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!info->attrs[NL80211_ATTR_TSID] || !info->attrs[NL80211_ATTR_MAC] ||\n\t    !info->attrs[NL80211_ATTR_USER_PRIO])\n\t\treturn -EINVAL;\n\n\ttsid = nla_get_u8(info->attrs[NL80211_ATTR_TSID]);\n\tup = nla_get_u8(info->attrs[NL80211_ATTR_USER_PRIO]);\n\n\t/* WMM uses TIDs 0-7 even for TSPEC */\n\tif (tsid >= IEEE80211_FIRST_TSPEC_TSID) {\n\t\t/* TODO: handle 802.11 TSPEC/admission control\n\t\t * need more attributes for that (e.g. BA session requirement);\n\t\t * change the WMM adminssion test above to allow both then\n\t\t */\n\t\treturn -EINVAL;\n\t}\n\n\tpeer = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\n\tif (info->attrs[NL80211_ATTR_ADMITTED_TIME]) {\n\t\tadmitted_time =\n\t\t\tnla_get_u16(info->attrs[NL80211_ATTR_ADMITTED_TIME]);\n\t\tif (!admitted_time)\n\t\t\treturn -EINVAL;\n\t}\n\n\twdev_lock(wdev);\n\tswitch (wdev->iftype) {\n\tcase NL80211_IFTYPE_STATION:\n\tcase NL80211_IFTYPE_P2P_CLIENT:\n\t\tif (wdev->current_bss)\n\t\t\tbreak;\n\t\terr = -ENOTCONN;\n\t\tgoto out;\n\tdefault:\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\terr = rdev_add_tx_ts(rdev, dev, tsid, peer, up, admitted_time);\n\n out:\n\twdev_unlock(wdev);\n\treturn err;\n}\n\nstatic int nl80211_del_tx_ts(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tconst u8 *peer;\n\tu8 tsid;\n\tint err;\n\n\tif (!info->attrs[NL80211_ATTR_TSID] || !info->attrs[NL80211_ATTR_MAC])\n\t\treturn -EINVAL;\n\n\ttsid = nla_get_u8(info->attrs[NL80211_ATTR_TSID]);\n\tpeer = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\n\twdev_lock(wdev);\n\terr = rdev_del_tx_ts(rdev, dev, tsid, peer);\n\twdev_unlock(wdev);\n\n\treturn err;\n}\n\nstatic int nl80211_tdls_channel_switch(struct sk_buff *skb,\n\t\t\t\t       struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct cfg80211_chan_def chandef = {};\n\tconst u8 *addr;\n\tu8 oper_class;\n\tint err;\n\n\tif (!rdev->ops->tdls_channel_switch ||\n\t    !(rdev->wiphy.features & NL80211_FEATURE_TDLS_CHANNEL_SWITCH))\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (dev->ieee80211_ptr->iftype) {\n\tcase NL80211_IFTYPE_STATION:\n\tcase NL80211_IFTYPE_P2P_CLIENT:\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (!info->attrs[NL80211_ATTR_MAC] ||\n\t    !info->attrs[NL80211_ATTR_OPER_CLASS])\n\t\treturn -EINVAL;\n\n\terr = nl80211_parse_chandef(rdev, info, &chandef);\n\tif (err)\n\t\treturn err;\n\n\t/*\n\t * Don't allow wide channels on the 2.4Ghz band, as per IEEE802.11-2012\n\t * section 10.22.6.2.1. Disallow 5/10Mhz channels as well for now, the\n\t * specification is not defined for them.\n\t */\n\tif (chandef.chan->band == NL80211_BAND_2GHZ &&\n\t    chandef.width != NL80211_CHAN_WIDTH_20_NOHT &&\n\t    chandef.width != NL80211_CHAN_WIDTH_20)\n\t\treturn -EINVAL;\n\n\t/* we will be active on the TDLS link */\n\tif (!cfg80211_reg_can_beacon_relax(&rdev->wiphy, &chandef,\n\t\t\t\t\t   wdev->iftype))\n\t\treturn -EINVAL;\n\n\t/* don't allow switching to DFS channels */\n\tif (cfg80211_chandef_dfs_required(wdev->wiphy, &chandef, wdev->iftype))\n\t\treturn -EINVAL;\n\n\taddr = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\toper_class = nla_get_u8(info->attrs[NL80211_ATTR_OPER_CLASS]);\n\n\twdev_lock(wdev);\n\terr = rdev_tdls_channel_switch(rdev, dev, addr, oper_class, &chandef);\n\twdev_unlock(wdev);\n\n\treturn err;\n}\n\nstatic int nl80211_tdls_cancel_channel_switch(struct sk_buff *skb,\n\t\t\t\t\t      struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tconst u8 *addr;\n\n\tif (!rdev->ops->tdls_channel_switch ||\n\t    !rdev->ops->tdls_cancel_channel_switch ||\n\t    !(rdev->wiphy.features & NL80211_FEATURE_TDLS_CHANNEL_SWITCH))\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (dev->ieee80211_ptr->iftype) {\n\tcase NL80211_IFTYPE_STATION:\n\tcase NL80211_IFTYPE_P2P_CLIENT:\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (!info->attrs[NL80211_ATTR_MAC])\n\t\treturn -EINVAL;\n\n\taddr = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\n\twdev_lock(wdev);\n\trdev_tdls_cancel_channel_switch(rdev, dev, addr);\n\twdev_unlock(wdev);\n\n\treturn 0;\n}\n\nstatic int nl80211_set_multicast_to_unicast(struct sk_buff *skb,\n\t\t\t\t\t    struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tconst struct nlattr *nla;\n\tbool enabled;\n\n\tif (!rdev->ops->set_multicast_to_unicast)\n\t\treturn -EOPNOTSUPP;\n\n\tif (wdev->iftype != NL80211_IFTYPE_AP &&\n\t    wdev->iftype != NL80211_IFTYPE_P2P_GO)\n\t\treturn -EOPNOTSUPP;\n\n\tnla = info->attrs[NL80211_ATTR_MULTICAST_TO_UNICAST_ENABLED];\n\tenabled = nla_get_flag(nla);\n\n\treturn rdev_set_multicast_to_unicast(rdev, dev, enabled);\n}\n\nstatic int nl80211_set_pmk(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct cfg80211_pmk_conf pmk_conf = {};\n\tint ret;\n\n\tif (wdev->iftype != NL80211_IFTYPE_STATION &&\n\t    wdev->iftype != NL80211_IFTYPE_P2P_CLIENT)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t     NL80211_EXT_FEATURE_4WAY_HANDSHAKE_STA_1X))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!info->attrs[NL80211_ATTR_MAC] || !info->attrs[NL80211_ATTR_PMK])\n\t\treturn -EINVAL;\n\n\twdev_lock(wdev);\n\tif (!wdev->current_bss) {\n\t\tret = -ENOTCONN;\n\t\tgoto out;\n\t}\n\n\tpmk_conf.aa = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\tif (memcmp(pmk_conf.aa, wdev->current_bss->pub.bssid, ETH_ALEN)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tpmk_conf.pmk = nla_data(info->attrs[NL80211_ATTR_PMK]);\n\tpmk_conf.pmk_len = nla_len(info->attrs[NL80211_ATTR_PMK]);\n\tif (pmk_conf.pmk_len != WLAN_PMK_LEN &&\n\t    pmk_conf.pmk_len != WLAN_PMK_LEN_SUITE_B_192) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (info->attrs[NL80211_ATTR_PMKR0_NAME])\n\t\tpmk_conf.pmk_r0_name =\n\t\t\tnla_data(info->attrs[NL80211_ATTR_PMKR0_NAME]);\n\n\tret = rdev_set_pmk(rdev, dev, &pmk_conf);\nout:\n\twdev_unlock(wdev);\n\treturn ret;\n}\n\nstatic int nl80211_del_pmk(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tconst u8 *aa;\n\tint ret;\n\n\tif (wdev->iftype != NL80211_IFTYPE_STATION &&\n\t    wdev->iftype != NL80211_IFTYPE_P2P_CLIENT)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t     NL80211_EXT_FEATURE_4WAY_HANDSHAKE_STA_1X))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!info->attrs[NL80211_ATTR_MAC])\n\t\treturn -EINVAL;\n\n\twdev_lock(wdev);\n\taa = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\tret = rdev_del_pmk(rdev, dev, aa);\n\twdev_unlock(wdev);\n\n\treturn ret;\n}\n\nstatic int nl80211_external_auth(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct cfg80211_external_auth_params params;\n\n\tif (!rdev->ops->external_auth)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!info->attrs[NL80211_ATTR_SSID] &&\n\t    dev->ieee80211_ptr->iftype != NL80211_IFTYPE_AP &&\n\t    dev->ieee80211_ptr->iftype != NL80211_IFTYPE_P2P_GO)\n\t\treturn -EINVAL;\n\n\tif (!info->attrs[NL80211_ATTR_BSSID])\n\t\treturn -EINVAL;\n\n\tif (!info->attrs[NL80211_ATTR_STATUS_CODE])\n\t\treturn -EINVAL;\n\n\tmemset(&params, 0, sizeof(params));\n\n\tif (info->attrs[NL80211_ATTR_SSID]) {\n\t\tparams.ssid.ssid_len = nla_len(info->attrs[NL80211_ATTR_SSID]);\n\t\tif (params.ssid.ssid_len == 0)\n\t\t\treturn -EINVAL;\n\t\tmemcpy(params.ssid.ssid,\n\t\t       nla_data(info->attrs[NL80211_ATTR_SSID]),\n\t\t       params.ssid.ssid_len);\n\t}\n\n\tmemcpy(params.bssid, nla_data(info->attrs[NL80211_ATTR_BSSID]),\n\t       ETH_ALEN);\n\n\tparams.status = nla_get_u16(info->attrs[NL80211_ATTR_STATUS_CODE]);\n\n\tif (info->attrs[NL80211_ATTR_PMKID])\n\t\tparams.pmkid = nla_data(info->attrs[NL80211_ATTR_PMKID]);\n\n\treturn rdev_external_auth(rdev, dev, &params);\n}\n\nstatic int nl80211_tx_control_port(struct sk_buff *skb, struct genl_info *info)\n{\n\tbool dont_wait_for_ack = info->attrs[NL80211_ATTR_DONT_WAIT_FOR_ACK];\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tconst u8 *buf;\n\tsize_t len;\n\tu8 *dest;\n\tu16 proto;\n\tbool noencrypt;\n\tu64 cookie = 0;\n\tint err;\n\n\tif (!wiphy_ext_feature_isset(&rdev->wiphy,\n\t\t\t\t     NL80211_EXT_FEATURE_CONTROL_PORT_OVER_NL80211))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!rdev->ops->tx_control_port)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!info->attrs[NL80211_ATTR_FRAME] ||\n\t    !info->attrs[NL80211_ATTR_MAC] ||\n\t    !info->attrs[NL80211_ATTR_CONTROL_PORT_ETHERTYPE]) {\n\t\tGENL_SET_ERR_MSG(info, \"Frame, MAC or ethertype missing\");\n\t\treturn -EINVAL;\n\t}\n\n\twdev_lock(wdev);\n\n\tswitch (wdev->iftype) {\n\tcase NL80211_IFTYPE_AP:\n\tcase NL80211_IFTYPE_P2P_GO:\n\tcase NL80211_IFTYPE_MESH_POINT:\n\t\tbreak;\n\tcase NL80211_IFTYPE_ADHOC:\n\tcase NL80211_IFTYPE_STATION:\n\tcase NL80211_IFTYPE_P2P_CLIENT:\n\t\tif (wdev->current_bss)\n\t\t\tbreak;\n\t\terr = -ENOTCONN;\n\t\tgoto out;\n\tdefault:\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\twdev_unlock(wdev);\n\n\tbuf = nla_data(info->attrs[NL80211_ATTR_FRAME]);\n\tlen = nla_len(info->attrs[NL80211_ATTR_FRAME]);\n\tdest = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\tproto = nla_get_u16(info->attrs[NL80211_ATTR_CONTROL_PORT_ETHERTYPE]);\n\tnoencrypt =\n\t\tnla_get_flag(info->attrs[NL80211_ATTR_CONTROL_PORT_NO_ENCRYPT]);\n\n\terr = rdev_tx_control_port(rdev, dev, buf, len,\n\t\t\t\t   dest, cpu_to_be16(proto), noencrypt,\n\t\t\t\t   dont_wait_for_ack ? NULL : &cookie);\n\tif (!err && !dont_wait_for_ack)\n\t\tnl_set_extack_cookie_u64(info->extack, cookie);\n\treturn err;\n out:\n\twdev_unlock(wdev);\n\treturn err;\n}\n\nstatic int nl80211_get_ftm_responder_stats(struct sk_buff *skb,\n\t\t\t\t\t   struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct cfg80211_ftm_responder_stats ftm_stats = {};\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\tstruct nlattr *ftm_stats_attr;\n\tint err;\n\n\tif (wdev->iftype != NL80211_IFTYPE_AP || !wdev->beacon_interval)\n\t\treturn -EOPNOTSUPP;\n\n\terr = rdev_get_ftm_responder_stats(rdev, dev, &ftm_stats);\n\tif (err)\n\t\treturn err;\n\n\tif (!ftm_stats.filled)\n\t\treturn -ENODATA;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\thdr = nl80211hdr_put(msg, info->snd_portid, info->snd_seq, 0,\n\t\t\t     NL80211_CMD_GET_FTM_RESPONDER_STATS);\n\tif (!hdr)\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_IFINDEX, dev->ifindex))\n\t\tgoto nla_put_failure;\n\n\tftm_stats_attr = nla_nest_start_noflag(msg,\n\t\t\t\t\t       NL80211_ATTR_FTM_RESPONDER_STATS);\n\tif (!ftm_stats_attr)\n\t\tgoto nla_put_failure;\n\n#define SET_FTM(field, name, type)\t\t\t\t\t \\\n\tdo { if ((ftm_stats.filled & BIT(NL80211_FTM_STATS_ ## name)) && \\\n\t    nla_put_ ## type(msg, NL80211_FTM_STATS_ ## name,\t\t \\\n\t\t\t     ftm_stats.field))\t\t\t\t \\\n\t\tgoto nla_put_failure; } while (0)\n#define SET_FTM_U64(field, name)\t\t\t\t\t \\\n\tdo { if ((ftm_stats.filled & BIT(NL80211_FTM_STATS_ ## name)) && \\\n\t    nla_put_u64_64bit(msg, NL80211_FTM_STATS_ ## name,\t\t \\\n\t\t\t      ftm_stats.field, NL80211_FTM_STATS_PAD))\t \\\n\t\tgoto nla_put_failure; } while (0)\n\n\tSET_FTM(success_num, SUCCESS_NUM, u32);\n\tSET_FTM(partial_num, PARTIAL_NUM, u32);\n\tSET_FTM(failed_num, FAILED_NUM, u32);\n\tSET_FTM(asap_num, ASAP_NUM, u32);\n\tSET_FTM(non_asap_num, NON_ASAP_NUM, u32);\n\tSET_FTM_U64(total_duration_ms, TOTAL_DURATION_MSEC);\n\tSET_FTM(unknown_triggers_num, UNKNOWN_TRIGGERS_NUM, u32);\n\tSET_FTM(reschedule_requests_num, RESCHEDULE_REQUESTS_NUM, u32);\n\tSET_FTM(out_of_window_triggers_num, OUT_OF_WINDOW_TRIGGERS_NUM, u32);\n#undef SET_FTM\n\n\tnla_nest_end(msg, ftm_stats_attr);\n\n\tgenlmsg_end(msg, hdr);\n\treturn genlmsg_reply(msg, info);\n\nnla_put_failure:\n\tnlmsg_free(msg);\n\treturn -ENOBUFS;\n}\n\nstatic int nl80211_update_owe_info(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct cfg80211_update_owe_info owe_info;\n\tstruct net_device *dev = info->user_ptr[1];\n\n\tif (!rdev->ops->update_owe_info)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!info->attrs[NL80211_ATTR_STATUS_CODE] ||\n\t    !info->attrs[NL80211_ATTR_MAC])\n\t\treturn -EINVAL;\n\n\tmemset(&owe_info, 0, sizeof(owe_info));\n\towe_info.status = nla_get_u16(info->attrs[NL80211_ATTR_STATUS_CODE]);\n\tnla_memcpy(owe_info.peer, info->attrs[NL80211_ATTR_MAC], ETH_ALEN);\n\n\tif (info->attrs[NL80211_ATTR_IE]) {\n\t\towe_info.ie = nla_data(info->attrs[NL80211_ATTR_IE]);\n\t\towe_info.ie_len = nla_len(info->attrs[NL80211_ATTR_IE]);\n\t}\n\n\treturn rdev_update_owe_info(rdev, dev, &owe_info);\n}\n\nstatic int nl80211_probe_mesh_link(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct station_info sinfo = {};\n\tconst u8 *buf;\n\tsize_t len;\n\tu8 *dest;\n\tint err;\n\n\tif (!rdev->ops->probe_mesh_link || !rdev->ops->get_station)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!info->attrs[NL80211_ATTR_MAC] ||\n\t    !info->attrs[NL80211_ATTR_FRAME]) {\n\t\tGENL_SET_ERR_MSG(info, \"Frame or MAC missing\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (wdev->iftype != NL80211_IFTYPE_MESH_POINT)\n\t\treturn -EOPNOTSUPP;\n\n\tdest = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\tbuf = nla_data(info->attrs[NL80211_ATTR_FRAME]);\n\tlen = nla_len(info->attrs[NL80211_ATTR_FRAME]);\n\n\tif (len < sizeof(struct ethhdr))\n\t\treturn -EINVAL;\n\n\tif (!ether_addr_equal(buf, dest) || is_multicast_ether_addr(buf) ||\n\t    !ether_addr_equal(buf + ETH_ALEN, dev->dev_addr))\n\t\treturn -EINVAL;\n\n\terr = rdev_get_station(rdev, dev, dest, &sinfo);\n\tif (err)\n\t\treturn err;\n\n\tcfg80211_sinfo_release_content(&sinfo);\n\n\treturn rdev_probe_mesh_link(rdev, dev, dest, buf, len);\n}\n\nstatic int parse_tid_conf(struct cfg80211_registered_device *rdev,\n\t\t\t  struct nlattr *attrs[], struct net_device *dev,\n\t\t\t  struct cfg80211_tid_cfg *tid_conf,\n\t\t\t  struct genl_info *info, const u8 *peer)\n{\n\tstruct netlink_ext_ack *extack = info->extack;\n\tu64 mask;\n\tint err;\n\n\tif (!attrs[NL80211_TID_CONFIG_ATTR_TIDS])\n\t\treturn -EINVAL;\n\n\ttid_conf->config_override =\n\t\t\tnla_get_flag(attrs[NL80211_TID_CONFIG_ATTR_OVERRIDE]);\n\ttid_conf->tids = nla_get_u16(attrs[NL80211_TID_CONFIG_ATTR_TIDS]);\n\n\tif (tid_conf->config_override) {\n\t\tif (rdev->ops->reset_tid_config) {\n\t\t\terr = rdev_reset_tid_config(rdev, dev, peer,\n\t\t\t\t\t\t    tid_conf->tids);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (attrs[NL80211_TID_CONFIG_ATTR_NOACK]) {\n\t\ttid_conf->mask |= BIT(NL80211_TID_CONFIG_ATTR_NOACK);\n\t\ttid_conf->noack =\n\t\t\tnla_get_u8(attrs[NL80211_TID_CONFIG_ATTR_NOACK]);\n\t}\n\n\tif (attrs[NL80211_TID_CONFIG_ATTR_RETRY_SHORT]) {\n\t\ttid_conf->mask |= BIT(NL80211_TID_CONFIG_ATTR_RETRY_SHORT);\n\t\ttid_conf->retry_short =\n\t\t\tnla_get_u8(attrs[NL80211_TID_CONFIG_ATTR_RETRY_SHORT]);\n\n\t\tif (tid_conf->retry_short > rdev->wiphy.max_data_retry_count)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (attrs[NL80211_TID_CONFIG_ATTR_RETRY_LONG]) {\n\t\ttid_conf->mask |= BIT(NL80211_TID_CONFIG_ATTR_RETRY_LONG);\n\t\ttid_conf->retry_long =\n\t\t\tnla_get_u8(attrs[NL80211_TID_CONFIG_ATTR_RETRY_LONG]);\n\n\t\tif (tid_conf->retry_long > rdev->wiphy.max_data_retry_count)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (attrs[NL80211_TID_CONFIG_ATTR_AMPDU_CTRL]) {\n\t\ttid_conf->mask |= BIT(NL80211_TID_CONFIG_ATTR_AMPDU_CTRL);\n\t\ttid_conf->ampdu =\n\t\t\tnla_get_u8(attrs[NL80211_TID_CONFIG_ATTR_AMPDU_CTRL]);\n\t}\n\n\tif (attrs[NL80211_TID_CONFIG_ATTR_RTSCTS_CTRL]) {\n\t\ttid_conf->mask |= BIT(NL80211_TID_CONFIG_ATTR_RTSCTS_CTRL);\n\t\ttid_conf->rtscts =\n\t\t\tnla_get_u8(attrs[NL80211_TID_CONFIG_ATTR_RTSCTS_CTRL]);\n\t}\n\n\tif (attrs[NL80211_TID_CONFIG_ATTR_AMSDU_CTRL]) {\n\t\ttid_conf->mask |= BIT(NL80211_TID_CONFIG_ATTR_AMSDU_CTRL);\n\t\ttid_conf->amsdu =\n\t\t\tnla_get_u8(attrs[NL80211_TID_CONFIG_ATTR_AMSDU_CTRL]);\n\t}\n\n\tif (attrs[NL80211_TID_CONFIG_ATTR_TX_RATE_TYPE]) {\n\t\tu32 idx = NL80211_TID_CONFIG_ATTR_TX_RATE_TYPE, attr;\n\n\t\ttid_conf->txrate_type = nla_get_u8(attrs[idx]);\n\n\t\tif (tid_conf->txrate_type != NL80211_TX_RATE_AUTOMATIC) {\n\t\t\tattr = NL80211_TID_CONFIG_ATTR_TX_RATE;\n\t\t\terr = nl80211_parse_tx_bitrate_mask(info, attrs, attr,\n\t\t\t\t\t\t    &tid_conf->txrate_mask, dev,\n\t\t\t\t\t\t    true);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\ttid_conf->mask |= BIT(NL80211_TID_CONFIG_ATTR_TX_RATE);\n\t\t}\n\t\ttid_conf->mask |= BIT(NL80211_TID_CONFIG_ATTR_TX_RATE_TYPE);\n\t}\n\n\tif (peer)\n\t\tmask = rdev->wiphy.tid_config_support.peer;\n\telse\n\t\tmask = rdev->wiphy.tid_config_support.vif;\n\n\tif (tid_conf->mask & ~mask) {\n\t\tNL_SET_ERR_MSG(extack, \"unsupported TID configuration\");\n\t\treturn -ENOTSUPP;\n\t}\n\n\treturn 0;\n}\n\nstatic int nl80211_set_tid_config(struct sk_buff *skb,\n\t\t\t\t  struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct nlattr *attrs[NL80211_TID_CONFIG_ATTR_MAX + 1];\n\tstruct net_device *dev = info->user_ptr[1];\n\tstruct cfg80211_tid_config *tid_config;\n\tstruct nlattr *tid;\n\tint conf_idx = 0, rem_conf;\n\tint ret = -EINVAL;\n\tu32 num_conf = 0;\n\n\tif (!info->attrs[NL80211_ATTR_TID_CONFIG])\n\t\treturn -EINVAL;\n\n\tif (!rdev->ops->set_tid_config)\n\t\treturn -EOPNOTSUPP;\n\n\tnla_for_each_nested(tid, info->attrs[NL80211_ATTR_TID_CONFIG],\n\t\t\t    rem_conf)\n\t\tnum_conf++;\n\n\ttid_config = kzalloc(struct_size(tid_config, tid_conf, num_conf),\n\t\t\t     GFP_KERNEL);\n\tif (!tid_config)\n\t\treturn -ENOMEM;\n\n\ttid_config->n_tid_conf = num_conf;\n\n\tif (info->attrs[NL80211_ATTR_MAC])\n\t\ttid_config->peer = nla_data(info->attrs[NL80211_ATTR_MAC]);\n\n\tnla_for_each_nested(tid, info->attrs[NL80211_ATTR_TID_CONFIG],\n\t\t\t    rem_conf) {\n\t\tret = nla_parse_nested(attrs, NL80211_TID_CONFIG_ATTR_MAX,\n\t\t\t\t       tid, NULL, NULL);\n\n\t\tif (ret)\n\t\t\tgoto bad_tid_conf;\n\n\t\tret = parse_tid_conf(rdev, attrs, dev,\n\t\t\t\t     &tid_config->tid_conf[conf_idx],\n\t\t\t\t     info, tid_config->peer);\n\t\tif (ret)\n\t\t\tgoto bad_tid_conf;\n\n\t\tconf_idx++;\n\t}\n\n\tret = rdev_set_tid_config(rdev, dev, tid_config);\n\nbad_tid_conf:\n\tkfree(tid_config);\n\treturn ret;\n}\n\n#define NL80211_FLAG_NEED_WIPHY\t\t0x01\n#define NL80211_FLAG_NEED_NETDEV\t0x02\n#define NL80211_FLAG_NEED_RTNL\t\t0x04\n#define NL80211_FLAG_CHECK_NETDEV_UP\t0x08\n#define NL80211_FLAG_NEED_NETDEV_UP\t(NL80211_FLAG_NEED_NETDEV |\\\n\t\t\t\t\t NL80211_FLAG_CHECK_NETDEV_UP)\n#define NL80211_FLAG_NEED_WDEV\t\t0x10\n/* If a netdev is associated, it must be UP, P2P must be started */\n#define NL80211_FLAG_NEED_WDEV_UP\t(NL80211_FLAG_NEED_WDEV |\\\n\t\t\t\t\t NL80211_FLAG_CHECK_NETDEV_UP)\n#define NL80211_FLAG_CLEAR_SKB\t\t0x20\n\nstatic int nl80211_pre_doit(const struct genl_ops *ops, struct sk_buff *skb,\n\t\t\t    struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = NULL;\n\tstruct wireless_dev *wdev;\n\tstruct net_device *dev;\n\n\trtnl_lock();\n\tif (ops->internal_flags & NL80211_FLAG_NEED_WIPHY) {\n\t\trdev = cfg80211_get_dev_from_info(genl_info_net(info), info);\n\t\tif (IS_ERR(rdev)) {\n\t\t\trtnl_unlock();\n\t\t\treturn PTR_ERR(rdev);\n\t\t}\n\t\tinfo->user_ptr[0] = rdev;\n\t} else if (ops->internal_flags & NL80211_FLAG_NEED_NETDEV ||\n\t\t   ops->internal_flags & NL80211_FLAG_NEED_WDEV) {\n\t\twdev = __cfg80211_wdev_from_attrs(NULL, genl_info_net(info),\n\t\t\t\t\t\t  info->attrs);\n\t\tif (IS_ERR(wdev)) {\n\t\t\trtnl_unlock();\n\t\t\treturn PTR_ERR(wdev);\n\t\t}\n\n\t\tdev = wdev->netdev;\n\t\trdev = wiphy_to_rdev(wdev->wiphy);\n\n\t\tif (ops->internal_flags & NL80211_FLAG_NEED_NETDEV) {\n\t\t\tif (!dev) {\n\t\t\t\trtnl_unlock();\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tinfo->user_ptr[1] = dev;\n\t\t} else {\n\t\t\tinfo->user_ptr[1] = wdev;\n\t\t}\n\n\t\tif (ops->internal_flags & NL80211_FLAG_CHECK_NETDEV_UP &&\n\t\t    !wdev_running(wdev)) {\n\t\t\trtnl_unlock();\n\t\t\treturn -ENETDOWN;\n\t\t}\n\n\t\tif (dev)\n\t\t\tdev_hold(dev);\n\n\t\tinfo->user_ptr[0] = rdev;\n\t}\n\n\tif (rdev) {\n\t\twiphy_lock(&rdev->wiphy);\n\t\t/* we keep the mutex locked until post_doit */\n\t\t__release(&rdev->wiphy.mtx);\n\t}\n\tif (!(ops->internal_flags & NL80211_FLAG_NEED_RTNL))\n\t\trtnl_unlock();\n\n\treturn 0;\n}\n\nstatic void nl80211_post_doit(const struct genl_ops *ops, struct sk_buff *skb,\n\t\t\t      struct genl_info *info)\n{\n\tif (info->user_ptr[1]) {\n\t\tif (ops->internal_flags & NL80211_FLAG_NEED_WDEV) {\n\t\t\tstruct wireless_dev *wdev = info->user_ptr[1];\n\n\t\t\tif (wdev->netdev)\n\t\t\t\tdev_put(wdev->netdev);\n\t\t} else {\n\t\t\tdev_put(info->user_ptr[1]);\n\t\t}\n\t}\n\n\tif (info->user_ptr[0]) {\n\t\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\n\t\t/* we kept the mutex locked since pre_doit */\n\t\t__acquire(&rdev->wiphy.mtx);\n\t\twiphy_unlock(&rdev->wiphy);\n\t}\n\n\tif (ops->internal_flags & NL80211_FLAG_NEED_RTNL)\n\t\trtnl_unlock();\n\n\t/* If needed, clear the netlink message payload from the SKB\n\t * as it might contain key data that shouldn't stick around on\n\t * the heap after the SKB is freed. The netlink message header\n\t * is still needed for further processing, so leave it intact.\n\t */\n\tif (ops->internal_flags & NL80211_FLAG_CLEAR_SKB) {\n\t\tstruct nlmsghdr *nlh = nlmsg_hdr(skb);\n\n\t\tmemset(nlmsg_data(nlh), 0, nlmsg_len(nlh));\n\t}\n}\n\nstatic int nl80211_set_sar_sub_specs(struct cfg80211_registered_device *rdev,\n\t\t\t\t     struct cfg80211_sar_specs *sar_specs,\n\t\t\t\t     struct nlattr *spec[], int index)\n{\n\tu32 range_index, i;\n\n\tif (!sar_specs || !spec)\n\t\treturn -EINVAL;\n\n\tif (!spec[NL80211_SAR_ATTR_SPECS_POWER] ||\n\t    !spec[NL80211_SAR_ATTR_SPECS_RANGE_INDEX])\n\t\treturn -EINVAL;\n\n\trange_index = nla_get_u32(spec[NL80211_SAR_ATTR_SPECS_RANGE_INDEX]);\n\n\t/* check if range_index exceeds num_freq_ranges */\n\tif (range_index >= rdev->wiphy.sar_capa->num_freq_ranges)\n\t\treturn -EINVAL;\n\n\t/* check if range_index duplicates */\n\tfor (i = 0; i < index; i++) {\n\t\tif (sar_specs->sub_specs[i].freq_range_index == range_index)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsar_specs->sub_specs[index].power =\n\t\tnla_get_s32(spec[NL80211_SAR_ATTR_SPECS_POWER]);\n\n\tsar_specs->sub_specs[index].freq_range_index = range_index;\n\n\treturn 0;\n}\n\nstatic int nl80211_set_sar_specs(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct cfg80211_registered_device *rdev = info->user_ptr[0];\n\tstruct nlattr *spec[NL80211_SAR_ATTR_SPECS_MAX + 1];\n\tstruct nlattr *tb[NL80211_SAR_ATTR_MAX + 1];\n\tstruct cfg80211_sar_specs *sar_spec;\n\tenum nl80211_sar_type type;\n\tstruct nlattr *spec_list;\n\tu32 specs;\n\tint rem, err;\n\n\tif (!rdev->wiphy.sar_capa || !rdev->ops->set_sar_specs)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!info->attrs[NL80211_ATTR_SAR_SPEC])\n\t\treturn -EINVAL;\n\n\tnla_parse_nested(tb, NL80211_SAR_ATTR_MAX,\n\t\t\t info->attrs[NL80211_ATTR_SAR_SPEC],\n\t\t\t NULL, NULL);\n\n\tif (!tb[NL80211_SAR_ATTR_TYPE] || !tb[NL80211_SAR_ATTR_SPECS])\n\t\treturn -EINVAL;\n\n\ttype = nla_get_u32(tb[NL80211_SAR_ATTR_TYPE]);\n\tif (type != rdev->wiphy.sar_capa->type)\n\t\treturn -EINVAL;\n\n\tspecs = 0;\n\tnla_for_each_nested(spec_list, tb[NL80211_SAR_ATTR_SPECS], rem)\n\t\tspecs++;\n\n\tif (specs > rdev->wiphy.sar_capa->num_freq_ranges)\n\t\treturn -EINVAL;\n\n\tsar_spec = kzalloc(sizeof(*sar_spec) +\n\t\t\t   specs * sizeof(struct cfg80211_sar_sub_specs),\n\t\t\t   GFP_KERNEL);\n\tif (!sar_spec)\n\t\treturn -ENOMEM;\n\n\tsar_spec->type = type;\n\tspecs = 0;\n\tnla_for_each_nested(spec_list, tb[NL80211_SAR_ATTR_SPECS], rem) {\n\t\tnla_parse_nested(spec, NL80211_SAR_ATTR_SPECS_MAX,\n\t\t\t\t spec_list, NULL, NULL);\n\n\t\tswitch (type) {\n\t\tcase NL80211_SAR_TYPE_POWER:\n\t\t\tif (nl80211_set_sar_sub_specs(rdev, sar_spec,\n\t\t\t\t\t\t      spec, specs)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto error;\n\t\t}\n\t\tspecs++;\n\t}\n\n\tsar_spec->num_sub_specs = specs;\n\n\trdev->cur_cmd_info = info;\n\terr = rdev_set_sar_specs(rdev, sar_spec);\n\trdev->cur_cmd_info = NULL;\nerror:\n\tkfree(sar_spec);\n\treturn err;\n}\n\nstatic const struct genl_ops nl80211_ops[] = {\n\t{\n\t\t.cmd = NL80211_CMD_GET_WIPHY,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_get_wiphy,\n\t\t.dumpit = nl80211_dump_wiphy,\n\t\t.done = nl80211_dump_wiphy_done,\n\t\t/* can be retrieved by unprivileged users */\n\t\t.internal_flags = NL80211_FLAG_NEED_WIPHY,\n\t},\n};\n\nstatic const struct genl_small_ops nl80211_small_ops[] = {\n\t{\n\t\t.cmd = NL80211_CMD_SET_WIPHY,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_set_wiphy,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_GET_INTERFACE,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_get_interface,\n\t\t.dumpit = nl80211_dump_interface,\n\t\t/* can be retrieved by unprivileged users */\n\t\t.internal_flags = NL80211_FLAG_NEED_WDEV,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_INTERFACE,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_set_interface,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV |\n\t\t\t\t  NL80211_FLAG_NEED_RTNL,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_NEW_INTERFACE,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_new_interface,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WIPHY |\n\t\t\t\t  NL80211_FLAG_NEED_RTNL,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_DEL_INTERFACE,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_del_interface,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WDEV |\n\t\t\t\t  NL80211_FLAG_NEED_RTNL,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_GET_KEY,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_get_key,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_KEY,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_set_key,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP |\n\t\t\t\t  NL80211_FLAG_CLEAR_SKB,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_NEW_KEY,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_new_key,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP |\n\t\t\t\t  NL80211_FLAG_CLEAR_SKB,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_DEL_KEY,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_del_key,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_BEACON,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.doit = nl80211_set_beacon,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_START_AP,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.doit = nl80211_start_ap,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_STOP_AP,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.doit = nl80211_stop_ap,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_GET_STATION,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_get_station,\n\t\t.dumpit = nl80211_dump_station,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_STATION,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_set_station,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_NEW_STATION,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_new_station,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_DEL_STATION,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_del_station,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_GET_MPATH,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_get_mpath,\n\t\t.dumpit = nl80211_dump_mpath,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_GET_MPP,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_get_mpp,\n\t\t.dumpit = nl80211_dump_mpp,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_MPATH,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_set_mpath,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_NEW_MPATH,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_new_mpath,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_DEL_MPATH,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_del_mpath,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_BSS,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_set_bss,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_GET_REG,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_get_reg_do,\n\t\t.dumpit = nl80211_get_reg_dump,\n\t\t.internal_flags = 0,\n\t\t/* can be retrieved by unprivileged users */\n\t},\n#ifdef CONFIG_CFG80211_CRDA_SUPPORT\n\t{\n\t\t.cmd = NL80211_CMD_SET_REG,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_set_reg,\n\t\t.flags = GENL_ADMIN_PERM,\n\t\t.internal_flags = 0,\n\t},\n#endif\n\t{\n\t\t.cmd = NL80211_CMD_REQ_SET_REG,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_req_set_reg,\n\t\t.flags = GENL_ADMIN_PERM,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_RELOAD_REGDB,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_reload_regdb,\n\t\t.flags = GENL_ADMIN_PERM,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_GET_MESH_CONFIG,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_get_mesh_config,\n\t\t/* can be retrieved by unprivileged users */\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_MESH_CONFIG,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_update_mesh_config,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_TRIGGER_SCAN,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_trigger_scan,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_ABORT_SCAN,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_abort_scan,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_GET_SCAN,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.dumpit = nl80211_dump_scan,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_START_SCHED_SCAN,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_start_sched_scan,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_STOP_SCHED_SCAN,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_stop_sched_scan,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_AUTHENTICATE,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_authenticate,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP |\n\t\t\t\t  0 |\n\t\t\t\t  NL80211_FLAG_CLEAR_SKB,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_ASSOCIATE,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_associate,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP |\n\t\t\t\t  0 |\n\t\t\t\t  NL80211_FLAG_CLEAR_SKB,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_DEAUTHENTICATE,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_deauthenticate,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_DISASSOCIATE,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_disassociate,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_JOIN_IBSS,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_join_ibss,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_LEAVE_IBSS,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_leave_ibss,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n#ifdef CONFIG_NL80211_TESTMODE\n\t{\n\t\t.cmd = NL80211_CMD_TESTMODE,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_testmode_do,\n\t\t.dumpit = nl80211_testmode_dump,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WIPHY,\n\t},\n#endif\n\t{\n\t\t.cmd = NL80211_CMD_CONNECT,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_connect,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP |\n\t\t\t\t  0 |\n\t\t\t\t  NL80211_FLAG_CLEAR_SKB,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_UPDATE_CONNECT_PARAMS,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_update_connect_params,\n\t\t.flags = GENL_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP |\n\t\t\t\t  0 |\n\t\t\t\t  NL80211_FLAG_CLEAR_SKB,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_DISCONNECT,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_disconnect,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_WIPHY_NETNS,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_wiphy_netns,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WIPHY,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_GET_SURVEY,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.dumpit = nl80211_dump_survey,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_PMKSA,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_setdel_pmksa,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP |\n\t\t\t\t  0 |\n\t\t\t\t  NL80211_FLAG_CLEAR_SKB,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_DEL_PMKSA,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_setdel_pmksa,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_FLUSH_PMKSA,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_flush_pmksa,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_REMAIN_ON_CHANNEL,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_remain_on_channel,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_CANCEL_REMAIN_ON_CHANNEL,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_cancel_remain_on_channel,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_TX_BITRATE_MASK,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_set_tx_bitrate_mask,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_REGISTER_FRAME,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_register_mgmt,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WDEV,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_FRAME,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_tx_mgmt,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_FRAME_WAIT_CANCEL,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_tx_mgmt_cancel_wait,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_POWER_SAVE,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_set_power_save,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_GET_POWER_SAVE,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_get_power_save,\n\t\t/* can be retrieved by unprivileged users */\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_CQM,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_set_cqm,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_CHANNEL,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_set_channel,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_JOIN_MESH,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_join_mesh,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_LEAVE_MESH,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_leave_mesh,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_JOIN_OCB,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_join_ocb,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_LEAVE_OCB,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_leave_ocb,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n#ifdef CONFIG_PM\n\t{\n\t\t.cmd = NL80211_CMD_GET_WOWLAN,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_get_wowlan,\n\t\t/* can be retrieved by unprivileged users */\n\t\t.internal_flags = NL80211_FLAG_NEED_WIPHY,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_WOWLAN,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_set_wowlan,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WIPHY,\n\t},\n#endif\n\t{\n\t\t.cmd = NL80211_CMD_SET_REKEY_OFFLOAD,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_set_rekey_data,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP |\n\t\t\t\t  0 |\n\t\t\t\t  NL80211_FLAG_CLEAR_SKB,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_TDLS_MGMT,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_tdls_mgmt,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_TDLS_OPER,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_tdls_oper,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_UNEXPECTED_FRAME,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_register_unexpected_frame,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_PROBE_CLIENT,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_probe_client,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_REGISTER_BEACONS,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_register_beacons,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WIPHY,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_NOACK_MAP,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_set_noack_map,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_START_P2P_DEVICE,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_start_p2p_device,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WDEV |\n\t\t\t\t  NL80211_FLAG_NEED_RTNL,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_STOP_P2P_DEVICE,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_stop_p2p_device,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WDEV_UP |\n\t\t\t\t  NL80211_FLAG_NEED_RTNL,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_START_NAN,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_start_nan,\n\t\t.flags = GENL_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WDEV |\n\t\t\t\t  NL80211_FLAG_NEED_RTNL,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_STOP_NAN,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_stop_nan,\n\t\t.flags = GENL_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WDEV_UP |\n\t\t\t\t  NL80211_FLAG_NEED_RTNL,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_ADD_NAN_FUNCTION,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_nan_add_func,\n\t\t.flags = GENL_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_DEL_NAN_FUNCTION,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_nan_del_func,\n\t\t.flags = GENL_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_CHANGE_NAN_CONFIG,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_nan_change_config,\n\t\t.flags = GENL_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_MCAST_RATE,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_set_mcast_rate,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_MAC_ACL,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_set_mac_acl,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_RADAR_DETECT,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_start_radar_detection,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_GET_PROTOCOL_FEATURES,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_get_protocol_features,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_UPDATE_FT_IES,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_update_ft_ies,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_CRIT_PROTOCOL_START,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_crit_protocol_start,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_CRIT_PROTOCOL_STOP,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_crit_protocol_stop,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_GET_COALESCE,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_get_coalesce,\n\t\t.internal_flags = NL80211_FLAG_NEED_WIPHY,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_COALESCE,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_set_coalesce,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WIPHY,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_CHANNEL_SWITCH,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_channel_switch,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_VENDOR,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_vendor_cmd,\n\t\t.dumpit = nl80211_vendor_cmd_dump,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WIPHY |\n\t\t\t\t  0 |\n\t\t\t\t  NL80211_FLAG_CLEAR_SKB,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_QOS_MAP,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_set_qos_map,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_ADD_TX_TS,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_add_tx_ts,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_DEL_TX_TS,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_del_tx_ts,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_TDLS_CHANNEL_SWITCH,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_tdls_channel_switch,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_TDLS_CANCEL_CHANNEL_SWITCH,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_tdls_cancel_channel_switch,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_MULTICAST_TO_UNICAST,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_set_multicast_to_unicast,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_PMK,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_set_pmk,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP |\n\t\t\t\t  0 |\n\t\t\t\t  NL80211_FLAG_CLEAR_SKB,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_DEL_PMK,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_del_pmk,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_EXTERNAL_AUTH,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_external_auth,\n\t\t.flags = GENL_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_CONTROL_PORT_FRAME,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_tx_control_port,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_GET_FTM_RESPONDER_STATS,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_get_ftm_responder_stats,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_PEER_MEASUREMENT_START,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_pmsr_start,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_NOTIFY_RADAR,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_notify_radar_detection,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_UPDATE_OWE_INFO,\n\t\t.doit = nl80211_update_owe_info,\n\t\t.flags = GENL_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_PROBE_MESH_LINK,\n\t\t.doit = nl80211_probe_mesh_link,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV_UP,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_TID_CONFIG,\n\t\t.doit = nl80211_set_tid_config,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_NETDEV,\n\t},\n\t{\n\t\t.cmd = NL80211_CMD_SET_SAR_SPECS,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.doit = nl80211_set_sar_specs,\n\t\t.flags = GENL_UNS_ADMIN_PERM,\n\t\t.internal_flags = NL80211_FLAG_NEED_WIPHY |\n\t\t\t\t  NL80211_FLAG_NEED_RTNL,\n\t},\n};\n\nstatic struct genl_family nl80211_fam __ro_after_init = {\n\t.name = NL80211_GENL_NAME,\t/* have users key off the name instead */\n\t.hdrsize = 0,\t\t\t/* no private header */\n\t.version = 1,\t\t\t/* no particular meaning now */\n\t.maxattr = NL80211_ATTR_MAX,\n\t.policy = nl80211_policy,\n\t.netnsok = true,\n\t.pre_doit = nl80211_pre_doit,\n\t.post_doit = nl80211_post_doit,\n\t.module = THIS_MODULE,\n\t.ops = nl80211_ops,\n\t.n_ops = ARRAY_SIZE(nl80211_ops),\n\t.small_ops = nl80211_small_ops,\n\t.n_small_ops = ARRAY_SIZE(nl80211_small_ops),\n\t.mcgrps = nl80211_mcgrps,\n\t.n_mcgrps = ARRAY_SIZE(nl80211_mcgrps),\n\t.parallel_ops = true,\n};\n\n/* notification functions */\n\nvoid nl80211_notify_wiphy(struct cfg80211_registered_device *rdev,\n\t\t\t  enum nl80211_commands cmd)\n{\n\tstruct sk_buff *msg;\n\tstruct nl80211_dump_wiphy_state state = {};\n\n\tWARN_ON(cmd != NL80211_CMD_NEW_WIPHY &&\n\t\tcmd != NL80211_CMD_DEL_WIPHY);\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn;\n\n\tif (nl80211_send_wiphy(rdev, cmd, msg, 0, 0, 0, &state) < 0) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_CONFIG, GFP_KERNEL);\n}\n\nvoid nl80211_notify_iface(struct cfg80211_registered_device *rdev,\n\t\t\t\tstruct wireless_dev *wdev,\n\t\t\t\tenum nl80211_commands cmd)\n{\n\tstruct sk_buff *msg;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn;\n\n\tif (nl80211_send_iface(msg, 0, 0, 0, rdev, wdev, cmd) < 0) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_CONFIG, GFP_KERNEL);\n}\n\nstatic int nl80211_add_scan_req(struct sk_buff *msg,\n\t\t\t\tstruct cfg80211_registered_device *rdev)\n{\n\tstruct cfg80211_scan_request *req = rdev->scan_req;\n\tstruct nlattr *nest;\n\tint i;\n\tstruct cfg80211_scan_info *info;\n\n\tif (WARN_ON(!req))\n\t\treturn 0;\n\n\tnest = nla_nest_start_noflag(msg, NL80211_ATTR_SCAN_SSIDS);\n\tif (!nest)\n\t\tgoto nla_put_failure;\n\tfor (i = 0; i < req->n_ssids; i++) {\n\t\tif (nla_put(msg, i, req->ssids[i].ssid_len, req->ssids[i].ssid))\n\t\t\tgoto nla_put_failure;\n\t}\n\tnla_nest_end(msg, nest);\n\n\tif (req->flags & NL80211_SCAN_FLAG_FREQ_KHZ) {\n\t\tnest = nla_nest_start(msg, NL80211_ATTR_SCAN_FREQ_KHZ);\n\t\tif (!nest)\n\t\t\tgoto nla_put_failure;\n\t\tfor (i = 0; i < req->n_channels; i++) {\n\t\t\tif (nla_put_u32(msg, i,\n\t\t\t\t   ieee80211_channel_to_khz(req->channels[i])))\n\t\t\t\tgoto nla_put_failure;\n\t\t}\n\t\tnla_nest_end(msg, nest);\n\t} else {\n\t\tnest = nla_nest_start_noflag(msg,\n\t\t\t\t\t     NL80211_ATTR_SCAN_FREQUENCIES);\n\t\tif (!nest)\n\t\t\tgoto nla_put_failure;\n\t\tfor (i = 0; i < req->n_channels; i++) {\n\t\t\tif (nla_put_u32(msg, i, req->channels[i]->center_freq))\n\t\t\t\tgoto nla_put_failure;\n\t\t}\n\t\tnla_nest_end(msg, nest);\n\t}\n\n\tif (req->ie &&\n\t    nla_put(msg, NL80211_ATTR_IE, req->ie_len, req->ie))\n\t\tgoto nla_put_failure;\n\n\tif (req->flags &&\n\t    nla_put_u32(msg, NL80211_ATTR_SCAN_FLAGS, req->flags))\n\t\tgoto nla_put_failure;\n\n\tinfo = rdev->int_scan_req ? &rdev->int_scan_req->info :\n\t\t&rdev->scan_req->info;\n\tif (info->scan_start_tsf &&\n\t    (nla_put_u64_64bit(msg, NL80211_ATTR_SCAN_START_TIME_TSF,\n\t\t\t       info->scan_start_tsf, NL80211_BSS_PAD) ||\n\t     nla_put(msg, NL80211_ATTR_SCAN_START_TIME_TSF_BSSID, ETH_ALEN,\n\t\t     info->tsf_bssid)))\n\t\tgoto nla_put_failure;\n\n\treturn 0;\n nla_put_failure:\n\treturn -ENOBUFS;\n}\n\nstatic int nl80211_prep_scan_msg(struct sk_buff *msg,\n\t\t\t\t struct cfg80211_registered_device *rdev,\n\t\t\t\t struct wireless_dev *wdev,\n\t\t\t\t u32 portid, u32 seq, int flags,\n\t\t\t\t u32 cmd)\n{\n\tvoid *hdr;\n\n\thdr = nl80211hdr_put(msg, portid, seq, flags, cmd);\n\tif (!hdr)\n\t\treturn -1;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    (wdev->netdev && nla_put_u32(msg, NL80211_ATTR_IFINDEX,\n\t\t\t\t\t wdev->netdev->ifindex)) ||\n\t    nla_put_u64_64bit(msg, NL80211_ATTR_WDEV, wdev_id(wdev),\n\t\t\t      NL80211_ATTR_PAD))\n\t\tgoto nla_put_failure;\n\n\t/* ignore errors and send incomplete event anyway */\n\tnl80211_add_scan_req(msg, rdev);\n\n\tgenlmsg_end(msg, hdr);\n\treturn 0;\n\n nla_put_failure:\n\tgenlmsg_cancel(msg, hdr);\n\treturn -EMSGSIZE;\n}\n\nstatic int\nnl80211_prep_sched_scan_msg(struct sk_buff *msg,\n\t\t\t    struct cfg80211_sched_scan_request *req, u32 cmd)\n{\n\tvoid *hdr;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, cmd);\n\tif (!hdr)\n\t\treturn -1;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY,\n\t\t\twiphy_to_rdev(req->wiphy)->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, req->dev->ifindex) ||\n\t    nla_put_u64_64bit(msg, NL80211_ATTR_COOKIE, req->reqid,\n\t\t\t      NL80211_ATTR_PAD))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\treturn 0;\n\n nla_put_failure:\n\tgenlmsg_cancel(msg, hdr);\n\treturn -EMSGSIZE;\n}\n\nvoid nl80211_send_scan_start(struct cfg80211_registered_device *rdev,\n\t\t\t     struct wireless_dev *wdev)\n{\n\tstruct sk_buff *msg;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn;\n\n\tif (nl80211_prep_scan_msg(msg, rdev, wdev, 0, 0, 0,\n\t\t\t\t  NL80211_CMD_TRIGGER_SCAN) < 0) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_SCAN, GFP_KERNEL);\n}\n\nstruct sk_buff *nl80211_build_scan_msg(struct cfg80211_registered_device *rdev,\n\t\t\t\t       struct wireless_dev *wdev, bool aborted)\n{\n\tstruct sk_buff *msg;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn NULL;\n\n\tif (nl80211_prep_scan_msg(msg, rdev, wdev, 0, 0, 0,\n\t\t\t\t  aborted ? NL80211_CMD_SCAN_ABORTED :\n\t\t\t\t\t    NL80211_CMD_NEW_SCAN_RESULTS) < 0) {\n\t\tnlmsg_free(msg);\n\t\treturn NULL;\n\t}\n\n\treturn msg;\n}\n\n/* send message created by nl80211_build_scan_msg() */\nvoid nl80211_send_scan_msg(struct cfg80211_registered_device *rdev,\n\t\t\t   struct sk_buff *msg)\n{\n\tif (!msg)\n\t\treturn;\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_SCAN, GFP_KERNEL);\n}\n\nvoid nl80211_send_sched_scan(struct cfg80211_sched_scan_request *req, u32 cmd)\n{\n\tstruct sk_buff *msg;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn;\n\n\tif (nl80211_prep_sched_scan_msg(msg, req, cmd) < 0) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(req->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_SCAN, GFP_KERNEL);\n}\n\nstatic bool nl80211_reg_change_event_fill(struct sk_buff *msg,\n\t\t\t\t\t  struct regulatory_request *request)\n{\n\t/* Userspace can always count this one always being set */\n\tif (nla_put_u8(msg, NL80211_ATTR_REG_INITIATOR, request->initiator))\n\t\tgoto nla_put_failure;\n\n\tif (request->alpha2[0] == '0' && request->alpha2[1] == '0') {\n\t\tif (nla_put_u8(msg, NL80211_ATTR_REG_TYPE,\n\t\t\t       NL80211_REGDOM_TYPE_WORLD))\n\t\t\tgoto nla_put_failure;\n\t} else if (request->alpha2[0] == '9' && request->alpha2[1] == '9') {\n\t\tif (nla_put_u8(msg, NL80211_ATTR_REG_TYPE,\n\t\t\t       NL80211_REGDOM_TYPE_CUSTOM_WORLD))\n\t\t\tgoto nla_put_failure;\n\t} else if ((request->alpha2[0] == '9' && request->alpha2[1] == '8') ||\n\t\t   request->intersect) {\n\t\tif (nla_put_u8(msg, NL80211_ATTR_REG_TYPE,\n\t\t\t       NL80211_REGDOM_TYPE_INTERSECTION))\n\t\t\tgoto nla_put_failure;\n\t} else {\n\t\tif (nla_put_u8(msg, NL80211_ATTR_REG_TYPE,\n\t\t\t       NL80211_REGDOM_TYPE_COUNTRY) ||\n\t\t    nla_put_string(msg, NL80211_ATTR_REG_ALPHA2,\n\t\t\t\t   request->alpha2))\n\t\t\tgoto nla_put_failure;\n\t}\n\n\tif (request->wiphy_idx != WIPHY_IDX_INVALID) {\n\t\tstruct wiphy *wiphy = wiphy_idx_to_wiphy(request->wiphy_idx);\n\n\t\tif (wiphy &&\n\t\t    nla_put_u32(msg, NL80211_ATTR_WIPHY, request->wiphy_idx))\n\t\t\tgoto nla_put_failure;\n\n\t\tif (wiphy &&\n\t\t    wiphy->regulatory_flags & REGULATORY_WIPHY_SELF_MANAGED &&\n\t\t    nla_put_flag(msg, NL80211_ATTR_WIPHY_SELF_MANAGED_REG))\n\t\t\tgoto nla_put_failure;\n\t}\n\n\treturn true;\n\nnla_put_failure:\n\treturn false;\n}\n\n/*\n * This can happen on global regulatory changes or device specific settings\n * based on custom regulatory domains.\n */\nvoid nl80211_common_reg_change_event(enum nl80211_commands cmd_id,\n\t\t\t\t     struct regulatory_request *request)\n{\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, cmd_id);\n\tif (!hdr)\n\t\tgoto nla_put_failure;\n\n\tif (!nl80211_reg_change_event_fill(msg, request))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\n\trcu_read_lock();\n\tgenlmsg_multicast_allns(&nl80211_fam, msg, 0,\n\t\t\t\tNL80211_MCGRP_REGULATORY, GFP_ATOMIC);\n\trcu_read_unlock();\n\n\treturn;\n\nnla_put_failure:\n\tnlmsg_free(msg);\n}\n\nstatic void nl80211_send_mlme_event(struct cfg80211_registered_device *rdev,\n\t\t\t\t    struct net_device *netdev,\n\t\t\t\t    const u8 *buf, size_t len,\n\t\t\t\t    enum nl80211_commands cmd, gfp_t gfp,\n\t\t\t\t    int uapsd_queues, const u8 *req_ies,\n\t\t\t\t    size_t req_ies_len, bool reconnect)\n{\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\tmsg = nlmsg_new(100 + len + req_ies_len, gfp);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, cmd);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, netdev->ifindex) ||\n\t    nla_put(msg, NL80211_ATTR_FRAME, len, buf) ||\n\t    (req_ies &&\n\t     nla_put(msg, NL80211_ATTR_REQ_IE, req_ies_len, req_ies)))\n\t\tgoto nla_put_failure;\n\n\tif (reconnect && nla_put_flag(msg, NL80211_ATTR_RECONNECT_REQUESTED))\n\t\tgoto nla_put_failure;\n\n\tif (uapsd_queues >= 0) {\n\t\tstruct nlattr *nla_wmm =\n\t\t\tnla_nest_start_noflag(msg, NL80211_ATTR_STA_WME);\n\t\tif (!nla_wmm)\n\t\t\tgoto nla_put_failure;\n\n\t\tif (nla_put_u8(msg, NL80211_STA_WME_UAPSD_QUEUES,\n\t\t\t       uapsd_queues))\n\t\t\tgoto nla_put_failure;\n\n\t\tnla_nest_end(msg, nla_wmm);\n\t}\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\n\nvoid nl80211_send_rx_auth(struct cfg80211_registered_device *rdev,\n\t\t\t  struct net_device *netdev, const u8 *buf,\n\t\t\t  size_t len, gfp_t gfp)\n{\n\tnl80211_send_mlme_event(rdev, netdev, buf, len,\n\t\t\t\tNL80211_CMD_AUTHENTICATE, gfp, -1, NULL, 0,\n\t\t\t\tfalse);\n}\n\nvoid nl80211_send_rx_assoc(struct cfg80211_registered_device *rdev,\n\t\t\t   struct net_device *netdev, const u8 *buf,\n\t\t\t   size_t len, gfp_t gfp, int uapsd_queues,\n\t\t\t   const u8 *req_ies, size_t req_ies_len)\n{\n\tnl80211_send_mlme_event(rdev, netdev, buf, len,\n\t\t\t\tNL80211_CMD_ASSOCIATE, gfp, uapsd_queues,\n\t\t\t\treq_ies, req_ies_len, false);\n}\n\nvoid nl80211_send_deauth(struct cfg80211_registered_device *rdev,\n\t\t\t struct net_device *netdev, const u8 *buf,\n\t\t\t size_t len, bool reconnect, gfp_t gfp)\n{\n\tnl80211_send_mlme_event(rdev, netdev, buf, len,\n\t\t\t\tNL80211_CMD_DEAUTHENTICATE, gfp, -1, NULL, 0,\n\t\t\t\treconnect);\n}\n\nvoid nl80211_send_disassoc(struct cfg80211_registered_device *rdev,\n\t\t\t   struct net_device *netdev, const u8 *buf,\n\t\t\t   size_t len, bool reconnect, gfp_t gfp)\n{\n\tnl80211_send_mlme_event(rdev, netdev, buf, len,\n\t\t\t\tNL80211_CMD_DISASSOCIATE, gfp, -1, NULL, 0,\n\t\t\t\treconnect);\n}\n\nvoid cfg80211_rx_unprot_mlme_mgmt(struct net_device *dev, const u8 *buf,\n\t\t\t\t  size_t len)\n{\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct wiphy *wiphy = wdev->wiphy;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\tconst struct ieee80211_mgmt *mgmt = (void *)buf;\n\tu32 cmd;\n\n\tif (WARN_ON(len < 2))\n\t\treturn;\n\n\tif (ieee80211_is_deauth(mgmt->frame_control)) {\n\t\tcmd = NL80211_CMD_UNPROT_DEAUTHENTICATE;\n\t} else if (ieee80211_is_disassoc(mgmt->frame_control)) {\n\t\tcmd = NL80211_CMD_UNPROT_DISASSOCIATE;\n\t} else if (ieee80211_is_beacon(mgmt->frame_control)) {\n\t\tif (wdev->unprot_beacon_reported &&\n\t\t    elapsed_jiffies_msecs(wdev->unprot_beacon_reported) < 10000)\n\t\t\treturn;\n\t\tcmd = NL80211_CMD_UNPROT_BEACON;\n\t\twdev->unprot_beacon_reported = jiffies;\n\t} else {\n\t\treturn;\n\t}\n\n\ttrace_cfg80211_rx_unprot_mlme_mgmt(dev, buf, len);\n\tnl80211_send_mlme_event(rdev, dev, buf, len, cmd, GFP_ATOMIC, -1,\n\t\t\t\tNULL, 0, false);\n}\nEXPORT_SYMBOL(cfg80211_rx_unprot_mlme_mgmt);\n\nstatic void nl80211_send_mlme_timeout(struct cfg80211_registered_device *rdev,\n\t\t\t\t      struct net_device *netdev, int cmd,\n\t\t\t\t      const u8 *addr, gfp_t gfp)\n{\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, gfp);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, cmd);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, netdev->ifindex) ||\n\t    nla_put_flag(msg, NL80211_ATTR_TIMED_OUT) ||\n\t    nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, addr))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\n\nvoid nl80211_send_auth_timeout(struct cfg80211_registered_device *rdev,\n\t\t\t       struct net_device *netdev, const u8 *addr,\n\t\t\t       gfp_t gfp)\n{\n\tnl80211_send_mlme_timeout(rdev, netdev, NL80211_CMD_AUTHENTICATE,\n\t\t\t\t  addr, gfp);\n}\n\nvoid nl80211_send_assoc_timeout(struct cfg80211_registered_device *rdev,\n\t\t\t\tstruct net_device *netdev, const u8 *addr,\n\t\t\t\tgfp_t gfp)\n{\n\tnl80211_send_mlme_timeout(rdev, netdev, NL80211_CMD_ASSOCIATE,\n\t\t\t\t  addr, gfp);\n}\n\nvoid nl80211_send_connect_result(struct cfg80211_registered_device *rdev,\n\t\t\t\t struct net_device *netdev,\n\t\t\t\t struct cfg80211_connect_resp_params *cr,\n\t\t\t\t gfp_t gfp)\n{\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\tmsg = nlmsg_new(100 + cr->req_ie_len + cr->resp_ie_len +\n\t\t\tcr->fils.kek_len + cr->fils.pmk_len +\n\t\t\t(cr->fils.pmkid ? WLAN_PMKID_LEN : 0), gfp);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_CONNECT);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, netdev->ifindex) ||\n\t    (cr->bssid &&\n\t     nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, cr->bssid)) ||\n\t    nla_put_u16(msg, NL80211_ATTR_STATUS_CODE,\n\t\t\tcr->status < 0 ? WLAN_STATUS_UNSPECIFIED_FAILURE :\n\t\t\tcr->status) ||\n\t    (cr->status < 0 &&\n\t     (nla_put_flag(msg, NL80211_ATTR_TIMED_OUT) ||\n\t      nla_put_u32(msg, NL80211_ATTR_TIMEOUT_REASON,\n\t\t\t  cr->timeout_reason))) ||\n\t    (cr->req_ie &&\n\t     nla_put(msg, NL80211_ATTR_REQ_IE, cr->req_ie_len, cr->req_ie)) ||\n\t    (cr->resp_ie &&\n\t     nla_put(msg, NL80211_ATTR_RESP_IE, cr->resp_ie_len,\n\t\t     cr->resp_ie)) ||\n\t    (cr->fils.update_erp_next_seq_num &&\n\t     nla_put_u16(msg, NL80211_ATTR_FILS_ERP_NEXT_SEQ_NUM,\n\t\t\t cr->fils.erp_next_seq_num)) ||\n\t    (cr->status == WLAN_STATUS_SUCCESS &&\n\t     ((cr->fils.kek &&\n\t       nla_put(msg, NL80211_ATTR_FILS_KEK, cr->fils.kek_len,\n\t\t       cr->fils.kek)) ||\n\t      (cr->fils.pmk &&\n\t       nla_put(msg, NL80211_ATTR_PMK, cr->fils.pmk_len, cr->fils.pmk)) ||\n\t      (cr->fils.pmkid &&\n\t       nla_put(msg, NL80211_ATTR_PMKID, WLAN_PMKID_LEN, cr->fils.pmkid)))))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\n\nvoid nl80211_send_roamed(struct cfg80211_registered_device *rdev,\n\t\t\t struct net_device *netdev,\n\t\t\t struct cfg80211_roam_info *info, gfp_t gfp)\n{\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\tconst u8 *bssid = info->bss ? info->bss->bssid : info->bssid;\n\n\tmsg = nlmsg_new(100 + info->req_ie_len + info->resp_ie_len +\n\t\t\tinfo->fils.kek_len + info->fils.pmk_len +\n\t\t\t(info->fils.pmkid ? WLAN_PMKID_LEN : 0), gfp);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_ROAM);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, netdev->ifindex) ||\n\t    nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, bssid) ||\n\t    (info->req_ie &&\n\t     nla_put(msg, NL80211_ATTR_REQ_IE, info->req_ie_len,\n\t\t     info->req_ie)) ||\n\t    (info->resp_ie &&\n\t     nla_put(msg, NL80211_ATTR_RESP_IE, info->resp_ie_len,\n\t\t     info->resp_ie)) ||\n\t    (info->fils.update_erp_next_seq_num &&\n\t     nla_put_u16(msg, NL80211_ATTR_FILS_ERP_NEXT_SEQ_NUM,\n\t\t\t info->fils.erp_next_seq_num)) ||\n\t    (info->fils.kek &&\n\t     nla_put(msg, NL80211_ATTR_FILS_KEK, info->fils.kek_len,\n\t\t     info->fils.kek)) ||\n\t    (info->fils.pmk &&\n\t     nla_put(msg, NL80211_ATTR_PMK, info->fils.pmk_len, info->fils.pmk)) ||\n\t    (info->fils.pmkid &&\n\t     nla_put(msg, NL80211_ATTR_PMKID, WLAN_PMKID_LEN, info->fils.pmkid)))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\n\nvoid nl80211_send_port_authorized(struct cfg80211_registered_device *rdev,\n\t\t\t\t  struct net_device *netdev, const u8 *bssid)\n{\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_PORT_AUTHORIZED);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, netdev->ifindex) ||\n\t    nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, bssid))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, GFP_KERNEL);\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\n\nvoid nl80211_send_disconnected(struct cfg80211_registered_device *rdev,\n\t\t\t       struct net_device *netdev, u16 reason,\n\t\t\t       const u8 *ie, size_t ie_len, bool from_ap)\n{\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\tmsg = nlmsg_new(100 + ie_len, GFP_KERNEL);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_DISCONNECT);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, netdev->ifindex) ||\n\t    (reason &&\n\t     nla_put_u16(msg, NL80211_ATTR_REASON_CODE, reason)) ||\n\t    (from_ap &&\n\t     nla_put_flag(msg, NL80211_ATTR_DISCONNECTED_BY_AP)) ||\n\t    (ie && nla_put(msg, NL80211_ATTR_IE, ie_len, ie)))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, GFP_KERNEL);\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\n\nvoid nl80211_send_ibss_bssid(struct cfg80211_registered_device *rdev,\n\t\t\t     struct net_device *netdev, const u8 *bssid,\n\t\t\t     gfp_t gfp)\n{\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, gfp);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_JOIN_IBSS);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, netdev->ifindex) ||\n\t    nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, bssid))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\n\nvoid cfg80211_notify_new_peer_candidate(struct net_device *dev, const u8 *addr,\n\t\t\t\t\tconst u8 *ie, u8 ie_len,\n\t\t\t\t\tint sig_dbm, gfp_t gfp)\n{\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wdev->wiphy);\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\tif (WARN_ON(wdev->iftype != NL80211_IFTYPE_MESH_POINT))\n\t\treturn;\n\n\ttrace_cfg80211_notify_new_peer_candidate(dev, addr);\n\n\tmsg = nlmsg_new(100 + ie_len, gfp);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_NEW_PEER_CANDIDATE);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, dev->ifindex) ||\n\t    nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, addr) ||\n\t    (ie_len && ie &&\n\t     nla_put(msg, NL80211_ATTR_IE, ie_len, ie)) ||\n\t    (sig_dbm &&\n\t     nla_put_u32(msg, NL80211_ATTR_RX_SIGNAL_DBM, sig_dbm)))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\nEXPORT_SYMBOL(cfg80211_notify_new_peer_candidate);\n\nvoid nl80211_michael_mic_failure(struct cfg80211_registered_device *rdev,\n\t\t\t\t struct net_device *netdev, const u8 *addr,\n\t\t\t\t enum nl80211_key_type key_type, int key_id,\n\t\t\t\t const u8 *tsc, gfp_t gfp)\n{\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, gfp);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_MICHAEL_MIC_FAILURE);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, netdev->ifindex) ||\n\t    (addr && nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, addr)) ||\n\t    nla_put_u32(msg, NL80211_ATTR_KEY_TYPE, key_type) ||\n\t    (key_id != -1 &&\n\t     nla_put_u8(msg, NL80211_ATTR_KEY_IDX, key_id)) ||\n\t    (tsc && nla_put(msg, NL80211_ATTR_KEY_SEQ, 6, tsc)))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\n\nvoid nl80211_send_beacon_hint_event(struct wiphy *wiphy,\n\t\t\t\t    struct ieee80211_channel *channel_before,\n\t\t\t\t    struct ieee80211_channel *channel_after)\n{\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\tstruct nlattr *nl_freq;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_ATOMIC);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_REG_BEACON_HINT);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\t/*\n\t * Since we are applying the beacon hint to a wiphy we know its\n\t * wiphy_idx is valid\n\t */\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, get_wiphy_idx(wiphy)))\n\t\tgoto nla_put_failure;\n\n\t/* Before */\n\tnl_freq = nla_nest_start_noflag(msg, NL80211_ATTR_FREQ_BEFORE);\n\tif (!nl_freq)\n\t\tgoto nla_put_failure;\n\n\tif (nl80211_msg_put_channel(msg, wiphy, channel_before, false))\n\t\tgoto nla_put_failure;\n\tnla_nest_end(msg, nl_freq);\n\n\t/* After */\n\tnl_freq = nla_nest_start_noflag(msg, NL80211_ATTR_FREQ_AFTER);\n\tif (!nl_freq)\n\t\tgoto nla_put_failure;\n\n\tif (nl80211_msg_put_channel(msg, wiphy, channel_after, false))\n\t\tgoto nla_put_failure;\n\tnla_nest_end(msg, nl_freq);\n\n\tgenlmsg_end(msg, hdr);\n\n\trcu_read_lock();\n\tgenlmsg_multicast_allns(&nl80211_fam, msg, 0,\n\t\t\t\tNL80211_MCGRP_REGULATORY, GFP_ATOMIC);\n\trcu_read_unlock();\n\n\treturn;\n\nnla_put_failure:\n\tnlmsg_free(msg);\n}\n\nstatic void nl80211_send_remain_on_chan_event(\n\tint cmd, struct cfg80211_registered_device *rdev,\n\tstruct wireless_dev *wdev, u64 cookie,\n\tstruct ieee80211_channel *chan,\n\tunsigned int duration, gfp_t gfp)\n{\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, gfp);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, cmd);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    (wdev->netdev && nla_put_u32(msg, NL80211_ATTR_IFINDEX,\n\t\t\t\t\t wdev->netdev->ifindex)) ||\n\t    nla_put_u64_64bit(msg, NL80211_ATTR_WDEV, wdev_id(wdev),\n\t\t\t      NL80211_ATTR_PAD) ||\n\t    nla_put_u32(msg, NL80211_ATTR_WIPHY_FREQ, chan->center_freq) ||\n\t    nla_put_u32(msg, NL80211_ATTR_WIPHY_CHANNEL_TYPE,\n\t\t\tNL80211_CHAN_NO_HT) ||\n\t    nla_put_u64_64bit(msg, NL80211_ATTR_COOKIE, cookie,\n\t\t\t      NL80211_ATTR_PAD))\n\t\tgoto nla_put_failure;\n\n\tif (cmd == NL80211_CMD_REMAIN_ON_CHANNEL &&\n\t    nla_put_u32(msg, NL80211_ATTR_DURATION, duration))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\n\nvoid cfg80211_ready_on_channel(struct wireless_dev *wdev, u64 cookie,\n\t\t\t       struct ieee80211_channel *chan,\n\t\t\t       unsigned int duration, gfp_t gfp)\n{\n\tstruct wiphy *wiphy = wdev->wiphy;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\n\ttrace_cfg80211_ready_on_channel(wdev, cookie, chan, duration);\n\tnl80211_send_remain_on_chan_event(NL80211_CMD_REMAIN_ON_CHANNEL,\n\t\t\t\t\t  rdev, wdev, cookie, chan,\n\t\t\t\t\t  duration, gfp);\n}\nEXPORT_SYMBOL(cfg80211_ready_on_channel);\n\nvoid cfg80211_remain_on_channel_expired(struct wireless_dev *wdev, u64 cookie,\n\t\t\t\t\tstruct ieee80211_channel *chan,\n\t\t\t\t\tgfp_t gfp)\n{\n\tstruct wiphy *wiphy = wdev->wiphy;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\n\ttrace_cfg80211_ready_on_channel_expired(wdev, cookie, chan);\n\tnl80211_send_remain_on_chan_event(NL80211_CMD_CANCEL_REMAIN_ON_CHANNEL,\n\t\t\t\t\t  rdev, wdev, cookie, chan, 0, gfp);\n}\nEXPORT_SYMBOL(cfg80211_remain_on_channel_expired);\n\nvoid cfg80211_tx_mgmt_expired(struct wireless_dev *wdev, u64 cookie,\n\t\t\t\t\tstruct ieee80211_channel *chan,\n\t\t\t\t\tgfp_t gfp)\n{\n\tstruct wiphy *wiphy = wdev->wiphy;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\n\ttrace_cfg80211_tx_mgmt_expired(wdev, cookie, chan);\n\tnl80211_send_remain_on_chan_event(NL80211_CMD_FRAME_WAIT_CANCEL,\n\t\t\t\t\t  rdev, wdev, cookie, chan, 0, gfp);\n}\nEXPORT_SYMBOL(cfg80211_tx_mgmt_expired);\n\nvoid cfg80211_new_sta(struct net_device *dev, const u8 *mac_addr,\n\t\t      struct station_info *sinfo, gfp_t gfp)\n{\n\tstruct wiphy *wiphy = dev->ieee80211_ptr->wiphy;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\tstruct sk_buff *msg;\n\n\ttrace_cfg80211_new_sta(dev, mac_addr, sinfo);\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, gfp);\n\tif (!msg)\n\t\treturn;\n\n\tif (nl80211_send_station(msg, NL80211_CMD_NEW_STATION, 0, 0, 0,\n\t\t\t\t rdev, dev, mac_addr, sinfo) < 0) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n}\nEXPORT_SYMBOL(cfg80211_new_sta);\n\nvoid cfg80211_del_sta_sinfo(struct net_device *dev, const u8 *mac_addr,\n\t\t\t    struct station_info *sinfo, gfp_t gfp)\n{\n\tstruct wiphy *wiphy = dev->ieee80211_ptr->wiphy;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\tstruct sk_buff *msg;\n\tstruct station_info empty_sinfo = {};\n\n\tif (!sinfo)\n\t\tsinfo = &empty_sinfo;\n\n\ttrace_cfg80211_del_sta(dev, mac_addr);\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, gfp);\n\tif (!msg) {\n\t\tcfg80211_sinfo_release_content(sinfo);\n\t\treturn;\n\t}\n\n\tif (nl80211_send_station(msg, NL80211_CMD_DEL_STATION, 0, 0, 0,\n\t\t\t\t rdev, dev, mac_addr, sinfo) < 0) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n}\nEXPORT_SYMBOL(cfg80211_del_sta_sinfo);\n\nvoid cfg80211_conn_failed(struct net_device *dev, const u8 *mac_addr,\n\t\t\t  enum nl80211_connect_failed_reason reason,\n\t\t\t  gfp_t gfp)\n{\n\tstruct wiphy *wiphy = dev->ieee80211_ptr->wiphy;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\tmsg = nlmsg_new(NLMSG_GOODSIZE, gfp);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_CONN_FAILED);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_IFINDEX, dev->ifindex) ||\n\t    nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, mac_addr) ||\n\t    nla_put_u32(msg, NL80211_ATTR_CONN_FAILED_REASON, reason))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\nEXPORT_SYMBOL(cfg80211_conn_failed);\n\nstatic bool __nl80211_unexpected_frame(struct net_device *dev, u8 cmd,\n\t\t\t\t       const u8 *addr, gfp_t gfp)\n{\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wdev->wiphy);\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\tu32 nlportid = READ_ONCE(wdev->ap_unexpected_nlportid);\n\n\tif (!nlportid)\n\t\treturn false;\n\n\tmsg = nlmsg_new(100, gfp);\n\tif (!msg)\n\t\treturn true;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, cmd);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn true;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, dev->ifindex) ||\n\t    nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, addr))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\tgenlmsg_unicast(wiphy_net(&rdev->wiphy), msg, nlportid);\n\treturn true;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n\treturn true;\n}\n\nbool cfg80211_rx_spurious_frame(struct net_device *dev,\n\t\t\t\tconst u8 *addr, gfp_t gfp)\n{\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tbool ret;\n\n\ttrace_cfg80211_rx_spurious_frame(dev, addr);\n\n\tif (WARN_ON(wdev->iftype != NL80211_IFTYPE_AP &&\n\t\t    wdev->iftype != NL80211_IFTYPE_P2P_GO)) {\n\t\ttrace_cfg80211_return_bool(false);\n\t\treturn false;\n\t}\n\tret = __nl80211_unexpected_frame(dev, NL80211_CMD_UNEXPECTED_FRAME,\n\t\t\t\t\t addr, gfp);\n\ttrace_cfg80211_return_bool(ret);\n\treturn ret;\n}\nEXPORT_SYMBOL(cfg80211_rx_spurious_frame);\n\nbool cfg80211_rx_unexpected_4addr_frame(struct net_device *dev,\n\t\t\t\t\tconst u8 *addr, gfp_t gfp)\n{\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tbool ret;\n\n\ttrace_cfg80211_rx_unexpected_4addr_frame(dev, addr);\n\n\tif (WARN_ON(wdev->iftype != NL80211_IFTYPE_AP &&\n\t\t    wdev->iftype != NL80211_IFTYPE_P2P_GO &&\n\t\t    wdev->iftype != NL80211_IFTYPE_AP_VLAN)) {\n\t\ttrace_cfg80211_return_bool(false);\n\t\treturn false;\n\t}\n\tret = __nl80211_unexpected_frame(dev,\n\t\t\t\t\t NL80211_CMD_UNEXPECTED_4ADDR_FRAME,\n\t\t\t\t\t addr, gfp);\n\ttrace_cfg80211_return_bool(ret);\n\treturn ret;\n}\nEXPORT_SYMBOL(cfg80211_rx_unexpected_4addr_frame);\n\nint nl80211_send_mgmt(struct cfg80211_registered_device *rdev,\n\t\t      struct wireless_dev *wdev, u32 nlportid,\n\t\t      int freq, int sig_dbm,\n\t\t      const u8 *buf, size_t len, u32 flags, gfp_t gfp)\n{\n\tstruct net_device *netdev = wdev->netdev;\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\tmsg = nlmsg_new(100 + len, gfp);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_FRAME);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    (netdev && nla_put_u32(msg, NL80211_ATTR_IFINDEX,\n\t\t\t\t\tnetdev->ifindex)) ||\n\t    nla_put_u64_64bit(msg, NL80211_ATTR_WDEV, wdev_id(wdev),\n\t\t\t      NL80211_ATTR_PAD) ||\n\t    nla_put_u32(msg, NL80211_ATTR_WIPHY_FREQ, KHZ_TO_MHZ(freq)) ||\n\t    nla_put_u32(msg, NL80211_ATTR_WIPHY_FREQ_OFFSET, freq % 1000) ||\n\t    (sig_dbm &&\n\t     nla_put_u32(msg, NL80211_ATTR_RX_SIGNAL_DBM, sig_dbm)) ||\n\t    nla_put(msg, NL80211_ATTR_FRAME, len, buf) ||\n\t    (flags &&\n\t     nla_put_u32(msg, NL80211_ATTR_RXMGMT_FLAGS, flags)))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\n\treturn genlmsg_unicast(wiphy_net(&rdev->wiphy), msg, nlportid);\n\n nla_put_failure:\n\tnlmsg_free(msg);\n\treturn -ENOBUFS;\n}\n\nstatic void nl80211_frame_tx_status(struct wireless_dev *wdev, u64 cookie,\n\t\t\t\t    const u8 *buf, size_t len, bool ack,\n\t\t\t\t    gfp_t gfp, enum nl80211_commands command)\n{\n\tstruct wiphy *wiphy = wdev->wiphy;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\tstruct net_device *netdev = wdev->netdev;\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\tif (command == NL80211_CMD_FRAME_TX_STATUS)\n\t\ttrace_cfg80211_mgmt_tx_status(wdev, cookie, ack);\n\telse\n\t\ttrace_cfg80211_control_port_tx_status(wdev, cookie, ack);\n\n\tmsg = nlmsg_new(100 + len, gfp);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, command);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    (netdev && nla_put_u32(msg, NL80211_ATTR_IFINDEX,\n\t\t\t\t   netdev->ifindex)) ||\n\t    nla_put_u64_64bit(msg, NL80211_ATTR_WDEV, wdev_id(wdev),\n\t\t\t      NL80211_ATTR_PAD) ||\n\t    nla_put(msg, NL80211_ATTR_FRAME, len, buf) ||\n\t    nla_put_u64_64bit(msg, NL80211_ATTR_COOKIE, cookie,\n\t\t\t      NL80211_ATTR_PAD) ||\n\t    (ack && nla_put_flag(msg, NL80211_ATTR_ACK)))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n\treturn;\n\nnla_put_failure:\n\tnlmsg_free(msg);\n}\n\nvoid cfg80211_control_port_tx_status(struct wireless_dev *wdev, u64 cookie,\n\t\t\t\t     const u8 *buf, size_t len, bool ack,\n\t\t\t\t     gfp_t gfp)\n{\n\tnl80211_frame_tx_status(wdev, cookie, buf, len, ack, gfp,\n\t\t\t\tNL80211_CMD_CONTROL_PORT_FRAME_TX_STATUS);\n}\nEXPORT_SYMBOL(cfg80211_control_port_tx_status);\n\nvoid cfg80211_mgmt_tx_status(struct wireless_dev *wdev, u64 cookie,\n\t\t\t     const u8 *buf, size_t len, bool ack, gfp_t gfp)\n{\n\tnl80211_frame_tx_status(wdev, cookie, buf, len, ack, gfp,\n\t\t\t\tNL80211_CMD_FRAME_TX_STATUS);\n}\nEXPORT_SYMBOL(cfg80211_mgmt_tx_status);\n\nstatic int __nl80211_rx_control_port(struct net_device *dev,\n\t\t\t\t     struct sk_buff *skb,\n\t\t\t\t     bool unencrypted, gfp_t gfp)\n{\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wdev->wiphy);\n\tstruct ethhdr *ehdr = eth_hdr(skb);\n\tconst u8 *addr = ehdr->h_source;\n\tu16 proto = be16_to_cpu(skb->protocol);\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\tstruct nlattr *frame;\n\n\tu32 nlportid = READ_ONCE(wdev->conn_owner_nlportid);\n\n\tif (!nlportid)\n\t\treturn -ENOENT;\n\n\tmsg = nlmsg_new(100 + skb->len, gfp);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_CONTROL_PORT_FRAME);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn -ENOBUFS;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, dev->ifindex) ||\n\t    nla_put_u64_64bit(msg, NL80211_ATTR_WDEV, wdev_id(wdev),\n\t\t\t      NL80211_ATTR_PAD) ||\n\t    nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, addr) ||\n\t    nla_put_u16(msg, NL80211_ATTR_CONTROL_PORT_ETHERTYPE, proto) ||\n\t    (unencrypted && nla_put_flag(msg,\n\t\t\t\t\t NL80211_ATTR_CONTROL_PORT_NO_ENCRYPT)))\n\t\tgoto nla_put_failure;\n\n\tframe = nla_reserve(msg, NL80211_ATTR_FRAME, skb->len);\n\tif (!frame)\n\t\tgoto nla_put_failure;\n\n\tskb_copy_bits(skb, 0, nla_data(frame), skb->len);\n\tgenlmsg_end(msg, hdr);\n\n\treturn genlmsg_unicast(wiphy_net(&rdev->wiphy), msg, nlportid);\n\n nla_put_failure:\n\tnlmsg_free(msg);\n\treturn -ENOBUFS;\n}\n\nbool cfg80211_rx_control_port(struct net_device *dev,\n\t\t\t      struct sk_buff *skb, bool unencrypted)\n{\n\tint ret;\n\n\ttrace_cfg80211_rx_control_port(dev, skb, unencrypted);\n\tret = __nl80211_rx_control_port(dev, skb, unencrypted, GFP_ATOMIC);\n\ttrace_cfg80211_return_bool(ret == 0);\n\treturn ret == 0;\n}\nEXPORT_SYMBOL(cfg80211_rx_control_port);\n\nstatic struct sk_buff *cfg80211_prepare_cqm(struct net_device *dev,\n\t\t\t\t\t    const char *mac, gfp_t gfp)\n{\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wdev->wiphy);\n\tstruct sk_buff *msg = nlmsg_new(NLMSG_DEFAULT_SIZE, gfp);\n\tvoid **cb;\n\n\tif (!msg)\n\t\treturn NULL;\n\n\tcb = (void **)msg->cb;\n\n\tcb[0] = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_NOTIFY_CQM);\n\tif (!cb[0]) {\n\t\tnlmsg_free(msg);\n\t\treturn NULL;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, dev->ifindex))\n\t\tgoto nla_put_failure;\n\n\tif (mac && nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, mac))\n\t\tgoto nla_put_failure;\n\n\tcb[1] = nla_nest_start_noflag(msg, NL80211_ATTR_CQM);\n\tif (!cb[1])\n\t\tgoto nla_put_failure;\n\n\tcb[2] = rdev;\n\n\treturn msg;\n nla_put_failure:\n\tnlmsg_free(msg);\n\treturn NULL;\n}\n\nstatic void cfg80211_send_cqm(struct sk_buff *msg, gfp_t gfp)\n{\n\tvoid **cb = (void **)msg->cb;\n\tstruct cfg80211_registered_device *rdev = cb[2];\n\n\tnla_nest_end(msg, cb[1]);\n\tgenlmsg_end(msg, cb[0]);\n\n\tmemset(msg->cb, 0, sizeof(msg->cb));\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n}\n\nvoid cfg80211_cqm_rssi_notify(struct net_device *dev,\n\t\t\t      enum nl80211_cqm_rssi_threshold_event rssi_event,\n\t\t\t      s32 rssi_level, gfp_t gfp)\n{\n\tstruct sk_buff *msg;\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wdev->wiphy);\n\n\ttrace_cfg80211_cqm_rssi_notify(dev, rssi_event, rssi_level);\n\n\tif (WARN_ON(rssi_event != NL80211_CQM_RSSI_THRESHOLD_EVENT_LOW &&\n\t\t    rssi_event != NL80211_CQM_RSSI_THRESHOLD_EVENT_HIGH))\n\t\treturn;\n\n\tif (wdev->cqm_config) {\n\t\twdev->cqm_config->last_rssi_event_value = rssi_level;\n\n\t\tcfg80211_cqm_rssi_update(rdev, dev);\n\n\t\tif (rssi_level == 0)\n\t\t\trssi_level = wdev->cqm_config->last_rssi_event_value;\n\t}\n\n\tmsg = cfg80211_prepare_cqm(dev, NULL, gfp);\n\tif (!msg)\n\t\treturn;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_CQM_RSSI_THRESHOLD_EVENT,\n\t\t\trssi_event))\n\t\tgoto nla_put_failure;\n\n\tif (rssi_level && nla_put_s32(msg, NL80211_ATTR_CQM_RSSI_LEVEL,\n\t\t\t\t      rssi_level))\n\t\tgoto nla_put_failure;\n\n\tcfg80211_send_cqm(msg, gfp);\n\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\nEXPORT_SYMBOL(cfg80211_cqm_rssi_notify);\n\nvoid cfg80211_cqm_txe_notify(struct net_device *dev,\n\t\t\t     const u8 *peer, u32 num_packets,\n\t\t\t     u32 rate, u32 intvl, gfp_t gfp)\n{\n\tstruct sk_buff *msg;\n\n\tmsg = cfg80211_prepare_cqm(dev, peer, gfp);\n\tif (!msg)\n\t\treturn;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_CQM_TXE_PKTS, num_packets))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_CQM_TXE_RATE, rate))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_CQM_TXE_INTVL, intvl))\n\t\tgoto nla_put_failure;\n\n\tcfg80211_send_cqm(msg, gfp);\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\nEXPORT_SYMBOL(cfg80211_cqm_txe_notify);\n\nvoid cfg80211_cqm_pktloss_notify(struct net_device *dev,\n\t\t\t\t const u8 *peer, u32 num_packets, gfp_t gfp)\n{\n\tstruct sk_buff *msg;\n\n\ttrace_cfg80211_cqm_pktloss_notify(dev, peer, num_packets);\n\n\tmsg = cfg80211_prepare_cqm(dev, peer, gfp);\n\tif (!msg)\n\t\treturn;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_CQM_PKT_LOSS_EVENT, num_packets))\n\t\tgoto nla_put_failure;\n\n\tcfg80211_send_cqm(msg, gfp);\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\nEXPORT_SYMBOL(cfg80211_cqm_pktloss_notify);\n\nvoid cfg80211_cqm_beacon_loss_notify(struct net_device *dev, gfp_t gfp)\n{\n\tstruct sk_buff *msg;\n\n\tmsg = cfg80211_prepare_cqm(dev, NULL, gfp);\n\tif (!msg)\n\t\treturn;\n\n\tif (nla_put_flag(msg, NL80211_ATTR_CQM_BEACON_LOSS_EVENT))\n\t\tgoto nla_put_failure;\n\n\tcfg80211_send_cqm(msg, gfp);\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\nEXPORT_SYMBOL(cfg80211_cqm_beacon_loss_notify);\n\nstatic void nl80211_gtk_rekey_notify(struct cfg80211_registered_device *rdev,\n\t\t\t\t     struct net_device *netdev, const u8 *bssid,\n\t\t\t\t     const u8 *replay_ctr, gfp_t gfp)\n{\n\tstruct sk_buff *msg;\n\tstruct nlattr *rekey_attr;\n\tvoid *hdr;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, gfp);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_SET_REKEY_OFFLOAD);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, netdev->ifindex) ||\n\t    nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, bssid))\n\t\tgoto nla_put_failure;\n\n\trekey_attr = nla_nest_start_noflag(msg, NL80211_ATTR_REKEY_DATA);\n\tif (!rekey_attr)\n\t\tgoto nla_put_failure;\n\n\tif (nla_put(msg, NL80211_REKEY_DATA_REPLAY_CTR,\n\t\t    NL80211_REPLAY_CTR_LEN, replay_ctr))\n\t\tgoto nla_put_failure;\n\n\tnla_nest_end(msg, rekey_attr);\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\n\nvoid cfg80211_gtk_rekey_notify(struct net_device *dev, const u8 *bssid,\n\t\t\t       const u8 *replay_ctr, gfp_t gfp)\n{\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct wiphy *wiphy = wdev->wiphy;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\n\ttrace_cfg80211_gtk_rekey_notify(dev, bssid);\n\tnl80211_gtk_rekey_notify(rdev, dev, bssid, replay_ctr, gfp);\n}\nEXPORT_SYMBOL(cfg80211_gtk_rekey_notify);\n\nstatic void\nnl80211_pmksa_candidate_notify(struct cfg80211_registered_device *rdev,\n\t\t\t       struct net_device *netdev, int index,\n\t\t\t       const u8 *bssid, bool preauth, gfp_t gfp)\n{\n\tstruct sk_buff *msg;\n\tstruct nlattr *attr;\n\tvoid *hdr;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, gfp);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_PMKSA_CANDIDATE);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, netdev->ifindex))\n\t\tgoto nla_put_failure;\n\n\tattr = nla_nest_start_noflag(msg, NL80211_ATTR_PMKSA_CANDIDATE);\n\tif (!attr)\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(msg, NL80211_PMKSA_CANDIDATE_INDEX, index) ||\n\t    nla_put(msg, NL80211_PMKSA_CANDIDATE_BSSID, ETH_ALEN, bssid) ||\n\t    (preauth &&\n\t     nla_put_flag(msg, NL80211_PMKSA_CANDIDATE_PREAUTH)))\n\t\tgoto nla_put_failure;\n\n\tnla_nest_end(msg, attr);\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\n\nvoid cfg80211_pmksa_candidate_notify(struct net_device *dev, int index,\n\t\t\t\t     const u8 *bssid, bool preauth, gfp_t gfp)\n{\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct wiphy *wiphy = wdev->wiphy;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\n\ttrace_cfg80211_pmksa_candidate_notify(dev, index, bssid, preauth);\n\tnl80211_pmksa_candidate_notify(rdev, dev, index, bssid, preauth, gfp);\n}\nEXPORT_SYMBOL(cfg80211_pmksa_candidate_notify);\n\nstatic void nl80211_ch_switch_notify(struct cfg80211_registered_device *rdev,\n\t\t\t\t     struct net_device *netdev,\n\t\t\t\t     struct cfg80211_chan_def *chandef,\n\t\t\t\t     gfp_t gfp,\n\t\t\t\t     enum nl80211_commands notif,\n\t\t\t\t     u8 count, bool quiet)\n{\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, gfp);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, notif);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_IFINDEX, netdev->ifindex))\n\t\tgoto nla_put_failure;\n\n\tif (nl80211_send_chandef(msg, chandef))\n\t\tgoto nla_put_failure;\n\n\tif (notif == NL80211_CMD_CH_SWITCH_STARTED_NOTIFY) {\n\t\tif (nla_put_u32(msg, NL80211_ATTR_CH_SWITCH_COUNT, count))\n\t\t\tgoto nla_put_failure;\n\t\tif (quiet &&\n\t\t    nla_put_flag(msg, NL80211_ATTR_CH_SWITCH_BLOCK_TX))\n\t\t\tgoto nla_put_failure;\n\t}\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\n\nvoid cfg80211_ch_switch_notify(struct net_device *dev,\n\t\t\t       struct cfg80211_chan_def *chandef)\n{\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct wiphy *wiphy = wdev->wiphy;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\n\tASSERT_WDEV_LOCK(wdev);\n\n\ttrace_cfg80211_ch_switch_notify(dev, chandef);\n\n\twdev->chandef = *chandef;\n\twdev->preset_chandef = *chandef;\n\n\tif (wdev->iftype == NL80211_IFTYPE_STATION &&\n\t    !WARN_ON(!wdev->current_bss))\n\t\tcfg80211_update_assoc_bss_entry(wdev, chandef->chan);\n\n\tcfg80211_sched_dfs_chan_update(rdev);\n\n\tnl80211_ch_switch_notify(rdev, dev, chandef, GFP_KERNEL,\n\t\t\t\t NL80211_CMD_CH_SWITCH_NOTIFY, 0, false);\n}\nEXPORT_SYMBOL(cfg80211_ch_switch_notify);\n\nvoid cfg80211_ch_switch_started_notify(struct net_device *dev,\n\t\t\t\t       struct cfg80211_chan_def *chandef,\n\t\t\t\t       u8 count, bool quiet)\n{\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct wiphy *wiphy = wdev->wiphy;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\n\ttrace_cfg80211_ch_switch_started_notify(dev, chandef);\n\n\tnl80211_ch_switch_notify(rdev, dev, chandef, GFP_KERNEL,\n\t\t\t\t NL80211_CMD_CH_SWITCH_STARTED_NOTIFY,\n\t\t\t\t count, quiet);\n}\nEXPORT_SYMBOL(cfg80211_ch_switch_started_notify);\n\nvoid\nnl80211_radar_notify(struct cfg80211_registered_device *rdev,\n\t\t     const struct cfg80211_chan_def *chandef,\n\t\t     enum nl80211_radar_event event,\n\t\t     struct net_device *netdev, gfp_t gfp)\n{\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, gfp);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_RADAR_DETECT);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx))\n\t\tgoto nla_put_failure;\n\n\t/* NOP and radar events don't need a netdev parameter */\n\tif (netdev) {\n\t\tstruct wireless_dev *wdev = netdev->ieee80211_ptr;\n\n\t\tif (nla_put_u32(msg, NL80211_ATTR_IFINDEX, netdev->ifindex) ||\n\t\t    nla_put_u64_64bit(msg, NL80211_ATTR_WDEV, wdev_id(wdev),\n\t\t\t\t      NL80211_ATTR_PAD))\n\t\t\tgoto nla_put_failure;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_RADAR_EVENT, event))\n\t\tgoto nla_put_failure;\n\n\tif (nl80211_send_chandef(msg, chandef))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\n\nvoid cfg80211_sta_opmode_change_notify(struct net_device *dev, const u8 *mac,\n\t\t\t\t       struct sta_opmode_info *sta_opmode,\n\t\t\t\t       gfp_t gfp)\n{\n\tstruct sk_buff *msg;\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wdev->wiphy);\n\tvoid *hdr;\n\n\tif (WARN_ON(!mac))\n\t\treturn;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, gfp);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_STA_OPMODE_CHANGED);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_IFINDEX, dev->ifindex))\n\t\tgoto nla_put_failure;\n\n\tif (nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, mac))\n\t\tgoto nla_put_failure;\n\n\tif ((sta_opmode->changed & STA_OPMODE_SMPS_MODE_CHANGED) &&\n\t    nla_put_u8(msg, NL80211_ATTR_SMPS_MODE, sta_opmode->smps_mode))\n\t\tgoto nla_put_failure;\n\n\tif ((sta_opmode->changed & STA_OPMODE_MAX_BW_CHANGED) &&\n\t    nla_put_u32(msg, NL80211_ATTR_CHANNEL_WIDTH, sta_opmode->bw))\n\t\tgoto nla_put_failure;\n\n\tif ((sta_opmode->changed & STA_OPMODE_N_SS_CHANGED) &&\n\t    nla_put_u8(msg, NL80211_ATTR_NSS, sta_opmode->rx_nss))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n\n\treturn;\n\nnla_put_failure:\n\tnlmsg_free(msg);\n}\nEXPORT_SYMBOL(cfg80211_sta_opmode_change_notify);\n\nvoid cfg80211_probe_status(struct net_device *dev, const u8 *addr,\n\t\t\t   u64 cookie, bool acked, s32 ack_signal,\n\t\t\t   bool is_valid_ack_signal, gfp_t gfp)\n{\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wdev->wiphy);\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\ttrace_cfg80211_probe_status(dev, addr, cookie, acked);\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, gfp);\n\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_PROBE_CLIENT);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, dev->ifindex) ||\n\t    nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, addr) ||\n\t    nla_put_u64_64bit(msg, NL80211_ATTR_COOKIE, cookie,\n\t\t\t      NL80211_ATTR_PAD) ||\n\t    (acked && nla_put_flag(msg, NL80211_ATTR_ACK)) ||\n\t    (is_valid_ack_signal && nla_put_s32(msg, NL80211_ATTR_ACK_SIGNAL,\n\t\t\t\t\t\tack_signal)))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\nEXPORT_SYMBOL(cfg80211_probe_status);\n\nvoid cfg80211_report_obss_beacon_khz(struct wiphy *wiphy, const u8 *frame,\n\t\t\t\t     size_t len, int freq, int sig_dbm)\n{\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\tstruct cfg80211_beacon_registration *reg;\n\n\ttrace_cfg80211_report_obss_beacon(wiphy, frame, len, freq, sig_dbm);\n\n\tspin_lock_bh(&rdev->beacon_registrations_lock);\n\tlist_for_each_entry(reg, &rdev->beacon_registrations, list) {\n\t\tmsg = nlmsg_new(len + 100, GFP_ATOMIC);\n\t\tif (!msg) {\n\t\t\tspin_unlock_bh(&rdev->beacon_registrations_lock);\n\t\t\treturn;\n\t\t}\n\n\t\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_FRAME);\n\t\tif (!hdr)\n\t\t\tgoto nla_put_failure;\n\n\t\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t\t    (freq &&\n\t\t     (nla_put_u32(msg, NL80211_ATTR_WIPHY_FREQ,\n\t\t\t\t  KHZ_TO_MHZ(freq)) ||\n\t\t      nla_put_u32(msg, NL80211_ATTR_WIPHY_FREQ_OFFSET,\n\t\t\t\t  freq % 1000))) ||\n\t\t    (sig_dbm &&\n\t\t     nla_put_u32(msg, NL80211_ATTR_RX_SIGNAL_DBM, sig_dbm)) ||\n\t\t    nla_put(msg, NL80211_ATTR_FRAME, len, frame))\n\t\t\tgoto nla_put_failure;\n\n\t\tgenlmsg_end(msg, hdr);\n\n\t\tgenlmsg_unicast(wiphy_net(&rdev->wiphy), msg, reg->nlportid);\n\t}\n\tspin_unlock_bh(&rdev->beacon_registrations_lock);\n\treturn;\n\n nla_put_failure:\n\tspin_unlock_bh(&rdev->beacon_registrations_lock);\n\tnlmsg_free(msg);\n}\nEXPORT_SYMBOL(cfg80211_report_obss_beacon_khz);\n\n#ifdef CONFIG_PM\nstatic int cfg80211_net_detect_results(struct sk_buff *msg,\n\t\t\t\t       struct cfg80211_wowlan_wakeup *wakeup)\n{\n\tstruct cfg80211_wowlan_nd_info *nd = wakeup->net_detect;\n\tstruct nlattr *nl_results, *nl_match, *nl_freqs;\n\tint i, j;\n\n\tnl_results = nla_nest_start_noflag(msg,\n\t\t\t\t\t   NL80211_WOWLAN_TRIG_NET_DETECT_RESULTS);\n\tif (!nl_results)\n\t\treturn -EMSGSIZE;\n\n\tfor (i = 0; i < nd->n_matches; i++) {\n\t\tstruct cfg80211_wowlan_nd_match *match = nd->matches[i];\n\n\t\tnl_match = nla_nest_start_noflag(msg, i);\n\t\tif (!nl_match)\n\t\t\tbreak;\n\n\t\t/* The SSID attribute is optional in nl80211, but for\n\t\t * simplicity reasons it's always present in the\n\t\t * cfg80211 structure.  If a driver can't pass the\n\t\t * SSID, that needs to be changed.  A zero length SSID\n\t\t * is still a valid SSID (wildcard), so it cannot be\n\t\t * used for this purpose.\n\t\t */\n\t\tif (nla_put(msg, NL80211_ATTR_SSID, match->ssid.ssid_len,\n\t\t\t    match->ssid.ssid)) {\n\t\t\tnla_nest_cancel(msg, nl_match);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (match->n_channels) {\n\t\t\tnl_freqs = nla_nest_start_noflag(msg,\n\t\t\t\t\t\t\t NL80211_ATTR_SCAN_FREQUENCIES);\n\t\t\tif (!nl_freqs) {\n\t\t\t\tnla_nest_cancel(msg, nl_match);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tfor (j = 0; j < match->n_channels; j++) {\n\t\t\t\tif (nla_put_u32(msg, j, match->channels[j])) {\n\t\t\t\t\tnla_nest_cancel(msg, nl_freqs);\n\t\t\t\t\tnla_nest_cancel(msg, nl_match);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tnla_nest_end(msg, nl_freqs);\n\t\t}\n\n\t\tnla_nest_end(msg, nl_match);\n\t}\n\nout:\n\tnla_nest_end(msg, nl_results);\n\treturn 0;\n}\n\nvoid cfg80211_report_wowlan_wakeup(struct wireless_dev *wdev,\n\t\t\t\t   struct cfg80211_wowlan_wakeup *wakeup,\n\t\t\t\t   gfp_t gfp)\n{\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wdev->wiphy);\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\tint size = 200;\n\n\ttrace_cfg80211_report_wowlan_wakeup(wdev->wiphy, wdev, wakeup);\n\n\tif (wakeup)\n\t\tsize += wakeup->packet_present_len;\n\n\tmsg = nlmsg_new(size, gfp);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_SET_WOWLAN);\n\tif (!hdr)\n\t\tgoto free_msg;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u64_64bit(msg, NL80211_ATTR_WDEV, wdev_id(wdev),\n\t\t\t      NL80211_ATTR_PAD))\n\t\tgoto free_msg;\n\n\tif (wdev->netdev && nla_put_u32(msg, NL80211_ATTR_IFINDEX,\n\t\t\t\t\twdev->netdev->ifindex))\n\t\tgoto free_msg;\n\n\tif (wakeup) {\n\t\tstruct nlattr *reasons;\n\n\t\treasons = nla_nest_start_noflag(msg,\n\t\t\t\t\t\tNL80211_ATTR_WOWLAN_TRIGGERS);\n\t\tif (!reasons)\n\t\t\tgoto free_msg;\n\n\t\tif (wakeup->disconnect &&\n\t\t    nla_put_flag(msg, NL80211_WOWLAN_TRIG_DISCONNECT))\n\t\t\tgoto free_msg;\n\t\tif (wakeup->magic_pkt &&\n\t\t    nla_put_flag(msg, NL80211_WOWLAN_TRIG_MAGIC_PKT))\n\t\t\tgoto free_msg;\n\t\tif (wakeup->gtk_rekey_failure &&\n\t\t    nla_put_flag(msg, NL80211_WOWLAN_TRIG_GTK_REKEY_FAILURE))\n\t\t\tgoto free_msg;\n\t\tif (wakeup->eap_identity_req &&\n\t\t    nla_put_flag(msg, NL80211_WOWLAN_TRIG_EAP_IDENT_REQUEST))\n\t\t\tgoto free_msg;\n\t\tif (wakeup->four_way_handshake &&\n\t\t    nla_put_flag(msg, NL80211_WOWLAN_TRIG_4WAY_HANDSHAKE))\n\t\t\tgoto free_msg;\n\t\tif (wakeup->rfkill_release &&\n\t\t    nla_put_flag(msg, NL80211_WOWLAN_TRIG_RFKILL_RELEASE))\n\t\t\tgoto free_msg;\n\n\t\tif (wakeup->pattern_idx >= 0 &&\n\t\t    nla_put_u32(msg, NL80211_WOWLAN_TRIG_PKT_PATTERN,\n\t\t\t\twakeup->pattern_idx))\n\t\t\tgoto free_msg;\n\n\t\tif (wakeup->tcp_match &&\n\t\t    nla_put_flag(msg, NL80211_WOWLAN_TRIG_WAKEUP_TCP_MATCH))\n\t\t\tgoto free_msg;\n\n\t\tif (wakeup->tcp_connlost &&\n\t\t    nla_put_flag(msg, NL80211_WOWLAN_TRIG_WAKEUP_TCP_CONNLOST))\n\t\t\tgoto free_msg;\n\n\t\tif (wakeup->tcp_nomoretokens &&\n\t\t    nla_put_flag(msg,\n\t\t\t\t NL80211_WOWLAN_TRIG_WAKEUP_TCP_NOMORETOKENS))\n\t\t\tgoto free_msg;\n\n\t\tif (wakeup->packet) {\n\t\t\tu32 pkt_attr = NL80211_WOWLAN_TRIG_WAKEUP_PKT_80211;\n\t\t\tu32 len_attr = NL80211_WOWLAN_TRIG_WAKEUP_PKT_80211_LEN;\n\n\t\t\tif (!wakeup->packet_80211) {\n\t\t\t\tpkt_attr =\n\t\t\t\t\tNL80211_WOWLAN_TRIG_WAKEUP_PKT_8023;\n\t\t\t\tlen_attr =\n\t\t\t\t\tNL80211_WOWLAN_TRIG_WAKEUP_PKT_8023_LEN;\n\t\t\t}\n\n\t\t\tif (wakeup->packet_len &&\n\t\t\t    nla_put_u32(msg, len_attr, wakeup->packet_len))\n\t\t\t\tgoto free_msg;\n\n\t\t\tif (nla_put(msg, pkt_attr, wakeup->packet_present_len,\n\t\t\t\t    wakeup->packet))\n\t\t\t\tgoto free_msg;\n\t\t}\n\n\t\tif (wakeup->net_detect &&\n\t\t    cfg80211_net_detect_results(msg, wakeup))\n\t\t\t\tgoto free_msg;\n\n\t\tnla_nest_end(msg, reasons);\n\t}\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n\treturn;\n\n free_msg:\n\tnlmsg_free(msg);\n}\nEXPORT_SYMBOL(cfg80211_report_wowlan_wakeup);\n#endif\n\nvoid cfg80211_tdls_oper_request(struct net_device *dev, const u8 *peer,\n\t\t\t\tenum nl80211_tdls_operation oper,\n\t\t\t\tu16 reason_code, gfp_t gfp)\n{\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wdev->wiphy);\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\ttrace_cfg80211_tdls_oper_request(wdev->wiphy, dev, peer, oper,\n\t\t\t\t\t reason_code);\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, gfp);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_TDLS_OPER);\n\tif (!hdr) {\n\t\tnlmsg_free(msg);\n\t\treturn;\n\t}\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, dev->ifindex) ||\n\t    nla_put_u8(msg, NL80211_ATTR_TDLS_OPERATION, oper) ||\n\t    nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, peer) ||\n\t    (reason_code > 0 &&\n\t     nla_put_u16(msg, NL80211_ATTR_REASON_CODE, reason_code)))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\nEXPORT_SYMBOL(cfg80211_tdls_oper_request);\n\nstatic int nl80211_netlink_notify(struct notifier_block * nb,\n\t\t\t\t  unsigned long state,\n\t\t\t\t  void *_notify)\n{\n\tstruct netlink_notify *notify = _notify;\n\tstruct cfg80211_registered_device *rdev;\n\tstruct wireless_dev *wdev;\n\tstruct cfg80211_beacon_registration *reg, *tmp;\n\n\tif (state != NETLINK_URELEASE || notify->protocol != NETLINK_GENERIC)\n\t\treturn NOTIFY_DONE;\n\n\trcu_read_lock();\n\n\tlist_for_each_entry_rcu(rdev, &cfg80211_rdev_list, list) {\n\t\tstruct cfg80211_sched_scan_request *sched_scan_req;\n\n\t\tlist_for_each_entry_rcu(sched_scan_req,\n\t\t\t\t\t&rdev->sched_scan_req_list,\n\t\t\t\t\tlist) {\n\t\t\tif (sched_scan_req->owner_nlportid == notify->portid) {\n\t\t\t\tsched_scan_req->nl_owner_dead = true;\n\t\t\t\tschedule_work(&rdev->sched_scan_stop_wk);\n\t\t\t}\n\t\t}\n\n\t\tlist_for_each_entry_rcu(wdev, &rdev->wiphy.wdev_list, list) {\n\t\t\tcfg80211_mlme_unregister_socket(wdev, notify->portid);\n\n\t\t\tif (wdev->owner_nlportid == notify->portid) {\n\t\t\t\twdev->nl_owner_dead = true;\n\t\t\t\tschedule_work(&rdev->destroy_work);\n\t\t\t} else if (wdev->conn_owner_nlportid == notify->portid) {\n\t\t\t\tschedule_work(&wdev->disconnect_wk);\n\t\t\t}\n\n\t\t\tcfg80211_release_pmsr(wdev, notify->portid);\n\t\t}\n\n\t\tspin_lock_bh(&rdev->beacon_registrations_lock);\n\t\tlist_for_each_entry_safe(reg, tmp, &rdev->beacon_registrations,\n\t\t\t\t\t list) {\n\t\t\tif (reg->nlportid == notify->portid) {\n\t\t\t\tlist_del(&reg->list);\n\t\t\t\tkfree(reg);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_bh(&rdev->beacon_registrations_lock);\n\t}\n\n\trcu_read_unlock();\n\n\t/*\n\t * It is possible that the user space process that is controlling the\n\t * indoor setting disappeared, so notify the regulatory core.\n\t */\n\tregulatory_netlink_notify(notify->portid);\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block nl80211_netlink_notifier = {\n\t.notifier_call = nl80211_netlink_notify,\n};\n\nvoid cfg80211_ft_event(struct net_device *netdev,\n\t\t       struct cfg80211_ft_event_params *ft_event)\n{\n\tstruct wiphy *wiphy = netdev->ieee80211_ptr->wiphy;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\ttrace_cfg80211_ft_event(wiphy, netdev, ft_event);\n\n\tif (!ft_event->target_ap)\n\t\treturn;\n\n\tmsg = nlmsg_new(100 + ft_event->ies_len + ft_event->ric_ies_len,\n\t\t\tGFP_KERNEL);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_FT_EVENT);\n\tif (!hdr)\n\t\tgoto out;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, netdev->ifindex) ||\n\t    nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, ft_event->target_ap))\n\t\tgoto out;\n\n\tif (ft_event->ies &&\n\t    nla_put(msg, NL80211_ATTR_IE, ft_event->ies_len, ft_event->ies))\n\t\tgoto out;\n\tif (ft_event->ric_ies &&\n\t    nla_put(msg, NL80211_ATTR_IE_RIC, ft_event->ric_ies_len,\n\t\t    ft_event->ric_ies))\n\t\tgoto out;\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, GFP_KERNEL);\n\treturn;\n out:\n\tnlmsg_free(msg);\n}\nEXPORT_SYMBOL(cfg80211_ft_event);\n\nvoid cfg80211_crit_proto_stopped(struct wireless_dev *wdev, gfp_t gfp)\n{\n\tstruct cfg80211_registered_device *rdev;\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\tu32 nlportid;\n\n\trdev = wiphy_to_rdev(wdev->wiphy);\n\tif (!rdev->crit_proto_nlportid)\n\t\treturn;\n\n\tnlportid = rdev->crit_proto_nlportid;\n\trdev->crit_proto_nlportid = 0;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, gfp);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_CRIT_PROTOCOL_STOP);\n\tif (!hdr)\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u64_64bit(msg, NL80211_ATTR_WDEV, wdev_id(wdev),\n\t\t\t      NL80211_ATTR_PAD))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_unicast(wiphy_net(&rdev->wiphy), msg, nlportid);\n\treturn;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n}\nEXPORT_SYMBOL(cfg80211_crit_proto_stopped);\n\nvoid nl80211_send_ap_stopped(struct wireless_dev *wdev)\n{\n\tstruct wiphy *wiphy = wdev->wiphy;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_STOP_AP);\n\tif (!hdr)\n\t\tgoto out;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, wdev->netdev->ifindex) ||\n\t    nla_put_u64_64bit(msg, NL80211_ATTR_WDEV, wdev_id(wdev),\n\t\t\t      NL80211_ATTR_PAD))\n\t\tgoto out;\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, GFP_KERNEL);\n\treturn;\n out:\n\tnlmsg_free(msg);\n}\n\nint cfg80211_external_auth_request(struct net_device *dev,\n\t\t\t\t   struct cfg80211_external_auth_params *params,\n\t\t\t\t   gfp_t gfp)\n{\n\tstruct wireless_dev *wdev = dev->ieee80211_ptr;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wdev->wiphy);\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\tif (!wdev->conn_owner_nlportid)\n\t\treturn -EINVAL;\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, gfp);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_EXTERNAL_AUTH);\n\tif (!hdr)\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, dev->ifindex) ||\n\t    nla_put_u32(msg, NL80211_ATTR_AKM_SUITES, params->key_mgmt_suite) ||\n\t    nla_put_u32(msg, NL80211_ATTR_EXTERNAL_AUTH_ACTION,\n\t\t\tparams->action) ||\n\t    nla_put(msg, NL80211_ATTR_BSSID, ETH_ALEN, params->bssid) ||\n\t    nla_put(msg, NL80211_ATTR_SSID, params->ssid.ssid_len,\n\t\t    params->ssid.ssid))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\tgenlmsg_unicast(wiphy_net(&rdev->wiphy), msg,\n\t\t\twdev->conn_owner_nlportid);\n\treturn 0;\n\n nla_put_failure:\n\tnlmsg_free(msg);\n\treturn -ENOBUFS;\n}\nEXPORT_SYMBOL(cfg80211_external_auth_request);\n\nvoid cfg80211_update_owe_info_event(struct net_device *netdev,\n\t\t\t\t    struct cfg80211_update_owe_info *owe_info,\n\t\t\t\t    gfp_t gfp)\n{\n\tstruct wiphy *wiphy = netdev->ieee80211_ptr->wiphy;\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\tstruct sk_buff *msg;\n\tvoid *hdr;\n\n\ttrace_cfg80211_update_owe_info_event(wiphy, netdev, owe_info);\n\n\tmsg = nlmsg_new(NLMSG_DEFAULT_SIZE, gfp);\n\tif (!msg)\n\t\treturn;\n\n\thdr = nl80211hdr_put(msg, 0, 0, 0, NL80211_CMD_UPDATE_OWE_INFO);\n\tif (!hdr)\n\t\tgoto nla_put_failure;\n\n\tif (nla_put_u32(msg, NL80211_ATTR_WIPHY, rdev->wiphy_idx) ||\n\t    nla_put_u32(msg, NL80211_ATTR_IFINDEX, netdev->ifindex) ||\n\t    nla_put(msg, NL80211_ATTR_MAC, ETH_ALEN, owe_info->peer))\n\t\tgoto nla_put_failure;\n\n\tif (!owe_info->ie_len ||\n\t    nla_put(msg, NL80211_ATTR_IE, owe_info->ie_len, owe_info->ie))\n\t\tgoto nla_put_failure;\n\n\tgenlmsg_end(msg, hdr);\n\n\tgenlmsg_multicast_netns(&nl80211_fam, wiphy_net(&rdev->wiphy), msg, 0,\n\t\t\t\tNL80211_MCGRP_MLME, gfp);\n\treturn;\n\nnla_put_failure:\n\tgenlmsg_cancel(msg, hdr);\n\tnlmsg_free(msg);\n}\nEXPORT_SYMBOL(cfg80211_update_owe_info_event);\n\n/* initialisation/exit functions */\n\nint __init nl80211_init(void)\n{\n\tint err;\n\n\terr = genl_register_family(&nl80211_fam);\n\tif (err)\n\t\treturn err;\n\n\terr = netlink_register_notifier(&nl80211_netlink_notifier);\n\tif (err)\n\t\tgoto err_out;\n\n\treturn 0;\n err_out:\n\tgenl_unregister_family(&nl80211_fam);\n\treturn err;\n}\n\nvoid nl80211_exit(void)\n{\n\tnetlink_unregister_notifier(&nl80211_netlink_notifier);\n\tgenl_unregister_family(&nl80211_fam);\n}\n"}, "21": {"id": 21, "path": "/src/include/net/cfg80211.h", "content": "/* SPDX-License-Identifier: GPL-2.0-only */\n#ifndef __NET_CFG80211_H\n#define __NET_CFG80211_H\n/*\n * 802.11 device and configuration interface\n *\n * Copyright 2006-2010\tJohannes Berg <johannes@sipsolutions.net>\n * Copyright 2013-2014 Intel Mobile Communications GmbH\n * Copyright 2015-2017\tIntel Deutschland GmbH\n * Copyright (C) 2018-2020 Intel Corporation\n */\n\n#include <linux/ethtool.h>\n#include <linux/netdevice.h>\n#include <linux/debugfs.h>\n#include <linux/list.h>\n#include <linux/bug.h>\n#include <linux/netlink.h>\n#include <linux/skbuff.h>\n#include <linux/nl80211.h>\n#include <linux/if_ether.h>\n#include <linux/ieee80211.h>\n#include <linux/net.h>\n#include <net/regulatory.h>\n\n/**\n * DOC: Introduction\n *\n * cfg80211 is the configuration API for 802.11 devices in Linux. It bridges\n * userspace and drivers, and offers some utility functionality associated\n * with 802.11. cfg80211 must, directly or indirectly via mac80211, be used\n * by all modern wireless drivers in Linux, so that they offer a consistent\n * API through nl80211. For backward compatibility, cfg80211 also offers\n * wireless extensions to userspace, but hides them from drivers completely.\n *\n * Additionally, cfg80211 contains code to help enforce regulatory spectrum\n * use restrictions.\n */\n\n\n/**\n * DOC: Device registration\n *\n * In order for a driver to use cfg80211, it must register the hardware device\n * with cfg80211. This happens through a number of hardware capability structs\n * described below.\n *\n * The fundamental structure for each device is the 'wiphy', of which each\n * instance describes a physical wireless device connected to the system. Each\n * such wiphy can have zero, one, or many virtual interfaces associated with\n * it, which need to be identified as such by pointing the network interface's\n * @ieee80211_ptr pointer to a &struct wireless_dev which further describes\n * the wireless part of the interface, normally this struct is embedded in the\n * network interface's private data area. Drivers can optionally allow creating\n * or destroying virtual interfaces on the fly, but without at least one or the\n * ability to create some the wireless device isn't useful.\n *\n * Each wiphy structure contains device capability information, and also has\n * a pointer to the various operations the driver offers. The definitions and\n * structures here describe these capabilities in detail.\n */\n\nstruct wiphy;\n\n/*\n * wireless hardware capability structures\n */\n\n/**\n * enum ieee80211_channel_flags - channel flags\n *\n * Channel flags set by the regulatory control code.\n *\n * @IEEE80211_CHAN_DISABLED: This channel is disabled.\n * @IEEE80211_CHAN_NO_IR: do not initiate radiation, this includes\n *\tsending probe requests or beaconing.\n * @IEEE80211_CHAN_RADAR: Radar detection is required on this channel.\n * @IEEE80211_CHAN_NO_HT40PLUS: extension channel above this channel\n *\tis not permitted.\n * @IEEE80211_CHAN_NO_HT40MINUS: extension channel below this channel\n *\tis not permitted.\n * @IEEE80211_CHAN_NO_OFDM: OFDM is not allowed on this channel.\n * @IEEE80211_CHAN_NO_80MHZ: If the driver supports 80 MHz on the band,\n *\tthis flag indicates that an 80 MHz channel cannot use this\n *\tchannel as the control or any of the secondary channels.\n *\tThis may be due to the driver or due to regulatory bandwidth\n *\trestrictions.\n * @IEEE80211_CHAN_NO_160MHZ: If the driver supports 160 MHz on the band,\n *\tthis flag indicates that an 160 MHz channel cannot use this\n *\tchannel as the control or any of the secondary channels.\n *\tThis may be due to the driver or due to regulatory bandwidth\n *\trestrictions.\n * @IEEE80211_CHAN_INDOOR_ONLY: see %NL80211_FREQUENCY_ATTR_INDOOR_ONLY\n * @IEEE80211_CHAN_IR_CONCURRENT: see %NL80211_FREQUENCY_ATTR_IR_CONCURRENT\n * @IEEE80211_CHAN_NO_20MHZ: 20 MHz bandwidth is not permitted\n *\ton this channel.\n * @IEEE80211_CHAN_NO_10MHZ: 10 MHz bandwidth is not permitted\n *\ton this channel.\n * @IEEE80211_CHAN_NO_HE: HE operation is not permitted on this channel.\n * @IEEE80211_CHAN_1MHZ: 1 MHz bandwidth is permitted\n *\ton this channel.\n * @IEEE80211_CHAN_2MHZ: 2 MHz bandwidth is permitted\n *\ton this channel.\n * @IEEE80211_CHAN_4MHZ: 4 MHz bandwidth is permitted\n *\ton this channel.\n * @IEEE80211_CHAN_8MHZ: 8 MHz bandwidth is permitted\n *\ton this channel.\n * @IEEE80211_CHAN_16MHZ: 16 MHz bandwidth is permitted\n *\ton this channel.\n *\n */\nenum ieee80211_channel_flags {\n\tIEEE80211_CHAN_DISABLED\t\t= 1<<0,\n\tIEEE80211_CHAN_NO_IR\t\t= 1<<1,\n\t/* hole at 1<<2 */\n\tIEEE80211_CHAN_RADAR\t\t= 1<<3,\n\tIEEE80211_CHAN_NO_HT40PLUS\t= 1<<4,\n\tIEEE80211_CHAN_NO_HT40MINUS\t= 1<<5,\n\tIEEE80211_CHAN_NO_OFDM\t\t= 1<<6,\n\tIEEE80211_CHAN_NO_80MHZ\t\t= 1<<7,\n\tIEEE80211_CHAN_NO_160MHZ\t= 1<<8,\n\tIEEE80211_CHAN_INDOOR_ONLY\t= 1<<9,\n\tIEEE80211_CHAN_IR_CONCURRENT\t= 1<<10,\n\tIEEE80211_CHAN_NO_20MHZ\t\t= 1<<11,\n\tIEEE80211_CHAN_NO_10MHZ\t\t= 1<<12,\n\tIEEE80211_CHAN_NO_HE\t\t= 1<<13,\n\tIEEE80211_CHAN_1MHZ\t\t= 1<<14,\n\tIEEE80211_CHAN_2MHZ\t\t= 1<<15,\n\tIEEE80211_CHAN_4MHZ\t\t= 1<<16,\n\tIEEE80211_CHAN_8MHZ\t\t= 1<<17,\n\tIEEE80211_CHAN_16MHZ\t\t= 1<<18,\n};\n\n#define IEEE80211_CHAN_NO_HT40 \\\n\t(IEEE80211_CHAN_NO_HT40PLUS | IEEE80211_CHAN_NO_HT40MINUS)\n\n#define IEEE80211_DFS_MIN_CAC_TIME_MS\t\t60000\n#define IEEE80211_DFS_MIN_NOP_TIME_MS\t\t(30 * 60 * 1000)\n\n/**\n * struct ieee80211_channel - channel definition\n *\n * This structure describes a single channel for use\n * with cfg80211.\n *\n * @center_freq: center frequency in MHz\n * @freq_offset: offset from @center_freq, in KHz\n * @hw_value: hardware-specific value for the channel\n * @flags: channel flags from &enum ieee80211_channel_flags.\n * @orig_flags: channel flags at registration time, used by regulatory\n *\tcode to support devices with additional restrictions\n * @band: band this channel belongs to.\n * @max_antenna_gain: maximum antenna gain in dBi\n * @max_power: maximum transmission power (in dBm)\n * @max_reg_power: maximum regulatory transmission power (in dBm)\n * @beacon_found: helper to regulatory code to indicate when a beacon\n *\thas been found on this channel. Use regulatory_hint_found_beacon()\n *\tto enable this, this is useful only on 5 GHz band.\n * @orig_mag: internal use\n * @orig_mpwr: internal use\n * @dfs_state: current state of this channel. Only relevant if radar is required\n *\ton this channel.\n * @dfs_state_entered: timestamp (jiffies) when the dfs state was entered.\n * @dfs_cac_ms: DFS CAC time in milliseconds, this is valid for DFS channels.\n */\nstruct ieee80211_channel {\n\tenum nl80211_band band;\n\tu32 center_freq;\n\tu16 freq_offset;\n\tu16 hw_value;\n\tu32 flags;\n\tint max_antenna_gain;\n\tint max_power;\n\tint max_reg_power;\n\tbool beacon_found;\n\tu32 orig_flags;\n\tint orig_mag, orig_mpwr;\n\tenum nl80211_dfs_state dfs_state;\n\tunsigned long dfs_state_entered;\n\tunsigned int dfs_cac_ms;\n};\n\n/**\n * enum ieee80211_rate_flags - rate flags\n *\n * Hardware/specification flags for rates. These are structured\n * in a way that allows using the same bitrate structure for\n * different bands/PHY modes.\n *\n * @IEEE80211_RATE_SHORT_PREAMBLE: Hardware can send with short\n *\tpreamble on this bitrate; only relevant in 2.4GHz band and\n *\twith CCK rates.\n * @IEEE80211_RATE_MANDATORY_A: This bitrate is a mandatory rate\n *\twhen used with 802.11a (on the 5 GHz band); filled by the\n *\tcore code when registering the wiphy.\n * @IEEE80211_RATE_MANDATORY_B: This bitrate is a mandatory rate\n *\twhen used with 802.11b (on the 2.4 GHz band); filled by the\n *\tcore code when registering the wiphy.\n * @IEEE80211_RATE_MANDATORY_G: This bitrate is a mandatory rate\n *\twhen used with 802.11g (on the 2.4 GHz band); filled by the\n *\tcore code when registering the wiphy.\n * @IEEE80211_RATE_ERP_G: This is an ERP rate in 802.11g mode.\n * @IEEE80211_RATE_SUPPORTS_5MHZ: Rate can be used in 5 MHz mode\n * @IEEE80211_RATE_SUPPORTS_10MHZ: Rate can be used in 10 MHz mode\n */\nenum ieee80211_rate_flags {\n\tIEEE80211_RATE_SHORT_PREAMBLE\t= 1<<0,\n\tIEEE80211_RATE_MANDATORY_A\t= 1<<1,\n\tIEEE80211_RATE_MANDATORY_B\t= 1<<2,\n\tIEEE80211_RATE_MANDATORY_G\t= 1<<3,\n\tIEEE80211_RATE_ERP_G\t\t= 1<<4,\n\tIEEE80211_RATE_SUPPORTS_5MHZ\t= 1<<5,\n\tIEEE80211_RATE_SUPPORTS_10MHZ\t= 1<<6,\n};\n\n/**\n * enum ieee80211_bss_type - BSS type filter\n *\n * @IEEE80211_BSS_TYPE_ESS: Infrastructure BSS\n * @IEEE80211_BSS_TYPE_PBSS: Personal BSS\n * @IEEE80211_BSS_TYPE_IBSS: Independent BSS\n * @IEEE80211_BSS_TYPE_MBSS: Mesh BSS\n * @IEEE80211_BSS_TYPE_ANY: Wildcard value for matching any BSS type\n */\nenum ieee80211_bss_type {\n\tIEEE80211_BSS_TYPE_ESS,\n\tIEEE80211_BSS_TYPE_PBSS,\n\tIEEE80211_BSS_TYPE_IBSS,\n\tIEEE80211_BSS_TYPE_MBSS,\n\tIEEE80211_BSS_TYPE_ANY\n};\n\n/**\n * enum ieee80211_privacy - BSS privacy filter\n *\n * @IEEE80211_PRIVACY_ON: privacy bit set\n * @IEEE80211_PRIVACY_OFF: privacy bit clear\n * @IEEE80211_PRIVACY_ANY: Wildcard value for matching any privacy setting\n */\nenum ieee80211_privacy {\n\tIEEE80211_PRIVACY_ON,\n\tIEEE80211_PRIVACY_OFF,\n\tIEEE80211_PRIVACY_ANY\n};\n\n#define IEEE80211_PRIVACY(x)\t\\\n\t((x) ? IEEE80211_PRIVACY_ON : IEEE80211_PRIVACY_OFF)\n\n/**\n * struct ieee80211_rate - bitrate definition\n *\n * This structure describes a bitrate that an 802.11 PHY can\n * operate with. The two values @hw_value and @hw_value_short\n * are only for driver use when pointers to this structure are\n * passed around.\n *\n * @flags: rate-specific flags\n * @bitrate: bitrate in units of 100 Kbps\n * @hw_value: driver/hardware value for this rate\n * @hw_value_short: driver/hardware value for this rate when\n *\tshort preamble is used\n */\nstruct ieee80211_rate {\n\tu32 flags;\n\tu16 bitrate;\n\tu16 hw_value, hw_value_short;\n};\n\n/**\n * struct ieee80211_he_obss_pd - AP settings for spatial reuse\n *\n * @enable: is the feature enabled.\n * @sr_ctrl: The SR Control field of SRP element.\n * @non_srg_max_offset: non-SRG maximum tx power offset\n * @min_offset: minimal tx power offset an associated station shall use\n * @max_offset: maximum tx power offset an associated station shall use\n * @bss_color_bitmap: bitmap that indicates the BSS color values used by\n *\tmembers of the SRG\n * @partial_bssid_bitmap: bitmap that indicates the partial BSSID values\n *\tused by members of the SRG\n */\nstruct ieee80211_he_obss_pd {\n\tbool enable;\n\tu8 sr_ctrl;\n\tu8 non_srg_max_offset;\n\tu8 min_offset;\n\tu8 max_offset;\n\tu8 bss_color_bitmap[8];\n\tu8 partial_bssid_bitmap[8];\n};\n\n/**\n * struct cfg80211_he_bss_color - AP settings for BSS coloring\n *\n * @color: the current color.\n * @enabled: HE BSS color is used\n * @partial: define the AID equation.\n */\nstruct cfg80211_he_bss_color {\n\tu8 color;\n\tbool enabled;\n\tbool partial;\n};\n\n/**\n * struct ieee80211_sta_ht_cap - STA's HT capabilities\n *\n * This structure describes most essential parameters needed\n * to describe 802.11n HT capabilities for an STA.\n *\n * @ht_supported: is HT supported by the STA\n * @cap: HT capabilities map as described in 802.11n spec\n * @ampdu_factor: Maximum A-MPDU length factor\n * @ampdu_density: Minimum A-MPDU spacing\n * @mcs: Supported MCS rates\n */\nstruct ieee80211_sta_ht_cap {\n\tu16 cap; /* use IEEE80211_HT_CAP_ */\n\tbool ht_supported;\n\tu8 ampdu_factor;\n\tu8 ampdu_density;\n\tstruct ieee80211_mcs_info mcs;\n};\n\n/**\n * struct ieee80211_sta_vht_cap - STA's VHT capabilities\n *\n * This structure describes most essential parameters needed\n * to describe 802.11ac VHT capabilities for an STA.\n *\n * @vht_supported: is VHT supported by the STA\n * @cap: VHT capabilities map as described in 802.11ac spec\n * @vht_mcs: Supported VHT MCS rates\n */\nstruct ieee80211_sta_vht_cap {\n\tbool vht_supported;\n\tu32 cap; /* use IEEE80211_VHT_CAP_ */\n\tstruct ieee80211_vht_mcs_info vht_mcs;\n};\n\n#define IEEE80211_HE_PPE_THRES_MAX_LEN\t\t25\n\n/**\n * struct ieee80211_sta_he_cap - STA's HE capabilities\n *\n * This structure describes most essential parameters needed\n * to describe 802.11ax HE capabilities for a STA.\n *\n * @has_he: true iff HE data is valid.\n * @he_cap_elem: Fixed portion of the HE capabilities element.\n * @he_mcs_nss_supp: The supported NSS/MCS combinations.\n * @ppe_thres: Holds the PPE Thresholds data.\n */\nstruct ieee80211_sta_he_cap {\n\tbool has_he;\n\tstruct ieee80211_he_cap_elem he_cap_elem;\n\tstruct ieee80211_he_mcs_nss_supp he_mcs_nss_supp;\n\tu8 ppe_thres[IEEE80211_HE_PPE_THRES_MAX_LEN];\n};\n\n/**\n * struct ieee80211_sband_iftype_data\n *\n * This structure encapsulates sband data that is relevant for the\n * interface types defined in @types_mask.  Each type in the\n * @types_mask must be unique across all instances of iftype_data.\n *\n * @types_mask: interface types mask\n * @he_cap: holds the HE capabilities\n * @he_6ghz_capa: HE 6 GHz capabilities, must be filled in for a\n *\t6 GHz band channel (and 0 may be valid value).\n */\nstruct ieee80211_sband_iftype_data {\n\tu16 types_mask;\n\tstruct ieee80211_sta_he_cap he_cap;\n\tstruct ieee80211_he_6ghz_capa he_6ghz_capa;\n};\n\n/**\n * enum ieee80211_edmg_bw_config - allowed channel bandwidth configurations\n *\n * @IEEE80211_EDMG_BW_CONFIG_4: 2.16GHz\n * @IEEE80211_EDMG_BW_CONFIG_5: 2.16GHz and 4.32GHz\n * @IEEE80211_EDMG_BW_CONFIG_6: 2.16GHz, 4.32GHz and 6.48GHz\n * @IEEE80211_EDMG_BW_CONFIG_7: 2.16GHz, 4.32GHz, 6.48GHz and 8.64GHz\n * @IEEE80211_EDMG_BW_CONFIG_8: 2.16GHz and 2.16GHz + 2.16GHz\n * @IEEE80211_EDMG_BW_CONFIG_9: 2.16GHz, 4.32GHz and 2.16GHz + 2.16GHz\n * @IEEE80211_EDMG_BW_CONFIG_10: 2.16GHz, 4.32GHz, 6.48GHz and 2.16GHz+2.16GHz\n * @IEEE80211_EDMG_BW_CONFIG_11: 2.16GHz, 4.32GHz, 6.48GHz, 8.64GHz and\n *\t2.16GHz+2.16GHz\n * @IEEE80211_EDMG_BW_CONFIG_12: 2.16GHz, 2.16GHz + 2.16GHz and\n *\t4.32GHz + 4.32GHz\n * @IEEE80211_EDMG_BW_CONFIG_13: 2.16GHz, 4.32GHz, 2.16GHz + 2.16GHz and\n *\t4.32GHz + 4.32GHz\n * @IEEE80211_EDMG_BW_CONFIG_14: 2.16GHz, 4.32GHz, 6.48GHz, 2.16GHz + 2.16GHz\n *\tand 4.32GHz + 4.32GHz\n * @IEEE80211_EDMG_BW_CONFIG_15: 2.16GHz, 4.32GHz, 6.48GHz, 8.64GHz,\n *\t2.16GHz + 2.16GHz and 4.32GHz + 4.32GHz\n */\nenum ieee80211_edmg_bw_config {\n\tIEEE80211_EDMG_BW_CONFIG_4\t= 4,\n\tIEEE80211_EDMG_BW_CONFIG_5\t= 5,\n\tIEEE80211_EDMG_BW_CONFIG_6\t= 6,\n\tIEEE80211_EDMG_BW_CONFIG_7\t= 7,\n\tIEEE80211_EDMG_BW_CONFIG_8\t= 8,\n\tIEEE80211_EDMG_BW_CONFIG_9\t= 9,\n\tIEEE80211_EDMG_BW_CONFIG_10\t= 10,\n\tIEEE80211_EDMG_BW_CONFIG_11\t= 11,\n\tIEEE80211_EDMG_BW_CONFIG_12\t= 12,\n\tIEEE80211_EDMG_BW_CONFIG_13\t= 13,\n\tIEEE80211_EDMG_BW_CONFIG_14\t= 14,\n\tIEEE80211_EDMG_BW_CONFIG_15\t= 15,\n};\n\n/**\n * struct ieee80211_edmg - EDMG configuration\n *\n * This structure describes most essential parameters needed\n * to describe 802.11ay EDMG configuration\n *\n * @channels: bitmap that indicates the 2.16 GHz channel(s)\n *\tthat are allowed to be used for transmissions.\n *\tBit 0 indicates channel 1, bit 1 indicates channel 2, etc.\n *\tSet to 0 indicate EDMG not supported.\n * @bw_config: Channel BW Configuration subfield encodes\n *\tthe allowed channel bandwidth configurations\n */\nstruct ieee80211_edmg {\n\tu8 channels;\n\tenum ieee80211_edmg_bw_config bw_config;\n};\n\n/**\n * struct ieee80211_sta_s1g_cap - STA's S1G capabilities\n *\n * This structure describes most essential parameters needed\n * to describe 802.11ah S1G capabilities for a STA.\n *\n * @s1g_supported: is STA an S1G STA\n * @cap: S1G capabilities information\n * @nss_mcs: Supported NSS MCS set\n */\nstruct ieee80211_sta_s1g_cap {\n\tbool s1g;\n\tu8 cap[10]; /* use S1G_CAPAB_ */\n\tu8 nss_mcs[5];\n};\n\n/**\n * struct ieee80211_supported_band - frequency band definition\n *\n * This structure describes a frequency band a wiphy\n * is able to operate in.\n *\n * @channels: Array of channels the hardware can operate with\n *\tin this band.\n * @band: the band this structure represents\n * @n_channels: Number of channels in @channels\n * @bitrates: Array of bitrates the hardware can operate with\n *\tin this band. Must be sorted to give a valid \"supported\n *\trates\" IE, i.e. CCK rates first, then OFDM.\n * @n_bitrates: Number of bitrates in @bitrates\n * @ht_cap: HT capabilities in this band\n * @vht_cap: VHT capabilities in this band\n * @s1g_cap: S1G capabilities in this band\n * @edmg_cap: EDMG capabilities in this band\n * @s1g_cap: S1G capabilities in this band (S1B band only, of course)\n * @n_iftype_data: number of iftype data entries\n * @iftype_data: interface type data entries.  Note that the bits in\n *\t@types_mask inside this structure cannot overlap (i.e. only\n *\tone occurrence of each type is allowed across all instances of\n *\tiftype_data).\n */\nstruct ieee80211_supported_band {\n\tstruct ieee80211_channel *channels;\n\tstruct ieee80211_rate *bitrates;\n\tenum nl80211_band band;\n\tint n_channels;\n\tint n_bitrates;\n\tstruct ieee80211_sta_ht_cap ht_cap;\n\tstruct ieee80211_sta_vht_cap vht_cap;\n\tstruct ieee80211_sta_s1g_cap s1g_cap;\n\tstruct ieee80211_edmg edmg_cap;\n\tu16 n_iftype_data;\n\tconst struct ieee80211_sband_iftype_data *iftype_data;\n};\n\n/**\n * ieee80211_get_sband_iftype_data - return sband data for a given iftype\n * @sband: the sband to search for the STA on\n * @iftype: enum nl80211_iftype\n *\n * Return: pointer to struct ieee80211_sband_iftype_data, or NULL is none found\n */\nstatic inline const struct ieee80211_sband_iftype_data *\nieee80211_get_sband_iftype_data(const struct ieee80211_supported_band *sband,\n\t\t\t\tu8 iftype)\n{\n\tint i;\n\n\tif (WARN_ON(iftype >= NL80211_IFTYPE_MAX))\n\t\treturn NULL;\n\n\tfor (i = 0; i < sband->n_iftype_data; i++)  {\n\t\tconst struct ieee80211_sband_iftype_data *data =\n\t\t\t&sband->iftype_data[i];\n\n\t\tif (data->types_mask & BIT(iftype))\n\t\t\treturn data;\n\t}\n\n\treturn NULL;\n}\n\n/**\n * ieee80211_get_he_iftype_cap - return HE capabilities for an sband's iftype\n * @sband: the sband to search for the iftype on\n * @iftype: enum nl80211_iftype\n *\n * Return: pointer to the struct ieee80211_sta_he_cap, or NULL is none found\n */\nstatic inline const struct ieee80211_sta_he_cap *\nieee80211_get_he_iftype_cap(const struct ieee80211_supported_band *sband,\n\t\t\t    u8 iftype)\n{\n\tconst struct ieee80211_sband_iftype_data *data =\n\t\tieee80211_get_sband_iftype_data(sband, iftype);\n\n\tif (data && data->he_cap.has_he)\n\t\treturn &data->he_cap;\n\n\treturn NULL;\n}\n\n/**\n * ieee80211_get_he_sta_cap - return HE capabilities for an sband's STA\n * @sband: the sband to search for the STA on\n *\n * Return: pointer to the struct ieee80211_sta_he_cap, or NULL is none found\n */\nstatic inline const struct ieee80211_sta_he_cap *\nieee80211_get_he_sta_cap(const struct ieee80211_supported_band *sband)\n{\n\treturn ieee80211_get_he_iftype_cap(sband, NL80211_IFTYPE_STATION);\n}\n\n/**\n * ieee80211_get_he_6ghz_capa - return HE 6 GHz capabilities\n * @sband: the sband to search for the STA on\n * @iftype: the iftype to search for\n *\n * Return: the 6GHz capabilities\n */\nstatic inline __le16\nieee80211_get_he_6ghz_capa(const struct ieee80211_supported_band *sband,\n\t\t\t   enum nl80211_iftype iftype)\n{\n\tconst struct ieee80211_sband_iftype_data *data =\n\t\tieee80211_get_sband_iftype_data(sband, iftype);\n\n\tif (WARN_ON(!data || !data->he_cap.has_he))\n\t\treturn 0;\n\n\treturn data->he_6ghz_capa.capa;\n}\n\n/**\n * wiphy_read_of_freq_limits - read frequency limits from device tree\n *\n * @wiphy: the wireless device to get extra limits for\n *\n * Some devices may have extra limitations specified in DT. This may be useful\n * for chipsets that normally support more bands but are limited due to board\n * design (e.g. by antennas or external power amplifier).\n *\n * This function reads info from DT and uses it to *modify* channels (disable\n * unavailable ones). It's usually a *bad* idea to use it in drivers with\n * shared channel data as DT limitations are device specific. You should make\n * sure to call it only if channels in wiphy are copied and can be modified\n * without affecting other devices.\n *\n * As this function access device node it has to be called after set_wiphy_dev.\n * It also modifies channels so they have to be set first.\n * If using this helper, call it before wiphy_register().\n */\n#ifdef CONFIG_OF\nvoid wiphy_read_of_freq_limits(struct wiphy *wiphy);\n#else /* CONFIG_OF */\nstatic inline void wiphy_read_of_freq_limits(struct wiphy *wiphy)\n{\n}\n#endif /* !CONFIG_OF */\n\n\n/*\n * Wireless hardware/device configuration structures and methods\n */\n\n/**\n * DOC: Actions and configuration\n *\n * Each wireless device and each virtual interface offer a set of configuration\n * operations and other actions that are invoked by userspace. Each of these\n * actions is described in the operations structure, and the parameters these\n * operations use are described separately.\n *\n * Additionally, some operations are asynchronous and expect to get status\n * information via some functions that drivers need to call.\n *\n * Scanning and BSS list handling with its associated functionality is described\n * in a separate chapter.\n */\n\n#define VHT_MUMIMO_GROUPS_DATA_LEN (WLAN_MEMBERSHIP_LEN +\\\n\t\t\t\t    WLAN_USER_POSITION_LEN)\n\n/**\n * struct vif_params - describes virtual interface parameters\n * @flags: monitor interface flags, unchanged if 0, otherwise\n *\t%MONITOR_FLAG_CHANGED will be set\n * @use_4addr: use 4-address frames\n * @macaddr: address to use for this virtual interface.\n *\tIf this parameter is set to zero address the driver may\n *\tdetermine the address as needed.\n *\tThis feature is only fully supported by drivers that enable the\n *\t%NL80211_FEATURE_MAC_ON_CREATE flag.  Others may support creating\n **\tonly p2p devices with specified MAC.\n * @vht_mumimo_groups: MU-MIMO groupID, used for monitoring MU-MIMO packets\n *\tbelonging to that MU-MIMO groupID; %NULL if not changed\n * @vht_mumimo_follow_addr: MU-MIMO follow address, used for monitoring\n *\tMU-MIMO packets going to the specified station; %NULL if not changed\n */\nstruct vif_params {\n\tu32 flags;\n\tint use_4addr;\n\tu8 macaddr[ETH_ALEN];\n\tconst u8 *vht_mumimo_groups;\n\tconst u8 *vht_mumimo_follow_addr;\n};\n\n/**\n * struct key_params - key information\n *\n * Information about a key\n *\n * @key: key material\n * @key_len: length of key material\n * @cipher: cipher suite selector\n * @seq: sequence counter (IV/PN) for TKIP and CCMP keys, only used\n *\twith the get_key() callback, must be in little endian,\n *\tlength given by @seq_len.\n * @seq_len: length of @seq.\n * @vlan_id: vlan_id for VLAN group key (if nonzero)\n * @mode: key install mode (RX_TX, NO_TX or SET_TX)\n */\nstruct key_params {\n\tconst u8 *key;\n\tconst u8 *seq;\n\tint key_len;\n\tint seq_len;\n\tu16 vlan_id;\n\tu32 cipher;\n\tenum nl80211_key_mode mode;\n};\n\n/**\n * struct cfg80211_chan_def - channel definition\n * @chan: the (control) channel\n * @width: channel width\n * @center_freq1: center frequency of first segment\n * @center_freq2: center frequency of second segment\n *\t(only with 80+80 MHz)\n * @edmg: define the EDMG channels configuration.\n *\tIf edmg is requested (i.e. the .channels member is non-zero),\n *\tchan will define the primary channel and all other\n *\tparameters are ignored.\n * @freq1_offset: offset from @center_freq1, in KHz\n */\nstruct cfg80211_chan_def {\n\tstruct ieee80211_channel *chan;\n\tenum nl80211_chan_width width;\n\tu32 center_freq1;\n\tu32 center_freq2;\n\tstruct ieee80211_edmg edmg;\n\tu16 freq1_offset;\n};\n\n/*\n * cfg80211_bitrate_mask - masks for bitrate control\n */\nstruct cfg80211_bitrate_mask {\n\tstruct {\n\t\tu32 legacy;\n\t\tu8 ht_mcs[IEEE80211_HT_MCS_MASK_LEN];\n\t\tu16 vht_mcs[NL80211_VHT_NSS_MAX];\n\t\tu16 he_mcs[NL80211_HE_NSS_MAX];\n\t\tenum nl80211_txrate_gi gi;\n\t\tenum nl80211_he_gi he_gi;\n\t\tenum nl80211_he_ltf he_ltf;\n\t} control[NUM_NL80211_BANDS];\n};\n\n\n/**\n * struct cfg80211_tid_cfg - TID specific configuration\n * @config_override: Flag to notify driver to reset TID configuration\n *\tof the peer.\n * @tids: bitmap of TIDs to modify\n * @mask: bitmap of attributes indicating which parameter changed,\n *\tsimilar to &nl80211_tid_config_supp.\n * @noack: noack configuration value for the TID\n * @retry_long: retry count value\n * @retry_short: retry count value\n * @ampdu: Enable/Disable MPDU aggregation\n * @rtscts: Enable/Disable RTS/CTS\n * @amsdu: Enable/Disable MSDU aggregation\n * @txrate_type: Tx bitrate mask type\n * @txrate_mask: Tx bitrate to be applied for the TID\n */\nstruct cfg80211_tid_cfg {\n\tbool config_override;\n\tu8 tids;\n\tu64 mask;\n\tenum nl80211_tid_config noack;\n\tu8 retry_long, retry_short;\n\tenum nl80211_tid_config ampdu;\n\tenum nl80211_tid_config rtscts;\n\tenum nl80211_tid_config amsdu;\n\tenum nl80211_tx_rate_setting txrate_type;\n\tstruct cfg80211_bitrate_mask txrate_mask;\n};\n\n/**\n * struct cfg80211_tid_config - TID configuration\n * @peer: Station's MAC address\n * @n_tid_conf: Number of TID specific configurations to be applied\n * @tid_conf: Configuration change info\n */\nstruct cfg80211_tid_config {\n\tconst u8 *peer;\n\tu32 n_tid_conf;\n\tstruct cfg80211_tid_cfg tid_conf[];\n};\n\n/**\n * cfg80211_get_chandef_type - return old channel type from chandef\n * @chandef: the channel definition\n *\n * Return: The old channel type (NOHT, HT20, HT40+/-) from a given\n * chandef, which must have a bandwidth allowing this conversion.\n */\nstatic inline enum nl80211_channel_type\ncfg80211_get_chandef_type(const struct cfg80211_chan_def *chandef)\n{\n\tswitch (chandef->width) {\n\tcase NL80211_CHAN_WIDTH_20_NOHT:\n\t\treturn NL80211_CHAN_NO_HT;\n\tcase NL80211_CHAN_WIDTH_20:\n\t\treturn NL80211_CHAN_HT20;\n\tcase NL80211_CHAN_WIDTH_40:\n\t\tif (chandef->center_freq1 > chandef->chan->center_freq)\n\t\t\treturn NL80211_CHAN_HT40PLUS;\n\t\treturn NL80211_CHAN_HT40MINUS;\n\tdefault:\n\t\tWARN_ON(1);\n\t\treturn NL80211_CHAN_NO_HT;\n\t}\n}\n\n/**\n * cfg80211_chandef_create - create channel definition using channel type\n * @chandef: the channel definition struct to fill\n * @channel: the control channel\n * @chantype: the channel type\n *\n * Given a channel type, create a channel definition.\n */\nvoid cfg80211_chandef_create(struct cfg80211_chan_def *chandef,\n\t\t\t     struct ieee80211_channel *channel,\n\t\t\t     enum nl80211_channel_type chantype);\n\n/**\n * cfg80211_chandef_identical - check if two channel definitions are identical\n * @chandef1: first channel definition\n * @chandef2: second channel definition\n *\n * Return: %true if the channels defined by the channel definitions are\n * identical, %false otherwise.\n */\nstatic inline bool\ncfg80211_chandef_identical(const struct cfg80211_chan_def *chandef1,\n\t\t\t   const struct cfg80211_chan_def *chandef2)\n{\n\treturn (chandef1->chan == chandef2->chan &&\n\t\tchandef1->width == chandef2->width &&\n\t\tchandef1->center_freq1 == chandef2->center_freq1 &&\n\t\tchandef1->freq1_offset == chandef2->freq1_offset &&\n\t\tchandef1->center_freq2 == chandef2->center_freq2);\n}\n\n/**\n * cfg80211_chandef_is_edmg - check if chandef represents an EDMG channel\n *\n * @chandef: the channel definition\n *\n * Return: %true if EDMG defined, %false otherwise.\n */\nstatic inline bool\ncfg80211_chandef_is_edmg(const struct cfg80211_chan_def *chandef)\n{\n\treturn chandef->edmg.channels || chandef->edmg.bw_config;\n}\n\n/**\n * cfg80211_chandef_compatible - check if two channel definitions are compatible\n * @chandef1: first channel definition\n * @chandef2: second channel definition\n *\n * Return: %NULL if the given channel definitions are incompatible,\n * chandef1 or chandef2 otherwise.\n */\nconst struct cfg80211_chan_def *\ncfg80211_chandef_compatible(const struct cfg80211_chan_def *chandef1,\n\t\t\t    const struct cfg80211_chan_def *chandef2);\n\n/**\n * cfg80211_chandef_valid - check if a channel definition is valid\n * @chandef: the channel definition to check\n * Return: %true if the channel definition is valid. %false otherwise.\n */\nbool cfg80211_chandef_valid(const struct cfg80211_chan_def *chandef);\n\n/**\n * cfg80211_chandef_usable - check if secondary channels can be used\n * @wiphy: the wiphy to validate against\n * @chandef: the channel definition to check\n * @prohibited_flags: the regulatory channel flags that must not be set\n * Return: %true if secondary channels are usable. %false otherwise.\n */\nbool cfg80211_chandef_usable(struct wiphy *wiphy,\n\t\t\t     const struct cfg80211_chan_def *chandef,\n\t\t\t     u32 prohibited_flags);\n\n/**\n * cfg80211_chandef_dfs_required - checks if radar detection is required\n * @wiphy: the wiphy to validate against\n * @chandef: the channel definition to check\n * @iftype: the interface type as specified in &enum nl80211_iftype\n * Returns:\n *\t1 if radar detection is required, 0 if it is not, < 0 on error\n */\nint cfg80211_chandef_dfs_required(struct wiphy *wiphy,\n\t\t\t\t  const struct cfg80211_chan_def *chandef,\n\t\t\t\t  enum nl80211_iftype iftype);\n\n/**\n * ieee80211_chandef_rate_flags - returns rate flags for a channel\n *\n * In some channel types, not all rates may be used - for example CCK\n * rates may not be used in 5/10 MHz channels.\n *\n * @chandef: channel definition for the channel\n *\n * Returns: rate flags which apply for this channel\n */\nstatic inline enum ieee80211_rate_flags\nieee80211_chandef_rate_flags(struct cfg80211_chan_def *chandef)\n{\n\tswitch (chandef->width) {\n\tcase NL80211_CHAN_WIDTH_5:\n\t\treturn IEEE80211_RATE_SUPPORTS_5MHZ;\n\tcase NL80211_CHAN_WIDTH_10:\n\t\treturn IEEE80211_RATE_SUPPORTS_10MHZ;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\n/**\n * ieee80211_chandef_max_power - maximum transmission power for the chandef\n *\n * In some regulations, the transmit power may depend on the configured channel\n * bandwidth which may be defined as dBm/MHz. This function returns the actual\n * max_power for non-standard (20 MHz) channels.\n *\n * @chandef: channel definition for the channel\n *\n * Returns: maximum allowed transmission power in dBm for the chandef\n */\nstatic inline int\nieee80211_chandef_max_power(struct cfg80211_chan_def *chandef)\n{\n\tswitch (chandef->width) {\n\tcase NL80211_CHAN_WIDTH_5:\n\t\treturn min(chandef->chan->max_reg_power - 6,\n\t\t\t   chandef->chan->max_power);\n\tcase NL80211_CHAN_WIDTH_10:\n\t\treturn min(chandef->chan->max_reg_power - 3,\n\t\t\t   chandef->chan->max_power);\n\tdefault:\n\t\tbreak;\n\t}\n\treturn chandef->chan->max_power;\n}\n\n/**\n * enum survey_info_flags - survey information flags\n *\n * @SURVEY_INFO_NOISE_DBM: noise (in dBm) was filled in\n * @SURVEY_INFO_IN_USE: channel is currently being used\n * @SURVEY_INFO_TIME: active time (in ms) was filled in\n * @SURVEY_INFO_TIME_BUSY: busy time was filled in\n * @SURVEY_INFO_TIME_EXT_BUSY: extension channel busy time was filled in\n * @SURVEY_INFO_TIME_RX: receive time was filled in\n * @SURVEY_INFO_TIME_TX: transmit time was filled in\n * @SURVEY_INFO_TIME_SCAN: scan time was filled in\n * @SURVEY_INFO_TIME_BSS_RX: local BSS receive time was filled in\n *\n * Used by the driver to indicate which info in &struct survey_info\n * it has filled in during the get_survey().\n */\nenum survey_info_flags {\n\tSURVEY_INFO_NOISE_DBM\t\t= BIT(0),\n\tSURVEY_INFO_IN_USE\t\t= BIT(1),\n\tSURVEY_INFO_TIME\t\t= BIT(2),\n\tSURVEY_INFO_TIME_BUSY\t\t= BIT(3),\n\tSURVEY_INFO_TIME_EXT_BUSY\t= BIT(4),\n\tSURVEY_INFO_TIME_RX\t\t= BIT(5),\n\tSURVEY_INFO_TIME_TX\t\t= BIT(6),\n\tSURVEY_INFO_TIME_SCAN\t\t= BIT(7),\n\tSURVEY_INFO_TIME_BSS_RX\t\t= BIT(8),\n};\n\n/**\n * struct survey_info - channel survey response\n *\n * @channel: the channel this survey record reports, may be %NULL for a single\n *\trecord to report global statistics\n * @filled: bitflag of flags from &enum survey_info_flags\n * @noise: channel noise in dBm. This and all following fields are\n *\toptional\n * @time: amount of time in ms the radio was turn on (on the channel)\n * @time_busy: amount of time the primary channel was sensed busy\n * @time_ext_busy: amount of time the extension channel was sensed busy\n * @time_rx: amount of time the radio spent receiving data\n * @time_tx: amount of time the radio spent transmitting data\n * @time_scan: amount of time the radio spent for scanning\n * @time_bss_rx: amount of time the radio spent receiving data on a local BSS\n *\n * Used by dump_survey() to report back per-channel survey information.\n *\n * This structure can later be expanded with things like\n * channel duty cycle etc.\n */\nstruct survey_info {\n\tstruct ieee80211_channel *channel;\n\tu64 time;\n\tu64 time_busy;\n\tu64 time_ext_busy;\n\tu64 time_rx;\n\tu64 time_tx;\n\tu64 time_scan;\n\tu64 time_bss_rx;\n\tu32 filled;\n\ts8 noise;\n};\n\n#define CFG80211_MAX_WEP_KEYS\t4\n\n/**\n * struct cfg80211_crypto_settings - Crypto settings\n * @wpa_versions: indicates which, if any, WPA versions are enabled\n *\t(from enum nl80211_wpa_versions)\n * @cipher_group: group key cipher suite (or 0 if unset)\n * @n_ciphers_pairwise: number of AP supported unicast ciphers\n * @ciphers_pairwise: unicast key cipher suites\n * @n_akm_suites: number of AKM suites\n * @akm_suites: AKM suites\n * @control_port: Whether user space controls IEEE 802.1X port, i.e.,\n *\tsets/clears %NL80211_STA_FLAG_AUTHORIZED. If true, the driver is\n *\trequired to assume that the port is unauthorized until authorized by\n *\tuser space. Otherwise, port is marked authorized by default.\n * @control_port_ethertype: the control port protocol that should be\n *\tallowed through even on unauthorized ports\n * @control_port_no_encrypt: TRUE to prevent encryption of control port\n *\tprotocol frames.\n * @control_port_over_nl80211: TRUE if userspace expects to exchange control\n *\tport frames over NL80211 instead of the network interface.\n * @control_port_no_preauth: disables pre-auth rx over the nl80211 control\n *\tport for mac80211\n * @wep_keys: static WEP keys, if not NULL points to an array of\n *\tCFG80211_MAX_WEP_KEYS WEP keys\n * @wep_tx_key: key index (0..3) of the default TX static WEP key\n * @psk: PSK (for devices supporting 4-way-handshake offload)\n * @sae_pwd: password for SAE authentication (for devices supporting SAE\n *\toffload)\n * @sae_pwd_len: length of SAE password (for devices supporting SAE offload)\n * @sae_pwe: The mechanisms allowed for SAE PWE derivation:\n *\n *\tNL80211_SAE_PWE_UNSPECIFIED\n *\t  Not-specified, used to indicate userspace did not specify any\n *\t  preference. The driver should follow its internal policy in\n *\t  such a scenario.\n *\n *\tNL80211_SAE_PWE_HUNT_AND_PECK\n *\t  Allow hunting-and-pecking loop only\n *\n *\tNL80211_SAE_PWE_HASH_TO_ELEMENT\n *\t  Allow hash-to-element only\n *\n *\tNL80211_SAE_PWE_BOTH\n *\t  Allow either hunting-and-pecking loop or hash-to-element\n */\nstruct cfg80211_crypto_settings {\n\tu32 wpa_versions;\n\tu32 cipher_group;\n\tint n_ciphers_pairwise;\n\tu32 ciphers_pairwise[NL80211_MAX_NR_CIPHER_SUITES];\n\tint n_akm_suites;\n\tu32 akm_suites[NL80211_MAX_NR_AKM_SUITES];\n\tbool control_port;\n\t__be16 control_port_ethertype;\n\tbool control_port_no_encrypt;\n\tbool control_port_over_nl80211;\n\tbool control_port_no_preauth;\n\tstruct key_params *wep_keys;\n\tint wep_tx_key;\n\tconst u8 *psk;\n\tconst u8 *sae_pwd;\n\tu8 sae_pwd_len;\n\tenum nl80211_sae_pwe_mechanism sae_pwe;\n};\n\n/**\n * struct cfg80211_beacon_data - beacon data\n * @head: head portion of beacon (before TIM IE)\n *\tor %NULL if not changed\n * @tail: tail portion of beacon (after TIM IE)\n *\tor %NULL if not changed\n * @head_len: length of @head\n * @tail_len: length of @tail\n * @beacon_ies: extra information element(s) to add into Beacon frames or %NULL\n * @beacon_ies_len: length of beacon_ies in octets\n * @proberesp_ies: extra information element(s) to add into Probe Response\n *\tframes or %NULL\n * @proberesp_ies_len: length of proberesp_ies in octets\n * @assocresp_ies: extra information element(s) to add into (Re)Association\n *\tResponse frames or %NULL\n * @assocresp_ies_len: length of assocresp_ies in octets\n * @probe_resp_len: length of probe response template (@probe_resp)\n * @probe_resp: probe response template (AP mode only)\n * @ftm_responder: enable FTM responder functionality; -1 for no change\n *\t(which also implies no change in LCI/civic location data)\n * @lci: Measurement Report element content, starting with Measurement Token\n *\t(measurement type 8)\n * @civicloc: Measurement Report element content, starting with Measurement\n *\tToken (measurement type 11)\n * @lci_len: LCI data length\n * @civicloc_len: Civic location data length\n */\nstruct cfg80211_beacon_data {\n\tconst u8 *head, *tail;\n\tconst u8 *beacon_ies;\n\tconst u8 *proberesp_ies;\n\tconst u8 *assocresp_ies;\n\tconst u8 *probe_resp;\n\tconst u8 *lci;\n\tconst u8 *civicloc;\n\ts8 ftm_responder;\n\n\tsize_t head_len, tail_len;\n\tsize_t beacon_ies_len;\n\tsize_t proberesp_ies_len;\n\tsize_t assocresp_ies_len;\n\tsize_t probe_resp_len;\n\tsize_t lci_len;\n\tsize_t civicloc_len;\n};\n\nstruct mac_address {\n\tu8 addr[ETH_ALEN];\n};\n\n/**\n * struct cfg80211_acl_data - Access control list data\n *\n * @acl_policy: ACL policy to be applied on the station's\n *\tentry specified by mac_addr\n * @n_acl_entries: Number of MAC address entries passed\n * @mac_addrs: List of MAC addresses of stations to be used for ACL\n */\nstruct cfg80211_acl_data {\n\tenum nl80211_acl_policy acl_policy;\n\tint n_acl_entries;\n\n\t/* Keep it last */\n\tstruct mac_address mac_addrs[];\n};\n\n/**\n * struct cfg80211_fils_discovery - FILS discovery parameters from\n * IEEE Std 802.11ai-2016, Annex C.3 MIB detail.\n *\n * @min_interval: Minimum packet interval in TUs (0 - 10000)\n * @max_interval: Maximum packet interval in TUs (0 - 10000)\n * @tmpl_len: Template length\n * @tmpl: Template data for FILS discovery frame including the action\n *\tframe headers.\n */\nstruct cfg80211_fils_discovery {\n\tu32 min_interval;\n\tu32 max_interval;\n\tsize_t tmpl_len;\n\tconst u8 *tmpl;\n};\n\n/**\n * struct cfg80211_unsol_bcast_probe_resp - Unsolicited broadcast probe\n *\tresponse parameters in 6GHz.\n *\n * @interval: Packet interval in TUs. Maximum allowed is 20 TU, as mentioned\n *\tin IEEE P802.11ax/D6.0 26.17.2.3.2 - AP behavior for fast passive\n *\tscanning\n * @tmpl_len: Template length\n * @tmpl: Template data for probe response\n */\nstruct cfg80211_unsol_bcast_probe_resp {\n\tu32 interval;\n\tsize_t tmpl_len;\n\tconst u8 *tmpl;\n};\n\n/**\n * enum cfg80211_ap_settings_flags - AP settings flags\n *\n * Used by cfg80211_ap_settings\n *\n * @AP_SETTINGS_EXTERNAL_AUTH_SUPPORT: AP supports external authentication\n */\nenum cfg80211_ap_settings_flags {\n\tAP_SETTINGS_EXTERNAL_AUTH_SUPPORT = BIT(0),\n};\n\n/**\n * struct cfg80211_ap_settings - AP configuration\n *\n * Used to configure an AP interface.\n *\n * @chandef: defines the channel to use\n * @beacon: beacon data\n * @beacon_interval: beacon interval\n * @dtim_period: DTIM period\n * @ssid: SSID to be used in the BSS (note: may be %NULL if not provided from\n *\tuser space)\n * @ssid_len: length of @ssid\n * @hidden_ssid: whether to hide the SSID in Beacon/Probe Response frames\n * @crypto: crypto settings\n * @privacy: the BSS uses privacy\n * @auth_type: Authentication type (algorithm)\n * @smps_mode: SMPS mode\n * @inactivity_timeout: time in seconds to determine station's inactivity.\n * @p2p_ctwindow: P2P CT Window\n * @p2p_opp_ps: P2P opportunistic PS\n * @acl: ACL configuration used by the drivers which has support for\n *\tMAC address based access control\n * @pbss: If set, start as a PCP instead of AP. Relevant for DMG\n *\tnetworks.\n * @beacon_rate: bitrate to be used for beacons\n * @ht_cap: HT capabilities (or %NULL if HT isn't enabled)\n * @vht_cap: VHT capabilities (or %NULL if VHT isn't enabled)\n * @he_cap: HE capabilities (or %NULL if HE isn't enabled)\n * @ht_required: stations must support HT\n * @vht_required: stations must support VHT\n * @twt_responder: Enable Target Wait Time\n * @he_required: stations must support HE\n * @sae_h2e_required: stations must support direct H2E technique in SAE\n * @flags: flags, as defined in enum cfg80211_ap_settings_flags\n * @he_obss_pd: OBSS Packet Detection settings\n * @he_bss_color: BSS Color settings\n * @he_oper: HE operation IE (or %NULL if HE isn't enabled)\n * @fils_discovery: FILS discovery transmission parameters\n * @unsol_bcast_probe_resp: Unsolicited broadcast probe response parameters\n */\nstruct cfg80211_ap_settings {\n\tstruct cfg80211_chan_def chandef;\n\n\tstruct cfg80211_beacon_data beacon;\n\n\tint beacon_interval, dtim_period;\n\tconst u8 *ssid;\n\tsize_t ssid_len;\n\tenum nl80211_hidden_ssid hidden_ssid;\n\tstruct cfg80211_crypto_settings crypto;\n\tbool privacy;\n\tenum nl80211_auth_type auth_type;\n\tenum nl80211_smps_mode smps_mode;\n\tint inactivity_timeout;\n\tu8 p2p_ctwindow;\n\tbool p2p_opp_ps;\n\tconst struct cfg80211_acl_data *acl;\n\tbool pbss;\n\tstruct cfg80211_bitrate_mask beacon_rate;\n\n\tconst struct ieee80211_ht_cap *ht_cap;\n\tconst struct ieee80211_vht_cap *vht_cap;\n\tconst struct ieee80211_he_cap_elem *he_cap;\n\tconst struct ieee80211_he_operation *he_oper;\n\tbool ht_required, vht_required, he_required, sae_h2e_required;\n\tbool twt_responder;\n\tu32 flags;\n\tstruct ieee80211_he_obss_pd he_obss_pd;\n\tstruct cfg80211_he_bss_color he_bss_color;\n\tstruct cfg80211_fils_discovery fils_discovery;\n\tstruct cfg80211_unsol_bcast_probe_resp unsol_bcast_probe_resp;\n};\n\n/**\n * struct cfg80211_csa_settings - channel switch settings\n *\n * Used for channel switch\n *\n * @chandef: defines the channel to use after the switch\n * @beacon_csa: beacon data while performing the switch\n * @counter_offsets_beacon: offsets of the counters within the beacon (tail)\n * @counter_offsets_presp: offsets of the counters within the probe response\n * @n_counter_offsets_beacon: number of csa counters the beacon (tail)\n * @n_counter_offsets_presp: number of csa counters in the probe response\n * @beacon_after: beacon data to be used on the new channel\n * @radar_required: whether radar detection is required on the new channel\n * @block_tx: whether transmissions should be blocked while changing\n * @count: number of beacons until switch\n */\nstruct cfg80211_csa_settings {\n\tstruct cfg80211_chan_def chandef;\n\tstruct cfg80211_beacon_data beacon_csa;\n\tconst u16 *counter_offsets_beacon;\n\tconst u16 *counter_offsets_presp;\n\tunsigned int n_counter_offsets_beacon;\n\tunsigned int n_counter_offsets_presp;\n\tstruct cfg80211_beacon_data beacon_after;\n\tbool radar_required;\n\tbool block_tx;\n\tu8 count;\n};\n\n#define CFG80211_MAX_NUM_DIFFERENT_CHANNELS 10\n\n/**\n * struct iface_combination_params - input parameters for interface combinations\n *\n * Used to pass interface combination parameters\n *\n * @num_different_channels: the number of different channels we want\n *\tto use for verification\n * @radar_detect: a bitmap where each bit corresponds to a channel\n *\twidth where radar detection is needed, as in the definition of\n *\t&struct ieee80211_iface_combination.@radar_detect_widths\n * @iftype_num: array with the number of interfaces of each interface\n *\ttype.  The index is the interface type as specified in &enum\n *\tnl80211_iftype.\n * @new_beacon_int: set this to the beacon interval of a new interface\n *\tthat's not operating yet, if such is to be checked as part of\n *\tthe verification\n */\nstruct iface_combination_params {\n\tint num_different_channels;\n\tu8 radar_detect;\n\tint iftype_num[NUM_NL80211_IFTYPES];\n\tu32 new_beacon_int;\n};\n\n/**\n * enum station_parameters_apply_mask - station parameter values to apply\n * @STATION_PARAM_APPLY_UAPSD: apply new uAPSD parameters (uapsd_queues, max_sp)\n * @STATION_PARAM_APPLY_CAPABILITY: apply new capability\n * @STATION_PARAM_APPLY_PLINK_STATE: apply new plink state\n *\n * Not all station parameters have in-band \"no change\" signalling,\n * for those that don't these flags will are used.\n */\nenum station_parameters_apply_mask {\n\tSTATION_PARAM_APPLY_UAPSD = BIT(0),\n\tSTATION_PARAM_APPLY_CAPABILITY = BIT(1),\n\tSTATION_PARAM_APPLY_PLINK_STATE = BIT(2),\n\tSTATION_PARAM_APPLY_STA_TXPOWER = BIT(3),\n};\n\n/**\n * struct sta_txpwr - station txpower configuration\n *\n * Used to configure txpower for station.\n *\n * @power: tx power (in dBm) to be used for sending data traffic. If tx power\n *\tis not provided, the default per-interface tx power setting will be\n *\toverriding. Driver should be picking up the lowest tx power, either tx\n *\tpower per-interface or per-station.\n * @type: In particular if TPC %type is NL80211_TX_POWER_LIMITED then tx power\n *\twill be less than or equal to specified from userspace, whereas if TPC\n *\t%type is NL80211_TX_POWER_AUTOMATIC then it indicates default tx power.\n *\tNL80211_TX_POWER_FIXED is not a valid configuration option for\n *\tper peer TPC.\n */\nstruct sta_txpwr {\n\ts16 power;\n\tenum nl80211_tx_power_setting type;\n};\n\n/**\n * struct station_parameters - station parameters\n *\n * Used to change and create a new station.\n *\n * @vlan: vlan interface station should belong to\n * @supported_rates: supported rates in IEEE 802.11 format\n *\t(or NULL for no change)\n * @supported_rates_len: number of supported rates\n * @sta_flags_mask: station flags that changed\n *\t(bitmask of BIT(%NL80211_STA_FLAG_...))\n * @sta_flags_set: station flags values\n *\t(bitmask of BIT(%NL80211_STA_FLAG_...))\n * @listen_interval: listen interval or -1 for no change\n * @aid: AID or zero for no change\n * @vlan_id: VLAN ID for station (if nonzero)\n * @peer_aid: mesh peer AID or zero for no change\n * @plink_action: plink action to take\n * @plink_state: set the peer link state for a station\n * @ht_capa: HT capabilities of station\n * @vht_capa: VHT capabilities of station\n * @uapsd_queues: bitmap of queues configured for uapsd. same format\n *\tas the AC bitmap in the QoS info field\n * @max_sp: max Service Period. same format as the MAX_SP in the\n *\tQoS info field (but already shifted down)\n * @sta_modify_mask: bitmap indicating which parameters changed\n *\t(for those that don't have a natural \"no change\" value),\n *\tsee &enum station_parameters_apply_mask\n * @local_pm: local link-specific mesh power save mode (no change when set\n *\tto unknown)\n * @capability: station capability\n * @ext_capab: extended capabilities of the station\n * @ext_capab_len: number of extended capabilities\n * @supported_channels: supported channels in IEEE 802.11 format\n * @supported_channels_len: number of supported channels\n * @supported_oper_classes: supported oper classes in IEEE 802.11 format\n * @supported_oper_classes_len: number of supported operating classes\n * @opmode_notif: operating mode field from Operating Mode Notification\n * @opmode_notif_used: information if operating mode field is used\n * @support_p2p_ps: information if station supports P2P PS mechanism\n * @he_capa: HE capabilities of station\n * @he_capa_len: the length of the HE capabilities\n * @airtime_weight: airtime scheduler weight for this station\n * @txpwr: transmit power for an associated station\n * @he_6ghz_capa: HE 6 GHz Band capabilities of station\n */\nstruct station_parameters {\n\tconst u8 *supported_rates;\n\tstruct net_device *vlan;\n\tu32 sta_flags_mask, sta_flags_set;\n\tu32 sta_modify_mask;\n\tint listen_interval;\n\tu16 aid;\n\tu16 vlan_id;\n\tu16 peer_aid;\n\tu8 supported_rates_len;\n\tu8 plink_action;\n\tu8 plink_state;\n\tconst struct ieee80211_ht_cap *ht_capa;\n\tconst struct ieee80211_vht_cap *vht_capa;\n\tu8 uapsd_queues;\n\tu8 max_sp;\n\tenum nl80211_mesh_power_mode local_pm;\n\tu16 capability;\n\tconst u8 *ext_capab;\n\tu8 ext_capab_len;\n\tconst u8 *supported_channels;\n\tu8 supported_channels_len;\n\tconst u8 *supported_oper_classes;\n\tu8 supported_oper_classes_len;\n\tu8 opmode_notif;\n\tbool opmode_notif_used;\n\tint support_p2p_ps;\n\tconst struct ieee80211_he_cap_elem *he_capa;\n\tu8 he_capa_len;\n\tu16 airtime_weight;\n\tstruct sta_txpwr txpwr;\n\tconst struct ieee80211_he_6ghz_capa *he_6ghz_capa;\n};\n\n/**\n * struct station_del_parameters - station deletion parameters\n *\n * Used to delete a station entry (or all stations).\n *\n * @mac: MAC address of the station to remove or NULL to remove all stations\n * @subtype: Management frame subtype to use for indicating removal\n *\t(10 = Disassociation, 12 = Deauthentication)\n * @reason_code: Reason code for the Disassociation/Deauthentication frame\n */\nstruct station_del_parameters {\n\tconst u8 *mac;\n\tu8 subtype;\n\tu16 reason_code;\n};\n\n/**\n * enum cfg80211_station_type - the type of station being modified\n * @CFG80211_STA_AP_CLIENT: client of an AP interface\n * @CFG80211_STA_AP_CLIENT_UNASSOC: client of an AP interface that is still\n *\tunassociated (update properties for this type of client is permitted)\n * @CFG80211_STA_AP_MLME_CLIENT: client of an AP interface that has\n *\tthe AP MLME in the device\n * @CFG80211_STA_AP_STA: AP station on managed interface\n * @CFG80211_STA_IBSS: IBSS station\n * @CFG80211_STA_TDLS_PEER_SETUP: TDLS peer on managed interface (dummy entry\n *\twhile TDLS setup is in progress, it moves out of this state when\n *\tbeing marked authorized; use this only if TDLS with external setup is\n *\tsupported/used)\n * @CFG80211_STA_TDLS_PEER_ACTIVE: TDLS peer on managed interface (active\n *\tentry that is operating, has been marked authorized by userspace)\n * @CFG80211_STA_MESH_PEER_KERNEL: peer on mesh interface (kernel managed)\n * @CFG80211_STA_MESH_PEER_USER: peer on mesh interface (user managed)\n */\nenum cfg80211_station_type {\n\tCFG80211_STA_AP_CLIENT,\n\tCFG80211_STA_AP_CLIENT_UNASSOC,\n\tCFG80211_STA_AP_MLME_CLIENT,\n\tCFG80211_STA_AP_STA,\n\tCFG80211_STA_IBSS,\n\tCFG80211_STA_TDLS_PEER_SETUP,\n\tCFG80211_STA_TDLS_PEER_ACTIVE,\n\tCFG80211_STA_MESH_PEER_KERNEL,\n\tCFG80211_STA_MESH_PEER_USER,\n};\n\n/**\n * cfg80211_check_station_change - validate parameter changes\n * @wiphy: the wiphy this operates on\n * @params: the new parameters for a station\n * @statype: the type of station being modified\n *\n * Utility function for the @change_station driver method. Call this function\n * with the appropriate station type looking up the station (and checking that\n * it exists). It will verify whether the station change is acceptable, and if\n * not will return an error code. Note that it may modify the parameters for\n * backward compatibility reasons, so don't use them before calling this.\n */\nint cfg80211_check_station_change(struct wiphy *wiphy,\n\t\t\t\t  struct station_parameters *params,\n\t\t\t\t  enum cfg80211_station_type statype);\n\n/**\n * enum rate_info_flags - bitrate info flags\n *\n * Used by the driver to indicate the specific rate transmission\n * type for 802.11n transmissions.\n *\n * @RATE_INFO_FLAGS_MCS: mcs field filled with HT MCS\n * @RATE_INFO_FLAGS_VHT_MCS: mcs field filled with VHT MCS\n * @RATE_INFO_FLAGS_SHORT_GI: 400ns guard interval\n * @RATE_INFO_FLAGS_DMG: 60GHz MCS\n * @RATE_INFO_FLAGS_HE_MCS: HE MCS information\n * @RATE_INFO_FLAGS_EDMG: 60GHz MCS in EDMG mode\n * @RATE_INFO_FLAGS_EXTENDED_SC_DMG: 60GHz extended SC MCS\n */\nenum rate_info_flags {\n\tRATE_INFO_FLAGS_MCS\t\t\t= BIT(0),\n\tRATE_INFO_FLAGS_VHT_MCS\t\t\t= BIT(1),\n\tRATE_INFO_FLAGS_SHORT_GI\t\t= BIT(2),\n\tRATE_INFO_FLAGS_DMG\t\t\t= BIT(3),\n\tRATE_INFO_FLAGS_HE_MCS\t\t\t= BIT(4),\n\tRATE_INFO_FLAGS_EDMG\t\t\t= BIT(5),\n\tRATE_INFO_FLAGS_EXTENDED_SC_DMG\t\t= BIT(6),\n};\n\n/**\n * enum rate_info_bw - rate bandwidth information\n *\n * Used by the driver to indicate the rate bandwidth.\n *\n * @RATE_INFO_BW_5: 5 MHz bandwidth\n * @RATE_INFO_BW_10: 10 MHz bandwidth\n * @RATE_INFO_BW_20: 20 MHz bandwidth\n * @RATE_INFO_BW_40: 40 MHz bandwidth\n * @RATE_INFO_BW_80: 80 MHz bandwidth\n * @RATE_INFO_BW_160: 160 MHz bandwidth\n * @RATE_INFO_BW_HE_RU: bandwidth determined by HE RU allocation\n */\nenum rate_info_bw {\n\tRATE_INFO_BW_20 = 0,\n\tRATE_INFO_BW_5,\n\tRATE_INFO_BW_10,\n\tRATE_INFO_BW_40,\n\tRATE_INFO_BW_80,\n\tRATE_INFO_BW_160,\n\tRATE_INFO_BW_HE_RU,\n};\n\n/**\n * struct rate_info - bitrate information\n *\n * Information about a receiving or transmitting bitrate\n *\n * @flags: bitflag of flags from &enum rate_info_flags\n * @mcs: mcs index if struct describes an HT/VHT/HE rate\n * @legacy: bitrate in 100kbit/s for 802.11abg\n * @nss: number of streams (VHT & HE only)\n * @bw: bandwidth (from &enum rate_info_bw)\n * @he_gi: HE guard interval (from &enum nl80211_he_gi)\n * @he_dcm: HE DCM value\n * @he_ru_alloc: HE RU allocation (from &enum nl80211_he_ru_alloc,\n *\tonly valid if bw is %RATE_INFO_BW_HE_RU)\n * @n_bonded_ch: In case of EDMG the number of bonded channels (1-4)\n */\nstruct rate_info {\n\tu8 flags;\n\tu8 mcs;\n\tu16 legacy;\n\tu8 nss;\n\tu8 bw;\n\tu8 he_gi;\n\tu8 he_dcm;\n\tu8 he_ru_alloc;\n\tu8 n_bonded_ch;\n};\n\n/**\n * enum bss_param_flags - bitrate info flags\n *\n * Used by the driver to indicate the specific rate transmission\n * type for 802.11n transmissions.\n *\n * @BSS_PARAM_FLAGS_CTS_PROT: whether CTS protection is enabled\n * @BSS_PARAM_FLAGS_SHORT_PREAMBLE: whether short preamble is enabled\n * @BSS_PARAM_FLAGS_SHORT_SLOT_TIME: whether short slot time is enabled\n */\nenum bss_param_flags {\n\tBSS_PARAM_FLAGS_CTS_PROT\t= 1<<0,\n\tBSS_PARAM_FLAGS_SHORT_PREAMBLE\t= 1<<1,\n\tBSS_PARAM_FLAGS_SHORT_SLOT_TIME\t= 1<<2,\n};\n\n/**\n * struct sta_bss_parameters - BSS parameters for the attached station\n *\n * Information about the currently associated BSS\n *\n * @flags: bitflag of flags from &enum bss_param_flags\n * @dtim_period: DTIM period for the BSS\n * @beacon_interval: beacon interval\n */\nstruct sta_bss_parameters {\n\tu8 flags;\n\tu8 dtim_period;\n\tu16 beacon_interval;\n};\n\n/**\n * struct cfg80211_txq_stats - TXQ statistics for this TID\n * @filled: bitmap of flags using the bits of &enum nl80211_txq_stats to\n *\tindicate the relevant values in this struct are filled\n * @backlog_bytes: total number of bytes currently backlogged\n * @backlog_packets: total number of packets currently backlogged\n * @flows: number of new flows seen\n * @drops: total number of packets dropped\n * @ecn_marks: total number of packets marked with ECN CE\n * @overlimit: number of drops due to queue space overflow\n * @overmemory: number of drops due to memory limit overflow\n * @collisions: number of hash collisions\n * @tx_bytes: total number of bytes dequeued\n * @tx_packets: total number of packets dequeued\n * @max_flows: maximum number of flows supported\n */\nstruct cfg80211_txq_stats {\n\tu32 filled;\n\tu32 backlog_bytes;\n\tu32 backlog_packets;\n\tu32 flows;\n\tu32 drops;\n\tu32 ecn_marks;\n\tu32 overlimit;\n\tu32 overmemory;\n\tu32 collisions;\n\tu32 tx_bytes;\n\tu32 tx_packets;\n\tu32 max_flows;\n};\n\n/**\n * struct cfg80211_tid_stats - per-TID statistics\n * @filled: bitmap of flags using the bits of &enum nl80211_tid_stats to\n *\tindicate the relevant values in this struct are filled\n * @rx_msdu: number of received MSDUs\n * @tx_msdu: number of (attempted) transmitted MSDUs\n * @tx_msdu_retries: number of retries (not counting the first) for\n *\ttransmitted MSDUs\n * @tx_msdu_failed: number of failed transmitted MSDUs\n * @txq_stats: TXQ statistics\n */\nstruct cfg80211_tid_stats {\n\tu32 filled;\n\tu64 rx_msdu;\n\tu64 tx_msdu;\n\tu64 tx_msdu_retries;\n\tu64 tx_msdu_failed;\n\tstruct cfg80211_txq_stats txq_stats;\n};\n\n#define IEEE80211_MAX_CHAINS\t4\n\n/**\n * struct station_info - station information\n *\n * Station information filled by driver for get_station() and dump_station.\n *\n * @filled: bitflag of flags using the bits of &enum nl80211_sta_info to\n *\tindicate the relevant values in this struct for them\n * @connected_time: time(in secs) since a station is last connected\n * @inactive_time: time since last station activity (tx/rx) in milliseconds\n * @assoc_at: bootime (ns) of the last association\n * @rx_bytes: bytes (size of MPDUs) received from this station\n * @tx_bytes: bytes (size of MPDUs) transmitted to this station\n * @llid: mesh local link id\n * @plid: mesh peer link id\n * @plink_state: mesh peer link state\n * @signal: The signal strength, type depends on the wiphy's signal_type.\n *\tFor CFG80211_SIGNAL_TYPE_MBM, value is expressed in _dBm_.\n * @signal_avg: Average signal strength, type depends on the wiphy's signal_type.\n *\tFor CFG80211_SIGNAL_TYPE_MBM, value is expressed in _dBm_.\n * @chains: bitmask for filled values in @chain_signal, @chain_signal_avg\n * @chain_signal: per-chain signal strength of last received packet in dBm\n * @chain_signal_avg: per-chain signal strength average in dBm\n * @txrate: current unicast bitrate from this station\n * @rxrate: current unicast bitrate to this station\n * @rx_packets: packets (MSDUs & MMPDUs) received from this station\n * @tx_packets: packets (MSDUs & MMPDUs) transmitted to this station\n * @tx_retries: cumulative retry counts (MPDUs)\n * @tx_failed: number of failed transmissions (MPDUs) (retries exceeded, no ACK)\n * @rx_dropped_misc:  Dropped for un-specified reason.\n * @bss_param: current BSS parameters\n * @generation: generation number for nl80211 dumps.\n *\tThis number should increase every time the list of stations\n *\tchanges, i.e. when a station is added or removed, so that\n *\tuserspace can tell whether it got a consistent snapshot.\n * @assoc_req_ies: IEs from (Re)Association Request.\n *\tThis is used only when in AP mode with drivers that do not use\n *\tuser space MLME/SME implementation. The information is provided for\n *\tthe cfg80211_new_sta() calls to notify user space of the IEs.\n * @assoc_req_ies_len: Length of assoc_req_ies buffer in octets.\n * @sta_flags: station flags mask & values\n * @beacon_loss_count: Number of times beacon loss event has triggered.\n * @t_offset: Time offset of the station relative to this host.\n * @local_pm: local mesh STA power save mode\n * @peer_pm: peer mesh STA power save mode\n * @nonpeer_pm: non-peer mesh STA power save mode\n * @expected_throughput: expected throughput in kbps (including 802.11 headers)\n *\ttowards this station.\n * @rx_beacon: number of beacons received from this peer\n * @rx_beacon_signal_avg: signal strength average (in dBm) for beacons received\n *\tfrom this peer\n * @connected_to_gate: true if mesh STA has a path to mesh gate\n * @rx_duration: aggregate PPDU duration(usecs) for all the frames from a peer\n * @tx_duration: aggregate PPDU duration(usecs) for all the frames to a peer\n * @airtime_weight: current airtime scheduling weight\n * @pertid: per-TID statistics, see &struct cfg80211_tid_stats, using the last\n *\t(IEEE80211_NUM_TIDS) index for MSDUs not encapsulated in QoS-MPDUs.\n *\tNote that this doesn't use the @filled bit, but is used if non-NULL.\n * @ack_signal: signal strength (in dBm) of the last ACK frame.\n * @avg_ack_signal: average rssi value of ack packet for the no of msdu's has\n *\tbeen sent.\n * @rx_mpdu_count: number of MPDUs received from this station\n * @fcs_err_count: number of packets (MPDUs) received from this station with\n *\tan FCS error. This counter should be incremented only when TA of the\n *\treceived packet with an FCS error matches the peer MAC address.\n * @airtime_link_metric: mesh airtime link metric.\n * @connected_to_as: true if mesh STA has a path to authentication server\n */\nstruct station_info {\n\tu64 filled;\n\tu32 connected_time;\n\tu32 inactive_time;\n\tu64 assoc_at;\n\tu64 rx_bytes;\n\tu64 tx_bytes;\n\tu16 llid;\n\tu16 plid;\n\tu8 plink_state;\n\ts8 signal;\n\ts8 signal_avg;\n\n\tu8 chains;\n\ts8 chain_signal[IEEE80211_MAX_CHAINS];\n\ts8 chain_signal_avg[IEEE80211_MAX_CHAINS];\n\n\tstruct rate_info txrate;\n\tstruct rate_info rxrate;\n\tu32 rx_packets;\n\tu32 tx_packets;\n\tu32 tx_retries;\n\tu32 tx_failed;\n\tu32 rx_dropped_misc;\n\tstruct sta_bss_parameters bss_param;\n\tstruct nl80211_sta_flag_update sta_flags;\n\n\tint generation;\n\n\tconst u8 *assoc_req_ies;\n\tsize_t assoc_req_ies_len;\n\n\tu32 beacon_loss_count;\n\ts64 t_offset;\n\tenum nl80211_mesh_power_mode local_pm;\n\tenum nl80211_mesh_power_mode peer_pm;\n\tenum nl80211_mesh_power_mode nonpeer_pm;\n\n\tu32 expected_throughput;\n\n\tu64 tx_duration;\n\tu64 rx_duration;\n\tu64 rx_beacon;\n\tu8 rx_beacon_signal_avg;\n\tu8 connected_to_gate;\n\n\tstruct cfg80211_tid_stats *pertid;\n\ts8 ack_signal;\n\ts8 avg_ack_signal;\n\n\tu16 airtime_weight;\n\n\tu32 rx_mpdu_count;\n\tu32 fcs_err_count;\n\n\tu32 airtime_link_metric;\n\n\tu8 connected_to_as;\n};\n\n/**\n * struct cfg80211_sar_sub_specs - sub specs limit\n * @power: power limitation in 0.25dbm\n * @freq_range_index: index the power limitation applies to\n */\nstruct cfg80211_sar_sub_specs {\n\ts32 power;\n\tu32 freq_range_index;\n};\n\n/**\n * struct cfg80211_sar_specs - sar limit specs\n * @type: it's set with power in 0.25dbm or other types\n * @num_sub_specs: number of sar sub specs\n * @sub_specs: memory to hold the sar sub specs\n */\nstruct cfg80211_sar_specs {\n\tenum nl80211_sar_type type;\n\tu32 num_sub_specs;\n\tstruct cfg80211_sar_sub_specs sub_specs[];\n};\n\n\n/**\n * struct cfg80211_sar_freq_ranges - sar frequency ranges\n * @start_freq:  start range edge frequency\n * @end_freq:    end range edge frequency\n */\nstruct cfg80211_sar_freq_ranges {\n\tu32 start_freq;\n\tu32 end_freq;\n};\n\n/**\n * struct cfg80211_sar_capa - sar limit capability\n * @type: it's set via power in 0.25dbm or other types\n * @num_freq_ranges: number of frequency ranges\n * @freq_ranges: memory to hold the freq ranges.\n *\n * Note: WLAN driver may append new ranges or split an existing\n * range to small ones and then append them.\n */\nstruct cfg80211_sar_capa {\n\tenum nl80211_sar_type type;\n\tu32 num_freq_ranges;\n\tconst struct cfg80211_sar_freq_ranges *freq_ranges;\n};\n\n#if IS_ENABLED(CONFIG_CFG80211)\n/**\n * cfg80211_get_station - retrieve information about a given station\n * @dev: the device where the station is supposed to be connected to\n * @mac_addr: the mac address of the station of interest\n * @sinfo: pointer to the structure to fill with the information\n *\n * Returns 0 on success and sinfo is filled with the available information\n * otherwise returns a negative error code and the content of sinfo has to be\n * considered undefined.\n */\nint cfg80211_get_station(struct net_device *dev, const u8 *mac_addr,\n\t\t\t struct station_info *sinfo);\n#else\nstatic inline int cfg80211_get_station(struct net_device *dev,\n\t\t\t\t       const u8 *mac_addr,\n\t\t\t\t       struct station_info *sinfo)\n{\n\treturn -ENOENT;\n}\n#endif\n\n/**\n * enum monitor_flags - monitor flags\n *\n * Monitor interface configuration flags. Note that these must be the bits\n * according to the nl80211 flags.\n *\n * @MONITOR_FLAG_CHANGED: set if the flags were changed\n * @MONITOR_FLAG_FCSFAIL: pass frames with bad FCS\n * @MONITOR_FLAG_PLCPFAIL: pass frames with bad PLCP\n * @MONITOR_FLAG_CONTROL: pass control frames\n * @MONITOR_FLAG_OTHER_BSS: disable BSSID filtering\n * @MONITOR_FLAG_COOK_FRAMES: report frames after processing\n * @MONITOR_FLAG_ACTIVE: active monitor, ACKs frames on its MAC address\n */\nenum monitor_flags {\n\tMONITOR_FLAG_CHANGED\t\t= 1<<__NL80211_MNTR_FLAG_INVALID,\n\tMONITOR_FLAG_FCSFAIL\t\t= 1<<NL80211_MNTR_FLAG_FCSFAIL,\n\tMONITOR_FLAG_PLCPFAIL\t\t= 1<<NL80211_MNTR_FLAG_PLCPFAIL,\n\tMONITOR_FLAG_CONTROL\t\t= 1<<NL80211_MNTR_FLAG_CONTROL,\n\tMONITOR_FLAG_OTHER_BSS\t\t= 1<<NL80211_MNTR_FLAG_OTHER_BSS,\n\tMONITOR_FLAG_COOK_FRAMES\t= 1<<NL80211_MNTR_FLAG_COOK_FRAMES,\n\tMONITOR_FLAG_ACTIVE\t\t= 1<<NL80211_MNTR_FLAG_ACTIVE,\n};\n\n/**\n * enum mpath_info_flags -  mesh path information flags\n *\n * Used by the driver to indicate which info in &struct mpath_info it has filled\n * in during get_station() or dump_station().\n *\n * @MPATH_INFO_FRAME_QLEN: @frame_qlen filled\n * @MPATH_INFO_SN: @sn filled\n * @MPATH_INFO_METRIC: @metric filled\n * @MPATH_INFO_EXPTIME: @exptime filled\n * @MPATH_INFO_DISCOVERY_TIMEOUT: @discovery_timeout filled\n * @MPATH_INFO_DISCOVERY_RETRIES: @discovery_retries filled\n * @MPATH_INFO_FLAGS: @flags filled\n * @MPATH_INFO_HOP_COUNT: @hop_count filled\n * @MPATH_INFO_PATH_CHANGE: @path_change_count filled\n */\nenum mpath_info_flags {\n\tMPATH_INFO_FRAME_QLEN\t\t= BIT(0),\n\tMPATH_INFO_SN\t\t\t= BIT(1),\n\tMPATH_INFO_METRIC\t\t= BIT(2),\n\tMPATH_INFO_EXPTIME\t\t= BIT(3),\n\tMPATH_INFO_DISCOVERY_TIMEOUT\t= BIT(4),\n\tMPATH_INFO_DISCOVERY_RETRIES\t= BIT(5),\n\tMPATH_INFO_FLAGS\t\t= BIT(6),\n\tMPATH_INFO_HOP_COUNT\t\t= BIT(7),\n\tMPATH_INFO_PATH_CHANGE\t\t= BIT(8),\n};\n\n/**\n * struct mpath_info - mesh path information\n *\n * Mesh path information filled by driver for get_mpath() and dump_mpath().\n *\n * @filled: bitfield of flags from &enum mpath_info_flags\n * @frame_qlen: number of queued frames for this destination\n * @sn: target sequence number\n * @metric: metric (cost) of this mesh path\n * @exptime: expiration time for the mesh path from now, in msecs\n * @flags: mesh path flags\n * @discovery_timeout: total mesh path discovery timeout, in msecs\n * @discovery_retries: mesh path discovery retries\n * @generation: generation number for nl80211 dumps.\n *\tThis number should increase every time the list of mesh paths\n *\tchanges, i.e. when a station is added or removed, so that\n *\tuserspace can tell whether it got a consistent snapshot.\n * @hop_count: hops to destination\n * @path_change_count: total number of path changes to destination\n */\nstruct mpath_info {\n\tu32 filled;\n\tu32 frame_qlen;\n\tu32 sn;\n\tu32 metric;\n\tu32 exptime;\n\tu32 discovery_timeout;\n\tu8 discovery_retries;\n\tu8 flags;\n\tu8 hop_count;\n\tu32 path_change_count;\n\n\tint generation;\n};\n\n/**\n * struct bss_parameters - BSS parameters\n *\n * Used to change BSS parameters (mainly for AP mode).\n *\n * @use_cts_prot: Whether to use CTS protection\n *\t(0 = no, 1 = yes, -1 = do not change)\n * @use_short_preamble: Whether the use of short preambles is allowed\n *\t(0 = no, 1 = yes, -1 = do not change)\n * @use_short_slot_time: Whether the use of short slot time is allowed\n *\t(0 = no, 1 = yes, -1 = do not change)\n * @basic_rates: basic rates in IEEE 802.11 format\n *\t(or NULL for no change)\n * @basic_rates_len: number of basic rates\n * @ap_isolate: do not forward packets between connected stations\n *\t(0 = no, 1 = yes, -1 = do not change)\n * @ht_opmode: HT Operation mode\n *\t(u16 = opmode, -1 = do not change)\n * @p2p_ctwindow: P2P CT Window (-1 = no change)\n * @p2p_opp_ps: P2P opportunistic PS (-1 = no change)\n */\nstruct bss_parameters {\n\tint use_cts_prot;\n\tint use_short_preamble;\n\tint use_short_slot_time;\n\tconst u8 *basic_rates;\n\tu8 basic_rates_len;\n\tint ap_isolate;\n\tint ht_opmode;\n\ts8 p2p_ctwindow, p2p_opp_ps;\n};\n\n/**\n * struct mesh_config - 802.11s mesh configuration\n *\n * These parameters can be changed while the mesh is active.\n *\n * @dot11MeshRetryTimeout: the initial retry timeout in millisecond units used\n *\tby the Mesh Peering Open message\n * @dot11MeshConfirmTimeout: the initial retry timeout in millisecond units\n *\tused by the Mesh Peering Open message\n * @dot11MeshHoldingTimeout: the confirm timeout in millisecond units used by\n *\tthe mesh peering management to close a mesh peering\n * @dot11MeshMaxPeerLinks: the maximum number of peer links allowed on this\n *\tmesh interface\n * @dot11MeshMaxRetries: the maximum number of peer link open retries that can\n *\tbe sent to establish a new peer link instance in a mesh\n * @dot11MeshTTL: the value of TTL field set at a source mesh STA\n * @element_ttl: the value of TTL field set at a mesh STA for path selection\n *\telements\n * @auto_open_plinks: whether we should automatically open peer links when we\n *\tdetect compatible mesh peers\n * @dot11MeshNbrOffsetMaxNeighbor: the maximum number of neighbors to\n *\tsynchronize to for 11s default synchronization method\n * @dot11MeshHWMPmaxPREQretries: the number of action frames containing a PREQ\n *\tthat an originator mesh STA can send to a particular path target\n * @path_refresh_time: how frequently to refresh mesh paths in milliseconds\n * @min_discovery_timeout: the minimum length of time to wait until giving up on\n *\ta path discovery in milliseconds\n * @dot11MeshHWMPactivePathTimeout: the time (in TUs) for which mesh STAs\n *\treceiving a PREQ shall consider the forwarding information from the\n *\troot to be valid. (TU = time unit)\n * @dot11MeshHWMPpreqMinInterval: the minimum interval of time (in TUs) during\n *\twhich a mesh STA can send only one action frame containing a PREQ\n *\telement\n * @dot11MeshHWMPperrMinInterval: the minimum interval of time (in TUs) during\n *\twhich a mesh STA can send only one Action frame containing a PERR\n *\telement\n * @dot11MeshHWMPnetDiameterTraversalTime: the interval of time (in TUs) that\n *\tit takes for an HWMP information element to propagate across the mesh\n * @dot11MeshHWMPRootMode: the configuration of a mesh STA as root mesh STA\n * @dot11MeshHWMPRannInterval: the interval of time (in TUs) between root\n *\tannouncements are transmitted\n * @dot11MeshGateAnnouncementProtocol: whether to advertise that this mesh\n *\tstation has access to a broader network beyond the MBSS. (This is\n *\tmissnamed in draft 12.0: dot11MeshGateAnnouncementProtocol set to true\n *\tonly means that the station will announce others it's a mesh gate, but\n *\tnot necessarily using the gate announcement protocol. Still keeping the\n *\tsame nomenclature to be in sync with the spec)\n * @dot11MeshForwarding: whether the Mesh STA is forwarding or non-forwarding\n *\tentity (default is TRUE - forwarding entity)\n * @rssi_threshold: the threshold for average signal strength of candidate\n *\tstation to establish a peer link\n * @ht_opmode: mesh HT protection mode\n *\n * @dot11MeshHWMPactivePathToRootTimeout: The time (in TUs) for which mesh STAs\n *\treceiving a proactive PREQ shall consider the forwarding information to\n *\tthe root mesh STA to be valid.\n *\n * @dot11MeshHWMProotInterval: The interval of time (in TUs) between proactive\n *\tPREQs are transmitted.\n * @dot11MeshHWMPconfirmationInterval: The minimum interval of time (in TUs)\n *\tduring which a mesh STA can send only one Action frame containing\n *\ta PREQ element for root path confirmation.\n * @power_mode: The default mesh power save mode which will be the initial\n *\tsetting for new peer links.\n * @dot11MeshAwakeWindowDuration: The duration in TUs the STA will remain awake\n *\tafter transmitting its beacon.\n * @plink_timeout: If no tx activity is seen from a STA we've established\n *\tpeering with for longer than this time (in seconds), then remove it\n *\tfrom the STA's list of peers.  Default is 30 minutes.\n * @dot11MeshConnectedToMeshGate: if set to true, advertise that this STA is\n *      connected to a mesh gate in mesh formation info.  If false, the\n *      value in mesh formation is determined by the presence of root paths\n *      in the mesh path table\n * @dot11MeshNolearn: Try to avoid multi-hop path discovery (e.g. PREQ/PREP\n *      for HWMP) if the destination is a direct neighbor. Note that this might\n *      not be the optimal decision as a multi-hop route might be better. So\n *      if using this setting you will likely also want to disable\n *      dot11MeshForwarding and use another mesh routing protocol on top.\n */\nstruct mesh_config {\n\tu16 dot11MeshRetryTimeout;\n\tu16 dot11MeshConfirmTimeout;\n\tu16 dot11MeshHoldingTimeout;\n\tu16 dot11MeshMaxPeerLinks;\n\tu8 dot11MeshMaxRetries;\n\tu8 dot11MeshTTL;\n\tu8 element_ttl;\n\tbool auto_open_plinks;\n\tu32 dot11MeshNbrOffsetMaxNeighbor;\n\tu8 dot11MeshHWMPmaxPREQretries;\n\tu32 path_refresh_time;\n\tu16 min_discovery_timeout;\n\tu32 dot11MeshHWMPactivePathTimeout;\n\tu16 dot11MeshHWMPpreqMinInterval;\n\tu16 dot11MeshHWMPperrMinInterval;\n\tu16 dot11MeshHWMPnetDiameterTraversalTime;\n\tu8 dot11MeshHWMPRootMode;\n\tbool dot11MeshConnectedToMeshGate;\n\tbool dot11MeshConnectedToAuthServer;\n\tu16 dot11MeshHWMPRannInterval;\n\tbool dot11MeshGateAnnouncementProtocol;\n\tbool dot11MeshForwarding;\n\ts32 rssi_threshold;\n\tu16 ht_opmode;\n\tu32 dot11MeshHWMPactivePathToRootTimeout;\n\tu16 dot11MeshHWMProotInterval;\n\tu16 dot11MeshHWMPconfirmationInterval;\n\tenum nl80211_mesh_power_mode power_mode;\n\tu16 dot11MeshAwakeWindowDuration;\n\tu32 plink_timeout;\n\tbool dot11MeshNolearn;\n};\n\n/**\n * struct mesh_setup - 802.11s mesh setup configuration\n * @chandef: defines the channel to use\n * @mesh_id: the mesh ID\n * @mesh_id_len: length of the mesh ID, at least 1 and at most 32 bytes\n * @sync_method: which synchronization method to use\n * @path_sel_proto: which path selection protocol to use\n * @path_metric: which metric to use\n * @auth_id: which authentication method this mesh is using\n * @ie: vendor information elements (optional)\n * @ie_len: length of vendor information elements\n * @is_authenticated: this mesh requires authentication\n * @is_secure: this mesh uses security\n * @user_mpm: userspace handles all MPM functions\n * @dtim_period: DTIM period to use\n * @beacon_interval: beacon interval to use\n * @mcast_rate: multicat rate for Mesh Node [6Mbps is the default for 802.11a]\n * @basic_rates: basic rates to use when creating the mesh\n * @beacon_rate: bitrate to be used for beacons\n * @userspace_handles_dfs: whether user space controls DFS operation, i.e.\n *\tchanges the channel when a radar is detected. This is required\n *\tto operate on DFS channels.\n * @control_port_over_nl80211: TRUE if userspace expects to exchange control\n *\tport frames over NL80211 instead of the network interface.\n *\n * These parameters are fixed when the mesh is created.\n */\nstruct mesh_setup {\n\tstruct cfg80211_chan_def chandef;\n\tconst u8 *mesh_id;\n\tu8 mesh_id_len;\n\tu8 sync_method;\n\tu8 path_sel_proto;\n\tu8 path_metric;\n\tu8 auth_id;\n\tconst u8 *ie;\n\tu8 ie_len;\n\tbool is_authenticated;\n\tbool is_secure;\n\tbool user_mpm;\n\tu8 dtim_period;\n\tu16 beacon_interval;\n\tint mcast_rate[NUM_NL80211_BANDS];\n\tu32 basic_rates;\n\tstruct cfg80211_bitrate_mask beacon_rate;\n\tbool userspace_handles_dfs;\n\tbool control_port_over_nl80211;\n};\n\n/**\n * struct ocb_setup - 802.11p OCB mode setup configuration\n * @chandef: defines the channel to use\n *\n * These parameters are fixed when connecting to the network\n */\nstruct ocb_setup {\n\tstruct cfg80211_chan_def chandef;\n};\n\n/**\n * struct ieee80211_txq_params - TX queue parameters\n * @ac: AC identifier\n * @txop: Maximum burst time in units of 32 usecs, 0 meaning disabled\n * @cwmin: Minimum contention window [a value of the form 2^n-1 in the range\n *\t1..32767]\n * @cwmax: Maximum contention window [a value of the form 2^n-1 in the range\n *\t1..32767]\n * @aifs: Arbitration interframe space [0..255]\n */\nstruct ieee80211_txq_params {\n\tenum nl80211_ac ac;\n\tu16 txop;\n\tu16 cwmin;\n\tu16 cwmax;\n\tu8 aifs;\n};\n\n/**\n * DOC: Scanning and BSS list handling\n *\n * The scanning process itself is fairly simple, but cfg80211 offers quite\n * a bit of helper functionality. To start a scan, the scan operation will\n * be invoked with a scan definition. This scan definition contains the\n * channels to scan, and the SSIDs to send probe requests for (including the\n * wildcard, if desired). A passive scan is indicated by having no SSIDs to\n * probe. Additionally, a scan request may contain extra information elements\n * that should be added to the probe request. The IEs are guaranteed to be\n * well-formed, and will not exceed the maximum length the driver advertised\n * in the wiphy structure.\n *\n * When scanning finds a BSS, cfg80211 needs to be notified of that, because\n * it is responsible for maintaining the BSS list; the driver should not\n * maintain a list itself. For this notification, various functions exist.\n *\n * Since drivers do not maintain a BSS list, there are also a number of\n * functions to search for a BSS and obtain information about it from the\n * BSS structure cfg80211 maintains. The BSS list is also made available\n * to userspace.\n */\n\n/**\n * struct cfg80211_ssid - SSID description\n * @ssid: the SSID\n * @ssid_len: length of the ssid\n */\nstruct cfg80211_ssid {\n\tu8 ssid[IEEE80211_MAX_SSID_LEN];\n\tu8 ssid_len;\n};\n\n/**\n * struct cfg80211_scan_info - information about completed scan\n * @scan_start_tsf: scan start time in terms of the TSF of the BSS that the\n *\twireless device that requested the scan is connected to. If this\n *\tinformation is not available, this field is left zero.\n * @tsf_bssid: the BSSID according to which %scan_start_tsf is set.\n * @aborted: set to true if the scan was aborted for any reason,\n *\tuserspace will be notified of that\n */\nstruct cfg80211_scan_info {\n\tu64 scan_start_tsf;\n\tu8 tsf_bssid[ETH_ALEN] __aligned(2);\n\tbool aborted;\n};\n\n/**\n * struct cfg80211_scan_6ghz_params - relevant for 6 GHz only\n *\n * @short_bssid: short ssid to scan for\n * @bssid: bssid to scan for\n * @channel_idx: idx of the channel in the channel array in the scan request\n *\t which the above info relvant to\n * @unsolicited_probe: the AP transmits unsolicited probe response every 20 TU\n * @short_ssid_valid: short_ssid is valid and can be used\n * @psc_no_listen: when set, and the channel is a PSC channel, no need to wait\n *       20 TUs before starting to send probe requests.\n */\nstruct cfg80211_scan_6ghz_params {\n\tu32 short_ssid;\n\tu32 channel_idx;\n\tu8 bssid[ETH_ALEN];\n\tbool unsolicited_probe;\n\tbool short_ssid_valid;\n\tbool psc_no_listen;\n};\n\n/**\n * struct cfg80211_scan_request - scan request description\n *\n * @ssids: SSIDs to scan for (active scan only)\n * @n_ssids: number of SSIDs\n * @channels: channels to scan on.\n * @n_channels: total number of channels to scan\n * @scan_width: channel width for scanning\n * @ie: optional information element(s) to add into Probe Request or %NULL\n * @ie_len: length of ie in octets\n * @duration: how long to listen on each channel, in TUs. If\n *\t%duration_mandatory is not set, this is the maximum dwell time and\n *\tthe actual dwell time may be shorter.\n * @duration_mandatory: if set, the scan duration must be as specified by the\n *\t%duration field.\n * @flags: bit field of flags controlling operation\n * @rates: bitmap of rates to advertise for each band\n * @wiphy: the wiphy this was for\n * @scan_start: time (in jiffies) when the scan started\n * @wdev: the wireless device to scan for\n * @info: (internal) information about completed scan\n * @notified: (internal) scan request was notified as done or aborted\n * @no_cck: used to send probe requests at non CCK rate in 2GHz band\n * @mac_addr: MAC address used with randomisation\n * @mac_addr_mask: MAC address mask used with randomisation, bits that\n *\tare 0 in the mask should be randomised, bits that are 1 should\n *\tbe taken from the @mac_addr\n * @scan_6ghz: relevant for split scan request only,\n *\ttrue if this is the second scan request\n * @n_6ghz_params: number of 6 GHz params\n * @scan_6ghz_params: 6 GHz params\n * @bssid: BSSID to scan for (most commonly, the wildcard BSSID)\n */\nstruct cfg80211_scan_request {\n\tstruct cfg80211_ssid *ssids;\n\tint n_ssids;\n\tu32 n_channels;\n\tenum nl80211_bss_scan_width scan_width;\n\tconst u8 *ie;\n\tsize_t ie_len;\n\tu16 duration;\n\tbool duration_mandatory;\n\tu32 flags;\n\n\tu32 rates[NUM_NL80211_BANDS];\n\n\tstruct wireless_dev *wdev;\n\n\tu8 mac_addr[ETH_ALEN] __aligned(2);\n\tu8 mac_addr_mask[ETH_ALEN] __aligned(2);\n\tu8 bssid[ETH_ALEN] __aligned(2);\n\n\t/* internal */\n\tstruct wiphy *wiphy;\n\tunsigned long scan_start;\n\tstruct cfg80211_scan_info info;\n\tbool notified;\n\tbool no_cck;\n\tbool scan_6ghz;\n\tu32 n_6ghz_params;\n\tstruct cfg80211_scan_6ghz_params *scan_6ghz_params;\n\n\t/* keep last */\n\tstruct ieee80211_channel *channels[];\n};\n\nstatic inline void get_random_mask_addr(u8 *buf, const u8 *addr, const u8 *mask)\n{\n\tint i;\n\n\tget_random_bytes(buf, ETH_ALEN);\n\tfor (i = 0; i < ETH_ALEN; i++) {\n\t\tbuf[i] &= ~mask[i];\n\t\tbuf[i] |= addr[i] & mask[i];\n\t}\n}\n\n/**\n * struct cfg80211_match_set - sets of attributes to match\n *\n * @ssid: SSID to be matched; may be zero-length in case of BSSID match\n *\tor no match (RSSI only)\n * @bssid: BSSID to be matched; may be all-zero BSSID in case of SSID match\n *\tor no match (RSSI only)\n * @rssi_thold: don't report scan results below this threshold (in s32 dBm)\n * @per_band_rssi_thold: Minimum rssi threshold for each band to be applied\n *\tfor filtering out scan results received. Drivers advertize this support\n *\tof band specific rssi based filtering through the feature capability\n *\t%NL80211_EXT_FEATURE_SCHED_SCAN_BAND_SPECIFIC_RSSI_THOLD. These band\n *\tspecific rssi thresholds take precedence over rssi_thold, if specified.\n *\tIf not specified for any band, it will be assigned with rssi_thold of\n *\tcorresponding matchset.\n */\nstruct cfg80211_match_set {\n\tstruct cfg80211_ssid ssid;\n\tu8 bssid[ETH_ALEN];\n\ts32 rssi_thold;\n\ts32 per_band_rssi_thold[NUM_NL80211_BANDS];\n};\n\n/**\n * struct cfg80211_sched_scan_plan - scan plan for scheduled scan\n *\n * @interval: interval between scheduled scan iterations. In seconds.\n * @iterations: number of scan iterations in this scan plan. Zero means\n *\tinfinite loop.\n *\tThe last scan plan will always have this parameter set to zero,\n *\tall other scan plans will have a finite number of iterations.\n */\nstruct cfg80211_sched_scan_plan {\n\tu32 interval;\n\tu32 iterations;\n};\n\n/**\n * struct cfg80211_bss_select_adjust - BSS selection with RSSI adjustment.\n *\n * @band: band of BSS which should match for RSSI level adjustment.\n * @delta: value of RSSI level adjustment.\n */\nstruct cfg80211_bss_select_adjust {\n\tenum nl80211_band band;\n\ts8 delta;\n};\n\n/**\n * struct cfg80211_sched_scan_request - scheduled scan request description\n *\n * @reqid: identifies this request.\n * @ssids: SSIDs to scan for (passed in the probe_reqs in active scans)\n * @n_ssids: number of SSIDs\n * @n_channels: total number of channels to scan\n * @scan_width: channel width for scanning\n * @ie: optional information element(s) to add into Probe Request or %NULL\n * @ie_len: length of ie in octets\n * @flags: bit field of flags controlling operation\n * @match_sets: sets of parameters to be matched for a scan result\n *\tentry to be considered valid and to be passed to the host\n *\t(others are filtered out).\n *\tIf ommited, all results are passed.\n * @n_match_sets: number of match sets\n * @report_results: indicates that results were reported for this request\n * @wiphy: the wiphy this was for\n * @dev: the interface\n * @scan_start: start time of the scheduled scan\n * @channels: channels to scan\n * @min_rssi_thold: for drivers only supporting a single threshold, this\n *\tcontains the minimum over all matchsets\n * @mac_addr: MAC address used with randomisation\n * @mac_addr_mask: MAC address mask used with randomisation, bits that\n *\tare 0 in the mask should be randomised, bits that are 1 should\n *\tbe taken from the @mac_addr\n * @scan_plans: scan plans to be executed in this scheduled scan. Lowest\n *\tindex must be executed first.\n * @n_scan_plans: number of scan plans, at least 1.\n * @rcu_head: RCU callback used to free the struct\n * @owner_nlportid: netlink portid of owner (if this should is a request\n *\towned by a particular socket)\n * @nl_owner_dead: netlink owner socket was closed - this request be freed\n * @list: for keeping list of requests.\n * @delay: delay in seconds to use before starting the first scan\n *\tcycle.  The driver may ignore this parameter and start\n *\timmediately (or at any other time), if this feature is not\n *\tsupported.\n * @relative_rssi_set: Indicates whether @relative_rssi is set or not.\n * @relative_rssi: Relative RSSI threshold in dB to restrict scan result\n *\treporting in connected state to cases where a matching BSS is determined\n *\tto have better or slightly worse RSSI than the current connected BSS.\n *\tThe relative RSSI threshold values are ignored in disconnected state.\n * @rssi_adjust: delta dB of RSSI preference to be given to the BSSs that belong\n *\tto the specified band while deciding whether a better BSS is reported\n *\tusing @relative_rssi. If delta is a negative number, the BSSs that\n *\tbelong to the specified band will be penalized by delta dB in relative\n *\tcomparisions.\n */\nstruct cfg80211_sched_scan_request {\n\tu64 reqid;\n\tstruct cfg80211_ssid *ssids;\n\tint n_ssids;\n\tu32 n_channels;\n\tenum nl80211_bss_scan_width scan_width;\n\tconst u8 *ie;\n\tsize_t ie_len;\n\tu32 flags;\n\tstruct cfg80211_match_set *match_sets;\n\tint n_match_sets;\n\ts32 min_rssi_thold;\n\tu32 delay;\n\tstruct cfg80211_sched_scan_plan *scan_plans;\n\tint n_scan_plans;\n\n\tu8 mac_addr[ETH_ALEN] __aligned(2);\n\tu8 mac_addr_mask[ETH_ALEN] __aligned(2);\n\n\tbool relative_rssi_set;\n\ts8 relative_rssi;\n\tstruct cfg80211_bss_select_adjust rssi_adjust;\n\n\t/* internal */\n\tstruct wiphy *wiphy;\n\tstruct net_device *dev;\n\tunsigned long scan_start;\n\tbool report_results;\n\tstruct rcu_head rcu_head;\n\tu32 owner_nlportid;\n\tbool nl_owner_dead;\n\tstruct list_head list;\n\n\t/* keep last */\n\tstruct ieee80211_channel *channels[];\n};\n\n/**\n * enum cfg80211_signal_type - signal type\n *\n * @CFG80211_SIGNAL_TYPE_NONE: no signal strength information available\n * @CFG80211_SIGNAL_TYPE_MBM: signal strength in mBm (100*dBm)\n * @CFG80211_SIGNAL_TYPE_UNSPEC: signal strength, increasing from 0 through 100\n */\nenum cfg80211_signal_type {\n\tCFG80211_SIGNAL_TYPE_NONE,\n\tCFG80211_SIGNAL_TYPE_MBM,\n\tCFG80211_SIGNAL_TYPE_UNSPEC,\n};\n\n/**\n * struct cfg80211_inform_bss - BSS inform data\n * @chan: channel the frame was received on\n * @scan_width: scan width that was used\n * @signal: signal strength value, according to the wiphy's\n *\tsignal type\n * @boottime_ns: timestamp (CLOCK_BOOTTIME) when the information was\n *\treceived; should match the time when the frame was actually\n *\treceived by the device (not just by the host, in case it was\n *\tbuffered on the device) and be accurate to about 10ms.\n *\tIf the frame isn't buffered, just passing the return value of\n *\tktime_get_boottime_ns() is likely appropriate.\n * @parent_tsf: the time at the start of reception of the first octet of the\n *\ttimestamp field of the frame. The time is the TSF of the BSS specified\n *\tby %parent_bssid.\n * @parent_bssid: the BSS according to which %parent_tsf is set. This is set to\n *\tthe BSS that requested the scan in which the beacon/probe was received.\n * @chains: bitmask for filled values in @chain_signal.\n * @chain_signal: per-chain signal strength of last received BSS in dBm.\n */\nstruct cfg80211_inform_bss {\n\tstruct ieee80211_channel *chan;\n\tenum nl80211_bss_scan_width scan_width;\n\ts32 signal;\n\tu64 boottime_ns;\n\tu64 parent_tsf;\n\tu8 parent_bssid[ETH_ALEN] __aligned(2);\n\tu8 chains;\n\ts8 chain_signal[IEEE80211_MAX_CHAINS];\n};\n\n/**\n * struct cfg80211_bss_ies - BSS entry IE data\n * @tsf: TSF contained in the frame that carried these IEs\n * @rcu_head: internal use, for freeing\n * @len: length of the IEs\n * @from_beacon: these IEs are known to come from a beacon\n * @data: IE data\n */\nstruct cfg80211_bss_ies {\n\tu64 tsf;\n\tstruct rcu_head rcu_head;\n\tint len;\n\tbool from_beacon;\n\tu8 data[];\n};\n\n/**\n * struct cfg80211_bss - BSS description\n *\n * This structure describes a BSS (which may also be a mesh network)\n * for use in scan results and similar.\n *\n * @channel: channel this BSS is on\n * @scan_width: width of the control channel\n * @bssid: BSSID of the BSS\n * @beacon_interval: the beacon interval as from the frame\n * @capability: the capability field in host byte order\n * @ies: the information elements (Note that there is no guarantee that these\n *\tare well-formed!); this is a pointer to either the beacon_ies or\n *\tproberesp_ies depending on whether Probe Response frame has been\n *\treceived. It is always non-%NULL.\n * @beacon_ies: the information elements from the last Beacon frame\n *\t(implementation note: if @hidden_beacon_bss is set this struct doesn't\n *\town the beacon_ies, but they're just pointers to the ones from the\n *\t@hidden_beacon_bss struct)\n * @proberesp_ies: the information elements from the last Probe Response frame\n * @hidden_beacon_bss: in case this BSS struct represents a probe response from\n *\ta BSS that hides the SSID in its beacon, this points to the BSS struct\n *\tthat holds the beacon data. @beacon_ies is still valid, of course, and\n *\tpoints to the same data as hidden_beacon_bss->beacon_ies in that case.\n * @transmitted_bss: pointer to the transmitted BSS, if this is a\n *\tnon-transmitted one (multi-BSSID support)\n * @nontrans_list: list of non-transmitted BSS, if this is a transmitted one\n *\t(multi-BSSID support)\n * @signal: signal strength value (type depends on the wiphy's signal_type)\n * @chains: bitmask for filled values in @chain_signal.\n * @chain_signal: per-chain signal strength of last received BSS in dBm.\n * @bssid_index: index in the multiple BSS set\n * @max_bssid_indicator: max number of members in the BSS set\n * @priv: private area for driver use, has at least wiphy->bss_priv_size bytes\n */\nstruct cfg80211_bss {\n\tstruct ieee80211_channel *channel;\n\tenum nl80211_bss_scan_width scan_width;\n\n\tconst struct cfg80211_bss_ies __rcu *ies;\n\tconst struct cfg80211_bss_ies __rcu *beacon_ies;\n\tconst struct cfg80211_bss_ies __rcu *proberesp_ies;\n\n\tstruct cfg80211_bss *hidden_beacon_bss;\n\tstruct cfg80211_bss *transmitted_bss;\n\tstruct list_head nontrans_list;\n\n\ts32 signal;\n\n\tu16 beacon_interval;\n\tu16 capability;\n\n\tu8 bssid[ETH_ALEN];\n\tu8 chains;\n\ts8 chain_signal[IEEE80211_MAX_CHAINS];\n\n\tu8 bssid_index;\n\tu8 max_bssid_indicator;\n\n\tu8 priv[] __aligned(sizeof(void *));\n};\n\n/**\n * ieee80211_bss_get_elem - find element with given ID\n * @bss: the bss to search\n * @id: the element ID\n *\n * Note that the return value is an RCU-protected pointer, so\n * rcu_read_lock() must be held when calling this function.\n * Return: %NULL if not found.\n */\nconst struct element *ieee80211_bss_get_elem(struct cfg80211_bss *bss, u8 id);\n\n/**\n * ieee80211_bss_get_ie - find IE with given ID\n * @bss: the bss to search\n * @id: the element ID\n *\n * Note that the return value is an RCU-protected pointer, so\n * rcu_read_lock() must be held when calling this function.\n * Return: %NULL if not found.\n */\nstatic inline const u8 *ieee80211_bss_get_ie(struct cfg80211_bss *bss, u8 id)\n{\n\treturn (void *)ieee80211_bss_get_elem(bss, id);\n}\n\n\n/**\n * struct cfg80211_auth_request - Authentication request data\n *\n * This structure provides information needed to complete IEEE 802.11\n * authentication.\n *\n * @bss: The BSS to authenticate with, the callee must obtain a reference\n *\tto it if it needs to keep it.\n * @auth_type: Authentication type (algorithm)\n * @ie: Extra IEs to add to Authentication frame or %NULL\n * @ie_len: Length of ie buffer in octets\n * @key_len: length of WEP key for shared key authentication\n * @key_idx: index of WEP key for shared key authentication\n * @key: WEP key for shared key authentication\n * @auth_data: Fields and elements in Authentication frames. This contains\n *\tthe authentication frame body (non-IE and IE data), excluding the\n *\tAuthentication algorithm number, i.e., starting at the Authentication\n *\ttransaction sequence number field.\n * @auth_data_len: Length of auth_data buffer in octets\n */\nstruct cfg80211_auth_request {\n\tstruct cfg80211_bss *bss;\n\tconst u8 *ie;\n\tsize_t ie_len;\n\tenum nl80211_auth_type auth_type;\n\tconst u8 *key;\n\tu8 key_len, key_idx;\n\tconst u8 *auth_data;\n\tsize_t auth_data_len;\n};\n\n/**\n * enum cfg80211_assoc_req_flags - Over-ride default behaviour in association.\n *\n * @ASSOC_REQ_DISABLE_HT:  Disable HT (802.11n)\n * @ASSOC_REQ_DISABLE_VHT:  Disable VHT\n * @ASSOC_REQ_USE_RRM: Declare RRM capability in this association\n * @CONNECT_REQ_EXTERNAL_AUTH_SUPPORT: User space indicates external\n *\tauthentication capability. Drivers can offload authentication to\n *\tuserspace if this flag is set. Only applicable for cfg80211_connect()\n *\trequest (connect callback).\n * @ASSOC_REQ_DISABLE_HE:  Disable HE\n */\nenum cfg80211_assoc_req_flags {\n\tASSOC_REQ_DISABLE_HT\t\t\t= BIT(0),\n\tASSOC_REQ_DISABLE_VHT\t\t\t= BIT(1),\n\tASSOC_REQ_USE_RRM\t\t\t= BIT(2),\n\tCONNECT_REQ_EXTERNAL_AUTH_SUPPORT\t= BIT(3),\n\tASSOC_REQ_DISABLE_HE\t\t\t= BIT(4),\n};\n\n/**\n * struct cfg80211_assoc_request - (Re)Association request data\n *\n * This structure provides information needed to complete IEEE 802.11\n * (re)association.\n * @bss: The BSS to associate with. If the call is successful the driver is\n *\tgiven a reference that it must give back to cfg80211_send_rx_assoc()\n *\tor to cfg80211_assoc_timeout(). To ensure proper refcounting, new\n *\tassociation requests while already associating must be rejected.\n * @ie: Extra IEs to add to (Re)Association Request frame or %NULL\n * @ie_len: Length of ie buffer in octets\n * @use_mfp: Use management frame protection (IEEE 802.11w) in this association\n * @crypto: crypto settings\n * @prev_bssid: previous BSSID, if not %NULL use reassociate frame. This is used\n *\tto indicate a request to reassociate within the ESS instead of a request\n *\tdo the initial association with the ESS. When included, this is set to\n *\tthe BSSID of the current association, i.e., to the value that is\n *\tincluded in the Current AP address field of the Reassociation Request\n *\tframe.\n * @flags:  See &enum cfg80211_assoc_req_flags\n * @ht_capa:  HT Capabilities over-rides.  Values set in ht_capa_mask\n *\twill be used in ht_capa.  Un-supported values will be ignored.\n * @ht_capa_mask:  The bits of ht_capa which are to be used.\n * @vht_capa: VHT capability override\n * @vht_capa_mask: VHT capability mask indicating which fields to use\n * @fils_kek: FILS KEK for protecting (Re)Association Request/Response frame or\n *\t%NULL if FILS is not used.\n * @fils_kek_len: Length of fils_kek in octets\n * @fils_nonces: FILS nonces (part of AAD) for protecting (Re)Association\n *\tRequest/Response frame or %NULL if FILS is not used. This field starts\n *\twith 16 octets of STA Nonce followed by 16 octets of AP Nonce.\n * @s1g_capa: S1G capability override\n * @s1g_capa_mask: S1G capability override mask\n */\nstruct cfg80211_assoc_request {\n\tstruct cfg80211_bss *bss;\n\tconst u8 *ie, *prev_bssid;\n\tsize_t ie_len;\n\tstruct cfg80211_crypto_settings crypto;\n\tbool use_mfp;\n\tu32 flags;\n\tstruct ieee80211_ht_cap ht_capa;\n\tstruct ieee80211_ht_cap ht_capa_mask;\n\tstruct ieee80211_vht_cap vht_capa, vht_capa_mask;\n\tconst u8 *fils_kek;\n\tsize_t fils_kek_len;\n\tconst u8 *fils_nonces;\n\tstruct ieee80211_s1g_cap s1g_capa, s1g_capa_mask;\n};\n\n/**\n * struct cfg80211_deauth_request - Deauthentication request data\n *\n * This structure provides information needed to complete IEEE 802.11\n * deauthentication.\n *\n * @bssid: the BSSID of the BSS to deauthenticate from\n * @ie: Extra IEs to add to Deauthentication frame or %NULL\n * @ie_len: Length of ie buffer in octets\n * @reason_code: The reason code for the deauthentication\n * @local_state_change: if set, change local state only and\n *\tdo not set a deauth frame\n */\nstruct cfg80211_deauth_request {\n\tconst u8 *bssid;\n\tconst u8 *ie;\n\tsize_t ie_len;\n\tu16 reason_code;\n\tbool local_state_change;\n};\n\n/**\n * struct cfg80211_disassoc_request - Disassociation request data\n *\n * This structure provides information needed to complete IEEE 802.11\n * disassociation.\n *\n * @bss: the BSS to disassociate from\n * @ie: Extra IEs to add to Disassociation frame or %NULL\n * @ie_len: Length of ie buffer in octets\n * @reason_code: The reason code for the disassociation\n * @local_state_change: This is a request for a local state only, i.e., no\n *\tDisassociation frame is to be transmitted.\n */\nstruct cfg80211_disassoc_request {\n\tstruct cfg80211_bss *bss;\n\tconst u8 *ie;\n\tsize_t ie_len;\n\tu16 reason_code;\n\tbool local_state_change;\n};\n\n/**\n * struct cfg80211_ibss_params - IBSS parameters\n *\n * This structure defines the IBSS parameters for the join_ibss()\n * method.\n *\n * @ssid: The SSID, will always be non-null.\n * @ssid_len: The length of the SSID, will always be non-zero.\n * @bssid: Fixed BSSID requested, maybe be %NULL, if set do not\n *\tsearch for IBSSs with a different BSSID.\n * @chandef: defines the channel to use if no other IBSS to join can be found\n * @channel_fixed: The channel should be fixed -- do not search for\n *\tIBSSs to join on other channels.\n * @ie: information element(s) to include in the beacon\n * @ie_len: length of that\n * @beacon_interval: beacon interval to use\n * @privacy: this is a protected network, keys will be configured\n *\tafter joining\n * @control_port: whether user space controls IEEE 802.1X port, i.e.,\n *\tsets/clears %NL80211_STA_FLAG_AUTHORIZED. If true, the driver is\n *\trequired to assume that the port is unauthorized until authorized by\n *\tuser space. Otherwise, port is marked authorized by default.\n * @control_port_over_nl80211: TRUE if userspace expects to exchange control\n *\tport frames over NL80211 instead of the network interface.\n * @userspace_handles_dfs: whether user space controls DFS operation, i.e.\n *\tchanges the channel when a radar is detected. This is required\n *\tto operate on DFS channels.\n * @basic_rates: bitmap of basic rates to use when creating the IBSS\n * @mcast_rate: per-band multicast rate index + 1 (0: disabled)\n * @ht_capa:  HT Capabilities over-rides.  Values set in ht_capa_mask\n *\twill be used in ht_capa.  Un-supported values will be ignored.\n * @ht_capa_mask:  The bits of ht_capa which are to be used.\n * @wep_keys: static WEP keys, if not NULL points to an array of\n *\tCFG80211_MAX_WEP_KEYS WEP keys\n * @wep_tx_key: key index (0..3) of the default TX static WEP key\n */\nstruct cfg80211_ibss_params {\n\tconst u8 *ssid;\n\tconst u8 *bssid;\n\tstruct cfg80211_chan_def chandef;\n\tconst u8 *ie;\n\tu8 ssid_len, ie_len;\n\tu16 beacon_interval;\n\tu32 basic_rates;\n\tbool channel_fixed;\n\tbool privacy;\n\tbool control_port;\n\tbool control_port_over_nl80211;\n\tbool userspace_handles_dfs;\n\tint mcast_rate[NUM_NL80211_BANDS];\n\tstruct ieee80211_ht_cap ht_capa;\n\tstruct ieee80211_ht_cap ht_capa_mask;\n\tstruct key_params *wep_keys;\n\tint wep_tx_key;\n};\n\n/**\n * struct cfg80211_bss_selection - connection parameters for BSS selection.\n *\n * @behaviour: requested BSS selection behaviour.\n * @param: parameters for requestion behaviour.\n * @band_pref: preferred band for %NL80211_BSS_SELECT_ATTR_BAND_PREF.\n * @adjust: parameters for %NL80211_BSS_SELECT_ATTR_RSSI_ADJUST.\n */\nstruct cfg80211_bss_selection {\n\tenum nl80211_bss_select_attr behaviour;\n\tunion {\n\t\tenum nl80211_band band_pref;\n\t\tstruct cfg80211_bss_select_adjust adjust;\n\t} param;\n};\n\n/**\n * struct cfg80211_connect_params - Connection parameters\n *\n * This structure provides information needed to complete IEEE 802.11\n * authentication and association.\n *\n * @channel: The channel to use or %NULL if not specified (auto-select based\n *\ton scan results)\n * @channel_hint: The channel of the recommended BSS for initial connection or\n *\t%NULL if not specified\n * @bssid: The AP BSSID or %NULL if not specified (auto-select based on scan\n *\tresults)\n * @bssid_hint: The recommended AP BSSID for initial connection to the BSS or\n *\t%NULL if not specified. Unlike the @bssid parameter, the driver is\n *\tallowed to ignore this @bssid_hint if it has knowledge of a better BSS\n *\tto use.\n * @ssid: SSID\n * @ssid_len: Length of ssid in octets\n * @auth_type: Authentication type (algorithm)\n * @ie: IEs for association request\n * @ie_len: Length of assoc_ie in octets\n * @privacy: indicates whether privacy-enabled APs should be used\n * @mfp: indicate whether management frame protection is used\n * @crypto: crypto settings\n * @key_len: length of WEP key for shared key authentication\n * @key_idx: index of WEP key for shared key authentication\n * @key: WEP key for shared key authentication\n * @flags:  See &enum cfg80211_assoc_req_flags\n * @bg_scan_period:  Background scan period in seconds\n *\tor -1 to indicate that default value is to be used.\n * @ht_capa:  HT Capabilities over-rides.  Values set in ht_capa_mask\n *\twill be used in ht_capa.  Un-supported values will be ignored.\n * @ht_capa_mask:  The bits of ht_capa which are to be used.\n * @vht_capa:  VHT Capability overrides\n * @vht_capa_mask: The bits of vht_capa which are to be used.\n * @pbss: if set, connect to a PCP instead of AP. Valid for DMG\n *\tnetworks.\n * @bss_select: criteria to be used for BSS selection.\n * @prev_bssid: previous BSSID, if not %NULL use reassociate frame. This is used\n *\tto indicate a request to reassociate within the ESS instead of a request\n *\tdo the initial association with the ESS. When included, this is set to\n *\tthe BSSID of the current association, i.e., to the value that is\n *\tincluded in the Current AP address field of the Reassociation Request\n *\tframe.\n * @fils_erp_username: EAP re-authentication protocol (ERP) username part of the\n *\tNAI or %NULL if not specified. This is used to construct FILS wrapped\n *\tdata IE.\n * @fils_erp_username_len: Length of @fils_erp_username in octets.\n * @fils_erp_realm: EAP re-authentication protocol (ERP) realm part of NAI or\n *\t%NULL if not specified. This specifies the domain name of ER server and\n *\tis used to construct FILS wrapped data IE.\n * @fils_erp_realm_len: Length of @fils_erp_realm in octets.\n * @fils_erp_next_seq_num: The next sequence number to use in the FILS ERP\n *\tmessages. This is also used to construct FILS wrapped data IE.\n * @fils_erp_rrk: ERP re-authentication Root Key (rRK) used to derive additional\n *\tkeys in FILS or %NULL if not specified.\n * @fils_erp_rrk_len: Length of @fils_erp_rrk in octets.\n * @want_1x: indicates user-space supports and wants to use 802.1X driver\n *\toffload of 4-way handshake.\n * @edmg: define the EDMG channels.\n *\tThis may specify multiple channels and bonding options for the driver\n *\tto choose from, based on BSS configuration.\n */\nstruct cfg80211_connect_params {\n\tstruct ieee80211_channel *channel;\n\tstruct ieee80211_channel *channel_hint;\n\tconst u8 *bssid;\n\tconst u8 *bssid_hint;\n\tconst u8 *ssid;\n\tsize_t ssid_len;\n\tenum nl80211_auth_type auth_type;\n\tconst u8 *ie;\n\tsize_t ie_len;\n\tbool privacy;\n\tenum nl80211_mfp mfp;\n\tstruct cfg80211_crypto_settings crypto;\n\tconst u8 *key;\n\tu8 key_len, key_idx;\n\tu32 flags;\n\tint bg_scan_period;\n\tstruct ieee80211_ht_cap ht_capa;\n\tstruct ieee80211_ht_cap ht_capa_mask;\n\tstruct ieee80211_vht_cap vht_capa;\n\tstruct ieee80211_vht_cap vht_capa_mask;\n\tbool pbss;\n\tstruct cfg80211_bss_selection bss_select;\n\tconst u8 *prev_bssid;\n\tconst u8 *fils_erp_username;\n\tsize_t fils_erp_username_len;\n\tconst u8 *fils_erp_realm;\n\tsize_t fils_erp_realm_len;\n\tu16 fils_erp_next_seq_num;\n\tconst u8 *fils_erp_rrk;\n\tsize_t fils_erp_rrk_len;\n\tbool want_1x;\n\tstruct ieee80211_edmg edmg;\n};\n\n/**\n * enum cfg80211_connect_params_changed - Connection parameters being updated\n *\n * This enum provides information of all connect parameters that\n * have to be updated as part of update_connect_params() call.\n *\n * @UPDATE_ASSOC_IES: Indicates whether association request IEs are updated\n * @UPDATE_FILS_ERP_INFO: Indicates that FILS connection parameters (realm,\n *\tusername, erp sequence number and rrk) are updated\n * @UPDATE_AUTH_TYPE: Indicates that authentication type is updated\n */\nenum cfg80211_connect_params_changed {\n\tUPDATE_ASSOC_IES\t\t= BIT(0),\n\tUPDATE_FILS_ERP_INFO\t\t= BIT(1),\n\tUPDATE_AUTH_TYPE\t\t= BIT(2),\n};\n\n/**\n * enum wiphy_params_flags - set_wiphy_params bitfield values\n * @WIPHY_PARAM_RETRY_SHORT: wiphy->retry_short has changed\n * @WIPHY_PARAM_RETRY_LONG: wiphy->retry_long has changed\n * @WIPHY_PARAM_FRAG_THRESHOLD: wiphy->frag_threshold has changed\n * @WIPHY_PARAM_RTS_THRESHOLD: wiphy->rts_threshold has changed\n * @WIPHY_PARAM_COVERAGE_CLASS: coverage class changed\n * @WIPHY_PARAM_DYN_ACK: dynack has been enabled\n * @WIPHY_PARAM_TXQ_LIMIT: TXQ packet limit has been changed\n * @WIPHY_PARAM_TXQ_MEMORY_LIMIT: TXQ memory limit has been changed\n * @WIPHY_PARAM_TXQ_QUANTUM: TXQ scheduler quantum\n */\nenum wiphy_params_flags {\n\tWIPHY_PARAM_RETRY_SHORT\t\t= 1 << 0,\n\tWIPHY_PARAM_RETRY_LONG\t\t= 1 << 1,\n\tWIPHY_PARAM_FRAG_THRESHOLD\t= 1 << 2,\n\tWIPHY_PARAM_RTS_THRESHOLD\t= 1 << 3,\n\tWIPHY_PARAM_COVERAGE_CLASS\t= 1 << 4,\n\tWIPHY_PARAM_DYN_ACK\t\t= 1 << 5,\n\tWIPHY_PARAM_TXQ_LIMIT\t\t= 1 << 6,\n\tWIPHY_PARAM_TXQ_MEMORY_LIMIT\t= 1 << 7,\n\tWIPHY_PARAM_TXQ_QUANTUM\t\t= 1 << 8,\n};\n\n#define IEEE80211_DEFAULT_AIRTIME_WEIGHT\t256\n\n/* The per TXQ device queue limit in airtime */\n#define IEEE80211_DEFAULT_AQL_TXQ_LIMIT_L\t5000\n#define IEEE80211_DEFAULT_AQL_TXQ_LIMIT_H\t12000\n\n/* The per interface airtime threshold to switch to lower queue limit */\n#define IEEE80211_AQL_THRESHOLD\t\t\t24000\n\n/**\n * struct cfg80211_pmksa - PMK Security Association\n *\n * This structure is passed to the set/del_pmksa() method for PMKSA\n * caching.\n *\n * @bssid: The AP's BSSID (may be %NULL).\n * @pmkid: The identifier to refer a PMKSA.\n * @pmk: The PMK for the PMKSA identified by @pmkid. This is used for key\n *\tderivation by a FILS STA. Otherwise, %NULL.\n * @pmk_len: Length of the @pmk. The length of @pmk can differ depending on\n *\tthe hash algorithm used to generate this.\n * @ssid: SSID to specify the ESS within which a PMKSA is valid when using FILS\n *\tcache identifier (may be %NULL).\n * @ssid_len: Length of the @ssid in octets.\n * @cache_id: 2-octet cache identifier advertized by a FILS AP identifying the\n *\tscope of PMKSA. This is valid only if @ssid_len is non-zero (may be\n *\t%NULL).\n * @pmk_lifetime: Maximum lifetime for PMKSA in seconds\n *\t(dot11RSNAConfigPMKLifetime) or 0 if not specified.\n *\tThe configured PMKSA must not be used for PMKSA caching after\n *\texpiration and any keys derived from this PMK become invalid on\n *\texpiration, i.e., the current association must be dropped if the PMK\n *\tused for it expires.\n * @pmk_reauth_threshold: Threshold time for reauthentication (percentage of\n *\tPMK lifetime, dot11RSNAConfigPMKReauthThreshold) or 0 if not specified.\n *\tDrivers are expected to trigger a full authentication instead of using\n *\tthis PMKSA for caching when reassociating to a new BSS after this\n *\tthreshold to generate a new PMK before the current one expires.\n */\nstruct cfg80211_pmksa {\n\tconst u8 *bssid;\n\tconst u8 *pmkid;\n\tconst u8 *pmk;\n\tsize_t pmk_len;\n\tconst u8 *ssid;\n\tsize_t ssid_len;\n\tconst u8 *cache_id;\n\tu32 pmk_lifetime;\n\tu8 pmk_reauth_threshold;\n};\n\n/**\n * struct cfg80211_pkt_pattern - packet pattern\n * @mask: bitmask where to match pattern and where to ignore bytes,\n *\tone bit per byte, in same format as nl80211\n * @pattern: bytes to match where bitmask is 1\n * @pattern_len: length of pattern (in bytes)\n * @pkt_offset: packet offset (in bytes)\n *\n * Internal note: @mask and @pattern are allocated in one chunk of\n * memory, free @mask only!\n */\nstruct cfg80211_pkt_pattern {\n\tconst u8 *mask, *pattern;\n\tint pattern_len;\n\tint pkt_offset;\n};\n\n/**\n * struct cfg80211_wowlan_tcp - TCP connection parameters\n *\n * @sock: (internal) socket for source port allocation\n * @src: source IP address\n * @dst: destination IP address\n * @dst_mac: destination MAC address\n * @src_port: source port\n * @dst_port: destination port\n * @payload_len: data payload length\n * @payload: data payload buffer\n * @payload_seq: payload sequence stamping configuration\n * @data_interval: interval at which to send data packets\n * @wake_len: wakeup payload match length\n * @wake_data: wakeup payload match data\n * @wake_mask: wakeup payload match mask\n * @tokens_size: length of the tokens buffer\n * @payload_tok: payload token usage configuration\n */\nstruct cfg80211_wowlan_tcp {\n\tstruct socket *sock;\n\t__be32 src, dst;\n\tu16 src_port, dst_port;\n\tu8 dst_mac[ETH_ALEN];\n\tint payload_len;\n\tconst u8 *payload;\n\tstruct nl80211_wowlan_tcp_data_seq payload_seq;\n\tu32 data_interval;\n\tu32 wake_len;\n\tconst u8 *wake_data, *wake_mask;\n\tu32 tokens_size;\n\t/* must be last, variable member */\n\tstruct nl80211_wowlan_tcp_data_token payload_tok;\n};\n\n/**\n * struct cfg80211_wowlan - Wake on Wireless-LAN support info\n *\n * This structure defines the enabled WoWLAN triggers for the device.\n * @any: wake up on any activity -- special trigger if device continues\n *\toperating as normal during suspend\n * @disconnect: wake up if getting disconnected\n * @magic_pkt: wake up on receiving magic packet\n * @patterns: wake up on receiving packet matching a pattern\n * @n_patterns: number of patterns\n * @gtk_rekey_failure: wake up on GTK rekey failure\n * @eap_identity_req: wake up on EAP identity request packet\n * @four_way_handshake: wake up on 4-way handshake\n * @rfkill_release: wake up when rfkill is released\n * @tcp: TCP connection establishment/wakeup parameters, see nl80211.h.\n *\tNULL if not configured.\n * @nd_config: configuration for the scan to be used for net detect wake.\n */\nstruct cfg80211_wowlan {\n\tbool any, disconnect, magic_pkt, gtk_rekey_failure,\n\t     eap_identity_req, four_way_handshake,\n\t     rfkill_release;\n\tstruct cfg80211_pkt_pattern *patterns;\n\tstruct cfg80211_wowlan_tcp *tcp;\n\tint n_patterns;\n\tstruct cfg80211_sched_scan_request *nd_config;\n};\n\n/**\n * struct cfg80211_coalesce_rules - Coalesce rule parameters\n *\n * This structure defines coalesce rule for the device.\n * @delay: maximum coalescing delay in msecs.\n * @condition: condition for packet coalescence.\n *\tsee &enum nl80211_coalesce_condition.\n * @patterns: array of packet patterns\n * @n_patterns: number of patterns\n */\nstruct cfg80211_coalesce_rules {\n\tint delay;\n\tenum nl80211_coalesce_condition condition;\n\tstruct cfg80211_pkt_pattern *patterns;\n\tint n_patterns;\n};\n\n/**\n * struct cfg80211_coalesce - Packet coalescing settings\n *\n * This structure defines coalescing settings.\n * @rules: array of coalesce rules\n * @n_rules: number of rules\n */\nstruct cfg80211_coalesce {\n\tstruct cfg80211_coalesce_rules *rules;\n\tint n_rules;\n};\n\n/**\n * struct cfg80211_wowlan_nd_match - information about the match\n *\n * @ssid: SSID of the match that triggered the wake up\n * @n_channels: Number of channels where the match occurred.  This\n *\tvalue may be zero if the driver can't report the channels.\n * @channels: center frequencies of the channels where a match\n *\toccurred (in MHz)\n */\nstruct cfg80211_wowlan_nd_match {\n\tstruct cfg80211_ssid ssid;\n\tint n_channels;\n\tu32 channels[];\n};\n\n/**\n * struct cfg80211_wowlan_nd_info - net detect wake up information\n *\n * @n_matches: Number of match information instances provided in\n *\t@matches.  This value may be zero if the driver can't provide\n *\tmatch information.\n * @matches: Array of pointers to matches containing information about\n *\tthe matches that triggered the wake up.\n */\nstruct cfg80211_wowlan_nd_info {\n\tint n_matches;\n\tstruct cfg80211_wowlan_nd_match *matches[];\n};\n\n/**\n * struct cfg80211_wowlan_wakeup - wakeup report\n * @disconnect: woke up by getting disconnected\n * @magic_pkt: woke up by receiving magic packet\n * @gtk_rekey_failure: woke up by GTK rekey failure\n * @eap_identity_req: woke up by EAP identity request packet\n * @four_way_handshake: woke up by 4-way handshake\n * @rfkill_release: woke up by rfkill being released\n * @pattern_idx: pattern that caused wakeup, -1 if not due to pattern\n * @packet_present_len: copied wakeup packet data\n * @packet_len: original wakeup packet length\n * @packet: The packet causing the wakeup, if any.\n * @packet_80211:  For pattern match, magic packet and other data\n *\tframe triggers an 802.3 frame should be reported, for\n *\tdisconnect due to deauth 802.11 frame. This indicates which\n *\tit is.\n * @tcp_match: TCP wakeup packet received\n * @tcp_connlost: TCP connection lost or failed to establish\n * @tcp_nomoretokens: TCP data ran out of tokens\n * @net_detect: if not %NULL, woke up because of net detect\n */\nstruct cfg80211_wowlan_wakeup {\n\tbool disconnect, magic_pkt, gtk_rekey_failure,\n\t     eap_identity_req, four_way_handshake,\n\t     rfkill_release, packet_80211,\n\t     tcp_match, tcp_connlost, tcp_nomoretokens;\n\ts32 pattern_idx;\n\tu32 packet_present_len, packet_len;\n\tconst void *packet;\n\tstruct cfg80211_wowlan_nd_info *net_detect;\n};\n\n/**\n * struct cfg80211_gtk_rekey_data - rekey data\n * @kek: key encryption key (@kek_len bytes)\n * @kck: key confirmation key (@kck_len bytes)\n * @replay_ctr: replay counter (NL80211_REPLAY_CTR_LEN bytes)\n * @kek_len: length of kek\n * @kck_len length of kck\n * @akm: akm (oui, id)\n */\nstruct cfg80211_gtk_rekey_data {\n\tconst u8 *kek, *kck, *replay_ctr;\n\tu32 akm;\n\tu8 kek_len, kck_len;\n};\n\n/**\n * struct cfg80211_update_ft_ies_params - FT IE Information\n *\n * This structure provides information needed to update the fast transition IE\n *\n * @md: The Mobility Domain ID, 2 Octet value\n * @ie: Fast Transition IEs\n * @ie_len: Length of ft_ie in octets\n */\nstruct cfg80211_update_ft_ies_params {\n\tu16 md;\n\tconst u8 *ie;\n\tsize_t ie_len;\n};\n\n/**\n * struct cfg80211_mgmt_tx_params - mgmt tx parameters\n *\n * This structure provides information needed to transmit a mgmt frame\n *\n * @chan: channel to use\n * @offchan: indicates wether off channel operation is required\n * @wait: duration for ROC\n * @buf: buffer to transmit\n * @len: buffer length\n * @no_cck: don't use cck rates for this frame\n * @dont_wait_for_ack: tells the low level not to wait for an ack\n * @n_csa_offsets: length of csa_offsets array\n * @csa_offsets: array of all the csa offsets in the frame\n */\nstruct cfg80211_mgmt_tx_params {\n\tstruct ieee80211_channel *chan;\n\tbool offchan;\n\tunsigned int wait;\n\tconst u8 *buf;\n\tsize_t len;\n\tbool no_cck;\n\tbool dont_wait_for_ack;\n\tint n_csa_offsets;\n\tconst u16 *csa_offsets;\n};\n\n/**\n * struct cfg80211_dscp_exception - DSCP exception\n *\n * @dscp: DSCP value that does not adhere to the user priority range definition\n * @up: user priority value to which the corresponding DSCP value belongs\n */\nstruct cfg80211_dscp_exception {\n\tu8 dscp;\n\tu8 up;\n};\n\n/**\n * struct cfg80211_dscp_range - DSCP range definition for user priority\n *\n * @low: lowest DSCP value of this user priority range, inclusive\n * @high: highest DSCP value of this user priority range, inclusive\n */\nstruct cfg80211_dscp_range {\n\tu8 low;\n\tu8 high;\n};\n\n/* QoS Map Set element length defined in IEEE Std 802.11-2012, 8.4.2.97 */\n#define IEEE80211_QOS_MAP_MAX_EX\t21\n#define IEEE80211_QOS_MAP_LEN_MIN\t16\n#define IEEE80211_QOS_MAP_LEN_MAX \\\n\t(IEEE80211_QOS_MAP_LEN_MIN + 2 * IEEE80211_QOS_MAP_MAX_EX)\n\n/**\n * struct cfg80211_qos_map - QoS Map Information\n *\n * This struct defines the Interworking QoS map setting for DSCP values\n *\n * @num_des: number of DSCP exceptions (0..21)\n * @dscp_exception: optionally up to maximum of 21 DSCP exceptions from\n *\tthe user priority DSCP range definition\n * @up: DSCP range definition for a particular user priority\n */\nstruct cfg80211_qos_map {\n\tu8 num_des;\n\tstruct cfg80211_dscp_exception dscp_exception[IEEE80211_QOS_MAP_MAX_EX];\n\tstruct cfg80211_dscp_range up[8];\n};\n\n/**\n * struct cfg80211_nan_conf - NAN configuration\n *\n * This struct defines NAN configuration parameters\n *\n * @master_pref: master preference (1 - 255)\n * @bands: operating bands, a bitmap of &enum nl80211_band values.\n *\tFor instance, for NL80211_BAND_2GHZ, bit 0 would be set\n *\t(i.e. BIT(NL80211_BAND_2GHZ)).\n */\nstruct cfg80211_nan_conf {\n\tu8 master_pref;\n\tu8 bands;\n};\n\n/**\n * enum cfg80211_nan_conf_changes - indicates changed fields in NAN\n * configuration\n *\n * @CFG80211_NAN_CONF_CHANGED_PREF: master preference\n * @CFG80211_NAN_CONF_CHANGED_BANDS: operating bands\n */\nenum cfg80211_nan_conf_changes {\n\tCFG80211_NAN_CONF_CHANGED_PREF = BIT(0),\n\tCFG80211_NAN_CONF_CHANGED_BANDS = BIT(1),\n};\n\n/**\n * struct cfg80211_nan_func_filter - a NAN function Rx / Tx filter\n *\n * @filter: the content of the filter\n * @len: the length of the filter\n */\nstruct cfg80211_nan_func_filter {\n\tconst u8 *filter;\n\tu8 len;\n};\n\n/**\n * struct cfg80211_nan_func - a NAN function\n *\n * @type: &enum nl80211_nan_function_type\n * @service_id: the service ID of the function\n * @publish_type: &nl80211_nan_publish_type\n * @close_range: if true, the range should be limited. Threshold is\n *\timplementation specific.\n * @publish_bcast: if true, the solicited publish should be broadcasted\n * @subscribe_active: if true, the subscribe is active\n * @followup_id: the instance ID for follow up\n * @followup_reqid: the requestor instance ID for follow up\n * @followup_dest: MAC address of the recipient of the follow up\n * @ttl: time to live counter in DW.\n * @serv_spec_info: Service Specific Info\n * @serv_spec_info_len: Service Specific Info length\n * @srf_include: if true, SRF is inclusive\n * @srf_bf: Bloom Filter\n * @srf_bf_len: Bloom Filter length\n * @srf_bf_idx: Bloom Filter index\n * @srf_macs: SRF MAC addresses\n * @srf_num_macs: number of MAC addresses in SRF\n * @rx_filters: rx filters that are matched with corresponding peer's tx_filter\n * @tx_filters: filters that should be transmitted in the SDF.\n * @num_rx_filters: length of &rx_filters.\n * @num_tx_filters: length of &tx_filters.\n * @instance_id: driver allocated id of the function.\n * @cookie: unique NAN function identifier.\n */\nstruct cfg80211_nan_func {\n\tenum nl80211_nan_function_type type;\n\tu8 service_id[NL80211_NAN_FUNC_SERVICE_ID_LEN];\n\tu8 publish_type;\n\tbool close_range;\n\tbool publish_bcast;\n\tbool subscribe_active;\n\tu8 followup_id;\n\tu8 followup_reqid;\n\tstruct mac_address followup_dest;\n\tu32 ttl;\n\tconst u8 *serv_spec_info;\n\tu8 serv_spec_info_len;\n\tbool srf_include;\n\tconst u8 *srf_bf;\n\tu8 srf_bf_len;\n\tu8 srf_bf_idx;\n\tstruct mac_address *srf_macs;\n\tint srf_num_macs;\n\tstruct cfg80211_nan_func_filter *rx_filters;\n\tstruct cfg80211_nan_func_filter *tx_filters;\n\tu8 num_tx_filters;\n\tu8 num_rx_filters;\n\tu8 instance_id;\n\tu64 cookie;\n};\n\n/**\n * struct cfg80211_pmk_conf - PMK configuration\n *\n * @aa: authenticator address\n * @pmk_len: PMK length in bytes.\n * @pmk: the PMK material\n * @pmk_r0_name: PMK-R0 Name. NULL if not applicable (i.e., the PMK\n *\tis not PMK-R0). When pmk_r0_name is not NULL, the pmk field\n *\tholds PMK-R0.\n */\nstruct cfg80211_pmk_conf {\n\tconst u8 *aa;\n\tu8 pmk_len;\n\tconst u8 *pmk;\n\tconst u8 *pmk_r0_name;\n};\n\n/**\n * struct cfg80211_external_auth_params - Trigger External authentication.\n *\n * Commonly used across the external auth request and event interfaces.\n *\n * @action: action type / trigger for external authentication. Only significant\n *\tfor the authentication request event interface (driver to user space).\n * @bssid: BSSID of the peer with which the authentication has\n *\tto happen. Used by both the authentication request event and\n *\tauthentication response command interface.\n * @ssid: SSID of the AP.  Used by both the authentication request event and\n *\tauthentication response command interface.\n * @key_mgmt_suite: AKM suite of the respective authentication. Used by the\n *\tauthentication request event interface.\n * @status: status code, %WLAN_STATUS_SUCCESS for successful authentication,\n *\tuse %WLAN_STATUS_UNSPECIFIED_FAILURE if user space cannot give you\n *\tthe real status code for failures. Used only for the authentication\n *\tresponse command interface (user space to driver).\n * @pmkid: The identifier to refer a PMKSA.\n */\nstruct cfg80211_external_auth_params {\n\tenum nl80211_external_auth_action action;\n\tu8 bssid[ETH_ALEN] __aligned(2);\n\tstruct cfg80211_ssid ssid;\n\tunsigned int key_mgmt_suite;\n\tu16 status;\n\tconst u8 *pmkid;\n};\n\n/**\n * struct cfg80211_ftm_responder_stats - FTM responder statistics\n *\n * @filled: bitflag of flags using the bits of &enum nl80211_ftm_stats to\n *\tindicate the relevant values in this struct for them\n * @success_num: number of FTM sessions in which all frames were successfully\n *\tanswered\n * @partial_num: number of FTM sessions in which part of frames were\n *\tsuccessfully answered\n * @failed_num: number of failed FTM sessions\n * @asap_num: number of ASAP FTM sessions\n * @non_asap_num: number of  non-ASAP FTM sessions\n * @total_duration_ms: total sessions durations - gives an indication\n *\tof how much time the responder was busy\n * @unknown_triggers_num: number of unknown FTM triggers - triggers from\n *\tinitiators that didn't finish successfully the negotiation phase with\n *\tthe responder\n * @reschedule_requests_num: number of FTM reschedule requests - initiator asks\n *\tfor a new scheduling although it already has scheduled FTM slot\n * @out_of_window_triggers_num: total FTM triggers out of scheduled window\n */\nstruct cfg80211_ftm_responder_stats {\n\tu32 filled;\n\tu32 success_num;\n\tu32 partial_num;\n\tu32 failed_num;\n\tu32 asap_num;\n\tu32 non_asap_num;\n\tu64 total_duration_ms;\n\tu32 unknown_triggers_num;\n\tu32 reschedule_requests_num;\n\tu32 out_of_window_triggers_num;\n};\n\n/**\n * struct cfg80211_pmsr_ftm_result - FTM result\n * @failure_reason: if this measurement failed (PMSR status is\n *\t%NL80211_PMSR_STATUS_FAILURE), this gives a more precise\n *\treason than just \"failure\"\n * @burst_index: if reporting partial results, this is the index\n *\tin [0 .. num_bursts-1] of the burst that's being reported\n * @num_ftmr_attempts: number of FTM request frames transmitted\n * @num_ftmr_successes: number of FTM request frames acked\n * @busy_retry_time: if failure_reason is %NL80211_PMSR_FTM_FAILURE_PEER_BUSY,\n *\tfill this to indicate in how many seconds a retry is deemed possible\n *\tby the responder\n * @num_bursts_exp: actual number of bursts exponent negotiated\n * @burst_duration: actual burst duration negotiated\n * @ftms_per_burst: actual FTMs per burst negotiated\n * @lci_len: length of LCI information (if present)\n * @civicloc_len: length of civic location information (if present)\n * @lci: LCI data (may be %NULL)\n * @civicloc: civic location data (may be %NULL)\n * @rssi_avg: average RSSI over FTM action frames reported\n * @rssi_spread: spread of the RSSI over FTM action frames reported\n * @tx_rate: bitrate for transmitted FTM action frame response\n * @rx_rate: bitrate of received FTM action frame\n * @rtt_avg: average of RTTs measured (must have either this or @dist_avg)\n * @rtt_variance: variance of RTTs measured (note that standard deviation is\n *\tthe square root of the variance)\n * @rtt_spread: spread of the RTTs measured\n * @dist_avg: average of distances (mm) measured\n *\t(must have either this or @rtt_avg)\n * @dist_variance: variance of distances measured (see also @rtt_variance)\n * @dist_spread: spread of distances measured (see also @rtt_spread)\n * @num_ftmr_attempts_valid: @num_ftmr_attempts is valid\n * @num_ftmr_successes_valid: @num_ftmr_successes is valid\n * @rssi_avg_valid: @rssi_avg is valid\n * @rssi_spread_valid: @rssi_spread is valid\n * @tx_rate_valid: @tx_rate is valid\n * @rx_rate_valid: @rx_rate is valid\n * @rtt_avg_valid: @rtt_avg is valid\n * @rtt_variance_valid: @rtt_variance is valid\n * @rtt_spread_valid: @rtt_spread is valid\n * @dist_avg_valid: @dist_avg is valid\n * @dist_variance_valid: @dist_variance is valid\n * @dist_spread_valid: @dist_spread is valid\n */\nstruct cfg80211_pmsr_ftm_result {\n\tconst u8 *lci;\n\tconst u8 *civicloc;\n\tunsigned int lci_len;\n\tunsigned int civicloc_len;\n\tenum nl80211_peer_measurement_ftm_failure_reasons failure_reason;\n\tu32 num_ftmr_attempts, num_ftmr_successes;\n\ts16 burst_index;\n\tu8 busy_retry_time;\n\tu8 num_bursts_exp;\n\tu8 burst_duration;\n\tu8 ftms_per_burst;\n\ts32 rssi_avg;\n\ts32 rssi_spread;\n\tstruct rate_info tx_rate, rx_rate;\n\ts64 rtt_avg;\n\ts64 rtt_variance;\n\ts64 rtt_spread;\n\ts64 dist_avg;\n\ts64 dist_variance;\n\ts64 dist_spread;\n\n\tu16 num_ftmr_attempts_valid:1,\n\t    num_ftmr_successes_valid:1,\n\t    rssi_avg_valid:1,\n\t    rssi_spread_valid:1,\n\t    tx_rate_valid:1,\n\t    rx_rate_valid:1,\n\t    rtt_avg_valid:1,\n\t    rtt_variance_valid:1,\n\t    rtt_spread_valid:1,\n\t    dist_avg_valid:1,\n\t    dist_variance_valid:1,\n\t    dist_spread_valid:1;\n};\n\n/**\n * struct cfg80211_pmsr_result - peer measurement result\n * @addr: address of the peer\n * @host_time: host time (use ktime_get_boottime() adjust to the time when the\n *\tmeasurement was made)\n * @ap_tsf: AP's TSF at measurement time\n * @status: status of the measurement\n * @final: if reporting partial results, mark this as the last one; if not\n *\treporting partial results always set this flag\n * @ap_tsf_valid: indicates the @ap_tsf value is valid\n * @type: type of the measurement reported, note that we only support reporting\n *\tone type at a time, but you can report multiple results separately and\n *\tthey're all aggregated for userspace.\n */\nstruct cfg80211_pmsr_result {\n\tu64 host_time, ap_tsf;\n\tenum nl80211_peer_measurement_status status;\n\n\tu8 addr[ETH_ALEN];\n\n\tu8 final:1,\n\t   ap_tsf_valid:1;\n\n\tenum nl80211_peer_measurement_type type;\n\n\tunion {\n\t\tstruct cfg80211_pmsr_ftm_result ftm;\n\t};\n};\n\n/**\n * struct cfg80211_pmsr_ftm_request_peer - FTM request data\n * @requested: indicates FTM is requested\n * @preamble: frame preamble to use\n * @burst_period: burst period to use\n * @asap: indicates to use ASAP mode\n * @num_bursts_exp: number of bursts exponent\n * @burst_duration: burst duration\n * @ftms_per_burst: number of FTMs per burst\n * @ftmr_retries: number of retries for FTM request\n * @request_lci: request LCI information\n * @request_civicloc: request civic location information\n * @trigger_based: use trigger based ranging for the measurement\n *\t\t If neither @trigger_based nor @non_trigger_based is set,\n *\t\t EDCA based ranging will be used.\n * @non_trigger_based: use non trigger based ranging for the measurement\n *\t\t If neither @trigger_based nor @non_trigger_based is set,\n *\t\t EDCA based ranging will be used.\n *\n * See also nl80211 for the respective attribute documentation.\n */\nstruct cfg80211_pmsr_ftm_request_peer {\n\tenum nl80211_preamble preamble;\n\tu16 burst_period;\n\tu8 requested:1,\n\t   asap:1,\n\t   request_lci:1,\n\t   request_civicloc:1,\n\t   trigger_based:1,\n\t   non_trigger_based:1;\n\tu8 num_bursts_exp;\n\tu8 burst_duration;\n\tu8 ftms_per_burst;\n\tu8 ftmr_retries;\n};\n\n/**\n * struct cfg80211_pmsr_request_peer - peer data for a peer measurement request\n * @addr: MAC address\n * @chandef: channel to use\n * @report_ap_tsf: report the associated AP's TSF\n * @ftm: FTM data, see &struct cfg80211_pmsr_ftm_request_peer\n */\nstruct cfg80211_pmsr_request_peer {\n\tu8 addr[ETH_ALEN];\n\tstruct cfg80211_chan_def chandef;\n\tu8 report_ap_tsf:1;\n\tstruct cfg80211_pmsr_ftm_request_peer ftm;\n};\n\n/**\n * struct cfg80211_pmsr_request - peer measurement request\n * @cookie: cookie, set by cfg80211\n * @nl_portid: netlink portid - used by cfg80211\n * @drv_data: driver data for this request, if required for aborting,\n *\tnot otherwise freed or anything by cfg80211\n * @mac_addr: MAC address used for (randomised) request\n * @mac_addr_mask: MAC address mask used for randomisation, bits that\n *\tare 0 in the mask should be randomised, bits that are 1 should\n *\tbe taken from the @mac_addr\n * @list: used by cfg80211 to hold on to the request\n * @timeout: timeout (in milliseconds) for the whole operation, if\n *\tzero it means there's no timeout\n * @n_peers: number of peers to do measurements with\n * @peers: per-peer measurement request data\n */\nstruct cfg80211_pmsr_request {\n\tu64 cookie;\n\tvoid *drv_data;\n\tu32 n_peers;\n\tu32 nl_portid;\n\n\tu32 timeout;\n\n\tu8 mac_addr[ETH_ALEN] __aligned(2);\n\tu8 mac_addr_mask[ETH_ALEN] __aligned(2);\n\n\tstruct list_head list;\n\n\tstruct cfg80211_pmsr_request_peer peers[];\n};\n\n/**\n * struct cfg80211_update_owe_info - OWE Information\n *\n * This structure provides information needed for the drivers to offload OWE\n * (Opportunistic Wireless Encryption) processing to the user space.\n *\n * Commonly used across update_owe_info request and event interfaces.\n *\n * @peer: MAC address of the peer device for which the OWE processing\n *\thas to be done.\n * @status: status code, %WLAN_STATUS_SUCCESS for successful OWE info\n *\tprocessing, use %WLAN_STATUS_UNSPECIFIED_FAILURE if user space\n *\tcannot give you the real status code for failures. Used only for\n *\tOWE update request command interface (user space to driver).\n * @ie: IEs obtained from the peer or constructed by the user space. These are\n *\tthe IEs of the remote peer in the event from the host driver and\n *\tthe constructed IEs by the user space in the request interface.\n * @ie_len: Length of IEs in octets.\n */\nstruct cfg80211_update_owe_info {\n\tu8 peer[ETH_ALEN] __aligned(2);\n\tu16 status;\n\tconst u8 *ie;\n\tsize_t ie_len;\n};\n\n/**\n * struct mgmt_frame_regs - management frame registrations data\n * @global_stypes: bitmap of management frame subtypes registered\n *\tfor the entire device\n * @interface_stypes: bitmap of management frame subtypes registered\n *\tfor the given interface\n * @global_mcast_rx: mcast RX is needed globally for these subtypes\n * @interface_mcast_stypes: mcast RX is needed on this interface\n *\tfor these subtypes\n */\nstruct mgmt_frame_regs {\n\tu32 global_stypes, interface_stypes;\n\tu32 global_mcast_stypes, interface_mcast_stypes;\n};\n\n/**\n * struct cfg80211_ops - backend description for wireless configuration\n *\n * This struct is registered by fullmac card drivers and/or wireless stacks\n * in order to handle configuration requests on their interfaces.\n *\n * All callbacks except where otherwise noted should return 0\n * on success or a negative error code.\n *\n * All operations are invoked with the wiphy mutex held. The RTNL may be\n * held in addition (due to wireless extensions) but this cannot be relied\n * upon except in cases where documented below. Note that due to ordering,\n * the RTNL also cannot be acquired in any handlers.\n *\n * @suspend: wiphy device needs to be suspended. The variable @wow will\n *\tbe %NULL or contain the enabled Wake-on-Wireless triggers that are\n *\tconfigured for the device.\n * @resume: wiphy device needs to be resumed\n * @set_wakeup: Called when WoWLAN is enabled/disabled, use this callback\n *\tto call device_set_wakeup_enable() to enable/disable wakeup from\n *\tthe device.\n *\n * @add_virtual_intf: create a new virtual interface with the given name,\n *\tmust set the struct wireless_dev's iftype. Beware: You must create\n *\tthe new netdev in the wiphy's network namespace! Returns the struct\n *\twireless_dev, or an ERR_PTR. For P2P device wdevs, the driver must\n *\talso set the address member in the wdev.\n *\tThis additionally holds the RTNL to be able to do netdev changes.\n *\n * @del_virtual_intf: remove the virtual interface\n *\tThis additionally holds the RTNL to be able to do netdev changes.\n *\n * @change_virtual_intf: change type/configuration of virtual interface,\n *\tkeep the struct wireless_dev's iftype updated.\n *\tThis additionally holds the RTNL to be able to do netdev changes.\n *\n * @add_key: add a key with the given parameters. @mac_addr will be %NULL\n *\twhen adding a group key.\n *\n * @get_key: get information about the key with the given parameters.\n *\t@mac_addr will be %NULL when requesting information for a group\n *\tkey. All pointers given to the @callback function need not be valid\n *\tafter it returns. This function should return an error if it is\n *\tnot possible to retrieve the key, -ENOENT if it doesn't exist.\n *\n * @del_key: remove a key given the @mac_addr (%NULL for a group key)\n *\tand @key_index, return -ENOENT if the key doesn't exist.\n *\n * @set_default_key: set the default key on an interface\n *\n * @set_default_mgmt_key: set the default management frame key on an interface\n *\n * @set_default_beacon_key: set the default Beacon frame key on an interface\n *\n * @set_rekey_data: give the data necessary for GTK rekeying to the driver\n *\n * @start_ap: Start acting in AP mode defined by the parameters.\n * @change_beacon: Change the beacon parameters for an access point mode\n *\tinterface. This should reject the call when AP mode wasn't started.\n * @stop_ap: Stop being an AP, including stopping beaconing.\n *\n * @add_station: Add a new station.\n * @del_station: Remove a station\n * @change_station: Modify a given station. Note that flags changes are not much\n *\tvalidated in cfg80211, in particular the auth/assoc/authorized flags\n *\tmight come to the driver in invalid combinations -- make sure to check\n *\tthem, also against the existing state! Drivers must call\n *\tcfg80211_check_station_change() to validate the information.\n * @get_station: get station information for the station identified by @mac\n * @dump_station: dump station callback -- resume dump at index @idx\n *\n * @add_mpath: add a fixed mesh path\n * @del_mpath: delete a given mesh path\n * @change_mpath: change a given mesh path\n * @get_mpath: get a mesh path for the given parameters\n * @dump_mpath: dump mesh path callback -- resume dump at index @idx\n * @get_mpp: get a mesh proxy path for the given parameters\n * @dump_mpp: dump mesh proxy path callback -- resume dump at index @idx\n * @join_mesh: join the mesh network with the specified parameters\n *\t(invoked with the wireless_dev mutex held)\n * @leave_mesh: leave the current mesh network\n *\t(invoked with the wireless_dev mutex held)\n *\n * @get_mesh_config: Get the current mesh configuration\n *\n * @update_mesh_config: Update mesh parameters on a running mesh.\n *\tThe mask is a bitfield which tells us which parameters to\n *\tset, and which to leave alone.\n *\n * @change_bss: Modify parameters for a given BSS.\n *\n * @set_txq_params: Set TX queue parameters\n *\n * @libertas_set_mesh_channel: Only for backward compatibility for libertas,\n *\tas it doesn't implement join_mesh and needs to set the channel to\n *\tjoin the mesh instead.\n *\n * @set_monitor_channel: Set the monitor mode channel for the device. If other\n *\tinterfaces are active this callback should reject the configuration.\n *\tIf no interfaces are active or the device is down, the channel should\n *\tbe stored for when a monitor interface becomes active.\n *\n * @scan: Request to do a scan. If returning zero, the scan request is given\n *\tthe driver, and will be valid until passed to cfg80211_scan_done().\n *\tFor scan results, call cfg80211_inform_bss(); you can call this outside\n *\tthe scan/scan_done bracket too.\n * @abort_scan: Tell the driver to abort an ongoing scan. The driver shall\n *\tindicate the status of the scan through cfg80211_scan_done().\n *\n * @auth: Request to authenticate with the specified peer\n *\t(invoked with the wireless_dev mutex held)\n * @assoc: Request to (re)associate with the specified peer\n *\t(invoked with the wireless_dev mutex held)\n * @deauth: Request to deauthenticate from the specified peer\n *\t(invoked with the wireless_dev mutex held)\n * @disassoc: Request to disassociate from the specified peer\n *\t(invoked with the wireless_dev mutex held)\n *\n * @connect: Connect to the ESS with the specified parameters. When connected,\n *\tcall cfg80211_connect_result()/cfg80211_connect_bss() with status code\n *\t%WLAN_STATUS_SUCCESS. If the connection fails for some reason, call\n *\tcfg80211_connect_result()/cfg80211_connect_bss() with the status code\n *\tfrom the AP or cfg80211_connect_timeout() if no frame with status code\n *\twas received.\n *\tThe driver is allowed to roam to other BSSes within the ESS when the\n *\tother BSS matches the connect parameters. When such roaming is initiated\n *\tby the driver, the driver is expected to verify that the target matches\n *\tthe configured security parameters and to use Reassociation Request\n *\tframe instead of Association Request frame.\n *\tThe connect function can also be used to request the driver to perform a\n *\tspecific roam when connected to an ESS. In that case, the prev_bssid\n *\tparameter is set to the BSSID of the currently associated BSS as an\n *\tindication of requesting reassociation.\n *\tIn both the driver-initiated and new connect() call initiated roaming\n *\tcases, the result of roaming is indicated with a call to\n *\tcfg80211_roamed(). (invoked with the wireless_dev mutex held)\n * @update_connect_params: Update the connect parameters while connected to a\n *\tBSS. The updated parameters can be used by driver/firmware for\n *\tsubsequent BSS selection (roaming) decisions and to form the\n *\tAuthentication/(Re)Association Request frames. This call does not\n *\trequest an immediate disassociation or reassociation with the current\n *\tBSS, i.e., this impacts only subsequent (re)associations. The bits in\n *\tchanged are defined in &enum cfg80211_connect_params_changed.\n *\t(invoked with the wireless_dev mutex held)\n * @disconnect: Disconnect from the BSS/ESS or stop connection attempts if\n *      connection is in progress. Once done, call cfg80211_disconnected() in\n *      case connection was already established (invoked with the\n *      wireless_dev mutex held), otherwise call cfg80211_connect_timeout().\n *\n * @join_ibss: Join the specified IBSS (or create if necessary). Once done, call\n *\tcfg80211_ibss_joined(), also call that function when changing BSSID due\n *\tto a merge.\n *\t(invoked with the wireless_dev mutex held)\n * @leave_ibss: Leave the IBSS.\n *\t(invoked with the wireless_dev mutex held)\n *\n * @set_mcast_rate: Set the specified multicast rate (only if vif is in ADHOC or\n *\tMESH mode)\n *\n * @set_wiphy_params: Notify that wiphy parameters have changed;\n *\t@changed bitfield (see &enum wiphy_params_flags) describes which values\n *\thave changed. The actual parameter values are available in\n *\tstruct wiphy. If returning an error, no value should be changed.\n *\n * @set_tx_power: set the transmit power according to the parameters,\n *\tthe power passed is in mBm, to get dBm use MBM_TO_DBM(). The\n *\twdev may be %NULL if power was set for the wiphy, and will\n *\talways be %NULL unless the driver supports per-vif TX power\n *\t(as advertised by the nl80211 feature flag.)\n * @get_tx_power: store the current TX power into the dbm variable;\n *\treturn 0 if successful\n *\n * @rfkill_poll: polls the hw rfkill line, use cfg80211 reporting\n *\tfunctions to adjust rfkill hw state\n *\n * @dump_survey: get site survey information.\n *\n * @remain_on_channel: Request the driver to remain awake on the specified\n *\tchannel for the specified duration to complete an off-channel\n *\toperation (e.g., public action frame exchange). When the driver is\n *\tready on the requested channel, it must indicate this with an event\n *\tnotification by calling cfg80211_ready_on_channel().\n * @cancel_remain_on_channel: Cancel an on-going remain-on-channel operation.\n *\tThis allows the operation to be terminated prior to timeout based on\n *\tthe duration value.\n * @mgmt_tx: Transmit a management frame.\n * @mgmt_tx_cancel_wait: Cancel the wait time from transmitting a management\n *\tframe on another channel\n *\n * @testmode_cmd: run a test mode command; @wdev may be %NULL\n * @testmode_dump: Implement a test mode dump. The cb->args[2] and up may be\n *\tused by the function, but 0 and 1 must not be touched. Additionally,\n *\treturn error codes other than -ENOBUFS and -ENOENT will terminate the\n *\tdump and return to userspace with an error, so be careful. If any data\n *\twas passed in from userspace then the data/len arguments will be present\n *\tand point to the data contained in %NL80211_ATTR_TESTDATA.\n *\n * @set_bitrate_mask: set the bitrate mask configuration\n *\n * @set_pmksa: Cache a PMKID for a BSSID. This is mostly useful for fullmac\n *\tdevices running firmwares capable of generating the (re) association\n *\tRSN IE. It allows for faster roaming between WPA2 BSSIDs.\n * @del_pmksa: Delete a cached PMKID.\n * @flush_pmksa: Flush all cached PMKIDs.\n * @set_power_mgmt: Configure WLAN power management. A timeout value of -1\n *\tallows the driver to adjust the dynamic ps timeout value.\n * @set_cqm_rssi_config: Configure connection quality monitor RSSI threshold.\n *\tAfter configuration, the driver should (soon) send an event indicating\n *\tthe current level is above/below the configured threshold; this may\n *\tneed some care when the configuration is changed (without first being\n *\tdisabled.)\n * @set_cqm_rssi_range_config: Configure two RSSI thresholds in the\n *\tconnection quality monitor.  An event is to be sent only when the\n *\tsignal level is found to be outside the two values.  The driver should\n *\tset %NL80211_EXT_FEATURE_CQM_RSSI_LIST if this method is implemented.\n *\tIf it is provided then there's no point providing @set_cqm_rssi_config.\n * @set_cqm_txe_config: Configure connection quality monitor TX error\n *\tthresholds.\n * @sched_scan_start: Tell the driver to start a scheduled scan.\n * @sched_scan_stop: Tell the driver to stop an ongoing scheduled scan with\n *\tgiven request id. This call must stop the scheduled scan and be ready\n *\tfor starting a new one before it returns, i.e. @sched_scan_start may be\n *\tcalled immediately after that again and should not fail in that case.\n *\tThe driver should not call cfg80211_sched_scan_stopped() for a requested\n *\tstop (when this method returns 0).\n *\n * @update_mgmt_frame_registrations: Notify the driver that management frame\n *\tregistrations were updated. The callback is allowed to sleep.\n *\n * @set_antenna: Set antenna configuration (tx_ant, rx_ant) on the device.\n *\tParameters are bitmaps of allowed antennas to use for TX/RX. Drivers may\n *\treject TX/RX mask combinations they cannot support by returning -EINVAL\n *\t(also see nl80211.h @NL80211_ATTR_WIPHY_ANTENNA_TX).\n *\n * @get_antenna: Get current antenna configuration from device (tx_ant, rx_ant).\n *\n * @tdls_mgmt: Transmit a TDLS management frame.\n * @tdls_oper: Perform a high-level TDLS operation (e.g. TDLS link setup).\n *\n * @probe_client: probe an associated client, must return a cookie that it\n *\tlater passes to cfg80211_probe_status().\n *\n * @set_noack_map: Set the NoAck Map for the TIDs.\n *\n * @get_channel: Get the current operating channel for the virtual interface.\n *\tFor monitor interfaces, it should return %NULL unless there's a single\n *\tcurrent monitoring channel.\n *\n * @start_p2p_device: Start the given P2P device.\n * @stop_p2p_device: Stop the given P2P device.\n *\n * @set_mac_acl: Sets MAC address control list in AP and P2P GO mode.\n *\tParameters include ACL policy, an array of MAC address of stations\n *\tand the number of MAC addresses. If there is already a list in driver\n *\tthis new list replaces the existing one. Driver has to clear its ACL\n *\twhen number of MAC addresses entries is passed as 0. Drivers which\n *\tadvertise the support for MAC based ACL have to implement this callback.\n *\n * @start_radar_detection: Start radar detection in the driver.\n *\n * @end_cac: End running CAC, probably because a related CAC\n *\twas finished on another phy.\n *\n * @update_ft_ies: Provide updated Fast BSS Transition information to the\n *\tdriver. If the SME is in the driver/firmware, this information can be\n *\tused in building Authentication and Reassociation Request frames.\n *\n * @crit_proto_start: Indicates a critical protocol needs more link reliability\n *\tfor a given duration (milliseconds). The protocol is provided so the\n *\tdriver can take the most appropriate actions.\n * @crit_proto_stop: Indicates critical protocol no longer needs increased link\n *\treliability. This operation can not fail.\n * @set_coalesce: Set coalesce parameters.\n *\n * @channel_switch: initiate channel-switch procedure (with CSA). Driver is\n *\tresponsible for veryfing if the switch is possible. Since this is\n *\tinherently tricky driver may decide to disconnect an interface later\n *\twith cfg80211_stop_iface(). This doesn't mean driver can accept\n *\teverything. It should do it's best to verify requests and reject them\n *\tas soon as possible.\n *\n * @set_qos_map: Set QoS mapping information to the driver\n *\n * @set_ap_chanwidth: Set the AP (including P2P GO) mode channel width for the\n *\tgiven interface This is used e.g. for dynamic HT 20/40 MHz channel width\n *\tchanges during the lifetime of the BSS.\n *\n * @add_tx_ts: validate (if admitted_time is 0) or add a TX TS to the device\n *\twith the given parameters; action frame exchange has been handled by\n *\tuserspace so this just has to modify the TX path to take the TS into\n *\taccount.\n *\tIf the admitted time is 0 just validate the parameters to make sure\n *\tthe session can be created at all; it is valid to just always return\n *\tsuccess for that but that may result in inefficient behaviour (handshake\n *\twith the peer followed by immediate teardown when the addition is later\n *\trejected)\n * @del_tx_ts: remove an existing TX TS\n *\n * @join_ocb: join the OCB network with the specified parameters\n *\t(invoked with the wireless_dev mutex held)\n * @leave_ocb: leave the current OCB network\n *\t(invoked with the wireless_dev mutex held)\n *\n * @tdls_channel_switch: Start channel-switching with a TDLS peer. The driver\n *\tis responsible for continually initiating channel-switching operations\n *\tand returning to the base channel for communication with the AP.\n * @tdls_cancel_channel_switch: Stop channel-switching with a TDLS peer. Both\n *\tpeers must be on the base channel when the call completes.\n * @start_nan: Start the NAN interface.\n * @stop_nan: Stop the NAN interface.\n * @add_nan_func: Add a NAN function. Returns negative value on failure.\n *\tOn success @nan_func ownership is transferred to the driver and\n *\tit may access it outside of the scope of this function. The driver\n *\tshould free the @nan_func when no longer needed by calling\n *\tcfg80211_free_nan_func().\n *\tOn success the driver should assign an instance_id in the\n *\tprovided @nan_func.\n * @del_nan_func: Delete a NAN function.\n * @nan_change_conf: changes NAN configuration. The changed parameters must\n *\tbe specified in @changes (using &enum cfg80211_nan_conf_changes);\n *\tAll other parameters must be ignored.\n *\n * @set_multicast_to_unicast: configure multicast to unicast conversion for BSS\n *\n * @get_txq_stats: Get TXQ stats for interface or phy. If wdev is %NULL, this\n *      function should return phy stats, and interface stats otherwise.\n *\n * @set_pmk: configure the PMK to be used for offloaded 802.1X 4-Way handshake.\n *\tIf not deleted through @del_pmk the PMK remains valid until disconnect\n *\tupon which the driver should clear it.\n *\t(invoked with the wireless_dev mutex held)\n * @del_pmk: delete the previously configured PMK for the given authenticator.\n *\t(invoked with the wireless_dev mutex held)\n *\n * @external_auth: indicates result of offloaded authentication processing from\n *     user space\n *\n * @tx_control_port: TX a control port frame (EAPoL).  The noencrypt parameter\n *\ttells the driver that the frame should not be encrypted.\n *\n * @get_ftm_responder_stats: Retrieve FTM responder statistics, if available.\n *\tStatistics should be cumulative, currently no way to reset is provided.\n * @start_pmsr: start peer measurement (e.g. FTM)\n * @abort_pmsr: abort peer measurement\n *\n * @update_owe_info: Provide updated OWE info to driver. Driver implementing SME\n *\tbut offloading OWE processing to the user space will get the updated\n *\tDH IE through this interface.\n *\n * @probe_mesh_link: Probe direct Mesh peer's link quality by sending data frame\n *\tand overrule HWMP path selection algorithm.\n * @set_tid_config: TID specific configuration, this can be peer or BSS specific\n *\tThis callback may sleep.\n * @reset_tid_config: Reset TID specific configuration for the peer, for the\n *\tgiven TIDs. This callback may sleep.\n *\n * @set_sar_specs: Update the SAR (TX power) settings.\n */\nstruct cfg80211_ops {\n\tint\t(*suspend)(struct wiphy *wiphy, struct cfg80211_wowlan *wow);\n\tint\t(*resume)(struct wiphy *wiphy);\n\tvoid\t(*set_wakeup)(struct wiphy *wiphy, bool enabled);\n\n\tstruct wireless_dev * (*add_virtual_intf)(struct wiphy *wiphy,\n\t\t\t\t\t\t  const char *name,\n\t\t\t\t\t\t  unsigned char name_assign_type,\n\t\t\t\t\t\t  enum nl80211_iftype type,\n\t\t\t\t\t\t  struct vif_params *params);\n\tint\t(*del_virtual_intf)(struct wiphy *wiphy,\n\t\t\t\t    struct wireless_dev *wdev);\n\tint\t(*change_virtual_intf)(struct wiphy *wiphy,\n\t\t\t\t       struct net_device *dev,\n\t\t\t\t       enum nl80211_iftype type,\n\t\t\t\t       struct vif_params *params);\n\n\tint\t(*add_key)(struct wiphy *wiphy, struct net_device *netdev,\n\t\t\t   u8 key_index, bool pairwise, const u8 *mac_addr,\n\t\t\t   struct key_params *params);\n\tint\t(*get_key)(struct wiphy *wiphy, struct net_device *netdev,\n\t\t\t   u8 key_index, bool pairwise, const u8 *mac_addr,\n\t\t\t   void *cookie,\n\t\t\t   void (*callback)(void *cookie, struct key_params*));\n\tint\t(*del_key)(struct wiphy *wiphy, struct net_device *netdev,\n\t\t\t   u8 key_index, bool pairwise, const u8 *mac_addr);\n\tint\t(*set_default_key)(struct wiphy *wiphy,\n\t\t\t\t   struct net_device *netdev,\n\t\t\t\t   u8 key_index, bool unicast, bool multicast);\n\tint\t(*set_default_mgmt_key)(struct wiphy *wiphy,\n\t\t\t\t\tstruct net_device *netdev,\n\t\t\t\t\tu8 key_index);\n\tint\t(*set_default_beacon_key)(struct wiphy *wiphy,\n\t\t\t\t\t  struct net_device *netdev,\n\t\t\t\t\t  u8 key_index);\n\n\tint\t(*start_ap)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t    struct cfg80211_ap_settings *settings);\n\tint\t(*change_beacon)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t\t struct cfg80211_beacon_data *info);\n\tint\t(*stop_ap)(struct wiphy *wiphy, struct net_device *dev);\n\n\n\tint\t(*add_station)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t       const u8 *mac,\n\t\t\t       struct station_parameters *params);\n\tint\t(*del_station)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t       struct station_del_parameters *params);\n\tint\t(*change_station)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t\t  const u8 *mac,\n\t\t\t\t  struct station_parameters *params);\n\tint\t(*get_station)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t       const u8 *mac, struct station_info *sinfo);\n\tint\t(*dump_station)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t\tint idx, u8 *mac, struct station_info *sinfo);\n\n\tint\t(*add_mpath)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t       const u8 *dst, const u8 *next_hop);\n\tint\t(*del_mpath)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t       const u8 *dst);\n\tint\t(*change_mpath)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t\t  const u8 *dst, const u8 *next_hop);\n\tint\t(*get_mpath)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t     u8 *dst, u8 *next_hop, struct mpath_info *pinfo);\n\tint\t(*dump_mpath)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t      int idx, u8 *dst, u8 *next_hop,\n\t\t\t      struct mpath_info *pinfo);\n\tint\t(*get_mpp)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t   u8 *dst, u8 *mpp, struct mpath_info *pinfo);\n\tint\t(*dump_mpp)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t    int idx, u8 *dst, u8 *mpp,\n\t\t\t    struct mpath_info *pinfo);\n\tint\t(*get_mesh_config)(struct wiphy *wiphy,\n\t\t\t\tstruct net_device *dev,\n\t\t\t\tstruct mesh_config *conf);\n\tint\t(*update_mesh_config)(struct wiphy *wiphy,\n\t\t\t\t      struct net_device *dev, u32 mask,\n\t\t\t\t      const struct mesh_config *nconf);\n\tint\t(*join_mesh)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t     const struct mesh_config *conf,\n\t\t\t     const struct mesh_setup *setup);\n\tint\t(*leave_mesh)(struct wiphy *wiphy, struct net_device *dev);\n\n\tint\t(*join_ocb)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t    struct ocb_setup *setup);\n\tint\t(*leave_ocb)(struct wiphy *wiphy, struct net_device *dev);\n\n\tint\t(*change_bss)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t      struct bss_parameters *params);\n\n\tint\t(*set_txq_params)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t\t  struct ieee80211_txq_params *params);\n\n\tint\t(*libertas_set_mesh_channel)(struct wiphy *wiphy,\n\t\t\t\t\t     struct net_device *dev,\n\t\t\t\t\t     struct ieee80211_channel *chan);\n\n\tint\t(*set_monitor_channel)(struct wiphy *wiphy,\n\t\t\t\t       struct cfg80211_chan_def *chandef);\n\n\tint\t(*scan)(struct wiphy *wiphy,\n\t\t\tstruct cfg80211_scan_request *request);\n\tvoid\t(*abort_scan)(struct wiphy *wiphy, struct wireless_dev *wdev);\n\n\tint\t(*auth)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\tstruct cfg80211_auth_request *req);\n\tint\t(*assoc)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t struct cfg80211_assoc_request *req);\n\tint\t(*deauth)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t  struct cfg80211_deauth_request *req);\n\tint\t(*disassoc)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t    struct cfg80211_disassoc_request *req);\n\n\tint\t(*connect)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t   struct cfg80211_connect_params *sme);\n\tint\t(*update_connect_params)(struct wiphy *wiphy,\n\t\t\t\t\t struct net_device *dev,\n\t\t\t\t\t struct cfg80211_connect_params *sme,\n\t\t\t\t\t u32 changed);\n\tint\t(*disconnect)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t      u16 reason_code);\n\n\tint\t(*join_ibss)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t     struct cfg80211_ibss_params *params);\n\tint\t(*leave_ibss)(struct wiphy *wiphy, struct net_device *dev);\n\n\tint\t(*set_mcast_rate)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t\t  int rate[NUM_NL80211_BANDS]);\n\n\tint\t(*set_wiphy_params)(struct wiphy *wiphy, u32 changed);\n\n\tint\t(*set_tx_power)(struct wiphy *wiphy, struct wireless_dev *wdev,\n\t\t\t\tenum nl80211_tx_power_setting type, int mbm);\n\tint\t(*get_tx_power)(struct wiphy *wiphy, struct wireless_dev *wdev,\n\t\t\t\tint *dbm);\n\n\tvoid\t(*rfkill_poll)(struct wiphy *wiphy);\n\n#ifdef CONFIG_NL80211_TESTMODE\n\tint\t(*testmode_cmd)(struct wiphy *wiphy, struct wireless_dev *wdev,\n\t\t\t\tvoid *data, int len);\n\tint\t(*testmode_dump)(struct wiphy *wiphy, struct sk_buff *skb,\n\t\t\t\t struct netlink_callback *cb,\n\t\t\t\t void *data, int len);\n#endif\n\n\tint\t(*set_bitrate_mask)(struct wiphy *wiphy,\n\t\t\t\t    struct net_device *dev,\n\t\t\t\t    const u8 *peer,\n\t\t\t\t    const struct cfg80211_bitrate_mask *mask);\n\n\tint\t(*dump_survey)(struct wiphy *wiphy, struct net_device *netdev,\n\t\t\tint idx, struct survey_info *info);\n\n\tint\t(*set_pmksa)(struct wiphy *wiphy, struct net_device *netdev,\n\t\t\t     struct cfg80211_pmksa *pmksa);\n\tint\t(*del_pmksa)(struct wiphy *wiphy, struct net_device *netdev,\n\t\t\t     struct cfg80211_pmksa *pmksa);\n\tint\t(*flush_pmksa)(struct wiphy *wiphy, struct net_device *netdev);\n\n\tint\t(*remain_on_channel)(struct wiphy *wiphy,\n\t\t\t\t     struct wireless_dev *wdev,\n\t\t\t\t     struct ieee80211_channel *chan,\n\t\t\t\t     unsigned int duration,\n\t\t\t\t     u64 *cookie);\n\tint\t(*cancel_remain_on_channel)(struct wiphy *wiphy,\n\t\t\t\t\t    struct wireless_dev *wdev,\n\t\t\t\t\t    u64 cookie);\n\n\tint\t(*mgmt_tx)(struct wiphy *wiphy, struct wireless_dev *wdev,\n\t\t\t   struct cfg80211_mgmt_tx_params *params,\n\t\t\t   u64 *cookie);\n\tint\t(*mgmt_tx_cancel_wait)(struct wiphy *wiphy,\n\t\t\t\t       struct wireless_dev *wdev,\n\t\t\t\t       u64 cookie);\n\n\tint\t(*set_power_mgmt)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t\t  bool enabled, int timeout);\n\n\tint\t(*set_cqm_rssi_config)(struct wiphy *wiphy,\n\t\t\t\t       struct net_device *dev,\n\t\t\t\t       s32 rssi_thold, u32 rssi_hyst);\n\n\tint\t(*set_cqm_rssi_range_config)(struct wiphy *wiphy,\n\t\t\t\t\t     struct net_device *dev,\n\t\t\t\t\t     s32 rssi_low, s32 rssi_high);\n\n\tint\t(*set_cqm_txe_config)(struct wiphy *wiphy,\n\t\t\t\t      struct net_device *dev,\n\t\t\t\t      u32 rate, u32 pkts, u32 intvl);\n\n\tvoid\t(*update_mgmt_frame_registrations)(struct wiphy *wiphy,\n\t\t\t\t\t\t   struct wireless_dev *wdev,\n\t\t\t\t\t\t   struct mgmt_frame_regs *upd);\n\n\tint\t(*set_antenna)(struct wiphy *wiphy, u32 tx_ant, u32 rx_ant);\n\tint\t(*get_antenna)(struct wiphy *wiphy, u32 *tx_ant, u32 *rx_ant);\n\n\tint\t(*sched_scan_start)(struct wiphy *wiphy,\n\t\t\t\tstruct net_device *dev,\n\t\t\t\tstruct cfg80211_sched_scan_request *request);\n\tint\t(*sched_scan_stop)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t\t   u64 reqid);\n\n\tint\t(*set_rekey_data)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t\t  struct cfg80211_gtk_rekey_data *data);\n\n\tint\t(*tdls_mgmt)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t     const u8 *peer, u8 action_code,  u8 dialog_token,\n\t\t\t     u16 status_code, u32 peer_capability,\n\t\t\t     bool initiator, const u8 *buf, size_t len);\n\tint\t(*tdls_oper)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t     const u8 *peer, enum nl80211_tdls_operation oper);\n\n\tint\t(*probe_client)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t\tconst u8 *peer, u64 *cookie);\n\n\tint\t(*set_noack_map)(struct wiphy *wiphy,\n\t\t\t\t  struct net_device *dev,\n\t\t\t\t  u16 noack_map);\n\n\tint\t(*get_channel)(struct wiphy *wiphy,\n\t\t\t       struct wireless_dev *wdev,\n\t\t\t       struct cfg80211_chan_def *chandef);\n\n\tint\t(*start_p2p_device)(struct wiphy *wiphy,\n\t\t\t\t    struct wireless_dev *wdev);\n\tvoid\t(*stop_p2p_device)(struct wiphy *wiphy,\n\t\t\t\t   struct wireless_dev *wdev);\n\n\tint\t(*set_mac_acl)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t       const struct cfg80211_acl_data *params);\n\n\tint\t(*start_radar_detection)(struct wiphy *wiphy,\n\t\t\t\t\t struct net_device *dev,\n\t\t\t\t\t struct cfg80211_chan_def *chandef,\n\t\t\t\t\t u32 cac_time_ms);\n\tvoid\t(*end_cac)(struct wiphy *wiphy,\n\t\t\t\tstruct net_device *dev);\n\tint\t(*update_ft_ies)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t\t struct cfg80211_update_ft_ies_params *ftie);\n\tint\t(*crit_proto_start)(struct wiphy *wiphy,\n\t\t\t\t    struct wireless_dev *wdev,\n\t\t\t\t    enum nl80211_crit_proto_id protocol,\n\t\t\t\t    u16 duration);\n\tvoid\t(*crit_proto_stop)(struct wiphy *wiphy,\n\t\t\t\t   struct wireless_dev *wdev);\n\tint\t(*set_coalesce)(struct wiphy *wiphy,\n\t\t\t\tstruct cfg80211_coalesce *coalesce);\n\n\tint\t(*channel_switch)(struct wiphy *wiphy,\n\t\t\t\t  struct net_device *dev,\n\t\t\t\t  struct cfg80211_csa_settings *params);\n\n\tint     (*set_qos_map)(struct wiphy *wiphy,\n\t\t\t       struct net_device *dev,\n\t\t\t       struct cfg80211_qos_map *qos_map);\n\n\tint\t(*set_ap_chanwidth)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t\t    struct cfg80211_chan_def *chandef);\n\n\tint\t(*add_tx_ts)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t     u8 tsid, const u8 *peer, u8 user_prio,\n\t\t\t     u16 admitted_time);\n\tint\t(*del_tx_ts)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t     u8 tsid, const u8 *peer);\n\n\tint\t(*tdls_channel_switch)(struct wiphy *wiphy,\n\t\t\t\t       struct net_device *dev,\n\t\t\t\t       const u8 *addr, u8 oper_class,\n\t\t\t\t       struct cfg80211_chan_def *chandef);\n\tvoid\t(*tdls_cancel_channel_switch)(struct wiphy *wiphy,\n\t\t\t\t\t      struct net_device *dev,\n\t\t\t\t\t      const u8 *addr);\n\tint\t(*start_nan)(struct wiphy *wiphy, struct wireless_dev *wdev,\n\t\t\t     struct cfg80211_nan_conf *conf);\n\tvoid\t(*stop_nan)(struct wiphy *wiphy, struct wireless_dev *wdev);\n\tint\t(*add_nan_func)(struct wiphy *wiphy, struct wireless_dev *wdev,\n\t\t\t\tstruct cfg80211_nan_func *nan_func);\n\tvoid\t(*del_nan_func)(struct wiphy *wiphy, struct wireless_dev *wdev,\n\t\t\t       u64 cookie);\n\tint\t(*nan_change_conf)(struct wiphy *wiphy,\n\t\t\t\t   struct wireless_dev *wdev,\n\t\t\t\t   struct cfg80211_nan_conf *conf,\n\t\t\t\t   u32 changes);\n\n\tint\t(*set_multicast_to_unicast)(struct wiphy *wiphy,\n\t\t\t\t\t    struct net_device *dev,\n\t\t\t\t\t    const bool enabled);\n\n\tint\t(*get_txq_stats)(struct wiphy *wiphy,\n\t\t\t\t struct wireless_dev *wdev,\n\t\t\t\t struct cfg80211_txq_stats *txqstats);\n\n\tint\t(*set_pmk)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t   const struct cfg80211_pmk_conf *conf);\n\tint\t(*del_pmk)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t   const u8 *aa);\n\tint     (*external_auth)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t\t struct cfg80211_external_auth_params *params);\n\n\tint\t(*tx_control_port)(struct wiphy *wiphy,\n\t\t\t\t   struct net_device *dev,\n\t\t\t\t   const u8 *buf, size_t len,\n\t\t\t\t   const u8 *dest, const __be16 proto,\n\t\t\t\t   const bool noencrypt,\n\t\t\t\t   u64 *cookie);\n\n\tint\t(*get_ftm_responder_stats)(struct wiphy *wiphy,\n\t\t\t\tstruct net_device *dev,\n\t\t\t\tstruct cfg80211_ftm_responder_stats *ftm_stats);\n\n\tint\t(*start_pmsr)(struct wiphy *wiphy, struct wireless_dev *wdev,\n\t\t\t      struct cfg80211_pmsr_request *request);\n\tvoid\t(*abort_pmsr)(struct wiphy *wiphy, struct wireless_dev *wdev,\n\t\t\t      struct cfg80211_pmsr_request *request);\n\tint\t(*update_owe_info)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t\t   struct cfg80211_update_owe_info *owe_info);\n\tint\t(*probe_mesh_link)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t\t   const u8 *buf, size_t len);\n\tint     (*set_tid_config)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t\t  struct cfg80211_tid_config *tid_conf);\n\tint\t(*reset_tid_config)(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t\t    const u8 *peer, u8 tids);\n\tint\t(*set_sar_specs)(struct wiphy *wiphy,\n\t\t\t\t struct cfg80211_sar_specs *sar);\n};\n\n/*\n * wireless hardware and networking interfaces structures\n * and registration/helper functions\n */\n\n/**\n * enum wiphy_flags - wiphy capability flags\n *\n * @WIPHY_FLAG_SPLIT_SCAN_6GHZ: if set to true, the scan request will be split\n *\t into two, first for legacy bands and second for UHB.\n * @WIPHY_FLAG_NETNS_OK: if not set, do not allow changing the netns of this\n *\twiphy at all\n * @WIPHY_FLAG_PS_ON_BY_DEFAULT: if set to true, powersave will be enabled\n *\tby default -- this flag will be set depending on the kernel's default\n *\ton wiphy_new(), but can be changed by the driver if it has a good\n *\treason to override the default\n * @WIPHY_FLAG_4ADDR_AP: supports 4addr mode even on AP (with a single station\n *\ton a VLAN interface). This flag also serves an extra purpose of\n *\tsupporting 4ADDR AP mode on devices which do not support AP/VLAN iftype.\n * @WIPHY_FLAG_4ADDR_STATION: supports 4addr mode even as a station\n * @WIPHY_FLAG_CONTROL_PORT_PROTOCOL: This device supports setting the\n *\tcontrol port protocol ethertype. The device also honours the\n *\tcontrol_port_no_encrypt flag.\n * @WIPHY_FLAG_IBSS_RSN: The device supports IBSS RSN.\n * @WIPHY_FLAG_MESH_AUTH: The device supports mesh authentication by routing\n *\tauth frames to userspace. See @NL80211_MESH_SETUP_USERSPACE_AUTH.\n * @WIPHY_FLAG_SUPPORTS_FW_ROAM: The device supports roaming feature in the\n *\tfirmware.\n * @WIPHY_FLAG_AP_UAPSD: The device supports uapsd on AP.\n * @WIPHY_FLAG_SUPPORTS_TDLS: The device supports TDLS (802.11z) operation.\n * @WIPHY_FLAG_TDLS_EXTERNAL_SETUP: The device does not handle TDLS (802.11z)\n *\tlink setup/discovery operations internally. Setup, discovery and\n *\tteardown packets should be sent through the @NL80211_CMD_TDLS_MGMT\n *\tcommand. When this flag is not set, @NL80211_CMD_TDLS_OPER should be\n *\tused for asking the driver/firmware to perform a TDLS operation.\n * @WIPHY_FLAG_HAVE_AP_SME: device integrates AP SME\n * @WIPHY_FLAG_REPORTS_OBSS: the device will report beacons from other BSSes\n *\twhen there are virtual interfaces in AP mode by calling\n *\tcfg80211_report_obss_beacon().\n * @WIPHY_FLAG_AP_PROBE_RESP_OFFLOAD: When operating as an AP, the device\n *\tresponds to probe-requests in hardware.\n * @WIPHY_FLAG_OFFCHAN_TX: Device supports direct off-channel TX.\n * @WIPHY_FLAG_HAS_REMAIN_ON_CHANNEL: Device supports remain-on-channel call.\n * @WIPHY_FLAG_SUPPORTS_5_10_MHZ: Device supports 5 MHz and 10 MHz channels.\n * @WIPHY_FLAG_HAS_CHANNEL_SWITCH: Device supports channel switch in\n *\tbeaconing mode (AP, IBSS, Mesh, ...).\n * @WIPHY_FLAG_HAS_STATIC_WEP: The device supports static WEP key installation\n *\tbefore connection.\n * @WIPHY_FLAG_SUPPORTS_EXT_KEK_KCK: The device supports bigger kek and kck keys\n */\nenum wiphy_flags {\n\tWIPHY_FLAG_SUPPORTS_EXT_KEK_KCK\t\t= BIT(0),\n\t/* use hole at 1 */\n\tWIPHY_FLAG_SPLIT_SCAN_6GHZ\t\t= BIT(2),\n\tWIPHY_FLAG_NETNS_OK\t\t\t= BIT(3),\n\tWIPHY_FLAG_PS_ON_BY_DEFAULT\t\t= BIT(4),\n\tWIPHY_FLAG_4ADDR_AP\t\t\t= BIT(5),\n\tWIPHY_FLAG_4ADDR_STATION\t\t= BIT(6),\n\tWIPHY_FLAG_CONTROL_PORT_PROTOCOL\t= BIT(7),\n\tWIPHY_FLAG_IBSS_RSN\t\t\t= BIT(8),\n\tWIPHY_FLAG_MESH_AUTH\t\t\t= BIT(10),\n\t/* use hole at 11 */\n\t/* use hole at 12 */\n\tWIPHY_FLAG_SUPPORTS_FW_ROAM\t\t= BIT(13),\n\tWIPHY_FLAG_AP_UAPSD\t\t\t= BIT(14),\n\tWIPHY_FLAG_SUPPORTS_TDLS\t\t= BIT(15),\n\tWIPHY_FLAG_TDLS_EXTERNAL_SETUP\t\t= BIT(16),\n\tWIPHY_FLAG_HAVE_AP_SME\t\t\t= BIT(17),\n\tWIPHY_FLAG_REPORTS_OBSS\t\t\t= BIT(18),\n\tWIPHY_FLAG_AP_PROBE_RESP_OFFLOAD\t= BIT(19),\n\tWIPHY_FLAG_OFFCHAN_TX\t\t\t= BIT(20),\n\tWIPHY_FLAG_HAS_REMAIN_ON_CHANNEL\t= BIT(21),\n\tWIPHY_FLAG_SUPPORTS_5_10_MHZ\t\t= BIT(22),\n\tWIPHY_FLAG_HAS_CHANNEL_SWITCH\t\t= BIT(23),\n\tWIPHY_FLAG_HAS_STATIC_WEP\t\t= BIT(24),\n};\n\n/**\n * struct ieee80211_iface_limit - limit on certain interface types\n * @max: maximum number of interfaces of these types\n * @types: interface types (bits)\n */\nstruct ieee80211_iface_limit {\n\tu16 max;\n\tu16 types;\n};\n\n/**\n * struct ieee80211_iface_combination - possible interface combination\n *\n * With this structure the driver can describe which interface\n * combinations it supports concurrently.\n *\n * Examples:\n *\n * 1. Allow #STA <= 1, #AP <= 1, matching BI, channels = 1, 2 total:\n *\n *    .. code-block:: c\n *\n *\tstruct ieee80211_iface_limit limits1[] = {\n *\t\t{ .max = 1, .types = BIT(NL80211_IFTYPE_STATION), },\n *\t\t{ .max = 1, .types = BIT(NL80211_IFTYPE_AP}, },\n *\t};\n *\tstruct ieee80211_iface_combination combination1 = {\n *\t\t.limits = limits1,\n *\t\t.n_limits = ARRAY_SIZE(limits1),\n *\t\t.max_interfaces = 2,\n *\t\t.beacon_int_infra_match = true,\n *\t};\n *\n *\n * 2. Allow #{AP, P2P-GO} <= 8, channels = 1, 8 total:\n *\n *    .. code-block:: c\n *\n *\tstruct ieee80211_iface_limit limits2[] = {\n *\t\t{ .max = 8, .types = BIT(NL80211_IFTYPE_AP) |\n *\t\t\t\t     BIT(NL80211_IFTYPE_P2P_GO), },\n *\t};\n *\tstruct ieee80211_iface_combination combination2 = {\n *\t\t.limits = limits2,\n *\t\t.n_limits = ARRAY_SIZE(limits2),\n *\t\t.max_interfaces = 8,\n *\t\t.num_different_channels = 1,\n *\t};\n *\n *\n * 3. Allow #STA <= 1, #{P2P-client,P2P-GO} <= 3 on two channels, 4 total.\n *\n *    This allows for an infrastructure connection and three P2P connections.\n *\n *    .. code-block:: c\n *\n *\tstruct ieee80211_iface_limit limits3[] = {\n *\t\t{ .max = 1, .types = BIT(NL80211_IFTYPE_STATION), },\n *\t\t{ .max = 3, .types = BIT(NL80211_IFTYPE_P2P_GO) |\n *\t\t\t\t     BIT(NL80211_IFTYPE_P2P_CLIENT), },\n *\t};\n *\tstruct ieee80211_iface_combination combination3 = {\n *\t\t.limits = limits3,\n *\t\t.n_limits = ARRAY_SIZE(limits3),\n *\t\t.max_interfaces = 4,\n *\t\t.num_different_channels = 2,\n *\t};\n *\n */\nstruct ieee80211_iface_combination {\n\t/**\n\t * @limits:\n\t * limits for the given interface types\n\t */\n\tconst struct ieee80211_iface_limit *limits;\n\n\t/**\n\t * @num_different_channels:\n\t * can use up to this many different channels\n\t */\n\tu32 num_different_channels;\n\n\t/**\n\t * @max_interfaces:\n\t * maximum number of interfaces in total allowed in this group\n\t */\n\tu16 max_interfaces;\n\n\t/**\n\t * @n_limits:\n\t * number of limitations\n\t */\n\tu8 n_limits;\n\n\t/**\n\t * @beacon_int_infra_match:\n\t * In this combination, the beacon intervals between infrastructure\n\t * and AP types must match. This is required only in special cases.\n\t */\n\tbool beacon_int_infra_match;\n\n\t/**\n\t * @radar_detect_widths:\n\t * bitmap of channel widths supported for radar detection\n\t */\n\tu8 radar_detect_widths;\n\n\t/**\n\t * @radar_detect_regions:\n\t * bitmap of regions supported for radar detection\n\t */\n\tu8 radar_detect_regions;\n\n\t/**\n\t * @beacon_int_min_gcd:\n\t * This interface combination supports different beacon intervals.\n\t *\n\t * = 0\n\t *   all beacon intervals for different interface must be same.\n\t * > 0\n\t *   any beacon interval for the interface part of this combination AND\n\t *   GCD of all beacon intervals from beaconing interfaces of this\n\t *   combination must be greater or equal to this value.\n\t */\n\tu32 beacon_int_min_gcd;\n};\n\nstruct ieee80211_txrx_stypes {\n\tu16 tx, rx;\n};\n\n/**\n * enum wiphy_wowlan_support_flags - WoWLAN support flags\n * @WIPHY_WOWLAN_ANY: supports wakeup for the special \"any\"\n *\ttrigger that keeps the device operating as-is and\n *\twakes up the host on any activity, for example a\n *\treceived packet that passed filtering; note that the\n *\tpacket should be preserved in that case\n * @WIPHY_WOWLAN_MAGIC_PKT: supports wakeup on magic packet\n *\t(see nl80211.h)\n * @WIPHY_WOWLAN_DISCONNECT: supports wakeup on disconnect\n * @WIPHY_WOWLAN_SUPPORTS_GTK_REKEY: supports GTK rekeying while asleep\n * @WIPHY_WOWLAN_GTK_REKEY_FAILURE: supports wakeup on GTK rekey failure\n * @WIPHY_WOWLAN_EAP_IDENTITY_REQ: supports wakeup on EAP identity request\n * @WIPHY_WOWLAN_4WAY_HANDSHAKE: supports wakeup on 4-way handshake failure\n * @WIPHY_WOWLAN_RFKILL_RELEASE: supports wakeup on RF-kill release\n * @WIPHY_WOWLAN_NET_DETECT: supports wakeup on network detection\n */\nenum wiphy_wowlan_support_flags {\n\tWIPHY_WOWLAN_ANY\t\t= BIT(0),\n\tWIPHY_WOWLAN_MAGIC_PKT\t\t= BIT(1),\n\tWIPHY_WOWLAN_DISCONNECT\t\t= BIT(2),\n\tWIPHY_WOWLAN_SUPPORTS_GTK_REKEY\t= BIT(3),\n\tWIPHY_WOWLAN_GTK_REKEY_FAILURE\t= BIT(4),\n\tWIPHY_WOWLAN_EAP_IDENTITY_REQ\t= BIT(5),\n\tWIPHY_WOWLAN_4WAY_HANDSHAKE\t= BIT(6),\n\tWIPHY_WOWLAN_RFKILL_RELEASE\t= BIT(7),\n\tWIPHY_WOWLAN_NET_DETECT\t\t= BIT(8),\n};\n\nstruct wiphy_wowlan_tcp_support {\n\tconst struct nl80211_wowlan_tcp_data_token_feature *tok;\n\tu32 data_payload_max;\n\tu32 data_interval_max;\n\tu32 wake_payload_max;\n\tbool seq;\n};\n\n/**\n * struct wiphy_wowlan_support - WoWLAN support data\n * @flags: see &enum wiphy_wowlan_support_flags\n * @n_patterns: number of supported wakeup patterns\n *\t(see nl80211.h for the pattern definition)\n * @pattern_max_len: maximum length of each pattern\n * @pattern_min_len: minimum length of each pattern\n * @max_pkt_offset: maximum Rx packet offset\n * @max_nd_match_sets: maximum number of matchsets for net-detect,\n *\tsimilar, but not necessarily identical, to max_match_sets for\n *\tscheduled scans.\n *\tSee &struct cfg80211_sched_scan_request.@match_sets for more\n *\tdetails.\n * @tcp: TCP wakeup support information\n */\nstruct wiphy_wowlan_support {\n\tu32 flags;\n\tint n_patterns;\n\tint pattern_max_len;\n\tint pattern_min_len;\n\tint max_pkt_offset;\n\tint max_nd_match_sets;\n\tconst struct wiphy_wowlan_tcp_support *tcp;\n};\n\n/**\n * struct wiphy_coalesce_support - coalesce support data\n * @n_rules: maximum number of coalesce rules\n * @max_delay: maximum supported coalescing delay in msecs\n * @n_patterns: number of supported patterns in a rule\n *\t(see nl80211.h for the pattern definition)\n * @pattern_max_len: maximum length of each pattern\n * @pattern_min_len: minimum length of each pattern\n * @max_pkt_offset: maximum Rx packet offset\n */\nstruct wiphy_coalesce_support {\n\tint n_rules;\n\tint max_delay;\n\tint n_patterns;\n\tint pattern_max_len;\n\tint pattern_min_len;\n\tint max_pkt_offset;\n};\n\n/**\n * enum wiphy_vendor_command_flags - validation flags for vendor commands\n * @WIPHY_VENDOR_CMD_NEED_WDEV: vendor command requires wdev\n * @WIPHY_VENDOR_CMD_NEED_NETDEV: vendor command requires netdev\n * @WIPHY_VENDOR_CMD_NEED_RUNNING: interface/wdev must be up & running\n *\t(must be combined with %_WDEV or %_NETDEV)\n */\nenum wiphy_vendor_command_flags {\n\tWIPHY_VENDOR_CMD_NEED_WDEV = BIT(0),\n\tWIPHY_VENDOR_CMD_NEED_NETDEV = BIT(1),\n\tWIPHY_VENDOR_CMD_NEED_RUNNING = BIT(2),\n};\n\n/**\n * enum wiphy_opmode_flag - Station's ht/vht operation mode information flags\n *\n * @STA_OPMODE_MAX_BW_CHANGED: Max Bandwidth changed\n * @STA_OPMODE_SMPS_MODE_CHANGED: SMPS mode changed\n * @STA_OPMODE_N_SS_CHANGED: max N_SS (number of spatial streams) changed\n *\n */\nenum wiphy_opmode_flag {\n\tSTA_OPMODE_MAX_BW_CHANGED\t= BIT(0),\n\tSTA_OPMODE_SMPS_MODE_CHANGED\t= BIT(1),\n\tSTA_OPMODE_N_SS_CHANGED\t\t= BIT(2),\n};\n\n/**\n * struct sta_opmode_info - Station's ht/vht operation mode information\n * @changed: contains value from &enum wiphy_opmode_flag\n * @smps_mode: New SMPS mode value from &enum nl80211_smps_mode of a station\n * @bw: new max bandwidth value from &enum nl80211_chan_width of a station\n * @rx_nss: new rx_nss value of a station\n */\n\nstruct sta_opmode_info {\n\tu32 changed;\n\tenum nl80211_smps_mode smps_mode;\n\tenum nl80211_chan_width bw;\n\tu8 rx_nss;\n};\n\n#define VENDOR_CMD_RAW_DATA ((const struct nla_policy *)(long)(-ENODATA))\n\n/**\n * struct wiphy_vendor_command - vendor command definition\n * @info: vendor command identifying information, as used in nl80211\n * @flags: flags, see &enum wiphy_vendor_command_flags\n * @doit: callback for the operation, note that wdev is %NULL if the\n *\tflags didn't ask for a wdev and non-%NULL otherwise; the data\n *\tpointer may be %NULL if userspace provided no data at all\n * @dumpit: dump callback, for transferring bigger/multiple items. The\n *\t@storage points to cb->args[5], ie. is preserved over the multiple\n *\tdumpit calls.\n * @policy: policy pointer for attributes within %NL80211_ATTR_VENDOR_DATA.\n *\tSet this to %VENDOR_CMD_RAW_DATA if no policy can be given and the\n *\tattribute is just raw data (e.g. a firmware command).\n * @maxattr: highest attribute number in policy\n * It's recommended to not have the same sub command with both @doit and\n * @dumpit, so that userspace can assume certain ones are get and others\n * are used with dump requests.\n */\nstruct wiphy_vendor_command {\n\tstruct nl80211_vendor_cmd_info info;\n\tu32 flags;\n\tint (*doit)(struct wiphy *wiphy, struct wireless_dev *wdev,\n\t\t    const void *data, int data_len);\n\tint (*dumpit)(struct wiphy *wiphy, struct wireless_dev *wdev,\n\t\t      struct sk_buff *skb, const void *data, int data_len,\n\t\t      unsigned long *storage);\n\tconst struct nla_policy *policy;\n\tunsigned int maxattr;\n};\n\n/**\n * struct wiphy_iftype_ext_capab - extended capabilities per interface type\n * @iftype: interface type\n * @extended_capabilities: extended capabilities supported by the driver,\n *\tadditional capabilities might be supported by userspace; these are the\n *\t802.11 extended capabilities (\"Extended Capabilities element\") and are\n *\tin the same format as in the information element. See IEEE Std\n *\t802.11-2012 8.4.2.29 for the defined fields.\n * @extended_capabilities_mask: mask of the valid values\n * @extended_capabilities_len: length of the extended capabilities\n */\nstruct wiphy_iftype_ext_capab {\n\tenum nl80211_iftype iftype;\n\tconst u8 *extended_capabilities;\n\tconst u8 *extended_capabilities_mask;\n\tu8 extended_capabilities_len;\n};\n\n/**\n * struct cfg80211_pmsr_capabilities - cfg80211 peer measurement capabilities\n * @max_peers: maximum number of peers in a single measurement\n * @report_ap_tsf: can report assoc AP's TSF for radio resource measurement\n * @randomize_mac_addr: can randomize MAC address for measurement\n * @ftm.supported: FTM measurement is supported\n * @ftm.asap: ASAP-mode is supported\n * @ftm.non_asap: non-ASAP-mode is supported\n * @ftm.request_lci: can request LCI data\n * @ftm.request_civicloc: can request civic location data\n * @ftm.preambles: bitmap of preambles supported (&enum nl80211_preamble)\n * @ftm.bandwidths: bitmap of bandwidths supported (&enum nl80211_chan_width)\n * @ftm.max_bursts_exponent: maximum burst exponent supported\n *\t(set to -1 if not limited; note that setting this will necessarily\n *\tforbid using the value 15 to let the responder pick)\n * @ftm.max_ftms_per_burst: maximum FTMs per burst supported (set to 0 if\n *\tnot limited)\n * @ftm.trigger_based: trigger based ranging measurement is supported\n * @ftm.non_trigger_based: non trigger based ranging measurement is supported\n */\nstruct cfg80211_pmsr_capabilities {\n\tunsigned int max_peers;\n\tu8 report_ap_tsf:1,\n\t   randomize_mac_addr:1;\n\n\tstruct {\n\t\tu32 preambles;\n\t\tu32 bandwidths;\n\t\ts8 max_bursts_exponent;\n\t\tu8 max_ftms_per_burst;\n\t\tu8 supported:1,\n\t\t   asap:1,\n\t\t   non_asap:1,\n\t\t   request_lci:1,\n\t\t   request_civicloc:1,\n\t\t   trigger_based:1,\n\t\t   non_trigger_based:1;\n\t} ftm;\n};\n\n/**\n * struct wiphy_iftype_akm_suites - This structure encapsulates supported akm\n * suites for interface types defined in @iftypes_mask. Each type in the\n * @iftypes_mask must be unique across all instances of iftype_akm_suites.\n *\n * @iftypes_mask: bitmask of interfaces types\n * @akm_suites: points to an array of supported akm suites\n * @n_akm_suites: number of supported AKM suites\n */\nstruct wiphy_iftype_akm_suites {\n\tu16 iftypes_mask;\n\tconst u32 *akm_suites;\n\tint n_akm_suites;\n};\n\n/**\n * struct wiphy - wireless hardware description\n * @mtx: mutex for the data (structures) of this device\n * @reg_notifier: the driver's regulatory notification callback,\n *\tnote that if your driver uses wiphy_apply_custom_regulatory()\n *\tthe reg_notifier's request can be passed as NULL\n * @regd: the driver's regulatory domain, if one was requested via\n *\tthe regulatory_hint() API. This can be used by the driver\n *\ton the reg_notifier() if it chooses to ignore future\n *\tregulatory domain changes caused by other drivers.\n * @signal_type: signal type reported in &struct cfg80211_bss.\n * @cipher_suites: supported cipher suites\n * @n_cipher_suites: number of supported cipher suites\n * @akm_suites: supported AKM suites. These are the default AKMs supported if\n *\tthe supported AKMs not advertized for a specific interface type in\n *\tiftype_akm_suites.\n * @n_akm_suites: number of supported AKM suites\n * @iftype_akm_suites: array of supported akm suites info per interface type.\n *\tNote that the bits in @iftypes_mask inside this structure cannot\n *\toverlap (i.e. only one occurrence of each type is allowed across all\n *\tinstances of iftype_akm_suites).\n * @num_iftype_akm_suites: number of interface types for which supported akm\n *\tsuites are specified separately.\n * @retry_short: Retry limit for short frames (dot11ShortRetryLimit)\n * @retry_long: Retry limit for long frames (dot11LongRetryLimit)\n * @frag_threshold: Fragmentation threshold (dot11FragmentationThreshold);\n *\t-1 = fragmentation disabled, only odd values >= 256 used\n * @rts_threshold: RTS threshold (dot11RTSThreshold); -1 = RTS/CTS disabled\n * @_net: the network namespace this wiphy currently lives in\n * @perm_addr: permanent MAC address of this device\n * @addr_mask: If the device supports multiple MAC addresses by masking,\n *\tset this to a mask with variable bits set to 1, e.g. if the last\n *\tfour bits are variable then set it to 00-00-00-00-00-0f. The actual\n *\tvariable bits shall be determined by the interfaces added, with\n *\tinterfaces not matching the mask being rejected to be brought up.\n * @n_addresses: number of addresses in @addresses.\n * @addresses: If the device has more than one address, set this pointer\n *\tto a list of addresses (6 bytes each). The first one will be used\n *\tby default for perm_addr. In this case, the mask should be set to\n *\tall-zeroes. In this case it is assumed that the device can handle\n *\tthe same number of arbitrary MAC addresses.\n * @registered: protects ->resume and ->suspend sysfs callbacks against\n *\tunregister hardware\n * @debugfsdir: debugfs directory used for this wiphy (ieee80211/<wiphyname>).\n *\tIt will be renamed automatically on wiphy renames\n * @dev: (virtual) struct device for this wiphy. The item in\n *\t/sys/class/ieee80211/ points to this. You need use set_wiphy_dev()\n *\t(see below).\n * @wext: wireless extension handlers\n * @priv: driver private data (sized according to wiphy_new() parameter)\n * @interface_modes: bitmask of interfaces types valid for this wiphy,\n *\tmust be set by driver\n * @iface_combinations: Valid interface combinations array, should not\n *\tlist single interface types.\n * @n_iface_combinations: number of entries in @iface_combinations array.\n * @software_iftypes: bitmask of software interface types, these are not\n *\tsubject to any restrictions since they are purely managed in SW.\n * @flags: wiphy flags, see &enum wiphy_flags\n * @regulatory_flags: wiphy regulatory flags, see\n *\t&enum ieee80211_regulatory_flags\n * @features: features advertised to nl80211, see &enum nl80211_feature_flags.\n * @ext_features: extended features advertised to nl80211, see\n *\t&enum nl80211_ext_feature_index.\n * @bss_priv_size: each BSS struct has private data allocated with it,\n *\tthis variable determines its size\n * @max_scan_ssids: maximum number of SSIDs the device can scan for in\n *\tany given scan\n * @max_sched_scan_reqs: maximum number of scheduled scan requests that\n *\tthe device can run concurrently.\n * @max_sched_scan_ssids: maximum number of SSIDs the device can scan\n *\tfor in any given scheduled scan\n * @max_match_sets: maximum number of match sets the device can handle\n *\twhen performing a scheduled scan, 0 if filtering is not\n *\tsupported.\n * @max_scan_ie_len: maximum length of user-controlled IEs device can\n *\tadd to probe request frames transmitted during a scan, must not\n *\tinclude fixed IEs like supported rates\n * @max_sched_scan_ie_len: same as max_scan_ie_len, but for scheduled\n *\tscans\n * @max_sched_scan_plans: maximum number of scan plans (scan interval and number\n *\tof iterations) for scheduled scan supported by the device.\n * @max_sched_scan_plan_interval: maximum interval (in seconds) for a\n *\tsingle scan plan supported by the device.\n * @max_sched_scan_plan_iterations: maximum number of iterations for a single\n *\tscan plan supported by the device.\n * @coverage_class: current coverage class\n * @fw_version: firmware version for ethtool reporting\n * @hw_version: hardware version for ethtool reporting\n * @max_num_pmkids: maximum number of PMKIDs supported by device\n * @privid: a pointer that drivers can use to identify if an arbitrary\n *\twiphy is theirs, e.g. in global notifiers\n * @bands: information about bands/channels supported by this device\n *\n * @mgmt_stypes: bitmasks of frame subtypes that can be subscribed to or\n *\ttransmitted through nl80211, points to an array indexed by interface\n *\ttype\n *\n * @available_antennas_tx: bitmap of antennas which are available to be\n *\tconfigured as TX antennas. Antenna configuration commands will be\n *\trejected unless this or @available_antennas_rx is set.\n *\n * @available_antennas_rx: bitmap of antennas which are available to be\n *\tconfigured as RX antennas. Antenna configuration commands will be\n *\trejected unless this or @available_antennas_tx is set.\n *\n * @probe_resp_offload:\n *\t Bitmap of supported protocols for probe response offloading.\n *\t See &enum nl80211_probe_resp_offload_support_attr. Only valid\n *\t when the wiphy flag @WIPHY_FLAG_AP_PROBE_RESP_OFFLOAD is set.\n *\n * @max_remain_on_channel_duration: Maximum time a remain-on-channel operation\n *\tmay request, if implemented.\n *\n * @wowlan: WoWLAN support information\n * @wowlan_config: current WoWLAN configuration; this should usually not be\n *\tused since access to it is necessarily racy, use the parameter passed\n *\tto the suspend() operation instead.\n *\n * @ap_sme_capa: AP SME capabilities, flags from &enum nl80211_ap_sme_features.\n * @ht_capa_mod_mask:  Specify what ht_cap values can be over-ridden.\n *\tIf null, then none can be over-ridden.\n * @vht_capa_mod_mask:  Specify what VHT capabilities can be over-ridden.\n *\tIf null, then none can be over-ridden.\n *\n * @wdev_list: the list of associated (virtual) interfaces; this list must\n *\tnot be modified by the driver, but can be read with RTNL/RCU protection.\n *\n * @max_acl_mac_addrs: Maximum number of MAC addresses that the device\n *\tsupports for ACL.\n *\n * @extended_capabilities: extended capabilities supported by the driver,\n *\tadditional capabilities might be supported by userspace; these are\n *\tthe 802.11 extended capabilities (\"Extended Capabilities element\")\n *\tand are in the same format as in the information element. See\n *\t802.11-2012 8.4.2.29 for the defined fields. These are the default\n *\textended capabilities to be used if the capabilities are not specified\n *\tfor a specific interface type in iftype_ext_capab.\n * @extended_capabilities_mask: mask of the valid values\n * @extended_capabilities_len: length of the extended capabilities\n * @iftype_ext_capab: array of extended capabilities per interface type\n * @num_iftype_ext_capab: number of interface types for which extended\n *\tcapabilities are specified separately.\n * @coalesce: packet coalescing support information\n *\n * @vendor_commands: array of vendor commands supported by the hardware\n * @n_vendor_commands: number of vendor commands\n * @vendor_events: array of vendor events supported by the hardware\n * @n_vendor_events: number of vendor events\n *\n * @max_ap_assoc_sta: maximum number of associated stations supported in AP mode\n *\t(including P2P GO) or 0 to indicate no such limit is advertised. The\n *\tdriver is allowed to advertise a theoretical limit that it can reach in\n *\tsome cases, but may not always reach.\n *\n * @max_num_csa_counters: Number of supported csa_counters in beacons\n *\tand probe responses.  This value should be set if the driver\n *\twishes to limit the number of csa counters. Default (0) means\n *\tinfinite.\n * @bss_select_support: bitmask indicating the BSS selection criteria supported\n *\tby the driver in the .connect() callback. The bit position maps to the\n *\tattribute indices defined in &enum nl80211_bss_select_attr.\n *\n * @nan_supported_bands: bands supported by the device in NAN mode, a\n *\tbitmap of &enum nl80211_band values.  For instance, for\n *\tNL80211_BAND_2GHZ, bit 0 would be set\n *\t(i.e. BIT(NL80211_BAND_2GHZ)).\n *\n * @txq_limit: configuration of internal TX queue frame limit\n * @txq_memory_limit: configuration internal TX queue memory limit\n * @txq_quantum: configuration of internal TX queue scheduler quantum\n *\n * @tx_queue_len: allow setting transmit queue len for drivers not using\n *\twake_tx_queue\n *\n * @support_mbssid: can HW support association with nontransmitted AP\n * @support_only_he_mbssid: don't parse MBSSID elements if it is not\n *\tHE AP, in order to avoid compatibility issues.\n *\t@support_mbssid must be set for this to have any effect.\n *\n * @pmsr_capa: peer measurement capabilities\n *\n * @tid_config_support: describes the per-TID config support that the\n *\tdevice has\n * @tid_config_support.vif: bitmap of attributes (configurations)\n *\tsupported by the driver for each vif\n * @tid_config_support.peer: bitmap of attributes (configurations)\n *\tsupported by the driver for each peer\n * @tid_config_support.max_retry: maximum supported retry count for\n *\tlong/short retry configuration\n *\n * @max_data_retry_count: maximum supported per TID retry count for\n *\tconfiguration through the %NL80211_TID_CONFIG_ATTR_RETRY_SHORT and\n *\t%NL80211_TID_CONFIG_ATTR_RETRY_LONG attributes\n * @sar_capa: SAR control capabilities\n */\nstruct wiphy {\n\tstruct mutex mtx;\n\n\t/* assign these fields before you register the wiphy */\n\n\tu8 perm_addr[ETH_ALEN];\n\tu8 addr_mask[ETH_ALEN];\n\n\tstruct mac_address *addresses;\n\n\tconst struct ieee80211_txrx_stypes *mgmt_stypes;\n\n\tconst struct ieee80211_iface_combination *iface_combinations;\n\tint n_iface_combinations;\n\tu16 software_iftypes;\n\n\tu16 n_addresses;\n\n\t/* Supported interface modes, OR together BIT(NL80211_IFTYPE_...) */\n\tu16 interface_modes;\n\n\tu16 max_acl_mac_addrs;\n\n\tu32 flags, regulatory_flags, features;\n\tu8 ext_features[DIV_ROUND_UP(NUM_NL80211_EXT_FEATURES, 8)];\n\n\tu32 ap_sme_capa;\n\n\tenum cfg80211_signal_type signal_type;\n\n\tint bss_priv_size;\n\tu8 max_scan_ssids;\n\tu8 max_sched_scan_reqs;\n\tu8 max_sched_scan_ssids;\n\tu8 max_match_sets;\n\tu16 max_scan_ie_len;\n\tu16 max_sched_scan_ie_len;\n\tu32 max_sched_scan_plans;\n\tu32 max_sched_scan_plan_interval;\n\tu32 max_sched_scan_plan_iterations;\n\n\tint n_cipher_suites;\n\tconst u32 *cipher_suites;\n\n\tint n_akm_suites;\n\tconst u32 *akm_suites;\n\n\tconst struct wiphy_iftype_akm_suites *iftype_akm_suites;\n\tunsigned int num_iftype_akm_suites;\n\n\tu8 retry_short;\n\tu8 retry_long;\n\tu32 frag_threshold;\n\tu32 rts_threshold;\n\tu8 coverage_class;\n\n\tchar fw_version[ETHTOOL_FWVERS_LEN];\n\tu32 hw_version;\n\n#ifdef CONFIG_PM\n\tconst struct wiphy_wowlan_support *wowlan;\n\tstruct cfg80211_wowlan *wowlan_config;\n#endif\n\n\tu16 max_remain_on_channel_duration;\n\n\tu8 max_num_pmkids;\n\n\tu32 available_antennas_tx;\n\tu32 available_antennas_rx;\n\n\tu32 probe_resp_offload;\n\n\tconst u8 *extended_capabilities, *extended_capabilities_mask;\n\tu8 extended_capabilities_len;\n\n\tconst struct wiphy_iftype_ext_capab *iftype_ext_capab;\n\tunsigned int num_iftype_ext_capab;\n\n\tconst void *privid;\n\n\tstruct ieee80211_supported_band *bands[NUM_NL80211_BANDS];\n\n\tvoid (*reg_notifier)(struct wiphy *wiphy,\n\t\t\t     struct regulatory_request *request);\n\n\t/* fields below are read-only, assigned by cfg80211 */\n\n\tconst struct ieee80211_regdomain __rcu *regd;\n\n\tstruct device dev;\n\n\tbool registered;\n\n\tstruct dentry *debugfsdir;\n\n\tconst struct ieee80211_ht_cap *ht_capa_mod_mask;\n\tconst struct ieee80211_vht_cap *vht_capa_mod_mask;\n\n\tstruct list_head wdev_list;\n\n\tpossible_net_t _net;\n\n#ifdef CONFIG_CFG80211_WEXT\n\tconst struct iw_handler_def *wext;\n#endif\n\n\tconst struct wiphy_coalesce_support *coalesce;\n\n\tconst struct wiphy_vendor_command *vendor_commands;\n\tconst struct nl80211_vendor_cmd_info *vendor_events;\n\tint n_vendor_commands, n_vendor_events;\n\n\tu16 max_ap_assoc_sta;\n\n\tu8 max_num_csa_counters;\n\n\tu32 bss_select_support;\n\n\tu8 nan_supported_bands;\n\n\tu32 txq_limit;\n\tu32 txq_memory_limit;\n\tu32 txq_quantum;\n\n\tunsigned long tx_queue_len;\n\n\tu8 support_mbssid:1,\n\t   support_only_he_mbssid:1;\n\n\tconst struct cfg80211_pmsr_capabilities *pmsr_capa;\n\n\tstruct {\n\t\tu64 peer, vif;\n\t\tu8 max_retry;\n\t} tid_config_support;\n\n\tu8 max_data_retry_count;\n\n\tconst struct cfg80211_sar_capa *sar_capa;\n\n\tchar priv[] __aligned(NETDEV_ALIGN);\n};\n\nstatic inline struct net *wiphy_net(struct wiphy *wiphy)\n{\n\treturn read_pnet(&wiphy->_net);\n}\n\nstatic inline void wiphy_net_set(struct wiphy *wiphy, struct net *net)\n{\n\twrite_pnet(&wiphy->_net, net);\n}\n\n/**\n * wiphy_priv - return priv from wiphy\n *\n * @wiphy: the wiphy whose priv pointer to return\n * Return: The priv of @wiphy.\n */\nstatic inline void *wiphy_priv(struct wiphy *wiphy)\n{\n\tBUG_ON(!wiphy);\n\treturn &wiphy->priv;\n}\n\n/**\n * priv_to_wiphy - return the wiphy containing the priv\n *\n * @priv: a pointer previously returned by wiphy_priv\n * Return: The wiphy of @priv.\n */\nstatic inline struct wiphy *priv_to_wiphy(void *priv)\n{\n\tBUG_ON(!priv);\n\treturn container_of(priv, struct wiphy, priv);\n}\n\n/**\n * set_wiphy_dev - set device pointer for wiphy\n *\n * @wiphy: The wiphy whose device to bind\n * @dev: The device to parent it to\n */\nstatic inline void set_wiphy_dev(struct wiphy *wiphy, struct device *dev)\n{\n\twiphy->dev.parent = dev;\n}\n\n/**\n * wiphy_dev - get wiphy dev pointer\n *\n * @wiphy: The wiphy whose device struct to look up\n * Return: The dev of @wiphy.\n */\nstatic inline struct device *wiphy_dev(struct wiphy *wiphy)\n{\n\treturn wiphy->dev.parent;\n}\n\n/**\n * wiphy_name - get wiphy name\n *\n * @wiphy: The wiphy whose name to return\n * Return: The name of @wiphy.\n */\nstatic inline const char *wiphy_name(const struct wiphy *wiphy)\n{\n\treturn dev_name(&wiphy->dev);\n}\n\n/**\n * wiphy_new_nm - create a new wiphy for use with cfg80211\n *\n * @ops: The configuration operations for this device\n * @sizeof_priv: The size of the private area to allocate\n * @requested_name: Request a particular name.\n *\tNULL is valid value, and means use the default phy%d naming.\n *\n * Create a new wiphy and associate the given operations with it.\n * @sizeof_priv bytes are allocated for private use.\n *\n * Return: A pointer to the new wiphy. This pointer must be\n * assigned to each netdev's ieee80211_ptr for proper operation.\n */\nstruct wiphy *wiphy_new_nm(const struct cfg80211_ops *ops, int sizeof_priv,\n\t\t\t   const char *requested_name);\n\n/**\n * wiphy_new - create a new wiphy for use with cfg80211\n *\n * @ops: The configuration operations for this device\n * @sizeof_priv: The size of the private area to allocate\n *\n * Create a new wiphy and associate the given operations with it.\n * @sizeof_priv bytes are allocated for private use.\n *\n * Return: A pointer to the new wiphy. This pointer must be\n * assigned to each netdev's ieee80211_ptr for proper operation.\n */\nstatic inline struct wiphy *wiphy_new(const struct cfg80211_ops *ops,\n\t\t\t\t      int sizeof_priv)\n{\n\treturn wiphy_new_nm(ops, sizeof_priv, NULL);\n}\n\n/**\n * wiphy_register - register a wiphy with cfg80211\n *\n * @wiphy: The wiphy to register.\n *\n * Return: A non-negative wiphy index or a negative error code.\n */\nint wiphy_register(struct wiphy *wiphy);\n\n/* this is a define for better error reporting (file/line) */\n#define lockdep_assert_wiphy(wiphy) lockdep_assert_held(&(wiphy)->mtx)\n\n/**\n * rcu_dereference_wiphy - rcu_dereference with debug checking\n * @wiphy: the wiphy to check the locking on\n * @p: The pointer to read, prior to dereferencing\n *\n * Do an rcu_dereference(p), but check caller either holds rcu_read_lock()\n * or RTNL. Note: Please prefer wiphy_dereference() or rcu_dereference().\n */\n#define rcu_dereference_wiphy(wiphy, p)\t\t\t\t\\\n        rcu_dereference_check(p, lockdep_is_held(&wiphy->mtx))\n\n/**\n * wiphy_dereference - fetch RCU pointer when updates are prevented by wiphy mtx\n * @wiphy: the wiphy to check the locking on\n * @p: The pointer to read, prior to dereferencing\n *\n * Return the value of the specified RCU-protected pointer, but omit the\n * READ_ONCE(), because caller holds the wiphy mutex used for updates.\n */\n#define wiphy_dereference(wiphy, p)\t\t\t\t\\\n        rcu_dereference_protected(p, lockdep_is_held(&wiphy->mtx))\n\n/**\n * get_wiphy_regdom - get custom regdomain for the given wiphy\n * @wiphy: the wiphy to get the regdomain from\n */\nconst struct ieee80211_regdomain *get_wiphy_regdom(struct wiphy *wiphy);\n\n/**\n * wiphy_unregister - deregister a wiphy from cfg80211\n *\n * @wiphy: The wiphy to unregister.\n *\n * After this call, no more requests can be made with this priv\n * pointer, but the call may sleep to wait for an outstanding\n * request that is being handled.\n */\nvoid wiphy_unregister(struct wiphy *wiphy);\n\n/**\n * wiphy_free - free wiphy\n *\n * @wiphy: The wiphy to free\n */\nvoid wiphy_free(struct wiphy *wiphy);\n\n/* internal structs */\nstruct cfg80211_conn;\nstruct cfg80211_internal_bss;\nstruct cfg80211_cached_keys;\nstruct cfg80211_cqm_config;\n\n/**\n * wiphy_lock - lock the wiphy\n * @wiphy: the wiphy to lock\n *\n * This is mostly exposed so it can be done around registering and\n * unregistering netdevs that aren't created through cfg80211 calls,\n * since that requires locking in cfg80211 when the notifiers is\n * called, but that cannot differentiate which way it's called.\n *\n * When cfg80211 ops are called, the wiphy is already locked.\n */\nstatic inline void wiphy_lock(struct wiphy *wiphy)\n\t__acquires(&wiphy->mtx)\n{\n\tmutex_lock(&wiphy->mtx);\n\t__acquire(&wiphy->mtx);\n}\n\n/**\n * wiphy_unlock - unlock the wiphy again\n * @wiphy: the wiphy to unlock\n */\nstatic inline void wiphy_unlock(struct wiphy *wiphy)\n\t__releases(&wiphy->mtx)\n{\n\t__release(&wiphy->mtx);\n\tmutex_unlock(&wiphy->mtx);\n}\n\n/**\n * struct wireless_dev - wireless device state\n *\n * For netdevs, this structure must be allocated by the driver\n * that uses the ieee80211_ptr field in struct net_device (this\n * is intentional so it can be allocated along with the netdev.)\n * It need not be registered then as netdev registration will\n * be intercepted by cfg80211 to see the new wireless device,\n * however, drivers must lock the wiphy before registering or\n * unregistering netdevs if they pre-create any netdevs (in ops\n * called from cfg80211, the wiphy is already locked.)\n *\n * For non-netdev uses, it must also be allocated by the driver\n * in response to the cfg80211 callbacks that require it, as\n * there's no netdev registration in that case it may not be\n * allocated outside of callback operations that return it.\n *\n * @wiphy: pointer to hardware description\n * @iftype: interface type\n * @registered: is this wdev already registered with cfg80211\n * @registering: indicates we're doing registration under wiphy lock\n *\tfor the notifier\n * @list: (private) Used to collect the interfaces\n * @netdev: (private) Used to reference back to the netdev, may be %NULL\n * @identifier: (private) Identifier used in nl80211 to identify this\n *\twireless device if it has no netdev\n * @current_bss: (private) Used by the internal configuration code\n * @chandef: (private) Used by the internal configuration code to track\n *\tthe user-set channel definition.\n * @preset_chandef: (private) Used by the internal configuration code to\n *\ttrack the channel to be used for AP later\n * @bssid: (private) Used by the internal configuration code\n * @ssid: (private) Used by the internal configuration code\n * @ssid_len: (private) Used by the internal configuration code\n * @mesh_id_len: (private) Used by the internal configuration code\n * @mesh_id_up_len: (private) Used by the internal configuration code\n * @wext: (private) Used by the internal wireless extensions compat code\n * @wext.ibss: (private) IBSS data part of wext handling\n * @wext.connect: (private) connection handling data\n * @wext.keys: (private) (WEP) key data\n * @wext.ie: (private) extra elements for association\n * @wext.ie_len: (private) length of extra elements\n * @wext.bssid: (private) selected network BSSID\n * @wext.ssid: (private) selected network SSID\n * @wext.default_key: (private) selected default key index\n * @wext.default_mgmt_key: (private) selected default management key index\n * @wext.prev_bssid: (private) previous BSSID for reassociation\n * @wext.prev_bssid_valid: (private) previous BSSID validity\n * @use_4addr: indicates 4addr mode is used on this interface, must be\n *\tset by driver (if supported) on add_interface BEFORE registering the\n *\tnetdev and may otherwise be used by driver read-only, will be update\n *\tby cfg80211 on change_interface\n * @mgmt_registrations: list of registrations for management frames\n * @mgmt_registrations_lock: lock for the list\n * @mgmt_registrations_need_update: mgmt registrations were updated,\n *\tneed to propagate the update to the driver\n * @mtx: mutex used to lock data in this struct, may be used by drivers\n *\tand some API functions require it held\n * @beacon_interval: beacon interval used on this device for transmitting\n *\tbeacons, 0 when not valid\n * @address: The address for this device, valid only if @netdev is %NULL\n * @is_running: true if this is a non-netdev device that has been started, e.g.\n *\tthe P2P Device.\n * @cac_started: true if DFS channel availability check has been started\n * @cac_start_time: timestamp (jiffies) when the dfs state was entered.\n * @cac_time_ms: CAC time in ms\n * @ps: powersave mode is enabled\n * @ps_timeout: dynamic powersave timeout\n * @ap_unexpected_nlportid: (private) netlink port ID of application\n *\tregistered for unexpected class 3 frames (AP mode)\n * @conn: (private) cfg80211 software SME connection state machine data\n * @connect_keys: (private) keys to set after connection is established\n * @conn_bss_type: connecting/connected BSS type\n * @conn_owner_nlportid: (private) connection owner socket port ID\n * @disconnect_wk: (private) auto-disconnect work\n * @disconnect_bssid: (private) the BSSID to use for auto-disconnect\n * @ibss_fixed: (private) IBSS is using fixed BSSID\n * @ibss_dfs_possible: (private) IBSS may change to a DFS channel\n * @event_list: (private) list for internal event processing\n * @event_lock: (private) lock for event list\n * @owner_nlportid: (private) owner socket port ID\n * @nl_owner_dead: (private) owner socket went away\n * @cqm_config: (private) nl80211 RSSI monitor state\n * @pmsr_list: (private) peer measurement requests\n * @pmsr_lock: (private) peer measurements requests/results lock\n * @pmsr_free_wk: (private) peer measurements cleanup work\n * @unprot_beacon_reported: (private) timestamp of last\n *\tunprotected beacon report\n */\nstruct wireless_dev {\n\tstruct wiphy *wiphy;\n\tenum nl80211_iftype iftype;\n\n\t/* the remainder of this struct should be private to cfg80211 */\n\tstruct list_head list;\n\tstruct net_device *netdev;\n\n\tu32 identifier;\n\n\tstruct list_head mgmt_registrations;\n\tspinlock_t mgmt_registrations_lock;\n\tu8 mgmt_registrations_need_update:1;\n\n\tstruct mutex mtx;\n\n\tbool use_4addr, is_running, registered, registering;\n\n\tu8 address[ETH_ALEN] __aligned(sizeof(u16));\n\n\t/* currently used for IBSS and SME - might be rearranged later */\n\tu8 ssid[IEEE80211_MAX_SSID_LEN];\n\tu8 ssid_len, mesh_id_len, mesh_id_up_len;\n\tstruct cfg80211_conn *conn;\n\tstruct cfg80211_cached_keys *connect_keys;\n\tenum ieee80211_bss_type conn_bss_type;\n\tu32 conn_owner_nlportid;\n\n\tstruct work_struct disconnect_wk;\n\tu8 disconnect_bssid[ETH_ALEN];\n\n\tstruct list_head event_list;\n\tspinlock_t event_lock;\n\n\tstruct cfg80211_internal_bss *current_bss; /* associated / joined */\n\tstruct cfg80211_chan_def preset_chandef;\n\tstruct cfg80211_chan_def chandef;\n\n\tbool ibss_fixed;\n\tbool ibss_dfs_possible;\n\n\tbool ps;\n\tint ps_timeout;\n\n\tint beacon_interval;\n\n\tu32 ap_unexpected_nlportid;\n\n\tu32 owner_nlportid;\n\tbool nl_owner_dead;\n\n\tbool cac_started;\n\tunsigned long cac_start_time;\n\tunsigned int cac_time_ms;\n\n#ifdef CONFIG_CFG80211_WEXT\n\t/* wext data */\n\tstruct {\n\t\tstruct cfg80211_ibss_params ibss;\n\t\tstruct cfg80211_connect_params connect;\n\t\tstruct cfg80211_cached_keys *keys;\n\t\tconst u8 *ie;\n\t\tsize_t ie_len;\n\t\tu8 bssid[ETH_ALEN];\n\t\tu8 prev_bssid[ETH_ALEN];\n\t\tu8 ssid[IEEE80211_MAX_SSID_LEN];\n\t\ts8 default_key, default_mgmt_key;\n\t\tbool prev_bssid_valid;\n\t} wext;\n#endif\n\n\tstruct cfg80211_cqm_config *cqm_config;\n\n\tstruct list_head pmsr_list;\n\tspinlock_t pmsr_lock;\n\tstruct work_struct pmsr_free_wk;\n\n\tunsigned long unprot_beacon_reported;\n};\n\nstatic inline u8 *wdev_address(struct wireless_dev *wdev)\n{\n\tif (wdev->netdev)\n\t\treturn wdev->netdev->dev_addr;\n\treturn wdev->address;\n}\n\nstatic inline bool wdev_running(struct wireless_dev *wdev)\n{\n\tif (wdev->netdev)\n\t\treturn netif_running(wdev->netdev);\n\treturn wdev->is_running;\n}\n\n/**\n * wdev_priv - return wiphy priv from wireless_dev\n *\n * @wdev: The wireless device whose wiphy's priv pointer to return\n * Return: The wiphy priv of @wdev.\n */\nstatic inline void *wdev_priv(struct wireless_dev *wdev)\n{\n\tBUG_ON(!wdev);\n\treturn wiphy_priv(wdev->wiphy);\n}\n\n/**\n * DOC: Utility functions\n *\n * cfg80211 offers a number of utility functions that can be useful.\n */\n\n/**\n * ieee80211_channel_equal - compare two struct ieee80211_channel\n *\n * @a: 1st struct ieee80211_channel\n * @b: 2nd struct ieee80211_channel\n * Return: true if center frequency of @a == @b\n */\nstatic inline bool\nieee80211_channel_equal(struct ieee80211_channel *a,\n\t\t\tstruct ieee80211_channel *b)\n{\n\treturn (a->center_freq == b->center_freq &&\n\t\ta->freq_offset == b->freq_offset);\n}\n\n/**\n * ieee80211_channel_to_khz - convert ieee80211_channel to frequency in KHz\n * @chan: struct ieee80211_channel to convert\n * Return: The corresponding frequency (in KHz)\n */\nstatic inline u32\nieee80211_channel_to_khz(const struct ieee80211_channel *chan)\n{\n\treturn MHZ_TO_KHZ(chan->center_freq) + chan->freq_offset;\n}\n\n/**\n * ieee80211_s1g_channel_width - get allowed channel width from @chan\n *\n * Only allowed for band NL80211_BAND_S1GHZ\n * @chan: channel\n * Return: The allowed channel width for this center_freq\n */\nenum nl80211_chan_width\nieee80211_s1g_channel_width(const struct ieee80211_channel *chan);\n\n/**\n * ieee80211_channel_to_freq_khz - convert channel number to frequency\n * @chan: channel number\n * @band: band, necessary due to channel number overlap\n * Return: The corresponding frequency (in KHz), or 0 if the conversion failed.\n */\nu32 ieee80211_channel_to_freq_khz(int chan, enum nl80211_band band);\n\n/**\n * ieee80211_channel_to_frequency - convert channel number to frequency\n * @chan: channel number\n * @band: band, necessary due to channel number overlap\n * Return: The corresponding frequency (in MHz), or 0 if the conversion failed.\n */\nstatic inline int\nieee80211_channel_to_frequency(int chan, enum nl80211_band band)\n{\n\treturn KHZ_TO_MHZ(ieee80211_channel_to_freq_khz(chan, band));\n}\n\n/**\n * ieee80211_freq_khz_to_channel - convert frequency to channel number\n * @freq: center frequency in KHz\n * Return: The corresponding channel, or 0 if the conversion failed.\n */\nint ieee80211_freq_khz_to_channel(u32 freq);\n\n/**\n * ieee80211_frequency_to_channel - convert frequency to channel number\n * @freq: center frequency in MHz\n * Return: The corresponding channel, or 0 if the conversion failed.\n */\nstatic inline int\nieee80211_frequency_to_channel(int freq)\n{\n\treturn ieee80211_freq_khz_to_channel(MHZ_TO_KHZ(freq));\n}\n\n/**\n * ieee80211_get_channel_khz - get channel struct from wiphy for specified\n * frequency\n * @wiphy: the struct wiphy to get the channel for\n * @freq: the center frequency (in KHz) of the channel\n * Return: The channel struct from @wiphy at @freq.\n */\nstruct ieee80211_channel *\nieee80211_get_channel_khz(struct wiphy *wiphy, u32 freq);\n\n/**\n * ieee80211_get_channel - get channel struct from wiphy for specified frequency\n *\n * @wiphy: the struct wiphy to get the channel for\n * @freq: the center frequency (in MHz) of the channel\n * Return: The channel struct from @wiphy at @freq.\n */\nstatic inline struct ieee80211_channel *\nieee80211_get_channel(struct wiphy *wiphy, int freq)\n{\n\treturn ieee80211_get_channel_khz(wiphy, MHZ_TO_KHZ(freq));\n}\n\n/**\n * cfg80211_channel_is_psc - Check if the channel is a 6 GHz PSC\n * @chan: control channel to check\n *\n * The Preferred Scanning Channels (PSC) are defined in\n * Draft IEEE P802.11ax/D5.0, 26.17.2.3.3\n */\nstatic inline bool cfg80211_channel_is_psc(struct ieee80211_channel *chan)\n{\n\tif (chan->band != NL80211_BAND_6GHZ)\n\t\treturn false;\n\n\treturn ieee80211_frequency_to_channel(chan->center_freq) % 16 == 5;\n}\n\n/**\n * ieee80211_get_response_rate - get basic rate for a given rate\n *\n * @sband: the band to look for rates in\n * @basic_rates: bitmap of basic rates\n * @bitrate: the bitrate for which to find the basic rate\n *\n * Return: The basic rate corresponding to a given bitrate, that\n * is the next lower bitrate contained in the basic rate map,\n * which is, for this function, given as a bitmap of indices of\n * rates in the band's bitrate table.\n */\nstruct ieee80211_rate *\nieee80211_get_response_rate(struct ieee80211_supported_band *sband,\n\t\t\t    u32 basic_rates, int bitrate);\n\n/**\n * ieee80211_mandatory_rates - get mandatory rates for a given band\n * @sband: the band to look for rates in\n * @scan_width: width of the control channel\n *\n * This function returns a bitmap of the mandatory rates for the given\n * band, bits are set according to the rate position in the bitrates array.\n */\nu32 ieee80211_mandatory_rates(struct ieee80211_supported_band *sband,\n\t\t\t      enum nl80211_bss_scan_width scan_width);\n\n/*\n * Radiotap parsing functions -- for controlled injection support\n *\n * Implemented in net/wireless/radiotap.c\n * Documentation in Documentation/networking/radiotap-headers.rst\n */\n\nstruct radiotap_align_size {\n\tuint8_t align:4, size:4;\n};\n\nstruct ieee80211_radiotap_namespace {\n\tconst struct radiotap_align_size *align_size;\n\tint n_bits;\n\tuint32_t oui;\n\tuint8_t subns;\n};\n\nstruct ieee80211_radiotap_vendor_namespaces {\n\tconst struct ieee80211_radiotap_namespace *ns;\n\tint n_ns;\n};\n\n/**\n * struct ieee80211_radiotap_iterator - tracks walk thru present radiotap args\n * @this_arg_index: index of current arg, valid after each successful call\n *\tto ieee80211_radiotap_iterator_next()\n * @this_arg: pointer to current radiotap arg; it is valid after each\n *\tcall to ieee80211_radiotap_iterator_next() but also after\n *\tieee80211_radiotap_iterator_init() where it will point to\n *\tthe beginning of the actual data portion\n * @this_arg_size: length of the current arg, for convenience\n * @current_namespace: pointer to the current namespace definition\n *\t(or internally %NULL if the current namespace is unknown)\n * @is_radiotap_ns: indicates whether the current namespace is the default\n *\tradiotap namespace or not\n *\n * @_rtheader: pointer to the radiotap header we are walking through\n * @_max_length: length of radiotap header in cpu byte ordering\n * @_arg_index: next argument index\n * @_arg: next argument pointer\n * @_next_bitmap: internal pointer to next present u32\n * @_bitmap_shifter: internal shifter for curr u32 bitmap, b0 set == arg present\n * @_vns: vendor namespace definitions\n * @_next_ns_data: beginning of the next namespace's data\n * @_reset_on_ext: internal; reset the arg index to 0 when going to the\n *\tnext bitmap word\n *\n * Describes the radiotap parser state. Fields prefixed with an underscore\n * must not be used by users of the parser, only by the parser internally.\n */\n\nstruct ieee80211_radiotap_iterator {\n\tstruct ieee80211_radiotap_header *_rtheader;\n\tconst struct ieee80211_radiotap_vendor_namespaces *_vns;\n\tconst struct ieee80211_radiotap_namespace *current_namespace;\n\n\tunsigned char *_arg, *_next_ns_data;\n\t__le32 *_next_bitmap;\n\n\tunsigned char *this_arg;\n\tint this_arg_index;\n\tint this_arg_size;\n\n\tint is_radiotap_ns;\n\n\tint _max_length;\n\tint _arg_index;\n\tuint32_t _bitmap_shifter;\n\tint _reset_on_ext;\n};\n\nint\nieee80211_radiotap_iterator_init(struct ieee80211_radiotap_iterator *iterator,\n\t\t\t\t struct ieee80211_radiotap_header *radiotap_header,\n\t\t\t\t int max_length,\n\t\t\t\t const struct ieee80211_radiotap_vendor_namespaces *vns);\n\nint\nieee80211_radiotap_iterator_next(struct ieee80211_radiotap_iterator *iterator);\n\n\nextern const unsigned char rfc1042_header[6];\nextern const unsigned char bridge_tunnel_header[6];\n\n/**\n * ieee80211_get_hdrlen_from_skb - get header length from data\n *\n * @skb: the frame\n *\n * Given an skb with a raw 802.11 header at the data pointer this function\n * returns the 802.11 header length.\n *\n * Return: The 802.11 header length in bytes (not including encryption\n * headers). Or 0 if the data in the sk_buff is too short to contain a valid\n * 802.11 header.\n */\nunsigned int ieee80211_get_hdrlen_from_skb(const struct sk_buff *skb);\n\n/**\n * ieee80211_hdrlen - get header length in bytes from frame control\n * @fc: frame control field in little-endian format\n * Return: The header length in bytes.\n */\nunsigned int __attribute_const__ ieee80211_hdrlen(__le16 fc);\n\n/**\n * ieee80211_get_mesh_hdrlen - get mesh extension header length\n * @meshhdr: the mesh extension header, only the flags field\n *\t(first byte) will be accessed\n * Return: The length of the extension header, which is always at\n * least 6 bytes and at most 18 if address 5 and 6 are present.\n */\nunsigned int ieee80211_get_mesh_hdrlen(struct ieee80211s_hdr *meshhdr);\n\n/**\n * DOC: Data path helpers\n *\n * In addition to generic utilities, cfg80211 also offers\n * functions that help implement the data path for devices\n * that do not do the 802.11/802.3 conversion on the device.\n */\n\n/**\n * ieee80211_data_to_8023_exthdr - convert an 802.11 data frame to 802.3\n * @skb: the 802.11 data frame\n * @ehdr: pointer to a &struct ethhdr that will get the header, instead\n *\tof it being pushed into the SKB\n * @addr: the device MAC address\n * @iftype: the virtual interface type\n * @data_offset: offset of payload after the 802.11 header\n * Return: 0 on success. Non-zero on error.\n */\nint ieee80211_data_to_8023_exthdr(struct sk_buff *skb, struct ethhdr *ehdr,\n\t\t\t\t  const u8 *addr, enum nl80211_iftype iftype,\n\t\t\t\t  u8 data_offset);\n\n/**\n * ieee80211_data_to_8023 - convert an 802.11 data frame to 802.3\n * @skb: the 802.11 data frame\n * @addr: the device MAC address\n * @iftype: the virtual interface type\n * Return: 0 on success. Non-zero on error.\n */\nstatic inline int ieee80211_data_to_8023(struct sk_buff *skb, const u8 *addr,\n\t\t\t\t\t enum nl80211_iftype iftype)\n{\n\treturn ieee80211_data_to_8023_exthdr(skb, NULL, addr, iftype, 0);\n}\n\n/**\n * ieee80211_amsdu_to_8023s - decode an IEEE 802.11n A-MSDU frame\n *\n * Decode an IEEE 802.11 A-MSDU and convert it to a list of 802.3 frames.\n * The @list will be empty if the decode fails. The @skb must be fully\n * header-less before being passed in here; it is freed in this function.\n *\n * @skb: The input A-MSDU frame without any headers.\n * @list: The output list of 802.3 frames. It must be allocated and\n *\tinitialized by the caller.\n * @addr: The device MAC address.\n * @iftype: The device interface type.\n * @extra_headroom: The hardware extra headroom for SKBs in the @list.\n * @check_da: DA to check in the inner ethernet header, or NULL\n * @check_sa: SA to check in the inner ethernet header, or NULL\n */\nvoid ieee80211_amsdu_to_8023s(struct sk_buff *skb, struct sk_buff_head *list,\n\t\t\t      const u8 *addr, enum nl80211_iftype iftype,\n\t\t\t      const unsigned int extra_headroom,\n\t\t\t      const u8 *check_da, const u8 *check_sa);\n\n/**\n * cfg80211_classify8021d - determine the 802.1p/1d tag for a data frame\n * @skb: the data frame\n * @qos_map: Interworking QoS mapping or %NULL if not in use\n * Return: The 802.1p/1d tag.\n */\nunsigned int cfg80211_classify8021d(struct sk_buff *skb,\n\t\t\t\t    struct cfg80211_qos_map *qos_map);\n\n/**\n * cfg80211_find_elem_match - match information element and byte array in data\n *\n * @eid: element ID\n * @ies: data consisting of IEs\n * @len: length of data\n * @match: byte array to match\n * @match_len: number of bytes in the match array\n * @match_offset: offset in the IE data where the byte array should match.\n *\tNote the difference to cfg80211_find_ie_match() which considers\n *\tthe offset to start from the element ID byte, but here we take\n *\tthe data portion instead.\n *\n * Return: %NULL if the element ID could not be found or if\n * the element is invalid (claims to be longer than the given\n * data) or if the byte array doesn't match; otherwise return the\n * requested element struct.\n *\n * Note: There are no checks on the element length other than\n * having to fit into the given data and being large enough for the\n * byte array to match.\n */\nconst struct element *\ncfg80211_find_elem_match(u8 eid, const u8 *ies, unsigned int len,\n\t\t\t const u8 *match, unsigned int match_len,\n\t\t\t unsigned int match_offset);\n\n/**\n * cfg80211_find_ie_match - match information element and byte array in data\n *\n * @eid: element ID\n * @ies: data consisting of IEs\n * @len: length of data\n * @match: byte array to match\n * @match_len: number of bytes in the match array\n * @match_offset: offset in the IE where the byte array should match.\n *\tIf match_len is zero, this must also be set to zero.\n *\tOtherwise this must be set to 2 or more, because the first\n *\tbyte is the element id, which is already compared to eid, and\n *\tthe second byte is the IE length.\n *\n * Return: %NULL if the element ID could not be found or if\n * the element is invalid (claims to be longer than the given\n * data) or if the byte array doesn't match, or a pointer to the first\n * byte of the requested element, that is the byte containing the\n * element ID.\n *\n * Note: There are no checks on the element length other than\n * having to fit into the given data and being large enough for the\n * byte array to match.\n */\nstatic inline const u8 *\ncfg80211_find_ie_match(u8 eid, const u8 *ies, unsigned int len,\n\t\t       const u8 *match, unsigned int match_len,\n\t\t       unsigned int match_offset)\n{\n\t/* match_offset can't be smaller than 2, unless match_len is\n\t * zero, in which case match_offset must be zero as well.\n\t */\n\tif (WARN_ON((match_len && match_offset < 2) ||\n\t\t    (!match_len && match_offset)))\n\t\treturn NULL;\n\n\treturn (void *)cfg80211_find_elem_match(eid, ies, len,\n\t\t\t\t\t\tmatch, match_len,\n\t\t\t\t\t\tmatch_offset ?\n\t\t\t\t\t\t\tmatch_offset - 2 : 0);\n}\n\n/**\n * cfg80211_find_elem - find information element in data\n *\n * @eid: element ID\n * @ies: data consisting of IEs\n * @len: length of data\n *\n * Return: %NULL if the element ID could not be found or if\n * the element is invalid (claims to be longer than the given\n * data) or if the byte array doesn't match; otherwise return the\n * requested element struct.\n *\n * Note: There are no checks on the element length other than\n * having to fit into the given data.\n */\nstatic inline const struct element *\ncfg80211_find_elem(u8 eid, const u8 *ies, int len)\n{\n\treturn cfg80211_find_elem_match(eid, ies, len, NULL, 0, 0);\n}\n\n/**\n * cfg80211_find_ie - find information element in data\n *\n * @eid: element ID\n * @ies: data consisting of IEs\n * @len: length of data\n *\n * Return: %NULL if the element ID could not be found or if\n * the element is invalid (claims to be longer than the given\n * data), or a pointer to the first byte of the requested\n * element, that is the byte containing the element ID.\n *\n * Note: There are no checks on the element length other than\n * having to fit into the given data.\n */\nstatic inline const u8 *cfg80211_find_ie(u8 eid, const u8 *ies, int len)\n{\n\treturn cfg80211_find_ie_match(eid, ies, len, NULL, 0, 0);\n}\n\n/**\n * cfg80211_find_ext_elem - find information element with EID Extension in data\n *\n * @ext_eid: element ID Extension\n * @ies: data consisting of IEs\n * @len: length of data\n *\n * Return: %NULL if the etended element could not be found or if\n * the element is invalid (claims to be longer than the given\n * data) or if the byte array doesn't match; otherwise return the\n * requested element struct.\n *\n * Note: There are no checks on the element length other than\n * having to fit into the given data.\n */\nstatic inline const struct element *\ncfg80211_find_ext_elem(u8 ext_eid, const u8 *ies, int len)\n{\n\treturn cfg80211_find_elem_match(WLAN_EID_EXTENSION, ies, len,\n\t\t\t\t\t&ext_eid, 1, 0);\n}\n\n/**\n * cfg80211_find_ext_ie - find information element with EID Extension in data\n *\n * @ext_eid: element ID Extension\n * @ies: data consisting of IEs\n * @len: length of data\n *\n * Return: %NULL if the extended element ID could not be found or if\n * the element is invalid (claims to be longer than the given\n * data), or a pointer to the first byte of the requested\n * element, that is the byte containing the element ID.\n *\n * Note: There are no checks on the element length other than\n * having to fit into the given data.\n */\nstatic inline const u8 *cfg80211_find_ext_ie(u8 ext_eid, const u8 *ies, int len)\n{\n\treturn cfg80211_find_ie_match(WLAN_EID_EXTENSION, ies, len,\n\t\t\t\t      &ext_eid, 1, 2);\n}\n\n/**\n * cfg80211_find_vendor_elem - find vendor specific information element in data\n *\n * @oui: vendor OUI\n * @oui_type: vendor-specific OUI type (must be < 0xff), negative means any\n * @ies: data consisting of IEs\n * @len: length of data\n *\n * Return: %NULL if the vendor specific element ID could not be found or if the\n * element is invalid (claims to be longer than the given data); otherwise\n * return the element structure for the requested element.\n *\n * Note: There are no checks on the element length other than having to fit into\n * the given data.\n */\nconst struct element *cfg80211_find_vendor_elem(unsigned int oui, int oui_type,\n\t\t\t\t\t\tconst u8 *ies,\n\t\t\t\t\t\tunsigned int len);\n\n/**\n * cfg80211_find_vendor_ie - find vendor specific information element in data\n *\n * @oui: vendor OUI\n * @oui_type: vendor-specific OUI type (must be < 0xff), negative means any\n * @ies: data consisting of IEs\n * @len: length of data\n *\n * Return: %NULL if the vendor specific element ID could not be found or if the\n * element is invalid (claims to be longer than the given data), or a pointer to\n * the first byte of the requested element, that is the byte containing the\n * element ID.\n *\n * Note: There are no checks on the element length other than having to fit into\n * the given data.\n */\nstatic inline const u8 *\ncfg80211_find_vendor_ie(unsigned int oui, int oui_type,\n\t\t\tconst u8 *ies, unsigned int len)\n{\n\treturn (void *)cfg80211_find_vendor_elem(oui, oui_type, ies, len);\n}\n\n/**\n * cfg80211_send_layer2_update - send layer 2 update frame\n *\n * @dev: network device\n * @addr: STA MAC address\n *\n * Wireless drivers can use this function to update forwarding tables in bridge\n * devices upon STA association.\n */\nvoid cfg80211_send_layer2_update(struct net_device *dev, const u8 *addr);\n\n/**\n * DOC: Regulatory enforcement infrastructure\n *\n * TODO\n */\n\n/**\n * regulatory_hint - driver hint to the wireless core a regulatory domain\n * @wiphy: the wireless device giving the hint (used only for reporting\n *\tconflicts)\n * @alpha2: the ISO/IEC 3166 alpha2 the driver claims its regulatory domain\n *\tshould be in. If @rd is set this should be NULL. Note that if you\n *\tset this to NULL you should still set rd->alpha2 to some accepted\n *\talpha2.\n *\n * Wireless drivers can use this function to hint to the wireless core\n * what it believes should be the current regulatory domain by\n * giving it an ISO/IEC 3166 alpha2 country code it knows its regulatory\n * domain should be in or by providing a completely build regulatory domain.\n * If the driver provides an ISO/IEC 3166 alpha2 userspace will be queried\n * for a regulatory domain structure for the respective country.\n *\n * The wiphy must have been registered to cfg80211 prior to this call.\n * For cfg80211 drivers this means you must first use wiphy_register(),\n * for mac80211 drivers you must first use ieee80211_register_hw().\n *\n * Drivers should check the return value, its possible you can get\n * an -ENOMEM.\n *\n * Return: 0 on success. -ENOMEM.\n */\nint regulatory_hint(struct wiphy *wiphy, const char *alpha2);\n\n/**\n * regulatory_set_wiphy_regd - set regdom info for self managed drivers\n * @wiphy: the wireless device we want to process the regulatory domain on\n * @rd: the regulatory domain informatoin to use for this wiphy\n *\n * Set the regulatory domain information for self-managed wiphys, only they\n * may use this function. See %REGULATORY_WIPHY_SELF_MANAGED for more\n * information.\n *\n * Return: 0 on success. -EINVAL, -EPERM\n */\nint regulatory_set_wiphy_regd(struct wiphy *wiphy,\n\t\t\t      struct ieee80211_regdomain *rd);\n\n/**\n * regulatory_set_wiphy_regd_sync - set regdom for self-managed drivers\n * @wiphy: the wireless device we want to process the regulatory domain on\n * @rd: the regulatory domain information to use for this wiphy\n *\n * This functions requires the RTNL and the wiphy mutex to be held and\n * applies the new regdomain synchronously to this wiphy. For more details\n * see regulatory_set_wiphy_regd().\n *\n * Return: 0 on success. -EINVAL, -EPERM\n */\nint regulatory_set_wiphy_regd_sync(struct wiphy *wiphy,\n\t\t\t\t   struct ieee80211_regdomain *rd);\n\n/**\n * wiphy_apply_custom_regulatory - apply a custom driver regulatory domain\n * @wiphy: the wireless device we want to process the regulatory domain on\n * @regd: the custom regulatory domain to use for this wiphy\n *\n * Drivers can sometimes have custom regulatory domains which do not apply\n * to a specific country. Drivers can use this to apply such custom regulatory\n * domains. This routine must be called prior to wiphy registration. The\n * custom regulatory domain will be trusted completely and as such previous\n * default channel settings will be disregarded. If no rule is found for a\n * channel on the regulatory domain the channel will be disabled.\n * Drivers using this for a wiphy should also set the wiphy flag\n * REGULATORY_CUSTOM_REG or cfg80211 will set it for the wiphy\n * that called this helper.\n */\nvoid wiphy_apply_custom_regulatory(struct wiphy *wiphy,\n\t\t\t\t   const struct ieee80211_regdomain *regd);\n\n/**\n * freq_reg_info - get regulatory information for the given frequency\n * @wiphy: the wiphy for which we want to process this rule for\n * @center_freq: Frequency in KHz for which we want regulatory information for\n *\n * Use this function to get the regulatory rule for a specific frequency on\n * a given wireless device. If the device has a specific regulatory domain\n * it wants to follow we respect that unless a country IE has been received\n * and processed already.\n *\n * Return: A valid pointer, or, when an error occurs, for example if no rule\n * can be found, the return value is encoded using ERR_PTR(). Use IS_ERR() to\n * check and PTR_ERR() to obtain the numeric return value. The numeric return\n * value will be -ERANGE if we determine the given center_freq does not even\n * have a regulatory rule for a frequency range in the center_freq's band.\n * See freq_in_rule_band() for our current definition of a band -- this is\n * purely subjective and right now it's 802.11 specific.\n */\nconst struct ieee80211_reg_rule *freq_reg_info(struct wiphy *wiphy,\n\t\t\t\t\t       u32 center_freq);\n\n/**\n * reg_initiator_name - map regulatory request initiator enum to name\n * @initiator: the regulatory request initiator\n *\n * You can use this to map the regulatory request initiator enum to a\n * proper string representation.\n */\nconst char *reg_initiator_name(enum nl80211_reg_initiator initiator);\n\n/**\n * regulatory_pre_cac_allowed - check if pre-CAC allowed in the current regdom\n * @wiphy: wiphy for which pre-CAC capability is checked.\n *\n * Pre-CAC is allowed only in some regdomains (notable ETSI).\n */\nbool regulatory_pre_cac_allowed(struct wiphy *wiphy);\n\n/**\n * DOC: Internal regulatory db functions\n *\n */\n\n/**\n * reg_query_regdb_wmm -  Query internal regulatory db for wmm rule\n * Regulatory self-managed driver can use it to proactively\n *\n * @alpha2: the ISO/IEC 3166 alpha2 wmm rule to be queried.\n * @freq: the freqency(in MHz) to be queried.\n * @rule: pointer to store the wmm rule from the regulatory db.\n *\n * Self-managed wireless drivers can use this function to  query\n * the internal regulatory database to check whether the given\n * ISO/IEC 3166 alpha2 country and freq have wmm rule limitations.\n *\n * Drivers should check the return value, its possible you can get\n * an -ENODATA.\n *\n * Return: 0 on success. -ENODATA.\n */\nint reg_query_regdb_wmm(char *alpha2, int freq,\n\t\t\tstruct ieee80211_reg_rule *rule);\n\n/*\n * callbacks for asynchronous cfg80211 methods, notification\n * functions and BSS handling helpers\n */\n\n/**\n * cfg80211_scan_done - notify that scan finished\n *\n * @request: the corresponding scan request\n * @info: information about the completed scan\n */\nvoid cfg80211_scan_done(struct cfg80211_scan_request *request,\n\t\t\tstruct cfg80211_scan_info *info);\n\n/**\n * cfg80211_sched_scan_results - notify that new scan results are available\n *\n * @wiphy: the wiphy which got scheduled scan results\n * @reqid: identifier for the related scheduled scan request\n */\nvoid cfg80211_sched_scan_results(struct wiphy *wiphy, u64 reqid);\n\n/**\n * cfg80211_sched_scan_stopped - notify that the scheduled scan has stopped\n *\n * @wiphy: the wiphy on which the scheduled scan stopped\n * @reqid: identifier for the related scheduled scan request\n *\n * The driver can call this function to inform cfg80211 that the\n * scheduled scan had to be stopped, for whatever reason.  The driver\n * is then called back via the sched_scan_stop operation when done.\n */\nvoid cfg80211_sched_scan_stopped(struct wiphy *wiphy, u64 reqid);\n\n/**\n * cfg80211_sched_scan_stopped_locked - notify that the scheduled scan has stopped\n *\n * @wiphy: the wiphy on which the scheduled scan stopped\n * @reqid: identifier for the related scheduled scan request\n *\n * The driver can call this function to inform cfg80211 that the\n * scheduled scan had to be stopped, for whatever reason.  The driver\n * is then called back via the sched_scan_stop operation when done.\n * This function should be called with the wiphy mutex held.\n */\nvoid cfg80211_sched_scan_stopped_locked(struct wiphy *wiphy, u64 reqid);\n\n/**\n * cfg80211_inform_bss_frame_data - inform cfg80211 of a received BSS frame\n * @wiphy: the wiphy reporting the BSS\n * @data: the BSS metadata\n * @mgmt: the management frame (probe response or beacon)\n * @len: length of the management frame\n * @gfp: context flags\n *\n * This informs cfg80211 that BSS information was found and\n * the BSS should be updated/added.\n *\n * Return: A referenced struct, must be released with cfg80211_put_bss()!\n * Or %NULL on error.\n */\nstruct cfg80211_bss * __must_check\ncfg80211_inform_bss_frame_data(struct wiphy *wiphy,\n\t\t\t       struct cfg80211_inform_bss *data,\n\t\t\t       struct ieee80211_mgmt *mgmt, size_t len,\n\t\t\t       gfp_t gfp);\n\nstatic inline struct cfg80211_bss * __must_check\ncfg80211_inform_bss_width_frame(struct wiphy *wiphy,\n\t\t\t\tstruct ieee80211_channel *rx_channel,\n\t\t\t\tenum nl80211_bss_scan_width scan_width,\n\t\t\t\tstruct ieee80211_mgmt *mgmt, size_t len,\n\t\t\t\ts32 signal, gfp_t gfp)\n{\n\tstruct cfg80211_inform_bss data = {\n\t\t.chan = rx_channel,\n\t\t.scan_width = scan_width,\n\t\t.signal = signal,\n\t};\n\n\treturn cfg80211_inform_bss_frame_data(wiphy, &data, mgmt, len, gfp);\n}\n\nstatic inline struct cfg80211_bss * __must_check\ncfg80211_inform_bss_frame(struct wiphy *wiphy,\n\t\t\t  struct ieee80211_channel *rx_channel,\n\t\t\t  struct ieee80211_mgmt *mgmt, size_t len,\n\t\t\t  s32 signal, gfp_t gfp)\n{\n\tstruct cfg80211_inform_bss data = {\n\t\t.chan = rx_channel,\n\t\t.scan_width = NL80211_BSS_CHAN_WIDTH_20,\n\t\t.signal = signal,\n\t};\n\n\treturn cfg80211_inform_bss_frame_data(wiphy, &data, mgmt, len, gfp);\n}\n\n/**\n * cfg80211_gen_new_bssid - generate a nontransmitted BSSID for multi-BSSID\n * @bssid: transmitter BSSID\n * @max_bssid: max BSSID indicator, taken from Multiple BSSID element\n * @mbssid_index: BSSID index, taken from Multiple BSSID index element\n * @new_bssid: calculated nontransmitted BSSID\n */\nstatic inline void cfg80211_gen_new_bssid(const u8 *bssid, u8 max_bssid,\n\t\t\t\t\t  u8 mbssid_index, u8 *new_bssid)\n{\n\tu64 bssid_u64 = ether_addr_to_u64(bssid);\n\tu64 mask = GENMASK_ULL(max_bssid - 1, 0);\n\tu64 new_bssid_u64;\n\n\tnew_bssid_u64 = bssid_u64 & ~mask;\n\n\tnew_bssid_u64 |= ((bssid_u64 & mask) + mbssid_index) & mask;\n\n\tu64_to_ether_addr(new_bssid_u64, new_bssid);\n}\n\n/**\n * cfg80211_is_element_inherited - returns if element ID should be inherited\n * @element: element to check\n * @non_inherit_element: non inheritance element\n */\nbool cfg80211_is_element_inherited(const struct element *element,\n\t\t\t\t   const struct element *non_inherit_element);\n\n/**\n * cfg80211_merge_profile - merges a MBSSID profile if it is split between IEs\n * @ie: ies\n * @ielen: length of IEs\n * @mbssid_elem: current MBSSID element\n * @sub_elem: current MBSSID subelement (profile)\n * @merged_ie: location of the merged profile\n * @max_copy_len: max merged profile length\n */\nsize_t cfg80211_merge_profile(const u8 *ie, size_t ielen,\n\t\t\t      const struct element *mbssid_elem,\n\t\t\t      const struct element *sub_elem,\n\t\t\t      u8 *merged_ie, size_t max_copy_len);\n\n/**\n * enum cfg80211_bss_frame_type - frame type that the BSS data came from\n * @CFG80211_BSS_FTYPE_UNKNOWN: driver doesn't know whether the data is\n *\tfrom a beacon or probe response\n * @CFG80211_BSS_FTYPE_BEACON: data comes from a beacon\n * @CFG80211_BSS_FTYPE_PRESP: data comes from a probe response\n */\nenum cfg80211_bss_frame_type {\n\tCFG80211_BSS_FTYPE_UNKNOWN,\n\tCFG80211_BSS_FTYPE_BEACON,\n\tCFG80211_BSS_FTYPE_PRESP,\n};\n\n/**\n * cfg80211_inform_bss_data - inform cfg80211 of a new BSS\n *\n * @wiphy: the wiphy reporting the BSS\n * @data: the BSS metadata\n * @ftype: frame type (if known)\n * @bssid: the BSSID of the BSS\n * @tsf: the TSF sent by the peer in the beacon/probe response (or 0)\n * @capability: the capability field sent by the peer\n * @beacon_interval: the beacon interval announced by the peer\n * @ie: additional IEs sent by the peer\n * @ielen: length of the additional IEs\n * @gfp: context flags\n *\n * This informs cfg80211 that BSS information was found and\n * the BSS should be updated/added.\n *\n * Return: A referenced struct, must be released with cfg80211_put_bss()!\n * Or %NULL on error.\n */\nstruct cfg80211_bss * __must_check\ncfg80211_inform_bss_data(struct wiphy *wiphy,\n\t\t\t struct cfg80211_inform_bss *data,\n\t\t\t enum cfg80211_bss_frame_type ftype,\n\t\t\t const u8 *bssid, u64 tsf, u16 capability,\n\t\t\t u16 beacon_interval, const u8 *ie, size_t ielen,\n\t\t\t gfp_t gfp);\n\nstatic inline struct cfg80211_bss * __must_check\ncfg80211_inform_bss_width(struct wiphy *wiphy,\n\t\t\t  struct ieee80211_channel *rx_channel,\n\t\t\t  enum nl80211_bss_scan_width scan_width,\n\t\t\t  enum cfg80211_bss_frame_type ftype,\n\t\t\t  const u8 *bssid, u64 tsf, u16 capability,\n\t\t\t  u16 beacon_interval, const u8 *ie, size_t ielen,\n\t\t\t  s32 signal, gfp_t gfp)\n{\n\tstruct cfg80211_inform_bss data = {\n\t\t.chan = rx_channel,\n\t\t.scan_width = scan_width,\n\t\t.signal = signal,\n\t};\n\n\treturn cfg80211_inform_bss_data(wiphy, &data, ftype, bssid, tsf,\n\t\t\t\t\tcapability, beacon_interval, ie, ielen,\n\t\t\t\t\tgfp);\n}\n\nstatic inline struct cfg80211_bss * __must_check\ncfg80211_inform_bss(struct wiphy *wiphy,\n\t\t    struct ieee80211_channel *rx_channel,\n\t\t    enum cfg80211_bss_frame_type ftype,\n\t\t    const u8 *bssid, u64 tsf, u16 capability,\n\t\t    u16 beacon_interval, const u8 *ie, size_t ielen,\n\t\t    s32 signal, gfp_t gfp)\n{\n\tstruct cfg80211_inform_bss data = {\n\t\t.chan = rx_channel,\n\t\t.scan_width = NL80211_BSS_CHAN_WIDTH_20,\n\t\t.signal = signal,\n\t};\n\n\treturn cfg80211_inform_bss_data(wiphy, &data, ftype, bssid, tsf,\n\t\t\t\t\tcapability, beacon_interval, ie, ielen,\n\t\t\t\t\tgfp);\n}\n\n/**\n * cfg80211_get_bss - get a BSS reference\n * @wiphy: the wiphy this BSS struct belongs to\n * @channel: the channel to search on (or %NULL)\n * @bssid: the desired BSSID (or %NULL)\n * @ssid: the desired SSID (or %NULL)\n * @ssid_len: length of the SSID (or 0)\n * @bss_type: type of BSS, see &enum ieee80211_bss_type\n * @privacy: privacy filter, see &enum ieee80211_privacy\n */\nstruct cfg80211_bss *cfg80211_get_bss(struct wiphy *wiphy,\n\t\t\t\t      struct ieee80211_channel *channel,\n\t\t\t\t      const u8 *bssid,\n\t\t\t\t      const u8 *ssid, size_t ssid_len,\n\t\t\t\t      enum ieee80211_bss_type bss_type,\n\t\t\t\t      enum ieee80211_privacy privacy);\nstatic inline struct cfg80211_bss *\ncfg80211_get_ibss(struct wiphy *wiphy,\n\t\t  struct ieee80211_channel *channel,\n\t\t  const u8 *ssid, size_t ssid_len)\n{\n\treturn cfg80211_get_bss(wiphy, channel, NULL, ssid, ssid_len,\n\t\t\t\tIEEE80211_BSS_TYPE_IBSS,\n\t\t\t\tIEEE80211_PRIVACY_ANY);\n}\n\n/**\n * cfg80211_ref_bss - reference BSS struct\n * @wiphy: the wiphy this BSS struct belongs to\n * @bss: the BSS struct to reference\n *\n * Increments the refcount of the given BSS struct.\n */\nvoid cfg80211_ref_bss(struct wiphy *wiphy, struct cfg80211_bss *bss);\n\n/**\n * cfg80211_put_bss - unref BSS struct\n * @wiphy: the wiphy this BSS struct belongs to\n * @bss: the BSS struct\n *\n * Decrements the refcount of the given BSS struct.\n */\nvoid cfg80211_put_bss(struct wiphy *wiphy, struct cfg80211_bss *bss);\n\n/**\n * cfg80211_unlink_bss - unlink BSS from internal data structures\n * @wiphy: the wiphy\n * @bss: the bss to remove\n *\n * This function removes the given BSS from the internal data structures\n * thereby making it no longer show up in scan results etc. Use this\n * function when you detect a BSS is gone. Normally BSSes will also time\n * out, so it is not necessary to use this function at all.\n */\nvoid cfg80211_unlink_bss(struct wiphy *wiphy, struct cfg80211_bss *bss);\n\n/**\n * cfg80211_bss_iter - iterate all BSS entries\n *\n * This function iterates over the BSS entries associated with the given wiphy\n * and calls the callback for the iterated BSS. The iterator function is not\n * allowed to call functions that might modify the internal state of the BSS DB.\n *\n * @wiphy: the wiphy\n * @chandef: if given, the iterator function will be called only if the channel\n *     of the currently iterated BSS is a subset of the given channel.\n * @iter: the iterator function to call\n * @iter_data: an argument to the iterator function\n */\nvoid cfg80211_bss_iter(struct wiphy *wiphy,\n\t\t       struct cfg80211_chan_def *chandef,\n\t\t       void (*iter)(struct wiphy *wiphy,\n\t\t\t\t    struct cfg80211_bss *bss,\n\t\t\t\t    void *data),\n\t\t       void *iter_data);\n\nstatic inline enum nl80211_bss_scan_width\ncfg80211_chandef_to_scan_width(const struct cfg80211_chan_def *chandef)\n{\n\tswitch (chandef->width) {\n\tcase NL80211_CHAN_WIDTH_5:\n\t\treturn NL80211_BSS_CHAN_WIDTH_5;\n\tcase NL80211_CHAN_WIDTH_10:\n\t\treturn NL80211_BSS_CHAN_WIDTH_10;\n\tdefault:\n\t\treturn NL80211_BSS_CHAN_WIDTH_20;\n\t}\n}\n\n/**\n * cfg80211_rx_mlme_mgmt - notification of processed MLME management frame\n * @dev: network device\n * @buf: authentication frame (header + body)\n * @len: length of the frame data\n *\n * This function is called whenever an authentication, disassociation or\n * deauthentication frame has been received and processed in station mode.\n * After being asked to authenticate via cfg80211_ops::auth() the driver must\n * call either this function or cfg80211_auth_timeout().\n * After being asked to associate via cfg80211_ops::assoc() the driver must\n * call either this function or cfg80211_auth_timeout().\n * While connected, the driver must calls this for received and processed\n * disassociation and deauthentication frames. If the frame couldn't be used\n * because it was unprotected, the driver must call the function\n * cfg80211_rx_unprot_mlme_mgmt() instead.\n *\n * This function may sleep. The caller must hold the corresponding wdev's mutex.\n */\nvoid cfg80211_rx_mlme_mgmt(struct net_device *dev, const u8 *buf, size_t len);\n\n/**\n * cfg80211_auth_timeout - notification of timed out authentication\n * @dev: network device\n * @addr: The MAC address of the device with which the authentication timed out\n *\n * This function may sleep. The caller must hold the corresponding wdev's\n * mutex.\n */\nvoid cfg80211_auth_timeout(struct net_device *dev, const u8 *addr);\n\n/**\n * cfg80211_rx_assoc_resp - notification of processed association response\n * @dev: network device\n * @bss: the BSS that association was requested with, ownership of the pointer\n *\tmoves to cfg80211 in this call\n * @buf: (Re)Association Response frame (header + body)\n * @len: length of the frame data\n * @uapsd_queues: bitmap of queues configured for uapsd. Same format\n *\tas the AC bitmap in the QoS info field\n * @req_ies: information elements from the (Re)Association Request frame\n * @req_ies_len: length of req_ies data\n *\n * After being asked to associate via cfg80211_ops::assoc() the driver must\n * call either this function or cfg80211_auth_timeout().\n *\n * This function may sleep. The caller must hold the corresponding wdev's mutex.\n */\nvoid cfg80211_rx_assoc_resp(struct net_device *dev,\n\t\t\t    struct cfg80211_bss *bss,\n\t\t\t    const u8 *buf, size_t len,\n\t\t\t    int uapsd_queues,\n\t\t\t    const u8 *req_ies, size_t req_ies_len);\n\n/**\n * cfg80211_assoc_timeout - notification of timed out association\n * @dev: network device\n * @bss: The BSS entry with which association timed out.\n *\n * This function may sleep. The caller must hold the corresponding wdev's mutex.\n */\nvoid cfg80211_assoc_timeout(struct net_device *dev, struct cfg80211_bss *bss);\n\n/**\n * cfg80211_abandon_assoc - notify cfg80211 of abandoned association attempt\n * @dev: network device\n * @bss: The BSS entry with which association was abandoned.\n *\n * Call this whenever - for reasons reported through other API, like deauth RX,\n * an association attempt was abandoned.\n * This function may sleep. The caller must hold the corresponding wdev's mutex.\n */\nvoid cfg80211_abandon_assoc(struct net_device *dev, struct cfg80211_bss *bss);\n\n/**\n * cfg80211_tx_mlme_mgmt - notification of transmitted deauth/disassoc frame\n * @dev: network device\n * @buf: 802.11 frame (header + body)\n * @len: length of the frame data\n * @reconnect: immediate reconnect is desired (include the nl80211 attribute)\n *\n * This function is called whenever deauthentication has been processed in\n * station mode. This includes both received deauthentication frames and\n * locally generated ones. This function may sleep. The caller must hold the\n * corresponding wdev's mutex.\n */\nvoid cfg80211_tx_mlme_mgmt(struct net_device *dev, const u8 *buf, size_t len,\n\t\t\t   bool reconnect);\n\n/**\n * cfg80211_rx_unprot_mlme_mgmt - notification of unprotected mlme mgmt frame\n * @dev: network device\n * @buf: received management frame (header + body)\n * @len: length of the frame data\n *\n * This function is called whenever a received deauthentication or dissassoc\n * frame has been dropped in station mode because of MFP being used but the\n * frame was not protected. This is also used to notify reception of a Beacon\n * frame that was dropped because it did not include a valid MME MIC while\n * beacon protection was enabled (BIGTK configured in station mode).\n *\n * This function may sleep.\n */\nvoid cfg80211_rx_unprot_mlme_mgmt(struct net_device *dev,\n\t\t\t\t  const u8 *buf, size_t len);\n\n/**\n * cfg80211_michael_mic_failure - notification of Michael MIC failure (TKIP)\n * @dev: network device\n * @addr: The source MAC address of the frame\n * @key_type: The key type that the received frame used\n * @key_id: Key identifier (0..3). Can be -1 if missing.\n * @tsc: The TSC value of the frame that generated the MIC failure (6 octets)\n * @gfp: allocation flags\n *\n * This function is called whenever the local MAC detects a MIC failure in a\n * received frame. This matches with MLME-MICHAELMICFAILURE.indication()\n * primitive.\n */\nvoid cfg80211_michael_mic_failure(struct net_device *dev, const u8 *addr,\n\t\t\t\t  enum nl80211_key_type key_type, int key_id,\n\t\t\t\t  const u8 *tsc, gfp_t gfp);\n\n/**\n * cfg80211_ibss_joined - notify cfg80211 that device joined an IBSS\n *\n * @dev: network device\n * @bssid: the BSSID of the IBSS joined\n * @channel: the channel of the IBSS joined\n * @gfp: allocation flags\n *\n * This function notifies cfg80211 that the device joined an IBSS or\n * switched to a different BSSID. Before this function can be called,\n * either a beacon has to have been received from the IBSS, or one of\n * the cfg80211_inform_bss{,_frame} functions must have been called\n * with the locally generated beacon -- this guarantees that there is\n * always a scan result for this IBSS. cfg80211 will handle the rest.\n */\nvoid cfg80211_ibss_joined(struct net_device *dev, const u8 *bssid,\n\t\t\t  struct ieee80211_channel *channel, gfp_t gfp);\n\n/**\n * cfg80211_notify_new_peer_candidate - notify cfg80211 of a new mesh peer\n * \t\t\t\t\tcandidate\n *\n * @dev: network device\n * @macaddr: the MAC address of the new candidate\n * @ie: information elements advertised by the peer candidate\n * @ie_len: length of the information elements buffer\n * @gfp: allocation flags\n *\n * This function notifies cfg80211 that the mesh peer candidate has been\n * detected, most likely via a beacon or, less likely, via a probe response.\n * cfg80211 then sends a notification to userspace.\n */\nvoid cfg80211_notify_new_peer_candidate(struct net_device *dev,\n\t\tconst u8 *macaddr, const u8 *ie, u8 ie_len,\n\t\tint sig_dbm, gfp_t gfp);\n\n/**\n * DOC: RFkill integration\n *\n * RFkill integration in cfg80211 is almost invisible to drivers,\n * as cfg80211 automatically registers an rfkill instance for each\n * wireless device it knows about. Soft kill is also translated\n * into disconnecting and turning all interfaces off, drivers are\n * expected to turn off the device when all interfaces are down.\n *\n * However, devices may have a hard RFkill line, in which case they\n * also need to interact with the rfkill subsystem, via cfg80211.\n * They can do this with a few helper functions documented here.\n */\n\n/**\n * wiphy_rfkill_set_hw_state - notify cfg80211 about hw block state\n * @wiphy: the wiphy\n * @blocked: block status\n */\nvoid wiphy_rfkill_set_hw_state(struct wiphy *wiphy, bool blocked);\n\n/**\n * wiphy_rfkill_start_polling - start polling rfkill\n * @wiphy: the wiphy\n */\nvoid wiphy_rfkill_start_polling(struct wiphy *wiphy);\n\n/**\n * wiphy_rfkill_stop_polling - stop polling rfkill\n * @wiphy: the wiphy\n */\nvoid wiphy_rfkill_stop_polling(struct wiphy *wiphy);\n\n/**\n * DOC: Vendor commands\n *\n * Occasionally, there are special protocol or firmware features that\n * can't be implemented very openly. For this and similar cases, the\n * vendor command functionality allows implementing the features with\n * (typically closed-source) userspace and firmware, using nl80211 as\n * the configuration mechanism.\n *\n * A driver supporting vendor commands must register them as an array\n * in struct wiphy, with handlers for each one, each command has an\n * OUI and sub command ID to identify it.\n *\n * Note that this feature should not be (ab)used to implement protocol\n * features that could openly be shared across drivers. In particular,\n * it must never be required to use vendor commands to implement any\n * \"normal\" functionality that higher-level userspace like connection\n * managers etc. need.\n */\n\nstruct sk_buff *__cfg80211_alloc_reply_skb(struct wiphy *wiphy,\n\t\t\t\t\t   enum nl80211_commands cmd,\n\t\t\t\t\t   enum nl80211_attrs attr,\n\t\t\t\t\t   int approxlen);\n\nstruct sk_buff *__cfg80211_alloc_event_skb(struct wiphy *wiphy,\n\t\t\t\t\t   struct wireless_dev *wdev,\n\t\t\t\t\t   enum nl80211_commands cmd,\n\t\t\t\t\t   enum nl80211_attrs attr,\n\t\t\t\t\t   unsigned int portid,\n\t\t\t\t\t   int vendor_event_idx,\n\t\t\t\t\t   int approxlen, gfp_t gfp);\n\nvoid __cfg80211_send_event_skb(struct sk_buff *skb, gfp_t gfp);\n\n/**\n * cfg80211_vendor_cmd_alloc_reply_skb - allocate vendor command reply\n * @wiphy: the wiphy\n * @approxlen: an upper bound of the length of the data that will\n *\tbe put into the skb\n *\n * This function allocates and pre-fills an skb for a reply to\n * a vendor command. Since it is intended for a reply, calling\n * it outside of a vendor command's doit() operation is invalid.\n *\n * The returned skb is pre-filled with some identifying data in\n * a way that any data that is put into the skb (with skb_put(),\n * nla_put() or similar) will end up being within the\n * %NL80211_ATTR_VENDOR_DATA attribute, so all that needs to be done\n * with the skb is adding data for the corresponding userspace tool\n * which can then read that data out of the vendor data attribute.\n * You must not modify the skb in any other way.\n *\n * When done, call cfg80211_vendor_cmd_reply() with the skb and return\n * its error code as the result of the doit() operation.\n *\n * Return: An allocated and pre-filled skb. %NULL if any errors happen.\n */\nstatic inline struct sk_buff *\ncfg80211_vendor_cmd_alloc_reply_skb(struct wiphy *wiphy, int approxlen)\n{\n\treturn __cfg80211_alloc_reply_skb(wiphy, NL80211_CMD_VENDOR,\n\t\t\t\t\t  NL80211_ATTR_VENDOR_DATA, approxlen);\n}\n\n/**\n * cfg80211_vendor_cmd_reply - send the reply skb\n * @skb: The skb, must have been allocated with\n *\tcfg80211_vendor_cmd_alloc_reply_skb()\n *\n * Since calling this function will usually be the last thing\n * before returning from the vendor command doit() you should\n * return the error code.  Note that this function consumes the\n * skb regardless of the return value.\n *\n * Return: An error code or 0 on success.\n */\nint cfg80211_vendor_cmd_reply(struct sk_buff *skb);\n\n/**\n * cfg80211_vendor_cmd_get_sender\n * @wiphy: the wiphy\n *\n * Return the current netlink port ID in a vendor command handler.\n * Valid to call only there.\n */\nunsigned int cfg80211_vendor_cmd_get_sender(struct wiphy *wiphy);\n\n/**\n * cfg80211_vendor_event_alloc - allocate vendor-specific event skb\n * @wiphy: the wiphy\n * @wdev: the wireless device\n * @event_idx: index of the vendor event in the wiphy's vendor_events\n * @approxlen: an upper bound of the length of the data that will\n *\tbe put into the skb\n * @gfp: allocation flags\n *\n * This function allocates and pre-fills an skb for an event on the\n * vendor-specific multicast group.\n *\n * If wdev != NULL, both the ifindex and identifier of the specified\n * wireless device are added to the event message before the vendor data\n * attribute.\n *\n * When done filling the skb, call cfg80211_vendor_event() with the\n * skb to send the event.\n *\n * Return: An allocated and pre-filled skb. %NULL if any errors happen.\n */\nstatic inline struct sk_buff *\ncfg80211_vendor_event_alloc(struct wiphy *wiphy, struct wireless_dev *wdev,\n\t\t\t     int approxlen, int event_idx, gfp_t gfp)\n{\n\treturn __cfg80211_alloc_event_skb(wiphy, wdev, NL80211_CMD_VENDOR,\n\t\t\t\t\t  NL80211_ATTR_VENDOR_DATA,\n\t\t\t\t\t  0, event_idx, approxlen, gfp);\n}\n\n/**\n * cfg80211_vendor_event_alloc_ucast - alloc unicast vendor-specific event skb\n * @wiphy: the wiphy\n * @wdev: the wireless device\n * @event_idx: index of the vendor event in the wiphy's vendor_events\n * @portid: port ID of the receiver\n * @approxlen: an upper bound of the length of the data that will\n *\tbe put into the skb\n * @gfp: allocation flags\n *\n * This function allocates and pre-fills an skb for an event to send to\n * a specific (userland) socket. This socket would previously have been\n * obtained by cfg80211_vendor_cmd_get_sender(), and the caller MUST take\n * care to register a netlink notifier to see when the socket closes.\n *\n * If wdev != NULL, both the ifindex and identifier of the specified\n * wireless device are added to the event message before the vendor data\n * attribute.\n *\n * When done filling the skb, call cfg80211_vendor_event() with the\n * skb to send the event.\n *\n * Return: An allocated and pre-filled skb. %NULL if any errors happen.\n */\nstatic inline struct sk_buff *\ncfg80211_vendor_event_alloc_ucast(struct wiphy *wiphy,\n\t\t\t\t  struct wireless_dev *wdev,\n\t\t\t\t  unsigned int portid, int approxlen,\n\t\t\t\t  int event_idx, gfp_t gfp)\n{\n\treturn __cfg80211_alloc_event_skb(wiphy, wdev, NL80211_CMD_VENDOR,\n\t\t\t\t\t  NL80211_ATTR_VENDOR_DATA,\n\t\t\t\t\t  portid, event_idx, approxlen, gfp);\n}\n\n/**\n * cfg80211_vendor_event - send the event\n * @skb: The skb, must have been allocated with cfg80211_vendor_event_alloc()\n * @gfp: allocation flags\n *\n * This function sends the given @skb, which must have been allocated\n * by cfg80211_vendor_event_alloc(), as an event. It always consumes it.\n */\nstatic inline void cfg80211_vendor_event(struct sk_buff *skb, gfp_t gfp)\n{\n\t__cfg80211_send_event_skb(skb, gfp);\n}\n\n#ifdef CONFIG_NL80211_TESTMODE\n/**\n * DOC: Test mode\n *\n * Test mode is a set of utility functions to allow drivers to\n * interact with driver-specific tools to aid, for instance,\n * factory programming.\n *\n * This chapter describes how drivers interact with it, for more\n * information see the nl80211 book's chapter on it.\n */\n\n/**\n * cfg80211_testmode_alloc_reply_skb - allocate testmode reply\n * @wiphy: the wiphy\n * @approxlen: an upper bound of the length of the data that will\n *\tbe put into the skb\n *\n * This function allocates and pre-fills an skb for a reply to\n * the testmode command. Since it is intended for a reply, calling\n * it outside of the @testmode_cmd operation is invalid.\n *\n * The returned skb is pre-filled with the wiphy index and set up in\n * a way that any data that is put into the skb (with skb_put(),\n * nla_put() or similar) will end up being within the\n * %NL80211_ATTR_TESTDATA attribute, so all that needs to be done\n * with the skb is adding data for the corresponding userspace tool\n * which can then read that data out of the testdata attribute. You\n * must not modify the skb in any other way.\n *\n * When done, call cfg80211_testmode_reply() with the skb and return\n * its error code as the result of the @testmode_cmd operation.\n *\n * Return: An allocated and pre-filled skb. %NULL if any errors happen.\n */\nstatic inline struct sk_buff *\ncfg80211_testmode_alloc_reply_skb(struct wiphy *wiphy, int approxlen)\n{\n\treturn __cfg80211_alloc_reply_skb(wiphy, NL80211_CMD_TESTMODE,\n\t\t\t\t\t  NL80211_ATTR_TESTDATA, approxlen);\n}\n\n/**\n * cfg80211_testmode_reply - send the reply skb\n * @skb: The skb, must have been allocated with\n *\tcfg80211_testmode_alloc_reply_skb()\n *\n * Since calling this function will usually be the last thing\n * before returning from the @testmode_cmd you should return\n * the error code.  Note that this function consumes the skb\n * regardless of the return value.\n *\n * Return: An error code or 0 on success.\n */\nstatic inline int cfg80211_testmode_reply(struct sk_buff *skb)\n{\n\treturn cfg80211_vendor_cmd_reply(skb);\n}\n\n/**\n * cfg80211_testmode_alloc_event_skb - allocate testmode event\n * @wiphy: the wiphy\n * @approxlen: an upper bound of the length of the data that will\n *\tbe put into the skb\n * @gfp: allocation flags\n *\n * This function allocates and pre-fills an skb for an event on the\n * testmode multicast group.\n *\n * The returned skb is set up in the same way as with\n * cfg80211_testmode_alloc_reply_skb() but prepared for an event. As\n * there, you should simply add data to it that will then end up in the\n * %NL80211_ATTR_TESTDATA attribute. Again, you must not modify the skb\n * in any other way.\n *\n * When done filling the skb, call cfg80211_testmode_event() with the\n * skb to send the event.\n *\n * Return: An allocated and pre-filled skb. %NULL if any errors happen.\n */\nstatic inline struct sk_buff *\ncfg80211_testmode_alloc_event_skb(struct wiphy *wiphy, int approxlen, gfp_t gfp)\n{\n\treturn __cfg80211_alloc_event_skb(wiphy, NULL, NL80211_CMD_TESTMODE,\n\t\t\t\t\t  NL80211_ATTR_TESTDATA, 0, -1,\n\t\t\t\t\t  approxlen, gfp);\n}\n\n/**\n * cfg80211_testmode_event - send the event\n * @skb: The skb, must have been allocated with\n *\tcfg80211_testmode_alloc_event_skb()\n * @gfp: allocation flags\n *\n * This function sends the given @skb, which must have been allocated\n * by cfg80211_testmode_alloc_event_skb(), as an event. It always\n * consumes it.\n */\nstatic inline void cfg80211_testmode_event(struct sk_buff *skb, gfp_t gfp)\n{\n\t__cfg80211_send_event_skb(skb, gfp);\n}\n\n#define CFG80211_TESTMODE_CMD(cmd)\t.testmode_cmd = (cmd),\n#define CFG80211_TESTMODE_DUMP(cmd)\t.testmode_dump = (cmd),\n#else\n#define CFG80211_TESTMODE_CMD(cmd)\n#define CFG80211_TESTMODE_DUMP(cmd)\n#endif\n\n/**\n * struct cfg80211_fils_resp_params - FILS connection response params\n * @kek: KEK derived from a successful FILS connection (may be %NULL)\n * @kek_len: Length of @fils_kek in octets\n * @update_erp_next_seq_num: Boolean value to specify whether the value in\n *\t@erp_next_seq_num is valid.\n * @erp_next_seq_num: The next sequence number to use in ERP message in\n *\tFILS Authentication. This value should be specified irrespective of the\n *\tstatus for a FILS connection.\n * @pmk: A new PMK if derived from a successful FILS connection (may be %NULL).\n * @pmk_len: Length of @pmk in octets\n * @pmkid: A new PMKID if derived from a successful FILS connection or the PMKID\n *\tused for this FILS connection (may be %NULL).\n */\nstruct cfg80211_fils_resp_params {\n\tconst u8 *kek;\n\tsize_t kek_len;\n\tbool update_erp_next_seq_num;\n\tu16 erp_next_seq_num;\n\tconst u8 *pmk;\n\tsize_t pmk_len;\n\tconst u8 *pmkid;\n};\n\n/**\n * struct cfg80211_connect_resp_params - Connection response params\n * @status: Status code, %WLAN_STATUS_SUCCESS for successful connection, use\n *\t%WLAN_STATUS_UNSPECIFIED_FAILURE if your device cannot give you\n *\tthe real status code for failures. If this call is used to report a\n *\tfailure due to a timeout (e.g., not receiving an Authentication frame\n *\tfrom the AP) instead of an explicit rejection by the AP, -1 is used to\n *\tindicate that this is a failure, but without a status code.\n *\t@timeout_reason is used to report the reason for the timeout in that\n *\tcase.\n * @bssid: The BSSID of the AP (may be %NULL)\n * @bss: Entry of bss to which STA got connected to, can be obtained through\n *\tcfg80211_get_bss() (may be %NULL). But it is recommended to store the\n *\tbss from the connect_request and hold a reference to it and return\n *\tthrough this param to avoid a warning if the bss is expired during the\n *\tconnection, esp. for those drivers implementing connect op.\n *\tOnly one parameter among @bssid and @bss needs to be specified.\n * @req_ie: Association request IEs (may be %NULL)\n * @req_ie_len: Association request IEs length\n * @resp_ie: Association response IEs (may be %NULL)\n * @resp_ie_len: Association response IEs length\n * @fils: FILS connection response parameters.\n * @timeout_reason: Reason for connection timeout. This is used when the\n *\tconnection fails due to a timeout instead of an explicit rejection from\n *\tthe AP. %NL80211_TIMEOUT_UNSPECIFIED is used when the timeout reason is\n *\tnot known. This value is used only if @status < 0 to indicate that the\n *\tfailure is due to a timeout and not due to explicit rejection by the AP.\n *\tThis value is ignored in other cases (@status >= 0).\n */\nstruct cfg80211_connect_resp_params {\n\tint status;\n\tconst u8 *bssid;\n\tstruct cfg80211_bss *bss;\n\tconst u8 *req_ie;\n\tsize_t req_ie_len;\n\tconst u8 *resp_ie;\n\tsize_t resp_ie_len;\n\tstruct cfg80211_fils_resp_params fils;\n\tenum nl80211_timeout_reason timeout_reason;\n};\n\n/**\n * cfg80211_connect_done - notify cfg80211 of connection result\n *\n * @dev: network device\n * @params: connection response parameters\n * @gfp: allocation flags\n *\n * It should be called by the underlying driver once execution of the connection\n * request from connect() has been completed. This is similar to\n * cfg80211_connect_bss(), but takes a structure pointer for connection response\n * parameters. Only one of the functions among cfg80211_connect_bss(),\n * cfg80211_connect_result(), cfg80211_connect_timeout(),\n * and cfg80211_connect_done() should be called.\n */\nvoid cfg80211_connect_done(struct net_device *dev,\n\t\t\t   struct cfg80211_connect_resp_params *params,\n\t\t\t   gfp_t gfp);\n\n/**\n * cfg80211_connect_bss - notify cfg80211 of connection result\n *\n * @dev: network device\n * @bssid: the BSSID of the AP\n * @bss: Entry of bss to which STA got connected to, can be obtained through\n *\tcfg80211_get_bss() (may be %NULL). But it is recommended to store the\n *\tbss from the connect_request and hold a reference to it and return\n *\tthrough this param to avoid a warning if the bss is expired during the\n *\tconnection, esp. for those drivers implementing connect op.\n *\tOnly one parameter among @bssid and @bss needs to be specified.\n * @req_ie: association request IEs (maybe be %NULL)\n * @req_ie_len: association request IEs length\n * @resp_ie: association response IEs (may be %NULL)\n * @resp_ie_len: assoc response IEs length\n * @status: status code, %WLAN_STATUS_SUCCESS for successful connection, use\n *\t%WLAN_STATUS_UNSPECIFIED_FAILURE if your device cannot give you\n *\tthe real status code for failures. If this call is used to report a\n *\tfailure due to a timeout (e.g., not receiving an Authentication frame\n *\tfrom the AP) instead of an explicit rejection by the AP, -1 is used to\n *\tindicate that this is a failure, but without a status code.\n *\t@timeout_reason is used to report the reason for the timeout in that\n *\tcase.\n * @gfp: allocation flags\n * @timeout_reason: reason for connection timeout. This is used when the\n *\tconnection fails due to a timeout instead of an explicit rejection from\n *\tthe AP. %NL80211_TIMEOUT_UNSPECIFIED is used when the timeout reason is\n *\tnot known. This value is used only if @status < 0 to indicate that the\n *\tfailure is due to a timeout and not due to explicit rejection by the AP.\n *\tThis value is ignored in other cases (@status >= 0).\n *\n * It should be called by the underlying driver once execution of the connection\n * request from connect() has been completed. This is similar to\n * cfg80211_connect_result(), but with the option of identifying the exact bss\n * entry for the connection. Only one of the functions among\n * cfg80211_connect_bss(), cfg80211_connect_result(),\n * cfg80211_connect_timeout(), and cfg80211_connect_done() should be called.\n */\nstatic inline void\ncfg80211_connect_bss(struct net_device *dev, const u8 *bssid,\n\t\t     struct cfg80211_bss *bss, const u8 *req_ie,\n\t\t     size_t req_ie_len, const u8 *resp_ie,\n\t\t     size_t resp_ie_len, int status, gfp_t gfp,\n\t\t     enum nl80211_timeout_reason timeout_reason)\n{\n\tstruct cfg80211_connect_resp_params params;\n\n\tmemset(&params, 0, sizeof(params));\n\tparams.status = status;\n\tparams.bssid = bssid;\n\tparams.bss = bss;\n\tparams.req_ie = req_ie;\n\tparams.req_ie_len = req_ie_len;\n\tparams.resp_ie = resp_ie;\n\tparams.resp_ie_len = resp_ie_len;\n\tparams.timeout_reason = timeout_reason;\n\n\tcfg80211_connect_done(dev, &params, gfp);\n}\n\n/**\n * cfg80211_connect_result - notify cfg80211 of connection result\n *\n * @dev: network device\n * @bssid: the BSSID of the AP\n * @req_ie: association request IEs (maybe be %NULL)\n * @req_ie_len: association request IEs length\n * @resp_ie: association response IEs (may be %NULL)\n * @resp_ie_len: assoc response IEs length\n * @status: status code, %WLAN_STATUS_SUCCESS for successful connection, use\n *\t%WLAN_STATUS_UNSPECIFIED_FAILURE if your device cannot give you\n *\tthe real status code for failures.\n * @gfp: allocation flags\n *\n * It should be called by the underlying driver once execution of the connection\n * request from connect() has been completed. This is similar to\n * cfg80211_connect_bss() which allows the exact bss entry to be specified. Only\n * one of the functions among cfg80211_connect_bss(), cfg80211_connect_result(),\n * cfg80211_connect_timeout(), and cfg80211_connect_done() should be called.\n */\nstatic inline void\ncfg80211_connect_result(struct net_device *dev, const u8 *bssid,\n\t\t\tconst u8 *req_ie, size_t req_ie_len,\n\t\t\tconst u8 *resp_ie, size_t resp_ie_len,\n\t\t\tu16 status, gfp_t gfp)\n{\n\tcfg80211_connect_bss(dev, bssid, NULL, req_ie, req_ie_len, resp_ie,\n\t\t\t     resp_ie_len, status, gfp,\n\t\t\t     NL80211_TIMEOUT_UNSPECIFIED);\n}\n\n/**\n * cfg80211_connect_timeout - notify cfg80211 of connection timeout\n *\n * @dev: network device\n * @bssid: the BSSID of the AP\n * @req_ie: association request IEs (maybe be %NULL)\n * @req_ie_len: association request IEs length\n * @gfp: allocation flags\n * @timeout_reason: reason for connection timeout.\n *\n * It should be called by the underlying driver whenever connect() has failed\n * in a sequence where no explicit authentication/association rejection was\n * received from the AP. This could happen, e.g., due to not being able to send\n * out the Authentication or Association Request frame or timing out while\n * waiting for the response. Only one of the functions among\n * cfg80211_connect_bss(), cfg80211_connect_result(),\n * cfg80211_connect_timeout(), and cfg80211_connect_done() should be called.\n */\nstatic inline void\ncfg80211_connect_timeout(struct net_device *dev, const u8 *bssid,\n\t\t\t const u8 *req_ie, size_t req_ie_len, gfp_t gfp,\n\t\t\t enum nl80211_timeout_reason timeout_reason)\n{\n\tcfg80211_connect_bss(dev, bssid, NULL, req_ie, req_ie_len, NULL, 0, -1,\n\t\t\t     gfp, timeout_reason);\n}\n\n/**\n * struct cfg80211_roam_info - driver initiated roaming information\n *\n * @channel: the channel of the new AP\n * @bss: entry of bss to which STA got roamed (may be %NULL if %bssid is set)\n * @bssid: the BSSID of the new AP (may be %NULL if %bss is set)\n * @req_ie: association request IEs (maybe be %NULL)\n * @req_ie_len: association request IEs length\n * @resp_ie: association response IEs (may be %NULL)\n * @resp_ie_len: assoc response IEs length\n * @fils: FILS related roaming information.\n */\nstruct cfg80211_roam_info {\n\tstruct ieee80211_channel *channel;\n\tstruct cfg80211_bss *bss;\n\tconst u8 *bssid;\n\tconst u8 *req_ie;\n\tsize_t req_ie_len;\n\tconst u8 *resp_ie;\n\tsize_t resp_ie_len;\n\tstruct cfg80211_fils_resp_params fils;\n};\n\n/**\n * cfg80211_roamed - notify cfg80211 of roaming\n *\n * @dev: network device\n * @info: information about the new BSS. struct &cfg80211_roam_info.\n * @gfp: allocation flags\n *\n * This function may be called with the driver passing either the BSSID of the\n * new AP or passing the bss entry to avoid a race in timeout of the bss entry.\n * It should be called by the underlying driver whenever it roamed from one AP\n * to another while connected. Drivers which have roaming implemented in\n * firmware should pass the bss entry to avoid a race in bss entry timeout where\n * the bss entry of the new AP is seen in the driver, but gets timed out by the\n * time it is accessed in __cfg80211_roamed() due to delay in scheduling\n * rdev->event_work. In case of any failures, the reference is released\n * either in cfg80211_roamed() or in __cfg80211_romed(), Otherwise, it will be\n * released while disconnecting from the current bss.\n */\nvoid cfg80211_roamed(struct net_device *dev, struct cfg80211_roam_info *info,\n\t\t     gfp_t gfp);\n\n/**\n * cfg80211_port_authorized - notify cfg80211 of successful security association\n *\n * @dev: network device\n * @bssid: the BSSID of the AP\n * @gfp: allocation flags\n *\n * This function should be called by a driver that supports 4 way handshake\n * offload after a security association was successfully established (i.e.,\n * the 4 way handshake was completed successfully). The call to this function\n * should be preceded with a call to cfg80211_connect_result(),\n * cfg80211_connect_done(), cfg80211_connect_bss() or cfg80211_roamed() to\n * indicate the 802.11 association.\n */\nvoid cfg80211_port_authorized(struct net_device *dev, const u8 *bssid,\n\t\t\t      gfp_t gfp);\n\n/**\n * cfg80211_disconnected - notify cfg80211 that connection was dropped\n *\n * @dev: network device\n * @ie: information elements of the deauth/disassoc frame (may be %NULL)\n * @ie_len: length of IEs\n * @reason: reason code for the disconnection, set it to 0 if unknown\n * @locally_generated: disconnection was requested locally\n * @gfp: allocation flags\n *\n * After it calls this function, the driver should enter an idle state\n * and not try to connect to any AP any more.\n */\nvoid cfg80211_disconnected(struct net_device *dev, u16 reason,\n\t\t\t   const u8 *ie, size_t ie_len,\n\t\t\t   bool locally_generated, gfp_t gfp);\n\n/**\n * cfg80211_ready_on_channel - notification of remain_on_channel start\n * @wdev: wireless device\n * @cookie: the request cookie\n * @chan: The current channel (from remain_on_channel request)\n * @duration: Duration in milliseconds that the driver intents to remain on the\n *\tchannel\n * @gfp: allocation flags\n */\nvoid cfg80211_ready_on_channel(struct wireless_dev *wdev, u64 cookie,\n\t\t\t       struct ieee80211_channel *chan,\n\t\t\t       unsigned int duration, gfp_t gfp);\n\n/**\n * cfg80211_remain_on_channel_expired - remain_on_channel duration expired\n * @wdev: wireless device\n * @cookie: the request cookie\n * @chan: The current channel (from remain_on_channel request)\n * @gfp: allocation flags\n */\nvoid cfg80211_remain_on_channel_expired(struct wireless_dev *wdev, u64 cookie,\n\t\t\t\t\tstruct ieee80211_channel *chan,\n\t\t\t\t\tgfp_t gfp);\n\n/**\n * cfg80211_tx_mgmt_expired - tx_mgmt duration expired\n * @wdev: wireless device\n * @cookie: the requested cookie\n * @chan: The current channel (from tx_mgmt request)\n * @gfp: allocation flags\n */\nvoid cfg80211_tx_mgmt_expired(struct wireless_dev *wdev, u64 cookie,\n\t\t\t      struct ieee80211_channel *chan, gfp_t gfp);\n\n/**\n * cfg80211_sinfo_alloc_tid_stats - allocate per-tid statistics.\n *\n * @sinfo: the station information\n * @gfp: allocation flags\n */\nint cfg80211_sinfo_alloc_tid_stats(struct station_info *sinfo, gfp_t gfp);\n\n/**\n * cfg80211_sinfo_release_content - release contents of station info\n * @sinfo: the station information\n *\n * Releases any potentially allocated sub-information of the station\n * information, but not the struct itself (since it's typically on\n * the stack.)\n */\nstatic inline void cfg80211_sinfo_release_content(struct station_info *sinfo)\n{\n\tkfree(sinfo->pertid);\n}\n\n/**\n * cfg80211_new_sta - notify userspace about station\n *\n * @dev: the netdev\n * @mac_addr: the station's address\n * @sinfo: the station information\n * @gfp: allocation flags\n */\nvoid cfg80211_new_sta(struct net_device *dev, const u8 *mac_addr,\n\t\t      struct station_info *sinfo, gfp_t gfp);\n\n/**\n * cfg80211_del_sta_sinfo - notify userspace about deletion of a station\n * @dev: the netdev\n * @mac_addr: the station's address\n * @sinfo: the station information/statistics\n * @gfp: allocation flags\n */\nvoid cfg80211_del_sta_sinfo(struct net_device *dev, const u8 *mac_addr,\n\t\t\t    struct station_info *sinfo, gfp_t gfp);\n\n/**\n * cfg80211_del_sta - notify userspace about deletion of a station\n *\n * @dev: the netdev\n * @mac_addr: the station's address\n * @gfp: allocation flags\n */\nstatic inline void cfg80211_del_sta(struct net_device *dev,\n\t\t\t\t    const u8 *mac_addr, gfp_t gfp)\n{\n\tcfg80211_del_sta_sinfo(dev, mac_addr, NULL, gfp);\n}\n\n/**\n * cfg80211_conn_failed - connection request failed notification\n *\n * @dev: the netdev\n * @mac_addr: the station's address\n * @reason: the reason for connection failure\n * @gfp: allocation flags\n *\n * Whenever a station tries to connect to an AP and if the station\n * could not connect to the AP as the AP has rejected the connection\n * for some reasons, this function is called.\n *\n * The reason for connection failure can be any of the value from\n * nl80211_connect_failed_reason enum\n */\nvoid cfg80211_conn_failed(struct net_device *dev, const u8 *mac_addr,\n\t\t\t  enum nl80211_connect_failed_reason reason,\n\t\t\t  gfp_t gfp);\n\n/**\n * cfg80211_rx_mgmt_khz - notification of received, unprocessed management frame\n * @wdev: wireless device receiving the frame\n * @freq: Frequency on which the frame was received in KHz\n * @sig_dbm: signal strength in dBm, or 0 if unknown\n * @buf: Management frame (header + body)\n * @len: length of the frame data\n * @flags: flags, as defined in enum nl80211_rxmgmt_flags\n *\n * This function is called whenever an Action frame is received for a station\n * mode interface, but is not processed in kernel.\n *\n * Return: %true if a user space application has registered for this frame.\n * For action frames, that makes it responsible for rejecting unrecognized\n * action frames; %false otherwise, in which case for action frames the\n * driver is responsible for rejecting the frame.\n */\nbool cfg80211_rx_mgmt_khz(struct wireless_dev *wdev, int freq, int sig_dbm,\n\t\t\t  const u8 *buf, size_t len, u32 flags);\n\n/**\n * cfg80211_rx_mgmt - notification of received, unprocessed management frame\n * @wdev: wireless device receiving the frame\n * @freq: Frequency on which the frame was received in MHz\n * @sig_dbm: signal strength in dBm, or 0 if unknown\n * @buf: Management frame (header + body)\n * @len: length of the frame data\n * @flags: flags, as defined in enum nl80211_rxmgmt_flags\n *\n * This function is called whenever an Action frame is received for a station\n * mode interface, but is not processed in kernel.\n *\n * Return: %true if a user space application has registered for this frame.\n * For action frames, that makes it responsible for rejecting unrecognized\n * action frames; %false otherwise, in which case for action frames the\n * driver is responsible for rejecting the frame.\n */\nstatic inline bool cfg80211_rx_mgmt(struct wireless_dev *wdev, int freq,\n\t\t\t\t    int sig_dbm, const u8 *buf, size_t len,\n\t\t\t\t    u32 flags)\n{\n\treturn cfg80211_rx_mgmt_khz(wdev, MHZ_TO_KHZ(freq), sig_dbm, buf, len,\n\t\t\t\t    flags);\n}\n\n/**\n * cfg80211_mgmt_tx_status - notification of TX status for management frame\n * @wdev: wireless device receiving the frame\n * @cookie: Cookie returned by cfg80211_ops::mgmt_tx()\n * @buf: Management frame (header + body)\n * @len: length of the frame data\n * @ack: Whether frame was acknowledged\n * @gfp: context flags\n *\n * This function is called whenever a management frame was requested to be\n * transmitted with cfg80211_ops::mgmt_tx() to report the TX status of the\n * transmission attempt.\n */\nvoid cfg80211_mgmt_tx_status(struct wireless_dev *wdev, u64 cookie,\n\t\t\t     const u8 *buf, size_t len, bool ack, gfp_t gfp);\n\n/**\n * cfg80211_control_port_tx_status - notification of TX status for control\n *                                   port frames\n * @wdev: wireless device receiving the frame\n * @cookie: Cookie returned by cfg80211_ops::tx_control_port()\n * @buf: Data frame (header + body)\n * @len: length of the frame data\n * @ack: Whether frame was acknowledged\n * @gfp: context flags\n *\n * This function is called whenever a control port frame was requested to be\n * transmitted with cfg80211_ops::tx_control_port() to report the TX status of\n * the transmission attempt.\n */\nvoid cfg80211_control_port_tx_status(struct wireless_dev *wdev, u64 cookie,\n\t\t\t\t     const u8 *buf, size_t len, bool ack,\n\t\t\t\t     gfp_t gfp);\n\n/**\n * cfg80211_rx_control_port - notification about a received control port frame\n * @dev: The device the frame matched to\n * @skb: The skbuf with the control port frame.  It is assumed that the skbuf\n *\tis 802.3 formatted (with 802.3 header).  The skb can be non-linear.\n *\tThis function does not take ownership of the skb, so the caller is\n *\tresponsible for any cleanup.  The caller must also ensure that\n *\tskb->protocol is set appropriately.\n * @unencrypted: Whether the frame was received unencrypted\n *\n * This function is used to inform userspace about a received control port\n * frame.  It should only be used if userspace indicated it wants to receive\n * control port frames over nl80211.\n *\n * The frame is the data portion of the 802.3 or 802.11 data frame with all\n * network layer headers removed (e.g. the raw EAPoL frame).\n *\n * Return: %true if the frame was passed to userspace\n */\nbool cfg80211_rx_control_port(struct net_device *dev,\n\t\t\t      struct sk_buff *skb, bool unencrypted);\n\n/**\n * cfg80211_cqm_rssi_notify - connection quality monitoring rssi event\n * @dev: network device\n * @rssi_event: the triggered RSSI event\n * @rssi_level: new RSSI level value or 0 if not available\n * @gfp: context flags\n *\n * This function is called when a configured connection quality monitoring\n * rssi threshold reached event occurs.\n */\nvoid cfg80211_cqm_rssi_notify(struct net_device *dev,\n\t\t\t      enum nl80211_cqm_rssi_threshold_event rssi_event,\n\t\t\t      s32 rssi_level, gfp_t gfp);\n\n/**\n * cfg80211_cqm_pktloss_notify - notify userspace about packetloss to peer\n * @dev: network device\n * @peer: peer's MAC address\n * @num_packets: how many packets were lost -- should be a fixed threshold\n *\tbut probably no less than maybe 50, or maybe a throughput dependent\n *\tthreshold (to account for temporary interference)\n * @gfp: context flags\n */\nvoid cfg80211_cqm_pktloss_notify(struct net_device *dev,\n\t\t\t\t const u8 *peer, u32 num_packets, gfp_t gfp);\n\n/**\n * cfg80211_cqm_txe_notify - TX error rate event\n * @dev: network device\n * @peer: peer's MAC address\n * @num_packets: how many packets were lost\n * @rate: % of packets which failed transmission\n * @intvl: interval (in s) over which the TX failure threshold was breached.\n * @gfp: context flags\n *\n * Notify userspace when configured % TX failures over number of packets in a\n * given interval is exceeded.\n */\nvoid cfg80211_cqm_txe_notify(struct net_device *dev, const u8 *peer,\n\t\t\t     u32 num_packets, u32 rate, u32 intvl, gfp_t gfp);\n\n/**\n * cfg80211_cqm_beacon_loss_notify - beacon loss event\n * @dev: network device\n * @gfp: context flags\n *\n * Notify userspace about beacon loss from the connected AP.\n */\nvoid cfg80211_cqm_beacon_loss_notify(struct net_device *dev, gfp_t gfp);\n\n/**\n * cfg80211_radar_event - radar detection event\n * @wiphy: the wiphy\n * @chandef: chandef for the current channel\n * @gfp: context flags\n *\n * This function is called when a radar is detected on the current chanenl.\n */\nvoid cfg80211_radar_event(struct wiphy *wiphy,\n\t\t\t  struct cfg80211_chan_def *chandef, gfp_t gfp);\n\n/**\n * cfg80211_sta_opmode_change_notify - STA's ht/vht operation mode change event\n * @dev: network device\n * @mac: MAC address of a station which opmode got modified\n * @sta_opmode: station's current opmode value\n * @gfp: context flags\n *\n * Driver should call this function when station's opmode modified via action\n * frame.\n */\nvoid cfg80211_sta_opmode_change_notify(struct net_device *dev, const u8 *mac,\n\t\t\t\t       struct sta_opmode_info *sta_opmode,\n\t\t\t\t       gfp_t gfp);\n\n/**\n * cfg80211_cac_event - Channel availability check (CAC) event\n * @netdev: network device\n * @chandef: chandef for the current channel\n * @event: type of event\n * @gfp: context flags\n *\n * This function is called when a Channel availability check (CAC) is finished\n * or aborted. This must be called to notify the completion of a CAC process,\n * also by full-MAC drivers.\n */\nvoid cfg80211_cac_event(struct net_device *netdev,\n\t\t\tconst struct cfg80211_chan_def *chandef,\n\t\t\tenum nl80211_radar_event event, gfp_t gfp);\n\n\n/**\n * cfg80211_gtk_rekey_notify - notify userspace about driver rekeying\n * @dev: network device\n * @bssid: BSSID of AP (to avoid races)\n * @replay_ctr: new replay counter\n * @gfp: allocation flags\n */\nvoid cfg80211_gtk_rekey_notify(struct net_device *dev, const u8 *bssid,\n\t\t\t       const u8 *replay_ctr, gfp_t gfp);\n\n/**\n * cfg80211_pmksa_candidate_notify - notify about PMKSA caching candidate\n * @dev: network device\n * @index: candidate index (the smaller the index, the higher the priority)\n * @bssid: BSSID of AP\n * @preauth: Whether AP advertises support for RSN pre-authentication\n * @gfp: allocation flags\n */\nvoid cfg80211_pmksa_candidate_notify(struct net_device *dev, int index,\n\t\t\t\t     const u8 *bssid, bool preauth, gfp_t gfp);\n\n/**\n * cfg80211_rx_spurious_frame - inform userspace about a spurious frame\n * @dev: The device the frame matched to\n * @addr: the transmitter address\n * @gfp: context flags\n *\n * This function is used in AP mode (only!) to inform userspace that\n * a spurious class 3 frame was received, to be able to deauth the\n * sender.\n * Return: %true if the frame was passed to userspace (or this failed\n * for a reason other than not having a subscription.)\n */\nbool cfg80211_rx_spurious_frame(struct net_device *dev,\n\t\t\t\tconst u8 *addr, gfp_t gfp);\n\n/**\n * cfg80211_rx_unexpected_4addr_frame - inform about unexpected WDS frame\n * @dev: The device the frame matched to\n * @addr: the transmitter address\n * @gfp: context flags\n *\n * This function is used in AP mode (only!) to inform userspace that\n * an associated station sent a 4addr frame but that wasn't expected.\n * It is allowed and desirable to send this event only once for each\n * station to avoid event flooding.\n * Return: %true if the frame was passed to userspace (or this failed\n * for a reason other than not having a subscription.)\n */\nbool cfg80211_rx_unexpected_4addr_frame(struct net_device *dev,\n\t\t\t\t\tconst u8 *addr, gfp_t gfp);\n\n/**\n * cfg80211_probe_status - notify userspace about probe status\n * @dev: the device the probe was sent on\n * @addr: the address of the peer\n * @cookie: the cookie filled in @probe_client previously\n * @acked: indicates whether probe was acked or not\n * @ack_signal: signal strength (in dBm) of the ACK frame.\n * @is_valid_ack_signal: indicates the ack_signal is valid or not.\n * @gfp: allocation flags\n */\nvoid cfg80211_probe_status(struct net_device *dev, const u8 *addr,\n\t\t\t   u64 cookie, bool acked, s32 ack_signal,\n\t\t\t   bool is_valid_ack_signal, gfp_t gfp);\n\n/**\n * cfg80211_report_obss_beacon_khz - report beacon from other APs\n * @wiphy: The wiphy that received the beacon\n * @frame: the frame\n * @len: length of the frame\n * @freq: frequency the frame was received on in KHz\n * @sig_dbm: signal strength in dBm, or 0 if unknown\n *\n * Use this function to report to userspace when a beacon was\n * received. It is not useful to call this when there is no\n * netdev that is in AP/GO mode.\n */\nvoid cfg80211_report_obss_beacon_khz(struct wiphy *wiphy, const u8 *frame,\n\t\t\t\t     size_t len, int freq, int sig_dbm);\n\n/**\n * cfg80211_report_obss_beacon - report beacon from other APs\n * @wiphy: The wiphy that received the beacon\n * @frame: the frame\n * @len: length of the frame\n * @freq: frequency the frame was received on\n * @sig_dbm: signal strength in dBm, or 0 if unknown\n *\n * Use this function to report to userspace when a beacon was\n * received. It is not useful to call this when there is no\n * netdev that is in AP/GO mode.\n */\nstatic inline void cfg80211_report_obss_beacon(struct wiphy *wiphy,\n\t\t\t\t\t       const u8 *frame, size_t len,\n\t\t\t\t\t       int freq, int sig_dbm)\n{\n\tcfg80211_report_obss_beacon_khz(wiphy, frame, len, MHZ_TO_KHZ(freq),\n\t\t\t\t\tsig_dbm);\n}\n\n/**\n * cfg80211_reg_can_beacon - check if beaconing is allowed\n * @wiphy: the wiphy\n * @chandef: the channel definition\n * @iftype: interface type\n *\n * Return: %true if there is no secondary channel or the secondary channel(s)\n * can be used for beaconing (i.e. is not a radar channel etc.)\n */\nbool cfg80211_reg_can_beacon(struct wiphy *wiphy,\n\t\t\t     struct cfg80211_chan_def *chandef,\n\t\t\t     enum nl80211_iftype iftype);\n\n/**\n * cfg80211_reg_can_beacon_relax - check if beaconing is allowed with relaxation\n * @wiphy: the wiphy\n * @chandef: the channel definition\n * @iftype: interface type\n *\n * Return: %true if there is no secondary channel or the secondary channel(s)\n * can be used for beaconing (i.e. is not a radar channel etc.). This version\n * also checks if IR-relaxation conditions apply, to allow beaconing under\n * more permissive conditions.\n *\n * Requires the wiphy mutex to be held.\n */\nbool cfg80211_reg_can_beacon_relax(struct wiphy *wiphy,\n\t\t\t\t   struct cfg80211_chan_def *chandef,\n\t\t\t\t   enum nl80211_iftype iftype);\n\n/*\n * cfg80211_ch_switch_notify - update wdev channel and notify userspace\n * @dev: the device which switched channels\n * @chandef: the new channel definition\n *\n * Caller must acquire wdev_lock, therefore must only be called from sleepable\n * driver context!\n */\nvoid cfg80211_ch_switch_notify(struct net_device *dev,\n\t\t\t       struct cfg80211_chan_def *chandef);\n\n/*\n * cfg80211_ch_switch_started_notify - notify channel switch start\n * @dev: the device on which the channel switch started\n * @chandef: the future channel definition\n * @count: the number of TBTTs until the channel switch happens\n * @quiet: whether or not immediate quiet was requested by the AP\n *\n * Inform the userspace about the channel switch that has just\n * started, so that it can take appropriate actions (eg. starting\n * channel switch on other vifs), if necessary.\n */\nvoid cfg80211_ch_switch_started_notify(struct net_device *dev,\n\t\t\t\t       struct cfg80211_chan_def *chandef,\n\t\t\t\t       u8 count, bool quiet);\n\n/**\n * ieee80211_operating_class_to_band - convert operating class to band\n *\n * @operating_class: the operating class to convert\n * @band: band pointer to fill\n *\n * Returns %true if the conversion was successful, %false otherwise.\n */\nbool ieee80211_operating_class_to_band(u8 operating_class,\n\t\t\t\t       enum nl80211_band *band);\n\n/**\n * ieee80211_chandef_to_operating_class - convert chandef to operation class\n *\n * @chandef: the chandef to convert\n * @op_class: a pointer to the resulting operating class\n *\n * Returns %true if the conversion was successful, %false otherwise.\n */\nbool ieee80211_chandef_to_operating_class(struct cfg80211_chan_def *chandef,\n\t\t\t\t\t  u8 *op_class);\n\n/**\n * ieee80211_chandef_to_khz - convert chandef to frequency in KHz\n *\n * @chandef: the chandef to convert\n *\n * Returns the center frequency of chandef (1st segment) in KHz.\n */\nstatic inline u32\nieee80211_chandef_to_khz(const struct cfg80211_chan_def *chandef)\n{\n\treturn MHZ_TO_KHZ(chandef->center_freq1) + chandef->freq1_offset;\n}\n\n/*\n * cfg80211_tdls_oper_request - request userspace to perform TDLS operation\n * @dev: the device on which the operation is requested\n * @peer: the MAC address of the peer device\n * @oper: the requested TDLS operation (NL80211_TDLS_SETUP or\n *\tNL80211_TDLS_TEARDOWN)\n * @reason_code: the reason code for teardown request\n * @gfp: allocation flags\n *\n * This function is used to request userspace to perform TDLS operation that\n * requires knowledge of keys, i.e., link setup or teardown when the AP\n * connection uses encryption. This is optional mechanism for the driver to use\n * if it can automatically determine when a TDLS link could be useful (e.g.,\n * based on traffic and signal strength for a peer).\n */\nvoid cfg80211_tdls_oper_request(struct net_device *dev, const u8 *peer,\n\t\t\t\tenum nl80211_tdls_operation oper,\n\t\t\t\tu16 reason_code, gfp_t gfp);\n\n/*\n * cfg80211_calculate_bitrate - calculate actual bitrate (in 100Kbps units)\n * @rate: given rate_info to calculate bitrate from\n *\n * return 0 if MCS index >= 32\n */\nu32 cfg80211_calculate_bitrate(struct rate_info *rate);\n\n/**\n * cfg80211_unregister_wdev - remove the given wdev\n * @wdev: struct wireless_dev to remove\n *\n * This function removes the device so it can no longer be used. It is necessary\n * to call this function even when cfg80211 requests the removal of the device\n * by calling the del_virtual_intf() callback. The function must also be called\n * when the driver wishes to unregister the wdev, e.g. when the hardware device\n * is unbound from the driver.\n *\n * Requires the RTNL and wiphy mutex to be held.\n */\nvoid cfg80211_unregister_wdev(struct wireless_dev *wdev);\n\n/**\n * cfg80211_register_netdevice - register the given netdev\n * @dev: the netdev to register\n *\n * Note: In contexts coming from cfg80211 callbacks, you must call this rather\n * than register_netdevice(), unregister_netdev() is impossible as the RTNL is\n * held. Otherwise, both register_netdevice() and register_netdev() are usable\n * instead as well.\n *\n * Requires the RTNL and wiphy mutex to be held.\n */\nint cfg80211_register_netdevice(struct net_device *dev);\n\n/**\n * cfg80211_unregister_netdevice - unregister the given netdev\n * @dev: the netdev to register\n *\n * Note: In contexts coming from cfg80211 callbacks, you must call this rather\n * than unregister_netdevice(), unregister_netdev() is impossible as the RTNL\n * is held. Otherwise, both unregister_netdevice() and unregister_netdev() are\n * usable instead as well.\n *\n * Requires the RTNL and wiphy mutex to be held.\n */\nstatic inline void cfg80211_unregister_netdevice(struct net_device *dev)\n{\n\tcfg80211_unregister_wdev(dev->ieee80211_ptr);\n}\n\n/**\n * struct cfg80211_ft_event_params - FT Information Elements\n * @ies: FT IEs\n * @ies_len: length of the FT IE in bytes\n * @target_ap: target AP's MAC address\n * @ric_ies: RIC IE\n * @ric_ies_len: length of the RIC IE in bytes\n */\nstruct cfg80211_ft_event_params {\n\tconst u8 *ies;\n\tsize_t ies_len;\n\tconst u8 *target_ap;\n\tconst u8 *ric_ies;\n\tsize_t ric_ies_len;\n};\n\n/**\n * cfg80211_ft_event - notify userspace about FT IE and RIC IE\n * @netdev: network device\n * @ft_event: IE information\n */\nvoid cfg80211_ft_event(struct net_device *netdev,\n\t\t       struct cfg80211_ft_event_params *ft_event);\n\n/**\n * cfg80211_get_p2p_attr - find and copy a P2P attribute from IE buffer\n * @ies: the input IE buffer\n * @len: the input length\n * @attr: the attribute ID to find\n * @buf: output buffer, can be %NULL if the data isn't needed, e.g.\n *\tif the function is only called to get the needed buffer size\n * @bufsize: size of the output buffer\n *\n * The function finds a given P2P attribute in the (vendor) IEs and\n * copies its contents to the given buffer.\n *\n * Return: A negative error code (-%EILSEQ or -%ENOENT) if the data is\n * malformed or the attribute can't be found (respectively), or the\n * length of the found attribute (which can be zero).\n */\nint cfg80211_get_p2p_attr(const u8 *ies, unsigned int len,\n\t\t\t  enum ieee80211_p2p_attr_id attr,\n\t\t\t  u8 *buf, unsigned int bufsize);\n\n/**\n * ieee80211_ie_split_ric - split an IE buffer according to ordering (with RIC)\n * @ies: the IE buffer\n * @ielen: the length of the IE buffer\n * @ids: an array with element IDs that are allowed before\n *\tthe split. A WLAN_EID_EXTENSION value means that the next\n *\tEID in the list is a sub-element of the EXTENSION IE.\n * @n_ids: the size of the element ID array\n * @after_ric: array IE types that come after the RIC element\n * @n_after_ric: size of the @after_ric array\n * @offset: offset where to start splitting in the buffer\n *\n * This function splits an IE buffer by updating the @offset\n * variable to point to the location where the buffer should be\n * split.\n *\n * It assumes that the given IE buffer is well-formed, this\n * has to be guaranteed by the caller!\n *\n * It also assumes that the IEs in the buffer are ordered\n * correctly, if not the result of using this function will not\n * be ordered correctly either, i.e. it does no reordering.\n *\n * The function returns the offset where the next part of the\n * buffer starts, which may be @ielen if the entire (remainder)\n * of the buffer should be used.\n */\nsize_t ieee80211_ie_split_ric(const u8 *ies, size_t ielen,\n\t\t\t      const u8 *ids, int n_ids,\n\t\t\t      const u8 *after_ric, int n_after_ric,\n\t\t\t      size_t offset);\n\n/**\n * ieee80211_ie_split - split an IE buffer according to ordering\n * @ies: the IE buffer\n * @ielen: the length of the IE buffer\n * @ids: an array with element IDs that are allowed before\n *\tthe split. A WLAN_EID_EXTENSION value means that the next\n *\tEID in the list is a sub-element of the EXTENSION IE.\n * @n_ids: the size of the element ID array\n * @offset: offset where to start splitting in the buffer\n *\n * This function splits an IE buffer by updating the @offset\n * variable to point to the location where the buffer should be\n * split.\n *\n * It assumes that the given IE buffer is well-formed, this\n * has to be guaranteed by the caller!\n *\n * It also assumes that the IEs in the buffer are ordered\n * correctly, if not the result of using this function will not\n * be ordered correctly either, i.e. it does no reordering.\n *\n * The function returns the offset where the next part of the\n * buffer starts, which may be @ielen if the entire (remainder)\n * of the buffer should be used.\n */\nstatic inline size_t ieee80211_ie_split(const u8 *ies, size_t ielen,\n\t\t\t\t\tconst u8 *ids, int n_ids, size_t offset)\n{\n\treturn ieee80211_ie_split_ric(ies, ielen, ids, n_ids, NULL, 0, offset);\n}\n\n/**\n * cfg80211_report_wowlan_wakeup - report wakeup from WoWLAN\n * @wdev: the wireless device reporting the wakeup\n * @wakeup: the wakeup report\n * @gfp: allocation flags\n *\n * This function reports that the given device woke up. If it\n * caused the wakeup, report the reason(s), otherwise you may\n * pass %NULL as the @wakeup parameter to advertise that something\n * else caused the wakeup.\n */\nvoid cfg80211_report_wowlan_wakeup(struct wireless_dev *wdev,\n\t\t\t\t   struct cfg80211_wowlan_wakeup *wakeup,\n\t\t\t\t   gfp_t gfp);\n\n/**\n * cfg80211_crit_proto_stopped() - indicate critical protocol stopped by driver.\n *\n * @wdev: the wireless device for which critical protocol is stopped.\n * @gfp: allocation flags\n *\n * This function can be called by the driver to indicate it has reverted\n * operation back to normal. One reason could be that the duration given\n * by .crit_proto_start() has expired.\n */\nvoid cfg80211_crit_proto_stopped(struct wireless_dev *wdev, gfp_t gfp);\n\n/**\n * ieee80211_get_num_supported_channels - get number of channels device has\n * @wiphy: the wiphy\n *\n * Return: the number of channels supported by the device.\n */\nunsigned int ieee80211_get_num_supported_channels(struct wiphy *wiphy);\n\n/**\n * cfg80211_check_combinations - check interface combinations\n *\n * @wiphy: the wiphy\n * @params: the interface combinations parameter\n *\n * This function can be called by the driver to check whether a\n * combination of interfaces and their types are allowed according to\n * the interface combinations.\n */\nint cfg80211_check_combinations(struct wiphy *wiphy,\n\t\t\t\tstruct iface_combination_params *params);\n\n/**\n * cfg80211_iter_combinations - iterate over matching combinations\n *\n * @wiphy: the wiphy\n * @params: the interface combinations parameter\n * @iter: function to call for each matching combination\n * @data: pointer to pass to iter function\n *\n * This function can be called by the driver to check what possible\n * combinations it fits in at a given moment, e.g. for channel switching\n * purposes.\n */\nint cfg80211_iter_combinations(struct wiphy *wiphy,\n\t\t\t       struct iface_combination_params *params,\n\t\t\t       void (*iter)(const struct ieee80211_iface_combination *c,\n\t\t\t\t\t    void *data),\n\t\t\t       void *data);\n\n/*\n * cfg80211_stop_iface - trigger interface disconnection\n *\n * @wiphy: the wiphy\n * @wdev: wireless device\n * @gfp: context flags\n *\n * Trigger interface to be stopped as if AP was stopped, IBSS/mesh left, STA\n * disconnected.\n *\n * Note: This doesn't need any locks and is asynchronous.\n */\nvoid cfg80211_stop_iface(struct wiphy *wiphy, struct wireless_dev *wdev,\n\t\t\t gfp_t gfp);\n\n/**\n * cfg80211_shutdown_all_interfaces - shut down all interfaces for a wiphy\n * @wiphy: the wiphy to shut down\n *\n * This function shuts down all interfaces belonging to this wiphy by\n * calling dev_close() (and treating non-netdev interfaces as needed).\n * It shouldn't really be used unless there are some fatal device errors\n * that really can't be recovered in any other way.\n *\n * Callers must hold the RTNL and be able to deal with callbacks into\n * the driver while the function is running.\n */\nvoid cfg80211_shutdown_all_interfaces(struct wiphy *wiphy);\n\n/**\n * wiphy_ext_feature_set - set the extended feature flag\n *\n * @wiphy: the wiphy to modify.\n * @ftidx: extended feature bit index.\n *\n * The extended features are flagged in multiple bytes (see\n * &struct wiphy.@ext_features)\n */\nstatic inline void wiphy_ext_feature_set(struct wiphy *wiphy,\n\t\t\t\t\t enum nl80211_ext_feature_index ftidx)\n{\n\tu8 *ft_byte;\n\n\tft_byte = &wiphy->ext_features[ftidx / 8];\n\t*ft_byte |= BIT(ftidx % 8);\n}\n\n/**\n * wiphy_ext_feature_isset - check the extended feature flag\n *\n * @wiphy: the wiphy to modify.\n * @ftidx: extended feature bit index.\n *\n * The extended features are flagged in multiple bytes (see\n * &struct wiphy.@ext_features)\n */\nstatic inline bool\nwiphy_ext_feature_isset(struct wiphy *wiphy,\n\t\t\tenum nl80211_ext_feature_index ftidx)\n{\n\tu8 ft_byte;\n\n\tft_byte = wiphy->ext_features[ftidx / 8];\n\treturn (ft_byte & BIT(ftidx % 8)) != 0;\n}\n\n/**\n * cfg80211_free_nan_func - free NAN function\n * @f: NAN function that should be freed\n *\n * Frees all the NAN function and all it's allocated members.\n */\nvoid cfg80211_free_nan_func(struct cfg80211_nan_func *f);\n\n/**\n * struct cfg80211_nan_match_params - NAN match parameters\n * @type: the type of the function that triggered a match. If it is\n *\t %NL80211_NAN_FUNC_SUBSCRIBE it means that we replied to a subscriber.\n *\t If it is %NL80211_NAN_FUNC_PUBLISH, it means that we got a discovery\n *\t result.\n *\t If it is %NL80211_NAN_FUNC_FOLLOW_UP, we received a follow up.\n * @inst_id: the local instance id\n * @peer_inst_id: the instance id of the peer's function\n * @addr: the MAC address of the peer\n * @info_len: the length of the &info\n * @info: the Service Specific Info from the peer (if any)\n * @cookie: unique identifier of the corresponding function\n */\nstruct cfg80211_nan_match_params {\n\tenum nl80211_nan_function_type type;\n\tu8 inst_id;\n\tu8 peer_inst_id;\n\tconst u8 *addr;\n\tu8 info_len;\n\tconst u8 *info;\n\tu64 cookie;\n};\n\n/**\n * cfg80211_nan_match - report a match for a NAN function.\n * @wdev: the wireless device reporting the match\n * @match: match notification parameters\n * @gfp: allocation flags\n *\n * This function reports that the a NAN function had a match. This\n * can be a subscribe that had a match or a solicited publish that\n * was sent. It can also be a follow up that was received.\n */\nvoid cfg80211_nan_match(struct wireless_dev *wdev,\n\t\t\tstruct cfg80211_nan_match_params *match, gfp_t gfp);\n\n/**\n * cfg80211_nan_func_terminated - notify about NAN function termination.\n *\n * @wdev: the wireless device reporting the match\n * @inst_id: the local instance id\n * @reason: termination reason (one of the NL80211_NAN_FUNC_TERM_REASON_*)\n * @cookie: unique NAN function identifier\n * @gfp: allocation flags\n *\n * This function reports that the a NAN function is terminated.\n */\nvoid cfg80211_nan_func_terminated(struct wireless_dev *wdev,\n\t\t\t\t  u8 inst_id,\n\t\t\t\t  enum nl80211_nan_func_term_reason reason,\n\t\t\t\t  u64 cookie, gfp_t gfp);\n\n/* ethtool helper */\nvoid cfg80211_get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *info);\n\n/**\n * cfg80211_external_auth_request - userspace request for authentication\n * @netdev: network device\n * @params: External authentication parameters\n * @gfp: allocation flags\n * Returns: 0 on success, < 0 on error\n */\nint cfg80211_external_auth_request(struct net_device *netdev,\n\t\t\t\t   struct cfg80211_external_auth_params *params,\n\t\t\t\t   gfp_t gfp);\n\n/**\n * cfg80211_pmsr_report - report peer measurement result data\n * @wdev: the wireless device reporting the measurement\n * @req: the original measurement request\n * @result: the result data\n * @gfp: allocation flags\n */\nvoid cfg80211_pmsr_report(struct wireless_dev *wdev,\n\t\t\t  struct cfg80211_pmsr_request *req,\n\t\t\t  struct cfg80211_pmsr_result *result,\n\t\t\t  gfp_t gfp);\n\n/**\n * cfg80211_pmsr_complete - report peer measurement completed\n * @wdev: the wireless device reporting the measurement\n * @req: the original measurement request\n * @gfp: allocation flags\n *\n * Report that the entire measurement completed, after this\n * the request pointer will no longer be valid.\n */\nvoid cfg80211_pmsr_complete(struct wireless_dev *wdev,\n\t\t\t    struct cfg80211_pmsr_request *req,\n\t\t\t    gfp_t gfp);\n\n/**\n * cfg80211_iftype_allowed - check whether the interface can be allowed\n * @wiphy: the wiphy\n * @iftype: interface type\n * @is_4addr: use_4addr flag, must be '0' when check_swif is '1'\n * @check_swif: check iftype against software interfaces\n *\n * Check whether the interface is allowed to operate; additionally, this API\n * can be used to check iftype against the software interfaces when\n * check_swif is '1'.\n */\nbool cfg80211_iftype_allowed(struct wiphy *wiphy, enum nl80211_iftype iftype,\n\t\t\t     bool is_4addr, u8 check_swif);\n\n\n/* Logging, debugging and troubleshooting/diagnostic helpers. */\n\n/* wiphy_printk helpers, similar to dev_printk */\n\n#define wiphy_printk(level, wiphy, format, args...)\t\t\\\n\tdev_printk(level, &(wiphy)->dev, format, ##args)\n#define wiphy_emerg(wiphy, format, args...)\t\t\t\\\n\tdev_emerg(&(wiphy)->dev, format, ##args)\n#define wiphy_alert(wiphy, format, args...)\t\t\t\\\n\tdev_alert(&(wiphy)->dev, format, ##args)\n#define wiphy_crit(wiphy, format, args...)\t\t\t\\\n\tdev_crit(&(wiphy)->dev, format, ##args)\n#define wiphy_err(wiphy, format, args...)\t\t\t\\\n\tdev_err(&(wiphy)->dev, format, ##args)\n#define wiphy_warn(wiphy, format, args...)\t\t\t\\\n\tdev_warn(&(wiphy)->dev, format, ##args)\n#define wiphy_notice(wiphy, format, args...)\t\t\t\\\n\tdev_notice(&(wiphy)->dev, format, ##args)\n#define wiphy_info(wiphy, format, args...)\t\t\t\\\n\tdev_info(&(wiphy)->dev, format, ##args)\n\n#define wiphy_err_ratelimited(wiphy, format, args...)\t\t\\\n\tdev_err_ratelimited(&(wiphy)->dev, format, ##args)\n#define wiphy_warn_ratelimited(wiphy, format, args...)\t\t\\\n\tdev_warn_ratelimited(&(wiphy)->dev, format, ##args)\n\n#define wiphy_debug(wiphy, format, args...)\t\t\t\\\n\twiphy_printk(KERN_DEBUG, wiphy, format, ##args)\n\n#define wiphy_dbg(wiphy, format, args...)\t\t\t\\\n\tdev_dbg(&(wiphy)->dev, format, ##args)\n\n#if defined(VERBOSE_DEBUG)\n#define wiphy_vdbg\twiphy_dbg\n#else\n#define wiphy_vdbg(wiphy, format, args...)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\t\\\n\t\twiphy_printk(KERN_DEBUG, wiphy, format, ##args);\t\\\n\t0;\t\t\t\t\t\t\t\t\\\n})\n#endif\n\n/*\n * wiphy_WARN() acts like wiphy_printk(), but with the key difference\n * of using a WARN/WARN_ON to get the message out, including the\n * file/line information and a backtrace.\n */\n#define wiphy_WARN(wiphy, format, args...)\t\t\t\\\n\tWARN(1, \"wiphy: %s\\n\" format, wiphy_name(wiphy), ##args);\n\n/**\n * cfg80211_update_owe_info_event - Notify the peer's OWE info to user space\n * @netdev: network device\n * @owe_info: peer's owe info\n * @gfp: allocation flags\n */\nvoid cfg80211_update_owe_info_event(struct net_device *netdev,\n\t\t\t\t    struct cfg80211_update_owe_info *owe_info,\n\t\t\t\t    gfp_t gfp);\n\n/**\n * cfg80211_bss_flush - resets all the scan entries\n * @wiphy: the wiphy\n */\nvoid cfg80211_bss_flush(struct wiphy *wiphy);\n\n#endif /* __NET_CFG80211_H */\n"}, "22": {"id": 22, "path": "/src/include/linux/slab.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n/*\n * Written by Mark Hemment, 1996 (markhe@nextd.demon.co.uk).\n *\n * (C) SGI 2006, Christoph Lameter\n * \tCleaned up and restructured to ease the addition of alternative\n * \timplementations of SLAB allocators.\n * (C) Linux Foundation 2008-2013\n *      Unified interface for all slab allocators\n */\n\n#ifndef _LINUX_SLAB_H\n#define\t_LINUX_SLAB_H\n\n#include <linux/gfp.h>\n#include <linux/overflow.h>\n#include <linux/types.h>\n#include <linux/workqueue.h>\n#include <linux/percpu-refcount.h>\n\n\n/*\n * Flags to pass to kmem_cache_create().\n * The ones marked DEBUG are only valid if CONFIG_DEBUG_SLAB is set.\n */\n/* DEBUG: Perform (expensive) checks on alloc/free */\n#define SLAB_CONSISTENCY_CHECKS\t((slab_flags_t __force)0x00000100U)\n/* DEBUG: Red zone objs in a cache */\n#define SLAB_RED_ZONE\t\t((slab_flags_t __force)0x00000400U)\n/* DEBUG: Poison objects */\n#define SLAB_POISON\t\t((slab_flags_t __force)0x00000800U)\n/* Align objs on cache lines */\n#define SLAB_HWCACHE_ALIGN\t((slab_flags_t __force)0x00002000U)\n/* Use GFP_DMA memory */\n#define SLAB_CACHE_DMA\t\t((slab_flags_t __force)0x00004000U)\n/* Use GFP_DMA32 memory */\n#define SLAB_CACHE_DMA32\t((slab_flags_t __force)0x00008000U)\n/* DEBUG: Store the last owner for bug hunting */\n#define SLAB_STORE_USER\t\t((slab_flags_t __force)0x00010000U)\n/* Panic if kmem_cache_create() fails */\n#define SLAB_PANIC\t\t((slab_flags_t __force)0x00040000U)\n/*\n * SLAB_TYPESAFE_BY_RCU - **WARNING** READ THIS!\n *\n * This delays freeing the SLAB page by a grace period, it does _NOT_\n * delay object freeing. This means that if you do kmem_cache_free()\n * that memory location is free to be reused at any time. Thus it may\n * be possible to see another object there in the same RCU grace period.\n *\n * This feature only ensures the memory location backing the object\n * stays valid, the trick to using this is relying on an independent\n * object validation pass. Something like:\n *\n *  rcu_read_lock()\n * again:\n *  obj = lockless_lookup(key);\n *  if (obj) {\n *    if (!try_get_ref(obj)) // might fail for free objects\n *      goto again;\n *\n *    if (obj->key != key) { // not the object we expected\n *      put_ref(obj);\n *      goto again;\n *    }\n *  }\n *  rcu_read_unlock();\n *\n * This is useful if we need to approach a kernel structure obliquely,\n * from its address obtained without the usual locking. We can lock\n * the structure to stabilize it and check it's still at the given address,\n * only if we can be sure that the memory has not been meanwhile reused\n * for some other kind of object (which our subsystem's lock might corrupt).\n *\n * rcu_read_lock before reading the address, then rcu_read_unlock after\n * taking the spinlock within the structure expected at that address.\n *\n * Note that SLAB_TYPESAFE_BY_RCU was originally named SLAB_DESTROY_BY_RCU.\n */\n/* Defer freeing slabs to RCU */\n#define SLAB_TYPESAFE_BY_RCU\t((slab_flags_t __force)0x00080000U)\n/* Spread some memory over cpuset */\n#define SLAB_MEM_SPREAD\t\t((slab_flags_t __force)0x00100000U)\n/* Trace allocations and frees */\n#define SLAB_TRACE\t\t((slab_flags_t __force)0x00200000U)\n\n/* Flag to prevent checks on free */\n#ifdef CONFIG_DEBUG_OBJECTS\n# define SLAB_DEBUG_OBJECTS\t((slab_flags_t __force)0x00400000U)\n#else\n# define SLAB_DEBUG_OBJECTS\t0\n#endif\n\n/* Avoid kmemleak tracing */\n#define SLAB_NOLEAKTRACE\t((slab_flags_t __force)0x00800000U)\n\n/* Fault injection mark */\n#ifdef CONFIG_FAILSLAB\n# define SLAB_FAILSLAB\t\t((slab_flags_t __force)0x02000000U)\n#else\n# define SLAB_FAILSLAB\t\t0\n#endif\n/* Account to memcg */\n#ifdef CONFIG_MEMCG_KMEM\n# define SLAB_ACCOUNT\t\t((slab_flags_t __force)0x04000000U)\n#else\n# define SLAB_ACCOUNT\t\t0\n#endif\n\n#ifdef CONFIG_KASAN\n#define SLAB_KASAN\t\t((slab_flags_t __force)0x08000000U)\n#else\n#define SLAB_KASAN\t\t0\n#endif\n\n/* The following flags affect the page allocator grouping pages by mobility */\n/* Objects are reclaimable */\n#define SLAB_RECLAIM_ACCOUNT\t((slab_flags_t __force)0x00020000U)\n#define SLAB_TEMPORARY\t\tSLAB_RECLAIM_ACCOUNT\t/* Objects are short-lived */\n\n/* Slab deactivation flag */\n#define SLAB_DEACTIVATED\t((slab_flags_t __force)0x10000000U)\n\n/*\n * ZERO_SIZE_PTR will be returned for zero sized kmalloc requests.\n *\n * Dereferencing ZERO_SIZE_PTR will lead to a distinct access fault.\n *\n * ZERO_SIZE_PTR can be passed to kfree though in the same way that NULL can.\n * Both make kfree a no-op.\n */\n#define ZERO_SIZE_PTR ((void *)16)\n\n#define ZERO_OR_NULL_PTR(x) ((unsigned long)(x) <= \\\n\t\t\t\t(unsigned long)ZERO_SIZE_PTR)\n\n#include <linux/kasan.h>\n\nstruct mem_cgroup;\n/*\n * struct kmem_cache related prototypes\n */\nvoid __init kmem_cache_init(void);\nbool slab_is_available(void);\n\nextern bool usercopy_fallback;\n\nstruct kmem_cache *kmem_cache_create(const char *name, unsigned int size,\n\t\t\tunsigned int align, slab_flags_t flags,\n\t\t\tvoid (*ctor)(void *));\nstruct kmem_cache *kmem_cache_create_usercopy(const char *name,\n\t\t\tunsigned int size, unsigned int align,\n\t\t\tslab_flags_t flags,\n\t\t\tunsigned int useroffset, unsigned int usersize,\n\t\t\tvoid (*ctor)(void *));\nvoid kmem_cache_destroy(struct kmem_cache *);\nint kmem_cache_shrink(struct kmem_cache *);\n\n/*\n * Please use this macro to create slab caches. Simply specify the\n * name of the structure and maybe some flags that are listed above.\n *\n * The alignment of the struct determines object alignment. If you\n * f.e. add ____cacheline_aligned_in_smp to the struct declaration\n * then the objects will be properly aligned in SMP configurations.\n */\n#define KMEM_CACHE(__struct, __flags)\t\t\t\t\t\\\n\t\tkmem_cache_create(#__struct, sizeof(struct __struct),\t\\\n\t\t\t__alignof__(struct __struct), (__flags), NULL)\n\n/*\n * To whitelist a single field for copying to/from usercopy, use this\n * macro instead for KMEM_CACHE() above.\n */\n#define KMEM_CACHE_USERCOPY(__struct, __flags, __field)\t\t\t\\\n\t\tkmem_cache_create_usercopy(#__struct,\t\t\t\\\n\t\t\tsizeof(struct __struct),\t\t\t\\\n\t\t\t__alignof__(struct __struct), (__flags),\t\\\n\t\t\toffsetof(struct __struct, __field),\t\t\\\n\t\t\tsizeof_field(struct __struct, __field), NULL)\n\n/*\n * Common kmalloc functions provided by all allocators\n */\nvoid * __must_check krealloc(const void *, size_t, gfp_t);\nvoid kfree(const void *);\nvoid kfree_sensitive(const void *);\nsize_t __ksize(const void *);\nsize_t ksize(const void *);\nbool kmem_valid_obj(void *object);\nvoid kmem_dump_obj(void *object);\n\n#ifdef CONFIG_HAVE_HARDENED_USERCOPY_ALLOCATOR\nvoid __check_heap_object(const void *ptr, unsigned long n, struct page *page,\n\t\t\tbool to_user);\n#else\nstatic inline void __check_heap_object(const void *ptr, unsigned long n,\n\t\t\t\t       struct page *page, bool to_user) { }\n#endif\n\n/*\n * Some archs want to perform DMA into kmalloc caches and need a guaranteed\n * alignment larger than the alignment of a 64-bit integer.\n * Setting ARCH_KMALLOC_MINALIGN in arch headers allows that.\n */\n#if defined(ARCH_DMA_MINALIGN) && ARCH_DMA_MINALIGN > 8\n#define ARCH_KMALLOC_MINALIGN ARCH_DMA_MINALIGN\n#define KMALLOC_MIN_SIZE ARCH_DMA_MINALIGN\n#define KMALLOC_SHIFT_LOW ilog2(ARCH_DMA_MINALIGN)\n#else\n#define ARCH_KMALLOC_MINALIGN __alignof__(unsigned long long)\n#endif\n\n/*\n * Setting ARCH_SLAB_MINALIGN in arch headers allows a different alignment.\n * Intended for arches that get misalignment faults even for 64 bit integer\n * aligned buffers.\n */\n#ifndef ARCH_SLAB_MINALIGN\n#define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)\n#endif\n\n/*\n * kmalloc and friends return ARCH_KMALLOC_MINALIGN aligned\n * pointers. kmem_cache_alloc and friends return ARCH_SLAB_MINALIGN\n * aligned pointers.\n */\n#define __assume_kmalloc_alignment __assume_aligned(ARCH_KMALLOC_MINALIGN)\n#define __assume_slab_alignment __assume_aligned(ARCH_SLAB_MINALIGN)\n#define __assume_page_alignment __assume_aligned(PAGE_SIZE)\n\n/*\n * Kmalloc array related definitions\n */\n\n#ifdef CONFIG_SLAB\n/*\n * The largest kmalloc size supported by the SLAB allocators is\n * 32 megabyte (2^25) or the maximum allocatable page order if that is\n * less than 32 MB.\n *\n * WARNING: Its not easy to increase this value since the allocators have\n * to do various tricks to work around compiler limitations in order to\n * ensure proper constant folding.\n */\n#define KMALLOC_SHIFT_HIGH\t((MAX_ORDER + PAGE_SHIFT - 1) <= 25 ? \\\n\t\t\t\t(MAX_ORDER + PAGE_SHIFT - 1) : 25)\n#define KMALLOC_SHIFT_MAX\tKMALLOC_SHIFT_HIGH\n#ifndef KMALLOC_SHIFT_LOW\n#define KMALLOC_SHIFT_LOW\t5\n#endif\n#endif\n\n#ifdef CONFIG_SLUB\n/*\n * SLUB directly allocates requests fitting in to an order-1 page\n * (PAGE_SIZE*2).  Larger requests are passed to the page allocator.\n */\n#define KMALLOC_SHIFT_HIGH\t(PAGE_SHIFT + 1)\n#define KMALLOC_SHIFT_MAX\t(MAX_ORDER + PAGE_SHIFT - 1)\n#ifndef KMALLOC_SHIFT_LOW\n#define KMALLOC_SHIFT_LOW\t3\n#endif\n#endif\n\n#ifdef CONFIG_SLOB\n/*\n * SLOB passes all requests larger than one page to the page allocator.\n * No kmalloc array is necessary since objects of different sizes can\n * be allocated from the same page.\n */\n#define KMALLOC_SHIFT_HIGH\tPAGE_SHIFT\n#define KMALLOC_SHIFT_MAX\t(MAX_ORDER + PAGE_SHIFT - 1)\n#ifndef KMALLOC_SHIFT_LOW\n#define KMALLOC_SHIFT_LOW\t3\n#endif\n#endif\n\n/* Maximum allocatable size */\n#define KMALLOC_MAX_SIZE\t(1UL << KMALLOC_SHIFT_MAX)\n/* Maximum size for which we actually use a slab cache */\n#define KMALLOC_MAX_CACHE_SIZE\t(1UL << KMALLOC_SHIFT_HIGH)\n/* Maximum order allocatable via the slab allocator */\n#define KMALLOC_MAX_ORDER\t(KMALLOC_SHIFT_MAX - PAGE_SHIFT)\n\n/*\n * Kmalloc subsystem.\n */\n#ifndef KMALLOC_MIN_SIZE\n#define KMALLOC_MIN_SIZE (1 << KMALLOC_SHIFT_LOW)\n#endif\n\n/*\n * This restriction comes from byte sized index implementation.\n * Page size is normally 2^12 bytes and, in this case, if we want to use\n * byte sized index which can represent 2^8 entries, the size of the object\n * should be equal or greater to 2^12 / 2^8 = 2^4 = 16.\n * If minimum size of kmalloc is less than 16, we use it as minimum object\n * size and give up to use byte sized index.\n */\n#define SLAB_OBJ_MIN_SIZE      (KMALLOC_MIN_SIZE < 16 ? \\\n                               (KMALLOC_MIN_SIZE) : 16)\n\n/*\n * Whenever changing this, take care of that kmalloc_type() and\n * create_kmalloc_caches() still work as intended.\n */\nenum kmalloc_cache_type {\n\tKMALLOC_NORMAL = 0,\n\tKMALLOC_RECLAIM,\n#ifdef CONFIG_ZONE_DMA\n\tKMALLOC_DMA,\n#endif\n\tNR_KMALLOC_TYPES\n};\n\n#ifndef CONFIG_SLOB\nextern struct kmem_cache *\nkmalloc_caches[NR_KMALLOC_TYPES][KMALLOC_SHIFT_HIGH + 1];\n\nstatic __always_inline enum kmalloc_cache_type kmalloc_type(gfp_t flags)\n{\n#ifdef CONFIG_ZONE_DMA\n\t/*\n\t * The most common case is KMALLOC_NORMAL, so test for it\n\t * with a single branch for both flags.\n\t */\n\tif (likely((flags & (__GFP_DMA | __GFP_RECLAIMABLE)) == 0))\n\t\treturn KMALLOC_NORMAL;\n\n\t/*\n\t * At least one of the flags has to be set. If both are, __GFP_DMA\n\t * is more important.\n\t */\n\treturn flags & __GFP_DMA ? KMALLOC_DMA : KMALLOC_RECLAIM;\n#else\n\treturn flags & __GFP_RECLAIMABLE ? KMALLOC_RECLAIM : KMALLOC_NORMAL;\n#endif\n}\n\n/*\n * Figure out which kmalloc slab an allocation of a certain size\n * belongs to.\n * 0 = zero alloc\n * 1 =  65 .. 96 bytes\n * 2 = 129 .. 192 bytes\n * n = 2^(n-1)+1 .. 2^n\n */\nstatic __always_inline unsigned int kmalloc_index(size_t size)\n{\n\tif (!size)\n\t\treturn 0;\n\n\tif (size <= KMALLOC_MIN_SIZE)\n\t\treturn KMALLOC_SHIFT_LOW;\n\n\tif (KMALLOC_MIN_SIZE <= 32 && size > 64 && size <= 96)\n\t\treturn 1;\n\tif (KMALLOC_MIN_SIZE <= 64 && size > 128 && size <= 192)\n\t\treturn 2;\n\tif (size <=          8) return 3;\n\tif (size <=         16) return 4;\n\tif (size <=         32) return 5;\n\tif (size <=         64) return 6;\n\tif (size <=        128) return 7;\n\tif (size <=        256) return 8;\n\tif (size <=        512) return 9;\n\tif (size <=       1024) return 10;\n\tif (size <=   2 * 1024) return 11;\n\tif (size <=   4 * 1024) return 12;\n\tif (size <=   8 * 1024) return 13;\n\tif (size <=  16 * 1024) return 14;\n\tif (size <=  32 * 1024) return 15;\n\tif (size <=  64 * 1024) return 16;\n\tif (size <= 128 * 1024) return 17;\n\tif (size <= 256 * 1024) return 18;\n\tif (size <= 512 * 1024) return 19;\n\tif (size <= 1024 * 1024) return 20;\n\tif (size <=  2 * 1024 * 1024) return 21;\n\tif (size <=  4 * 1024 * 1024) return 22;\n\tif (size <=  8 * 1024 * 1024) return 23;\n\tif (size <=  16 * 1024 * 1024) return 24;\n\tif (size <=  32 * 1024 * 1024) return 25;\n\tif (size <=  64 * 1024 * 1024) return 26;\n\tBUG();\n\n\t/* Will never be reached. Needed because the compiler may complain */\n\treturn -1;\n}\n#endif /* !CONFIG_SLOB */\n\nvoid *__kmalloc(size_t size, gfp_t flags) __assume_kmalloc_alignment __malloc;\nvoid *kmem_cache_alloc(struct kmem_cache *, gfp_t flags) __assume_slab_alignment __malloc;\nvoid kmem_cache_free(struct kmem_cache *, void *);\n\n/*\n * Bulk allocation and freeing operations. These are accelerated in an\n * allocator specific way to avoid taking locks repeatedly or building\n * metadata structures unnecessarily.\n *\n * Note that interrupts must be enabled when calling these functions.\n */\nvoid kmem_cache_free_bulk(struct kmem_cache *, size_t, void **);\nint kmem_cache_alloc_bulk(struct kmem_cache *, gfp_t, size_t, void **);\n\n/*\n * Caller must not use kfree_bulk() on memory not originally allocated\n * by kmalloc(), because the SLOB allocator cannot handle this.\n */\nstatic __always_inline void kfree_bulk(size_t size, void **p)\n{\n\tkmem_cache_free_bulk(NULL, size, p);\n}\n\n#ifdef CONFIG_NUMA\nvoid *__kmalloc_node(size_t size, gfp_t flags, int node) __assume_kmalloc_alignment __malloc;\nvoid *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node) __assume_slab_alignment __malloc;\n#else\nstatic __always_inline void *__kmalloc_node(size_t size, gfp_t flags, int node)\n{\n\treturn __kmalloc(size, flags);\n}\n\nstatic __always_inline void *kmem_cache_alloc_node(struct kmem_cache *s, gfp_t flags, int node)\n{\n\treturn kmem_cache_alloc(s, flags);\n}\n#endif\n\n#ifdef CONFIG_TRACING\nextern void *kmem_cache_alloc_trace(struct kmem_cache *, gfp_t, size_t) __assume_slab_alignment __malloc;\n\n#ifdef CONFIG_NUMA\nextern void *kmem_cache_alloc_node_trace(struct kmem_cache *s,\n\t\t\t\t\t   gfp_t gfpflags,\n\t\t\t\t\t   int node, size_t size) __assume_slab_alignment __malloc;\n#else\nstatic __always_inline void *\nkmem_cache_alloc_node_trace(struct kmem_cache *s,\n\t\t\t      gfp_t gfpflags,\n\t\t\t      int node, size_t size)\n{\n\treturn kmem_cache_alloc_trace(s, gfpflags, size);\n}\n#endif /* CONFIG_NUMA */\n\n#else /* CONFIG_TRACING */\nstatic __always_inline void *kmem_cache_alloc_trace(struct kmem_cache *s,\n\t\tgfp_t flags, size_t size)\n{\n\tvoid *ret = kmem_cache_alloc(s, flags);\n\n\tret = kasan_kmalloc(s, ret, size, flags);\n\treturn ret;\n}\n\nstatic __always_inline void *\nkmem_cache_alloc_node_trace(struct kmem_cache *s,\n\t\t\t      gfp_t gfpflags,\n\t\t\t      int node, size_t size)\n{\n\tvoid *ret = kmem_cache_alloc_node(s, gfpflags, node);\n\n\tret = kasan_kmalloc(s, ret, size, gfpflags);\n\treturn ret;\n}\n#endif /* CONFIG_TRACING */\n\nextern void *kmalloc_order(size_t size, gfp_t flags, unsigned int order) __assume_page_alignment __malloc;\n\n#ifdef CONFIG_TRACING\nextern void *kmalloc_order_trace(size_t size, gfp_t flags, unsigned int order) __assume_page_alignment __malloc;\n#else\nstatic __always_inline void *\nkmalloc_order_trace(size_t size, gfp_t flags, unsigned int order)\n{\n\treturn kmalloc_order(size, flags, order);\n}\n#endif\n\nstatic __always_inline void *kmalloc_large(size_t size, gfp_t flags)\n{\n\tunsigned int order = get_order(size);\n\treturn kmalloc_order_trace(size, flags, order);\n}\n\n/**\n * kmalloc - allocate memory\n * @size: how many bytes of memory are required.\n * @flags: the type of memory to allocate.\n *\n * kmalloc is the normal method of allocating memory\n * for objects smaller than page size in the kernel.\n *\n * The allocated object address is aligned to at least ARCH_KMALLOC_MINALIGN\n * bytes. For @size of power of two bytes, the alignment is also guaranteed\n * to be at least to the size.\n *\n * The @flags argument may be one of the GFP flags defined at\n * include/linux/gfp.h and described at\n * :ref:`Documentation/core-api/mm-api.rst <mm-api-gfp-flags>`\n *\n * The recommended usage of the @flags is described at\n * :ref:`Documentation/core-api/memory-allocation.rst <memory_allocation>`\n *\n * Below is a brief outline of the most useful GFP flags\n *\n * %GFP_KERNEL\n *\tAllocate normal kernel ram. May sleep.\n *\n * %GFP_NOWAIT\n *\tAllocation will not sleep.\n *\n * %GFP_ATOMIC\n *\tAllocation will not sleep.  May use emergency pools.\n *\n * %GFP_HIGHUSER\n *\tAllocate memory from high memory on behalf of user.\n *\n * Also it is possible to set different flags by OR'ing\n * in one or more of the following additional @flags:\n *\n * %__GFP_HIGH\n *\tThis allocation has high priority and may use emergency pools.\n *\n * %__GFP_NOFAIL\n *\tIndicate that this allocation is in no way allowed to fail\n *\t(think twice before using).\n *\n * %__GFP_NORETRY\n *\tIf memory is not immediately available,\n *\tthen give up at once.\n *\n * %__GFP_NOWARN\n *\tIf allocation fails, don't issue any warnings.\n *\n * %__GFP_RETRY_MAYFAIL\n *\tTry really hard to succeed the allocation but fail\n *\teventually.\n */\nstatic __always_inline void *kmalloc(size_t size, gfp_t flags)\n{\n\tif (__builtin_constant_p(size)) {\n#ifndef CONFIG_SLOB\n\t\tunsigned int index;\n#endif\n\t\tif (size > KMALLOC_MAX_CACHE_SIZE)\n\t\t\treturn kmalloc_large(size, flags);\n#ifndef CONFIG_SLOB\n\t\tindex = kmalloc_index(size);\n\n\t\tif (!index)\n\t\t\treturn ZERO_SIZE_PTR;\n\n\t\treturn kmem_cache_alloc_trace(\n\t\t\t\tkmalloc_caches[kmalloc_type(flags)][index],\n\t\t\t\tflags, size);\n#endif\n\t}\n\treturn __kmalloc(size, flags);\n}\n\nstatic __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)\n{\n#ifndef CONFIG_SLOB\n\tif (__builtin_constant_p(size) &&\n\t\tsize <= KMALLOC_MAX_CACHE_SIZE) {\n\t\tunsigned int i = kmalloc_index(size);\n\n\t\tif (!i)\n\t\t\treturn ZERO_SIZE_PTR;\n\n\t\treturn kmem_cache_alloc_node_trace(\n\t\t\t\tkmalloc_caches[kmalloc_type(flags)][i],\n\t\t\t\t\t\tflags, node, size);\n\t}\n#endif\n\treturn __kmalloc_node(size, flags, node);\n}\n\n/**\n * kmalloc_array - allocate memory for an array.\n * @n: number of elements.\n * @size: element size.\n * @flags: the type of memory to allocate (see kmalloc).\n */\nstatic inline void *kmalloc_array(size_t n, size_t size, gfp_t flags)\n{\n\tsize_t bytes;\n\n\tif (unlikely(check_mul_overflow(n, size, &bytes)))\n\t\treturn NULL;\n\tif (__builtin_constant_p(n) && __builtin_constant_p(size))\n\t\treturn kmalloc(bytes, flags);\n\treturn __kmalloc(bytes, flags);\n}\n\n/**\n * krealloc_array - reallocate memory for an array.\n * @p: pointer to the memory chunk to reallocate\n * @new_n: new number of elements to alloc\n * @new_size: new size of a single member of the array\n * @flags: the type of memory to allocate (see kmalloc)\n */\nstatic __must_check inline void *\nkrealloc_array(void *p, size_t new_n, size_t new_size, gfp_t flags)\n{\n\tsize_t bytes;\n\n\tif (unlikely(check_mul_overflow(new_n, new_size, &bytes)))\n\t\treturn NULL;\n\n\treturn krealloc(p, bytes, flags);\n}\n\n/**\n * kcalloc - allocate memory for an array. The memory is set to zero.\n * @n: number of elements.\n * @size: element size.\n * @flags: the type of memory to allocate (see kmalloc).\n */\nstatic inline void *kcalloc(size_t n, size_t size, gfp_t flags)\n{\n\treturn kmalloc_array(n, size, flags | __GFP_ZERO);\n}\n\n/*\n * kmalloc_track_caller is a special version of kmalloc that records the\n * calling function of the routine calling it for slab leak tracking instead\n * of just the calling function (confusing, eh?).\n * It's useful when the call to kmalloc comes from a widely-used standard\n * allocator where we care about the real place the memory allocation\n * request comes from.\n */\nextern void *__kmalloc_track_caller(size_t, gfp_t, unsigned long);\n#define kmalloc_track_caller(size, flags) \\\n\t__kmalloc_track_caller(size, flags, _RET_IP_)\n\nstatic inline void *kmalloc_array_node(size_t n, size_t size, gfp_t flags,\n\t\t\t\t       int node)\n{\n\tsize_t bytes;\n\n\tif (unlikely(check_mul_overflow(n, size, &bytes)))\n\t\treturn NULL;\n\tif (__builtin_constant_p(n) && __builtin_constant_p(size))\n\t\treturn kmalloc_node(bytes, flags, node);\n\treturn __kmalloc_node(bytes, flags, node);\n}\n\nstatic inline void *kcalloc_node(size_t n, size_t size, gfp_t flags, int node)\n{\n\treturn kmalloc_array_node(n, size, flags | __GFP_ZERO, node);\n}\n\n\n#ifdef CONFIG_NUMA\nextern void *__kmalloc_node_track_caller(size_t, gfp_t, int, unsigned long);\n#define kmalloc_node_track_caller(size, flags, node) \\\n\t__kmalloc_node_track_caller(size, flags, node, \\\n\t\t\t_RET_IP_)\n\n#else /* CONFIG_NUMA */\n\n#define kmalloc_node_track_caller(size, flags, node) \\\n\tkmalloc_track_caller(size, flags)\n\n#endif /* CONFIG_NUMA */\n\n/*\n * Shortcuts\n */\nstatic inline void *kmem_cache_zalloc(struct kmem_cache *k, gfp_t flags)\n{\n\treturn kmem_cache_alloc(k, flags | __GFP_ZERO);\n}\n\n/**\n * kzalloc - allocate memory. The memory is set to zero.\n * @size: how many bytes of memory are required.\n * @flags: the type of memory to allocate (see kmalloc).\n */\nstatic inline void *kzalloc(size_t size, gfp_t flags)\n{\n\treturn kmalloc(size, flags | __GFP_ZERO);\n}\n\n/**\n * kzalloc_node - allocate zeroed memory from a particular memory node.\n * @size: how many bytes of memory are required.\n * @flags: the type of memory to allocate (see kmalloc).\n * @node: memory node from which to allocate\n */\nstatic inline void *kzalloc_node(size_t size, gfp_t flags, int node)\n{\n\treturn kmalloc_node(size, flags | __GFP_ZERO, node);\n}\n\nunsigned int kmem_cache_size(struct kmem_cache *s);\nvoid __init kmem_cache_init_late(void);\n\n#if defined(CONFIG_SMP) && defined(CONFIG_SLAB)\nint slab_prepare_cpu(unsigned int cpu);\nint slab_dead_cpu(unsigned int cpu);\n#else\n#define slab_prepare_cpu\tNULL\n#define slab_dead_cpu\t\tNULL\n#endif\n\n#endif\t/* _LINUX_SLAB_H */\n"}, "23": {"id": 23, "path": "/src/include/net/netlink.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __NET_NETLINK_H\n#define __NET_NETLINK_H\n\n#include <linux/types.h>\n#include <linux/netlink.h>\n#include <linux/jiffies.h>\n#include <linux/in6.h>\n\n/* ========================================================================\n *         Netlink Messages and Attributes Interface (As Seen On TV)\n * ------------------------------------------------------------------------\n *                          Messages Interface\n * ------------------------------------------------------------------------\n *\n * Message Format:\n *    <--- nlmsg_total_size(payload)  --->\n *    <-- nlmsg_msg_size(payload) ->\n *   +----------+- - -+-------------+- - -+-------- - -\n *   | nlmsghdr | Pad |   Payload   | Pad | nlmsghdr\n *   +----------+- - -+-------------+- - -+-------- - -\n *   nlmsg_data(nlh)---^                   ^\n *   nlmsg_next(nlh)-----------------------+\n *\n * Payload Format:\n *    <---------------------- nlmsg_len(nlh) --------------------->\n *    <------ hdrlen ------>       <- nlmsg_attrlen(nlh, hdrlen) ->\n *   +----------------------+- - -+--------------------------------+\n *   |     Family Header    | Pad |           Attributes           |\n *   +----------------------+- - -+--------------------------------+\n *   nlmsg_attrdata(nlh, hdrlen)---^\n *\n * Data Structures:\n *   struct nlmsghdr\t\t\tnetlink message header\n *\n * Message Construction:\n *   nlmsg_new()\t\t\tcreate a new netlink message\n *   nlmsg_put()\t\t\tadd a netlink message to an skb\n *   nlmsg_put_answer()\t\t\tcallback based nlmsg_put()\n *   nlmsg_end()\t\t\tfinalize netlink message\n *   nlmsg_get_pos()\t\t\treturn current position in message\n *   nlmsg_trim()\t\t\ttrim part of message\n *   nlmsg_cancel()\t\t\tcancel message construction\n *   nlmsg_free()\t\t\tfree a netlink message\n *\n * Message Sending:\n *   nlmsg_multicast()\t\t\tmulticast message to several groups\n *   nlmsg_unicast()\t\t\tunicast a message to a single socket\n *   nlmsg_notify()\t\t\tsend notification message\n *\n * Message Length Calculations:\n *   nlmsg_msg_size(payload)\t\tlength of message w/o padding\n *   nlmsg_total_size(payload)\t\tlength of message w/ padding\n *   nlmsg_padlen(payload)\t\tlength of padding at tail\n *\n * Message Payload Access:\n *   nlmsg_data(nlh)\t\t\thead of message payload\n *   nlmsg_len(nlh)\t\t\tlength of message payload\n *   nlmsg_attrdata(nlh, hdrlen)\thead of attributes data\n *   nlmsg_attrlen(nlh, hdrlen)\t\tlength of attributes data\n *\n * Message Parsing:\n *   nlmsg_ok(nlh, remaining)\t\tdoes nlh fit into remaining bytes?\n *   nlmsg_next(nlh, remaining)\t\tget next netlink message\n *   nlmsg_parse()\t\t\tparse attributes of a message\n *   nlmsg_find_attr()\t\t\tfind an attribute in a message\n *   nlmsg_for_each_msg()\t\tloop over all messages\n *   nlmsg_validate()\t\t\tvalidate netlink message incl. attrs\n *   nlmsg_for_each_attr()\t\tloop over all attributes\n *\n * Misc:\n *   nlmsg_report()\t\t\treport back to application?\n *\n * ------------------------------------------------------------------------\n *                          Attributes Interface\n * ------------------------------------------------------------------------\n *\n * Attribute Format:\n *    <------- nla_total_size(payload) ------->\n *    <---- nla_attr_size(payload) ----->\n *   +----------+- - -+- - - - - - - - - +- - -+-------- - -\n *   |  Header  | Pad |     Payload      | Pad |  Header\n *   +----------+- - -+- - - - - - - - - +- - -+-------- - -\n *                     <- nla_len(nla) ->      ^\n *   nla_data(nla)----^                        |\n *   nla_next(nla)-----------------------------'\n *\n * Data Structures:\n *   struct nlattr\t\t\tnetlink attribute header\n *\n * Attribute Construction:\n *   nla_reserve(skb, type, len)\treserve room for an attribute\n *   nla_reserve_nohdr(skb, len)\treserve room for an attribute w/o hdr\n *   nla_put(skb, type, len, data)\tadd attribute to skb\n *   nla_put_nohdr(skb, len, data)\tadd attribute w/o hdr\n *   nla_append(skb, len, data)\t\tappend data to skb\n *\n * Attribute Construction for Basic Types:\n *   nla_put_u8(skb, type, value)\tadd u8 attribute to skb\n *   nla_put_u16(skb, type, value)\tadd u16 attribute to skb\n *   nla_put_u32(skb, type, value)\tadd u32 attribute to skb\n *   nla_put_u64_64bit(skb, type,\n *                     value, padattr)\tadd u64 attribute to skb\n *   nla_put_s8(skb, type, value)\tadd s8 attribute to skb\n *   nla_put_s16(skb, type, value)\tadd s16 attribute to skb\n *   nla_put_s32(skb, type, value)\tadd s32 attribute to skb\n *   nla_put_s64(skb, type, value,\n *               padattr)\t\tadd s64 attribute to skb\n *   nla_put_string(skb, type, str)\tadd string attribute to skb\n *   nla_put_flag(skb, type)\t\tadd flag attribute to skb\n *   nla_put_msecs(skb, type, jiffies,\n *                 padattr)\t\tadd msecs attribute to skb\n *   nla_put_in_addr(skb, type, addr)\tadd IPv4 address attribute to skb\n *   nla_put_in6_addr(skb, type, addr)\tadd IPv6 address attribute to skb\n *\n * Nested Attributes Construction:\n *   nla_nest_start(skb, type)\t\tstart a nested attribute\n *   nla_nest_end(skb, nla)\t\tfinalize a nested attribute\n *   nla_nest_cancel(skb, nla)\t\tcancel nested attribute construction\n *\n * Attribute Length Calculations:\n *   nla_attr_size(payload)\t\tlength of attribute w/o padding\n *   nla_total_size(payload)\t\tlength of attribute w/ padding\n *   nla_padlen(payload)\t\tlength of padding\n *\n * Attribute Payload Access:\n *   nla_data(nla)\t\t\thead of attribute payload\n *   nla_len(nla)\t\t\tlength of attribute payload\n *\n * Attribute Payload Access for Basic Types:\n *   nla_get_u8(nla)\t\t\tget payload for a u8 attribute\n *   nla_get_u16(nla)\t\t\tget payload for a u16 attribute\n *   nla_get_u32(nla)\t\t\tget payload for a u32 attribute\n *   nla_get_u64(nla)\t\t\tget payload for a u64 attribute\n *   nla_get_s8(nla)\t\t\tget payload for a s8 attribute\n *   nla_get_s16(nla)\t\t\tget payload for a s16 attribute\n *   nla_get_s32(nla)\t\t\tget payload for a s32 attribute\n *   nla_get_s64(nla)\t\t\tget payload for a s64 attribute\n *   nla_get_flag(nla)\t\t\treturn 1 if flag is true\n *   nla_get_msecs(nla)\t\t\tget payload for a msecs attribute\n *\n * Attribute Misc:\n *   nla_memcpy(dest, nla, count)\tcopy attribute into memory\n *   nla_memcmp(nla, data, size)\tcompare attribute with memory area\n *   nla_strscpy(dst, nla, size)\tcopy attribute to a sized string\n *   nla_strcmp(nla, str)\t\tcompare attribute with string\n *\n * Attribute Parsing:\n *   nla_ok(nla, remaining)\t\tdoes nla fit into remaining bytes?\n *   nla_next(nla, remaining)\t\tget next netlink attribute\n *   nla_validate()\t\t\tvalidate a stream of attributes\n *   nla_validate_nested()\t\tvalidate a stream of nested attributes\n *   nla_find()\t\t\t\tfind attribute in stream of attributes\n *   nla_find_nested()\t\t\tfind attribute in nested attributes\n *   nla_parse()\t\t\tparse and validate stream of attrs\n *   nla_parse_nested()\t\t\tparse nested attributes\n *   nla_for_each_attr()\t\tloop over all attributes\n *   nla_for_each_nested()\t\tloop over the nested attributes\n *=========================================================================\n */\n\n /**\n  * Standard attribute types to specify validation policy\n  */\nenum {\n\tNLA_UNSPEC,\n\tNLA_U8,\n\tNLA_U16,\n\tNLA_U32,\n\tNLA_U64,\n\tNLA_STRING,\n\tNLA_FLAG,\n\tNLA_MSECS,\n\tNLA_NESTED,\n\tNLA_NESTED_ARRAY,\n\tNLA_NUL_STRING,\n\tNLA_BINARY,\n\tNLA_S8,\n\tNLA_S16,\n\tNLA_S32,\n\tNLA_S64,\n\tNLA_BITFIELD32,\n\tNLA_REJECT,\n\t__NLA_TYPE_MAX,\n};\n\n#define NLA_TYPE_MAX (__NLA_TYPE_MAX - 1)\n\nstruct netlink_range_validation {\n\tu64 min, max;\n};\n\nstruct netlink_range_validation_signed {\n\ts64 min, max;\n};\n\nenum nla_policy_validation {\n\tNLA_VALIDATE_NONE,\n\tNLA_VALIDATE_RANGE,\n\tNLA_VALIDATE_RANGE_WARN_TOO_LONG,\n\tNLA_VALIDATE_MIN,\n\tNLA_VALIDATE_MAX,\n\tNLA_VALIDATE_MASK,\n\tNLA_VALIDATE_RANGE_PTR,\n\tNLA_VALIDATE_FUNCTION,\n};\n\n/**\n * struct nla_policy - attribute validation policy\n * @type: Type of attribute or NLA_UNSPEC\n * @validation_type: type of attribute validation done in addition to\n *\ttype-specific validation (e.g. range, function call), see\n *\t&enum nla_policy_validation\n * @len: Type specific length of payload\n *\n * Policies are defined as arrays of this struct, the array must be\n * accessible by attribute type up to the highest identifier to be expected.\n *\n * Meaning of `len' field:\n *    NLA_STRING           Maximum length of string\n *    NLA_NUL_STRING       Maximum length of string (excluding NUL)\n *    NLA_FLAG             Unused\n *    NLA_BINARY           Maximum length of attribute payload\n *                         (but see also below with the validation type)\n *    NLA_NESTED,\n *    NLA_NESTED_ARRAY     Length verification is done by checking len of\n *                         nested header (or empty); len field is used if\n *                         nested_policy is also used, for the max attr\n *                         number in the nested policy.\n *    NLA_U8, NLA_U16,\n *    NLA_U32, NLA_U64,\n *    NLA_S8, NLA_S16,\n *    NLA_S32, NLA_S64,\n *    NLA_MSECS            Leaving the length field zero will verify the\n *                         given type fits, using it verifies minimum length\n *                         just like \"All other\"\n *    NLA_BITFIELD32       Unused\n *    NLA_REJECT           Unused\n *    All other            Minimum length of attribute payload\n *\n * Meaning of validation union:\n *    NLA_BITFIELD32       This is a 32-bit bitmap/bitselector attribute and\n *                         `bitfield32_valid' is the u32 value of valid flags\n *    NLA_REJECT           This attribute is always rejected and `reject_message'\n *                         may point to a string to report as the error instead\n *                         of the generic one in extended ACK.\n *    NLA_NESTED           `nested_policy' to a nested policy to validate, must\n *                         also set `len' to the max attribute number. Use the\n *                         provided NLA_POLICY_NESTED() macro.\n *                         Note that nla_parse() will validate, but of course not\n *                         parse, the nested sub-policies.\n *    NLA_NESTED_ARRAY     `nested_policy' points to a nested policy to validate,\n *                         must also set `len' to the max attribute number. Use\n *                         the provided NLA_POLICY_NESTED_ARRAY() macro.\n *                         The difference to NLA_NESTED is the structure:\n *                         NLA_NESTED has the nested attributes directly inside\n *                         while an array has the nested attributes at another\n *                         level down and the attribute types directly in the\n *                         nesting don't matter.\n *    NLA_U8,\n *    NLA_U16,\n *    NLA_U32,\n *    NLA_U64,\n *    NLA_S8,\n *    NLA_S16,\n *    NLA_S32,\n *    NLA_S64              The `min' and `max' fields are used depending on the\n *                         validation_type field, if that is min/max/range then\n *                         the min, max or both are used (respectively) to check\n *                         the value of the integer attribute.\n *                         Note that in the interest of code simplicity and\n *                         struct size both limits are s16, so you cannot\n *                         enforce a range that doesn't fall within the range\n *                         of s16 - do that as usual in the code instead.\n *                         Use the NLA_POLICY_MIN(), NLA_POLICY_MAX() and\n *                         NLA_POLICY_RANGE() macros.\n *    NLA_U8,\n *    NLA_U16,\n *    NLA_U32,\n *    NLA_U64              If the validation_type field instead is set to\n *                         NLA_VALIDATE_RANGE_PTR, `range' must be a pointer\n *                         to a struct netlink_range_validation that indicates\n *                         the min/max values.\n *                         Use NLA_POLICY_FULL_RANGE().\n *    NLA_S8,\n *    NLA_S16,\n *    NLA_S32,\n *    NLA_S64              If the validation_type field instead is set to\n *                         NLA_VALIDATE_RANGE_PTR, `range_signed' must be a\n *                         pointer to a struct netlink_range_validation_signed\n *                         that indicates the min/max values.\n *                         Use NLA_POLICY_FULL_RANGE_SIGNED().\n *\n *    NLA_BINARY           If the validation type is like the ones for integers\n *                         above, then the min/max length (not value like for\n *                         integers) of the attribute is enforced.\n *\n *    All other            Unused - but note that it's a union\n *\n * Meaning of `validate' field, use via NLA_POLICY_VALIDATE_FN:\n *    NLA_BINARY           Validation function called for the attribute.\n *    All other            Unused - but note that it's a union\n *\n * Example:\n *\n * static const u32 myvalidflags = 0xff231023;\n *\n * static const struct nla_policy my_policy[ATTR_MAX+1] = {\n * \t[ATTR_FOO] = { .type = NLA_U16 },\n *\t[ATTR_BAR] = { .type = NLA_STRING, .len = BARSIZ },\n *\t[ATTR_BAZ] = NLA_POLICY_EXACT_LEN(sizeof(struct mystruct)),\n *\t[ATTR_GOO] = NLA_POLICY_BITFIELD32(myvalidflags),\n * };\n */\nstruct nla_policy {\n\tu8\t\ttype;\n\tu8\t\tvalidation_type;\n\tu16\t\tlen;\n\tunion {\n\t\tconst u32 bitfield32_valid;\n\t\tconst u32 mask;\n\t\tconst char *reject_message;\n\t\tconst struct nla_policy *nested_policy;\n\t\tstruct netlink_range_validation *range;\n\t\tstruct netlink_range_validation_signed *range_signed;\n\t\tstruct {\n\t\t\ts16 min, max;\n\t\t};\n\t\tint (*validate)(const struct nlattr *attr,\n\t\t\t\tstruct netlink_ext_ack *extack);\n\t\t/* This entry is special, and used for the attribute at index 0\n\t\t * only, and specifies special data about the policy, namely it\n\t\t * specifies the \"boundary type\" where strict length validation\n\t\t * starts for any attribute types >= this value, also, strict\n\t\t * nesting validation starts here.\n\t\t *\n\t\t * Additionally, it means that NLA_UNSPEC is actually NLA_REJECT\n\t\t * for any types >= this, so need to use NLA_POLICY_MIN_LEN() to\n\t\t * get the previous pure { .len = xyz } behaviour. The advantage\n\t\t * of this is that types not specified in the policy will be\n\t\t * rejected.\n\t\t *\n\t\t * For completely new families it should be set to 1 so that the\n\t\t * validation is enforced for all attributes. For existing ones\n\t\t * it should be set at least when new attributes are added to\n\t\t * the enum used by the policy, and be set to the new value that\n\t\t * was added to enforce strict validation from thereon.\n\t\t */\n\t\tu16 strict_start_type;\n\t};\n};\n\n#define NLA_POLICY_ETH_ADDR\t\tNLA_POLICY_EXACT_LEN(ETH_ALEN)\n#define NLA_POLICY_ETH_ADDR_COMPAT\tNLA_POLICY_EXACT_LEN_WARN(ETH_ALEN)\n\n#define _NLA_POLICY_NESTED(maxattr, policy) \\\n\t{ .type = NLA_NESTED, .nested_policy = policy, .len = maxattr }\n#define _NLA_POLICY_NESTED_ARRAY(maxattr, policy) \\\n\t{ .type = NLA_NESTED_ARRAY, .nested_policy = policy, .len = maxattr }\n#define NLA_POLICY_NESTED(policy) \\\n\t_NLA_POLICY_NESTED(ARRAY_SIZE(policy) - 1, policy)\n#define NLA_POLICY_NESTED_ARRAY(policy) \\\n\t_NLA_POLICY_NESTED_ARRAY(ARRAY_SIZE(policy) - 1, policy)\n#define NLA_POLICY_BITFIELD32(valid) \\\n\t{ .type = NLA_BITFIELD32, .bitfield32_valid = valid }\n\n#define __NLA_IS_UINT_TYPE(tp)\t\t\t\t\t\t\\\n\t(tp == NLA_U8 || tp == NLA_U16 || tp == NLA_U32 || tp == NLA_U64)\n#define __NLA_IS_SINT_TYPE(tp)\t\t\t\t\t\t\\\n\t(tp == NLA_S8 || tp == NLA_S16 || tp == NLA_S32 || tp == NLA_S64)\n\n#define __NLA_ENSURE(condition) BUILD_BUG_ON_ZERO(!(condition))\n#define NLA_ENSURE_UINT_TYPE(tp)\t\t\t\\\n\t(__NLA_ENSURE(__NLA_IS_UINT_TYPE(tp)) + tp)\n#define NLA_ENSURE_UINT_OR_BINARY_TYPE(tp)\t\t\\\n\t(__NLA_ENSURE(__NLA_IS_UINT_TYPE(tp) ||\t\\\n\t\t      tp == NLA_MSECS ||\t\t\\\n\t\t      tp == NLA_BINARY) + tp)\n#define NLA_ENSURE_SINT_TYPE(tp)\t\t\t\\\n\t(__NLA_ENSURE(__NLA_IS_SINT_TYPE(tp)) + tp)\n#define NLA_ENSURE_INT_OR_BINARY_TYPE(tp)\t\t\\\n\t(__NLA_ENSURE(__NLA_IS_UINT_TYPE(tp) ||\t\t\\\n\t\t      __NLA_IS_SINT_TYPE(tp) ||\t\t\\\n\t\t      tp == NLA_MSECS ||\t\t\\\n\t\t      tp == NLA_BINARY) + tp)\n#define NLA_ENSURE_NO_VALIDATION_PTR(tp)\t\t\\\n\t(__NLA_ENSURE(tp != NLA_BITFIELD32 &&\t\t\\\n\t\t      tp != NLA_REJECT &&\t\t\\\n\t\t      tp != NLA_NESTED &&\t\t\\\n\t\t      tp != NLA_NESTED_ARRAY) + tp)\n\n#define NLA_POLICY_RANGE(tp, _min, _max) {\t\t\\\n\t.type = NLA_ENSURE_INT_OR_BINARY_TYPE(tp),\t\\\n\t.validation_type = NLA_VALIDATE_RANGE,\t\t\\\n\t.min = _min,\t\t\t\t\t\\\n\t.max = _max\t\t\t\t\t\\\n}\n\n#define NLA_POLICY_FULL_RANGE(tp, _range) {\t\t\\\n\t.type = NLA_ENSURE_UINT_OR_BINARY_TYPE(tp),\t\\\n\t.validation_type = NLA_VALIDATE_RANGE_PTR,\t\\\n\t.range = _range,\t\t\t\t\\\n}\n\n#define NLA_POLICY_FULL_RANGE_SIGNED(tp, _range) {\t\\\n\t.type = NLA_ENSURE_SINT_TYPE(tp),\t\t\\\n\t.validation_type = NLA_VALIDATE_RANGE_PTR,\t\\\n\t.range_signed = _range,\t\t\t\t\\\n}\n\n#define NLA_POLICY_MIN(tp, _min) {\t\t\t\\\n\t.type = NLA_ENSURE_INT_OR_BINARY_TYPE(tp),\t\\\n\t.validation_type = NLA_VALIDATE_MIN,\t\t\\\n\t.min = _min,\t\t\t\t\t\\\n}\n\n#define NLA_POLICY_MAX(tp, _max) {\t\t\t\\\n\t.type = NLA_ENSURE_INT_OR_BINARY_TYPE(tp),\t\\\n\t.validation_type = NLA_VALIDATE_MAX,\t\t\\\n\t.max = _max,\t\t\t\t\t\\\n}\n\n#define NLA_POLICY_MASK(tp, _mask) {\t\t\t\\\n\t.type = NLA_ENSURE_UINT_TYPE(tp),\t\t\\\n\t.validation_type = NLA_VALIDATE_MASK,\t\t\\\n\t.mask = _mask,\t\t\t\t\t\\\n}\n\n#define NLA_POLICY_VALIDATE_FN(tp, fn, ...) {\t\t\\\n\t.type = NLA_ENSURE_NO_VALIDATION_PTR(tp),\t\\\n\t.validation_type = NLA_VALIDATE_FUNCTION,\t\\\n\t.validate = fn,\t\t\t\t\t\\\n\t.len = __VA_ARGS__ + 0,\t\t\t\t\\\n}\n\n#define NLA_POLICY_EXACT_LEN(_len)\tNLA_POLICY_RANGE(NLA_BINARY, _len, _len)\n#define NLA_POLICY_EXACT_LEN_WARN(_len) {\t\t\t\\\n\t.type = NLA_BINARY,\t\t\t\t\t\\\n\t.validation_type = NLA_VALIDATE_RANGE_WARN_TOO_LONG,\t\\\n\t.min = _len,\t\t\t\t\t\t\\\n\t.max = _len\t\t\t\t\t\t\\\n}\n#define NLA_POLICY_MIN_LEN(_len)\tNLA_POLICY_MIN(NLA_BINARY, _len)\n\n/**\n * struct nl_info - netlink source information\n * @nlh: Netlink message header of original request\n * @nl_net: Network namespace\n * @portid: Netlink PORTID of requesting application\n * @skip_notify: Skip netlink notifications to user space\n * @skip_notify_kernel: Skip selected in-kernel notifications\n */\nstruct nl_info {\n\tstruct nlmsghdr\t\t*nlh;\n\tstruct net\t\t*nl_net;\n\tu32\t\t\tportid;\n\tu8\t\t\tskip_notify:1,\n\t\t\t\tskip_notify_kernel:1;\n};\n\n/**\n * enum netlink_validation - netlink message/attribute validation levels\n * @NL_VALIDATE_LIBERAL: Old-style \"be liberal\" validation, not caring about\n *\textra data at the end of the message, attributes being longer than\n *\tthey should be, or unknown attributes being present.\n * @NL_VALIDATE_TRAILING: Reject junk data encountered after attribute parsing.\n * @NL_VALIDATE_MAXTYPE: Reject attributes > max type; Together with _TRAILING\n *\tthis is equivalent to the old nla_parse_strict()/nlmsg_parse_strict().\n * @NL_VALIDATE_UNSPEC: Reject attributes with NLA_UNSPEC in the policy.\n *\tThis can safely be set by the kernel when the given policy has no\n *\tNLA_UNSPEC anymore, and can thus be used to ensure policy entries\n *\tare enforced going forward.\n * @NL_VALIDATE_STRICT_ATTRS: strict attribute policy parsing (e.g.\n *\tU8, U16, U32 must have exact size, etc.)\n * @NL_VALIDATE_NESTED: Check that NLA_F_NESTED is set for NLA_NESTED(_ARRAY)\n *\tand unset for other policies.\n */\nenum netlink_validation {\n\tNL_VALIDATE_LIBERAL = 0,\n\tNL_VALIDATE_TRAILING = BIT(0),\n\tNL_VALIDATE_MAXTYPE = BIT(1),\n\tNL_VALIDATE_UNSPEC = BIT(2),\n\tNL_VALIDATE_STRICT_ATTRS = BIT(3),\n\tNL_VALIDATE_NESTED = BIT(4),\n};\n\n#define NL_VALIDATE_DEPRECATED_STRICT (NL_VALIDATE_TRAILING |\\\n\t\t\t\t       NL_VALIDATE_MAXTYPE)\n#define NL_VALIDATE_STRICT (NL_VALIDATE_TRAILING |\\\n\t\t\t    NL_VALIDATE_MAXTYPE |\\\n\t\t\t    NL_VALIDATE_UNSPEC |\\\n\t\t\t    NL_VALIDATE_STRICT_ATTRS |\\\n\t\t\t    NL_VALIDATE_NESTED)\n\nint netlink_rcv_skb(struct sk_buff *skb,\n\t\t    int (*cb)(struct sk_buff *, struct nlmsghdr *,\n\t\t\t      struct netlink_ext_ack *));\nint nlmsg_notify(struct sock *sk, struct sk_buff *skb, u32 portid,\n\t\t unsigned int group, int report, gfp_t flags);\n\nint __nla_validate(const struct nlattr *head, int len, int maxtype,\n\t\t   const struct nla_policy *policy, unsigned int validate,\n\t\t   struct netlink_ext_ack *extack);\nint __nla_parse(struct nlattr **tb, int maxtype, const struct nlattr *head,\n\t\tint len, const struct nla_policy *policy, unsigned int validate,\n\t\tstruct netlink_ext_ack *extack);\nint nla_policy_len(const struct nla_policy *, int);\nstruct nlattr *nla_find(const struct nlattr *head, int len, int attrtype);\nssize_t nla_strscpy(char *dst, const struct nlattr *nla, size_t dstsize);\nchar *nla_strdup(const struct nlattr *nla, gfp_t flags);\nint nla_memcpy(void *dest, const struct nlattr *src, int count);\nint nla_memcmp(const struct nlattr *nla, const void *data, size_t size);\nint nla_strcmp(const struct nlattr *nla, const char *str);\nstruct nlattr *__nla_reserve(struct sk_buff *skb, int attrtype, int attrlen);\nstruct nlattr *__nla_reserve_64bit(struct sk_buff *skb, int attrtype,\n\t\t\t\t   int attrlen, int padattr);\nvoid *__nla_reserve_nohdr(struct sk_buff *skb, int attrlen);\nstruct nlattr *nla_reserve(struct sk_buff *skb, int attrtype, int attrlen);\nstruct nlattr *nla_reserve_64bit(struct sk_buff *skb, int attrtype,\n\t\t\t\t int attrlen, int padattr);\nvoid *nla_reserve_nohdr(struct sk_buff *skb, int attrlen);\nvoid __nla_put(struct sk_buff *skb, int attrtype, int attrlen,\n\t       const void *data);\nvoid __nla_put_64bit(struct sk_buff *skb, int attrtype, int attrlen,\n\t\t     const void *data, int padattr);\nvoid __nla_put_nohdr(struct sk_buff *skb, int attrlen, const void *data);\nint nla_put(struct sk_buff *skb, int attrtype, int attrlen, const void *data);\nint nla_put_64bit(struct sk_buff *skb, int attrtype, int attrlen,\n\t\t  const void *data, int padattr);\nint nla_put_nohdr(struct sk_buff *skb, int attrlen, const void *data);\nint nla_append(struct sk_buff *skb, int attrlen, const void *data);\n\n/**************************************************************************\n * Netlink Messages\n **************************************************************************/\n\n/**\n * nlmsg_msg_size - length of netlink message not including padding\n * @payload: length of message payload\n */\nstatic inline int nlmsg_msg_size(int payload)\n{\n\treturn NLMSG_HDRLEN + payload;\n}\n\n/**\n * nlmsg_total_size - length of netlink message including padding\n * @payload: length of message payload\n */\nstatic inline int nlmsg_total_size(int payload)\n{\n\treturn NLMSG_ALIGN(nlmsg_msg_size(payload));\n}\n\n/**\n * nlmsg_padlen - length of padding at the message's tail\n * @payload: length of message payload\n */\nstatic inline int nlmsg_padlen(int payload)\n{\n\treturn nlmsg_total_size(payload) - nlmsg_msg_size(payload);\n}\n\n/**\n * nlmsg_data - head of message payload\n * @nlh: netlink message header\n */\nstatic inline void *nlmsg_data(const struct nlmsghdr *nlh)\n{\n\treturn (unsigned char *) nlh + NLMSG_HDRLEN;\n}\n\n/**\n * nlmsg_len - length of message payload\n * @nlh: netlink message header\n */\nstatic inline int nlmsg_len(const struct nlmsghdr *nlh)\n{\n\treturn nlh->nlmsg_len - NLMSG_HDRLEN;\n}\n\n/**\n * nlmsg_attrdata - head of attributes data\n * @nlh: netlink message header\n * @hdrlen: length of family specific header\n */\nstatic inline struct nlattr *nlmsg_attrdata(const struct nlmsghdr *nlh,\n\t\t\t\t\t    int hdrlen)\n{\n\tunsigned char *data = nlmsg_data(nlh);\n\treturn (struct nlattr *) (data + NLMSG_ALIGN(hdrlen));\n}\n\n/**\n * nlmsg_attrlen - length of attributes data\n * @nlh: netlink message header\n * @hdrlen: length of family specific header\n */\nstatic inline int nlmsg_attrlen(const struct nlmsghdr *nlh, int hdrlen)\n{\n\treturn nlmsg_len(nlh) - NLMSG_ALIGN(hdrlen);\n}\n\n/**\n * nlmsg_ok - check if the netlink message fits into the remaining bytes\n * @nlh: netlink message header\n * @remaining: number of bytes remaining in message stream\n */\nstatic inline int nlmsg_ok(const struct nlmsghdr *nlh, int remaining)\n{\n\treturn (remaining >= (int) sizeof(struct nlmsghdr) &&\n\t\tnlh->nlmsg_len >= sizeof(struct nlmsghdr) &&\n\t\tnlh->nlmsg_len <= remaining);\n}\n\n/**\n * nlmsg_next - next netlink message in message stream\n * @nlh: netlink message header\n * @remaining: number of bytes remaining in message stream\n *\n * Returns the next netlink message in the message stream and\n * decrements remaining by the size of the current message.\n */\nstatic inline struct nlmsghdr *\nnlmsg_next(const struct nlmsghdr *nlh, int *remaining)\n{\n\tint totlen = NLMSG_ALIGN(nlh->nlmsg_len);\n\n\t*remaining -= totlen;\n\n\treturn (struct nlmsghdr *) ((unsigned char *) nlh + totlen);\n}\n\n/**\n * nla_parse - Parse a stream of attributes into a tb buffer\n * @tb: destination array with maxtype+1 elements\n * @maxtype: maximum attribute type to be expected\n * @head: head of attribute stream\n * @len: length of attribute stream\n * @policy: validation policy\n * @extack: extended ACK pointer\n *\n * Parses a stream of attributes and stores a pointer to each attribute in\n * the tb array accessible via the attribute type. Attributes with a type\n * exceeding maxtype will be rejected, policy must be specified, attributes\n * will be validated in the strictest way possible.\n *\n * Returns 0 on success or a negative error code.\n */\nstatic inline int nla_parse(struct nlattr **tb, int maxtype,\n\t\t\t    const struct nlattr *head, int len,\n\t\t\t    const struct nla_policy *policy,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\treturn __nla_parse(tb, maxtype, head, len, policy,\n\t\t\t   NL_VALIDATE_STRICT, extack);\n}\n\n/**\n * nla_parse_deprecated - Parse a stream of attributes into a tb buffer\n * @tb: destination array with maxtype+1 elements\n * @maxtype: maximum attribute type to be expected\n * @head: head of attribute stream\n * @len: length of attribute stream\n * @policy: validation policy\n * @extack: extended ACK pointer\n *\n * Parses a stream of attributes and stores a pointer to each attribute in\n * the tb array accessible via the attribute type. Attributes with a type\n * exceeding maxtype will be ignored and attributes from the policy are not\n * always strictly validated (only for new attributes).\n *\n * Returns 0 on success or a negative error code.\n */\nstatic inline int nla_parse_deprecated(struct nlattr **tb, int maxtype,\n\t\t\t\t       const struct nlattr *head, int len,\n\t\t\t\t       const struct nla_policy *policy,\n\t\t\t\t       struct netlink_ext_ack *extack)\n{\n\treturn __nla_parse(tb, maxtype, head, len, policy,\n\t\t\t   NL_VALIDATE_LIBERAL, extack);\n}\n\n/**\n * nla_parse_deprecated_strict - Parse a stream of attributes into a tb buffer\n * @tb: destination array with maxtype+1 elements\n * @maxtype: maximum attribute type to be expected\n * @head: head of attribute stream\n * @len: length of attribute stream\n * @policy: validation policy\n * @extack: extended ACK pointer\n *\n * Parses a stream of attributes and stores a pointer to each attribute in\n * the tb array accessible via the attribute type. Attributes with a type\n * exceeding maxtype will be rejected as well as trailing data, but the\n * policy is not completely strictly validated (only for new attributes).\n *\n * Returns 0 on success or a negative error code.\n */\nstatic inline int nla_parse_deprecated_strict(struct nlattr **tb, int maxtype,\n\t\t\t\t\t      const struct nlattr *head,\n\t\t\t\t\t      int len,\n\t\t\t\t\t      const struct nla_policy *policy,\n\t\t\t\t\t      struct netlink_ext_ack *extack)\n{\n\treturn __nla_parse(tb, maxtype, head, len, policy,\n\t\t\t   NL_VALIDATE_DEPRECATED_STRICT, extack);\n}\n\n/**\n * __nlmsg_parse - parse attributes of a netlink message\n * @nlh: netlink message header\n * @hdrlen: length of family specific header\n * @tb: destination array with maxtype+1 elements\n * @maxtype: maximum attribute type to be expected\n * @policy: validation policy\n * @validate: validation strictness\n * @extack: extended ACK report struct\n *\n * See nla_parse()\n */\nstatic inline int __nlmsg_parse(const struct nlmsghdr *nlh, int hdrlen,\n\t\t\t\tstruct nlattr *tb[], int maxtype,\n\t\t\t\tconst struct nla_policy *policy,\n\t\t\t\tunsigned int validate,\n\t\t\t\tstruct netlink_ext_ack *extack)\n{\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen)) {\n\t\tNL_SET_ERR_MSG(extack, \"Invalid header length\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn __nla_parse(tb, maxtype, nlmsg_attrdata(nlh, hdrlen),\n\t\t\t   nlmsg_attrlen(nlh, hdrlen), policy, validate,\n\t\t\t   extack);\n}\n\n/**\n * nlmsg_parse - parse attributes of a netlink message\n * @nlh: netlink message header\n * @hdrlen: length of family specific header\n * @tb: destination array with maxtype+1 elements\n * @maxtype: maximum attribute type to be expected\n * @extack: extended ACK report struct\n *\n * See nla_parse()\n */\nstatic inline int nlmsg_parse(const struct nlmsghdr *nlh, int hdrlen,\n\t\t\t      struct nlattr *tb[], int maxtype,\n\t\t\t      const struct nla_policy *policy,\n\t\t\t      struct netlink_ext_ack *extack)\n{\n\treturn __nlmsg_parse(nlh, hdrlen, tb, maxtype, policy,\n\t\t\t     NL_VALIDATE_STRICT, extack);\n}\n\n/**\n * nlmsg_parse_deprecated - parse attributes of a netlink message\n * @nlh: netlink message header\n * @hdrlen: length of family specific header\n * @tb: destination array with maxtype+1 elements\n * @maxtype: maximum attribute type to be expected\n * @extack: extended ACK report struct\n *\n * See nla_parse_deprecated()\n */\nstatic inline int nlmsg_parse_deprecated(const struct nlmsghdr *nlh, int hdrlen,\n\t\t\t\t\t struct nlattr *tb[], int maxtype,\n\t\t\t\t\t const struct nla_policy *policy,\n\t\t\t\t\t struct netlink_ext_ack *extack)\n{\n\treturn __nlmsg_parse(nlh, hdrlen, tb, maxtype, policy,\n\t\t\t     NL_VALIDATE_LIBERAL, extack);\n}\n\n/**\n * nlmsg_parse_deprecated_strict - parse attributes of a netlink message\n * @nlh: netlink message header\n * @hdrlen: length of family specific header\n * @tb: destination array with maxtype+1 elements\n * @maxtype: maximum attribute type to be expected\n * @extack: extended ACK report struct\n *\n * See nla_parse_deprecated_strict()\n */\nstatic inline int\nnlmsg_parse_deprecated_strict(const struct nlmsghdr *nlh, int hdrlen,\n\t\t\t      struct nlattr *tb[], int maxtype,\n\t\t\t      const struct nla_policy *policy,\n\t\t\t      struct netlink_ext_ack *extack)\n{\n\treturn __nlmsg_parse(nlh, hdrlen, tb, maxtype, policy,\n\t\t\t     NL_VALIDATE_DEPRECATED_STRICT, extack);\n}\n\n/**\n * nlmsg_find_attr - find a specific attribute in a netlink message\n * @nlh: netlink message header\n * @hdrlen: length of familiy specific header\n * @attrtype: type of attribute to look for\n *\n * Returns the first attribute which matches the specified type.\n */\nstatic inline struct nlattr *nlmsg_find_attr(const struct nlmsghdr *nlh,\n\t\t\t\t\t     int hdrlen, int attrtype)\n{\n\treturn nla_find(nlmsg_attrdata(nlh, hdrlen),\n\t\t\tnlmsg_attrlen(nlh, hdrlen), attrtype);\n}\n\n/**\n * nla_validate_deprecated - Validate a stream of attributes\n * @head: head of attribute stream\n * @len: length of attribute stream\n * @maxtype: maximum attribute type to be expected\n * @policy: validation policy\n * @validate: validation strictness\n * @extack: extended ACK report struct\n *\n * Validates all attributes in the specified attribute stream against the\n * specified policy. Validation is done in liberal mode.\n * See documenation of struct nla_policy for more details.\n *\n * Returns 0 on success or a negative error code.\n */\nstatic inline int nla_validate_deprecated(const struct nlattr *head, int len,\n\t\t\t\t\t  int maxtype,\n\t\t\t\t\t  const struct nla_policy *policy,\n\t\t\t\t\t  struct netlink_ext_ack *extack)\n{\n\treturn __nla_validate(head, len, maxtype, policy, NL_VALIDATE_LIBERAL,\n\t\t\t      extack);\n}\n\n/**\n * nla_validate - Validate a stream of attributes\n * @head: head of attribute stream\n * @len: length of attribute stream\n * @maxtype: maximum attribute type to be expected\n * @policy: validation policy\n * @extack: extended ACK report struct\n *\n * Validates all attributes in the specified attribute stream against the\n * specified policy. Validation is done in strict mode.\n * See documenation of struct nla_policy for more details.\n *\n * Returns 0 on success or a negative error code.\n */\nstatic inline int nla_validate(const struct nlattr *head, int len, int maxtype,\n\t\t\t       const struct nla_policy *policy,\n\t\t\t       struct netlink_ext_ack *extack)\n{\n\treturn __nla_validate(head, len, maxtype, policy, NL_VALIDATE_STRICT,\n\t\t\t      extack);\n}\n\n/**\n * nlmsg_validate_deprecated - validate a netlink message including attributes\n * @nlh: netlinket message header\n * @hdrlen: length of familiy specific header\n * @maxtype: maximum attribute type to be expected\n * @policy: validation policy\n * @extack: extended ACK report struct\n */\nstatic inline int nlmsg_validate_deprecated(const struct nlmsghdr *nlh,\n\t\t\t\t\t    int hdrlen, int maxtype,\n\t\t\t\t\t    const struct nla_policy *policy,\n\t\t\t\t\t    struct netlink_ext_ack *extack)\n{\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\treturn __nla_validate(nlmsg_attrdata(nlh, hdrlen),\n\t\t\t      nlmsg_attrlen(nlh, hdrlen), maxtype,\n\t\t\t      policy, NL_VALIDATE_LIBERAL, extack);\n}\n\n\n\n/**\n * nlmsg_report - need to report back to application?\n * @nlh: netlink message header\n *\n * Returns 1 if a report back to the application is requested.\n */\nstatic inline int nlmsg_report(const struct nlmsghdr *nlh)\n{\n\treturn !!(nlh->nlmsg_flags & NLM_F_ECHO);\n}\n\n/**\n * nlmsg_for_each_attr - iterate over a stream of attributes\n * @pos: loop counter, set to current attribute\n * @nlh: netlink message header\n * @hdrlen: length of familiy specific header\n * @rem: initialized to len, holds bytes currently remaining in stream\n */\n#define nlmsg_for_each_attr(pos, nlh, hdrlen, rem) \\\n\tnla_for_each_attr(pos, nlmsg_attrdata(nlh, hdrlen), \\\n\t\t\t  nlmsg_attrlen(nlh, hdrlen), rem)\n\n/**\n * nlmsg_put - Add a new netlink message to an skb\n * @skb: socket buffer to store message in\n * @portid: netlink PORTID of requesting application\n * @seq: sequence number of message\n * @type: message type\n * @payload: length of message payload\n * @flags: message flags\n *\n * Returns NULL if the tailroom of the skb is insufficient to store\n * the message header and payload.\n */\nstatic inline struct nlmsghdr *nlmsg_put(struct sk_buff *skb, u32 portid, u32 seq,\n\t\t\t\t\t int type, int payload, int flags)\n{\n\tif (unlikely(skb_tailroom(skb) < nlmsg_total_size(payload)))\n\t\treturn NULL;\n\n\treturn __nlmsg_put(skb, portid, seq, type, payload, flags);\n}\n\n/**\n * nlmsg_put_answer - Add a new callback based netlink message to an skb\n * @skb: socket buffer to store message in\n * @cb: netlink callback\n * @type: message type\n * @payload: length of message payload\n * @flags: message flags\n *\n * Returns NULL if the tailroom of the skb is insufficient to store\n * the message header and payload.\n */\nstatic inline struct nlmsghdr *nlmsg_put_answer(struct sk_buff *skb,\n\t\t\t\t\t\tstruct netlink_callback *cb,\n\t\t\t\t\t\tint type, int payload,\n\t\t\t\t\t\tint flags)\n{\n\treturn nlmsg_put(skb, NETLINK_CB(cb->skb).portid, cb->nlh->nlmsg_seq,\n\t\t\t type, payload, flags);\n}\n\n/**\n * nlmsg_new - Allocate a new netlink message\n * @payload: size of the message payload\n * @flags: the type of memory to allocate.\n *\n * Use NLMSG_DEFAULT_SIZE if the size of the payload isn't known\n * and a good default is needed.\n */\nstatic inline struct sk_buff *nlmsg_new(size_t payload, gfp_t flags)\n{\n\treturn alloc_skb(nlmsg_total_size(payload), flags);\n}\n\n/**\n * nlmsg_end - Finalize a netlink message\n * @skb: socket buffer the message is stored in\n * @nlh: netlink message header\n *\n * Corrects the netlink message header to include the appeneded\n * attributes. Only necessary if attributes have been added to\n * the message.\n */\nstatic inline void nlmsg_end(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tnlh->nlmsg_len = skb_tail_pointer(skb) - (unsigned char *)nlh;\n}\n\n/**\n * nlmsg_get_pos - return current position in netlink message\n * @skb: socket buffer the message is stored in\n *\n * Returns a pointer to the current tail of the message.\n */\nstatic inline void *nlmsg_get_pos(struct sk_buff *skb)\n{\n\treturn skb_tail_pointer(skb);\n}\n\n/**\n * nlmsg_trim - Trim message to a mark\n * @skb: socket buffer the message is stored in\n * @mark: mark to trim to\n *\n * Trims the message to the provided mark.\n */\nstatic inline void nlmsg_trim(struct sk_buff *skb, const void *mark)\n{\n\tif (mark) {\n\t\tWARN_ON((unsigned char *) mark < skb->data);\n\t\tskb_trim(skb, (unsigned char *) mark - skb->data);\n\t}\n}\n\n/**\n * nlmsg_cancel - Cancel construction of a netlink message\n * @skb: socket buffer the message is stored in\n * @nlh: netlink message header\n *\n * Removes the complete netlink message including all\n * attributes from the socket buffer again.\n */\nstatic inline void nlmsg_cancel(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tnlmsg_trim(skb, nlh);\n}\n\n/**\n * nlmsg_free - free a netlink message\n * @skb: socket buffer of netlink message\n */\nstatic inline void nlmsg_free(struct sk_buff *skb)\n{\n\tkfree_skb(skb);\n}\n\n/**\n * nlmsg_multicast - multicast a netlink message\n * @sk: netlink socket to spread messages to\n * @skb: netlink message as socket buffer\n * @portid: own netlink portid to avoid sending to yourself\n * @group: multicast group id\n * @flags: allocation flags\n */\nstatic inline int nlmsg_multicast(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  u32 portid, unsigned int group, gfp_t flags)\n{\n\tint err;\n\n\tNETLINK_CB(skb).dst_group = group;\n\n\terr = netlink_broadcast(sk, skb, portid, group, flags);\n\tif (err > 0)\n\t\terr = 0;\n\n\treturn err;\n}\n\n/**\n * nlmsg_unicast - unicast a netlink message\n * @sk: netlink socket to spread message to\n * @skb: netlink message as socket buffer\n * @portid: netlink portid of the destination socket\n */\nstatic inline int nlmsg_unicast(struct sock *sk, struct sk_buff *skb, u32 portid)\n{\n\tint err;\n\n\terr = netlink_unicast(sk, skb, portid, MSG_DONTWAIT);\n\tif (err > 0)\n\t\terr = 0;\n\n\treturn err;\n}\n\n/**\n * nlmsg_for_each_msg - iterate over a stream of messages\n * @pos: loop counter, set to current message\n * @head: head of message stream\n * @len: length of message stream\n * @rem: initialized to len, holds bytes currently remaining in stream\n */\n#define nlmsg_for_each_msg(pos, head, len, rem) \\\n\tfor (pos = head, rem = len; \\\n\t     nlmsg_ok(pos, rem); \\\n\t     pos = nlmsg_next(pos, &(rem)))\n\n/**\n * nl_dump_check_consistent - check if sequence is consistent and advertise if not\n * @cb: netlink callback structure that stores the sequence number\n * @nlh: netlink message header to write the flag to\n *\n * This function checks if the sequence (generation) number changed during dump\n * and if it did, advertises it in the netlink message header.\n *\n * The correct way to use it is to set cb->seq to the generation counter when\n * all locks for dumping have been acquired, and then call this function for\n * each message that is generated.\n *\n * Note that due to initialisation concerns, 0 is an invalid sequence number\n * and must not be used by code that uses this functionality.\n */\nstatic inline void\nnl_dump_check_consistent(struct netlink_callback *cb,\n\t\t\t struct nlmsghdr *nlh)\n{\n\tif (cb->prev_seq && cb->seq != cb->prev_seq)\n\t\tnlh->nlmsg_flags |= NLM_F_DUMP_INTR;\n\tcb->prev_seq = cb->seq;\n}\n\n/**************************************************************************\n * Netlink Attributes\n **************************************************************************/\n\n/**\n * nla_attr_size - length of attribute not including padding\n * @payload: length of payload\n */\nstatic inline int nla_attr_size(int payload)\n{\n\treturn NLA_HDRLEN + payload;\n}\n\n/**\n * nla_total_size - total length of attribute including padding\n * @payload: length of payload\n */\nstatic inline int nla_total_size(int payload)\n{\n\treturn NLA_ALIGN(nla_attr_size(payload));\n}\n\n/**\n * nla_padlen - length of padding at the tail of attribute\n * @payload: length of payload\n */\nstatic inline int nla_padlen(int payload)\n{\n\treturn nla_total_size(payload) - nla_attr_size(payload);\n}\n\n/**\n * nla_type - attribute type\n * @nla: netlink attribute\n */\nstatic inline int nla_type(const struct nlattr *nla)\n{\n\treturn nla->nla_type & NLA_TYPE_MASK;\n}\n\n/**\n * nla_data - head of payload\n * @nla: netlink attribute\n */\nstatic inline void *nla_data(const struct nlattr *nla)\n{\n\treturn (char *) nla + NLA_HDRLEN;\n}\n\n/**\n * nla_len - length of payload\n * @nla: netlink attribute\n */\nstatic inline int nla_len(const struct nlattr *nla)\n{\n\treturn nla->nla_len - NLA_HDRLEN;\n}\n\n/**\n * nla_ok - check if the netlink attribute fits into the remaining bytes\n * @nla: netlink attribute\n * @remaining: number of bytes remaining in attribute stream\n */\nstatic inline int nla_ok(const struct nlattr *nla, int remaining)\n{\n\treturn remaining >= (int) sizeof(*nla) &&\n\t       nla->nla_len >= sizeof(*nla) &&\n\t       nla->nla_len <= remaining;\n}\n\n/**\n * nla_next - next netlink attribute in attribute stream\n * @nla: netlink attribute\n * @remaining: number of bytes remaining in attribute stream\n *\n * Returns the next netlink attribute in the attribute stream and\n * decrements remaining by the size of the current attribute.\n */\nstatic inline struct nlattr *nla_next(const struct nlattr *nla, int *remaining)\n{\n\tunsigned int totlen = NLA_ALIGN(nla->nla_len);\n\n\t*remaining -= totlen;\n\treturn (struct nlattr *) ((char *) nla + totlen);\n}\n\n/**\n * nla_find_nested - find attribute in a set of nested attributes\n * @nla: attribute containing the nested attributes\n * @attrtype: type of attribute to look for\n *\n * Returns the first attribute which matches the specified type.\n */\nstatic inline struct nlattr *\nnla_find_nested(const struct nlattr *nla, int attrtype)\n{\n\treturn nla_find(nla_data(nla), nla_len(nla), attrtype);\n}\n\n/**\n * nla_parse_nested - parse nested attributes\n * @tb: destination array with maxtype+1 elements\n * @maxtype: maximum attribute type to be expected\n * @nla: attribute containing the nested attributes\n * @policy: validation policy\n * @extack: extended ACK report struct\n *\n * See nla_parse()\n */\nstatic inline int nla_parse_nested(struct nlattr *tb[], int maxtype,\n\t\t\t\t   const struct nlattr *nla,\n\t\t\t\t   const struct nla_policy *policy,\n\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tif (!(nla->nla_type & NLA_F_NESTED)) {\n\t\tNL_SET_ERR_MSG_ATTR(extack, nla, \"NLA_F_NESTED is missing\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn __nla_parse(tb, maxtype, nla_data(nla), nla_len(nla), policy,\n\t\t\t   NL_VALIDATE_STRICT, extack);\n}\n\n/**\n * nla_parse_nested_deprecated - parse nested attributes\n * @tb: destination array with maxtype+1 elements\n * @maxtype: maximum attribute type to be expected\n * @nla: attribute containing the nested attributes\n * @policy: validation policy\n * @extack: extended ACK report struct\n *\n * See nla_parse_deprecated()\n */\nstatic inline int nla_parse_nested_deprecated(struct nlattr *tb[], int maxtype,\n\t\t\t\t\t      const struct nlattr *nla,\n\t\t\t\t\t      const struct nla_policy *policy,\n\t\t\t\t\t      struct netlink_ext_ack *extack)\n{\n\treturn __nla_parse(tb, maxtype, nla_data(nla), nla_len(nla), policy,\n\t\t\t   NL_VALIDATE_LIBERAL, extack);\n}\n\n/**\n * nla_put_u8 - Add a u8 netlink attribute to a socket buffer\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @value: numeric value\n */\nstatic inline int nla_put_u8(struct sk_buff *skb, int attrtype, u8 value)\n{\n\t/* temporary variables to work around GCC PR81715 with asan-stack=1 */\n\tu8 tmp = value;\n\n\treturn nla_put(skb, attrtype, sizeof(u8), &tmp);\n}\n\n/**\n * nla_put_u16 - Add a u16 netlink attribute to a socket buffer\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @value: numeric value\n */\nstatic inline int nla_put_u16(struct sk_buff *skb, int attrtype, u16 value)\n{\n\tu16 tmp = value;\n\n\treturn nla_put(skb, attrtype, sizeof(u16), &tmp);\n}\n\n/**\n * nla_put_be16 - Add a __be16 netlink attribute to a socket buffer\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @value: numeric value\n */\nstatic inline int nla_put_be16(struct sk_buff *skb, int attrtype, __be16 value)\n{\n\t__be16 tmp = value;\n\n\treturn nla_put(skb, attrtype, sizeof(__be16), &tmp);\n}\n\n/**\n * nla_put_net16 - Add 16-bit network byte order netlink attribute to a socket buffer\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @value: numeric value\n */\nstatic inline int nla_put_net16(struct sk_buff *skb, int attrtype, __be16 value)\n{\n\t__be16 tmp = value;\n\n\treturn nla_put_be16(skb, attrtype | NLA_F_NET_BYTEORDER, tmp);\n}\n\n/**\n * nla_put_le16 - Add a __le16 netlink attribute to a socket buffer\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @value: numeric value\n */\nstatic inline int nla_put_le16(struct sk_buff *skb, int attrtype, __le16 value)\n{\n\t__le16 tmp = value;\n\n\treturn nla_put(skb, attrtype, sizeof(__le16), &tmp);\n}\n\n/**\n * nla_put_u32 - Add a u32 netlink attribute to a socket buffer\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @value: numeric value\n */\nstatic inline int nla_put_u32(struct sk_buff *skb, int attrtype, u32 value)\n{\n\tu32 tmp = value;\n\n\treturn nla_put(skb, attrtype, sizeof(u32), &tmp);\n}\n\n/**\n * nla_put_be32 - Add a __be32 netlink attribute to a socket buffer\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @value: numeric value\n */\nstatic inline int nla_put_be32(struct sk_buff *skb, int attrtype, __be32 value)\n{\n\t__be32 tmp = value;\n\n\treturn nla_put(skb, attrtype, sizeof(__be32), &tmp);\n}\n\n/**\n * nla_put_net32 - Add 32-bit network byte order netlink attribute to a socket buffer\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @value: numeric value\n */\nstatic inline int nla_put_net32(struct sk_buff *skb, int attrtype, __be32 value)\n{\n\t__be32 tmp = value;\n\n\treturn nla_put_be32(skb, attrtype | NLA_F_NET_BYTEORDER, tmp);\n}\n\n/**\n * nla_put_le32 - Add a __le32 netlink attribute to a socket buffer\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @value: numeric value\n */\nstatic inline int nla_put_le32(struct sk_buff *skb, int attrtype, __le32 value)\n{\n\t__le32 tmp = value;\n\n\treturn nla_put(skb, attrtype, sizeof(__le32), &tmp);\n}\n\n/**\n * nla_put_u64_64bit - Add a u64 netlink attribute to a skb and align it\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @value: numeric value\n * @padattr: attribute type for the padding\n */\nstatic inline int nla_put_u64_64bit(struct sk_buff *skb, int attrtype,\n\t\t\t\t    u64 value, int padattr)\n{\n\tu64 tmp = value;\n\n\treturn nla_put_64bit(skb, attrtype, sizeof(u64), &tmp, padattr);\n}\n\n/**\n * nla_put_be64 - Add a __be64 netlink attribute to a socket buffer and align it\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @value: numeric value\n * @padattr: attribute type for the padding\n */\nstatic inline int nla_put_be64(struct sk_buff *skb, int attrtype, __be64 value,\n\t\t\t       int padattr)\n{\n\t__be64 tmp = value;\n\n\treturn nla_put_64bit(skb, attrtype, sizeof(__be64), &tmp, padattr);\n}\n\n/**\n * nla_put_net64 - Add 64-bit network byte order nlattr to a skb and align it\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @value: numeric value\n * @padattr: attribute type for the padding\n */\nstatic inline int nla_put_net64(struct sk_buff *skb, int attrtype, __be64 value,\n\t\t\t\tint padattr)\n{\n\t__be64 tmp = value;\n\n\treturn nla_put_be64(skb, attrtype | NLA_F_NET_BYTEORDER, tmp,\n\t\t\t    padattr);\n}\n\n/**\n * nla_put_le64 - Add a __le64 netlink attribute to a socket buffer and align it\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @value: numeric value\n * @padattr: attribute type for the padding\n */\nstatic inline int nla_put_le64(struct sk_buff *skb, int attrtype, __le64 value,\n\t\t\t       int padattr)\n{\n\t__le64 tmp = value;\n\n\treturn nla_put_64bit(skb, attrtype, sizeof(__le64), &tmp, padattr);\n}\n\n/**\n * nla_put_s8 - Add a s8 netlink attribute to a socket buffer\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @value: numeric value\n */\nstatic inline int nla_put_s8(struct sk_buff *skb, int attrtype, s8 value)\n{\n\ts8 tmp = value;\n\n\treturn nla_put(skb, attrtype, sizeof(s8), &tmp);\n}\n\n/**\n * nla_put_s16 - Add a s16 netlink attribute to a socket buffer\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @value: numeric value\n */\nstatic inline int nla_put_s16(struct sk_buff *skb, int attrtype, s16 value)\n{\n\ts16 tmp = value;\n\n\treturn nla_put(skb, attrtype, sizeof(s16), &tmp);\n}\n\n/**\n * nla_put_s32 - Add a s32 netlink attribute to a socket buffer\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @value: numeric value\n */\nstatic inline int nla_put_s32(struct sk_buff *skb, int attrtype, s32 value)\n{\n\ts32 tmp = value;\n\n\treturn nla_put(skb, attrtype, sizeof(s32), &tmp);\n}\n\n/**\n * nla_put_s64 - Add a s64 netlink attribute to a socket buffer and align it\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @value: numeric value\n * @padattr: attribute type for the padding\n */\nstatic inline int nla_put_s64(struct sk_buff *skb, int attrtype, s64 value,\n\t\t\t      int padattr)\n{\n\ts64 tmp = value;\n\n\treturn nla_put_64bit(skb, attrtype, sizeof(s64), &tmp, padattr);\n}\n\n/**\n * nla_put_string - Add a string netlink attribute to a socket buffer\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @str: NUL terminated string\n */\nstatic inline int nla_put_string(struct sk_buff *skb, int attrtype,\n\t\t\t\t const char *str)\n{\n\treturn nla_put(skb, attrtype, strlen(str) + 1, str);\n}\n\n/**\n * nla_put_flag - Add a flag netlink attribute to a socket buffer\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n */\nstatic inline int nla_put_flag(struct sk_buff *skb, int attrtype)\n{\n\treturn nla_put(skb, attrtype, 0, NULL);\n}\n\n/**\n * nla_put_msecs - Add a msecs netlink attribute to a skb and align it\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @njiffies: number of jiffies to convert to msecs\n * @padattr: attribute type for the padding\n */\nstatic inline int nla_put_msecs(struct sk_buff *skb, int attrtype,\n\t\t\t\tunsigned long njiffies, int padattr)\n{\n\tu64 tmp = jiffies_to_msecs(njiffies);\n\n\treturn nla_put_64bit(skb, attrtype, sizeof(u64), &tmp, padattr);\n}\n\n/**\n * nla_put_in_addr - Add an IPv4 address netlink attribute to a socket\n * buffer\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @addr: IPv4 address\n */\nstatic inline int nla_put_in_addr(struct sk_buff *skb, int attrtype,\n\t\t\t\t  __be32 addr)\n{\n\t__be32 tmp = addr;\n\n\treturn nla_put_be32(skb, attrtype, tmp);\n}\n\n/**\n * nla_put_in6_addr - Add an IPv6 address netlink attribute to a socket\n * buffer\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @addr: IPv6 address\n */\nstatic inline int nla_put_in6_addr(struct sk_buff *skb, int attrtype,\n\t\t\t\t   const struct in6_addr *addr)\n{\n\treturn nla_put(skb, attrtype, sizeof(*addr), addr);\n}\n\n/**\n * nla_put_bitfield32 - Add a bitfield32 netlink attribute to a socket buffer\n * @skb: socket buffer to add attribute to\n * @attrtype: attribute type\n * @value: value carrying bits\n * @selector: selector of valid bits\n */\nstatic inline int nla_put_bitfield32(struct sk_buff *skb, int attrtype,\n\t\t\t\t     __u32 value, __u32 selector)\n{\n\tstruct nla_bitfield32 tmp = { value, selector, };\n\n\treturn nla_put(skb, attrtype, sizeof(tmp), &tmp);\n}\n\n/**\n * nla_get_u32 - return payload of u32 attribute\n * @nla: u32 netlink attribute\n */\nstatic inline u32 nla_get_u32(const struct nlattr *nla)\n{\n\treturn *(u32 *) nla_data(nla);\n}\n\n/**\n * nla_get_be32 - return payload of __be32 attribute\n * @nla: __be32 netlink attribute\n */\nstatic inline __be32 nla_get_be32(const struct nlattr *nla)\n{\n\treturn *(__be32 *) nla_data(nla);\n}\n\n/**\n * nla_get_le32 - return payload of __le32 attribute\n * @nla: __le32 netlink attribute\n */\nstatic inline __le32 nla_get_le32(const struct nlattr *nla)\n{\n\treturn *(__le32 *) nla_data(nla);\n}\n\n/**\n * nla_get_u16 - return payload of u16 attribute\n * @nla: u16 netlink attribute\n */\nstatic inline u16 nla_get_u16(const struct nlattr *nla)\n{\n\treturn *(u16 *) nla_data(nla);\n}\n\n/**\n * nla_get_be16 - return payload of __be16 attribute\n * @nla: __be16 netlink attribute\n */\nstatic inline __be16 nla_get_be16(const struct nlattr *nla)\n{\n\treturn *(__be16 *) nla_data(nla);\n}\n\n/**\n * nla_get_le16 - return payload of __le16 attribute\n * @nla: __le16 netlink attribute\n */\nstatic inline __le16 nla_get_le16(const struct nlattr *nla)\n{\n\treturn *(__le16 *) nla_data(nla);\n}\n\n/**\n * nla_get_u8 - return payload of u8 attribute\n * @nla: u8 netlink attribute\n */\nstatic inline u8 nla_get_u8(const struct nlattr *nla)\n{\n\treturn *(u8 *) nla_data(nla);\n}\n\n/**\n * nla_get_u64 - return payload of u64 attribute\n * @nla: u64 netlink attribute\n */\nstatic inline u64 nla_get_u64(const struct nlattr *nla)\n{\n\tu64 tmp;\n\n\tnla_memcpy(&tmp, nla, sizeof(tmp));\n\n\treturn tmp;\n}\n\n/**\n * nla_get_be64 - return payload of __be64 attribute\n * @nla: __be64 netlink attribute\n */\nstatic inline __be64 nla_get_be64(const struct nlattr *nla)\n{\n\t__be64 tmp;\n\n\tnla_memcpy(&tmp, nla, sizeof(tmp));\n\n\treturn tmp;\n}\n\n/**\n * nla_get_le64 - return payload of __le64 attribute\n * @nla: __le64 netlink attribute\n */\nstatic inline __le64 nla_get_le64(const struct nlattr *nla)\n{\n\treturn *(__le64 *) nla_data(nla);\n}\n\n/**\n * nla_get_s32 - return payload of s32 attribute\n * @nla: s32 netlink attribute\n */\nstatic inline s32 nla_get_s32(const struct nlattr *nla)\n{\n\treturn *(s32 *) nla_data(nla);\n}\n\n/**\n * nla_get_s16 - return payload of s16 attribute\n * @nla: s16 netlink attribute\n */\nstatic inline s16 nla_get_s16(const struct nlattr *nla)\n{\n\treturn *(s16 *) nla_data(nla);\n}\n\n/**\n * nla_get_s8 - return payload of s8 attribute\n * @nla: s8 netlink attribute\n */\nstatic inline s8 nla_get_s8(const struct nlattr *nla)\n{\n\treturn *(s8 *) nla_data(nla);\n}\n\n/**\n * nla_get_s64 - return payload of s64 attribute\n * @nla: s64 netlink attribute\n */\nstatic inline s64 nla_get_s64(const struct nlattr *nla)\n{\n\ts64 tmp;\n\n\tnla_memcpy(&tmp, nla, sizeof(tmp));\n\n\treturn tmp;\n}\n\n/**\n * nla_get_flag - return payload of flag attribute\n * @nla: flag netlink attribute\n */\nstatic inline int nla_get_flag(const struct nlattr *nla)\n{\n\treturn !!nla;\n}\n\n/**\n * nla_get_msecs - return payload of msecs attribute\n * @nla: msecs netlink attribute\n *\n * Returns the number of milliseconds in jiffies.\n */\nstatic inline unsigned long nla_get_msecs(const struct nlattr *nla)\n{\n\tu64 msecs = nla_get_u64(nla);\n\n\treturn msecs_to_jiffies((unsigned long) msecs);\n}\n\n/**\n * nla_get_in_addr - return payload of IPv4 address attribute\n * @nla: IPv4 address netlink attribute\n */\nstatic inline __be32 nla_get_in_addr(const struct nlattr *nla)\n{\n\treturn *(__be32 *) nla_data(nla);\n}\n\n/**\n * nla_get_in6_addr - return payload of IPv6 address attribute\n * @nla: IPv6 address netlink attribute\n */\nstatic inline struct in6_addr nla_get_in6_addr(const struct nlattr *nla)\n{\n\tstruct in6_addr tmp;\n\n\tnla_memcpy(&tmp, nla, sizeof(tmp));\n\treturn tmp;\n}\n\n/**\n * nla_get_bitfield32 - return payload of 32 bitfield attribute\n * @nla: nla_bitfield32 attribute\n */\nstatic inline struct nla_bitfield32 nla_get_bitfield32(const struct nlattr *nla)\n{\n\tstruct nla_bitfield32 tmp;\n\n\tnla_memcpy(&tmp, nla, sizeof(tmp));\n\treturn tmp;\n}\n\n/**\n * nla_memdup - duplicate attribute memory (kmemdup)\n * @src: netlink attribute to duplicate from\n * @gfp: GFP mask\n */\nstatic inline void *nla_memdup(const struct nlattr *src, gfp_t gfp)\n{\n\treturn kmemdup(nla_data(src), nla_len(src), gfp);\n}\n\n/**\n * nla_nest_start_noflag - Start a new level of nested attributes\n * @skb: socket buffer to add attributes to\n * @attrtype: attribute type of container\n *\n * This function exists for backward compatibility to use in APIs which never\n * marked their nest attributes with NLA_F_NESTED flag. New APIs should use\n * nla_nest_start() which sets the flag.\n *\n * Returns the container attribute or NULL on error\n */\nstatic inline struct nlattr *nla_nest_start_noflag(struct sk_buff *skb,\n\t\t\t\t\t\t   int attrtype)\n{\n\tstruct nlattr *start = (struct nlattr *)skb_tail_pointer(skb);\n\n\tif (nla_put(skb, attrtype, 0, NULL) < 0)\n\t\treturn NULL;\n\n\treturn start;\n}\n\n/**\n * nla_nest_start - Start a new level of nested attributes, with NLA_F_NESTED\n * @skb: socket buffer to add attributes to\n * @attrtype: attribute type of container\n *\n * Unlike nla_nest_start_noflag(), mark the nest attribute with NLA_F_NESTED\n * flag. This is the preferred function to use in new code.\n *\n * Returns the container attribute or NULL on error\n */\nstatic inline struct nlattr *nla_nest_start(struct sk_buff *skb, int attrtype)\n{\n\treturn nla_nest_start_noflag(skb, attrtype | NLA_F_NESTED);\n}\n\n/**\n * nla_nest_end - Finalize nesting of attributes\n * @skb: socket buffer the attributes are stored in\n * @start: container attribute\n *\n * Corrects the container attribute header to include the all\n * appeneded attributes.\n *\n * Returns the total data length of the skb.\n */\nstatic inline int nla_nest_end(struct sk_buff *skb, struct nlattr *start)\n{\n\tstart->nla_len = skb_tail_pointer(skb) - (unsigned char *)start;\n\treturn skb->len;\n}\n\n/**\n * nla_nest_cancel - Cancel nesting of attributes\n * @skb: socket buffer the message is stored in\n * @start: container attribute\n *\n * Removes the container attribute and including all nested\n * attributes. Returns -EMSGSIZE\n */\nstatic inline void nla_nest_cancel(struct sk_buff *skb, struct nlattr *start)\n{\n\tnlmsg_trim(skb, start);\n}\n\n/**\n * __nla_validate_nested - Validate a stream of nested attributes\n * @start: container attribute\n * @maxtype: maximum attribute type to be expected\n * @policy: validation policy\n * @validate: validation strictness\n * @extack: extended ACK report struct\n *\n * Validates all attributes in the nested attribute stream against the\n * specified policy. Attributes with a type exceeding maxtype will be\n * ignored. See documenation of struct nla_policy for more details.\n *\n * Returns 0 on success or a negative error code.\n */\nstatic inline int __nla_validate_nested(const struct nlattr *start, int maxtype,\n\t\t\t\t\tconst struct nla_policy *policy,\n\t\t\t\t\tunsigned int validate,\n\t\t\t\t\tstruct netlink_ext_ack *extack)\n{\n\treturn __nla_validate(nla_data(start), nla_len(start), maxtype, policy,\n\t\t\t      validate, extack);\n}\n\nstatic inline int\nnla_validate_nested(const struct nlattr *start, int maxtype,\n\t\t    const struct nla_policy *policy,\n\t\t    struct netlink_ext_ack *extack)\n{\n\treturn __nla_validate_nested(start, maxtype, policy,\n\t\t\t\t     NL_VALIDATE_STRICT, extack);\n}\n\nstatic inline int\nnla_validate_nested_deprecated(const struct nlattr *start, int maxtype,\n\t\t\t       const struct nla_policy *policy,\n\t\t\t       struct netlink_ext_ack *extack)\n{\n\treturn __nla_validate_nested(start, maxtype, policy,\n\t\t\t\t     NL_VALIDATE_LIBERAL, extack);\n}\n\n/**\n * nla_need_padding_for_64bit - test 64-bit alignment of the next attribute\n * @skb: socket buffer the message is stored in\n *\n * Return true if padding is needed to align the next attribute (nla_data()) to\n * a 64-bit aligned area.\n */\nstatic inline bool nla_need_padding_for_64bit(struct sk_buff *skb)\n{\n#ifndef CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS\n\t/* The nlattr header is 4 bytes in size, that's why we test\n\t * if the skb->data _is_ aligned.  A NOP attribute, plus\n\t * nlattr header for next attribute, will make nla_data()\n\t * 8-byte aligned.\n\t */\n\tif (IS_ALIGNED((unsigned long)skb_tail_pointer(skb), 8))\n\t\treturn true;\n#endif\n\treturn false;\n}\n\n/**\n * nla_align_64bit - 64-bit align the nla_data() of next attribute\n * @skb: socket buffer the message is stored in\n * @padattr: attribute type for the padding\n *\n * Conditionally emit a padding netlink attribute in order to make\n * the next attribute we emit have a 64-bit aligned nla_data() area.\n * This will only be done in architectures which do not have\n * CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS defined.\n *\n * Returns zero on success or a negative error code.\n */\nstatic inline int nla_align_64bit(struct sk_buff *skb, int padattr)\n{\n\tif (nla_need_padding_for_64bit(skb) &&\n\t    !nla_reserve(skb, padattr, 0))\n\t\treturn -EMSGSIZE;\n\n\treturn 0;\n}\n\n/**\n * nla_total_size_64bit - total length of attribute including padding\n * @payload: length of payload\n */\nstatic inline int nla_total_size_64bit(int payload)\n{\n\treturn NLA_ALIGN(nla_attr_size(payload))\n#ifndef CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS\n\t\t+ NLA_ALIGN(nla_attr_size(0))\n#endif\n\t\t;\n}\n\n/**\n * nla_for_each_attr - iterate over a stream of attributes\n * @pos: loop counter, set to current attribute\n * @head: head of attribute stream\n * @len: length of attribute stream\n * @rem: initialized to len, holds bytes currently remaining in stream\n */\n#define nla_for_each_attr(pos, head, len, rem) \\\n\tfor (pos = head, rem = len; \\\n\t     nla_ok(pos, rem); \\\n\t     pos = nla_next(pos, &(rem)))\n\n/**\n * nla_for_each_nested - iterate over nested attributes\n * @pos: loop counter, set to current attribute\n * @nla: attribute containing the nested attributes\n * @rem: initialized to len, holds bytes currently remaining in stream\n */\n#define nla_for_each_nested(pos, nla, rem) \\\n\tnla_for_each_attr(pos, nla_data(nla), nla_len(nla), rem)\n\n/**\n * nla_is_last - Test if attribute is last in stream\n * @nla: attribute to test\n * @rem: bytes remaining in stream\n */\nstatic inline bool nla_is_last(const struct nlattr *nla, int rem)\n{\n\treturn nla->nla_len == rem;\n}\n\nvoid nla_get_range_unsigned(const struct nla_policy *pt,\n\t\t\t    struct netlink_range_validation *range);\nvoid nla_get_range_signed(const struct nla_policy *pt,\n\t\t\t  struct netlink_range_validation_signed *range);\n\nstruct netlink_policy_dump_state;\n\nint netlink_policy_dump_add_policy(struct netlink_policy_dump_state **pstate,\n\t\t\t\t   const struct nla_policy *policy,\n\t\t\t\t   unsigned int maxtype);\nint netlink_policy_dump_get_policy_idx(struct netlink_policy_dump_state *state,\n\t\t\t\t       const struct nla_policy *policy,\n\t\t\t\t       unsigned int maxtype);\nbool netlink_policy_dump_loop(struct netlink_policy_dump_state *state);\nint netlink_policy_dump_write(struct sk_buff *skb,\n\t\t\t      struct netlink_policy_dump_state *state);\nint netlink_policy_dump_attr_size_estimate(const struct nla_policy *pt);\nint netlink_policy_dump_write_attr(struct sk_buff *skb,\n\t\t\t\t   const struct nla_policy *pt,\n\t\t\t\t   int nestattr);\nvoid netlink_policy_dump_free(struct netlink_policy_dump_state *state);\n\n#endif\n"}, "24": {"id": 24, "path": "/src/net/mac80211/sta_info.c", "content": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n * Copyright 2002-2005, Instant802 Networks, Inc.\n * Copyright 2006-2007\tJiri Benc <jbenc@suse.cz>\n * Copyright 2013-2014  Intel Mobile Communications GmbH\n * Copyright (C) 2015 - 2017 Intel Deutschland GmbH\n * Copyright (C) 2018-2020 Intel Corporation\n */\n\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/etherdevice.h>\n#include <linux/netdevice.h>\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/skbuff.h>\n#include <linux/if_arp.h>\n#include <linux/timer.h>\n#include <linux/rtnetlink.h>\n\n#include <net/codel.h>\n#include <net/mac80211.h>\n#include \"ieee80211_i.h\"\n#include \"driver-ops.h\"\n#include \"rate.h\"\n#include \"sta_info.h\"\n#include \"debugfs_sta.h\"\n#include \"mesh.h\"\n#include \"wme.h\"\n\n/**\n * DOC: STA information lifetime rules\n *\n * STA info structures (&struct sta_info) are managed in a hash table\n * for faster lookup and a list for iteration. They are managed using\n * RCU, i.e. access to the list and hash table is protected by RCU.\n *\n * Upon allocating a STA info structure with sta_info_alloc(), the caller\n * owns that structure. It must then insert it into the hash table using\n * either sta_info_insert() or sta_info_insert_rcu(); only in the latter\n * case (which acquires an rcu read section but must not be called from\n * within one) will the pointer still be valid after the call. Note that\n * the caller may not do much with the STA info before inserting it, in\n * particular, it may not start any mesh peer link management or add\n * encryption keys.\n *\n * When the insertion fails (sta_info_insert()) returns non-zero), the\n * structure will have been freed by sta_info_insert()!\n *\n * Station entries are added by mac80211 when you establish a link with a\n * peer. This means different things for the different type of interfaces\n * we support. For a regular station this mean we add the AP sta when we\n * receive an association response from the AP. For IBSS this occurs when\n * get to know about a peer on the same IBSS. For WDS we add the sta for\n * the peer immediately upon device open. When using AP mode we add stations\n * for each respective station upon request from userspace through nl80211.\n *\n * In order to remove a STA info structure, various sta_info_destroy_*()\n * calls are available.\n *\n * There is no concept of ownership on a STA entry, each structure is\n * owned by the global hash table/list until it is removed. All users of\n * the structure need to be RCU protected so that the structure won't be\n * freed before they are done using it.\n */\n\nstatic const struct rhashtable_params sta_rht_params = {\n\t.nelem_hint = 3, /* start small */\n\t.automatic_shrinking = true,\n\t.head_offset = offsetof(struct sta_info, hash_node),\n\t.key_offset = offsetof(struct sta_info, addr),\n\t.key_len = ETH_ALEN,\n\t.max_size = CONFIG_MAC80211_STA_HASH_MAX_SIZE,\n};\n\n/* Caller must hold local->sta_mtx */\nstatic int sta_info_hash_del(struct ieee80211_local *local,\n\t\t\t     struct sta_info *sta)\n{\n\treturn rhltable_remove(&local->sta_hash, &sta->hash_node,\n\t\t\t       sta_rht_params);\n}\n\nstatic void __cleanup_single_sta(struct sta_info *sta)\n{\n\tint ac, i;\n\tstruct tid_ampdu_tx *tid_tx;\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct ps_data *ps;\n\n\tif (test_sta_flag(sta, WLAN_STA_PS_STA) ||\n\t    test_sta_flag(sta, WLAN_STA_PS_DRIVER) ||\n\t    test_sta_flag(sta, WLAN_STA_PS_DELIVER)) {\n\t\tif (sta->sdata->vif.type == NL80211_IFTYPE_AP ||\n\t\t    sta->sdata->vif.type == NL80211_IFTYPE_AP_VLAN)\n\t\t\tps = &sdata->bss->ps;\n\t\telse if (ieee80211_vif_is_mesh(&sdata->vif))\n\t\t\tps = &sdata->u.mesh.ps;\n\t\telse\n\t\t\treturn;\n\n\t\tclear_sta_flag(sta, WLAN_STA_PS_STA);\n\t\tclear_sta_flag(sta, WLAN_STA_PS_DRIVER);\n\t\tclear_sta_flag(sta, WLAN_STA_PS_DELIVER);\n\n\t\tatomic_dec(&ps->num_sta_ps);\n\t}\n\n\tif (sta->sta.txq[0]) {\n\t\tfor (i = 0; i < ARRAY_SIZE(sta->sta.txq); i++) {\n\t\t\tstruct txq_info *txqi;\n\n\t\t\tif (!sta->sta.txq[i])\n\t\t\t\tcontinue;\n\n\t\t\ttxqi = to_txq_info(sta->sta.txq[i]);\n\n\t\t\tieee80211_txq_purge(local, txqi);\n\t\t}\n\t}\n\n\tfor (ac = 0; ac < IEEE80211_NUM_ACS; ac++) {\n\t\tlocal->total_ps_buffered -= skb_queue_len(&sta->ps_tx_buf[ac]);\n\t\tieee80211_purge_tx_queue(&local->hw, &sta->ps_tx_buf[ac]);\n\t\tieee80211_purge_tx_queue(&local->hw, &sta->tx_filtered[ac]);\n\t}\n\n\tif (ieee80211_vif_is_mesh(&sdata->vif))\n\t\tmesh_sta_cleanup(sta);\n\n\tcancel_work_sync(&sta->drv_deliver_wk);\n\n\t/*\n\t * Destroy aggregation state here. It would be nice to wait for the\n\t * driver to finish aggregation stop and then clean up, but for now\n\t * drivers have to handle aggregation stop being requested, followed\n\t * directly by station destruction.\n\t */\n\tfor (i = 0; i < IEEE80211_NUM_TIDS; i++) {\n\t\tkfree(sta->ampdu_mlme.tid_start_tx[i]);\n\t\ttid_tx = rcu_dereference_raw(sta->ampdu_mlme.tid_tx[i]);\n\t\tif (!tid_tx)\n\t\t\tcontinue;\n\t\tieee80211_purge_tx_queue(&local->hw, &tid_tx->pending);\n\t\tkfree(tid_tx);\n\t}\n}\n\nstatic void cleanup_single_sta(struct sta_info *sta)\n{\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct ieee80211_local *local = sdata->local;\n\n\t__cleanup_single_sta(sta);\n\tsta_info_free(local, sta);\n}\n\nstruct rhlist_head *sta_info_hash_lookup(struct ieee80211_local *local,\n\t\t\t\t\t const u8 *addr)\n{\n\treturn rhltable_lookup(&local->sta_hash, addr, sta_rht_params);\n}\n\n/* protected by RCU */\nstruct sta_info *sta_info_get(struct ieee80211_sub_if_data *sdata,\n\t\t\t      const u8 *addr)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct rhlist_head *tmp;\n\tstruct sta_info *sta;\n\n\trcu_read_lock();\n\tfor_each_sta_info(local, addr, sta, tmp) {\n\t\tif (sta->sdata == sdata) {\n\t\t\trcu_read_unlock();\n\t\t\t/* this is safe as the caller must already hold\n\t\t\t * another rcu read section or the mutex\n\t\t\t */\n\t\t\treturn sta;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn NULL;\n}\n\n/*\n * Get sta info either from the specified interface\n * or from one of its vlans\n */\nstruct sta_info *sta_info_get_bss(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t  const u8 *addr)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct rhlist_head *tmp;\n\tstruct sta_info *sta;\n\n\trcu_read_lock();\n\tfor_each_sta_info(local, addr, sta, tmp) {\n\t\tif (sta->sdata == sdata ||\n\t\t    (sta->sdata->bss && sta->sdata->bss == sdata->bss)) {\n\t\t\trcu_read_unlock();\n\t\t\t/* this is safe as the caller must already hold\n\t\t\t * another rcu read section or the mutex\n\t\t\t */\n\t\t\treturn sta;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn NULL;\n}\n\nstruct sta_info *sta_info_get_by_addrs(struct ieee80211_local *local,\n\t\t\t\t       const u8 *sta_addr, const u8 *vif_addr)\n{\n\tstruct rhlist_head *tmp;\n\tstruct sta_info *sta;\n\n\tfor_each_sta_info(local, sta_addr, sta, tmp) {\n\t\tif (ether_addr_equal(vif_addr, sta->sdata->vif.addr))\n\t\t\treturn sta;\n\t}\n\n\treturn NULL;\n}\n\nstruct sta_info *sta_info_get_by_idx(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t     int idx)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sta_info *sta;\n\tint i = 0;\n\n\tlist_for_each_entry_rcu(sta, &local->sta_list, list,\n\t\t\t\tlockdep_is_held(&local->sta_mtx)) {\n\t\tif (sdata != sta->sdata)\n\t\t\tcontinue;\n\t\tif (i < idx) {\n\t\t\t++i;\n\t\t\tcontinue;\n\t\t}\n\t\treturn sta;\n\t}\n\n\treturn NULL;\n}\n\n/**\n * sta_info_free - free STA\n *\n * @local: pointer to the global information\n * @sta: STA info to free\n *\n * This function must undo everything done by sta_info_alloc()\n * that may happen before sta_info_insert(). It may only be\n * called when sta_info_insert() has not been attempted (and\n * if that fails, the station is freed anyway.)\n */\nvoid sta_info_free(struct ieee80211_local *local, struct sta_info *sta)\n{\n\t/*\n\t * If we had used sta_info_pre_move_state() then we might not\n\t * have gone through the state transitions down again, so do\n\t * it here now (and warn if it's inserted).\n\t *\n\t * This will clear state such as fast TX/RX that may have been\n\t * allocated during state transitions.\n\t */\n\twhile (sta->sta_state > IEEE80211_STA_NONE) {\n\t\tint ret;\n\n\t\tWARN_ON_ONCE(test_sta_flag(sta, WLAN_STA_INSERTED));\n\n\t\tret = sta_info_move_state(sta, sta->sta_state - 1);\n\t\tif (WARN_ONCE(ret, \"sta_info_move_state() returned %d\\n\", ret))\n\t\t\tbreak;\n\t}\n\n\tif (sta->rate_ctrl)\n\t\trate_control_free_sta(sta);\n\n\tsta_dbg(sta->sdata, \"Destroyed STA %pM\\n\", sta->sta.addr);\n\n\tif (sta->sta.txq[0])\n\t\tkfree(to_txq_info(sta->sta.txq[0]));\n\tkfree(rcu_dereference_raw(sta->sta.rates));\n#ifdef CONFIG_MAC80211_MESH\n\tkfree(sta->mesh);\n#endif\n\tfree_percpu(sta->pcpu_rx_stats);\n\tkfree(sta);\n}\n\n/* Caller must hold local->sta_mtx */\nstatic int sta_info_hash_add(struct ieee80211_local *local,\n\t\t\t     struct sta_info *sta)\n{\n\treturn rhltable_insert(&local->sta_hash, &sta->hash_node,\n\t\t\t       sta_rht_params);\n}\n\nstatic void sta_deliver_ps_frames(struct work_struct *wk)\n{\n\tstruct sta_info *sta;\n\n\tsta = container_of(wk, struct sta_info, drv_deliver_wk);\n\n\tif (sta->dead)\n\t\treturn;\n\n\tlocal_bh_disable();\n\tif (!test_sta_flag(sta, WLAN_STA_PS_STA))\n\t\tieee80211_sta_ps_deliver_wakeup(sta);\n\telse if (test_and_clear_sta_flag(sta, WLAN_STA_PSPOLL))\n\t\tieee80211_sta_ps_deliver_poll_response(sta);\n\telse if (test_and_clear_sta_flag(sta, WLAN_STA_UAPSD))\n\t\tieee80211_sta_ps_deliver_uapsd(sta);\n\tlocal_bh_enable();\n}\n\nstatic int sta_prepare_rate_control(struct ieee80211_local *local,\n\t\t\t\t    struct sta_info *sta, gfp_t gfp)\n{\n\tif (ieee80211_hw_check(&local->hw, HAS_RATE_CONTROL))\n\t\treturn 0;\n\n\tsta->rate_ctrl = local->rate_ctrl;\n\tsta->rate_ctrl_priv = rate_control_alloc_sta(sta->rate_ctrl,\n\t\t\t\t\t\t     sta, gfp);\n\tif (!sta->rate_ctrl_priv)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstruct sta_info *sta_info_alloc(struct ieee80211_sub_if_data *sdata,\n\t\t\t\tconst u8 *addr, gfp_t gfp)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct ieee80211_hw *hw = &local->hw;\n\tstruct sta_info *sta;\n\tint i;\n\n\tsta = kzalloc(sizeof(*sta) + hw->sta_data_size, gfp);\n\tif (!sta)\n\t\treturn NULL;\n\n\tif (ieee80211_hw_check(hw, USES_RSS)) {\n\t\tsta->pcpu_rx_stats =\n\t\t\talloc_percpu_gfp(struct ieee80211_sta_rx_stats, gfp);\n\t\tif (!sta->pcpu_rx_stats)\n\t\t\tgoto free;\n\t}\n\n\tspin_lock_init(&sta->lock);\n\tspin_lock_init(&sta->ps_lock);\n\tINIT_WORK(&sta->drv_deliver_wk, sta_deliver_ps_frames);\n\tINIT_WORK(&sta->ampdu_mlme.work, ieee80211_ba_session_work);\n\tmutex_init(&sta->ampdu_mlme.mtx);\n#ifdef CONFIG_MAC80211_MESH\n\tif (ieee80211_vif_is_mesh(&sdata->vif)) {\n\t\tsta->mesh = kzalloc(sizeof(*sta->mesh), gfp);\n\t\tif (!sta->mesh)\n\t\t\tgoto free;\n\t\tsta->mesh->plink_sta = sta;\n\t\tspin_lock_init(&sta->mesh->plink_lock);\n\t\tif (ieee80211_vif_is_mesh(&sdata->vif) &&\n\t\t    !sdata->u.mesh.user_mpm)\n\t\t\ttimer_setup(&sta->mesh->plink_timer, mesh_plink_timer,\n\t\t\t\t    0);\n\t\tsta->mesh->nonpeer_pm = NL80211_MESH_POWER_ACTIVE;\n\t}\n#endif\n\n\tmemcpy(sta->addr, addr, ETH_ALEN);\n\tmemcpy(sta->sta.addr, addr, ETH_ALEN);\n\tsta->sta.max_rx_aggregation_subframes =\n\t\tlocal->hw.max_rx_aggregation_subframes;\n\n\t/* Extended Key ID needs to install keys for keyid 0 and 1 Rx-only.\n\t * The Tx path starts to use a key as soon as the key slot ptk_idx\n\t * references to is not NULL. To not use the initial Rx-only key\n\t * prematurely for Tx initialize ptk_idx to an impossible PTK keyid\n\t * which always will refer to a NULL key.\n\t */\n\tBUILD_BUG_ON(ARRAY_SIZE(sta->ptk) <= INVALID_PTK_KEYIDX);\n\tsta->ptk_idx = INVALID_PTK_KEYIDX;\n\n\tsta->local = local;\n\tsta->sdata = sdata;\n\tsta->rx_stats.last_rx = jiffies;\n\n\tu64_stats_init(&sta->rx_stats.syncp);\n\n\tsta->sta_state = IEEE80211_STA_NONE;\n\n\t/* Mark TID as unreserved */\n\tsta->reserved_tid = IEEE80211_TID_UNRESERVED;\n\n\tsta->last_connected = ktime_get_seconds();\n\tewma_signal_init(&sta->rx_stats_avg.signal);\n\tewma_avg_signal_init(&sta->status_stats.avg_ack_signal);\n\tfor (i = 0; i < ARRAY_SIZE(sta->rx_stats_avg.chain_signal); i++)\n\t\tewma_signal_init(&sta->rx_stats_avg.chain_signal[i]);\n\n\tif (local->ops->wake_tx_queue) {\n\t\tvoid *txq_data;\n\t\tint size = sizeof(struct txq_info) +\n\t\t\t   ALIGN(hw->txq_data_size, sizeof(void *));\n\n\t\ttxq_data = kcalloc(ARRAY_SIZE(sta->sta.txq), size, gfp);\n\t\tif (!txq_data)\n\t\t\tgoto free;\n\n\t\tfor (i = 0; i < ARRAY_SIZE(sta->sta.txq); i++) {\n\t\t\tstruct txq_info *txq = txq_data + i * size;\n\n\t\t\t/* might not do anything for the bufferable MMPDU TXQ */\n\t\t\tieee80211_txq_init(sdata, sta, txq, i);\n\t\t}\n\t}\n\n\tif (sta_prepare_rate_control(local, sta, gfp))\n\t\tgoto free_txq;\n\n\tsta->airtime_weight = IEEE80211_DEFAULT_AIRTIME_WEIGHT;\n\n\tfor (i = 0; i < IEEE80211_NUM_ACS; i++) {\n\t\tskb_queue_head_init(&sta->ps_tx_buf[i]);\n\t\tskb_queue_head_init(&sta->tx_filtered[i]);\n\t\tsta->airtime[i].deficit = sta->airtime_weight;\n\t\tatomic_set(&sta->airtime[i].aql_tx_pending, 0);\n\t\tsta->airtime[i].aql_limit_low = local->aql_txq_limit_low[i];\n\t\tsta->airtime[i].aql_limit_high = local->aql_txq_limit_high[i];\n\t}\n\n\tfor (i = 0; i < IEEE80211_NUM_TIDS; i++)\n\t\tsta->last_seq_ctrl[i] = cpu_to_le16(USHRT_MAX);\n\n\tfor (i = 0; i < NUM_NL80211_BANDS; i++) {\n\t\tu32 mandatory = 0;\n\t\tint r;\n\n\t\tif (!hw->wiphy->bands[i])\n\t\t\tcontinue;\n\n\t\tswitch (i) {\n\t\tcase NL80211_BAND_2GHZ:\n\t\t\t/*\n\t\t\t * We use both here, even if we cannot really know for\n\t\t\t * sure the station will support both, but the only use\n\t\t\t * for this is when we don't know anything yet and send\n\t\t\t * management frames, and then we'll pick the lowest\n\t\t\t * possible rate anyway.\n\t\t\t * If we don't include _G here, we cannot find a rate\n\t\t\t * in P2P, and thus trigger the WARN_ONCE() in rate.c\n\t\t\t */\n\t\t\tmandatory = IEEE80211_RATE_MANDATORY_B |\n\t\t\t\t    IEEE80211_RATE_MANDATORY_G;\n\t\t\tbreak;\n\t\tcase NL80211_BAND_5GHZ:\n\t\t\tmandatory = IEEE80211_RATE_MANDATORY_A;\n\t\t\tbreak;\n\t\tcase NL80211_BAND_60GHZ:\n\t\t\tWARN_ON(1);\n\t\t\tmandatory = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tfor (r = 0; r < hw->wiphy->bands[i]->n_bitrates; r++) {\n\t\t\tstruct ieee80211_rate *rate;\n\n\t\t\trate = &hw->wiphy->bands[i]->bitrates[r];\n\n\t\t\tif (!(rate->flags & mandatory))\n\t\t\t\tcontinue;\n\t\t\tsta->sta.supp_rates[i] |= BIT(r);\n\t\t}\n\t}\n\n\tsta->sta.smps_mode = IEEE80211_SMPS_OFF;\n\tif (sdata->vif.type == NL80211_IFTYPE_AP ||\n\t    sdata->vif.type == NL80211_IFTYPE_AP_VLAN) {\n\t\tstruct ieee80211_supported_band *sband;\n\t\tu8 smps;\n\n\t\tsband = ieee80211_get_sband(sdata);\n\t\tif (!sband)\n\t\t\tgoto free_txq;\n\n\t\tsmps = (sband->ht_cap.cap & IEEE80211_HT_CAP_SM_PS) >>\n\t\t\tIEEE80211_HT_CAP_SM_PS_SHIFT;\n\t\t/*\n\t\t * Assume that hostapd advertises our caps in the beacon and\n\t\t * this is the known_smps_mode for a station that just assciated\n\t\t */\n\t\tswitch (smps) {\n\t\tcase WLAN_HT_SMPS_CONTROL_DISABLED:\n\t\t\tsta->known_smps_mode = IEEE80211_SMPS_OFF;\n\t\t\tbreak;\n\t\tcase WLAN_HT_SMPS_CONTROL_STATIC:\n\t\t\tsta->known_smps_mode = IEEE80211_SMPS_STATIC;\n\t\t\tbreak;\n\t\tcase WLAN_HT_SMPS_CONTROL_DYNAMIC:\n\t\t\tsta->known_smps_mode = IEEE80211_SMPS_DYNAMIC;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ON(1);\n\t\t}\n\t}\n\n\tsta->sta.max_rc_amsdu_len = IEEE80211_MAX_MPDU_LEN_HT_BA;\n\n\tsta->cparams.ce_threshold = CODEL_DISABLED_THRESHOLD;\n\tsta->cparams.target = MS2TIME(20);\n\tsta->cparams.interval = MS2TIME(100);\n\tsta->cparams.ecn = true;\n\n\tsta_dbg(sdata, \"Allocated STA %pM\\n\", sta->sta.addr);\n\n\treturn sta;\n\nfree_txq:\n\tif (sta->sta.txq[0])\n\t\tkfree(to_txq_info(sta->sta.txq[0]));\nfree:\n\tfree_percpu(sta->pcpu_rx_stats);\n#ifdef CONFIG_MAC80211_MESH\n\tkfree(sta->mesh);\n#endif\n\tkfree(sta);\n\treturn NULL;\n}\n\nstatic int sta_info_insert_check(struct sta_info *sta)\n{\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\n\t/*\n\t * Can't be a WARN_ON because it can be triggered through a race:\n\t * something inserts a STA (on one CPU) without holding the RTNL\n\t * and another CPU turns off the net device.\n\t */\n\tif (unlikely(!ieee80211_sdata_running(sdata)))\n\t\treturn -ENETDOWN;\n\n\tif (WARN_ON(ether_addr_equal(sta->sta.addr, sdata->vif.addr) ||\n\t\t    is_multicast_ether_addr(sta->sta.addr)))\n\t\treturn -EINVAL;\n\n\t/* The RCU read lock is required by rhashtable due to\n\t * asynchronous resize/rehash.  We also require the mutex\n\t * for correctness.\n\t */\n\trcu_read_lock();\n\tlockdep_assert_held(&sdata->local->sta_mtx);\n\tif (ieee80211_hw_check(&sdata->local->hw, NEEDS_UNIQUE_STA_ADDR) &&\n\t    ieee80211_find_sta_by_ifaddr(&sdata->local->hw, sta->addr, NULL)) {\n\t\trcu_read_unlock();\n\t\treturn -ENOTUNIQ;\n\t}\n\trcu_read_unlock();\n\n\treturn 0;\n}\n\nstatic int sta_info_insert_drv_state(struct ieee80211_local *local,\n\t\t\t\t     struct ieee80211_sub_if_data *sdata,\n\t\t\t\t     struct sta_info *sta)\n{\n\tenum ieee80211_sta_state state;\n\tint err = 0;\n\n\tfor (state = IEEE80211_STA_NOTEXIST; state < sta->sta_state; state++) {\n\t\terr = drv_sta_state(local, sdata, sta, state, state + 1);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\tif (!err) {\n\t\t/*\n\t\t * Drivers using legacy sta_add/sta_remove callbacks only\n\t\t * get uploaded set to true after sta_add is called.\n\t\t */\n\t\tif (!local->ops->sta_add)\n\t\t\tsta->uploaded = true;\n\t\treturn 0;\n\t}\n\n\tif (sdata->vif.type == NL80211_IFTYPE_ADHOC) {\n\t\tsdata_info(sdata,\n\t\t\t   \"failed to move IBSS STA %pM to state %d (%d) - keeping it anyway\\n\",\n\t\t\t   sta->sta.addr, state + 1, err);\n\t\terr = 0;\n\t}\n\n\t/* unwind on error */\n\tfor (; state > IEEE80211_STA_NOTEXIST; state--)\n\t\tWARN_ON(drv_sta_state(local, sdata, sta, state, state - 1));\n\n\treturn err;\n}\n\nstatic void\nieee80211_recalc_p2p_go_ps_allowed(struct ieee80211_sub_if_data *sdata)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tbool allow_p2p_go_ps = sdata->vif.p2p;\n\tstruct sta_info *sta;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(sta, &local->sta_list, list) {\n\t\tif (sdata != sta->sdata ||\n\t\t    !test_sta_flag(sta, WLAN_STA_ASSOC))\n\t\t\tcontinue;\n\t\tif (!sta->sta.support_p2p_ps) {\n\t\t\tallow_p2p_go_ps = false;\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tif (allow_p2p_go_ps != sdata->vif.bss_conf.allow_p2p_go_ps) {\n\t\tsdata->vif.bss_conf.allow_p2p_go_ps = allow_p2p_go_ps;\n\t\tieee80211_bss_info_change_notify(sdata, BSS_CHANGED_P2P_PS);\n\t}\n}\n\n/*\n * should be called with sta_mtx locked\n * this function replaces the mutex lock\n * with a RCU lock\n */\nstatic int sta_info_insert_finish(struct sta_info *sta) __acquires(RCU)\n{\n\tstruct ieee80211_local *local = sta->local;\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct station_info *sinfo = NULL;\n\tint err = 0;\n\n\tlockdep_assert_held(&local->sta_mtx);\n\n\t/* check if STA exists already */\n\tif (sta_info_get_bss(sdata, sta->sta.addr)) {\n\t\terr = -EEXIST;\n\t\tgoto out_err;\n\t}\n\n\tsinfo = kzalloc(sizeof(struct station_info), GFP_KERNEL);\n\tif (!sinfo) {\n\t\terr = -ENOMEM;\n\t\tgoto out_err;\n\t}\n\n\tlocal->num_sta++;\n\tlocal->sta_generation++;\n\tsmp_mb();\n\n\t/* simplify things and don't accept BA sessions yet */\n\tset_sta_flag(sta, WLAN_STA_BLOCK_BA);\n\n\t/* make the station visible */\n\terr = sta_info_hash_add(local, sta);\n\tif (err)\n\t\tgoto out_drop_sta;\n\n\tlist_add_tail_rcu(&sta->list, &local->sta_list);\n\n\t/* notify driver */\n\terr = sta_info_insert_drv_state(local, sdata, sta);\n\tif (err)\n\t\tgoto out_remove;\n\n\tset_sta_flag(sta, WLAN_STA_INSERTED);\n\n\tif (sta->sta_state >= IEEE80211_STA_ASSOC) {\n\t\tieee80211_recalc_min_chandef(sta->sdata);\n\t\tif (!sta->sta.support_p2p_ps)\n\t\t\tieee80211_recalc_p2p_go_ps_allowed(sta->sdata);\n\t}\n\n\t/* accept BA sessions now */\n\tclear_sta_flag(sta, WLAN_STA_BLOCK_BA);\n\n\tieee80211_sta_debugfs_add(sta);\n\trate_control_add_sta_debugfs(sta);\n\n\tsinfo->generation = local->sta_generation;\n\tcfg80211_new_sta(sdata->dev, sta->sta.addr, sinfo, GFP_KERNEL);\n\tkfree(sinfo);\n\n\tsta_dbg(sdata, \"Inserted STA %pM\\n\", sta->sta.addr);\n\n\t/* move reference to rcu-protected */\n\trcu_read_lock();\n\tmutex_unlock(&local->sta_mtx);\n\n\tif (ieee80211_vif_is_mesh(&sdata->vif))\n\t\tmesh_accept_plinks_update(sdata);\n\n\treturn 0;\n out_remove:\n\tsta_info_hash_del(local, sta);\n\tlist_del_rcu(&sta->list);\n out_drop_sta:\n\tlocal->num_sta--;\n\tsynchronize_net();\n\tcleanup_single_sta(sta);\n out_err:\n\tmutex_unlock(&local->sta_mtx);\n\tkfree(sinfo);\n\trcu_read_lock();\n\treturn err;\n}\n\nint sta_info_insert_rcu(struct sta_info *sta) __acquires(RCU)\n{\n\tstruct ieee80211_local *local = sta->local;\n\tint err;\n\n\tmight_sleep();\n\n\tmutex_lock(&local->sta_mtx);\n\n\terr = sta_info_insert_check(sta);\n\tif (err) {\n\t\tsta_info_free(local, sta);\n\t\tmutex_unlock(&local->sta_mtx);\n\t\trcu_read_lock();\n\t\treturn err;\n\t}\n\n\treturn sta_info_insert_finish(sta);\n}\n\nint sta_info_insert(struct sta_info *sta)\n{\n\tint err = sta_info_insert_rcu(sta);\n\n\trcu_read_unlock();\n\n\treturn err;\n}\n\nstatic inline void __bss_tim_set(u8 *tim, u16 id)\n{\n\t/*\n\t * This format has been mandated by the IEEE specifications,\n\t * so this line may not be changed to use the __set_bit() format.\n\t */\n\ttim[id / 8] |= (1 << (id % 8));\n}\n\nstatic inline void __bss_tim_clear(u8 *tim, u16 id)\n{\n\t/*\n\t * This format has been mandated by the IEEE specifications,\n\t * so this line may not be changed to use the __clear_bit() format.\n\t */\n\ttim[id / 8] &= ~(1 << (id % 8));\n}\n\nstatic inline bool __bss_tim_get(u8 *tim, u16 id)\n{\n\t/*\n\t * This format has been mandated by the IEEE specifications,\n\t * so this line may not be changed to use the test_bit() format.\n\t */\n\treturn tim[id / 8] & (1 << (id % 8));\n}\n\nstatic unsigned long ieee80211_tids_for_ac(int ac)\n{\n\t/* If we ever support TIDs > 7, this obviously needs to be adjusted */\n\tswitch (ac) {\n\tcase IEEE80211_AC_VO:\n\t\treturn BIT(6) | BIT(7);\n\tcase IEEE80211_AC_VI:\n\t\treturn BIT(4) | BIT(5);\n\tcase IEEE80211_AC_BE:\n\t\treturn BIT(0) | BIT(3);\n\tcase IEEE80211_AC_BK:\n\t\treturn BIT(1) | BIT(2);\n\tdefault:\n\t\tWARN_ON(1);\n\t\treturn 0;\n\t}\n}\n\nstatic void __sta_info_recalc_tim(struct sta_info *sta, bool ignore_pending)\n{\n\tstruct ieee80211_local *local = sta->local;\n\tstruct ps_data *ps;\n\tbool indicate_tim = false;\n\tu8 ignore_for_tim = sta->sta.uapsd_queues;\n\tint ac;\n\tu16 id = sta->sta.aid;\n\n\tif (sta->sdata->vif.type == NL80211_IFTYPE_AP ||\n\t    sta->sdata->vif.type == NL80211_IFTYPE_AP_VLAN) {\n\t\tif (WARN_ON_ONCE(!sta->sdata->bss))\n\t\t\treturn;\n\n\t\tps = &sta->sdata->bss->ps;\n#ifdef CONFIG_MAC80211_MESH\n\t} else if (ieee80211_vif_is_mesh(&sta->sdata->vif)) {\n\t\tps = &sta->sdata->u.mesh.ps;\n#endif\n\t} else {\n\t\treturn;\n\t}\n\n\t/* No need to do anything if the driver does all */\n\tif (ieee80211_hw_check(&local->hw, AP_LINK_PS) && !local->ops->set_tim)\n\t\treturn;\n\n\tif (sta->dead)\n\t\tgoto done;\n\n\t/*\n\t * If all ACs are delivery-enabled then we should build\n\t * the TIM bit for all ACs anyway; if only some are then\n\t * we ignore those and build the TIM bit using only the\n\t * non-enabled ones.\n\t */\n\tif (ignore_for_tim == BIT(IEEE80211_NUM_ACS) - 1)\n\t\tignore_for_tim = 0;\n\n\tif (ignore_pending)\n\t\tignore_for_tim = BIT(IEEE80211_NUM_ACS) - 1;\n\n\tfor (ac = 0; ac < IEEE80211_NUM_ACS; ac++) {\n\t\tunsigned long tids;\n\n\t\tif (ignore_for_tim & ieee80211_ac_to_qos_mask[ac])\n\t\t\tcontinue;\n\n\t\tindicate_tim |= !skb_queue_empty(&sta->tx_filtered[ac]) ||\n\t\t\t\t!skb_queue_empty(&sta->ps_tx_buf[ac]);\n\t\tif (indicate_tim)\n\t\t\tbreak;\n\n\t\ttids = ieee80211_tids_for_ac(ac);\n\n\t\tindicate_tim |=\n\t\t\tsta->driver_buffered_tids & tids;\n\t\tindicate_tim |=\n\t\t\tsta->txq_buffered_tids & tids;\n\t}\n\n done:\n\tspin_lock_bh(&local->tim_lock);\n\n\tif (indicate_tim == __bss_tim_get(ps->tim, id))\n\t\tgoto out_unlock;\n\n\tif (indicate_tim)\n\t\t__bss_tim_set(ps->tim, id);\n\telse\n\t\t__bss_tim_clear(ps->tim, id);\n\n\tif (local->ops->set_tim && !WARN_ON(sta->dead)) {\n\t\tlocal->tim_in_locked_section = true;\n\t\tdrv_set_tim(local, &sta->sta, indicate_tim);\n\t\tlocal->tim_in_locked_section = false;\n\t}\n\nout_unlock:\n\tspin_unlock_bh(&local->tim_lock);\n}\n\nvoid sta_info_recalc_tim(struct sta_info *sta)\n{\n\t__sta_info_recalc_tim(sta, false);\n}\n\nstatic bool sta_info_buffer_expired(struct sta_info *sta, struct sk_buff *skb)\n{\n\tstruct ieee80211_tx_info *info;\n\tint timeout;\n\n\tif (!skb)\n\t\treturn false;\n\n\tinfo = IEEE80211_SKB_CB(skb);\n\n\t/* Timeout: (2 * listen_interval * beacon_int * 1024 / 1000000) sec */\n\ttimeout = (sta->listen_interval *\n\t\t   sta->sdata->vif.bss_conf.beacon_int *\n\t\t   32 / 15625) * HZ;\n\tif (timeout < STA_TX_BUFFER_EXPIRE)\n\t\ttimeout = STA_TX_BUFFER_EXPIRE;\n\treturn time_after(jiffies, info->control.jiffies + timeout);\n}\n\n\nstatic bool sta_info_cleanup_expire_buffered_ac(struct ieee80211_local *local,\n\t\t\t\t\t\tstruct sta_info *sta, int ac)\n{\n\tunsigned long flags;\n\tstruct sk_buff *skb;\n\n\t/*\n\t * First check for frames that should expire on the filtered\n\t * queue. Frames here were rejected by the driver and are on\n\t * a separate queue to avoid reordering with normal PS-buffered\n\t * frames. They also aren't accounted for right now in the\n\t * total_ps_buffered counter.\n\t */\n\tfor (;;) {\n\t\tspin_lock_irqsave(&sta->tx_filtered[ac].lock, flags);\n\t\tskb = skb_peek(&sta->tx_filtered[ac]);\n\t\tif (sta_info_buffer_expired(sta, skb))\n\t\t\tskb = __skb_dequeue(&sta->tx_filtered[ac]);\n\t\telse\n\t\t\tskb = NULL;\n\t\tspin_unlock_irqrestore(&sta->tx_filtered[ac].lock, flags);\n\n\t\t/*\n\t\t * Frames are queued in order, so if this one\n\t\t * hasn't expired yet we can stop testing. If\n\t\t * we actually reached the end of the queue we\n\t\t * also need to stop, of course.\n\t\t */\n\t\tif (!skb)\n\t\t\tbreak;\n\t\tieee80211_free_txskb(&local->hw, skb);\n\t}\n\n\t/*\n\t * Now also check the normal PS-buffered queue, this will\n\t * only find something if the filtered queue was emptied\n\t * since the filtered frames are all before the normal PS\n\t * buffered frames.\n\t */\n\tfor (;;) {\n\t\tspin_lock_irqsave(&sta->ps_tx_buf[ac].lock, flags);\n\t\tskb = skb_peek(&sta->ps_tx_buf[ac]);\n\t\tif (sta_info_buffer_expired(sta, skb))\n\t\t\tskb = __skb_dequeue(&sta->ps_tx_buf[ac]);\n\t\telse\n\t\t\tskb = NULL;\n\t\tspin_unlock_irqrestore(&sta->ps_tx_buf[ac].lock, flags);\n\n\t\t/*\n\t\t * frames are queued in order, so if this one\n\t\t * hasn't expired yet (or we reached the end of\n\t\t * the queue) we can stop testing\n\t\t */\n\t\tif (!skb)\n\t\t\tbreak;\n\n\t\tlocal->total_ps_buffered--;\n\t\tps_dbg(sta->sdata, \"Buffered frame expired (STA %pM)\\n\",\n\t\t       sta->sta.addr);\n\t\tieee80211_free_txskb(&local->hw, skb);\n\t}\n\n\t/*\n\t * Finally, recalculate the TIM bit for this station -- it might\n\t * now be clear because the station was too slow to retrieve its\n\t * frames.\n\t */\n\tsta_info_recalc_tim(sta);\n\n\t/*\n\t * Return whether there are any frames still buffered, this is\n\t * used to check whether the cleanup timer still needs to run,\n\t * if there are no frames we don't need to rearm the timer.\n\t */\n\treturn !(skb_queue_empty(&sta->ps_tx_buf[ac]) &&\n\t\t skb_queue_empty(&sta->tx_filtered[ac]));\n}\n\nstatic bool sta_info_cleanup_expire_buffered(struct ieee80211_local *local,\n\t\t\t\t\t     struct sta_info *sta)\n{\n\tbool have_buffered = false;\n\tint ac;\n\n\t/* This is only necessary for stations on BSS/MBSS interfaces */\n\tif (!sta->sdata->bss &&\n\t    !ieee80211_vif_is_mesh(&sta->sdata->vif))\n\t\treturn false;\n\n\tfor (ac = 0; ac < IEEE80211_NUM_ACS; ac++)\n\t\thave_buffered |=\n\t\t\tsta_info_cleanup_expire_buffered_ac(local, sta, ac);\n\n\treturn have_buffered;\n}\n\nstatic int __must_check __sta_info_destroy_part1(struct sta_info *sta)\n{\n\tstruct ieee80211_local *local;\n\tstruct ieee80211_sub_if_data *sdata;\n\tint ret;\n\n\tmight_sleep();\n\n\tif (!sta)\n\t\treturn -ENOENT;\n\n\tlocal = sta->local;\n\tsdata = sta->sdata;\n\n\tlockdep_assert_held(&local->sta_mtx);\n\n\t/*\n\t * Before removing the station from the driver and\n\t * rate control, it might still start new aggregation\n\t * sessions -- block that to make sure the tear-down\n\t * will be sufficient.\n\t */\n\tset_sta_flag(sta, WLAN_STA_BLOCK_BA);\n\tieee80211_sta_tear_down_BA_sessions(sta, AGG_STOP_DESTROY_STA);\n\n\t/*\n\t * Before removing the station from the driver there might be pending\n\t * rx frames on RSS queues sent prior to the disassociation - wait for\n\t * all such frames to be processed.\n\t */\n\tdrv_sync_rx_queues(local, sta);\n\n\tret = sta_info_hash_del(local, sta);\n\tif (WARN_ON(ret))\n\t\treturn ret;\n\n\t/*\n\t * for TDLS peers, make sure to return to the base channel before\n\t * removal.\n\t */\n\tif (test_sta_flag(sta, WLAN_STA_TDLS_OFF_CHANNEL)) {\n\t\tdrv_tdls_cancel_channel_switch(local, sdata, &sta->sta);\n\t\tclear_sta_flag(sta, WLAN_STA_TDLS_OFF_CHANNEL);\n\t}\n\n\tlist_del_rcu(&sta->list);\n\tsta->removed = true;\n\n\tdrv_sta_pre_rcu_remove(local, sta->sdata, sta);\n\n\tif (sdata->vif.type == NL80211_IFTYPE_AP_VLAN &&\n\t    rcu_access_pointer(sdata->u.vlan.sta) == sta)\n\t\tRCU_INIT_POINTER(sdata->u.vlan.sta, NULL);\n\n\treturn 0;\n}\n\nstatic void __sta_info_destroy_part2(struct sta_info *sta)\n{\n\tstruct ieee80211_local *local = sta->local;\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct station_info *sinfo;\n\tint ret;\n\n\t/*\n\t * NOTE: This assumes at least synchronize_net() was done\n\t *\t after _part1 and before _part2!\n\t */\n\n\tmight_sleep();\n\tlockdep_assert_held(&local->sta_mtx);\n\n\tif (sta->sta_state == IEEE80211_STA_AUTHORIZED) {\n\t\tret = sta_info_move_state(sta, IEEE80211_STA_ASSOC);\n\t\tWARN_ON_ONCE(ret);\n\t}\n\n\t/* now keys can no longer be reached */\n\tieee80211_free_sta_keys(local, sta);\n\n\t/* disable TIM bit - last chance to tell driver */\n\t__sta_info_recalc_tim(sta, true);\n\n\tsta->dead = true;\n\n\tlocal->num_sta--;\n\tlocal->sta_generation++;\n\n\twhile (sta->sta_state > IEEE80211_STA_NONE) {\n\t\tret = sta_info_move_state(sta, sta->sta_state - 1);\n\t\tif (ret) {\n\t\t\tWARN_ON_ONCE(1);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (sta->uploaded) {\n\t\tret = drv_sta_state(local, sdata, sta, IEEE80211_STA_NONE,\n\t\t\t\t    IEEE80211_STA_NOTEXIST);\n\t\tWARN_ON_ONCE(ret != 0);\n\t}\n\n\tsta_dbg(sdata, \"Removed STA %pM\\n\", sta->sta.addr);\n\n\tsinfo = kzalloc(sizeof(*sinfo), GFP_KERNEL);\n\tif (sinfo)\n\t\tsta_set_sinfo(sta, sinfo, true);\n\tcfg80211_del_sta_sinfo(sdata->dev, sta->sta.addr, sinfo, GFP_KERNEL);\n\tkfree(sinfo);\n\n\tieee80211_sta_debugfs_remove(sta);\n\n\tcleanup_single_sta(sta);\n}\n\nint __must_check __sta_info_destroy(struct sta_info *sta)\n{\n\tint err = __sta_info_destroy_part1(sta);\n\n\tif (err)\n\t\treturn err;\n\n\tsynchronize_net();\n\n\t__sta_info_destroy_part2(sta);\n\n\treturn 0;\n}\n\nint sta_info_destroy_addr(struct ieee80211_sub_if_data *sdata, const u8 *addr)\n{\n\tstruct sta_info *sta;\n\tint ret;\n\n\tmutex_lock(&sdata->local->sta_mtx);\n\tsta = sta_info_get(sdata, addr);\n\tret = __sta_info_destroy(sta);\n\tmutex_unlock(&sdata->local->sta_mtx);\n\n\treturn ret;\n}\n\nint sta_info_destroy_addr_bss(struct ieee80211_sub_if_data *sdata,\n\t\t\t      const u8 *addr)\n{\n\tstruct sta_info *sta;\n\tint ret;\n\n\tmutex_lock(&sdata->local->sta_mtx);\n\tsta = sta_info_get_bss(sdata, addr);\n\tret = __sta_info_destroy(sta);\n\tmutex_unlock(&sdata->local->sta_mtx);\n\n\treturn ret;\n}\n\nstatic void sta_info_cleanup(struct timer_list *t)\n{\n\tstruct ieee80211_local *local = from_timer(local, t, sta_cleanup);\n\tstruct sta_info *sta;\n\tbool timer_needed = false;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(sta, &local->sta_list, list)\n\t\tif (sta_info_cleanup_expire_buffered(local, sta))\n\t\t\ttimer_needed = true;\n\trcu_read_unlock();\n\n\tif (local->quiescing)\n\t\treturn;\n\n\tif (!timer_needed)\n\t\treturn;\n\n\tmod_timer(&local->sta_cleanup,\n\t\t  round_jiffies(jiffies + STA_INFO_CLEANUP_INTERVAL));\n}\n\nint sta_info_init(struct ieee80211_local *local)\n{\n\tint err;\n\n\terr = rhltable_init(&local->sta_hash, &sta_rht_params);\n\tif (err)\n\t\treturn err;\n\n\tspin_lock_init(&local->tim_lock);\n\tmutex_init(&local->sta_mtx);\n\tINIT_LIST_HEAD(&local->sta_list);\n\n\ttimer_setup(&local->sta_cleanup, sta_info_cleanup, 0);\n\treturn 0;\n}\n\nvoid sta_info_stop(struct ieee80211_local *local)\n{\n\tdel_timer_sync(&local->sta_cleanup);\n\trhltable_destroy(&local->sta_hash);\n}\n\n\nint __sta_info_flush(struct ieee80211_sub_if_data *sdata, bool vlans)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sta_info *sta, *tmp;\n\tLIST_HEAD(free_list);\n\tint ret = 0;\n\n\tmight_sleep();\n\n\tWARN_ON(vlans && sdata->vif.type != NL80211_IFTYPE_AP);\n\tWARN_ON(vlans && !sdata->bss);\n\n\tmutex_lock(&local->sta_mtx);\n\tlist_for_each_entry_safe(sta, tmp, &local->sta_list, list) {\n\t\tif (sdata == sta->sdata ||\n\t\t    (vlans && sdata->bss == sta->sdata->bss)) {\n\t\t\tif (!WARN_ON(__sta_info_destroy_part1(sta)))\n\t\t\t\tlist_add(&sta->free_list, &free_list);\n\t\t\tret++;\n\t\t}\n\t}\n\n\tif (!list_empty(&free_list)) {\n\t\tsynchronize_net();\n\t\tlist_for_each_entry_safe(sta, tmp, &free_list, free_list)\n\t\t\t__sta_info_destroy_part2(sta);\n\t}\n\tmutex_unlock(&local->sta_mtx);\n\n\treturn ret;\n}\n\nvoid ieee80211_sta_expire(struct ieee80211_sub_if_data *sdata,\n\t\t\t  unsigned long exp_time)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sta_info *sta, *tmp;\n\n\tmutex_lock(&local->sta_mtx);\n\n\tlist_for_each_entry_safe(sta, tmp, &local->sta_list, list) {\n\t\tunsigned long last_active = ieee80211_sta_last_active(sta);\n\n\t\tif (sdata != sta->sdata)\n\t\t\tcontinue;\n\n\t\tif (time_is_before_jiffies(last_active + exp_time)) {\n\t\t\tsta_dbg(sta->sdata, \"expiring inactive STA %pM\\n\",\n\t\t\t\tsta->sta.addr);\n\n\t\t\tif (ieee80211_vif_is_mesh(&sdata->vif) &&\n\t\t\t    test_sta_flag(sta, WLAN_STA_PS_STA))\n\t\t\t\tatomic_dec(&sdata->u.mesh.ps.num_sta_ps);\n\n\t\t\tWARN_ON(__sta_info_destroy(sta));\n\t\t}\n\t}\n\n\tmutex_unlock(&local->sta_mtx);\n}\n\nstruct ieee80211_sta *ieee80211_find_sta_by_ifaddr(struct ieee80211_hw *hw,\n\t\t\t\t\t\t   const u8 *addr,\n\t\t\t\t\t\t   const u8 *localaddr)\n{\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\tstruct rhlist_head *tmp;\n\tstruct sta_info *sta;\n\n\t/*\n\t * Just return a random station if localaddr is NULL\n\t * ... first in list.\n\t */\n\tfor_each_sta_info(local, addr, sta, tmp) {\n\t\tif (localaddr &&\n\t\t    !ether_addr_equal(sta->sdata->vif.addr, localaddr))\n\t\t\tcontinue;\n\t\tif (!sta->uploaded)\n\t\t\treturn NULL;\n\t\treturn &sta->sta;\n\t}\n\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(ieee80211_find_sta_by_ifaddr);\n\nstruct ieee80211_sta *ieee80211_find_sta(struct ieee80211_vif *vif,\n\t\t\t\t\t const u8 *addr)\n{\n\tstruct sta_info *sta;\n\n\tif (!vif)\n\t\treturn NULL;\n\n\tsta = sta_info_get_bss(vif_to_sdata(vif), addr);\n\tif (!sta)\n\t\treturn NULL;\n\n\tif (!sta->uploaded)\n\t\treturn NULL;\n\n\treturn &sta->sta;\n}\nEXPORT_SYMBOL(ieee80211_find_sta);\n\n/* powersave support code */\nvoid ieee80211_sta_ps_deliver_wakeup(struct sta_info *sta)\n{\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sk_buff_head pending;\n\tint filtered = 0, buffered = 0, ac, i;\n\tunsigned long flags;\n\tstruct ps_data *ps;\n\n\tif (sdata->vif.type == NL80211_IFTYPE_AP_VLAN)\n\t\tsdata = container_of(sdata->bss, struct ieee80211_sub_if_data,\n\t\t\t\t     u.ap);\n\n\tif (sdata->vif.type == NL80211_IFTYPE_AP)\n\t\tps = &sdata->bss->ps;\n\telse if (ieee80211_vif_is_mesh(&sdata->vif))\n\t\tps = &sdata->u.mesh.ps;\n\telse\n\t\treturn;\n\n\tclear_sta_flag(sta, WLAN_STA_SP);\n\n\tBUILD_BUG_ON(BITS_TO_LONGS(IEEE80211_NUM_TIDS) > 1);\n\tsta->driver_buffered_tids = 0;\n\tsta->txq_buffered_tids = 0;\n\n\tif (!ieee80211_hw_check(&local->hw, AP_LINK_PS))\n\t\tdrv_sta_notify(local, sdata, STA_NOTIFY_AWAKE, &sta->sta);\n\n\tfor (i = 0; i < ARRAY_SIZE(sta->sta.txq); i++) {\n\t\tif (!sta->sta.txq[i] || !txq_has_queue(sta->sta.txq[i]))\n\t\t\tcontinue;\n\n\t\tschedule_and_wake_txq(local, to_txq_info(sta->sta.txq[i]));\n\t}\n\n\tskb_queue_head_init(&pending);\n\n\t/* sync with ieee80211_tx_h_unicast_ps_buf */\n\tspin_lock(&sta->ps_lock);\n\t/* Send all buffered frames to the station */\n\tfor (ac = 0; ac < IEEE80211_NUM_ACS; ac++) {\n\t\tint count = skb_queue_len(&pending), tmp;\n\n\t\tspin_lock_irqsave(&sta->tx_filtered[ac].lock, flags);\n\t\tskb_queue_splice_tail_init(&sta->tx_filtered[ac], &pending);\n\t\tspin_unlock_irqrestore(&sta->tx_filtered[ac].lock, flags);\n\t\ttmp = skb_queue_len(&pending);\n\t\tfiltered += tmp - count;\n\t\tcount = tmp;\n\n\t\tspin_lock_irqsave(&sta->ps_tx_buf[ac].lock, flags);\n\t\tskb_queue_splice_tail_init(&sta->ps_tx_buf[ac], &pending);\n\t\tspin_unlock_irqrestore(&sta->ps_tx_buf[ac].lock, flags);\n\t\ttmp = skb_queue_len(&pending);\n\t\tbuffered += tmp - count;\n\t}\n\n\tieee80211_add_pending_skbs(local, &pending);\n\n\t/* now we're no longer in the deliver code */\n\tclear_sta_flag(sta, WLAN_STA_PS_DELIVER);\n\n\t/* The station might have polled and then woken up before we responded,\n\t * so clear these flags now to avoid them sticking around.\n\t */\n\tclear_sta_flag(sta, WLAN_STA_PSPOLL);\n\tclear_sta_flag(sta, WLAN_STA_UAPSD);\n\tspin_unlock(&sta->ps_lock);\n\n\tatomic_dec(&ps->num_sta_ps);\n\n\tlocal->total_ps_buffered -= buffered;\n\n\tsta_info_recalc_tim(sta);\n\n\tps_dbg(sdata,\n\t       \"STA %pM aid %d sending %d filtered/%d PS frames since STA woke up\\n\",\n\t       sta->sta.addr, sta->sta.aid, filtered, buffered);\n\n\tieee80211_check_fast_xmit(sta);\n}\n\nstatic void ieee80211_send_null_response(struct sta_info *sta, int tid,\n\t\t\t\t\t enum ieee80211_frame_release_type reason,\n\t\t\t\t\t bool call_driver, bool more_data)\n{\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct ieee80211_qos_hdr *nullfunc;\n\tstruct sk_buff *skb;\n\tint size = sizeof(*nullfunc);\n\t__le16 fc;\n\tbool qos = sta->sta.wme;\n\tstruct ieee80211_tx_info *info;\n\tstruct ieee80211_chanctx_conf *chanctx_conf;\n\n\t/* Don't send NDPs when STA is connected HE */\n\tif (sdata->vif.type == NL80211_IFTYPE_STATION &&\n\t    !(sdata->u.mgd.flags & IEEE80211_STA_DISABLE_HE))\n\t\treturn;\n\n\tif (qos) {\n\t\tfc = cpu_to_le16(IEEE80211_FTYPE_DATA |\n\t\t\t\t IEEE80211_STYPE_QOS_NULLFUNC |\n\t\t\t\t IEEE80211_FCTL_FROMDS);\n\t} else {\n\t\tsize -= 2;\n\t\tfc = cpu_to_le16(IEEE80211_FTYPE_DATA |\n\t\t\t\t IEEE80211_STYPE_NULLFUNC |\n\t\t\t\t IEEE80211_FCTL_FROMDS);\n\t}\n\n\tskb = dev_alloc_skb(local->hw.extra_tx_headroom + size);\n\tif (!skb)\n\t\treturn;\n\n\tskb_reserve(skb, local->hw.extra_tx_headroom);\n\n\tnullfunc = skb_put(skb, size);\n\tnullfunc->frame_control = fc;\n\tnullfunc->duration_id = 0;\n\tmemcpy(nullfunc->addr1, sta->sta.addr, ETH_ALEN);\n\tmemcpy(nullfunc->addr2, sdata->vif.addr, ETH_ALEN);\n\tmemcpy(nullfunc->addr3, sdata->vif.addr, ETH_ALEN);\n\tnullfunc->seq_ctrl = 0;\n\n\tskb->priority = tid;\n\tskb_set_queue_mapping(skb, ieee802_1d_to_ac[tid]);\n\tif (qos) {\n\t\tnullfunc->qos_ctrl = cpu_to_le16(tid);\n\n\t\tif (reason == IEEE80211_FRAME_RELEASE_UAPSD) {\n\t\t\tnullfunc->qos_ctrl |=\n\t\t\t\tcpu_to_le16(IEEE80211_QOS_CTL_EOSP);\n\t\t\tif (more_data)\n\t\t\t\tnullfunc->frame_control |=\n\t\t\t\t\tcpu_to_le16(IEEE80211_FCTL_MOREDATA);\n\t\t}\n\t}\n\n\tinfo = IEEE80211_SKB_CB(skb);\n\n\t/*\n\t * Tell TX path to send this frame even though the\n\t * STA may still remain is PS mode after this frame\n\t * exchange. Also set EOSP to indicate this packet\n\t * ends the poll/service period.\n\t */\n\tinfo->flags |= IEEE80211_TX_CTL_NO_PS_BUFFER |\n\t\t       IEEE80211_TX_STATUS_EOSP |\n\t\t       IEEE80211_TX_CTL_REQ_TX_STATUS;\n\n\tinfo->control.flags |= IEEE80211_TX_CTRL_PS_RESPONSE;\n\n\tif (call_driver)\n\t\tdrv_allow_buffered_frames(local, sta, BIT(tid), 1,\n\t\t\t\t\t  reason, false);\n\n\tskb->dev = sdata->dev;\n\n\trcu_read_lock();\n\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\tif (WARN_ON(!chanctx_conf)) {\n\t\trcu_read_unlock();\n\t\tkfree_skb(skb);\n\t\treturn;\n\t}\n\n\tinfo->band = chanctx_conf->def.chan->band;\n\tieee80211_xmit(sdata, sta, skb);\n\trcu_read_unlock();\n}\n\nstatic int find_highest_prio_tid(unsigned long tids)\n{\n\t/* lower 3 TIDs aren't ordered perfectly */\n\tif (tids & 0xF8)\n\t\treturn fls(tids) - 1;\n\t/* TID 0 is BE just like TID 3 */\n\tif (tids & BIT(0))\n\t\treturn 0;\n\treturn fls(tids) - 1;\n}\n\n/* Indicates if the MORE_DATA bit should be set in the last\n * frame obtained by ieee80211_sta_ps_get_frames.\n * Note that driver_release_tids is relevant only if\n * reason = IEEE80211_FRAME_RELEASE_PSPOLL\n */\nstatic bool\nieee80211_sta_ps_more_data(struct sta_info *sta, u8 ignored_acs,\n\t\t\t   enum ieee80211_frame_release_type reason,\n\t\t\t   unsigned long driver_release_tids)\n{\n\tint ac;\n\n\t/* If the driver has data on more than one TID then\n\t * certainly there's more data if we release just a\n\t * single frame now (from a single TID). This will\n\t * only happen for PS-Poll.\n\t */\n\tif (reason == IEEE80211_FRAME_RELEASE_PSPOLL &&\n\t    hweight16(driver_release_tids) > 1)\n\t\treturn true;\n\n\tfor (ac = 0; ac < IEEE80211_NUM_ACS; ac++) {\n\t\tif (ignored_acs & ieee80211_ac_to_qos_mask[ac])\n\t\t\tcontinue;\n\n\t\tif (!skb_queue_empty(&sta->tx_filtered[ac]) ||\n\t\t    !skb_queue_empty(&sta->ps_tx_buf[ac]))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void\nieee80211_sta_ps_get_frames(struct sta_info *sta, int n_frames, u8 ignored_acs,\n\t\t\t    enum ieee80211_frame_release_type reason,\n\t\t\t    struct sk_buff_head *frames,\n\t\t\t    unsigned long *driver_release_tids)\n{\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct ieee80211_local *local = sdata->local;\n\tint ac;\n\n\t/* Get response frame(s) and more data bit for the last one. */\n\tfor (ac = 0; ac < IEEE80211_NUM_ACS; ac++) {\n\t\tunsigned long tids;\n\n\t\tif (ignored_acs & ieee80211_ac_to_qos_mask[ac])\n\t\t\tcontinue;\n\n\t\ttids = ieee80211_tids_for_ac(ac);\n\n\t\t/* if we already have frames from software, then we can't also\n\t\t * release from hardware queues\n\t\t */\n\t\tif (skb_queue_empty(frames)) {\n\t\t\t*driver_release_tids |=\n\t\t\t\tsta->driver_buffered_tids & tids;\n\t\t\t*driver_release_tids |= sta->txq_buffered_tids & tids;\n\t\t}\n\n\t\tif (!*driver_release_tids) {\n\t\t\tstruct sk_buff *skb;\n\n\t\t\twhile (n_frames > 0) {\n\t\t\t\tskb = skb_dequeue(&sta->tx_filtered[ac]);\n\t\t\t\tif (!skb) {\n\t\t\t\t\tskb = skb_dequeue(\n\t\t\t\t\t\t&sta->ps_tx_buf[ac]);\n\t\t\t\t\tif (skb)\n\t\t\t\t\t\tlocal->total_ps_buffered--;\n\t\t\t\t}\n\t\t\t\tif (!skb)\n\t\t\t\t\tbreak;\n\t\t\t\tn_frames--;\n\t\t\t\t__skb_queue_tail(frames, skb);\n\t\t\t}\n\t\t}\n\n\t\t/* If we have more frames buffered on this AC, then abort the\n\t\t * loop since we can't send more data from other ACs before\n\t\t * the buffered frames from this.\n\t\t */\n\t\tif (!skb_queue_empty(&sta->tx_filtered[ac]) ||\n\t\t    !skb_queue_empty(&sta->ps_tx_buf[ac]))\n\t\t\tbreak;\n\t}\n}\n\nstatic void\nieee80211_sta_ps_deliver_response(struct sta_info *sta,\n\t\t\t\t  int n_frames, u8 ignored_acs,\n\t\t\t\t  enum ieee80211_frame_release_type reason)\n{\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct ieee80211_local *local = sdata->local;\n\tunsigned long driver_release_tids = 0;\n\tstruct sk_buff_head frames;\n\tbool more_data;\n\n\t/* Service or PS-Poll period starts */\n\tset_sta_flag(sta, WLAN_STA_SP);\n\n\t__skb_queue_head_init(&frames);\n\n\tieee80211_sta_ps_get_frames(sta, n_frames, ignored_acs, reason,\n\t\t\t\t    &frames, &driver_release_tids);\n\n\tmore_data = ieee80211_sta_ps_more_data(sta, ignored_acs, reason, driver_release_tids);\n\n\tif (driver_release_tids && reason == IEEE80211_FRAME_RELEASE_PSPOLL)\n\t\tdriver_release_tids =\n\t\t\tBIT(find_highest_prio_tid(driver_release_tids));\n\n\tif (skb_queue_empty(&frames) && !driver_release_tids) {\n\t\tint tid, ac;\n\n\t\t/*\n\t\t * For PS-Poll, this can only happen due to a race condition\n\t\t * when we set the TIM bit and the station notices it, but\n\t\t * before it can poll for the frame we expire it.\n\t\t *\n\t\t * For uAPSD, this is said in the standard (11.2.1.5 h):\n\t\t *\tAt each unscheduled SP for a non-AP STA, the AP shall\n\t\t *\tattempt to transmit at least one MSDU or MMPDU, but no\n\t\t *\tmore than the value specified in the Max SP Length field\n\t\t *\tin the QoS Capability element from delivery-enabled ACs,\n\t\t *\tthat are destined for the non-AP STA.\n\t\t *\n\t\t * Since we have no other MSDU/MMPDU, transmit a QoS null frame.\n\t\t */\n\n\t\t/* This will evaluate to 1, 3, 5 or 7. */\n\t\tfor (ac = IEEE80211_AC_VO; ac < IEEE80211_NUM_ACS; ac++)\n\t\t\tif (!(ignored_acs & ieee80211_ac_to_qos_mask[ac]))\n\t\t\t\tbreak;\n\t\ttid = 7 - 2 * ac;\n\n\t\tieee80211_send_null_response(sta, tid, reason, true, false);\n\t} else if (!driver_release_tids) {\n\t\tstruct sk_buff_head pending;\n\t\tstruct sk_buff *skb;\n\t\tint num = 0;\n\t\tu16 tids = 0;\n\t\tbool need_null = false;\n\n\t\tskb_queue_head_init(&pending);\n\n\t\twhile ((skb = __skb_dequeue(&frames))) {\n\t\t\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\t\t\tstruct ieee80211_hdr *hdr = (void *) skb->data;\n\t\t\tu8 *qoshdr = NULL;\n\n\t\t\tnum++;\n\n\t\t\t/*\n\t\t\t * Tell TX path to send this frame even though the\n\t\t\t * STA may still remain is PS mode after this frame\n\t\t\t * exchange.\n\t\t\t */\n\t\t\tinfo->flags |= IEEE80211_TX_CTL_NO_PS_BUFFER;\n\t\t\tinfo->control.flags |= IEEE80211_TX_CTRL_PS_RESPONSE;\n\n\t\t\t/*\n\t\t\t * Use MoreData flag to indicate whether there are\n\t\t\t * more buffered frames for this STA\n\t\t\t */\n\t\t\tif (more_data || !skb_queue_empty(&frames))\n\t\t\t\thdr->frame_control |=\n\t\t\t\t\tcpu_to_le16(IEEE80211_FCTL_MOREDATA);\n\t\t\telse\n\t\t\t\thdr->frame_control &=\n\t\t\t\t\tcpu_to_le16(~IEEE80211_FCTL_MOREDATA);\n\n\t\t\tif (ieee80211_is_data_qos(hdr->frame_control) ||\n\t\t\t    ieee80211_is_qos_nullfunc(hdr->frame_control))\n\t\t\t\tqoshdr = ieee80211_get_qos_ctl(hdr);\n\n\t\t\ttids |= BIT(skb->priority);\n\n\t\t\t__skb_queue_tail(&pending, skb);\n\n\t\t\t/* end service period after last frame or add one */\n\t\t\tif (!skb_queue_empty(&frames))\n\t\t\t\tcontinue;\n\n\t\t\tif (reason != IEEE80211_FRAME_RELEASE_UAPSD) {\n\t\t\t\t/* for PS-Poll, there's only one frame */\n\t\t\t\tinfo->flags |= IEEE80211_TX_STATUS_EOSP |\n\t\t\t\t\t       IEEE80211_TX_CTL_REQ_TX_STATUS;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* For uAPSD, things are a bit more complicated. If the\n\t\t\t * last frame has a QoS header (i.e. is a QoS-data or\n\t\t\t * QoS-nulldata frame) then just set the EOSP bit there\n\t\t\t * and be done.\n\t\t\t * If the frame doesn't have a QoS header (which means\n\t\t\t * it should be a bufferable MMPDU) then we can't set\n\t\t\t * the EOSP bit in the QoS header; add a QoS-nulldata\n\t\t\t * frame to the list to send it after the MMPDU.\n\t\t\t *\n\t\t\t * Note that this code is only in the mac80211-release\n\t\t\t * code path, we assume that the driver will not buffer\n\t\t\t * anything but QoS-data frames, or if it does, will\n\t\t\t * create the QoS-nulldata frame by itself if needed.\n\t\t\t *\n\t\t\t * Cf. 802.11-2012 10.2.1.10 (c).\n\t\t\t */\n\t\t\tif (qoshdr) {\n\t\t\t\t*qoshdr |= IEEE80211_QOS_CTL_EOSP;\n\n\t\t\t\tinfo->flags |= IEEE80211_TX_STATUS_EOSP |\n\t\t\t\t\t       IEEE80211_TX_CTL_REQ_TX_STATUS;\n\t\t\t} else {\n\t\t\t\t/* The standard isn't completely clear on this\n\t\t\t\t * as it says the more-data bit should be set\n\t\t\t\t * if there are more BUs. The QoS-Null frame\n\t\t\t\t * we're about to send isn't buffered yet, we\n\t\t\t\t * only create it below, but let's pretend it\n\t\t\t\t * was buffered just in case some clients only\n\t\t\t\t * expect more-data=0 when eosp=1.\n\t\t\t\t */\n\t\t\t\thdr->frame_control |=\n\t\t\t\t\tcpu_to_le16(IEEE80211_FCTL_MOREDATA);\n\t\t\t\tneed_null = true;\n\t\t\t\tnum++;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tdrv_allow_buffered_frames(local, sta, tids, num,\n\t\t\t\t\t  reason, more_data);\n\n\t\tieee80211_add_pending_skbs(local, &pending);\n\n\t\tif (need_null)\n\t\t\tieee80211_send_null_response(\n\t\t\t\tsta, find_highest_prio_tid(tids),\n\t\t\t\treason, false, false);\n\n\t\tsta_info_recalc_tim(sta);\n\t} else {\n\t\tint tid;\n\n\t\t/*\n\t\t * We need to release a frame that is buffered somewhere in the\n\t\t * driver ... it'll have to handle that.\n\t\t * Note that the driver also has to check the number of frames\n\t\t * on the TIDs we're releasing from - if there are more than\n\t\t * n_frames it has to set the more-data bit (if we didn't ask\n\t\t * it to set it anyway due to other buffered frames); if there\n\t\t * are fewer than n_frames it has to make sure to adjust that\n\t\t * to allow the service period to end properly.\n\t\t */\n\t\tdrv_release_buffered_frames(local, sta, driver_release_tids,\n\t\t\t\t\t    n_frames, reason, more_data);\n\n\t\t/*\n\t\t * Note that we don't recalculate the TIM bit here as it would\n\t\t * most likely have no effect at all unless the driver told us\n\t\t * that the TID(s) became empty before returning here from the\n\t\t * release function.\n\t\t * Either way, however, when the driver tells us that the TID(s)\n\t\t * became empty or we find that a txq became empty, we'll do the\n\t\t * TIM recalculation.\n\t\t */\n\n\t\tif (!sta->sta.txq[0])\n\t\t\treturn;\n\n\t\tfor (tid = 0; tid < ARRAY_SIZE(sta->sta.txq); tid++) {\n\t\t\tif (!sta->sta.txq[tid] ||\n\t\t\t    !(driver_release_tids & BIT(tid)) ||\n\t\t\t    txq_has_queue(sta->sta.txq[tid]))\n\t\t\t\tcontinue;\n\n\t\t\tsta_info_recalc_tim(sta);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nvoid ieee80211_sta_ps_deliver_poll_response(struct sta_info *sta)\n{\n\tu8 ignore_for_response = sta->sta.uapsd_queues;\n\n\t/*\n\t * If all ACs are delivery-enabled then we should reply\n\t * from any of them, if only some are enabled we reply\n\t * only from the non-enabled ones.\n\t */\n\tif (ignore_for_response == BIT(IEEE80211_NUM_ACS) - 1)\n\t\tignore_for_response = 0;\n\n\tieee80211_sta_ps_deliver_response(sta, 1, ignore_for_response,\n\t\t\t\t\t  IEEE80211_FRAME_RELEASE_PSPOLL);\n}\n\nvoid ieee80211_sta_ps_deliver_uapsd(struct sta_info *sta)\n{\n\tint n_frames = sta->sta.max_sp;\n\tu8 delivery_enabled = sta->sta.uapsd_queues;\n\n\t/*\n\t * If we ever grow support for TSPEC this might happen if\n\t * the TSPEC update from hostapd comes in between a trigger\n\t * frame setting WLAN_STA_UAPSD in the RX path and this\n\t * actually getting called.\n\t */\n\tif (!delivery_enabled)\n\t\treturn;\n\n\tswitch (sta->sta.max_sp) {\n\tcase 1:\n\t\tn_frames = 2;\n\t\tbreak;\n\tcase 2:\n\t\tn_frames = 4;\n\t\tbreak;\n\tcase 3:\n\t\tn_frames = 6;\n\t\tbreak;\n\tcase 0:\n\t\t/* XXX: what is a good value? */\n\t\tn_frames = 128;\n\t\tbreak;\n\t}\n\n\tieee80211_sta_ps_deliver_response(sta, n_frames, ~delivery_enabled,\n\t\t\t\t\t  IEEE80211_FRAME_RELEASE_UAPSD);\n}\n\nvoid ieee80211_sta_block_awake(struct ieee80211_hw *hw,\n\t\t\t       struct ieee80211_sta *pubsta, bool block)\n{\n\tstruct sta_info *sta = container_of(pubsta, struct sta_info, sta);\n\n\ttrace_api_sta_block_awake(sta->local, pubsta, block);\n\n\tif (block) {\n\t\tset_sta_flag(sta, WLAN_STA_PS_DRIVER);\n\t\tieee80211_clear_fast_xmit(sta);\n\t\treturn;\n\t}\n\n\tif (!test_sta_flag(sta, WLAN_STA_PS_DRIVER))\n\t\treturn;\n\n\tif (!test_sta_flag(sta, WLAN_STA_PS_STA)) {\n\t\tset_sta_flag(sta, WLAN_STA_PS_DELIVER);\n\t\tclear_sta_flag(sta, WLAN_STA_PS_DRIVER);\n\t\tieee80211_queue_work(hw, &sta->drv_deliver_wk);\n\t} else if (test_sta_flag(sta, WLAN_STA_PSPOLL) ||\n\t\t   test_sta_flag(sta, WLAN_STA_UAPSD)) {\n\t\t/* must be asleep in this case */\n\t\tclear_sta_flag(sta, WLAN_STA_PS_DRIVER);\n\t\tieee80211_queue_work(hw, &sta->drv_deliver_wk);\n\t} else {\n\t\tclear_sta_flag(sta, WLAN_STA_PS_DRIVER);\n\t\tieee80211_check_fast_xmit(sta);\n\t}\n}\nEXPORT_SYMBOL(ieee80211_sta_block_awake);\n\nvoid ieee80211_sta_eosp(struct ieee80211_sta *pubsta)\n{\n\tstruct sta_info *sta = container_of(pubsta, struct sta_info, sta);\n\tstruct ieee80211_local *local = sta->local;\n\n\ttrace_api_eosp(local, pubsta);\n\n\tclear_sta_flag(sta, WLAN_STA_SP);\n}\nEXPORT_SYMBOL(ieee80211_sta_eosp);\n\nvoid ieee80211_send_eosp_nullfunc(struct ieee80211_sta *pubsta, int tid)\n{\n\tstruct sta_info *sta = container_of(pubsta, struct sta_info, sta);\n\tenum ieee80211_frame_release_type reason;\n\tbool more_data;\n\n\ttrace_api_send_eosp_nullfunc(sta->local, pubsta, tid);\n\n\treason = IEEE80211_FRAME_RELEASE_UAPSD;\n\tmore_data = ieee80211_sta_ps_more_data(sta, ~sta->sta.uapsd_queues,\n\t\t\t\t\t       reason, 0);\n\n\tieee80211_send_null_response(sta, tid, reason, false, more_data);\n}\nEXPORT_SYMBOL(ieee80211_send_eosp_nullfunc);\n\nvoid ieee80211_sta_set_buffered(struct ieee80211_sta *pubsta,\n\t\t\t\tu8 tid, bool buffered)\n{\n\tstruct sta_info *sta = container_of(pubsta, struct sta_info, sta);\n\n\tif (WARN_ON(tid >= IEEE80211_NUM_TIDS))\n\t\treturn;\n\n\ttrace_api_sta_set_buffered(sta->local, pubsta, tid, buffered);\n\n\tif (buffered)\n\t\tset_bit(tid, &sta->driver_buffered_tids);\n\telse\n\t\tclear_bit(tid, &sta->driver_buffered_tids);\n\n\tsta_info_recalc_tim(sta);\n}\nEXPORT_SYMBOL(ieee80211_sta_set_buffered);\n\nvoid ieee80211_sta_register_airtime(struct ieee80211_sta *pubsta, u8 tid,\n\t\t\t\t    u32 tx_airtime, u32 rx_airtime)\n{\n\tstruct sta_info *sta = container_of(pubsta, struct sta_info, sta);\n\tstruct ieee80211_local *local = sta->sdata->local;\n\tu8 ac = ieee80211_ac_from_tid(tid);\n\tu32 airtime = 0;\n\n\tif (sta->local->airtime_flags & AIRTIME_USE_TX)\n\t\tairtime += tx_airtime;\n\tif (sta->local->airtime_flags & AIRTIME_USE_RX)\n\t\tairtime += rx_airtime;\n\n\tspin_lock_bh(&local->active_txq_lock[ac]);\n\tsta->airtime[ac].tx_airtime += tx_airtime;\n\tsta->airtime[ac].rx_airtime += rx_airtime;\n\tsta->airtime[ac].deficit -= airtime;\n\tspin_unlock_bh(&local->active_txq_lock[ac]);\n}\nEXPORT_SYMBOL(ieee80211_sta_register_airtime);\n\nvoid ieee80211_sta_update_pending_airtime(struct ieee80211_local *local,\n\t\t\t\t\t  struct sta_info *sta, u8 ac,\n\t\t\t\t\t  u16 tx_airtime, bool tx_completed)\n{\n\tint tx_pending;\n\n\tif (!wiphy_ext_feature_isset(local->hw.wiphy, NL80211_EXT_FEATURE_AQL))\n\t\treturn;\n\n\tif (!tx_completed) {\n\t\tif (sta)\n\t\t\tatomic_add(tx_airtime,\n\t\t\t\t   &sta->airtime[ac].aql_tx_pending);\n\n\t\tatomic_add(tx_airtime, &local->aql_total_pending_airtime);\n\t\treturn;\n\t}\n\n\tif (sta) {\n\t\ttx_pending = atomic_sub_return(tx_airtime,\n\t\t\t\t\t       &sta->airtime[ac].aql_tx_pending);\n\t\tif (tx_pending < 0)\n\t\t\tatomic_cmpxchg(&sta->airtime[ac].aql_tx_pending,\n\t\t\t\t       tx_pending, 0);\n\t}\n\n\ttx_pending = atomic_sub_return(tx_airtime,\n\t\t\t\t       &local->aql_total_pending_airtime);\n\tif (WARN_ONCE(tx_pending < 0,\n\t\t      \"Device %s AC %d pending airtime underflow: %u, %u\",\n\t\t      wiphy_name(local->hw.wiphy), ac, tx_pending,\n\t\t      tx_airtime))\n\t\tatomic_cmpxchg(&local->aql_total_pending_airtime,\n\t\t\t       tx_pending, 0);\n}\n\nint sta_info_move_state(struct sta_info *sta,\n\t\t\tenum ieee80211_sta_state new_state)\n{\n\tmight_sleep();\n\n\tif (sta->sta_state == new_state)\n\t\treturn 0;\n\n\t/* check allowed transitions first */\n\n\tswitch (new_state) {\n\tcase IEEE80211_STA_NONE:\n\t\tif (sta->sta_state != IEEE80211_STA_AUTH)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase IEEE80211_STA_AUTH:\n\t\tif (sta->sta_state != IEEE80211_STA_NONE &&\n\t\t    sta->sta_state != IEEE80211_STA_ASSOC)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase IEEE80211_STA_ASSOC:\n\t\tif (sta->sta_state != IEEE80211_STA_AUTH &&\n\t\t    sta->sta_state != IEEE80211_STA_AUTHORIZED)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase IEEE80211_STA_AUTHORIZED:\n\t\tif (sta->sta_state != IEEE80211_STA_ASSOC)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"invalid state %d\", new_state);\n\t\treturn -EINVAL;\n\t}\n\n\tsta_dbg(sta->sdata, \"moving STA %pM to state %d\\n\",\n\t\tsta->sta.addr, new_state);\n\n\t/*\n\t * notify the driver before the actual changes so it can\n\t * fail the transition\n\t */\n\tif (test_sta_flag(sta, WLAN_STA_INSERTED)) {\n\t\tint err = drv_sta_state(sta->local, sta->sdata, sta,\n\t\t\t\t\tsta->sta_state, new_state);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/* reflect the change in all state variables */\n\n\tswitch (new_state) {\n\tcase IEEE80211_STA_NONE:\n\t\tif (sta->sta_state == IEEE80211_STA_AUTH)\n\t\t\tclear_bit(WLAN_STA_AUTH, &sta->_flags);\n\t\tbreak;\n\tcase IEEE80211_STA_AUTH:\n\t\tif (sta->sta_state == IEEE80211_STA_NONE) {\n\t\t\tset_bit(WLAN_STA_AUTH, &sta->_flags);\n\t\t} else if (sta->sta_state == IEEE80211_STA_ASSOC) {\n\t\t\tclear_bit(WLAN_STA_ASSOC, &sta->_flags);\n\t\t\tieee80211_recalc_min_chandef(sta->sdata);\n\t\t\tif (!sta->sta.support_p2p_ps)\n\t\t\t\tieee80211_recalc_p2p_go_ps_allowed(sta->sdata);\n\t\t}\n\t\tbreak;\n\tcase IEEE80211_STA_ASSOC:\n\t\tif (sta->sta_state == IEEE80211_STA_AUTH) {\n\t\t\tset_bit(WLAN_STA_ASSOC, &sta->_flags);\n\t\t\tsta->assoc_at = ktime_get_boottime_ns();\n\t\t\tieee80211_recalc_min_chandef(sta->sdata);\n\t\t\tif (!sta->sta.support_p2p_ps)\n\t\t\t\tieee80211_recalc_p2p_go_ps_allowed(sta->sdata);\n\t\t} else if (sta->sta_state == IEEE80211_STA_AUTHORIZED) {\n\t\t\tieee80211_vif_dec_num_mcast(sta->sdata);\n\t\t\tclear_bit(WLAN_STA_AUTHORIZED, &sta->_flags);\n\t\t\tieee80211_clear_fast_xmit(sta);\n\t\t\tieee80211_clear_fast_rx(sta);\n\t\t}\n\t\tbreak;\n\tcase IEEE80211_STA_AUTHORIZED:\n\t\tif (sta->sta_state == IEEE80211_STA_ASSOC) {\n\t\t\tieee80211_vif_inc_num_mcast(sta->sdata);\n\t\t\tset_bit(WLAN_STA_AUTHORIZED, &sta->_flags);\n\t\t\tieee80211_check_fast_xmit(sta);\n\t\t\tieee80211_check_fast_rx(sta);\n\t\t}\n\t\tif (sta->sdata->vif.type == NL80211_IFTYPE_AP_VLAN ||\n\t\t    sta->sdata->vif.type == NL80211_IFTYPE_AP)\n\t\t\tcfg80211_send_layer2_update(sta->sdata->dev,\n\t\t\t\t\t\t    sta->sta.addr);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tsta->sta_state = new_state;\n\n\treturn 0;\n}\n\nu8 sta_info_tx_streams(struct sta_info *sta)\n{\n\tstruct ieee80211_sta_ht_cap *ht_cap = &sta->sta.ht_cap;\n\tu8 rx_streams;\n\n\tif (!sta->sta.ht_cap.ht_supported)\n\t\treturn 1;\n\n\tif (sta->sta.vht_cap.vht_supported) {\n\t\tint i;\n\t\tu16 tx_mcs_map =\n\t\t\tle16_to_cpu(sta->sta.vht_cap.vht_mcs.tx_mcs_map);\n\n\t\tfor (i = 7; i >= 0; i--)\n\t\t\tif ((tx_mcs_map & (0x3 << (i * 2))) !=\n\t\t\t    IEEE80211_VHT_MCS_NOT_SUPPORTED)\n\t\t\t\treturn i + 1;\n\t}\n\n\tif (ht_cap->mcs.rx_mask[3])\n\t\trx_streams = 4;\n\telse if (ht_cap->mcs.rx_mask[2])\n\t\trx_streams = 3;\n\telse if (ht_cap->mcs.rx_mask[1])\n\t\trx_streams = 2;\n\telse\n\t\trx_streams = 1;\n\n\tif (!(ht_cap->mcs.tx_params & IEEE80211_HT_MCS_TX_RX_DIFF))\n\t\treturn rx_streams;\n\n\treturn ((ht_cap->mcs.tx_params & IEEE80211_HT_MCS_TX_MAX_STREAMS_MASK)\n\t\t\t>> IEEE80211_HT_MCS_TX_MAX_STREAMS_SHIFT) + 1;\n}\n\nstatic struct ieee80211_sta_rx_stats *\nsta_get_last_rx_stats(struct sta_info *sta)\n{\n\tstruct ieee80211_sta_rx_stats *stats = &sta->rx_stats;\n\tstruct ieee80211_local *local = sta->local;\n\tint cpu;\n\n\tif (!ieee80211_hw_check(&local->hw, USES_RSS))\n\t\treturn stats;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct ieee80211_sta_rx_stats *cpustats;\n\n\t\tcpustats = per_cpu_ptr(sta->pcpu_rx_stats, cpu);\n\n\t\tif (time_after(cpustats->last_rx, stats->last_rx))\n\t\t\tstats = cpustats;\n\t}\n\n\treturn stats;\n}\n\nstatic void sta_stats_decode_rate(struct ieee80211_local *local, u32 rate,\n\t\t\t\t  struct rate_info *rinfo)\n{\n\trinfo->bw = STA_STATS_GET(BW, rate);\n\n\tswitch (STA_STATS_GET(TYPE, rate)) {\n\tcase STA_STATS_RATE_TYPE_VHT:\n\t\trinfo->flags = RATE_INFO_FLAGS_VHT_MCS;\n\t\trinfo->mcs = STA_STATS_GET(VHT_MCS, rate);\n\t\trinfo->nss = STA_STATS_GET(VHT_NSS, rate);\n\t\tif (STA_STATS_GET(SGI, rate))\n\t\t\trinfo->flags |= RATE_INFO_FLAGS_SHORT_GI;\n\t\tbreak;\n\tcase STA_STATS_RATE_TYPE_HT:\n\t\trinfo->flags = RATE_INFO_FLAGS_MCS;\n\t\trinfo->mcs = STA_STATS_GET(HT_MCS, rate);\n\t\tif (STA_STATS_GET(SGI, rate))\n\t\t\trinfo->flags |= RATE_INFO_FLAGS_SHORT_GI;\n\t\tbreak;\n\tcase STA_STATS_RATE_TYPE_LEGACY: {\n\t\tstruct ieee80211_supported_band *sband;\n\t\tu16 brate;\n\t\tunsigned int shift;\n\t\tint band = STA_STATS_GET(LEGACY_BAND, rate);\n\t\tint rate_idx = STA_STATS_GET(LEGACY_IDX, rate);\n\n\t\tsband = local->hw.wiphy->bands[band];\n\n\t\tif (WARN_ON_ONCE(!sband->bitrates))\n\t\t\tbreak;\n\n\t\tbrate = sband->bitrates[rate_idx].bitrate;\n\t\tif (rinfo->bw == RATE_INFO_BW_5)\n\t\t\tshift = 2;\n\t\telse if (rinfo->bw == RATE_INFO_BW_10)\n\t\t\tshift = 1;\n\t\telse\n\t\t\tshift = 0;\n\t\trinfo->legacy = DIV_ROUND_UP(brate, 1 << shift);\n\t\tbreak;\n\t\t}\n\tcase STA_STATS_RATE_TYPE_HE:\n\t\trinfo->flags = RATE_INFO_FLAGS_HE_MCS;\n\t\trinfo->mcs = STA_STATS_GET(HE_MCS, rate);\n\t\trinfo->nss = STA_STATS_GET(HE_NSS, rate);\n\t\trinfo->he_gi = STA_STATS_GET(HE_GI, rate);\n\t\trinfo->he_ru_alloc = STA_STATS_GET(HE_RU, rate);\n\t\trinfo->he_dcm = STA_STATS_GET(HE_DCM, rate);\n\t\tbreak;\n\t}\n}\n\nstatic int sta_set_rate_info_rx(struct sta_info *sta, struct rate_info *rinfo)\n{\n\tu16 rate = READ_ONCE(sta_get_last_rx_stats(sta)->last_rate);\n\n\tif (rate == STA_STATS_RATE_INVALID)\n\t\treturn -EINVAL;\n\n\tsta_stats_decode_rate(sta->local, rate, rinfo);\n\treturn 0;\n}\n\nstatic inline u64 sta_get_tidstats_msdu(struct ieee80211_sta_rx_stats *rxstats,\n\t\t\t\t\tint tid)\n{\n\tunsigned int start;\n\tu64 value;\n\n\tdo {\n\t\tstart = u64_stats_fetch_begin(&rxstats->syncp);\n\t\tvalue = rxstats->msdu[tid];\n\t} while (u64_stats_fetch_retry(&rxstats->syncp, start));\n\n\treturn value;\n}\n\nstatic void sta_set_tidstats(struct sta_info *sta,\n\t\t\t     struct cfg80211_tid_stats *tidstats,\n\t\t\t     int tid)\n{\n\tstruct ieee80211_local *local = sta->local;\n\tint cpu;\n\n\tif (!(tidstats->filled & BIT(NL80211_TID_STATS_RX_MSDU))) {\n\t\tif (!ieee80211_hw_check(&local->hw, USES_RSS))\n\t\t\ttidstats->rx_msdu +=\n\t\t\t\tsta_get_tidstats_msdu(&sta->rx_stats, tid);\n\n\t\tif (sta->pcpu_rx_stats) {\n\t\t\tfor_each_possible_cpu(cpu) {\n\t\t\t\tstruct ieee80211_sta_rx_stats *cpurxs;\n\n\t\t\t\tcpurxs = per_cpu_ptr(sta->pcpu_rx_stats, cpu);\n\t\t\t\ttidstats->rx_msdu +=\n\t\t\t\t\tsta_get_tidstats_msdu(cpurxs, tid);\n\t\t\t}\n\t\t}\n\n\t\ttidstats->filled |= BIT(NL80211_TID_STATS_RX_MSDU);\n\t}\n\n\tif (!(tidstats->filled & BIT(NL80211_TID_STATS_TX_MSDU))) {\n\t\ttidstats->filled |= BIT(NL80211_TID_STATS_TX_MSDU);\n\t\ttidstats->tx_msdu = sta->tx_stats.msdu[tid];\n\t}\n\n\tif (!(tidstats->filled & BIT(NL80211_TID_STATS_TX_MSDU_RETRIES)) &&\n\t    ieee80211_hw_check(&local->hw, REPORTS_TX_ACK_STATUS)) {\n\t\ttidstats->filled |= BIT(NL80211_TID_STATS_TX_MSDU_RETRIES);\n\t\ttidstats->tx_msdu_retries = sta->status_stats.msdu_retries[tid];\n\t}\n\n\tif (!(tidstats->filled & BIT(NL80211_TID_STATS_TX_MSDU_FAILED)) &&\n\t    ieee80211_hw_check(&local->hw, REPORTS_TX_ACK_STATUS)) {\n\t\ttidstats->filled |= BIT(NL80211_TID_STATS_TX_MSDU_FAILED);\n\t\ttidstats->tx_msdu_failed = sta->status_stats.msdu_failed[tid];\n\t}\n\n\tif (local->ops->wake_tx_queue && tid < IEEE80211_NUM_TIDS) {\n\t\tspin_lock_bh(&local->fq.lock);\n\t\trcu_read_lock();\n\n\t\ttidstats->filled |= BIT(NL80211_TID_STATS_TXQ_STATS);\n\t\tieee80211_fill_txq_stats(&tidstats->txq_stats,\n\t\t\t\t\t to_txq_info(sta->sta.txq[tid]));\n\n\t\trcu_read_unlock();\n\t\tspin_unlock_bh(&local->fq.lock);\n\t}\n}\n\nstatic inline u64 sta_get_stats_bytes(struct ieee80211_sta_rx_stats *rxstats)\n{\n\tunsigned int start;\n\tu64 value;\n\n\tdo {\n\t\tstart = u64_stats_fetch_begin(&rxstats->syncp);\n\t\tvalue = rxstats->bytes;\n\t} while (u64_stats_fetch_retry(&rxstats->syncp, start));\n\n\treturn value;\n}\n\nvoid sta_set_sinfo(struct sta_info *sta, struct station_info *sinfo,\n\t\t   bool tidstats)\n{\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct ieee80211_local *local = sdata->local;\n\tu32 thr = 0;\n\tint i, ac, cpu;\n\tstruct ieee80211_sta_rx_stats *last_rxstats;\n\n\tlast_rxstats = sta_get_last_rx_stats(sta);\n\n\tsinfo->generation = sdata->local->sta_generation;\n\n\t/* do before driver, so beacon filtering drivers have a\n\t * chance to e.g. just add the number of filtered beacons\n\t * (or just modify the value entirely, of course)\n\t */\n\tif (sdata->vif.type == NL80211_IFTYPE_STATION)\n\t\tsinfo->rx_beacon = sdata->u.mgd.count_beacon_signal;\n\n\tdrv_sta_statistics(local, sdata, &sta->sta, sinfo);\n\n\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_INACTIVE_TIME) |\n\t\t\t BIT_ULL(NL80211_STA_INFO_STA_FLAGS) |\n\t\t\t BIT_ULL(NL80211_STA_INFO_BSS_PARAM) |\n\t\t\t BIT_ULL(NL80211_STA_INFO_CONNECTED_TIME) |\n\t\t\t BIT_ULL(NL80211_STA_INFO_ASSOC_AT_BOOTTIME) |\n\t\t\t BIT_ULL(NL80211_STA_INFO_RX_DROP_MISC);\n\n\tif (sdata->vif.type == NL80211_IFTYPE_STATION) {\n\t\tsinfo->beacon_loss_count = sdata->u.mgd.beacon_loss_count;\n\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_BEACON_LOSS);\n\t}\n\n\tsinfo->connected_time = ktime_get_seconds() - sta->last_connected;\n\tsinfo->assoc_at = sta->assoc_at;\n\tsinfo->inactive_time =\n\t\tjiffies_to_msecs(jiffies - ieee80211_sta_last_active(sta));\n\n\tif (!(sinfo->filled & (BIT_ULL(NL80211_STA_INFO_TX_BYTES64) |\n\t\t\t       BIT_ULL(NL80211_STA_INFO_TX_BYTES)))) {\n\t\tsinfo->tx_bytes = 0;\n\t\tfor (ac = 0; ac < IEEE80211_NUM_ACS; ac++)\n\t\t\tsinfo->tx_bytes += sta->tx_stats.bytes[ac];\n\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_TX_BYTES64);\n\t}\n\n\tif (!(sinfo->filled & BIT_ULL(NL80211_STA_INFO_TX_PACKETS))) {\n\t\tsinfo->tx_packets = 0;\n\t\tfor (ac = 0; ac < IEEE80211_NUM_ACS; ac++)\n\t\t\tsinfo->tx_packets += sta->tx_stats.packets[ac];\n\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_TX_PACKETS);\n\t}\n\n\tif (!(sinfo->filled & (BIT_ULL(NL80211_STA_INFO_RX_BYTES64) |\n\t\t\t       BIT_ULL(NL80211_STA_INFO_RX_BYTES)))) {\n\t\tif (!ieee80211_hw_check(&local->hw, USES_RSS))\n\t\t\tsinfo->rx_bytes += sta_get_stats_bytes(&sta->rx_stats);\n\n\t\tif (sta->pcpu_rx_stats) {\n\t\t\tfor_each_possible_cpu(cpu) {\n\t\t\t\tstruct ieee80211_sta_rx_stats *cpurxs;\n\n\t\t\t\tcpurxs = per_cpu_ptr(sta->pcpu_rx_stats, cpu);\n\t\t\t\tsinfo->rx_bytes += sta_get_stats_bytes(cpurxs);\n\t\t\t}\n\t\t}\n\n\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_RX_BYTES64);\n\t}\n\n\tif (!(sinfo->filled & BIT_ULL(NL80211_STA_INFO_RX_PACKETS))) {\n\t\tsinfo->rx_packets = sta->rx_stats.packets;\n\t\tif (sta->pcpu_rx_stats) {\n\t\t\tfor_each_possible_cpu(cpu) {\n\t\t\t\tstruct ieee80211_sta_rx_stats *cpurxs;\n\n\t\t\t\tcpurxs = per_cpu_ptr(sta->pcpu_rx_stats, cpu);\n\t\t\t\tsinfo->rx_packets += cpurxs->packets;\n\t\t\t}\n\t\t}\n\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_RX_PACKETS);\n\t}\n\n\tif (!(sinfo->filled & BIT_ULL(NL80211_STA_INFO_TX_RETRIES))) {\n\t\tsinfo->tx_retries = sta->status_stats.retry_count;\n\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_TX_RETRIES);\n\t}\n\n\tif (!(sinfo->filled & BIT_ULL(NL80211_STA_INFO_TX_FAILED))) {\n\t\tsinfo->tx_failed = sta->status_stats.retry_failed;\n\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_TX_FAILED);\n\t}\n\n\tif (!(sinfo->filled & BIT_ULL(NL80211_STA_INFO_RX_DURATION))) {\n\t\tfor (ac = 0; ac < IEEE80211_NUM_ACS; ac++)\n\t\t\tsinfo->rx_duration += sta->airtime[ac].rx_airtime;\n\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_RX_DURATION);\n\t}\n\n\tif (!(sinfo->filled & BIT_ULL(NL80211_STA_INFO_TX_DURATION))) {\n\t\tfor (ac = 0; ac < IEEE80211_NUM_ACS; ac++)\n\t\t\tsinfo->tx_duration += sta->airtime[ac].tx_airtime;\n\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_TX_DURATION);\n\t}\n\n\tif (!(sinfo->filled & BIT_ULL(NL80211_STA_INFO_AIRTIME_WEIGHT))) {\n\t\tsinfo->airtime_weight = sta->airtime_weight;\n\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_AIRTIME_WEIGHT);\n\t}\n\n\tsinfo->rx_dropped_misc = sta->rx_stats.dropped;\n\tif (sta->pcpu_rx_stats) {\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tstruct ieee80211_sta_rx_stats *cpurxs;\n\n\t\t\tcpurxs = per_cpu_ptr(sta->pcpu_rx_stats, cpu);\n\t\t\tsinfo->rx_dropped_misc += cpurxs->dropped;\n\t\t}\n\t}\n\n\tif (sdata->vif.type == NL80211_IFTYPE_STATION &&\n\t    !(sdata->vif.driver_flags & IEEE80211_VIF_BEACON_FILTER)) {\n\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_BEACON_RX) |\n\t\t\t\t BIT_ULL(NL80211_STA_INFO_BEACON_SIGNAL_AVG);\n\t\tsinfo->rx_beacon_signal_avg = ieee80211_ave_rssi(&sdata->vif);\n\t}\n\n\tif (ieee80211_hw_check(&sta->local->hw, SIGNAL_DBM) ||\n\t    ieee80211_hw_check(&sta->local->hw, SIGNAL_UNSPEC)) {\n\t\tif (!(sinfo->filled & BIT_ULL(NL80211_STA_INFO_SIGNAL))) {\n\t\t\tsinfo->signal = (s8)last_rxstats->last_signal;\n\t\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_SIGNAL);\n\t\t}\n\n\t\tif (!sta->pcpu_rx_stats &&\n\t\t    !(sinfo->filled & BIT_ULL(NL80211_STA_INFO_SIGNAL_AVG))) {\n\t\t\tsinfo->signal_avg =\n\t\t\t\t-ewma_signal_read(&sta->rx_stats_avg.signal);\n\t\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_SIGNAL_AVG);\n\t\t}\n\t}\n\n\t/* for the average - if pcpu_rx_stats isn't set - rxstats must point to\n\t * the sta->rx_stats struct, so the check here is fine with and without\n\t * pcpu statistics\n\t */\n\tif (last_rxstats->chains &&\n\t    !(sinfo->filled & (BIT_ULL(NL80211_STA_INFO_CHAIN_SIGNAL) |\n\t\t\t       BIT_ULL(NL80211_STA_INFO_CHAIN_SIGNAL_AVG)))) {\n\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_CHAIN_SIGNAL);\n\t\tif (!sta->pcpu_rx_stats)\n\t\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_CHAIN_SIGNAL_AVG);\n\n\t\tsinfo->chains = last_rxstats->chains;\n\n\t\tfor (i = 0; i < ARRAY_SIZE(sinfo->chain_signal); i++) {\n\t\t\tsinfo->chain_signal[i] =\n\t\t\t\tlast_rxstats->chain_signal_last[i];\n\t\t\tsinfo->chain_signal_avg[i] =\n\t\t\t\t-ewma_signal_read(&sta->rx_stats_avg.chain_signal[i]);\n\t\t}\n\t}\n\n\tif (!(sinfo->filled & BIT_ULL(NL80211_STA_INFO_TX_BITRATE))) {\n\t\tsta_set_rate_info_tx(sta, &sta->tx_stats.last_rate,\n\t\t\t\t     &sinfo->txrate);\n\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_TX_BITRATE);\n\t}\n\n\tif (!(sinfo->filled & BIT_ULL(NL80211_STA_INFO_RX_BITRATE))) {\n\t\tif (sta_set_rate_info_rx(sta, &sinfo->rxrate) == 0)\n\t\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_RX_BITRATE);\n\t}\n\n\tif (tidstats && !cfg80211_sinfo_alloc_tid_stats(sinfo, GFP_KERNEL)) {\n\t\tfor (i = 0; i < IEEE80211_NUM_TIDS + 1; i++)\n\t\t\tsta_set_tidstats(sta, &sinfo->pertid[i], i);\n\t}\n\n\tif (ieee80211_vif_is_mesh(&sdata->vif)) {\n#ifdef CONFIG_MAC80211_MESH\n\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_LLID) |\n\t\t\t\t BIT_ULL(NL80211_STA_INFO_PLID) |\n\t\t\t\t BIT_ULL(NL80211_STA_INFO_PLINK_STATE) |\n\t\t\t\t BIT_ULL(NL80211_STA_INFO_LOCAL_PM) |\n\t\t\t\t BIT_ULL(NL80211_STA_INFO_PEER_PM) |\n\t\t\t\t BIT_ULL(NL80211_STA_INFO_NONPEER_PM) |\n\t\t\t\t BIT_ULL(NL80211_STA_INFO_CONNECTED_TO_GATE) |\n\t\t\t\t BIT_ULL(NL80211_STA_INFO_CONNECTED_TO_AS);\n\n\t\tsinfo->llid = sta->mesh->llid;\n\t\tsinfo->plid = sta->mesh->plid;\n\t\tsinfo->plink_state = sta->mesh->plink_state;\n\t\tif (test_sta_flag(sta, WLAN_STA_TOFFSET_KNOWN)) {\n\t\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_T_OFFSET);\n\t\t\tsinfo->t_offset = sta->mesh->t_offset;\n\t\t}\n\t\tsinfo->local_pm = sta->mesh->local_pm;\n\t\tsinfo->peer_pm = sta->mesh->peer_pm;\n\t\tsinfo->nonpeer_pm = sta->mesh->nonpeer_pm;\n\t\tsinfo->connected_to_gate = sta->mesh->connected_to_gate;\n\t\tsinfo->connected_to_as = sta->mesh->connected_to_as;\n#endif\n\t}\n\n\tsinfo->bss_param.flags = 0;\n\tif (sdata->vif.bss_conf.use_cts_prot)\n\t\tsinfo->bss_param.flags |= BSS_PARAM_FLAGS_CTS_PROT;\n\tif (sdata->vif.bss_conf.use_short_preamble)\n\t\tsinfo->bss_param.flags |= BSS_PARAM_FLAGS_SHORT_PREAMBLE;\n\tif (sdata->vif.bss_conf.use_short_slot)\n\t\tsinfo->bss_param.flags |= BSS_PARAM_FLAGS_SHORT_SLOT_TIME;\n\tsinfo->bss_param.dtim_period = sdata->vif.bss_conf.dtim_period;\n\tsinfo->bss_param.beacon_interval = sdata->vif.bss_conf.beacon_int;\n\n\tsinfo->sta_flags.set = 0;\n\tsinfo->sta_flags.mask = BIT(NL80211_STA_FLAG_AUTHORIZED) |\n\t\t\t\tBIT(NL80211_STA_FLAG_SHORT_PREAMBLE) |\n\t\t\t\tBIT(NL80211_STA_FLAG_WME) |\n\t\t\t\tBIT(NL80211_STA_FLAG_MFP) |\n\t\t\t\tBIT(NL80211_STA_FLAG_AUTHENTICATED) |\n\t\t\t\tBIT(NL80211_STA_FLAG_ASSOCIATED) |\n\t\t\t\tBIT(NL80211_STA_FLAG_TDLS_PEER);\n\tif (test_sta_flag(sta, WLAN_STA_AUTHORIZED))\n\t\tsinfo->sta_flags.set |= BIT(NL80211_STA_FLAG_AUTHORIZED);\n\tif (test_sta_flag(sta, WLAN_STA_SHORT_PREAMBLE))\n\t\tsinfo->sta_flags.set |= BIT(NL80211_STA_FLAG_SHORT_PREAMBLE);\n\tif (sta->sta.wme)\n\t\tsinfo->sta_flags.set |= BIT(NL80211_STA_FLAG_WME);\n\tif (test_sta_flag(sta, WLAN_STA_MFP))\n\t\tsinfo->sta_flags.set |= BIT(NL80211_STA_FLAG_MFP);\n\tif (test_sta_flag(sta, WLAN_STA_AUTH))\n\t\tsinfo->sta_flags.set |= BIT(NL80211_STA_FLAG_AUTHENTICATED);\n\tif (test_sta_flag(sta, WLAN_STA_ASSOC))\n\t\tsinfo->sta_flags.set |= BIT(NL80211_STA_FLAG_ASSOCIATED);\n\tif (test_sta_flag(sta, WLAN_STA_TDLS_PEER))\n\t\tsinfo->sta_flags.set |= BIT(NL80211_STA_FLAG_TDLS_PEER);\n\n\tthr = sta_get_expected_throughput(sta);\n\n\tif (thr != 0) {\n\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_EXPECTED_THROUGHPUT);\n\t\tsinfo->expected_throughput = thr;\n\t}\n\n\tif (!(sinfo->filled & BIT_ULL(NL80211_STA_INFO_ACK_SIGNAL)) &&\n\t    sta->status_stats.ack_signal_filled) {\n\t\tsinfo->ack_signal = sta->status_stats.last_ack_signal;\n\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_ACK_SIGNAL);\n\t}\n\n\tif (!(sinfo->filled & BIT_ULL(NL80211_STA_INFO_ACK_SIGNAL_AVG)) &&\n\t    sta->status_stats.ack_signal_filled) {\n\t\tsinfo->avg_ack_signal =\n\t\t\t-(s8)ewma_avg_signal_read(\n\t\t\t\t&sta->status_stats.avg_ack_signal);\n\t\tsinfo->filled |=\n\t\t\tBIT_ULL(NL80211_STA_INFO_ACK_SIGNAL_AVG);\n\t}\n\n\tif (ieee80211_vif_is_mesh(&sdata->vif)) {\n\t\tsinfo->filled |= BIT_ULL(NL80211_STA_INFO_AIRTIME_LINK_METRIC);\n\t\tsinfo->airtime_link_metric =\n\t\t\tairtime_link_metric_get(local, sta);\n\t}\n}\n\nu32 sta_get_expected_throughput(struct sta_info *sta)\n{\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct rate_control_ref *ref = NULL;\n\tu32 thr = 0;\n\n\tif (test_sta_flag(sta, WLAN_STA_RATE_CONTROL))\n\t\tref = local->rate_ctrl;\n\n\t/* check if the driver has a SW RC implementation */\n\tif (ref && ref->ops->get_expected_throughput)\n\t\tthr = ref->ops->get_expected_throughput(sta->rate_ctrl_priv);\n\telse\n\t\tthr = drv_get_expected_throughput(local, sta);\n\n\treturn thr;\n}\n\nunsigned long ieee80211_sta_last_active(struct sta_info *sta)\n{\n\tstruct ieee80211_sta_rx_stats *stats = sta_get_last_rx_stats(sta);\n\n\tif (!sta->status_stats.last_ack ||\n\t    time_after(stats->last_rx, sta->status_stats.last_ack))\n\t\treturn stats->last_rx;\n\treturn sta->status_stats.last_ack;\n}\n\nstatic void sta_update_codel_params(struct sta_info *sta, u32 thr)\n{\n\tif (!sta->sdata->local->ops->wake_tx_queue)\n\t\treturn;\n\n\tif (thr && thr < STA_SLOW_THRESHOLD * sta->local->num_sta) {\n\t\tsta->cparams.target = MS2TIME(50);\n\t\tsta->cparams.interval = MS2TIME(300);\n\t\tsta->cparams.ecn = false;\n\t} else {\n\t\tsta->cparams.target = MS2TIME(20);\n\t\tsta->cparams.interval = MS2TIME(100);\n\t\tsta->cparams.ecn = true;\n\t}\n}\n\nvoid ieee80211_sta_set_expected_throughput(struct ieee80211_sta *pubsta,\n\t\t\t\t\t   u32 thr)\n{\n\tstruct sta_info *sta = container_of(pubsta, struct sta_info, sta);\n\n\tsta_update_codel_params(sta, thr);\n}\n"}, "25": {"id": 25, "path": "/src/net/mac80211/rx.c", "content": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n * Copyright 2002-2005, Instant802 Networks, Inc.\n * Copyright 2005-2006, Devicescape Software, Inc.\n * Copyright 2006-2007\tJiri Benc <jbenc@suse.cz>\n * Copyright 2007-2010\tJohannes Berg <johannes@sipsolutions.net>\n * Copyright 2013-2014  Intel Mobile Communications GmbH\n * Copyright(c) 2015 - 2017 Intel Deutschland GmbH\n * Copyright (C) 2018-2020 Intel Corporation\n */\n\n#include <linux/jiffies.h>\n#include <linux/slab.h>\n#include <linux/kernel.h>\n#include <linux/skbuff.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/rcupdate.h>\n#include <linux/export.h>\n#include <linux/bitops.h>\n#include <net/mac80211.h>\n#include <net/ieee80211_radiotap.h>\n#include <asm/unaligned.h>\n\n#include \"ieee80211_i.h\"\n#include \"driver-ops.h\"\n#include \"led.h\"\n#include \"mesh.h\"\n#include \"wep.h\"\n#include \"wpa.h\"\n#include \"tkip.h\"\n#include \"wme.h\"\n#include \"rate.h\"\n\n/*\n * monitor mode reception\n *\n * This function cleans up the SKB, i.e. it removes all the stuff\n * only useful for monitoring.\n */\nstatic struct sk_buff *ieee80211_clean_skb(struct sk_buff *skb,\n\t\t\t\t\t   unsigned int present_fcs_len,\n\t\t\t\t\t   unsigned int rtap_space)\n{\n\tstruct ieee80211_hdr *hdr;\n\tunsigned int hdrlen;\n\t__le16 fc;\n\n\tif (present_fcs_len)\n\t\t__pskb_trim(skb, skb->len - present_fcs_len);\n\t__pskb_pull(skb, rtap_space);\n\n\thdr = (void *)skb->data;\n\tfc = hdr->frame_control;\n\n\t/*\n\t * Remove the HT-Control field (if present) on management\n\t * frames after we've sent the frame to monitoring. We\n\t * (currently) don't need it, and don't properly parse\n\t * frames with it present, due to the assumption of a\n\t * fixed management header length.\n\t */\n\tif (likely(!ieee80211_is_mgmt(fc) || !ieee80211_has_order(fc)))\n\t\treturn skb;\n\n\thdrlen = ieee80211_hdrlen(fc);\n\thdr->frame_control &= ~cpu_to_le16(IEEE80211_FCTL_ORDER);\n\n\tif (!pskb_may_pull(skb, hdrlen)) {\n\t\tdev_kfree_skb(skb);\n\t\treturn NULL;\n\t}\n\n\tmemmove(skb->data + IEEE80211_HT_CTL_LEN, skb->data,\n\t\thdrlen - IEEE80211_HT_CTL_LEN);\n\t__pskb_pull(skb, IEEE80211_HT_CTL_LEN);\n\n\treturn skb;\n}\n\nstatic inline bool should_drop_frame(struct sk_buff *skb, int present_fcs_len,\n\t\t\t\t     unsigned int rtap_space)\n{\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(skb);\n\tstruct ieee80211_hdr *hdr;\n\n\thdr = (void *)(skb->data + rtap_space);\n\n\tif (status->flag & (RX_FLAG_FAILED_FCS_CRC |\n\t\t\t    RX_FLAG_FAILED_PLCP_CRC |\n\t\t\t    RX_FLAG_ONLY_MONITOR |\n\t\t\t    RX_FLAG_NO_PSDU))\n\t\treturn true;\n\n\tif (unlikely(skb->len < 16 + present_fcs_len + rtap_space))\n\t\treturn true;\n\n\tif (ieee80211_is_ctl(hdr->frame_control) &&\n\t    !ieee80211_is_pspoll(hdr->frame_control) &&\n\t    !ieee80211_is_back_req(hdr->frame_control))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic int\nieee80211_rx_radiotap_hdrlen(struct ieee80211_local *local,\n\t\t\t     struct ieee80211_rx_status *status,\n\t\t\t     struct sk_buff *skb)\n{\n\tint len;\n\n\t/* always present fields */\n\tlen = sizeof(struct ieee80211_radiotap_header) + 8;\n\n\t/* allocate extra bitmaps */\n\tif (status->chains)\n\t\tlen += 4 * hweight8(status->chains);\n\t/* vendor presence bitmap */\n\tif (status->flag & RX_FLAG_RADIOTAP_VENDOR_DATA)\n\t\tlen += 4;\n\n\tif (ieee80211_have_rx_timestamp(status)) {\n\t\tlen = ALIGN(len, 8);\n\t\tlen += 8;\n\t}\n\tif (ieee80211_hw_check(&local->hw, SIGNAL_DBM))\n\t\tlen += 1;\n\n\t/* antenna field, if we don't have per-chain info */\n\tif (!status->chains)\n\t\tlen += 1;\n\n\t/* padding for RX_FLAGS if necessary */\n\tlen = ALIGN(len, 2);\n\n\tif (status->encoding == RX_ENC_HT) /* HT info */\n\t\tlen += 3;\n\n\tif (status->flag & RX_FLAG_AMPDU_DETAILS) {\n\t\tlen = ALIGN(len, 4);\n\t\tlen += 8;\n\t}\n\n\tif (status->encoding == RX_ENC_VHT) {\n\t\tlen = ALIGN(len, 2);\n\t\tlen += 12;\n\t}\n\n\tif (local->hw.radiotap_timestamp.units_pos >= 0) {\n\t\tlen = ALIGN(len, 8);\n\t\tlen += 12;\n\t}\n\n\tif (status->encoding == RX_ENC_HE &&\n\t    status->flag & RX_FLAG_RADIOTAP_HE) {\n\t\tlen = ALIGN(len, 2);\n\t\tlen += 12;\n\t\tBUILD_BUG_ON(sizeof(struct ieee80211_radiotap_he) != 12);\n\t}\n\n\tif (status->encoding == RX_ENC_HE &&\n\t    status->flag & RX_FLAG_RADIOTAP_HE_MU) {\n\t\tlen = ALIGN(len, 2);\n\t\tlen += 12;\n\t\tBUILD_BUG_ON(sizeof(struct ieee80211_radiotap_he_mu) != 12);\n\t}\n\n\tif (status->flag & RX_FLAG_NO_PSDU)\n\t\tlen += 1;\n\n\tif (status->flag & RX_FLAG_RADIOTAP_LSIG) {\n\t\tlen = ALIGN(len, 2);\n\t\tlen += 4;\n\t\tBUILD_BUG_ON(sizeof(struct ieee80211_radiotap_lsig) != 4);\n\t}\n\n\tif (status->chains) {\n\t\t/* antenna and antenna signal fields */\n\t\tlen += 2 * hweight8(status->chains);\n\t}\n\n\tif (status->flag & RX_FLAG_RADIOTAP_VENDOR_DATA) {\n\t\tstruct ieee80211_vendor_radiotap *rtap;\n\t\tint vendor_data_offset = 0;\n\n\t\t/*\n\t\t * The position to look at depends on the existence (or non-\n\t\t * existence) of other elements, so take that into account...\n\t\t */\n\t\tif (status->flag & RX_FLAG_RADIOTAP_HE)\n\t\t\tvendor_data_offset +=\n\t\t\t\tsizeof(struct ieee80211_radiotap_he);\n\t\tif (status->flag & RX_FLAG_RADIOTAP_HE_MU)\n\t\t\tvendor_data_offset +=\n\t\t\t\tsizeof(struct ieee80211_radiotap_he_mu);\n\t\tif (status->flag & RX_FLAG_RADIOTAP_LSIG)\n\t\t\tvendor_data_offset +=\n\t\t\t\tsizeof(struct ieee80211_radiotap_lsig);\n\n\t\trtap = (void *)&skb->data[vendor_data_offset];\n\n\t\t/* alignment for fixed 6-byte vendor data header */\n\t\tlen = ALIGN(len, 2);\n\t\t/* vendor data header */\n\t\tlen += 6;\n\t\tif (WARN_ON(rtap->align == 0))\n\t\t\trtap->align = 1;\n\t\tlen = ALIGN(len, rtap->align);\n\t\tlen += rtap->len + rtap->pad;\n\t}\n\n\treturn len;\n}\n\nstatic void ieee80211_handle_mu_mimo_mon(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t int rtap_space)\n{\n\tstruct {\n\t\tstruct ieee80211_hdr_3addr hdr;\n\t\tu8 category;\n\t\tu8 action_code;\n\t} __packed __aligned(2) action;\n\n\tif (!sdata)\n\t\treturn;\n\n\tBUILD_BUG_ON(sizeof(action) != IEEE80211_MIN_ACTION_SIZE + 1);\n\n\tif (skb->len < rtap_space + sizeof(action) +\n\t\t       VHT_MUMIMO_GROUPS_DATA_LEN)\n\t\treturn;\n\n\tif (!is_valid_ether_addr(sdata->u.mntr.mu_follow_addr))\n\t\treturn;\n\n\tskb_copy_bits(skb, rtap_space, &action, sizeof(action));\n\n\tif (!ieee80211_is_action(action.hdr.frame_control))\n\t\treturn;\n\n\tif (action.category != WLAN_CATEGORY_VHT)\n\t\treturn;\n\n\tif (action.action_code != WLAN_VHT_ACTION_GROUPID_MGMT)\n\t\treturn;\n\n\tif (!ether_addr_equal(action.hdr.addr1, sdata->u.mntr.mu_follow_addr))\n\t\treturn;\n\n\tskb = skb_copy(skb, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn;\n\n\tskb_queue_tail(&sdata->skb_queue, skb);\n\tieee80211_queue_work(&sdata->local->hw, &sdata->work);\n}\n\n/*\n * ieee80211_add_rx_radiotap_header - add radiotap header\n *\n * add a radiotap header containing all the fields which the hardware provided.\n */\nstatic void\nieee80211_add_rx_radiotap_header(struct ieee80211_local *local,\n\t\t\t\t struct sk_buff *skb,\n\t\t\t\t struct ieee80211_rate *rate,\n\t\t\t\t int rtap_len, bool has_fcs)\n{\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(skb);\n\tstruct ieee80211_radiotap_header *rthdr;\n\tunsigned char *pos;\n\t__le32 *it_present;\n\tu32 it_present_val;\n\tu16 rx_flags = 0;\n\tu16 channel_flags = 0;\n\tint mpdulen, chain;\n\tunsigned long chains = status->chains;\n\tstruct ieee80211_vendor_radiotap rtap = {};\n\tstruct ieee80211_radiotap_he he = {};\n\tstruct ieee80211_radiotap_he_mu he_mu = {};\n\tstruct ieee80211_radiotap_lsig lsig = {};\n\n\tif (status->flag & RX_FLAG_RADIOTAP_HE) {\n\t\the = *(struct ieee80211_radiotap_he *)skb->data;\n\t\tskb_pull(skb, sizeof(he));\n\t\tWARN_ON_ONCE(status->encoding != RX_ENC_HE);\n\t}\n\n\tif (status->flag & RX_FLAG_RADIOTAP_HE_MU) {\n\t\the_mu = *(struct ieee80211_radiotap_he_mu *)skb->data;\n\t\tskb_pull(skb, sizeof(he_mu));\n\t}\n\n\tif (status->flag & RX_FLAG_RADIOTAP_LSIG) {\n\t\tlsig = *(struct ieee80211_radiotap_lsig *)skb->data;\n\t\tskb_pull(skb, sizeof(lsig));\n\t}\n\n\tif (status->flag & RX_FLAG_RADIOTAP_VENDOR_DATA) {\n\t\trtap = *(struct ieee80211_vendor_radiotap *)skb->data;\n\t\t/* rtap.len and rtap.pad are undone immediately */\n\t\tskb_pull(skb, sizeof(rtap) + rtap.len + rtap.pad);\n\t}\n\n\tmpdulen = skb->len;\n\tif (!(has_fcs && ieee80211_hw_check(&local->hw, RX_INCLUDES_FCS)))\n\t\tmpdulen += FCS_LEN;\n\n\trthdr = skb_push(skb, rtap_len);\n\tmemset(rthdr, 0, rtap_len - rtap.len - rtap.pad);\n\tit_present = &rthdr->it_present;\n\n\t/* radiotap header, set always present flags */\n\trthdr->it_len = cpu_to_le16(rtap_len);\n\tit_present_val = BIT(IEEE80211_RADIOTAP_FLAGS) |\n\t\t\t BIT(IEEE80211_RADIOTAP_CHANNEL) |\n\t\t\t BIT(IEEE80211_RADIOTAP_RX_FLAGS);\n\n\tif (!status->chains)\n\t\tit_present_val |= BIT(IEEE80211_RADIOTAP_ANTENNA);\n\n\tfor_each_set_bit(chain, &chains, IEEE80211_MAX_CHAINS) {\n\t\tit_present_val |=\n\t\t\tBIT(IEEE80211_RADIOTAP_EXT) |\n\t\t\tBIT(IEEE80211_RADIOTAP_RADIOTAP_NAMESPACE);\n\t\tput_unaligned_le32(it_present_val, it_present);\n\t\tit_present++;\n\t\tit_present_val = BIT(IEEE80211_RADIOTAP_ANTENNA) |\n\t\t\t\t BIT(IEEE80211_RADIOTAP_DBM_ANTSIGNAL);\n\t}\n\n\tif (status->flag & RX_FLAG_RADIOTAP_VENDOR_DATA) {\n\t\tit_present_val |= BIT(IEEE80211_RADIOTAP_VENDOR_NAMESPACE) |\n\t\t\t\t  BIT(IEEE80211_RADIOTAP_EXT);\n\t\tput_unaligned_le32(it_present_val, it_present);\n\t\tit_present++;\n\t\tit_present_val = rtap.present;\n\t}\n\n\tput_unaligned_le32(it_present_val, it_present);\n\n\tpos = (void *)(it_present + 1);\n\n\t/* the order of the following fields is important */\n\n\t/* IEEE80211_RADIOTAP_TSFT */\n\tif (ieee80211_have_rx_timestamp(status)) {\n\t\t/* padding */\n\t\twhile ((pos - (u8 *)rthdr) & 7)\n\t\t\t*pos++ = 0;\n\t\tput_unaligned_le64(\n\t\t\tieee80211_calculate_rx_timestamp(local, status,\n\t\t\t\t\t\t\t mpdulen, 0),\n\t\t\tpos);\n\t\trthdr->it_present |= cpu_to_le32(1 << IEEE80211_RADIOTAP_TSFT);\n\t\tpos += 8;\n\t}\n\n\t/* IEEE80211_RADIOTAP_FLAGS */\n\tif (has_fcs && ieee80211_hw_check(&local->hw, RX_INCLUDES_FCS))\n\t\t*pos |= IEEE80211_RADIOTAP_F_FCS;\n\tif (status->flag & (RX_FLAG_FAILED_FCS_CRC | RX_FLAG_FAILED_PLCP_CRC))\n\t\t*pos |= IEEE80211_RADIOTAP_F_BADFCS;\n\tif (status->enc_flags & RX_ENC_FLAG_SHORTPRE)\n\t\t*pos |= IEEE80211_RADIOTAP_F_SHORTPRE;\n\tpos++;\n\n\t/* IEEE80211_RADIOTAP_RATE */\n\tif (!rate || status->encoding != RX_ENC_LEGACY) {\n\t\t/*\n\t\t * Without rate information don't add it. If we have,\n\t\t * MCS information is a separate field in radiotap,\n\t\t * added below. The byte here is needed as padding\n\t\t * for the channel though, so initialise it to 0.\n\t\t */\n\t\t*pos = 0;\n\t} else {\n\t\tint shift = 0;\n\t\trthdr->it_present |= cpu_to_le32(1 << IEEE80211_RADIOTAP_RATE);\n\t\tif (status->bw == RATE_INFO_BW_10)\n\t\t\tshift = 1;\n\t\telse if (status->bw == RATE_INFO_BW_5)\n\t\t\tshift = 2;\n\t\t*pos = DIV_ROUND_UP(rate->bitrate, 5 * (1 << shift));\n\t}\n\tpos++;\n\n\t/* IEEE80211_RADIOTAP_CHANNEL */\n\t/* TODO: frequency offset in KHz */\n\tput_unaligned_le16(status->freq, pos);\n\tpos += 2;\n\tif (status->bw == RATE_INFO_BW_10)\n\t\tchannel_flags |= IEEE80211_CHAN_HALF;\n\telse if (status->bw == RATE_INFO_BW_5)\n\t\tchannel_flags |= IEEE80211_CHAN_QUARTER;\n\n\tif (status->band == NL80211_BAND_5GHZ ||\n\t    status->band == NL80211_BAND_6GHZ)\n\t\tchannel_flags |= IEEE80211_CHAN_OFDM | IEEE80211_CHAN_5GHZ;\n\telse if (status->encoding != RX_ENC_LEGACY)\n\t\tchannel_flags |= IEEE80211_CHAN_DYN | IEEE80211_CHAN_2GHZ;\n\telse if (rate && rate->flags & IEEE80211_RATE_ERP_G)\n\t\tchannel_flags |= IEEE80211_CHAN_OFDM | IEEE80211_CHAN_2GHZ;\n\telse if (rate)\n\t\tchannel_flags |= IEEE80211_CHAN_CCK | IEEE80211_CHAN_2GHZ;\n\telse\n\t\tchannel_flags |= IEEE80211_CHAN_2GHZ;\n\tput_unaligned_le16(channel_flags, pos);\n\tpos += 2;\n\n\t/* IEEE80211_RADIOTAP_DBM_ANTSIGNAL */\n\tif (ieee80211_hw_check(&local->hw, SIGNAL_DBM) &&\n\t    !(status->flag & RX_FLAG_NO_SIGNAL_VAL)) {\n\t\t*pos = status->signal;\n\t\trthdr->it_present |=\n\t\t\tcpu_to_le32(1 << IEEE80211_RADIOTAP_DBM_ANTSIGNAL);\n\t\tpos++;\n\t}\n\n\t/* IEEE80211_RADIOTAP_LOCK_QUALITY is missing */\n\n\tif (!status->chains) {\n\t\t/* IEEE80211_RADIOTAP_ANTENNA */\n\t\t*pos = status->antenna;\n\t\tpos++;\n\t}\n\n\t/* IEEE80211_RADIOTAP_DB_ANTNOISE is not used */\n\n\t/* IEEE80211_RADIOTAP_RX_FLAGS */\n\t/* ensure 2 byte alignment for the 2 byte field as required */\n\tif ((pos - (u8 *)rthdr) & 1)\n\t\t*pos++ = 0;\n\tif (status->flag & RX_FLAG_FAILED_PLCP_CRC)\n\t\trx_flags |= IEEE80211_RADIOTAP_F_RX_BADPLCP;\n\tput_unaligned_le16(rx_flags, pos);\n\tpos += 2;\n\n\tif (status->encoding == RX_ENC_HT) {\n\t\tunsigned int stbc;\n\n\t\trthdr->it_present |= cpu_to_le32(1 << IEEE80211_RADIOTAP_MCS);\n\t\t*pos++ = local->hw.radiotap_mcs_details;\n\t\t*pos = 0;\n\t\tif (status->enc_flags & RX_ENC_FLAG_SHORT_GI)\n\t\t\t*pos |= IEEE80211_RADIOTAP_MCS_SGI;\n\t\tif (status->bw == RATE_INFO_BW_40)\n\t\t\t*pos |= IEEE80211_RADIOTAP_MCS_BW_40;\n\t\tif (status->enc_flags & RX_ENC_FLAG_HT_GF)\n\t\t\t*pos |= IEEE80211_RADIOTAP_MCS_FMT_GF;\n\t\tif (status->enc_flags & RX_ENC_FLAG_LDPC)\n\t\t\t*pos |= IEEE80211_RADIOTAP_MCS_FEC_LDPC;\n\t\tstbc = (status->enc_flags & RX_ENC_FLAG_STBC_MASK) >> RX_ENC_FLAG_STBC_SHIFT;\n\t\t*pos |= stbc << IEEE80211_RADIOTAP_MCS_STBC_SHIFT;\n\t\tpos++;\n\t\t*pos++ = status->rate_idx;\n\t}\n\n\tif (status->flag & RX_FLAG_AMPDU_DETAILS) {\n\t\tu16 flags = 0;\n\n\t\t/* ensure 4 byte alignment */\n\t\twhile ((pos - (u8 *)rthdr) & 3)\n\t\t\tpos++;\n\t\trthdr->it_present |=\n\t\t\tcpu_to_le32(1 << IEEE80211_RADIOTAP_AMPDU_STATUS);\n\t\tput_unaligned_le32(status->ampdu_reference, pos);\n\t\tpos += 4;\n\t\tif (status->flag & RX_FLAG_AMPDU_LAST_KNOWN)\n\t\t\tflags |= IEEE80211_RADIOTAP_AMPDU_LAST_KNOWN;\n\t\tif (status->flag & RX_FLAG_AMPDU_IS_LAST)\n\t\t\tflags |= IEEE80211_RADIOTAP_AMPDU_IS_LAST;\n\t\tif (status->flag & RX_FLAG_AMPDU_DELIM_CRC_ERROR)\n\t\t\tflags |= IEEE80211_RADIOTAP_AMPDU_DELIM_CRC_ERR;\n\t\tif (status->flag & RX_FLAG_AMPDU_DELIM_CRC_KNOWN)\n\t\t\tflags |= IEEE80211_RADIOTAP_AMPDU_DELIM_CRC_KNOWN;\n\t\tif (status->flag & RX_FLAG_AMPDU_EOF_BIT_KNOWN)\n\t\t\tflags |= IEEE80211_RADIOTAP_AMPDU_EOF_KNOWN;\n\t\tif (status->flag & RX_FLAG_AMPDU_EOF_BIT)\n\t\t\tflags |= IEEE80211_RADIOTAP_AMPDU_EOF;\n\t\tput_unaligned_le16(flags, pos);\n\t\tpos += 2;\n\t\tif (status->flag & RX_FLAG_AMPDU_DELIM_CRC_KNOWN)\n\t\t\t*pos++ = status->ampdu_delimiter_crc;\n\t\telse\n\t\t\t*pos++ = 0;\n\t\t*pos++ = 0;\n\t}\n\n\tif (status->encoding == RX_ENC_VHT) {\n\t\tu16 known = local->hw.radiotap_vht_details;\n\n\t\trthdr->it_present |= cpu_to_le32(1 << IEEE80211_RADIOTAP_VHT);\n\t\tput_unaligned_le16(known, pos);\n\t\tpos += 2;\n\t\t/* flags */\n\t\tif (status->enc_flags & RX_ENC_FLAG_SHORT_GI)\n\t\t\t*pos |= IEEE80211_RADIOTAP_VHT_FLAG_SGI;\n\t\t/* in VHT, STBC is binary */\n\t\tif (status->enc_flags & RX_ENC_FLAG_STBC_MASK)\n\t\t\t*pos |= IEEE80211_RADIOTAP_VHT_FLAG_STBC;\n\t\tif (status->enc_flags & RX_ENC_FLAG_BF)\n\t\t\t*pos |= IEEE80211_RADIOTAP_VHT_FLAG_BEAMFORMED;\n\t\tpos++;\n\t\t/* bandwidth */\n\t\tswitch (status->bw) {\n\t\tcase RATE_INFO_BW_80:\n\t\t\t*pos++ = 4;\n\t\t\tbreak;\n\t\tcase RATE_INFO_BW_160:\n\t\t\t*pos++ = 11;\n\t\t\tbreak;\n\t\tcase RATE_INFO_BW_40:\n\t\t\t*pos++ = 1;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t*pos++ = 0;\n\t\t}\n\t\t/* MCS/NSS */\n\t\t*pos = (status->rate_idx << 4) | status->nss;\n\t\tpos += 4;\n\t\t/* coding field */\n\t\tif (status->enc_flags & RX_ENC_FLAG_LDPC)\n\t\t\t*pos |= IEEE80211_RADIOTAP_CODING_LDPC_USER0;\n\t\tpos++;\n\t\t/* group ID */\n\t\tpos++;\n\t\t/* partial_aid */\n\t\tpos += 2;\n\t}\n\n\tif (local->hw.radiotap_timestamp.units_pos >= 0) {\n\t\tu16 accuracy = 0;\n\t\tu8 flags = IEEE80211_RADIOTAP_TIMESTAMP_FLAG_32BIT;\n\n\t\trthdr->it_present |=\n\t\t\tcpu_to_le32(1 << IEEE80211_RADIOTAP_TIMESTAMP);\n\n\t\t/* ensure 8 byte alignment */\n\t\twhile ((pos - (u8 *)rthdr) & 7)\n\t\t\tpos++;\n\n\t\tput_unaligned_le64(status->device_timestamp, pos);\n\t\tpos += sizeof(u64);\n\n\t\tif (local->hw.radiotap_timestamp.accuracy >= 0) {\n\t\t\taccuracy = local->hw.radiotap_timestamp.accuracy;\n\t\t\tflags |= IEEE80211_RADIOTAP_TIMESTAMP_FLAG_ACCURACY;\n\t\t}\n\t\tput_unaligned_le16(accuracy, pos);\n\t\tpos += sizeof(u16);\n\n\t\t*pos++ = local->hw.radiotap_timestamp.units_pos;\n\t\t*pos++ = flags;\n\t}\n\n\tif (status->encoding == RX_ENC_HE &&\n\t    status->flag & RX_FLAG_RADIOTAP_HE) {\n#define HE_PREP(f, val)\tle16_encode_bits(val, IEEE80211_RADIOTAP_HE_##f)\n\n\t\tif (status->enc_flags & RX_ENC_FLAG_STBC_MASK) {\n\t\t\the.data6 |= HE_PREP(DATA6_NSTS,\n\t\t\t\t\t    FIELD_GET(RX_ENC_FLAG_STBC_MASK,\n\t\t\t\t\t\t      status->enc_flags));\n\t\t\the.data3 |= HE_PREP(DATA3_STBC, 1);\n\t\t} else {\n\t\t\the.data6 |= HE_PREP(DATA6_NSTS, status->nss);\n\t\t}\n\n#define CHECK_GI(s) \\\n\tBUILD_BUG_ON(IEEE80211_RADIOTAP_HE_DATA5_GI_##s != \\\n\t\t     (int)NL80211_RATE_INFO_HE_GI_##s)\n\n\t\tCHECK_GI(0_8);\n\t\tCHECK_GI(1_6);\n\t\tCHECK_GI(3_2);\n\n\t\the.data3 |= HE_PREP(DATA3_DATA_MCS, status->rate_idx);\n\t\the.data3 |= HE_PREP(DATA3_DATA_DCM, status->he_dcm);\n\t\the.data3 |= HE_PREP(DATA3_CODING,\n\t\t\t\t    !!(status->enc_flags & RX_ENC_FLAG_LDPC));\n\n\t\the.data5 |= HE_PREP(DATA5_GI, status->he_gi);\n\n\t\tswitch (status->bw) {\n\t\tcase RATE_INFO_BW_20:\n\t\t\the.data5 |= HE_PREP(DATA5_DATA_BW_RU_ALLOC,\n\t\t\t\t\t    IEEE80211_RADIOTAP_HE_DATA5_DATA_BW_RU_ALLOC_20MHZ);\n\t\t\tbreak;\n\t\tcase RATE_INFO_BW_40:\n\t\t\the.data5 |= HE_PREP(DATA5_DATA_BW_RU_ALLOC,\n\t\t\t\t\t    IEEE80211_RADIOTAP_HE_DATA5_DATA_BW_RU_ALLOC_40MHZ);\n\t\t\tbreak;\n\t\tcase RATE_INFO_BW_80:\n\t\t\the.data5 |= HE_PREP(DATA5_DATA_BW_RU_ALLOC,\n\t\t\t\t\t    IEEE80211_RADIOTAP_HE_DATA5_DATA_BW_RU_ALLOC_80MHZ);\n\t\t\tbreak;\n\t\tcase RATE_INFO_BW_160:\n\t\t\the.data5 |= HE_PREP(DATA5_DATA_BW_RU_ALLOC,\n\t\t\t\t\t    IEEE80211_RADIOTAP_HE_DATA5_DATA_BW_RU_ALLOC_160MHZ);\n\t\t\tbreak;\n\t\tcase RATE_INFO_BW_HE_RU:\n#define CHECK_RU_ALLOC(s) \\\n\tBUILD_BUG_ON(IEEE80211_RADIOTAP_HE_DATA5_DATA_BW_RU_ALLOC_##s##T != \\\n\t\t     NL80211_RATE_INFO_HE_RU_ALLOC_##s + 4)\n\n\t\t\tCHECK_RU_ALLOC(26);\n\t\t\tCHECK_RU_ALLOC(52);\n\t\t\tCHECK_RU_ALLOC(106);\n\t\t\tCHECK_RU_ALLOC(242);\n\t\t\tCHECK_RU_ALLOC(484);\n\t\t\tCHECK_RU_ALLOC(996);\n\t\t\tCHECK_RU_ALLOC(2x996);\n\n\t\t\the.data5 |= HE_PREP(DATA5_DATA_BW_RU_ALLOC,\n\t\t\t\t\t    status->he_ru + 4);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ONCE(1, \"Invalid SU BW %d\\n\", status->bw);\n\t\t}\n\n\t\t/* ensure 2 byte alignment */\n\t\twhile ((pos - (u8 *)rthdr) & 1)\n\t\t\tpos++;\n\t\trthdr->it_present |= cpu_to_le32(1 << IEEE80211_RADIOTAP_HE);\n\t\tmemcpy(pos, &he, sizeof(he));\n\t\tpos += sizeof(he);\n\t}\n\n\tif (status->encoding == RX_ENC_HE &&\n\t    status->flag & RX_FLAG_RADIOTAP_HE_MU) {\n\t\t/* ensure 2 byte alignment */\n\t\twhile ((pos - (u8 *)rthdr) & 1)\n\t\t\tpos++;\n\t\trthdr->it_present |= cpu_to_le32(1 << IEEE80211_RADIOTAP_HE_MU);\n\t\tmemcpy(pos, &he_mu, sizeof(he_mu));\n\t\tpos += sizeof(he_mu);\n\t}\n\n\tif (status->flag & RX_FLAG_NO_PSDU) {\n\t\trthdr->it_present |=\n\t\t\tcpu_to_le32(1 << IEEE80211_RADIOTAP_ZERO_LEN_PSDU);\n\t\t*pos++ = status->zero_length_psdu_type;\n\t}\n\n\tif (status->flag & RX_FLAG_RADIOTAP_LSIG) {\n\t\t/* ensure 2 byte alignment */\n\t\twhile ((pos - (u8 *)rthdr) & 1)\n\t\t\tpos++;\n\t\trthdr->it_present |= cpu_to_le32(1 << IEEE80211_RADIOTAP_LSIG);\n\t\tmemcpy(pos, &lsig, sizeof(lsig));\n\t\tpos += sizeof(lsig);\n\t}\n\n\tfor_each_set_bit(chain, &chains, IEEE80211_MAX_CHAINS) {\n\t\t*pos++ = status->chain_signal[chain];\n\t\t*pos++ = chain;\n\t}\n\n\tif (status->flag & RX_FLAG_RADIOTAP_VENDOR_DATA) {\n\t\t/* ensure 2 byte alignment for the vendor field as required */\n\t\tif ((pos - (u8 *)rthdr) & 1)\n\t\t\t*pos++ = 0;\n\t\t*pos++ = rtap.oui[0];\n\t\t*pos++ = rtap.oui[1];\n\t\t*pos++ = rtap.oui[2];\n\t\t*pos++ = rtap.subns;\n\t\tput_unaligned_le16(rtap.len, pos);\n\t\tpos += 2;\n\t\t/* align the actual payload as requested */\n\t\twhile ((pos - (u8 *)rthdr) & (rtap.align - 1))\n\t\t\t*pos++ = 0;\n\t\t/* data (and possible padding) already follows */\n\t}\n}\n\nstatic struct sk_buff *\nieee80211_make_monitor_skb(struct ieee80211_local *local,\n\t\t\t   struct sk_buff **origskb,\n\t\t\t   struct ieee80211_rate *rate,\n\t\t\t   int rtap_space, bool use_origskb)\n{\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(*origskb);\n\tint rt_hdrlen, needed_headroom;\n\tstruct sk_buff *skb;\n\n\t/* room for the radiotap header based on driver features */\n\trt_hdrlen = ieee80211_rx_radiotap_hdrlen(local, status, *origskb);\n\tneeded_headroom = rt_hdrlen - rtap_space;\n\n\tif (use_origskb) {\n\t\t/* only need to expand headroom if necessary */\n\t\tskb = *origskb;\n\t\t*origskb = NULL;\n\n\t\t/*\n\t\t * This shouldn't trigger often because most devices have an\n\t\t * RX header they pull before we get here, and that should\n\t\t * be big enough for our radiotap information. We should\n\t\t * probably export the length to drivers so that we can have\n\t\t * them allocate enough headroom to start with.\n\t\t */\n\t\tif (skb_headroom(skb) < needed_headroom &&\n\t\t    pskb_expand_head(skb, needed_headroom, 0, GFP_ATOMIC)) {\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn NULL;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * Need to make a copy and possibly remove radiotap header\n\t\t * and FCS from the original.\n\t\t */\n\t\tskb = skb_copy_expand(*origskb, needed_headroom, 0, GFP_ATOMIC);\n\n\t\tif (!skb)\n\t\t\treturn NULL;\n\t}\n\n\t/* prepend radiotap information */\n\tieee80211_add_rx_radiotap_header(local, skb, rate, rt_hdrlen, true);\n\n\tskb_reset_mac_header(skb);\n\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\tskb->pkt_type = PACKET_OTHERHOST;\n\tskb->protocol = htons(ETH_P_802_2);\n\n\treturn skb;\n}\n\n/*\n * This function copies a received frame to all monitor interfaces and\n * returns a cleaned-up SKB that no longer includes the FCS nor the\n * radiotap header the driver might have added.\n */\nstatic struct sk_buff *\nieee80211_rx_monitor(struct ieee80211_local *local, struct sk_buff *origskb,\n\t\t     struct ieee80211_rate *rate)\n{\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(origskb);\n\tstruct ieee80211_sub_if_data *sdata;\n\tstruct sk_buff *monskb = NULL;\n\tint present_fcs_len = 0;\n\tunsigned int rtap_space = 0;\n\tstruct ieee80211_sub_if_data *monitor_sdata =\n\t\trcu_dereference(local->monitor_sdata);\n\tbool only_monitor = false;\n\tunsigned int min_head_len;\n\n\tif (status->flag & RX_FLAG_RADIOTAP_HE)\n\t\trtap_space += sizeof(struct ieee80211_radiotap_he);\n\n\tif (status->flag & RX_FLAG_RADIOTAP_HE_MU)\n\t\trtap_space += sizeof(struct ieee80211_radiotap_he_mu);\n\n\tif (status->flag & RX_FLAG_RADIOTAP_LSIG)\n\t\trtap_space += sizeof(struct ieee80211_radiotap_lsig);\n\n\tif (unlikely(status->flag & RX_FLAG_RADIOTAP_VENDOR_DATA)) {\n\t\tstruct ieee80211_vendor_radiotap *rtap =\n\t\t\t(void *)(origskb->data + rtap_space);\n\n\t\trtap_space += sizeof(*rtap) + rtap->len + rtap->pad;\n\t}\n\n\tmin_head_len = rtap_space;\n\n\t/*\n\t * First, we may need to make a copy of the skb because\n\t *  (1) we need to modify it for radiotap (if not present), and\n\t *  (2) the other RX handlers will modify the skb we got.\n\t *\n\t * We don't need to, of course, if we aren't going to return\n\t * the SKB because it has a bad FCS/PLCP checksum.\n\t */\n\n\tif (!(status->flag & RX_FLAG_NO_PSDU)) {\n\t\tif (ieee80211_hw_check(&local->hw, RX_INCLUDES_FCS)) {\n\t\t\tif (unlikely(origskb->len <= FCS_LEN + rtap_space)) {\n\t\t\t\t/* driver bug */\n\t\t\t\tWARN_ON(1);\n\t\t\t\tdev_kfree_skb(origskb);\n\t\t\t\treturn NULL;\n\t\t\t}\n\t\t\tpresent_fcs_len = FCS_LEN;\n\t\t}\n\n\t\t/* also consider the hdr->frame_control */\n\t\tmin_head_len += 2;\n\t}\n\n\t/* ensure that the expected data elements are in skb head */\n\tif (!pskb_may_pull(origskb, min_head_len)) {\n\t\tdev_kfree_skb(origskb);\n\t\treturn NULL;\n\t}\n\n\tonly_monitor = should_drop_frame(origskb, present_fcs_len, rtap_space);\n\n\tif (!local->monitors || (status->flag & RX_FLAG_SKIP_MONITOR)) {\n\t\tif (only_monitor) {\n\t\t\tdev_kfree_skb(origskb);\n\t\t\treturn NULL;\n\t\t}\n\n\t\treturn ieee80211_clean_skb(origskb, present_fcs_len,\n\t\t\t\t\t   rtap_space);\n\t}\n\n\tieee80211_handle_mu_mimo_mon(monitor_sdata, origskb, rtap_space);\n\n\tlist_for_each_entry_rcu(sdata, &local->mon_list, u.mntr.list) {\n\t\tbool last_monitor = list_is_last(&sdata->u.mntr.list,\n\t\t\t\t\t\t &local->mon_list);\n\n\t\tif (!monskb)\n\t\t\tmonskb = ieee80211_make_monitor_skb(local, &origskb,\n\t\t\t\t\t\t\t    rate, rtap_space,\n\t\t\t\t\t\t\t    only_monitor &&\n\t\t\t\t\t\t\t    last_monitor);\n\n\t\tif (monskb) {\n\t\t\tstruct sk_buff *skb;\n\n\t\t\tif (last_monitor) {\n\t\t\t\tskb = monskb;\n\t\t\t\tmonskb = NULL;\n\t\t\t} else {\n\t\t\t\tskb = skb_clone(monskb, GFP_ATOMIC);\n\t\t\t}\n\n\t\t\tif (skb) {\n\t\t\t\tskb->dev = sdata->dev;\n\t\t\t\tdev_sw_netstats_rx_add(skb->dev, skb->len);\n\t\t\t\tnetif_receive_skb(skb);\n\t\t\t}\n\t\t}\n\n\t\tif (last_monitor)\n\t\t\tbreak;\n\t}\n\n\t/* this happens if last_monitor was erroneously false */\n\tdev_kfree_skb(monskb);\n\n\t/* ditto */\n\tif (!origskb)\n\t\treturn NULL;\n\n\treturn ieee80211_clean_skb(origskb, present_fcs_len, rtap_space);\n}\n\nstatic void ieee80211_parse_qos(struct ieee80211_rx_data *rx)\n{\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)rx->skb->data;\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(rx->skb);\n\tint tid, seqno_idx, security_idx;\n\n\t/* does the frame have a qos control field? */\n\tif (ieee80211_is_data_qos(hdr->frame_control)) {\n\t\tu8 *qc = ieee80211_get_qos_ctl(hdr);\n\t\t/* frame has qos control */\n\t\ttid = *qc & IEEE80211_QOS_CTL_TID_MASK;\n\t\tif (*qc & IEEE80211_QOS_CTL_A_MSDU_PRESENT)\n\t\t\tstatus->rx_flags |= IEEE80211_RX_AMSDU;\n\n\t\tseqno_idx = tid;\n\t\tsecurity_idx = tid;\n\t} else {\n\t\t/*\n\t\t * IEEE 802.11-2007, 7.1.3.4.1 (\"Sequence Number field\"):\n\t\t *\n\t\t *\tSequence numbers for management frames, QoS data\n\t\t *\tframes with a broadcast/multicast address in the\n\t\t *\tAddress 1 field, and all non-QoS data frames sent\n\t\t *\tby QoS STAs are assigned using an additional single\n\t\t *\tmodulo-4096 counter, [...]\n\t\t *\n\t\t * We also use that counter for non-QoS STAs.\n\t\t */\n\t\tseqno_idx = IEEE80211_NUM_TIDS;\n\t\tsecurity_idx = 0;\n\t\tif (ieee80211_is_mgmt(hdr->frame_control))\n\t\t\tsecurity_idx = IEEE80211_NUM_TIDS;\n\t\ttid = 0;\n\t}\n\n\trx->seqno_idx = seqno_idx;\n\trx->security_idx = security_idx;\n\t/* Set skb->priority to 1d tag if highest order bit of TID is not set.\n\t * For now, set skb->priority to 0 for other cases. */\n\trx->skb->priority = (tid > 7) ? 0 : tid;\n}\n\n/**\n * DOC: Packet alignment\n *\n * Drivers always need to pass packets that are aligned to two-byte boundaries\n * to the stack.\n *\n * Additionally, should, if possible, align the payload data in a way that\n * guarantees that the contained IP header is aligned to a four-byte\n * boundary. In the case of regular frames, this simply means aligning the\n * payload to a four-byte boundary (because either the IP header is directly\n * contained, or IV/RFC1042 headers that have a length divisible by four are\n * in front of it).  If the payload data is not properly aligned and the\n * architecture doesn't support efficient unaligned operations, mac80211\n * will align the data.\n *\n * With A-MSDU frames, however, the payload data address must yield two modulo\n * four because there are 14-byte 802.3 headers within the A-MSDU frames that\n * push the IP header further back to a multiple of four again. Thankfully, the\n * specs were sane enough this time around to require padding each A-MSDU\n * subframe to a length that is a multiple of four.\n *\n * Padding like Atheros hardware adds which is between the 802.11 header and\n * the payload is not supported, the driver is required to move the 802.11\n * header to be directly in front of the payload in that case.\n */\nstatic void ieee80211_verify_alignment(struct ieee80211_rx_data *rx)\n{\n#ifdef CONFIG_MAC80211_VERBOSE_DEBUG\n\tWARN_ON_ONCE((unsigned long)rx->skb->data & 1);\n#endif\n}\n\n\n/* rx handlers */\n\nstatic int ieee80211_is_unicast_robust_mgmt_frame(struct sk_buff *skb)\n{\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *) skb->data;\n\n\tif (is_multicast_ether_addr(hdr->addr1))\n\t\treturn 0;\n\n\treturn ieee80211_is_robust_mgmt_frame(skb);\n}\n\n\nstatic int ieee80211_is_multicast_robust_mgmt_frame(struct sk_buff *skb)\n{\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *) skb->data;\n\n\tif (!is_multicast_ether_addr(hdr->addr1))\n\t\treturn 0;\n\n\treturn ieee80211_is_robust_mgmt_frame(skb);\n}\n\n\n/* Get the BIP key index from MMIE; return -1 if this is not a BIP frame */\nstatic int ieee80211_get_mmie_keyidx(struct sk_buff *skb)\n{\n\tstruct ieee80211_mgmt *hdr = (struct ieee80211_mgmt *) skb->data;\n\tstruct ieee80211_mmie *mmie;\n\tstruct ieee80211_mmie_16 *mmie16;\n\n\tif (skb->len < 24 + sizeof(*mmie) || !is_multicast_ether_addr(hdr->da))\n\t\treturn -1;\n\n\tif (!ieee80211_is_robust_mgmt_frame(skb) &&\n\t    !ieee80211_is_beacon(hdr->frame_control))\n\t\treturn -1; /* not a robust management frame */\n\n\tmmie = (struct ieee80211_mmie *)\n\t\t(skb->data + skb->len - sizeof(*mmie));\n\tif (mmie->element_id == WLAN_EID_MMIE &&\n\t    mmie->length == sizeof(*mmie) - 2)\n\t\treturn le16_to_cpu(mmie->key_id);\n\n\tmmie16 = (struct ieee80211_mmie_16 *)\n\t\t(skb->data + skb->len - sizeof(*mmie16));\n\tif (skb->len >= 24 + sizeof(*mmie16) &&\n\t    mmie16->element_id == WLAN_EID_MMIE &&\n\t    mmie16->length == sizeof(*mmie16) - 2)\n\t\treturn le16_to_cpu(mmie16->key_id);\n\n\treturn -1;\n}\n\nstatic int ieee80211_get_keyid(struct sk_buff *skb,\n\t\t\t       const struct ieee80211_cipher_scheme *cs)\n{\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)skb->data;\n\t__le16 fc;\n\tint hdrlen;\n\tint minlen;\n\tu8 key_idx_off;\n\tu8 key_idx_shift;\n\tu8 keyid;\n\n\tfc = hdr->frame_control;\n\thdrlen = ieee80211_hdrlen(fc);\n\n\tif (cs) {\n\t\tminlen = hdrlen + cs->hdr_len;\n\t\tkey_idx_off = hdrlen + cs->key_idx_off;\n\t\tkey_idx_shift = cs->key_idx_shift;\n\t} else {\n\t\t/* WEP, TKIP, CCMP and GCMP */\n\t\tminlen = hdrlen + IEEE80211_WEP_IV_LEN;\n\t\tkey_idx_off = hdrlen + 3;\n\t\tkey_idx_shift = 6;\n\t}\n\n\tif (unlikely(skb->len < minlen))\n\t\treturn -EINVAL;\n\n\tskb_copy_bits(skb, key_idx_off, &keyid, 1);\n\n\tif (cs)\n\t\tkeyid &= cs->key_idx_mask;\n\tkeyid >>= key_idx_shift;\n\n\t/* cs could use more than the usual two bits for the keyid */\n\tif (unlikely(keyid >= NUM_DEFAULT_KEYS))\n\t\treturn -EINVAL;\n\n\treturn keyid;\n}\n\nstatic ieee80211_rx_result ieee80211_rx_mesh_check(struct ieee80211_rx_data *rx)\n{\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)rx->skb->data;\n\tchar *dev_addr = rx->sdata->vif.addr;\n\n\tif (ieee80211_is_data(hdr->frame_control)) {\n\t\tif (is_multicast_ether_addr(hdr->addr1)) {\n\t\t\tif (ieee80211_has_tods(hdr->frame_control) ||\n\t\t\t    !ieee80211_has_fromds(hdr->frame_control))\n\t\t\t\treturn RX_DROP_MONITOR;\n\t\t\tif (ether_addr_equal(hdr->addr3, dev_addr))\n\t\t\t\treturn RX_DROP_MONITOR;\n\t\t} else {\n\t\t\tif (!ieee80211_has_a4(hdr->frame_control))\n\t\t\t\treturn RX_DROP_MONITOR;\n\t\t\tif (ether_addr_equal(hdr->addr4, dev_addr))\n\t\t\t\treturn RX_DROP_MONITOR;\n\t\t}\n\t}\n\n\t/* If there is not an established peer link and this is not a peer link\n\t * establisment frame, beacon or probe, drop the frame.\n\t */\n\n\tif (!rx->sta || sta_plink_state(rx->sta) != NL80211_PLINK_ESTAB) {\n\t\tstruct ieee80211_mgmt *mgmt;\n\n\t\tif (!ieee80211_is_mgmt(hdr->frame_control))\n\t\t\treturn RX_DROP_MONITOR;\n\n\t\tif (ieee80211_is_action(hdr->frame_control)) {\n\t\t\tu8 category;\n\n\t\t\t/* make sure category field is present */\n\t\t\tif (rx->skb->len < IEEE80211_MIN_ACTION_SIZE)\n\t\t\t\treturn RX_DROP_MONITOR;\n\n\t\t\tmgmt = (struct ieee80211_mgmt *)hdr;\n\t\t\tcategory = mgmt->u.action.category;\n\t\t\tif (category != WLAN_CATEGORY_MESH_ACTION &&\n\t\t\t    category != WLAN_CATEGORY_SELF_PROTECTED)\n\t\t\t\treturn RX_DROP_MONITOR;\n\t\t\treturn RX_CONTINUE;\n\t\t}\n\n\t\tif (ieee80211_is_probe_req(hdr->frame_control) ||\n\t\t    ieee80211_is_probe_resp(hdr->frame_control) ||\n\t\t    ieee80211_is_beacon(hdr->frame_control) ||\n\t\t    ieee80211_is_auth(hdr->frame_control))\n\t\t\treturn RX_CONTINUE;\n\n\t\treturn RX_DROP_MONITOR;\n\t}\n\n\treturn RX_CONTINUE;\n}\n\nstatic inline bool ieee80211_rx_reorder_ready(struct tid_ampdu_rx *tid_agg_rx,\n\t\t\t\t\t      int index)\n{\n\tstruct sk_buff_head *frames = &tid_agg_rx->reorder_buf[index];\n\tstruct sk_buff *tail = skb_peek_tail(frames);\n\tstruct ieee80211_rx_status *status;\n\n\tif (tid_agg_rx->reorder_buf_filtered & BIT_ULL(index))\n\t\treturn true;\n\n\tif (!tail)\n\t\treturn false;\n\n\tstatus = IEEE80211_SKB_RXCB(tail);\n\tif (status->flag & RX_FLAG_AMSDU_MORE)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void ieee80211_release_reorder_frame(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t\t    struct tid_ampdu_rx *tid_agg_rx,\n\t\t\t\t\t    int index,\n\t\t\t\t\t    struct sk_buff_head *frames)\n{\n\tstruct sk_buff_head *skb_list = &tid_agg_rx->reorder_buf[index];\n\tstruct sk_buff *skb;\n\tstruct ieee80211_rx_status *status;\n\n\tlockdep_assert_held(&tid_agg_rx->reorder_lock);\n\n\tif (skb_queue_empty(skb_list))\n\t\tgoto no_frame;\n\n\tif (!ieee80211_rx_reorder_ready(tid_agg_rx, index)) {\n\t\t__skb_queue_purge(skb_list);\n\t\tgoto no_frame;\n\t}\n\n\t/* release frames from the reorder ring buffer */\n\ttid_agg_rx->stored_mpdu_num--;\n\twhile ((skb = __skb_dequeue(skb_list))) {\n\t\tstatus = IEEE80211_SKB_RXCB(skb);\n\t\tstatus->rx_flags |= IEEE80211_RX_DEFERRED_RELEASE;\n\t\t__skb_queue_tail(frames, skb);\n\t}\n\nno_frame:\n\ttid_agg_rx->reorder_buf_filtered &= ~BIT_ULL(index);\n\ttid_agg_rx->head_seq_num = ieee80211_sn_inc(tid_agg_rx->head_seq_num);\n}\n\nstatic void ieee80211_release_reorder_frames(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t\t     struct tid_ampdu_rx *tid_agg_rx,\n\t\t\t\t\t     u16 head_seq_num,\n\t\t\t\t\t     struct sk_buff_head *frames)\n{\n\tint index;\n\n\tlockdep_assert_held(&tid_agg_rx->reorder_lock);\n\n\twhile (ieee80211_sn_less(tid_agg_rx->head_seq_num, head_seq_num)) {\n\t\tindex = tid_agg_rx->head_seq_num % tid_agg_rx->buf_size;\n\t\tieee80211_release_reorder_frame(sdata, tid_agg_rx, index,\n\t\t\t\t\t\tframes);\n\t}\n}\n\n/*\n * Timeout (in jiffies) for skb's that are waiting in the RX reorder buffer. If\n * the skb was added to the buffer longer than this time ago, the earlier\n * frames that have not yet been received are assumed to be lost and the skb\n * can be released for processing. This may also release other skb's from the\n * reorder buffer if there are no additional gaps between the frames.\n *\n * Callers must hold tid_agg_rx->reorder_lock.\n */\n#define HT_RX_REORDER_BUF_TIMEOUT (HZ / 10)\n\nstatic void ieee80211_sta_reorder_release(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t\t  struct tid_ampdu_rx *tid_agg_rx,\n\t\t\t\t\t  struct sk_buff_head *frames)\n{\n\tint index, i, j;\n\n\tlockdep_assert_held(&tid_agg_rx->reorder_lock);\n\n\t/* release the buffer until next missing frame */\n\tindex = tid_agg_rx->head_seq_num % tid_agg_rx->buf_size;\n\tif (!ieee80211_rx_reorder_ready(tid_agg_rx, index) &&\n\t    tid_agg_rx->stored_mpdu_num) {\n\t\t/*\n\t\t * No buffers ready to be released, but check whether any\n\t\t * frames in the reorder buffer have timed out.\n\t\t */\n\t\tint skipped = 1;\n\t\tfor (j = (index + 1) % tid_agg_rx->buf_size; j != index;\n\t\t     j = (j + 1) % tid_agg_rx->buf_size) {\n\t\t\tif (!ieee80211_rx_reorder_ready(tid_agg_rx, j)) {\n\t\t\t\tskipped++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (skipped &&\n\t\t\t    !time_after(jiffies, tid_agg_rx->reorder_time[j] +\n\t\t\t\t\tHT_RX_REORDER_BUF_TIMEOUT))\n\t\t\t\tgoto set_release_timer;\n\n\t\t\t/* don't leave incomplete A-MSDUs around */\n\t\t\tfor (i = (index + 1) % tid_agg_rx->buf_size; i != j;\n\t\t\t     i = (i + 1) % tid_agg_rx->buf_size)\n\t\t\t\t__skb_queue_purge(&tid_agg_rx->reorder_buf[i]);\n\n\t\t\tht_dbg_ratelimited(sdata,\n\t\t\t\t\t   \"release an RX reorder frame due to timeout on earlier frames\\n\");\n\t\t\tieee80211_release_reorder_frame(sdata, tid_agg_rx, j,\n\t\t\t\t\t\t\tframes);\n\n\t\t\t/*\n\t\t\t * Increment the head seq# also for the skipped slots.\n\t\t\t */\n\t\t\ttid_agg_rx->head_seq_num =\n\t\t\t\t(tid_agg_rx->head_seq_num +\n\t\t\t\t skipped) & IEEE80211_SN_MASK;\n\t\t\tskipped = 0;\n\t\t}\n\t} else while (ieee80211_rx_reorder_ready(tid_agg_rx, index)) {\n\t\tieee80211_release_reorder_frame(sdata, tid_agg_rx, index,\n\t\t\t\t\t\tframes);\n\t\tindex =\ttid_agg_rx->head_seq_num % tid_agg_rx->buf_size;\n\t}\n\n\tif (tid_agg_rx->stored_mpdu_num) {\n\t\tj = index = tid_agg_rx->head_seq_num % tid_agg_rx->buf_size;\n\n\t\tfor (; j != (index - 1) % tid_agg_rx->buf_size;\n\t\t     j = (j + 1) % tid_agg_rx->buf_size) {\n\t\t\tif (ieee80211_rx_reorder_ready(tid_agg_rx, j))\n\t\t\t\tbreak;\n\t\t}\n\n set_release_timer:\n\n\t\tif (!tid_agg_rx->removed)\n\t\t\tmod_timer(&tid_agg_rx->reorder_timer,\n\t\t\t\t  tid_agg_rx->reorder_time[j] + 1 +\n\t\t\t\t  HT_RX_REORDER_BUF_TIMEOUT);\n\t} else {\n\t\tdel_timer(&tid_agg_rx->reorder_timer);\n\t}\n}\n\n/*\n * As this function belongs to the RX path it must be under\n * rcu_read_lock protection. It returns false if the frame\n * can be processed immediately, true if it was consumed.\n */\nstatic bool ieee80211_sta_manage_reorder_buf(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t\t     struct tid_ampdu_rx *tid_agg_rx,\n\t\t\t\t\t     struct sk_buff *skb,\n\t\t\t\t\t     struct sk_buff_head *frames)\n{\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *) skb->data;\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(skb);\n\tu16 sc = le16_to_cpu(hdr->seq_ctrl);\n\tu16 mpdu_seq_num = (sc & IEEE80211_SCTL_SEQ) >> 4;\n\tu16 head_seq_num, buf_size;\n\tint index;\n\tbool ret = true;\n\n\tspin_lock(&tid_agg_rx->reorder_lock);\n\n\t/*\n\t * Offloaded BA sessions have no known starting sequence number so pick\n\t * one from first Rxed frame for this tid after BA was started.\n\t */\n\tif (unlikely(tid_agg_rx->auto_seq)) {\n\t\ttid_agg_rx->auto_seq = false;\n\t\ttid_agg_rx->ssn = mpdu_seq_num;\n\t\ttid_agg_rx->head_seq_num = mpdu_seq_num;\n\t}\n\n\tbuf_size = tid_agg_rx->buf_size;\n\thead_seq_num = tid_agg_rx->head_seq_num;\n\n\t/*\n\t * If the current MPDU's SN is smaller than the SSN, it shouldn't\n\t * be reordered.\n\t */\n\tif (unlikely(!tid_agg_rx->started)) {\n\t\tif (ieee80211_sn_less(mpdu_seq_num, head_seq_num)) {\n\t\t\tret = false;\n\t\t\tgoto out;\n\t\t}\n\t\ttid_agg_rx->started = true;\n\t}\n\n\t/* frame with out of date sequence number */\n\tif (ieee80211_sn_less(mpdu_seq_num, head_seq_num)) {\n\t\tdev_kfree_skb(skb);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If frame the sequence number exceeds our buffering window\n\t * size release some previous frames to make room for this one.\n\t */\n\tif (!ieee80211_sn_less(mpdu_seq_num, head_seq_num + buf_size)) {\n\t\thead_seq_num = ieee80211_sn_inc(\n\t\t\t\tieee80211_sn_sub(mpdu_seq_num, buf_size));\n\t\t/* release stored frames up to new head to stack */\n\t\tieee80211_release_reorder_frames(sdata, tid_agg_rx,\n\t\t\t\t\t\t head_seq_num, frames);\n\t}\n\n\t/* Now the new frame is always in the range of the reordering buffer */\n\n\tindex = mpdu_seq_num % tid_agg_rx->buf_size;\n\n\t/* check if we already stored this frame */\n\tif (ieee80211_rx_reorder_ready(tid_agg_rx, index)) {\n\t\tdev_kfree_skb(skb);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If the current MPDU is in the right order and nothing else\n\t * is stored we can process it directly, no need to buffer it.\n\t * If it is first but there's something stored, we may be able\n\t * to release frames after this one.\n\t */\n\tif (mpdu_seq_num == tid_agg_rx->head_seq_num &&\n\t    tid_agg_rx->stored_mpdu_num == 0) {\n\t\tif (!(status->flag & RX_FLAG_AMSDU_MORE))\n\t\t\ttid_agg_rx->head_seq_num =\n\t\t\t\tieee80211_sn_inc(tid_agg_rx->head_seq_num);\n\t\tret = false;\n\t\tgoto out;\n\t}\n\n\t/* put the frame in the reordering buffer */\n\t__skb_queue_tail(&tid_agg_rx->reorder_buf[index], skb);\n\tif (!(status->flag & RX_FLAG_AMSDU_MORE)) {\n\t\ttid_agg_rx->reorder_time[index] = jiffies;\n\t\ttid_agg_rx->stored_mpdu_num++;\n\t\tieee80211_sta_reorder_release(sdata, tid_agg_rx, frames);\n\t}\n\n out:\n\tspin_unlock(&tid_agg_rx->reorder_lock);\n\treturn ret;\n}\n\n/*\n * Reorder MPDUs from A-MPDUs, keeping them on a buffer. Returns\n * true if the MPDU was buffered, false if it should be processed.\n */\nstatic void ieee80211_rx_reorder_ampdu(struct ieee80211_rx_data *rx,\n\t\t\t\t       struct sk_buff_head *frames)\n{\n\tstruct sk_buff *skb = rx->skb;\n\tstruct ieee80211_local *local = rx->local;\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *) skb->data;\n\tstruct sta_info *sta = rx->sta;\n\tstruct tid_ampdu_rx *tid_agg_rx;\n\tu16 sc;\n\tu8 tid, ack_policy;\n\n\tif (!ieee80211_is_data_qos(hdr->frame_control) ||\n\t    is_multicast_ether_addr(hdr->addr1))\n\t\tgoto dont_reorder;\n\n\t/*\n\t * filter the QoS data rx stream according to\n\t * STA/TID and check if this STA/TID is on aggregation\n\t */\n\n\tif (!sta)\n\t\tgoto dont_reorder;\n\n\tack_policy = *ieee80211_get_qos_ctl(hdr) &\n\t\t     IEEE80211_QOS_CTL_ACK_POLICY_MASK;\n\ttid = ieee80211_get_tid(hdr);\n\n\ttid_agg_rx = rcu_dereference(sta->ampdu_mlme.tid_rx[tid]);\n\tif (!tid_agg_rx) {\n\t\tif (ack_policy == IEEE80211_QOS_CTL_ACK_POLICY_BLOCKACK &&\n\t\t    !test_bit(tid, rx->sta->ampdu_mlme.agg_session_valid) &&\n\t\t    !test_and_set_bit(tid, rx->sta->ampdu_mlme.unexpected_agg))\n\t\t\tieee80211_send_delba(rx->sdata, rx->sta->sta.addr, tid,\n\t\t\t\t\t     WLAN_BACK_RECIPIENT,\n\t\t\t\t\t     WLAN_REASON_QSTA_REQUIRE_SETUP);\n\t\tgoto dont_reorder;\n\t}\n\n\t/* qos null data frames are excluded */\n\tif (unlikely(hdr->frame_control & cpu_to_le16(IEEE80211_STYPE_NULLFUNC)))\n\t\tgoto dont_reorder;\n\n\t/* not part of a BA session */\n\tif (ack_policy != IEEE80211_QOS_CTL_ACK_POLICY_BLOCKACK &&\n\t    ack_policy != IEEE80211_QOS_CTL_ACK_POLICY_NORMAL)\n\t\tgoto dont_reorder;\n\n\t/* new, potentially un-ordered, ampdu frame - process it */\n\n\t/* reset session timer */\n\tif (tid_agg_rx->timeout)\n\t\ttid_agg_rx->last_rx = jiffies;\n\n\t/* if this mpdu is fragmented - terminate rx aggregation session */\n\tsc = le16_to_cpu(hdr->seq_ctrl);\n\tif (sc & IEEE80211_SCTL_FRAG) {\n\t\tskb_queue_tail(&rx->sdata->skb_queue, skb);\n\t\tieee80211_queue_work(&local->hw, &rx->sdata->work);\n\t\treturn;\n\t}\n\n\t/*\n\t * No locking needed -- we will only ever process one\n\t * RX packet at a time, and thus own tid_agg_rx. All\n\t * other code manipulating it needs to (and does) make\n\t * sure that we cannot get to it any more before doing\n\t * anything with it.\n\t */\n\tif (ieee80211_sta_manage_reorder_buf(rx->sdata, tid_agg_rx, skb,\n\t\t\t\t\t     frames))\n\t\treturn;\n\n dont_reorder:\n\t__skb_queue_tail(frames, skb);\n}\n\nstatic ieee80211_rx_result debug_noinline\nieee80211_rx_h_check_dup(struct ieee80211_rx_data *rx)\n{\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)rx->skb->data;\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(rx->skb);\n\n\tif (status->flag & RX_FLAG_DUP_VALIDATED)\n\t\treturn RX_CONTINUE;\n\n\t/*\n\t * Drop duplicate 802.11 retransmissions\n\t * (IEEE 802.11-2012: 9.3.2.10 \"Duplicate detection and recovery\")\n\t */\n\n\tif (rx->skb->len < 24)\n\t\treturn RX_CONTINUE;\n\n\tif (ieee80211_is_ctl(hdr->frame_control) ||\n\t    ieee80211_is_any_nullfunc(hdr->frame_control) ||\n\t    is_multicast_ether_addr(hdr->addr1))\n\t\treturn RX_CONTINUE;\n\n\tif (!rx->sta)\n\t\treturn RX_CONTINUE;\n\n\tif (unlikely(ieee80211_has_retry(hdr->frame_control) &&\n\t\t     rx->sta->last_seq_ctrl[rx->seqno_idx] == hdr->seq_ctrl)) {\n\t\tI802_DEBUG_INC(rx->local->dot11FrameDuplicateCount);\n\t\trx->sta->rx_stats.num_duplicates++;\n\t\treturn RX_DROP_UNUSABLE;\n\t} else if (!(status->flag & RX_FLAG_AMSDU_MORE)) {\n\t\trx->sta->last_seq_ctrl[rx->seqno_idx] = hdr->seq_ctrl;\n\t}\n\n\treturn RX_CONTINUE;\n}\n\nstatic ieee80211_rx_result debug_noinline\nieee80211_rx_h_check(struct ieee80211_rx_data *rx)\n{\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)rx->skb->data;\n\n\t/* Drop disallowed frame classes based on STA auth/assoc state;\n\t * IEEE 802.11, Chap 5.5.\n\t *\n\t * mac80211 filters only based on association state, i.e. it drops\n\t * Class 3 frames from not associated stations. hostapd sends\n\t * deauth/disassoc frames when needed. In addition, hostapd is\n\t * responsible for filtering on both auth and assoc states.\n\t */\n\n\tif (ieee80211_vif_is_mesh(&rx->sdata->vif))\n\t\treturn ieee80211_rx_mesh_check(rx);\n\n\tif (unlikely((ieee80211_is_data(hdr->frame_control) ||\n\t\t      ieee80211_is_pspoll(hdr->frame_control)) &&\n\t\t     rx->sdata->vif.type != NL80211_IFTYPE_ADHOC &&\n\t\t     rx->sdata->vif.type != NL80211_IFTYPE_OCB &&\n\t\t     (!rx->sta || !test_sta_flag(rx->sta, WLAN_STA_ASSOC)))) {\n\t\t/*\n\t\t * accept port control frames from the AP even when it's not\n\t\t * yet marked ASSOC to prevent a race where we don't set the\n\t\t * assoc bit quickly enough before it sends the first frame\n\t\t */\n\t\tif (rx->sta && rx->sdata->vif.type == NL80211_IFTYPE_STATION &&\n\t\t    ieee80211_is_data_present(hdr->frame_control)) {\n\t\t\tunsigned int hdrlen;\n\t\t\t__be16 ethertype;\n\n\t\t\thdrlen = ieee80211_hdrlen(hdr->frame_control);\n\n\t\t\tif (rx->skb->len < hdrlen + 8)\n\t\t\t\treturn RX_DROP_MONITOR;\n\n\t\t\tskb_copy_bits(rx->skb, hdrlen + 6, &ethertype, 2);\n\t\t\tif (ethertype == rx->sdata->control_port_protocol)\n\t\t\t\treturn RX_CONTINUE;\n\t\t}\n\n\t\tif (rx->sdata->vif.type == NL80211_IFTYPE_AP &&\n\t\t    cfg80211_rx_spurious_frame(rx->sdata->dev,\n\t\t\t\t\t       hdr->addr2,\n\t\t\t\t\t       GFP_ATOMIC))\n\t\t\treturn RX_DROP_UNUSABLE;\n\n\t\treturn RX_DROP_MONITOR;\n\t}\n\n\treturn RX_CONTINUE;\n}\n\n\nstatic ieee80211_rx_result debug_noinline\nieee80211_rx_h_check_more_data(struct ieee80211_rx_data *rx)\n{\n\tstruct ieee80211_local *local;\n\tstruct ieee80211_hdr *hdr;\n\tstruct sk_buff *skb;\n\n\tlocal = rx->local;\n\tskb = rx->skb;\n\thdr = (struct ieee80211_hdr *) skb->data;\n\n\tif (!local->pspolling)\n\t\treturn RX_CONTINUE;\n\n\tif (!ieee80211_has_fromds(hdr->frame_control))\n\t\t/* this is not from AP */\n\t\treturn RX_CONTINUE;\n\n\tif (!ieee80211_is_data(hdr->frame_control))\n\t\treturn RX_CONTINUE;\n\n\tif (!ieee80211_has_moredata(hdr->frame_control)) {\n\t\t/* AP has no more frames buffered for us */\n\t\tlocal->pspolling = false;\n\t\treturn RX_CONTINUE;\n\t}\n\n\t/* more data bit is set, let's request a new frame from the AP */\n\tieee80211_send_pspoll(local, rx->sdata);\n\n\treturn RX_CONTINUE;\n}\n\nstatic void sta_ps_start(struct sta_info *sta)\n{\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct ps_data *ps;\n\tint tid;\n\n\tif (sta->sdata->vif.type == NL80211_IFTYPE_AP ||\n\t    sta->sdata->vif.type == NL80211_IFTYPE_AP_VLAN)\n\t\tps = &sdata->bss->ps;\n\telse\n\t\treturn;\n\n\tatomic_inc(&ps->num_sta_ps);\n\tset_sta_flag(sta, WLAN_STA_PS_STA);\n\tif (!ieee80211_hw_check(&local->hw, AP_LINK_PS))\n\t\tdrv_sta_notify(local, sdata, STA_NOTIFY_SLEEP, &sta->sta);\n\tps_dbg(sdata, \"STA %pM aid %d enters power save mode\\n\",\n\t       sta->sta.addr, sta->sta.aid);\n\n\tieee80211_clear_fast_xmit(sta);\n\n\tif (!sta->sta.txq[0])\n\t\treturn;\n\n\tfor (tid = 0; tid < IEEE80211_NUM_TIDS; tid++) {\n\t\tstruct ieee80211_txq *txq = sta->sta.txq[tid];\n\t\tstruct txq_info *txqi = to_txq_info(txq);\n\n\t\tspin_lock(&local->active_txq_lock[txq->ac]);\n\t\tif (!list_empty(&txqi->schedule_order))\n\t\t\tlist_del_init(&txqi->schedule_order);\n\t\tspin_unlock(&local->active_txq_lock[txq->ac]);\n\n\t\tif (txq_has_queue(txq))\n\t\t\tset_bit(tid, &sta->txq_buffered_tids);\n\t\telse\n\t\t\tclear_bit(tid, &sta->txq_buffered_tids);\n\t}\n}\n\nstatic void sta_ps_end(struct sta_info *sta)\n{\n\tps_dbg(sta->sdata, \"STA %pM aid %d exits power save mode\\n\",\n\t       sta->sta.addr, sta->sta.aid);\n\n\tif (test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {\n\t\t/*\n\t\t * Clear the flag only if the other one is still set\n\t\t * so that the TX path won't start TX'ing new frames\n\t\t * directly ... In the case that the driver flag isn't\n\t\t * set ieee80211_sta_ps_deliver_wakeup() will clear it.\n\t\t */\n\t\tclear_sta_flag(sta, WLAN_STA_PS_STA);\n\t\tps_dbg(sta->sdata, \"STA %pM aid %d driver-ps-blocked\\n\",\n\t\t       sta->sta.addr, sta->sta.aid);\n\t\treturn;\n\t}\n\n\tset_sta_flag(sta, WLAN_STA_PS_DELIVER);\n\tclear_sta_flag(sta, WLAN_STA_PS_STA);\n\tieee80211_sta_ps_deliver_wakeup(sta);\n}\n\nint ieee80211_sta_ps_transition(struct ieee80211_sta *pubsta, bool start)\n{\n\tstruct sta_info *sta = container_of(pubsta, struct sta_info, sta);\n\tbool in_ps;\n\n\tWARN_ON(!ieee80211_hw_check(&sta->local->hw, AP_LINK_PS));\n\n\t/* Don't let the same PS state be set twice */\n\tin_ps = test_sta_flag(sta, WLAN_STA_PS_STA);\n\tif ((start && in_ps) || (!start && !in_ps))\n\t\treturn -EINVAL;\n\n\tif (start)\n\t\tsta_ps_start(sta);\n\telse\n\t\tsta_ps_end(sta);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(ieee80211_sta_ps_transition);\n\nvoid ieee80211_sta_pspoll(struct ieee80211_sta *pubsta)\n{\n\tstruct sta_info *sta = container_of(pubsta, struct sta_info, sta);\n\n\tif (test_sta_flag(sta, WLAN_STA_SP))\n\t\treturn;\n\n\tif (!test_sta_flag(sta, WLAN_STA_PS_DRIVER))\n\t\tieee80211_sta_ps_deliver_poll_response(sta);\n\telse\n\t\tset_sta_flag(sta, WLAN_STA_PSPOLL);\n}\nEXPORT_SYMBOL(ieee80211_sta_pspoll);\n\nvoid ieee80211_sta_uapsd_trigger(struct ieee80211_sta *pubsta, u8 tid)\n{\n\tstruct sta_info *sta = container_of(pubsta, struct sta_info, sta);\n\tint ac = ieee80211_ac_from_tid(tid);\n\n\t/*\n\t * If this AC is not trigger-enabled do nothing unless the\n\t * driver is calling us after it already checked.\n\t *\n\t * NB: This could/should check a separate bitmap of trigger-\n\t * enabled queues, but for now we only implement uAPSD w/o\n\t * TSPEC changes to the ACs, so they're always the same.\n\t */\n\tif (!(sta->sta.uapsd_queues & ieee80211_ac_to_qos_mask[ac]) &&\n\t    tid != IEEE80211_NUM_TIDS)\n\t\treturn;\n\n\t/* if we are in a service period, do nothing */\n\tif (test_sta_flag(sta, WLAN_STA_SP))\n\t\treturn;\n\n\tif (!test_sta_flag(sta, WLAN_STA_PS_DRIVER))\n\t\tieee80211_sta_ps_deliver_uapsd(sta);\n\telse\n\t\tset_sta_flag(sta, WLAN_STA_UAPSD);\n}\nEXPORT_SYMBOL(ieee80211_sta_uapsd_trigger);\n\nstatic ieee80211_rx_result debug_noinline\nieee80211_rx_h_uapsd_and_pspoll(struct ieee80211_rx_data *rx)\n{\n\tstruct ieee80211_sub_if_data *sdata = rx->sdata;\n\tstruct ieee80211_hdr *hdr = (void *)rx->skb->data;\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(rx->skb);\n\n\tif (!rx->sta)\n\t\treturn RX_CONTINUE;\n\n\tif (sdata->vif.type != NL80211_IFTYPE_AP &&\n\t    sdata->vif.type != NL80211_IFTYPE_AP_VLAN)\n\t\treturn RX_CONTINUE;\n\n\t/*\n\t * The device handles station powersave, so don't do anything about\n\t * uAPSD and PS-Poll frames (the latter shouldn't even come up from\n\t * it to mac80211 since they're handled.)\n\t */\n\tif (ieee80211_hw_check(&sdata->local->hw, AP_LINK_PS))\n\t\treturn RX_CONTINUE;\n\n\t/*\n\t * Don't do anything if the station isn't already asleep. In\n\t * the uAPSD case, the station will probably be marked asleep,\n\t * in the PS-Poll case the station must be confused ...\n\t */\n\tif (!test_sta_flag(rx->sta, WLAN_STA_PS_STA))\n\t\treturn RX_CONTINUE;\n\n\tif (unlikely(ieee80211_is_pspoll(hdr->frame_control))) {\n\t\tieee80211_sta_pspoll(&rx->sta->sta);\n\n\t\t/* Free PS Poll skb here instead of returning RX_DROP that would\n\t\t * count as an dropped frame. */\n\t\tdev_kfree_skb(rx->skb);\n\n\t\treturn RX_QUEUED;\n\t} else if (!ieee80211_has_morefrags(hdr->frame_control) &&\n\t\t   !(status->rx_flags & IEEE80211_RX_DEFERRED_RELEASE) &&\n\t\t   ieee80211_has_pm(hdr->frame_control) &&\n\t\t   (ieee80211_is_data_qos(hdr->frame_control) ||\n\t\t    ieee80211_is_qos_nullfunc(hdr->frame_control))) {\n\t\tu8 tid = ieee80211_get_tid(hdr);\n\n\t\tieee80211_sta_uapsd_trigger(&rx->sta->sta, tid);\n\t}\n\n\treturn RX_CONTINUE;\n}\n\nstatic ieee80211_rx_result debug_noinline\nieee80211_rx_h_sta_process(struct ieee80211_rx_data *rx)\n{\n\tstruct sta_info *sta = rx->sta;\n\tstruct sk_buff *skb = rx->skb;\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(skb);\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)skb->data;\n\tint i;\n\n\tif (!sta)\n\t\treturn RX_CONTINUE;\n\n\t/*\n\t * Update last_rx only for IBSS packets which are for the current\n\t * BSSID and for station already AUTHORIZED to avoid keeping the\n\t * current IBSS network alive in cases where other STAs start\n\t * using different BSSID. This will also give the station another\n\t * chance to restart the authentication/authorization in case\n\t * something went wrong the first time.\n\t */\n\tif (rx->sdata->vif.type == NL80211_IFTYPE_ADHOC) {\n\t\tu8 *bssid = ieee80211_get_bssid(hdr, rx->skb->len,\n\t\t\t\t\t\tNL80211_IFTYPE_ADHOC);\n\t\tif (ether_addr_equal(bssid, rx->sdata->u.ibss.bssid) &&\n\t\t    test_sta_flag(sta, WLAN_STA_AUTHORIZED)) {\n\t\t\tsta->rx_stats.last_rx = jiffies;\n\t\t\tif (ieee80211_is_data(hdr->frame_control) &&\n\t\t\t    !is_multicast_ether_addr(hdr->addr1))\n\t\t\t\tsta->rx_stats.last_rate =\n\t\t\t\t\tsta_stats_encode_rate(status);\n\t\t}\n\t} else if (rx->sdata->vif.type == NL80211_IFTYPE_OCB) {\n\t\tsta->rx_stats.last_rx = jiffies;\n\t} else if (!ieee80211_is_s1g_beacon(hdr->frame_control) &&\n\t\t   !is_multicast_ether_addr(hdr->addr1)) {\n\t\t/*\n\t\t * Mesh beacons will update last_rx when if they are found to\n\t\t * match the current local configuration when processed.\n\t\t */\n\t\tsta->rx_stats.last_rx = jiffies;\n\t\tif (ieee80211_is_data(hdr->frame_control))\n\t\t\tsta->rx_stats.last_rate = sta_stats_encode_rate(status);\n\t}\n\n\tsta->rx_stats.fragments++;\n\n\tu64_stats_update_begin(&rx->sta->rx_stats.syncp);\n\tsta->rx_stats.bytes += rx->skb->len;\n\tu64_stats_update_end(&rx->sta->rx_stats.syncp);\n\n\tif (!(status->flag & RX_FLAG_NO_SIGNAL_VAL)) {\n\t\tsta->rx_stats.last_signal = status->signal;\n\t\tewma_signal_add(&sta->rx_stats_avg.signal, -status->signal);\n\t}\n\n\tif (status->chains) {\n\t\tsta->rx_stats.chains = status->chains;\n\t\tfor (i = 0; i < ARRAY_SIZE(status->chain_signal); i++) {\n\t\t\tint signal = status->chain_signal[i];\n\n\t\t\tif (!(status->chains & BIT(i)))\n\t\t\t\tcontinue;\n\n\t\t\tsta->rx_stats.chain_signal_last[i] = signal;\n\t\t\tewma_signal_add(&sta->rx_stats_avg.chain_signal[i],\n\t\t\t\t\t-signal);\n\t\t}\n\t}\n\n\tif (ieee80211_is_s1g_beacon(hdr->frame_control))\n\t\treturn RX_CONTINUE;\n\n\t/*\n\t * Change STA power saving mode only at the end of a frame\n\t * exchange sequence, and only for a data or management\n\t * frame as specified in IEEE 802.11-2016 11.2.3.2\n\t */\n\tif (!ieee80211_hw_check(&sta->local->hw, AP_LINK_PS) &&\n\t    !ieee80211_has_morefrags(hdr->frame_control) &&\n\t    !is_multicast_ether_addr(hdr->addr1) &&\n\t    (ieee80211_is_mgmt(hdr->frame_control) ||\n\t     ieee80211_is_data(hdr->frame_control)) &&\n\t    !(status->rx_flags & IEEE80211_RX_DEFERRED_RELEASE) &&\n\t    (rx->sdata->vif.type == NL80211_IFTYPE_AP ||\n\t     rx->sdata->vif.type == NL80211_IFTYPE_AP_VLAN)) {\n\t\tif (test_sta_flag(sta, WLAN_STA_PS_STA)) {\n\t\t\tif (!ieee80211_has_pm(hdr->frame_control))\n\t\t\t\tsta_ps_end(sta);\n\t\t} else {\n\t\t\tif (ieee80211_has_pm(hdr->frame_control))\n\t\t\t\tsta_ps_start(sta);\n\t\t}\n\t}\n\n\t/* mesh power save support */\n\tif (ieee80211_vif_is_mesh(&rx->sdata->vif))\n\t\tieee80211_mps_rx_h_sta_process(sta, hdr);\n\n\t/*\n\t * Drop (qos-)data::nullfunc frames silently, since they\n\t * are used only to control station power saving mode.\n\t */\n\tif (ieee80211_is_any_nullfunc(hdr->frame_control)) {\n\t\tI802_DEBUG_INC(rx->local->rx_handlers_drop_nullfunc);\n\n\t\t/*\n\t\t * If we receive a 4-addr nullfunc frame from a STA\n\t\t * that was not moved to a 4-addr STA vlan yet send\n\t\t * the event to userspace and for older hostapd drop\n\t\t * the frame to the monitor interface.\n\t\t */\n\t\tif (ieee80211_has_a4(hdr->frame_control) &&\n\t\t    (rx->sdata->vif.type == NL80211_IFTYPE_AP ||\n\t\t     (rx->sdata->vif.type == NL80211_IFTYPE_AP_VLAN &&\n\t\t      !rx->sdata->u.vlan.sta))) {\n\t\t\tif (!test_and_set_sta_flag(sta, WLAN_STA_4ADDR_EVENT))\n\t\t\t\tcfg80211_rx_unexpected_4addr_frame(\n\t\t\t\t\trx->sdata->dev, sta->sta.addr,\n\t\t\t\t\tGFP_ATOMIC);\n\t\t\treturn RX_DROP_MONITOR;\n\t\t}\n\t\t/*\n\t\t * Update counter and free packet here to avoid\n\t\t * counting this as a dropped packed.\n\t\t */\n\t\tsta->rx_stats.packets++;\n\t\tdev_kfree_skb(rx->skb);\n\t\treturn RX_QUEUED;\n\t}\n\n\treturn RX_CONTINUE;\n} /* ieee80211_rx_h_sta_process */\n\nstatic struct ieee80211_key *\nieee80211_rx_get_bigtk(struct ieee80211_rx_data *rx, int idx)\n{\n\tstruct ieee80211_key *key = NULL;\n\tstruct ieee80211_sub_if_data *sdata = rx->sdata;\n\tint idx2;\n\n\t/* Make sure key gets set if either BIGTK key index is set so that\n\t * ieee80211_drop_unencrypted_mgmt() can properly drop both unprotected\n\t * Beacon frames and Beacon frames that claim to use another BIGTK key\n\t * index (i.e., a key that we do not have).\n\t */\n\n\tif (idx < 0) {\n\t\tidx = NUM_DEFAULT_KEYS + NUM_DEFAULT_MGMT_KEYS;\n\t\tidx2 = idx + 1;\n\t} else {\n\t\tif (idx == NUM_DEFAULT_KEYS + NUM_DEFAULT_MGMT_KEYS)\n\t\t\tidx2 = idx + 1;\n\t\telse\n\t\t\tidx2 = idx - 1;\n\t}\n\n\tif (rx->sta)\n\t\tkey = rcu_dereference(rx->sta->gtk[idx]);\n\tif (!key)\n\t\tkey = rcu_dereference(sdata->keys[idx]);\n\tif (!key && rx->sta)\n\t\tkey = rcu_dereference(rx->sta->gtk[idx2]);\n\tif (!key)\n\t\tkey = rcu_dereference(sdata->keys[idx2]);\n\n\treturn key;\n}\n\nstatic ieee80211_rx_result debug_noinline\nieee80211_rx_h_decrypt(struct ieee80211_rx_data *rx)\n{\n\tstruct sk_buff *skb = rx->skb;\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(skb);\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)skb->data;\n\tint keyidx;\n\tieee80211_rx_result result = RX_DROP_UNUSABLE;\n\tstruct ieee80211_key *sta_ptk = NULL;\n\tstruct ieee80211_key *ptk_idx = NULL;\n\tint mmie_keyidx = -1;\n\t__le16 fc;\n\tconst struct ieee80211_cipher_scheme *cs = NULL;\n\n\tif (ieee80211_is_ext(hdr->frame_control))\n\t\treturn RX_CONTINUE;\n\n\t/*\n\t * Key selection 101\n\t *\n\t * There are five types of keys:\n\t *  - GTK (group keys)\n\t *  - IGTK (group keys for management frames)\n\t *  - BIGTK (group keys for Beacon frames)\n\t *  - PTK (pairwise keys)\n\t *  - STK (station-to-station pairwise keys)\n\t *\n\t * When selecting a key, we have to distinguish between multicast\n\t * (including broadcast) and unicast frames, the latter can only\n\t * use PTKs and STKs while the former always use GTKs, IGTKs, and\n\t * BIGTKs. Unless, of course, actual WEP keys (\"pre-RSNA\") are used,\n\t * then unicast frames can also use key indices like GTKs. Hence, if we\n\t * don't have a PTK/STK we check the key index for a WEP key.\n\t *\n\t * Note that in a regular BSS, multicast frames are sent by the\n\t * AP only, associated stations unicast the frame to the AP first\n\t * which then multicasts it on their behalf.\n\t *\n\t * There is also a slight problem in IBSS mode: GTKs are negotiated\n\t * with each station, that is something we don't currently handle.\n\t * The spec seems to expect that one negotiates the same key with\n\t * every station but there's no such requirement; VLANs could be\n\t * possible.\n\t */\n\n\t/* start without a key */\n\trx->key = NULL;\n\tfc = hdr->frame_control;\n\n\tif (rx->sta) {\n\t\tint keyid = rx->sta->ptk_idx;\n\t\tsta_ptk = rcu_dereference(rx->sta->ptk[keyid]);\n\n\t\tif (ieee80211_has_protected(fc)) {\n\t\t\tcs = rx->sta->cipher_scheme;\n\t\t\tkeyid = ieee80211_get_keyid(rx->skb, cs);\n\n\t\t\tif (unlikely(keyid < 0))\n\t\t\t\treturn RX_DROP_UNUSABLE;\n\n\t\t\tptk_idx = rcu_dereference(rx->sta->ptk[keyid]);\n\t\t}\n\t}\n\n\tif (!ieee80211_has_protected(fc))\n\t\tmmie_keyidx = ieee80211_get_mmie_keyidx(rx->skb);\n\n\tif (!is_multicast_ether_addr(hdr->addr1) && sta_ptk) {\n\t\trx->key = ptk_idx ? ptk_idx : sta_ptk;\n\t\tif ((status->flag & RX_FLAG_DECRYPTED) &&\n\t\t    (status->flag & RX_FLAG_IV_STRIPPED))\n\t\t\treturn RX_CONTINUE;\n\t\t/* Skip decryption if the frame is not protected. */\n\t\tif (!ieee80211_has_protected(fc))\n\t\t\treturn RX_CONTINUE;\n\t} else if (mmie_keyidx >= 0 && ieee80211_is_beacon(fc)) {\n\t\t/* Broadcast/multicast robust management frame / BIP */\n\t\tif ((status->flag & RX_FLAG_DECRYPTED) &&\n\t\t    (status->flag & RX_FLAG_IV_STRIPPED))\n\t\t\treturn RX_CONTINUE;\n\n\t\tif (mmie_keyidx < NUM_DEFAULT_KEYS + NUM_DEFAULT_MGMT_KEYS ||\n\t\t    mmie_keyidx >= NUM_DEFAULT_KEYS + NUM_DEFAULT_MGMT_KEYS +\n\t\t    NUM_DEFAULT_BEACON_KEYS) {\n\t\t\tcfg80211_rx_unprot_mlme_mgmt(rx->sdata->dev,\n\t\t\t\t\t\t     skb->data,\n\t\t\t\t\t\t     skb->len);\n\t\t\treturn RX_DROP_MONITOR; /* unexpected BIP keyidx */\n\t\t}\n\n\t\trx->key = ieee80211_rx_get_bigtk(rx, mmie_keyidx);\n\t\tif (!rx->key)\n\t\t\treturn RX_CONTINUE; /* Beacon protection not in use */\n\t} else if (mmie_keyidx >= 0) {\n\t\t/* Broadcast/multicast robust management frame / BIP */\n\t\tif ((status->flag & RX_FLAG_DECRYPTED) &&\n\t\t    (status->flag & RX_FLAG_IV_STRIPPED))\n\t\t\treturn RX_CONTINUE;\n\n\t\tif (mmie_keyidx < NUM_DEFAULT_KEYS ||\n\t\t    mmie_keyidx >= NUM_DEFAULT_KEYS + NUM_DEFAULT_MGMT_KEYS)\n\t\t\treturn RX_DROP_MONITOR; /* unexpected BIP keyidx */\n\t\tif (rx->sta) {\n\t\t\tif (ieee80211_is_group_privacy_action(skb) &&\n\t\t\t    test_sta_flag(rx->sta, WLAN_STA_MFP))\n\t\t\t\treturn RX_DROP_MONITOR;\n\n\t\t\trx->key = rcu_dereference(rx->sta->gtk[mmie_keyidx]);\n\t\t}\n\t\tif (!rx->key)\n\t\t\trx->key = rcu_dereference(rx->sdata->keys[mmie_keyidx]);\n\t} else if (!ieee80211_has_protected(fc)) {\n\t\t/*\n\t\t * The frame was not protected, so skip decryption. However, we\n\t\t * need to set rx->key if there is a key that could have been\n\t\t * used so that the frame may be dropped if encryption would\n\t\t * have been expected.\n\t\t */\n\t\tstruct ieee80211_key *key = NULL;\n\t\tstruct ieee80211_sub_if_data *sdata = rx->sdata;\n\t\tint i;\n\n\t\tif (ieee80211_is_beacon(fc)) {\n\t\t\tkey = ieee80211_rx_get_bigtk(rx, -1);\n\t\t} else if (ieee80211_is_mgmt(fc) &&\n\t\t\t   is_multicast_ether_addr(hdr->addr1)) {\n\t\t\tkey = rcu_dereference(rx->sdata->default_mgmt_key);\n\t\t} else {\n\t\t\tif (rx->sta) {\n\t\t\t\tfor (i = 0; i < NUM_DEFAULT_KEYS; i++) {\n\t\t\t\t\tkey = rcu_dereference(rx->sta->gtk[i]);\n\t\t\t\t\tif (key)\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (!key) {\n\t\t\t\tfor (i = 0; i < NUM_DEFAULT_KEYS; i++) {\n\t\t\t\t\tkey = rcu_dereference(sdata->keys[i]);\n\t\t\t\t\tif (key)\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif (key)\n\t\t\trx->key = key;\n\t\treturn RX_CONTINUE;\n\t} else {\n\t\t/*\n\t\t * The device doesn't give us the IV so we won't be\n\t\t * able to look up the key. That's ok though, we\n\t\t * don't need to decrypt the frame, we just won't\n\t\t * be able to keep statistics accurate.\n\t\t * Except for key threshold notifications, should\n\t\t * we somehow allow the driver to tell us which key\n\t\t * the hardware used if this flag is set?\n\t\t */\n\t\tif ((status->flag & RX_FLAG_DECRYPTED) &&\n\t\t    (status->flag & RX_FLAG_IV_STRIPPED))\n\t\t\treturn RX_CONTINUE;\n\n\t\tkeyidx = ieee80211_get_keyid(rx->skb, cs);\n\n\t\tif (unlikely(keyidx < 0))\n\t\t\treturn RX_DROP_UNUSABLE;\n\n\t\t/* check per-station GTK first, if multicast packet */\n\t\tif (is_multicast_ether_addr(hdr->addr1) && rx->sta)\n\t\t\trx->key = rcu_dereference(rx->sta->gtk[keyidx]);\n\n\t\t/* if not found, try default key */\n\t\tif (!rx->key) {\n\t\t\trx->key = rcu_dereference(rx->sdata->keys[keyidx]);\n\n\t\t\t/*\n\t\t\t * RSNA-protected unicast frames should always be\n\t\t\t * sent with pairwise or station-to-station keys,\n\t\t\t * but for WEP we allow using a key index as well.\n\t\t\t */\n\t\t\tif (rx->key &&\n\t\t\t    rx->key->conf.cipher != WLAN_CIPHER_SUITE_WEP40 &&\n\t\t\t    rx->key->conf.cipher != WLAN_CIPHER_SUITE_WEP104 &&\n\t\t\t    !is_multicast_ether_addr(hdr->addr1))\n\t\t\t\trx->key = NULL;\n\t\t}\n\t}\n\n\tif (rx->key) {\n\t\tif (unlikely(rx->key->flags & KEY_FLAG_TAINTED))\n\t\t\treturn RX_DROP_MONITOR;\n\n\t\t/* TODO: add threshold stuff again */\n\t} else {\n\t\treturn RX_DROP_MONITOR;\n\t}\n\n\tswitch (rx->key->conf.cipher) {\n\tcase WLAN_CIPHER_SUITE_WEP40:\n\tcase WLAN_CIPHER_SUITE_WEP104:\n\t\tresult = ieee80211_crypto_wep_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_TKIP:\n\t\tresult = ieee80211_crypto_tkip_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_CCMP:\n\t\tresult = ieee80211_crypto_ccmp_decrypt(\n\t\t\trx, IEEE80211_CCMP_MIC_LEN);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_CCMP_256:\n\t\tresult = ieee80211_crypto_ccmp_decrypt(\n\t\t\trx, IEEE80211_CCMP_256_MIC_LEN);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_AES_CMAC:\n\t\tresult = ieee80211_crypto_aes_cmac_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_BIP_CMAC_256:\n\t\tresult = ieee80211_crypto_aes_cmac_256_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_BIP_GMAC_128:\n\tcase WLAN_CIPHER_SUITE_BIP_GMAC_256:\n\t\tresult = ieee80211_crypto_aes_gmac_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_GCMP:\n\tcase WLAN_CIPHER_SUITE_GCMP_256:\n\t\tresult = ieee80211_crypto_gcmp_decrypt(rx);\n\t\tbreak;\n\tdefault:\n\t\tresult = ieee80211_crypto_hw_decrypt(rx);\n\t}\n\n\t/* the hdr variable is invalid after the decrypt handlers */\n\n\t/* either the frame has been decrypted or will be dropped */\n\tstatus->flag |= RX_FLAG_DECRYPTED;\n\n\tif (unlikely(ieee80211_is_beacon(fc) && result == RX_DROP_UNUSABLE))\n\t\tcfg80211_rx_unprot_mlme_mgmt(rx->sdata->dev,\n\t\t\t\t\t     skb->data, skb->len);\n\n\treturn result;\n}\n\nstatic inline struct ieee80211_fragment_entry *\nieee80211_reassemble_add(struct ieee80211_sub_if_data *sdata,\n\t\t\t unsigned int frag, unsigned int seq, int rx_queue,\n\t\t\t struct sk_buff **skb)\n{\n\tstruct ieee80211_fragment_entry *entry;\n\n\tentry = &sdata->fragments[sdata->fragment_next++];\n\tif (sdata->fragment_next >= IEEE80211_FRAGMENT_MAX)\n\t\tsdata->fragment_next = 0;\n\n\tif (!skb_queue_empty(&entry->skb_list))\n\t\t__skb_queue_purge(&entry->skb_list);\n\n\t__skb_queue_tail(&entry->skb_list, *skb); /* no need for locking */\n\t*skb = NULL;\n\tentry->first_frag_time = jiffies;\n\tentry->seq = seq;\n\tentry->rx_queue = rx_queue;\n\tentry->last_frag = frag;\n\tentry->check_sequential_pn = false;\n\tentry->extra_len = 0;\n\n\treturn entry;\n}\n\nstatic inline struct ieee80211_fragment_entry *\nieee80211_reassemble_find(struct ieee80211_sub_if_data *sdata,\n\t\t\t  unsigned int frag, unsigned int seq,\n\t\t\t  int rx_queue, struct ieee80211_hdr *hdr)\n{\n\tstruct ieee80211_fragment_entry *entry;\n\tint i, idx;\n\n\tidx = sdata->fragment_next;\n\tfor (i = 0; i < IEEE80211_FRAGMENT_MAX; i++) {\n\t\tstruct ieee80211_hdr *f_hdr;\n\t\tstruct sk_buff *f_skb;\n\n\t\tidx--;\n\t\tif (idx < 0)\n\t\t\tidx = IEEE80211_FRAGMENT_MAX - 1;\n\n\t\tentry = &sdata->fragments[idx];\n\t\tif (skb_queue_empty(&entry->skb_list) || entry->seq != seq ||\n\t\t    entry->rx_queue != rx_queue ||\n\t\t    entry->last_frag + 1 != frag)\n\t\t\tcontinue;\n\n\t\tf_skb = __skb_peek(&entry->skb_list);\n\t\tf_hdr = (struct ieee80211_hdr *) f_skb->data;\n\n\t\t/*\n\t\t * Check ftype and addresses are equal, else check next fragment\n\t\t */\n\t\tif (((hdr->frame_control ^ f_hdr->frame_control) &\n\t\t     cpu_to_le16(IEEE80211_FCTL_FTYPE)) ||\n\t\t    !ether_addr_equal(hdr->addr1, f_hdr->addr1) ||\n\t\t    !ether_addr_equal(hdr->addr2, f_hdr->addr2))\n\t\t\tcontinue;\n\n\t\tif (time_after(jiffies, entry->first_frag_time + 2 * HZ)) {\n\t\t\t__skb_queue_purge(&entry->skb_list);\n\t\t\tcontinue;\n\t\t}\n\t\treturn entry;\n\t}\n\n\treturn NULL;\n}\n\nstatic ieee80211_rx_result debug_noinline\nieee80211_rx_h_defragment(struct ieee80211_rx_data *rx)\n{\n\tstruct ieee80211_hdr *hdr;\n\tu16 sc;\n\t__le16 fc;\n\tunsigned int frag, seq;\n\tstruct ieee80211_fragment_entry *entry;\n\tstruct sk_buff *skb;\n\n\thdr = (struct ieee80211_hdr *)rx->skb->data;\n\tfc = hdr->frame_control;\n\n\tif (ieee80211_is_ctl(fc) || ieee80211_is_ext(fc))\n\t\treturn RX_CONTINUE;\n\n\tsc = le16_to_cpu(hdr->seq_ctrl);\n\tfrag = sc & IEEE80211_SCTL_FRAG;\n\n\tif (is_multicast_ether_addr(hdr->addr1)) {\n\t\tI802_DEBUG_INC(rx->local->dot11MulticastReceivedFrameCount);\n\t\tgoto out_no_led;\n\t}\n\n\tif (likely(!ieee80211_has_morefrags(fc) && frag == 0))\n\t\tgoto out;\n\n\tI802_DEBUG_INC(rx->local->rx_handlers_fragments);\n\n\tif (skb_linearize(rx->skb))\n\t\treturn RX_DROP_UNUSABLE;\n\n\t/*\n\t *  skb_linearize() might change the skb->data and\n\t *  previously cached variables (in this case, hdr) need to\n\t *  be refreshed with the new data.\n\t */\n\thdr = (struct ieee80211_hdr *)rx->skb->data;\n\tseq = (sc & IEEE80211_SCTL_SEQ) >> 4;\n\n\tif (frag == 0) {\n\t\t/* This is the first fragment of a new frame. */\n\t\tentry = ieee80211_reassemble_add(rx->sdata, frag, seq,\n\t\t\t\t\t\t rx->seqno_idx, &(rx->skb));\n\t\tif (rx->key &&\n\t\t    (rx->key->conf.cipher == WLAN_CIPHER_SUITE_CCMP ||\n\t\t     rx->key->conf.cipher == WLAN_CIPHER_SUITE_CCMP_256 ||\n\t\t     rx->key->conf.cipher == WLAN_CIPHER_SUITE_GCMP ||\n\t\t     rx->key->conf.cipher == WLAN_CIPHER_SUITE_GCMP_256) &&\n\t\t    ieee80211_has_protected(fc)) {\n\t\t\tint queue = rx->security_idx;\n\n\t\t\t/* Store CCMP/GCMP PN so that we can verify that the\n\t\t\t * next fragment has a sequential PN value.\n\t\t\t */\n\t\t\tentry->check_sequential_pn = true;\n\t\t\tmemcpy(entry->last_pn,\n\t\t\t       rx->key->u.ccmp.rx_pn[queue],\n\t\t\t       IEEE80211_CCMP_PN_LEN);\n\t\t\tBUILD_BUG_ON(offsetof(struct ieee80211_key,\n\t\t\t\t\t      u.ccmp.rx_pn) !=\n\t\t\t\t     offsetof(struct ieee80211_key,\n\t\t\t\t\t      u.gcmp.rx_pn));\n\t\t\tBUILD_BUG_ON(sizeof(rx->key->u.ccmp.rx_pn[queue]) !=\n\t\t\t\t     sizeof(rx->key->u.gcmp.rx_pn[queue]));\n\t\t\tBUILD_BUG_ON(IEEE80211_CCMP_PN_LEN !=\n\t\t\t\t     IEEE80211_GCMP_PN_LEN);\n\t\t}\n\t\treturn RX_QUEUED;\n\t}\n\n\t/* This is a fragment for a frame that should already be pending in\n\t * fragment cache. Add this fragment to the end of the pending entry.\n\t */\n\tentry = ieee80211_reassemble_find(rx->sdata, frag, seq,\n\t\t\t\t\t  rx->seqno_idx, hdr);\n\tif (!entry) {\n\t\tI802_DEBUG_INC(rx->local->rx_handlers_drop_defrag);\n\t\treturn RX_DROP_MONITOR;\n\t}\n\n\t/* \"The receiver shall discard MSDUs and MMPDUs whose constituent\n\t *  MPDU PN values are not incrementing in steps of 1.\"\n\t * see IEEE P802.11-REVmc/D5.0, 12.5.3.4.4, item d (for CCMP)\n\t * and IEEE P802.11-REVmc/D5.0, 12.5.5.4.4, item d (for GCMP)\n\t */\n\tif (entry->check_sequential_pn) {\n\t\tint i;\n\t\tu8 pn[IEEE80211_CCMP_PN_LEN], *rpn;\n\t\tint queue;\n\n\t\tif (!rx->key ||\n\t\t    (rx->key->conf.cipher != WLAN_CIPHER_SUITE_CCMP &&\n\t\t     rx->key->conf.cipher != WLAN_CIPHER_SUITE_CCMP_256 &&\n\t\t     rx->key->conf.cipher != WLAN_CIPHER_SUITE_GCMP &&\n\t\t     rx->key->conf.cipher != WLAN_CIPHER_SUITE_GCMP_256))\n\t\t\treturn RX_DROP_UNUSABLE;\n\t\tmemcpy(pn, entry->last_pn, IEEE80211_CCMP_PN_LEN);\n\t\tfor (i = IEEE80211_CCMP_PN_LEN - 1; i >= 0; i--) {\n\t\t\tpn[i]++;\n\t\t\tif (pn[i])\n\t\t\t\tbreak;\n\t\t}\n\t\tqueue = rx->security_idx;\n\t\trpn = rx->key->u.ccmp.rx_pn[queue];\n\t\tif (memcmp(pn, rpn, IEEE80211_CCMP_PN_LEN))\n\t\t\treturn RX_DROP_UNUSABLE;\n\t\tmemcpy(entry->last_pn, pn, IEEE80211_CCMP_PN_LEN);\n\t}\n\n\tskb_pull(rx->skb, ieee80211_hdrlen(fc));\n\t__skb_queue_tail(&entry->skb_list, rx->skb);\n\tentry->last_frag = frag;\n\tentry->extra_len += rx->skb->len;\n\tif (ieee80211_has_morefrags(fc)) {\n\t\trx->skb = NULL;\n\t\treturn RX_QUEUED;\n\t}\n\n\trx->skb = __skb_dequeue(&entry->skb_list);\n\tif (skb_tailroom(rx->skb) < entry->extra_len) {\n\t\tI802_DEBUG_INC(rx->local->rx_expand_skb_head_defrag);\n\t\tif (unlikely(pskb_expand_head(rx->skb, 0, entry->extra_len,\n\t\t\t\t\t      GFP_ATOMIC))) {\n\t\t\tI802_DEBUG_INC(rx->local->rx_handlers_drop_defrag);\n\t\t\t__skb_queue_purge(&entry->skb_list);\n\t\t\treturn RX_DROP_UNUSABLE;\n\t\t}\n\t}\n\twhile ((skb = __skb_dequeue(&entry->skb_list))) {\n\t\tskb_put_data(rx->skb, skb->data, skb->len);\n\t\tdev_kfree_skb(skb);\n\t}\n\n out:\n\tieee80211_led_rx(rx->local);\n out_no_led:\n\tif (rx->sta)\n\t\trx->sta->rx_stats.packets++;\n\treturn RX_CONTINUE;\n}\n\nstatic int ieee80211_802_1x_port_control(struct ieee80211_rx_data *rx)\n{\n\tif (unlikely(!rx->sta || !test_sta_flag(rx->sta, WLAN_STA_AUTHORIZED)))\n\t\treturn -EACCES;\n\n\treturn 0;\n}\n\nstatic int ieee80211_drop_unencrypted(struct ieee80211_rx_data *rx, __le16 fc)\n{\n\tstruct ieee80211_hdr *hdr = (void *)rx->skb->data;\n\tstruct sk_buff *skb = rx->skb;\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(skb);\n\n\t/*\n\t * Pass through unencrypted frames if the hardware has\n\t * decrypted them already.\n\t */\n\tif (status->flag & RX_FLAG_DECRYPTED)\n\t\treturn 0;\n\n\t/* check mesh EAPOL frames first */\n\tif (unlikely(rx->sta && ieee80211_vif_is_mesh(&rx->sdata->vif) &&\n\t\t     ieee80211_is_data(fc))) {\n\t\tstruct ieee80211s_hdr *mesh_hdr;\n\t\tu16 hdr_len = ieee80211_hdrlen(fc);\n\t\tu16 ethertype_offset;\n\t\t__be16 ethertype;\n\n\t\tif (!ether_addr_equal(hdr->addr1, rx->sdata->vif.addr))\n\t\t\tgoto drop_check;\n\n\t\t/* make sure fixed part of mesh header is there, also checks skb len */\n\t\tif (!pskb_may_pull(rx->skb, hdr_len + 6))\n\t\t\tgoto drop_check;\n\n\t\tmesh_hdr = (struct ieee80211s_hdr *)(skb->data + hdr_len);\n\t\tethertype_offset = hdr_len + ieee80211_get_mesh_hdrlen(mesh_hdr) +\n\t\t\t\t   sizeof(rfc1042_header);\n\n\t\tif (skb_copy_bits(rx->skb, ethertype_offset, &ethertype, 2) == 0 &&\n\t\t    ethertype == rx->sdata->control_port_protocol)\n\t\t\treturn 0;\n\t}\n\ndrop_check:\n\t/* Drop unencrypted frames if key is set. */\n\tif (unlikely(!ieee80211_has_protected(fc) &&\n\t\t     !ieee80211_is_any_nullfunc(fc) &&\n\t\t     ieee80211_is_data(fc) && rx->key))\n\t\treturn -EACCES;\n\n\treturn 0;\n}\n\nstatic int ieee80211_drop_unencrypted_mgmt(struct ieee80211_rx_data *rx)\n{\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)rx->skb->data;\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(rx->skb);\n\t__le16 fc = hdr->frame_control;\n\n\t/*\n\t * Pass through unencrypted frames if the hardware has\n\t * decrypted them already.\n\t */\n\tif (status->flag & RX_FLAG_DECRYPTED)\n\t\treturn 0;\n\n\tif (rx->sta && test_sta_flag(rx->sta, WLAN_STA_MFP)) {\n\t\tif (unlikely(!ieee80211_has_protected(fc) &&\n\t\t\t     ieee80211_is_unicast_robust_mgmt_frame(rx->skb) &&\n\t\t\t     rx->key)) {\n\t\t\tif (ieee80211_is_deauth(fc) ||\n\t\t\t    ieee80211_is_disassoc(fc))\n\t\t\t\tcfg80211_rx_unprot_mlme_mgmt(rx->sdata->dev,\n\t\t\t\t\t\t\t     rx->skb->data,\n\t\t\t\t\t\t\t     rx->skb->len);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* BIP does not use Protected field, so need to check MMIE */\n\t\tif (unlikely(ieee80211_is_multicast_robust_mgmt_frame(rx->skb) &&\n\t\t\t     ieee80211_get_mmie_keyidx(rx->skb) < 0)) {\n\t\t\tif (ieee80211_is_deauth(fc) ||\n\t\t\t    ieee80211_is_disassoc(fc))\n\t\t\t\tcfg80211_rx_unprot_mlme_mgmt(rx->sdata->dev,\n\t\t\t\t\t\t\t     rx->skb->data,\n\t\t\t\t\t\t\t     rx->skb->len);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (unlikely(ieee80211_is_beacon(fc) && rx->key &&\n\t\t\t     ieee80211_get_mmie_keyidx(rx->skb) < 0)) {\n\t\t\tcfg80211_rx_unprot_mlme_mgmt(rx->sdata->dev,\n\t\t\t\t\t\t     rx->skb->data,\n\t\t\t\t\t\t     rx->skb->len);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/*\n\t\t * When using MFP, Action frames are not allowed prior to\n\t\t * having configured keys.\n\t\t */\n\t\tif (unlikely(ieee80211_is_action(fc) && !rx->key &&\n\t\t\t     ieee80211_is_robust_mgmt_frame(rx->skb)))\n\t\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int\n__ieee80211_data_to_8023(struct ieee80211_rx_data *rx, bool *port_control)\n{\n\tstruct ieee80211_sub_if_data *sdata = rx->sdata;\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)rx->skb->data;\n\tbool check_port_control = false;\n\tstruct ethhdr *ehdr;\n\tint ret;\n\n\t*port_control = false;\n\tif (ieee80211_has_a4(hdr->frame_control) &&\n\t    sdata->vif.type == NL80211_IFTYPE_AP_VLAN && !sdata->u.vlan.sta)\n\t\treturn -1;\n\n\tif (sdata->vif.type == NL80211_IFTYPE_STATION &&\n\t    !!sdata->u.mgd.use_4addr != !!ieee80211_has_a4(hdr->frame_control)) {\n\n\t\tif (!sdata->u.mgd.use_4addr)\n\t\t\treturn -1;\n\t\telse if (!ether_addr_equal(hdr->addr1, sdata->vif.addr))\n\t\t\tcheck_port_control = true;\n\t}\n\n\tif (is_multicast_ether_addr(hdr->addr1) &&\n\t    sdata->vif.type == NL80211_IFTYPE_AP_VLAN && sdata->u.vlan.sta)\n\t\treturn -1;\n\n\tret = ieee80211_data_to_8023(rx->skb, sdata->vif.addr, sdata->vif.type);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tehdr = (struct ethhdr *) rx->skb->data;\n\tif (ehdr->h_proto == rx->sdata->control_port_protocol)\n\t\t*port_control = true;\n\telse if (check_port_control)\n\t\treturn -1;\n\n\treturn 0;\n}\n\n/*\n * requires that rx->skb is a frame with ethernet header\n */\nstatic bool ieee80211_frame_allowed(struct ieee80211_rx_data *rx, __le16 fc)\n{\n\tstatic const u8 pae_group_addr[ETH_ALEN] __aligned(2)\n\t\t= { 0x01, 0x80, 0xC2, 0x00, 0x00, 0x03 };\n\tstruct ethhdr *ehdr = (struct ethhdr *) rx->skb->data;\n\n\t/*\n\t * Allow EAPOL frames to us/the PAE group address regardless\n\t * of whether the frame was encrypted or not.\n\t */\n\tif (ehdr->h_proto == rx->sdata->control_port_protocol &&\n\t    (ether_addr_equal(ehdr->h_dest, rx->sdata->vif.addr) ||\n\t     ether_addr_equal(ehdr->h_dest, pae_group_addr)))\n\t\treturn true;\n\n\tif (ieee80211_802_1x_port_control(rx) ||\n\t    ieee80211_drop_unencrypted(rx, fc))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void ieee80211_deliver_skb_to_local_stack(struct sk_buff *skb,\n\t\t\t\t\t\t struct ieee80211_rx_data *rx)\n{\n\tstruct ieee80211_sub_if_data *sdata = rx->sdata;\n\tstruct net_device *dev = sdata->dev;\n\n\tif (unlikely((skb->protocol == sdata->control_port_protocol ||\n\t\t     (skb->protocol == cpu_to_be16(ETH_P_PREAUTH) &&\n\t\t      !sdata->control_port_no_preauth)) &&\n\t\t     sdata->control_port_over_nl80211)) {\n\t\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(skb);\n\t\tbool noencrypt = !(status->flag & RX_FLAG_DECRYPTED);\n\n\t\tcfg80211_rx_control_port(dev, skb, noencrypt);\n\t\tdev_kfree_skb(skb);\n\t} else {\n\t\tmemset(skb->cb, 0, sizeof(skb->cb));\n\n\t\t/* deliver to local stack */\n\t\tif (rx->list)\n\t\t\tlist_add_tail(&skb->list, rx->list);\n\t\telse\n\t\t\tnetif_receive_skb(skb);\n\t}\n}\n\n/*\n * requires that rx->skb is a frame with ethernet header\n */\nstatic void\nieee80211_deliver_skb(struct ieee80211_rx_data *rx)\n{\n\tstruct ieee80211_sub_if_data *sdata = rx->sdata;\n\tstruct net_device *dev = sdata->dev;\n\tstruct sk_buff *skb, *xmit_skb;\n\tstruct ethhdr *ehdr = (struct ethhdr *) rx->skb->data;\n\tstruct sta_info *dsta;\n\n\tskb = rx->skb;\n\txmit_skb = NULL;\n\n\tdev_sw_netstats_rx_add(dev, skb->len);\n\n\tif (rx->sta) {\n\t\t/* The seqno index has the same property as needed\n\t\t * for the rx_msdu field, i.e. it is IEEE80211_NUM_TIDS\n\t\t * for non-QoS-data frames. Here we know it's a data\n\t\t * frame, so count MSDUs.\n\t\t */\n\t\tu64_stats_update_begin(&rx->sta->rx_stats.syncp);\n\t\trx->sta->rx_stats.msdu[rx->seqno_idx]++;\n\t\tu64_stats_update_end(&rx->sta->rx_stats.syncp);\n\t}\n\n\tif ((sdata->vif.type == NL80211_IFTYPE_AP ||\n\t     sdata->vif.type == NL80211_IFTYPE_AP_VLAN) &&\n\t    !(sdata->flags & IEEE80211_SDATA_DONT_BRIDGE_PACKETS) &&\n\t    (sdata->vif.type != NL80211_IFTYPE_AP_VLAN || !sdata->u.vlan.sta)) {\n\t\tif (is_multicast_ether_addr(ehdr->h_dest) &&\n\t\t    ieee80211_vif_get_num_mcast_if(sdata) != 0) {\n\t\t\t/*\n\t\t\t * send multicast frames both to higher layers in\n\t\t\t * local net stack and back to the wireless medium\n\t\t\t */\n\t\t\txmit_skb = skb_copy(skb, GFP_ATOMIC);\n\t\t\tif (!xmit_skb)\n\t\t\t\tnet_info_ratelimited(\"%s: failed to clone multicast frame\\n\",\n\t\t\t\t\t\t    dev->name);\n\t\t} else if (!is_multicast_ether_addr(ehdr->h_dest) &&\n\t\t\t   !ether_addr_equal(ehdr->h_dest, ehdr->h_source)) {\n\t\t\tdsta = sta_info_get(sdata, ehdr->h_dest);\n\t\t\tif (dsta) {\n\t\t\t\t/*\n\t\t\t\t * The destination station is associated to\n\t\t\t\t * this AP (in this VLAN), so send the frame\n\t\t\t\t * directly to it and do not pass it to local\n\t\t\t\t * net stack.\n\t\t\t\t */\n\t\t\t\txmit_skb = skb;\n\t\t\t\tskb = NULL;\n\t\t\t}\n\t\t}\n\t}\n\n#ifndef CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS\n\tif (skb) {\n\t\t/* 'align' will only take the values 0 or 2 here since all\n\t\t * frames are required to be aligned to 2-byte boundaries\n\t\t * when being passed to mac80211; the code here works just\n\t\t * as well if that isn't true, but mac80211 assumes it can\n\t\t * access fields as 2-byte aligned (e.g. for ether_addr_equal)\n\t\t */\n\t\tint align;\n\n\t\talign = (unsigned long)(skb->data + sizeof(struct ethhdr)) & 3;\n\t\tif (align) {\n\t\t\tif (WARN_ON(skb_headroom(skb) < 3)) {\n\t\t\t\tdev_kfree_skb(skb);\n\t\t\t\tskb = NULL;\n\t\t\t} else {\n\t\t\t\tu8 *data = skb->data;\n\t\t\t\tsize_t len = skb_headlen(skb);\n\t\t\t\tskb->data -= align;\n\t\t\t\tmemmove(skb->data, data, len);\n\t\t\t\tskb_set_tail_pointer(skb, len);\n\t\t\t}\n\t\t}\n\t}\n#endif\n\n\tif (skb) {\n\t\tskb->protocol = eth_type_trans(skb, dev);\n\t\tieee80211_deliver_skb_to_local_stack(skb, rx);\n\t}\n\n\tif (xmit_skb) {\n\t\t/*\n\t\t * Send to wireless media and increase priority by 256 to\n\t\t * keep the received priority instead of reclassifying\n\t\t * the frame (see cfg80211_classify8021d).\n\t\t */\n\t\txmit_skb->priority += 256;\n\t\txmit_skb->protocol = htons(ETH_P_802_3);\n\t\tskb_reset_network_header(xmit_skb);\n\t\tskb_reset_mac_header(xmit_skb);\n\t\tdev_queue_xmit(xmit_skb);\n\t}\n}\n\nstatic ieee80211_rx_result debug_noinline\n__ieee80211_rx_h_amsdu(struct ieee80211_rx_data *rx, u8 data_offset)\n{\n\tstruct net_device *dev = rx->sdata->dev;\n\tstruct sk_buff *skb = rx->skb;\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)skb->data;\n\t__le16 fc = hdr->frame_control;\n\tstruct sk_buff_head frame_list;\n\tstruct ethhdr ethhdr;\n\tconst u8 *check_da = ethhdr.h_dest, *check_sa = ethhdr.h_source;\n\n\tif (unlikely(ieee80211_has_a4(hdr->frame_control))) {\n\t\tcheck_da = NULL;\n\t\tcheck_sa = NULL;\n\t} else switch (rx->sdata->vif.type) {\n\t\tcase NL80211_IFTYPE_AP:\n\t\tcase NL80211_IFTYPE_AP_VLAN:\n\t\t\tcheck_da = NULL;\n\t\t\tbreak;\n\t\tcase NL80211_IFTYPE_STATION:\n\t\t\tif (!rx->sta ||\n\t\t\t    !test_sta_flag(rx->sta, WLAN_STA_TDLS_PEER))\n\t\t\t\tcheck_sa = NULL;\n\t\t\tbreak;\n\t\tcase NL80211_IFTYPE_MESH_POINT:\n\t\t\tcheck_sa = NULL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t}\n\n\tskb->dev = dev;\n\t__skb_queue_head_init(&frame_list);\n\n\tif (ieee80211_data_to_8023_exthdr(skb, &ethhdr,\n\t\t\t\t\t  rx->sdata->vif.addr,\n\t\t\t\t\t  rx->sdata->vif.type,\n\t\t\t\t\t  data_offset))\n\t\treturn RX_DROP_UNUSABLE;\n\n\tieee80211_amsdu_to_8023s(skb, &frame_list, dev->dev_addr,\n\t\t\t\t rx->sdata->vif.type,\n\t\t\t\t rx->local->hw.extra_tx_headroom,\n\t\t\t\t check_da, check_sa);\n\n\twhile (!skb_queue_empty(&frame_list)) {\n\t\trx->skb = __skb_dequeue(&frame_list);\n\n\t\tif (!ieee80211_frame_allowed(rx, fc)) {\n\t\t\tdev_kfree_skb(rx->skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tieee80211_deliver_skb(rx);\n\t}\n\n\treturn RX_QUEUED;\n}\n\nstatic ieee80211_rx_result debug_noinline\nieee80211_rx_h_amsdu(struct ieee80211_rx_data *rx)\n{\n\tstruct sk_buff *skb = rx->skb;\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(skb);\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)skb->data;\n\t__le16 fc = hdr->frame_control;\n\n\tif (!(status->rx_flags & IEEE80211_RX_AMSDU))\n\t\treturn RX_CONTINUE;\n\n\tif (unlikely(!ieee80211_is_data(fc)))\n\t\treturn RX_CONTINUE;\n\n\tif (unlikely(!ieee80211_is_data_present(fc)))\n\t\treturn RX_DROP_MONITOR;\n\n\tif (unlikely(ieee80211_has_a4(hdr->frame_control))) {\n\t\tswitch (rx->sdata->vif.type) {\n\t\tcase NL80211_IFTYPE_AP_VLAN:\n\t\t\tif (!rx->sdata->u.vlan.sta)\n\t\t\t\treturn RX_DROP_UNUSABLE;\n\t\t\tbreak;\n\t\tcase NL80211_IFTYPE_STATION:\n\t\t\tif (!rx->sdata->u.mgd.use_4addr)\n\t\t\t\treturn RX_DROP_UNUSABLE;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn RX_DROP_UNUSABLE;\n\t\t}\n\t}\n\n\tif (is_multicast_ether_addr(hdr->addr1))\n\t\treturn RX_DROP_UNUSABLE;\n\n\treturn __ieee80211_rx_h_amsdu(rx, 0);\n}\n\n#ifdef CONFIG_MAC80211_MESH\nstatic ieee80211_rx_result\nieee80211_rx_h_mesh_fwding(struct ieee80211_rx_data *rx)\n{\n\tstruct ieee80211_hdr *fwd_hdr, *hdr;\n\tstruct ieee80211_tx_info *info;\n\tstruct ieee80211s_hdr *mesh_hdr;\n\tstruct sk_buff *skb = rx->skb, *fwd_skb;\n\tstruct ieee80211_local *local = rx->local;\n\tstruct ieee80211_sub_if_data *sdata = rx->sdata;\n\tstruct ieee80211_if_mesh *ifmsh = &sdata->u.mesh;\n\tu16 ac, q, hdrlen;\n\tint tailroom = 0;\n\n\thdr = (struct ieee80211_hdr *) skb->data;\n\thdrlen = ieee80211_hdrlen(hdr->frame_control);\n\n\t/* make sure fixed part of mesh header is there, also checks skb len */\n\tif (!pskb_may_pull(rx->skb, hdrlen + 6))\n\t\treturn RX_DROP_MONITOR;\n\n\tmesh_hdr = (struct ieee80211s_hdr *) (skb->data + hdrlen);\n\n\t/* make sure full mesh header is there, also checks skb len */\n\tif (!pskb_may_pull(rx->skb,\n\t\t\t   hdrlen + ieee80211_get_mesh_hdrlen(mesh_hdr)))\n\t\treturn RX_DROP_MONITOR;\n\n\t/* reload pointers */\n\thdr = (struct ieee80211_hdr *) skb->data;\n\tmesh_hdr = (struct ieee80211s_hdr *) (skb->data + hdrlen);\n\n\tif (ieee80211_drop_unencrypted(rx, hdr->frame_control))\n\t\treturn RX_DROP_MONITOR;\n\n\t/* frame is in RMC, don't forward */\n\tif (ieee80211_is_data(hdr->frame_control) &&\n\t    is_multicast_ether_addr(hdr->addr1) &&\n\t    mesh_rmc_check(rx->sdata, hdr->addr3, mesh_hdr))\n\t\treturn RX_DROP_MONITOR;\n\n\tif (!ieee80211_is_data(hdr->frame_control))\n\t\treturn RX_CONTINUE;\n\n\tif (!mesh_hdr->ttl)\n\t\treturn RX_DROP_MONITOR;\n\n\tif (mesh_hdr->flags & MESH_FLAGS_AE) {\n\t\tstruct mesh_path *mppath;\n\t\tchar *proxied_addr;\n\t\tchar *mpp_addr;\n\n\t\tif (is_multicast_ether_addr(hdr->addr1)) {\n\t\t\tmpp_addr = hdr->addr3;\n\t\t\tproxied_addr = mesh_hdr->eaddr1;\n\t\t} else if ((mesh_hdr->flags & MESH_FLAGS_AE) ==\n\t\t\t    MESH_FLAGS_AE_A5_A6) {\n\t\t\t/* has_a4 already checked in ieee80211_rx_mesh_check */\n\t\t\tmpp_addr = hdr->addr4;\n\t\t\tproxied_addr = mesh_hdr->eaddr2;\n\t\t} else {\n\t\t\treturn RX_DROP_MONITOR;\n\t\t}\n\n\t\trcu_read_lock();\n\t\tmppath = mpp_path_lookup(sdata, proxied_addr);\n\t\tif (!mppath) {\n\t\t\tmpp_path_add(sdata, proxied_addr, mpp_addr);\n\t\t} else {\n\t\t\tspin_lock_bh(&mppath->state_lock);\n\t\t\tif (!ether_addr_equal(mppath->mpp, mpp_addr))\n\t\t\t\tmemcpy(mppath->mpp, mpp_addr, ETH_ALEN);\n\t\t\tmppath->exp_time = jiffies;\n\t\t\tspin_unlock_bh(&mppath->state_lock);\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\t/* Frame has reached destination.  Don't forward */\n\tif (!is_multicast_ether_addr(hdr->addr1) &&\n\t    ether_addr_equal(sdata->vif.addr, hdr->addr3))\n\t\treturn RX_CONTINUE;\n\n\tac = ieee80211_select_queue_80211(sdata, skb, hdr);\n\tq = sdata->vif.hw_queue[ac];\n\tif (ieee80211_queue_stopped(&local->hw, q)) {\n\t\tIEEE80211_IFSTA_MESH_CTR_INC(ifmsh, dropped_frames_congestion);\n\t\treturn RX_DROP_MONITOR;\n\t}\n\tskb_set_queue_mapping(skb, q);\n\n\tif (!--mesh_hdr->ttl) {\n\t\tif (!is_multicast_ether_addr(hdr->addr1))\n\t\t\tIEEE80211_IFSTA_MESH_CTR_INC(ifmsh,\n\t\t\t\t\t\t     dropped_frames_ttl);\n\t\tgoto out;\n\t}\n\n\tif (!ifmsh->mshcfg.dot11MeshForwarding)\n\t\tgoto out;\n\n\tif (sdata->crypto_tx_tailroom_needed_cnt)\n\t\ttailroom = IEEE80211_ENCRYPT_TAILROOM;\n\n\tfwd_skb = skb_copy_expand(skb, local->tx_headroom +\n\t\t\t\t       sdata->encrypt_headroom,\n\t\t\t\t  tailroom, GFP_ATOMIC);\n\tif (!fwd_skb)\n\t\tgoto out;\n\n\tfwd_hdr =  (struct ieee80211_hdr *) fwd_skb->data;\n\tfwd_hdr->frame_control &= ~cpu_to_le16(IEEE80211_FCTL_RETRY);\n\tinfo = IEEE80211_SKB_CB(fwd_skb);\n\tmemset(info, 0, sizeof(*info));\n\tinfo->control.flags |= IEEE80211_TX_INTCFL_NEED_TXPROCESSING;\n\tinfo->control.vif = &rx->sdata->vif;\n\tinfo->control.jiffies = jiffies;\n\tif (is_multicast_ether_addr(fwd_hdr->addr1)) {\n\t\tIEEE80211_IFSTA_MESH_CTR_INC(ifmsh, fwded_mcast);\n\t\tmemcpy(fwd_hdr->addr2, sdata->vif.addr, ETH_ALEN);\n\t\t/* update power mode indication when forwarding */\n\t\tieee80211_mps_set_frame_flags(sdata, NULL, fwd_hdr);\n\t} else if (!mesh_nexthop_lookup(sdata, fwd_skb)) {\n\t\t/* mesh power mode flags updated in mesh_nexthop_lookup */\n\t\tIEEE80211_IFSTA_MESH_CTR_INC(ifmsh, fwded_unicast);\n\t} else {\n\t\t/* unable to resolve next hop */\n\t\tmesh_path_error_tx(sdata, ifmsh->mshcfg.element_ttl,\n\t\t\t\t   fwd_hdr->addr3, 0,\n\t\t\t\t   WLAN_REASON_MESH_PATH_NOFORWARD,\n\t\t\t\t   fwd_hdr->addr2);\n\t\tIEEE80211_IFSTA_MESH_CTR_INC(ifmsh, dropped_frames_no_route);\n\t\tkfree_skb(fwd_skb);\n\t\treturn RX_DROP_MONITOR;\n\t}\n\n\tIEEE80211_IFSTA_MESH_CTR_INC(ifmsh, fwded_frames);\n\tieee80211_add_pending_skb(local, fwd_skb);\n out:\n\tif (is_multicast_ether_addr(hdr->addr1))\n\t\treturn RX_CONTINUE;\n\treturn RX_DROP_MONITOR;\n}\n#endif\n\nstatic ieee80211_rx_result debug_noinline\nieee80211_rx_h_data(struct ieee80211_rx_data *rx)\n{\n\tstruct ieee80211_sub_if_data *sdata = rx->sdata;\n\tstruct ieee80211_local *local = rx->local;\n\tstruct net_device *dev = sdata->dev;\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)rx->skb->data;\n\t__le16 fc = hdr->frame_control;\n\tbool port_control;\n\tint err;\n\n\tif (unlikely(!ieee80211_is_data(hdr->frame_control)))\n\t\treturn RX_CONTINUE;\n\n\tif (unlikely(!ieee80211_is_data_present(hdr->frame_control)))\n\t\treturn RX_DROP_MONITOR;\n\n\t/*\n\t * Send unexpected-4addr-frame event to hostapd. For older versions,\n\t * also drop the frame to cooked monitor interfaces.\n\t */\n\tif (ieee80211_has_a4(hdr->frame_control) &&\n\t    sdata->vif.type == NL80211_IFTYPE_AP) {\n\t\tif (rx->sta &&\n\t\t    !test_and_set_sta_flag(rx->sta, WLAN_STA_4ADDR_EVENT))\n\t\t\tcfg80211_rx_unexpected_4addr_frame(\n\t\t\t\trx->sdata->dev, rx->sta->sta.addr, GFP_ATOMIC);\n\t\treturn RX_DROP_MONITOR;\n\t}\n\n\terr = __ieee80211_data_to_8023(rx, &port_control);\n\tif (unlikely(err))\n\t\treturn RX_DROP_UNUSABLE;\n\n\tif (!ieee80211_frame_allowed(rx, fc))\n\t\treturn RX_DROP_MONITOR;\n\n\t/* directly handle TDLS channel switch requests/responses */\n\tif (unlikely(((struct ethhdr *)rx->skb->data)->h_proto ==\n\t\t\t\t\t\tcpu_to_be16(ETH_P_TDLS))) {\n\t\tstruct ieee80211_tdls_data *tf = (void *)rx->skb->data;\n\n\t\tif (pskb_may_pull(rx->skb,\n\t\t\t\t  offsetof(struct ieee80211_tdls_data, u)) &&\n\t\t    tf->payload_type == WLAN_TDLS_SNAP_RFTYPE &&\n\t\t    tf->category == WLAN_CATEGORY_TDLS &&\n\t\t    (tf->action_code == WLAN_TDLS_CHANNEL_SWITCH_REQUEST ||\n\t\t     tf->action_code == WLAN_TDLS_CHANNEL_SWITCH_RESPONSE)) {\n\t\t\tskb_queue_tail(&local->skb_queue_tdls_chsw, rx->skb);\n\t\t\tschedule_work(&local->tdls_chsw_work);\n\t\t\tif (rx->sta)\n\t\t\t\trx->sta->rx_stats.packets++;\n\n\t\t\treturn RX_QUEUED;\n\t\t}\n\t}\n\n\tif (rx->sdata->vif.type == NL80211_IFTYPE_AP_VLAN &&\n\t    unlikely(port_control) && sdata->bss) {\n\t\tsdata = container_of(sdata->bss, struct ieee80211_sub_if_data,\n\t\t\t\t     u.ap);\n\t\tdev = sdata->dev;\n\t\trx->sdata = sdata;\n\t}\n\n\trx->skb->dev = dev;\n\n\tif (!ieee80211_hw_check(&local->hw, SUPPORTS_DYNAMIC_PS) &&\n\t    local->ps_sdata && local->hw.conf.dynamic_ps_timeout > 0 &&\n\t    !is_multicast_ether_addr(\n\t\t    ((struct ethhdr *)rx->skb->data)->h_dest) &&\n\t    (!local->scanning &&\n\t     !test_bit(SDATA_STATE_OFFCHANNEL, &sdata->state)))\n\t\tmod_timer(&local->dynamic_ps_timer, jiffies +\n\t\t\t  msecs_to_jiffies(local->hw.conf.dynamic_ps_timeout));\n\n\tieee80211_deliver_skb(rx);\n\n\treturn RX_QUEUED;\n}\n\nstatic ieee80211_rx_result debug_noinline\nieee80211_rx_h_ctrl(struct ieee80211_rx_data *rx, struct sk_buff_head *frames)\n{\n\tstruct sk_buff *skb = rx->skb;\n\tstruct ieee80211_bar *bar = (struct ieee80211_bar *)skb->data;\n\tstruct tid_ampdu_rx *tid_agg_rx;\n\tu16 start_seq_num;\n\tu16 tid;\n\n\tif (likely(!ieee80211_is_ctl(bar->frame_control)))\n\t\treturn RX_CONTINUE;\n\n\tif (ieee80211_is_back_req(bar->frame_control)) {\n\t\tstruct {\n\t\t\t__le16 control, start_seq_num;\n\t\t} __packed bar_data;\n\t\tstruct ieee80211_event event = {\n\t\t\t.type = BAR_RX_EVENT,\n\t\t};\n\n\t\tif (!rx->sta)\n\t\t\treturn RX_DROP_MONITOR;\n\n\t\tif (skb_copy_bits(skb, offsetof(struct ieee80211_bar, control),\n\t\t\t\t  &bar_data, sizeof(bar_data)))\n\t\t\treturn RX_DROP_MONITOR;\n\n\t\ttid = le16_to_cpu(bar_data.control) >> 12;\n\n\t\tif (!test_bit(tid, rx->sta->ampdu_mlme.agg_session_valid) &&\n\t\t    !test_and_set_bit(tid, rx->sta->ampdu_mlme.unexpected_agg))\n\t\t\tieee80211_send_delba(rx->sdata, rx->sta->sta.addr, tid,\n\t\t\t\t\t     WLAN_BACK_RECIPIENT,\n\t\t\t\t\t     WLAN_REASON_QSTA_REQUIRE_SETUP);\n\n\t\ttid_agg_rx = rcu_dereference(rx->sta->ampdu_mlme.tid_rx[tid]);\n\t\tif (!tid_agg_rx)\n\t\t\treturn RX_DROP_MONITOR;\n\n\t\tstart_seq_num = le16_to_cpu(bar_data.start_seq_num) >> 4;\n\t\tevent.u.ba.tid = tid;\n\t\tevent.u.ba.ssn = start_seq_num;\n\t\tevent.u.ba.sta = &rx->sta->sta;\n\n\t\t/* reset session timer */\n\t\tif (tid_agg_rx->timeout)\n\t\t\tmod_timer(&tid_agg_rx->session_timer,\n\t\t\t\t  TU_TO_EXP_TIME(tid_agg_rx->timeout));\n\n\t\tspin_lock(&tid_agg_rx->reorder_lock);\n\t\t/* release stored frames up to start of BAR */\n\t\tieee80211_release_reorder_frames(rx->sdata, tid_agg_rx,\n\t\t\t\t\t\t start_seq_num, frames);\n\t\tspin_unlock(&tid_agg_rx->reorder_lock);\n\n\t\tdrv_event_callback(rx->local, rx->sdata, &event);\n\n\t\tkfree_skb(skb);\n\t\treturn RX_QUEUED;\n\t}\n\n\t/*\n\t * After this point, we only want management frames,\n\t * so we can drop all remaining control frames to\n\t * cooked monitor interfaces.\n\t */\n\treturn RX_DROP_MONITOR;\n}\n\nstatic void ieee80211_process_sa_query_req(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t\t   struct ieee80211_mgmt *mgmt,\n\t\t\t\t\t   size_t len)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sk_buff *skb;\n\tstruct ieee80211_mgmt *resp;\n\n\tif (!ether_addr_equal(mgmt->da, sdata->vif.addr)) {\n\t\t/* Not to own unicast address */\n\t\treturn;\n\t}\n\n\tif (!ether_addr_equal(mgmt->sa, sdata->u.mgd.bssid) ||\n\t    !ether_addr_equal(mgmt->bssid, sdata->u.mgd.bssid)) {\n\t\t/* Not from the current AP or not associated yet. */\n\t\treturn;\n\t}\n\n\tif (len < 24 + 1 + sizeof(resp->u.action.u.sa_query)) {\n\t\t/* Too short SA Query request frame */\n\t\treturn;\n\t}\n\n\tskb = dev_alloc_skb(sizeof(*resp) + local->hw.extra_tx_headroom);\n\tif (skb == NULL)\n\t\treturn;\n\n\tskb_reserve(skb, local->hw.extra_tx_headroom);\n\tresp = skb_put_zero(skb, 24);\n\tmemcpy(resp->da, mgmt->sa, ETH_ALEN);\n\tmemcpy(resp->sa, sdata->vif.addr, ETH_ALEN);\n\tmemcpy(resp->bssid, sdata->u.mgd.bssid, ETH_ALEN);\n\tresp->frame_control = cpu_to_le16(IEEE80211_FTYPE_MGMT |\n\t\t\t\t\t  IEEE80211_STYPE_ACTION);\n\tskb_put(skb, 1 + sizeof(resp->u.action.u.sa_query));\n\tresp->u.action.category = WLAN_CATEGORY_SA_QUERY;\n\tresp->u.action.u.sa_query.action = WLAN_ACTION_SA_QUERY_RESPONSE;\n\tmemcpy(resp->u.action.u.sa_query.trans_id,\n\t       mgmt->u.action.u.sa_query.trans_id,\n\t       WLAN_SA_QUERY_TR_ID_LEN);\n\n\tieee80211_tx_skb(sdata, skb);\n}\n\nstatic ieee80211_rx_result debug_noinline\nieee80211_rx_h_mgmt_check(struct ieee80211_rx_data *rx)\n{\n\tstruct ieee80211_mgmt *mgmt = (struct ieee80211_mgmt *) rx->skb->data;\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(rx->skb);\n\n\tif (ieee80211_is_s1g_beacon(mgmt->frame_control))\n\t\treturn RX_CONTINUE;\n\n\t/*\n\t * From here on, look only at management frames.\n\t * Data and control frames are already handled,\n\t * and unknown (reserved) frames are useless.\n\t */\n\tif (rx->skb->len < 24)\n\t\treturn RX_DROP_MONITOR;\n\n\tif (!ieee80211_is_mgmt(mgmt->frame_control))\n\t\treturn RX_DROP_MONITOR;\n\n\tif (rx->sdata->vif.type == NL80211_IFTYPE_AP &&\n\t    ieee80211_is_beacon(mgmt->frame_control) &&\n\t    !(rx->flags & IEEE80211_RX_BEACON_REPORTED)) {\n\t\tint sig = 0;\n\n\t\tif (ieee80211_hw_check(&rx->local->hw, SIGNAL_DBM) &&\n\t\t    !(status->flag & RX_FLAG_NO_SIGNAL_VAL))\n\t\t\tsig = status->signal;\n\n\t\tcfg80211_report_obss_beacon_khz(rx->local->hw.wiphy,\n\t\t\t\t\t\trx->skb->data, rx->skb->len,\n\t\t\t\t\t\tieee80211_rx_status_to_khz(status),\n\t\t\t\t\t\tsig);\n\t\trx->flags |= IEEE80211_RX_BEACON_REPORTED;\n\t}\n\n\tif (ieee80211_drop_unencrypted_mgmt(rx))\n\t\treturn RX_DROP_UNUSABLE;\n\n\treturn RX_CONTINUE;\n}\n\nstatic ieee80211_rx_result debug_noinline\nieee80211_rx_h_action(struct ieee80211_rx_data *rx)\n{\n\tstruct ieee80211_local *local = rx->local;\n\tstruct ieee80211_sub_if_data *sdata = rx->sdata;\n\tstruct ieee80211_mgmt *mgmt = (struct ieee80211_mgmt *) rx->skb->data;\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(rx->skb);\n\tint len = rx->skb->len;\n\n\tif (!ieee80211_is_action(mgmt->frame_control))\n\t\treturn RX_CONTINUE;\n\n\t/* drop too small frames */\n\tif (len < IEEE80211_MIN_ACTION_SIZE)\n\t\treturn RX_DROP_UNUSABLE;\n\n\tif (!rx->sta && mgmt->u.action.category != WLAN_CATEGORY_PUBLIC &&\n\t    mgmt->u.action.category != WLAN_CATEGORY_SELF_PROTECTED &&\n\t    mgmt->u.action.category != WLAN_CATEGORY_SPECTRUM_MGMT)\n\t\treturn RX_DROP_UNUSABLE;\n\n\tswitch (mgmt->u.action.category) {\n\tcase WLAN_CATEGORY_HT:\n\t\t/* reject HT action frames from stations not supporting HT */\n\t\tif (!rx->sta->sta.ht_cap.ht_supported)\n\t\t\tgoto invalid;\n\n\t\tif (sdata->vif.type != NL80211_IFTYPE_STATION &&\n\t\t    sdata->vif.type != NL80211_IFTYPE_MESH_POINT &&\n\t\t    sdata->vif.type != NL80211_IFTYPE_AP_VLAN &&\n\t\t    sdata->vif.type != NL80211_IFTYPE_AP &&\n\t\t    sdata->vif.type != NL80211_IFTYPE_ADHOC)\n\t\t\tbreak;\n\n\t\t/* verify action & smps_control/chanwidth are present */\n\t\tif (len < IEEE80211_MIN_ACTION_SIZE + 2)\n\t\t\tgoto invalid;\n\n\t\tswitch (mgmt->u.action.u.ht_smps.action) {\n\t\tcase WLAN_HT_ACTION_SMPS: {\n\t\t\tstruct ieee80211_supported_band *sband;\n\t\t\tenum ieee80211_smps_mode smps_mode;\n\t\t\tstruct sta_opmode_info sta_opmode = {};\n\n\t\t\tif (sdata->vif.type != NL80211_IFTYPE_AP &&\n\t\t\t    sdata->vif.type != NL80211_IFTYPE_AP_VLAN)\n\t\t\t\tgoto handled;\n\n\t\t\t/* convert to HT capability */\n\t\t\tswitch (mgmt->u.action.u.ht_smps.smps_control) {\n\t\t\tcase WLAN_HT_SMPS_CONTROL_DISABLED:\n\t\t\t\tsmps_mode = IEEE80211_SMPS_OFF;\n\t\t\t\tbreak;\n\t\t\tcase WLAN_HT_SMPS_CONTROL_STATIC:\n\t\t\t\tsmps_mode = IEEE80211_SMPS_STATIC;\n\t\t\t\tbreak;\n\t\t\tcase WLAN_HT_SMPS_CONTROL_DYNAMIC:\n\t\t\t\tsmps_mode = IEEE80211_SMPS_DYNAMIC;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tgoto invalid;\n\t\t\t}\n\n\t\t\t/* if no change do nothing */\n\t\t\tif (rx->sta->sta.smps_mode == smps_mode)\n\t\t\t\tgoto handled;\n\t\t\trx->sta->sta.smps_mode = smps_mode;\n\t\t\tsta_opmode.smps_mode =\n\t\t\t\tieee80211_smps_mode_to_smps_mode(smps_mode);\n\t\t\tsta_opmode.changed = STA_OPMODE_SMPS_MODE_CHANGED;\n\n\t\t\tsband = rx->local->hw.wiphy->bands[status->band];\n\n\t\t\trate_control_rate_update(local, sband, rx->sta,\n\t\t\t\t\t\t IEEE80211_RC_SMPS_CHANGED);\n\t\t\tcfg80211_sta_opmode_change_notify(sdata->dev,\n\t\t\t\t\t\t\t  rx->sta->addr,\n\t\t\t\t\t\t\t  &sta_opmode,\n\t\t\t\t\t\t\t  GFP_ATOMIC);\n\t\t\tgoto handled;\n\t\t}\n\t\tcase WLAN_HT_ACTION_NOTIFY_CHANWIDTH: {\n\t\t\tstruct ieee80211_supported_band *sband;\n\t\t\tu8 chanwidth = mgmt->u.action.u.ht_notify_cw.chanwidth;\n\t\t\tenum ieee80211_sta_rx_bandwidth max_bw, new_bw;\n\t\t\tstruct sta_opmode_info sta_opmode = {};\n\n\t\t\t/* If it doesn't support 40 MHz it can't change ... */\n\t\t\tif (!(rx->sta->sta.ht_cap.cap &\n\t\t\t\t\tIEEE80211_HT_CAP_SUP_WIDTH_20_40))\n\t\t\t\tgoto handled;\n\n\t\t\tif (chanwidth == IEEE80211_HT_CHANWIDTH_20MHZ)\n\t\t\t\tmax_bw = IEEE80211_STA_RX_BW_20;\n\t\t\telse\n\t\t\t\tmax_bw = ieee80211_sta_cap_rx_bw(rx->sta);\n\n\t\t\t/* set cur_max_bandwidth and recalc sta bw */\n\t\t\trx->sta->cur_max_bandwidth = max_bw;\n\t\t\tnew_bw = ieee80211_sta_cur_vht_bw(rx->sta);\n\n\t\t\tif (rx->sta->sta.bandwidth == new_bw)\n\t\t\t\tgoto handled;\n\n\t\t\trx->sta->sta.bandwidth = new_bw;\n\t\t\tsband = rx->local->hw.wiphy->bands[status->band];\n\t\t\tsta_opmode.bw =\n\t\t\t\tieee80211_sta_rx_bw_to_chan_width(rx->sta);\n\t\t\tsta_opmode.changed = STA_OPMODE_MAX_BW_CHANGED;\n\n\t\t\trate_control_rate_update(local, sband, rx->sta,\n\t\t\t\t\t\t IEEE80211_RC_BW_CHANGED);\n\t\t\tcfg80211_sta_opmode_change_notify(sdata->dev,\n\t\t\t\t\t\t\t  rx->sta->addr,\n\t\t\t\t\t\t\t  &sta_opmode,\n\t\t\t\t\t\t\t  GFP_ATOMIC);\n\t\t\tgoto handled;\n\t\t}\n\t\tdefault:\n\t\t\tgoto invalid;\n\t\t}\n\n\t\tbreak;\n\tcase WLAN_CATEGORY_PUBLIC:\n\t\tif (len < IEEE80211_MIN_ACTION_SIZE + 1)\n\t\t\tgoto invalid;\n\t\tif (sdata->vif.type != NL80211_IFTYPE_STATION)\n\t\t\tbreak;\n\t\tif (!rx->sta)\n\t\t\tbreak;\n\t\tif (!ether_addr_equal(mgmt->bssid, sdata->u.mgd.bssid))\n\t\t\tbreak;\n\t\tif (mgmt->u.action.u.ext_chan_switch.action_code !=\n\t\t\t\tWLAN_PUB_ACTION_EXT_CHANSW_ANN)\n\t\t\tbreak;\n\t\tif (len < offsetof(struct ieee80211_mgmt,\n\t\t\t\t   u.action.u.ext_chan_switch.variable))\n\t\t\tgoto invalid;\n\t\tgoto queue;\n\tcase WLAN_CATEGORY_VHT:\n\t\tif (sdata->vif.type != NL80211_IFTYPE_STATION &&\n\t\t    sdata->vif.type != NL80211_IFTYPE_MESH_POINT &&\n\t\t    sdata->vif.type != NL80211_IFTYPE_AP_VLAN &&\n\t\t    sdata->vif.type != NL80211_IFTYPE_AP &&\n\t\t    sdata->vif.type != NL80211_IFTYPE_ADHOC)\n\t\t\tbreak;\n\n\t\t/* verify action code is present */\n\t\tif (len < IEEE80211_MIN_ACTION_SIZE + 1)\n\t\t\tgoto invalid;\n\n\t\tswitch (mgmt->u.action.u.vht_opmode_notif.action_code) {\n\t\tcase WLAN_VHT_ACTION_OPMODE_NOTIF: {\n\t\t\t/* verify opmode is present */\n\t\t\tif (len < IEEE80211_MIN_ACTION_SIZE + 2)\n\t\t\t\tgoto invalid;\n\t\t\tgoto queue;\n\t\t}\n\t\tcase WLAN_VHT_ACTION_GROUPID_MGMT: {\n\t\t\tif (len < IEEE80211_MIN_ACTION_SIZE + 25)\n\t\t\t\tgoto invalid;\n\t\t\tgoto queue;\n\t\t}\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase WLAN_CATEGORY_BACK:\n\t\tif (sdata->vif.type != NL80211_IFTYPE_STATION &&\n\t\t    sdata->vif.type != NL80211_IFTYPE_MESH_POINT &&\n\t\t    sdata->vif.type != NL80211_IFTYPE_AP_VLAN &&\n\t\t    sdata->vif.type != NL80211_IFTYPE_AP &&\n\t\t    sdata->vif.type != NL80211_IFTYPE_ADHOC)\n\t\t\tbreak;\n\n\t\t/* verify action_code is present */\n\t\tif (len < IEEE80211_MIN_ACTION_SIZE + 1)\n\t\t\tbreak;\n\n\t\tswitch (mgmt->u.action.u.addba_req.action_code) {\n\t\tcase WLAN_ACTION_ADDBA_REQ:\n\t\t\tif (len < (IEEE80211_MIN_ACTION_SIZE +\n\t\t\t\t   sizeof(mgmt->u.action.u.addba_req)))\n\t\t\t\tgoto invalid;\n\t\t\tbreak;\n\t\tcase WLAN_ACTION_ADDBA_RESP:\n\t\t\tif (len < (IEEE80211_MIN_ACTION_SIZE +\n\t\t\t\t   sizeof(mgmt->u.action.u.addba_resp)))\n\t\t\t\tgoto invalid;\n\t\t\tbreak;\n\t\tcase WLAN_ACTION_DELBA:\n\t\t\tif (len < (IEEE80211_MIN_ACTION_SIZE +\n\t\t\t\t   sizeof(mgmt->u.action.u.delba)))\n\t\t\t\tgoto invalid;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto invalid;\n\t\t}\n\n\t\tgoto queue;\n\tcase WLAN_CATEGORY_SPECTRUM_MGMT:\n\t\t/* verify action_code is present */\n\t\tif (len < IEEE80211_MIN_ACTION_SIZE + 1)\n\t\t\tbreak;\n\n\t\tswitch (mgmt->u.action.u.measurement.action_code) {\n\t\tcase WLAN_ACTION_SPCT_MSR_REQ:\n\t\t\tif (status->band != NL80211_BAND_5GHZ)\n\t\t\t\tbreak;\n\n\t\t\tif (len < (IEEE80211_MIN_ACTION_SIZE +\n\t\t\t\t   sizeof(mgmt->u.action.u.measurement)))\n\t\t\t\tbreak;\n\n\t\t\tif (sdata->vif.type != NL80211_IFTYPE_STATION)\n\t\t\t\tbreak;\n\n\t\t\tieee80211_process_measurement_req(sdata, mgmt, len);\n\t\t\tgoto handled;\n\t\tcase WLAN_ACTION_SPCT_CHL_SWITCH: {\n\t\t\tu8 *bssid;\n\t\t\tif (len < (IEEE80211_MIN_ACTION_SIZE +\n\t\t\t\t   sizeof(mgmt->u.action.u.chan_switch)))\n\t\t\t\tbreak;\n\n\t\t\tif (sdata->vif.type != NL80211_IFTYPE_STATION &&\n\t\t\t    sdata->vif.type != NL80211_IFTYPE_ADHOC &&\n\t\t\t    sdata->vif.type != NL80211_IFTYPE_MESH_POINT)\n\t\t\t\tbreak;\n\n\t\t\tif (sdata->vif.type == NL80211_IFTYPE_STATION)\n\t\t\t\tbssid = sdata->u.mgd.bssid;\n\t\t\telse if (sdata->vif.type == NL80211_IFTYPE_ADHOC)\n\t\t\t\tbssid = sdata->u.ibss.bssid;\n\t\t\telse if (sdata->vif.type == NL80211_IFTYPE_MESH_POINT)\n\t\t\t\tbssid = mgmt->sa;\n\t\t\telse\n\t\t\t\tbreak;\n\n\t\t\tif (!ether_addr_equal(mgmt->bssid, bssid))\n\t\t\t\tbreak;\n\n\t\t\tgoto queue;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase WLAN_CATEGORY_SELF_PROTECTED:\n\t\tif (len < (IEEE80211_MIN_ACTION_SIZE +\n\t\t\t   sizeof(mgmt->u.action.u.self_prot.action_code)))\n\t\t\tbreak;\n\n\t\tswitch (mgmt->u.action.u.self_prot.action_code) {\n\t\tcase WLAN_SP_MESH_PEERING_OPEN:\n\t\tcase WLAN_SP_MESH_PEERING_CLOSE:\n\t\tcase WLAN_SP_MESH_PEERING_CONFIRM:\n\t\t\tif (!ieee80211_vif_is_mesh(&sdata->vif))\n\t\t\t\tgoto invalid;\n\t\t\tif (sdata->u.mesh.user_mpm)\n\t\t\t\t/* userspace handles this frame */\n\t\t\t\tbreak;\n\t\t\tgoto queue;\n\t\tcase WLAN_SP_MGK_INFORM:\n\t\tcase WLAN_SP_MGK_ACK:\n\t\t\tif (!ieee80211_vif_is_mesh(&sdata->vif))\n\t\t\t\tgoto invalid;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase WLAN_CATEGORY_MESH_ACTION:\n\t\tif (len < (IEEE80211_MIN_ACTION_SIZE +\n\t\t\t   sizeof(mgmt->u.action.u.mesh_action.action_code)))\n\t\t\tbreak;\n\n\t\tif (!ieee80211_vif_is_mesh(&sdata->vif))\n\t\t\tbreak;\n\t\tif (mesh_action_is_path_sel(mgmt) &&\n\t\t    !mesh_path_sel_is_hwmp(sdata))\n\t\t\tbreak;\n\t\tgoto queue;\n\t}\n\n\treturn RX_CONTINUE;\n\n invalid:\n\tstatus->rx_flags |= IEEE80211_RX_MALFORMED_ACTION_FRM;\n\t/* will return in the next handlers */\n\treturn RX_CONTINUE;\n\n handled:\n\tif (rx->sta)\n\t\trx->sta->rx_stats.packets++;\n\tdev_kfree_skb(rx->skb);\n\treturn RX_QUEUED;\n\n queue:\n\tskb_queue_tail(&sdata->skb_queue, rx->skb);\n\tieee80211_queue_work(&local->hw, &sdata->work);\n\tif (rx->sta)\n\t\trx->sta->rx_stats.packets++;\n\treturn RX_QUEUED;\n}\n\nstatic ieee80211_rx_result debug_noinline\nieee80211_rx_h_userspace_mgmt(struct ieee80211_rx_data *rx)\n{\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(rx->skb);\n\tint sig = 0;\n\n\t/* skip known-bad action frames and return them in the next handler */\n\tif (status->rx_flags & IEEE80211_RX_MALFORMED_ACTION_FRM)\n\t\treturn RX_CONTINUE;\n\n\t/*\n\t * Getting here means the kernel doesn't know how to handle\n\t * it, but maybe userspace does ... include returned frames\n\t * so userspace can register for those to know whether ones\n\t * it transmitted were processed or returned.\n\t */\n\n\tif (ieee80211_hw_check(&rx->local->hw, SIGNAL_DBM) &&\n\t    !(status->flag & RX_FLAG_NO_SIGNAL_VAL))\n\t\tsig = status->signal;\n\n\tif (cfg80211_rx_mgmt_khz(&rx->sdata->wdev,\n\t\t\t\t ieee80211_rx_status_to_khz(status), sig,\n\t\t\t\t rx->skb->data, rx->skb->len, 0)) {\n\t\tif (rx->sta)\n\t\t\trx->sta->rx_stats.packets++;\n\t\tdev_kfree_skb(rx->skb);\n\t\treturn RX_QUEUED;\n\t}\n\n\treturn RX_CONTINUE;\n}\n\nstatic ieee80211_rx_result debug_noinline\nieee80211_rx_h_action_post_userspace(struct ieee80211_rx_data *rx)\n{\n\tstruct ieee80211_sub_if_data *sdata = rx->sdata;\n\tstruct ieee80211_mgmt *mgmt = (struct ieee80211_mgmt *) rx->skb->data;\n\tint len = rx->skb->len;\n\n\tif (!ieee80211_is_action(mgmt->frame_control))\n\t\treturn RX_CONTINUE;\n\n\tswitch (mgmt->u.action.category) {\n\tcase WLAN_CATEGORY_SA_QUERY:\n\t\tif (len < (IEEE80211_MIN_ACTION_SIZE +\n\t\t\t   sizeof(mgmt->u.action.u.sa_query)))\n\t\t\tbreak;\n\n\t\tswitch (mgmt->u.action.u.sa_query.action) {\n\t\tcase WLAN_ACTION_SA_QUERY_REQUEST:\n\t\t\tif (sdata->vif.type != NL80211_IFTYPE_STATION)\n\t\t\t\tbreak;\n\t\t\tieee80211_process_sa_query_req(sdata, mgmt, len);\n\t\t\tgoto handled;\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn RX_CONTINUE;\n\n handled:\n\tif (rx->sta)\n\t\trx->sta->rx_stats.packets++;\n\tdev_kfree_skb(rx->skb);\n\treturn RX_QUEUED;\n}\n\nstatic ieee80211_rx_result debug_noinline\nieee80211_rx_h_action_return(struct ieee80211_rx_data *rx)\n{\n\tstruct ieee80211_local *local = rx->local;\n\tstruct ieee80211_mgmt *mgmt = (struct ieee80211_mgmt *) rx->skb->data;\n\tstruct sk_buff *nskb;\n\tstruct ieee80211_sub_if_data *sdata = rx->sdata;\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(rx->skb);\n\n\tif (!ieee80211_is_action(mgmt->frame_control))\n\t\treturn RX_CONTINUE;\n\n\t/*\n\t * For AP mode, hostapd is responsible for handling any action\n\t * frames that we didn't handle, including returning unknown\n\t * ones. For all other modes we will return them to the sender,\n\t * setting the 0x80 bit in the action category, as required by\n\t * 802.11-2012 9.24.4.\n\t * Newer versions of hostapd shall also use the management frame\n\t * registration mechanisms, but older ones still use cooked\n\t * monitor interfaces so push all frames there.\n\t */\n\tif (!(status->rx_flags & IEEE80211_RX_MALFORMED_ACTION_FRM) &&\n\t    (sdata->vif.type == NL80211_IFTYPE_AP ||\n\t     sdata->vif.type == NL80211_IFTYPE_AP_VLAN))\n\t\treturn RX_DROP_MONITOR;\n\n\tif (is_multicast_ether_addr(mgmt->da))\n\t\treturn RX_DROP_MONITOR;\n\n\t/* do not return rejected action frames */\n\tif (mgmt->u.action.category & 0x80)\n\t\treturn RX_DROP_UNUSABLE;\n\n\tnskb = skb_copy_expand(rx->skb, local->hw.extra_tx_headroom, 0,\n\t\t\t       GFP_ATOMIC);\n\tif (nskb) {\n\t\tstruct ieee80211_mgmt *nmgmt = (void *)nskb->data;\n\n\t\tnmgmt->u.action.category |= 0x80;\n\t\tmemcpy(nmgmt->da, nmgmt->sa, ETH_ALEN);\n\t\tmemcpy(nmgmt->sa, rx->sdata->vif.addr, ETH_ALEN);\n\n\t\tmemset(nskb->cb, 0, sizeof(nskb->cb));\n\n\t\tif (rx->sdata->vif.type == NL80211_IFTYPE_P2P_DEVICE) {\n\t\t\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(nskb);\n\n\t\t\tinfo->flags = IEEE80211_TX_CTL_TX_OFFCHAN |\n\t\t\t\t      IEEE80211_TX_INTFL_OFFCHAN_TX_OK |\n\t\t\t\t      IEEE80211_TX_CTL_NO_CCK_RATE;\n\t\t\tif (ieee80211_hw_check(&local->hw, QUEUE_CONTROL))\n\t\t\t\tinfo->hw_queue =\n\t\t\t\t\tlocal->hw.offchannel_tx_hw_queue;\n\t\t}\n\n\t\t__ieee80211_tx_skb_tid_band(rx->sdata, nskb, 7,\n\t\t\t\t\t    status->band);\n\t}\n\tdev_kfree_skb(rx->skb);\n\treturn RX_QUEUED;\n}\n\nstatic ieee80211_rx_result debug_noinline\nieee80211_rx_h_ext(struct ieee80211_rx_data *rx)\n{\n\tstruct ieee80211_sub_if_data *sdata = rx->sdata;\n\tstruct ieee80211_hdr *hdr = (void *)rx->skb->data;\n\n\tif (!ieee80211_is_ext(hdr->frame_control))\n\t\treturn RX_CONTINUE;\n\n\tif (sdata->vif.type != NL80211_IFTYPE_STATION)\n\t\treturn RX_DROP_MONITOR;\n\n\t/* for now only beacons are ext, so queue them */\n\tskb_queue_tail(&sdata->skb_queue, rx->skb);\n\tieee80211_queue_work(&rx->local->hw, &sdata->work);\n\tif (rx->sta)\n\t\trx->sta->rx_stats.packets++;\n\n\treturn RX_QUEUED;\n}\n\nstatic ieee80211_rx_result debug_noinline\nieee80211_rx_h_mgmt(struct ieee80211_rx_data *rx)\n{\n\tstruct ieee80211_sub_if_data *sdata = rx->sdata;\n\tstruct ieee80211_mgmt *mgmt = (void *)rx->skb->data;\n\t__le16 stype;\n\n\tstype = mgmt->frame_control & cpu_to_le16(IEEE80211_FCTL_STYPE);\n\n\tif (!ieee80211_vif_is_mesh(&sdata->vif) &&\n\t    sdata->vif.type != NL80211_IFTYPE_ADHOC &&\n\t    sdata->vif.type != NL80211_IFTYPE_OCB &&\n\t    sdata->vif.type != NL80211_IFTYPE_STATION)\n\t\treturn RX_DROP_MONITOR;\n\n\tswitch (stype) {\n\tcase cpu_to_le16(IEEE80211_STYPE_AUTH):\n\tcase cpu_to_le16(IEEE80211_STYPE_BEACON):\n\tcase cpu_to_le16(IEEE80211_STYPE_PROBE_RESP):\n\t\t/* process for all: mesh, mlme, ibss */\n\t\tbreak;\n\tcase cpu_to_le16(IEEE80211_STYPE_DEAUTH):\n\t\tif (is_multicast_ether_addr(mgmt->da) &&\n\t\t    !is_broadcast_ether_addr(mgmt->da))\n\t\t\treturn RX_DROP_MONITOR;\n\n\t\t/* process only for station/IBSS */\n\t\tif (sdata->vif.type != NL80211_IFTYPE_STATION &&\n\t\t    sdata->vif.type != NL80211_IFTYPE_ADHOC)\n\t\t\treturn RX_DROP_MONITOR;\n\t\tbreak;\n\tcase cpu_to_le16(IEEE80211_STYPE_ASSOC_RESP):\n\tcase cpu_to_le16(IEEE80211_STYPE_REASSOC_RESP):\n\tcase cpu_to_le16(IEEE80211_STYPE_DISASSOC):\n\t\tif (is_multicast_ether_addr(mgmt->da) &&\n\t\t    !is_broadcast_ether_addr(mgmt->da))\n\t\t\treturn RX_DROP_MONITOR;\n\n\t\t/* process only for station */\n\t\tif (sdata->vif.type != NL80211_IFTYPE_STATION)\n\t\t\treturn RX_DROP_MONITOR;\n\t\tbreak;\n\tcase cpu_to_le16(IEEE80211_STYPE_PROBE_REQ):\n\t\t/* process only for ibss and mesh */\n\t\tif (sdata->vif.type != NL80211_IFTYPE_ADHOC &&\n\t\t    sdata->vif.type != NL80211_IFTYPE_MESH_POINT)\n\t\t\treturn RX_DROP_MONITOR;\n\t\tbreak;\n\tdefault:\n\t\treturn RX_DROP_MONITOR;\n\t}\n\n\t/* queue up frame and kick off work to process it */\n\tskb_queue_tail(&sdata->skb_queue, rx->skb);\n\tieee80211_queue_work(&rx->local->hw, &sdata->work);\n\tif (rx->sta)\n\t\trx->sta->rx_stats.packets++;\n\n\treturn RX_QUEUED;\n}\n\nstatic void ieee80211_rx_cooked_monitor(struct ieee80211_rx_data *rx,\n\t\t\t\t\tstruct ieee80211_rate *rate)\n{\n\tstruct ieee80211_sub_if_data *sdata;\n\tstruct ieee80211_local *local = rx->local;\n\tstruct sk_buff *skb = rx->skb, *skb2;\n\tstruct net_device *prev_dev = NULL;\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(skb);\n\tint needed_headroom;\n\n\t/*\n\t * If cooked monitor has been processed already, then\n\t * don't do it again. If not, set the flag.\n\t */\n\tif (rx->flags & IEEE80211_RX_CMNTR)\n\t\tgoto out_free_skb;\n\trx->flags |= IEEE80211_RX_CMNTR;\n\n\t/* If there are no cooked monitor interfaces, just free the SKB */\n\tif (!local->cooked_mntrs)\n\t\tgoto out_free_skb;\n\n\t/* vendor data is long removed here */\n\tstatus->flag &= ~RX_FLAG_RADIOTAP_VENDOR_DATA;\n\t/* room for the radiotap header based on driver features */\n\tneeded_headroom = ieee80211_rx_radiotap_hdrlen(local, status, skb);\n\n\tif (skb_headroom(skb) < needed_headroom &&\n\t    pskb_expand_head(skb, needed_headroom, 0, GFP_ATOMIC))\n\t\tgoto out_free_skb;\n\n\t/* prepend radiotap information */\n\tieee80211_add_rx_radiotap_header(local, skb, rate, needed_headroom,\n\t\t\t\t\t false);\n\n\tskb_reset_mac_header(skb);\n\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\tskb->pkt_type = PACKET_OTHERHOST;\n\tskb->protocol = htons(ETH_P_802_2);\n\n\tlist_for_each_entry_rcu(sdata, &local->interfaces, list) {\n\t\tif (!ieee80211_sdata_running(sdata))\n\t\t\tcontinue;\n\n\t\tif (sdata->vif.type != NL80211_IFTYPE_MONITOR ||\n\t\t    !(sdata->u.mntr.flags & MONITOR_FLAG_COOK_FRAMES))\n\t\t\tcontinue;\n\n\t\tif (prev_dev) {\n\t\t\tskb2 = skb_clone(skb, GFP_ATOMIC);\n\t\t\tif (skb2) {\n\t\t\t\tskb2->dev = prev_dev;\n\t\t\t\tnetif_receive_skb(skb2);\n\t\t\t}\n\t\t}\n\n\t\tprev_dev = sdata->dev;\n\t\tdev_sw_netstats_rx_add(sdata->dev, skb->len);\n\t}\n\n\tif (prev_dev) {\n\t\tskb->dev = prev_dev;\n\t\tnetif_receive_skb(skb);\n\t\treturn;\n\t}\n\n out_free_skb:\n\tdev_kfree_skb(skb);\n}\n\nstatic void ieee80211_rx_handlers_result(struct ieee80211_rx_data *rx,\n\t\t\t\t\t ieee80211_rx_result res)\n{\n\tswitch (res) {\n\tcase RX_DROP_MONITOR:\n\t\tI802_DEBUG_INC(rx->sdata->local->rx_handlers_drop);\n\t\tif (rx->sta)\n\t\t\trx->sta->rx_stats.dropped++;\n\t\tfallthrough;\n\tcase RX_CONTINUE: {\n\t\tstruct ieee80211_rate *rate = NULL;\n\t\tstruct ieee80211_supported_band *sband;\n\t\tstruct ieee80211_rx_status *status;\n\n\t\tstatus = IEEE80211_SKB_RXCB((rx->skb));\n\n\t\tsband = rx->local->hw.wiphy->bands[status->band];\n\t\tif (status->encoding == RX_ENC_LEGACY)\n\t\t\trate = &sband->bitrates[status->rate_idx];\n\n\t\tieee80211_rx_cooked_monitor(rx, rate);\n\t\tbreak;\n\t\t}\n\tcase RX_DROP_UNUSABLE:\n\t\tI802_DEBUG_INC(rx->sdata->local->rx_handlers_drop);\n\t\tif (rx->sta)\n\t\t\trx->sta->rx_stats.dropped++;\n\t\tdev_kfree_skb(rx->skb);\n\t\tbreak;\n\tcase RX_QUEUED:\n\t\tI802_DEBUG_INC(rx->sdata->local->rx_handlers_queued);\n\t\tbreak;\n\t}\n}\n\nstatic void ieee80211_rx_handlers(struct ieee80211_rx_data *rx,\n\t\t\t\t  struct sk_buff_head *frames)\n{\n\tieee80211_rx_result res = RX_DROP_MONITOR;\n\tstruct sk_buff *skb;\n\n#define CALL_RXH(rxh)\t\t\t\\\n\tdo {\t\t\t\t\\\n\t\tres = rxh(rx);\t\t\\\n\t\tif (res != RX_CONTINUE)\t\\\n\t\t\tgoto rxh_next;  \\\n\t} while (0)\n\n\t/* Lock here to avoid hitting all of the data used in the RX\n\t * path (e.g. key data, station data, ...) concurrently when\n\t * a frame is released from the reorder buffer due to timeout\n\t * from the timer, potentially concurrently with RX from the\n\t * driver.\n\t */\n\tspin_lock_bh(&rx->local->rx_path_lock);\n\n\twhile ((skb = __skb_dequeue(frames))) {\n\t\t/*\n\t\t * all the other fields are valid across frames\n\t\t * that belong to an aMPDU since they are on the\n\t\t * same TID from the same station\n\t\t */\n\t\trx->skb = skb;\n\n\t\tCALL_RXH(ieee80211_rx_h_check_more_data);\n\t\tCALL_RXH(ieee80211_rx_h_uapsd_and_pspoll);\n\t\tCALL_RXH(ieee80211_rx_h_sta_process);\n\t\tCALL_RXH(ieee80211_rx_h_decrypt);\n\t\tCALL_RXH(ieee80211_rx_h_defragment);\n\t\tCALL_RXH(ieee80211_rx_h_michael_mic_verify);\n\t\t/* must be after MMIC verify so header is counted in MPDU mic */\n#ifdef CONFIG_MAC80211_MESH\n\t\tif (ieee80211_vif_is_mesh(&rx->sdata->vif))\n\t\t\tCALL_RXH(ieee80211_rx_h_mesh_fwding);\n#endif\n\t\tCALL_RXH(ieee80211_rx_h_amsdu);\n\t\tCALL_RXH(ieee80211_rx_h_data);\n\n\t\t/* special treatment -- needs the queue */\n\t\tres = ieee80211_rx_h_ctrl(rx, frames);\n\t\tif (res != RX_CONTINUE)\n\t\t\tgoto rxh_next;\n\n\t\tCALL_RXH(ieee80211_rx_h_mgmt_check);\n\t\tCALL_RXH(ieee80211_rx_h_action);\n\t\tCALL_RXH(ieee80211_rx_h_userspace_mgmt);\n\t\tCALL_RXH(ieee80211_rx_h_action_post_userspace);\n\t\tCALL_RXH(ieee80211_rx_h_action_return);\n\t\tCALL_RXH(ieee80211_rx_h_ext);\n\t\tCALL_RXH(ieee80211_rx_h_mgmt);\n\n rxh_next:\n\t\tieee80211_rx_handlers_result(rx, res);\n\n#undef CALL_RXH\n\t}\n\n\tspin_unlock_bh(&rx->local->rx_path_lock);\n}\n\nstatic void ieee80211_invoke_rx_handlers(struct ieee80211_rx_data *rx)\n{\n\tstruct sk_buff_head reorder_release;\n\tieee80211_rx_result res = RX_DROP_MONITOR;\n\n\t__skb_queue_head_init(&reorder_release);\n\n#define CALL_RXH(rxh)\t\t\t\\\n\tdo {\t\t\t\t\\\n\t\tres = rxh(rx);\t\t\\\n\t\tif (res != RX_CONTINUE)\t\\\n\t\t\tgoto rxh_next;  \\\n\t} while (0)\n\n\tCALL_RXH(ieee80211_rx_h_check_dup);\n\tCALL_RXH(ieee80211_rx_h_check);\n\n\tieee80211_rx_reorder_ampdu(rx, &reorder_release);\n\n\tieee80211_rx_handlers(rx, &reorder_release);\n\treturn;\n\n rxh_next:\n\tieee80211_rx_handlers_result(rx, res);\n\n#undef CALL_RXH\n}\n\n/*\n * This function makes calls into the RX path, therefore\n * it has to be invoked under RCU read lock.\n */\nvoid ieee80211_release_reorder_timeout(struct sta_info *sta, int tid)\n{\n\tstruct sk_buff_head frames;\n\tstruct ieee80211_rx_data rx = {\n\t\t.sta = sta,\n\t\t.sdata = sta->sdata,\n\t\t.local = sta->local,\n\t\t/* This is OK -- must be QoS data frame */\n\t\t.security_idx = tid,\n\t\t.seqno_idx = tid,\n\t};\n\tstruct tid_ampdu_rx *tid_agg_rx;\n\n\ttid_agg_rx = rcu_dereference(sta->ampdu_mlme.tid_rx[tid]);\n\tif (!tid_agg_rx)\n\t\treturn;\n\n\t__skb_queue_head_init(&frames);\n\n\tspin_lock(&tid_agg_rx->reorder_lock);\n\tieee80211_sta_reorder_release(sta->sdata, tid_agg_rx, &frames);\n\tspin_unlock(&tid_agg_rx->reorder_lock);\n\n\tif (!skb_queue_empty(&frames)) {\n\t\tstruct ieee80211_event event = {\n\t\t\t.type = BA_FRAME_TIMEOUT,\n\t\t\t.u.ba.tid = tid,\n\t\t\t.u.ba.sta = &sta->sta,\n\t\t};\n\t\tdrv_event_callback(rx.local, rx.sdata, &event);\n\t}\n\n\tieee80211_rx_handlers(&rx, &frames);\n}\n\nvoid ieee80211_mark_rx_ba_filtered_frames(struct ieee80211_sta *pubsta, u8 tid,\n\t\t\t\t\t  u16 ssn, u64 filtered,\n\t\t\t\t\t  u16 received_mpdus)\n{\n\tstruct sta_info *sta;\n\tstruct tid_ampdu_rx *tid_agg_rx;\n\tstruct sk_buff_head frames;\n\tstruct ieee80211_rx_data rx = {\n\t\t/* This is OK -- must be QoS data frame */\n\t\t.security_idx = tid,\n\t\t.seqno_idx = tid,\n\t};\n\tint i, diff;\n\n\tif (WARN_ON(!pubsta || tid >= IEEE80211_NUM_TIDS))\n\t\treturn;\n\n\t__skb_queue_head_init(&frames);\n\n\tsta = container_of(pubsta, struct sta_info, sta);\n\n\trx.sta = sta;\n\trx.sdata = sta->sdata;\n\trx.local = sta->local;\n\n\trcu_read_lock();\n\ttid_agg_rx = rcu_dereference(sta->ampdu_mlme.tid_rx[tid]);\n\tif (!tid_agg_rx)\n\t\tgoto out;\n\n\tspin_lock_bh(&tid_agg_rx->reorder_lock);\n\n\tif (received_mpdus >= IEEE80211_SN_MODULO >> 1) {\n\t\tint release;\n\n\t\t/* release all frames in the reorder buffer */\n\t\trelease = (tid_agg_rx->head_seq_num + tid_agg_rx->buf_size) %\n\t\t\t   IEEE80211_SN_MODULO;\n\t\tieee80211_release_reorder_frames(sta->sdata, tid_agg_rx,\n\t\t\t\t\t\t release, &frames);\n\t\t/* update ssn to match received ssn */\n\t\ttid_agg_rx->head_seq_num = ssn;\n\t} else {\n\t\tieee80211_release_reorder_frames(sta->sdata, tid_agg_rx, ssn,\n\t\t\t\t\t\t &frames);\n\t}\n\n\t/* handle the case that received ssn is behind the mac ssn.\n\t * it can be tid_agg_rx->buf_size behind and still be valid */\n\tdiff = (tid_agg_rx->head_seq_num - ssn) & IEEE80211_SN_MASK;\n\tif (diff >= tid_agg_rx->buf_size) {\n\t\ttid_agg_rx->reorder_buf_filtered = 0;\n\t\tgoto release;\n\t}\n\tfiltered = filtered >> diff;\n\tssn += diff;\n\n\t/* update bitmap */\n\tfor (i = 0; i < tid_agg_rx->buf_size; i++) {\n\t\tint index = (ssn + i) % tid_agg_rx->buf_size;\n\n\t\ttid_agg_rx->reorder_buf_filtered &= ~BIT_ULL(index);\n\t\tif (filtered & BIT_ULL(i))\n\t\t\ttid_agg_rx->reorder_buf_filtered |= BIT_ULL(index);\n\t}\n\n\t/* now process also frames that the filter marking released */\n\tieee80211_sta_reorder_release(sta->sdata, tid_agg_rx, &frames);\n\nrelease:\n\tspin_unlock_bh(&tid_agg_rx->reorder_lock);\n\n\tieee80211_rx_handlers(&rx, &frames);\n\n out:\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(ieee80211_mark_rx_ba_filtered_frames);\n\n/* main receive path */\n\nstatic bool ieee80211_accept_frame(struct ieee80211_rx_data *rx)\n{\n\tstruct ieee80211_sub_if_data *sdata = rx->sdata;\n\tstruct sk_buff *skb = rx->skb;\n\tstruct ieee80211_hdr *hdr = (void *)skb->data;\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(skb);\n\tu8 *bssid = ieee80211_get_bssid(hdr, skb->len, sdata->vif.type);\n\tbool multicast = is_multicast_ether_addr(hdr->addr1) ||\n\t\t\t ieee80211_is_s1g_beacon(hdr->frame_control);\n\n\tswitch (sdata->vif.type) {\n\tcase NL80211_IFTYPE_STATION:\n\t\tif (!bssid && !sdata->u.mgd.use_4addr)\n\t\t\treturn false;\n\t\tif (ieee80211_is_robust_mgmt_frame(skb) && !rx->sta)\n\t\t\treturn false;\n\t\tif (multicast)\n\t\t\treturn true;\n\t\treturn ether_addr_equal(sdata->vif.addr, hdr->addr1);\n\tcase NL80211_IFTYPE_ADHOC:\n\t\tif (!bssid)\n\t\t\treturn false;\n\t\tif (ether_addr_equal(sdata->vif.addr, hdr->addr2) ||\n\t\t    ether_addr_equal(sdata->u.ibss.bssid, hdr->addr2))\n\t\t\treturn false;\n\t\tif (ieee80211_is_beacon(hdr->frame_control))\n\t\t\treturn true;\n\t\tif (!ieee80211_bssid_match(bssid, sdata->u.ibss.bssid))\n\t\t\treturn false;\n\t\tif (!multicast &&\n\t\t    !ether_addr_equal(sdata->vif.addr, hdr->addr1))\n\t\t\treturn false;\n\t\tif (!rx->sta) {\n\t\t\tint rate_idx;\n\t\t\tif (status->encoding != RX_ENC_LEGACY)\n\t\t\t\trate_idx = 0; /* TODO: HT/VHT rates */\n\t\t\telse\n\t\t\t\trate_idx = status->rate_idx;\n\t\t\tieee80211_ibss_rx_no_sta(sdata, bssid, hdr->addr2,\n\t\t\t\t\t\t BIT(rate_idx));\n\t\t}\n\t\treturn true;\n\tcase NL80211_IFTYPE_OCB:\n\t\tif (!bssid)\n\t\t\treturn false;\n\t\tif (!ieee80211_is_data_present(hdr->frame_control))\n\t\t\treturn false;\n\t\tif (!is_broadcast_ether_addr(bssid))\n\t\t\treturn false;\n\t\tif (!multicast &&\n\t\t    !ether_addr_equal(sdata->dev->dev_addr, hdr->addr1))\n\t\t\treturn false;\n\t\tif (!rx->sta) {\n\t\t\tint rate_idx;\n\t\t\tif (status->encoding != RX_ENC_LEGACY)\n\t\t\t\trate_idx = 0; /* TODO: HT rates */\n\t\t\telse\n\t\t\t\trate_idx = status->rate_idx;\n\t\t\tieee80211_ocb_rx_no_sta(sdata, bssid, hdr->addr2,\n\t\t\t\t\t\tBIT(rate_idx));\n\t\t}\n\t\treturn true;\n\tcase NL80211_IFTYPE_MESH_POINT:\n\t\tif (ether_addr_equal(sdata->vif.addr, hdr->addr2))\n\t\t\treturn false;\n\t\tif (multicast)\n\t\t\treturn true;\n\t\treturn ether_addr_equal(sdata->vif.addr, hdr->addr1);\n\tcase NL80211_IFTYPE_AP_VLAN:\n\tcase NL80211_IFTYPE_AP:\n\t\tif (!bssid)\n\t\t\treturn ether_addr_equal(sdata->vif.addr, hdr->addr1);\n\n\t\tif (!ieee80211_bssid_match(bssid, sdata->vif.addr)) {\n\t\t\t/*\n\t\t\t * Accept public action frames even when the\n\t\t\t * BSSID doesn't match, this is used for P2P\n\t\t\t * and location updates. Note that mac80211\n\t\t\t * itself never looks at these frames.\n\t\t\t */\n\t\t\tif (!multicast &&\n\t\t\t    !ether_addr_equal(sdata->vif.addr, hdr->addr1))\n\t\t\t\treturn false;\n\t\t\tif (ieee80211_is_public_action(hdr, skb->len))\n\t\t\t\treturn true;\n\t\t\treturn ieee80211_is_beacon(hdr->frame_control);\n\t\t}\n\n\t\tif (!ieee80211_has_tods(hdr->frame_control)) {\n\t\t\t/* ignore data frames to TDLS-peers */\n\t\t\tif (ieee80211_is_data(hdr->frame_control))\n\t\t\t\treturn false;\n\t\t\t/* ignore action frames to TDLS-peers */\n\t\t\tif (ieee80211_is_action(hdr->frame_control) &&\n\t\t\t    !is_broadcast_ether_addr(bssid) &&\n\t\t\t    !ether_addr_equal(bssid, hdr->addr1))\n\t\t\t\treturn false;\n\t\t}\n\n\t\t/*\n\t\t * 802.11-2016 Table 9-26 says that for data frames, A1 must be\n\t\t * the BSSID - we've checked that already but may have accepted\n\t\t * the wildcard (ff:ff:ff:ff:ff:ff).\n\t\t *\n\t\t * It also says:\n\t\t *\tThe BSSID of the Data frame is determined as follows:\n\t\t *\ta) If the STA is contained within an AP or is associated\n\t\t *\t   with an AP, the BSSID is the address currently in use\n\t\t *\t   by the STA contained in the AP.\n\t\t *\n\t\t * So we should not accept data frames with an address that's\n\t\t * multicast.\n\t\t *\n\t\t * Accepting it also opens a security problem because stations\n\t\t * could encrypt it with the GTK and inject traffic that way.\n\t\t */\n\t\tif (ieee80211_is_data(hdr->frame_control) && multicast)\n\t\t\treturn false;\n\n\t\treturn true;\n\tcase NL80211_IFTYPE_P2P_DEVICE:\n\t\treturn ieee80211_is_public_action(hdr, skb->len) ||\n\t\t       ieee80211_is_probe_req(hdr->frame_control) ||\n\t\t       ieee80211_is_probe_resp(hdr->frame_control) ||\n\t\t       ieee80211_is_beacon(hdr->frame_control);\n\tcase NL80211_IFTYPE_NAN:\n\t\t/* Currently no frames on NAN interface are allowed */\n\t\treturn false;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tWARN_ON_ONCE(1);\n\treturn false;\n}\n\nvoid ieee80211_check_fast_rx(struct sta_info *sta)\n{\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct ieee80211_key *key;\n\tstruct ieee80211_fast_rx fastrx = {\n\t\t.dev = sdata->dev,\n\t\t.vif_type = sdata->vif.type,\n\t\t.control_port_protocol = sdata->control_port_protocol,\n\t}, *old, *new = NULL;\n\tbool set_offload = false;\n\tbool assign = false;\n\tbool offload;\n\n\t/* use sparse to check that we don't return without updating */\n\t__acquire(check_fast_rx);\n\n\tBUILD_BUG_ON(sizeof(fastrx.rfc1042_hdr) != sizeof(rfc1042_header));\n\tBUILD_BUG_ON(sizeof(fastrx.rfc1042_hdr) != ETH_ALEN);\n\tether_addr_copy(fastrx.rfc1042_hdr, rfc1042_header);\n\tether_addr_copy(fastrx.vif_addr, sdata->vif.addr);\n\n\tfastrx.uses_rss = ieee80211_hw_check(&local->hw, USES_RSS);\n\n\t/* fast-rx doesn't do reordering */\n\tif (ieee80211_hw_check(&local->hw, AMPDU_AGGREGATION) &&\n\t    !ieee80211_hw_check(&local->hw, SUPPORTS_REORDERING_BUFFER))\n\t\tgoto clear;\n\n\tswitch (sdata->vif.type) {\n\tcase NL80211_IFTYPE_STATION:\n\t\tif (sta->sta.tdls) {\n\t\t\tfastrx.da_offs = offsetof(struct ieee80211_hdr, addr1);\n\t\t\tfastrx.sa_offs = offsetof(struct ieee80211_hdr, addr2);\n\t\t\tfastrx.expected_ds_bits = 0;\n\t\t} else {\n\t\t\tfastrx.da_offs = offsetof(struct ieee80211_hdr, addr1);\n\t\t\tfastrx.sa_offs = offsetof(struct ieee80211_hdr, addr3);\n\t\t\tfastrx.expected_ds_bits =\n\t\t\t\tcpu_to_le16(IEEE80211_FCTL_FROMDS);\n\t\t}\n\n\t\tif (sdata->u.mgd.use_4addr && !sta->sta.tdls) {\n\t\t\tfastrx.expected_ds_bits |=\n\t\t\t\tcpu_to_le16(IEEE80211_FCTL_TODS);\n\t\t\tfastrx.da_offs = offsetof(struct ieee80211_hdr, addr3);\n\t\t\tfastrx.sa_offs = offsetof(struct ieee80211_hdr, addr4);\n\t\t}\n\n\t\tif (!sdata->u.mgd.powersave)\n\t\t\tbreak;\n\n\t\t/* software powersave is a huge mess, avoid all of it */\n\t\tif (ieee80211_hw_check(&local->hw, PS_NULLFUNC_STACK))\n\t\t\tgoto clear;\n\t\tif (ieee80211_hw_check(&local->hw, SUPPORTS_PS) &&\n\t\t    !ieee80211_hw_check(&local->hw, SUPPORTS_DYNAMIC_PS))\n\t\t\tgoto clear;\n\t\tbreak;\n\tcase NL80211_IFTYPE_AP_VLAN:\n\tcase NL80211_IFTYPE_AP:\n\t\t/* parallel-rx requires this, at least with calls to\n\t\t * ieee80211_sta_ps_transition()\n\t\t */\n\t\tif (!ieee80211_hw_check(&local->hw, AP_LINK_PS))\n\t\t\tgoto clear;\n\t\tfastrx.da_offs = offsetof(struct ieee80211_hdr, addr3);\n\t\tfastrx.sa_offs = offsetof(struct ieee80211_hdr, addr2);\n\t\tfastrx.expected_ds_bits = cpu_to_le16(IEEE80211_FCTL_TODS);\n\n\t\tfastrx.internal_forward =\n\t\t\t!(sdata->flags & IEEE80211_SDATA_DONT_BRIDGE_PACKETS) &&\n\t\t\t(sdata->vif.type != NL80211_IFTYPE_AP_VLAN ||\n\t\t\t !sdata->u.vlan.sta);\n\n\t\tif (sdata->vif.type == NL80211_IFTYPE_AP_VLAN &&\n\t\t    sdata->u.vlan.sta) {\n\t\t\tfastrx.expected_ds_bits |=\n\t\t\t\tcpu_to_le16(IEEE80211_FCTL_FROMDS);\n\t\t\tfastrx.sa_offs = offsetof(struct ieee80211_hdr, addr4);\n\t\t\tfastrx.internal_forward = 0;\n\t\t}\n\n\t\tbreak;\n\tdefault:\n\t\tgoto clear;\n\t}\n\n\tif (!test_sta_flag(sta, WLAN_STA_AUTHORIZED))\n\t\tgoto clear;\n\n\trcu_read_lock();\n\tkey = rcu_dereference(sta->ptk[sta->ptk_idx]);\n\tif (!key)\n\t\tkey = rcu_dereference(sdata->default_unicast_key);\n\tif (key) {\n\t\tswitch (key->conf.cipher) {\n\t\tcase WLAN_CIPHER_SUITE_TKIP:\n\t\t\t/* we don't want to deal with MMIC in fast-rx */\n\t\t\tgoto clear_rcu;\n\t\tcase WLAN_CIPHER_SUITE_CCMP:\n\t\tcase WLAN_CIPHER_SUITE_CCMP_256:\n\t\tcase WLAN_CIPHER_SUITE_GCMP:\n\t\tcase WLAN_CIPHER_SUITE_GCMP_256:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t/* We also don't want to deal with\n\t\t\t * WEP or cipher scheme.\n\t\t\t */\n\t\t\tgoto clear_rcu;\n\t\t}\n\n\t\tfastrx.key = true;\n\t\tfastrx.icv_len = key->conf.icv_len;\n\t}\n\n\tassign = true;\n clear_rcu:\n\trcu_read_unlock();\n clear:\n\t__release(check_fast_rx);\n\n\tif (assign)\n\t\tnew = kmemdup(&fastrx, sizeof(fastrx), GFP_KERNEL);\n\n\toffload = assign &&\n\t\t  (sdata->vif.offload_flags & IEEE80211_OFFLOAD_DECAP_ENABLED);\n\n\tif (offload)\n\t\tset_offload = !test_and_set_sta_flag(sta, WLAN_STA_DECAP_OFFLOAD);\n\telse\n\t\tset_offload = test_and_clear_sta_flag(sta, WLAN_STA_DECAP_OFFLOAD);\n\n\tif (set_offload)\n\t\tdrv_sta_set_decap_offload(local, sdata, &sta->sta, assign);\n\n\tspin_lock_bh(&sta->lock);\n\told = rcu_dereference_protected(sta->fast_rx, true);\n\trcu_assign_pointer(sta->fast_rx, new);\n\tspin_unlock_bh(&sta->lock);\n\n\tif (old)\n\t\tkfree_rcu(old, rcu_head);\n}\n\nvoid ieee80211_clear_fast_rx(struct sta_info *sta)\n{\n\tstruct ieee80211_fast_rx *old;\n\n\tspin_lock_bh(&sta->lock);\n\told = rcu_dereference_protected(sta->fast_rx, true);\n\tRCU_INIT_POINTER(sta->fast_rx, NULL);\n\tspin_unlock_bh(&sta->lock);\n\n\tif (old)\n\t\tkfree_rcu(old, rcu_head);\n}\n\nvoid __ieee80211_check_fast_rx_iface(struct ieee80211_sub_if_data *sdata)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sta_info *sta;\n\n\tlockdep_assert_held(&local->sta_mtx);\n\n\tlist_for_each_entry(sta, &local->sta_list, list) {\n\t\tif (sdata != sta->sdata &&\n\t\t    (!sta->sdata->bss || sta->sdata->bss != sdata->bss))\n\t\t\tcontinue;\n\t\tieee80211_check_fast_rx(sta);\n\t}\n}\n\nvoid ieee80211_check_fast_rx_iface(struct ieee80211_sub_if_data *sdata)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\n\tmutex_lock(&local->sta_mtx);\n\t__ieee80211_check_fast_rx_iface(sdata);\n\tmutex_unlock(&local->sta_mtx);\n}\n\nstatic void ieee80211_rx_8023(struct ieee80211_rx_data *rx,\n\t\t\t      struct ieee80211_fast_rx *fast_rx,\n\t\t\t      int orig_len)\n{\n\tstruct ieee80211_sta_rx_stats *stats;\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(rx->skb);\n\tstruct sta_info *sta = rx->sta;\n\tstruct sk_buff *skb = rx->skb;\n\tvoid *sa = skb->data + ETH_ALEN;\n\tvoid *da = skb->data;\n\n\tstats = &sta->rx_stats;\n\tif (fast_rx->uses_rss)\n\t\tstats = this_cpu_ptr(sta->pcpu_rx_stats);\n\n\t/* statistics part of ieee80211_rx_h_sta_process() */\n\tif (!(status->flag & RX_FLAG_NO_SIGNAL_VAL)) {\n\t\tstats->last_signal = status->signal;\n\t\tif (!fast_rx->uses_rss)\n\t\t\tewma_signal_add(&sta->rx_stats_avg.signal,\n\t\t\t\t\t-status->signal);\n\t}\n\n\tif (status->chains) {\n\t\tint i;\n\n\t\tstats->chains = status->chains;\n\t\tfor (i = 0; i < ARRAY_SIZE(status->chain_signal); i++) {\n\t\t\tint signal = status->chain_signal[i];\n\n\t\t\tif (!(status->chains & BIT(i)))\n\t\t\t\tcontinue;\n\n\t\t\tstats->chain_signal_last[i] = signal;\n\t\t\tif (!fast_rx->uses_rss)\n\t\t\t\tewma_signal_add(&sta->rx_stats_avg.chain_signal[i],\n\t\t\t\t\t\t-signal);\n\t\t}\n\t}\n\t/* end of statistics */\n\n\tstats->last_rx = jiffies;\n\tstats->last_rate = sta_stats_encode_rate(status);\n\n\tstats->fragments++;\n\tstats->packets++;\n\n\tskb->dev = fast_rx->dev;\n\n\tdev_sw_netstats_rx_add(fast_rx->dev, skb->len);\n\n\t/* The seqno index has the same property as needed\n\t * for the rx_msdu field, i.e. it is IEEE80211_NUM_TIDS\n\t * for non-QoS-data frames. Here we know it's a data\n\t * frame, so count MSDUs.\n\t */\n\tu64_stats_update_begin(&stats->syncp);\n\tstats->msdu[rx->seqno_idx]++;\n\tstats->bytes += orig_len;\n\tu64_stats_update_end(&stats->syncp);\n\n\tif (fast_rx->internal_forward) {\n\t\tstruct sk_buff *xmit_skb = NULL;\n\t\tif (is_multicast_ether_addr(da)) {\n\t\t\txmit_skb = skb_copy(skb, GFP_ATOMIC);\n\t\t} else if (!ether_addr_equal(da, sa) &&\n\t\t\t   sta_info_get(rx->sdata, da)) {\n\t\t\txmit_skb = skb;\n\t\t\tskb = NULL;\n\t\t}\n\n\t\tif (xmit_skb) {\n\t\t\t/*\n\t\t\t * Send to wireless media and increase priority by 256\n\t\t\t * to keep the received priority instead of\n\t\t\t * reclassifying the frame (see cfg80211_classify8021d).\n\t\t\t */\n\t\t\txmit_skb->priority += 256;\n\t\t\txmit_skb->protocol = htons(ETH_P_802_3);\n\t\t\tskb_reset_network_header(xmit_skb);\n\t\t\tskb_reset_mac_header(xmit_skb);\n\t\t\tdev_queue_xmit(xmit_skb);\n\t\t}\n\n\t\tif (!skb)\n\t\t\treturn;\n\t}\n\n\t/* deliver to local stack */\n\tskb->protocol = eth_type_trans(skb, fast_rx->dev);\n\tmemset(skb->cb, 0, sizeof(skb->cb));\n\tif (rx->list)\n\t\tlist_add_tail(&skb->list, rx->list);\n\telse\n\t\tnetif_receive_skb(skb);\n\n}\n\nstatic bool ieee80211_invoke_fast_rx(struct ieee80211_rx_data *rx,\n\t\t\t\t     struct ieee80211_fast_rx *fast_rx)\n{\n\tstruct sk_buff *skb = rx->skb;\n\tstruct ieee80211_hdr *hdr = (void *)skb->data;\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(skb);\n\tstruct sta_info *sta = rx->sta;\n\tint orig_len = skb->len;\n\tint hdrlen = ieee80211_hdrlen(hdr->frame_control);\n\tint snap_offs = hdrlen;\n\tstruct {\n\t\tu8 snap[sizeof(rfc1042_header)];\n\t\t__be16 proto;\n\t} *payload __aligned(2);\n\tstruct {\n\t\tu8 da[ETH_ALEN];\n\t\tu8 sa[ETH_ALEN];\n\t} addrs __aligned(2);\n\tstruct ieee80211_sta_rx_stats *stats = &sta->rx_stats;\n\n\t/* for parallel-rx, we need to have DUP_VALIDATED, otherwise we write\n\t * to a common data structure; drivers can implement that per queue\n\t * but we don't have that information in mac80211\n\t */\n\tif (!(status->flag & RX_FLAG_DUP_VALIDATED))\n\t\treturn false;\n\n#define FAST_RX_CRYPT_FLAGS\t(RX_FLAG_PN_VALIDATED | RX_FLAG_DECRYPTED)\n\n\t/* If using encryption, we also need to have:\n\t *  - PN_VALIDATED: similar, but the implementation is tricky\n\t *  - DECRYPTED: necessary for PN_VALIDATED\n\t */\n\tif (fast_rx->key &&\n\t    (status->flag & FAST_RX_CRYPT_FLAGS) != FAST_RX_CRYPT_FLAGS)\n\t\treturn false;\n\n\tif (unlikely(!ieee80211_is_data_present(hdr->frame_control)))\n\t\treturn false;\n\n\tif (unlikely(ieee80211_is_frag(hdr)))\n\t\treturn false;\n\n\t/* Since our interface address cannot be multicast, this\n\t * implicitly also rejects multicast frames without the\n\t * explicit check.\n\t *\n\t * We shouldn't get any *data* frames not addressed to us\n\t * (AP mode will accept multicast *management* frames), but\n\t * punting here will make it go through the full checks in\n\t * ieee80211_accept_frame().\n\t */\n\tif (!ether_addr_equal(fast_rx->vif_addr, hdr->addr1))\n\t\treturn false;\n\n\tif ((hdr->frame_control & cpu_to_le16(IEEE80211_FCTL_FROMDS |\n\t\t\t\t\t      IEEE80211_FCTL_TODS)) !=\n\t    fast_rx->expected_ds_bits)\n\t\treturn false;\n\n\t/* assign the key to drop unencrypted frames (later)\n\t * and strip the IV/MIC if necessary\n\t */\n\tif (fast_rx->key && !(status->flag & RX_FLAG_IV_STRIPPED)) {\n\t\t/* GCMP header length is the same */\n\t\tsnap_offs += IEEE80211_CCMP_HDR_LEN;\n\t}\n\n\tif (!(status->rx_flags & IEEE80211_RX_AMSDU)) {\n\t\tif (!pskb_may_pull(skb, snap_offs + sizeof(*payload)))\n\t\t\tgoto drop;\n\n\t\tpayload = (void *)(skb->data + snap_offs);\n\n\t\tif (!ether_addr_equal(payload->snap, fast_rx->rfc1042_hdr))\n\t\t\treturn false;\n\n\t\t/* Don't handle these here since they require special code.\n\t\t * Accept AARP and IPX even though they should come with a\n\t\t * bridge-tunnel header - but if we get them this way then\n\t\t * there's little point in discarding them.\n\t\t */\n\t\tif (unlikely(payload->proto == cpu_to_be16(ETH_P_TDLS) ||\n\t\t\t     payload->proto == fast_rx->control_port_protocol))\n\t\t\treturn false;\n\t}\n\n\t/* after this point, don't punt to the slowpath! */\n\n\tif (rx->key && !(status->flag & RX_FLAG_MIC_STRIPPED) &&\n\t    pskb_trim(skb, skb->len - fast_rx->icv_len))\n\t\tgoto drop;\n\n\tif (rx->key && !ieee80211_has_protected(hdr->frame_control))\n\t\tgoto drop;\n\n\tif (status->rx_flags & IEEE80211_RX_AMSDU) {\n\t\tif (__ieee80211_rx_h_amsdu(rx, snap_offs - hdrlen) !=\n\t\t    RX_QUEUED)\n\t\t\tgoto drop;\n\n\t\treturn true;\n\t}\n\n\t/* do the header conversion - first grab the addresses */\n\tether_addr_copy(addrs.da, skb->data + fast_rx->da_offs);\n\tether_addr_copy(addrs.sa, skb->data + fast_rx->sa_offs);\n\t/* remove the SNAP but leave the ethertype */\n\tskb_pull(skb, snap_offs + sizeof(rfc1042_header));\n\t/* push the addresses in front */\n\tmemcpy(skb_push(skb, sizeof(addrs)), &addrs, sizeof(addrs));\n\n\tieee80211_rx_8023(rx, fast_rx, orig_len);\n\n\treturn true;\n drop:\n\tdev_kfree_skb(skb);\n\tif (fast_rx->uses_rss)\n\t\tstats = this_cpu_ptr(sta->pcpu_rx_stats);\n\n\tstats->dropped++;\n\treturn true;\n}\n\n/*\n * This function returns whether or not the SKB\n * was destined for RX processing or not, which,\n * if consume is true, is equivalent to whether\n * or not the skb was consumed.\n */\nstatic bool ieee80211_prepare_and_rx_handle(struct ieee80211_rx_data *rx,\n\t\t\t\t\t    struct sk_buff *skb, bool consume)\n{\n\tstruct ieee80211_local *local = rx->local;\n\tstruct ieee80211_sub_if_data *sdata = rx->sdata;\n\n\trx->skb = skb;\n\n\t/* See if we can do fast-rx; if we have to copy we already lost,\n\t * so punt in that case. We should never have to deliver a data\n\t * frame to multiple interfaces anyway.\n\t *\n\t * We skip the ieee80211_accept_frame() call and do the necessary\n\t * checking inside ieee80211_invoke_fast_rx().\n\t */\n\tif (consume && rx->sta) {\n\t\tstruct ieee80211_fast_rx *fast_rx;\n\n\t\tfast_rx = rcu_dereference(rx->sta->fast_rx);\n\t\tif (fast_rx && ieee80211_invoke_fast_rx(rx, fast_rx))\n\t\t\treturn true;\n\t}\n\n\tif (!ieee80211_accept_frame(rx))\n\t\treturn false;\n\n\tif (!consume) {\n\t\tskb = skb_copy(skb, GFP_ATOMIC);\n\t\tif (!skb) {\n\t\t\tif (net_ratelimit())\n\t\t\t\twiphy_debug(local->hw.wiphy,\n\t\t\t\t\t\"failed to copy skb for %s\\n\",\n\t\t\t\t\tsdata->name);\n\t\t\treturn true;\n\t\t}\n\n\t\trx->skb = skb;\n\t}\n\n\tieee80211_invoke_rx_handlers(rx);\n\treturn true;\n}\n\nstatic void __ieee80211_rx_handle_8023(struct ieee80211_hw *hw,\n\t\t\t\t       struct ieee80211_sta *pubsta,\n\t\t\t\t       struct sk_buff *skb,\n\t\t\t\t       struct list_head *list)\n{\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\tstruct ieee80211_fast_rx *fast_rx;\n\tstruct ieee80211_rx_data rx;\n\n\tmemset(&rx, 0, sizeof(rx));\n\trx.skb = skb;\n\trx.local = local;\n\trx.list = list;\n\n\tI802_DEBUG_INC(local->dot11ReceivedFragmentCount);\n\n\t/* drop frame if too short for header */\n\tif (skb->len < sizeof(struct ethhdr))\n\t\tgoto drop;\n\n\tif (!pubsta)\n\t\tgoto drop;\n\n\trx.sta = container_of(pubsta, struct sta_info, sta);\n\trx.sdata = rx.sta->sdata;\n\n\tfast_rx = rcu_dereference(rx.sta->fast_rx);\n\tif (!fast_rx)\n\t\tgoto drop;\n\n\tieee80211_rx_8023(&rx, fast_rx, skb->len);\n\treturn;\n\ndrop:\n\tdev_kfree_skb(skb);\n}\n\n/*\n * This is the actual Rx frames handler. as it belongs to Rx path it must\n * be called with rcu_read_lock protection.\n */\nstatic void __ieee80211_rx_handle_packet(struct ieee80211_hw *hw,\n\t\t\t\t\t struct ieee80211_sta *pubsta,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t struct list_head *list)\n{\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\tstruct ieee80211_sub_if_data *sdata;\n\tstruct ieee80211_hdr *hdr;\n\t__le16 fc;\n\tstruct ieee80211_rx_data rx;\n\tstruct ieee80211_sub_if_data *prev;\n\tstruct rhlist_head *tmp;\n\tint err = 0;\n\n\tfc = ((struct ieee80211_hdr *)skb->data)->frame_control;\n\tmemset(&rx, 0, sizeof(rx));\n\trx.skb = skb;\n\trx.local = local;\n\trx.list = list;\n\n\tif (ieee80211_is_data(fc) || ieee80211_is_mgmt(fc))\n\t\tI802_DEBUG_INC(local->dot11ReceivedFragmentCount);\n\n\tif (ieee80211_is_mgmt(fc)) {\n\t\t/* drop frame if too short for header */\n\t\tif (skb->len < ieee80211_hdrlen(fc))\n\t\t\terr = -ENOBUFS;\n\t\telse\n\t\t\terr = skb_linearize(skb);\n\t} else {\n\t\terr = !pskb_may_pull(skb, ieee80211_hdrlen(fc));\n\t}\n\n\tif (err) {\n\t\tdev_kfree_skb(skb);\n\t\treturn;\n\t}\n\n\thdr = (struct ieee80211_hdr *)skb->data;\n\tieee80211_parse_qos(&rx);\n\tieee80211_verify_alignment(&rx);\n\n\tif (unlikely(ieee80211_is_probe_resp(hdr->frame_control) ||\n\t\t     ieee80211_is_beacon(hdr->frame_control) ||\n\t\t     ieee80211_is_s1g_beacon(hdr->frame_control)))\n\t\tieee80211_scan_rx(local, skb);\n\n\tif (ieee80211_is_data(fc)) {\n\t\tstruct sta_info *sta, *prev_sta;\n\n\t\tif (pubsta) {\n\t\t\trx.sta = container_of(pubsta, struct sta_info, sta);\n\t\t\trx.sdata = rx.sta->sdata;\n\t\t\tif (ieee80211_prepare_and_rx_handle(&rx, skb, true))\n\t\t\t\treturn;\n\t\t\tgoto out;\n\t\t}\n\n\t\tprev_sta = NULL;\n\n\t\tfor_each_sta_info(local, hdr->addr2, sta, tmp) {\n\t\t\tif (!prev_sta) {\n\t\t\t\tprev_sta = sta;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\trx.sta = prev_sta;\n\t\t\trx.sdata = prev_sta->sdata;\n\t\t\tieee80211_prepare_and_rx_handle(&rx, skb, false);\n\n\t\t\tprev_sta = sta;\n\t\t}\n\n\t\tif (prev_sta) {\n\t\t\trx.sta = prev_sta;\n\t\t\trx.sdata = prev_sta->sdata;\n\n\t\t\tif (ieee80211_prepare_and_rx_handle(&rx, skb, true))\n\t\t\t\treturn;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tprev = NULL;\n\n\tlist_for_each_entry_rcu(sdata, &local->interfaces, list) {\n\t\tif (!ieee80211_sdata_running(sdata))\n\t\t\tcontinue;\n\n\t\tif (sdata->vif.type == NL80211_IFTYPE_MONITOR ||\n\t\t    sdata->vif.type == NL80211_IFTYPE_AP_VLAN)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * frame is destined for this interface, but if it's\n\t\t * not also for the previous one we handle that after\n\t\t * the loop to avoid copying the SKB once too much\n\t\t */\n\n\t\tif (!prev) {\n\t\t\tprev = sdata;\n\t\t\tcontinue;\n\t\t}\n\n\t\trx.sta = sta_info_get_bss(prev, hdr->addr2);\n\t\trx.sdata = prev;\n\t\tieee80211_prepare_and_rx_handle(&rx, skb, false);\n\n\t\tprev = sdata;\n\t}\n\n\tif (prev) {\n\t\trx.sta = sta_info_get_bss(prev, hdr->addr2);\n\t\trx.sdata = prev;\n\n\t\tif (ieee80211_prepare_and_rx_handle(&rx, skb, true))\n\t\t\treturn;\n\t}\n\n out:\n\tdev_kfree_skb(skb);\n}\n\n/*\n * This is the receive path handler. It is called by a low level driver when an\n * 802.11 MPDU is received from the hardware.\n */\nvoid ieee80211_rx_list(struct ieee80211_hw *hw, struct ieee80211_sta *pubsta,\n\t\t       struct sk_buff *skb, struct list_head *list)\n{\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\tstruct ieee80211_rate *rate = NULL;\n\tstruct ieee80211_supported_band *sband;\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(skb);\n\n\tWARN_ON_ONCE(softirq_count() == 0);\n\n\tif (WARN_ON(status->band >= NUM_NL80211_BANDS))\n\t\tgoto drop;\n\n\tsband = local->hw.wiphy->bands[status->band];\n\tif (WARN_ON(!sband))\n\t\tgoto drop;\n\n\t/*\n\t * If we're suspending, it is possible although not too likely\n\t * that we'd be receiving frames after having already partially\n\t * quiesced the stack. We can't process such frames then since\n\t * that might, for example, cause stations to be added or other\n\t * driver callbacks be invoked.\n\t */\n\tif (unlikely(local->quiescing || local->suspended))\n\t\tgoto drop;\n\n\t/* We might be during a HW reconfig, prevent Rx for the same reason */\n\tif (unlikely(local->in_reconfig))\n\t\tgoto drop;\n\n\t/*\n\t * The same happens when we're not even started,\n\t * but that's worth a warning.\n\t */\n\tif (WARN_ON(!local->started))\n\t\tgoto drop;\n\n\tif (likely(!(status->flag & RX_FLAG_FAILED_PLCP_CRC))) {\n\t\t/*\n\t\t * Validate the rate, unless a PLCP error means that\n\t\t * we probably can't have a valid rate here anyway.\n\t\t */\n\n\t\tswitch (status->encoding) {\n\t\tcase RX_ENC_HT:\n\t\t\t/*\n\t\t\t * rate_idx is MCS index, which can be [0-76]\n\t\t\t * as documented on:\n\t\t\t *\n\t\t\t * https://wireless.wiki.kernel.org/en/developers/Documentation/ieee80211/802.11n\n\t\t\t *\n\t\t\t * Anything else would be some sort of driver or\n\t\t\t * hardware error. The driver should catch hardware\n\t\t\t * errors.\n\t\t\t */\n\t\t\tif (WARN(status->rate_idx > 76,\n\t\t\t\t \"Rate marked as an HT rate but passed \"\n\t\t\t\t \"status->rate_idx is not \"\n\t\t\t\t \"an MCS index [0-76]: %d (0x%02x)\\n\",\n\t\t\t\t status->rate_idx,\n\t\t\t\t status->rate_idx))\n\t\t\t\tgoto drop;\n\t\t\tbreak;\n\t\tcase RX_ENC_VHT:\n\t\t\tif (WARN_ONCE(status->rate_idx > 9 ||\n\t\t\t\t      !status->nss ||\n\t\t\t\t      status->nss > 8,\n\t\t\t\t      \"Rate marked as a VHT rate but data is invalid: MCS: %d, NSS: %d\\n\",\n\t\t\t\t      status->rate_idx, status->nss))\n\t\t\t\tgoto drop;\n\t\t\tbreak;\n\t\tcase RX_ENC_HE:\n\t\t\tif (WARN_ONCE(status->rate_idx > 11 ||\n\t\t\t\t      !status->nss ||\n\t\t\t\t      status->nss > 8,\n\t\t\t\t      \"Rate marked as an HE rate but data is invalid: MCS: %d, NSS: %d\\n\",\n\t\t\t\t      status->rate_idx, status->nss))\n\t\t\t\tgoto drop;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ON_ONCE(1);\n\t\t\tfallthrough;\n\t\tcase RX_ENC_LEGACY:\n\t\t\tif (WARN_ON(status->rate_idx >= sband->n_bitrates))\n\t\t\t\tgoto drop;\n\t\t\trate = &sband->bitrates[status->rate_idx];\n\t\t}\n\t}\n\n\tstatus->rx_flags = 0;\n\n\tkcov_remote_start_common(skb_get_kcov_handle(skb));\n\n\t/*\n\t * Frames with failed FCS/PLCP checksum are not returned,\n\t * all other frames are returned without radiotap header\n\t * if it was previously present.\n\t * Also, frames with less than 16 bytes are dropped.\n\t */\n\tif (!(status->flag & RX_FLAG_8023))\n\t\tskb = ieee80211_rx_monitor(local, skb, rate);\n\tif (skb) {\n\t\tieee80211_tpt_led_trig_rx(local,\n\t\t\t\t\t  ((struct ieee80211_hdr *)skb->data)->frame_control,\n\t\t\t\t\t  skb->len);\n\n\t\tif (status->flag & RX_FLAG_8023)\n\t\t\t__ieee80211_rx_handle_8023(hw, pubsta, skb, list);\n\t\telse\n\t\t\t__ieee80211_rx_handle_packet(hw, pubsta, skb, list);\n\t}\n\n\tkcov_remote_stop();\n\treturn;\n drop:\n\tkfree_skb(skb);\n}\nEXPORT_SYMBOL(ieee80211_rx_list);\n\nvoid ieee80211_rx_napi(struct ieee80211_hw *hw, struct ieee80211_sta *pubsta,\n\t\t       struct sk_buff *skb, struct napi_struct *napi)\n{\n\tstruct sk_buff *tmp;\n\tLIST_HEAD(list);\n\n\n\t/*\n\t * key references and virtual interfaces are protected using RCU\n\t * and this requires that we are in a read-side RCU section during\n\t * receive processing\n\t */\n\trcu_read_lock();\n\tieee80211_rx_list(hw, pubsta, skb, &list);\n\trcu_read_unlock();\n\n\tif (!napi) {\n\t\tnetif_receive_skb_list(&list);\n\t\treturn;\n\t}\n\n\tlist_for_each_entry_safe(skb, tmp, &list, list) {\n\t\tskb_list_del_init(skb);\n\t\tnapi_gro_receive(napi, skb);\n\t}\n}\nEXPORT_SYMBOL(ieee80211_rx_napi);\n\n/* This is a version of the rx handler that can be called from hard irq\n * context. Post the skb on the queue and schedule the tasklet */\nvoid ieee80211_rx_irqsafe(struct ieee80211_hw *hw, struct sk_buff *skb)\n{\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\n\tBUILD_BUG_ON(sizeof(struct ieee80211_rx_status) > sizeof(skb->cb));\n\n\tskb->pkt_type = IEEE80211_RX_MSG;\n\tskb_queue_tail(&local->skb_queue, skb);\n\ttasklet_schedule(&local->tasklet);\n}\nEXPORT_SYMBOL(ieee80211_rx_irqsafe);\n"}, "26": {"id": 26, "path": "/src/include/linux/rcupdate.h", "content": "/* SPDX-License-Identifier: GPL-2.0+ */\n/*\n * Read-Copy Update mechanism for mutual exclusion\n *\n * Copyright IBM Corporation, 2001\n *\n * Author: Dipankar Sarma <dipankar@in.ibm.com>\n *\n * Based on the original work by Paul McKenney <paulmck@vnet.ibm.com>\n * and inputs from Rusty Russell, Andrea Arcangeli and Andi Kleen.\n * Papers:\n * http://www.rdrop.com/users/paulmck/paper/rclockpdcsproof.pdf\n * http://lse.sourceforge.net/locking/rclock_OLS.2001.05.01c.sc.pdf (OLS2001)\n *\n * For detailed explanation of Read-Copy Update mechanism see -\n *\t\thttp://lse.sourceforge.net/locking/rcupdate.html\n *\n */\n\n#ifndef __LINUX_RCUPDATE_H\n#define __LINUX_RCUPDATE_H\n\n#include <linux/types.h>\n#include <linux/compiler.h>\n#include <linux/atomic.h>\n#include <linux/irqflags.h>\n#include <linux/preempt.h>\n#include <linux/bottom_half.h>\n#include <linux/lockdep.h>\n#include <asm/processor.h>\n#include <linux/cpumask.h>\n\n#define ULONG_CMP_GE(a, b)\t(ULONG_MAX / 2 >= (a) - (b))\n#define ULONG_CMP_LT(a, b)\t(ULONG_MAX / 2 < (a) - (b))\n#define ulong2long(a)\t\t(*(long *)(&(a)))\n#define USHORT_CMP_GE(a, b)\t(USHRT_MAX / 2 >= (unsigned short)((a) - (b)))\n#define USHORT_CMP_LT(a, b)\t(USHRT_MAX / 2 < (unsigned short)((a) - (b)))\n\n/* Exported common interfaces */\nvoid call_rcu(struct rcu_head *head, rcu_callback_t func);\nvoid rcu_barrier_tasks(void);\nvoid rcu_barrier_tasks_rude(void);\nvoid synchronize_rcu(void);\n\n#ifdef CONFIG_PREEMPT_RCU\n\nvoid __rcu_read_lock(void);\nvoid __rcu_read_unlock(void);\n\n/*\n * Defined as a macro as it is a very low level header included from\n * areas that don't even know about current.  This gives the rcu_read_lock()\n * nesting depth, but makes sense only if CONFIG_PREEMPT_RCU -- in other\n * types of kernel builds, the rcu_read_lock() nesting depth is unknowable.\n */\n#define rcu_preempt_depth() (current->rcu_read_lock_nesting)\n\n#else /* #ifdef CONFIG_PREEMPT_RCU */\n\n#ifdef CONFIG_TINY_RCU\n#define rcu_read_unlock_strict() do { } while (0)\n#else\nvoid rcu_read_unlock_strict(void);\n#endif\n\nstatic inline void __rcu_read_lock(void)\n{\n\tpreempt_disable();\n}\n\nstatic inline void __rcu_read_unlock(void)\n{\n\tpreempt_enable();\n\trcu_read_unlock_strict();\n}\n\nstatic inline int rcu_preempt_depth(void)\n{\n\treturn 0;\n}\n\n#endif /* #else #ifdef CONFIG_PREEMPT_RCU */\n\n/* Internal to kernel */\nvoid rcu_init(void);\nextern int rcu_scheduler_active __read_mostly;\nvoid rcu_sched_clock_irq(int user);\nvoid rcu_report_dead(unsigned int cpu);\nvoid rcutree_migrate_callbacks(int cpu);\n\n#ifdef CONFIG_TASKS_RCU_GENERIC\nvoid rcu_init_tasks_generic(void);\n#else\nstatic inline void rcu_init_tasks_generic(void) { }\n#endif\n\n#ifdef CONFIG_RCU_STALL_COMMON\nvoid rcu_sysrq_start(void);\nvoid rcu_sysrq_end(void);\n#else /* #ifdef CONFIG_RCU_STALL_COMMON */\nstatic inline void rcu_sysrq_start(void) { }\nstatic inline void rcu_sysrq_end(void) { }\n#endif /* #else #ifdef CONFIG_RCU_STALL_COMMON */\n\n#ifdef CONFIG_NO_HZ_FULL\nvoid rcu_user_enter(void);\nvoid rcu_user_exit(void);\n#else\nstatic inline void rcu_user_enter(void) { }\nstatic inline void rcu_user_exit(void) { }\n#endif /* CONFIG_NO_HZ_FULL */\n\n#ifdef CONFIG_RCU_NOCB_CPU\nvoid rcu_init_nohz(void);\nint rcu_nocb_cpu_offload(int cpu);\nint rcu_nocb_cpu_deoffload(int cpu);\nvoid rcu_nocb_flush_deferred_wakeup(void);\n#else /* #ifdef CONFIG_RCU_NOCB_CPU */\nstatic inline void rcu_init_nohz(void) { }\nstatic inline int rcu_nocb_cpu_offload(int cpu) { return -EINVAL; }\nstatic inline int rcu_nocb_cpu_deoffload(int cpu) { return 0; }\nstatic inline void rcu_nocb_flush_deferred_wakeup(void) { }\n#endif /* #else #ifdef CONFIG_RCU_NOCB_CPU */\n\n/**\n * RCU_NONIDLE - Indicate idle-loop code that needs RCU readers\n * @a: Code that RCU needs to pay attention to.\n *\n * RCU read-side critical sections are forbidden in the inner idle loop,\n * that is, between the rcu_idle_enter() and the rcu_idle_exit() -- RCU\n * will happily ignore any such read-side critical sections.  However,\n * things like powertop need tracepoints in the inner idle loop.\n *\n * This macro provides the way out:  RCU_NONIDLE(do_something_with_RCU())\n * will tell RCU that it needs to pay attention, invoke its argument\n * (in this example, calling the do_something_with_RCU() function),\n * and then tell RCU to go back to ignoring this CPU.  It is permissible\n * to nest RCU_NONIDLE() wrappers, but not indefinitely (but the limit is\n * on the order of a million or so, even on 32-bit systems).  It is\n * not legal to block within RCU_NONIDLE(), nor is it permissible to\n * transfer control either into or out of RCU_NONIDLE()'s statement.\n */\n#define RCU_NONIDLE(a) \\\n\tdo { \\\n\t\trcu_irq_enter_irqson(); \\\n\t\tdo { a; } while (0); \\\n\t\trcu_irq_exit_irqson(); \\\n\t} while (0)\n\n/*\n * Note a quasi-voluntary context switch for RCU-tasks's benefit.\n * This is a macro rather than an inline function to avoid #include hell.\n */\n#ifdef CONFIG_TASKS_RCU_GENERIC\n\n# ifdef CONFIG_TASKS_RCU\n# define rcu_tasks_classic_qs(t, preempt)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (!(preempt) && READ_ONCE((t)->rcu_tasks_holdout))\t\\\n\t\t\tWRITE_ONCE((t)->rcu_tasks_holdout, false);\t\\\n\t} while (0)\nvoid call_rcu_tasks(struct rcu_head *head, rcu_callback_t func);\nvoid synchronize_rcu_tasks(void);\n# else\n# define rcu_tasks_classic_qs(t, preempt) do { } while (0)\n# define call_rcu_tasks call_rcu\n# define synchronize_rcu_tasks synchronize_rcu\n# endif\n\n# ifdef CONFIG_TASKS_RCU_TRACE\n# define rcu_tasks_trace_qs(t)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (!likely(READ_ONCE((t)->trc_reader_checked)) &&\t\\\n\t\t    !unlikely(READ_ONCE((t)->trc_reader_nesting))) {\t\\\n\t\t\tsmp_store_release(&(t)->trc_reader_checked, true); \\\n\t\t\tsmp_mb(); /* Readers partitioned by store. */\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n# else\n# define rcu_tasks_trace_qs(t) do { } while (0)\n# endif\n\n#define rcu_tasks_qs(t, preempt)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\trcu_tasks_classic_qs((t), (preempt));\t\t\t\t\\\n\trcu_tasks_trace_qs((t));\t\t\t\t\t\\\n} while (0)\n\n# ifdef CONFIG_TASKS_RUDE_RCU\nvoid call_rcu_tasks_rude(struct rcu_head *head, rcu_callback_t func);\nvoid synchronize_rcu_tasks_rude(void);\n# endif\n\n#define rcu_note_voluntary_context_switch(t) rcu_tasks_qs(t, false)\nvoid exit_tasks_rcu_start(void);\nvoid exit_tasks_rcu_finish(void);\n#else /* #ifdef CONFIG_TASKS_RCU_GENERIC */\n#define rcu_tasks_qs(t, preempt) do { } while (0)\n#define rcu_note_voluntary_context_switch(t) do { } while (0)\n#define call_rcu_tasks call_rcu\n#define synchronize_rcu_tasks synchronize_rcu\nstatic inline void exit_tasks_rcu_start(void) { }\nstatic inline void exit_tasks_rcu_finish(void) { }\n#endif /* #else #ifdef CONFIG_TASKS_RCU_GENERIC */\n\n/**\n * cond_resched_tasks_rcu_qs - Report potential quiescent states to RCU\n *\n * This macro resembles cond_resched(), except that it is defined to\n * report potential quiescent states to RCU-tasks even if the cond_resched()\n * machinery were to be shut off, as some advocate for PREEMPTION kernels.\n */\n#define cond_resched_tasks_rcu_qs() \\\ndo { \\\n\trcu_tasks_qs(current, false); \\\n\tcond_resched(); \\\n} while (0)\n\n/*\n * Infrastructure to implement the synchronize_() primitives in\n * TREE_RCU and rcu_barrier_() primitives in TINY_RCU.\n */\n\n#if defined(CONFIG_TREE_RCU)\n#include <linux/rcutree.h>\n#elif defined(CONFIG_TINY_RCU)\n#include <linux/rcutiny.h>\n#else\n#error \"Unknown RCU implementation specified to kernel configuration\"\n#endif\n\n/*\n * The init_rcu_head_on_stack() and destroy_rcu_head_on_stack() calls\n * are needed for dynamic initialization and destruction of rcu_head\n * on the stack, and init_rcu_head()/destroy_rcu_head() are needed for\n * dynamic initialization and destruction of statically allocated rcu_head\n * structures.  However, rcu_head structures allocated dynamically in the\n * heap don't need any initialization.\n */\n#ifdef CONFIG_DEBUG_OBJECTS_RCU_HEAD\nvoid init_rcu_head(struct rcu_head *head);\nvoid destroy_rcu_head(struct rcu_head *head);\nvoid init_rcu_head_on_stack(struct rcu_head *head);\nvoid destroy_rcu_head_on_stack(struct rcu_head *head);\n#else /* !CONFIG_DEBUG_OBJECTS_RCU_HEAD */\nstatic inline void init_rcu_head(struct rcu_head *head) { }\nstatic inline void destroy_rcu_head(struct rcu_head *head) { }\nstatic inline void init_rcu_head_on_stack(struct rcu_head *head) { }\nstatic inline void destroy_rcu_head_on_stack(struct rcu_head *head) { }\n#endif\t/* #else !CONFIG_DEBUG_OBJECTS_RCU_HEAD */\n\n#if defined(CONFIG_HOTPLUG_CPU) && defined(CONFIG_PROVE_RCU)\nbool rcu_lockdep_current_cpu_online(void);\n#else /* #if defined(CONFIG_HOTPLUG_CPU) && defined(CONFIG_PROVE_RCU) */\nstatic inline bool rcu_lockdep_current_cpu_online(void) { return true; }\n#endif /* #else #if defined(CONFIG_HOTPLUG_CPU) && defined(CONFIG_PROVE_RCU) */\n\nextern struct lockdep_map rcu_lock_map;\nextern struct lockdep_map rcu_bh_lock_map;\nextern struct lockdep_map rcu_sched_lock_map;\nextern struct lockdep_map rcu_callback_map;\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\nstatic inline void rcu_lock_acquire(struct lockdep_map *map)\n{\n\tlock_acquire(map, 0, 0, 2, 0, NULL, _THIS_IP_);\n}\n\nstatic inline void rcu_lock_release(struct lockdep_map *map)\n{\n\tlock_release(map, _THIS_IP_);\n}\n\nint debug_lockdep_rcu_enabled(void);\nint rcu_read_lock_held(void);\nint rcu_read_lock_bh_held(void);\nint rcu_read_lock_sched_held(void);\nint rcu_read_lock_any_held(void);\n\n#else /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */\n\n# define rcu_lock_acquire(a)\t\tdo { } while (0)\n# define rcu_lock_release(a)\t\tdo { } while (0)\n\nstatic inline int rcu_read_lock_held(void)\n{\n\treturn 1;\n}\n\nstatic inline int rcu_read_lock_bh_held(void)\n{\n\treturn 1;\n}\n\nstatic inline int rcu_read_lock_sched_held(void)\n{\n\treturn !preemptible();\n}\n\nstatic inline int rcu_read_lock_any_held(void)\n{\n\treturn !preemptible();\n}\n\n#endif /* #else #ifdef CONFIG_DEBUG_LOCK_ALLOC */\n\n#ifdef CONFIG_PROVE_RCU\n\n/**\n * RCU_LOCKDEP_WARN - emit lockdep splat if specified condition is met\n * @c: condition to check\n * @s: informative message\n */\n#define RCU_LOCKDEP_WARN(c, s)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tstatic bool __section(\".data.unlikely\") __warned;\t\\\n\t\tif (debug_lockdep_rcu_enabled() && !__warned && (c)) {\t\\\n\t\t\t__warned = true;\t\t\t\t\\\n\t\t\tlockdep_rcu_suspicious(__FILE__, __LINE__, s);\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n#if defined(CONFIG_PROVE_RCU) && !defined(CONFIG_PREEMPT_RCU)\nstatic inline void rcu_preempt_sleep_check(void)\n{\n\tRCU_LOCKDEP_WARN(lock_is_held(&rcu_lock_map),\n\t\t\t \"Illegal context switch in RCU read-side critical section\");\n}\n#else /* #ifdef CONFIG_PROVE_RCU */\nstatic inline void rcu_preempt_sleep_check(void) { }\n#endif /* #else #ifdef CONFIG_PROVE_RCU */\n\n#define rcu_sleep_check()\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\trcu_preempt_sleep_check();\t\t\t\t\\\n\t\tRCU_LOCKDEP_WARN(lock_is_held(&rcu_bh_lock_map),\t\\\n\t\t\t\t \"Illegal context switch in RCU-bh read-side critical section\"); \\\n\t\tRCU_LOCKDEP_WARN(lock_is_held(&rcu_sched_lock_map),\t\\\n\t\t\t\t \"Illegal context switch in RCU-sched read-side critical section\"); \\\n\t} while (0)\n\n#else /* #ifdef CONFIG_PROVE_RCU */\n\n#define RCU_LOCKDEP_WARN(c, s) do { } while (0 && (c))\n#define rcu_sleep_check() do { } while (0)\n\n#endif /* #else #ifdef CONFIG_PROVE_RCU */\n\n/*\n * Helper functions for rcu_dereference_check(), rcu_dereference_protected()\n * and rcu_assign_pointer().  Some of these could be folded into their\n * callers, but they are left separate in order to ease introduction of\n * multiple pointers markings to match different RCU implementations\n * (e.g., __srcu), should this make sense in the future.\n */\n\n#ifdef __CHECKER__\n#define rcu_check_sparse(p, space) \\\n\t((void)(((typeof(*p) space *)p) == p))\n#else /* #ifdef __CHECKER__ */\n#define rcu_check_sparse(p, space)\n#endif /* #else #ifdef __CHECKER__ */\n\n#define __rcu_access_pointer(p, space) \\\n({ \\\n\ttypeof(*p) *_________p1 = (typeof(*p) *__force)READ_ONCE(p); \\\n\trcu_check_sparse(p, space); \\\n\t((typeof(*p) __force __kernel *)(_________p1)); \\\n})\n#define __rcu_dereference_check(p, c, space) \\\n({ \\\n\t/* Dependency order vs. p above. */ \\\n\ttypeof(*p) *________p1 = (typeof(*p) *__force)READ_ONCE(p); \\\n\tRCU_LOCKDEP_WARN(!(c), \"suspicious rcu_dereference_check() usage\"); \\\n\trcu_check_sparse(p, space); \\\n\t((typeof(*p) __force __kernel *)(________p1)); \\\n})\n#define __rcu_dereference_protected(p, c, space) \\\n({ \\\n\tRCU_LOCKDEP_WARN(!(c), \"suspicious rcu_dereference_protected() usage\"); \\\n\trcu_check_sparse(p, space); \\\n\t((typeof(*p) __force __kernel *)(p)); \\\n})\n#define rcu_dereference_raw(p) \\\n({ \\\n\t/* Dependency order vs. p above. */ \\\n\ttypeof(p) ________p1 = READ_ONCE(p); \\\n\t((typeof(*p) __force __kernel *)(________p1)); \\\n})\n\n/**\n * RCU_INITIALIZER() - statically initialize an RCU-protected global variable\n * @v: The value to statically initialize with.\n */\n#define RCU_INITIALIZER(v) (typeof(*(v)) __force __rcu *)(v)\n\n/**\n * rcu_assign_pointer() - assign to RCU-protected pointer\n * @p: pointer to assign to\n * @v: value to assign (publish)\n *\n * Assigns the specified value to the specified RCU-protected\n * pointer, ensuring that any concurrent RCU readers will see\n * any prior initialization.\n *\n * Inserts memory barriers on architectures that require them\n * (which is most of them), and also prevents the compiler from\n * reordering the code that initializes the structure after the pointer\n * assignment.  More importantly, this call documents which pointers\n * will be dereferenced by RCU read-side code.\n *\n * In some special cases, you may use RCU_INIT_POINTER() instead\n * of rcu_assign_pointer().  RCU_INIT_POINTER() is a bit faster due\n * to the fact that it does not constrain either the CPU or the compiler.\n * That said, using RCU_INIT_POINTER() when you should have used\n * rcu_assign_pointer() is a very bad thing that results in\n * impossible-to-diagnose memory corruption.  So please be careful.\n * See the RCU_INIT_POINTER() comment header for details.\n *\n * Note that rcu_assign_pointer() evaluates each of its arguments only\n * once, appearances notwithstanding.  One of the \"extra\" evaluations\n * is in typeof() and the other visible only to sparse (__CHECKER__),\n * neither of which actually execute the argument.  As with most cpp\n * macros, this execute-arguments-only-once property is important, so\n * please be careful when making changes to rcu_assign_pointer() and the\n * other macros that it invokes.\n */\n#define rcu_assign_pointer(p, v)\t\t\t\t\t      \\\ndo {\t\t\t\t\t\t\t\t\t      \\\n\tuintptr_t _r_a_p__v = (uintptr_t)(v);\t\t\t\t      \\\n\trcu_check_sparse(p, __rcu);\t\t\t\t\t      \\\n\t\t\t\t\t\t\t\t\t      \\\n\tif (__builtin_constant_p(v) && (_r_a_p__v) == (uintptr_t)NULL)\t      \\\n\t\tWRITE_ONCE((p), (typeof(p))(_r_a_p__v));\t\t      \\\n\telse\t\t\t\t\t\t\t\t      \\\n\t\tsmp_store_release(&p, RCU_INITIALIZER((typeof(p))_r_a_p__v)); \\\n} while (0)\n\n/**\n * rcu_replace_pointer() - replace an RCU pointer, returning its old value\n * @rcu_ptr: RCU pointer, whose old value is returned\n * @ptr: regular pointer\n * @c: the lockdep conditions under which the dereference will take place\n *\n * Perform a replacement, where @rcu_ptr is an RCU-annotated\n * pointer and @c is the lockdep argument that is passed to the\n * rcu_dereference_protected() call used to read that pointer.  The old\n * value of @rcu_ptr is returned, and @rcu_ptr is set to @ptr.\n */\n#define rcu_replace_pointer(rcu_ptr, ptr, c)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\ttypeof(ptr) __tmp = rcu_dereference_protected((rcu_ptr), (c));\t\\\n\trcu_assign_pointer((rcu_ptr), (ptr));\t\t\t\t\\\n\t__tmp;\t\t\t\t\t\t\t\t\\\n})\n\n/**\n * rcu_access_pointer() - fetch RCU pointer with no dereferencing\n * @p: The pointer to read\n *\n * Return the value of the specified RCU-protected pointer, but omit the\n * lockdep checks for being in an RCU read-side critical section.  This is\n * useful when the value of this pointer is accessed, but the pointer is\n * not dereferenced, for example, when testing an RCU-protected pointer\n * against NULL.  Although rcu_access_pointer() may also be used in cases\n * where update-side locks prevent the value of the pointer from changing,\n * you should instead use rcu_dereference_protected() for this use case.\n *\n * It is also permissible to use rcu_access_pointer() when read-side\n * access to the pointer was removed at least one grace period ago, as\n * is the case in the context of the RCU callback that is freeing up\n * the data, or after a synchronize_rcu() returns.  This can be useful\n * when tearing down multi-linked structures after a grace period\n * has elapsed.\n */\n#define rcu_access_pointer(p) __rcu_access_pointer((p), __rcu)\n\n/**\n * rcu_dereference_check() - rcu_dereference with debug checking\n * @p: The pointer to read, prior to dereferencing\n * @c: The conditions under which the dereference will take place\n *\n * Do an rcu_dereference(), but check that the conditions under which the\n * dereference will take place are correct.  Typically the conditions\n * indicate the various locking conditions that should be held at that\n * point.  The check should return true if the conditions are satisfied.\n * An implicit check for being in an RCU read-side critical section\n * (rcu_read_lock()) is included.\n *\n * For example:\n *\n *\tbar = rcu_dereference_check(foo->bar, lockdep_is_held(&foo->lock));\n *\n * could be used to indicate to lockdep that foo->bar may only be dereferenced\n * if either rcu_read_lock() is held, or that the lock required to replace\n * the bar struct at foo->bar is held.\n *\n * Note that the list of conditions may also include indications of when a lock\n * need not be held, for example during initialisation or destruction of the\n * target struct:\n *\n *\tbar = rcu_dereference_check(foo->bar, lockdep_is_held(&foo->lock) ||\n *\t\t\t\t\t      atomic_read(&foo->usage) == 0);\n *\n * Inserts memory barriers on architectures that require them\n * (currently only the Alpha), prevents the compiler from refetching\n * (and from merging fetches), and, more importantly, documents exactly\n * which pointers are protected by RCU and checks that the pointer is\n * annotated as __rcu.\n */\n#define rcu_dereference_check(p, c) \\\n\t__rcu_dereference_check((p), (c) || rcu_read_lock_held(), __rcu)\n\n/**\n * rcu_dereference_bh_check() - rcu_dereference_bh with debug checking\n * @p: The pointer to read, prior to dereferencing\n * @c: The conditions under which the dereference will take place\n *\n * This is the RCU-bh counterpart to rcu_dereference_check().\n */\n#define rcu_dereference_bh_check(p, c) \\\n\t__rcu_dereference_check((p), (c) || rcu_read_lock_bh_held(), __rcu)\n\n/**\n * rcu_dereference_sched_check() - rcu_dereference_sched with debug checking\n * @p: The pointer to read, prior to dereferencing\n * @c: The conditions under which the dereference will take place\n *\n * This is the RCU-sched counterpart to rcu_dereference_check().\n */\n#define rcu_dereference_sched_check(p, c) \\\n\t__rcu_dereference_check((p), (c) || rcu_read_lock_sched_held(), \\\n\t\t\t\t__rcu)\n\n/*\n * The tracing infrastructure traces RCU (we want that), but unfortunately\n * some of the RCU checks causes tracing to lock up the system.\n *\n * The no-tracing version of rcu_dereference_raw() must not call\n * rcu_read_lock_held().\n */\n#define rcu_dereference_raw_check(p) __rcu_dereference_check((p), 1, __rcu)\n\n/**\n * rcu_dereference_protected() - fetch RCU pointer when updates prevented\n * @p: The pointer to read, prior to dereferencing\n * @c: The conditions under which the dereference will take place\n *\n * Return the value of the specified RCU-protected pointer, but omit\n * the READ_ONCE().  This is useful in cases where update-side locks\n * prevent the value of the pointer from changing.  Please note that this\n * primitive does *not* prevent the compiler from repeating this reference\n * or combining it with other references, so it should not be used without\n * protection of appropriate locks.\n *\n * This function is only for update-side use.  Using this function\n * when protected only by rcu_read_lock() will result in infrequent\n * but very ugly failures.\n */\n#define rcu_dereference_protected(p, c) \\\n\t__rcu_dereference_protected((p), (c), __rcu)\n\n\n/**\n * rcu_dereference() - fetch RCU-protected pointer for dereferencing\n * @p: The pointer to read, prior to dereferencing\n *\n * This is a simple wrapper around rcu_dereference_check().\n */\n#define rcu_dereference(p) rcu_dereference_check(p, 0)\n\n/**\n * rcu_dereference_bh() - fetch an RCU-bh-protected pointer for dereferencing\n * @p: The pointer to read, prior to dereferencing\n *\n * Makes rcu_dereference_check() do the dirty work.\n */\n#define rcu_dereference_bh(p) rcu_dereference_bh_check(p, 0)\n\n/**\n * rcu_dereference_sched() - fetch RCU-sched-protected pointer for dereferencing\n * @p: The pointer to read, prior to dereferencing\n *\n * Makes rcu_dereference_check() do the dirty work.\n */\n#define rcu_dereference_sched(p) rcu_dereference_sched_check(p, 0)\n\n/**\n * rcu_pointer_handoff() - Hand off a pointer from RCU to other mechanism\n * @p: The pointer to hand off\n *\n * This is simply an identity function, but it documents where a pointer\n * is handed off from RCU to some other synchronization mechanism, for\n * example, reference counting or locking.  In C11, it would map to\n * kill_dependency().  It could be used as follows::\n *\n *\trcu_read_lock();\n *\tp = rcu_dereference(gp);\n *\tlong_lived = is_long_lived(p);\n *\tif (long_lived) {\n *\t\tif (!atomic_inc_not_zero(p->refcnt))\n *\t\t\tlong_lived = false;\n *\t\telse\n *\t\t\tp = rcu_pointer_handoff(p);\n *\t}\n *\trcu_read_unlock();\n */\n#define rcu_pointer_handoff(p) (p)\n\n/**\n * rcu_read_lock() - mark the beginning of an RCU read-side critical section\n *\n * When synchronize_rcu() is invoked on one CPU while other CPUs\n * are within RCU read-side critical sections, then the\n * synchronize_rcu() is guaranteed to block until after all the other\n * CPUs exit their critical sections.  Similarly, if call_rcu() is invoked\n * on one CPU while other CPUs are within RCU read-side critical\n * sections, invocation of the corresponding RCU callback is deferred\n * until after the all the other CPUs exit their critical sections.\n *\n * Note, however, that RCU callbacks are permitted to run concurrently\n * with new RCU read-side critical sections.  One way that this can happen\n * is via the following sequence of events: (1) CPU 0 enters an RCU\n * read-side critical section, (2) CPU 1 invokes call_rcu() to register\n * an RCU callback, (3) CPU 0 exits the RCU read-side critical section,\n * (4) CPU 2 enters a RCU read-side critical section, (5) the RCU\n * callback is invoked.  This is legal, because the RCU read-side critical\n * section that was running concurrently with the call_rcu() (and which\n * therefore might be referencing something that the corresponding RCU\n * callback would free up) has completed before the corresponding\n * RCU callback is invoked.\n *\n * RCU read-side critical sections may be nested.  Any deferred actions\n * will be deferred until the outermost RCU read-side critical section\n * completes.\n *\n * You can avoid reading and understanding the next paragraph by\n * following this rule: don't put anything in an rcu_read_lock() RCU\n * read-side critical section that would block in a !PREEMPTION kernel.\n * But if you want the full story, read on!\n *\n * In non-preemptible RCU implementations (pure TREE_RCU and TINY_RCU),\n * it is illegal to block while in an RCU read-side critical section.\n * In preemptible RCU implementations (PREEMPT_RCU) in CONFIG_PREEMPTION\n * kernel builds, RCU read-side critical sections may be preempted,\n * but explicit blocking is illegal.  Finally, in preemptible RCU\n * implementations in real-time (with -rt patchset) kernel builds, RCU\n * read-side critical sections may be preempted and they may also block, but\n * only when acquiring spinlocks that are subject to priority inheritance.\n */\nstatic __always_inline void rcu_read_lock(void)\n{\n\t__rcu_read_lock();\n\t__acquire(RCU);\n\trcu_lock_acquire(&rcu_lock_map);\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(),\n\t\t\t \"rcu_read_lock() used illegally while idle\");\n}\n\n/*\n * So where is rcu_write_lock()?  It does not exist, as there is no\n * way for writers to lock out RCU readers.  This is a feature, not\n * a bug -- this property is what provides RCU's performance benefits.\n * Of course, writers must coordinate with each other.  The normal\n * spinlock primitives work well for this, but any other technique may be\n * used as well.  RCU does not care how the writers keep out of each\n * others' way, as long as they do so.\n */\n\n/**\n * rcu_read_unlock() - marks the end of an RCU read-side critical section.\n *\n * In most situations, rcu_read_unlock() is immune from deadlock.\n * However, in kernels built with CONFIG_RCU_BOOST, rcu_read_unlock()\n * is responsible for deboosting, which it does via rt_mutex_unlock().\n * Unfortunately, this function acquires the scheduler's runqueue and\n * priority-inheritance spinlocks.  This means that deadlock could result\n * if the caller of rcu_read_unlock() already holds one of these locks or\n * any lock that is ever acquired while holding them.\n *\n * That said, RCU readers are never priority boosted unless they were\n * preempted.  Therefore, one way to avoid deadlock is to make sure\n * that preemption never happens within any RCU read-side critical\n * section whose outermost rcu_read_unlock() is called with one of\n * rt_mutex_unlock()'s locks held.  Such preemption can be avoided in\n * a number of ways, for example, by invoking preempt_disable() before\n * critical section's outermost rcu_read_lock().\n *\n * Given that the set of locks acquired by rt_mutex_unlock() might change\n * at any time, a somewhat more future-proofed approach is to make sure\n * that that preemption never happens within any RCU read-side critical\n * section whose outermost rcu_read_unlock() is called with irqs disabled.\n * This approach relies on the fact that rt_mutex_unlock() currently only\n * acquires irq-disabled locks.\n *\n * The second of these two approaches is best in most situations,\n * however, the first approach can also be useful, at least to those\n * developers willing to keep abreast of the set of locks acquired by\n * rt_mutex_unlock().\n *\n * See rcu_read_lock() for more information.\n */\nstatic inline void rcu_read_unlock(void)\n{\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(),\n\t\t\t \"rcu_read_unlock() used illegally while idle\");\n\t__release(RCU);\n\t__rcu_read_unlock();\n\trcu_lock_release(&rcu_lock_map); /* Keep acq info for rls diags. */\n}\n\n/**\n * rcu_read_lock_bh() - mark the beginning of an RCU-bh critical section\n *\n * This is equivalent of rcu_read_lock(), but also disables softirqs.\n * Note that anything else that disables softirqs can also serve as\n * an RCU read-side critical section.\n *\n * Note that rcu_read_lock_bh() and the matching rcu_read_unlock_bh()\n * must occur in the same context, for example, it is illegal to invoke\n * rcu_read_unlock_bh() from one task if the matching rcu_read_lock_bh()\n * was invoked from some other task.\n */\nstatic inline void rcu_read_lock_bh(void)\n{\n\tlocal_bh_disable();\n\t__acquire(RCU_BH);\n\trcu_lock_acquire(&rcu_bh_lock_map);\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(),\n\t\t\t \"rcu_read_lock_bh() used illegally while idle\");\n}\n\n/**\n * rcu_read_unlock_bh() - marks the end of a softirq-only RCU critical section\n *\n * See rcu_read_lock_bh() for more information.\n */\nstatic inline void rcu_read_unlock_bh(void)\n{\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(),\n\t\t\t \"rcu_read_unlock_bh() used illegally while idle\");\n\trcu_lock_release(&rcu_bh_lock_map);\n\t__release(RCU_BH);\n\tlocal_bh_enable();\n}\n\n/**\n * rcu_read_lock_sched() - mark the beginning of a RCU-sched critical section\n *\n * This is equivalent of rcu_read_lock(), but disables preemption.\n * Read-side critical sections can also be introduced by anything else\n * that disables preemption, including local_irq_disable() and friends.\n *\n * Note that rcu_read_lock_sched() and the matching rcu_read_unlock_sched()\n * must occur in the same context, for example, it is illegal to invoke\n * rcu_read_unlock_sched() from process context if the matching\n * rcu_read_lock_sched() was invoked from an NMI handler.\n */\nstatic inline void rcu_read_lock_sched(void)\n{\n\tpreempt_disable();\n\t__acquire(RCU_SCHED);\n\trcu_lock_acquire(&rcu_sched_lock_map);\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(),\n\t\t\t \"rcu_read_lock_sched() used illegally while idle\");\n}\n\n/* Used by lockdep and tracing: cannot be traced, cannot call lockdep. */\nstatic inline notrace void rcu_read_lock_sched_notrace(void)\n{\n\tpreempt_disable_notrace();\n\t__acquire(RCU_SCHED);\n}\n\n/**\n * rcu_read_unlock_sched() - marks the end of a RCU-classic critical section\n *\n * See rcu_read_lock_sched() for more information.\n */\nstatic inline void rcu_read_unlock_sched(void)\n{\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(),\n\t\t\t \"rcu_read_unlock_sched() used illegally while idle\");\n\trcu_lock_release(&rcu_sched_lock_map);\n\t__release(RCU_SCHED);\n\tpreempt_enable();\n}\n\n/* Used by lockdep and tracing: cannot be traced, cannot call lockdep. */\nstatic inline notrace void rcu_read_unlock_sched_notrace(void)\n{\n\t__release(RCU_SCHED);\n\tpreempt_enable_notrace();\n}\n\n/**\n * RCU_INIT_POINTER() - initialize an RCU protected pointer\n * @p: The pointer to be initialized.\n * @v: The value to initialized the pointer to.\n *\n * Initialize an RCU-protected pointer in special cases where readers\n * do not need ordering constraints on the CPU or the compiler.  These\n * special cases are:\n *\n * 1.\tThis use of RCU_INIT_POINTER() is NULLing out the pointer *or*\n * 2.\tThe caller has taken whatever steps are required to prevent\n *\tRCU readers from concurrently accessing this pointer *or*\n * 3.\tThe referenced data structure has already been exposed to\n *\treaders either at compile time or via rcu_assign_pointer() *and*\n *\n *\ta.\tYou have not made *any* reader-visible changes to\n *\t\tthis structure since then *or*\n *\tb.\tIt is OK for readers accessing this structure from its\n *\t\tnew location to see the old state of the structure.  (For\n *\t\texample, the changes were to statistical counters or to\n *\t\tother state where exact synchronization is not required.)\n *\n * Failure to follow these rules governing use of RCU_INIT_POINTER() will\n * result in impossible-to-diagnose memory corruption.  As in the structures\n * will look OK in crash dumps, but any concurrent RCU readers might\n * see pre-initialized values of the referenced data structure.  So\n * please be very careful how you use RCU_INIT_POINTER()!!!\n *\n * If you are creating an RCU-protected linked structure that is accessed\n * by a single external-to-structure RCU-protected pointer, then you may\n * use RCU_INIT_POINTER() to initialize the internal RCU-protected\n * pointers, but you must use rcu_assign_pointer() to initialize the\n * external-to-structure pointer *after* you have completely initialized\n * the reader-accessible portions of the linked structure.\n *\n * Note that unlike rcu_assign_pointer(), RCU_INIT_POINTER() provides no\n * ordering guarantees for either the CPU or the compiler.\n */\n#define RCU_INIT_POINTER(p, v) \\\n\tdo { \\\n\t\trcu_check_sparse(p, __rcu); \\\n\t\tWRITE_ONCE(p, RCU_INITIALIZER(v)); \\\n\t} while (0)\n\n/**\n * RCU_POINTER_INITIALIZER() - statically initialize an RCU protected pointer\n * @p: The pointer to be initialized.\n * @v: The value to initialized the pointer to.\n *\n * GCC-style initialization for an RCU-protected pointer in a structure field.\n */\n#define RCU_POINTER_INITIALIZER(p, v) \\\n\t\t.p = RCU_INITIALIZER(v)\n\n/*\n * Does the specified offset indicate that the corresponding rcu_head\n * structure can be handled by kvfree_rcu()?\n */\n#define __is_kvfree_rcu_offset(offset) ((offset) < 4096)\n\n/**\n * kfree_rcu() - kfree an object after a grace period.\n * @ptr: pointer to kfree for both single- and double-argument invocations.\n * @rhf: the name of the struct rcu_head within the type of @ptr,\n *       but only for double-argument invocations.\n *\n * Many rcu callbacks functions just call kfree() on the base structure.\n * These functions are trivial, but their size adds up, and furthermore\n * when they are used in a kernel module, that module must invoke the\n * high-latency rcu_barrier() function at module-unload time.\n *\n * The kfree_rcu() function handles this issue.  Rather than encoding a\n * function address in the embedded rcu_head structure, kfree_rcu() instead\n * encodes the offset of the rcu_head structure within the base structure.\n * Because the functions are not allowed in the low-order 4096 bytes of\n * kernel virtual memory, offsets up to 4095 bytes can be accommodated.\n * If the offset is larger than 4095 bytes, a compile-time error will\n * be generated in kvfree_rcu_arg_2(). If this error is triggered, you can\n * either fall back to use of call_rcu() or rearrange the structure to\n * position the rcu_head structure into the first 4096 bytes.\n *\n * Note that the allowable offset might decrease in the future, for example,\n * to allow something like kmem_cache_free_rcu().\n *\n * The BUILD_BUG_ON check must not involve any function calls, hence the\n * checks are done in macros here.\n */\n#define kfree_rcu kvfree_rcu\n\n/**\n * kvfree_rcu() - kvfree an object after a grace period.\n *\n * This macro consists of one or two arguments and it is\n * based on whether an object is head-less or not. If it\n * has a head then a semantic stays the same as it used\n * to be before:\n *\n *     kvfree_rcu(ptr, rhf);\n *\n * where @ptr is a pointer to kvfree(), @rhf is the name\n * of the rcu_head structure within the type of @ptr.\n *\n * When it comes to head-less variant, only one argument\n * is passed and that is just a pointer which has to be\n * freed after a grace period. Therefore the semantic is\n *\n *     kvfree_rcu(ptr);\n *\n * where @ptr is a pointer to kvfree().\n *\n * Please note, head-less way of freeing is permitted to\n * use from a context that has to follow might_sleep()\n * annotation. Otherwise, please switch and embed the\n * rcu_head structure within the type of @ptr.\n */\n#define kvfree_rcu(...) KVFREE_GET_MACRO(__VA_ARGS__,\t\t\\\n\tkvfree_rcu_arg_2, kvfree_rcu_arg_1)(__VA_ARGS__)\n\n#define KVFREE_GET_MACRO(_1, _2, NAME, ...) NAME\n#define kvfree_rcu_arg_2(ptr, rhf)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\ttypeof (ptr) ___p = (ptr);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (___p) {\t\t\t\t\t\t\t\t\t\\\n\t\tBUILD_BUG_ON(!__is_kvfree_rcu_offset(offsetof(typeof(*(ptr)), rhf)));\t\\\n\t\tkvfree_call_rcu(&((___p)->rhf), (rcu_callback_t)(unsigned long)\t\t\\\n\t\t\t(offsetof(typeof(*(ptr)), rhf)));\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#define kvfree_rcu_arg_1(ptr)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\ttypeof(ptr) ___p = (ptr);\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (___p)\t\t\t\t\t\t\\\n\t\tkvfree_call_rcu(NULL, (rcu_callback_t) (___p));\t\\\n} while (0)\n\n/*\n * Place this after a lock-acquisition primitive to guarantee that\n * an UNLOCK+LOCK pair acts as a full barrier.  This guarantee applies\n * if the UNLOCK and LOCK are executed by the same CPU or if the\n * UNLOCK and LOCK operate on the same lock variable.\n */\n#ifdef CONFIG_ARCH_WEAK_RELEASE_ACQUIRE\n#define smp_mb__after_unlock_lock()\tsmp_mb()  /* Full ordering for lock. */\n#else /* #ifdef CONFIG_ARCH_WEAK_RELEASE_ACQUIRE */\n#define smp_mb__after_unlock_lock()\tdo { } while (0)\n#endif /* #else #ifdef CONFIG_ARCH_WEAK_RELEASE_ACQUIRE */\n\n\n/* Has the specified rcu_head structure been handed to call_rcu()? */\n\n/**\n * rcu_head_init - Initialize rcu_head for rcu_head_after_call_rcu()\n * @rhp: The rcu_head structure to initialize.\n *\n * If you intend to invoke rcu_head_after_call_rcu() to test whether a\n * given rcu_head structure has already been passed to call_rcu(), then\n * you must also invoke this rcu_head_init() function on it just after\n * allocating that structure.  Calls to this function must not race with\n * calls to call_rcu(), rcu_head_after_call_rcu(), or callback invocation.\n */\nstatic inline void rcu_head_init(struct rcu_head *rhp)\n{\n\trhp->func = (rcu_callback_t)~0L;\n}\n\n/**\n * rcu_head_after_call_rcu() - Has this rcu_head been passed to call_rcu()?\n * @rhp: The rcu_head structure to test.\n * @f: The function passed to call_rcu() along with @rhp.\n *\n * Returns @true if the @rhp has been passed to call_rcu() with @func,\n * and @false otherwise.  Emits a warning in any other case, including\n * the case where @rhp has already been invoked after a grace period.\n * Calls to this function must not race with callback invocation.  One way\n * to avoid such races is to enclose the call to rcu_head_after_call_rcu()\n * in an RCU read-side critical section that includes a read-side fetch\n * of the pointer to the structure containing @rhp.\n */\nstatic inline bool\nrcu_head_after_call_rcu(struct rcu_head *rhp, rcu_callback_t f)\n{\n\trcu_callback_t func = READ_ONCE(rhp->func);\n\n\tif (func == f)\n\t\treturn true;\n\tWARN_ON_ONCE(func != (rcu_callback_t)~0L);\n\treturn false;\n}\n\n/* kernel/ksysfs.c definitions */\nextern int rcu_expedited;\nextern int rcu_normal;\n\n#endif /* __LINUX_RCUPDATE_H */\n"}, "27": {"id": 27, "path": "/src/net/mac80211/tx.c", "content": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n * Copyright 2002-2005, Instant802 Networks, Inc.\n * Copyright 2005-2006, Devicescape Software, Inc.\n * Copyright 2006-2007\tJiri Benc <jbenc@suse.cz>\n * Copyright 2007\tJohannes Berg <johannes@sipsolutions.net>\n * Copyright 2013-2014  Intel Mobile Communications GmbH\n * Copyright (C) 2018-2020 Intel Corporation\n *\n * Transmit and frame generation functions.\n */\n\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/skbuff.h>\n#include <linux/if_vlan.h>\n#include <linux/etherdevice.h>\n#include <linux/bitmap.h>\n#include <linux/rcupdate.h>\n#include <linux/export.h>\n#include <net/net_namespace.h>\n#include <net/ieee80211_radiotap.h>\n#include <net/cfg80211.h>\n#include <net/mac80211.h>\n#include <net/codel.h>\n#include <net/codel_impl.h>\n#include <asm/unaligned.h>\n#include <net/fq_impl.h>\n\n#include \"ieee80211_i.h\"\n#include \"driver-ops.h\"\n#include \"led.h\"\n#include \"mesh.h\"\n#include \"wep.h\"\n#include \"wpa.h\"\n#include \"wme.h\"\n#include \"rate.h\"\n\n/* misc utils */\n\nstatic __le16 ieee80211_duration(struct ieee80211_tx_data *tx,\n\t\t\t\t struct sk_buff *skb, int group_addr,\n\t\t\t\t int next_frag_len)\n{\n\tint rate, mrate, erp, dur, i, shift = 0;\n\tstruct ieee80211_rate *txrate;\n\tstruct ieee80211_local *local = tx->local;\n\tstruct ieee80211_supported_band *sband;\n\tstruct ieee80211_hdr *hdr;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_chanctx_conf *chanctx_conf;\n\tu32 rate_flags = 0;\n\n\t/* assume HW handles this */\n\tif (tx->rate.flags & (IEEE80211_TX_RC_MCS | IEEE80211_TX_RC_VHT_MCS))\n\t\treturn 0;\n\n\trcu_read_lock();\n\tchanctx_conf = rcu_dereference(tx->sdata->vif.chanctx_conf);\n\tif (chanctx_conf) {\n\t\tshift = ieee80211_chandef_get_shift(&chanctx_conf->def);\n\t\trate_flags = ieee80211_chandef_rate_flags(&chanctx_conf->def);\n\t}\n\trcu_read_unlock();\n\n\t/* uh huh? */\n\tif (WARN_ON_ONCE(tx->rate.idx < 0))\n\t\treturn 0;\n\n\tsband = local->hw.wiphy->bands[info->band];\n\ttxrate = &sband->bitrates[tx->rate.idx];\n\n\terp = txrate->flags & IEEE80211_RATE_ERP_G;\n\n\t/* device is expected to do this */\n\tif (sband->band == NL80211_BAND_S1GHZ)\n\t\treturn 0;\n\n\t/*\n\t * data and mgmt (except PS Poll):\n\t * - during CFP: 32768\n\t * - during contention period:\n\t *   if addr1 is group address: 0\n\t *   if more fragments = 0 and addr1 is individual address: time to\n\t *      transmit one ACK plus SIFS\n\t *   if more fragments = 1 and addr1 is individual address: time to\n\t *      transmit next fragment plus 2 x ACK plus 3 x SIFS\n\t *\n\t * IEEE 802.11, 9.6:\n\t * - control response frame (CTS or ACK) shall be transmitted using the\n\t *   same rate as the immediately previous frame in the frame exchange\n\t *   sequence, if this rate belongs to the PHY mandatory rates, or else\n\t *   at the highest possible rate belonging to the PHY rates in the\n\t *   BSSBasicRateSet\n\t */\n\thdr = (struct ieee80211_hdr *)skb->data;\n\tif (ieee80211_is_ctl(hdr->frame_control)) {\n\t\t/* TODO: These control frames are not currently sent by\n\t\t * mac80211, but should they be implemented, this function\n\t\t * needs to be updated to support duration field calculation.\n\t\t *\n\t\t * RTS: time needed to transmit pending data/mgmt frame plus\n\t\t *    one CTS frame plus one ACK frame plus 3 x SIFS\n\t\t * CTS: duration of immediately previous RTS minus time\n\t\t *    required to transmit CTS and its SIFS\n\t\t * ACK: 0 if immediately previous directed data/mgmt had\n\t\t *    more=0, with more=1 duration in ACK frame is duration\n\t\t *    from previous frame minus time needed to transmit ACK\n\t\t *    and its SIFS\n\t\t * PS Poll: BIT(15) | BIT(14) | aid\n\t\t */\n\t\treturn 0;\n\t}\n\n\t/* data/mgmt */\n\tif (0 /* FIX: data/mgmt during CFP */)\n\t\treturn cpu_to_le16(32768);\n\n\tif (group_addr) /* Group address as the destination - no ACK */\n\t\treturn 0;\n\n\t/* Individual destination address:\n\t * IEEE 802.11, Ch. 9.6 (after IEEE 802.11g changes)\n\t * CTS and ACK frames shall be transmitted using the highest rate in\n\t * basic rate set that is less than or equal to the rate of the\n\t * immediately previous frame and that is using the same modulation\n\t * (CCK or OFDM). If no basic rate set matches with these requirements,\n\t * the highest mandatory rate of the PHY that is less than or equal to\n\t * the rate of the previous frame is used.\n\t * Mandatory rates for IEEE 802.11g PHY: 1, 2, 5.5, 11, 6, 12, 24 Mbps\n\t */\n\trate = -1;\n\t/* use lowest available if everything fails */\n\tmrate = sband->bitrates[0].bitrate;\n\tfor (i = 0; i < sband->n_bitrates; i++) {\n\t\tstruct ieee80211_rate *r = &sband->bitrates[i];\n\n\t\tif (r->bitrate > txrate->bitrate)\n\t\t\tbreak;\n\n\t\tif ((rate_flags & r->flags) != rate_flags)\n\t\t\tcontinue;\n\n\t\tif (tx->sdata->vif.bss_conf.basic_rates & BIT(i))\n\t\t\trate = DIV_ROUND_UP(r->bitrate, 1 << shift);\n\n\t\tswitch (sband->band) {\n\t\tcase NL80211_BAND_2GHZ: {\n\t\t\tu32 flag;\n\t\t\tif (tx->sdata->flags & IEEE80211_SDATA_OPERATING_GMODE)\n\t\t\t\tflag = IEEE80211_RATE_MANDATORY_G;\n\t\t\telse\n\t\t\t\tflag = IEEE80211_RATE_MANDATORY_B;\n\t\t\tif (r->flags & flag)\n\t\t\t\tmrate = r->bitrate;\n\t\t\tbreak;\n\t\t}\n\t\tcase NL80211_BAND_5GHZ:\n\t\tcase NL80211_BAND_6GHZ:\n\t\t\tif (r->flags & IEEE80211_RATE_MANDATORY_A)\n\t\t\t\tmrate = r->bitrate;\n\t\t\tbreak;\n\t\tcase NL80211_BAND_S1GHZ:\n\t\tcase NL80211_BAND_60GHZ:\n\t\t\t/* TODO, for now fall through */\n\t\tcase NUM_NL80211_BANDS:\n\t\t\tWARN_ON(1);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (rate == -1) {\n\t\t/* No matching basic rate found; use highest suitable mandatory\n\t\t * PHY rate */\n\t\trate = DIV_ROUND_UP(mrate, 1 << shift);\n\t}\n\n\t/* Don't calculate ACKs for QoS Frames with NoAck Policy set */\n\tif (ieee80211_is_data_qos(hdr->frame_control) &&\n\t    *(ieee80211_get_qos_ctl(hdr)) & IEEE80211_QOS_CTL_ACK_POLICY_NOACK)\n\t\tdur = 0;\n\telse\n\t\t/* Time needed to transmit ACK\n\t\t * (10 bytes + 4-byte FCS = 112 bits) plus SIFS; rounded up\n\t\t * to closest integer */\n\t\tdur = ieee80211_frame_duration(sband->band, 10, rate, erp,\n\t\t\t\ttx->sdata->vif.bss_conf.use_short_preamble,\n\t\t\t\tshift);\n\n\tif (next_frag_len) {\n\t\t/* Frame is fragmented: duration increases with time needed to\n\t\t * transmit next fragment plus ACK and 2 x SIFS. */\n\t\tdur *= 2; /* ACK + SIFS */\n\t\t/* next fragment */\n\t\tdur += ieee80211_frame_duration(sband->band, next_frag_len,\n\t\t\t\ttxrate->bitrate, erp,\n\t\t\t\ttx->sdata->vif.bss_conf.use_short_preamble,\n\t\t\t\tshift);\n\t}\n\n\treturn cpu_to_le16(dur);\n}\n\n/* tx handlers */\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_dynamic_ps(struct ieee80211_tx_data *tx)\n{\n\tstruct ieee80211_local *local = tx->local;\n\tstruct ieee80211_if_managed *ifmgd;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\n\t/* driver doesn't support power save */\n\tif (!ieee80211_hw_check(&local->hw, SUPPORTS_PS))\n\t\treturn TX_CONTINUE;\n\n\t/* hardware does dynamic power save */\n\tif (ieee80211_hw_check(&local->hw, SUPPORTS_DYNAMIC_PS))\n\t\treturn TX_CONTINUE;\n\n\t/* dynamic power save disabled */\n\tif (local->hw.conf.dynamic_ps_timeout <= 0)\n\t\treturn TX_CONTINUE;\n\n\t/* we are scanning, don't enable power save */\n\tif (local->scanning)\n\t\treturn TX_CONTINUE;\n\n\tif (!local->ps_sdata)\n\t\treturn TX_CONTINUE;\n\n\t/* No point if we're going to suspend */\n\tif (local->quiescing)\n\t\treturn TX_CONTINUE;\n\n\t/* dynamic ps is supported only in managed mode */\n\tif (tx->sdata->vif.type != NL80211_IFTYPE_STATION)\n\t\treturn TX_CONTINUE;\n\n\tif (unlikely(info->flags & IEEE80211_TX_INTFL_OFFCHAN_TX_OK))\n\t\treturn TX_CONTINUE;\n\n\tifmgd = &tx->sdata->u.mgd;\n\n\t/*\n\t * Don't wakeup from power save if u-apsd is enabled, voip ac has\n\t * u-apsd enabled and the frame is in voip class. This effectively\n\t * means that even if all access categories have u-apsd enabled, in\n\t * practise u-apsd is only used with the voip ac. This is a\n\t * workaround for the case when received voip class packets do not\n\t * have correct qos tag for some reason, due the network or the\n\t * peer application.\n\t *\n\t * Note: ifmgd->uapsd_queues access is racy here. If the value is\n\t * changed via debugfs, user needs to reassociate manually to have\n\t * everything in sync.\n\t */\n\tif ((ifmgd->flags & IEEE80211_STA_UAPSD_ENABLED) &&\n\t    (ifmgd->uapsd_queues & IEEE80211_WMM_IE_STA_QOSINFO_AC_VO) &&\n\t    skb_get_queue_mapping(tx->skb) == IEEE80211_AC_VO)\n\t\treturn TX_CONTINUE;\n\n\tif (local->hw.conf.flags & IEEE80211_CONF_PS) {\n\t\tieee80211_stop_queues_by_reason(&local->hw,\n\t\t\t\t\t\tIEEE80211_MAX_QUEUE_MAP,\n\t\t\t\t\t\tIEEE80211_QUEUE_STOP_REASON_PS,\n\t\t\t\t\t\tfalse);\n\t\tifmgd->flags &= ~IEEE80211_STA_NULLFUNC_ACKED;\n\t\tieee80211_queue_work(&local->hw,\n\t\t\t\t     &local->dynamic_ps_disable_work);\n\t}\n\n\t/* Don't restart the timer if we're not disassociated */\n\tif (!ifmgd->associated)\n\t\treturn TX_CONTINUE;\n\n\tmod_timer(&local->dynamic_ps_timer, jiffies +\n\t\t  msecs_to_jiffies(local->hw.conf.dynamic_ps_timeout));\n\n\treturn TX_CONTINUE;\n}\n\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_check_assoc(struct ieee80211_tx_data *tx)\n{\n\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)tx->skb->data;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\tbool assoc = false;\n\n\tif (unlikely(info->flags & IEEE80211_TX_CTL_INJECTED))\n\t\treturn TX_CONTINUE;\n\n\tif (unlikely(test_bit(SCAN_SW_SCANNING, &tx->local->scanning)) &&\n\t    test_bit(SDATA_STATE_OFFCHANNEL, &tx->sdata->state) &&\n\t    !ieee80211_is_probe_req(hdr->frame_control) &&\n\t    !ieee80211_is_any_nullfunc(hdr->frame_control))\n\t\t/*\n\t\t * When software scanning only nullfunc frames (to notify\n\t\t * the sleep state to the AP) and probe requests (for the\n\t\t * active scan) are allowed, all other frames should not be\n\t\t * sent and we should not get here, but if we do\n\t\t * nonetheless, drop them to avoid sending them\n\t\t * off-channel. See the link below and\n\t\t * ieee80211_start_scan() for more.\n\t\t *\n\t\t * http://article.gmane.org/gmane.linux.kernel.wireless.general/30089\n\t\t */\n\t\treturn TX_DROP;\n\n\tif (tx->sdata->vif.type == NL80211_IFTYPE_OCB)\n\t\treturn TX_CONTINUE;\n\n\tif (tx->flags & IEEE80211_TX_PS_BUFFERED)\n\t\treturn TX_CONTINUE;\n\n\tif (tx->sta)\n\t\tassoc = test_sta_flag(tx->sta, WLAN_STA_ASSOC);\n\n\tif (likely(tx->flags & IEEE80211_TX_UNICAST)) {\n\t\tif (unlikely(!assoc &&\n\t\t\t     ieee80211_is_data(hdr->frame_control))) {\n#ifdef CONFIG_MAC80211_VERBOSE_DEBUG\n\t\t\tsdata_info(tx->sdata,\n\t\t\t\t   \"dropped data frame to not associated station %pM\\n\",\n\t\t\t\t   hdr->addr1);\n#endif\n\t\t\tI802_DEBUG_INC(tx->local->tx_handlers_drop_not_assoc);\n\t\t\treturn TX_DROP;\n\t\t}\n\t} else if (unlikely(ieee80211_is_data(hdr->frame_control) &&\n\t\t\t    ieee80211_vif_get_num_mcast_if(tx->sdata) == 0)) {\n\t\t/*\n\t\t * No associated STAs - no need to send multicast\n\t\t * frames.\n\t\t */\n\t\treturn TX_DROP;\n\t}\n\n\treturn TX_CONTINUE;\n}\n\n/* This function is called whenever the AP is about to exceed the maximum limit\n * of buffered frames for power saving STAs. This situation should not really\n * happen often during normal operation, so dropping the oldest buffered packet\n * from each queue should be OK to make some room for new frames. */\nstatic void purge_old_ps_buffers(struct ieee80211_local *local)\n{\n\tint total = 0, purged = 0;\n\tstruct sk_buff *skb;\n\tstruct ieee80211_sub_if_data *sdata;\n\tstruct sta_info *sta;\n\n\tlist_for_each_entry_rcu(sdata, &local->interfaces, list) {\n\t\tstruct ps_data *ps;\n\n\t\tif (sdata->vif.type == NL80211_IFTYPE_AP)\n\t\t\tps = &sdata->u.ap.ps;\n\t\telse if (ieee80211_vif_is_mesh(&sdata->vif))\n\t\t\tps = &sdata->u.mesh.ps;\n\t\telse\n\t\t\tcontinue;\n\n\t\tskb = skb_dequeue(&ps->bc_buf);\n\t\tif (skb) {\n\t\t\tpurged++;\n\t\t\tieee80211_free_txskb(&local->hw, skb);\n\t\t}\n\t\ttotal += skb_queue_len(&ps->bc_buf);\n\t}\n\n\t/*\n\t * Drop one frame from each station from the lowest-priority\n\t * AC that has frames at all.\n\t */\n\tlist_for_each_entry_rcu(sta, &local->sta_list, list) {\n\t\tint ac;\n\n\t\tfor (ac = IEEE80211_AC_BK; ac >= IEEE80211_AC_VO; ac--) {\n\t\t\tskb = skb_dequeue(&sta->ps_tx_buf[ac]);\n\t\t\ttotal += skb_queue_len(&sta->ps_tx_buf[ac]);\n\t\t\tif (skb) {\n\t\t\t\tpurged++;\n\t\t\t\tieee80211_free_txskb(&local->hw, skb);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tlocal->total_ps_buffered = total;\n\tps_dbg_hw(&local->hw, \"PS buffers full - purged %d frames\\n\", purged);\n}\n\nstatic ieee80211_tx_result\nieee80211_tx_h_multicast_ps_buf(struct ieee80211_tx_data *tx)\n{\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)tx->skb->data;\n\tstruct ps_data *ps;\n\n\t/*\n\t * broadcast/multicast frame\n\t *\n\t * If any of the associated/peer stations is in power save mode,\n\t * the frame is buffered to be sent after DTIM beacon frame.\n\t * This is done either by the hardware or us.\n\t */\n\n\t/* powersaving STAs currently only in AP/VLAN/mesh mode */\n\tif (tx->sdata->vif.type == NL80211_IFTYPE_AP ||\n\t    tx->sdata->vif.type == NL80211_IFTYPE_AP_VLAN) {\n\t\tif (!tx->sdata->bss)\n\t\t\treturn TX_CONTINUE;\n\n\t\tps = &tx->sdata->bss->ps;\n\t} else if (ieee80211_vif_is_mesh(&tx->sdata->vif)) {\n\t\tps = &tx->sdata->u.mesh.ps;\n\t} else {\n\t\treturn TX_CONTINUE;\n\t}\n\n\n\t/* no buffering for ordered frames */\n\tif (ieee80211_has_order(hdr->frame_control))\n\t\treturn TX_CONTINUE;\n\n\tif (ieee80211_is_probe_req(hdr->frame_control))\n\t\treturn TX_CONTINUE;\n\n\tif (ieee80211_hw_check(&tx->local->hw, QUEUE_CONTROL))\n\t\tinfo->hw_queue = tx->sdata->vif.cab_queue;\n\n\t/* no stations in PS mode and no buffered packets */\n\tif (!atomic_read(&ps->num_sta_ps) && skb_queue_empty(&ps->bc_buf))\n\t\treturn TX_CONTINUE;\n\n\tinfo->flags |= IEEE80211_TX_CTL_SEND_AFTER_DTIM;\n\n\t/* device releases frame after DTIM beacon */\n\tif (!ieee80211_hw_check(&tx->local->hw, HOST_BROADCAST_PS_BUFFERING))\n\t\treturn TX_CONTINUE;\n\n\t/* buffered in mac80211 */\n\tif (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)\n\t\tpurge_old_ps_buffers(tx->local);\n\n\tif (skb_queue_len(&ps->bc_buf) >= AP_MAX_BC_BUFFER) {\n\t\tps_dbg(tx->sdata,\n\t\t       \"BC TX buffer full - dropping the oldest frame\\n\");\n\t\tieee80211_free_txskb(&tx->local->hw, skb_dequeue(&ps->bc_buf));\n\t} else\n\t\ttx->local->total_ps_buffered++;\n\n\tskb_queue_tail(&ps->bc_buf, tx->skb);\n\n\treturn TX_QUEUED;\n}\n\nstatic int ieee80211_use_mfp(__le16 fc, struct sta_info *sta,\n\t\t\t     struct sk_buff *skb)\n{\n\tif (!ieee80211_is_mgmt(fc))\n\t\treturn 0;\n\n\tif (sta == NULL || !test_sta_flag(sta, WLAN_STA_MFP))\n\t\treturn 0;\n\n\tif (!ieee80211_is_robust_mgmt_frame(skb))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic ieee80211_tx_result\nieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)\n{\n\tstruct sta_info *sta = tx->sta;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)tx->skb->data;\n\tstruct ieee80211_local *local = tx->local;\n\n\tif (unlikely(!sta))\n\t\treturn TX_CONTINUE;\n\n\tif (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||\n\t\t      test_sta_flag(sta, WLAN_STA_PS_DRIVER) ||\n\t\t      test_sta_flag(sta, WLAN_STA_PS_DELIVER)) &&\n\t\t     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {\n\t\tint ac = skb_get_queue_mapping(tx->skb);\n\n\t\tif (ieee80211_is_mgmt(hdr->frame_control) &&\n\t\t    !ieee80211_is_bufferable_mmpdu(hdr->frame_control)) {\n\t\t\tinfo->flags |= IEEE80211_TX_CTL_NO_PS_BUFFER;\n\t\t\treturn TX_CONTINUE;\n\t\t}\n\n\t\tps_dbg(sta->sdata, \"STA %pM aid %d: PS buffer for AC %d\\n\",\n\t\t       sta->sta.addr, sta->sta.aid, ac);\n\t\tif (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)\n\t\t\tpurge_old_ps_buffers(tx->local);\n\n\t\t/* sync with ieee80211_sta_ps_deliver_wakeup */\n\t\tspin_lock(&sta->ps_lock);\n\t\t/*\n\t\t * STA woke up the meantime and all the frames on ps_tx_buf have\n\t\t * been queued to pending queue. No reordering can happen, go\n\t\t * ahead and Tx the packet.\n\t\t */\n\t\tif (!test_sta_flag(sta, WLAN_STA_PS_STA) &&\n\t\t    !test_sta_flag(sta, WLAN_STA_PS_DRIVER) &&\n\t\t    !test_sta_flag(sta, WLAN_STA_PS_DELIVER)) {\n\t\t\tspin_unlock(&sta->ps_lock);\n\t\t\treturn TX_CONTINUE;\n\t\t}\n\n\t\tif (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {\n\t\t\tstruct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);\n\t\t\tps_dbg(tx->sdata,\n\t\t\t       \"STA %pM TX buffer for AC %d full - dropping oldest frame\\n\",\n\t\t\t       sta->sta.addr, ac);\n\t\t\tieee80211_free_txskb(&local->hw, old);\n\t\t} else\n\t\t\ttx->local->total_ps_buffered++;\n\n\t\tinfo->control.jiffies = jiffies;\n\t\tinfo->control.vif = &tx->sdata->vif;\n\t\tinfo->control.flags |= IEEE80211_TX_INTCFL_NEED_TXPROCESSING;\n\t\tinfo->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;\n\t\tskb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);\n\t\tspin_unlock(&sta->ps_lock);\n\n\t\tif (!timer_pending(&local->sta_cleanup))\n\t\t\tmod_timer(&local->sta_cleanup,\n\t\t\t\t  round_jiffies(jiffies +\n\t\t\t\t\t\tSTA_INFO_CLEANUP_INTERVAL));\n\n\t\t/*\n\t\t * We queued up some frames, so the TIM bit might\n\t\t * need to be set, recalculate it.\n\t\t */\n\t\tsta_info_recalc_tim(sta);\n\n\t\treturn TX_QUEUED;\n\t} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {\n\t\tps_dbg(tx->sdata,\n\t\t       \"STA %pM in PS mode, but polling/in SP -> send frame\\n\",\n\t\t       sta->sta.addr);\n\t}\n\n\treturn TX_CONTINUE;\n}\n\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_ps_buf(struct ieee80211_tx_data *tx)\n{\n\tif (unlikely(tx->flags & IEEE80211_TX_PS_BUFFERED))\n\t\treturn TX_CONTINUE;\n\n\tif (tx->flags & IEEE80211_TX_UNICAST)\n\t\treturn ieee80211_tx_h_unicast_ps_buf(tx);\n\telse\n\t\treturn ieee80211_tx_h_multicast_ps_buf(tx);\n}\n\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_check_control_port_protocol(struct ieee80211_tx_data *tx)\n{\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\n\tif (unlikely(tx->sdata->control_port_protocol == tx->skb->protocol)) {\n\t\tif (tx->sdata->control_port_no_encrypt)\n\t\t\tinfo->flags |= IEEE80211_TX_INTFL_DONT_ENCRYPT;\n\t\tinfo->control.flags |= IEEE80211_TX_CTRL_PORT_CTRL_PROTO;\n\t\tinfo->flags |= IEEE80211_TX_CTL_USE_MINRATE;\n\t}\n\n\treturn TX_CONTINUE;\n}\n\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_select_key(struct ieee80211_tx_data *tx)\n{\n\tstruct ieee80211_key *key;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)tx->skb->data;\n\n\tif (unlikely(info->flags & IEEE80211_TX_INTFL_DONT_ENCRYPT)) {\n\t\ttx->key = NULL;\n\t\treturn TX_CONTINUE;\n\t}\n\n\tif (tx->sta &&\n\t    (key = rcu_dereference(tx->sta->ptk[tx->sta->ptk_idx])))\n\t\ttx->key = key;\n\telse if (ieee80211_is_group_privacy_action(tx->skb) &&\n\t\t(key = rcu_dereference(tx->sdata->default_multicast_key)))\n\t\ttx->key = key;\n\telse if (ieee80211_is_mgmt(hdr->frame_control) &&\n\t\t is_multicast_ether_addr(hdr->addr1) &&\n\t\t ieee80211_is_robust_mgmt_frame(tx->skb) &&\n\t\t (key = rcu_dereference(tx->sdata->default_mgmt_key)))\n\t\ttx->key = key;\n\telse if (is_multicast_ether_addr(hdr->addr1) &&\n\t\t (key = rcu_dereference(tx->sdata->default_multicast_key)))\n\t\ttx->key = key;\n\telse if (!is_multicast_ether_addr(hdr->addr1) &&\n\t\t (key = rcu_dereference(tx->sdata->default_unicast_key)))\n\t\ttx->key = key;\n\telse\n\t\ttx->key = NULL;\n\n\tif (tx->key) {\n\t\tbool skip_hw = false;\n\n\t\t/* TODO: add threshold stuff again */\n\n\t\tswitch (tx->key->conf.cipher) {\n\t\tcase WLAN_CIPHER_SUITE_WEP40:\n\t\tcase WLAN_CIPHER_SUITE_WEP104:\n\t\tcase WLAN_CIPHER_SUITE_TKIP:\n\t\t\tif (!ieee80211_is_data_present(hdr->frame_control))\n\t\t\t\ttx->key = NULL;\n\t\t\tbreak;\n\t\tcase WLAN_CIPHER_SUITE_CCMP:\n\t\tcase WLAN_CIPHER_SUITE_CCMP_256:\n\t\tcase WLAN_CIPHER_SUITE_GCMP:\n\t\tcase WLAN_CIPHER_SUITE_GCMP_256:\n\t\t\tif (!ieee80211_is_data_present(hdr->frame_control) &&\n\t\t\t    !ieee80211_use_mfp(hdr->frame_control, tx->sta,\n\t\t\t\t\t       tx->skb) &&\n\t\t\t    !ieee80211_is_group_privacy_action(tx->skb))\n\t\t\t\ttx->key = NULL;\n\t\t\telse\n\t\t\t\tskip_hw = (tx->key->conf.flags &\n\t\t\t\t\t   IEEE80211_KEY_FLAG_SW_MGMT_TX) &&\n\t\t\t\t\tieee80211_is_mgmt(hdr->frame_control);\n\t\t\tbreak;\n\t\tcase WLAN_CIPHER_SUITE_AES_CMAC:\n\t\tcase WLAN_CIPHER_SUITE_BIP_CMAC_256:\n\t\tcase WLAN_CIPHER_SUITE_BIP_GMAC_128:\n\t\tcase WLAN_CIPHER_SUITE_BIP_GMAC_256:\n\t\t\tif (!ieee80211_is_mgmt(hdr->frame_control))\n\t\t\t\ttx->key = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (unlikely(tx->key && tx->key->flags & KEY_FLAG_TAINTED &&\n\t\t\t     !ieee80211_is_deauth(hdr->frame_control)))\n\t\t\treturn TX_DROP;\n\n\t\tif (!skip_hw && tx->key &&\n\t\t    tx->key->flags & KEY_FLAG_UPLOADED_TO_HARDWARE)\n\t\t\tinfo->control.hw_key = &tx->key->conf;\n\t} else if (ieee80211_is_data_present(hdr->frame_control) && tx->sta &&\n\t\t   test_sta_flag(tx->sta, WLAN_STA_USES_ENCRYPTION)) {\n\t\treturn TX_DROP;\n\t}\n\n\treturn TX_CONTINUE;\n}\n\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_rate_ctrl(struct ieee80211_tx_data *tx)\n{\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\tstruct ieee80211_hdr *hdr = (void *)tx->skb->data;\n\tstruct ieee80211_supported_band *sband;\n\tu32 len;\n\tstruct ieee80211_tx_rate_control txrc;\n\tstruct ieee80211_sta_rates *ratetbl = NULL;\n\tbool assoc = false;\n\n\tmemset(&txrc, 0, sizeof(txrc));\n\n\tsband = tx->local->hw.wiphy->bands[info->band];\n\n\tlen = min_t(u32, tx->skb->len + FCS_LEN,\n\t\t\t tx->local->hw.wiphy->frag_threshold);\n\n\t/* set up the tx rate control struct we give the RC algo */\n\ttxrc.hw = &tx->local->hw;\n\ttxrc.sband = sband;\n\ttxrc.bss_conf = &tx->sdata->vif.bss_conf;\n\ttxrc.skb = tx->skb;\n\ttxrc.reported_rate.idx = -1;\n\ttxrc.rate_idx_mask = tx->sdata->rc_rateidx_mask[info->band];\n\n\tif (tx->sdata->rc_has_mcs_mask[info->band])\n\t\ttxrc.rate_idx_mcs_mask =\n\t\t\ttx->sdata->rc_rateidx_mcs_mask[info->band];\n\n\ttxrc.bss = (tx->sdata->vif.type == NL80211_IFTYPE_AP ||\n\t\t    tx->sdata->vif.type == NL80211_IFTYPE_MESH_POINT ||\n\t\t    tx->sdata->vif.type == NL80211_IFTYPE_ADHOC ||\n\t\t    tx->sdata->vif.type == NL80211_IFTYPE_OCB);\n\n\t/* set up RTS protection if desired */\n\tif (len > tx->local->hw.wiphy->rts_threshold) {\n\t\ttxrc.rts = true;\n\t}\n\n\tinfo->control.use_rts = txrc.rts;\n\tinfo->control.use_cts_prot = tx->sdata->vif.bss_conf.use_cts_prot;\n\n\t/*\n\t * Use short preamble if the BSS can handle it, but not for\n\t * management frames unless we know the receiver can handle\n\t * that -- the management frame might be to a station that\n\t * just wants a probe response.\n\t */\n\tif (tx->sdata->vif.bss_conf.use_short_preamble &&\n\t    (ieee80211_is_data(hdr->frame_control) ||\n\t     (tx->sta && test_sta_flag(tx->sta, WLAN_STA_SHORT_PREAMBLE))))\n\t\ttxrc.short_preamble = true;\n\n\tinfo->control.short_preamble = txrc.short_preamble;\n\n\t/* don't ask rate control when rate already injected via radiotap */\n\tif (info->control.flags & IEEE80211_TX_CTRL_RATE_INJECT)\n\t\treturn TX_CONTINUE;\n\n\tif (tx->sta)\n\t\tassoc = test_sta_flag(tx->sta, WLAN_STA_ASSOC);\n\n\t/*\n\t * Lets not bother rate control if we're associated and cannot\n\t * talk to the sta. This should not happen.\n\t */\n\tif (WARN(test_bit(SCAN_SW_SCANNING, &tx->local->scanning) && assoc &&\n\t\t !rate_usable_index_exists(sband, &tx->sta->sta),\n\t\t \"%s: Dropped data frame as no usable bitrate found while \"\n\t\t \"scanning and associated. Target station: \"\n\t\t \"%pM on %d GHz band\\n\",\n\t\t tx->sdata->name, hdr->addr1,\n\t\t info->band ? 5 : 2))\n\t\treturn TX_DROP;\n\n\t/*\n\t * If we're associated with the sta at this point we know we can at\n\t * least send the frame at the lowest bit rate.\n\t */\n\trate_control_get_rate(tx->sdata, tx->sta, &txrc);\n\n\tif (tx->sta && !info->control.skip_table)\n\t\tratetbl = rcu_dereference(tx->sta->sta.rates);\n\n\tif (unlikely(info->control.rates[0].idx < 0)) {\n\t\tif (ratetbl) {\n\t\t\tstruct ieee80211_tx_rate rate = {\n\t\t\t\t.idx = ratetbl->rate[0].idx,\n\t\t\t\t.flags = ratetbl->rate[0].flags,\n\t\t\t\t.count = ratetbl->rate[0].count\n\t\t\t};\n\n\t\t\tif (ratetbl->rate[0].idx < 0)\n\t\t\t\treturn TX_DROP;\n\n\t\t\ttx->rate = rate;\n\t\t} else {\n\t\t\treturn TX_DROP;\n\t\t}\n\t} else {\n\t\ttx->rate = info->control.rates[0];\n\t}\n\n\tif (txrc.reported_rate.idx < 0) {\n\t\ttxrc.reported_rate = tx->rate;\n\t\tif (tx->sta && ieee80211_is_data(hdr->frame_control))\n\t\t\ttx->sta->tx_stats.last_rate = txrc.reported_rate;\n\t} else if (tx->sta)\n\t\ttx->sta->tx_stats.last_rate = txrc.reported_rate;\n\n\tif (ratetbl)\n\t\treturn TX_CONTINUE;\n\n\tif (unlikely(!info->control.rates[0].count))\n\t\tinfo->control.rates[0].count = 1;\n\n\tif (WARN_ON_ONCE((info->control.rates[0].count > 1) &&\n\t\t\t (info->flags & IEEE80211_TX_CTL_NO_ACK)))\n\t\tinfo->control.rates[0].count = 1;\n\n\treturn TX_CONTINUE;\n}\n\nstatic __le16 ieee80211_tx_next_seq(struct sta_info *sta, int tid)\n{\n\tu16 *seq = &sta->tid_seq[tid];\n\t__le16 ret = cpu_to_le16(*seq);\n\n\t/* Increase the sequence number. */\n\t*seq = (*seq + 0x10) & IEEE80211_SCTL_SEQ;\n\n\treturn ret;\n}\n\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_sequence(struct ieee80211_tx_data *tx)\n{\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)tx->skb->data;\n\tint tid;\n\n\t/*\n\t * Packet injection may want to control the sequence\n\t * number, if we have no matching interface then we\n\t * neither assign one ourselves nor ask the driver to.\n\t */\n\tif (unlikely(info->control.vif->type == NL80211_IFTYPE_MONITOR))\n\t\treturn TX_CONTINUE;\n\n\tif (unlikely(ieee80211_is_ctl(hdr->frame_control)))\n\t\treturn TX_CONTINUE;\n\n\tif (ieee80211_hdrlen(hdr->frame_control) < 24)\n\t\treturn TX_CONTINUE;\n\n\tif (ieee80211_is_qos_nullfunc(hdr->frame_control))\n\t\treturn TX_CONTINUE;\n\n\tif (info->control.flags & IEEE80211_TX_CTRL_NO_SEQNO)\n\t\treturn TX_CONTINUE;\n\n\t/*\n\t * Anything but QoS data that has a sequence number field\n\t * (is long enough) gets a sequence number from the global\n\t * counter.  QoS data frames with a multicast destination\n\t * also use the global counter (802.11-2012 9.3.2.10).\n\t */\n\tif (!ieee80211_is_data_qos(hdr->frame_control) ||\n\t    is_multicast_ether_addr(hdr->addr1)) {\n\t\t/* driver should assign sequence number */\n\t\tinfo->flags |= IEEE80211_TX_CTL_ASSIGN_SEQ;\n\t\t/* for pure STA mode without beacons, we can do it */\n\t\thdr->seq_ctrl = cpu_to_le16(tx->sdata->sequence_number);\n\t\ttx->sdata->sequence_number += 0x10;\n\t\tif (tx->sta)\n\t\t\ttx->sta->tx_stats.msdu[IEEE80211_NUM_TIDS]++;\n\t\treturn TX_CONTINUE;\n\t}\n\n\t/*\n\t * This should be true for injected/management frames only, for\n\t * management frames we have set the IEEE80211_TX_CTL_ASSIGN_SEQ\n\t * above since they are not QoS-data frames.\n\t */\n\tif (!tx->sta)\n\t\treturn TX_CONTINUE;\n\n\t/* include per-STA, per-TID sequence counter */\n\ttid = ieee80211_get_tid(hdr);\n\ttx->sta->tx_stats.msdu[tid]++;\n\n\thdr->seq_ctrl = ieee80211_tx_next_seq(tx->sta, tid);\n\n\treturn TX_CONTINUE;\n}\n\nstatic int ieee80211_fragment(struct ieee80211_tx_data *tx,\n\t\t\t      struct sk_buff *skb, int hdrlen,\n\t\t\t      int frag_threshold)\n{\n\tstruct ieee80211_local *local = tx->local;\n\tstruct ieee80211_tx_info *info;\n\tstruct sk_buff *tmp;\n\tint per_fragm = frag_threshold - hdrlen - FCS_LEN;\n\tint pos = hdrlen + per_fragm;\n\tint rem = skb->len - hdrlen - per_fragm;\n\n\tif (WARN_ON(rem < 0))\n\t\treturn -EINVAL;\n\n\t/* first fragment was already added to queue by caller */\n\n\twhile (rem) {\n\t\tint fraglen = per_fragm;\n\n\t\tif (fraglen > rem)\n\t\t\tfraglen = rem;\n\t\trem -= fraglen;\n\t\ttmp = dev_alloc_skb(local->tx_headroom +\n\t\t\t\t    frag_threshold +\n\t\t\t\t    tx->sdata->encrypt_headroom +\n\t\t\t\t    IEEE80211_ENCRYPT_TAILROOM);\n\t\tif (!tmp)\n\t\t\treturn -ENOMEM;\n\n\t\t__skb_queue_tail(&tx->skbs, tmp);\n\n\t\tskb_reserve(tmp,\n\t\t\t    local->tx_headroom + tx->sdata->encrypt_headroom);\n\n\t\t/* copy control information */\n\t\tmemcpy(tmp->cb, skb->cb, sizeof(tmp->cb));\n\n\t\tinfo = IEEE80211_SKB_CB(tmp);\n\t\tinfo->flags &= ~(IEEE80211_TX_CTL_CLEAR_PS_FILT |\n\t\t\t\t IEEE80211_TX_CTL_FIRST_FRAGMENT);\n\n\t\tif (rem)\n\t\t\tinfo->flags |= IEEE80211_TX_CTL_MORE_FRAMES;\n\n\t\tskb_copy_queue_mapping(tmp, skb);\n\t\ttmp->priority = skb->priority;\n\t\ttmp->dev = skb->dev;\n\n\t\t/* copy header and data */\n\t\tskb_put_data(tmp, skb->data, hdrlen);\n\t\tskb_put_data(tmp, skb->data + pos, fraglen);\n\n\t\tpos += fraglen;\n\t}\n\n\t/* adjust first fragment's length */\n\tskb_trim(skb, hdrlen + per_fragm);\n\treturn 0;\n}\n\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_fragment(struct ieee80211_tx_data *tx)\n{\n\tstruct sk_buff *skb = tx->skb;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_hdr *hdr = (void *)skb->data;\n\tint frag_threshold = tx->local->hw.wiphy->frag_threshold;\n\tint hdrlen;\n\tint fragnum;\n\n\t/* no matter what happens, tx->skb moves to tx->skbs */\n\t__skb_queue_tail(&tx->skbs, skb);\n\ttx->skb = NULL;\n\n\tif (info->flags & IEEE80211_TX_CTL_DONTFRAG)\n\t\treturn TX_CONTINUE;\n\n\tif (ieee80211_hw_check(&tx->local->hw, SUPPORTS_TX_FRAG))\n\t\treturn TX_CONTINUE;\n\n\t/*\n\t * Warn when submitting a fragmented A-MPDU frame and drop it.\n\t * This scenario is handled in ieee80211_tx_prepare but extra\n\t * caution taken here as fragmented ampdu may cause Tx stop.\n\t */\n\tif (WARN_ON(info->flags & IEEE80211_TX_CTL_AMPDU))\n\t\treturn TX_DROP;\n\n\thdrlen = ieee80211_hdrlen(hdr->frame_control);\n\n\t/* internal error, why isn't DONTFRAG set? */\n\tif (WARN_ON(skb->len + FCS_LEN <= frag_threshold))\n\t\treturn TX_DROP;\n\n\t/*\n\t * Now fragment the frame. This will allocate all the fragments and\n\t * chain them (using skb as the first fragment) to skb->next.\n\t * During transmission, we will remove the successfully transmitted\n\t * fragments from this list. When the low-level driver rejects one\n\t * of the fragments then we will simply pretend to accept the skb\n\t * but store it away as pending.\n\t */\n\tif (ieee80211_fragment(tx, skb, hdrlen, frag_threshold))\n\t\treturn TX_DROP;\n\n\t/* update duration/seq/flags of fragments */\n\tfragnum = 0;\n\n\tskb_queue_walk(&tx->skbs, skb) {\n\t\tconst __le16 morefrags = cpu_to_le16(IEEE80211_FCTL_MOREFRAGS);\n\n\t\thdr = (void *)skb->data;\n\t\tinfo = IEEE80211_SKB_CB(skb);\n\n\t\tif (!skb_queue_is_last(&tx->skbs, skb)) {\n\t\t\thdr->frame_control |= morefrags;\n\t\t\t/*\n\t\t\t * No multi-rate retries for fragmented frames, that\n\t\t\t * would completely throw off the NAV at other STAs.\n\t\t\t */\n\t\t\tinfo->control.rates[1].idx = -1;\n\t\t\tinfo->control.rates[2].idx = -1;\n\t\t\tinfo->control.rates[3].idx = -1;\n\t\t\tBUILD_BUG_ON(IEEE80211_TX_MAX_RATES != 4);\n\t\t\tinfo->flags &= ~IEEE80211_TX_CTL_RATE_CTRL_PROBE;\n\t\t} else {\n\t\t\thdr->frame_control &= ~morefrags;\n\t\t}\n\t\thdr->seq_ctrl |= cpu_to_le16(fragnum & IEEE80211_SCTL_FRAG);\n\t\tfragnum++;\n\t}\n\n\treturn TX_CONTINUE;\n}\n\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_stats(struct ieee80211_tx_data *tx)\n{\n\tstruct sk_buff *skb;\n\tint ac = -1;\n\n\tif (!tx->sta)\n\t\treturn TX_CONTINUE;\n\n\tskb_queue_walk(&tx->skbs, skb) {\n\t\tac = skb_get_queue_mapping(skb);\n\t\ttx->sta->tx_stats.bytes[ac] += skb->len;\n\t}\n\tif (ac >= 0)\n\t\ttx->sta->tx_stats.packets[ac]++;\n\n\treturn TX_CONTINUE;\n}\n\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_encrypt(struct ieee80211_tx_data *tx)\n{\n\tif (!tx->key)\n\t\treturn TX_CONTINUE;\n\n\tswitch (tx->key->conf.cipher) {\n\tcase WLAN_CIPHER_SUITE_WEP40:\n\tcase WLAN_CIPHER_SUITE_WEP104:\n\t\treturn ieee80211_crypto_wep_encrypt(tx);\n\tcase WLAN_CIPHER_SUITE_TKIP:\n\t\treturn ieee80211_crypto_tkip_encrypt(tx);\n\tcase WLAN_CIPHER_SUITE_CCMP:\n\t\treturn ieee80211_crypto_ccmp_encrypt(\n\t\t\ttx, IEEE80211_CCMP_MIC_LEN);\n\tcase WLAN_CIPHER_SUITE_CCMP_256:\n\t\treturn ieee80211_crypto_ccmp_encrypt(\n\t\t\ttx, IEEE80211_CCMP_256_MIC_LEN);\n\tcase WLAN_CIPHER_SUITE_AES_CMAC:\n\t\treturn ieee80211_crypto_aes_cmac_encrypt(tx);\n\tcase WLAN_CIPHER_SUITE_BIP_CMAC_256:\n\t\treturn ieee80211_crypto_aes_cmac_256_encrypt(tx);\n\tcase WLAN_CIPHER_SUITE_BIP_GMAC_128:\n\tcase WLAN_CIPHER_SUITE_BIP_GMAC_256:\n\t\treturn ieee80211_crypto_aes_gmac_encrypt(tx);\n\tcase WLAN_CIPHER_SUITE_GCMP:\n\tcase WLAN_CIPHER_SUITE_GCMP_256:\n\t\treturn ieee80211_crypto_gcmp_encrypt(tx);\n\tdefault:\n\t\treturn ieee80211_crypto_hw_encrypt(tx);\n\t}\n\n\treturn TX_DROP;\n}\n\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_calculate_duration(struct ieee80211_tx_data *tx)\n{\n\tstruct sk_buff *skb;\n\tstruct ieee80211_hdr *hdr;\n\tint next_len;\n\tbool group_addr;\n\n\tskb_queue_walk(&tx->skbs, skb) {\n\t\thdr = (void *) skb->data;\n\t\tif (unlikely(ieee80211_is_pspoll(hdr->frame_control)))\n\t\t\tbreak; /* must not overwrite AID */\n\t\tif (!skb_queue_is_last(&tx->skbs, skb)) {\n\t\t\tstruct sk_buff *next = skb_queue_next(&tx->skbs, skb);\n\t\t\tnext_len = next->len;\n\t\t} else\n\t\t\tnext_len = 0;\n\t\tgroup_addr = is_multicast_ether_addr(hdr->addr1);\n\n\t\thdr->duration_id =\n\t\t\tieee80211_duration(tx, skb, group_addr, next_len);\n\t}\n\n\treturn TX_CONTINUE;\n}\n\n/* actual transmit path */\n\nstatic bool ieee80211_tx_prep_agg(struct ieee80211_tx_data *tx,\n\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t  struct ieee80211_tx_info *info,\n\t\t\t\t  struct tid_ampdu_tx *tid_tx,\n\t\t\t\t  int tid)\n{\n\tbool queued = false;\n\tbool reset_agg_timer = false;\n\tstruct sk_buff *purge_skb = NULL;\n\n\tif (test_bit(HT_AGG_STATE_OPERATIONAL, &tid_tx->state)) {\n\t\tinfo->flags |= IEEE80211_TX_CTL_AMPDU;\n\t\treset_agg_timer = true;\n\t} else if (test_bit(HT_AGG_STATE_WANT_START, &tid_tx->state)) {\n\t\t/*\n\t\t * nothing -- this aggregation session is being started\n\t\t * but that might still fail with the driver\n\t\t */\n\t} else if (!tx->sta->sta.txq[tid]) {\n\t\tspin_lock(&tx->sta->lock);\n\t\t/*\n\t\t * Need to re-check now, because we may get here\n\t\t *\n\t\t *  1) in the window during which the setup is actually\n\t\t *     already done, but not marked yet because not all\n\t\t *     packets are spliced over to the driver pending\n\t\t *     queue yet -- if this happened we acquire the lock\n\t\t *     either before or after the splice happens, but\n\t\t *     need to recheck which of these cases happened.\n\t\t *\n\t\t *  2) during session teardown, if the OPERATIONAL bit\n\t\t *     was cleared due to the teardown but the pointer\n\t\t *     hasn't been assigned NULL yet (or we loaded it\n\t\t *     before it was assigned) -- in this case it may\n\t\t *     now be NULL which means we should just let the\n\t\t *     packet pass through because splicing the frames\n\t\t *     back is already done.\n\t\t */\n\t\ttid_tx = rcu_dereference_protected_tid_tx(tx->sta, tid);\n\n\t\tif (!tid_tx) {\n\t\t\t/* do nothing, let packet pass through */\n\t\t} else if (test_bit(HT_AGG_STATE_OPERATIONAL, &tid_tx->state)) {\n\t\t\tinfo->flags |= IEEE80211_TX_CTL_AMPDU;\n\t\t\treset_agg_timer = true;\n\t\t} else {\n\t\t\tqueued = true;\n\t\t\tif (info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER) {\n\t\t\t\tclear_sta_flag(tx->sta, WLAN_STA_SP);\n\t\t\t\tps_dbg(tx->sta->sdata,\n\t\t\t\t       \"STA %pM aid %d: SP frame queued, close the SP w/o telling the peer\\n\",\n\t\t\t\t       tx->sta->sta.addr, tx->sta->sta.aid);\n\t\t\t}\n\t\t\tinfo->control.vif = &tx->sdata->vif;\n\t\t\tinfo->control.flags |= IEEE80211_TX_INTCFL_NEED_TXPROCESSING;\n\t\t\tinfo->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;\n\t\t\t__skb_queue_tail(&tid_tx->pending, skb);\n\t\t\tif (skb_queue_len(&tid_tx->pending) > STA_MAX_TX_BUFFER)\n\t\t\t\tpurge_skb = __skb_dequeue(&tid_tx->pending);\n\t\t}\n\t\tspin_unlock(&tx->sta->lock);\n\n\t\tif (purge_skb)\n\t\t\tieee80211_free_txskb(&tx->local->hw, purge_skb);\n\t}\n\n\t/* reset session timer */\n\tif (reset_agg_timer)\n\t\ttid_tx->last_tx = jiffies;\n\n\treturn queued;\n}\n\n/*\n * initialises @tx\n * pass %NULL for the station if unknown, a valid pointer if known\n * or an ERR_PTR() if the station is known not to exist\n */\nstatic ieee80211_tx_result\nieee80211_tx_prepare(struct ieee80211_sub_if_data *sdata,\n\t\t     struct ieee80211_tx_data *tx,\n\t\t     struct sta_info *sta, struct sk_buff *skb)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct ieee80211_hdr *hdr;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tint tid;\n\n\tmemset(tx, 0, sizeof(*tx));\n\ttx->skb = skb;\n\ttx->local = local;\n\ttx->sdata = sdata;\n\t__skb_queue_head_init(&tx->skbs);\n\n\t/*\n\t * If this flag is set to true anywhere, and we get here,\n\t * we are doing the needed processing, so remove the flag\n\t * now.\n\t */\n\tinfo->control.flags &= ~IEEE80211_TX_INTCFL_NEED_TXPROCESSING;\n\n\thdr = (struct ieee80211_hdr *) skb->data;\n\n\tif (likely(sta)) {\n\t\tif (!IS_ERR(sta))\n\t\t\ttx->sta = sta;\n\t} else {\n\t\tif (sdata->vif.type == NL80211_IFTYPE_AP_VLAN) {\n\t\t\ttx->sta = rcu_dereference(sdata->u.vlan.sta);\n\t\t\tif (!tx->sta && sdata->wdev.use_4addr)\n\t\t\t\treturn TX_DROP;\n\t\t} else if (tx->sdata->control_port_protocol == tx->skb->protocol) {\n\t\t\ttx->sta = sta_info_get_bss(sdata, hdr->addr1);\n\t\t}\n\t\tif (!tx->sta && !is_multicast_ether_addr(hdr->addr1))\n\t\t\ttx->sta = sta_info_get(sdata, hdr->addr1);\n\t}\n\n\tif (tx->sta && ieee80211_is_data_qos(hdr->frame_control) &&\n\t    !ieee80211_is_qos_nullfunc(hdr->frame_control) &&\n\t    ieee80211_hw_check(&local->hw, AMPDU_AGGREGATION) &&\n\t    !ieee80211_hw_check(&local->hw, TX_AMPDU_SETUP_IN_HW)) {\n\t\tstruct tid_ampdu_tx *tid_tx;\n\n\t\ttid = ieee80211_get_tid(hdr);\n\n\t\ttid_tx = rcu_dereference(tx->sta->ampdu_mlme.tid_tx[tid]);\n\t\tif (tid_tx) {\n\t\t\tbool queued;\n\n\t\t\tqueued = ieee80211_tx_prep_agg(tx, skb, info,\n\t\t\t\t\t\t       tid_tx, tid);\n\n\t\t\tif (unlikely(queued))\n\t\t\t\treturn TX_QUEUED;\n\t\t}\n\t}\n\n\tif (is_multicast_ether_addr(hdr->addr1)) {\n\t\ttx->flags &= ~IEEE80211_TX_UNICAST;\n\t\tinfo->flags |= IEEE80211_TX_CTL_NO_ACK;\n\t} else\n\t\ttx->flags |= IEEE80211_TX_UNICAST;\n\n\tif (!(info->flags & IEEE80211_TX_CTL_DONTFRAG)) {\n\t\tif (!(tx->flags & IEEE80211_TX_UNICAST) ||\n\t\t    skb->len + FCS_LEN <= local->hw.wiphy->frag_threshold ||\n\t\t    info->flags & IEEE80211_TX_CTL_AMPDU)\n\t\t\tinfo->flags |= IEEE80211_TX_CTL_DONTFRAG;\n\t}\n\n\tif (!tx->sta)\n\t\tinfo->flags |= IEEE80211_TX_CTL_CLEAR_PS_FILT;\n\telse if (test_and_clear_sta_flag(tx->sta, WLAN_STA_CLEAR_PS_FILT)) {\n\t\tinfo->flags |= IEEE80211_TX_CTL_CLEAR_PS_FILT;\n\t\tieee80211_check_fast_xmit(tx->sta);\n\t}\n\n\tinfo->flags |= IEEE80211_TX_CTL_FIRST_FRAGMENT;\n\n\treturn TX_CONTINUE;\n}\n\nstatic struct txq_info *ieee80211_get_txq(struct ieee80211_local *local,\n\t\t\t\t\t  struct ieee80211_vif *vif,\n\t\t\t\t\t  struct sta_info *sta,\n\t\t\t\t\t  struct sk_buff *skb)\n{\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *) skb->data;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_txq *txq = NULL;\n\n\tif ((info->flags & IEEE80211_TX_CTL_SEND_AFTER_DTIM) ||\n\t    (info->control.flags & IEEE80211_TX_CTRL_PS_RESPONSE))\n\t\treturn NULL;\n\n\tif (!(info->flags & IEEE80211_TX_CTL_HW_80211_ENCAP) &&\n\t    unlikely(!ieee80211_is_data_present(hdr->frame_control))) {\n\t\tif ((!ieee80211_is_mgmt(hdr->frame_control) ||\n\t\t     ieee80211_is_bufferable_mmpdu(hdr->frame_control) ||\n\t\t     vif->type == NL80211_IFTYPE_STATION) &&\n\t\t    sta && sta->uploaded) {\n\t\t\t/*\n\t\t\t * This will be NULL if the driver didn't set the\n\t\t\t * opt-in hardware flag.\n\t\t\t */\n\t\t\ttxq = sta->sta.txq[IEEE80211_NUM_TIDS];\n\t\t}\n\t} else if (sta) {\n\t\tu8 tid = skb->priority & IEEE80211_QOS_CTL_TID_MASK;\n\n\t\tif (!sta->uploaded)\n\t\t\treturn NULL;\n\n\t\ttxq = sta->sta.txq[tid];\n\t} else if (vif) {\n\t\ttxq = vif->txq;\n\t}\n\n\tif (!txq)\n\t\treturn NULL;\n\n\treturn to_txq_info(txq);\n}\n\nstatic void ieee80211_set_skb_enqueue_time(struct sk_buff *skb)\n{\n\tIEEE80211_SKB_CB(skb)->control.enqueue_time = codel_get_time();\n}\n\nstatic u32 codel_skb_len_func(const struct sk_buff *skb)\n{\n\treturn skb->len;\n}\n\nstatic codel_time_t codel_skb_time_func(const struct sk_buff *skb)\n{\n\tconst struct ieee80211_tx_info *info;\n\n\tinfo = (const struct ieee80211_tx_info *)skb->cb;\n\treturn info->control.enqueue_time;\n}\n\nstatic struct sk_buff *codel_dequeue_func(struct codel_vars *cvars,\n\t\t\t\t\t  void *ctx)\n{\n\tstruct ieee80211_local *local;\n\tstruct txq_info *txqi;\n\tstruct fq *fq;\n\tstruct fq_flow *flow;\n\n\ttxqi = ctx;\n\tlocal = vif_to_sdata(txqi->txq.vif)->local;\n\tfq = &local->fq;\n\n\tif (cvars == &txqi->def_cvars)\n\t\tflow = &txqi->tin.default_flow;\n\telse\n\t\tflow = &fq->flows[cvars - local->cvars];\n\n\treturn fq_flow_dequeue(fq, flow);\n}\n\nstatic void codel_drop_func(struct sk_buff *skb,\n\t\t\t    void *ctx)\n{\n\tstruct ieee80211_local *local;\n\tstruct ieee80211_hw *hw;\n\tstruct txq_info *txqi;\n\n\ttxqi = ctx;\n\tlocal = vif_to_sdata(txqi->txq.vif)->local;\n\thw = &local->hw;\n\n\tieee80211_free_txskb(hw, skb);\n}\n\nstatic struct sk_buff *fq_tin_dequeue_func(struct fq *fq,\n\t\t\t\t\t   struct fq_tin *tin,\n\t\t\t\t\t   struct fq_flow *flow)\n{\n\tstruct ieee80211_local *local;\n\tstruct txq_info *txqi;\n\tstruct codel_vars *cvars;\n\tstruct codel_params *cparams;\n\tstruct codel_stats *cstats;\n\n\tlocal = container_of(fq, struct ieee80211_local, fq);\n\ttxqi = container_of(tin, struct txq_info, tin);\n\tcstats = &txqi->cstats;\n\n\tif (txqi->txq.sta) {\n\t\tstruct sta_info *sta = container_of(txqi->txq.sta,\n\t\t\t\t\t\t    struct sta_info, sta);\n\t\tcparams = &sta->cparams;\n\t} else {\n\t\tcparams = &local->cparams;\n\t}\n\n\tif (flow == &tin->default_flow)\n\t\tcvars = &txqi->def_cvars;\n\telse\n\t\tcvars = &local->cvars[flow - fq->flows];\n\n\treturn codel_dequeue(txqi,\n\t\t\t     &flow->backlog,\n\t\t\t     cparams,\n\t\t\t     cvars,\n\t\t\t     cstats,\n\t\t\t     codel_skb_len_func,\n\t\t\t     codel_skb_time_func,\n\t\t\t     codel_drop_func,\n\t\t\t     codel_dequeue_func);\n}\n\nstatic void fq_skb_free_func(struct fq *fq,\n\t\t\t     struct fq_tin *tin,\n\t\t\t     struct fq_flow *flow,\n\t\t\t     struct sk_buff *skb)\n{\n\tstruct ieee80211_local *local;\n\n\tlocal = container_of(fq, struct ieee80211_local, fq);\n\tieee80211_free_txskb(&local->hw, skb);\n}\n\nstatic void ieee80211_txq_enqueue(struct ieee80211_local *local,\n\t\t\t\t  struct txq_info *txqi,\n\t\t\t\t  struct sk_buff *skb)\n{\n\tstruct fq *fq = &local->fq;\n\tstruct fq_tin *tin = &txqi->tin;\n\tu32 flow_idx = fq_flow_idx(fq, skb);\n\n\tieee80211_set_skb_enqueue_time(skb);\n\n\tspin_lock_bh(&fq->lock);\n\tfq_tin_enqueue(fq, tin, flow_idx, skb,\n\t\t       fq_skb_free_func);\n\tspin_unlock_bh(&fq->lock);\n}\n\nstatic bool fq_vlan_filter_func(struct fq *fq, struct fq_tin *tin,\n\t\t\t\tstruct fq_flow *flow, struct sk_buff *skb,\n\t\t\t\tvoid *data)\n{\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\n\treturn info->control.vif == data;\n}\n\nvoid ieee80211_txq_remove_vlan(struct ieee80211_local *local,\n\t\t\t       struct ieee80211_sub_if_data *sdata)\n{\n\tstruct fq *fq = &local->fq;\n\tstruct txq_info *txqi;\n\tstruct fq_tin *tin;\n\tstruct ieee80211_sub_if_data *ap;\n\n\tif (WARN_ON(sdata->vif.type != NL80211_IFTYPE_AP_VLAN))\n\t\treturn;\n\n\tap = container_of(sdata->bss, struct ieee80211_sub_if_data, u.ap);\n\n\tif (!ap->vif.txq)\n\t\treturn;\n\n\ttxqi = to_txq_info(ap->vif.txq);\n\ttin = &txqi->tin;\n\n\tspin_lock_bh(&fq->lock);\n\tfq_tin_filter(fq, tin, fq_vlan_filter_func, &sdata->vif,\n\t\t      fq_skb_free_func);\n\tspin_unlock_bh(&fq->lock);\n}\n\nvoid ieee80211_txq_init(struct ieee80211_sub_if_data *sdata,\n\t\t\tstruct sta_info *sta,\n\t\t\tstruct txq_info *txqi, int tid)\n{\n\tfq_tin_init(&txqi->tin);\n\tcodel_vars_init(&txqi->def_cvars);\n\tcodel_stats_init(&txqi->cstats);\n\t__skb_queue_head_init(&txqi->frags);\n\tINIT_LIST_HEAD(&txqi->schedule_order);\n\n\ttxqi->txq.vif = &sdata->vif;\n\n\tif (!sta) {\n\t\tsdata->vif.txq = &txqi->txq;\n\t\ttxqi->txq.tid = 0;\n\t\ttxqi->txq.ac = IEEE80211_AC_BE;\n\n\t\treturn;\n\t}\n\n\tif (tid == IEEE80211_NUM_TIDS) {\n\t\tif (sdata->vif.type == NL80211_IFTYPE_STATION) {\n\t\t\t/* Drivers need to opt in to the management MPDU TXQ */\n\t\t\tif (!ieee80211_hw_check(&sdata->local->hw,\n\t\t\t\t\t\tSTA_MMPDU_TXQ))\n\t\t\t\treturn;\n\t\t} else if (!ieee80211_hw_check(&sdata->local->hw,\n\t\t\t\t\t       BUFF_MMPDU_TXQ)) {\n\t\t\t/* Drivers need to opt in to the bufferable MMPDU TXQ */\n\t\t\treturn;\n\t\t}\n\t\ttxqi->txq.ac = IEEE80211_AC_VO;\n\t} else {\n\t\ttxqi->txq.ac = ieee80211_ac_from_tid(tid);\n\t}\n\n\ttxqi->txq.sta = &sta->sta;\n\ttxqi->txq.tid = tid;\n\tsta->sta.txq[tid] = &txqi->txq;\n}\n\nvoid ieee80211_txq_purge(struct ieee80211_local *local,\n\t\t\t struct txq_info *txqi)\n{\n\tstruct fq *fq = &local->fq;\n\tstruct fq_tin *tin = &txqi->tin;\n\n\tspin_lock_bh(&fq->lock);\n\tfq_tin_reset(fq, tin, fq_skb_free_func);\n\tieee80211_purge_tx_queue(&local->hw, &txqi->frags);\n\tspin_unlock_bh(&fq->lock);\n\n\tspin_lock_bh(&local->active_txq_lock[txqi->txq.ac]);\n\tlist_del_init(&txqi->schedule_order);\n\tspin_unlock_bh(&local->active_txq_lock[txqi->txq.ac]);\n}\n\nvoid ieee80211_txq_set_params(struct ieee80211_local *local)\n{\n\tif (local->hw.wiphy->txq_limit)\n\t\tlocal->fq.limit = local->hw.wiphy->txq_limit;\n\telse\n\t\tlocal->hw.wiphy->txq_limit = local->fq.limit;\n\n\tif (local->hw.wiphy->txq_memory_limit)\n\t\tlocal->fq.memory_limit = local->hw.wiphy->txq_memory_limit;\n\telse\n\t\tlocal->hw.wiphy->txq_memory_limit = local->fq.memory_limit;\n\n\tif (local->hw.wiphy->txq_quantum)\n\t\tlocal->fq.quantum = local->hw.wiphy->txq_quantum;\n\telse\n\t\tlocal->hw.wiphy->txq_quantum = local->fq.quantum;\n}\n\nint ieee80211_txq_setup_flows(struct ieee80211_local *local)\n{\n\tstruct fq *fq = &local->fq;\n\tint ret;\n\tint i;\n\tbool supp_vht = false;\n\tenum nl80211_band band;\n\n\tif (!local->ops->wake_tx_queue)\n\t\treturn 0;\n\n\tret = fq_init(fq, 4096);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * If the hardware doesn't support VHT, it is safe to limit the maximum\n\t * queue size. 4 Mbytes is 64 max-size aggregates in 802.11n.\n\t */\n\tfor (band = 0; band < NUM_NL80211_BANDS; band++) {\n\t\tstruct ieee80211_supported_band *sband;\n\n\t\tsband = local->hw.wiphy->bands[band];\n\t\tif (!sband)\n\t\t\tcontinue;\n\n\t\tsupp_vht = supp_vht || sband->vht_cap.vht_supported;\n\t}\n\n\tif (!supp_vht)\n\t\tfq->memory_limit = 4 << 20; /* 4 Mbytes */\n\n\tcodel_params_init(&local->cparams);\n\tlocal->cparams.interval = MS2TIME(100);\n\tlocal->cparams.target = MS2TIME(20);\n\tlocal->cparams.ecn = true;\n\n\tlocal->cvars = kcalloc(fq->flows_cnt, sizeof(local->cvars[0]),\n\t\t\t       GFP_KERNEL);\n\tif (!local->cvars) {\n\t\tspin_lock_bh(&fq->lock);\n\t\tfq_reset(fq, fq_skb_free_func);\n\t\tspin_unlock_bh(&fq->lock);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < fq->flows_cnt; i++)\n\t\tcodel_vars_init(&local->cvars[i]);\n\n\tieee80211_txq_set_params(local);\n\n\treturn 0;\n}\n\nvoid ieee80211_txq_teardown_flows(struct ieee80211_local *local)\n{\n\tstruct fq *fq = &local->fq;\n\n\tif (!local->ops->wake_tx_queue)\n\t\treturn;\n\n\tkfree(local->cvars);\n\tlocal->cvars = NULL;\n\n\tspin_lock_bh(&fq->lock);\n\tfq_reset(fq, fq_skb_free_func);\n\tspin_unlock_bh(&fq->lock);\n}\n\nstatic bool ieee80211_queue_skb(struct ieee80211_local *local,\n\t\t\t\tstruct ieee80211_sub_if_data *sdata,\n\t\t\t\tstruct sta_info *sta,\n\t\t\t\tstruct sk_buff *skb)\n{\n\tstruct ieee80211_vif *vif;\n\tstruct txq_info *txqi;\n\n\tif (!local->ops->wake_tx_queue ||\n\t    sdata->vif.type == NL80211_IFTYPE_MONITOR)\n\t\treturn false;\n\n\tif (sdata->vif.type == NL80211_IFTYPE_AP_VLAN)\n\t\tsdata = container_of(sdata->bss,\n\t\t\t\t     struct ieee80211_sub_if_data, u.ap);\n\n\tvif = &sdata->vif;\n\ttxqi = ieee80211_get_txq(local, vif, sta, skb);\n\n\tif (!txqi)\n\t\treturn false;\n\n\tieee80211_txq_enqueue(local, txqi, skb);\n\n\tschedule_and_wake_txq(local, txqi);\n\n\treturn true;\n}\n\nstatic bool ieee80211_tx_frags(struct ieee80211_local *local,\n\t\t\t       struct ieee80211_vif *vif,\n\t\t\t       struct sta_info *sta,\n\t\t\t       struct sk_buff_head *skbs,\n\t\t\t       bool txpending)\n{\n\tstruct ieee80211_tx_control control = {};\n\tstruct sk_buff *skb, *tmp;\n\tunsigned long flags;\n\n\tskb_queue_walk_safe(skbs, skb, tmp) {\n\t\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\t\tint q = info->hw_queue;\n\n#ifdef CONFIG_MAC80211_VERBOSE_DEBUG\n\t\tif (WARN_ON_ONCE(q >= local->hw.queues)) {\n\t\t\t__skb_unlink(skb, skbs);\n\t\t\tieee80211_free_txskb(&local->hw, skb);\n\t\t\tcontinue;\n\t\t}\n#endif\n\n\t\tspin_lock_irqsave(&local->queue_stop_reason_lock, flags);\n\t\tif (local->queue_stop_reasons[q] ||\n\t\t    (!txpending && !skb_queue_empty(&local->pending[q]))) {\n\t\t\tif (unlikely(info->flags &\n\t\t\t\t     IEEE80211_TX_INTFL_OFFCHAN_TX_OK)) {\n\t\t\t\tif (local->queue_stop_reasons[q] &\n\t\t\t\t    ~BIT(IEEE80211_QUEUE_STOP_REASON_OFFCHANNEL)) {\n\t\t\t\t\t/*\n\t\t\t\t\t * Drop off-channel frames if queues\n\t\t\t\t\t * are stopped for any reason other\n\t\t\t\t\t * than off-channel operation. Never\n\t\t\t\t\t * queue them.\n\t\t\t\t\t */\n\t\t\t\t\tspin_unlock_irqrestore(\n\t\t\t\t\t\t&local->queue_stop_reason_lock,\n\t\t\t\t\t\tflags);\n\t\t\t\t\tieee80211_purge_tx_queue(&local->hw,\n\t\t\t\t\t\t\t\t skbs);\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t} else {\n\n\t\t\t\t/*\n\t\t\t\t * Since queue is stopped, queue up frames for\n\t\t\t\t * later transmission from the tx-pending\n\t\t\t\t * tasklet when the queue is woken again.\n\t\t\t\t */\n\t\t\t\tif (txpending)\n\t\t\t\t\tskb_queue_splice_init(skbs,\n\t\t\t\t\t\t\t      &local->pending[q]);\n\t\t\t\telse\n\t\t\t\t\tskb_queue_splice_tail_init(skbs,\n\t\t\t\t\t\t\t\t   &local->pending[q]);\n\n\t\t\t\tspin_unlock_irqrestore(&local->queue_stop_reason_lock,\n\t\t\t\t\t\t       flags);\n\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&local->queue_stop_reason_lock, flags);\n\n\t\tinfo->control.vif = vif;\n\t\tcontrol.sta = sta ? &sta->sta : NULL;\n\n\t\t__skb_unlink(skb, skbs);\n\t\tdrv_tx(local, &control, skb);\n\t}\n\n\treturn true;\n}\n\n/*\n * Returns false if the frame couldn't be transmitted but was queued instead.\n */\nstatic bool __ieee80211_tx(struct ieee80211_local *local,\n\t\t\t   struct sk_buff_head *skbs, int led_len,\n\t\t\t   struct sta_info *sta, bool txpending)\n{\n\tstruct ieee80211_tx_info *info;\n\tstruct ieee80211_sub_if_data *sdata;\n\tstruct ieee80211_vif *vif;\n\tstruct sk_buff *skb;\n\tbool result = true;\n\t__le16 fc;\n\n\tif (WARN_ON(skb_queue_empty(skbs)))\n\t\treturn true;\n\n\tskb = skb_peek(skbs);\n\tfc = ((struct ieee80211_hdr *)skb->data)->frame_control;\n\tinfo = IEEE80211_SKB_CB(skb);\n\tsdata = vif_to_sdata(info->control.vif);\n\tif (sta && !sta->uploaded)\n\t\tsta = NULL;\n\n\tswitch (sdata->vif.type) {\n\tcase NL80211_IFTYPE_MONITOR:\n\t\tif (sdata->u.mntr.flags & MONITOR_FLAG_ACTIVE) {\n\t\t\tvif = &sdata->vif;\n\t\t\tbreak;\n\t\t}\n\t\tsdata = rcu_dereference(local->monitor_sdata);\n\t\tif (sdata) {\n\t\t\tvif = &sdata->vif;\n\t\t\tinfo->hw_queue =\n\t\t\t\tvif->hw_queue[skb_get_queue_mapping(skb)];\n\t\t} else if (ieee80211_hw_check(&local->hw, QUEUE_CONTROL)) {\n\t\t\tieee80211_purge_tx_queue(&local->hw, skbs);\n\t\t\treturn true;\n\t\t} else\n\t\t\tvif = NULL;\n\t\tbreak;\n\tcase NL80211_IFTYPE_AP_VLAN:\n\t\tsdata = container_of(sdata->bss,\n\t\t\t\t     struct ieee80211_sub_if_data, u.ap);\n\t\tfallthrough;\n\tdefault:\n\t\tvif = &sdata->vif;\n\t\tbreak;\n\t}\n\n\tresult = ieee80211_tx_frags(local, vif, sta, skbs, txpending);\n\n\tieee80211_tpt_led_trig_tx(local, fc, led_len);\n\n\tWARN_ON_ONCE(!skb_queue_empty(skbs));\n\n\treturn result;\n}\n\n/*\n * Invoke TX handlers, return 0 on success and non-zero if the\n * frame was dropped or queued.\n *\n * The handlers are split into an early and late part. The latter is everything\n * that can be sensitive to reordering, and will be deferred to after packets\n * are dequeued from the intermediate queues (when they are enabled).\n */\nstatic int invoke_tx_handlers_early(struct ieee80211_tx_data *tx)\n{\n\tieee80211_tx_result res = TX_DROP;\n\n#define CALL_TXH(txh) \\\n\tdo {\t\t\t\t\\\n\t\tres = txh(tx);\t\t\\\n\t\tif (res != TX_CONTINUE)\t\\\n\t\t\tgoto txh_done;\t\\\n\t} while (0)\n\n\tCALL_TXH(ieee80211_tx_h_dynamic_ps);\n\tCALL_TXH(ieee80211_tx_h_check_assoc);\n\tCALL_TXH(ieee80211_tx_h_ps_buf);\n\tCALL_TXH(ieee80211_tx_h_check_control_port_protocol);\n\tCALL_TXH(ieee80211_tx_h_select_key);\n\tif (!ieee80211_hw_check(&tx->local->hw, HAS_RATE_CONTROL))\n\t\tCALL_TXH(ieee80211_tx_h_rate_ctrl);\n\n txh_done:\n\tif (unlikely(res == TX_DROP)) {\n\t\tI802_DEBUG_INC(tx->local->tx_handlers_drop);\n\t\tif (tx->skb)\n\t\t\tieee80211_free_txskb(&tx->local->hw, tx->skb);\n\t\telse\n\t\t\tieee80211_purge_tx_queue(&tx->local->hw, &tx->skbs);\n\t\treturn -1;\n\t} else if (unlikely(res == TX_QUEUED)) {\n\t\tI802_DEBUG_INC(tx->local->tx_handlers_queued);\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Late handlers can be called while the sta lock is held. Handlers that can\n * cause packets to be generated will cause deadlock!\n */\nstatic int invoke_tx_handlers_late(struct ieee80211_tx_data *tx)\n{\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\tieee80211_tx_result res = TX_CONTINUE;\n\n\tif (unlikely(info->flags & IEEE80211_TX_INTFL_RETRANSMISSION)) {\n\t\t__skb_queue_tail(&tx->skbs, tx->skb);\n\t\ttx->skb = NULL;\n\t\tgoto txh_done;\n\t}\n\n\tCALL_TXH(ieee80211_tx_h_michael_mic_add);\n\tCALL_TXH(ieee80211_tx_h_sequence);\n\tCALL_TXH(ieee80211_tx_h_fragment);\n\t/* handlers after fragment must be aware of tx info fragmentation! */\n\tCALL_TXH(ieee80211_tx_h_stats);\n\tCALL_TXH(ieee80211_tx_h_encrypt);\n\tif (!ieee80211_hw_check(&tx->local->hw, HAS_RATE_CONTROL))\n\t\tCALL_TXH(ieee80211_tx_h_calculate_duration);\n#undef CALL_TXH\n\n txh_done:\n\tif (unlikely(res == TX_DROP)) {\n\t\tI802_DEBUG_INC(tx->local->tx_handlers_drop);\n\t\tif (tx->skb)\n\t\t\tieee80211_free_txskb(&tx->local->hw, tx->skb);\n\t\telse\n\t\t\tieee80211_purge_tx_queue(&tx->local->hw, &tx->skbs);\n\t\treturn -1;\n\t} else if (unlikely(res == TX_QUEUED)) {\n\t\tI802_DEBUG_INC(tx->local->tx_handlers_queued);\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\nstatic int invoke_tx_handlers(struct ieee80211_tx_data *tx)\n{\n\tint r = invoke_tx_handlers_early(tx);\n\n\tif (r)\n\t\treturn r;\n\treturn invoke_tx_handlers_late(tx);\n}\n\nbool ieee80211_tx_prepare_skb(struct ieee80211_hw *hw,\n\t\t\t      struct ieee80211_vif *vif, struct sk_buff *skb,\n\t\t\t      int band, struct ieee80211_sta **sta)\n{\n\tstruct ieee80211_sub_if_data *sdata = vif_to_sdata(vif);\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_tx_data tx;\n\tstruct sk_buff *skb2;\n\n\tif (ieee80211_tx_prepare(sdata, &tx, NULL, skb) == TX_DROP)\n\t\treturn false;\n\n\tinfo->band = band;\n\tinfo->control.vif = vif;\n\tinfo->hw_queue = vif->hw_queue[skb_get_queue_mapping(skb)];\n\n\tif (invoke_tx_handlers(&tx))\n\t\treturn false;\n\n\tif (sta) {\n\t\tif (tx.sta)\n\t\t\t*sta = &tx.sta->sta;\n\t\telse\n\t\t\t*sta = NULL;\n\t}\n\n\t/* this function isn't suitable for fragmented data frames */\n\tskb2 = __skb_dequeue(&tx.skbs);\n\tif (WARN_ON(skb2 != skb || !skb_queue_empty(&tx.skbs))) {\n\t\tieee80211_free_txskb(hw, skb2);\n\t\tieee80211_purge_tx_queue(hw, &tx.skbs);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\nEXPORT_SYMBOL(ieee80211_tx_prepare_skb);\n\n/*\n * Returns false if the frame couldn't be transmitted but was queued instead.\n */\nstatic bool ieee80211_tx(struct ieee80211_sub_if_data *sdata,\n\t\t\t struct sta_info *sta, struct sk_buff *skb,\n\t\t\t bool txpending)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct ieee80211_tx_data tx;\n\tieee80211_tx_result res_prepare;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tbool result = true;\n\tint led_len;\n\n\tif (unlikely(skb->len < 10)) {\n\t\tdev_kfree_skb(skb);\n\t\treturn true;\n\t}\n\n\t/* initialises tx */\n\tled_len = skb->len;\n\tres_prepare = ieee80211_tx_prepare(sdata, &tx, sta, skb);\n\n\tif (unlikely(res_prepare == TX_DROP)) {\n\t\tieee80211_free_txskb(&local->hw, skb);\n\t\treturn true;\n\t} else if (unlikely(res_prepare == TX_QUEUED)) {\n\t\treturn true;\n\t}\n\n\t/* set up hw_queue value early */\n\tif (!(info->flags & IEEE80211_TX_CTL_TX_OFFCHAN) ||\n\t    !ieee80211_hw_check(&local->hw, QUEUE_CONTROL))\n\t\tinfo->hw_queue =\n\t\t\tsdata->vif.hw_queue[skb_get_queue_mapping(skb)];\n\n\tif (invoke_tx_handlers_early(&tx))\n\t\treturn true;\n\n\tif (ieee80211_queue_skb(local, sdata, tx.sta, tx.skb))\n\t\treturn true;\n\n\tif (!invoke_tx_handlers_late(&tx))\n\t\tresult = __ieee80211_tx(local, &tx.skbs, led_len,\n\t\t\t\t\ttx.sta, txpending);\n\n\treturn result;\n}\n\n/* device xmit handlers */\n\nenum ieee80211_encrypt {\n\tENCRYPT_NO,\n\tENCRYPT_MGMT,\n\tENCRYPT_DATA,\n};\n\nstatic int ieee80211_skb_resize(struct ieee80211_sub_if_data *sdata,\n\t\t\t\tstruct sk_buff *skb,\n\t\t\t\tint head_need,\n\t\t\t\tenum ieee80211_encrypt encrypt)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tbool enc_tailroom;\n\tint tail_need = 0;\n\n\tenc_tailroom = encrypt == ENCRYPT_MGMT ||\n\t\t       (encrypt == ENCRYPT_DATA &&\n\t\t\tsdata->crypto_tx_tailroom_needed_cnt);\n\n\tif (enc_tailroom) {\n\t\ttail_need = IEEE80211_ENCRYPT_TAILROOM;\n\t\ttail_need -= skb_tailroom(skb);\n\t\ttail_need = max_t(int, tail_need, 0);\n\t}\n\n\tif (skb_cloned(skb) &&\n\t    (!ieee80211_hw_check(&local->hw, SUPPORTS_CLONED_SKBS) ||\n\t     !skb_clone_writable(skb, ETH_HLEN) || enc_tailroom))\n\t\tI802_DEBUG_INC(local->tx_expand_skb_head_cloned);\n\telse if (head_need || tail_need)\n\t\tI802_DEBUG_INC(local->tx_expand_skb_head);\n\telse\n\t\treturn 0;\n\n\tif (pskb_expand_head(skb, head_need, tail_need, GFP_ATOMIC)) {\n\t\twiphy_debug(local->hw.wiphy,\n\t\t\t    \"failed to reallocate TX buffer\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nvoid ieee80211_xmit(struct ieee80211_sub_if_data *sdata,\n\t\t    struct sta_info *sta, struct sk_buff *skb)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *) skb->data;\n\tint headroom;\n\tenum ieee80211_encrypt encrypt;\n\n\tif (info->flags & IEEE80211_TX_INTFL_DONT_ENCRYPT)\n\t\tencrypt = ENCRYPT_NO;\n\telse if (ieee80211_is_mgmt(hdr->frame_control))\n\t\tencrypt = ENCRYPT_MGMT;\n\telse\n\t\tencrypt = ENCRYPT_DATA;\n\n\theadroom = local->tx_headroom;\n\tif (encrypt != ENCRYPT_NO)\n\t\theadroom += sdata->encrypt_headroom;\n\theadroom -= skb_headroom(skb);\n\theadroom = max_t(int, 0, headroom);\n\n\tif (ieee80211_skb_resize(sdata, skb, headroom, encrypt)) {\n\t\tieee80211_free_txskb(&local->hw, skb);\n\t\treturn;\n\t}\n\n\t/* reload after potential resize */\n\thdr = (struct ieee80211_hdr *) skb->data;\n\tinfo->control.vif = &sdata->vif;\n\n\tif (ieee80211_vif_is_mesh(&sdata->vif)) {\n\t\tif (ieee80211_is_data(hdr->frame_control) &&\n\t\t    is_unicast_ether_addr(hdr->addr1)) {\n\t\t\tif (mesh_nexthop_resolve(sdata, skb))\n\t\t\t\treturn; /* skb queued: don't free */\n\t\t} else {\n\t\t\tieee80211_mps_set_frame_flags(sdata, NULL, hdr);\n\t\t}\n\t}\n\n\tieee80211_set_qos_hdr(sdata, skb);\n\tieee80211_tx(sdata, sta, skb, false);\n}\n\nbool ieee80211_parse_tx_radiotap(struct sk_buff *skb,\n\t\t\t\t struct net_device *dev)\n{\n\tstruct ieee80211_local *local = wdev_priv(dev->ieee80211_ptr);\n\tstruct ieee80211_radiotap_iterator iterator;\n\tstruct ieee80211_radiotap_header *rthdr =\n\t\t(struct ieee80211_radiotap_header *) skb->data;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_supported_band *sband =\n\t\tlocal->hw.wiphy->bands[info->band];\n\tint ret = ieee80211_radiotap_iterator_init(&iterator, rthdr, skb->len,\n\t\t\t\t\t\t   NULL);\n\tu16 txflags;\n\tu16 rate = 0;\n\tbool rate_found = false;\n\tu8 rate_retries = 0;\n\tu16 rate_flags = 0;\n\tu8 mcs_known, mcs_flags, mcs_bw;\n\tu16 vht_known;\n\tu8 vht_mcs = 0, vht_nss = 0;\n\tint i;\n\n\t/* check for not even having the fixed radiotap header part */\n\tif (unlikely(skb->len < sizeof(struct ieee80211_radiotap_header)))\n\t\treturn false; /* too short to be possibly valid */\n\n\t/* is it a header version we can trust to find length from? */\n\tif (unlikely(rthdr->it_version))\n\t\treturn false; /* only version 0 is supported */\n\n\t/* does the skb contain enough to deliver on the alleged length? */\n\tif (unlikely(skb->len < ieee80211_get_radiotap_len(skb->data)))\n\t\treturn false; /* skb too short for claimed rt header extent */\n\n\tinfo->flags |= IEEE80211_TX_INTFL_DONT_ENCRYPT |\n\t\t       IEEE80211_TX_CTL_DONTFRAG;\n\n\t/*\n\t * for every radiotap entry that is present\n\t * (ieee80211_radiotap_iterator_next returns -ENOENT when no more\n\t * entries present, or -EINVAL on error)\n\t */\n\n\twhile (!ret) {\n\t\tret = ieee80211_radiotap_iterator_next(&iterator);\n\n\t\tif (ret)\n\t\t\tcontinue;\n\n\t\t/* see if this argument is something we can use */\n\t\tswitch (iterator.this_arg_index) {\n\t\t/*\n\t\t * You must take care when dereferencing iterator.this_arg\n\t\t * for multibyte types... the pointer is not aligned.  Use\n\t\t * get_unaligned((type *)iterator.this_arg) to dereference\n\t\t * iterator.this_arg for type \"type\" safely on all arches.\n\t\t*/\n\t\tcase IEEE80211_RADIOTAP_FLAGS:\n\t\t\tif (*iterator.this_arg & IEEE80211_RADIOTAP_F_FCS) {\n\t\t\t\t/*\n\t\t\t\t * this indicates that the skb we have been\n\t\t\t\t * handed has the 32-bit FCS CRC at the end...\n\t\t\t\t * we should react to that by snipping it off\n\t\t\t\t * because it will be recomputed and added\n\t\t\t\t * on transmission\n\t\t\t\t */\n\t\t\t\tif (skb->len < (iterator._max_length + FCS_LEN))\n\t\t\t\t\treturn false;\n\n\t\t\t\tskb_trim(skb, skb->len - FCS_LEN);\n\t\t\t}\n\t\t\tif (*iterator.this_arg & IEEE80211_RADIOTAP_F_WEP)\n\t\t\t\tinfo->flags &= ~IEEE80211_TX_INTFL_DONT_ENCRYPT;\n\t\t\tif (*iterator.this_arg & IEEE80211_RADIOTAP_F_FRAG)\n\t\t\t\tinfo->flags &= ~IEEE80211_TX_CTL_DONTFRAG;\n\t\t\tbreak;\n\n\t\tcase IEEE80211_RADIOTAP_TX_FLAGS:\n\t\t\ttxflags = get_unaligned_le16(iterator.this_arg);\n\t\t\tif (txflags & IEEE80211_RADIOTAP_F_TX_NOACK)\n\t\t\t\tinfo->flags |= IEEE80211_TX_CTL_NO_ACK;\n\t\t\tif (txflags & IEEE80211_RADIOTAP_F_TX_NOSEQNO)\n\t\t\t\tinfo->control.flags |= IEEE80211_TX_CTRL_NO_SEQNO;\n\t\t\tif (txflags & IEEE80211_RADIOTAP_F_TX_ORDER)\n\t\t\t\tinfo->control.flags |=\n\t\t\t\t\tIEEE80211_TX_CTRL_DONT_REORDER;\n\t\t\tbreak;\n\n\t\tcase IEEE80211_RADIOTAP_RATE:\n\t\t\trate = *iterator.this_arg;\n\t\t\trate_flags = 0;\n\t\t\trate_found = true;\n\t\t\tbreak;\n\n\t\tcase IEEE80211_RADIOTAP_DATA_RETRIES:\n\t\t\trate_retries = *iterator.this_arg;\n\t\t\tbreak;\n\n\t\tcase IEEE80211_RADIOTAP_MCS:\n\t\t\tmcs_known = iterator.this_arg[0];\n\t\t\tmcs_flags = iterator.this_arg[1];\n\t\t\tif (!(mcs_known & IEEE80211_RADIOTAP_MCS_HAVE_MCS))\n\t\t\t\tbreak;\n\n\t\t\trate_found = true;\n\t\t\trate = iterator.this_arg[2];\n\t\t\trate_flags = IEEE80211_TX_RC_MCS;\n\n\t\t\tif (mcs_known & IEEE80211_RADIOTAP_MCS_HAVE_GI &&\n\t\t\t    mcs_flags & IEEE80211_RADIOTAP_MCS_SGI)\n\t\t\t\trate_flags |= IEEE80211_TX_RC_SHORT_GI;\n\n\t\t\tmcs_bw = mcs_flags & IEEE80211_RADIOTAP_MCS_BW_MASK;\n\t\t\tif (mcs_known & IEEE80211_RADIOTAP_MCS_HAVE_BW &&\n\t\t\t    mcs_bw == IEEE80211_RADIOTAP_MCS_BW_40)\n\t\t\t\trate_flags |= IEEE80211_TX_RC_40_MHZ_WIDTH;\n\n\t\t\tif (mcs_known & IEEE80211_RADIOTAP_MCS_HAVE_FEC &&\n\t\t\t    mcs_flags & IEEE80211_RADIOTAP_MCS_FEC_LDPC)\n\t\t\t\tinfo->flags |= IEEE80211_TX_CTL_LDPC;\n\n\t\t\tif (mcs_known & IEEE80211_RADIOTAP_MCS_HAVE_STBC) {\n\t\t\t\tu8 stbc = u8_get_bits(mcs_flags,\n\t\t\t\t\t\t      IEEE80211_RADIOTAP_MCS_STBC_MASK);\n\n\t\t\t\tinfo->flags |=\n\t\t\t\t\tu32_encode_bits(stbc,\n\t\t\t\t\t\t\tIEEE80211_TX_CTL_STBC);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase IEEE80211_RADIOTAP_VHT:\n\t\t\tvht_known = get_unaligned_le16(iterator.this_arg);\n\t\t\trate_found = true;\n\n\t\t\trate_flags = IEEE80211_TX_RC_VHT_MCS;\n\t\t\tif ((vht_known & IEEE80211_RADIOTAP_VHT_KNOWN_GI) &&\n\t\t\t    (iterator.this_arg[2] &\n\t\t\t     IEEE80211_RADIOTAP_VHT_FLAG_SGI))\n\t\t\t\trate_flags |= IEEE80211_TX_RC_SHORT_GI;\n\t\t\tif (vht_known &\n\t\t\t    IEEE80211_RADIOTAP_VHT_KNOWN_BANDWIDTH) {\n\t\t\t\tif (iterator.this_arg[3] == 1)\n\t\t\t\t\trate_flags |=\n\t\t\t\t\t\tIEEE80211_TX_RC_40_MHZ_WIDTH;\n\t\t\t\telse if (iterator.this_arg[3] == 4)\n\t\t\t\t\trate_flags |=\n\t\t\t\t\t\tIEEE80211_TX_RC_80_MHZ_WIDTH;\n\t\t\t\telse if (iterator.this_arg[3] == 11)\n\t\t\t\t\trate_flags |=\n\t\t\t\t\t\tIEEE80211_TX_RC_160_MHZ_WIDTH;\n\t\t\t}\n\n\t\t\tvht_mcs = iterator.this_arg[4] >> 4;\n\t\t\tvht_nss = iterator.this_arg[4] & 0xF;\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Please update the file\n\t\t * Documentation/networking/mac80211-injection.rst\n\t\t * when parsing new fields here.\n\t\t */\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (ret != -ENOENT) /* ie, if we didn't simply run out of fields */\n\t\treturn false;\n\n\tif (rate_found) {\n\t\tinfo->control.flags |= IEEE80211_TX_CTRL_RATE_INJECT;\n\n\t\tfor (i = 0; i < IEEE80211_TX_MAX_RATES; i++) {\n\t\t\tinfo->control.rates[i].idx = -1;\n\t\t\tinfo->control.rates[i].flags = 0;\n\t\t\tinfo->control.rates[i].count = 0;\n\t\t}\n\n\t\tif (rate_flags & IEEE80211_TX_RC_MCS) {\n\t\t\tinfo->control.rates[0].idx = rate;\n\t\t} else if (rate_flags & IEEE80211_TX_RC_VHT_MCS) {\n\t\t\tieee80211_rate_set_vht(info->control.rates, vht_mcs,\n\t\t\t\t\t       vht_nss);\n\t\t} else {\n\t\t\tfor (i = 0; i < sband->n_bitrates; i++) {\n\t\t\t\tif (rate * 5 != sband->bitrates[i].bitrate)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tinfo->control.rates[0].idx = i;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (info->control.rates[0].idx < 0)\n\t\t\tinfo->control.flags &= ~IEEE80211_TX_CTRL_RATE_INJECT;\n\n\t\tinfo->control.rates[0].flags = rate_flags;\n\t\tinfo->control.rates[0].count = min_t(u8, rate_retries + 1,\n\t\t\t\t\t\t     local->hw.max_rate_tries);\n\t}\n\n\treturn true;\n}\n\nnetdev_tx_t ieee80211_monitor_start_xmit(struct sk_buff *skb,\n\t\t\t\t\t struct net_device *dev)\n{\n\tstruct ieee80211_local *local = wdev_priv(dev->ieee80211_ptr);\n\tstruct ieee80211_chanctx_conf *chanctx_conf;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_hdr *hdr;\n\tstruct ieee80211_sub_if_data *tmp_sdata, *sdata;\n\tstruct cfg80211_chan_def *chandef;\n\tu16 len_rthdr;\n\tint hdrlen;\n\n\tmemset(info, 0, sizeof(*info));\n\tinfo->flags = IEEE80211_TX_CTL_REQ_TX_STATUS |\n\t\t      IEEE80211_TX_CTL_INJECTED;\n\n\t/* Sanity-check and process the injection radiotap header */\n\tif (!ieee80211_parse_tx_radiotap(skb, dev))\n\t\tgoto fail;\n\n\t/* we now know there is a radiotap header with a length we can use */\n\tlen_rthdr = ieee80211_get_radiotap_len(skb->data);\n\n\t/*\n\t * fix up the pointers accounting for the radiotap\n\t * header still being in there.  We are being given\n\t * a precooked IEEE80211 header so no need for\n\t * normal processing\n\t */\n\tskb_set_mac_header(skb, len_rthdr);\n\t/*\n\t * these are just fixed to the end of the rt area since we\n\t * don't have any better information and at this point, nobody cares\n\t */\n\tskb_set_network_header(skb, len_rthdr);\n\tskb_set_transport_header(skb, len_rthdr);\n\n\tif (skb->len < len_rthdr + 2)\n\t\tgoto fail;\n\n\thdr = (struct ieee80211_hdr *)(skb->data + len_rthdr);\n\thdrlen = ieee80211_hdrlen(hdr->frame_control);\n\n\tif (skb->len < len_rthdr + hdrlen)\n\t\tgoto fail;\n\n\t/*\n\t * Initialize skb->protocol if the injected frame is a data frame\n\t * carrying a rfc1042 header\n\t */\n\tif (ieee80211_is_data(hdr->frame_control) &&\n\t    skb->len >= len_rthdr + hdrlen + sizeof(rfc1042_header) + 2) {\n\t\tu8 *payload = (u8 *)hdr + hdrlen;\n\n\t\tif (ether_addr_equal(payload, rfc1042_header))\n\t\t\tskb->protocol = cpu_to_be16((payload[6] << 8) |\n\t\t\t\t\t\t    payload[7]);\n\t}\n\n\t/* Initialize skb->priority for QoS frames. If the DONT_REORDER flag\n\t * is set, stick to the default value for skb->priority to assure\n\t * frames injected with this flag are not reordered relative to each\n\t * other.\n\t */\n\tif (ieee80211_is_data_qos(hdr->frame_control) &&\n\t    !(info->control.flags & IEEE80211_TX_CTRL_DONT_REORDER)) {\n\t\tu8 *p = ieee80211_get_qos_ctl(hdr);\n\t\tskb->priority = *p & IEEE80211_QOS_CTL_TAG1D_MASK;\n\t}\n\n\trcu_read_lock();\n\n\t/*\n\t * We process outgoing injected frames that have a local address\n\t * we handle as though they are non-injected frames.\n\t * This code here isn't entirely correct, the local MAC address\n\t * isn't always enough to find the interface to use; for proper\n\t * VLAN support we have an nl80211-based mechanism.\n\t *\n\t * This is necessary, for example, for old hostapd versions that\n\t * don't use nl80211-based management TX/RX.\n\t */\n\tsdata = IEEE80211_DEV_TO_SUB_IF(dev);\n\n\tlist_for_each_entry_rcu(tmp_sdata, &local->interfaces, list) {\n\t\tif (!ieee80211_sdata_running(tmp_sdata))\n\t\t\tcontinue;\n\t\tif (tmp_sdata->vif.type == NL80211_IFTYPE_MONITOR ||\n\t\t    tmp_sdata->vif.type == NL80211_IFTYPE_AP_VLAN)\n\t\t\tcontinue;\n\t\tif (ether_addr_equal(tmp_sdata->vif.addr, hdr->addr2)) {\n\t\t\tsdata = tmp_sdata;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\tif (!chanctx_conf) {\n\t\ttmp_sdata = rcu_dereference(local->monitor_sdata);\n\t\tif (tmp_sdata)\n\t\t\tchanctx_conf =\n\t\t\t\trcu_dereference(tmp_sdata->vif.chanctx_conf);\n\t}\n\n\tif (chanctx_conf)\n\t\tchandef = &chanctx_conf->def;\n\telse if (!local->use_chanctx)\n\t\tchandef = &local->_oper_chandef;\n\telse\n\t\tgoto fail_rcu;\n\n\t/*\n\t * Frame injection is not allowed if beaconing is not allowed\n\t * or if we need radar detection. Beaconing is usually not allowed when\n\t * the mode or operation (Adhoc, AP, Mesh) does not support DFS.\n\t * Passive scan is also used in world regulatory domains where\n\t * your country is not known and as such it should be treated as\n\t * NO TX unless the channel is explicitly allowed in which case\n\t * your current regulatory domain would not have the passive scan\n\t * flag.\n\t *\n\t * Since AP mode uses monitor interfaces to inject/TX management\n\t * frames we can make AP mode the exception to this rule once it\n\t * supports radar detection as its implementation can deal with\n\t * radar detection by itself. We can do that later by adding a\n\t * monitor flag interfaces used for AP support.\n\t */\n\tif (!cfg80211_reg_can_beacon(local->hw.wiphy, chandef,\n\t\t\t\t     sdata->vif.type))\n\t\tgoto fail_rcu;\n\n\tinfo->band = chandef->chan->band;\n\n\t/* remove the injection radiotap header */\n\tskb_pull(skb, len_rthdr);\n\n\tieee80211_xmit(sdata, NULL, skb);\n\trcu_read_unlock();\n\n\treturn NETDEV_TX_OK;\n\nfail_rcu:\n\trcu_read_unlock();\nfail:\n\tdev_kfree_skb(skb);\n\treturn NETDEV_TX_OK; /* meaning, we dealt with the skb */\n}\n\nstatic inline bool ieee80211_is_tdls_setup(struct sk_buff *skb)\n{\n\tu16 ethertype = (skb->data[12] << 8) | skb->data[13];\n\n\treturn ethertype == ETH_P_TDLS &&\n\t       skb->len > 14 &&\n\t       skb->data[14] == WLAN_TDLS_SNAP_RFTYPE;\n}\n\nint ieee80211_lookup_ra_sta(struct ieee80211_sub_if_data *sdata,\n\t\t\t    struct sk_buff *skb,\n\t\t\t    struct sta_info **sta_out)\n{\n\tstruct sta_info *sta;\n\n\tswitch (sdata->vif.type) {\n\tcase NL80211_IFTYPE_AP_VLAN:\n\t\tsta = rcu_dereference(sdata->u.vlan.sta);\n\t\tif (sta) {\n\t\t\t*sta_out = sta;\n\t\t\treturn 0;\n\t\t} else if (sdata->wdev.use_4addr) {\n\t\t\treturn -ENOLINK;\n\t\t}\n\t\tfallthrough;\n\tcase NL80211_IFTYPE_AP:\n\tcase NL80211_IFTYPE_OCB:\n\tcase NL80211_IFTYPE_ADHOC:\n\t\tif (is_multicast_ether_addr(skb->data)) {\n\t\t\t*sta_out = ERR_PTR(-ENOENT);\n\t\t\treturn 0;\n\t\t}\n\t\tsta = sta_info_get_bss(sdata, skb->data);\n\t\tbreak;\n#ifdef CONFIG_MAC80211_MESH\n\tcase NL80211_IFTYPE_MESH_POINT:\n\t\t/* determined much later */\n\t\t*sta_out = NULL;\n\t\treturn 0;\n#endif\n\tcase NL80211_IFTYPE_STATION:\n\t\tif (sdata->wdev.wiphy->flags & WIPHY_FLAG_SUPPORTS_TDLS) {\n\t\t\tsta = sta_info_get(sdata, skb->data);\n\t\t\tif (sta && test_sta_flag(sta, WLAN_STA_TDLS_PEER)) {\n\t\t\t\tif (test_sta_flag(sta,\n\t\t\t\t\t\t  WLAN_STA_TDLS_PEER_AUTH)) {\n\t\t\t\t\t*sta_out = sta;\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\n\t\t\t\t/*\n\t\t\t\t * TDLS link during setup - throw out frames to\n\t\t\t\t * peer. Allow TDLS-setup frames to unauthorized\n\t\t\t\t * peers for the special case of a link teardown\n\t\t\t\t * after a TDLS sta is removed due to being\n\t\t\t\t * unreachable.\n\t\t\t\t */\n\t\t\t\tif (!ieee80211_is_tdls_setup(skb))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t}\n\n\t\tsta = sta_info_get(sdata, sdata->u.mgd.bssid);\n\t\tif (!sta)\n\t\t\treturn -ENOLINK;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t*sta_out = sta ?: ERR_PTR(-ENOENT);\n\treturn 0;\n}\n\nstatic u16 ieee80211_store_ack_skb(struct ieee80211_local *local,\n\t\t\t\t   struct sk_buff *skb,\n\t\t\t\t   u32 *info_flags,\n\t\t\t\t   u64 *cookie)\n{\n\tstruct sk_buff *ack_skb;\n\tu16 info_id = 0;\n\n\tif (skb->sk)\n\t\tack_skb = skb_clone_sk(skb);\n\telse\n\t\tack_skb = skb_clone(skb, GFP_ATOMIC);\n\n\tif (ack_skb) {\n\t\tunsigned long flags;\n\t\tint id;\n\n\t\tspin_lock_irqsave(&local->ack_status_lock, flags);\n\t\tid = idr_alloc(&local->ack_status_frames, ack_skb,\n\t\t\t       1, 0x2000, GFP_ATOMIC);\n\t\tspin_unlock_irqrestore(&local->ack_status_lock, flags);\n\n\t\tif (id >= 0) {\n\t\t\tinfo_id = id;\n\t\t\t*info_flags |= IEEE80211_TX_CTL_REQ_TX_STATUS;\n\t\t\tif (cookie) {\n\t\t\t\t*cookie = ieee80211_mgmt_tx_cookie(local);\n\t\t\t\tIEEE80211_SKB_CB(ack_skb)->ack.cookie = *cookie;\n\t\t\t}\n\t\t} else {\n\t\t\tkfree_skb(ack_skb);\n\t\t}\n\t}\n\n\treturn info_id;\n}\n\n/**\n * ieee80211_build_hdr - build 802.11 header in the given frame\n * @sdata: virtual interface to build the header for\n * @skb: the skb to build the header in\n * @info_flags: skb flags to set\n * @sta: the station pointer\n * @ctrl_flags: info control flags to set\n * @cookie: cookie pointer to fill (if not %NULL)\n *\n * This function takes the skb with 802.3 header and reformats the header to\n * the appropriate IEEE 802.11 header based on which interface the packet is\n * being transmitted on.\n *\n * Note that this function also takes care of the TX status request and\n * potential unsharing of the SKB - this needs to be interleaved with the\n * header building.\n *\n * The function requires the read-side RCU lock held\n *\n * Returns: the (possibly reallocated) skb or an ERR_PTR() code\n */\nstatic struct sk_buff *ieee80211_build_hdr(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t\t   struct sk_buff *skb, u32 info_flags,\n\t\t\t\t\t   struct sta_info *sta, u32 ctrl_flags,\n\t\t\t\t\t   u64 *cookie)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct ieee80211_tx_info *info;\n\tint head_need;\n\tu16 ethertype, hdrlen,  meshhdrlen = 0;\n\t__le16 fc;\n\tstruct ieee80211_hdr hdr;\n\tstruct ieee80211s_hdr mesh_hdr __maybe_unused;\n\tstruct mesh_path __maybe_unused *mppath = NULL, *mpath = NULL;\n\tconst u8 *encaps_data;\n\tint encaps_len, skip_header_bytes;\n\tbool wme_sta = false, authorized = false;\n\tbool tdls_peer;\n\tbool multicast;\n\tu16 info_id = 0;\n\tstruct ieee80211_chanctx_conf *chanctx_conf;\n\tstruct ieee80211_sub_if_data *ap_sdata;\n\tenum nl80211_band band;\n\tint ret;\n\n\tif (IS_ERR(sta))\n\t\tsta = NULL;\n\n#ifdef CONFIG_MAC80211_DEBUGFS\n\tif (local->force_tx_status)\n\t\tinfo_flags |= IEEE80211_TX_CTL_REQ_TX_STATUS;\n#endif\n\n\t/* convert Ethernet header to proper 802.11 header (based on\n\t * operation mode) */\n\tethertype = (skb->data[12] << 8) | skb->data[13];\n\tfc = cpu_to_le16(IEEE80211_FTYPE_DATA | IEEE80211_STYPE_DATA);\n\n\tswitch (sdata->vif.type) {\n\tcase NL80211_IFTYPE_AP_VLAN:\n\t\tif (sdata->wdev.use_4addr) {\n\t\t\tfc |= cpu_to_le16(IEEE80211_FCTL_FROMDS | IEEE80211_FCTL_TODS);\n\t\t\t/* RA TA DA SA */\n\t\t\tmemcpy(hdr.addr1, sta->sta.addr, ETH_ALEN);\n\t\t\tmemcpy(hdr.addr2, sdata->vif.addr, ETH_ALEN);\n\t\t\tmemcpy(hdr.addr3, skb->data, ETH_ALEN);\n\t\t\tmemcpy(hdr.addr4, skb->data + ETH_ALEN, ETH_ALEN);\n\t\t\thdrlen = 30;\n\t\t\tauthorized = test_sta_flag(sta, WLAN_STA_AUTHORIZED);\n\t\t\twme_sta = sta->sta.wme;\n\t\t}\n\t\tap_sdata = container_of(sdata->bss, struct ieee80211_sub_if_data,\n\t\t\t\t\tu.ap);\n\t\tchanctx_conf = rcu_dereference(ap_sdata->vif.chanctx_conf);\n\t\tif (!chanctx_conf) {\n\t\t\tret = -ENOTCONN;\n\t\t\tgoto free;\n\t\t}\n\t\tband = chanctx_conf->def.chan->band;\n\t\tif (sdata->wdev.use_4addr)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase NL80211_IFTYPE_AP:\n\t\tif (sdata->vif.type == NL80211_IFTYPE_AP)\n\t\t\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\t\tif (!chanctx_conf) {\n\t\t\tret = -ENOTCONN;\n\t\t\tgoto free;\n\t\t}\n\t\tfc |= cpu_to_le16(IEEE80211_FCTL_FROMDS);\n\t\t/* DA BSSID SA */\n\t\tmemcpy(hdr.addr1, skb->data, ETH_ALEN);\n\t\tmemcpy(hdr.addr2, sdata->vif.addr, ETH_ALEN);\n\t\tmemcpy(hdr.addr3, skb->data + ETH_ALEN, ETH_ALEN);\n\t\thdrlen = 24;\n\t\tband = chanctx_conf->def.chan->band;\n\t\tbreak;\n#ifdef CONFIG_MAC80211_MESH\n\tcase NL80211_IFTYPE_MESH_POINT:\n\t\tif (!is_multicast_ether_addr(skb->data)) {\n\t\t\tstruct sta_info *next_hop;\n\t\t\tbool mpp_lookup = true;\n\n\t\t\tmpath = mesh_path_lookup(sdata, skb->data);\n\t\t\tif (mpath) {\n\t\t\t\tmpp_lookup = false;\n\t\t\t\tnext_hop = rcu_dereference(mpath->next_hop);\n\t\t\t\tif (!next_hop ||\n\t\t\t\t    !(mpath->flags & (MESH_PATH_ACTIVE |\n\t\t\t\t\t\t      MESH_PATH_RESOLVING)))\n\t\t\t\t\tmpp_lookup = true;\n\t\t\t}\n\n\t\t\tif (mpp_lookup) {\n\t\t\t\tmppath = mpp_path_lookup(sdata, skb->data);\n\t\t\t\tif (mppath)\n\t\t\t\t\tmppath->exp_time = jiffies;\n\t\t\t}\n\n\t\t\tif (mppath && mpath)\n\t\t\t\tmesh_path_del(sdata, mpath->dst);\n\t\t}\n\n\t\t/*\n\t\t * Use address extension if it is a packet from\n\t\t * another interface or if we know the destination\n\t\t * is being proxied by a portal (i.e. portal address\n\t\t * differs from proxied address)\n\t\t */\n\t\tif (ether_addr_equal(sdata->vif.addr, skb->data + ETH_ALEN) &&\n\t\t    !(mppath && !ether_addr_equal(mppath->mpp, skb->data))) {\n\t\t\thdrlen = ieee80211_fill_mesh_addresses(&hdr, &fc,\n\t\t\t\t\tskb->data, skb->data + ETH_ALEN);\n\t\t\tmeshhdrlen = ieee80211_new_mesh_header(sdata, &mesh_hdr,\n\t\t\t\t\t\t\t       NULL, NULL);\n\t\t} else {\n\t\t\t/* DS -> MBSS (802.11-2012 13.11.3.3).\n\t\t\t * For unicast with unknown forwarding information,\n\t\t\t * destination might be in the MBSS or if that fails\n\t\t\t * forwarded to another mesh gate. In either case\n\t\t\t * resolution will be handled in ieee80211_xmit(), so\n\t\t\t * leave the original DA. This also works for mcast */\n\t\t\tconst u8 *mesh_da = skb->data;\n\n\t\t\tif (mppath)\n\t\t\t\tmesh_da = mppath->mpp;\n\t\t\telse if (mpath)\n\t\t\t\tmesh_da = mpath->dst;\n\n\t\t\thdrlen = ieee80211_fill_mesh_addresses(&hdr, &fc,\n\t\t\t\t\tmesh_da, sdata->vif.addr);\n\t\t\tif (is_multicast_ether_addr(mesh_da))\n\t\t\t\t/* DA TA mSA AE:SA */\n\t\t\t\tmeshhdrlen = ieee80211_new_mesh_header(\n\t\t\t\t\t\tsdata, &mesh_hdr,\n\t\t\t\t\t\tskb->data + ETH_ALEN, NULL);\n\t\t\telse\n\t\t\t\t/* RA TA mDA mSA AE:DA SA */\n\t\t\t\tmeshhdrlen = ieee80211_new_mesh_header(\n\t\t\t\t\t\tsdata, &mesh_hdr, skb->data,\n\t\t\t\t\t\tskb->data + ETH_ALEN);\n\n\t\t}\n\t\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\t\tif (!chanctx_conf) {\n\t\t\tret = -ENOTCONN;\n\t\t\tgoto free;\n\t\t}\n\t\tband = chanctx_conf->def.chan->band;\n\n\t\t/* For injected frames, fill RA right away as nexthop lookup\n\t\t * will be skipped.\n\t\t */\n\t\tif ((ctrl_flags & IEEE80211_TX_CTRL_SKIP_MPATH_LOOKUP) &&\n\t\t    is_zero_ether_addr(hdr.addr1))\n\t\t\tmemcpy(hdr.addr1, skb->data, ETH_ALEN);\n\t\tbreak;\n#endif\n\tcase NL80211_IFTYPE_STATION:\n\t\t/* we already did checks when looking up the RA STA */\n\t\ttdls_peer = test_sta_flag(sta, WLAN_STA_TDLS_PEER);\n\n\t\tif (tdls_peer) {\n\t\t\t/* DA SA BSSID */\n\t\t\tmemcpy(hdr.addr1, skb->data, ETH_ALEN);\n\t\t\tmemcpy(hdr.addr2, skb->data + ETH_ALEN, ETH_ALEN);\n\t\t\tmemcpy(hdr.addr3, sdata->u.mgd.bssid, ETH_ALEN);\n\t\t\thdrlen = 24;\n\t\t}  else if (sdata->u.mgd.use_4addr &&\n\t\t\t    cpu_to_be16(ethertype) != sdata->control_port_protocol) {\n\t\t\tfc |= cpu_to_le16(IEEE80211_FCTL_FROMDS |\n\t\t\t\t\t  IEEE80211_FCTL_TODS);\n\t\t\t/* RA TA DA SA */\n\t\t\tmemcpy(hdr.addr1, sdata->u.mgd.bssid, ETH_ALEN);\n\t\t\tmemcpy(hdr.addr2, sdata->vif.addr, ETH_ALEN);\n\t\t\tmemcpy(hdr.addr3, skb->data, ETH_ALEN);\n\t\t\tmemcpy(hdr.addr4, skb->data + ETH_ALEN, ETH_ALEN);\n\t\t\thdrlen = 30;\n\t\t} else {\n\t\t\tfc |= cpu_to_le16(IEEE80211_FCTL_TODS);\n\t\t\t/* BSSID SA DA */\n\t\t\tmemcpy(hdr.addr1, sdata->u.mgd.bssid, ETH_ALEN);\n\t\t\tmemcpy(hdr.addr2, skb->data + ETH_ALEN, ETH_ALEN);\n\t\t\tmemcpy(hdr.addr3, skb->data, ETH_ALEN);\n\t\t\thdrlen = 24;\n\t\t}\n\t\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\t\tif (!chanctx_conf) {\n\t\t\tret = -ENOTCONN;\n\t\t\tgoto free;\n\t\t}\n\t\tband = chanctx_conf->def.chan->band;\n\t\tbreak;\n\tcase NL80211_IFTYPE_OCB:\n\t\t/* DA SA BSSID */\n\t\tmemcpy(hdr.addr1, skb->data, ETH_ALEN);\n\t\tmemcpy(hdr.addr2, skb->data + ETH_ALEN, ETH_ALEN);\n\t\teth_broadcast_addr(hdr.addr3);\n\t\thdrlen = 24;\n\t\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\t\tif (!chanctx_conf) {\n\t\t\tret = -ENOTCONN;\n\t\t\tgoto free;\n\t\t}\n\t\tband = chanctx_conf->def.chan->band;\n\t\tbreak;\n\tcase NL80211_IFTYPE_ADHOC:\n\t\t/* DA SA BSSID */\n\t\tmemcpy(hdr.addr1, skb->data, ETH_ALEN);\n\t\tmemcpy(hdr.addr2, skb->data + ETH_ALEN, ETH_ALEN);\n\t\tmemcpy(hdr.addr3, sdata->u.ibss.bssid, ETH_ALEN);\n\t\thdrlen = 24;\n\t\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\t\tif (!chanctx_conf) {\n\t\t\tret = -ENOTCONN;\n\t\t\tgoto free;\n\t\t}\n\t\tband = chanctx_conf->def.chan->band;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tgoto free;\n\t}\n\n\tmulticast = is_multicast_ether_addr(hdr.addr1);\n\n\t/* sta is always NULL for mesh */\n\tif (sta) {\n\t\tauthorized = test_sta_flag(sta, WLAN_STA_AUTHORIZED);\n\t\twme_sta = sta->sta.wme;\n\t} else if (ieee80211_vif_is_mesh(&sdata->vif)) {\n\t\t/* For mesh, the use of the QoS header is mandatory */\n\t\twme_sta = true;\n\t}\n\n\t/* receiver does QoS (which also means we do) use it */\n\tif (wme_sta) {\n\t\tfc |= cpu_to_le16(IEEE80211_STYPE_QOS_DATA);\n\t\thdrlen += 2;\n\t}\n\n\t/*\n\t * Drop unicast frames to unauthorised stations unless they are\n\t * EAPOL frames from the local station.\n\t */\n\tif (unlikely(!ieee80211_vif_is_mesh(&sdata->vif) &&\n\t\t     (sdata->vif.type != NL80211_IFTYPE_OCB) &&\n\t\t     !multicast && !authorized &&\n\t\t     (cpu_to_be16(ethertype) != sdata->control_port_protocol ||\n\t\t      !ether_addr_equal(sdata->vif.addr, skb->data + ETH_ALEN)))) {\n#ifdef CONFIG_MAC80211_VERBOSE_DEBUG\n\t\tnet_info_ratelimited(\"%s: dropped frame to %pM (unauthorized port)\\n\",\n\t\t\t\t    sdata->name, hdr.addr1);\n#endif\n\n\t\tI802_DEBUG_INC(local->tx_handlers_drop_unauth_port);\n\n\t\tret = -EPERM;\n\t\tgoto free;\n\t}\n\n\tif (unlikely(!multicast && ((skb->sk &&\n\t\t     skb_shinfo(skb)->tx_flags & SKBTX_WIFI_STATUS) ||\n\t\t     ctrl_flags & IEEE80211_TX_CTL_REQ_TX_STATUS)))\n\t\tinfo_id = ieee80211_store_ack_skb(local, skb, &info_flags,\n\t\t\t\t\t\t  cookie);\n\n\t/*\n\t * If the skb is shared we need to obtain our own copy.\n\t */\n\tif (skb_shared(skb)) {\n\t\tstruct sk_buff *tmp_skb = skb;\n\n\t\t/* can't happen -- skb is a clone if info_id != 0 */\n\t\tWARN_ON(info_id);\n\n\t\tskb = skb_clone(skb, GFP_ATOMIC);\n\t\tkfree_skb(tmp_skb);\n\n\t\tif (!skb) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto free;\n\t\t}\n\t}\n\n\thdr.frame_control = fc;\n\thdr.duration_id = 0;\n\thdr.seq_ctrl = 0;\n\n\tskip_header_bytes = ETH_HLEN;\n\tif (ethertype == ETH_P_AARP || ethertype == ETH_P_IPX) {\n\t\tencaps_data = bridge_tunnel_header;\n\t\tencaps_len = sizeof(bridge_tunnel_header);\n\t\tskip_header_bytes -= 2;\n\t} else if (ethertype >= ETH_P_802_3_MIN) {\n\t\tencaps_data = rfc1042_header;\n\t\tencaps_len = sizeof(rfc1042_header);\n\t\tskip_header_bytes -= 2;\n\t} else {\n\t\tencaps_data = NULL;\n\t\tencaps_len = 0;\n\t}\n\n\tskb_pull(skb, skip_header_bytes);\n\thead_need = hdrlen + encaps_len + meshhdrlen - skb_headroom(skb);\n\n\t/*\n\t * So we need to modify the skb header and hence need a copy of\n\t * that. The head_need variable above doesn't, so far, include\n\t * the needed header space that we don't need right away. If we\n\t * can, then we don't reallocate right now but only after the\n\t * frame arrives at the master device (if it does...)\n\t *\n\t * If we cannot, however, then we will reallocate to include all\n\t * the ever needed space. Also, if we need to reallocate it anyway,\n\t * make it big enough for everything we may ever need.\n\t */\n\n\tif (head_need > 0 || skb_cloned(skb)) {\n\t\thead_need += sdata->encrypt_headroom;\n\t\thead_need += local->tx_headroom;\n\t\thead_need = max_t(int, 0, head_need);\n\t\tif (ieee80211_skb_resize(sdata, skb, head_need, ENCRYPT_DATA)) {\n\t\t\tieee80211_free_txskb(&local->hw, skb);\n\t\t\tskb = NULL;\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t}\n\n\tif (encaps_data)\n\t\tmemcpy(skb_push(skb, encaps_len), encaps_data, encaps_len);\n\n#ifdef CONFIG_MAC80211_MESH\n\tif (meshhdrlen > 0)\n\t\tmemcpy(skb_push(skb, meshhdrlen), &mesh_hdr, meshhdrlen);\n#endif\n\n\tif (ieee80211_is_data_qos(fc)) {\n\t\t__le16 *qos_control;\n\n\t\tqos_control = skb_push(skb, 2);\n\t\tmemcpy(skb_push(skb, hdrlen - 2), &hdr, hdrlen - 2);\n\t\t/*\n\t\t * Maybe we could actually set some fields here, for now just\n\t\t * initialise to zero to indicate no special operation.\n\t\t */\n\t\t*qos_control = 0;\n\t} else\n\t\tmemcpy(skb_push(skb, hdrlen), &hdr, hdrlen);\n\n\tskb_reset_mac_header(skb);\n\n\tinfo = IEEE80211_SKB_CB(skb);\n\tmemset(info, 0, sizeof(*info));\n\n\tinfo->flags = info_flags;\n\tinfo->ack_frame_id = info_id;\n\tinfo->band = band;\n\tinfo->control.flags = ctrl_flags;\n\n\treturn skb;\n free:\n\tkfree_skb(skb);\n\treturn ERR_PTR(ret);\n}\n\n/*\n * fast-xmit overview\n *\n * The core idea of this fast-xmit is to remove per-packet checks by checking\n * them out of band. ieee80211_check_fast_xmit() implements the out-of-band\n * checks that are needed to get the sta->fast_tx pointer assigned, after which\n * much less work can be done per packet. For example, fragmentation must be\n * disabled or the fast_tx pointer will not be set. All the conditions are seen\n * in the code here.\n *\n * Once assigned, the fast_tx data structure also caches the per-packet 802.11\n * header and other data to aid packet processing in ieee80211_xmit_fast().\n *\n * The most difficult part of this is that when any of these assumptions\n * change, an external trigger (i.e. a call to ieee80211_clear_fast_xmit(),\n * ieee80211_check_fast_xmit() or friends) is required to reset the data,\n * since the per-packet code no longer checks the conditions. This is reflected\n * by the calls to these functions throughout the rest of the code, and must be\n * maintained if any of the TX path checks change.\n */\n\nvoid ieee80211_check_fast_xmit(struct sta_info *sta)\n{\n\tstruct ieee80211_fast_tx build = {}, *fast_tx = NULL, *old;\n\tstruct ieee80211_local *local = sta->local;\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct ieee80211_hdr *hdr = (void *)build.hdr;\n\tstruct ieee80211_chanctx_conf *chanctx_conf;\n\t__le16 fc;\n\n\tif (!ieee80211_hw_check(&local->hw, SUPPORT_FAST_XMIT))\n\t\treturn;\n\n\t/* Locking here protects both the pointer itself, and against concurrent\n\t * invocations winning data access races to, e.g., the key pointer that\n\t * is used.\n\t * Without it, the invocation of this function right after the key\n\t * pointer changes wouldn't be sufficient, as another CPU could access\n\t * the pointer, then stall, and then do the cache update after the CPU\n\t * that invalidated the key.\n\t * With the locking, such scenarios cannot happen as the check for the\n\t * key and the fast-tx assignment are done atomically, so the CPU that\n\t * modifies the key will either wait or other one will see the key\n\t * cleared/changed already.\n\t */\n\tspin_lock_bh(&sta->lock);\n\tif (ieee80211_hw_check(&local->hw, SUPPORTS_PS) &&\n\t    !ieee80211_hw_check(&local->hw, SUPPORTS_DYNAMIC_PS) &&\n\t    sdata->vif.type == NL80211_IFTYPE_STATION)\n\t\tgoto out;\n\n\tif (!test_sta_flag(sta, WLAN_STA_AUTHORIZED))\n\t\tgoto out;\n\n\tif (test_sta_flag(sta, WLAN_STA_PS_STA) ||\n\t    test_sta_flag(sta, WLAN_STA_PS_DRIVER) ||\n\t    test_sta_flag(sta, WLAN_STA_PS_DELIVER) ||\n\t    test_sta_flag(sta, WLAN_STA_CLEAR_PS_FILT))\n\t\tgoto out;\n\n\tif (sdata->noack_map)\n\t\tgoto out;\n\n\t/* fast-xmit doesn't handle fragmentation at all */\n\tif (local->hw.wiphy->frag_threshold != (u32)-1 &&\n\t    !ieee80211_hw_check(&local->hw, SUPPORTS_TX_FRAG))\n\t\tgoto out;\n\n\trcu_read_lock();\n\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\tif (!chanctx_conf) {\n\t\trcu_read_unlock();\n\t\tgoto out;\n\t}\n\tbuild.band = chanctx_conf->def.chan->band;\n\trcu_read_unlock();\n\n\tfc = cpu_to_le16(IEEE80211_FTYPE_DATA | IEEE80211_STYPE_DATA);\n\n\tswitch (sdata->vif.type) {\n\tcase NL80211_IFTYPE_ADHOC:\n\t\t/* DA SA BSSID */\n\t\tbuild.da_offs = offsetof(struct ieee80211_hdr, addr1);\n\t\tbuild.sa_offs = offsetof(struct ieee80211_hdr, addr2);\n\t\tmemcpy(hdr->addr3, sdata->u.ibss.bssid, ETH_ALEN);\n\t\tbuild.hdr_len = 24;\n\t\tbreak;\n\tcase NL80211_IFTYPE_STATION:\n\t\tif (test_sta_flag(sta, WLAN_STA_TDLS_PEER)) {\n\t\t\t/* DA SA BSSID */\n\t\t\tbuild.da_offs = offsetof(struct ieee80211_hdr, addr1);\n\t\t\tbuild.sa_offs = offsetof(struct ieee80211_hdr, addr2);\n\t\t\tmemcpy(hdr->addr3, sdata->u.mgd.bssid, ETH_ALEN);\n\t\t\tbuild.hdr_len = 24;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (sdata->u.mgd.use_4addr) {\n\t\t\t/* non-regular ethertype cannot use the fastpath */\n\t\t\tfc |= cpu_to_le16(IEEE80211_FCTL_FROMDS |\n\t\t\t\t\t  IEEE80211_FCTL_TODS);\n\t\t\t/* RA TA DA SA */\n\t\t\tmemcpy(hdr->addr1, sdata->u.mgd.bssid, ETH_ALEN);\n\t\t\tmemcpy(hdr->addr2, sdata->vif.addr, ETH_ALEN);\n\t\t\tbuild.da_offs = offsetof(struct ieee80211_hdr, addr3);\n\t\t\tbuild.sa_offs = offsetof(struct ieee80211_hdr, addr4);\n\t\t\tbuild.hdr_len = 30;\n\t\t\tbreak;\n\t\t}\n\t\tfc |= cpu_to_le16(IEEE80211_FCTL_TODS);\n\t\t/* BSSID SA DA */\n\t\tmemcpy(hdr->addr1, sdata->u.mgd.bssid, ETH_ALEN);\n\t\tbuild.da_offs = offsetof(struct ieee80211_hdr, addr3);\n\t\tbuild.sa_offs = offsetof(struct ieee80211_hdr, addr2);\n\t\tbuild.hdr_len = 24;\n\t\tbreak;\n\tcase NL80211_IFTYPE_AP_VLAN:\n\t\tif (sdata->wdev.use_4addr) {\n\t\t\tfc |= cpu_to_le16(IEEE80211_FCTL_FROMDS |\n\t\t\t\t\t  IEEE80211_FCTL_TODS);\n\t\t\t/* RA TA DA SA */\n\t\t\tmemcpy(hdr->addr1, sta->sta.addr, ETH_ALEN);\n\t\t\tmemcpy(hdr->addr2, sdata->vif.addr, ETH_ALEN);\n\t\t\tbuild.da_offs = offsetof(struct ieee80211_hdr, addr3);\n\t\t\tbuild.sa_offs = offsetof(struct ieee80211_hdr, addr4);\n\t\t\tbuild.hdr_len = 30;\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\tcase NL80211_IFTYPE_AP:\n\t\tfc |= cpu_to_le16(IEEE80211_FCTL_FROMDS);\n\t\t/* DA BSSID SA */\n\t\tbuild.da_offs = offsetof(struct ieee80211_hdr, addr1);\n\t\tmemcpy(hdr->addr2, sdata->vif.addr, ETH_ALEN);\n\t\tbuild.sa_offs = offsetof(struct ieee80211_hdr, addr3);\n\t\tbuild.hdr_len = 24;\n\t\tbreak;\n\tdefault:\n\t\t/* not handled on fast-xmit */\n\t\tgoto out;\n\t}\n\n\tif (sta->sta.wme) {\n\t\tbuild.hdr_len += 2;\n\t\tfc |= cpu_to_le16(IEEE80211_STYPE_QOS_DATA);\n\t}\n\n\t/* We store the key here so there's no point in using rcu_dereference()\n\t * but that's fine because the code that changes the pointers will call\n\t * this function after doing so. For a single CPU that would be enough,\n\t * for multiple see the comment above.\n\t */\n\tbuild.key = rcu_access_pointer(sta->ptk[sta->ptk_idx]);\n\tif (!build.key)\n\t\tbuild.key = rcu_access_pointer(sdata->default_unicast_key);\n\tif (build.key) {\n\t\tbool gen_iv, iv_spc, mmic;\n\n\t\tgen_iv = build.key->conf.flags & IEEE80211_KEY_FLAG_GENERATE_IV;\n\t\tiv_spc = build.key->conf.flags & IEEE80211_KEY_FLAG_PUT_IV_SPACE;\n\t\tmmic = build.key->conf.flags &\n\t\t\t(IEEE80211_KEY_FLAG_GENERATE_MMIC |\n\t\t\t IEEE80211_KEY_FLAG_PUT_MIC_SPACE);\n\n\t\t/* don't handle software crypto */\n\t\tif (!(build.key->flags & KEY_FLAG_UPLOADED_TO_HARDWARE))\n\t\t\tgoto out;\n\n\t\t/* Key is being removed */\n\t\tif (build.key->flags & KEY_FLAG_TAINTED)\n\t\t\tgoto out;\n\n\t\tswitch (build.key->conf.cipher) {\n\t\tcase WLAN_CIPHER_SUITE_CCMP:\n\t\tcase WLAN_CIPHER_SUITE_CCMP_256:\n\t\t\tif (gen_iv)\n\t\t\t\tbuild.pn_offs = build.hdr_len;\n\t\t\tif (gen_iv || iv_spc)\n\t\t\t\tbuild.hdr_len += IEEE80211_CCMP_HDR_LEN;\n\t\t\tbreak;\n\t\tcase WLAN_CIPHER_SUITE_GCMP:\n\t\tcase WLAN_CIPHER_SUITE_GCMP_256:\n\t\t\tif (gen_iv)\n\t\t\t\tbuild.pn_offs = build.hdr_len;\n\t\t\tif (gen_iv || iv_spc)\n\t\t\t\tbuild.hdr_len += IEEE80211_GCMP_HDR_LEN;\n\t\t\tbreak;\n\t\tcase WLAN_CIPHER_SUITE_TKIP:\n\t\t\t/* cannot handle MMIC or IV generation in xmit-fast */\n\t\t\tif (mmic || gen_iv)\n\t\t\t\tgoto out;\n\t\t\tif (iv_spc)\n\t\t\t\tbuild.hdr_len += IEEE80211_TKIP_IV_LEN;\n\t\t\tbreak;\n\t\tcase WLAN_CIPHER_SUITE_WEP40:\n\t\tcase WLAN_CIPHER_SUITE_WEP104:\n\t\t\t/* cannot handle IV generation in fast-xmit */\n\t\t\tif (gen_iv)\n\t\t\t\tgoto out;\n\t\t\tif (iv_spc)\n\t\t\t\tbuild.hdr_len += IEEE80211_WEP_IV_LEN;\n\t\t\tbreak;\n\t\tcase WLAN_CIPHER_SUITE_AES_CMAC:\n\t\tcase WLAN_CIPHER_SUITE_BIP_CMAC_256:\n\t\tcase WLAN_CIPHER_SUITE_BIP_GMAC_128:\n\t\tcase WLAN_CIPHER_SUITE_BIP_GMAC_256:\n\t\t\tWARN(1,\n\t\t\t     \"management cipher suite 0x%x enabled for data\\n\",\n\t\t\t     build.key->conf.cipher);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\t/* we don't know how to generate IVs for this at all */\n\t\t\tif (WARN_ON(gen_iv))\n\t\t\t\tgoto out;\n\t\t\t/* pure hardware keys are OK, of course */\n\t\t\tif (!(build.key->flags & KEY_FLAG_CIPHER_SCHEME))\n\t\t\t\tbreak;\n\t\t\t/* cipher scheme might require space allocation */\n\t\t\tif (iv_spc &&\n\t\t\t    build.key->conf.iv_len > IEEE80211_FAST_XMIT_MAX_IV)\n\t\t\t\tgoto out;\n\t\t\tif (iv_spc)\n\t\t\t\tbuild.hdr_len += build.key->conf.iv_len;\n\t\t}\n\n\t\tfc |= cpu_to_le16(IEEE80211_FCTL_PROTECTED);\n\t}\n\n\thdr->frame_control = fc;\n\n\tmemcpy(build.hdr + build.hdr_len,\n\t       rfc1042_header,  sizeof(rfc1042_header));\n\tbuild.hdr_len += sizeof(rfc1042_header);\n\n\tfast_tx = kmemdup(&build, sizeof(build), GFP_ATOMIC);\n\t/* if the kmemdup fails, continue w/o fast_tx */\n\tif (!fast_tx)\n\t\tgoto out;\n\n out:\n\t/* we might have raced against another call to this function */\n\told = rcu_dereference_protected(sta->fast_tx,\n\t\t\t\t\tlockdep_is_held(&sta->lock));\n\trcu_assign_pointer(sta->fast_tx, fast_tx);\n\tif (old)\n\t\tkfree_rcu(old, rcu_head);\n\tspin_unlock_bh(&sta->lock);\n}\n\nvoid ieee80211_check_fast_xmit_all(struct ieee80211_local *local)\n{\n\tstruct sta_info *sta;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(sta, &local->sta_list, list)\n\t\tieee80211_check_fast_xmit(sta);\n\trcu_read_unlock();\n}\n\nvoid ieee80211_check_fast_xmit_iface(struct ieee80211_sub_if_data *sdata)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sta_info *sta;\n\n\trcu_read_lock();\n\n\tlist_for_each_entry_rcu(sta, &local->sta_list, list) {\n\t\tif (sdata != sta->sdata &&\n\t\t    (!sta->sdata->bss || sta->sdata->bss != sdata->bss))\n\t\t\tcontinue;\n\t\tieee80211_check_fast_xmit(sta);\n\t}\n\n\trcu_read_unlock();\n}\n\nvoid ieee80211_clear_fast_xmit(struct sta_info *sta)\n{\n\tstruct ieee80211_fast_tx *fast_tx;\n\n\tspin_lock_bh(&sta->lock);\n\tfast_tx = rcu_dereference_protected(sta->fast_tx,\n\t\t\t\t\t    lockdep_is_held(&sta->lock));\n\tRCU_INIT_POINTER(sta->fast_tx, NULL);\n\tspin_unlock_bh(&sta->lock);\n\n\tif (fast_tx)\n\t\tkfree_rcu(fast_tx, rcu_head);\n}\n\nstatic bool ieee80211_amsdu_realloc_pad(struct ieee80211_local *local,\n\t\t\t\t\tstruct sk_buff *skb, int headroom)\n{\n\tif (skb_headroom(skb) < headroom) {\n\t\tI802_DEBUG_INC(local->tx_expand_skb_head);\n\n\t\tif (pskb_expand_head(skb, headroom, 0, GFP_ATOMIC)) {\n\t\t\twiphy_debug(local->hw.wiphy,\n\t\t\t\t    \"failed to reallocate TX buffer\\n\");\n\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic bool ieee80211_amsdu_prepare_head(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t\t struct ieee80211_fast_tx *fast_tx,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_hdr *hdr;\n\tstruct ethhdr *amsdu_hdr;\n\tint hdr_len = fast_tx->hdr_len - sizeof(rfc1042_header);\n\tint subframe_len = skb->len - hdr_len;\n\tvoid *data;\n\tu8 *qc, *h_80211_src, *h_80211_dst;\n\tconst u8 *bssid;\n\n\tif (info->flags & IEEE80211_TX_CTL_RATE_CTRL_PROBE)\n\t\treturn false;\n\n\tif (info->control.flags & IEEE80211_TX_CTRL_AMSDU)\n\t\treturn true;\n\n\tif (!ieee80211_amsdu_realloc_pad(local, skb, sizeof(*amsdu_hdr)))\n\t\treturn false;\n\n\tdata = skb_push(skb, sizeof(*amsdu_hdr));\n\tmemmove(data, data + sizeof(*amsdu_hdr), hdr_len);\n\thdr = data;\n\tamsdu_hdr = data + hdr_len;\n\t/* h_80211_src/dst is addr* field within hdr */\n\th_80211_src = data + fast_tx->sa_offs;\n\th_80211_dst = data + fast_tx->da_offs;\n\n\tamsdu_hdr->h_proto = cpu_to_be16(subframe_len);\n\tether_addr_copy(amsdu_hdr->h_source, h_80211_src);\n\tether_addr_copy(amsdu_hdr->h_dest, h_80211_dst);\n\n\t/* according to IEEE 802.11-2012 8.3.2 table 8-19, the outer SA/DA\n\t * fields needs to be changed to BSSID for A-MSDU frames depending\n\t * on FromDS/ToDS values.\n\t */\n\tswitch (sdata->vif.type) {\n\tcase NL80211_IFTYPE_STATION:\n\t\tbssid = sdata->u.mgd.bssid;\n\t\tbreak;\n\tcase NL80211_IFTYPE_AP:\n\tcase NL80211_IFTYPE_AP_VLAN:\n\t\tbssid = sdata->vif.addr;\n\t\tbreak;\n\tdefault:\n\t\tbssid = NULL;\n\t}\n\n\tif (bssid && ieee80211_has_fromds(hdr->frame_control))\n\t\tether_addr_copy(h_80211_src, bssid);\n\n\tif (bssid && ieee80211_has_tods(hdr->frame_control))\n\t\tether_addr_copy(h_80211_dst, bssid);\n\n\tqc = ieee80211_get_qos_ctl(hdr);\n\t*qc |= IEEE80211_QOS_CTL_A_MSDU_PRESENT;\n\n\tinfo->control.flags |= IEEE80211_TX_CTRL_AMSDU;\n\n\treturn true;\n}\n\nstatic bool ieee80211_amsdu_aggregate(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t      struct sta_info *sta,\n\t\t\t\t      struct ieee80211_fast_tx *fast_tx,\n\t\t\t\t      struct sk_buff *skb)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct fq *fq = &local->fq;\n\tstruct fq_tin *tin;\n\tstruct fq_flow *flow;\n\tu8 tid = skb->priority & IEEE80211_QOS_CTL_TAG1D_MASK;\n\tstruct ieee80211_txq *txq = sta->sta.txq[tid];\n\tstruct txq_info *txqi;\n\tstruct sk_buff **frag_tail, *head;\n\tint subframe_len = skb->len - ETH_ALEN;\n\tu8 max_subframes = sta->sta.max_amsdu_subframes;\n\tint max_frags = local->hw.max_tx_fragments;\n\tint max_amsdu_len = sta->sta.max_amsdu_len;\n\tint orig_truesize;\n\tu32 flow_idx;\n\t__be16 len;\n\tvoid *data;\n\tbool ret = false;\n\tunsigned int orig_len;\n\tint n = 2, nfrags, pad = 0;\n\tu16 hdrlen;\n\n\tif (!ieee80211_hw_check(&local->hw, TX_AMSDU))\n\t\treturn false;\n\n\tif (skb_is_gso(skb))\n\t\treturn false;\n\n\tif (!txq)\n\t\treturn false;\n\n\ttxqi = to_txq_info(txq);\n\tif (test_bit(IEEE80211_TXQ_NO_AMSDU, &txqi->flags))\n\t\treturn false;\n\n\tif (sta->sta.max_rc_amsdu_len)\n\t\tmax_amsdu_len = min_t(int, max_amsdu_len,\n\t\t\t\t      sta->sta.max_rc_amsdu_len);\n\n\tif (sta->sta.max_tid_amsdu_len[tid])\n\t\tmax_amsdu_len = min_t(int, max_amsdu_len,\n\t\t\t\t      sta->sta.max_tid_amsdu_len[tid]);\n\n\tflow_idx = fq_flow_idx(fq, skb);\n\n\tspin_lock_bh(&fq->lock);\n\n\t/* TODO: Ideally aggregation should be done on dequeue to remain\n\t * responsive to environment changes.\n\t */\n\n\ttin = &txqi->tin;\n\tflow = fq_flow_classify(fq, tin, flow_idx, skb);\n\thead = skb_peek_tail(&flow->queue);\n\tif (!head || skb_is_gso(head))\n\t\tgoto out;\n\n\torig_truesize = head->truesize;\n\torig_len = head->len;\n\n\tif (skb->len + head->len > max_amsdu_len)\n\t\tgoto out;\n\n\tnfrags = 1 + skb_shinfo(skb)->nr_frags;\n\tnfrags += 1 + skb_shinfo(head)->nr_frags;\n\tfrag_tail = &skb_shinfo(head)->frag_list;\n\twhile (*frag_tail) {\n\t\tnfrags += 1 + skb_shinfo(*frag_tail)->nr_frags;\n\t\tfrag_tail = &(*frag_tail)->next;\n\t\tn++;\n\t}\n\n\tif (max_subframes && n > max_subframes)\n\t\tgoto out;\n\n\tif (max_frags && nfrags > max_frags)\n\t\tgoto out;\n\n\tif (!drv_can_aggregate_in_amsdu(local, head, skb))\n\t\tgoto out;\n\n\tif (!ieee80211_amsdu_prepare_head(sdata, fast_tx, head))\n\t\tgoto out;\n\n\t/*\n\t * Pad out the previous subframe to a multiple of 4 by adding the\n\t * padding to the next one, that's being added. Note that head->len\n\t * is the length of the full A-MSDU, but that works since each time\n\t * we add a new subframe we pad out the previous one to a multiple\n\t * of 4 and thus it no longer matters in the next round.\n\t */\n\thdrlen = fast_tx->hdr_len - sizeof(rfc1042_header);\n\tif ((head->len - hdrlen) & 3)\n\t\tpad = 4 - ((head->len - hdrlen) & 3);\n\n\tif (!ieee80211_amsdu_realloc_pad(local, skb, sizeof(rfc1042_header) +\n\t\t\t\t\t\t     2 + pad))\n\t\tgoto out_recalc;\n\n\tret = true;\n\tdata = skb_push(skb, ETH_ALEN + 2);\n\tmemmove(data, data + ETH_ALEN + 2, 2 * ETH_ALEN);\n\n\tdata += 2 * ETH_ALEN;\n\tlen = cpu_to_be16(subframe_len);\n\tmemcpy(data, &len, 2);\n\tmemcpy(data + 2, rfc1042_header, sizeof(rfc1042_header));\n\n\tmemset(skb_push(skb, pad), 0, pad);\n\n\thead->len += skb->len;\n\thead->data_len += skb->len;\n\t*frag_tail = skb;\n\nout_recalc:\n\tfq->memory_usage += head->truesize - orig_truesize;\n\tif (head->len != orig_len) {\n\t\tflow->backlog += head->len - orig_len;\n\t\ttin->backlog_bytes += head->len - orig_len;\n\t}\nout:\n\tspin_unlock_bh(&fq->lock);\n\n\treturn ret;\n}\n\n/*\n * Can be called while the sta lock is held. Anything that can cause packets to\n * be generated will cause deadlock!\n */\nstatic void ieee80211_xmit_fast_finish(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t       struct sta_info *sta, u8 pn_offs,\n\t\t\t\t       struct ieee80211_key *key,\n\t\t\t\t       struct sk_buff *skb)\n{\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_hdr *hdr = (void *)skb->data;\n\tu8 tid = IEEE80211_NUM_TIDS;\n\n\tif (key)\n\t\tinfo->control.hw_key = &key->conf;\n\n\tdev_sw_netstats_tx_add(skb->dev, 1, skb->len);\n\n\tif (hdr->frame_control & cpu_to_le16(IEEE80211_STYPE_QOS_DATA)) {\n\t\ttid = skb->priority & IEEE80211_QOS_CTL_TAG1D_MASK;\n\t\thdr->seq_ctrl = ieee80211_tx_next_seq(sta, tid);\n\t} else {\n\t\tinfo->flags |= IEEE80211_TX_CTL_ASSIGN_SEQ;\n\t\thdr->seq_ctrl = cpu_to_le16(sdata->sequence_number);\n\t\tsdata->sequence_number += 0x10;\n\t}\n\n\tif (skb_shinfo(skb)->gso_size)\n\t\tsta->tx_stats.msdu[tid] +=\n\t\t\tDIV_ROUND_UP(skb->len, skb_shinfo(skb)->gso_size);\n\telse\n\t\tsta->tx_stats.msdu[tid]++;\n\n\tinfo->hw_queue = sdata->vif.hw_queue[skb_get_queue_mapping(skb)];\n\n\t/* statistics normally done by ieee80211_tx_h_stats (but that\n\t * has to consider fragmentation, so is more complex)\n\t */\n\tsta->tx_stats.bytes[skb_get_queue_mapping(skb)] += skb->len;\n\tsta->tx_stats.packets[skb_get_queue_mapping(skb)]++;\n\n\tif (pn_offs) {\n\t\tu64 pn;\n\t\tu8 *crypto_hdr = skb->data + pn_offs;\n\n\t\tswitch (key->conf.cipher) {\n\t\tcase WLAN_CIPHER_SUITE_CCMP:\n\t\tcase WLAN_CIPHER_SUITE_CCMP_256:\n\t\tcase WLAN_CIPHER_SUITE_GCMP:\n\t\tcase WLAN_CIPHER_SUITE_GCMP_256:\n\t\t\tpn = atomic64_inc_return(&key->conf.tx_pn);\n\t\t\tcrypto_hdr[0] = pn;\n\t\t\tcrypto_hdr[1] = pn >> 8;\n\t\t\tcrypto_hdr[3] = 0x20 | (key->conf.keyidx << 6);\n\t\t\tcrypto_hdr[4] = pn >> 16;\n\t\t\tcrypto_hdr[5] = pn >> 24;\n\t\t\tcrypto_hdr[6] = pn >> 32;\n\t\t\tcrypto_hdr[7] = pn >> 40;\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic bool ieee80211_xmit_fast(struct ieee80211_sub_if_data *sdata,\n\t\t\t\tstruct sta_info *sta,\n\t\t\t\tstruct ieee80211_fast_tx *fast_tx,\n\t\t\t\tstruct sk_buff *skb)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tu16 ethertype = (skb->data[12] << 8) | skb->data[13];\n\tint extra_head = fast_tx->hdr_len - (ETH_HLEN - 2);\n\tint hw_headroom = sdata->local->hw.extra_tx_headroom;\n\tstruct ethhdr eth;\n\tstruct ieee80211_tx_info *info;\n\tstruct ieee80211_hdr *hdr = (void *)fast_tx->hdr;\n\tstruct ieee80211_tx_data tx;\n\tieee80211_tx_result r;\n\tstruct tid_ampdu_tx *tid_tx = NULL;\n\tu8 tid = IEEE80211_NUM_TIDS;\n\n\t/* control port protocol needs a lot of special handling */\n\tif (cpu_to_be16(ethertype) == sdata->control_port_protocol)\n\t\treturn false;\n\n\t/* only RFC 1042 SNAP */\n\tif (ethertype < ETH_P_802_3_MIN)\n\t\treturn false;\n\n\t/* don't handle TX status request here either */\n\tif (skb->sk && skb_shinfo(skb)->tx_flags & SKBTX_WIFI_STATUS)\n\t\treturn false;\n\n\tif (hdr->frame_control & cpu_to_le16(IEEE80211_STYPE_QOS_DATA)) {\n\t\ttid = skb->priority & IEEE80211_QOS_CTL_TAG1D_MASK;\n\t\ttid_tx = rcu_dereference(sta->ampdu_mlme.tid_tx[tid]);\n\t\tif (tid_tx) {\n\t\t\tif (!test_bit(HT_AGG_STATE_OPERATIONAL, &tid_tx->state))\n\t\t\t\treturn false;\n\t\t\tif (tid_tx->timeout)\n\t\t\t\ttid_tx->last_tx = jiffies;\n\t\t}\n\t}\n\n\t/* after this point (skb is modified) we cannot return false */\n\n\tif (skb_shared(skb)) {\n\t\tstruct sk_buff *tmp_skb = skb;\n\n\t\tskb = skb_clone(skb, GFP_ATOMIC);\n\t\tkfree_skb(tmp_skb);\n\n\t\tif (!skb)\n\t\t\treturn true;\n\t}\n\n\tif ((hdr->frame_control & cpu_to_le16(IEEE80211_STYPE_QOS_DATA)) &&\n\t    ieee80211_amsdu_aggregate(sdata, sta, fast_tx, skb))\n\t\treturn true;\n\n\t/* will not be crypto-handled beyond what we do here, so use false\n\t * as the may-encrypt argument for the resize to not account for\n\t * more room than we already have in 'extra_head'\n\t */\n\tif (unlikely(ieee80211_skb_resize(sdata, skb,\n\t\t\t\t\t  max_t(int, extra_head + hw_headroom -\n\t\t\t\t\t\t     skb_headroom(skb), 0),\n\t\t\t\t\t  ENCRYPT_NO))) {\n\t\tkfree_skb(skb);\n\t\treturn true;\n\t}\n\n\tmemcpy(&eth, skb->data, ETH_HLEN - 2);\n\thdr = skb_push(skb, extra_head);\n\tmemcpy(skb->data, fast_tx->hdr, fast_tx->hdr_len);\n\tmemcpy(skb->data + fast_tx->da_offs, eth.h_dest, ETH_ALEN);\n\tmemcpy(skb->data + fast_tx->sa_offs, eth.h_source, ETH_ALEN);\n\n\tinfo = IEEE80211_SKB_CB(skb);\n\tmemset(info, 0, sizeof(*info));\n\tinfo->band = fast_tx->band;\n\tinfo->control.vif = &sdata->vif;\n\tinfo->flags = IEEE80211_TX_CTL_FIRST_FRAGMENT |\n\t\t      IEEE80211_TX_CTL_DONTFRAG |\n\t\t      (tid_tx ? IEEE80211_TX_CTL_AMPDU : 0);\n\tinfo->control.flags = IEEE80211_TX_CTRL_FAST_XMIT;\n\n#ifdef CONFIG_MAC80211_DEBUGFS\n\tif (local->force_tx_status)\n\t\tinfo->flags |= IEEE80211_TX_CTL_REQ_TX_STATUS;\n#endif\n\n\tif (hdr->frame_control & cpu_to_le16(IEEE80211_STYPE_QOS_DATA)) {\n\t\ttid = skb->priority & IEEE80211_QOS_CTL_TAG1D_MASK;\n\t\t*ieee80211_get_qos_ctl(hdr) = tid;\n\t}\n\n\t__skb_queue_head_init(&tx.skbs);\n\n\ttx.flags = IEEE80211_TX_UNICAST;\n\ttx.local = local;\n\ttx.sdata = sdata;\n\ttx.sta = sta;\n\ttx.key = fast_tx->key;\n\n\tif (!ieee80211_hw_check(&local->hw, HAS_RATE_CONTROL)) {\n\t\ttx.skb = skb;\n\t\tr = ieee80211_tx_h_rate_ctrl(&tx);\n\t\tskb = tx.skb;\n\t\ttx.skb = NULL;\n\n\t\tif (r != TX_CONTINUE) {\n\t\t\tif (r != TX_QUEUED)\n\t\t\t\tkfree_skb(skb);\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tif (ieee80211_queue_skb(local, sdata, sta, skb))\n\t\treturn true;\n\n\tieee80211_xmit_fast_finish(sdata, sta, fast_tx->pn_offs,\n\t\t\t\t   fast_tx->key, skb);\n\n\tif (sdata->vif.type == NL80211_IFTYPE_AP_VLAN)\n\t\tsdata = container_of(sdata->bss,\n\t\t\t\t     struct ieee80211_sub_if_data, u.ap);\n\n\t__skb_queue_tail(&tx.skbs, skb);\n\tieee80211_tx_frags(local, &sdata->vif, sta, &tx.skbs, false);\n\treturn true;\n}\n\nstruct sk_buff *ieee80211_tx_dequeue(struct ieee80211_hw *hw,\n\t\t\t\t     struct ieee80211_txq *txq)\n{\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\tstruct txq_info *txqi = container_of(txq, struct txq_info, txq);\n\tstruct ieee80211_hdr *hdr;\n\tstruct sk_buff *skb = NULL;\n\tstruct fq *fq = &local->fq;\n\tstruct fq_tin *tin = &txqi->tin;\n\tstruct ieee80211_tx_info *info;\n\tstruct ieee80211_tx_data tx;\n\tieee80211_tx_result r;\n\tstruct ieee80211_vif *vif = txq->vif;\n\n\tWARN_ON_ONCE(softirq_count() == 0);\n\n\tif (!ieee80211_txq_airtime_check(hw, txq))\n\t\treturn NULL;\n\nbegin:\n\tspin_lock_bh(&fq->lock);\n\n\tif (test_bit(IEEE80211_TXQ_STOP, &txqi->flags) ||\n\t    test_bit(IEEE80211_TXQ_STOP_NETIF_TX, &txqi->flags))\n\t\tgoto out;\n\n\tif (vif->txqs_stopped[ieee80211_ac_from_tid(txq->tid)]) {\n\t\tset_bit(IEEE80211_TXQ_STOP_NETIF_TX, &txqi->flags);\n\t\tgoto out;\n\t}\n\n\t/* Make sure fragments stay together. */\n\tskb = __skb_dequeue(&txqi->frags);\n\tif (skb)\n\t\tgoto out;\n\n\tskb = fq_tin_dequeue(fq, tin, fq_tin_dequeue_func);\n\tif (!skb)\n\t\tgoto out;\n\n\tspin_unlock_bh(&fq->lock);\n\n\thdr = (struct ieee80211_hdr *)skb->data;\n\tinfo = IEEE80211_SKB_CB(skb);\n\n\tmemset(&tx, 0, sizeof(tx));\n\t__skb_queue_head_init(&tx.skbs);\n\ttx.local = local;\n\ttx.skb = skb;\n\ttx.sdata = vif_to_sdata(info->control.vif);\n\n\tif (txq->sta) {\n\t\ttx.sta = container_of(txq->sta, struct sta_info, sta);\n\t\t/*\n\t\t * Drop unicast frames to unauthorised stations unless they are\n\t\t * injected frames or EAPOL frames from the local station.\n\t\t */\n\t\tif (unlikely(!(info->flags & IEEE80211_TX_CTL_INJECTED) &&\n\t\t\t     ieee80211_is_data(hdr->frame_control) &&\n\t\t\t     !ieee80211_vif_is_mesh(&tx.sdata->vif) &&\n\t\t\t     tx.sdata->vif.type != NL80211_IFTYPE_OCB &&\n\t\t\t     !is_multicast_ether_addr(hdr->addr1) &&\n\t\t\t     !test_sta_flag(tx.sta, WLAN_STA_AUTHORIZED) &&\n\t\t\t     (!(info->control.flags &\n\t\t\t\tIEEE80211_TX_CTRL_PORT_CTRL_PROTO) ||\n\t\t\t      !ether_addr_equal(tx.sdata->vif.addr,\n\t\t\t\t\t\thdr->addr2)))) {\n\t\t\tI802_DEBUG_INC(local->tx_handlers_drop_unauth_port);\n\t\t\tieee80211_free_txskb(&local->hw, skb);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\n\t/*\n\t * The key can be removed while the packet was queued, so need to call\n\t * this here to get the current key.\n\t */\n\tr = ieee80211_tx_h_select_key(&tx);\n\tif (r != TX_CONTINUE) {\n\t\tieee80211_free_txskb(&local->hw, skb);\n\t\tgoto begin;\n\t}\n\n\tif (test_bit(IEEE80211_TXQ_AMPDU, &txqi->flags))\n\t\tinfo->flags |= IEEE80211_TX_CTL_AMPDU;\n\telse\n\t\tinfo->flags &= ~IEEE80211_TX_CTL_AMPDU;\n\n\tif (info->flags & IEEE80211_TX_CTL_HW_80211_ENCAP)\n\t\tgoto encap_out;\n\n\tif (info->control.flags & IEEE80211_TX_CTRL_FAST_XMIT) {\n\t\tstruct sta_info *sta = container_of(txq->sta, struct sta_info,\n\t\t\t\t\t\t    sta);\n\t\tu8 pn_offs = 0;\n\n\t\tif (tx.key &&\n\t\t    (tx.key->conf.flags & IEEE80211_KEY_FLAG_GENERATE_IV))\n\t\t\tpn_offs = ieee80211_hdrlen(hdr->frame_control);\n\n\t\tieee80211_xmit_fast_finish(sta->sdata, sta, pn_offs,\n\t\t\t\t\t   tx.key, skb);\n\t} else {\n\t\tif (invoke_tx_handlers_late(&tx))\n\t\t\tgoto begin;\n\n\t\tskb = __skb_dequeue(&tx.skbs);\n\n\t\tif (!skb_queue_empty(&tx.skbs)) {\n\t\t\tspin_lock_bh(&fq->lock);\n\t\t\tskb_queue_splice_tail(&tx.skbs, &txqi->frags);\n\t\t\tspin_unlock_bh(&fq->lock);\n\t\t}\n\t}\n\n\tif (skb_has_frag_list(skb) &&\n\t    !ieee80211_hw_check(&local->hw, TX_FRAG_LIST)) {\n\t\tif (skb_linearize(skb)) {\n\t\t\tieee80211_free_txskb(&local->hw, skb);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\n\tswitch (tx.sdata->vif.type) {\n\tcase NL80211_IFTYPE_MONITOR:\n\t\tif (tx.sdata->u.mntr.flags & MONITOR_FLAG_ACTIVE) {\n\t\t\tvif = &tx.sdata->vif;\n\t\t\tbreak;\n\t\t}\n\t\ttx.sdata = rcu_dereference(local->monitor_sdata);\n\t\tif (tx.sdata) {\n\t\t\tvif = &tx.sdata->vif;\n\t\t\tinfo->hw_queue =\n\t\t\t\tvif->hw_queue[skb_get_queue_mapping(skb)];\n\t\t} else if (ieee80211_hw_check(&local->hw, QUEUE_CONTROL)) {\n\t\t\tieee80211_free_txskb(&local->hw, skb);\n\t\t\tgoto begin;\n\t\t} else {\n\t\t\tvif = NULL;\n\t\t}\n\t\tbreak;\n\tcase NL80211_IFTYPE_AP_VLAN:\n\t\ttx.sdata = container_of(tx.sdata->bss,\n\t\t\t\t\tstruct ieee80211_sub_if_data, u.ap);\n\t\tfallthrough;\n\tdefault:\n\t\tvif = &tx.sdata->vif;\n\t\tbreak;\n\t}\n\nencap_out:\n\tIEEE80211_SKB_CB(skb)->control.vif = vif;\n\n\tif (vif &&\n\t    wiphy_ext_feature_isset(local->hw.wiphy, NL80211_EXT_FEATURE_AQL)) {\n\t\tbool ampdu = txq->ac != IEEE80211_AC_VO;\n\t\tu32 airtime;\n\n\t\tairtime = ieee80211_calc_expected_tx_airtime(hw, vif, txq->sta,\n\t\t\t\t\t\t\t     skb->len, ampdu);\n\t\tif (airtime) {\n\t\t\tairtime = ieee80211_info_set_tx_time_est(info, airtime);\n\t\t\tieee80211_sta_update_pending_airtime(local, tx.sta,\n\t\t\t\t\t\t\t     txq->ac,\n\t\t\t\t\t\t\t     airtime,\n\t\t\t\t\t\t\t     false);\n\t\t}\n\t}\n\n\treturn skb;\n\nout:\n\tspin_unlock_bh(&fq->lock);\n\n\treturn skb;\n}\nEXPORT_SYMBOL(ieee80211_tx_dequeue);\n\nstruct ieee80211_txq *ieee80211_next_txq(struct ieee80211_hw *hw, u8 ac)\n{\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\tstruct ieee80211_txq *ret = NULL;\n\tstruct txq_info *txqi = NULL, *head = NULL;\n\tbool found_eligible_txq = false;\n\n\tspin_lock_bh(&local->active_txq_lock[ac]);\n\n begin:\n\ttxqi = list_first_entry_or_null(&local->active_txqs[ac],\n\t\t\t\t\tstruct txq_info,\n\t\t\t\t\tschedule_order);\n\tif (!txqi)\n\t\tgoto out;\n\n\tif (txqi == head) {\n\t\tif (!found_eligible_txq)\n\t\t\tgoto out;\n\t\telse\n\t\t\tfound_eligible_txq = false;\n\t}\n\n\tif (!head)\n\t\thead = txqi;\n\n\tif (txqi->txq.sta) {\n\t\tstruct sta_info *sta = container_of(txqi->txq.sta,\n\t\t\t\t\t\t    struct sta_info, sta);\n\t\tbool aql_check = ieee80211_txq_airtime_check(hw, &txqi->txq);\n\t\ts64 deficit = sta->airtime[txqi->txq.ac].deficit;\n\n\t\tif (aql_check)\n\t\t\tfound_eligible_txq = true;\n\n\t\tif (deficit < 0)\n\t\t\tsta->airtime[txqi->txq.ac].deficit +=\n\t\t\t\tsta->airtime_weight;\n\n\t\tif (deficit < 0 || !aql_check) {\n\t\t\tlist_move_tail(&txqi->schedule_order,\n\t\t\t\t       &local->active_txqs[txqi->txq.ac]);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\n\n\tif (txqi->schedule_round == local->schedule_round[ac])\n\t\tgoto out;\n\n\tlist_del_init(&txqi->schedule_order);\n\ttxqi->schedule_round = local->schedule_round[ac];\n\tret = &txqi->txq;\n\nout:\n\tspin_unlock_bh(&local->active_txq_lock[ac]);\n\treturn ret;\n}\nEXPORT_SYMBOL(ieee80211_next_txq);\n\nvoid __ieee80211_schedule_txq(struct ieee80211_hw *hw,\n\t\t\t      struct ieee80211_txq *txq,\n\t\t\t      bool force)\n{\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\tstruct txq_info *txqi = to_txq_info(txq);\n\n\tspin_lock_bh(&local->active_txq_lock[txq->ac]);\n\n\tif (list_empty(&txqi->schedule_order) &&\n\t    (force || !skb_queue_empty(&txqi->frags) ||\n\t     txqi->tin.backlog_packets)) {\n\t\t/* If airtime accounting is active, always enqueue STAs at the\n\t\t * head of the list to ensure that they only get moved to the\n\t\t * back by the airtime DRR scheduler once they have a negative\n\t\t * deficit. A station that already has a negative deficit will\n\t\t * get immediately moved to the back of the list on the next\n\t\t * call to ieee80211_next_txq().\n\t\t */\n\t\tif (txqi->txq.sta && local->airtime_flags &&\n\t\t    wiphy_ext_feature_isset(local->hw.wiphy,\n\t\t\t\t\t    NL80211_EXT_FEATURE_AIRTIME_FAIRNESS))\n\t\t\tlist_add(&txqi->schedule_order,\n\t\t\t\t &local->active_txqs[txq->ac]);\n\t\telse\n\t\t\tlist_add_tail(&txqi->schedule_order,\n\t\t\t\t      &local->active_txqs[txq->ac]);\n\t}\n\n\tspin_unlock_bh(&local->active_txq_lock[txq->ac]);\n}\nEXPORT_SYMBOL(__ieee80211_schedule_txq);\n\nDEFINE_STATIC_KEY_FALSE(aql_disable);\n\nbool ieee80211_txq_airtime_check(struct ieee80211_hw *hw,\n\t\t\t\t struct ieee80211_txq *txq)\n{\n\tstruct sta_info *sta;\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\n\tif (!wiphy_ext_feature_isset(local->hw.wiphy, NL80211_EXT_FEATURE_AQL))\n\t\treturn true;\n\n\tif (static_branch_unlikely(&aql_disable))\n\t\treturn true;\n\n\tif (!txq->sta)\n\t\treturn true;\n\n\tsta = container_of(txq->sta, struct sta_info, sta);\n\tif (atomic_read(&sta->airtime[txq->ac].aql_tx_pending) <\n\t    sta->airtime[txq->ac].aql_limit_low)\n\t\treturn true;\n\n\tif (atomic_read(&local->aql_total_pending_airtime) <\n\t    local->aql_threshold &&\n\t    atomic_read(&sta->airtime[txq->ac].aql_tx_pending) <\n\t    sta->airtime[txq->ac].aql_limit_high)\n\t\treturn true;\n\n\treturn false;\n}\nEXPORT_SYMBOL(ieee80211_txq_airtime_check);\n\nbool ieee80211_txq_may_transmit(struct ieee80211_hw *hw,\n\t\t\t\tstruct ieee80211_txq *txq)\n{\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\tstruct txq_info *iter, *tmp, *txqi = to_txq_info(txq);\n\tstruct sta_info *sta;\n\tu8 ac = txq->ac;\n\n\tspin_lock_bh(&local->active_txq_lock[ac]);\n\n\tif (!txqi->txq.sta)\n\t\tgoto out;\n\n\tif (list_empty(&txqi->schedule_order))\n\t\tgoto out;\n\n\tlist_for_each_entry_safe(iter, tmp, &local->active_txqs[ac],\n\t\t\t\t schedule_order) {\n\t\tif (iter == txqi)\n\t\t\tbreak;\n\n\t\tif (!iter->txq.sta) {\n\t\t\tlist_move_tail(&iter->schedule_order,\n\t\t\t\t       &local->active_txqs[ac]);\n\t\t\tcontinue;\n\t\t}\n\t\tsta = container_of(iter->txq.sta, struct sta_info, sta);\n\t\tif (sta->airtime[ac].deficit < 0)\n\t\t\tsta->airtime[ac].deficit += sta->airtime_weight;\n\t\tlist_move_tail(&iter->schedule_order, &local->active_txqs[ac]);\n\t}\n\n\tsta = container_of(txqi->txq.sta, struct sta_info, sta);\n\tif (sta->airtime[ac].deficit >= 0)\n\t\tgoto out;\n\n\tsta->airtime[ac].deficit += sta->airtime_weight;\n\tlist_move_tail(&txqi->schedule_order, &local->active_txqs[ac]);\n\tspin_unlock_bh(&local->active_txq_lock[ac]);\n\n\treturn false;\nout:\n\tif (!list_empty(&txqi->schedule_order))\n\t\tlist_del_init(&txqi->schedule_order);\n\tspin_unlock_bh(&local->active_txq_lock[ac]);\n\n\treturn true;\n}\nEXPORT_SYMBOL(ieee80211_txq_may_transmit);\n\nvoid ieee80211_txq_schedule_start(struct ieee80211_hw *hw, u8 ac)\n{\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\n\tspin_lock_bh(&local->active_txq_lock[ac]);\n\tlocal->schedule_round[ac]++;\n\tspin_unlock_bh(&local->active_txq_lock[ac]);\n}\nEXPORT_SYMBOL(ieee80211_txq_schedule_start);\n\nvoid __ieee80211_subif_start_xmit(struct sk_buff *skb,\n\t\t\t\t  struct net_device *dev,\n\t\t\t\t  u32 info_flags,\n\t\t\t\t  u32 ctrl_flags,\n\t\t\t\t  u64 *cookie)\n{\n\tstruct ieee80211_sub_if_data *sdata = IEEE80211_DEV_TO_SUB_IF(dev);\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sta_info *sta;\n\tstruct sk_buff *next;\n\n\tif (unlikely(skb->len < ETH_HLEN)) {\n\t\tkfree_skb(skb);\n\t\treturn;\n\t}\n\n\trcu_read_lock();\n\n\tif (ieee80211_lookup_ra_sta(sdata, skb, &sta))\n\t\tgoto out_free;\n\n\tif (IS_ERR(sta))\n\t\tsta = NULL;\n\n\tif (local->ops->wake_tx_queue) {\n\t\tu16 queue = __ieee80211_select_queue(sdata, sta, skb);\n\t\tskb_set_queue_mapping(skb, queue);\n\t\tskb_get_hash(skb);\n\t}\n\n\tif (sta) {\n\t\tstruct ieee80211_fast_tx *fast_tx;\n\n\t\tsk_pacing_shift_update(skb->sk, sdata->local->hw.tx_sk_pacing_shift);\n\n\t\tfast_tx = rcu_dereference(sta->fast_tx);\n\n\t\tif (fast_tx &&\n\t\t    ieee80211_xmit_fast(sdata, sta, fast_tx, skb))\n\t\t\tgoto out;\n\t}\n\n\tif (skb_is_gso(skb)) {\n\t\tstruct sk_buff *segs;\n\n\t\tsegs = skb_gso_segment(skb, 0);\n\t\tif (IS_ERR(segs)) {\n\t\t\tgoto out_free;\n\t\t} else if (segs) {\n\t\t\tconsume_skb(skb);\n\t\t\tskb = segs;\n\t\t}\n\t} else {\n\t\t/* we cannot process non-linear frames on this path */\n\t\tif (skb_linearize(skb)) {\n\t\t\tkfree_skb(skb);\n\t\t\tgoto out;\n\t\t}\n\n\t\t/* the frame could be fragmented, software-encrypted, and other\n\t\t * things so we cannot really handle checksum offload with it -\n\t\t * fix it up in software before we handle anything else.\n\t\t */\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tskb_set_transport_header(skb,\n\t\t\t\t\t\t skb_checksum_start_offset(skb));\n\t\t\tif (skb_checksum_help(skb))\n\t\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\tskb_list_walk_safe(skb, skb, next) {\n\t\tskb_mark_not_on_list(skb);\n\n\t\tif (skb->protocol == sdata->control_port_protocol)\n\t\t\tctrl_flags |= IEEE80211_TX_CTRL_SKIP_MPATH_LOOKUP;\n\n\t\tskb = ieee80211_build_hdr(sdata, skb, info_flags,\n\t\t\t\t\t  sta, ctrl_flags, cookie);\n\t\tif (IS_ERR(skb)) {\n\t\t\tkfree_skb_list(next);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev_sw_netstats_tx_add(dev, 1, skb->len);\n\n\t\tieee80211_xmit(sdata, sta, skb);\n\t}\n\tgoto out;\n out_free:\n\tkfree_skb(skb);\n out:\n\trcu_read_unlock();\n}\n\nstatic int ieee80211_change_da(struct sk_buff *skb, struct sta_info *sta)\n{\n\tstruct ethhdr *eth;\n\tint err;\n\n\terr = skb_ensure_writable(skb, ETH_HLEN);\n\tif (unlikely(err))\n\t\treturn err;\n\n\teth = (void *)skb->data;\n\tether_addr_copy(eth->h_dest, sta->sta.addr);\n\n\treturn 0;\n}\n\nstatic bool ieee80211_multicast_to_unicast(struct sk_buff *skb,\n\t\t\t\t\t   struct net_device *dev)\n{\n\tstruct ieee80211_sub_if_data *sdata = IEEE80211_DEV_TO_SUB_IF(dev);\n\tconst struct ethhdr *eth = (void *)skb->data;\n\tconst struct vlan_ethhdr *ethvlan = (void *)skb->data;\n\t__be16 ethertype;\n\n\tif (likely(!is_multicast_ether_addr(eth->h_dest)))\n\t\treturn false;\n\n\tswitch (sdata->vif.type) {\n\tcase NL80211_IFTYPE_AP_VLAN:\n\t\tif (sdata->u.vlan.sta)\n\t\t\treturn false;\n\t\tif (sdata->wdev.use_4addr)\n\t\t\treturn false;\n\t\tfallthrough;\n\tcase NL80211_IFTYPE_AP:\n\t\t/* check runtime toggle for this bss */\n\t\tif (!sdata->bss->multicast_to_unicast)\n\t\t\treturn false;\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\t/* multicast to unicast conversion only for some payload */\n\tethertype = eth->h_proto;\n\tif (ethertype == htons(ETH_P_8021Q) && skb->len >= VLAN_ETH_HLEN)\n\t\tethertype = ethvlan->h_vlan_encapsulated_proto;\n\tswitch (ethertype) {\n\tcase htons(ETH_P_ARP):\n\tcase htons(ETH_P_IP):\n\tcase htons(ETH_P_IPV6):\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic void\nieee80211_convert_to_unicast(struct sk_buff *skb, struct net_device *dev,\n\t\t\t     struct sk_buff_head *queue)\n{\n\tstruct ieee80211_sub_if_data *sdata = IEEE80211_DEV_TO_SUB_IF(dev);\n\tstruct ieee80211_local *local = sdata->local;\n\tconst struct ethhdr *eth = (struct ethhdr *)skb->data;\n\tstruct sta_info *sta, *first = NULL;\n\tstruct sk_buff *cloned_skb;\n\n\trcu_read_lock();\n\n\tlist_for_each_entry_rcu(sta, &local->sta_list, list) {\n\t\tif (sdata != sta->sdata)\n\t\t\t/* AP-VLAN mismatch */\n\t\t\tcontinue;\n\t\tif (unlikely(ether_addr_equal(eth->h_source, sta->sta.addr)))\n\t\t\t/* do not send back to source */\n\t\t\tcontinue;\n\t\tif (!first) {\n\t\t\tfirst = sta;\n\t\t\tcontinue;\n\t\t}\n\t\tcloned_skb = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!cloned_skb)\n\t\t\tgoto multicast;\n\t\tif (unlikely(ieee80211_change_da(cloned_skb, sta))) {\n\t\t\tdev_kfree_skb(cloned_skb);\n\t\t\tgoto multicast;\n\t\t}\n\t\t__skb_queue_tail(queue, cloned_skb);\n\t}\n\n\tif (likely(first)) {\n\t\tif (unlikely(ieee80211_change_da(skb, first)))\n\t\t\tgoto multicast;\n\t\t__skb_queue_tail(queue, skb);\n\t} else {\n\t\t/* no STA connected, drop */\n\t\tkfree_skb(skb);\n\t\tskb = NULL;\n\t}\n\n\tgoto out;\nmulticast:\n\t__skb_queue_purge(queue);\n\t__skb_queue_tail(queue, skb);\nout:\n\trcu_read_unlock();\n}\n\n/**\n * ieee80211_subif_start_xmit - netif start_xmit function for 802.3 vifs\n * @skb: packet to be sent\n * @dev: incoming interface\n *\n * On failure skb will be freed.\n */\nnetdev_tx_t ieee80211_subif_start_xmit(struct sk_buff *skb,\n\t\t\t\t       struct net_device *dev)\n{\n\tif (unlikely(ieee80211_multicast_to_unicast(skb, dev))) {\n\t\tstruct sk_buff_head queue;\n\n\t\t__skb_queue_head_init(&queue);\n\t\tieee80211_convert_to_unicast(skb, dev, &queue);\n\t\twhile ((skb = __skb_dequeue(&queue)))\n\t\t\t__ieee80211_subif_start_xmit(skb, dev, 0, 0, NULL);\n\t} else {\n\t\t__ieee80211_subif_start_xmit(skb, dev, 0, 0, NULL);\n\t}\n\n\treturn NETDEV_TX_OK;\n}\n\nstatic bool ieee80211_tx_8023(struct ieee80211_sub_if_data *sdata,\n\t\t\t      struct sk_buff *skb, int led_len,\n\t\t\t      struct sta_info *sta,\n\t\t\t      bool txpending)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct ieee80211_tx_control control = {};\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_sta *pubsta = NULL;\n\tunsigned long flags;\n\tint q = info->hw_queue;\n\n\tif (ieee80211_queue_skb(local, sdata, sta, skb))\n\t\treturn true;\n\n\tspin_lock_irqsave(&local->queue_stop_reason_lock, flags);\n\n\tif (local->queue_stop_reasons[q] ||\n\t    (!txpending && !skb_queue_empty(&local->pending[q]))) {\n\t\tif (txpending)\n\t\t\tskb_queue_head(&local->pending[q], skb);\n\t\telse\n\t\t\tskb_queue_tail(&local->pending[q], skb);\n\n\t\tspin_unlock_irqrestore(&local->queue_stop_reason_lock, flags);\n\n\t\treturn false;\n\t}\n\n\tspin_unlock_irqrestore(&local->queue_stop_reason_lock, flags);\n\n\tif (sta && sta->uploaded)\n\t\tpubsta = &sta->sta;\n\n\tcontrol.sta = pubsta;\n\n\tdrv_tx(local, &control, skb);\n\n\treturn true;\n}\n\nstatic void ieee80211_8023_xmit(struct ieee80211_sub_if_data *sdata,\n\t\t\t\tstruct net_device *dev, struct sta_info *sta,\n\t\t\t\tstruct ieee80211_key *key, struct sk_buff *skb)\n{\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct tid_ampdu_tx *tid_tx;\n\tu8 tid;\n\n\tif (local->ops->wake_tx_queue) {\n\t\tu16 queue = __ieee80211_select_queue(sdata, sta, skb);\n\t\tskb_set_queue_mapping(skb, queue);\n\t\tskb_get_hash(skb);\n\t}\n\n\tif (unlikely(test_bit(SCAN_SW_SCANNING, &local->scanning)) &&\n\t    test_bit(SDATA_STATE_OFFCHANNEL, &sdata->state))\n\t\tgoto out_free;\n\n\tmemset(info, 0, sizeof(*info));\n\n\ttid = skb->priority & IEEE80211_QOS_CTL_TAG1D_MASK;\n\ttid_tx = rcu_dereference(sta->ampdu_mlme.tid_tx[tid]);\n\tif (tid_tx) {\n\t\tif (!test_bit(HT_AGG_STATE_OPERATIONAL, &tid_tx->state)) {\n\t\t\t/* fall back to non-offload slow path */\n\t\t\t__ieee80211_subif_start_xmit(skb, dev, 0, 0, NULL);\n\t\t\treturn;\n\t\t}\n\n\t\tinfo->flags |= IEEE80211_TX_CTL_AMPDU;\n\t\tif (tid_tx->timeout)\n\t\t\ttid_tx->last_tx = jiffies;\n\t}\n\n\tif (unlikely(skb->sk &&\n\t\t     skb_shinfo(skb)->tx_flags & SKBTX_WIFI_STATUS))\n\t\tinfo->ack_frame_id = ieee80211_store_ack_skb(local, skb,\n\t\t\t\t\t\t\t     &info->flags, NULL);\n\n\tinfo->hw_queue = sdata->vif.hw_queue[skb_get_queue_mapping(skb)];\n\n\tdev_sw_netstats_tx_add(dev, 1, skb->len);\n\n\tsta->tx_stats.bytes[skb_get_queue_mapping(skb)] += skb->len;\n\tsta->tx_stats.packets[skb_get_queue_mapping(skb)]++;\n\n\tif (sdata->vif.type == NL80211_IFTYPE_AP_VLAN)\n\t\tsdata = container_of(sdata->bss,\n\t\t\t\t     struct ieee80211_sub_if_data, u.ap);\n\n\tinfo->flags |= IEEE80211_TX_CTL_HW_80211_ENCAP;\n\tinfo->control.vif = &sdata->vif;\n\n\tif (key)\n\t\tinfo->control.hw_key = &key->conf;\n\n\tieee80211_tx_8023(sdata, skb, skb->len, sta, false);\n\n\treturn;\n\nout_free:\n\tkfree_skb(skb);\n}\n\nnetdev_tx_t ieee80211_subif_start_xmit_8023(struct sk_buff *skb,\n\t\t\t\t\t    struct net_device *dev)\n{\n\tstruct ieee80211_sub_if_data *sdata = IEEE80211_DEV_TO_SUB_IF(dev);\n\tstruct ethhdr *ehdr = (struct ethhdr *)skb->data;\n\tstruct ieee80211_key *key;\n\tstruct sta_info *sta;\n\n\tif (unlikely(skb->len < ETH_HLEN)) {\n\t\tkfree_skb(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\trcu_read_lock();\n\n\tif (ieee80211_lookup_ra_sta(sdata, skb, &sta)) {\n\t\tkfree_skb(skb);\n\t\tgoto out;\n\t}\n\n\tif (unlikely(IS_ERR_OR_NULL(sta) || !sta->uploaded ||\n\t    !test_sta_flag(sta, WLAN_STA_AUTHORIZED) ||\n\t    sdata->control_port_protocol == ehdr->h_proto))\n\t\tgoto skip_offload;\n\n\tkey = rcu_dereference(sta->ptk[sta->ptk_idx]);\n\tif (!key)\n\t\tkey = rcu_dereference(sdata->default_unicast_key);\n\n\tif (key && (!(key->flags & KEY_FLAG_UPLOADED_TO_HARDWARE) ||\n\t\t    key->conf.cipher == WLAN_CIPHER_SUITE_TKIP))\n\t\tgoto skip_offload;\n\n\tieee80211_8023_xmit(sdata, dev, sta, key, skb);\n\tgoto out;\n\nskip_offload:\n\tieee80211_subif_start_xmit(skb, dev);\nout:\n\trcu_read_unlock();\n\n\treturn NETDEV_TX_OK;\n}\n\nstruct sk_buff *\nieee80211_build_data_template(struct ieee80211_sub_if_data *sdata,\n\t\t\t      struct sk_buff *skb, u32 info_flags)\n{\n\tstruct ieee80211_hdr *hdr;\n\tstruct ieee80211_tx_data tx = {\n\t\t.local = sdata->local,\n\t\t.sdata = sdata,\n\t};\n\tstruct sta_info *sta;\n\n\trcu_read_lock();\n\n\tif (ieee80211_lookup_ra_sta(sdata, skb, &sta)) {\n\t\tkfree_skb(skb);\n\t\tskb = ERR_PTR(-EINVAL);\n\t\tgoto out;\n\t}\n\n\tskb = ieee80211_build_hdr(sdata, skb, info_flags, sta, 0, NULL);\n\tif (IS_ERR(skb))\n\t\tgoto out;\n\n\thdr = (void *)skb->data;\n\ttx.sta = sta_info_get(sdata, hdr->addr1);\n\ttx.skb = skb;\n\n\tif (ieee80211_tx_h_select_key(&tx) != TX_CONTINUE) {\n\t\trcu_read_unlock();\n\t\tkfree_skb(skb);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\nout:\n\trcu_read_unlock();\n\treturn skb;\n}\n\n/*\n * ieee80211_clear_tx_pending may not be called in a context where\n * it is possible that it packets could come in again.\n */\nvoid ieee80211_clear_tx_pending(struct ieee80211_local *local)\n{\n\tstruct sk_buff *skb;\n\tint i;\n\n\tfor (i = 0; i < local->hw.queues; i++) {\n\t\twhile ((skb = skb_dequeue(&local->pending[i])) != NULL)\n\t\t\tieee80211_free_txskb(&local->hw, skb);\n\t}\n}\n\n/*\n * Returns false if the frame couldn't be transmitted but was queued instead,\n * which in this case means re-queued -- take as an indication to stop sending\n * more pending frames.\n */\nstatic bool ieee80211_tx_pending_skb(struct ieee80211_local *local,\n\t\t\t\t     struct sk_buff *skb)\n{\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_sub_if_data *sdata;\n\tstruct sta_info *sta;\n\tstruct ieee80211_hdr *hdr;\n\tbool result;\n\tstruct ieee80211_chanctx_conf *chanctx_conf;\n\n\tsdata = vif_to_sdata(info->control.vif);\n\n\tif (info->control.flags & IEEE80211_TX_INTCFL_NEED_TXPROCESSING) {\n\t\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\t\tif (unlikely(!chanctx_conf)) {\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn true;\n\t\t}\n\t\tinfo->band = chanctx_conf->def.chan->band;\n\t\tresult = ieee80211_tx(sdata, NULL, skb, true);\n\t} else if (info->flags & IEEE80211_TX_CTL_HW_80211_ENCAP) {\n\t\tif (ieee80211_lookup_ra_sta(sdata, skb, &sta)) {\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn true;\n\t\t}\n\n\t\tif (IS_ERR(sta) || (sta && !sta->uploaded))\n\t\t\tsta = NULL;\n\n\t\tresult = ieee80211_tx_8023(sdata, skb, skb->len, sta, true);\n\t} else {\n\t\tstruct sk_buff_head skbs;\n\n\t\t__skb_queue_head_init(&skbs);\n\t\t__skb_queue_tail(&skbs, skb);\n\n\t\thdr = (struct ieee80211_hdr *)skb->data;\n\t\tsta = sta_info_get(sdata, hdr->addr1);\n\n\t\tresult = __ieee80211_tx(local, &skbs, skb->len, sta, true);\n\t}\n\n\treturn result;\n}\n\n/*\n * Transmit all pending packets. Called from tasklet.\n */\nvoid ieee80211_tx_pending(struct tasklet_struct *t)\n{\n\tstruct ieee80211_local *local = from_tasklet(local, t,\n\t\t\t\t\t\t     tx_pending_tasklet);\n\tunsigned long flags;\n\tint i;\n\tbool txok;\n\n\trcu_read_lock();\n\n\tspin_lock_irqsave(&local->queue_stop_reason_lock, flags);\n\tfor (i = 0; i < local->hw.queues; i++) {\n\t\t/*\n\t\t * If queue is stopped by something other than due to pending\n\t\t * frames, or we have no pending frames, proceed to next queue.\n\t\t */\n\t\tif (local->queue_stop_reasons[i] ||\n\t\t    skb_queue_empty(&local->pending[i]))\n\t\t\tcontinue;\n\n\t\twhile (!skb_queue_empty(&local->pending[i])) {\n\t\t\tstruct sk_buff *skb = __skb_dequeue(&local->pending[i]);\n\t\t\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\n\t\t\tif (WARN_ON(!info->control.vif)) {\n\t\t\t\tieee80211_free_txskb(&local->hw, skb);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tspin_unlock_irqrestore(&local->queue_stop_reason_lock,\n\t\t\t\t\t\tflags);\n\n\t\t\ttxok = ieee80211_tx_pending_skb(local, skb);\n\t\t\tspin_lock_irqsave(&local->queue_stop_reason_lock,\n\t\t\t\t\t  flags);\n\t\t\tif (!txok)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (skb_queue_empty(&local->pending[i]))\n\t\t\tieee80211_propagate_queue_wake(local, i);\n\t}\n\tspin_unlock_irqrestore(&local->queue_stop_reason_lock, flags);\n\n\trcu_read_unlock();\n}\n\n/* functions for drivers to get certain frames */\n\nstatic void __ieee80211_beacon_add_tim(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t       struct ps_data *ps, struct sk_buff *skb,\n\t\t\t\t       bool is_template)\n{\n\tu8 *pos, *tim;\n\tint aid0 = 0;\n\tint i, have_bits = 0, n1, n2;\n\n\t/* Generate bitmap for TIM only if there are any STAs in power save\n\t * mode. */\n\tif (atomic_read(&ps->num_sta_ps) > 0)\n\t\t/* in the hope that this is faster than\n\t\t * checking byte-for-byte */\n\t\thave_bits = !bitmap_empty((unsigned long *)ps->tim,\n\t\t\t\t\t  IEEE80211_MAX_AID+1);\n\tif (!is_template) {\n\t\tif (ps->dtim_count == 0)\n\t\t\tps->dtim_count = sdata->vif.bss_conf.dtim_period - 1;\n\t\telse\n\t\t\tps->dtim_count--;\n\t}\n\n\ttim = pos = skb_put(skb, 6);\n\t*pos++ = WLAN_EID_TIM;\n\t*pos++ = 4;\n\t*pos++ = ps->dtim_count;\n\t*pos++ = sdata->vif.bss_conf.dtim_period;\n\n\tif (ps->dtim_count == 0 && !skb_queue_empty(&ps->bc_buf))\n\t\taid0 = 1;\n\n\tps->dtim_bc_mc = aid0 == 1;\n\n\tif (have_bits) {\n\t\t/* Find largest even number N1 so that bits numbered 1 through\n\t\t * (N1 x 8) - 1 in the bitmap are 0 and number N2 so that bits\n\t\t * (N2 + 1) x 8 through 2007 are 0. */\n\t\tn1 = 0;\n\t\tfor (i = 0; i < IEEE80211_MAX_TIM_LEN; i++) {\n\t\t\tif (ps->tim[i]) {\n\t\t\t\tn1 = i & 0xfe;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tn2 = n1;\n\t\tfor (i = IEEE80211_MAX_TIM_LEN - 1; i >= n1; i--) {\n\t\t\tif (ps->tim[i]) {\n\t\t\t\tn2 = i;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t/* Bitmap control */\n\t\t*pos++ = n1 | aid0;\n\t\t/* Part Virt Bitmap */\n\t\tskb_put(skb, n2 - n1);\n\t\tmemcpy(pos, ps->tim + n1, n2 - n1 + 1);\n\n\t\ttim[1] = n2 - n1 + 4;\n\t} else {\n\t\t*pos++ = aid0; /* Bitmap control */\n\t\t*pos++ = 0; /* Part Virt Bitmap */\n\t}\n}\n\nstatic int ieee80211_beacon_add_tim(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t    struct ps_data *ps, struct sk_buff *skb,\n\t\t\t\t    bool is_template)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\n\t/*\n\t * Not very nice, but we want to allow the driver to call\n\t * ieee80211_beacon_get() as a response to the set_tim()\n\t * callback. That, however, is already invoked under the\n\t * sta_lock to guarantee consistent and race-free update\n\t * of the tim bitmap in mac80211 and the driver.\n\t */\n\tif (local->tim_in_locked_section) {\n\t\t__ieee80211_beacon_add_tim(sdata, ps, skb, is_template);\n\t} else {\n\t\tspin_lock_bh(&local->tim_lock);\n\t\t__ieee80211_beacon_add_tim(sdata, ps, skb, is_template);\n\t\tspin_unlock_bh(&local->tim_lock);\n\t}\n\n\treturn 0;\n}\n\nstatic void ieee80211_set_beacon_cntdwn(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t\tstruct beacon_data *beacon)\n{\n\tstruct probe_resp *resp;\n\tu8 *beacon_data;\n\tsize_t beacon_data_len;\n\tint i;\n\tu8 count = beacon->cntdwn_current_counter;\n\n\tswitch (sdata->vif.type) {\n\tcase NL80211_IFTYPE_AP:\n\t\tbeacon_data = beacon->tail;\n\t\tbeacon_data_len = beacon->tail_len;\n\t\tbreak;\n\tcase NL80211_IFTYPE_ADHOC:\n\t\tbeacon_data = beacon->head;\n\t\tbeacon_data_len = beacon->head_len;\n\t\tbreak;\n\tcase NL80211_IFTYPE_MESH_POINT:\n\t\tbeacon_data = beacon->head;\n\t\tbeacon_data_len = beacon->head_len;\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\trcu_read_lock();\n\tfor (i = 0; i < IEEE80211_MAX_CNTDWN_COUNTERS_NUM; ++i) {\n\t\tresp = rcu_dereference(sdata->u.ap.probe_resp);\n\n\t\tif (beacon->cntdwn_counter_offsets[i]) {\n\t\t\tif (WARN_ON_ONCE(beacon->cntdwn_counter_offsets[i] >=\n\t\t\t\t\t beacon_data_len)) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tbeacon_data[beacon->cntdwn_counter_offsets[i]] = count;\n\t\t}\n\n\t\tif (sdata->vif.type == NL80211_IFTYPE_AP && resp)\n\t\t\tresp->data[resp->cntdwn_counter_offsets[i]] = count;\n\t}\n\trcu_read_unlock();\n}\n\nstatic u8 __ieee80211_beacon_update_cntdwn(struct beacon_data *beacon)\n{\n\tbeacon->cntdwn_current_counter--;\n\n\t/* the counter should never reach 0 */\n\tWARN_ON_ONCE(!beacon->cntdwn_current_counter);\n\n\treturn beacon->cntdwn_current_counter;\n}\n\nu8 ieee80211_beacon_update_cntdwn(struct ieee80211_vif *vif)\n{\n\tstruct ieee80211_sub_if_data *sdata = vif_to_sdata(vif);\n\tstruct beacon_data *beacon = NULL;\n\tu8 count = 0;\n\n\trcu_read_lock();\n\n\tif (sdata->vif.type == NL80211_IFTYPE_AP)\n\t\tbeacon = rcu_dereference(sdata->u.ap.beacon);\n\telse if (sdata->vif.type == NL80211_IFTYPE_ADHOC)\n\t\tbeacon = rcu_dereference(sdata->u.ibss.presp);\n\telse if (ieee80211_vif_is_mesh(&sdata->vif))\n\t\tbeacon = rcu_dereference(sdata->u.mesh.beacon);\n\n\tif (!beacon)\n\t\tgoto unlock;\n\n\tcount = __ieee80211_beacon_update_cntdwn(beacon);\n\nunlock:\n\trcu_read_unlock();\n\treturn count;\n}\nEXPORT_SYMBOL(ieee80211_beacon_update_cntdwn);\n\nvoid ieee80211_beacon_set_cntdwn(struct ieee80211_vif *vif, u8 counter)\n{\n\tstruct ieee80211_sub_if_data *sdata = vif_to_sdata(vif);\n\tstruct beacon_data *beacon = NULL;\n\n\trcu_read_lock();\n\n\tif (sdata->vif.type == NL80211_IFTYPE_AP)\n\t\tbeacon = rcu_dereference(sdata->u.ap.beacon);\n\telse if (sdata->vif.type == NL80211_IFTYPE_ADHOC)\n\t\tbeacon = rcu_dereference(sdata->u.ibss.presp);\n\telse if (ieee80211_vif_is_mesh(&sdata->vif))\n\t\tbeacon = rcu_dereference(sdata->u.mesh.beacon);\n\n\tif (!beacon)\n\t\tgoto unlock;\n\n\tif (counter < beacon->cntdwn_current_counter)\n\t\tbeacon->cntdwn_current_counter = counter;\n\nunlock:\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(ieee80211_beacon_set_cntdwn);\n\nbool ieee80211_beacon_cntdwn_is_complete(struct ieee80211_vif *vif)\n{\n\tstruct ieee80211_sub_if_data *sdata = vif_to_sdata(vif);\n\tstruct beacon_data *beacon = NULL;\n\tu8 *beacon_data;\n\tsize_t beacon_data_len;\n\tint ret = false;\n\n\tif (!ieee80211_sdata_running(sdata))\n\t\treturn false;\n\n\trcu_read_lock();\n\tif (vif->type == NL80211_IFTYPE_AP) {\n\t\tstruct ieee80211_if_ap *ap = &sdata->u.ap;\n\n\t\tbeacon = rcu_dereference(ap->beacon);\n\t\tif (WARN_ON(!beacon || !beacon->tail))\n\t\t\tgoto out;\n\t\tbeacon_data = beacon->tail;\n\t\tbeacon_data_len = beacon->tail_len;\n\t} else if (vif->type == NL80211_IFTYPE_ADHOC) {\n\t\tstruct ieee80211_if_ibss *ifibss = &sdata->u.ibss;\n\n\t\tbeacon = rcu_dereference(ifibss->presp);\n\t\tif (!beacon)\n\t\t\tgoto out;\n\n\t\tbeacon_data = beacon->head;\n\t\tbeacon_data_len = beacon->head_len;\n\t} else if (vif->type == NL80211_IFTYPE_MESH_POINT) {\n\t\tstruct ieee80211_if_mesh *ifmsh = &sdata->u.mesh;\n\n\t\tbeacon = rcu_dereference(ifmsh->beacon);\n\t\tif (!beacon)\n\t\t\tgoto out;\n\n\t\tbeacon_data = beacon->head;\n\t\tbeacon_data_len = beacon->head_len;\n\t} else {\n\t\tWARN_ON(1);\n\t\tgoto out;\n\t}\n\n\tif (!beacon->cntdwn_counter_offsets[0])\n\t\tgoto out;\n\n\tif (WARN_ON_ONCE(beacon->cntdwn_counter_offsets[0] > beacon_data_len))\n\t\tgoto out;\n\n\tif (beacon_data[beacon->cntdwn_counter_offsets[0]] == 1)\n\t\tret = true;\n\n out:\n\trcu_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL(ieee80211_beacon_cntdwn_is_complete);\n\nstatic int ieee80211_beacon_protect(struct sk_buff *skb,\n\t\t\t\t    struct ieee80211_local *local,\n\t\t\t\t    struct ieee80211_sub_if_data *sdata)\n{\n\tieee80211_tx_result res;\n\tstruct ieee80211_tx_data tx;\n\tstruct sk_buff *check_skb;\n\n\tmemset(&tx, 0, sizeof(tx));\n\ttx.key = rcu_dereference(sdata->default_beacon_key);\n\tif (!tx.key)\n\t\treturn 0;\n\ttx.local = local;\n\ttx.sdata = sdata;\n\t__skb_queue_head_init(&tx.skbs);\n\t__skb_queue_tail(&tx.skbs, skb);\n\tres = ieee80211_tx_h_encrypt(&tx);\n\tcheck_skb = __skb_dequeue(&tx.skbs);\n\t/* we may crash after this, but it'd be a bug in crypto */\n\tWARN_ON(check_skb != skb);\n\tif (WARN_ON_ONCE(res != TX_CONTINUE))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic struct sk_buff *\n__ieee80211_beacon_get(struct ieee80211_hw *hw,\n\t\t       struct ieee80211_vif *vif,\n\t\t       struct ieee80211_mutable_offsets *offs,\n\t\t       bool is_template)\n{\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\tstruct beacon_data *beacon = NULL;\n\tstruct sk_buff *skb = NULL;\n\tstruct ieee80211_tx_info *info;\n\tstruct ieee80211_sub_if_data *sdata = NULL;\n\tenum nl80211_band band;\n\tstruct ieee80211_tx_rate_control txrc;\n\tstruct ieee80211_chanctx_conf *chanctx_conf;\n\tint csa_off_base = 0;\n\n\trcu_read_lock();\n\n\tsdata = vif_to_sdata(vif);\n\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\n\tif (!ieee80211_sdata_running(sdata) || !chanctx_conf)\n\t\tgoto out;\n\n\tif (offs)\n\t\tmemset(offs, 0, sizeof(*offs));\n\n\tif (sdata->vif.type == NL80211_IFTYPE_AP) {\n\t\tstruct ieee80211_if_ap *ap = &sdata->u.ap;\n\n\t\tbeacon = rcu_dereference(ap->beacon);\n\t\tif (beacon) {\n\t\t\tif (beacon->cntdwn_counter_offsets[0]) {\n\t\t\t\tif (!is_template)\n\t\t\t\t\tieee80211_beacon_update_cntdwn(vif);\n\n\t\t\t\tieee80211_set_beacon_cntdwn(sdata, beacon);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * headroom, head length,\n\t\t\t * tail length and maximum TIM length\n\t\t\t */\n\t\t\tskb = dev_alloc_skb(local->tx_headroom +\n\t\t\t\t\t    beacon->head_len +\n\t\t\t\t\t    beacon->tail_len + 256 +\n\t\t\t\t\t    local->hw.extra_beacon_tailroom);\n\t\t\tif (!skb)\n\t\t\t\tgoto out;\n\n\t\t\tskb_reserve(skb, local->tx_headroom);\n\t\t\tskb_put_data(skb, beacon->head, beacon->head_len);\n\n\t\t\tieee80211_beacon_add_tim(sdata, &ap->ps, skb,\n\t\t\t\t\t\t is_template);\n\n\t\t\tif (offs) {\n\t\t\t\toffs->tim_offset = beacon->head_len;\n\t\t\t\toffs->tim_length = skb->len - beacon->head_len;\n\n\t\t\t\t/* for AP the csa offsets are from tail */\n\t\t\t\tcsa_off_base = skb->len;\n\t\t\t}\n\n\t\t\tif (beacon->tail)\n\t\t\t\tskb_put_data(skb, beacon->tail,\n\t\t\t\t\t     beacon->tail_len);\n\n\t\t\tif (ieee80211_beacon_protect(skb, local, sdata) < 0)\n\t\t\t\tgoto out;\n\t\t} else\n\t\t\tgoto out;\n\t} else if (sdata->vif.type == NL80211_IFTYPE_ADHOC) {\n\t\tstruct ieee80211_if_ibss *ifibss = &sdata->u.ibss;\n\t\tstruct ieee80211_hdr *hdr;\n\n\t\tbeacon = rcu_dereference(ifibss->presp);\n\t\tif (!beacon)\n\t\t\tgoto out;\n\n\t\tif (beacon->cntdwn_counter_offsets[0]) {\n\t\t\tif (!is_template)\n\t\t\t\t__ieee80211_beacon_update_cntdwn(beacon);\n\n\t\t\tieee80211_set_beacon_cntdwn(sdata, beacon);\n\t\t}\n\n\t\tskb = dev_alloc_skb(local->tx_headroom + beacon->head_len +\n\t\t\t\t    local->hw.extra_beacon_tailroom);\n\t\tif (!skb)\n\t\t\tgoto out;\n\t\tskb_reserve(skb, local->tx_headroom);\n\t\tskb_put_data(skb, beacon->head, beacon->head_len);\n\n\t\thdr = (struct ieee80211_hdr *) skb->data;\n\t\thdr->frame_control = cpu_to_le16(IEEE80211_FTYPE_MGMT |\n\t\t\t\t\t\t IEEE80211_STYPE_BEACON);\n\t} else if (ieee80211_vif_is_mesh(&sdata->vif)) {\n\t\tstruct ieee80211_if_mesh *ifmsh = &sdata->u.mesh;\n\n\t\tbeacon = rcu_dereference(ifmsh->beacon);\n\t\tif (!beacon)\n\t\t\tgoto out;\n\n\t\tif (beacon->cntdwn_counter_offsets[0]) {\n\t\t\tif (!is_template)\n\t\t\t\t/* TODO: For mesh csa_counter is in TU, so\n\t\t\t\t * decrementing it by one isn't correct, but\n\t\t\t\t * for now we leave it consistent with overall\n\t\t\t\t * mac80211's behavior.\n\t\t\t\t */\n\t\t\t\t__ieee80211_beacon_update_cntdwn(beacon);\n\n\t\t\tieee80211_set_beacon_cntdwn(sdata, beacon);\n\t\t}\n\n\t\tif (ifmsh->sync_ops)\n\t\t\tifmsh->sync_ops->adjust_tsf(sdata, beacon);\n\n\t\tskb = dev_alloc_skb(local->tx_headroom +\n\t\t\t\t    beacon->head_len +\n\t\t\t\t    256 + /* TIM IE */\n\t\t\t\t    beacon->tail_len +\n\t\t\t\t    local->hw.extra_beacon_tailroom);\n\t\tif (!skb)\n\t\t\tgoto out;\n\t\tskb_reserve(skb, local->tx_headroom);\n\t\tskb_put_data(skb, beacon->head, beacon->head_len);\n\t\tieee80211_beacon_add_tim(sdata, &ifmsh->ps, skb, is_template);\n\n\t\tif (offs) {\n\t\t\toffs->tim_offset = beacon->head_len;\n\t\t\toffs->tim_length = skb->len - beacon->head_len;\n\t\t}\n\n\t\tskb_put_data(skb, beacon->tail, beacon->tail_len);\n\t} else {\n\t\tWARN_ON(1);\n\t\tgoto out;\n\t}\n\n\t/* CSA offsets */\n\tif (offs && beacon) {\n\t\tint i;\n\n\t\tfor (i = 0; i < IEEE80211_MAX_CNTDWN_COUNTERS_NUM; i++) {\n\t\t\tu16 csa_off = beacon->cntdwn_counter_offsets[i];\n\n\t\t\tif (!csa_off)\n\t\t\t\tcontinue;\n\n\t\t\toffs->cntdwn_counter_offs[i] = csa_off_base + csa_off;\n\t\t}\n\t}\n\n\tband = chanctx_conf->def.chan->band;\n\n\tinfo = IEEE80211_SKB_CB(skb);\n\n\tinfo->flags |= IEEE80211_TX_INTFL_DONT_ENCRYPT;\n\tinfo->flags |= IEEE80211_TX_CTL_NO_ACK;\n\tinfo->band = band;\n\n\tmemset(&txrc, 0, sizeof(txrc));\n\ttxrc.hw = hw;\n\ttxrc.sband = local->hw.wiphy->bands[band];\n\ttxrc.bss_conf = &sdata->vif.bss_conf;\n\ttxrc.skb = skb;\n\ttxrc.reported_rate.idx = -1;\n\tif (sdata->beacon_rate_set && sdata->beacon_rateidx_mask[band])\n\t\ttxrc.rate_idx_mask = sdata->beacon_rateidx_mask[band];\n\telse\n\t\ttxrc.rate_idx_mask = sdata->rc_rateidx_mask[band];\n\ttxrc.bss = true;\n\trate_control_get_rate(sdata, NULL, &txrc);\n\n\tinfo->control.vif = vif;\n\n\tinfo->flags |= IEEE80211_TX_CTL_CLEAR_PS_FILT |\n\t\t\tIEEE80211_TX_CTL_ASSIGN_SEQ |\n\t\t\tIEEE80211_TX_CTL_FIRST_FRAGMENT;\n out:\n\trcu_read_unlock();\n\treturn skb;\n\n}\n\nstruct sk_buff *\nieee80211_beacon_get_template(struct ieee80211_hw *hw,\n\t\t\t      struct ieee80211_vif *vif,\n\t\t\t      struct ieee80211_mutable_offsets *offs)\n{\n\treturn __ieee80211_beacon_get(hw, vif, offs, true);\n}\nEXPORT_SYMBOL(ieee80211_beacon_get_template);\n\nstruct sk_buff *ieee80211_beacon_get_tim(struct ieee80211_hw *hw,\n\t\t\t\t\t struct ieee80211_vif *vif,\n\t\t\t\t\t u16 *tim_offset, u16 *tim_length)\n{\n\tstruct ieee80211_mutable_offsets offs = {};\n\tstruct sk_buff *bcn = __ieee80211_beacon_get(hw, vif, &offs, false);\n\tstruct sk_buff *copy;\n\tstruct ieee80211_supported_band *sband;\n\tint shift;\n\n\tif (!bcn)\n\t\treturn bcn;\n\n\tif (tim_offset)\n\t\t*tim_offset = offs.tim_offset;\n\n\tif (tim_length)\n\t\t*tim_length = offs.tim_length;\n\n\tif (ieee80211_hw_check(hw, BEACON_TX_STATUS) ||\n\t    !hw_to_local(hw)->monitors)\n\t\treturn bcn;\n\n\t/* send a copy to monitor interfaces */\n\tcopy = skb_copy(bcn, GFP_ATOMIC);\n\tif (!copy)\n\t\treturn bcn;\n\n\tshift = ieee80211_vif_get_shift(vif);\n\tsband = ieee80211_get_sband(vif_to_sdata(vif));\n\tif (!sband)\n\t\treturn bcn;\n\n\tieee80211_tx_monitor(hw_to_local(hw), copy, sband, 1, shift, false,\n\t\t\t     NULL);\n\n\treturn bcn;\n}\nEXPORT_SYMBOL(ieee80211_beacon_get_tim);\n\nstruct sk_buff *ieee80211_proberesp_get(struct ieee80211_hw *hw,\n\t\t\t\t\tstruct ieee80211_vif *vif)\n{\n\tstruct ieee80211_if_ap *ap = NULL;\n\tstruct sk_buff *skb = NULL;\n\tstruct probe_resp *presp = NULL;\n\tstruct ieee80211_hdr *hdr;\n\tstruct ieee80211_sub_if_data *sdata = vif_to_sdata(vif);\n\n\tif (sdata->vif.type != NL80211_IFTYPE_AP)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\n\tap = &sdata->u.ap;\n\tpresp = rcu_dereference(ap->probe_resp);\n\tif (!presp)\n\t\tgoto out;\n\n\tskb = dev_alloc_skb(presp->len);\n\tif (!skb)\n\t\tgoto out;\n\n\tskb_put_data(skb, presp->data, presp->len);\n\n\thdr = (struct ieee80211_hdr *) skb->data;\n\tmemset(hdr->addr1, 0, sizeof(hdr->addr1));\n\nout:\n\trcu_read_unlock();\n\treturn skb;\n}\nEXPORT_SYMBOL(ieee80211_proberesp_get);\n\nstruct sk_buff *ieee80211_get_fils_discovery_tmpl(struct ieee80211_hw *hw,\n\t\t\t\t\t\t  struct ieee80211_vif *vif)\n{\n\tstruct sk_buff *skb = NULL;\n\tstruct fils_discovery_data *tmpl = NULL;\n\tstruct ieee80211_sub_if_data *sdata = vif_to_sdata(vif);\n\n\tif (sdata->vif.type != NL80211_IFTYPE_AP)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\ttmpl = rcu_dereference(sdata->u.ap.fils_discovery);\n\tif (!tmpl) {\n\t\trcu_read_unlock();\n\t\treturn NULL;\n\t}\n\n\tskb = dev_alloc_skb(sdata->local->hw.extra_tx_headroom + tmpl->len);\n\tif (skb) {\n\t\tskb_reserve(skb, sdata->local->hw.extra_tx_headroom);\n\t\tskb_put_data(skb, tmpl->data, tmpl->len);\n\t}\n\n\trcu_read_unlock();\n\treturn skb;\n}\nEXPORT_SYMBOL(ieee80211_get_fils_discovery_tmpl);\n\nstruct sk_buff *\nieee80211_get_unsol_bcast_probe_resp_tmpl(struct ieee80211_hw *hw,\n\t\t\t\t\t  struct ieee80211_vif *vif)\n{\n\tstruct sk_buff *skb = NULL;\n\tstruct unsol_bcast_probe_resp_data *tmpl = NULL;\n\tstruct ieee80211_sub_if_data *sdata = vif_to_sdata(vif);\n\n\tif (sdata->vif.type != NL80211_IFTYPE_AP)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\ttmpl = rcu_dereference(sdata->u.ap.unsol_bcast_probe_resp);\n\tif (!tmpl) {\n\t\trcu_read_unlock();\n\t\treturn NULL;\n\t}\n\n\tskb = dev_alloc_skb(sdata->local->hw.extra_tx_headroom + tmpl->len);\n\tif (skb) {\n\t\tskb_reserve(skb, sdata->local->hw.extra_tx_headroom);\n\t\tskb_put_data(skb, tmpl->data, tmpl->len);\n\t}\n\n\trcu_read_unlock();\n\treturn skb;\n}\nEXPORT_SYMBOL(ieee80211_get_unsol_bcast_probe_resp_tmpl);\n\nstruct sk_buff *ieee80211_pspoll_get(struct ieee80211_hw *hw,\n\t\t\t\t     struct ieee80211_vif *vif)\n{\n\tstruct ieee80211_sub_if_data *sdata;\n\tstruct ieee80211_if_managed *ifmgd;\n\tstruct ieee80211_pspoll *pspoll;\n\tstruct ieee80211_local *local;\n\tstruct sk_buff *skb;\n\n\tif (WARN_ON(vif->type != NL80211_IFTYPE_STATION))\n\t\treturn NULL;\n\n\tsdata = vif_to_sdata(vif);\n\tifmgd = &sdata->u.mgd;\n\tlocal = sdata->local;\n\n\tskb = dev_alloc_skb(local->hw.extra_tx_headroom + sizeof(*pspoll));\n\tif (!skb)\n\t\treturn NULL;\n\n\tskb_reserve(skb, local->hw.extra_tx_headroom);\n\n\tpspoll = skb_put_zero(skb, sizeof(*pspoll));\n\tpspoll->frame_control = cpu_to_le16(IEEE80211_FTYPE_CTL |\n\t\t\t\t\t    IEEE80211_STYPE_PSPOLL);\n\tpspoll->aid = cpu_to_le16(sdata->vif.bss_conf.aid);\n\n\t/* aid in PS-Poll has its two MSBs each set to 1 */\n\tpspoll->aid |= cpu_to_le16(1 << 15 | 1 << 14);\n\n\tmemcpy(pspoll->bssid, ifmgd->bssid, ETH_ALEN);\n\tmemcpy(pspoll->ta, vif->addr, ETH_ALEN);\n\n\treturn skb;\n}\nEXPORT_SYMBOL(ieee80211_pspoll_get);\n\nstruct sk_buff *ieee80211_nullfunc_get(struct ieee80211_hw *hw,\n\t\t\t\t       struct ieee80211_vif *vif,\n\t\t\t\t       bool qos_ok)\n{\n\tstruct ieee80211_hdr_3addr *nullfunc;\n\tstruct ieee80211_sub_if_data *sdata;\n\tstruct ieee80211_if_managed *ifmgd;\n\tstruct ieee80211_local *local;\n\tstruct sk_buff *skb;\n\tbool qos = false;\n\n\tif (WARN_ON(vif->type != NL80211_IFTYPE_STATION))\n\t\treturn NULL;\n\n\tsdata = vif_to_sdata(vif);\n\tifmgd = &sdata->u.mgd;\n\tlocal = sdata->local;\n\n\tif (qos_ok) {\n\t\tstruct sta_info *sta;\n\n\t\trcu_read_lock();\n\t\tsta = sta_info_get(sdata, ifmgd->bssid);\n\t\tqos = sta && sta->sta.wme;\n\t\trcu_read_unlock();\n\t}\n\n\tskb = dev_alloc_skb(local->hw.extra_tx_headroom +\n\t\t\t    sizeof(*nullfunc) + 2);\n\tif (!skb)\n\t\treturn NULL;\n\n\tskb_reserve(skb, local->hw.extra_tx_headroom);\n\n\tnullfunc = skb_put_zero(skb, sizeof(*nullfunc));\n\tnullfunc->frame_control = cpu_to_le16(IEEE80211_FTYPE_DATA |\n\t\t\t\t\t      IEEE80211_STYPE_NULLFUNC |\n\t\t\t\t\t      IEEE80211_FCTL_TODS);\n\tif (qos) {\n\t\t__le16 qoshdr = cpu_to_le16(7);\n\n\t\tBUILD_BUG_ON((IEEE80211_STYPE_QOS_NULLFUNC |\n\t\t\t      IEEE80211_STYPE_NULLFUNC) !=\n\t\t\t     IEEE80211_STYPE_QOS_NULLFUNC);\n\t\tnullfunc->frame_control |=\n\t\t\tcpu_to_le16(IEEE80211_STYPE_QOS_NULLFUNC);\n\t\tskb->priority = 7;\n\t\tskb_set_queue_mapping(skb, IEEE80211_AC_VO);\n\t\tskb_put_data(skb, &qoshdr, sizeof(qoshdr));\n\t}\n\n\tmemcpy(nullfunc->addr1, ifmgd->bssid, ETH_ALEN);\n\tmemcpy(nullfunc->addr2, vif->addr, ETH_ALEN);\n\tmemcpy(nullfunc->addr3, ifmgd->bssid, ETH_ALEN);\n\n\treturn skb;\n}\nEXPORT_SYMBOL(ieee80211_nullfunc_get);\n\nstruct sk_buff *ieee80211_probereq_get(struct ieee80211_hw *hw,\n\t\t\t\t       const u8 *src_addr,\n\t\t\t\t       const u8 *ssid, size_t ssid_len,\n\t\t\t\t       size_t tailroom)\n{\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\tstruct ieee80211_hdr_3addr *hdr;\n\tstruct sk_buff *skb;\n\tsize_t ie_ssid_len;\n\tu8 *pos;\n\n\tie_ssid_len = 2 + ssid_len;\n\n\tskb = dev_alloc_skb(local->hw.extra_tx_headroom + sizeof(*hdr) +\n\t\t\t    ie_ssid_len + tailroom);\n\tif (!skb)\n\t\treturn NULL;\n\n\tskb_reserve(skb, local->hw.extra_tx_headroom);\n\n\thdr = skb_put_zero(skb, sizeof(*hdr));\n\thdr->frame_control = cpu_to_le16(IEEE80211_FTYPE_MGMT |\n\t\t\t\t\t IEEE80211_STYPE_PROBE_REQ);\n\teth_broadcast_addr(hdr->addr1);\n\tmemcpy(hdr->addr2, src_addr, ETH_ALEN);\n\teth_broadcast_addr(hdr->addr3);\n\n\tpos = skb_put(skb, ie_ssid_len);\n\t*pos++ = WLAN_EID_SSID;\n\t*pos++ = ssid_len;\n\tif (ssid_len)\n\t\tmemcpy(pos, ssid, ssid_len);\n\tpos += ssid_len;\n\n\treturn skb;\n}\nEXPORT_SYMBOL(ieee80211_probereq_get);\n\nvoid ieee80211_rts_get(struct ieee80211_hw *hw, struct ieee80211_vif *vif,\n\t\t       const void *frame, size_t frame_len,\n\t\t       const struct ieee80211_tx_info *frame_txctl,\n\t\t       struct ieee80211_rts *rts)\n{\n\tconst struct ieee80211_hdr *hdr = frame;\n\n\trts->frame_control =\n\t    cpu_to_le16(IEEE80211_FTYPE_CTL | IEEE80211_STYPE_RTS);\n\trts->duration = ieee80211_rts_duration(hw, vif, frame_len,\n\t\t\t\t\t       frame_txctl);\n\tmemcpy(rts->ra, hdr->addr1, sizeof(rts->ra));\n\tmemcpy(rts->ta, hdr->addr2, sizeof(rts->ta));\n}\nEXPORT_SYMBOL(ieee80211_rts_get);\n\nvoid ieee80211_ctstoself_get(struct ieee80211_hw *hw, struct ieee80211_vif *vif,\n\t\t\t     const void *frame, size_t frame_len,\n\t\t\t     const struct ieee80211_tx_info *frame_txctl,\n\t\t\t     struct ieee80211_cts *cts)\n{\n\tconst struct ieee80211_hdr *hdr = frame;\n\n\tcts->frame_control =\n\t    cpu_to_le16(IEEE80211_FTYPE_CTL | IEEE80211_STYPE_CTS);\n\tcts->duration = ieee80211_ctstoself_duration(hw, vif,\n\t\t\t\t\t\t     frame_len, frame_txctl);\n\tmemcpy(cts->ra, hdr->addr1, sizeof(cts->ra));\n}\nEXPORT_SYMBOL(ieee80211_ctstoself_get);\n\nstruct sk_buff *\nieee80211_get_buffered_bc(struct ieee80211_hw *hw,\n\t\t\t  struct ieee80211_vif *vif)\n{\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\tstruct sk_buff *skb = NULL;\n\tstruct ieee80211_tx_data tx;\n\tstruct ieee80211_sub_if_data *sdata;\n\tstruct ps_data *ps;\n\tstruct ieee80211_tx_info *info;\n\tstruct ieee80211_chanctx_conf *chanctx_conf;\n\n\tsdata = vif_to_sdata(vif);\n\n\trcu_read_lock();\n\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\n\tif (!chanctx_conf)\n\t\tgoto out;\n\n\tif (sdata->vif.type == NL80211_IFTYPE_AP) {\n\t\tstruct beacon_data *beacon =\n\t\t\t\trcu_dereference(sdata->u.ap.beacon);\n\n\t\tif (!beacon || !beacon->head)\n\t\t\tgoto out;\n\n\t\tps = &sdata->u.ap.ps;\n\t} else if (ieee80211_vif_is_mesh(&sdata->vif)) {\n\t\tps = &sdata->u.mesh.ps;\n\t} else {\n\t\tgoto out;\n\t}\n\n\tif (ps->dtim_count != 0 || !ps->dtim_bc_mc)\n\t\tgoto out; /* send buffered bc/mc only after DTIM beacon */\n\n\twhile (1) {\n\t\tskb = skb_dequeue(&ps->bc_buf);\n\t\tif (!skb)\n\t\t\tgoto out;\n\t\tlocal->total_ps_buffered--;\n\n\t\tif (!skb_queue_empty(&ps->bc_buf) && skb->len >= 2) {\n\t\t\tstruct ieee80211_hdr *hdr =\n\t\t\t\t(struct ieee80211_hdr *) skb->data;\n\t\t\t/* more buffered multicast/broadcast frames ==> set\n\t\t\t * MoreData flag in IEEE 802.11 header to inform PS\n\t\t\t * STAs */\n\t\t\thdr->frame_control |=\n\t\t\t\tcpu_to_le16(IEEE80211_FCTL_MOREDATA);\n\t\t}\n\n\t\tif (sdata->vif.type == NL80211_IFTYPE_AP)\n\t\t\tsdata = IEEE80211_DEV_TO_SUB_IF(skb->dev);\n\t\tif (!ieee80211_tx_prepare(sdata, &tx, NULL, skb))\n\t\t\tbreak;\n\t\tieee80211_free_txskb(hw, skb);\n\t}\n\n\tinfo = IEEE80211_SKB_CB(skb);\n\n\ttx.flags |= IEEE80211_TX_PS_BUFFERED;\n\tinfo->band = chanctx_conf->def.chan->band;\n\n\tif (invoke_tx_handlers(&tx))\n\t\tskb = NULL;\n out:\n\trcu_read_unlock();\n\n\treturn skb;\n}\nEXPORT_SYMBOL(ieee80211_get_buffered_bc);\n\nint ieee80211_reserve_tid(struct ieee80211_sta *pubsta, u8 tid)\n{\n\tstruct sta_info *sta = container_of(pubsta, struct sta_info, sta);\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct ieee80211_local *local = sdata->local;\n\tint ret;\n\tu32 queues;\n\n\tlockdep_assert_held(&local->sta_mtx);\n\n\t/* only some cases are supported right now */\n\tswitch (sdata->vif.type) {\n\tcase NL80211_IFTYPE_STATION:\n\tcase NL80211_IFTYPE_AP:\n\tcase NL80211_IFTYPE_AP_VLAN:\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t\treturn -EINVAL;\n\t}\n\n\tif (WARN_ON(tid >= IEEE80211_NUM_UPS))\n\t\treturn -EINVAL;\n\n\tif (sta->reserved_tid == tid) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tif (sta->reserved_tid != IEEE80211_TID_UNRESERVED) {\n\t\tsdata_err(sdata, \"TID reservation already active\\n\");\n\t\tret = -EALREADY;\n\t\tgoto out;\n\t}\n\n\tieee80211_stop_vif_queues(sdata->local, sdata,\n\t\t\t\t  IEEE80211_QUEUE_STOP_REASON_RESERVE_TID);\n\n\tsynchronize_net();\n\n\t/* Tear down BA sessions so we stop aggregating on this TID */\n\tif (ieee80211_hw_check(&local->hw, AMPDU_AGGREGATION)) {\n\t\tset_sta_flag(sta, WLAN_STA_BLOCK_BA);\n\t\t__ieee80211_stop_tx_ba_session(sta, tid,\n\t\t\t\t\t       AGG_STOP_LOCAL_REQUEST);\n\t}\n\n\tqueues = BIT(sdata->vif.hw_queue[ieee802_1d_to_ac[tid]]);\n\t__ieee80211_flush_queues(local, sdata, queues, false);\n\n\tsta->reserved_tid = tid;\n\n\tieee80211_wake_vif_queues(local, sdata,\n\t\t\t\t  IEEE80211_QUEUE_STOP_REASON_RESERVE_TID);\n\n\tif (ieee80211_hw_check(&local->hw, AMPDU_AGGREGATION))\n\t\tclear_sta_flag(sta, WLAN_STA_BLOCK_BA);\n\n\tret = 0;\n out:\n\treturn ret;\n}\nEXPORT_SYMBOL(ieee80211_reserve_tid);\n\nvoid ieee80211_unreserve_tid(struct ieee80211_sta *pubsta, u8 tid)\n{\n\tstruct sta_info *sta = container_of(pubsta, struct sta_info, sta);\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\n\tlockdep_assert_held(&sdata->local->sta_mtx);\n\n\t/* only some cases are supported right now */\n\tswitch (sdata->vif.type) {\n\tcase NL80211_IFTYPE_STATION:\n\tcase NL80211_IFTYPE_AP:\n\tcase NL80211_IFTYPE_AP_VLAN:\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t\treturn;\n\t}\n\n\tif (tid != sta->reserved_tid) {\n\t\tsdata_err(sdata, \"TID to unreserve (%d) isn't reserved\\n\", tid);\n\t\treturn;\n\t}\n\n\tsta->reserved_tid = IEEE80211_TID_UNRESERVED;\n}\nEXPORT_SYMBOL(ieee80211_unreserve_tid);\n\nvoid __ieee80211_tx_skb_tid_band(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t struct sk_buff *skb, int tid,\n\t\t\t\t enum nl80211_band band)\n{\n\tint ac = ieee80211_ac_from_tid(tid);\n\n\tskb_reset_mac_header(skb);\n\tskb_set_queue_mapping(skb, ac);\n\tskb->priority = tid;\n\n\tskb->dev = sdata->dev;\n\n\t/*\n\t * The other path calling ieee80211_xmit is from the tasklet,\n\t * and while we can handle concurrent transmissions locking\n\t * requirements are that we do not come into tx with bhs on.\n\t */\n\tlocal_bh_disable();\n\tIEEE80211_SKB_CB(skb)->band = band;\n\tieee80211_xmit(sdata, NULL, skb);\n\tlocal_bh_enable();\n}\n\nint ieee80211_tx_control_port(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t      const u8 *buf, size_t len,\n\t\t\t      const u8 *dest, __be16 proto, bool unencrypted,\n\t\t\t      u64 *cookie)\n{\n\tstruct ieee80211_sub_if_data *sdata = IEEE80211_DEV_TO_SUB_IF(dev);\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sta_info *sta;\n\tstruct sk_buff *skb;\n\tstruct ethhdr *ehdr;\n\tu32 ctrl_flags = 0;\n\tu32 flags = 0;\n\n\t/* Only accept CONTROL_PORT_PROTOCOL configured in CONNECT/ASSOCIATE\n\t * or Pre-Authentication\n\t */\n\tif (proto != sdata->control_port_protocol &&\n\t    proto != cpu_to_be16(ETH_P_PREAUTH))\n\t\treturn -EINVAL;\n\n\tif (proto == sdata->control_port_protocol)\n\t\tctrl_flags |= IEEE80211_TX_CTRL_PORT_CTRL_PROTO |\n\t\t\t      IEEE80211_TX_CTRL_SKIP_MPATH_LOOKUP;\n\n\tif (unencrypted)\n\t\tflags |= IEEE80211_TX_INTFL_DONT_ENCRYPT;\n\n\tif (cookie)\n\t\tctrl_flags |= IEEE80211_TX_CTL_REQ_TX_STATUS;\n\n\tflags |= IEEE80211_TX_INTFL_NL80211_FRAME_TX;\n\n\tskb = dev_alloc_skb(local->hw.extra_tx_headroom +\n\t\t\t    sizeof(struct ethhdr) + len);\n\tif (!skb)\n\t\treturn -ENOMEM;\n\n\tskb_reserve(skb, local->hw.extra_tx_headroom + sizeof(struct ethhdr));\n\n\tskb_put_data(skb, buf, len);\n\n\tehdr = skb_push(skb, sizeof(struct ethhdr));\n\tmemcpy(ehdr->h_dest, dest, ETH_ALEN);\n\tmemcpy(ehdr->h_source, sdata->vif.addr, ETH_ALEN);\n\tehdr->h_proto = proto;\n\n\tskb->dev = dev;\n\tskb->protocol = proto;\n\tskb_reset_network_header(skb);\n\tskb_reset_mac_header(skb);\n\n\t/* update QoS header to prioritize control port frames if possible,\n\t * priorization also happens for control port frames send over\n\t * AF_PACKET\n\t */\n\trcu_read_lock();\n\n\tif (ieee80211_lookup_ra_sta(sdata, skb, &sta) == 0 && !IS_ERR(sta)) {\n\t\tu16 queue = __ieee80211_select_queue(sdata, sta, skb);\n\n\t\tskb_set_queue_mapping(skb, queue);\n\t\tskb_get_hash(skb);\n\t}\n\n\trcu_read_unlock();\n\n\t/* mutex lock is only needed for incrementing the cookie counter */\n\tmutex_lock(&local->mtx);\n\n\tlocal_bh_disable();\n\t__ieee80211_subif_start_xmit(skb, skb->dev, flags, ctrl_flags, cookie);\n\tlocal_bh_enable();\n\n\tmutex_unlock(&local->mtx);\n\n\treturn 0;\n}\n\nint ieee80211_probe_mesh_link(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t      const u8 *buf, size_t len)\n{\n\tstruct ieee80211_sub_if_data *sdata = IEEE80211_DEV_TO_SUB_IF(dev);\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sk_buff *skb;\n\n\tskb = dev_alloc_skb(local->hw.extra_tx_headroom + len +\n\t\t\t    30 + /* header size */\n\t\t\t    18); /* 11s header size */\n\tif (!skb)\n\t\treturn -ENOMEM;\n\n\tskb_reserve(skb, local->hw.extra_tx_headroom);\n\tskb_put_data(skb, buf, len);\n\n\tskb->dev = dev;\n\tskb->protocol = htons(ETH_P_802_3);\n\tskb_reset_network_header(skb);\n\tskb_reset_mac_header(skb);\n\n\tlocal_bh_disable();\n\t__ieee80211_subif_start_xmit(skb, skb->dev, 0,\n\t\t\t\t     IEEE80211_TX_CTRL_SKIP_MPATH_LOOKUP,\n\t\t\t\t     NULL);\n\tlocal_bh_enable();\n\n\treturn 0;\n}\n"}, "28": {"id": 28, "path": "/src/include/linux/interrupt.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n/* interrupt.h */\n#ifndef _LINUX_INTERRUPT_H\n#define _LINUX_INTERRUPT_H\n\n#include <linux/kernel.h>\n#include <linux/bitops.h>\n#include <linux/cpumask.h>\n#include <linux/irqreturn.h>\n#include <linux/irqnr.h>\n#include <linux/hardirq.h>\n#include <linux/irqflags.h>\n#include <linux/hrtimer.h>\n#include <linux/kref.h>\n#include <linux/workqueue.h>\n\n#include <linux/atomic.h>\n#include <asm/ptrace.h>\n#include <asm/irq.h>\n#include <asm/sections.h>\n\n/*\n * These correspond to the IORESOURCE_IRQ_* defines in\n * linux/ioport.h to select the interrupt line behaviour.  When\n * requesting an interrupt without specifying a IRQF_TRIGGER, the\n * setting should be assumed to be \"as already configured\", which\n * may be as per machine or firmware initialisation.\n */\n#define IRQF_TRIGGER_NONE\t0x00000000\n#define IRQF_TRIGGER_RISING\t0x00000001\n#define IRQF_TRIGGER_FALLING\t0x00000002\n#define IRQF_TRIGGER_HIGH\t0x00000004\n#define IRQF_TRIGGER_LOW\t0x00000008\n#define IRQF_TRIGGER_MASK\t(IRQF_TRIGGER_HIGH | IRQF_TRIGGER_LOW | \\\n\t\t\t\t IRQF_TRIGGER_RISING | IRQF_TRIGGER_FALLING)\n#define IRQF_TRIGGER_PROBE\t0x00000010\n\n/*\n * These flags used only by the kernel as part of the\n * irq handling routines.\n *\n * IRQF_SHARED - allow sharing the irq among several devices\n * IRQF_PROBE_SHARED - set by callers when they expect sharing mismatches to occur\n * IRQF_TIMER - Flag to mark this interrupt as timer interrupt\n * IRQF_PERCPU - Interrupt is per cpu\n * IRQF_NOBALANCING - Flag to exclude this interrupt from irq balancing\n * IRQF_IRQPOLL - Interrupt is used for polling (only the interrupt that is\n *                registered first in a shared interrupt is considered for\n *                performance reasons)\n * IRQF_ONESHOT - Interrupt is not reenabled after the hardirq handler finished.\n *                Used by threaded interrupts which need to keep the\n *                irq line disabled until the threaded handler has been run.\n * IRQF_NO_SUSPEND - Do not disable this IRQ during suspend.  Does not guarantee\n *                   that this interrupt will wake the system from a suspended\n *                   state.  See Documentation/power/suspend-and-interrupts.rst\n * IRQF_FORCE_RESUME - Force enable it on resume even if IRQF_NO_SUSPEND is set\n * IRQF_NO_THREAD - Interrupt cannot be threaded\n * IRQF_EARLY_RESUME - Resume IRQ early during syscore instead of at device\n *                resume time.\n * IRQF_COND_SUSPEND - If the IRQ is shared with a NO_SUSPEND user, execute this\n *                interrupt handler after suspending interrupts. For system\n *                wakeup devices users need to implement wakeup detection in\n *                their interrupt handlers.\n */\n#define IRQF_SHARED\t\t0x00000080\n#define IRQF_PROBE_SHARED\t0x00000100\n#define __IRQF_TIMER\t\t0x00000200\n#define IRQF_PERCPU\t\t0x00000400\n#define IRQF_NOBALANCING\t0x00000800\n#define IRQF_IRQPOLL\t\t0x00001000\n#define IRQF_ONESHOT\t\t0x00002000\n#define IRQF_NO_SUSPEND\t\t0x00004000\n#define IRQF_FORCE_RESUME\t0x00008000\n#define IRQF_NO_THREAD\t\t0x00010000\n#define IRQF_EARLY_RESUME\t0x00020000\n#define IRQF_COND_SUSPEND\t0x00040000\n\n#define IRQF_TIMER\t\t(__IRQF_TIMER | IRQF_NO_SUSPEND | IRQF_NO_THREAD)\n\n/*\n * These values can be returned by request_any_context_irq() and\n * describe the context the interrupt will be run in.\n *\n * IRQC_IS_HARDIRQ - interrupt runs in hardirq context\n * IRQC_IS_NESTED - interrupt runs in a nested threaded context\n */\nenum {\n\tIRQC_IS_HARDIRQ\t= 0,\n\tIRQC_IS_NESTED,\n};\n\ntypedef irqreturn_t (*irq_handler_t)(int, void *);\n\n/**\n * struct irqaction - per interrupt action descriptor\n * @handler:\tinterrupt handler function\n * @name:\tname of the device\n * @dev_id:\tcookie to identify the device\n * @percpu_dev_id:\tcookie to identify the device\n * @next:\tpointer to the next irqaction for shared interrupts\n * @irq:\tinterrupt number\n * @flags:\tflags (see IRQF_* above)\n * @thread_fn:\tinterrupt handler function for threaded interrupts\n * @thread:\tthread pointer for threaded interrupts\n * @secondary:\tpointer to secondary irqaction (force threading)\n * @thread_flags:\tflags related to @thread\n * @thread_mask:\tbitmask for keeping track of @thread activity\n * @dir:\tpointer to the proc/irq/NN/name entry\n */\nstruct irqaction {\n\tirq_handler_t\t\thandler;\n\tvoid\t\t\t*dev_id;\n\tvoid __percpu\t\t*percpu_dev_id;\n\tstruct irqaction\t*next;\n\tirq_handler_t\t\tthread_fn;\n\tstruct task_struct\t*thread;\n\tstruct irqaction\t*secondary;\n\tunsigned int\t\tirq;\n\tunsigned int\t\tflags;\n\tunsigned long\t\tthread_flags;\n\tunsigned long\t\tthread_mask;\n\tconst char\t\t*name;\n\tstruct proc_dir_entry\t*dir;\n} ____cacheline_internodealigned_in_smp;\n\nextern irqreturn_t no_action(int cpl, void *dev_id);\n\n/*\n * If a (PCI) device interrupt is not connected we set dev->irq to\n * IRQ_NOTCONNECTED. This causes request_irq() to fail with -ENOTCONN, so we\n * can distingiush that case from other error returns.\n *\n * 0x80000000 is guaranteed to be outside the available range of interrupts\n * and easy to distinguish from other possible incorrect values.\n */\n#define IRQ_NOTCONNECTED\t(1U << 31)\n\nextern int __must_check\nrequest_threaded_irq(unsigned int irq, irq_handler_t handler,\n\t\t     irq_handler_t thread_fn,\n\t\t     unsigned long flags, const char *name, void *dev);\n\n/**\n * request_irq - Add a handler for an interrupt line\n * @irq:\tThe interrupt line to allocate\n * @handler:\tFunction to be called when the IRQ occurs.\n *\t\tPrimary handler for threaded interrupts\n *\t\tIf NULL, the default primary handler is installed\n * @flags:\tHandling flags\n * @name:\tName of the device generating this interrupt\n * @dev:\tA cookie passed to the handler function\n *\n * This call allocates an interrupt and establishes a handler; see\n * the documentation for request_threaded_irq() for details.\n */\nstatic inline int __must_check\nrequest_irq(unsigned int irq, irq_handler_t handler, unsigned long flags,\n\t    const char *name, void *dev)\n{\n\treturn request_threaded_irq(irq, handler, NULL, flags, name, dev);\n}\n\nextern int __must_check\nrequest_any_context_irq(unsigned int irq, irq_handler_t handler,\n\t\t\tunsigned long flags, const char *name, void *dev_id);\n\nextern int __must_check\n__request_percpu_irq(unsigned int irq, irq_handler_t handler,\n\t\t     unsigned long flags, const char *devname,\n\t\t     void __percpu *percpu_dev_id);\n\nextern int __must_check\nrequest_nmi(unsigned int irq, irq_handler_t handler, unsigned long flags,\n\t    const char *name, void *dev);\n\nstatic inline int __must_check\nrequest_percpu_irq(unsigned int irq, irq_handler_t handler,\n\t\t   const char *devname, void __percpu *percpu_dev_id)\n{\n\treturn __request_percpu_irq(irq, handler, 0,\n\t\t\t\t    devname, percpu_dev_id);\n}\n\nextern int __must_check\nrequest_percpu_nmi(unsigned int irq, irq_handler_t handler,\n\t\t   const char *devname, void __percpu *dev);\n\nextern const void *free_irq(unsigned int, void *);\nextern void free_percpu_irq(unsigned int, void __percpu *);\n\nextern const void *free_nmi(unsigned int irq, void *dev_id);\nextern void free_percpu_nmi(unsigned int irq, void __percpu *percpu_dev_id);\n\nstruct device;\n\nextern int __must_check\ndevm_request_threaded_irq(struct device *dev, unsigned int irq,\n\t\t\t  irq_handler_t handler, irq_handler_t thread_fn,\n\t\t\t  unsigned long irqflags, const char *devname,\n\t\t\t  void *dev_id);\n\nstatic inline int __must_check\ndevm_request_irq(struct device *dev, unsigned int irq, irq_handler_t handler,\n\t\t unsigned long irqflags, const char *devname, void *dev_id)\n{\n\treturn devm_request_threaded_irq(dev, irq, handler, NULL, irqflags,\n\t\t\t\t\t devname, dev_id);\n}\n\nextern int __must_check\ndevm_request_any_context_irq(struct device *dev, unsigned int irq,\n\t\t irq_handler_t handler, unsigned long irqflags,\n\t\t const char *devname, void *dev_id);\n\nextern void devm_free_irq(struct device *dev, unsigned int irq, void *dev_id);\n\n/*\n * On lockdep we dont want to enable hardirqs in hardirq\n * context. Use local_irq_enable_in_hardirq() to annotate\n * kernel code that has to do this nevertheless (pretty much\n * the only valid case is for old/broken hardware that is\n * insanely slow).\n *\n * NOTE: in theory this might break fragile code that relies\n * on hardirq delivery - in practice we dont seem to have such\n * places left. So the only effect should be slightly increased\n * irqs-off latencies.\n */\n#ifdef CONFIG_LOCKDEP\n# define local_irq_enable_in_hardirq()\tdo { } while (0)\n#else\n# define local_irq_enable_in_hardirq()\tlocal_irq_enable()\n#endif\n\nbool irq_has_action(unsigned int irq);\nextern void disable_irq_nosync(unsigned int irq);\nextern bool disable_hardirq(unsigned int irq);\nextern void disable_irq(unsigned int irq);\nextern void disable_percpu_irq(unsigned int irq);\nextern void enable_irq(unsigned int irq);\nextern void enable_percpu_irq(unsigned int irq, unsigned int type);\nextern bool irq_percpu_is_enabled(unsigned int irq);\nextern void irq_wake_thread(unsigned int irq, void *dev_id);\n\nextern void disable_nmi_nosync(unsigned int irq);\nextern void disable_percpu_nmi(unsigned int irq);\nextern void enable_nmi(unsigned int irq);\nextern void enable_percpu_nmi(unsigned int irq, unsigned int type);\nextern int prepare_percpu_nmi(unsigned int irq);\nextern void teardown_percpu_nmi(unsigned int irq);\n\nextern int irq_inject_interrupt(unsigned int irq);\n\n/* The following three functions are for the core kernel use only. */\nextern void suspend_device_irqs(void);\nextern void resume_device_irqs(void);\nextern void rearm_wake_irq(unsigned int irq);\n\n/**\n * struct irq_affinity_notify - context for notification of IRQ affinity changes\n * @irq:\t\tInterrupt to which notification applies\n * @kref:\t\tReference count, for internal use\n * @work:\t\tWork item, for internal use\n * @notify:\t\tFunction to be called on change.  This will be\n *\t\t\tcalled in process context.\n * @release:\t\tFunction to be called on release.  This will be\n *\t\t\tcalled in process context.  Once registered, the\n *\t\t\tstructure must only be freed when this function is\n *\t\t\tcalled or later.\n */\nstruct irq_affinity_notify {\n\tunsigned int irq;\n\tstruct kref kref;\n\tstruct work_struct work;\n\tvoid (*notify)(struct irq_affinity_notify *, const cpumask_t *mask);\n\tvoid (*release)(struct kref *ref);\n};\n\n#define\tIRQ_AFFINITY_MAX_SETS  4\n\n/**\n * struct irq_affinity - Description for automatic irq affinity assignements\n * @pre_vectors:\tDon't apply affinity to @pre_vectors at beginning of\n *\t\t\tthe MSI(-X) vector space\n * @post_vectors:\tDon't apply affinity to @post_vectors at end of\n *\t\t\tthe MSI(-X) vector space\n * @nr_sets:\t\tThe number of interrupt sets for which affinity\n *\t\t\tspreading is required\n * @set_size:\t\tArray holding the size of each interrupt set\n * @calc_sets:\t\tCallback for calculating the number and size\n *\t\t\tof interrupt sets\n * @priv:\t\tPrivate data for usage by @calc_sets, usually a\n *\t\t\tpointer to driver/device specific data.\n */\nstruct irq_affinity {\n\tunsigned int\tpre_vectors;\n\tunsigned int\tpost_vectors;\n\tunsigned int\tnr_sets;\n\tunsigned int\tset_size[IRQ_AFFINITY_MAX_SETS];\n\tvoid\t\t(*calc_sets)(struct irq_affinity *, unsigned int nvecs);\n\tvoid\t\t*priv;\n};\n\n/**\n * struct irq_affinity_desc - Interrupt affinity descriptor\n * @mask:\tcpumask to hold the affinity assignment\n * @is_managed: 1 if the interrupt is managed internally\n */\nstruct irq_affinity_desc {\n\tstruct cpumask\tmask;\n\tunsigned int\tis_managed : 1;\n};\n\n#if defined(CONFIG_SMP)\n\nextern cpumask_var_t irq_default_affinity;\n\n/* Internal implementation. Use the helpers below */\nextern int __irq_set_affinity(unsigned int irq, const struct cpumask *cpumask,\n\t\t\t      bool force);\n\n/**\n * irq_set_affinity - Set the irq affinity of a given irq\n * @irq:\tInterrupt to set affinity\n * @cpumask:\tcpumask\n *\n * Fails if cpumask does not contain an online CPU\n */\nstatic inline int\nirq_set_affinity(unsigned int irq, const struct cpumask *cpumask)\n{\n\treturn __irq_set_affinity(irq, cpumask, false);\n}\n\n/**\n * irq_force_affinity - Force the irq affinity of a given irq\n * @irq:\tInterrupt to set affinity\n * @cpumask:\tcpumask\n *\n * Same as irq_set_affinity, but without checking the mask against\n * online cpus.\n *\n * Solely for low level cpu hotplug code, where we need to make per\n * cpu interrupts affine before the cpu becomes online.\n */\nstatic inline int\nirq_force_affinity(unsigned int irq, const struct cpumask *cpumask)\n{\n\treturn __irq_set_affinity(irq, cpumask, true);\n}\n\nextern int irq_can_set_affinity(unsigned int irq);\nextern int irq_select_affinity(unsigned int irq);\n\nextern int irq_set_affinity_hint(unsigned int irq, const struct cpumask *m);\nextern int irq_update_affinity_desc(unsigned int irq,\n\t\t\t\t    struct irq_affinity_desc *affinity);\n\nextern int\nirq_set_affinity_notifier(unsigned int irq, struct irq_affinity_notify *notify);\n\nstruct irq_affinity_desc *\nirq_create_affinity_masks(unsigned int nvec, struct irq_affinity *affd);\n\nunsigned int irq_calc_affinity_vectors(unsigned int minvec, unsigned int maxvec,\n\t\t\t\t       const struct irq_affinity *affd);\n\n#else /* CONFIG_SMP */\n\nstatic inline int irq_set_affinity(unsigned int irq, const struct cpumask *m)\n{\n\treturn -EINVAL;\n}\n\nstatic inline int irq_force_affinity(unsigned int irq, const struct cpumask *cpumask)\n{\n\treturn 0;\n}\n\nstatic inline int irq_can_set_affinity(unsigned int irq)\n{\n\treturn 0;\n}\n\nstatic inline int irq_select_affinity(unsigned int irq)  { return 0; }\n\nstatic inline int irq_set_affinity_hint(unsigned int irq,\n\t\t\t\t\tconst struct cpumask *m)\n{\n\treturn -EINVAL;\n}\n\nstatic inline int irq_update_affinity_desc(unsigned int irq,\n\t\t\t\t\t   struct irq_affinity_desc *affinity)\n{\n\treturn -EINVAL;\n}\n\nstatic inline int\nirq_set_affinity_notifier(unsigned int irq, struct irq_affinity_notify *notify)\n{\n\treturn 0;\n}\n\nstatic inline struct irq_affinity_desc *\nirq_create_affinity_masks(unsigned int nvec, struct irq_affinity *affd)\n{\n\treturn NULL;\n}\n\nstatic inline unsigned int\nirq_calc_affinity_vectors(unsigned int minvec, unsigned int maxvec,\n\t\t\t  const struct irq_affinity *affd)\n{\n\treturn maxvec;\n}\n\n#endif /* CONFIG_SMP */\n\n/*\n * Special lockdep variants of irq disabling/enabling.\n * These should be used for locking constructs that\n * know that a particular irq context which is disabled,\n * and which is the only irq-context user of a lock,\n * that it's safe to take the lock in the irq-disabled\n * section without disabling hardirqs.\n *\n * On !CONFIG_LOCKDEP they are equivalent to the normal\n * irq disable/enable methods.\n */\nstatic inline void disable_irq_nosync_lockdep(unsigned int irq)\n{\n\tdisable_irq_nosync(irq);\n#ifdef CONFIG_LOCKDEP\n\tlocal_irq_disable();\n#endif\n}\n\nstatic inline void disable_irq_nosync_lockdep_irqsave(unsigned int irq, unsigned long *flags)\n{\n\tdisable_irq_nosync(irq);\n#ifdef CONFIG_LOCKDEP\n\tlocal_irq_save(*flags);\n#endif\n}\n\nstatic inline void disable_irq_lockdep(unsigned int irq)\n{\n\tdisable_irq(irq);\n#ifdef CONFIG_LOCKDEP\n\tlocal_irq_disable();\n#endif\n}\n\nstatic inline void enable_irq_lockdep(unsigned int irq)\n{\n#ifdef CONFIG_LOCKDEP\n\tlocal_irq_enable();\n#endif\n\tenable_irq(irq);\n}\n\nstatic inline void enable_irq_lockdep_irqrestore(unsigned int irq, unsigned long *flags)\n{\n#ifdef CONFIG_LOCKDEP\n\tlocal_irq_restore(*flags);\n#endif\n\tenable_irq(irq);\n}\n\n/* IRQ wakeup (PM) control: */\nextern int irq_set_irq_wake(unsigned int irq, unsigned int on);\n\nstatic inline int enable_irq_wake(unsigned int irq)\n{\n\treturn irq_set_irq_wake(irq, 1);\n}\n\nstatic inline int disable_irq_wake(unsigned int irq)\n{\n\treturn irq_set_irq_wake(irq, 0);\n}\n\n/*\n * irq_get_irqchip_state/irq_set_irqchip_state specific flags\n */\nenum irqchip_irq_state {\n\tIRQCHIP_STATE_PENDING,\t\t/* Is interrupt pending? */\n\tIRQCHIP_STATE_ACTIVE,\t\t/* Is interrupt in progress? */\n\tIRQCHIP_STATE_MASKED,\t\t/* Is interrupt masked? */\n\tIRQCHIP_STATE_LINE_LEVEL,\t/* Is IRQ line high? */\n};\n\nextern int irq_get_irqchip_state(unsigned int irq, enum irqchip_irq_state which,\n\t\t\t\t bool *state);\nextern int irq_set_irqchip_state(unsigned int irq, enum irqchip_irq_state which,\n\t\t\t\t bool state);\n\n#ifdef CONFIG_IRQ_FORCED_THREADING\n# ifdef CONFIG_PREEMPT_RT\n#  define force_irqthreads\t(true)\n# else\nextern bool force_irqthreads;\n# endif\n#else\n#define force_irqthreads\t(0)\n#endif\n\n#ifndef local_softirq_pending\n\n#ifndef local_softirq_pending_ref\n#define local_softirq_pending_ref irq_stat.__softirq_pending\n#endif\n\n#define local_softirq_pending()\t(__this_cpu_read(local_softirq_pending_ref))\n#define set_softirq_pending(x)\t(__this_cpu_write(local_softirq_pending_ref, (x)))\n#define or_softirq_pending(x)\t(__this_cpu_or(local_softirq_pending_ref, (x)))\n\n#endif /* local_softirq_pending */\n\n/* Some architectures might implement lazy enabling/disabling of\n * interrupts. In some cases, such as stop_machine, we might want\n * to ensure that after a local_irq_disable(), interrupts have\n * really been disabled in hardware. Such architectures need to\n * implement the following hook.\n */\n#ifndef hard_irq_disable\n#define hard_irq_disable()\tdo { } while(0)\n#endif\n\n/* PLEASE, avoid to allocate new softirqs, if you need not _really_ high\n   frequency threaded job scheduling. For almost all the purposes\n   tasklets are more than enough. F.e. all serial device BHs et\n   al. should be converted to tasklets, not to softirqs.\n */\n\nenum\n{\n\tHI_SOFTIRQ=0,\n\tTIMER_SOFTIRQ,\n\tNET_TX_SOFTIRQ,\n\tNET_RX_SOFTIRQ,\n\tBLOCK_SOFTIRQ,\n\tIRQ_POLL_SOFTIRQ,\n\tTASKLET_SOFTIRQ,\n\tSCHED_SOFTIRQ,\n\tHRTIMER_SOFTIRQ,\n\tRCU_SOFTIRQ,    /* Preferable RCU should always be the last softirq */\n\n\tNR_SOFTIRQS\n};\n\n#define SOFTIRQ_STOP_IDLE_MASK (~(1 << RCU_SOFTIRQ))\n\n/* map softirq index to softirq name. update 'softirq_to_name' in\n * kernel/softirq.c when adding a new softirq.\n */\nextern const char * const softirq_to_name[NR_SOFTIRQS];\n\n/* softirq mask and active fields moved to irq_cpustat_t in\n * asm/hardirq.h to get better cache usage.  KAO\n */\n\nstruct softirq_action\n{\n\tvoid\t(*action)(struct softirq_action *);\n};\n\nasmlinkage void do_softirq(void);\nasmlinkage void __do_softirq(void);\n\nextern void open_softirq(int nr, void (*action)(struct softirq_action *));\nextern void softirq_init(void);\nextern void __raise_softirq_irqoff(unsigned int nr);\n\nextern void raise_softirq_irqoff(unsigned int nr);\nextern void raise_softirq(unsigned int nr);\n\nDECLARE_PER_CPU(struct task_struct *, ksoftirqd);\n\nstatic inline struct task_struct *this_cpu_ksoftirqd(void)\n{\n\treturn this_cpu_read(ksoftirqd);\n}\n\n/* Tasklets --- multithreaded analogue of BHs.\n\n   This API is deprecated. Please consider using threaded IRQs instead:\n   https://lore.kernel.org/lkml/20200716081538.2sivhkj4hcyrusem@linutronix.de\n\n   Main feature differing them of generic softirqs: tasklet\n   is running only on one CPU simultaneously.\n\n   Main feature differing them of BHs: different tasklets\n   may be run simultaneously on different CPUs.\n\n   Properties:\n   * If tasklet_schedule() is called, then tasklet is guaranteed\n     to be executed on some cpu at least once after this.\n   * If the tasklet is already scheduled, but its execution is still not\n     started, it will be executed only once.\n   * If this tasklet is already running on another CPU (or schedule is called\n     from tasklet itself), it is rescheduled for later.\n   * Tasklet is strictly serialized wrt itself, but not\n     wrt another tasklets. If client needs some intertask synchronization,\n     he makes it with spinlocks.\n */\n\nstruct tasklet_struct\n{\n\tstruct tasklet_struct *next;\n\tunsigned long state;\n\tatomic_t count;\n\tbool use_callback;\n\tunion {\n\t\tvoid (*func)(unsigned long data);\n\t\tvoid (*callback)(struct tasklet_struct *t);\n\t};\n\tunsigned long data;\n};\n\n#define DECLARE_TASKLET(name, _callback)\t\t\\\nstruct tasklet_struct name = {\t\t\t\t\\\n\t.count = ATOMIC_INIT(0),\t\t\t\\\n\t.callback = _callback,\t\t\t\t\\\n\t.use_callback = true,\t\t\t\t\\\n}\n\n#define DECLARE_TASKLET_DISABLED(name, _callback)\t\\\nstruct tasklet_struct name = {\t\t\t\t\\\n\t.count = ATOMIC_INIT(1),\t\t\t\\\n\t.callback = _callback,\t\t\t\t\\\n\t.use_callback = true,\t\t\t\t\\\n}\n\n#define from_tasklet(var, callback_tasklet, tasklet_fieldname)\t\\\n\tcontainer_of(callback_tasklet, typeof(*var), tasklet_fieldname)\n\n#define DECLARE_TASKLET_OLD(name, _func)\t\t\\\nstruct tasklet_struct name = {\t\t\t\t\\\n\t.count = ATOMIC_INIT(0),\t\t\t\\\n\t.func = _func,\t\t\t\t\t\\\n}\n\n#define DECLARE_TASKLET_DISABLED_OLD(name, _func)\t\\\nstruct tasklet_struct name = {\t\t\t\t\\\n\t.count = ATOMIC_INIT(1),\t\t\t\\\n\t.func = _func,\t\t\t\t\t\\\n}\n\nenum\n{\n\tTASKLET_STATE_SCHED,\t/* Tasklet is scheduled for execution */\n\tTASKLET_STATE_RUN\t/* Tasklet is running (SMP only) */\n};\n\n#ifdef CONFIG_SMP\nstatic inline int tasklet_trylock(struct tasklet_struct *t)\n{\n\treturn !test_and_set_bit(TASKLET_STATE_RUN, &(t)->state);\n}\n\nstatic inline void tasklet_unlock(struct tasklet_struct *t)\n{\n\tsmp_mb__before_atomic();\n\tclear_bit(TASKLET_STATE_RUN, &(t)->state);\n}\n\nstatic inline void tasklet_unlock_wait(struct tasklet_struct *t)\n{\n\twhile (test_bit(TASKLET_STATE_RUN, &(t)->state)) { barrier(); }\n}\n#else\n#define tasklet_trylock(t) 1\n#define tasklet_unlock_wait(t) do { } while (0)\n#define tasklet_unlock(t) do { } while (0)\n#endif\n\nextern void __tasklet_schedule(struct tasklet_struct *t);\n\nstatic inline void tasklet_schedule(struct tasklet_struct *t)\n{\n\tif (!test_and_set_bit(TASKLET_STATE_SCHED, &t->state))\n\t\t__tasklet_schedule(t);\n}\n\nextern void __tasklet_hi_schedule(struct tasklet_struct *t);\n\nstatic inline void tasklet_hi_schedule(struct tasklet_struct *t)\n{\n\tif (!test_and_set_bit(TASKLET_STATE_SCHED, &t->state))\n\t\t__tasklet_hi_schedule(t);\n}\n\nstatic inline void tasklet_disable_nosync(struct tasklet_struct *t)\n{\n\tatomic_inc(&t->count);\n\tsmp_mb__after_atomic();\n}\n\nstatic inline void tasklet_disable(struct tasklet_struct *t)\n{\n\ttasklet_disable_nosync(t);\n\ttasklet_unlock_wait(t);\n\tsmp_mb();\n}\n\nstatic inline void tasklet_enable(struct tasklet_struct *t)\n{\n\tsmp_mb__before_atomic();\n\tatomic_dec(&t->count);\n}\n\nextern void tasklet_kill(struct tasklet_struct *t);\nextern void tasklet_kill_immediate(struct tasklet_struct *t, unsigned int cpu);\nextern void tasklet_init(struct tasklet_struct *t,\n\t\t\t void (*func)(unsigned long), unsigned long data);\nextern void tasklet_setup(struct tasklet_struct *t,\n\t\t\t  void (*callback)(struct tasklet_struct *));\n\n/*\n * Autoprobing for irqs:\n *\n * probe_irq_on() and probe_irq_off() provide robust primitives\n * for accurate IRQ probing during kernel initialization.  They are\n * reasonably simple to use, are not \"fooled\" by spurious interrupts,\n * and, unlike other attempts at IRQ probing, they do not get hung on\n * stuck interrupts (such as unused PS2 mouse interfaces on ASUS boards).\n *\n * For reasonably foolproof probing, use them as follows:\n *\n * 1. clear and/or mask the device's internal interrupt.\n * 2. sti();\n * 3. irqs = probe_irq_on();      // \"take over\" all unassigned idle IRQs\n * 4. enable the device and cause it to trigger an interrupt.\n * 5. wait for the device to interrupt, using non-intrusive polling or a delay.\n * 6. irq = probe_irq_off(irqs);  // get IRQ number, 0=none, negative=multiple\n * 7. service the device to clear its pending interrupt.\n * 8. loop again if paranoia is required.\n *\n * probe_irq_on() returns a mask of allocated irq's.\n *\n * probe_irq_off() takes the mask as a parameter,\n * and returns the irq number which occurred,\n * or zero if none occurred, or a negative irq number\n * if more than one irq occurred.\n */\n\n#if !defined(CONFIG_GENERIC_IRQ_PROBE) \nstatic inline unsigned long probe_irq_on(void)\n{\n\treturn 0;\n}\nstatic inline int probe_irq_off(unsigned long val)\n{\n\treturn 0;\n}\nstatic inline unsigned int probe_irq_mask(unsigned long val)\n{\n\treturn 0;\n}\n#else\nextern unsigned long probe_irq_on(void);\t/* returns 0 on failure */\nextern int probe_irq_off(unsigned long);\t/* returns 0 or negative on failure */\nextern unsigned int probe_irq_mask(unsigned long);\t/* returns mask of ISA interrupts */\n#endif\n\n#ifdef CONFIG_PROC_FS\n/* Initialize /proc/irq/ */\nextern void init_irq_proc(void);\n#else\nstatic inline void init_irq_proc(void)\n{\n}\n#endif\n\n#ifdef CONFIG_IRQ_TIMINGS\nvoid irq_timings_enable(void);\nvoid irq_timings_disable(void);\nu64 irq_timings_next_event(u64 now);\n#endif\n\nstruct seq_file;\nint show_interrupts(struct seq_file *p, void *v);\nint arch_show_interrupts(struct seq_file *p, int prec);\n\nextern int early_irq_init(void);\nextern int arch_probe_nr_irqs(void);\nextern int arch_early_irq_init(void);\n\n/*\n * We want to know which function is an entrypoint of a hardirq or a softirq.\n */\n#ifndef __irq_entry\n# define __irq_entry\t __section(\".irqentry.text\")\n#endif\n\n#define __softirq_entry  __section(\".softirqentry.text\")\n\n#endif\n"}, "29": {"id": 29, "path": "/src/include/linux/spinlock.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_SPINLOCK_H\n#define __LINUX_SPINLOCK_H\n\n/*\n * include/linux/spinlock.h - generic spinlock/rwlock declarations\n *\n * here's the role of the various spinlock/rwlock related include files:\n *\n * on SMP builds:\n *\n *  asm/spinlock_types.h: contains the arch_spinlock_t/arch_rwlock_t and the\n *                        initializers\n *\n *  linux/spinlock_types.h:\n *                        defines the generic type and initializers\n *\n *  asm/spinlock.h:       contains the arch_spin_*()/etc. lowlevel\n *                        implementations, mostly inline assembly code\n *\n *   (also included on UP-debug builds:)\n *\n *  linux/spinlock_api_smp.h:\n *                        contains the prototypes for the _spin_*() APIs.\n *\n *  linux/spinlock.h:     builds the final spin_*() APIs.\n *\n * on UP builds:\n *\n *  linux/spinlock_type_up.h:\n *                        contains the generic, simplified UP spinlock type.\n *                        (which is an empty structure on non-debug builds)\n *\n *  linux/spinlock_types.h:\n *                        defines the generic type and initializers\n *\n *  linux/spinlock_up.h:\n *                        contains the arch_spin_*()/etc. version of UP\n *                        builds. (which are NOPs on non-debug, non-preempt\n *                        builds)\n *\n *   (included on UP-non-debug builds:)\n *\n *  linux/spinlock_api_up.h:\n *                        builds the _spin_*() APIs.\n *\n *  linux/spinlock.h:     builds the final spin_*() APIs.\n */\n\n#include <linux/typecheck.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n#include <linux/compiler.h>\n#include <linux/irqflags.h>\n#include <linux/thread_info.h>\n#include <linux/kernel.h>\n#include <linux/stringify.h>\n#include <linux/bottom_half.h>\n#include <linux/lockdep.h>\n#include <asm/barrier.h>\n#include <asm/mmiowb.h>\n\n\n/*\n * Must define these before including other files, inline functions need them\n */\n#define LOCK_SECTION_NAME \".text..lock.\"KBUILD_BASENAME\n\n#define LOCK_SECTION_START(extra)               \\\n        \".subsection 1\\n\\t\"                     \\\n        extra                                   \\\n        \".ifndef \" LOCK_SECTION_NAME \"\\n\\t\"     \\\n        LOCK_SECTION_NAME \":\\n\\t\"               \\\n        \".endif\\n\"\n\n#define LOCK_SECTION_END                        \\\n        \".previous\\n\\t\"\n\n#define __lockfunc __section(\".spinlock.text\")\n\n/*\n * Pull the arch_spinlock_t and arch_rwlock_t definitions:\n */\n#include <linux/spinlock_types.h>\n\n/*\n * Pull the arch_spin*() functions/declarations (UP-nondebug doesn't need them):\n */\n#ifdef CONFIG_SMP\n# include <asm/spinlock.h>\n#else\n# include <linux/spinlock_up.h>\n#endif\n\n#ifdef CONFIG_DEBUG_SPINLOCK\n  extern void __raw_spin_lock_init(raw_spinlock_t *lock, const char *name,\n\t\t\t\t   struct lock_class_key *key, short inner);\n\n# define raw_spin_lock_init(lock)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tstatic struct lock_class_key __key;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t__raw_spin_lock_init((lock), #lock, &__key, LD_WAIT_SPIN);\t\\\n} while (0)\n\n#else\n# define raw_spin_lock_init(lock)\t\t\t\t\\\n\tdo { *(lock) = __RAW_SPIN_LOCK_UNLOCKED(lock); } while (0)\n#endif\n\n#define raw_spin_is_locked(lock)\tarch_spin_is_locked(&(lock)->raw_lock)\n\n#ifdef arch_spin_is_contended\n#define raw_spin_is_contended(lock)\tarch_spin_is_contended(&(lock)->raw_lock)\n#else\n#define raw_spin_is_contended(lock)\t(((void)(lock), 0))\n#endif /*arch_spin_is_contended*/\n\n/*\n * smp_mb__after_spinlock() provides the equivalent of a full memory barrier\n * between program-order earlier lock acquisitions and program-order later\n * memory accesses.\n *\n * This guarantees that the following two properties hold:\n *\n *   1) Given the snippet:\n *\n *\t  { X = 0;  Y = 0; }\n *\n *\t  CPU0\t\t\t\tCPU1\n *\n *\t  WRITE_ONCE(X, 1);\t\tWRITE_ONCE(Y, 1);\n *\t  spin_lock(S);\t\t\tsmp_mb();\n *\t  smp_mb__after_spinlock();\tr1 = READ_ONCE(X);\n *\t  r0 = READ_ONCE(Y);\n *\t  spin_unlock(S);\n *\n *      it is forbidden that CPU0 does not observe CPU1's store to Y (r0 = 0)\n *      and CPU1 does not observe CPU0's store to X (r1 = 0); see the comments\n *      preceding the call to smp_mb__after_spinlock() in __schedule() and in\n *      try_to_wake_up().\n *\n *   2) Given the snippet:\n *\n *  { X = 0;  Y = 0; }\n *\n *  CPU0\t\tCPU1\t\t\t\tCPU2\n *\n *  spin_lock(S);\tspin_lock(S);\t\t\tr1 = READ_ONCE(Y);\n *  WRITE_ONCE(X, 1);\tsmp_mb__after_spinlock();\tsmp_rmb();\n *  spin_unlock(S);\tr0 = READ_ONCE(X);\t\tr2 = READ_ONCE(X);\n *\t\t\tWRITE_ONCE(Y, 1);\n *\t\t\tspin_unlock(S);\n *\n *      it is forbidden that CPU0's critical section executes before CPU1's\n *      critical section (r0 = 1), CPU2 observes CPU1's store to Y (r1 = 1)\n *      and CPU2 does not observe CPU0's store to X (r2 = 0); see the comments\n *      preceding the calls to smp_rmb() in try_to_wake_up() for similar\n *      snippets but \"projected\" onto two CPUs.\n *\n * Property (2) upgrades the lock to an RCsc lock.\n *\n * Since most load-store architectures implement ACQUIRE with an smp_mb() after\n * the LL/SC loop, they need no further barriers. Similarly all our TSO\n * architectures imply an smp_mb() for each atomic instruction and equally don't\n * need more.\n *\n * Architectures that can implement ACQUIRE better need to take care.\n */\n#ifndef smp_mb__after_spinlock\n#define smp_mb__after_spinlock()\tdo { } while (0)\n#endif\n\n#ifdef CONFIG_DEBUG_SPINLOCK\n extern void do_raw_spin_lock(raw_spinlock_t *lock) __acquires(lock);\n#define do_raw_spin_lock_flags(lock, flags) do_raw_spin_lock(lock)\n extern int do_raw_spin_trylock(raw_spinlock_t *lock);\n extern void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock);\n#else\nstatic inline void do_raw_spin_lock(raw_spinlock_t *lock) __acquires(lock)\n{\n\t__acquire(lock);\n\tarch_spin_lock(&lock->raw_lock);\n\tmmiowb_spin_lock();\n}\n\n#ifndef arch_spin_lock_flags\n#define arch_spin_lock_flags(lock, flags)\tarch_spin_lock(lock)\n#endif\n\nstatic inline void\ndo_raw_spin_lock_flags(raw_spinlock_t *lock, unsigned long *flags) __acquires(lock)\n{\n\t__acquire(lock);\n\tarch_spin_lock_flags(&lock->raw_lock, *flags);\n\tmmiowb_spin_lock();\n}\n\nstatic inline int do_raw_spin_trylock(raw_spinlock_t *lock)\n{\n\tint ret = arch_spin_trylock(&(lock)->raw_lock);\n\n\tif (ret)\n\t\tmmiowb_spin_lock();\n\n\treturn ret;\n}\n\nstatic inline void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock)\n{\n\tmmiowb_spin_unlock();\n\tarch_spin_unlock(&lock->raw_lock);\n\t__release(lock);\n}\n#endif\n\n/*\n * Define the various spin_lock methods.  Note we define these\n * regardless of whether CONFIG_SMP or CONFIG_PREEMPTION are set. The\n * various methods are defined as nops in the case they are not\n * required.\n */\n#define raw_spin_trylock(lock)\t__cond_lock(lock, _raw_spin_trylock(lock))\n\n#define raw_spin_lock(lock)\t_raw_spin_lock(lock)\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n# define raw_spin_lock_nested(lock, subclass) \\\n\t_raw_spin_lock_nested(lock, subclass)\n\n# define raw_spin_lock_nest_lock(lock, nest_lock)\t\t\t\\\n\t do {\t\t\t\t\t\t\t\t\\\n\t\t typecheck(struct lockdep_map *, &(nest_lock)->dep_map);\\\n\t\t _raw_spin_lock_nest_lock(lock, &(nest_lock)->dep_map);\t\\\n\t } while (0)\n#else\n/*\n * Always evaluate the 'subclass' argument to avoid that the compiler\n * warns about set-but-not-used variables when building with\n * CONFIG_DEBUG_LOCK_ALLOC=n and with W=1.\n */\n# define raw_spin_lock_nested(lock, subclass)\t\t\\\n\t_raw_spin_lock(((void)(subclass), (lock)))\n# define raw_spin_lock_nest_lock(lock, nest_lock)\t_raw_spin_lock(lock)\n#endif\n\n#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)\n\n#define raw_spin_lock_irqsave(lock, flags)\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\\\n\t\tflags = _raw_spin_lock_irqsave(lock);\t\\\n\t} while (0)\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n#define raw_spin_lock_irqsave_nested(lock, flags, subclass)\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\t\t\\\n\t\tflags = _raw_spin_lock_irqsave_nested(lock, subclass);\t\\\n\t} while (0)\n#else\n#define raw_spin_lock_irqsave_nested(lock, flags, subclass)\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\t\t\\\n\t\tflags = _raw_spin_lock_irqsave(lock);\t\t\t\\\n\t} while (0)\n#endif\n\n#else\n\n#define raw_spin_lock_irqsave(lock, flags)\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\\\n\t\t_raw_spin_lock_irqsave(lock, flags);\t\\\n\t} while (0)\n\n#define raw_spin_lock_irqsave_nested(lock, flags, subclass)\t\\\n\traw_spin_lock_irqsave(lock, flags)\n\n#endif\n\n#define raw_spin_lock_irq(lock)\t\t_raw_spin_lock_irq(lock)\n#define raw_spin_lock_bh(lock)\t\t_raw_spin_lock_bh(lock)\n#define raw_spin_unlock(lock)\t\t_raw_spin_unlock(lock)\n#define raw_spin_unlock_irq(lock)\t_raw_spin_unlock_irq(lock)\n\n#define raw_spin_unlock_irqrestore(lock, flags)\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\t\\\n\t\t_raw_spin_unlock_irqrestore(lock, flags);\t\\\n\t} while (0)\n#define raw_spin_unlock_bh(lock)\t_raw_spin_unlock_bh(lock)\n\n#define raw_spin_trylock_bh(lock) \\\n\t__cond_lock(lock, _raw_spin_trylock_bh(lock))\n\n#define raw_spin_trylock_irq(lock) \\\n({ \\\n\tlocal_irq_disable(); \\\n\traw_spin_trylock(lock) ? \\\n\t1 : ({ local_irq_enable(); 0;  }); \\\n})\n\n#define raw_spin_trylock_irqsave(lock, flags) \\\n({ \\\n\tlocal_irq_save(flags); \\\n\traw_spin_trylock(lock) ? \\\n\t1 : ({ local_irq_restore(flags); 0; }); \\\n})\n\n/* Include rwlock functions */\n#include <linux/rwlock.h>\n\n/*\n * Pull the _spin_*()/_read_*()/_write_*() functions/declarations:\n */\n#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)\n# include <linux/spinlock_api_smp.h>\n#else\n# include <linux/spinlock_api_up.h>\n#endif\n\n/*\n * Map the spin_lock functions to the raw variants for PREEMPT_RT=n\n */\n\nstatic __always_inline raw_spinlock_t *spinlock_check(spinlock_t *lock)\n{\n\treturn &lock->rlock;\n}\n\n#ifdef CONFIG_DEBUG_SPINLOCK\n\n# define spin_lock_init(lock)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tstatic struct lock_class_key __key;\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\t__raw_spin_lock_init(spinlock_check(lock),\t\t\\\n\t\t\t     #lock, &__key, LD_WAIT_CONFIG);\t\\\n} while (0)\n\n#else\n\n# define spin_lock_init(_lock)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tspinlock_check(_lock);\t\t\t\\\n\t*(_lock) = __SPIN_LOCK_UNLOCKED(_lock);\t\\\n} while (0)\n\n#endif\n\nstatic __always_inline void spin_lock(spinlock_t *lock)\n{\n\traw_spin_lock(&lock->rlock);\n}\n\nstatic __always_inline void spin_lock_bh(spinlock_t *lock)\n{\n\traw_spin_lock_bh(&lock->rlock);\n}\n\nstatic __always_inline int spin_trylock(spinlock_t *lock)\n{\n\treturn raw_spin_trylock(&lock->rlock);\n}\n\n#define spin_lock_nested(lock, subclass)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_nested(spinlock_check(lock), subclass);\t\\\n} while (0)\n\n#define spin_lock_nest_lock(lock, nest_lock)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_nest_lock(spinlock_check(lock), nest_lock);\t\\\n} while (0)\n\nstatic __always_inline void spin_lock_irq(spinlock_t *lock)\n{\n\traw_spin_lock_irq(&lock->rlock);\n}\n\n#define spin_lock_irqsave(lock, flags)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_irqsave(spinlock_check(lock), flags);\t\\\n} while (0)\n\n#define spin_lock_irqsave_nested(lock, flags, subclass)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \\\n} while (0)\n\nstatic __always_inline void spin_unlock(spinlock_t *lock)\n{\n\traw_spin_unlock(&lock->rlock);\n}\n\nstatic __always_inline void spin_unlock_bh(spinlock_t *lock)\n{\n\traw_spin_unlock_bh(&lock->rlock);\n}\n\nstatic __always_inline void spin_unlock_irq(spinlock_t *lock)\n{\n\traw_spin_unlock_irq(&lock->rlock);\n}\n\nstatic __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)\n{\n\traw_spin_unlock_irqrestore(&lock->rlock, flags);\n}\n\nstatic __always_inline int spin_trylock_bh(spinlock_t *lock)\n{\n\treturn raw_spin_trylock_bh(&lock->rlock);\n}\n\nstatic __always_inline int spin_trylock_irq(spinlock_t *lock)\n{\n\treturn raw_spin_trylock_irq(&lock->rlock);\n}\n\n#define spin_trylock_irqsave(lock, flags)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\traw_spin_trylock_irqsave(spinlock_check(lock), flags); \\\n})\n\n/**\n * spin_is_locked() - Check whether a spinlock is locked.\n * @lock: Pointer to the spinlock.\n *\n * This function is NOT required to provide any memory ordering\n * guarantees; it could be used for debugging purposes or, when\n * additional synchronization is needed, accompanied with other\n * constructs (memory barriers) enforcing the synchronization.\n *\n * Returns: 1 if @lock is locked, 0 otherwise.\n *\n * Note that the function only tells you that the spinlock is\n * seen to be locked, not that it is locked on your CPU.\n *\n * Further, on CONFIG_SMP=n builds with CONFIG_DEBUG_SPINLOCK=n,\n * the return value is always 0 (see include/linux/spinlock_up.h).\n * Therefore you should not rely heavily on the return value.\n */\nstatic __always_inline int spin_is_locked(spinlock_t *lock)\n{\n\treturn raw_spin_is_locked(&lock->rlock);\n}\n\nstatic __always_inline int spin_is_contended(spinlock_t *lock)\n{\n\treturn raw_spin_is_contended(&lock->rlock);\n}\n\n#define assert_spin_locked(lock)\tassert_raw_spin_locked(&(lock)->rlock)\n\n/*\n * Pull the atomic_t declaration:\n * (asm-mips/atomic.h needs above definitions)\n */\n#include <linux/atomic.h>\n/**\n * atomic_dec_and_lock - lock on reaching reference count zero\n * @atomic: the atomic counter\n * @lock: the spinlock in question\n *\n * Decrements @atomic by 1.  If the result is 0, returns true and locks\n * @lock.  Returns false for all other cases.\n */\nextern int _atomic_dec_and_lock(atomic_t *atomic, spinlock_t *lock);\n#define atomic_dec_and_lock(atomic, lock) \\\n\t\t__cond_lock(lock, _atomic_dec_and_lock(atomic, lock))\n\nextern int _atomic_dec_and_lock_irqsave(atomic_t *atomic, spinlock_t *lock,\n\t\t\t\t\tunsigned long *flags);\n#define atomic_dec_and_lock_irqsave(atomic, lock, flags) \\\n\t\t__cond_lock(lock, _atomic_dec_and_lock_irqsave(atomic, lock, &(flags)))\n\nint __alloc_bucket_spinlocks(spinlock_t **locks, unsigned int *lock_mask,\n\t\t\t     size_t max_size, unsigned int cpu_mult,\n\t\t\t     gfp_t gfp, const char *name,\n\t\t\t     struct lock_class_key *key);\n\n#define alloc_bucket_spinlocks(locks, lock_mask, max_size, cpu_mult, gfp)    \\\n\t({\t\t\t\t\t\t\t\t     \\\n\t\tstatic struct lock_class_key key;\t\t\t     \\\n\t\tint ret;\t\t\t\t\t\t     \\\n\t\t\t\t\t\t\t\t\t     \\\n\t\tret = __alloc_bucket_spinlocks(locks, lock_mask, max_size,   \\\n\t\t\t\t\t       cpu_mult, gfp, #locks, &key); \\\n\t\tret;\t\t\t\t\t\t\t     \\\n\t})\n\nvoid free_bucket_spinlocks(spinlock_t *locks);\n\n#endif /* __LINUX_SPINLOCK_H */\n"}}, "reports": [{"events": [{"location": {"col": 2, "file": 1, "line": 7318}, "message": "Calling '__skb_queue_head_init'"}, {"location": {"col": 2, "file": 1, "line": 7318}, "message": "Returning from '__skb_queue_head_init'"}, {"location": {"col": 2, "file": 1, "line": 7325}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 15, "file": 1, "line": 7329}, "message": "Assuming 'i' is >= field 'count'"}, {"location": {"col": 3, "file": 1, "line": 7329}, "message": "Loop condition is false. Execution continues on line 7353"}, {"location": {"col": 7, "file": 1, "line": 7353}, "message": "Assuming 'file' is non-null"}, {"location": {"col": 3, "file": 1, "line": 7353}, "message": "Taking false branch"}, {"location": {"col": 3, "file": 1, "line": 7356}, "message": "Calling '__skb_queue_tail'"}, {"location": {"col": 2, "file": 0, "line": 2067}, "message": "Calling '__skb_queue_before'"}, {"location": {"col": 2, "file": 0, "line": 2034}, "message": "2nd function call argument is an uninitialized value"}, {"location": {"col": 2, "file": 0, "line": 2034}, "message": "2nd function call argument is an uninitialized value"}], "macros": [], "notes": [], "path": "/src/include/linux/skbuff.h", "reportHash": "560da736fc6571dfe7d308f653e7a193", "checkerName": "clang-analyzer-core.CallAndMessage", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 6, "file": 2, "line": 928}, "message": "Assuming 'data' is non-null"}, {"location": {"col": 2, "file": 2, "line": 928}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 2, "line": 970}, "message": "Assuming 'skb' is null"}, {"location": {"col": 2, "file": 2, "line": 970}, "message": "Taking false branch"}, {"location": {"col": 15, "file": 2, "line": 979}, "message": "Assuming 'net' is null"}, {"location": {"col": 25, "file": 3, "line": 102}, "message": "expanded from macro 'WARN_ON_ONCE'"}, {"location": {"col": 2, "file": 2, "line": 979}, "message": "Taking true branch"}, {"location": {"col": 2, "file": 3, "line": 103}, "message": "expanded from macro 'WARN_ON_ONCE'"}, {"location": {"col": 2, "file": 2, "line": 979}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 3, "file": 3, "line": 104}, "message": "expanded from macro 'WARN_ON_ONCE'"}, {"location": {"col": 2, "file": 4, "line": 87}, "message": "expanded from macro '__WARN_FLAGS'"}, {"location": {"col": 34, "file": 5, "line": 53}, "message": "expanded from macro 'instrumentation_begin'"}, {"location": {"col": 2, "file": 2, "line": 979}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 3, "file": 3, "line": 104}, "message": "expanded from macro 'WARN_ON_ONCE'"}, {"location": {"col": 2, "file": 4, "line": 88}, "message": "expanded from macro '__WARN_FLAGS'"}, {"location": {"col": 37, "file": 4, "line": 34}, "message": "expanded from macro '_BUG_FLAGS'"}, {"location": {"col": 2, "file": 2, "line": 979}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 3, "file": 3, "line": 104}, "message": "expanded from macro 'WARN_ON_ONCE'"}, {"location": {"col": 2, "file": 4, "line": 90}, "message": "expanded from macro '__WARN_FLAGS'"}, {"location": {"col": 33, "file": 5, "line": 54}, "message": "expanded from macro 'instrumentation_end'"}, {"location": {"col": 2, "file": 2, "line": 979}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 3, "file": 3, "line": 104}, "message": "expanded from macro 'WARN_ON_ONCE'"}, {"location": {"col": 33, "file": 4, "line": 85}, "message": "expanded from macro '__WARN_FLAGS'"}, {"location": {"col": 6, "file": 2, "line": 980}, "message": "'net' is null"}, {"location": {"col": 2, "file": 2, "line": 980}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 2, "line": 1019}, "message": "Assuming the condition is true"}, {"location": {"col": 2, "file": 2, "line": 1019}, "message": "Taking true branch"}, {"location": {"col": 32, "file": 2, "line": 1021}, "message": "Passing null pointer value via 1st parameter 'skb'"}, {"location": {"col": 24, "file": 2, "line": 1021}, "message": "Calling 'eth_hdr'"}, {"location": {"col": 41, "file": 6, "line": 24}, "message": "Passing null pointer value via 1st parameter 'skb'"}, {"location": {"col": 26, "file": 6, "line": 24}, "message": "Calling 'skb_mac_header'"}, {"location": {"col": 9, "file": 0, "line": 2551}, "message": "Access to field 'head' results in a dereference of a null pointer (loaded from variable 'skb')"}, {"location": {"col": 9, "file": 0, "line": 2551}, "message": "Access to field 'head' results in a dereference of a null pointer (loaded from variable 'skb')"}], "macros": [], "notes": [], "path": "/src/include/linux/skbuff.h", "reportHash": "05235ceccaf9031cff3e1eaf2e0f0eed", "checkerName": "clang-analyzer-core.NullDereference", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 2, "file": 7, "line": 61}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 30, "file": 7, "line": 55}, "message": "expanded from macro '__WRITE_ONCE'"}, {"location": {"col": 6, "file": 8, "line": 1677}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 8, "line": 1677}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 8, "line": 1684}, "message": "Assuming field 'optlen' is 0"}, {"location": {"col": 2, "file": 8, "line": 1684}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 8, "line": 1692}, "message": "Assuming 'oif' is not equal to 0"}, {"location": {"col": 11, "file": 8, "line": 1692}, "message": "Left side of '&&' is false"}, {"location": {"col": 7, "file": 8, "line": 1696}, "message": "Assuming field 'sysctl_fwmark_reflect' is 0"}, {"location": {"col": 3, "file": 9, "line": 380}, "message": "expanded from macro 'IP4_REPLY_MARK'"}, {"location": {"col": 7, "file": 8, "line": 1696}, "message": "'?' condition is false"}, {"location": {"col": 3, "file": 9, "line": 380}, "message": "expanded from macro 'IP4_REPLY_MARK'"}, {"location": {"col": 7, "file": 8, "line": 1696}, "message": "'?' condition is false"}, {"location": {"col": 2, "file": 9, "line": 380}, "message": "expanded from macro 'IP4_REPLY_MARK'"}, {"location": {"col": 2, "file": 8, "line": 1705}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 8, "line": 1716}, "message": "Assuming 'err' is 0"}, {"location": {"col": 40, "file": 10, "line": 78}, "message": "expanded from macro 'unlikely'"}, {"location": {"col": 2, "file": 8, "line": 1716}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 8, "line": 1722}, "message": "Assuming 'nskb' is non-null"}, {"location": {"col": 2, "file": 8, "line": 1722}, "message": "Taking true branch"}, {"location": {"col": 7, "file": 8, "line": 1723}, "message": "Assuming field 'csumoffset' is < 0"}, {"location": {"col": 3, "file": 8, "line": 1723}, "message": "Taking false branch"}, {"location": {"col": 3, "file": 8, "line": 1728}, "message": "Calling 'ip_push_pending_frames'"}, {"location": {"col": 8, "file": 8, "line": 1583}, "message": "Calling 'ip_finish_skb'"}, {"location": {"col": 9, "file": 9, "line": 239}, "message": "Calling '__ip_make_skb'"}, {"location": {"col": 8, "file": 8, "line": 1486}, "message": "Calling '__skb_dequeue'"}, {"location": {"col": 6, "file": 0, "line": 2099}, "message": "'skb' is non-null"}, {"location": {"col": 2, "file": 0, "line": 2099}, "message": "Taking true branch"}, {"location": {"col": 3, "file": 0, "line": 2100}, "message": "Calling '__skb_unlink'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Left side of '||' is true"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 28, "file": 11, "line": 283}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 11, "line": 300}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 298}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 7, "line": 61}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 35, "file": 7, "line": 53}, "message": "expanded from macro '__WRITE_ONCE'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 33, "file": 7, "line": 58}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 0, "line": 2083}, "message": "Null pointer value stored to field 'next'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Left side of '||' is true"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 38, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 11, "line": 300}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 298}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 7, "line": 61}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 35, "file": 7, "line": 53}, "message": "expanded from macro '__WRITE_ONCE'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 33, "file": 7, "line": 58}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 0, "line": 2085}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2085}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2085}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2085}, "message": "Left side of '||' is true"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 38, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 0, "line": 2085}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 11, "line": 300}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 2085}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 298}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 2085}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 7, "line": 61}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 35, "file": 7, "line": 53}, "message": "expanded from macro '__WRITE_ONCE'"}, {"location": {"col": 2, "file": 0, "line": 2085}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 33, "file": 7, "line": 58}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 3, "file": 0, "line": 2100}, "message": "Returning from '__skb_unlink'"}, {"location": {"col": 8, "file": 8, "line": 1486}, "message": "Returning from '__skb_dequeue'"}, {"location": {"col": 7, "file": 8, "line": 1487}, "message": "'skb' is non-null"}, {"location": {"col": 2, "file": 8, "line": 1487}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 8, "line": 1492}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 8, "line": 1492}, "message": "Taking false branch"}, {"location": {"col": 20, "file": 8, "line": 1494}, "message": "Calling '__skb_dequeue'"}, {"location": {"col": 6, "file": 0, "line": 2099}, "message": "'skb' is non-null"}, {"location": {"col": 2, "file": 0, "line": 2099}, "message": "Taking true branch"}, {"location": {"col": 3, "file": 0, "line": 2100}, "message": "Calling '__skb_unlink'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Left side of '||' is true"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 28, "file": 11, "line": 283}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 11, "line": 300}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 298}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 7, "line": 61}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 35, "file": 7, "line": 53}, "message": "expanded from macro '__WRITE_ONCE'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 33, "file": 7, "line": 58}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 0, "line": 2081}, "message": "Null pointer value stored to 'next'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Left side of '||' is true"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 38, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 11, "line": 300}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 298}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Dereference of null pointer"}, {"location": {"col": 2, "file": 7, "line": 61}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 30, "file": 7, "line": 55}, "message": "expanded from macro '__WRITE_ONCE'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Dereference of null pointer"}], "macros": [], "notes": [], "path": "/src/include/linux/skbuff.h", "reportHash": "2bf3183453622ac3190d6bf26a2aa24b", "checkerName": "clang-analyzer-core.NullDereference", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 6, "file": 12, "line": 213}, "message": "Assuming 'gc_in_progress' is false"}, {"location": {"col": 2, "file": 12, "line": 213}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 12, "line": 232}, "message": "Left side of '&&' is false"}, {"location": {"col": 13, "file": 13, "line": 715}, "message": "expanded from macro 'list_for_each_entry_safe'"}, {"location": {"col": 2, "file": 13, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 13, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 61, "file": 14, "line": 709}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 2, "file": 12, "line": 232}, "message": "Taking false branch"}, {"location": {"col": 13, "file": 13, "line": 715}, "message": "expanded from macro 'list_for_each_entry_safe'"}, {"location": {"col": 2, "file": 13, "line": 522}, "message": "expanded from macro 'list_first_entry'"}, {"location": {"col": 2, "file": 13, "line": 511}, "message": "expanded from macro 'list_entry'"}, {"location": {"col": 2, "file": 0, "line": 2034}, "message": "2nd function call argument is an uninitialized value"}], "macros": [], "notes": [], "path": "/src/include/linux/skbuff.h", "reportHash": "560da736fc6571dfe7d308f653e7a193", "checkerName": "clang-analyzer-core.CallAndMessage", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 2, "file": 7, "line": 61}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 30, "file": 7, "line": 55}, "message": "expanded from macro '__WRITE_ONCE'"}, {"location": {"col": 8, "file": 15, "line": 1838}, "message": "Calling '__skb_dequeue'"}, {"location": {"col": 6, "file": 0, "line": 2099}, "message": "Assuming 'skb' is non-null"}, {"location": {"col": 2, "file": 0, "line": 2099}, "message": "Taking true branch"}, {"location": {"col": 3, "file": 0, "line": 2100}, "message": "Calling '__skb_unlink'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Left side of '||' is true"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 28, "file": 11, "line": 283}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 11, "line": 300}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 298}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 7, "line": 61}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 35, "file": 7, "line": 53}, "message": "expanded from macro '__WRITE_ONCE'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 33, "file": 7, "line": 58}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 0, "line": 2083}, "message": "Null pointer value stored to field 'next'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Left side of '||' is true"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 38, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 11, "line": 300}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 298}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 7, "line": 61}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 35, "file": 7, "line": 53}, "message": "expanded from macro '__WRITE_ONCE'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 33, "file": 7, "line": 58}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 0, "line": 2085}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2085}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2085}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2085}, "message": "Left side of '||' is true"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 38, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 0, "line": 2085}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 11, "line": 300}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 2085}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 298}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 2085}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 7, "line": 61}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 35, "file": 7, "line": 53}, "message": "expanded from macro '__WRITE_ONCE'"}, {"location": {"col": 2, "file": 0, "line": 2085}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 33, "file": 7, "line": 58}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 3, "file": 0, "line": 2100}, "message": "Returning from '__skb_unlink'"}, {"location": {"col": 8, "file": 15, "line": 1838}, "message": "Returning from '__skb_dequeue'"}, {"location": {"col": 7, "file": 15, "line": 1839}, "message": "'skb' is non-null"}, {"location": {"col": 2, "file": 15, "line": 1839}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 15, "line": 1844}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 15, "line": 1844}, "message": "Taking false branch"}, {"location": {"col": 20, "file": 15, "line": 1846}, "message": "Calling '__skb_dequeue'"}, {"location": {"col": 6, "file": 0, "line": 2099}, "message": "'skb' is non-null"}, {"location": {"col": 2, "file": 0, "line": 2099}, "message": "Taking true branch"}, {"location": {"col": 3, "file": 0, "line": 2100}, "message": "Calling '__skb_unlink'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Left side of '||' is true"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 28, "file": 11, "line": 283}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 11, "line": 300}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 298}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 7, "line": 61}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 35, "file": 7, "line": 53}, "message": "expanded from macro '__WRITE_ONCE'"}, {"location": {"col": 2, "file": 0, "line": 2080}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 33, "file": 7, "line": 58}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 0, "line": 2081}, "message": "Null pointer value stored to 'next'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Left side of '||' is false"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Left side of '||' is true"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 38, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 11, "line": 300}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 7, "line": 60}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 2, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 298}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Dereference of null pointer"}, {"location": {"col": 2, "file": 7, "line": 61}, "message": "expanded from macro 'WRITE_ONCE'"}, {"location": {"col": 30, "file": 7, "line": 55}, "message": "expanded from macro '__WRITE_ONCE'"}, {"location": {"col": 2, "file": 0, "line": 2084}, "message": "Dereference of null pointer"}], "macros": [], "notes": [], "path": "/src/include/linux/skbuff.h", "reportHash": "2bf3183453622ac3190d6bf26a2aa24b", "checkerName": "clang-analyzer-core.NullDereference", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 2, "file": 16, "line": 770}, "message": "Taking false branch"}, {"location": {"col": 16, "file": 17, "line": 206}, "message": "expanded from macro 'DECLARE_SOCKADDR'"}, {"location": {"col": 2, "file": 18, "line": 19}, "message": "expanded from macro '__sockaddr_check_size'"}, {"location": {"col": 2, "file": 19, "line": 50}, "message": "expanded from macro 'BUILD_BUG_ON'"}, {"location": {"col": 9, "file": 0, "line": 2518}, "message": "Access to field 'head' results in a dereference of a null pointer (loaded from variable 'skb')"}], "macros": [], "notes": [], "path": "/src/include/linux/skbuff.h", "reportHash": "7908d2b22dfeef12e683e7b6ae340aa3", "checkerName": "clang-analyzer-core.NullDereference", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 2, "file": 20, "line": 13171}, "message": "'msg' initialized to a null pointer value"}, {"location": {"col": 6, "file": 20, "line": 13175}, "message": "Assuming field 'iftype' is equal to NL80211_IFTYPE_NAN"}, {"location": {"col": 2, "file": 20, "line": 13175}, "message": "Taking false branch"}, {"location": {"col": 7, "file": 20, "line": 13178}, "message": "Calling 'wdev_running'"}, {"location": {"col": 6, "file": 21, "line": 5462}, "message": "Assuming field 'netdev' is null"}, {"location": {"col": 2, "file": 21, "line": 5462}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 21, "line": 5464}, "message": "Returning without writing to 'wdev->is_running', which participates in a condition later"}, {"location": {"col": 2, "file": 21, "line": 5464}, "message": "Returning value, which participates in a condition later"}, {"location": {"col": 7, "file": 20, "line": 13178}, "message": "Returning from 'wdev_running'"}, {"location": {"col": 6, "file": 20, "line": 13178}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 20, "line": 13178}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 20, "line": 13181}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 20, "line": 13181}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 20, "line": 13188}, "message": "Assuming 'err' is 0"}, {"location": {"col": 2, "file": 20, "line": 13188}, "message": "Taking false branch"}, {"location": {"col": 9, "file": 20, "line": 13191}, "message": "Calling 'kzalloc'"}, {"location": {"col": 9, "file": 22, "line": 684}, "message": "Calling 'kmalloc'"}, {"location": {"col": 2, "file": 22, "line": 542}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 22, "line": 559}, "message": "Returning pointer, which participates in a condition later"}, {"location": {"col": 9, "file": 22, "line": 684}, "message": "Returning from 'kmalloc'"}, {"location": {"col": 2, "file": 22, "line": 684}, "message": "Returning pointer, which participates in a condition later"}, {"location": {"col": 9, "file": 20, "line": 13191}, "message": "Returning from 'kzalloc'"}, {"location": {"col": 6, "file": 20, "line": 13192}, "message": "Assuming 'func' is non-null"}, {"location": {"col": 2, "file": 20, "line": 13192}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 20, "line": 13197}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 20, "line": 13197}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 20, "line": 13205}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 20, "line": 13205}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 20, "line": 13216}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 20, "line": 13216}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 20, "line": 13229}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 20, "line": 13229}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 20, "line": 13232}, "message": "Control jumps to 'case NL80211_NAN_FUNC_SUBSCRIBE:'  at line 13250"}, {"location": {"col": 3, "file": 20, "line": 13253}, "message": "Execution continues on line 13279"}, {"location": {"col": 6, "file": 20, "line": 13279}, "message": "Assuming the condition is true"}, {"location": {"col": 2, "file": 20, "line": 13279}, "message": "Taking true branch"}, {"location": {"col": 7, "file": 20, "line": 13287}, "message": "Assuming 'err' is not equal to 0"}, {"location": {"col": 3, "file": 20, "line": 13287}, "message": "Taking true branch"}, {"location": {"col": 4, "file": 20, "line": 13288}, "message": "Control jumps to line 13373"}, {"location": {"col": 6, "file": 20, "line": 13373}, "message": "Assuming 'err' is >= 0"}, {"location": {"col": 2, "file": 20, "line": 13373}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 20, "line": 13380}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 20, "line": 13380}, "message": "Taking false branch"}, {"location": {"col": 36, "file": 20, "line": 13384}, "message": "Passing null pointer value via 1st parameter 'skb'"}, {"location": {"col": 14, "file": 20, "line": 13384}, "message": "Calling 'nla_nest_start_noflag'"}, {"location": {"col": 59, "file": 23, "line": 1765}, "message": "Passing null pointer value via 1st parameter 'skb'"}, {"location": {"col": 42, "file": 23, "line": 1765}, "message": "Calling 'skb_tail_pointer'"}, {"location": {"col": 9, "file": 0, "line": 2211}, "message": "Access to field 'head' results in a dereference of a null pointer (loaded from variable 'skb')"}, {"location": {"col": 9, "file": 0, "line": 2211}, "message": "Access to field 'head' results in a dereference of a null pointer (loaded from variable 'skb')"}], "macros": [], "notes": [], "path": "/src/include/linux/skbuff.h", "reportHash": "f155f9295bf1e1677f984e579e7a8b8a", "checkerName": "clang-analyzer-core.NullDereference", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 8, "file": 24, "line": 306}, "message": "Left side of '&&' is false"}, {"location": {"col": 61, "file": 14, "line": 709}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 8, "file": 24, "line": 306}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 14, "line": 709}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 37, "file": 19, "line": 39}, "message": "expanded from macro 'BUILD_BUG_ON_MSG'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 11, "line": 300}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 8, "file": 24, "line": 306}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 14, "line": 709}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 37, "file": 19, "line": 39}, "message": "expanded from macro 'BUILD_BUG_ON_MSG'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 298}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 6, "file": 24, "line": 308}, "message": "Assuming field 'dead' is false"}, {"location": {"col": 2, "file": 24, "line": 308}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 24, "line": 312}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 24, "line": 312}, "message": "Taking false branch"}, {"location": {"col": 11, "file": 24, "line": 314}, "message": "Assuming the condition is true"}, {"location": {"col": 7, "file": 24, "line": 314}, "message": "Taking true branch"}, {"location": {"col": 3, "file": 24, "line": 315}, "message": "Calling 'ieee80211_sta_ps_deliver_poll_response'"}, {"location": {"col": 6, "file": 24, "line": 1777}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 24, "line": 1777}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 24, "line": 1780}, "message": "Calling 'ieee80211_sta_ps_deliver_response'"}, {"location": {"col": 2, "file": 24, "line": 1588}, "message": "Calling '__skb_queue_head_init'"}, {"location": {"col": 2, "file": 24, "line": 1588}, "message": "Returning from '__skb_queue_head_init'"}, {"location": {"col": 2, "file": 24, "line": 1590}, "message": "Calling 'ieee80211_sta_ps_get_frames'"}, {"location": {"col": 2, "file": 24, "line": 1529}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 7, "file": 24, "line": 1532}, "message": "Assuming the condition is false"}, {"location": {"col": 3, "file": 24, "line": 1532}, "message": "Taking false branch"}, {"location": {"col": 3, "file": 24, "line": 1540}, "message": "Taking true branch"}, {"location": {"col": 7, "file": 24, "line": 1546}, "message": "Assuming the condition is true"}, {"location": {"col": 3, "file": 24, "line": 1546}, "message": "Taking true branch"}, {"location": {"col": 4, "file": 24, "line": 1549}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 9, "file": 24, "line": 1551}, "message": "Assuming 'skb' is non-null"}, {"location": {"col": 5, "file": 24, "line": 1551}, "message": "Taking false branch"}, {"location": {"col": 10, "file": 24, "line": 1557}, "message": "'skb' is non-null"}, {"location": {"col": 5, "file": 24, "line": 1557}, "message": "Taking false branch"}, {"location": {"col": 5, "file": 24, "line": 1560}, "message": "Calling '__skb_queue_tail'"}, {"location": {"col": 2, "file": 0, "line": 2067}, "message": "Calling '__skb_queue_before'"}, {"location": {"col": 2, "file": 0, "line": 2034}, "message": "2nd function call argument is an uninitialized value"}, {"location": {"col": 2, "file": 0, "line": 2034}, "message": "2nd function call argument is an uninitialized value"}], "macros": [], "notes": [], "path": "/src/include/linux/skbuff.h", "reportHash": "560da736fc6571dfe7d308f653e7a193", "checkerName": "clang-analyzer-core.CallAndMessage", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 14, "file": 25, "line": 3885}, "message": "Assuming 'pubsta' is non-null"}, {"location": {"col": 25, "file": 3, "line": 119}, "message": "expanded from macro 'WARN_ON'"}, {"location": {"col": 14, "file": 25, "line": 3885}, "message": "Left side of '||' is false"}, {"location": {"col": 25, "file": 25, "line": 3885}, "message": "Assuming 'tid' is < 16"}, {"location": {"col": 25, "file": 3, "line": 119}, "message": "expanded from macro 'WARN_ON'"}, {"location": {"col": 6, "file": 25, "line": 3885}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 3, "line": 120}, "message": "expanded from macro 'WARN_ON'"}, {"location": {"col": 2, "file": 25, "line": 3885}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 25, "line": 3888}, "message": "Calling '__skb_queue_head_init'"}, {"location": {"col": 2, "file": 25, "line": 3888}, "message": "Returning from '__skb_queue_head_init'"}, {"location": {"col": 8, "file": 25, "line": 3890}, "message": "Left side of '&&' is false"}, {"location": {"col": 61, "file": 14, "line": 709}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 8, "file": 25, "line": 3890}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 14, "line": 709}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 37, "file": 19, "line": 39}, "message": "expanded from macro 'BUILD_BUG_ON_MSG'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 11, "line": 300}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 8, "file": 25, "line": 3890}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 14, "line": 709}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 37, "file": 19, "line": 39}, "message": "expanded from macro 'BUILD_BUG_ON_MSG'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 298}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 15, "file": 25, "line": 3897}, "message": "Left side of '||' is false"}, {"location": {"col": 28, "file": 26, "line": 571}, "message": "expanded from macro 'rcu_dereference'"}, {"location": {"col": 2, "file": 26, "line": 513}, "message": "expanded from macro 'rcu_dereference_check'"}, {"location": {"col": 48, "file": 26, "line": 374}, "message": "expanded from macro '__rcu_dereference_check'"}, {"location": {"col": 2, "file": 7, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 15, "file": 25, "line": 3897}, "message": "Left side of '||' is false"}, {"location": {"col": 28, "file": 26, "line": 571}, "message": "expanded from macro 'rcu_dereference'"}, {"location": {"col": 2, "file": 26, "line": 513}, "message": "expanded from macro 'rcu_dereference_check'"}, {"location": {"col": 48, "file": 26, "line": 374}, "message": "expanded from macro '__rcu_dereference_check'"}, {"location": {"col": 2, "file": 7, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 15, "file": 25, "line": 3897}, "message": "Left side of '||' is false"}, {"location": {"col": 28, "file": 26, "line": 571}, "message": "expanded from macro 'rcu_dereference'"}, {"location": {"col": 2, "file": 26, "line": 513}, "message": "expanded from macro 'rcu_dereference_check'"}, {"location": {"col": 48, "file": 26, "line": 374}, "message": "expanded from macro '__rcu_dereference_check'"}, {"location": {"col": 2, "file": 7, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 11, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 15, "file": 25, "line": 3897}, "message": "Left side of '||' is true"}, {"location": {"col": 28, "file": 26, "line": 571}, "message": "expanded from macro 'rcu_dereference'"}, {"location": {"col": 2, "file": 26, "line": 513}, "message": "expanded from macro 'rcu_dereference_check'"}, {"location": {"col": 48, "file": 26, "line": 374}, "message": "expanded from macro '__rcu_dereference_check'"}, {"location": {"col": 2, "file": 7, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 38, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 15, "file": 25, "line": 3897}, "message": "Taking false branch"}, {"location": {"col": 28, "file": 26, "line": 571}, "message": "expanded from macro 'rcu_dereference'"}, {"location": {"col": 2, "file": 26, "line": 513}, "message": "expanded from macro 'rcu_dereference_check'"}, {"location": {"col": 48, "file": 26, "line": 374}, "message": "expanded from macro '__rcu_dereference_check'"}, {"location": {"col": 2, "file": 0, "line": 2034}, "message": "2nd function call argument is an uninitialized value"}], "macros": [], "notes": [], "path": "/src/include/linux/skbuff.h", "reportHash": "560da736fc6571dfe7d308f653e7a193", "checkerName": "clang-analyzer-core.CallAndMessage", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 34, "file": 27, "line": 4399}, "message": "Left side of '&&' is false"}, {"location": {"col": 2, "file": 28, "line": 637}, "message": "expanded from macro 'from_tasklet'"}, {"location": {"col": 61, "file": 14, "line": 709}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 34, "file": 27, "line": 4399}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 28, "line": 637}, "message": "expanded from macro 'from_tasklet'"}, {"location": {"col": 2, "file": 14, "line": 709}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 37, "file": 19, "line": 39}, "message": "expanded from macro 'BUILD_BUG_ON_MSG'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 11, "line": 300}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 34, "file": 27, "line": 4399}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 28, "line": 637}, "message": "expanded from macro 'from_tasklet'"}, {"location": {"col": 2, "file": 14, "line": 709}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 37, "file": 19, "line": 39}, "message": "expanded from macro 'BUILD_BUG_ON_MSG'"}, {"location": {"col": 2, "file": 11, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 11, "line": 298}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 27, "line": 4407}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 29, "line": 384}, "message": "expanded from macro 'spin_lock_irqsave'"}, {"location": {"col": 2, "file": 29, "line": 250}, "message": "expanded from macro 'raw_spin_lock_irqsave'"}, {"location": {"col": 2, "file": 27, "line": 4407}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 43, "file": 29, "line": 382}, "message": "expanded from macro 'spin_lock_irqsave'"}, {"location": {"col": 14, "file": 27, "line": 4408}, "message": "Assuming 'i' is < field 'queues'"}, {"location": {"col": 2, "file": 27, "line": 4408}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 7, "file": 27, "line": 4413}, "message": "Assuming the condition is true"}, {"location": {"col": 36, "file": 27, "line": 4413}, "message": "Left side of '||' is true"}, {"location": {"col": 4, "file": 27, "line": 4415}, "message": "Execution continues on line 4408"}, {"location": {"col": 14, "file": 27, "line": 4408}, "message": "Assuming 'i' is < field 'queues'"}, {"location": {"col": 2, "file": 27, "line": 4408}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 7, "file": 27, "line": 4413}, "message": "Assuming the condition is false"}, {"location": {"col": 7, "file": 27, "line": 4413}, "message": "Left side of '||' is false"}, {"location": {"col": 7, "file": 27, "line": 4414}, "message": "Assuming the condition is true"}, {"location": {"col": 3, "file": 27, "line": 4413}, "message": "Taking true branch"}, {"location": {"col": 4, "file": 27, "line": 4415}, "message": "Execution continues on line 4408"}, {"location": {"col": 14, "file": 27, "line": 4408}, "message": "Assuming 'i' is < field 'queues'"}, {"location": {"col": 2, "file": 27, "line": 4408}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 7, "file": 27, "line": 4413}, "message": "Assuming the condition is false"}, {"location": {"col": 7, "file": 27, "line": 4413}, "message": "Left side of '||' is false"}, {"location": {"col": 7, "file": 27, "line": 4414}, "message": "Assuming the condition is false"}, {"location": {"col": 3, "file": 27, "line": 4413}, "message": "Taking false branch"}, {"location": {"col": 3, "file": 27, "line": 4417}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 16, "file": 27, "line": 4421}, "message": "Assuming field 'vif' is non-null"}, {"location": {"col": 25, "file": 3, "line": 119}, "message": "expanded from macro 'WARN_ON'"}, {"location": {"col": 8, "file": 27, "line": 4421}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 3, "line": 120}, "message": "expanded from macro 'WARN_ON'"}, {"location": {"col": 4, "file": 27, "line": 4421}, "message": "Taking false branch"}, {"location": {"col": 11, "file": 27, "line": 4429}, "message": "Calling 'ieee80211_tx_pending_skb'"}, {"location": {"col": 6, "file": 27, "line": 4361}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 27, "line": 4361}, "message": "Taking false branch"}, {"location": {"col": 13, "file": 27, "line": 4369}, "message": "Assuming the condition is false"}, {"location": {"col": 9, "file": 27, "line": 4369}, "message": "Taking false branch"}, {"location": {"col": 3, "file": 27, "line": 4382}, "message": "Calling '__skb_queue_head_init'"}, {"location": {"col": 3, "file": 27, "line": 4382}, "message": "Returning from '__skb_queue_head_init'"}, {"location": {"col": 3, "file": 27, "line": 4383}, "message": "Calling '__skb_queue_tail'"}, {"location": {"col": 2, "file": 0, "line": 2067}, "message": "Calling '__skb_queue_before'"}, {"location": {"col": 2, "file": 0, "line": 2034}, "message": "2nd function call argument is an uninitialized value"}, {"location": {"col": 2, "file": 0, "line": 2034}, "message": "2nd function call argument is an uninitialized value"}], "macros": [], "notes": [], "path": "/src/include/linux/skbuff.h", "reportHash": "560da736fc6571dfe7d308f653e7a193", "checkerName": "clang-analyzer-core.CallAndMessage", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
